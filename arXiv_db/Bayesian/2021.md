# 2021

## TOC

- [2021-01](#2021-01)
- [2021-02](#2021-02)
- [2021-03](#2021-03)
- [2021-04](#2021-04)
- [2021-05](#2021-05)
- [2021-06](#2021-06)
- [2021-07](#2021-07)
- [2021-08](#2021-08)
- [2021-09](#2021-09)
- [2021-10](#2021-10)
- [2021-11](#2021-11)
- [2021-12](#2021-12)

## 2021-01

<details>

<summary>2021-01-01 00:16:47 - A Two Stage Adaptive Metropolis Algorithm</summary>

- *Anirban Mondal, Kai Yin, Abhijit Mandal*

- `2101.00118v1` - [abs](http://arxiv.org/abs/2101.00118v1) - [pdf](http://arxiv.org/pdf/2101.00118v1)

> We propose a new sampling algorithm combining two quite powerful ideas in the Markov chain Monte Carlo literature -- adaptive Metropolis sampler and two-stage Metropolis-Hastings sampler. The proposed sampling method will be particularly very useful for high-dimensional posterior sampling in Bayesian models with expensive likelihoods. In the first stage of the proposed algorithm, an adaptive proposal is used based on the previously sampled states and the corresponding acceptance probability is computed based on an approximated inexpensive target density. The true expensive target density is evaluated while computing the second stage acceptance probability only if the proposal is accepted in the first stage. The adaptive nature of the algorithm guarantees faster convergence of the chain and very good mixing properties. On the other hand, the two-stage approach helps in rejecting the bad proposals in the inexpensive first stage, making the algorithm computationally efficient. As the proposals are dependent on the previous states the chain loses its Markov property, but we prove that it retains the desired ergodicity property. The performance of the proposed algorithm is compared with the existing algorithms in two simulated and two real data examples.

</details>

<details>

<summary>2021-01-01 02:04:29 - Sample Size Re-estimation Design in Phase II Dose Finding Study with Multiple Dose Groups: Frequentist and Bayesian Methods</summary>

- *Qingyang Liu, Guanyu Hu, Binqi Ye, Susan Wang, Yaoshi Wu*

- `2012.14589v2` - [abs](http://arxiv.org/abs/2012.14589v2) - [pdf](http://arxiv.org/pdf/2012.14589v2)

> Unblinded sample size re-estimation (SSR) is often planned in a clinical trial when there is large uncertainty about the true treatment effect. For Proof-of Concept (PoC) in a Phase II dose finding study, contrast test can be adopted to leverage information from all treatment groups. In this article, we propose two-stage SSR designs using frequentist conditional power and Bayesian posterior predictive power for both single and multiple contrast tests. The Bayesian SSR can be implemented under a wide range of prior settings to incorporate different prior knowledge. Taking the adaptivity into account, all type I errors of final analysis in this paper are rigorously protected. Simulation studies are carried out to demonstrate the advantages of unblinded SSR in multi-arm trials.

</details>

<details>

<summary>2021-01-01 14:59:15 - The Bayesian Method of Tensor Networks</summary>

- *Erdong Guo, David Draper*

- `2101.00245v1` - [abs](http://arxiv.org/abs/2101.00245v1) - [pdf](http://arxiv.org/pdf/2101.00245v1)

> Bayesian learning is a powerful learning framework which combines the external information of the data (background information) with the internal information (training data) in a logically consistent way in inference and prediction. By Bayes rule, the external information (prior distribution) and the internal information (training data likelihood) are combined coherently, and the posterior distribution and the posterior predictive (marginal) distribution obtained by Bayes rule summarize the total information needed in the inference and prediction, respectively. In this paper, we study the Bayesian framework of the Tensor Network from two perspective. First, we introduce the prior distribution to the weights in the Tensor Network and predict the labels of the new observations by the posterior predictive (marginal) distribution. Since the intractability of the parameter integral in the normalization constant computation, we approximate the posterior predictive distribution by Laplace approximation and obtain the out-product approximation of the hessian matrix of the posterior distribution of the Tensor Network model. Second, to estimate the parameters of the stationary mode, we propose a stable initialization trick to accelerate the inference process by which the Tensor Network can converge to the stationary path more efficiently and stably with gradient descent method. We verify our work on the MNIST, Phishing Website and Breast Cancer data set. We study the Bayesian properties of the Bayesian Tensor Network by visualizing the parameters of the model and the decision boundaries in the two dimensional synthetic data set. For a application purpose, our work can reduce the overfitting and improve the performance of normal Tensor Network model.

</details>

<details>

<summary>2021-01-01 18:33:15 - On Posterior Consistency of Bayesian Factor Models in High Dimensions</summary>

- *Yucong Ma, Jun S. Liu*

- `2006.01055v3` - [abs](http://arxiv.org/abs/2006.01055v3) - [pdf](http://arxiv.org/pdf/2006.01055v3)

> As a principled dimension reduction technique, factor models have been widely adopted in social science, economics, bioinformatics, and many other fields. However, in high-dimensional settings, conducting a 'correct' Bayesianfactor analysis can be subtle since it requires both a careful prescription of the prior distribution and a suitable computational strategy. In particular, we analyze the issues related to the attempt of being "noninformative" for elements of the factor loading matrix, especially for sparse Bayesian factor models in high dimensions, and propose solutions to them. We show here why adopting the orthogonal factor assumption is appropriate and can result in a consistent posterior inference of the loading matrix conditional on the true idiosyncratic variance and the allocation of nonzero elements in the true loading matrix. We also provide an efficient Gibbs sampler to conduct the full posterior inference based on the prior setup from Rockova and George (2016)and a uniform orthogonal factor assumption on the factor matrix.

</details>

<details>

<summary>2021-01-02 04:17:28 - Geometric ergodicity of Gibbs samplers for the Horseshoe and its regularized variants</summary>

- *Suman K. Bhattacharya, Kshitij Khare, Subhadip Pal*

- `2101.00366v1` - [abs](http://arxiv.org/abs/2101.00366v1) - [pdf](http://arxiv.org/pdf/2101.00366v1)

> The Horseshoe is a widely used and popular continuous shrinkage prior for high-dimensional Bayesian linear regression. Recently, regularized versions of the Horseshoe prior have also been introduced in the literature. Various Gibbs sampling Markov chains have been developed in the literature to generate approximate samples from the corresponding intractable posterior densities. Establishing geometric ergodicity of these Markov chains provides crucial technical justification for the accuracy of asymptotic standard errors for Markov chain based estimates of posterior quantities. In this paper, we establish geometric ergodicity for various Gibbs samplers corresponding to the Horseshoe prior and its regularized variants in the context of linear regression. First, we establish geometric ergodicity of a Gibbs sampler for the original Horseshoe posterior under strictly weaker conditions than existing analyses in the literature. Second, we consider the regularized Horseshoe prior introduced in Piironen and Vehtari (2017), and prove geometric ergodicity for a Gibbs sampling Markov chain to sample from the corresponding posterior without any truncation constraint on the global and local shrinkage parameters. Finally, we consider a variant of this regularized Horseshoe prior introduced in Nishimura and Suchard (2020), and again establish geometric ergodicity for a Gibbs sampling Markov chain to sample from the corresponding posterior.

</details>

<details>

<summary>2021-01-02 08:14:31 - An Elo-like System for Massive Multiplayer Competitions</summary>

- *Aram Ebtekar, Paul Liu*

- `2101.00400v1` - [abs](http://arxiv.org/abs/2101.00400v1) - [pdf](http://arxiv.org/pdf/2101.00400v1)

> Rating systems play an important role in competitive sports and games. They provide a measure of player skill, which incentivizes competitive performances and enables balanced match-ups. In this paper, we present a novel Bayesian rating system for contests with many participants. It is widely applicable to competition formats with discrete ranked matches, such as online programming competitions, obstacle courses races, and some video games. The simplicity of our system allows us to prove theoretical bounds on robustness and runtime. In addition, we show that the system aligns incentives: that is, a player who seeks to maximize their rating will never want to underperform. Experimentally, the rating system rivals or surpasses existing systems in prediction accuracy, and computes faster than existing systems by up to an order of magnitude.

</details>

<details>

<summary>2021-01-02 11:06:43 - COVID-19 spreading in financial networks: A semiparametric matrix regression model</summary>

- *Billio Monica, Casarin Roberto, Costola Michele, Iacopini Matteo*

- `2101.00422v1` - [abs](http://arxiv.org/abs/2101.00422v1) - [pdf](http://arxiv.org/pdf/2101.00422v1)

> Network models represent a useful tool to describe the complex set of financial relationships among heterogeneous firms in the system. In this paper, we propose a new semiparametric model for temporal multilayer causal networks with both intra- and inter-layer connectivity. A Bayesian model with a hierarchical mixture prior distribution is assumed to capture heterogeneity in the response of the network edges to a set of risk factors including the European COVID-19 cases. We measure the financial connectedness arising from the interactions between two layers defined by stock returns and volatilities. In the empirical analysis, we study the topology of the network before and after the spreading of the COVID-19 disease.

</details>

<details>

<summary>2021-01-02 16:28:40 - Neural Architecture Generator Optimization</summary>

- *Binxin Ru, Pedro Esperanca, Fabio Carlucci*

- `2004.01395v3` - [abs](http://arxiv.org/abs/2004.01395v3) - [pdf](http://arxiv.org/pdf/2004.01395v3)

> Neural Architecture Search (NAS) was first proposed to achieve state-of-the-art performance through the discovery of new architecture patterns, without human intervention. An over-reliance on expert knowledge in the search space design has however led to increased performance (local optima) without significant architectural breakthroughs, thus preventing truly novel solutions from being reached. In this work we 1) are the first to investigate casting NAS as a problem of finding the optimal network generator and 2) we propose a new, hierarchical and graph-based search space capable of representing an extremely large variety of network types, yet only requiring few continuous hyper-parameters. This greatly reduces the dimensionality of the problem, enabling the effective use of Bayesian Optimisation as a search strategy. At the same time, we expand the range of valid architectures, motivating a multi-objective learning approach. We demonstrate the effectiveness of this strategy on six benchmark datasets and show that our search space generates extremely lightweight yet highly competitive models.

</details>

<details>

<summary>2021-01-02 20:36:47 - Consumer Theory with Non-Parametric Taste Uncertainty and Individual Heterogeneity</summary>

- *Christopher Dobronyi, Christian Gouriéroux*

- `2010.13937v4` - [abs](http://arxiv.org/abs/2010.13937v4) - [pdf](http://arxiv.org/pdf/2010.13937v4)

> We introduce two models of non-parametric random utility for demand systems: the stochastic absolute risk aversion (SARA) model, and the stochastic safety-first (SSF) model. In each model, individual-level heterogeneity is characterized by a distribution $\pi\in\Pi$ of taste parameters, and heterogeneity across consumers is introduced using a distribution $F$ over the distributions in $\Pi$. Demand is non-separable and heterogeneity is infinite-dimensional. Both models admit corner solutions. We consider two frameworks for estimation: a Bayesian framework in which $F$ is known, and a hyperparametric (or empirical Bayesian) framework in which $F$ is a member of a known parametric family. Our methods are illustrated by an application to a large U.S. panel of scanner data on alcohol consumption.

</details>

<details>

<summary>2021-01-02 23:27:16 - Combining Geometric and Topological Information for Boundary Estimation</summary>

- *Hengrui Luo, Justin Strait*

- `1910.04778v3` - [abs](http://arxiv.org/abs/1910.04778v3) - [pdf](http://arxiv.org/pdf/1910.04778v3)

> A fundamental problem in computer vision is boundary estimation, where the goal is to delineate the boundary of objects in an image. In this paper, we propose a method which jointly incorporates geometric and topological information within an image to simultaneously estimate boundaries for objects within images with more complex topologies. We use a topological clustering-based method to assist initialization of the Bayesian active contour model. This combines pixel clustering, boundary smoothness, and potential prior shape information to produce an estimated object boundary. Active contour methods are knownto be extremely sensitive to algorithm initialization, relying on the user to provide a reasonable starting curve to the algorithm. In the presence of images featuring objects with complex topological structures, such as objects with holes or multiple objects, the user must initialize separate curves for each boundary of interest. Our proposed topologically-guided method can provide an interpretable, smart initialization in these settings, freeing up the user from potential pitfalls associated with objects of complex topological structure. We provide a detailed simulation study comparing our initialization to boundary estimates obtained from standard segmentation algorithms. The method is demonstrated on artificial image datasets from computer vision, as well as real-world applications to skin lesion and neural cellular images, for which multiple topological features can be identified.

</details>

<details>

<summary>2021-01-03 01:03:43 - Bayesian Pool-based Active Learning With Abstention Feedbacks</summary>

- *Cuong V. Nguyen, Lam Si Tung Ho, Huan Xu, Vu Dinh, Binh Nguyen*

- `1705.08481v3` - [abs](http://arxiv.org/abs/1705.08481v3) - [pdf](http://arxiv.org/pdf/1705.08481v3)

> We study pool-based active learning with abstention feedbacks, where a labeler can abstain from labeling a queried example with some unknown abstention rate. This is an important problem with many useful applications. We take a Bayesian approach to the problem and develop two new greedy algorithms that learn both the classification problem and the unknown abstention rate at the same time. These are achieved by simply incorporating the estimated abstention rate into the greedy criteria. We prove that both of our algorithms have near-optimality guarantees: they respectively achieve a ${(1-\frac{1}{e})}$ constant factor approximation of the optimal expected or worst-case value of a useful utility function. Our experiments show the algorithms perform well in various practical scenarios.

</details>

<details>

<summary>2021-01-03 16:56:46 - Tutorial on Variational Autoencoders</summary>

- *Carl Doersch*

- `1606.05908v3` - [abs](http://arxiv.org/abs/1606.05908v3) - [pdf](http://arxiv.org/pdf/1606.05908v3)

> In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.

</details>

<details>

<summary>2021-01-03 17:38:08 - Learning optimal Bayesian prior probabilities from data</summary>

- *Ozan Kaan Kayaalp*

- `2101.00672v1` - [abs](http://arxiv.org/abs/2101.00672v1) - [pdf](http://arxiv.org/pdf/2101.00672v1)

> Noninformative uniform priors are staples of Bayesian inference, especially in Bayesian machine learning. This study challenges the assumption that they are optimal and their use in Bayesian inference yields optimal outcomes. Instead of using arbitrary noninformative uniform priors, we propose a machine learning based alternative method, learning optimal priors from data by maximizing a target function of interest. Applying na\"ive Bayes text classification methodology and a search algorithm developed for this study, our system learned priors from data using the positive predictive value metric as the target function. The task was to find Wikipedia articles that had not (but should have) been categorized under certain Wikipedia categories. We conducted five sets of experiments using separate Wikipedia categories. While the baseline models used the popular Bayes-Laplace priors, the study models learned the optimal priors for each set of experiments separately before using them. The results showed that the study models consistently outperformed the baseline models with a wide margin of statistical significance (p < 0.001). The measured performance improvement of the study model over the baseline was as high as 443% with the mean value of 193% over five Wikipedia categories.

</details>

<details>

<summary>2021-01-04 16:13:24 - Information Aware Max-Norm Dirichlet Networks for Predictive Uncertainty Estimation</summary>

- *Theodoros Tsiligkaridis*

- `1910.04819v4` - [abs](http://arxiv.org/abs/1910.04819v4) - [pdf](http://arxiv.org/pdf/1910.04819v4)

> Precise estimation of uncertainty in predictions for AI systems is a critical factor in ensuring trust and safety. Deep neural networks trained with a conventional method are prone to over-confident predictions. In contrast to Bayesian neural networks that learn approximate distributions on weights to infer prediction confidence, we propose a novel method, Information Aware Dirichlet networks, that learn an explicit Dirichlet prior distribution on predictive distributions by minimizing a bound on the expected max norm of the prediction error and penalizing information associated with incorrect outcomes. Properties of the new cost function are derived to indicate how improved uncertainty estimation is achieved. Experiments using real datasets show that our technique outperforms, by a large margin, state-of-the-art neural networks for estimating within-distribution and out-of-distribution uncertainty, and detecting adversarial examples.

</details>

<details>

<summary>2021-01-05 13:42:50 - Structured Machine Learning Tools for Modelling Characteristics of Guided Waves</summary>

- *Marcus Haywood-Alexander, Nikolaos Dervilis, Keith Worden, Elizabeth J. Cross, Robin S. Mills, Timothy J. Rogers*

- `2101.01506v1` - [abs](http://arxiv.org/abs/2101.01506v1) - [pdf](http://arxiv.org/pdf/2101.01506v1)

> The use of ultrasonic guided waves to probe the materials/structures for damage continues to increase in popularity for non-destructive evaluation (NDE) and structural health monitoring (SHM). The use of high-frequency waves such as these offers an advantage over low-frequency methods from their ability to detect damage on a smaller scale. However, in order to assess damage in a structure, and implement any NDE or SHM tool, knowledge of the behaviour of a guided wave throughout the material/structure is important (especially when designing sensor placement for SHM systems). Determining this behaviour is extremely diffcult in complex materials, such as fibre-matrix composites, where unique phenomena such as continuous mode conversion takes place. This paper introduces a novel method for modelling the feature-space of guided waves in a composite material. This technique is based on a data-driven model, where prior physical knowledge can be used to create structured machine learning tools; where constraints are applied to provide said structure. The method shown makes use of Gaussian processes, a full Bayesian analysis tool, and in this paper it is shown how physical knowledge of the guided waves can be utilised in modelling using an ML tool. This paper shows that through careful consideration when applying machine learning techniques, more robust models can be generated which offer advantages such as extrapolation ability and physical interpretation.

</details>

<details>

<summary>2021-01-05 14:10:34 - A probabilistic risk-based decision framework for structural health monitoring</summary>

- *Aidan J. Hughes, Robert J. Barthorpe, N. Dervilis, Charles R. Farrar, Keith Worden*

- `2101.01521v1` - [abs](http://arxiv.org/abs/2101.01521v1) - [pdf](http://arxiv.org/pdf/2101.01521v1)

> Obtaining the ability to make informed decisions regarding the operation and maintenance of structures, provides a major incentive for the implementation of structural health monitoring (SHM) systems. Probabilistic risk assessment (PRA) is an established methodology that allows engineers to make risk-informed decisions regarding the design and operation of safety-critical and high-value assets in industries such as nuclear and aerospace. The current paper aims to formulate a risk-based decision framework for structural health monitoring that combines elements of PRA with the existing SHM paradigm. As an apt tool for reasoning and decision-making under uncertainty, probabilistic graphical models serve as the foundation of the framework. The framework involves modelling failure modes of structures as Bayesian network representations of fault trees and then assigning costs or utilities to the failure events. The fault trees allow for information to pass from probabilistic classifiers to influence diagram representations of decision processes whilst also providing nodes within the graphical model that may be queried to obtain marginal probability distributions over local damage states within a structure. Optimal courses of action for structures are selected by determining the strategies that maximise expected utility. The risk-based framework is demonstrated on a realistic truss-like structure and supported by experimental data. Finally, a discussion of the risk-based approach is made and further challenges pertaining to decision-making processes in the context of SHM are identified.

</details>

<details>

<summary>2021-01-05 14:35:36 - Combining Cox Regressions Across a Heterogeneous Distributed Research Network Facing Small and Zero Counts</summary>

- *Martijn J. Schuemie, Yong Chen, David Madigan, Marc A. Suchard*

- `2101.01551v1` - [abs](http://arxiv.org/abs/2101.01551v1) - [pdf](http://arxiv.org/pdf/2101.01551v1)

> Studies of the effects of medical interventions increasingly take place in distributed research settings using data from multiple clinical data sources including electronic health records and administrative claims. In such settings, privacy concerns typically prohibit sharing of individual patient data, and instead, analyses can only utilize summary statistics from the individual databases. In the specific but very common context of the Cox proportional hazards model, we show that standard meta analysis methods then lead to substantial bias when outcome counts are small. This bias derives primarily from the normal approximations that the methods utilize. Here we propose and evaluate methods that eschew normal approximations in favor of three more flexible approximations: a skew-normal, a one-dimensional grid, and a custom parametric function that mimics the behavior of the Cox likelihood function. In extensive simulation studies we demonstrate how these approximations impact bias in the context of both fixed-effects and (Bayesian) random-effects models. We then apply these approaches to three real-world studies of the comparative safety of antidepressants, each using data from four observational healthcare databases.

</details>

<details>

<summary>2021-01-05 17:06:56 - Data-efficient Domain Randomization with Bayesian Optimization</summary>

- *Fabio Muratore, Christian Eilers, Michael Gienger, Jan Peters*

- `2003.02471v4` - [abs](http://arxiv.org/abs/2003.02471v4) - [pdf](http://arxiv.org/pdf/2003.02471v4)

> When learning policies for robot control, the required real-world data is typically prohibitively expensive to acquire, so learning in simulation is a popular strategy. Unfortunately, such polices are often not transferable to the real world due to a mismatch between the simulation and reality, called 'reality gap'. Domain randomization methods tackle this problem by randomizing the physics simulator (source domain) during training according to a distribution over domain parameters in order to obtain more robust policies that are able to overcome the reality gap. Most domain randomization approaches sample the domain parameters from a fixed distribution. This solution is suboptimal in the context of sim-to-real transferability, since it yields policies that have been trained without explicitly optimizing for the reward on the real system (target domain). Additionally, a fixed distribution assumes there is prior knowledge about the uncertainty over the domain parameters. In this paper, we propose Bayesian Domain Randomization (BayRn), a black-box sim-to-real algorithm that solves tasks efficiently by adapting the domain parameter distribution during learning given sparse data from the real-world target domain. BayRn uses Bayesian optimization to search the space of source domain distribution parameters such that this leads to a policy which maximizes the real-word objective, allowing for adaptive distributions during policy optimization. We experimentally validate the proposed approach in sim-to-sim as well as in sim-to-real experiments, comparing against three baseline methods on two robotic tasks. Our results show that BayRn is able to perform sim-to-real transfer, while significantly reducing the required prior knowledge.

</details>

<details>

<summary>2021-01-06 03:14:13 - A Survey on Bayesian Deep Learning</summary>

- *Hao Wang, Dit-Yan Yeung*

- `1604.01662v4` - [abs](http://arxiv.org/abs/1604.01662v4) - [pdf](http://arxiv.org/pdf/1604.01662v4)

> A comprehensive artificial intelligence system needs to not only perceive the environment with different `senses' (e.g., seeing and hearing) but also infer the world's conditional (or even causal) relations and corresponding uncertainty. The past decade has seen major advances in many perception tasks such as visual object recognition and speech recognition using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. In recent years, Bayesian deep learning has emerged as a unified probabilistic framework to tightly integrate deep learning and Bayesian models. In this general framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in turn, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a comprehensive introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, control, etc. Besides, we also discuss the relationship and differences between Bayesian deep learning and other related topics such as Bayesian treatment of neural networks. For a constantly updating project page, please refer to https://github.com/js05212/BayesianDeepLearning-Survey.

</details>

<details>

<summary>2021-01-06 03:44:39 - Meta-Learning Bandit Policies by Gradient Ascent</summary>

- *Branislav Kveton, Martin Mladenov, Chih-Wei Hsu, Manzil Zaheer, Csaba Szepesvari, Craig Boutilier*

- `2006.05094v2` - [abs](http://arxiv.org/abs/2006.05094v2) - [pdf](http://arxiv.org/pdf/2006.05094v2)

> Most bandit policies are designed to either minimize regret in any problem instance, making very few assumptions about the underlying environment, or in a Bayesian sense, assuming a prior distribution over environment parameters. The former are often too conservative in practical settings, while the latter require assumptions that are hard to verify in practice. We study bandit problems that fall between these two extremes, where the learning agent has access to sampled bandit instances from an unknown prior distribution $\mathcal{P}$ and aims to achieve high reward on average over the bandit instances drawn from $\mathcal{P}$. This setting is of a particular importance because it lays foundations for meta-learning of bandit policies and reflects more realistic assumptions in many practical domains. We propose the use of parameterized bandit policies that are differentiable and can be optimized using policy gradients. This provides a broadly applicable framework that is easy to implement. We derive reward gradients that reflect the structure of bandit problems and policies, for both non-contextual and contextual settings, and propose a number of interesting policies that are both differentiable and have low regret. Our algorithmic and theoretical contributions are supported by extensive experiments that show the importance of baseline subtraction, learned biases, and the practicality of our approach on a range problems.

</details>

<details>

<summary>2021-01-06 10:07:56 - Incorporating Interpretable Output Constraints in Bayesian Neural Networks</summary>

- *Wanqian Yang, Lars Lorch, Moritz A. Graule, Himabindu Lakkaraju, Finale Doshi-Velez*

- `2010.10969v2` - [abs](http://arxiv.org/abs/2010.10969v2) - [pdf](http://arxiv.org/pdf/2010.10969v2)

> Domains where supervised models are deployed often come with task-specific constraints, such as prior expert knowledge on the ground-truth function, or desiderata like safety and fairness. We introduce a novel probabilistic framework for reasoning with such constraints and formulate a prior that enables us to effectively incorporate them into Bayesian neural networks (BNNs), including a variant that can be amortized over tasks. The resulting Output-Constrained BNN (OC-BNN) is fully consistent with the Bayesian framework for uncertainty quantification and is amenable to black-box inference. Unlike typical BNN inference in uninterpretable parameter space, OC-BNNs widen the range of functional knowledge that can be incorporated, especially for model users without expertise in machine learning. We demonstrate the efficacy of OC-BNNs on real-world datasets, spanning multiple domains such as healthcare, criminal justice, and credit scoring.

</details>

<details>

<summary>2021-01-06 17:45:06 - A Survey of Constrained Gaussian Process Regression: Approaches and Implementation Challenges</summary>

- *Laura Swiler, Mamikon Gulian, Ari Frankel, Cosmin Safta, John Jakeman*

- `2006.09319v3` - [abs](http://arxiv.org/abs/2006.09319v3) - [pdf](http://arxiv.org/pdf/2006.09319v3)

> Gaussian process regression is a popular Bayesian framework for surrogate modeling of expensive data sources. As part of a broader effort in scientific machine learning, many recent works have incorporated physical constraints or other a priori information within Gaussian process regression to supplement limited data and regularize the behavior of the model. We provide an overview and survey of several classes of Gaussian process constraints, including positivity or bound constraints, monotonicity and convexity constraints, differential equation constraints provided by linear PDEs, and boundary condition constraints. We compare the strategies behind each approach as well as the differences in implementation, concluding with a discussion of the computational challenges introduced by constraints.

</details>

<details>

<summary>2021-01-06 19:11:58 - Tractable Bayes of Skew-Elliptical Link Models for Correlated Binary Data</summary>

- *Zhongwei Zhang, Reinaldo B. Arellano-Valle, Marc G. Genton, Raphaël Huser*

- `2101.02233v1` - [abs](http://arxiv.org/abs/2101.02233v1) - [pdf](http://arxiv.org/pdf/2101.02233v1)

> Correlated binary response data with covariates are ubiquitous in longitudinal or spatial studies. Among the existing statistical models the most well-known one for this type of data is the multivariate probit model, which uses a Gaussian link to model dependence at the latent level. However, a symmetric link may not be appropriate if the data are highly imbalanced. Here, we propose a multivariate skew-elliptical link model for correlated binary responses, which includes the multivariate probit model as a special case. Furthermore, we perform Bayesian inference for this new model and prove that the regression coefficients have a closed-form unified skew-elliptical posterior. The new methodology is illustrated by application to COVID-19 pandemic data from three different counties of the state of California, USA. By jointly modeling extreme spikes in weekly new cases, our results show that the spatial dependence cannot be neglected. Furthermore, the results also show that the skewed latent structure of our proposed model improves the flexibility of the multivariate probit model and provides better fit to our highly imbalanced dataset.

</details>

<details>

<summary>2021-01-06 22:07:19 - Hyperboost: Hyperparameter Optimization by Gradient Boosting surrogate models</summary>

- *Jeroen van Hoof, Joaquin Vanschoren*

- `2101.02289v1` - [abs](http://arxiv.org/abs/2101.02289v1) - [pdf](http://arxiv.org/pdf/2101.02289v1)

> Bayesian Optimization is a popular tool for tuning algorithms in automatic machine learning (AutoML) systems. Current state-of-the-art methods leverage Random Forests or Gaussian processes to build a surrogate model that predicts algorithm performance given a certain set of hyperparameter settings. In this paper, we propose a new surrogate model based on gradient boosting, where we use quantile regression to provide optimistic estimates of the performance of an unobserved hyperparameter setting, and combine this with a distance metric between unobserved and observed hyperparameter settings to help regulate exploration. We demonstrate empirically that the new method is able to outperform some state-of-the art techniques across a reasonable sized set of classification problems.

</details>

<details>

<summary>2021-01-06 23:19:47 - A Bayesian Mixture Modelling of Stop Signal Reaction Time Distributions</summary>

- *Mohsen Soltanifar, Michael Escobar, Annie Dupuis, Russell Schachar*

- `2010.12705v2` - [abs](http://arxiv.org/abs/2010.12705v2) - [pdf](http://arxiv.org/pdf/2010.12705v2)

> The distribution of single Stop Signal Reaction Times (SSRT) in the stop signal task (SST) as a measurement of the latency of the unobservable stopping process has been modeled with a nonparametric method by Hans Colonius (1990) and with a Bayesian parametric method by Eric-Jan Wagenmakers and colleagues (2012). These methods assume equal impact of the preceding trial type (go/stop) in the SST trials on the SSRT distributional estimation without addressing the case of the violated assumption. This study presents the required model by considering two-state mixture model for the SSRT distribution. It then compares the Bayesian parametric single SSRT and mixture SSRT distributions in the usual stochastic order at the individual and the population level under the ex-Gaussian distributional format. It shows that compared to a single SSRT distribution, the mixture SSRT distribution is more diverse, more positively skewed, more leptokurtic, and larger in stochastic order. The size of the disparities in the results also depends on the choice of weights in the mixture SSRT distribution. This study confirms that mixture SSRT indices as a constant or distribution are significantly larger than their single SSRT counterparts in the related order. This offers a vital improvement in the SSRT estimations.

</details>

<details>

<summary>2021-01-07 02:29:15 - Infinitely Wide Tensor Networks as Gaussian Process</summary>

- *Erdong Guo, David Draper*

- `2101.02333v1` - [abs](http://arxiv.org/abs/2101.02333v1) - [pdf](http://arxiv.org/pdf/2101.02333v1)

> Gaussian Process is a non-parametric prior which can be understood as a distribution on the function space intuitively. It is known that by introducing appropriate prior to the weights of the neural networks, Gaussian Process can be obtained by taking the infinite-width limit of the Bayesian neural networks from a Bayesian perspective. In this paper, we explore the infinitely wide Tensor Networks and show the equivalence of the infinitely wide Tensor Networks and the Gaussian Process. We study the pure Tensor Network and another two extended Tensor Network structures: Neural Kernel Tensor Network and Tensor Network hidden layer Neural Network and prove that each one will converge to the Gaussian Process as the width of each model goes to infinity. (We note here that Gaussian Process can also be obtained by taking the infinite limit of at least one of the bond dimensions $\alpha_{i}$ in the product of tensor nodes, and the proofs can be done with the same ideas in the proofs of the infinite-width cases.) We calculate the mean function (mean vector) and the covariance function (covariance matrix) of the finite dimensional distribution of the induced Gaussian Process by the infinite-width tensor network with a general set-up. We study the properties of the covariance function and derive the approximation of the covariance function when the integral in the expectation operator is intractable. In the numerical experiments, we implement the Gaussian Process corresponding to the infinite limit tensor networks and plot the sample paths of these models. We study the hyperparameters and plot the sample path families in the induced Gaussian Process by varying the standard deviations of the prior distributions. As expected, the parameters in the prior distribution namely the hyper-parameters in the induced Gaussian Process controls the characteristic lengthscales of the Gaussian Process.

</details>

<details>

<summary>2021-01-07 14:55:29 - Informative Bayesian Neural Network Priors for Weak Signals</summary>

- *Tianyu Cui, Aki Havulinna, Pekka Marttinen, Samuel Kaski*

- `2002.10243v2` - [abs](http://arxiv.org/abs/2002.10243v2) - [pdf](http://arxiv.org/pdf/2002.10243v2)

> Encoding domain knowledge into the prior over the high-dimensional weight space of a neural network is challenging but essential in applications with limited data and weak signals. Two types of domain knowledge are commonly available in scientific applications: 1. feature sparsity (fraction of features deemed relevant); 2. signal-to-noise ratio, quantified, for instance, as the proportion of variance explained (PVE). We show how to encode both types of domain knowledge into the widely used Gaussian scale mixture priors with Automatic Relevance Determination. Specifically, we propose a new joint prior over the local (i.e., feature-specific) scale parameters that encodes knowledge about feature sparsity, and a Stein gradient optimization to tune the hyperparameters in such a way that the distribution induced on the model's PVE matches the prior distribution. We show empirically that the new prior improves prediction accuracy, compared to existing neural network priors, on several publicly available datasets and in a genetics application where signals are weak and sparse, often outperforming even computationally intensive cross-validation for hyperparameter tuning.

</details>

<details>

<summary>2021-01-07 16:16:54 - CompModels: A suite of computer model test functions for Bayesian optimization</summary>

- *Tony Pourmohamad*

- `2011.10589v2` - [abs](http://arxiv.org/abs/2011.10589v2) - [pdf](http://arxiv.org/pdf/2011.10589v2)

> The CompModels package for R provides a suite of computer model test functions that can be used for computer model prediction/emulation, uncertainty quantification, and calibration, but in particular, the sequential optimization of computer models. The package is a mix of real-world physics problems, known mathematical functions, and black-box functions that have been converted into computer models with the goal of Bayesian (i.e., sequential) optimization in mind. Likewise, the package contains computer models that represent either the constrained or unconstrained optimization case, each with varying levels of difficulty. In this paper, we illustrate the use of the package with both real-world examples and black-box functions by solving constrained optimization problems via Bayesian optimization. Ultimately, the package is shown to provide users with a source of computer model test functions that are reproducible, shareable, and that can be used for benchmarking of novel optimization methods.

</details>

<details>

<summary>2021-01-07 18:51:05 - The Effect of Prior Lipschitz Continuity on the Adversarial Robustness of Bayesian Neural Networks</summary>

- *Arno Blaas, Stephen J. Roberts*

- `2101.02689v1` - [abs](http://arxiv.org/abs/2101.02689v1) - [pdf](http://arxiv.org/pdf/2101.02689v1)

> It is desirable, and often a necessity, for machine learning models to be robust against adversarial attacks. This is particularly true for Bayesian models, as they are well-suited for safety-critical applications, in which adversarial attacks can have catastrophic outcomes. In this work, we take a deeper look at the adversarial robustness of Bayesian Neural Networks (BNNs). In particular, we consider whether the adversarial robustness of a BNN can be increased by model choices, particularly the Lipschitz continuity induced by the prior. Conducting in-depth analysis on the case of i.i.d., zero-mean Gaussian priors and posteriors approximated via mean-field variational inference, we find evidence that adversarial robustness is indeed sensitive to the prior variance.

</details>

<details>

<summary>2021-01-07 19:12:06 - A Novel Regression Loss for Non-Parametric Uncertainty Optimization</summary>

- *Joachim Sicking, Maram Akila, Maximilian Pintz, Tim Wirtz, Asja Fischer, Stefan Wrobel*

- `2101.02726v1` - [abs](http://arxiv.org/abs/2101.02726v1) - [pdf](http://arxiv.org/pdf/2101.02726v1)

> Quantification of uncertainty is one of the most promising approaches to establish safe machine learning. Despite its importance, it is far from being generally solved, especially for neural networks. One of the most commonly used approaches so far is Monte Carlo dropout, which is computationally cheap and easy to apply in practice. However, it can underestimate the uncertainty. We propose a new objective, referred to as second-moment loss (SML), to address this issue. While the full network is encouraged to model the mean, the dropout networks are explicitly used to optimize the model variance. We intensively study the performance of the new objective on various UCI regression datasets. Comparing to the state-of-the-art of deep ensembles, SML leads to comparable prediction accuracies and uncertainty estimates while only requiring a single model. Under distribution shift, we observe moderate improvements. As a side result, we introduce an intuitive Wasserstein distance-based uncertainty measure that is non-saturating and thus allows to resolve quality differences between any two uncertainty estimates.

</details>

<details>

<summary>2021-01-07 22:21:50 - Referenced Thermodynamic Integration for Bayesian Model Selection: Application to COVID-19 Model Selection</summary>

- *Iwona Hawryluk, Swapnil Mishra, Seth Flaxman, Samir Bhatt, Thomas A. Mellan*

- `2009.03851v3` - [abs](http://arxiv.org/abs/2009.03851v3) - [pdf](http://arxiv.org/pdf/2009.03851v3)

> Model selection is a fundamental part of the applied Bayesian statistical methodology. Metrics such as the Akaike Information Criterion are commonly used in practice to select models but do not incorporate the uncertainty of the models' parameters and can give misleading choices. One approach that uses the full posterior distribution is to compute the ratio of two models' normalising constants, known as the Bayes factor. Often in realistic problems, this involves the integration of analytically intractable, high-dimensional distributions, and therefore requires the use of stochastic methods such as thermodynamic integration (TI). In this paper we apply a variation of the TI method, referred to as referenced TI, which computes a single model's normalising constant in an efficient way by using a judiciously chosen reference density. The advantages of the approach and theoretical considerations are set out, along with explicit pedagogical 1 and 2D examples. Benchmarking is presented with comparable methods and we find favourable convergence performance. The approach is shown to be useful in practice when applied to a real problem - to perform model selection for a semi-mechanistic hierarchical Bayesian model of COVID-19 transmission in South Korea involving the integration of a 200D density.

</details>

<details>

<summary>2021-01-08 10:06:49 - Simultaneous inference of periods and period-luminosity relations for Mira variable stars</summary>

- *Shiyuan He, Zhenfeng Lin, Wenlong Yuan, Lucas M. Macri, Jianhua Z. Huang*

- `2101.02938v1` - [abs](http://arxiv.org/abs/2101.02938v1) - [pdf](http://arxiv.org/pdf/2101.02938v1)

> The Period--Luminosity relation (PLR) of Mira variable stars is an important tool to determine astronomical distances. The common approach of estimating the PLR is a two-step procedure that first estimates the Mira periods and then runs a linear regression of magnitude on log period. When the light curves are sparse and noisy, the accuracy of period estimation decreases and can suffer from aliasing effects. Some methods improve accuracy by incorporating complex model structures at the expense of significant computational costs. Another drawback of existing methods is that they only provide point estimation without proper estimation of uncertainty. To overcome these challenges, we develop a hierarchical Bayesian model that simultaneously models the quasi-periodic variations for a collection of Mira light curves while estimating their common PLR. By borrowing strengths through the PLR, our method automatically reduces the aliasing effect, improves the accuracy of period estimation, and is capable of characterizing the estimation uncertainty. We develop a scalable stochastic variational inference algorithm for computation that can effectively deal with the multimodal posterior of period. The effectiveness of the proposed method is demonstrated through simulations, and an application to observations of Miras in the Local Group galaxy M33. Without using ad-hoc period correction tricks, our method achieves a distance estimate of M33 that is consistent with published work. Our method also shows superior robustness to downsampling of the light curves.

</details>

<details>

<summary>2021-01-08 10:33:23 - Lessons Learned from the Bayesian Design and Analysis for the BNT162b2 COVID-19 Vaccine Phase 3 Trial</summary>

- *Yuan Ji, Shijie Yuan*

- `2103.05499v1` - [abs](http://arxiv.org/abs/2103.05499v1) - [pdf](http://arxiv.org/pdf/2103.05499v1)

> The phase III BNT162b2 mRNA COVID-19 vaccine trial is based on a Bayesian design and analysis, and the main evidence of vaccine efficacy is presented in Bayesian statistics. Confusion and mistakes are produced in the presentation of the Bayesian results. Some key statistics, such as Bayesian credible intervals, are mislabeled and stated as confidence intervals. Posterior probabilities of the vaccine efficacy are not reported as the main results. We illustrate the main differences in the reporting of Bayesian analysis results for a clinical trial and provide four recommendations. We argue that statistical evidence from a Bayesian trial, when presented properly, is easier to interpret and directly addresses the main clinical questions, thereby better supporting regulatory decision making. We also recommend using abbreviation "BI" to represent Bayesian credible intervals as a differentiation to "CI" which stands for confidence interval.

</details>

<details>

<summary>2021-01-08 11:46:11 - Bayesian optimization with improved scalability and derivative information for efficient design of nanophotonic structures</summary>

- *Xavier Garcia-Santiago, Sven Burger, Carsten Rockstuhl, Philipp-Immanuel Schneider*

- `2101.02972v1` - [abs](http://arxiv.org/abs/2101.02972v1) - [pdf](http://arxiv.org/pdf/2101.02972v1)

> We propose the combination of forward shape derivatives and the use of an iterative inversion scheme for Bayesian optimization to find optimal designs of nanophotonic devices. This approach widens the range of applicability of Bayesian optmization to situations where a larger number of iterations is required and where derivative information is available. This was previously impractical because the computational efforts required to identify the next evaluation point in the parameter space became much larger than the actual evaluation of the objective function. We demonstrate an implementation of the method by optimizing a waveguide edge coupler.

</details>

<details>

<summary>2021-01-08 13:02:20 - Gene-gene interaction analysis incorporating network information via a structured Bayesian approach</summary>

- *Xing Qin, Shuangge Ma, Mengyun Wu*

- `2010.10960v2` - [abs](http://arxiv.org/abs/2010.10960v2) - [pdf](http://arxiv.org/pdf/2010.10960v2)

> Increasing evidence has shown that gene-gene interactions have important effects on biological processes of human diseases. Due to the high dimensionality of genetic measurements, existing interaction analysis methods usually suffer from a lack of sufficient information and are still unsatisfactory. Biological networks have been massively accumulated, allowing researchers to identify biomarkers from a system perspective by utilizing network selection (consisting of functionally related biomarkers) as well as network structures. In the main-effect analysis, network information has been widely incorporated, leading to biologically more meaningful and more accurate estimates. However, there is still a big gap in the context of interaction analysis. In this study, we develop a novel structured Bayesian interaction analysis approach, effectively incorporating the network information. This study is among the first to identify gene-gene interactions with the assistance of network selection for phenotype prediction, while simultaneously accommodating the underlying network structures. It innovatively respects the multiple hierarchies among main effects, interactions, and networks. Bayesian method is adopted, which has been shown to have multiple advantages over some other techniques. An efficient variational inference algorithm is developed to explore the posterior distribution. Extensive simulation studies demonstrate the practical superiority of the proposed approach. The analysis of TCGA data on melanoma and lung cancer leads to biologically sensible findings with satisfactory prediction accuracy and selection stability.

</details>

<details>

<summary>2021-01-08 15:45:00 - Confidence sequences for sampling without replacement</summary>

- *Ian Waudby-Smith, Aaditya Ramdas*

- `2006.04347v4` - [abs](http://arxiv.org/abs/2006.04347v4) - [pdf](http://arxiv.org/pdf/2006.04347v4)

> Many practical tasks involve sampling sequentially without replacement (WoR) from a finite population of size $N$, in an attempt to estimate some parameter $\theta^\star$. Accurately quantifying uncertainty throughout this process is a nontrivial task, but is necessary because it often determines when we stop collecting samples and confidently report a result. We present a suite of tools for designing confidence sequences (CS) for $\theta^\star$. A CS is a sequence of confidence sets $(C_n)_{n=1}^N$, that shrink in size, and all contain $\theta^\star$ simultaneously with high probability. We present a generic approach to constructing a frequentist CS using Bayesian tools, based on the fact that the ratio of a prior to the posterior at the ground truth is a martingale. We then present Hoeffding- and empirical-Bernstein-type time-uniform CSs and fixed-time confidence intervals for sampling WoR, which improve on previous bounds in the literature and explicitly quantify the benefit of WoR sampling.

</details>

<details>

<summary>2021-01-08 16:52:30 - Variational Determinant Estimation with Spherical Normalizing Flows</summary>

- *Simon Passenheim, Emiel Hoogeboom*

- `2012.13311v3` - [abs](http://arxiv.org/abs/2012.13311v3) - [pdf](http://arxiv.org/pdf/2012.13311v3)

> This paper introduces the Variational Determinant Estimator (VDE), a variational extension of the recently proposed determinant estimator discovered by arXiv:2005.06553v2. Our estimator significantly reduces the variance even for low sample sizes by combining (importance-weighted) variational inference and a family of normalizing flows which allow density estimation on hyperspheres. In the ideal case of a tight variational bound, the VDE becomes a zero variance estimator, and a single sample is sufficient for an exact (log) determinant estimate.

</details>

<details>

<summary>2021-01-08 18:33:17 - Invertible DenseNets</summary>

- *Yura Perugachi-Diaz, Jakub M. Tomczak, Sandjai Bhulai*

- `2010.02125v3` - [abs](http://arxiv.org/abs/2010.02125v3) - [pdf](http://arxiv.org/pdf/2010.02125v3)

> We introduce Invertible Dense Networks (i-DenseNets), a more parameter efficient alternative to Residual Flows. The method relies on an analysis of the Lipschitz continuity of the concatenation in DenseNets, where we enforce the invertibility of the network by satisfying the Lipschitz constraint. Additionally, we extend this method by proposing a learnable concatenation, which not only improves the model performance but also indicates the importance of the concatenated representation. We demonstrate the performance of i-DenseNets and Residual Flows on toy, MNIST, and CIFAR10 data. Both i-DenseNets outperform Residual Flows evaluated in negative log-likelihood, on all considered datasets under an equal parameter budget.

</details>

<details>

<summary>2021-01-08 20:00:35 - Unforeseen Evidence</summary>

- *Evan Piermont*

- `1907.07019v3` - [abs](http://arxiv.org/abs/1907.07019v3) - [pdf](http://arxiv.org/pdf/1907.07019v3)

> I propose a normative updating rule, extended Bayesianism, for the incorporation of probabilistic information arising from the process of becoming more aware. Extended Bayesianism generalizes standard Bayesian updating to allow the posterior to reside on richer probability space than the prior. I then provide an observable criterion on prior and posterior beliefs such that they were consistent with extended Bayesianism.

</details>

<details>

<summary>2021-01-08 23:49:43 - Bayesian Consensus: Consensus Estimates from Miscalibrated Instruments under Heteroscedastic Noise</summary>

- *Chirag Nagpal, Robert E. Tillman, Prashant Reddy, Manuela Veloso*

- `2004.06565v2` - [abs](http://arxiv.org/abs/2004.06565v2) - [pdf](http://arxiv.org/pdf/2004.06565v2)

> We consider the problem of aggregating predictions or measurements from a set of human forecasters, models, sensors or other instruments which may be subject to bias or miscalibration and random heteroscedastic noise. We propose a Bayesian consensus estimator that adjusts for miscalibration and noise and show that this estimator is unbiased and asymptotically more efficient than naive alternatives. We further propose a Hierarchical Bayesian Model that leverages our proposed estimator and apply it to two real world forecasting challenges that require consensus estimates from error prone individual estimates: forecasting influenza like illness (ILI) weekly percentages and forecasting annual earnings of public companies. We demonstrate that our approach is effective at mitigating bias and error and results in more accurate forecasts than existing consensus models.

</details>

<details>

<summary>2021-01-09 17:01:44 - A Decentralized Approach to Bayesian Learning</summary>

- *Anjaly Parayil, He Bai, Jemin George, Prudhvi Gurram*

- `2007.06799v4` - [abs](http://arxiv.org/abs/2007.06799v4) - [pdf](http://arxiv.org/pdf/2007.06799v4)

> Motivated by decentralized approaches to machine learning, we propose a collaborative Bayesian learning algorithm taking the form of decentralized Langevin dynamics in a non-convex setting. Our analysis show that the initial KL-divergence between the Markov Chain and the target posterior distribution is exponentially decreasing while the error contributions to the overall KL-divergence from the additive noise is decreasing in polynomial time. We further show that the polynomial-term experiences speed-up with number of agents and provide sufficient conditions on the time-varying step-sizes to guarantee convergence to the desired distribution. The performance of the proposed algorithm is evaluated on a wide variety of machine learning tasks. The empirical results show that the performance of individual agents with locally available data is on par with the centralized setting with considerable improvement in the convergence rate.

</details>

<details>

<summary>2021-01-09 18:58:20 - Hierarchical Dynamic Modeling for Individualized Bayesian Forecasting</summary>

- *Anna K. Yanchenko, Di Daniel Deng, Jinglan Li, Andrew J. Cron, Mike West*

- `2101.03408v1` - [abs](http://arxiv.org/abs/2101.03408v1) - [pdf](http://arxiv.org/pdf/2101.03408v1)

> We present a case study and methodological developments in large-scale hierarchical dynamic modeling for personalized prediction in commerce. The context is supermarket sales, where improved forecasting of customer/household-specific purchasing behavior informs decisions about personalized pricing and promotions on a continuing basis. This is a big data, big modeling and forecasting setting involving many thousands of customers and items on sale, requiring sequential analysis, addressing information flows at multiple levels over time, and with heterogeneity of customer profiles and item categories. Models developed are fully Bayesian, interpretable and multi-scale, with hierarchical forms overlaid on the inherent structure of the retail setting. Customer behavior is modeled at several levels of aggregation, and information flows from aggregate to individual levels. Forecasting at an individual household level infers price sensitivity to inform personalized pricing and promotion decisions. Methodological innovations include extensions of Bayesian dynamic mixture models, their integration into multi-scale systems, and forecast evaluation with context-specific metrics. The use of simultaneous predictors from multiple hierarchical levels improves forecasts at the customer-item level of main interest. This is evidenced across many different households and items, indicating the utility of the modeling framework for this and other individualized forecasting applications.

</details>

<details>

<summary>2021-01-10 02:23:05 - Towards Efficient Local Causal Structure Learning</summary>

- *Shuai Yang, Hao Wang, Kui Yu, Fuyuan Cao, Xindong Wu*

- `1910.01288v3` - [abs](http://arxiv.org/abs/1910.01288v3) - [pdf](http://arxiv.org/pdf/1910.01288v3)

> Local causal structure learning aims to discover and distinguish direct causes (parents) and direct effects (children) of a variable of interest from data. While emerging successes have been made, existing methods need to search a large space to distinguish direct causes from direct effects of a target variable \emph{T}. To tackle this issue, we propose a novel Efficient Local Causal Structure learning algorithm, named ELCS. Specifically, we first propose the concept of N-structures, then design an efficient Markov Blanket (MB) discovery subroutine to integrate MB learning with N-structures to learn the MB of \emph{T} and simultaneously distinguish direct causes from direct effects of \emph{T}. With the proposed MB subroutine, ELCS starts from the target variable, sequentially finds MBs of variables connected to the target variable and simultaneously constructs local causal structures over MBs until the direct causes and direct effects of the target variable have been distinguished. Using eight Bayesian networks the extensive experiments have validated that ELCS achieves better accuracy and efficiency than the state-of-the-art algorithms.

</details>

<details>

<summary>2021-01-10 13:38:34 - Bayesian estimation of a competing risk model based on Weibull and exponential distributions under right censored data</summary>

- *Hamida Talhi, Hiba Aiachi, Nadji Rahmania*

- `2101.03550v1` - [abs](http://arxiv.org/abs/2101.03550v1) - [pdf](http://arxiv.org/pdf/2101.03550v1)

> In this paper we investigate the estimation of the unknown parameters of a competing risk model based on a Weibull distributed decreasing failure rate and an exponentially distributed constant failure rate, under right censored data.likelihood estimators.

</details>

<details>

<summary>2021-01-10 16:51:42 - Grid-Parametrize-Split (GriPS) for Improved Scalable Inference in Spatial Big Data Analysis</summary>

- *Michele Peruzzi, Sudipto Banerjee, David B. Dunson, Andrew O. Finley*

- `2101.03579v1` - [abs](http://arxiv.org/abs/2101.03579v1) - [pdf](http://arxiv.org/pdf/2101.03579v1)

> Rapid advancements in spatial technologies including Geographic Information Systems (GIS) and remote sensing have generated massive amounts of spatially referenced data in a variety of scientific and data-driven industrial applications. These advancements have led to a substantial, and still expanding, literature on the modeling and analysis of spatially oriented big data. In particular, Bayesian inferences for high-dimensional spatial processes are being sought in a variety of remote-sensing applications including, but not limited to, modeling next generation Light Detection and Ranging (LiDAR) systems and other remotely sensed data. Massively scalable spatial processes, in particular Gaussian processes (GPs), are being explored extensively for the increasingly encountered big data settings. Recent developments include GPs constructed from sparse Directed Acyclic Graphs (DAGs) with a limited number of neighbors (parents) to characterize dependence across the spatial domain. The DAG can be used to devise fast algorithms for posterior sampling of the latent process, but these may exhibit pathological behavior in estimating covariance parameters. While these issues are mitigated by considering marginalized samplers that exploit the underlying sparse precision matrix, these algorithms are slower, less flexible, and oblivious of structure in the data. The current article introduces the Grid-Parametrize-Split (GriPS) approach for conducting Bayesian inference in spatially oriented big data settings by a combination of careful model construction and algorithm design to effectuate substantial improvements in MCMC convergence. We demonstrate the effectiveness of our proposed methods through simulation experiments and subsequently undertake the modeling of LiDAR outcomes and production of their predictive maps using G-LiHT and other remotely sensed variables.

</details>

<details>

<summary>2021-01-11 18:23:04 - Statistical Methods for cis-Mendelian Randomization</summary>

- *Apostolos Gkatzionis, Stephen Burgess, Paul J. Newcombe*

- `2101.04081v1` - [abs](http://arxiv.org/abs/2101.04081v1) - [pdf](http://arxiv.org/pdf/2101.04081v1)

> Mendelian randomization is the use of genetic variants to assess the existence of a causal relationship between a risk factor and an outcome of interest. In this paper we focus on Mendelian randomization analyses with many correlated variants from a single gene region, and particularly on cis-Mendelian randomization studies which uses protein expression as a risk factor. Such studies must rely on a small, curated set of variants from the studied region; using all variants in the region requires inverting an ill-conditioned genetic correlation matrix and results in numerically unstable causal effect estimates. We review methods for variable selection and causal effect estimation in cis-Mendelian randomization, ranging from stepwise pruning and conditional analysis to principal components analysis, factor analysis and Bayesian variable selection. In a simulation study, we show that the various methods have a comparable performance in analyses with large sample sizes and strong genetic instruments. However, when weak instrument bias is suspected, factor analysis and Bayesian variable selection produce more reliable inference than simple pruning approaches, which are often used in practice. We conclude by examining two case studies, assessing the effects of LDL-cholesterol and serum testosterone on coronary heart disease risk using variants in the HMGCR and SHBG gene regions respectively.

</details>

<details>

<summary>2021-01-11 18:27:59 - Complexity analysis of Bayesian learning of high-dimensional DAG models and their equivalence classes</summary>

- *Quan Zhou, Hyunwoong Chang*

- `2101.04084v1` - [abs](http://arxiv.org/abs/2101.04084v1) - [pdf](http://arxiv.org/pdf/2101.04084v1)

> We consider MCMC methods for learning equivalence classes of sparse Gaussian DAG models when $p = e^{o(n)}$. The main contribution of this work is a rapid mixing result for a random walk Metropolis-Hastings algorithm, which we prove using a canonical path method. It reveals that the complexity of Bayesian learning of sparse equivalence classes grows only polynomially in $n$ and $p$, under some common high-dimensional assumptions. Further, a series of high-dimensional consistency results is obtained by the path method, including the strong selection consistency of an empirical Bayes model for structure learning and the consistency of a greedy local search on the restricted search space. Rapid mixing and slow mixing results for other structure-learning MCMC methods are also derived. Our path method and mixing time results yield crucial insights into the computational aspects of high-dimensional structure learning, which may be used to develop more efficient MCMC algorithms.

</details>

<details>

<summary>2021-01-11 19:00:00 - A Bayesian neural network predicts the dissolution of compact planetary systems</summary>

- *Miles Cranmer, Daniel Tamayo, Hanno Rein, Peter Battaglia, Samuel Hadden, Philip J. Armitage, Shirley Ho, David N. Spergel*

- `2101.04117v1` - [abs](http://arxiv.org/abs/2101.04117v1) - [pdf](http://arxiv.org/pdf/2101.04117v1)

> Despite over three hundred years of effort, no solutions exist for predicting when a general planetary configuration will become unstable. We introduce a deep learning architecture to push forward this problem for compact systems. While current machine learning algorithms in this area rely on scientist-derived instability metrics, our new technique learns its own metrics from scratch, enabled by a novel internal structure inspired from dynamics theory. Our Bayesian neural network model can accurately predict not only if, but also when a compact planetary system with three or more planets will go unstable. Our model, trained directly from short N-body time series of raw orbital elements, is more than two orders of magnitude more accurate at predicting instability times than analytical estimators, while also reducing the bias of existing machine learning algorithms by nearly a factor of three. Despite being trained on compact resonant and near-resonant three-planet configurations, the model demonstrates robust generalization to both non-resonant and higher multiplicity configurations, in the latter case outperforming models fit to that specific set of integrations. The model computes instability estimates up to five orders of magnitude faster than a numerical integrator, and unlike previous efforts provides confidence intervals on its predictions. Our inference model is publicly available in the SPOCK package, with training code open-sourced.

</details>

<details>

<summary>2021-01-12 08:40:59 - The Beta-Mixture Shrinkage Prior for Sparse Covariances with Posterior Minimax Rates</summary>

- *Kyoungjae Lee, Seongil Jo, Jaeyong Lee*

- `2101.04351v1` - [abs](http://arxiv.org/abs/2101.04351v1) - [pdf](http://arxiv.org/pdf/2101.04351v1)

> Statistical inference for sparse covariance matrices is crucial to reveal dependence structure of large multivariate data sets, but lacks scalable and theoretically supported Bayesian methods. In this paper, we propose beta-mixture shrinkage prior, computationally more efficient than the spike and slab prior, for sparse covariance matrices and establish its minimax optimality in high-dimensional settings. The proposed prior consists of beta-mixture shrinkage and gamma priors for off-diagonal and diagonal entries, respectively. To ensure positive definiteness of the resulting covariance matrix, we further restrict the support of the prior to a subspace of positive definite matrices. We obtain the posterior convergence rate of the induced posterior under the Frobenius norm and establish a minimax lower bound for sparse covariance matrices. The class of sparse covariance matrices for the minimax lower bound considered in this paper is controlled by the number of nonzero off-diagonal elements and has more intuitive appeal than those appeared in the literature. The obtained posterior convergence rate coincides with the minimax lower bound unless the true covariance matrix is extremely sparse. In the simulation study, we show that the proposed method is computationally more efficient than competitors, while achieving comparable performance. Advantages of the shrinkage prior are demonstrated based on two real data sets.

</details>

<details>

<summary>2021-01-12 12:21:57 - Bayesian equation selection on sparse data for discovery of stochastic dynamical systems</summary>

- *Kushagra Gupta, Dootika Vats, Snigdhansu Chatterjee*

- `2101.04437v1` - [abs](http://arxiv.org/abs/2101.04437v1) - [pdf](http://arxiv.org/pdf/2101.04437v1)

> Often the underlying system of differential equations driving a stochastic dynamical system is assumed to be known, with inference conditioned on this assumption. We present a Bayesian framework for discovering this system of differential equations under assumptions that align with real-life scenarios, including the availability of relatively sparse data. Further, we discuss computational strategies that are critical in teasing out the important details about the dynamical system and algorithmic innovations to solve for acute parameter interdependence in the absence of rich data. This gives a complete Bayesian pathway for model identification via a variable selection paradigm and parameter estimation of the corresponding model using only the observed data. We present detailed computations and analysis of the Lorenz-96, Lorenz-63, and the Orstein-Uhlenbeck system using the Bayesian framework we propose.

</details>

<details>

<summary>2021-01-12 14:10:53 - Bayesian inference in high-dimensional models</summary>

- *Sayantan Banerjee, Ismaël Castillo, Subhashis Ghosal*

- `2101.04491v1` - [abs](http://arxiv.org/abs/2101.04491v1) - [pdf](http://arxiv.org/pdf/2101.04491v1)

> Models with dimension more than the available sample size are now commonly used in various applications. A sensible inference is possible using a lower-dimensional structure. In regression problems with a large number of predictors, the model is often assumed to be sparse, with only a few predictors active. Interdependence between a large number of variables is succinctly described by a graphical model, where variables are represented by nodes on a graph and an edge between two nodes is used to indicate their conditional dependence given other variables. Many procedures for making inferences in the high-dimensional setting, typically using penalty functions to induce sparsity in the solution obtained by minimizing a loss function, were developed. Bayesian methods have been proposed for such problems more recently, where the prior takes care of the sparsity structure. These methods have the natural ability to also automatically quantify the uncertainty of the inference through the posterior distribution. Theoretical studies of Bayesian procedures in high-dimension have been carried out recently. Questions that arise are, whether the posterior distribution contracts near the true value of the parameter at the minimax optimal rate, whether the correct lower-dimensional structure is discovered with high posterior probability, and whether a credible region has adequate frequentist coverage. In this paper, we review these properties of Bayesian and related methods for several high-dimensional models such as many normal means problem, linear regression, generalized linear models, Gaussian and non-Gaussian graphical models. Effective computational approaches are also discussed.

</details>

<details>

<summary>2021-01-12 15:48:37 - Towards Automatic Bayesian Optimization: A first step involving acquisition functions</summary>

- *Eduardo C. Garrido Merchán, Luis C. Jariego Pérez*

- `2003.09643v2` - [abs](http://arxiv.org/abs/2003.09643v2) - [pdf](http://arxiv.org/pdf/2003.09643v2)

> Bayesian Optimization is the state of the art technique for the optimization of black boxes, i.e., functions where we do not have access to their analytical expression nor its gradients, they are expensive to evaluate and its evaluation is noisy. The most popular application of bayesian optimization is the automatic hyperparameter tuning of machine learning algorithms, where we obtain the best configuration of machine learning algorithms by optimizing the estimation of the generalization error of these algorithms. Despite being applied with success, bayesian optimization methodologies also have hyperparameters that need to be configured such as the probabilistic surrogate model or the acquisition function used. A bad decision over the configuration of these hyperparameters implies obtaining bad quality results. Typically, these hyperparameters are tuned by making assumptions of the objective function that we want to evaluate but there are scenarios where we do not have any prior information about the objective function. In this paper, we propose a first attempt over automatic bayesian optimization by exploring several heuristics that automatically tune the acquisition function of bayesian optimization. We illustrate the effectiveness of these heurisitcs in a set of benchmark problems and a hyperparameter tuning problem of a machine learning algorithm.

</details>

<details>

<summary>2021-01-12 15:54:14 - Neuro-symbolic Neurodegenerative Disease Modeling as Probabilistic Programmed Deep Kernels</summary>

- *Alexander Lavin*

- `2009.07738v3` - [abs](http://arxiv.org/abs/2009.07738v3) - [pdf](http://arxiv.org/pdf/2009.07738v3)

> We present a probabilistic programmed deep kernel learning approach to personalized, predictive modeling of neurodegenerative diseases. Our analysis considers a spectrum of neural and symbolic machine learning approaches, which we assess for predictive performance and important medical AI properties such as interpretability, uncertainty reasoning, data-efficiency, and leveraging domain knowledge. Our Bayesian approach combines the flexibility of Gaussian processes with the structural power of neural networks to model biomarker progressions, without needing clinical labels for training. We run evaluations on the problem of Alzheimer's disease prediction, yielding results that surpass deep learning in both accuracy and timeliness of predicting neurodegeneration, and with the practical advantages of Bayesian nonparametrics and probabilistic programming.

</details>

<details>

<summary>2021-01-12 16:12:21 - Bayesian ODE Solvers: The Maximum A Posteriori Estimate</summary>

- *Filip Tronarp, Simo Sarkka, Philipp Hennig*

- `2004.00623v2` - [abs](http://arxiv.org/abs/2004.00623v2) - [pdf](http://arxiv.org/pdf/2004.00623v2)

> It has recently been established that the numerical solution of ordinary differential equations can be posed as a nonlinear Bayesian inference problem, which can be approximately solved via Gaussian filtering and smoothing, whenever a Gauss--Markov prior is used. In this paper the class of $\nu$ times differentiable linear time invariant Gauss--Markov priors is considered. A taxonomy of Gaussian estimators is established, with the maximum a posteriori estimate at the top of the hierarchy, which can be computed with the iterated extended Kalman smoother. The remaining three classes are termed explicit, semi-implicit, and implicit, which are in similarity with the classical notions corresponding to conditions on the vector field, under which the filter update produces a local maximum a posteriori estimate. The maximum a posteriori estimate corresponds to an optimal interpolant in the reproducing Hilbert space associated with the prior, which in the present case is equivalent to a Sobolev space of smoothness $\nu+1$. Consequently, using methods from scattered data approximation and nonlinear analysis in Sobolev spaces, it is shown that the maximum a posteriori estimate converges to the true solution at a polynomial rate in the fill-distance (maximum step size) subject to mild conditions on the vector field. The methodology developed provides a novel and more natural approach to study the convergence of these estimators than classical methods of convergence analysis. The methods and theoretical results are demonstrated in numerical examples.

</details>

<details>

<summary>2021-01-13 03:47:49 - Tail-adaptive Bayesian shrinkage</summary>

- *Se Yoon Lee, Debdeep Pati, Bani K. Mallick*

- `2007.02192v2` - [abs](http://arxiv.org/abs/2007.02192v2) - [pdf](http://arxiv.org/pdf/2007.02192v2)

> Modern genomic studies are increasingly focused on discovering more and more interesting genes associated with a health response. Traditional shrinkage priors are primarily designed to detect a handful of signals from tens and thousands of predictors. Under diverse sparsity regimes, the nature of signal detection is associated with a tail behaviour of a prior. A desirable tail behaviour is called tail-adaptive shrinkage property where tail-heaviness of a prior gets adaptively larger (or smaller) as a sparsity level increases (or decreases) to accommodate more (or less) signals. We propose a global-local-tail (GLT) Gaussian mixture distribution to ensure this property and provide accurate inference under diverse sparsity regimes. Incorporating a peaks-over-threshold method in extreme value theory, we develop an automated tail learning algorithm for the GLT prior. We compare the performance of the GLT prior to the Horseshoe in two gene expression datasets and numerical examples. Results suggest that varying tail rule is advantageous over fixed tail rule under diverse sparsity domains.

</details>

<details>

<summary>2021-01-13 09:07:23 - On weakly informative prior distributions for the heterogeneity parameter in Bayesian random-effects meta-analysis</summary>

- *Christian Röver, Ralf Bender, Sofia Dias, Christopher H. Schmid, Heinz Schmidli, Sibylle Sturtz, Sebastian Weber, Tim Friede*

- `2007.08352v3` - [abs](http://arxiv.org/abs/2007.08352v3) - [pdf](http://arxiv.org/pdf/2007.08352v3)

> The normal-normal hierarchical model (NNHM) constitutes a simple and widely used framework for meta-analysis. In the common case of only few studies contributing to the meta-analysis, standard approaches to inference tend to perform poorly, and Bayesian meta-analysis has been suggested as a potential solution. The Bayesian approach, however, requires the sensible specification of prior distributions. While non-informative priors are commonly used for the overall mean effect, the use of weakly informative priors has been suggested for the heterogeneity parameter, in particular in the setting of (very) few studies. To date, however, a consensus on how to generally specify a weakly informative heterogeneity prior is lacking. Here we investigate the problem more closely and provide some guidance on prior specification.

</details>

<details>

<summary>2021-01-13 15:31:08 - A Latent Variable Model for Relational Events with Multiple Receivers</summary>

- *Joris Mulder, Peter D. Hoff*

- `2101.05135v1` - [abs](http://arxiv.org/abs/2101.05135v1) - [pdf](http://arxiv.org/pdf/2101.05135v1)

> Directional relational event data, such as email data, often include multiple receivers for each event. Statistical methods for adequately modeling such data are limited however. In this article, a multiplicative latent factor model is proposed for relational event data with multiple receivers. For a given event (or message) all potential receiver actors are given a suitability score. When this score exceeds a sender-specific threshold value, the actor is added to the receiver set. The suitability score of a receiver actor for a given message can depend on observed sender and receiver specific characteristics, and on the latent variables of the sender, of the receiver, and of the message. One way to view these latent variables as the degree of specific unobserved topics on which an actor can be active as sender, as receiver, or that are relevant for a given message. Bayesian estimation of the model is relatively straightforward due to the Gaussian distribution of the latent suitability scale. The applicability of the model is illustrated on simulated data and on Enron email data for which about a third of the messages have at least two receivers.

</details>

<details>

<summary>2021-01-13 16:08:57 - Bayesian classification, anomaly detection, and survival analysis using network inputs with application to the microbiome</summary>

- *Nathaniel Josephs, Lizhen Lin, Steven Rosenberg, Eric D. Kolaczyk*

- `2004.04765v2` - [abs](http://arxiv.org/abs/2004.04765v2) - [pdf](http://arxiv.org/pdf/2004.04765v2)

> While the study of a single network is well-established, technological advances now allow for the collection of multiple networks with relative ease. Increasingly, anywhere from several to thousands of networks can be created from brain imaging, gene co-expression data, or microbiome measurements. And these networks, in turn, are being looked to as potentially powerful features to be used in modeling. However, with networks being non-Euclidean in nature, how best to incorporate them into standard modeling tasks is not obvious. In this paper, we propose a Bayesian modeling framework that provides a unified approach to binary classification, anomaly detection, and survival analysis with network inputs. We encode the networks in the kernel of a Gaussian process prior via their pairwise differences and we discuss several choices of provably positive definite kernel that can be plugged into our models. Although our methods are widely applicable, we are motivated here in particular by microbiome research (where network analysis is emerging as the standard approach for capturing the interconnectedness of microbial taxa across both time and space) and its potential for reducing preterm delivery and improving personalization of prenatal care.

</details>

<details>

<summary>2021-01-13 17:15:01 - PAC-Bayes Bounds on Variational Tempered Posteriors for Markov Models</summary>

- *Imon Banerjee, Vinayak A. Rao, Harsha Honnappa*

- `2101.05197v1` - [abs](http://arxiv.org/abs/2101.05197v1) - [pdf](http://arxiv.org/pdf/2101.05197v1)

> Datasets displaying temporal dependencies abound in science and engineering applications, with Markov models representing a simplified and popular view of the temporal dependence structure. In this paper, we consider Bayesian settings that place prior distributions over the parameters of the transition kernel of a Markov model, and seeks to characterize the resulting, typically intractable, posterior distributions. We present a PAC-Bayesian analysis of variational Bayes (VB) approximations to tempered Bayesian posterior distributions, bounding the model risk of the VB approximations. Tempered posteriors are known to be robust to model misspecification, and their variational approximations do not suffer the usual problems of over confident approximations. Our results tie the risk bounds to the mixing and ergodic properties of the Markov data generating model. We illustrate the PAC-Bayes bounds through a number of example Markov models, and also consider the situation where the Markov model is misspecified.

</details>

<details>

<summary>2021-01-13 20:44:51 - Connections between statistical practice in elementary particle physics and the severity concept as discussed in Mayo's Statistical Inference as Severe Testing</summary>

- *Robert D. Cousins*

- `2002.09713v2` - [abs](http://arxiv.org/abs/2002.09713v2) - [pdf](http://arxiv.org/pdf/2002.09713v2)

> For many years, philosopher-of-statistics Deborah Mayo has been advocating the concept of severe testing as a key part of hypothesis testing. Her recent book, Statistical Inference as Severe Testing, is a comprehensive exposition of her arguments in the context of a historical study of many threads of statistical inference, both frequentist and Bayesian. Her foundational point of view is called error statistics, emphasizing frequentist evaluation of the errors called Type I and Type II in the Neyman-Pearson theory of frequentist hypothesis testing. Since the field of elementary particle physics (also known as high energy physics) has strong traditions in frequentist inference, one might expect that something like the severity concept was independently developed in the field. Indeed, I find that, at least operationally (numerically), we high-energy physicists have long interpreted data in ways that map directly onto severity. Whether or not we subscribe to Mayo's philosophical interpretations of severity is a more complicated story that I do not address here.

</details>

<details>

<summary>2021-01-13 21:26:25 - Bayesian Multiple Index Models for Environmental Mixtures</summary>

- *Glen McGee, Ander Wilson, Thomas F. Webster, Brent A. Coull*

- `2101.05352v1` - [abs](http://arxiv.org/abs/2101.05352v1) - [pdf](http://arxiv.org/pdf/2101.05352v1)

> An important goal of environmental health research is to assess the risk posed by mixtures of environmental exposures. Two popular classes of models for mixtures analyses are response-surface methods and exposure-index methods. Response-surface methods estimate high-dimensional surfaces and are thus highly flexible but difficult to interpret. In contrast, exposure-index methods decompose coefficients from a linear model into an overall mixture effect and individual index weights; these models yield easily interpretable effect estimates and efficient inferences when model assumptions hold, but, like most parsimonious models, incur bias when these assumptions do not hold. In this paper we propose a Bayesian multiple index model framework that combines the strengths of each, allowing for non-linear and non-additive relationships between exposure indices and a health outcome, while reducing the dimensionality of the exposure vector and estimating index weights with variable selection. This framework contains response-surface and exposure-index models as special cases, thereby unifying the two analysis strategies. This unification increases the range of models possible for analyzing environmental mixtures and health, allowing one to select an appropriate analysis from a spectrum of models varying in flexibility and interpretability. In an analysis of the association between telomere length and 18 organic pollutants in the National Health and Nutrition Examination Survey (NHANES), the proposed approach fits the data as well as more complex response-surface methods and yields more interpretable results.

</details>

<details>

<summary>2021-01-13 23:59:00 - Should Ensemble Members Be Calibrated?</summary>

- *Xixin Wu, Mark Gales*

- `2101.05397v1` - [abs](http://arxiv.org/abs/2101.05397v1) - [pdf](http://arxiv.org/pdf/2101.05397v1)

> Underlying the use of statistical approaches for a wide range of applications is the assumption that the probabilities obtained from a statistical model are representative of the "true" probability that event, or outcome, will occur. Unfortunately, for modern deep neural networks this is not the case, they are often observed to be poorly calibrated. Additionally, these deep learning approaches make use of large numbers of model parameters, motivating the use of Bayesian, or ensemble approximation, approaches to handle issues with parameter estimation. This paper explores the application of calibration schemes to deep ensembles from both a theoretical perspective and empirically on a standard image classification task, CIFAR-100. The underlying theoretical requirements for calibration, and associated calibration criteria, are first described. It is shown that well calibrated ensemble members will not necessarily yield a well calibrated ensemble prediction, and if the ensemble prediction is well calibrated its performance cannot exceed that of the average performance of the calibrated ensemble members. On CIFAR-100 the impact of calibration for ensemble prediction, and associated calibration is evaluated. Additionally the situation where multiple different topologies are combined together is discussed.

</details>

<details>

<summary>2021-01-14 02:14:16 - Physics-Based Learning for Robotic Environmental Sensing</summary>

- *Reza Khodayi-mehr, Michael M. Zavlanos*

- `1812.03894v4` - [abs](http://arxiv.org/abs/1812.03894v4) - [pdf](http://arxiv.org/pdf/1812.03894v4)

> We propose a physics-based method to learn environmental fields (EFs) using a mobile robot. Common purely data-driven methods require prohibitively many measurements to accurately learn such complex EFs. Alternatively, physics-based models provide global knowledge of EFs but require experimental validation, depend on uncertain parameters, and are intractable for mobile robots. To address these challenges, we propose a Bayesian framework to select the most likely physics-based models of EFs in real-time, from a pool of numerical solutions generated offline as a function of the uncertain parameters. Specifically, we focus on turbulent flow fields and utilize Gaussian processes (GPs) to construct statistical models for them, using the pool of numerical solutions to inform their prior mean. To incorporate flow measurements into these GPs, we control a custom-built mobile robot through a sequence of waypoints that maximize the information content of the measurements. We experimentally demonstrate that our proposed framework constructs a posterior distribution of the flow field that better approximates the real flow compared to the prior numerical solutions and purely data-driven methods.

</details>

<details>

<summary>2021-01-14 10:29:08 - Integrative Learning for Population of Dynamic Networks with Covariates</summary>

- *Suprateek Kundu, Jin Ming, Joe Nocera, Keith M. McGregor*

- `2101.05539v1` - [abs](http://arxiv.org/abs/2101.05539v1) - [pdf](http://arxiv.org/pdf/2101.05539v1)

> Although there is a rapidly growing literature on dynamic connectivity methods, the primary focus has been on separate network estimation for each individual, which fails to leverage common patterns of information. We propose novel graph-theoretic approaches for estimating a population of dynamic networks that are able to borrow information across multiple heterogeneous samples in an unsupervised manner and guided by covariate information. Specifically, we develop a Bayesian product mixture model that imposes independent mixture priors at each time scan and uses covariates to model the mixture weights, which results in time-varying clusters of samples designed to pool information. The computation is carried out using an efficient Expectation-Maximization algorithm. Extensive simulation studies illustrate sharp gains in recovering the true dynamic network over existing dynamic connectivity methods. An analysis of fMRI block task data with behavioral interventions reveal sub-groups of individuals having similar dynamic connectivity, and identifies intervention-related dynamic network changes that are concentrated in biologically interpretable brain regions. In contrast, existing dynamic connectivity approaches are able to detect minimal or no changes in connectivity over time, which seems biologically unrealistic and highlights the challenges resulting from the inability to systematically borrow information across samples.

</details>

<details>

<summary>2021-01-14 13:44:45 - Bayesian Semiparametric Modeling of Response Mechanism for Nonignorable Missing Data</summary>

- *Shonosuke Sugasawa, Kosuke Morikawa, Keisuke Takahata*

- `1909.02878v2` - [abs](http://arxiv.org/abs/1909.02878v2) - [pdf](http://arxiv.org/pdf/1909.02878v2)

> Statistical inference with nonresponse is quite challenging, especially when the response mechanism is nonignorable. In this case, the validity of statistical inference depends on untestable correct specification of the response model. To avoid the misspecification, we propose semiparametric Bayesian estimation in which an outcome model is parametric, but the response model is semiparametric in that we do not assume any parametric form for the nonresponse variable. We adopt penalized spline methods to estimate the unknown function. We also consider a fully nonparametric approach to modeling the response mechanism by using radial basis function methods. Using Polya-gamma data augmentation, we developed an efficient posterior computation algorithm via Gibbs sampling in which most full conditional distributions can be obtained in familiar forms. The performance of the proposed method is demonstrated in simulation studies and an application to longitudinal data.

</details>

<details>

<summary>2021-01-14 14:38:07 - Adaptive shrinkage of smooth functional effects towards a predefined functional subspace</summary>

- *Paul Wiemann, Thomas Kneib*

- `2101.05630v1` - [abs](http://arxiv.org/abs/2101.05630v1) - [pdf](http://arxiv.org/pdf/2101.05630v1)

> In this paper, we propose a new horseshoe-type prior hierarchy for adaptively shrinking spline-based functional effects towards a predefined vector space of parametric functions. Instead of shrinking each spline coefficient towards zero, we use an adapted horseshoe prior to control the deviation from the predefined vector space. For this purpose, the modified horseshoe prior is set up with one scale parameter per spline and not one per coefficient. The presented prior allows for a large number of basis functions to capture all kinds of functional effects while the estimated functional effect is prevented from a highly oscillating overfit. We achieve this by integrating a smoothing penalty similar to the random walk prior commonly applied in Bayesian P-spline priors. In a simulation study, we demonstrate the properties of the new prior specification and compare it to other approaches from the literature. Furthermore, we showcase the applicability of the proposed method by estimating the energy consumption in Germany over the course of a day. For inference, we rely on Markov chain Monte Carlo simulations combining Gibbs sampling for the spline coefficients with slice sampling for all scale parameters in the model.

</details>

<details>

<summary>2021-01-14 14:44:41 - Bayesian inference with tmbstan for a state-space model with VAR(1) state equation</summary>

- *Yihan Cao, Jarle Tufto*

- `2101.05635v1` - [abs](http://arxiv.org/abs/2101.05635v1) - [pdf](http://arxiv.org/pdf/2101.05635v1)

> When using R package tmbstan for Bayesian inference, the built-in feature Laplace approximation to the marginal likelihood with random effects integrated out can be switched on and off. There exists no guideline on whether Laplace approximation should be used to achieve better efficiency especially when the statistical model for estimating selection is complicated. To answer this question, we conducted simulation studies under different scenarios with a state-space model employing a VAR(1) state equation. We found that turning on Laplace approximation in tmbstan would probably lower the computational efficiency, and only when there is a good amount of data, both tmbstan with and without Laplace approximation are worth trying since in this case, Laplace approximation is more likely to be accurate and may also lead to slightly higher computational efficiency. The transition parameters and scale parameters in a VAR(1) process are hard to be estimated accurately and increasing the sample size at each time point do not help in the estimation, only more time points in the data contain more information on these parameters and make the likelihood dominate the posterior likelihood, thus lead to accurate estimates for them.

</details>

<details>

<summary>2021-01-14 17:32:55 - A Class of Spatially Correlated Self-Exciting Models</summary>

- *Nicholas J Clark, Philip M. Dixon*

- `1805.08323v3` - [abs](http://arxiv.org/abs/1805.08323v3) - [pdf](http://arxiv.org/pdf/1805.08323v3)

> The statistical modeling of multivariate count data observed on a space-time lattice has generally focused on using a hierarchical modeling approach where space-time correlation structure is placed on a continuous, latent, process. The count distribution is then assumed to be conditionally independent given the latent process. However, in many real-world applications, especially in the modeling of criminal or terrorism data, the conditional independence between the count distributions is inappropriate. In this manuscript we propose a class of models that capture spatial variation and also account for the possibility of data model dependence. The resulting model allows both data model dependence, or self-excitation, as well as spatial dependence in a latent structure. We demonstrate how second-order properties can be used to characterize the spatio-temporal process and how misspecificaiton of error may inflate self-excitation in a model. Finally, we give an algorithm for efficient Bayesian inference for the model demonstrating its use in capturing the spatio-temporal structure of burglaries in Chicago from 2010-2015.

</details>

<details>

<summary>2021-01-14 19:08:00 - Bayesian Approaches to Distribution Regression</summary>

- *Ho Chung Leon Law, Danica J. Sutherland, Dino Sejdinovic, Seth Flaxman*

- `1705.04293v4` - [abs](http://arxiv.org/abs/1705.04293v4) - [pdf](http://arxiv.org/pdf/1705.04293v4)

> Distribution regression has recently attracted much interest as a generic solution to the problem of supervised learning where labels are available at the group level, rather than at the individual level. Current approaches, however, do not propagate the uncertainty in observations due to sampling variability in the groups. This effectively assumes that small and large groups are estimated equally well, and should have equal weight in the final regression. We account for this uncertainty with a Bayesian distribution regression formalism, improving the robustness and performance of the model when group sizes vary. We frame our models in a neural network style, allowing for simple MAP inference using backpropagation to learn the parameters, as well as MCMC-based inference which can fully propagate uncertainty. We demonstrate our approach on illustrative toy datasets, as well as on a challenging problem of predicting age from images.

</details>

<details>

<summary>2021-01-14 22:37:16 - A hierarchical expected improvement method for Bayesian optimization</summary>

- *Zhehui Chen, Simon Mak, C. F. Jeff Wu*

- `1911.07285v2` - [abs](http://arxiv.org/abs/1911.07285v2) - [pdf](http://arxiv.org/pdf/1911.07285v2)

> The Expected Improvement (EI) method, proposed by Jones et al. (1998), is a widely-used Bayesian optimization method, which makes use of a fitted Gaussian process model for efficient black-box optimization. However, one key drawback of EI is that it is overly greedy in exploiting the fitted Gaussian process model for optimization, which results in suboptimal solutions even with large sample sizes. To address this, we propose a new hierarchical EI (HEI) framework, which makes use of a hierarchical Gaussian process model. HEI preserves a closed-form acquisition function, and corrects the over-greediness of EI by encouraging exploration of the optimization space. We then introduce hyperparameter estimation methods which allow HEI to mimic a fully Bayesian optimization procedure, while avoiding expensive Markov-chain Monte Carlo sampling steps. We prove the global convergence of HEI over a broad function space, and establish near-minimax convergence rates under certain prior specifications. Numerical experiments show the improvement of HEI over existing Bayesian optimization methods, for synthetic functions and a semiconductor manufacturing optimization problem.

</details>

<details>

<summary>2021-01-15 09:52:30 - Matrix-free Penalized Spline Smoothing with Multiple Covariates</summary>

- *Julian Wagner, Göran Kauermann, Ralf Münnich*

- `2101.06034v1` - [abs](http://arxiv.org/abs/2101.06034v1) - [pdf](http://arxiv.org/pdf/2101.06034v1)

> The paper motivates high dimensional smoothing with penalized splines and its numerical calculation in an efficient way. If smoothing is carried out over three or more covariates the classical tensor product spline bases explode in their dimension bringing the estimation to its numerical limits. A recent approach by Siebenborn and Wagner(2019) circumvents storage expensive implementations by proposing matrix-free calculations which allows to smooth over several covariates. We extend their approach here by linking penalized smoothing and its Bayesian formulation as mixed model which provides a matrix-free calculation of the smoothing parameter to avoid the use of high-computational cross validation. Further, we show how to extend the ideas towards generalized regression models. The extended approach is applied to remote sensing satellite data in combination with spatial smoothing.

</details>

<details>

<summary>2021-01-15 20:07:12 - On the relationship between a Gamma distributed precision parameter and the associated standard deviation in the context of Bayesian parameter inference</summary>

- *Manuel M. Eichenlaub*

- `2101.06289v1` - [abs](http://arxiv.org/abs/2101.06289v1) - [pdf](http://arxiv.org/pdf/2101.06289v1)

> In Bayesian inference, an unknown measurement uncertainty is often quantified in terms of a Gamma distributed precision parameter, which is impractical when prior information on the standard deviation of the measurement uncertainty shall be utilised during inference. This paper thus introduces a method for transforming between a gamma distributed precision parameter and the distribution of the associated standard deviation. The proposed method is based on numerical optimisation and shows adequate results for a wide range of scenarios.

</details>

<details>

<summary>2021-01-15 20:38:20 - Generalized Laplace Inference in Multiple Change-Points Models</summary>

- *Alessandro Casini, Pierre Perron*

- `1803.10871v4` - [abs](http://arxiv.org/abs/1803.10871v4) - [pdf](http://arxiv.org/pdf/1803.10871v4)

> Under the classical long-span asymptotic framework we develop a class of Generalized Laplace (GL) inference methods for the change-point dates in a linear time series regression model with multiple structural changes analyzed in, e.g., Bai and Perron (1998). The GL estimator is defined by an integration rather than optimization-based method and relies on the least-squares criterion function. It is interpreted as a classical (non-Bayesian) estimator and the inference methods proposed retain a frequentist interpretation. This approach provides a better approximation about the uncertainty in the data of the change-points relative to existing methods. On the theoretical side, depending on some input (smoothing) parameter, the class of GL estimators exhibits a dual limiting distribution; namely, the classical shrinkage asymptotic distribution, or a Bayes-type asymptotic distribution. We propose an inference method based on Highest Density Regions using the latter distribution. We show that it has attractive theoretical properties not shared by the other popular alternatives, i.e., it is bet-proof. Simulations confirm that these theoretical properties translate to better finite-sample performance.

</details>

<details>

<summary>2021-01-16 11:42:27 - Bayesian Optimization for Iterative Learning</summary>

- *Vu Nguyen, Sebastian Schulze, Michael A Osborne*

- `1909.09593v5` - [abs](http://arxiv.org/abs/1909.09593v5) - [pdf](http://arxiv.org/pdf/1909.09593v5)

> The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence. Traditional tuning algorithms only consider the final performance of hyperparameters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efficient hyperparameter tuning. We propose to learn an evaluation function compressing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the benefit of assessing a hyperparameter setting over additional training steps against their computation cost. We further increase model efficiency by selectively including scores from different training steps for any evaluated hyperparameter set. We demonstrate the efficiency of our algorithm by tuning hyperparameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time.

</details>

<details>

<summary>2021-01-16 16:15:42 - Empirical Decision Rules for Improving the Uncertainty Reporting of Small Sample System Usability Scale Scores</summary>

- *Nicholas Clark, Matthew Dabkowski, Patrick Driscoll, Dereck Kennedy, Ian Kloo, Heidy Shi*

- `2101.00455v2` - [abs](http://arxiv.org/abs/2101.00455v2) - [pdf](http://arxiv.org/pdf/2101.00455v2)

> The System Usability Scale (SUS) is a short, survey-based approach used to determine the usability of a system from an end user perspective once a prototype is available for assessment. Individual scores are gathered using a 10-question survey with the survey results reported in terms of central tendency (sample mean) as an estimate of the system's usability (the SUS study score), and confidence intervals on the sample mean are used to communicate uncertainty levels associated with this point estimate. When the number of individuals surveyed is large, the SUS study scores and accompanying confidence intervals relying upon the central limit theorem for support are appropriate. However, when only a small number of users are surveyed, reliance on the central limit theorem falls short, resulting in confidence intervals that suffer from parameter bound violations and interval widths that confound mappings to adjective and other constructed scales. These shortcomings are especially pronounced when the underlying SUS score data is skewed, as it is in many instances. This paper introduces an empirically-based remedy for such small-sample circumstances, proposing a set of decision rules that leverage either an extended bias-corrected accelerated (BCa) bootstrap confidence interval or an empirical Bayesian credibility interval about the sample mean to restore and bolster subsequent confidence interval accuracy. Data from historical SUS assessments are used to highlight shortfalls in current practices and to demonstrate the improvements these alternate approaches offer while remaining statistically defensible. A freely available, online application is introduced and discussed that automates SUS analysis under these decision rules, thereby assisting usability practitioners in adopting the advocated approaches.

</details>

<details>

<summary>2021-01-16 22:08:46 - Consistent Bayesian Community Detection</summary>

- *Sheng Jiang, Surya Tokdar*

- `2101.06531v1` - [abs](http://arxiv.org/abs/2101.06531v1) - [pdf](http://arxiv.org/pdf/2101.06531v1)

> Stochastic Block Models (SBMs) are a fundamental tool for community detection in network analysis. But little theoretical work exists on the statistical performance of Bayesian SBMs, especially when the community count is unknown. This paper studies a special class of SBMs whose community-wise connectivity probability matrix is diagonally dominant, i.e., members of the same community are more likely to connect with one another than with members from other communities. The diagonal dominance constraint is embedded within an otherwise weak prior, and, under mild regularity conditions, the resulting posterior distribution is shown to concentrate on the true community count and membership allocation as the network size grows to infinity. A reversible-jump Markov Chain Monte Carlo posterior computation strategy is developed by adapting the allocation sampler of Mcdaid et al (2013). Finite sample properties are examined via simulation studies in which the proposed method offers competitive estimation accuracy relative to existing methods under a variety of challenging scenarios.

</details>

<details>

<summary>2021-01-17 02:52:35 - Bayesian geoacoustic inversion using mixture density network</summary>

- *Guoli Wu, Hefeng Dong, Junqiang Song, Jingya Zhang*

- `2008.07902v3` - [abs](http://arxiv.org/abs/2008.07902v3) - [pdf](http://arxiv.org/pdf/2008.07902v3)

> Bayesian geoacoustic inversion problems are conventionally solved by Markov chain Monte Carlo methods or its variants, which are computationally expensive. This paper extends the classic Bayesian geoacoustic inversion framework by deriving important geoacoustic statistics of Bayesian geoacoustic inversion from the multidimensional posterior probability density (PPD) using the mixture density network (MDN) theory. These statistics make it convenient to train the network directly on the whole parameter space and get the multidimensional PPD of model parameters. The present approach provides a much more efficient way to solve geoacoustic inversion problems in Bayesian inference framework. The network is trained on a simulated dataset of surface-wave dispersion curves with shear-wave velocities as labels and tested on both synthetic and real data cases. The results show that the network gives reliable predictions and has good generalization performance on unseen data. Once trained, the network can rapidly (within seconds) give a fully probabilistic solution which is comparable to Monte Carlo methods. It provides an promising approach for real-time inversion.

</details>

<details>

<summary>2021-01-17 04:55:30 - Cost-Efficient Online Hyperparameter Optimization</summary>

- *Jingkang Wang, Mengye Ren, Ilija Bogunovic, Yuwen Xiong, Raquel Urtasun*

- `2101.06590v1` - [abs](http://arxiv.org/abs/2101.06590v1) - [pdf](http://arxiv.org/pdf/2101.06590v1)

> Recent work on hyperparameters optimization (HPO) has shown the possibility of training certain hyperparameters together with regular parameters. However, these online HPO algorithms still require running evaluation on a set of validation examples at each training step, steeply increasing the training cost. To decide when to query the validation loss, we model online HPO as a time-varying Bayesian optimization problem, on top of which we propose a novel \textit{costly feedback} setting to capture the concept of the query cost. Under this setting, standard algorithms are cost-inefficient as they evaluate on the validation set at every round. In contrast, the cost-efficient GP-UCB algorithm proposed in this paper queries the unknown function only when the model is less confident about current decisions. We evaluate our proposed algorithm by tuning hyperparameters online for VGG and ResNet on CIFAR-10 and ImageNet100. Our proposed online HPO algorithm reaches human expert-level performance within a single run of the experiment, while incurring only modest computational overhead compared to regular training.

</details>

<details>

<summary>2021-01-17 05:04:12 - TSEC: a framework for online experimentation under experimental constraints</summary>

- *Simon Mak, Yuanshuo Zhou, Lavonne Hoang, C. F. Jeff Wu*

- `2101.06592v1` - [abs](http://arxiv.org/abs/2101.06592v1) - [pdf](http://arxiv.org/pdf/2101.06592v1)

> Thompson sampling is a popular algorithm for solving multi-armed bandit problems, and has been applied in a wide range of applications, from website design to portfolio optimization. In such applications, however, the number of choices (or arms) $N$ can be large, and the data needed to make adaptive decisions require expensive experimentation. One is then faced with the constraint of experimenting on only a small subset of $K \ll N$ arms within each time period, which poses a problem for traditional Thompson sampling. We propose a new Thompson Sampling under Experimental Constraints (TSEC) method, which addresses this so-called "arm budget constraint". TSEC makes use of a Bayesian interaction model with effect hierarchy priors, to model correlations between rewards on different arms. This fitted model is then integrated within Thompson sampling, to jointly identify a good subset of arms for experimentation and to allocate resources over these arms. We demonstrate the effectiveness of TSEC in two problems with arm budget constraints. The first is a simulated website optimization study, where TSEC shows noticeable improvements over industry benchmarks. The second is a portfolio optimization application on industry-based exchange-traded funds, where TSEC provides more consistent and greater wealth accumulation over standard investment strategies.

</details>

<details>

<summary>2021-01-17 09:35:55 - Making the most of imprecise measurements: Changing patterns of arsenic concentrations in shallow wells of Bangladesh from laboratory and field data</summary>

- *Yuling Yao, Rajib Mozumder, Benjamin Bostick, Brian Mailloux, Charles F. Harvey, Andrew Gelman, Alexander van Geen*

- `2101.06631v1` - [abs](http://arxiv.org/abs/2101.06631v1) - [pdf](http://arxiv.org/pdf/2101.06631v1)

> Millions of people in Bangladesh drink well water contaminated with arsenic. Despite the severity of this heath crisis, little is known about the extent to which groundwater arsenic concentrations change over time: Are concentrations generally rising, or is arsenic being flushed out of aquifers? Are spatially patterns of high and low concentrations across wells homogenizing over time, or are these spatial gradients becoming more pronounced? To address these questions, we analyze a large set of arsenic concentrations that were sampled within a 25 km$^2$ area of Bangladesh over time. We compare two blanket survey collected in 2000/2001 and 2012/2013 from the same villages but relying on a largely different set of wells. The early set consists of 4574 accurate laboratory measurements, but the later set poses a challenge for analysis because it is composed of 8229 less accurate categorical measurements conducted in the field with a kit. We construct a Bayesian model that jointly calibrates the measurement errors, applies spatial smoothing, and describes the spatiotemporal dynamic with a diffusion-like process model. Our statistical analysis reveals that arsenic concentrations change over time and that their mean dropped from 110 to 96 $\mu$g/L over 12 years, although one quarter of individual wells are inferred to see an increase. The largest decreases occurred at the wells with locally high concentrations where the estimated Laplacian indicated that the arsenic surface was strongly concave. However, well with initially low concentrations were unlikely to be contaminated by nearby high concentration wells over a decade. We validate the model using a posterior predictive check on an external subset of laboratory measurements from the same 271 wells in the same study area available for 2000, 2014, and 2015.

</details>

<details>

<summary>2021-01-17 19:34:00 - Stochastic Zeroth-order Discretizations of Langevin Diffusions for Bayesian Inference</summary>

- *Abhishek Roy, Lingqing Shen, Krishnakumar Balasubramanian, Saeed Ghadimi*

- `1902.01373v4` - [abs](http://arxiv.org/abs/1902.01373v4) - [pdf](http://arxiv.org/pdf/1902.01373v4)

> Discretizations of Langevin diffusions provide a powerful method for sampling and Bayesian inference. However, such discretizations require evaluation of the gradient of the potential function. In several real-world scenarios, obtaining gradient evaluations might either be computationally expensive, or simply impossible. In this work, we propose and analyze stochastic zeroth-order sampling algorithms for discretizing overdamped and underdamped Langevin diffusions. Our approach is based on estimating the gradients, based on Gaussian Stein's identities, widely used in the stochastic optimization literature. We provide a comprehensive sample complexity analysis -- number noisy function evaluations to be made to obtain an $\epsilon$-approximate sample in Wasserstein distance -- of stochastic zeroth-order discretizations of both overdamped and underdamped Langevin diffusions, under various noise models. We also propose a variable selection technique based on zeroth-order gradient estimates and establish its theoretical guarantees. Our theoretical contributions extend the practical applicability of sampling algorithms to the noisy black-box and high-dimensional settings.

</details>

<details>

<summary>2021-01-18 00:59:03 - Inference for BART with Multinomial Outcomes</summary>

- *Yizhen Xu, Joseph W. Hogan, Michael J. Daniels, Rami Kantor, Ann Mwangi*

- `2101.06823v1` - [abs](http://arxiv.org/abs/2101.06823v1) - [pdf](http://arxiv.org/pdf/2101.06823v1)

> The multinomial probit Bayesian additive regression trees (MPBART) framework was proposed by Kindo et al. (KD), approximating the latent utilities in the multinomial probit (MNP) model with BART (Chipman et al. 2010). Compared to multinomial logistic models, MNP does not assume independent alternatives and the correlation structure among alternatives can be specified through multivariate Gaussian distributed latent utilities. We introduce two new algorithms for fitting the MPBART and show that the theoretical mixing rates of our proposals are equal or superior to the existing algorithm in KD. Through simulations, we explore the robustness of the methods to the choice of reference level, imbalance in outcome frequencies, and the specifications of prior hyperparameters for the utility error term. The work is motivated by the application of generating posterior predictive distributions for mortality and engagement in care among HIV-positive patients based on electronic health records (EHRs) from the Academic Model Providing Access to Healthcare (AMPATH) in Kenya. In both the application and simulations, we observe better performance using our proposals as compared to KD in terms of MCMC convergence rate and posterior predictive accuracy.

</details>

<details>

<summary>2021-01-18 14:40:09 - Langevin Dynamics for Adaptive Inverse Reinforcement Learning of Stochastic Gradient Algorithms</summary>

- *Vikram Krishnamurthy, George Yin*

- `2006.11674v2` - [abs](http://arxiv.org/abs/2006.11674v2) - [pdf](http://arxiv.org/pdf/2006.11674v2)

> Inverse reinforcement learning (IRL) aims to estimate the reward function of optimizing agents by observing their response (estimates or actions). This paper considers IRL when noisy estimates of the gradient of a reward function generated by multiple stochastic gradient agents are observed. We present a generalized Langevin dynamics algorithm to estimate the reward function $R(\theta)$; specifically, the resulting Langevin algorithm asymptotically generates samples from the distribution proportional to $\exp(R(\theta))$. The proposed IRL algorithms use kernel-based passive learning schemes. We also construct multi-kernel passive Langevin algorithms for IRL which are suitable for high dimensional data. The performance of the proposed IRL algorithms are illustrated on examples in adaptive Bayesian learning, logistic regression (high dimensional problem) and constrained Markov decision processes. We prove weak convergence of the proposed IRL algorithms using martingale averaging methods. We also analyze the tracking performance of the IRL algorithms in non-stationary environments where the utility function $R(\theta)$ jump changes over time as a slow Markov chain.

</details>

<details>

<summary>2021-01-19 01:09:15 - Detection of Gravitational Waves Using Bayesian Neural Networks</summary>

- *Yu-Chiung Lin, Jiun-Huei Proty Wu*

- `2007.04176v2` - [abs](http://arxiv.org/abs/2007.04176v2) - [pdf](http://arxiv.org/pdf/2007.04176v2)

> We propose a new model of Bayesian Neural Networks to not only detect the events of compact binary coalescence in the observational data of gravitational waves (GW) but also identify the full length of the event duration including the inspiral stage. This is achieved by incorporating the Bayesian approach into the CLDNN classifier, which integrates together the Convolutional Neural Network (CNN) and the Long Short-Term Memory Recurrent Neural Network (LSTM). Our model successfully detect all seven BBH events in the LIGO Livingston O2 data, with the periods of their GW waveforms correctly labeled. The ability of a Bayesian approach for uncertainty estimation enables a newly defined `awareness' state for recognizing the possible presence of signals of unknown types, which is otherwise rejected in a non-Bayesian model. Such data chunks labeled with the awareness state can then be further investigated rather than overlooked. Performance tests with 40,960 training samples against 512 chunks of 8-second real noise mixed with mock signals of various optimal signal-to-noise ratio $0 \leq \rho_\text{opt} \leq 18$ show that our model recognizes 90% of the events when $\rho_\text{opt} >7$ (100% when $\rho_\text{opt} >8.5$) and successfully labels more than 95% of the waveform periods when $\rho_\text{opt} >8$. The latency between the arrival of peak signal and generating an alert with the associated waveform period labeled is only about 20 seconds for an unoptimized code on a moderate GPU-equipped personal computer. This makes our model possible for nearly real-time detection and for forecasting the coalescence events when assisted with deeper training on a larger dataset using the state-of-art HPCs.

</details>

<details>

<summary>2021-01-19 05:11:15 - Sequential Bayesian Risk Set Inference for Robust Discrete Optimization via Simulation</summary>

- *Eunhye Song*

- `2101.07466v1` - [abs](http://arxiv.org/abs/2101.07466v1) - [pdf](http://arxiv.org/pdf/2101.07466v1)

> Optimization via simulation (OvS) procedures that assume the simulation inputs are generated from the real-world distributions are subject to the risk of selecting a suboptimal solution when the distributions are substituted with input models estimated from finite real-world data -- known as input model risk. Focusing on discrete OvS, this paper proposes a new Bayesian framework for analyzing input model risk of implementing an arbitrary solution, $x$, where uncertainty about the input models is captured by a posterior distribution. We define the $\alpha$-level risk set of solution $x$ as the set of solutions whose expected performance is better than $x$ by a practically meaningful margin $(>\delta)$ given common input models with significant probability ($>\alpha$) under the posterior distribution. The user-specified parameters, $\delta$ and $\alpha$, control robustness of the procedure to the desired level as well as guards against unnecessary conservatism. An empty risk set implies that there is no practically better solution than $x$ with significant probability even though the real-world input distributions are unknown. For efficient estimation of the risk set, the conditional mean performance of a solution given a set of input distributions is modeled as a Gaussian process (GP) that takes the solution-distributions pair as an input. In particular, our GP model allows both parametric and nonparametric input models. We propose the sequential risk set inference procedure that estimates the risk set and selects the next solution-distributions pair to simulate using the posterior GP at each iteration. We show that simulating the pair expected to change the risk set estimate the most in the next iteration is the asymptotic one-step optimal sampling rule that minimizes the number of incorrectly classified solutions, if the procedure runs without stopping.

</details>

<details>

<summary>2021-01-19 13:09:36 - Asymptotic behavior of the number of distinct values in a sample from the geometric stick-breaking process</summary>

- *Pierpaolo De Blasi, Ramsés H. Mena, Igor Prünster*

- `2101.07607v1` - [abs](http://arxiv.org/abs/2101.07607v1) - [pdf](http://arxiv.org/pdf/2101.07607v1)

> Discrete random probability measures are a key ingredient of Bayesian nonparametric inferential procedures. A sample generates ties with positive probability and a fundamental object of both theoretical and applied interest is the corresponding random number of distinct values. The growth rate can be determined from the rate of decay of the small frequencies implying that, when the decreasingly ordered frequencies admit a tractable form, the asymptotics of the number of distinct values can be conveniently assessed. We focus on the geometric stick-breaking process and we investigate the effect of the choice of the distribution for the success probability on the asymptotic behavior of the number of distinct values. We show that a whole range of logarithmic behaviors are obtained by appropriately tuning the prior. We also derive a two-term expansion and illustrate its use in a comparison with a larger family of discrete random probability measures having an additional parameter given by the scale of the negative binomial distribution.

</details>

<details>

<summary>2021-01-19 13:25:14 - Novel Bayesian Procrustes Variance-based Inferences in Geometric Morphometrics & Novel R package: BPviGM1</summary>

- *Debashis Chatterjee*

- `2101.06494v2` - [abs](http://arxiv.org/abs/2101.06494v2) - [pdf](http://arxiv.org/pdf/2101.06494v2)

> Compared to abundant classical statistics-based literature, to date, very little Bayesian literature exists on Procrustes shape analysis in Geometric Morphometrics, probably because of being a relatively new branch of statistical research and because of inherent computational difficulty associated with Bayesian analysis. Moreover, we may obtain a plethora of novel inferences from Bayesian Procrustes analysis of shape parameter distributions. In this paper, we propose to regard the posterior of Procrustes shape variance as morphological variability indicators. Here we propose novel Bayesian methodologies for Procrustes shape analysis based on landmark data's isotropic variance assumption and propose a Bayesian statistical test for model validation of new species discovery using morphological variation reflected in the posterior distribution of landmark-variance of objects studied under Geometric Morphometrics. We will consider Gaussian distribution-based and heavy-tailed t distribution-based models for Procrustes analysis.   To date, we are not aware of any direct R package for Bayesian Procrustes analysis for landmark-based Geometric Morphometrics. Hence, we introduce a novel, simple R package \textbf{BPviGM1} ("Bayesian Procrustes Variance-based inferences in Geometric Morphometrics 1"), which essentially contains the R code implementations of the computations for proposed models and methodologies, such as R function for Markov Chain Monte Carlo (MCMC) run for drawing samples from posterior of parameters of concern and R function for the proposed Bayesian test of model validation based on significance morphological variation. As an application, we can quantitatively show that primate male-face may be genetically viable to more shape-variation than the same for females.

</details>

<details>

<summary>2021-01-19 16:44:59 - Adaptive quadrature schemes for Bayesian inference via active learning</summary>

- *F. Llorente, L. Martino, V. Elvira, D. Delgado, J. López-Santiago*

- `2006.00535v3` - [abs](http://arxiv.org/abs/2006.00535v3) - [pdf](http://arxiv.org/pdf/2006.00535v3)

> Numerical integration and emulation are fundamental topics across scientific fields. We propose novel adaptive quadrature schemes based on an active learning procedure. We consider an interpolative approach for building a surrogate posterior density, combining it with Monte Carlo sampling methods and other quadrature rules. The nodes of the quadrature are sequentially chosen by maximizing a suitable acquisition function, which takes into account the current approximation of the posterior and the positions of the nodes. This maximization does not require additional evaluations of the true posterior. We introduce two specific schemes based on Gaussian and Nearest Neighbors (NN) bases. For the Gaussian case, we also provide a novel procedure for fitting the bandwidth parameter, in order to build a suitable emulator of a density function. With both techniques, we always obtain a positive estimation of the marginal likelihood (a.k.a., Bayesian evidence). An equivalent importance sampling interpretation is also described, which allows the design of extended schemes. Several theoretical results are provided and discussed. Numerical results show the advantage of the proposed approach, including a challenging inference problem in an astronomic dynamical model, with the goal of revealing the number of planets orbiting a star.

</details>

<details>

<summary>2021-01-19 17:26:12 - Learning the structure of Bayesian Networks via the bootstrap</summary>

- *Giulio Caravagna, Daniele Ramazzotti*

- `1706.02386v2` - [abs](http://arxiv.org/abs/1706.02386v2) - [pdf](http://arxiv.org/pdf/1706.02386v2)

> Learning the structure of dependencies among multiple random variables is a problem of considerable theoretical and practical interest. Within the context of Bayesian Networks, a practical and surprisingly successful solution to this learning problem is achieved by adopting score-functions optimisation schema, augmented with multiple restarts to avoid local optima. Yet, the conditions under which such strategies work well are poorly understood, and there are also some intrinsic limitations to learning the directionality of the interaction among the variables. Following an early intuition of Friedman and Koller, we propose to decouple the learning problem into two steps: first, we identify a partial ordering among input variables which constrains the structural learning problem, and then propose an effective bootstrap-based algorithm to simulate augmented data sets, and select the most important dependencies among the variables. By using several synthetic data sets, we show that our algorithm yields better recovery performance than the state of the art, increasing the chances of identifying a globally-optimal solution to the learning problem, and solving also well-known identifiability issues that affect the standard approach. We use our new algorithm to infer statistical dependencies between cancer driver somatic mutations detected by high-throughput genome sequencing data of multiple colorectal cancer patients. In this way, we also show how the proposed methods can shade new insights about cancer initiation, and progression. Code: https://github.com/caravagn/Bootstrap-based-Learning

</details>

<details>

<summary>2021-01-20 05:00:38 - A New Knowledge Gradient-based Method for Constrained Bayesian Optimization</summary>

- *Wenjie Chen, Shengcai Liu, Ke Tang*

- `2101.08743v1` - [abs](http://arxiv.org/abs/2101.08743v1) - [pdf](http://arxiv.org/pdf/2101.08743v1)

> Black-box problems are common in real life like structural design, drug experiments, and machine learning. When optimizing black-box systems, decision-makers always consider multiple performances and give the final decision by comprehensive evaluations. Motivated by such practical needs, we focus on constrained black-box problems where the objective and constraints lack known special structure, and evaluations are expensive and even with noise. We develop a novel constrained Bayesian optimization approach based on the knowledge gradient method ($c-\rm{KG}$). A new acquisition function is proposed to determine the next batch of samples considering optimality and feasibility. An unbiased estimator of the gradient of the new acquisition function is derived to implement the $c-\rm{KG}$ approach.

</details>

<details>

<summary>2021-01-20 09:18:16 - Colombian Women's Life Patterns: A Multivariate Density Regression Approach</summary>

- *Sara Wade, Raffaella Piccarreta, Andrea Cremaschi, Isadora Antoniano-Villalobos*

- `1905.07172v4` - [abs](http://arxiv.org/abs/1905.07172v4) - [pdf](http://arxiv.org/pdf/1905.07172v4)

> Women in Colombia face difficulties related to the patriarchal traits of their societies and well-known conflict afflicting the country since 1948. In this critical context, our aim is to study the relationship between baseline socio-demographic factors and variables associated to fertility, partnership patterns, and work activity. To best exploit the explanatory structure, we propose a Bayesian multivariate density regression model, which can accommodate mixed responses with censored, constrained, and binary traits. The flexible nature of the models allows for nonlinear regression functions and non-standard features in the errors, such as asymmetry or multi-modality. The model has interpretable covariate-dependent weights constructed through normalization, allowing for combinations of categorical and continuous covariates. Computational difficulties for inference are overcome through an adaptive truncation algorithm combining adaptive Metropolis-Hastings and sequential Monte Carlo to create a sequence of automatically truncated posterior mixtures. For our study on Colombian women's life patterns, a variety of quantities are visualised and described, and in particular, our findings highlight the detrimental impact of family violence on women's choices and behaviors.

</details>

<details>

<summary>2021-01-20 10:52:48 - A Similarity Measure of Gaussian Process Predictive Distributions</summary>

- *Lucia Asencio-Martín, Eduardo C. Garrido-Merchán*

- `2101.08061v1` - [abs](http://arxiv.org/abs/2101.08061v1) - [pdf](http://arxiv.org/pdf/2101.08061v1)

> Some scenarios require the computation of a predictive distribution of a new value evaluated on an objective function conditioned on previous observations. We are interested on using a model that makes valid assumptions on the objective function whose values we are trying to predict. Some of these assumptions may be smoothness or stationarity. Gaussian process (GPs) are probabilistic models that can be interpreted as flexible distributions over functions. They encode the assumptions through covariance functions, making hypotheses about new data through a predictive distribution by being fitted to old observations. We can face the case where several GPs are used to model different objective functions. GPs are non-parametric models whose complexity is cubic on the number of observations. A measure that represents how similar is one GP predictive distribution with respect to another would be useful to stop using one GP when they are modelling functions of the same input space. We are really inferring that two objective functions are correlated, so one GP is enough to model both of them by performing a transformation of the prediction of the other function in case of inverse correlation. We show empirical evidence in a set of synthetic and benchmark experiments that GPs predictive distributions can be compared and that one of them is enough to predict two correlated functions in the same input space. This similarity metric could be extremely useful used to discard objectives in Bayesian many-objective optimization.

</details>

<details>

<summary>2021-01-20 15:16:10 - Bayesian GARCH Modeling of Functional Sports Data</summary>

- *Patric Dolmeta, Raffaele Argiento, Silvia Montagna*

- `2101.08175v1` - [abs](http://arxiv.org/abs/2101.08175v1) - [pdf](http://arxiv.org/pdf/2101.08175v1)

> The use of statistical methods in sport analytics has gained a rapidly growing interest over the last decade, and nowadays is common practice. In particular, the interest in understanding and predicting an athlete's performance throughout his/her career is motivated by the need to evaluate the efficacy of training programs, anticipate fatigue to prevent injuries and detect unexpected of disproportionate increases in performance that might be indicative of doping. Moreover, fast evolving data gathering technologies require up to date modelling techniques that adapt to the distinctive features of sports data. In this work, we propose a hierarchical Bayesian model for describing and predicting the evolution of performance over time for shot put athletes. To account for seasonality and heterogeneity in recorded results, we rely both on a smooth functional contribution and on a linear mixed effect model with heteroskedastic errors to represent the athlete-specific trajectories. The resulting model provides an accurate description of the performance trajectories and helps specifying both the intra- and inter-seasonal variability of measurements. Further, the model allows for the prediction of athletes' performance in future seasons. We apply our model to an extensive real world data set on performance data of professional shot put athletes recorded at elite competitions.

</details>

<details>

<summary>2021-01-20 16:47:59 - Dive into Decision Trees and Forests: A Theoretical Demonstration</summary>

- *Jinxiong Zhang*

- `2101.08656v1` - [abs](http://arxiv.org/abs/2101.08656v1) - [pdf](http://arxiv.org/pdf/2101.08656v1)

> Based on decision trees, many fields have arguably made tremendous progress in recent years. In simple words, decision trees use the strategy of "divide-and-conquer" to divide the complex problem on the dependency between input features and labels into smaller ones. While decision trees have a long history, recent advances have greatly improved their performance in computational advertising, recommender system, information retrieval, etc. We introduce common tree-based models (e.g., Bayesian CART, Bayesian regression splines) and training techniques (e.g., mixed integer programming, alternating optimization, gradient descent). Along the way, we highlight probabilistic characteristics of tree-based models and explain their practical and theoretical benefits. Except machine learning and data mining, we try to show theoretical advances on tree-based models from other fields such as statistics and operation research. We list the reproducible resource at the end of each method.

</details>

<details>

<summary>2021-01-20 22:57:22 - Enhancing Generative Models via Quantum Correlations</summary>

- *Xun Gao, Eric R. Anschuetz, Sheng-Tao Wang, J. Ignacio Cirac, Mikhail D. Lukin*

- `2101.08354v1` - [abs](http://arxiv.org/abs/2101.08354v1) - [pdf](http://arxiv.org/pdf/2101.08354v1)

> Generative modeling using samples drawn from the probability distribution constitutes a powerful approach for unsupervised machine learning. Quantum mechanical systems can produce probability distributions that exhibit quantum correlations which are difficult to capture using classical models. We show theoretically that such quantum correlations provide a powerful resource for generative modeling. In particular, we provide an unconditional proof of separation in expressive power between a class of widely-used generative models, known as Bayesian networks, and its minimal quantum extension. We show that this expressivity advantage is associated with quantum nonlocality and quantum contextuality. Furthermore, we numerically test this separation on standard machine learning data sets and show that it holds for practical problems. The possibility of quantum advantage demonstrated in this work not only sheds light on the design of useful quantum machine learning protocols but also provides inspiration to draw on ideas from quantum foundations to improve purely classical algorithms.

</details>

<details>

<summary>2021-01-20 23:54:29 - Bayesian Bandwidths in Semiparametric Modelling for Nonnegative Orthant Data with Diagnostics</summary>

- *Célestin C. Kokonendji, Sobom M. Somé*

- `2101.08365v1` - [abs](http://arxiv.org/abs/2101.08365v1) - [pdf](http://arxiv.org/pdf/2101.08365v1)

> Multivariate nonnegative orthant data are real vectors bounded to the left by the null vector, and they can be continuous, discrete or mixed. We first review the recent relative variability indexes for multivariate nonnegative continuous and count distributions. As a prelude, the classification of two comparable distributions having the same mean vector is done through under-, equi- and over-variability with respect to the reference distribution. Multivariate associated kernel estimators are then reviewed with new proposals that can accommodate any nonnegative orthant dataset. We focus on bandwidth matrix selections by adaptive and local Bayesian methods for semicontinuous and counting supports, respectively. We finally introduce a flexible semiparametric approach for estimating all these distributions on nonnegative supports. The corresponding estimator is directed by a given parametric part, and a nonparametric part which is a weight function to be estimated through multivariate associated kernels. A diagnostic model is also discussed to make an appropriate choice between the parametric, semiparametric and nonparametric approaches. The retention of pure nonparametric means the inconvenience of parametric part used in the modelization. Multivariate real data examples in semicontinuous setup as reliability are gradually considered to illustrate the proposed approach. Concluding remarks are made for extension to other multiple functions.

</details>

<details>

<summary>2021-01-21 11:03:16 - Bayesian Evidential Deep Learning with PAC Regularization</summary>

- *Manuel Haussmann, Sebastian Gerwinn, Melih Kandemir*

- `1906.00816v3` - [abs](http://arxiv.org/abs/1906.00816v3) - [pdf](http://arxiv.org/pdf/1906.00816v3)

> We propose a novel method for closed-form predictive distribution modeling with neural nets. In quantifying prediction uncertainty, we build on Evidential Deep Learning, which has been impactful as being both simple to implement and giving closed-form access to predictive uncertainty. We employ it to model aleatoric uncertainty and extend it to account also for epistemic uncertainty by converting it to a Bayesian Neural Net. While extending its uncertainty quantification capabilities, we maintain its analytically accessible predictive distribution model by performing progressive moment matching for the first time for approximate weight marginalization. The eventual model introduces a prohibitively large number of hyperparameters for stable training. We overcome this drawback by deriving a vacuous PAC bound that comprises the marginal likelihood of the predictor and a complexity penalty. We observe on regression, classification, and out-of-domain detection benchmarks that our method improves model fit and uncertainty quantification.

</details>

<details>

<summary>2021-01-21 14:44:26 - Efficient inference for stochastic differential equation mixed-effects models using correlated particle pseudo-marginal algorithms</summary>

- *Samuel Wiqvist, Andrew Golightly, Ashleigh T. McLean, Umberto Picchini*

- `1907.09851v4` - [abs](http://arxiv.org/abs/1907.09851v4) - [pdf](http://arxiv.org/pdf/1907.09851v4)

> Stochastic differential equation mixed-effects models (SDEMEMs) are flexible hierarchical models that are able to account for random variability inherent in the underlying time-dynamics, as well as the variability between experimental units and, optionally, account for measurement error. Fully Bayesian inference for state-space SDEMEMs is performed, using data at discrete times that may be incomplete and subject to measurement error. However, the inference problem is complicated by the typical intractability of the observed data likelihood which motivates the use of sampling-based approaches such as Markov chain Monte Carlo. A Gibbs sampler is proposed to target the marginal posterior of all parameter values of interest. The algorithm is made computationally efficient through careful use of blocking strategies and correlated pseudo-marginal Metropolis-Hastings steps within the Gibbs scheme. The resulting methodology is flexible and is able to deal with a large class of SDEMEMs. The methodology is demonstrated on three case studies, including tumor growth dynamics and neuronal data. The gains in terms of increased computational efficiency are model and data dependent, but unless bespoke sampling strategies requiring analytical derivations are possible for a given model, we generally observe an efficiency increase of one order of magnitude when using correlated particle methods together with our blocked-Gibbs strategy.

</details>

<details>

<summary>2021-01-21 19:54:57 - Bayesian Few-Shot Classification with One-vs-Each Pólya-Gamma Augmented Gaussian Processes</summary>

- *Jake Snell, Richard Zemel*

- `2007.10417v2` - [abs](http://arxiv.org/abs/2007.10417v2) - [pdf](http://arxiv.org/pdf/2007.10417v2)

> Few-shot classification (FSC), the task of adapting a classifier to unseen classes given a small labeled dataset, is an important step on the path toward human-like machine learning. Bayesian methods are well-suited to tackling the fundamental issue of overfitting in the few-shot scenario because they allow practitioners to specify prior beliefs and update those beliefs in light of observed data. Contemporary approaches to Bayesian few-shot classification maintain a posterior distribution over model parameters, which is slow and requires storage that scales with model size. Instead, we propose a Gaussian process classifier based on a novel combination of P\'olya-Gamma augmentation and the one-vs-each softmax approximation that allows us to efficiently marginalize over functions rather than model parameters. We demonstrate improved accuracy and uncertainty quantification on both standard few-shot classification benchmarks and few-shot domain transfer tasks.

</details>

<details>

<summary>2021-01-21 20:34:07 - A Gibbs sampler for a class of random convex polytopes</summary>

- *Pierre E. Jacob, Ruobin Gong, Paul T. Edlefsen, Arthur P. Dempster*

- `1910.11953v3` - [abs](http://arxiv.org/abs/1910.11953v3) - [pdf](http://arxiv.org/pdf/1910.11953v3)

> We present a Gibbs sampler for the Dempster-Shafer (DS) approach to statistical inference for Categorical distributions. The DS framework extends the Bayesian approach, allows in particular the use of partial prior information, and yields three-valued uncertainty assessments representing probabilities "for", "against", and "don't know" about formal assertions of interest. The proposed algorithm targets the distribution of a class of random convex polytopes which encapsulate the DS inference. The sampler relies on an equivalence between the iterative constraints of the vertex configuration and the non-negativity of cycles in a fully connected directed graph. Illustrations include the testing of independence in 2x2 contingency tables and parameter estimation of the linkage model.

</details>

<details>

<summary>2021-01-21 21:10:27 - Towards Practical Lipschitz Bandits</summary>

- *Tianyu Wang, Weicheng Ye, Dawei Geng, Cynthia Rudin*

- `1901.09277v7` - [abs](http://arxiv.org/abs/1901.09277v7) - [pdf](http://arxiv.org/pdf/1901.09277v7)

> Stochastic Lipschitz bandit algorithms balance exploration and exploitation, and have been used for a variety of important task domains. In this paper, we present a framework for Lipschitz bandit methods that adaptively learns partitions of context- and arm-space. Due to this flexibility, the algorithm is able to efficiently optimize rewards and minimize regret, by focusing on the portions of the space that are most relevant. In our analysis, we link tree-based methods to Gaussian processes. In light of our analysis, we design a novel hierarchical Bayesian model for Lipschitz bandit problems. Our experiments show that our algorithms can achieve state-of-the-art performance in challenging real-world tasks such as neural network hyperparameter tuning.

</details>

<details>

<summary>2021-01-22 02:59:20 - Bayesian spectral density estimation using P-splines with quantile-based knot placement</summary>

- *Patricio Maturana-Russel, Renate Meyer*

- `1905.01832v3` - [abs](http://arxiv.org/abs/1905.01832v3) - [pdf](http://arxiv.org/pdf/1905.01832v3)

> This article proposes a Bayesian approach to estimating the spectral density of a stationary time series using a prior based on a mixture of P-spline distributions. Our proposal is motivated by the B-spline Dirichlet process prior of Edwards et al. (2019) in combination with Whittle's likelihood and aims at reducing the high computational complexity of its posterior computations. The strength of the B-spline Dirichlet process prior over the Bernstein-Dirichlet process prior of Choudhuri et al. (2004) lies in its ability to estimate spectral densities with sharp peaks and abrupt changes due to the flexibility of B-splines with variable number and location of knots. Here, we suggest to use P-splines of Eilers and Marx (1996) that combine a B-spline basis with a discrete penalty on the basis coefficients. In addition to equidistant knots, a novel strategy for a more expedient placement of knots is proposed that makes use of the information provided by the periodogram about the steepness of the spectral power distribution. We demonstrate in a simulation study and two real case studies that this approach retains the flexibility of the B-splines, achieves similar ability to accurately estimate peaks due to the new data-driven knot allocation scheme but significantly reduces the computational costs.

</details>

<details>

<summary>2021-01-22 12:30:22 - The statistical finite element method (statFEM) for coherent synthesis of observation data and model predictions</summary>

- *Mark Girolami, Eky Febrianto, Ge Yin, Fehmi Cirak*

- `1905.06391v3` - [abs](http://arxiv.org/abs/1905.06391v3) - [pdf](http://arxiv.org/pdf/1905.06391v3)

> The increased availability of observation data from engineering systems in operation poses the question of how to incorporate this data into finite element models. To this end, we propose a novel statistical construction of the finite element method that provides the means of synthesising measurement data and finite element models. The Bayesian statistical framework is adopted to treat all the uncertainties present in the data, the mathematical model and its finite element discretisation. From the outset, we postulate a data-generating model which additively decomposes data into a finite element, a model misspecification and a noise component. Each of the components may be uncertain and is considered as a random variable with a respective prior probability density. The prior of the finite element component is given by a conventional stochastic forward problem. The prior probabilities of the model misspecification and measurement noise, without loss of generality, are assumed to have zero-mean and known covariance structure. Our proposed statistical model is hierarchical in the sense that each of the three random components may depend on non-observable random hyperparameters. Because of the hierarchical structure of the statistical model, Bayes rule is applied on three different levels in turn to infer the posterior densities of the three random components and hyperparameters. On level one, we determine the posterior densities of the finite element component and the true system response using the prior finite element density given by the forward problem and the data likelihood. On the next level, we infer the hyperparameter posterior densities from their respective priors and the marginal likelihood of the first inference problem. Finally, on level three we use Bayes rule to choose the most suitable finite element model in light of the observed data by computing the model posteriors.

</details>

<details>

<summary>2021-01-22 22:05:11 - Orbit: Probabilistic Forecast with Exponential Smoothing</summary>

- *Edwin Ng, Zhishi Wang, Huigang Chen, Steve Yang, Slawek Smyl*

- `2004.08492v4` - [abs](http://arxiv.org/abs/2004.08492v4) - [pdf](http://arxiv.org/pdf/2004.08492v4)

> Time series forecasting is an active research topic in academia as well as industry. Although we see an increasing amount of adoptions of machine learning methods in solving some of those forecasting challenges, statistical methods remain powerful while dealing with low granularity data. This paper introduces a refined Bayesian exponential smoothing model with the help of probabilistic programming languages including Stan. Our model refinements include additional global trend, transformation for multiplicative form, noise distribution and choice of priors. A benchmark study is conducted on a rich set of time-series data sets for our models along with other well-known time series models.

</details>

<details>

<summary>2021-01-23 21:24:53 - Bayesian Edge Regression in Undirected Graphical Models to Characterize Interpatient Heterogeneity in Cancer</summary>

- *Zeya Wang, Veera Baladandayuthapan, Ahmed O. Kaseb, Hesham M. Amin, Manal M. Hassan, Wenyi Wang, Jeffrey S. Morris*

- `2101.09587v1` - [abs](http://arxiv.org/abs/2101.09587v1) - [pdf](http://arxiv.org/pdf/2101.09587v1)

> Graphical models are commonly used to discover associations within gene or protein networks for complex diseases such as cancer. Most existing methods estimate a single graph for a population, while in many cases, researchers are interested in characterizing the heterogeneity of individual networks across subjects with respect to subject-level covariates. Examples include assessments of how the network varies with patient-specific prognostic scores or comparisons of tumor and normal graphs while accounting for tumor purity as a continuous predictor. In this paper, we propose a novel edge regression model for undirected graphs, which estimates conditional dependencies as a function of subject-level covariates. Bayesian shrinkage algorithms are used to induce sparsity in the underlying graphical models. We assess our model performance through simulation studies focused on comparing tumor and normal graphs while adjusting for tumor purity and a case study assessing how blood protein networks in hepatocellular carcinoma patients vary with severity of disease, measured by HepatoScore, a novel biomarker signature measuring disease severity.

</details>

<details>

<summary>2021-01-25 02:35:03 - An Unsupervised Bayesian Neural Network for Truth Discovery in Social Networks</summary>

- *Jielong Yang, Wee Peng Tay*

- `1906.10470v2` - [abs](http://arxiv.org/abs/1906.10470v2) - [pdf](http://arxiv.org/pdf/1906.10470v2)

> The problem of estimating event truths from conflicting agent opinions in a social network is investigated. An autoencoder learns the complex relationships between event truths, agent reliabilities and agent observations. A Bayesian network model is proposed to guide the learning process by modeling the relationship of the autoencoder's outputs with different variables. At the same time, it also models the social relationships between agents in the network. The proposed approach is unsupervised and is applicable when ground truth labels of events are unavailable. A variational inference method is used to jointly estimate the hidden variables in the Bayesian network and the parameters in the autoencoder. Experiments on three real datasets demonstrate that our proposed approach is competitive with, and in most cases better than, several state-of-the-art benchmark methods.

</details>

<details>

<summary>2021-01-25 04:12:19 - Bayesian Optimization with Approximate Set Kernels</summary>

- *Jungtaek Kim, Michael McCourt, Tackgeun You, Saehoon Kim, Seungjin Choi*

- `1905.09780v3` - [abs](http://arxiv.org/abs/1905.09780v3) - [pdf](http://arxiv.org/pdf/1905.09780v3)

> We propose a practical Bayesian optimization method over sets, to minimize a black-box function that takes a set as a single input. Because set inputs are permutation-invariant, traditional Gaussian process-based Bayesian optimization strategies which assume vector inputs can fall short. To address this, we develop a Bayesian optimization method with \emph{set kernel} that is used to build surrogate functions. This kernel accumulates similarity over set elements to enforce permutation-invariance, but this comes at a greater computational cost. To reduce this burden, we propose two key components: (i) a more efficient approximate set kernel which is still positive-definite and is an unbiased estimator of the true set kernel with upper-bounded variance in terms of the number of subsamples, (ii) a constrained acquisition function optimization over sets, which uses symmetry of the feasible region that defines a set input. Finally, we present several numerical experiments which demonstrate that our method outperforms other methods.

</details>

<details>

<summary>2021-01-25 07:15:21 - Vector Quantized Bayesian Neural Network Inference for Data Streams</summary>

- *Namuk Park, Taekyu Lee, Songkuk Kim*

- `1907.05911v3` - [abs](http://arxiv.org/abs/1907.05911v3) - [pdf](http://arxiv.org/pdf/1907.05911v3)

> Bayesian neural networks (BNN) can estimate the uncertainty in predictions, as opposed to non-Bayesian neural networks (NNs). However, BNNs have been far less widely used than non-Bayesian NNs in practice since they need iterative NN executions to predict a result for one data, and it gives rise to prohibitive computational cost. This computational burden is a critical problem when processing data streams with low-latency. To address this problem, we propose a novel model VQ-BNN, which approximates BNN inference for data streams. In order to reduce the computational burden, VQ-BNN inference predicts NN only once and compensates the result with previously memorized predictions. To be specific, VQ-BNN inference for data streams is given by temporal exponential smoothing of recent predictions. The computational cost of this model is almost the same as that of non-Bayesian NNs. Experiments including semantic segmentation on real-world data show that this model performs significantly faster than BNNs while estimating predictive results comparable to or superior to the results of BNNs.

</details>

<details>

<summary>2021-01-25 12:01:38 - A heavy-tailed and overdispersed collective risk model</summary>

- *Pamela M. Chiroque-Solano, Fernando A. S. Moura*

- `2101.09022v2` - [abs](http://arxiv.org/abs/2101.09022v2) - [pdf](http://arxiv.org/pdf/2101.09022v2)

> Insurance data can be asymmetric with heavy tails, causing inadequate adjustments of the usually applied models. To deal with this issue, hierarchical models for collective risk with heavy-tails of the claims distributions that take also into account overdispersion of the number of claims are proposed. In particular, the distribution of the logarithm of the aggregate value of claims is assumed to follow a Student-t distribution. Additionally, to incorporate possible overdispersion, the number of claims is modeled as having a negative binomial distribution. Bayesian decision theory is invoked to calculate the fair premium based on the modified absolute deviation utility. An application to a health insurance dataset is presented together with some diagnostic measures to identify excess variability. The variability measures are analyzed using the marginal posterior predictive distribution of the premiums according to some competitive models. Finally, a simulation study is carried out to assess the predictive capability of the model and the adequacy of the Bayesian estimation procedure.   Keywords: Continuous ranked probability score (CRPS); decision theory; insurance data; marginal posterior predictive; tail value at risk; value at risk.

</details>

<details>

<summary>2021-01-25 15:49:34 - The Exact Asymptotic Form of Bayesian Generalization Error in Latent Dirichlet Allocation</summary>

- *Naoki Hayashi*

- `2008.01304v2` - [abs](http://arxiv.org/abs/2008.01304v2) - [pdf](http://arxiv.org/pdf/2008.01304v2)

> Latent Dirichlet allocation (LDA) obtains essential information from data by using Bayesian inference. It is applied to knowledge discovery via dimension reducing and clustering in many fields. However, its generalization error had not been yet clarified since it is a singular statistical model where there is no one-to-one mapping from parameters to probability distributions. In this paper, we give the exact asymptotic form of its generalization error and marginal likelihood, by theoretical analysis of its learning coefficient using algebraic geometry. The theoretical result shows that the Bayesian generalization error in LDA is expressed in terms of that in matrix factorization and a penalty from the simplex restriction of LDA's parameter region. A numerical experiment is consistent to the theoretical result.

</details>

<details>

<summary>2021-01-25 17:05:30 - Comparison of classical and Bayesian imaging in radio interferometry</summary>

- *Philipp Arras, Hertzog L. Bester, Richard A. Perley, Reimar Leike, Oleg Smirnov, Rüdiger Westermann, Torsten A. Enßlin*

- `2008.11435v4` - [abs](http://arxiv.org/abs/2008.11435v4) - [pdf](http://arxiv.org/pdf/2008.11435v4)

> CLEAN, the commonly employed imaging algorithm in radio interferometry, suffers from a number of shortcomings: in its basic version it does not have the concept of diffuse flux, and the common practice of convolving the CLEAN components with the CLEAN beam erases the potential for super-resolution; it does not output uncertainty information; it produces images with unphysical negative flux regions; and its results are highly dependent on the so-called weighting scheme as well as on any human choice of CLEAN masks to guiding the imaging. Here, we present the Bayesian imaging algorithm resolve which solves the above problems and naturally leads to super-resolution. We take a VLA observation of Cygnus~A at four different frequencies and image it with single-scale CLEAN, multi-scale CLEAN and resolve. Alongside the sky brightness distribution resolve estimates a baseline-dependent correction function for the noise budget, the Bayesian equivalent of weighting schemes. We report noise correction factors between 0.4 and 429. The enhancements achieved by resolve come at the cost of higher computational effort.

</details>

<details>

<summary>2021-01-25 18:28:26 - Sharp hypotheses and bispatial inference</summary>

- *Russell J. Bowater*

- `1911.09049v2` - [abs](http://arxiv.org/abs/1911.09049v2) - [pdf](http://arxiv.org/pdf/1911.09049v2)

> A fundamental class of inferential problems are those characterised by there having been a substantial degree of pre-data (or prior) belief that the value of a model parameter was equal or lay close to a specified value, which may, for example, be the value that indicates the absence of an effect. Standard ways of tackling problems of this type, including the Bayesian method, are often highly inadequate in practice. To address this issue, an inferential framework called bispatial inference is put forward, which can be viewed as both a generalisation and radical reinterpretation of existing approaches to inference that are based on P values. It is shown that to obtain an appropriate post-data density function for a given parameter, it is often convenient to combine a special type of bispatial inference, which is constructed around one-sided P values, with a previously outlined form of fiducial inference. Finally, by using what are called post-data opinion curves, this bispatial-fiducial theory is naturally extended to deal with the general scenario in which any number of parameters may be unknown. The application of the theory is illustrated in various examples, which are especially relevant to the analysis of clinical trial data.

</details>

<details>

<summary>2021-01-25 22:43:59 - The sociospatial factors of death: Analyzing effects of geospatially-distributed variables in a Bayesian mortality model for Hong Kong</summary>

- *Thayer Alshaabi, David Rushing Dewhurst, James P. Bagrow, Peter Sheridan Dodds, Christopher M. Danforth*

- `2006.08527v4` - [abs](http://arxiv.org/abs/2006.08527v4) - [pdf](http://arxiv.org/pdf/2006.08527v4)

> Human mortality is in part a function of multiple socioeconomic factors that differ both spatially and temporally. Adjusting for other covariates, the human lifespan is positively associated with household wealth. However, the extent to which mortality in a geographical region is a function of socioeconomic factors in both that region and its neighbors is unclear. There is also little information on the temporal components of this relationship. Using the districts of Hong Kong over multiple census years as a case study, we demonstrate that there are differences in how wealth indicator variables are associated with longevity in (a) areas that are affluent but neighbored by socially deprived districts versus (b) wealthy areas surrounded by similarly wealthy districts. We also show that the inclusion of spatially-distributed variables reduces uncertainty in mortality rate predictions in each census year when compared with a baseline model. Our results suggest that geographic mortality models should incorporate nonlocal information (e.g., spatial neighbors) to lower the variance of their mortality estimates, and point to a more in-depth analysis of sociospatial spillover effects on mortality rates.

</details>

<details>

<summary>2021-01-26 08:10:00 - Identification of brain states, transitions, and communities using functional MRI</summary>

- *Lingbin Bian, Tiangang Cui, B. T. Thomas Yeo, Alex Fornito, Adeel Razi, Jonathan Keith*

- `2101.10617v1` - [abs](http://arxiv.org/abs/2101.10617v1) - [pdf](http://arxiv.org/pdf/2101.10617v1)

> Brain function relies on a precisely coordinated and dynamic balance between the functional integration and segregation of distinct neural systems. Characterizing the way in which neural systems reconfigure their interactions to give rise to distinct but hidden brain states remains an open challenge. In this paper, we propose a Bayesian model-based characterization of latent brain states and showcase a novel method based on posterior predictive discrepancy using the latent block model to detect transitions between latent brain states in blood oxygen level-dependent (BOLD) time series. The set of estimated parameters in the model includes a latent label vector that assigns network nodes to communities, and also block model parameters that reflect the weighted connectivity within and between communities. Besides extensive in-silico model evaluation, we also provide empirical validation (and replication) using the Human Connectome Project (HCP) dataset of 100 healthy adults. Our results obtained through an analysis of task-fMRI data during working memory performance show appropriate lags between external task demands and change-points between brain states, with distinctive community patterns distinguishing fixation, low-demand and high-demand task conditions.

</details>

<details>

<summary>2021-01-26 18:59:26 - SDF-Bayes: Cautious Optimism in Safe Dose-Finding Clinical Trials with Drug Combinations and Heterogeneous Patient Groups</summary>

- *Hyun-Suk Lee, Cong Shen, William Zame, Jang-Won Lee, Mihaela van der Schaar*

- `2101.10998v1` - [abs](http://arxiv.org/abs/2101.10998v1) - [pdf](http://arxiv.org/pdf/2101.10998v1)

> Phase I clinical trials are designed to test the safety (non-toxicity) of drugs and find the maximum tolerated dose (MTD). This task becomes significantly more challenging when multiple-drug dose-combinations (DC) are involved, due to the inherent conflict between the exponentially increasing DC candidates and the limited patient budget. This paper proposes a novel Bayesian design, SDF-Bayes, for finding the MTD for drug combinations in the presence of safety constraints. Rather than the conventional principle of escalating or de-escalating the current dose of one drug (perhaps alternating between drugs), SDF-Bayes proceeds by cautious optimism: it chooses the next DC that, on the basis of current information, is most likely to be the MTD (optimism), subject to the constraint that it only chooses DCs that have a high probability of being safe (caution). We also propose an extension, SDF-Bayes-AR, that accounts for patient heterogeneity and enables heterogeneous patient recruitment. Extensive experiments based on both synthetic and real-world datasets demonstrate the advantages of SDF-Bayes over state of the art DC trial designs in terms of accuracy and safety.

</details>

<details>

<summary>2021-01-27 01:35:25 - An Early Stopping Bayesian Data Assimilation Approach for Mixed-Logit Estimation</summary>

- *Shanshan Xie, Tim Hillel, Ying Jin*

- `2101.11159v1` - [abs](http://arxiv.org/abs/2101.11159v1) - [pdf](http://arxiv.org/pdf/2101.11159v1)

> The mixed-logit model is a flexible tool in transportation choice analysis, which provides valuable insights into inter and intra-individual behavioural heterogeneity. However, applications of mixed-logit models are limited by the high computational and data requirements for model estimation. When estimating on small samples, the Bayesian estimation approach becomes vulnerable to over and under-fitting. This is problematic for investigating the behaviour of specific population sub-groups or market segments with low data availability. Similar challenges arise when transferring an existing model to a new location or time period, e.g., when estimating post-pandemic travel behaviour. We propose an Early Stopping Bayesian Data Assimilation (ESBDA) simulator for estimation of mixed-logit which combines a Bayesian statistical approach with Machine Learning methodologies. The aim is to improve the transferability of mixed-logit models and to enable the estimation of robust choice models with low data availability. This approach can provide new insights into choice behaviour where the traditional estimation of mixed-logit models was not possible due to low data availability, and open up new opportunities for investment and planning decisions support. The ESBDA estimator is benchmarked against the Direct Application approach, a basic Bayesian simulator with random starting parameter values and a Bayesian Data Assimilation (BDA) simulator without early stopping. The ESBDA approach is found to effectively overcome under and over-fitting and non-convergence issues in simulation. Its resulting models clearly outperform those of the reference simulators in predictive accuracy. Furthermore, models estimated with ESBDA tend to be more robust, with significant parameters with signs and values consistent with behavioural theory, even when estimated on small samples.

</details>

<details>

<summary>2021-01-27 08:26:54 - Functional inequalities for perturbed measures with applications to log-concave measures and to some Bayesian problems</summary>

- *Patrick Cattiaux, Arnaud Guillin*

- `2101.11257v1` - [abs](http://arxiv.org/abs/2101.11257v1) - [pdf](http://arxiv.org/pdf/2101.11257v1)

> We study functional inequalities (Poincar\'e, Cheeger, log-Sobolev) for probability measures obtained as perturbations. Several explicit results for general measures as well as log-concave distributions are given.The initial goal of this work was to obtain explicit bounds on the constants in view of statistical applications for instance. These results are then applied to the Langevin Monte-Carlo method used in statistics in order to compute Bayesian estimators.

</details>

<details>

<summary>2021-01-27 10:00:16 - Bridging Bayesian and Minimax Mean Square Error Estimation via Wasserstein Distributionally Robust Optimization</summary>

- *Viet Anh Nguyen, Soroosh Shafieezadeh-Abadeh, Daniel Kuhn, Peyman Mohajerin Esfahani*

- `1911.03539v2` - [abs](http://arxiv.org/abs/1911.03539v2) - [pdf](http://arxiv.org/pdf/1911.03539v2)

> We introduce a distributionally robust minimium mean square error estimation model with a Wasserstein ambiguity set to recover an unknown signal from a noisy observation. The proposed model can be viewed as a zero-sum game between a statistician choosing an estimator -- that is, a measurable function of the observation -- and a fictitious adversary choosing a prior -- that is, a pair of signal and noise distributions ranging over independent Wasserstein balls -- with the goal to minimize and maximize the expected squared estimation error, respectively. We show that if the Wasserstein balls are centered at normal distributions, then the zero-sum game admits a Nash equilibrium, where the players' optimal strategies are given by an {\em affine} estimator and a {\em normal} prior, respectively. We further prove that this Nash equilibrium can be computed by solving a tractable convex program. Finally, we develop a Frank-Wolfe algorithm that can solve this convex program orders of magnitude faster than state-of-the-art general purpose solvers. We show that this algorithm enjoys a linear convergence rate and that its direction-finding subproblems can be solved in quasi-closed form.

</details>

<details>

<summary>2021-01-27 10:47:59 - A unified framework for closed-form nonparametric regression, classification, preference and mixed problems with Skew Gaussian Processes</summary>

- *Alessio Benavoli, Dario Azzimonti, Dario Piga*

- `2012.06846v2` - [abs](http://arxiv.org/abs/2012.06846v2) - [pdf](http://arxiv.org/pdf/2012.06846v2)

> Skew-Gaussian processes (SkewGPs) extend the multivariate Unified Skew-Normal distributions over finite dimensional vectors to distribution over functions. SkewGPs are more general and flexible than Gaussian processes, as SkewGPs may also represent asymmetric distributions. In a recent contribution we showed that SkewGP and probit likelihood are conjugate, which allows us to compute the exact posterior for non-parametric binary classification and preference learning. In this paper, we generalize previous results and we prove that SkewGP is conjugate with both the normal and affine probit likelihood, and more in general, with their product. This allows us to (i) handle classification, preference, numeric and ordinal regression, and mixed problems in a unified framework; (ii) derive closed-form expression for the corresponding posterior distributions. We show empirically that the proposed framework based on SkewGP provides better performance than Gaussian processes in active learning and Bayesian (constrained) optimization. These two tasks are fundamental for design of experiments and in Data Science.

</details>

<details>

<summary>2021-01-27 12:57:35 - Bayesian Sparse Factor Analysis with Kernelized Observations</summary>

- *Carlos Sevilla-Salcedo, Alejandro Guerrero-López, Pablo M. Olmos, Vanessa Gómez-Verdejo*

- `2006.00968v3` - [abs](http://arxiv.org/abs/2006.00968v3) - [pdf](http://arxiv.org/pdf/2006.00968v3)

> Multi-view problems can be faced with latent variable models since they are able to find low-dimensional projections that fairly capture the correlations among the multiple views that characterise each datum. On the other hand, high-dimensionality and non-linear issues are traditionally handled by kernel methods, inducing a (non)-linear function between the latent projection and the data itself. However, they usually come with scalability issues and exposition to overfitting. Here, we propose merging both approaches into single model so that we can exploit the best features of multi-view latent models and kernel methods and, moreover, overcome their limitations.   In particular, we combine probabilistic factor analysis with what we refer to as kernelized observations, in which the model focuses on reconstructing not the data itself, but its relationship with other data points measured by a kernel function. This model can combine several types of views (kernelized or not), and it can handle heterogeneous data and work in semi-supervised settings. Additionally, by including adequate priors, it can provide compact solutions for the kernelized observations -- based in a automatic selection of Bayesian Relevance Vectors (RVs) -- and can include feature selection capabilities. Using several public databases, we demonstrate the potential of our approach (and its extensions) w.r.t. common multi-view learning models such as kernel canonical correlation analysis or manifold relevance determination.

</details>

<details>

<summary>2021-01-27 18:17:24 - Computational methods for Bayesian semiparametric Item Response Theory models</summary>

- *Sally Paganin, Christopher J. Paciorek, Claudia Wehrhahn, Abel Rodriguez, Sophia Rabe-Hesketh, Perry de Valpine*

- `2101.11583v1` - [abs](http://arxiv.org/abs/2101.11583v1) - [pdf](http://arxiv.org/pdf/2101.11583v1)

> Item response theory (IRT) models are widely used to obtain interpretable inference when analyzing data from questionnaires, scaling binary responses into continuous constructs. Typically, these models rely on a normality assumption for the latent trait characterizing individuals in the population under study. However, this assumption can be unrealistic and lead to biased results. We relax the normality assumption by considering a flexible Dirichlet Process mixture model as a nonparametric prior on the distribution of the individual latent traits. Although this approach has been considered in the literature before, there is a lack of comprehensive studies of such models or general software tools. To fill this gap, we show how the NIMBLE framework for hierarchical statistical modeling enables the use of flexible priors on the latent trait distribution, specifically illustrating the use of Dirichlet Process mixtures in two-parameter logistic (2PL) IRT models. We study how different sets of constraints can lead to model identifiability and give guidance on eliciting prior distributions. Using both simulated and real-world data, we conduct an in-depth study of Markov chain Monte Carlo posterior sampling efficiency for several sampling strategies. We conclude that having access to semiparametric models can be broadly useful, as it allows inference on the entire underlying ability distribution and its functionals, with NIMBLE being a flexible framework for estimation of such models.

</details>

<details>

<summary>2021-01-28 11:32:29 - A Bayesian approach for estimation of weight matrices in spatial autoregressive models</summary>

- *Tamás Krisztin, Philipp Piribauer*

- `2101.11938v1` - [abs](http://arxiv.org/abs/2101.11938v1) - [pdf](http://arxiv.org/pdf/2101.11938v1)

> We develop a Bayesian approach to estimate weight matrices in spatial autoregressive (or spatial lag) models. Our approach focuses on spatial weights which are binary prior to row-standardization. However, unlike recent literature our approach requires no strong a priori assumptions on (socio-)economic distances between the spatial units. The estimation approach relies on efficient Gibbs sampling techniques and can be easily combined with and extended to more flexible spatial specifications. In addition to geographic prior structures, we also discuss shrinkage priors on the neighbourhood size, which are particularly useful in spatial panels where T is small relative to N.

</details>

<details>

<summary>2021-01-28 13:31:59 - Seroprevalence of SARS-CoV-2 antibodies in South Korea</summary>

- *Kwangmin Lee, Seongil Jo, Jaeyong Lee*

- `2101.11991v1` - [abs](http://arxiv.org/abs/2101.11991v1) - [pdf](http://arxiv.org/pdf/2101.11991v1)

> In $2020$, Korea Disease Control and Prevention Agency reported three rounds of surveys on seroprevalence of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) antibodies in South Korea. We analyze the seroprevalence surveys using a Bayesian method with an informative prior distribution on the seroprevalence parameter, and the sensitivity and specificity of the diagnostic test. We construct the informative prior using the posterior distribution obtained from the clinical evaluation data based on the plaque reduction neutralization test. The constraint of the seroprevalence parameter induced from the known confirmed coronavirus 2019 cases can be imposed naturally in the proposed Bayesian model. We also prove that the confidence interval of the seroprevalence parameter based on the Rao's test can be the empty set, while the Bayesian method renders a reasonable interval estimator. As of the $30$th of October $2020$, the $95\%$ credible interval of the estimated SARS-CoV-2 positive population does not exceed $307,448$, approximately $0.6\%$ of the Korean population.

</details>

<details>

<summary>2021-01-28 16:59:31 - Low Complexity Approximate Bayesian Logistic Regression for Sparse Online Learning</summary>

- *Gil I. Shamir, Wojciech Szpankowski*

- `2101.12113v1` - [abs](http://arxiv.org/abs/2101.12113v1) - [pdf](http://arxiv.org/pdf/2101.12113v1)

> Theoretical results show that Bayesian methods can achieve lower bounds on regret for online logistic regression. In practice, however, such techniques may not be feasible especially for very large feature sets. Various approximations that, for huge sparse feature sets, diminish the theoretical advantages, must be used. Often, they apply stochastic gradient methods with hyper-parameters that must be tuned on some surrogate loss, defeating theoretical advantages of Bayesian methods. The surrogate loss, defined to approximate the mixture, requires techniques as Monte Carlo sampling, increasing computations per example. We propose low complexity analytical approximations for sparse online logistic and probit regressions. Unlike variational inference and other methods, our methods use analytical closed forms, substantially lowering computations. Unlike dense solutions, as Gaussian Mixtures, our methods allow for sparse problems with huge feature sets without increasing complexity. With the analytical closed forms, there is also no need for applying stochastic gradient methods on surrogate losses, and for tuning and balancing learning and regularization hyper-parameters. Empirical results top the performance of the more computationally involved methods. Like such methods, our methods still reveal per feature and per example uncertainty measures.

</details>

<details>

<summary>2021-01-28 20:45:11 - A Bayesian Model of Cash Bail Decisions</summary>

- *Joshua Williams, J. Zico Kolter*

- `2101.12267v1` - [abs](http://arxiv.org/abs/2101.12267v1) - [pdf](http://arxiv.org/pdf/2101.12267v1)

> The use of cash bail as a mechanism for detaining defendants pre-trial is an often-criticized system that many have argued violates the presumption of "innocent until proven guilty." Many studies have sought to understand both the long-term effects of cash bail's use and the disparate rate of cash bail assignments along demographic lines (race, gender, etc). However, such work is often susceptible to problems of infra-marginality -- that the data we observe can only describe average outcomes, and not the outcomes associated with the marginal decision.   In this work, we address this problem by creating a hierarchical Bayesian model of cash bail assignments. Specifically, our approach models cash bail decisions as a probabilistic process whereby judges balance the relative costs of assigning cash bail with the cost of defendants potentially skipping court dates, and where these skip probabilities are estimated based upon features of the individual case. We then use Monte Carlo inference to sample the distribution over these costs for different magistrates and across different races. We fit this model to a data set we have collected of over 50,000 court cases in the Allegheny and Philadelphia counties in Pennsylvania. Our analysis of 50 separate judges shows that they are uniformly more likely to assign cash bail to black defendants than to white defendants, even given identical likelihood of skipping a court appearance. This analysis raises further questions about the equity of the practice of cash bail, irrespective of its underlying legal justification.

</details>

<details>

<summary>2021-01-29 07:50:26 - A Study on the Association between Maternal Childhood Trauma Exposure and Placental-fetal Stress Physiology during Pregnancy</summary>

- *Eileen Zhang*

- `2101.12451v1` - [abs](http://arxiv.org/abs/2101.12451v1) - [pdf](http://arxiv.org/pdf/2101.12451v1)

> It has been found that the effect of childhood trauma (CT) exposure may pass on to the next generation. Scientists have hypothesized that the association between CT exposure and placental-fetal stress physiology is the mechanism. A study was conducted to examine the hypothesis. To examine the association between CT exposure and placental corticotrophin-releasing hormone (pCRH), linear mixed effect model and hierarchical Bayesian linear model were constructed. In Bayesian inference, by providing conditionally conjugate priors, Gibbs sampler was used to draw MCMC samples. Piecewise linear mixed effect model was conducted in order to adjust to the dramatic change of pCRH at around week 20 into pregnancy. Pearson residual, QQ, ACF and trace plots were used to justify the model adequacy. Likelihood ratio test and DIC were utilized to model selection. The association between CT exposure and pCRH during pregnancy is obvious. The effect of CT exposure on pCRH varies dramatically over gestational age. Women with one childhood trauma would experience 11.9% higher in pCRH towards the end of pregnancy than those without childhood trauma. The increase rate of pCRH after week 20 is almost four-fold larger than that before week 20. Frequentist and Bayesian inference produce similar results. The findings support the hypothesis that the effect of CT exposure on pCRH over GA exists. The effect changes dramatically at around week 20 into pregnancy.

</details>

<details>

<summary>2021-01-29 10:06:15 - Parsimonious Bayesian Factor Analysis for modelling latent structures in spectroscopy data</summary>

- *Alessandro Casa, Tom F. O'Callaghan, Thomas Brendan Murphy*

- `2101.12499v1` - [abs](http://arxiv.org/abs/2101.12499v1) - [pdf](http://arxiv.org/pdf/2101.12499v1)

> In recent years animal diet has been receiving increased attention, in particular examining the impact of pasture-based feeding strategies on the quality of milk and dairy products, in line with the increased prevalence of grass-fed dairy products appearing on market shelves. To date, there are limited testing methods available for the verification of grass-fed dairy therefore these products are susceptible to food fraud and adulteration. Hence statistical tools studying potential differences among milk samples coming from animals on different feeding systems are required, thus providing increased security around the authenticity of the products. Infrared spectroscopy techniques are widely used to collect data on milk samples and to predict milk related traits. While these data are routinely used to predict the composition of the macro components of milk, each spectrum provides a reservoir of unharnessed information about the sample. The interpretation of these data presents a number of challenges due to their high-dimensionality and the relationships amongst the spectral variables. In this work we propose a modification of the standard factor analysis to induce a parsimonious summary of spectroscopic data. The procedure maps the observations into a low-dimensional latent space while simultaneously clustering observed variables. The method indicates possible redundancies in the data and it helps disentangle the complex relationships among the wavelengths. A flexible Bayesian estimation procedure is proposed for model fitting, providing reasonable values for the number of latent factors and clusters. The method is applied on milk mid-infrared spectroscopy data from dairy cows on different pasture and non-pasture based diets, providing accurate modelling of the data correlation, the clustering of variables and information on differences between milk samples from cows on different diets.

</details>

<details>

<summary>2021-01-29 18:20:46 - Anomaly Detection in Large Scale Networks with Latent Space Models</summary>

- *Wesley Lee, Tyler H. McCormick, Joshua Neil, Cole Sodja, Yanran Cui*

- `1911.05522v2` - [abs](http://arxiv.org/abs/1911.05522v2) - [pdf](http://arxiv.org/pdf/1911.05522v2)

> We develop a real-time anomaly detection algorithm for directed activity on large, sparse networks. We model the propensity for future activity using a dynamic logistic model with interaction terms for sender- and receiver-specific latent factors in addition to sender- and receiver-specific popularity scores; deviations from this underlying model constitute potential anomalies. Latent nodal attributes are estimated via a variational Bayesian approach and may change over time, representing natural shifts in network activity. Estimation is augmented with a case-control approximation to take advantage of the sparsity of the network and reduces computational complexity from $O(N^2)$ to $O(E)$, where $N$ is the number of nodes and $E$ is the number of observed edges. We run our algorithm on network event records collected from an enterprise network of over 25,000 computers and are able to identify a red team attack with half the detection rate required of the model without latent interaction terms.

</details>

<details>

<summary>2021-01-30 05:13:23 - Conservative Updating</summary>

- *Matthew Kovach*

- `2102.00152v1` - [abs](http://arxiv.org/abs/2102.00152v1) - [pdf](http://arxiv.org/pdf/2102.00152v1)

> This paper provides a behavioral analysis of conservatism in beliefs. I introduce a new axiom, Dynamic Conservatism, that relaxes Dynamic Consistency when information and prior beliefs "conflict." When the agent is a subjective expected utility maximizer, Dynamic Conservatism implies that conditional beliefs are a convex combination of the prior and the Bayesian posterior. Conservatism may result in belief dynamics consistent with confirmation bias, representativeness, and the good news-bad news effect, suggesting a deeper behavioral connection between these biases. An index of conservatism and a notion of comparative conservatism are characterized. Finally, I extend conservatism to the case of an agent with incomplete preferences that admit a multiple priors representation.

</details>

<details>

<summary>2021-01-30 20:28:30 - Spike and slab Bayesian sparse principal component analysis</summary>

- *Bo Ning*

- `2102.00305v1` - [abs](http://arxiv.org/abs/2102.00305v1) - [pdf](http://arxiv.org/pdf/2102.00305v1)

> Sparse principal component analysis (PCA) is a popular tool for dimensional reduction of high-dimensional data. Despite its massive popularity, there is still a lack of theoretically justifiable Bayesian sparse PCA that is computationally scalable. A major challenge is choosing a suitable prior for the loadings matrix, as principal components are mutually orthogonal. We propose a spike and slab prior that meets this orthogonality constraint and show that the posterior enjoys both theoretical and computational advantages. Two computational algorithms, the PX-CAVI and the PX-EM algorithms, are developed. Both algorithms use parameter expansion to deal with the orthogonality constraint and to accelerate their convergence speeds. We found that the PX-CAVI algorithm has superior empirical performance than the PX-EM algorithm and two other penalty methods for sparse PCA. The PX-CAVI algorithm is then applied to study a lung cancer gene expression dataset. $\mathsf{R}$ package $\mathsf{VBsparsePCA}$ with an implementation of the algorithm is available on The Comprehensive R Archive Network.

</details>

<details>

<summary>2021-01-31 00:46:21 - Expectation propagation on the diluted Bayesian classifier</summary>

- *Alfredo Braunstein, Thomas Gueudré, Andrea Pagnani, Mirko Pieropan*

- `2009.09545v2` - [abs](http://arxiv.org/abs/2009.09545v2) - [pdf](http://arxiv.org/pdf/2009.09545v2)

> Efficient feature selection from high-dimensional datasets is a very important challenge in many data-driven fields of science and engineering. We introduce a statistical mechanics inspired strategy that addresses the problem of sparse feature selection in the context of binary classification by leveraging a computational scheme known as expectation propagation (EP). The algorithm is used in order to train a continuous-weights perceptron learning a classification rule from a set of (possibly partly mislabeled) examples provided by a teacher perceptron with diluted continuous weights. We test the method in the Bayes optimal setting under a variety of conditions and compare it to other state-of-the-art algorithms based on message passing and on expectation maximization approximate inference schemes. Overall, our simulations show that EP is a robust and competitive algorithm in terms of variable selection properties, estimation accuracy and computational complexity, especially when the student perceptron is trained from correlated patterns that prevent other iterative methods from converging. Furthermore, our numerical tests demonstrate that the algorithm is capable of learning online the unknown values of prior parameters, such as the dilution level of the weights of the teacher perceptron and the fraction of mislabeled examples, quite accurately. This is achieved by means of a simple maximum likelihood strategy that consists in minimizing the free energy associated with the EP algorithm.

</details>

<details>

<summary>2021-01-31 01:46:20 - Statistical challenges in the analysis of sequence and structure data for the COVID-19 spike protein</summary>

- *Shiyu He, Samuel W. K. Wong*

- `2101.02304v2` - [abs](http://arxiv.org/abs/2101.02304v2) - [pdf](http://arxiv.org/pdf/2101.02304v2)

> As the major target of many vaccines and neutralizing antibodies against SARS-CoV-2, the spike (S) protein is observed to mutate over time. In this paper, we present statistical approaches to tackle some challenges associated with the analysis of S-protein data. We build a Bayesian hierarchical model to study the temporal and spatial evolution of S-protein sequences, after grouping the sequences into representative clusters. We then apply sampling methods to investigate possible changes to the S-protein's 3-D structure as a result of commonly observed mutations. While the increasing spread of D614G variants has been noted in other research, our results also show that the co-occurring mutations of D614G together with S477N or A222V may spread even more rapidly, as quantified by our model estimates.

</details>

<details>

<summary>2021-01-31 12:39:45 - Importance Gaussian Quadrature</summary>

- *Víctor Elvira, Luca Martino, Pau Closas*

- `2001.03090v2` - [abs](http://arxiv.org/abs/2001.03090v2) - [pdf](http://arxiv.org/pdf/2001.03090v2)

> Importance sampling (IS) and numerical integration methods are usually employed for approximating moments of complicated target distributions. In its basic procedure, the IS methodology randomly draws samples from a proposal distribution and weights them accordingly, accounting for the mismatch between the target and proposal. In this work, we present a general framework of numerical integration techniques inspired by the IS methodology. The framework can also be seen as an incorporation of deterministic rules into IS methods, reducing the error of the estimators by several orders of magnitude in several problems of interest. The proposed approach extends the range of applicability of the Gaussian quadrature rules. For instance, the IS perspective allows us to use Gauss-Hermite rules in problems where the integrand is not involving a Gaussian distribution, and even more, when the integrand can only be evaluated up to a normalizing constant, as it is usually the case in Bayesian inference. The novel perspective makes use of recent advances on the multiple IS (MIS) and adaptive (AIS) literatures, and incorporates it to a wider numerical integration framework that combines several numerical integration rules that can be iteratively adapted. We analyze the convergence of the algorithms and provide some representative examples showing the superiority of the proposed approach in terms of performance.

</details>

<details>

<summary>2021-01-31 17:37:16 - Conditional independences and causal relations implied by sets of equations</summary>

- *Tineke Blom, Mirthe M. van Diepen, Joris M. Mooij*

- `2007.07183v2` - [abs](http://arxiv.org/abs/2007.07183v2) - [pdf](http://arxiv.org/pdf/2007.07183v2)

> Real-world complex systems are often modelled by sets of equations with endogenous and exogenous variables. What can we say about the causal and probabilistic aspects of variables that appear in these equations without explicitly solving the equations? We make use of Simon's causal ordering algorithm (Simon, 1953) to construct a causal ordering graph and prove that it expresses the effects of soft and perfect interventions on the equations under certain unique solvability assumptions. We further construct a Markov ordering graph and prove that it encodes conditional independences in the distribution implied by the equations with independent random exogenous variables, under a similar unique solvability assumption. We discuss how this approach reveals and addresses some of the limitations of existing causal modelling frameworks, such as causal Bayesian networks and structural causal models.

</details>

<details>

<summary>2021-01-31 19:09:45 - Parallel Iterated Extended and Sigma-point Kalman Smoothers</summary>

- *Fatemeh Yaghoobi, Adrien Corenflos, Sakira Hassan, Simo Särkkä*

- `2102.00514v1` - [abs](http://arxiv.org/abs/2102.00514v1) - [pdf](http://arxiv.org/pdf/2102.00514v1)

> The problem of Bayesian filtering and smoothing in nonlinear models with additive noise is an active area of research. Classical Taylor series as well as more recent sigma-point based methods are two well-known strategies to deal with these problems. However, these methods are inherently sequential and do not in their standard formulation allow for parallelization in the time domain. In this paper, we present a set of parallel formulas that replace the existing sequential ones in order to achieve lower time (span) complexity. Our experimental results done with a graphics processing unit (GPU) illustrate the efficiency of the proposed methods over their sequential counterparts.

</details>


## 2021-02

<details>

<summary>2021-02-01 11:23:10 - A Bayesian panel VAR model to analyze the impact of climate change on high-income economies</summary>

- *Florian Huber, Tamás Krisztin, Michael Pfarrhofer*

- `1804.01554v3` - [abs](http://arxiv.org/abs/1804.01554v3) - [pdf](http://arxiv.org/pdf/1804.01554v3)

> In this paper, we assess the impact of climate shocks on futures markets for agricultural commodities and a set of macroeconomic quantities for multiple high-income economies. To capture relations among countries, markets, and climate shocks, this paper proposes parsimonious methods to estimate high-dimensional panel VARs. We assume that coefficients associated with domestic lagged endogenous variables arise from a Gaussian mixture model while further parsimony is achieved using suitable global-local shrinkage priors on several regions of the parameter space. Our results point towards pronounced global reactions of key macroeconomic quantities to climate shocks. Moreover, the empirical findings highlight substantial linkages between regionally located climate shifts and global commodity markets.

</details>

<details>

<summary>2021-02-01 12:17:40 - Unit Information Prior for Adaptive Information Borrowing from Multiple Historical Datasets</summary>

- *Huaqing Jin, Guosheng Yin*

- `2102.00796v1` - [abs](http://arxiv.org/abs/2102.00796v1) - [pdf](http://arxiv.org/pdf/2102.00796v1)

> In clinical trials, there often exist multiple historical studies for the same or related treatment investigated in the current trial. Incorporating historical data in the analysis of the current study is of great importance, as it can help to gain more information, improve efficiency, and provide a more comprehensive evaluation of treatment. Enlightened by the unit information prior (UIP) concept in the reference Bayesian test, we propose a new informative prior called UIP from an information perspective that can adaptively borrow information from multiple historical datasets. We consider both binary and continuous data and also extend the new UIP methods to linear regression settings. Extensive simulation studies demonstrate that our method is comparable to other commonly used informative priors, while the interpretation of UIP is intuitive and its implementation is relatively easy. One distinctive feature of UIP is that its construction only requires summary statistics commonly reported in the literature rather than the patient-level data. By applying our UIP methods to phase III clinical trials for investigating the efficacy of memantine in Alzheimer's disease, we illustrate its ability of adaptively borrowing information from multiple historical datasets in the real application.

</details>

<details>

<summary>2021-02-01 15:35:55 - On proportional volume sampling for experimental design in general spaces</summary>

- *Arnaud Poinas, Rémi Bardenet*

- `2011.04562v2` - [abs](http://arxiv.org/abs/2011.04562v2) - [pdf](http://arxiv.org/pdf/2011.04562v2)

> Optimal design for linear regression is a fundamental task in statistics. For finite design spaces, recent progress has shown that random designs drawn using proportional volume sampling (PVS) lead to approximation guarantees for A-optimal design. PVS strikes the balance between design nodes that jointly fill the design space, while marginally staying in regions of high mass under the solution of a relaxed convex version of the original problem. In this paper, we examine some of the statistical implications of a new variant of PVS for (possibly Bayesian) optimal design. Using point process machinery, we treat the case of a generic Polish design space. We show that not only are the A-optimality approximation guarantees preserved, but we obtain similar guarantees for D-optimal design that tighten recent results. Moreover, we show that PVS can be sampled in polynomial time. Unfortunately, in spite of its elegance and tractability, we demonstrate on a simple example that the practical implications of general PVS are likely limited. In the second part of the paper, we focus on applications and investigate the use of PVS as a subroutine for stochastic search heuristics. We demonstrate that PVS is a robust addition to the practitioner's toolbox, especially when the regression functions are nonstandard and the design space, while low-dimensional, has a complicated shape (e.g., nonlinear boundaries, several connected components).

</details>

<details>

<summary>2021-02-01 18:14:48 - Data-driven aggregation in circular deconvolution</summary>

- *Jan Johannes, Xavier Loizeau*

- `2102.01037v1` - [abs](http://arxiv.org/abs/2102.01037v1) - [pdf](http://arxiv.org/pdf/2102.01037v1)

> In a circular deconvolution model we consider the fully data driven density estimation of a circular random variable where the density of the additive independent measurement error is unknown. We have at hand two independent iid samples, one of the contaminated version of the variable of interest, and the other of the additive noise. We show optimality,in an oracle and minimax sense, of a fully data-driven weighted sum of orthogonal series density estimators. Two shapes of random weights are considered, one motivated by a Bayesian approach and the other by a well known model selection method. We derive non-asymptotic upper bounds for the quadratic risk and the maximal quadratic risk over Sobolev-like ellipsoids of the fully data-driven estimator. We compute rates which can be obtained in different configurations for the smoothness of the density of interest and the error density. The rates (strictly) match the optimal oracle or minimax rates for a large variety of cases, and feature otherwise at most a deterioration by a logarithmic factor. We illustrate the performance of the fully data-driven weighted sum of orthogonal series estimators by a simulation study.

</details>

<details>

<summary>2021-02-01 20:04:41 - Closer than they appear: A Bayesian perspective on individual-level heterogeneity in risk assessment</summary>

- *Kristian Lum, David B. Dunson, James Johndrow*

- `2102.01135v1` - [abs](http://arxiv.org/abs/2102.01135v1) - [pdf](http://arxiv.org/pdf/2102.01135v1)

> Risk assessment instruments are used across the criminal justice system to estimate the probability of some future behavior given covariates. The estimated probabilities are then used in making decisions at the individual level. In the past, there has been controversy about whether the probabilities derived from group-level calculations can meaningfully be applied to individuals. Using Bayesian hierarchical models applied to a large longitudinal dataset from the court system in the state of Kentucky, we analyze variation in individual-level probabilities of failing to appear for court and the extent to which it is captured by covariates. We find that individuals within the same risk group vary widely in their probability of the outcome. In practice, this means that allocating individuals to risk groups based on standard approaches to risk assessment, in large part, results in creating distinctions among individuals who are not meaningfully different in terms of their likelihood of the outcome. This is because uncertainty about the probability that any particular individual will fail to appear is large relative to the difference in average probabilities among any reasonable set of risk groups.

</details>

<details>

<summary>2021-02-01 21:11:59 - Joint Latent Space Model for Social Networks with Multivariate Attributes</summary>

- *Selena Shuo Wang, Subhadeep Paul, Paul De Boeck*

- `1910.12128v2` - [abs](http://arxiv.org/abs/1910.12128v2) - [pdf](http://arxiv.org/pdf/1910.12128v2)

> In many application problems in social, behavioral, and economic sciences, researchers often have data on a social network among a group of individuals along with high dimensional multivariate measurements for each individual. To analyze such networked data structures, we propose a joint Attribute and Person Latent Space Model (APLSM) that summarizes information from the social network and the multiple attribute measurements in a person-attribute joint latent space. We develop a Variational Bayesian Expectation-Maximization estimation algorithm to estimate the posterior distribution of the attribute and person locations in the joint latent space. This methodology allows for effective integration, informative visualization, and prediction of social networks and high dimensional attribute measurements. Using APLSM, we explore the inner workings of the French financial elites based on their social networks and their career, political views, and social status. We observe a division in the social circles of the French elites in accordance with the differences in their individual characteristics.

</details>

<details>

<summary>2021-02-01 21:55:19 - Causal Inference with the Instrumental Variable Approach and Bayesian Nonparametric Machine Learning</summary>

- *Robert E. McCulloch, Rodney A. Sparapani, Brent R. Logan, Purushottam W. Laud*

- `2102.01199v1` - [abs](http://arxiv.org/abs/2102.01199v1) - [pdf](http://arxiv.org/pdf/2102.01199v1)

> We provide a new flexible framework for inference with the instrumental variable model. Rather than using linear specifications, functions characterizing the effects of instruments and other explanatory variables are estimated using machine learning via Bayesian Additive Regression Trees (BART). Error terms and their distribution are inferred using Dirichlet Process mixtures. Simulated and real examples show that when the true functions are linear, little is lost. But when nonlinearities are present, dramatic improvements are obtained with virtually no manual tuning.

</details>

<details>

<summary>2021-02-01 22:30:32 - Nonasymptotic analysis of Stochastic Gradient Hamiltonian Monte Carlo under local conditions for nonconvex optimization</summary>

- *Ömer Deniz Akyildiz, Sotirios Sabanis*

- `2002.05465v3` - [abs](http://arxiv.org/abs/2002.05465v3) - [pdf](http://arxiv.org/pdf/2002.05465v3)

> We provide a nonasymptotic analysis of the convergence of the stochastic gradient Hamiltonian Monte Carlo (SGHMC) to a target measure in Wasserstein-2 distance without assuming log-concavity. Our analysis quantifies key theoretical properties of the SGHMC as a sampler under local conditions which significantly improves the findings of previous results. In particular, we prove that the Wasserstein-2 distance between the target and the law of the SGHMC is uniformly controlled by the step-size of the algorithm, therefore demonstrate that the SGHMC can provide high-precision results uniformly in the number of iterations. The analysis also allows us to obtain nonasymptotic bounds for nonconvex optimization problems under local conditions and implies that the SGHMC, when viewed as a nonconvex optimizer, converges to a global minimum with the best known rates. We apply our results to obtain nonasymptotic bounds for scalable Bayesian inference and nonasymptotic generalization bounds.

</details>

<details>

<summary>2021-02-02 02:18:12 - Local Differential Privacy Is Equivalent to Contraction of $E_γ$-Divergence</summary>

- *Shahab Asoodeh, Maryam Aliakbarpour, Flavio P. Calmon*

- `2102.01258v1` - [abs](http://arxiv.org/abs/2102.01258v1) - [pdf](http://arxiv.org/pdf/2102.01258v1)

> We investigate the local differential privacy (LDP) guarantees of a randomized privacy mechanism via its contraction properties. We first show that LDP constraints can be equivalently cast in terms of the contraction coefficient of the $E_\gamma$-divergence. We then use this equivalent formula to express LDP guarantees of privacy mechanisms in terms of contraction coefficients of arbitrary $f$-divergences. When combined with standard estimation-theoretic tools (such as Le Cam's and Fano's converse methods), this result allows us to study the trade-off between privacy and utility in several testing and minimax and Bayesian estimation problems.

</details>

<details>

<summary>2021-02-02 11:05:34 - Robust data-driven discovery of partial differential equations with time-dependent coefficients</summary>

- *Aoxue Chen, Guang Lin*

- `2102.01432v1` - [abs](http://arxiv.org/abs/2102.01432v1) - [pdf](http://arxiv.org/pdf/2102.01432v1)

> In this work, we propose a robust Bayesian sparse learning algorithm based on Bayesian group Lasso with spike and slab priors for the discovery of partial differential equations with variable coefficients. Using the samples draw from the posterior distribution with a Gibbs sampler, we are able to estimate the values of coefficients, together with their standard errors and confidence intervals. Apart from constructing the error bars, uncertainty quantification can also be employed for designing new criteria of model selection and threshold setting. This enables our method more adjustable and robust in learning equations with time-dependent coefficients. Three criteria are introduced for model selection and threshold setting to identify the correct terms: the root mean square, total error bar, and group error bar. Moreover, three noise filters are integrated with the robust Bayesian sparse learning algorithm for better results with larger noise. Numerical results demonstrate that our method is more robust than sequential grouped threshold ridge regression and group Lasso in noisy situations through three examples.

</details>

<details>

<summary>2021-02-02 15:34:08 - Bayesian Data Synthesis and Disclosure Risk Quantification: An Application to the Consumer Expenditure Surveys</summary>

- *Jingchen Hu, Terrance D. Savitsky*

- `1809.10074v2` - [abs](http://arxiv.org/abs/1809.10074v2) - [pdf](http://arxiv.org/pdf/1809.10074v2)

> The release of synthetic data generated from a model estimated on the data helps statistical agencies disseminate respondent-level data with high utility and privacy protection. Motivated by the challenge of disseminating sensitive variables containing geographic information in the Consumer Expenditure Surveys (CE) at the U.S. Bureau of Labor Statistics, we propose two non-parametric Bayesian models as data synthesizers for the county identifier of each data record: a Bayesian latent class model and a Bayesian areal model. Both data synthesizers use Dirichlet Process priors to cluster observations of similar characteristics and allow borrowing information across observations. We develop innovative disclosure risks measures to quantify inherent risks in the confidential CE data and how those data risks are ameliorated by our proposed synthesizers. By creating a lower bound and an upper bound of disclosure risks under a minimum and a maximum disclosure risks scenarios respectively, our proposed inherent risks measures provide a range of acceptable disclosure risks for evaluating risks level in the synthetic datasets.

</details>

<details>

<summary>2021-02-02 17:14:44 - Bayesian analysis of population health data</summary>

- *Dorota Młynarczyk, Carmen Armero, Virgilio Gómez-Rubio, Pedro Puig*

- `2102.01612v1` - [abs](http://arxiv.org/abs/2102.01612v1) - [pdf](http://arxiv.org/pdf/2102.01612v1)

> The analysis of population-wide datasets can provide insight on the health status of large populations so that public health officials can make data-driven decisions. The analysis of such datasets often requires highly parameterized models with different types of fixed and randoms effects to account for risk factors, spatial and temporal variations, multilevel effects and other sources on uncertainty. To illustrate the potential of Bayesian hierarchical models, a dataset of about 500 000 inhabitants released by the Polish National Health Fund containing information about ischemic stroke incidence for a 2-year period is analyzed using different types of models. Spatial logistic regression and survival models are considered for analyzing the individual probabilities of stroke and the times to the occurrence of an ischemic stroke event. Demographic and socioeconomic variables as well as drug prescription information are available at an individual level. Spatial variation is considered by means of region-level random effects.

</details>

<details>

<summary>2021-02-02 18:59:31 - Exact Langevin Dynamics with Stochastic Gradients</summary>

- *Adrià Garriga-Alonso, Vincent Fortuin*

- `2102.01691v1` - [abs](http://arxiv.org/abs/2102.01691v1) - [pdf](http://arxiv.org/pdf/2102.01691v1)

> Stochastic gradient Markov Chain Monte Carlo algorithms are popular samplers for approximate inference, but they are generally biased. We show that many recent versions of these methods (e.g. Chen et al. (2014)) cannot be corrected using Metropolis-Hastings rejection sampling, because their acceptance probability is always zero. We can fix this by employing a sampler with realizable backwards trajectories, such as Gradient-Guided Monte Carlo (Horowitz, 1991), which generalizes stochastic gradient Langevin dynamics (Welling and Teh, 2011) and Hamiltonian Monte Carlo. We show that this sampler can be used with stochastic gradients, yielding nonzero acceptance probabilities, which can be computed even across multiple steps.

</details>

<details>

<summary>2021-02-03 11:25:39 - Forecasting temporal variation of aftershocks immediately after a main shock using Gaussian process regression</summary>

- *Kosuke Morikawa, Hiromichi Nagao, Shin-ichi Ito, Yoshikazu Terada, Shin'ichi Sakai, Naoshi Hirata*

- `2006.07949v4` - [abs](http://arxiv.org/abs/2006.07949v4) - [pdf](http://arxiv.org/pdf/2006.07949v4)

> Uncovering the distribution of magnitudes and arrival times of aftershocks is a key to comprehend the characteristics of the sequence of earthquakes, which enables us to predict seismic activities and hazard assessments. However, identifying the number of aftershocks immediately after the main shock is practically difficult due to contaminations of arriving seismic waves. To overcome the difficulty, we construct a likelihood based on the detected data incorporating a detection function to which the Gaussian process regression (GPR) is applied. The GPR is capable of estimating not only the parameters of the distribution of aftershocks together with the detection function but also credible intervals for both of the parameters and the detection function. A property that distributions of both the Gaussian process and aftershocks are exponential functions leads to an efficient Bayesian computational algorithm to estimate the hyperparameters. After the validations through numerical tests, the proposed method is retrospectively applied to the catalog data related to the 2004 Chuetsu earthquake towards early forecasting of the aftershocks. The result shows that the proposed method stably estimates the parameters of the distribution simultaneously their credible intervals even within three hours after the main shock.

</details>

<details>

<summary>2021-02-03 14:46:45 - Estimation of parameters of the logistic exponential distribution under progressive type-I hybrid censored sample</summary>

- *Subhankar Dutta, Suchandan Kayal*

- `2102.02091v1` - [abs](http://arxiv.org/abs/2102.02091v1) - [pdf](http://arxiv.org/pdf/2102.02091v1)

> The paper addresses the problem of estimation of the model parameters of the logistic exponential distribution based on progressive type-I hybrid censored sample. The maximum likelihood estimates are obtained and computed numerically using Newton-Raphson method. Further, the Bayes estimates are derived under squared error, LINEX and generalized entropy loss functions. Two types (independent and bivariate) of prior distributions are considered for the purpose of Bayesian estimation. It is seen that the Bayes estimates are not of explicit forms.Thus, Lindley's approximation technique is employed to get approximate Bayes estimates. Interval estimates of the parameters based on normal approximate of the maximum likelihood estimates and normal approximation of the log-transformed maximum likelihood estimates are constructed. The highest posterior density credible intervals are obtained by using the importance sampling method. Furthermore, numerical computations are reported to review some of the results obtained in the paper. A real life dataset is considered for the purpose of illustrations.

</details>

<details>

<summary>2021-02-03 16:03:09 - Bayesian Fusion: Scalable unification of distributed statistical analyses</summary>

- *Hongsheng Dai, Murray Pollock, Gareth Roberts*

- `2102.02123v1` - [abs](http://arxiv.org/abs/2102.02123v1) - [pdf](http://arxiv.org/pdf/2102.02123v1)

> There has recently been considerable interest in addressing the problem of unifying distributed statistical analyses into a single coherent inference. This problem naturally arises in a number of situations, including in big-data settings, when working under privacy constraints, and in Bayesian model choice. The majority of existing approaches have relied upon convenient approximations of the distributed analyses. Although typically being computationally efficient, and readily scaling with respect to the number of analyses being unified, approximate approaches can have significant shortcomings -- the quality of the inference can degrade rapidly with the number of analyses being unified, and can be substantially biased even when unifying a small number of analyses that do not concur. In contrast, the recent Fusion approach of Dai et al. (2019) is a rejection sampling scheme which is readily parallelisable and is exact (avoiding any form of approximation other than Monte Carlo error), albeit limited in applicability to unifying a small number of low-dimensional analyses. In this paper we introduce a practical Bayesian Fusion approach. We extend the theory underpinning the Fusion methodology and, by embedding it within a sequential Monte Carlo algorithm, we are able to recover the correct target distribution. By means of extensive guidance on the implementation of the approach, we demonstrate theoretically and empirically that Bayesian Fusion is robust to increasing numbers of analyses, and coherently unifying analyses which do not concur. This is achieved while being computationally competitive with approximate schemes.

</details>

<details>

<summary>2021-02-03 16:58:17 - Estimating the radii of air bubbles in water using passive acoustic monitoring</summary>

- *Paulo Hubert, Linilson Padovese*

- `2102.02143v1` - [abs](http://arxiv.org/abs/2102.02143v1) - [pdf](http://arxiv.org/pdf/2102.02143v1)

> The study of the acoustic emission of underwater gas bubbles is a subject of both theoretical and applied interest, since it finds an important application in the development of acoustic monitoring tools for detection and quantification of underwater gas leakages. An underlying physical model is essential in the study of such emissions, but is not enough: also some statistical procedure must be applied in order to deal with all uncertainties (including those caused by background noise). In this paper we take a probabilistic (Bayesian) methodology which is well known in the statistical signal analysis communitiy, and apply it to the problem of estimating the radii of air bubbles in water. We introduce the bubblegram, a feature extraction technique graphically similar to the traditional spectrogram but tailored to respond only to pulse structures that correspond to a given physical model. We investigate the performance of the bubblegram and our model in general using laboratory generated data.

</details>

<details>

<summary>2021-02-04 04:36:58 - Variational Inference for Deblending Crowded Starfields</summary>

- *Runjing Liu, Jon D. McAuliffe, Jeffrey Regier*

- `2102.02409v1` - [abs](http://arxiv.org/abs/2102.02409v1) - [pdf](http://arxiv.org/pdf/2102.02409v1)

> In the image data collected by astronomical surveys, stars and galaxies often overlap. Deblending is the task of distinguishing and characterizing individual light sources from survey images. We propose StarNet, a fully Bayesian method to deblend sources in astronomical images of crowded star fields. StarNet leverages recent advances in variational inference, including amortized variational distributions and the wake-sleep algorithm. Wake-sleep, which minimizes forward KL divergence, has significant benefits compared to traditional variational inference, which minimizes a reverse KL divergence. In our experiments with SDSS images of the M2 globular cluster, StarNet is substantially more accurate than two competing methods: Probablistic Cataloging (PCAT), a method that uses MCMC for inference, and a software pipeline employed by SDSS for deblending (DAOPHOT). In addition, StarNet is as much as $100,000$ times faster than PCAT, exhibiting the scaling characteristics necessary to perform fully Bayesian inference on modern astronomical surveys.

</details>

<details>

<summary>2021-02-04 07:08:31 - Shrinkage priors on complex-valued circular-symmetric autoregressive processes</summary>

- *Hidemasa Oda, Fumiyasu Komaki*

- `2004.02389v3` - [abs](http://arxiv.org/abs/2004.02389v3) - [pdf](http://arxiv.org/pdf/2004.02389v3)

> We investigate shrinkage priors on power spectral densities for complex-valued circular-symmetric autoregressive processes. We construct shrinkage predictive power spectral densities, which asymptotically dominate (i) the Bayesian predictive power spectral density based on the Jeffreys prior and (ii) the estimative power spectral density with the maximal likelihood estimator, where the Kullback-Leibler divergence from the true power spectral density to a predictive power spectral density is adopted as a risk. Furthermore, we propose general constructions of objective priors for K\"ahler parameter spaces, utilizing a positive continuous eigenfunction of the Laplace-Beltrami operator with a negative eigenvalue. We present numerical experiments on a complex-valued stationary autoregressive model of order $1$.

</details>

<details>

<summary>2021-02-04 08:59:49 - From a Point Cloud to a Simulation Model: Bayesian Segmentation and Entropy based Uncertainty Estimation for 3D Modelling</summary>

- *Christina Petschnigg, Markus Spitzner, Lucas Weitzendorf, Jürgen Pilz*

- `2102.02488v1` - [abs](http://arxiv.org/abs/2102.02488v1) - [pdf](http://arxiv.org/pdf/2102.02488v1)

> The 3D modelling of indoor environments and the generation of process simulations play an important role in factory and assembly planning. In brownfield planning cases existing data are often outdated and incomplete especially for older plants, which were mostly planned in 2D. Thus, current environment models cannot be generated directly on the basis of existing data and a holistic approach on how to build such a factory model in a highly automated fashion is mostly non-existent. Major steps in generating an environment model in a production plant include data collection and pre-processing, object identification as well as pose estimation. In this work, we elaborate a methodical workflow, which starts with the digitalization of large-scale indoor environments and ends with the generation of a static environment or simulation model. The object identification step is realized using a Bayesian neural network capable of point cloud segmentation. We elaborate how the information on network uncertainty generated by a Bayesian segmentation framework can be used in order to build up a more accurate environment model. The steps of data collection and point cloud segmentation as well as the resulting model accuracy are evaluated on a real-world data set collected at the assembly line of a large-scale automotive production plant. The segmentation network is further evaluated on the publicly available Stanford Large-Scale 3D Indoor Spaces data set. The Bayesian segmentation network clearly surpasses the performance of the frequentist baseline and allows us to increase the accuracy of the model placement in a simulation scene considerably.

</details>

<details>

<summary>2021-02-04 14:38:06 - Variational Federated Multi-Task Learning</summary>

- *Luca Corinzia, Ami Beuret, Joachim M. Buhmann*

- `1906.06268v2` - [abs](http://arxiv.org/abs/1906.06268v2) - [pdf](http://arxiv.org/pdf/1906.06268v2)

> In federated learning, a central server coordinates the training of a single model on a massively distributed network of devices. This setting can be naturally extended to a multi-task learning framework, to handle real-world federated datasets that typically show strong statistical heterogeneity among devices. Despite federated multi-task learning being shown to be an effective paradigm for real-world datasets, it has been applied only on convex models. In this work, we introduce VIRTUAL, an algorithm for federated multi-task learning for general non-convex models. In VIRTUAL the federated network of the server and the clients is treated as a star-shaped Bayesian network, and learning is performed on the network using approximated variational inference. We show that this method is effective on real-world federated datasets, outperforming the current state-of-the-art for federated learning, and concurrently allowing sparser gradient updates.

</details>

<details>

<summary>2021-02-04 22:05:59 - Hierarchical Multivariate Directed Acyclic Graph Auto-Regressive (MDAGAR) models for spatial diseases mapping</summary>

- *Leiwen Gao, Abhirup Datta, Sudipto Banerjee*

- `2102.02911v1` - [abs](http://arxiv.org/abs/2102.02911v1) - [pdf](http://arxiv.org/pdf/2102.02911v1)

> Disease mapping is an important statistical tool used by epidemiologists to assess geographic variation in disease rates and identify lurking environmental risk factors from spatial patterns. Such maps rely upon spatial models for regionally aggregated data, where neighboring regions tend to exhibit similar outcomes than those farther apart. We contribute to the literature on multivariate disease mapping, which deals with measurements on multiple (two or more) diseases in each region. We aim to disentangle associations among the multiple diseases from spatial autocorrelation in each disease. We develop Multivariate Directed Acyclic Graphical Autoregression (MDAGAR) models to accommodate spatial and inter-disease dependence. The hierarchical construction imparts flexibility and richness, interpretability of spatial autocorrelation and inter-disease relationships, and computational ease, but depends upon the order in which the cancers are modeled. To obviate this, we demonstrate how Bayesian model selection and averaging across orders are easily achieved using bridge sampling. We compare our method with a competitor using simulation studies and present an application to multiple cancer mapping using data from the Surveillance, Epidemiology, and End Results (SEER) Program.

</details>

<details>

<summary>2021-02-04 22:08:53 - aphBO-2GP-3B: A budgeted asynchronous parallel multi-acquisition functions for constrained Bayesian optimization on high-performing computing architecture</summary>

- *Anh Tran, Mike Eldred, Tim Wildey, Scott McCann, Jing Sun, Robert J. Visintainer*

- `2003.09436v2` - [abs](http://arxiv.org/abs/2003.09436v2) - [pdf](http://arxiv.org/pdf/2003.09436v2)

> High-fidelity complex engineering simulations are highly predictive, but also computationally expensive and often require substantial computational efforts. The mitigation of computational burden is usually enabled through parallelism in high-performance cluster (HPC) architecture. In this paper, an asynchronous constrained batch-parallel Bayesian optimization method is proposed to efficiently solve the computationally-expensive simulation-based optimization problems on the HPC platform, with a budgeted computational resource, where the maximum number of simulations is a constant. The advantages of this method are three-fold. First, the efficiency of the Bayesian optimization is improved, where multiple input locations are evaluated massively parallel in an asynchronous manner to accelerate the optimization convergence with respect to physical runtime. This efficiency feature is further improved so that when each of the inputs is finished, another input is queried without waiting for the whole batch to complete. Second, the method can handle both known and unknown constraints. Third, the proposed method considers several acquisition functions at the same time and sample based on an evolving probability mass distribution function using a modified GP-Hedge scheme, where parameters are corresponding to the performance of each acquisition function. The proposed framework is termed aphBO-2GP-3B, which corresponds to asynchronous parallel hedge Bayesian optimization with two Gaussian processes and three batches. The aphBO-2GP-3B framework is demonstrated using two high-fidelity expensive industrial applications, where the first one is based on finite element analysis (FEA) and the second one is based on computational fluid dynamics (CFD) simulations.

</details>

<details>

<summary>2021-02-05 00:40:22 - Fast Bayesian inference of Block Nearest Neighbor Gaussian process for large data</summary>

- *Zaida C. Quiroz, Marcos O. Prates, Dipak K. Dey, Håvard Rue*

- `1908.06437v3` - [abs](http://arxiv.org/abs/1908.06437v3) - [pdf](http://arxiv.org/pdf/1908.06437v3)

> This paper presents the development of a spatial block-Nearest Neighbor Gaussian process (block-NNGP) for location-referenced large spatial data. The key idea behind this approach is to divide the spatial domain into several blocks which are dependent under some constraints. The cross-blocks capture the large-scale spatial dependence, while each block captures the small-scale spatial dependence. The resulting block-NNGP enjoys Markov properties reflected on its sparse precision matrix. It is embedded as a prior within the class of latent Gaussian models, thus Bayesian inference is obtained using the integrated nested Laplace approximation (INLA). The performance of the block-NNGP is illustrated on simulated examples and massive real data for locations in the order of $10^4$.

</details>

<details>

<summary>2021-02-05 16:10:05 - On the estimating equations and objective functions for parameters of exponential power distribution: Application for disorder</summary>

- *Mehmet Niyazi Çankaya*

- `2102.03262v1` - [abs](http://arxiv.org/abs/2102.03262v1) - [pdf](http://arxiv.org/pdf/2102.03262v1)

> The efficient modeling for disorder in a phenomena depends on the chosen score and objective functions. The main parameters in modeling are location, scale and shape. The exponential power distribution known as generalized Gaussian is extensively used in modeling. In real world, the observations are member of different parametric models or disorder in a data set exists. In this study, estimating equations for the parameters of exponential power distribution are derived to have robust and also efficient M-estimators when the data set includes disorder or contamination. The robustness property of M-estimators for the parameters is examined. Fisher information matrices based on the derivative of score functions from $\log$, $\log_q$ and distorted log-likelihoods are proposed by use of Tsallis $q$-entropy in order to have variances of M-estimators. It is shown that matrices derived by score functions are positive semidefinite if conditions are satisfied. Information criteria inspired by Akaike and Bayesian are arranged by taking the absolute value of score functions. Fitting performances of score functions from estimating equations and objective functions are tested by applying volume, information criteria and mean absolute error which are essential tools in modeling to assess the fitting competence of the proposed functions. Applications from simulation and real data sets are carried out to compare the performance of estimating equations and objective functions. It is generally observed that the distorted log-likelihood for the estimations of parameters of exponential power distribution has superior performance than other score and objective functions for the contaminated data sets.

</details>

<details>

<summary>2021-02-05 21:34:20 - Discrete Max-Linear Bayesian Networks</summary>

- *Benjamin Hollering, Seth Sullivant*

- `2102.03426v1` - [abs](http://arxiv.org/abs/2102.03426v1) - [pdf](http://arxiv.org/pdf/2102.03426v1)

> Discrete max-linear Bayesian networks are directed graphical models specified by the same recursive structural equations as max-linear models but with discrete innovations. When all of the random variables in the model are binary, these models are isomorphic to the conjunctive Bayesian network (CBN) models of Beerenwinkel, Eriksson, and Sturmfels. Many of the techniques used to study CBN models can be extended to discrete max-linear models and similar results can be obtained. In particular, we extend the fact that CBN models are toric varieties after linear change of coordinates to all discrete max-linear models.

</details>

<details>

<summary>2021-02-05 21:57:57 - Multi-Block Sparse Functional Principal Components Analysis for Longitudinal Microbiome Multi-Omics Data</summary>

- *Lingjing Jiang, Chris Elrod, Jane J. Kim, Austin D. Swafford, Rob Knight, Wesley K. Thompson*

- `2102.00067v2` - [abs](http://arxiv.org/abs/2102.00067v2) - [pdf](http://arxiv.org/pdf/2102.00067v2)

> Microbiome researchers often need to model the temporal dynamics of multiple complex, nonlinear outcome trajectories simultaneously. This motivates our development of multivariate Sparse Functional Principal Components Analysis (mSFPCA), extending existing SFPCA methods to simultaneously characterize multiple temporal trajectories and their inter-relationships. As with existing SFPCA methods, the mSFPCA algorithm characterizes each trajectory as a smooth mean plus a weighted combination of the smooth major modes of variation about the mean, where the weights are given by the component scores for each subject. Unlike existing SFPCA methods, the mSFPCA algorithm allows estimation of multiple trajectories simultaneously, such that the component scores, which are constrained to be independent within a particular outcome for identifiability, may be arbitrarily correlated with component scores for other outcomes. A Cholesky decomposition is used to estimate the component score covariance matrix efficiently and guarantee positive semi-definiteness given these constraints. Mutual information is used to assess the strength of marginal and conditional temporal associations across outcome trajectories. Importantly, we implement mSFPCA as a Bayesian algorithm using R and stan, enabling easy use of packages such as PSIS-LOO for model selection and graphical posterior predictive checks to assess the validity of mSFPCA models. Although we focus on application of mSFPCA to microbiome data in this paper, the mSFPCA model is of general utility and can be used in a wide range of real-world applications.

</details>

<details>

<summary>2021-02-06 08:27:27 - Revisiting Explicit Regularization in Neural Networks for Well-Calibrated Predictive Uncertainty</summary>

- *Taejong Joo, Uijung Chung*

- `2006.06399v3` - [abs](http://arxiv.org/abs/2006.06399v3) - [pdf](http://arxiv.org/pdf/2006.06399v3)

> From the statistical learning perspective, complexity control via explicit regularization is a necessity for improving the generalization of over-parameterized models. However, the impressive generalization performance of neural networks with only implicit regularization may be at odds with this conventional wisdom. In this work, we revisit the importance of explicit regularization for obtaining well-calibrated predictive uncertainty. Specifically, we introduce a probabilistic measure of calibration performance, which is lower bounded by the log-likelihood. We then explore explicit regularization techniques for improving the log-likelihood on unseen samples, which provides well-calibrated predictive uncertainty. Our findings present a new direction to improve the predictive probability quality of deterministic neural networks, which can be an efficient and scalable alternative to Bayesian neural networks and ensemble methods.

</details>

<details>

<summary>2021-02-06 15:57:14 - Modeling Univariate and Multivariate Stochastic Volatility in R with stochvol and factorstochvol</summary>

- *Darjus Hosszejni, Gregor Kastner*

- `1906.12123v3` - [abs](http://arxiv.org/abs/1906.12123v3) - [pdf](http://arxiv.org/pdf/1906.12123v3)

> Stochastic volatility (SV) models are nonlinear state-space models that enjoy increasing popularity for fitting and predicting heteroskedastic time series. However, due to the large number of latent quantities, their efficient estimation is non-trivial and software that allows to easily fit SV models to data is rare. We aim to alleviate this issue by presenting novel implementations of four SV models delivered in two R packages. Several unique features are included and documented. As opposed to previous versions, stochvol is now capable of handling linear mean models, heavy-tailed SV, and SV with leverage. Moreover, we newly introduce factorstochvol which caters for multivariate SV. Both packages offer a user-friendly interface through the conventional R generics and a range of tailor-made methods. Computational efficiency is achieved via interfacing R to C++ and doing the heavy work in the latter. In the paper at hand, we provide a detailed discussion on Bayesian SV estimation and showcase the use of the new software through various examples.

</details>

<details>

<summary>2021-02-08 05:12:52 - WOMBAT: A fully Bayesian global flux-inversion framework</summary>

- *Andrew Zammit-Mangion, Michael Bertolacci, Jenny Fisher, Ann Stavert, Matthew L. Rigby, Yi Cao, Noel Cressie*

- `2102.04004v1` - [abs](http://arxiv.org/abs/2102.04004v1) - [pdf](http://arxiv.org/pdf/2102.04004v1)

> WOMBAT (the WOllongong Methodology for Bayesian Assimilation of Trace-gases) is a fully Bayesian hierarchical statistical framework for flux inversion of trace gases from flask, in situ, and remotely sensed data. WOMBAT extends the conventional Bayesian-synthesis framework through the consideration of a correlated error term, the capacity for online bias correction, and the provision of uncertainty quantification on all unknowns that appear in the Bayesian statistical model. We show, in an observing system simulation experiment (OSSE), that these extensions are crucial when the data are indeed biased and have errors that are correlated. Using the GEOS-Chem atmospheric transport model, we show that WOMBAT is able to obtain posterior means and uncertainties on non-fossil-fuel CO$_2$ fluxes from Orbiting Carbon Observatory-2 (OCO-2) data that are comparable to those from the Model Intercomparison Project (MIP) reported in Crowell et al. (2019, Atmos. Chem. Phys., vol. 19). We also find that our predictions of out-of-sample retrievals from the Total Column Carbon Observing Network are, for the most part, more accurate than those made by the MIP participants. Subsequent versions of the OCO-2 datasets will be ingested into WOMBAT as they become available.

</details>

<details>

<summary>2021-02-08 10:31:48 - Changepoint detection on a graph of time series</summary>

- *Karl L. Hallgren, Nicholas A. Heard, Melissa J. M. Turcotte*

- `2102.04112v1` - [abs](http://arxiv.org/abs/2102.04112v1) - [pdf](http://arxiv.org/pdf/2102.04112v1)

> When analysing multiple time series that may be subject to changepoints, it is sometimes possible to specify a priori, by means of a graph G, which pairs of time series are likely to be impacted by simultaneous changepoints. This article proposes a novel Bayesian changepoint model for multiple time series that borrows strength across clusters of connected time series in G to detect weak signals for synchronous changepoints. The graphical changepoint model is further extended to allow dependence between nearby but not necessarily synchronous changepoints across neighbour time series in G. A novel reversible jump MCMC algorithm making use of auxiliary variables is proposed to sample from the graphical changepoint model. The merit of the proposed model is demonstrated via a changepoint analysis of real network authentication data from Los Alamos National Laboratory (LANL), with some success at detecting weak signals for network intrusions across users that are linked by network connectivity, whilst limiting the number of false alerts.

</details>

<details>

<summary>2021-02-08 14:54:26 - Correlated Bandits for Dynamic Pricing via the ARC algorithm</summary>

- *Samuel Cohen, Tanut Treetanthiploet*

- `2102.04263v1` - [abs](http://arxiv.org/abs/2102.04263v1) - [pdf](http://arxiv.org/pdf/2102.04263v1)

> The Asymptotic Randomised Control (ARC) algorithm provides a rigorous approximation to the optimal strategy for a wide class of Bayesian bandits, while retaining reasonable computational complexity. In particular, it allows a decision maker to observe signals in addition to their rewards, to incorporate correlations between the outcomes of different choices, and to have nontrivial dynamics for their estimates. The algorithm is guaranteed to asymptotically optimise the expected discounted payoff, with error depending on the initial uncertainty of the bandit. In this paper, we consider a batched bandit problem where observations arrive from a generalised linear model; we extend the ARC algorithm to this setting. We apply this to a classic dynamic pricing problem based on a Bayesian hierarchical model and demonstrate that the ARC algorithm outperforms alternative approaches.

</details>

<details>

<summary>2021-02-08 16:21:08 - Bayesian Batch Active Learning as Sparse Subset Approximation</summary>

- *Robert Pinsler, Jonathan Gordon, Eric Nalisnick, José Miguel Hernández-Lobato*

- `1908.02144v4` - [abs](http://arxiv.org/abs/1908.02144v4) - [pdf](http://arxiv.org/pdf/1908.02144v4)

> Leveraging the wealth of unlabeled data produced in recent years provides great potential for improving supervised models. When the cost of acquiring labels is high, probabilistic active learning methods can be used to greedily select the most informative data points to be labeled. However, for many large-scale problems standard greedy procedures become computationally infeasible and suffer from negligible model change. In this paper, we introduce a novel Bayesian batch active learning approach that mitigates these issues. Our approach is motivated by approximating the complete data posterior of the model parameters. While naive batch construction methods result in correlated queries, our algorithm produces diverse batches that enable efficient active learning at scale. We derive interpretable closed-form solutions akin to existing active learning procedures for linear models, and generalize to arbitrary models using random projections. We demonstrate the benefits of our approach on several large-scale regression and classification tasks.

</details>

<details>

<summary>2021-02-08 17:42:10 - Assessing Sensitivity of Machine Learning Predictions.A Novel Toolbox with an Application to Financial Literacy</summary>

- *Falco J. Bargagli Stoffi, Kenneth De Beckker, Joana E. Maldonado, Kristof De Witte*

- `2102.04382v1` - [abs](http://arxiv.org/abs/2102.04382v1) - [pdf](http://arxiv.org/pdf/2102.04382v1)

> Despite their popularity, machine learning predictions are sensitive to potential unobserved predictors. This paper proposes a general algorithm that assesses how the omission of an unobserved variable with high explanatory power could affect the predictions of the model. Moreover, the algorithm extends the usage of machine learning from pointwise predictions to inference and sensitivity analysis. In the application, we show how the framework can be applied to data with inherent uncertainty, such as students' scores in a standardized assessment on financial literacy. First, using Bayesian Additive Regression Trees (BART), we predict students' financial literacy scores (FLS) for a subgroup of students with missing FLS. Then, we assess the sensitivity of predictions by comparing the predictions and performance of models with and without a highly explanatory synthetic predictor. We find no significant difference in the predictions and performances of the augmented (i.e., the model with the synthetic predictor) and original model. This evidence sheds a light on the stability of the predictive model used in the application. The proposed methodology can be used, above and beyond our motivating empirical example, in a wide range of machine learning applications in social and health sciences.

</details>

<details>

<summary>2021-02-08 17:50:05 - New bounds for $k$-means and information $k$-means</summary>

- *Gautier Appert, Olivier Catoni*

- `2101.05728v2` - [abs](http://arxiv.org/abs/2101.05728v2) - [pdf](http://arxiv.org/pdf/2101.05728v2)

> In this paper, we derive a new dimension-free non-asymptotic upper bound for the quadratic $k$-means excess risk related to the quantization of an i.i.d sample in a separable Hilbert space. We improve the bound of order $\mathcal{O} \bigl( k / \sqrt{n} \bigr)$ of Biau, Devroye and Lugosi, recovering the rate $\sqrt{k/n}$ that has already been proved by Fefferman, Mitter, and Narayanan and by Klochkov, Kroshnin and Zhivotovskiy but with worse log factors and constants. More precisely, we bound the mean excess risk of an empirical minimizer by the explicit upper bound $16 B^2 \log(n/k) \sqrt{k \log(k) / n}$, in the bounded case when $\mathbb{P}( \lVert X \rVert \leq B) = 1$. This is essentially optimal up to logarithmic factors since a lower bound of order $\mathcal{O} \bigl( \sqrt{k^{1 - 4/d}/n} \bigr)$ is known in dimension $d$. Our technique of proof is based on the linearization of the $k$-means criterion through a kernel trick and on PAC-Bayesian inequalities. To get a $1 / \sqrt{n}$ speed, we introduce a new PAC-Bayesian chaining method replacing the concept of $\delta$-net with the perturbation of the parameter by an infinite dimensional Gaussian process.   In the meantime, we embed the usual $k$-means criterion into a broader family built upon the Kullback divergence and its underlying properties. This results in a new algorithm that we named information $k$-means, well suited to the clustering of bags of words. Based on considerations from information theory, we also introduce a new bounded $k$-means criterion that uses a scale parameter but satisfies a generalization bound that does not require any boundedness or even integrability conditions on the sample. We describe the counterpart of Lloyd's algorithm and prove generalization bounds for these new $k$-means criteria.

</details>

<details>

<summary>2021-02-08 20:55:53 - ABCDP: Approximate Bayesian Computation with Differential Privacy</summary>

- *Mijung Park, Margarita Vinaroz, Wittawat Jitkrittum*

- `1910.05103v3` - [abs](http://arxiv.org/abs/1910.05103v3) - [pdf](http://arxiv.org/pdf/1910.05103v3)

> We develop a novel approximate Bayesian computation (ABC) framework, ABCDP, that produces differentially private (DP) and approximate posterior samples. Our framework takes advantage of the Sparse Vector Technique (SVT), widely studied in the differential privacy literature. SVT incurs the privacy cost only when a condition (whether a quantity of interest is above/below a threshold) is met. If the condition is met sparsely during the repeated queries, SVT can drastically reduces the cumulative privacy loss, unlike the usual case where every query incurs the privacy loss. In ABC, the quantity of interest is the distance between observed and simulated data, and only when the distance is below a threshold, we take the corresponding prior sample as a posterior sample. Hence, applying SVT to ABC is an organic way to transform an ABC algorithm to a privacy-preserving variant with minimal modification, but yields the posterior samples with a high privacy level. We theoretically analyze the interplay between the noise added for privacy and the accuracy of the posterior samples.

</details>

<details>

<summary>2021-02-08 21:47:24 - A Bayesian spatio-temporal nowcasting model for public health decision-making and surveillance</summary>

- *David Kline, Ayaz Hyder, Enhao Liu, Michael Rayo, Samuel Malloy, Elisabeth Root*

- `2102.04544v1` - [abs](http://arxiv.org/abs/2102.04544v1) - [pdf](http://arxiv.org/pdf/2102.04544v1)

> As COVID-19 spread through the United States in 2020, states began to set up alert systems to inform policy decisions and serve as risk communication tools for the general public. Many of these systems, like in Ohio, included indicators based on an assessment of trends in reported cases. However, when cases are indexed by date of disease onset, reporting delays complicate the interpretation of trends. Despite a foundation of statistical literature to address this problem, these methods have not been widely applied in practice. In this paper, we develop a Bayesian spatio-temporal nowcasting model for assessing trends in county-level COVID-19 cases in Ohio. We compare the performance of our model to the current approach used in Ohio and the approach that was recommended by the Centers for Disease Control and Prevention. We demonstrate gains in performance while still retaining interpretability using our model. In addition, we are able to fully account for uncertainty in both the time series of cases and in the reporting process. While we cannot eliminate all of the uncertainty in public health surveillance and subsequent decision-making, we must use approaches that embrace these challenges and deliver more accurate and honest assessments to policymakers.

</details>

<details>

<summary>2021-02-08 23:03:48 - Risk-Efficient Bayesian Data Synthesis for Privacy Protection</summary>

- *Jingchen Hu, Terrance D. Savitsky, Matthew R. Williams*

- `1908.07639v6` - [abs](http://arxiv.org/abs/1908.07639v6) - [pdf](http://arxiv.org/pdf/1908.07639v6)

> Statistical agencies utilize models to synthesize respondent-level data for release to the public for privacy protection. In this work, we efficiently induce privacy protection into any Bayesian synthesis model by employing a pseudo likelihood that exponentiates each likelihood contribution by an observation record-indexed weight in [0, 1], defined to be inversely proportional to the identification risk for that record. We start with the marginal probability of identification risk for a record, which is composed as the probability that the identity of the record may be disclosed. Our application to the Consumer Expenditure Surveys (CE) of the U.S. Bureau of Labor Statistics demonstrates that the marginally risk-adjusted synthesizer provides an overall improved privacy protection; however, the identification risks actually increase for some moderate-risk records after risk-adjusted pseudo posterior estimation synthesis due to increased isolation after weighting; a phenomenon we label "whack-a-mole". We proceed to construct a weight for each record from a collection of pairwise identification risk probabilities with other records, where each pairwise probability measures the joint probability of re-identification of the pair of records, which mitigates the whack-a-mole issue and produces a more efficient set of synthetic data with lower risk and higher utility for the CE data.

</details>

<details>

<summary>2021-02-09 16:50:52 - A Robust Bayesian Copas Selection Model for Quantifying and Correcting Publication Bias</summary>

- *Ray Bai, Lifeng Lin, Mary R. Boland, Yong Chen*

- `2005.02930v3` - [abs](http://arxiv.org/abs/2005.02930v3) - [pdf](http://arxiv.org/pdf/2005.02930v3)

> The validity of conclusions from meta-analysis is potentially threatened by publication bias. Most existing procedures for correcting publication bias assume normality of the study-specific effects that account for between-study heterogeneity. However, this assumption may not be valid, and the performance of these bias correction procedures can be highly sensitive to departures from normality. Further, there exist few measures to quantify the magnitude of publication bias based on selection models. In this paper, we address both of these issues. First, we explore the use of heavy-tailed distributions for the study-specific effects within a Bayesian hierarchical framework. The deviance information criterion (DIC) is used to determine the appropriate distribution to use for conducting the final analysis. Second, we develop a new measure to quantify the magnitude of publication bias based on Hellinger distance. Our measure is easy to interpret and takes advantage of the estimation uncertainty afforded naturally by the posterior distribution. We illustrate our proposed approach through simulation studies and meta-analyses on lung cancer and antidepressants. To assess the prevalence of publication bias, we apply our method to 1500 meta-analyses of dichotomous outcomes in the Cochrane Database of Systematic Reviews. Our methods are implemented in the publicly available R package RobustBayesianCopas.

</details>

<details>

<summary>2021-02-09 23:56:24 - Bayesian Bi-clustering Methods with Applications in Computational Biology</summary>

- *Han Yan, Jiexing Wu, Yang Li, Jun S. Liu*

- `2007.06136v2` - [abs](http://arxiv.org/abs/2007.06136v2) - [pdf](http://arxiv.org/pdf/2007.06136v2)

> Bi-clustering is a useful approach in analyzing biological data when observations come from heterogeneous groups and have a large number of features. We outline a general Bayesian approach in tackling bi-clustering problems in moderate to high dimensions, and propose three Bayesian bi-clustering models on categorical data, which increase in complexities in their modeling of the distributions of features across bi-clusters. Our proposed methods apply to a wide range of scenarios: from situations where data are cluster-distinguishable only among a small subset of features but masked by a large amount of noise, to situations where different groups of data are identified by different sets of features or data exhibit hierarchical structures. Through simulation studies, we show that our methods outperform existing (bi-)clustering methods in both identifying clusters and recovering feature distributional patterns across bi-clusters. We apply our methods to two genetic datasets, though the area of application of our methods is even broader.

</details>

<details>

<summary>2021-02-10 01:19:15 - Attentive Gaussian processes for probabilistic time-series generation</summary>

- *Kuilin Chen, Chi-Guhn Lee*

- `2102.05208v1` - [abs](http://arxiv.org/abs/2102.05208v1) - [pdf](http://arxiv.org/pdf/2102.05208v1)

> The transduction of sequence has been mostly done by recurrent networks, which are computationally demanding and often underestimate uncertainty severely. We propose a computationally efficient attention-based network combined with the Gaussian process regression to generate real-valued sequence, which we call the Attentive-GP. The proposed model not only improves the training efficiency by dispensing recurrence and convolutions but also learns the factorized generative distribution with Bayesian representation. However, the presence of the GP precludes the commonly used mini-batch approach to the training of the attention network. Therefore, we develop a block-wise training algorithm to allow mini-batch training of the network while the GP is trained using full-batch, resulting in a scalable training method. The algorithm has been proved to converge and shows comparable, if not better, quality of the found solution. As the algorithm does not assume any specific network architecture, it can be used with a wide range of hybrid models such as neural networks with kernel machine layers in the scarcity of resources for computation and memory.

</details>

<details>

<summary>2021-02-10 02:21:41 - Bayesian Knockoff Filter Using Gibbs Sampler</summary>

- *Jiaqi Gu, Guosheng Yin*

- `2102.05223v1` - [abs](http://arxiv.org/abs/2102.05223v1) - [pdf](http://arxiv.org/pdf/2102.05223v1)

> In many fields, researchers are interested in discovering features with substantial effect on the response from a large number of features and controlling the proportion of false discoveries. By incorporating the knockoff procedure in the Bayesian framework, we develop the Bayesian knockoff filter (BKF) for selecting features that have important effect on the response. In contrast to the fixed knockoff variables in the frequentist procedures, we allow the knockoff variables to be continuously updated in the Markov chain Monte Carlo. Based on the posterior samples and elaborated greedy selection procedures, our method can distinguish the truly important features as well as controlling the Bayesian false discovery rate at a desirable level. Numerical experiments on both synthetic and real data demonstrate the advantages of our method over existing knockoff methods and Bayesian variable selection approaches, i.e., the BKF possesses higher power and yields a lower false discovery rate.

</details>

<details>

<summary>2021-02-10 03:03:01 - Bayesian Parameter Identification for Jump Markov Linear Systems</summary>

- *Mark P. Balenzuela, Adrian G. Wills, Christopher Renton, Brett Ninness*

- `2004.08565v2` - [abs](http://arxiv.org/abs/2004.08565v2) - [pdf](http://arxiv.org/pdf/2004.08565v2)

> This paper presents a Bayesian method for identification of jump Markov linear system parameters. A primary motivation is to provide accurate quantification of parameter uncertainty without relying on asymptotic in data-length arguments. To achieve this, the paper details a particle-Gibbs sampling approach that provides samples from the desired posterior distribution. These samples are produced by utilising a modified discrete particle filter and carefully chosen conjugate priors.

</details>

<details>

<summary>2021-02-10 14:09:57 - Fitting stochastic predator-prey models using both population density and kill rate data</summary>

- *Frederic Barraquand, Olivier Gimenez*

- `1904.02145v3` - [abs](http://arxiv.org/abs/1904.02145v3) - [pdf](http://arxiv.org/pdf/1904.02145v3)

> Most mechanistic predator-prey modelling has involved either parameterization from process rate data or inverse modelling. Here, we take a median road: we aim at identifying the potential benefits of combining datasets, when both population growth and predation processes are viewed as stochastic. We fit a discrete-time, stochastic predator-prey model of the Leslie type to simulated time series of densities and kill rate data. Our model has both environmental stochasticity in the growth rates and interaction stochasticity, i.e., a stochastic functional response. We examine what the kill rate data brings to the quality of the estimates, and whether estimation is possible (for various time series lengths) solely with time series of population counts or biomass data. Both Bayesian and frequentist estimation are performed, providing multiple ways to check model identifiability. The Fisher Information Matrix suggests that models with and without kill rate data are all identifiable, although correlations remain between parameters that belong to the same functional form. However, our results show that if the attractor is a fixed point in the absence of stochasticity, identifying parameters in practice requires kill rate data as a complement to the time series of population densities, due to the relatively flat likelihood. Only noisy limit cycle attractors can be identified directly from population count data (as in inverse modelling), although even in this case, adding kill rate data - including in small amounts - can make the estimates much more precise. Overall, we show that under process stochasticity in interaction rates, interaction data might be essential to obtain identifiable dynamical models for multiple species. These results may extend to other biotic interactions than predation, for which similar models combining interaction rates and population counts could be developed.

</details>

<details>

<summary>2021-02-10 18:52:08 - Automatic structured variational inference</summary>

- *Luca Ambrogioni, Kate Lin, Emily Fertig, Sharad Vikram, Max Hinne, Dave Moore, Marcel van Gerven*

- `2002.00643v3` - [abs](http://arxiv.org/abs/2002.00643v3) - [pdf](http://arxiv.org/pdf/2002.00643v3)

> Stochastic variational inference offers an attractive option as a default method for differentiable probabilistic programming. However, the performance of the variational approach depends on the choice of an appropriate variational family. Here, we introduce automatic structured variational inference (ASVI), a fully automated method for constructing structured variational families, inspired by the closed-form update in conjugate Bayesian models. These convex-update families incorporate the forward pass of the input probabilistic program and can therefore capture complex statistical dependencies. Convex-update families have the same space and time complexity as the input probabilistic program and are therefore tractable for a very large family of models including both continuous and discrete variables. We validate our automatic variational method on a wide range of low- and high-dimensional inference problems. We find that ASVI provides a clear improvement in performance when compared with other popular approaches such as the mean-field approach and inverse autoregressive flows. We provide an open source implementation of ASVI in TensorFlow Probability.

</details>

<details>

<summary>2021-02-10 20:22:36 - Sequential Adaptive Design for Jump Regression Estimation</summary>

- *Chiwoo Park, Peihua Qiu, Jennifer Carpena-Núñez, Rahul Rao, Michael Susner, Benji Maruyama*

- `1904.01648v4` - [abs](http://arxiv.org/abs/1904.01648v4) - [pdf](http://arxiv.org/pdf/1904.01648v4)

> Selecting input variables or design points for statistical models has been of great interest in adaptive design and active learning. Motivated by two scientific examples, this paper presents a strategy of selecting the design points for a regression model when the underlying regression function is discontinuous. The first example we undertook was for the purpose of accelerating imaging speed in a high resolution material imaging; the second was use of sequential design for the purpose of mapping a chemical phase diagram. In both examples, the underlying regression functions have discontinuities, so many of the existing design optimization approaches cannot be applied because they mostly assume a continuous regression function. Although some existing adaptive design strategies developed from treed regression models can handle the discontinuities, the Bayesian approaches come with computationally expensive Markov Chain Monte Carlo techniques for posterior inferences and subsequent design point selections, which is not appropriate for the first motivating example that requires computation at least faster than the original imaging speed. In addition, the treed models are based on the domain partitioning that are inefficient when the discontinuities occurs over complex sub-domain boundaries. We propose a simple and effective adaptive design strategy for a regression analysis with discontinuities: some statistical properties with a fixed design will be presented first, and then these properties will be used to propose a new criterion of selecting the design points for the regression analysis. Sequential design with the new criterion will be presented with comprehensive simulated examples, and its application to the two motivating examples will be presented.

</details>

<details>

<summary>2021-02-11 04:05:45 - Bayesian multiscale deep generative model for the solution of high-dimensional inverse problems</summary>

- *Yingzhi Xia, Nicholas Zabaras*

- `2102.03169v2` - [abs](http://arxiv.org/abs/2102.03169v2) - [pdf](http://arxiv.org/pdf/2102.03169v2)

> Estimation of spatially-varying parameters for computationally expensive forward models governed by partial differential equations is addressed. A novel multiscale Bayesian inference approach is introduced based on deep probabilistic generative models. Such generative models provide a flexible representation by inferring on each scale a low-dimensional latent encoding while allowing hierarchical parameter generation from coarse- to fine-scales. Combining the multiscale generative model with Markov Chain Monte Carlo (MCMC), inference across scales is achieved enabling us to efficiently obtain posterior parameter samples at various scales. The estimation of coarse-scale parameters using a low-dimensional latent embedding captures global and notable parameter features using an inexpensive but inaccurate solver. MCMC sampling of the fine-scale parameters is enabled by utilizing the posterior information in the immediate coarser-scale. In this way, the global features are identified in the coarse-scale with inference of low-dimensional variables and inexpensive forward computation, and the local features are refined and corrected in the fine-scale. The developed method is demonstrated with two types of permeability estimation for flow in heterogeneous media. One is a Gaussian random field (GRF) with uncertain length scales, and the other is channelized permeability with the two regions defined by different GRFs. The obtained results indicate that the method allows high-dimensional parameter estimation while exhibiting stability, efficiency and accuracy.

</details>

<details>

<summary>2021-02-11 07:27:25 - A Bayesian nonparametric approach to count-min sketch under power-law data streams</summary>

- *Emanuele Dolera, Stefano Favaro, Stefano Peluchetti*

- `2102.03743v2` - [abs](http://arxiv.org/abs/2102.03743v2) - [pdf](http://arxiv.org/pdf/2102.03743v2)

> The count-min sketch (CMS) is a randomized data structure that provides estimates of tokens' frequencies in a large data stream using a compressed representation of the data by random hashing. In this paper, we rely on a recent Bayesian nonparametric (BNP) view on the CMS to develop a novel learning-augmented CMS under power-law data streams. We assume that tokens in the stream are drawn from an unknown discrete distribution, which is endowed with a normalized inverse Gaussian process (NIGP) prior. Then, using distributional properties of the NIGP, we compute the posterior distribution of a token's frequency in the stream, given the hashed data, and in turn corresponding BNP estimates. Applications to synthetic and real data show that our approach achieves a remarkable performance in the estimation of low-frequency tokens. This is known to be a desirable feature in the context of natural language processing, where it is indeed common in the context of the power-law behaviour of the data.

</details>

<details>

<summary>2021-02-11 09:18:13 - Physics-Constrained Predictive Molecular Latent Space Discovery with Graph Scattering Variational Autoencoder</summary>

- *Navid Shervani-Tabar, Nicholas Zabaras*

- `2009.13878v2` - [abs](http://arxiv.org/abs/2009.13878v2) - [pdf](http://arxiv.org/pdf/2009.13878v2)

> Recent advances in artificial intelligence have propelled the development of innovative computational materials modeling and design techniques. Generative deep learning models have been used for molecular representation, discovery, and design. In this work, we assess the predictive capabilities of a molecular generative model developed based on variational inference and graph theory in the small data regime. Physical constraints that encourage energetically stable molecules are proposed. The encoding network is based on the scattering transform with adaptive spectral filters to allow for better generalization of the model. The decoding network is a one-shot graph generative model that conditions atom types on molecular topology. A Bayesian formalism is considered to capture uncertainties in the predictive estimates of molecular properties. The model's performance is evaluated by generating molecules with desired target properties.

</details>

<details>

<summary>2021-02-11 17:36:30 - Variational Bayesian Sequence-to-Sequence Networks for Memory-Efficient Sign Language Translation</summary>

- *Harris Partaourides, Andreas Voskou, Dimitrios Kosmopoulos, Sotirios Chatzis, Dimitris N. Metaxas*

- `2102.06143v1` - [abs](http://arxiv.org/abs/2102.06143v1) - [pdf](http://arxiv.org/pdf/2102.06143v1)

> Memory-efficient continuous Sign Language Translation is a significant challenge for the development of assisted technologies with real-time applicability for the deaf. In this work, we introduce a paradigm of designing recurrent deep networks whereby the output of the recurrent layer is derived from appropriate arguments from nonparametric statistics. A novel variational Bayesian sequence-to-sequence network architecture is proposed that consists of a) a full Gaussian posterior distribution for data-driven memory compression and b) a nonparametric Indian Buffet Process prior for regularization applied on the Gated Recurrent Unit non-gate weights. We dub our approach Stick-Breaking Recurrent network and show that it can achieve a substantial weight compression without diminishing modeling performance.

</details>

<details>

<summary>2021-02-11 19:31:04 - Variable Selection via Thompson Sampling</summary>

- *Yi Liu, Veronika Rockova*

- `2007.00187v2` - [abs](http://arxiv.org/abs/2007.00187v2) - [pdf](http://arxiv.org/pdf/2007.00187v2)

> Thompson sampling is a heuristic algorithm for the multi-armed bandit problem which has a long tradition in machine learning. The algorithm has a Bayesian spirit in the sense that it selects arms based on posterior samples of reward probabilities of each arm. By forging a connection between combinatorial binary bandits and spike-and-slab variable selection, we propose a stochastic optimization approach to subset selection called Thompson Variable Selection (TVS). TVS is a framework for interpretable machine learning which does not rely on the underlying model to be linear. TVS brings together Bayesian reinforcement and machine learning in order to extend the reach of Bayesian subset selection to non-parametric models and large datasets with very many predictors and/or very many observations. Depending on the choice of a reward, TVS can be deployed in offline as well as online setups with streaming data batches. Tailoring multiplay bandits to variable selection, we provide regret bounds without necessarily assuming that the arm mean rewards be unrelated. We show a very strong empirical performance on both simulated and real data. Unlike deterministic optimization methods for spike-and-slab variable selection, the stochastic nature makes TVS less prone to local convergence and thereby more robust.

</details>

<details>

<summary>2021-02-12 01:13:00 - The AL-Gaussian Distribution as the Descriptive Model for the Internal Proactive Inhibition in the Standard Stop Signal Task</summary>

- *Mohsen Soltanifar, Michael Escobar, Annie Dupuis, Andre Chevrier, Russell Schachar*

- `2101.11682v2` - [abs](http://arxiv.org/abs/2101.11682v2) - [pdf](http://arxiv.org/pdf/2101.11682v2)

> Measurements of response inhibition components of reactive inhibition and proactive inhibition within the stop signal paradigm have been of special interest for researchers since the 1980s. While frequentist nonparametric and Bayesian parametric methods have been proposed to precisely estimate the entire distribution of reactive inhibition, quantified by stop signal reaction times(SSRT), there is no method yet in the stop-signal task literature to precisely estimate the entire distribution of proactive inhibition. We introduce an Asymmetric Laplace Gaussian (ALG) model to describe the distribution of proactive inhibition. The proposed method is based on two assumptions of independent trial type(go/stop) reaction times, and Ex-Gaussian (ExG) models for them. Results indicated that the four parametric, ALG model uniquely describes the proactive inhibition distribution and its key shape features; and, its hazard function is monotonically increasing as are its three parametric ExG components. In conclusion, both response inhibition components can be uniquely modeled via variations of the four parametric ALG model described with their associated similar distributional features.

</details>

<details>

<summary>2021-02-12 08:17:23 - Offline Meta Learning of Exploration</summary>

- *Ron Dorfman, Idan Shenfeld, Aviv Tamar*

- `2008.02598v3` - [abs](http://arxiv.org/abs/2008.02598v3) - [pdf](http://arxiv.org/pdf/2008.02598v3)

> Consider the following instance of the Offline Meta Reinforcement Learning (OMRL) problem: given the complete training logs of $N$ conventional RL agents, trained on $N$ different tasks, design a meta-agent that can quickly maximize reward in a new, unseen task from the same task distribution. In particular, while each conventional RL agent explored and exploited its own different task, the meta-agent must identify regularities in the data that lead to effective exploration/exploitation in the unseen task. Here, we take a Bayesian RL (BRL) view, and seek to learn a Bayes-optimal policy from the offline data. Building on the recent VariBAD BRL approach, we develop an off-policy BRL method that learns to plan an exploration strategy based on an adaptive neural belief estimate. However, learning to infer such a belief from offline data brings a new identifiability issue we term MDP ambiguity. We characterize the problem, and suggest resolutions via data collection and modification procedures. Finally, we evaluate our framework on a diverse set of domains, including difficult sparse reward tasks, and demonstrate learning of effective exploration behavior that is qualitatively different from the exploration used by any RL agent in the data.

</details>

<details>

<summary>2021-02-12 10:50:44 - Equivalence class selection of categorical graphical models</summary>

- *Federico Castelletti, Stefano Peluso*

- `2102.06437v1` - [abs](http://arxiv.org/abs/2102.06437v1) - [pdf](http://arxiv.org/pdf/2102.06437v1)

> Learning the structure of dependence relations between variables is a pervasive issue in the statistical literature. A directed acyclic graph (DAG) can represent a set of conditional independences, but different DAGs may encode the same set of relations and are indistinguishable using observational data. Equivalent DAGs can be collected into classes, each represented by a partially directed graph known as essential graph (EG). Structure learning directly conducted on the EG space, rather than on the allied space of DAGs, leads to theoretical and computational benefits. Still, the majority of efforts in the literature has been dedicated to Gaussian data, with less attention to methods designed for multivariate categorical data. We then propose a Bayesian methodology for structure learning of categorical EGs. Combining a constructive parameter prior elicitation with a graph-driven likelihood decomposition, we derive a closed-form expression for the marginal likelihood of a categorical EG model. Asymptotic properties are studied, and an MCMC sampler scheme developed for approximate posterior inference. We evaluate our methodology on both simulated scenarios and real data, with appreciable performance in comparison with state-of-the-art methods.

</details>

<details>

<summary>2021-02-12 11:37:22 - Active Multi-Information Source Bayesian Quadrature</summary>

- *Alexandra Gessner, Javier Gonzalez, Maren Mahsereci*

- `1903.11331v3` - [abs](http://arxiv.org/abs/1903.11331v3) - [pdf](http://arxiv.org/pdf/1903.11331v3)

> Bayesian quadrature (BQ) is a sample-efficient probabilistic numerical method to solve integrals of expensive-to-evaluate black-box functions, yet so far,active BQ learning schemes focus merely on the integrand itself as information source, and do not allow for information transfer from cheaper, related functions. Here, we set the scene for active learning in BQ when multiple related information sources of variable cost (in input and source) are accessible. This setting arises for example when evaluating the integrand requires a complex simulation to be run that can be approximated by simulating at lower levels of sophistication and at lesser expense. We construct meaningful cost-sensitive multi-source acquisition rates as an extension to common utility functions from vanilla BQ (VBQ),and discuss pitfalls that arise from blindly generalizing. Furthermore, we show that the VBQ acquisition policy is a corner-case of all considered cost-sensitive acquisition schemes, which collapse onto one single de-generate policy in the case of one source and constant cost. In proof-of-concept experiments we scrutinize the behavior of our generalized acquisition functions. On an epidemiological model, we demonstrate that active multi-source BQ (AMS-BQ) allocates budget more efficiently than VBQ for learning the integral to a good accuracy.

</details>

<details>

<summary>2021-02-12 11:38:55 - Identification and Inference Under Narrative Restrictions</summary>

- *Raffaella Giacomini, Toru Kitagawa, Matthew Read*

- `2102.06456v1` - [abs](http://arxiv.org/abs/2102.06456v1) - [pdf](http://arxiv.org/pdf/2102.06456v1)

> We consider structural vector autoregressions subject to 'narrative restrictions', which are inequality restrictions on functions of the structural shocks in specific periods. These restrictions raise novel problems related to identification and inference, and there is currently no frequentist procedure for conducting inference in these models. We propose a solution that is valid from both Bayesian and frequentist perspectives by: 1) formalizing the identification problem under narrative restrictions; 2) correcting a feature of the existing (single-prior) Bayesian approach that can distort inference; 3) proposing a robust (multiple-prior) Bayesian approach that is useful for assessing and eliminating the posterior sensitivity that arises in these models due to the likelihood having flat regions; and 4) showing that the robust Bayesian approach has asymptotic frequentist validity. We illustrate our methods by estimating the effects of US monetary policy under a variety of narrative restrictions.

</details>

<details>

<summary>2021-02-12 11:40:18 - Optimal quantisation of probability measures using maximum mean discrepancy</summary>

- *Onur Teymur, Jackson Gorham, Marina Riabiz, Chris. J. Oates*

- `2010.07064v4` - [abs](http://arxiv.org/abs/2010.07064v4) - [pdf](http://arxiv.org/pdf/2010.07064v4)

> Several researchers have proposed minimisation of maximum mean discrepancy (MMD) as a method to quantise probability measures, i.e., to approximate a target distribution by a representative point set. We consider sequential algorithms that greedily minimise MMD over a discrete candidate set. We propose a novel non-myopic algorithm and, in order to both improve statistical efficiency and reduce computational cost, we investigate a variant that applies this technique to a mini-batch of the candidate set at each iteration. When the candidate points are sampled from the target, the consistency of these new algorithm - and their mini-batch variants - is established. We demonstrate the algorithms on a range of important computational problems, including optimisation of nodes in Bayesian cubature and the thinning of Markov chain output.

</details>

<details>

<summary>2021-02-12 16:23:54 - A Dynamical Systems Approach for Convergence of the Bayesian EM Algorithm</summary>

- *Orlando Romero, Subhro Das, Pin-Yu Chen, Sérgio Pequito*

- `2006.12690v2` - [abs](http://arxiv.org/abs/2006.12690v2) - [pdf](http://arxiv.org/pdf/2006.12690v2)

> Out of the recent advances in systems and control (S\&C)-based analysis of optimization algorithms, not enough work has been specifically dedicated to machine learning (ML) algorithms and its applications. This paper addresses this gap by illustrating how (discrete-time) Lyapunov stability theory can serve as a powerful tool to aid, or even lead, in the analysis (and potential design) of optimization algorithms that are not necessarily gradient-based. The particular ML problem that this paper focuses on is that of parameter estimation in an incomplete-data Bayesian framework via the popular optimization algorithm known as maximum a posteriori expectation-maximization (MAP-EM). Following first principles from dynamical systems stability theory, conditions for convergence of MAP-EM are developed. Furthermore, if additional assumptions are met, we show that fast convergence (linear or quadratic) is achieved, which could have been difficult to unveil without our adopted S\&C approach. The convergence guarantees in this paper effectively expand the set of sufficient conditions for EM applications, thereby demonstrating the potential of similar S\&C-based convergence analysis of other ML algorithms.

</details>

<details>

<summary>2021-02-12 16:47:28 - Graphical Normalizing Flows</summary>

- *Antoine Wehenkel, Gilles Louppe*

- `2006.02548v3` - [abs](http://arxiv.org/abs/2006.02548v3) - [pdf](http://arxiv.org/pdf/2006.02548v3)

> Normalizing flows model complex probability distributions by combining a base distribution with a series of bijective neural networks. State-of-the-art architectures rely on coupling and autoregressive transformations to lift up invertible functions from scalars to vectors. In this work, we revisit these transformations as probabilistic graphical models, showing they reduce to Bayesian networks with a pre-defined topology and a learnable density at each node. From this new perspective, we propose the graphical normalizing flow, a new invertible transformation with either a prescribed or a learnable graphical structure. This model provides a promising way to inject domain knowledge into normalizing flows while preserving both the interpretability of Bayesian networks and the representation capacity of normalizing flows. We show that graphical conditioners discover relevant graph structure when we cannot hypothesize it. In addition, we analyze the effect of $\ell_1$-penalization on the recovered structure and on the quality of the resulting density estimation. Finally, we show that graphical conditioners lead to competitive white box density estimators. Our implementation is available at https://github.com/AWehenkel/DAG-NF.

</details>

<details>

<summary>2021-02-12 16:52:56 - Minimax Quasi-Bayesian estimation in sparse canonical correlation analysis via a Rayleigh quotient function</summary>

- *Qiuyun Zhu, Yves Atchade*

- `2010.08627v2` - [abs](http://arxiv.org/abs/2010.08627v2) - [pdf](http://arxiv.org/pdf/2010.08627v2)

> Canonical correlation analysis (CCA) is a popular statistical technique for exploring the relationship between datasets. The estimation of sparse canonical correlation vectors has emerged in recent years as an important but challenging variation of the CCA problem, with widespread applications. Currently available rate-optimal estimators for sparse canonical correlation vectors are expensive to compute. We propose a quasi-Bayesian estimation procedure that achieves the minimax estimation rate, and yet is easy to compute by Markov Chain Monte Carlo (MCMC). The method builds on ([37]) and uses a re-scaled Rayleigh quotient function as a quasi-log-likelihood. However unlike these authors, we adopt a Bayesian framework that combines this quasi-log-likelihood with a spike-and-slab prior that serves to regularize the inference and promote sparsity. We investigated the empirical behavior of the proposed method on both continuous and truncated data, and we noted that it outperforms several state-of-the-art methods. As an application, we use the methodology to maximally correlate clinical variables and proteomic data for a better understanding of covid-19 disease.

</details>

<details>

<summary>2021-02-12 19:22:18 - A Bayesian cohort component projection model to estimate adult populations at the subnational level in data-sparse settings</summary>

- *Monica Alexander, Leontine Alkema*

- `2102.06121v2` - [abs](http://arxiv.org/abs/2102.06121v2) - [pdf](http://arxiv.org/pdf/2102.06121v2)

> Accurate estimates of subnational populations are important for policy formulation and monitoring population health indicators. For example, estimates of the number of women of reproductive age are important to understand the population at risk to maternal mortality and unmet need for contraception. However, in many low-income countries, data on population counts and components of population change are limited, and so levels and trends subnationally are unclear. We present a Bayesian constrained cohort component model for the estimation and projection of subnational populations. The model builds on a cohort component projection framework, incorporates census data and estimates from the United Nation's World Population Prospects, and uses characteristic mortality schedules to obtain estimates of population counts and the components of population change, including internal migration. The data required as inputs to the model are minimal and available across a wide range of countries, including most low-income countries. The model is applied to estimate and project populations by county in Kenya for 1979-2019, and validated against the 2019 Kenyan census.

</details>

<details>

<summary>2021-02-12 20:39:07 - More for less: Predicting and maximizing genetic variant discovery via Bayesian nonparametrics</summary>

- *Lorenzo Masoero, Federico Camerlenghi, Stefano Favaro, Tamara Broderick*

- `1912.05516v3` - [abs](http://arxiv.org/abs/1912.05516v3) - [pdf](http://arxiv.org/pdf/1912.05516v3)

> While the cost of sequencing genomes has decreased dramatically in recent years, this expense often remains non-trivial. Under a fixed budget, then, scientists face a natural trade-off between quantity and quality; they can spend resources to sequence a greater number of genomes (quantity) or spend resources to sequence genomes with increased accuracy (quality). Our goal is to find the optimal allocation of resources between quantity and quality. Optimizing resource allocation promises to reveal as many new variations in the genome as possible, and thus as many new scientific insights as possible. In this paper, we consider the common setting where scientists have already conducted a pilot study to reveal variants in a genome and are contemplating a follow-up study. We introduce a Bayesian nonparametric methodology to predict the number of new variants in the follow-up study based on the pilot study. When experimental conditions are kept constant between the pilot and follow-up, we demonstrate on real data from the gnomAD project that our prediction is more accurate than three recent proposals, and competitive with a more classic proposal. Unlike existing methods, though, our method allows practitioners to change experimental conditions between the pilot and the follow-up. We demonstrate how this distinction allows our method to be used for (i) more realistic predictions and (ii) optimal allocation of a fixed budget between quality and quantity.

</details>

<details>

<summary>2021-02-13 11:29:52 - The Pearson Bayes factor: An analytic formula for computing evidential value from minimal summary statistics</summary>

- *Thomas J. Faulkenberry*

- `2011.09549v2` - [abs](http://arxiv.org/abs/2011.09549v2) - [pdf](http://arxiv.org/pdf/2011.09549v2)

> In Bayesian hypothesis testing, evidence for a statistical model is quantified by the Bayes factor, which represents the relative likelihood of observed data under that model compared to another competing model. In general, computing Bayes factors is difficult, as computing the marginal likelihood of data under a given model requires integrating over a prior distribution of model parameters. In this paper, I capitalize on a particular choice of prior distribution that allows the Bayes factor to be expressed without integral representation and I develop a simple formula -- the Pearson Bayes factor -- that requires only minimal summary statistics commonly reported in scientific papers, such as the $t$ or $F$ score and the degrees of freedom. In addition to presenting this new result, I provide several examples of its use and report a simulation study validating its performance. Importantly, the Pearson Bayes factor gives applied researchers the ability to compute exact Bayes factors from minimal summary data, and thus easily assess the evidential value of any data for which these summary statistics are provided, even when the original data is not available.

</details>

<details>

<summary>2021-02-14 08:53:43 - Healing Products of Gaussian Processes</summary>

- *Samuel Cohen, Rendani Mbuvha, Tshilidzi Marwala, Marc Peter Deisenroth*

- `2102.07106v1` - [abs](http://arxiv.org/abs/2102.07106v1) - [pdf](http://arxiv.org/pdf/2102.07106v1)

> Gaussian processes (GPs) are nonparametric Bayesian models that have been applied to regression and classification problems. One of the approaches to alleviate their cubic training cost is the use of local GP experts trained on subsets of the data. In particular, product-of-expert models combine the predictive distributions of local experts through a tractable product operation. While these expert models allow for massively distributed computation, their predictions typically suffer from erratic behaviour of the mean or uncalibrated uncertainty quantification. By calibrating predictions via a tempered softmax weighting, we provide a solution to these problems for multiple product-of-expert models, including the generalised product of experts and the robust Bayesian committee machine. Furthermore, we leverage the optimal transport literature and propose a new product-of-expert model that combines predictions of local experts by computing their Wasserstein barycenter, which can be applied to both regression and classification.

</details>

<details>

<summary>2021-02-14 19:16:15 - Global jump filters and quasi-likelihood analysis for volatility</summary>

- *Haruhiko Inatsugu, Nakahiro Yoshida*

- `1806.10706v3` - [abs](http://arxiv.org/abs/1806.10706v3) - [pdf](http://arxiv.org/pdf/1806.10706v3)

> We propose a new estimation scheme for estimation of the volatility parameters of a semimartingale with jumps based on a jump-detection filter. Our filter uses all of data to analyze the relative size of increments and to discriminate jumps more precisely. We construct quasi-maximum likelihood estimators and quasi-Bayesian estimators, and show limit theorems for them including $L^p$-estimates of the error and asymptotic mixed normality based on the framework of the quasi-likelihood analysis. The global jump filters do not need a restrictive condition for the distribution of the small jumps. By numerical simulation we show that our "global" method obtains better estimates of the volatility parameter than the previous "local" methods.

</details>

<details>

<summary>2021-02-15 02:07:12 - Spatial Factor Modeling: A Bayesian Matrix-Normal Approach for Misaligned Data</summary>

- *Lu Zhang, Sudipto Banerjee*

- `2006.00595v2` - [abs](http://arxiv.org/abs/2006.00595v2) - [pdf](http://arxiv.org/pdf/2006.00595v2)

> Multivariate spatially-oriented data sets are prevalent in the environmental and physical sciences. Scientists seek to jointly model multiple variables, each indexed by a spatial location, to capture any underlying spatial association for each variable and associations among the different dependent variables. Multivariate latent spatial process models have proved effective in driving statistical inference and rendering better predictive inference at arbitrary locations for the spatial process. High-dimensional multivariate spatial data, which is the theme of this article, refers to data sets where the number of spatial locations and the number of spatially dependent variables is very large. The field has witnessed substantial developments in scalable models for univariate spatial processes, but such methods for multivariate spatial processes, especially when the number of outcomes are moderately large, are limited in comparison. Here, we extend scalable modeling strategies for a single process to multivariate processes. We pursue Bayesian inference which is attractive for full uncertainty quantification of the latent spatial process. Our approach exploits distribution theory for the Matrix-Normal distribution, which we use to construct scalable versions of a hierarchical linear model of coregionalization (LMC) and spatial factor models that deliver inference over a high-dimensional parameter space including the latent spatial process. We illustrate the computational and inferential benefits of our algorithms over competing methods using simulation studies and an analysis of a massive vegetation index data set.

</details>

<details>

<summary>2021-02-15 02:16:10 - Projected Wasserstein gradient descent for high-dimensional Bayesian inference</summary>

- *Yifei Wang, Peng Chen, Wuchen Li*

- `2102.06350v2` - [abs](http://arxiv.org/abs/2102.06350v2) - [pdf](http://arxiv.org/pdf/2102.06350v2)

> We propose a projected Wasserstein gradient descent method (pWGD) for high-dimensional Bayesian inference problems. The underlying density function of a particle system of WGD is approximated by kernel density estimation (KDE), which faces the long-standing curse of dimensionality. We overcome this challenge by exploiting the intrinsic low-rank structure in the difference between the posterior and prior distributions. The parameters are projected into a low-dimensional subspace to alleviate the approximation error of KDE in high dimensions. We formulate a projected Wasserstein gradient flow and analyze its convergence property under mild assumptions. Several numerical experiments illustrate the accuracy, convergence, and complexity scalability of pWGD with respect to parameter dimension, sample size, and processor cores.

</details>

<details>

<summary>2021-02-15 11:34:34 - Linearly Constrained Gaussian Processes with Boundary Conditions</summary>

- *Markus Lange-Hegermann*

- `2002.00818v3` - [abs](http://arxiv.org/abs/2002.00818v3) - [pdf](http://arxiv.org/pdf/2002.00818v3)

> One goal in Bayesian machine learning is to encode prior knowledge into prior distributions, to model data efficiently. We consider prior knowledge from systems of linear partial differential equations together with their boundary conditions. We construct multi-output Gaussian process priors with realizations in the solution set of such systems, in particular only such solutions can be represented by Gaussian process regression. The construction is fully algorithmic via Gr\"obner bases and it does not employ any approximation. It builds these priors combining two parametrizations via a pullback: the first parametrizes the solutions for the system of differential equations and the second parametrizes all functions adhering to the boundary conditions.

</details>

<details>

<summary>2021-02-15 14:39:51 - Efficient Bayesian reduced rank regression using Langevin Monte Carlo approach</summary>

- *The Tien Mai*

- `2102.07579v1` - [abs](http://arxiv.org/abs/2102.07579v1) - [pdf](http://arxiv.org/pdf/2102.07579v1)

> The problem of Bayesian reduced rank regression is considered in this paper. We propose, for the first time, to use Langevin Monte Carlo method in this problem. A spectral scaled Student prior distrbution is used to exploit the underlying low-rank structure of the coefficient matrix. We show that our algorithms are significantly faster than the Gibbs sampler in high-dimensional setting. Simulation results show that our proposed algorithms for Bayesian reduced rank regression are comparable to the state-of-the-art method where the rank is chosen by cross validation.

</details>

<details>

<summary>2021-02-15 15:45:44 - Hospital Quality Risk Standardization via Approximate Balancing Weights</summary>

- *Luke Keele, Eli Ben-Michael, Avi Feller, Rachel Kelz, Luke Miratrix*

- `2007.09056v2` - [abs](http://arxiv.org/abs/2007.09056v2) - [pdf](http://arxiv.org/pdf/2007.09056v2)

> Comparing outcomes across hospitals, often to identify underperforming hospitals, is a critical task in health services research. However, naive comparisons of average outcomes, such as surgery complication rates, can be misleading because hospital case mixes differ -- a hospital's overall complication rate may be lower due to more effective treatments or simply because the hospital serves a healthier population overall. In this paper, we develop a method of ``direct standardization'' where we re-weight each hospital patient population to be representative of the overall population and then compare the weighted averages across hospitals. Adapting methods from survey sampling and causal inference, we find weights that directly control for imbalance between the hospital patient mix and the target population, even across many patient attributes. Critically, these balancing weights can also be tuned to preserve sample size for more precise estimates. We also derive principled measures of statistical precision, and use outcome modeling and Bayesian shrinkage to increase precision and account for variation in hospital size. We demonstrate these methods using claims data from Pennsylvania, Florida, and New York, estimating standardized hospital complication rates for general surgery patients. We conclude with a discussion of how to detect low performing hospitals.

</details>

<details>

<summary>2021-02-15 16:55:23 - Sequential Likelihood-Free Inference with Implicit Surrogate Proposal</summary>

- *Dongjun Kim, Kyungwoo Song, YoonYeong Kim, Yongjin Shin, Wanmo Kang, Il-Chul Moon*

- `2010.07604v2` - [abs](http://arxiv.org/abs/2010.07604v2) - [pdf](http://arxiv.org/pdf/2010.07604v2)

> Bayesian inference without the access of likelihood, or likelihood-free inference, has been a key research topic in simulations, to yield a more realistic generation result. Recent likelihood-free inference updates an approximate posterior sequentially with the dataset of the cumulative simulation input-output pairs over inference rounds. Therefore, the dataset is gathered through the iterative simulations with sampled inputs from a proposal distribution by MCMC, which becomes the key of inference quality in this sequential framework. This paper introduces a new proposal modeling, named as Implicit Surrogate Proposal (ISP), to generate a cumulated dataset with further sample efficiency. ISP constructs the cumulative dataset in the most diverse way by drawing i.i.d samples via a feed-forward fashion, so the posterior inference does not suffer from the disadvantages of MCMC caused by its non-i.i.d nature, such as auto-correlation and slow mixing. We analyze the convergence property of ISP in both theoretical and empirical aspects to guarantee that ISP provides an asymptotically exact sampler. We demonstrate that ISP outperforms the baseline inference algorithms on simulations with multi-modal posteriors.

</details>

<details>

<summary>2021-02-15 17:45:46 - Scalable nonparametric Bayesian learning for heterogeneous and dynamic velocity fields</summary>

- *Sunrit Chakraborty, Aritra Guha, Rayleigh Lei, XuanLong Nguyen*

- `2102.07695v1` - [abs](http://arxiv.org/abs/2102.07695v1) - [pdf](http://arxiv.org/pdf/2102.07695v1)

> Analysis of heterogeneous patterns in complex spatio-temporal data finds usage across various domains in applied science and engineering, including training autonomous vehicles to navigate in complex traffic scenarios. Motivated by applications arising in the transportation domain, in this paper we develop a model for learning heterogeneous and dynamic patterns of velocity field data. We draw from basic nonparameric Bayesian modeling elements such as hierarchical Dirichlet process and infinite hidden Markov model, while the smoothness of each homogeneous velocity field element is captured with a Gaussian process prior. Of particular focus is a scalable approximate inference method for the proposed model; this is achieved by employing sequential MAP estimates from the infinite HMM model and an efficient sequential GP posterior computation technique, which is shown to work effectively on simulated data sets. Finally, we demonstrate the effectiveness of our techniques to the NGSIM dataset of complex multi-vehicle interactions.

</details>

<details>

<summary>2021-02-15 20:34:56 - Bayesian Posterior Interval Calibration to Improve the Interpretability of Observational Studies</summary>

- *Jami J. Mulgrave, David Madigan, George Hripcsak*

- `2003.06002v2` - [abs](http://arxiv.org/abs/2003.06002v2) - [pdf](http://arxiv.org/pdf/2003.06002v2)

> Observational healthcare data offer the potential to estimate causal effects of medical products on a large scale. However, the confidence intervals and p-values produced by observational studies only account for random error and fail to account for systematic error. As a consequence, operating characteristics such as confidence interval coverage and Type I error rates often deviate sharply from their nominal values and render interpretation impossible. While there is longstanding awareness of systematic error in observational studies, analytic approaches to empirically account for systematic error are relatively new. Several authors have proposed approaches using negative controls (also known as "falsification hypotheses") and positive controls. The basic idea is to adjust confidence intervals and p-values in light of the bias (if any) detected in the analyses of the negative and positive control. In this work, we propose a Bayesian statistical procedure for posterior interval calibration that uses negative and positive controls. We show that the posterior interval calibration procedure restores nominal characteristics, such as 95% coverage of the true effect size by the 95% posterior interval.

</details>

<details>

<summary>2021-02-16 09:09:28 - On Smooth Change-Point Location Estimation for Poisson Processes</summary>

- *A. Amiri, S Dachian*

- `2009.13968v2` - [abs](http://arxiv.org/abs/2009.13968v2) - [pdf](http://arxiv.org/pdf/2009.13968v2)

> We are interested in estimating the location of what we call "smooth change-point" from $n$ independent observations of an inhomogeneous Poisson process. The smooth change-point is a transition of the intensity function of the process from one level to another which happens smoothly, but over such a small interval, that its length $\delta\_n$ is considered to be decreasing to $0$ as $n\to+\infty$. We show that if $\delta\_n$ goes to zero slower than $1/n$, our model is locally asymptotically normal (with a rather unusual rate $\sqrt{\delta\_n/n}$), and the maximum likelihood and Bayesian estimators are consistent, asymptotically normal and asymptotically efficient. If, on the contrary, $\delta\_n$ goes to zero faster than $1/n$, our model is non-regular and behaves like a change-point model. More precisely, in this case we show that the Bayesian estimators are consistent, converge at rate $1/n$, have non-Gaussian limit distributions and are asymptotically efficient. All these results are obtained using the likelihood ratio analysis method of Ibragimov and Khasminskii, which equally yields the convergence of polynomial moments of the considered estimators. However, in order to study the maximum likelihood estimator in the case where $\delta\_n$ goes to zero faster than $1/n$, this method cannot be applied using the usual topologies of convergence in functional spaces. So, this study should go through the use of an alternative topology and will be considered in a future work.

</details>

<details>

<summary>2021-02-17 01:31:44 - Generalized Bayes Quantification Learning under Dataset Shift</summary>

- *Jacob Fiksel, Abhirup Datta, Agbessi Amouzou, Scott Zeger*

- `2001.05360v2` - [abs](http://arxiv.org/abs/2001.05360v2) - [pdf](http://arxiv.org/pdf/2001.05360v2)

> Quantification learning is the task of prevalence estimation for a test population using predictions from a classifier trained on a different population. Quantification methods assume that the sensitivities and specificities of the classifier are either perfect or transportable from the training to the test population. These assumptions are inappropriate in the presence of dataset shift, when the misclassification rates in the training population are not representative of those for the test population. Quantification under dataset shift has been addressed only for single-class (categorical) predictions and assuming perfect knowledge of the true labels on a small subset of the test population. We propose generalized Bayes quantification learning (GBQL) that uses the entire compositional predictions from probabilistic classifiers and allows for uncertainty in true class labels for the limited labeled test data. Instead of positing a full model, we use a model-free Bayesian estimating equation approach to compositional data based only on a first-moment assumption. The idea will be useful in Bayesian compositional data analysis in general as it is robust to different generating mechanisms for compositional data and includes categorical outputs as a special case. We show how our method yields existing quantification approaches as special cases. Extension to an ensemble GBQL that uses predictions from multiple classifiers yielding inference robust to inclusion of a poor classifier is discussed. We outline a fast and efficient Gibbs sampler using a rounding and coarsening approximation to the loss functions. We also establish posterior consistency, asymptotic normality and valid coverage of interval estimates from GBQL, as well as finite sample posterior concentration rate. Empirical performance of GBQL is demonstrated through simulations and analysis of real data with evident dataset shift.

</details>

<details>

<summary>2021-02-17 05:25:23 - Bayesian inference for high-dimensional decomposable graphs</summary>

- *Kyoungjae Lee, Xuan Cao*

- `2004.08102v4` - [abs](http://arxiv.org/abs/2004.08102v4) - [pdf](http://arxiv.org/pdf/2004.08102v4)

> In this paper, we consider high-dimensional Gaussian graphical models where the true underlying graph is decomposable. A hierarchical $G$-Wishart prior is proposed to conduct a Bayesian inference for the precision matrix and its graph structure. Although the posterior asymptotics using the $G$-Wishart prior has received increasing attention in recent years, most of results assume moderate high-dimensional settings, where the number of variables $p$ is smaller than the sample size $n$. However, this assumption might not hold in many real applications such as genomics, speech recognition and climatology. Motivated by this gap, we investigate asymptotic properties of posteriors under the high-dimensional setting where $p$ can be much larger than $n$. The pairwise Bayes factor consistency, posterior ratio consistency and graph selection consistency are obtained in this high-dimensional setting. Furthermore, the posterior convergence rate for precision matrices under the matrix $\ell_1$-norm is derived, which turns out to coincide with the minimax convergence rate for sparse precision matrices. A simulation study confirms that the proposed Bayesian procedure outperforms competitors.

</details>

<details>

<summary>2021-02-17 05:51:25 - Quantifying the Privacy Risks of Learning High-Dimensional Graphical Models</summary>

- *Sasi Kumar Murakonda, Reza Shokri, George Theodorakopoulos*

- `1905.12774v3` - [abs](http://arxiv.org/abs/1905.12774v3) - [pdf](http://arxiv.org/pdf/1905.12774v3)

> Models leak information about their training data. This enables attackers to infer sensitive information about their training sets, notably determine if a data sample was part of the model's training set. The existing works empirically show the possibility of these membership inference (tracing) attacks against complex deep learning models. However, the attack results are dependent on the specific training data, can be obtained only after the tedious process of training the model and performing the attack, and are missing any measure of the confidence and unused potential power of the attack.   In this paper, we theoretically analyze the maximum power of tracing attacks against high-dimensional graphical models, with the focus on Bayesian networks. We provide a tight upper bound on the power (true positive rate) of these attacks, with respect to their error (false positive rate), for a given model structure even before learning its parameters. As it should be, the bound is independent of the knowledge and algorithm of any specific attack. It can help in identifying which model structures leak more information, how adding new parameters to the model increases its privacy risk, and what can be gained by adding new data points to decrease the overall information leakage. It provides a measure of the potential leakage of a model given its structure, as a function of the model complexity and the size of the training set.

</details>

<details>

<summary>2021-02-17 14:26:58 - Unified Bayesian theory of sparse linear regression with nuisance parameters</summary>

- *Seonghyun Jeong, Subhashis Ghosal*

- `2008.10230v2` - [abs](http://arxiv.org/abs/2008.10230v2) - [pdf](http://arxiv.org/pdf/2008.10230v2)

> We study frequentist asymptotic properties of Bayesian procedures for high-dimensional Gaussian sparse regression when unknown nuisance parameters are involved. Nuisance parameters can be finite-, high-, or infinite-dimensional. A mixture of point masses at zero and continuous distributions is used for the prior distribution on sparse regression coefficients, and appropriate prior distributions are used for nuisance parameters. The optimal posterior contraction of sparse regression coefficients, hampered by the presence of nuisance parameters, is also examined and discussed. It is shown that the procedure yields strong model selection consistency. A Bernstein-von Mises-type theorem for sparse regression coefficients is also obtained for uncertainty quantification through credible sets with guaranteed frequentist coverage. Asymptotic properties of numerous examples are investigated using the theories developed in this study.

</details>

<details>

<summary>2021-02-17 15:36:58 - Measuring Bayesian Robustness Using Rényi Divergence</summary>

- *Luai Al-Labadi, Ce Wang*

- `1905.05945v2` - [abs](http://arxiv.org/abs/1905.05945v2) - [pdf](http://arxiv.org/pdf/1905.05945v2)

> This paper deals with measuring the Bayesian robustness of classes of contaminated priors. Two different classes of priors in the neighborhood of the elicited prior are considered. The first one is the well-known $\epsilon$-contaminated class, while the second one is the geometric mixing class. The proposed measure of robustness is based on computing the curvature of R\'enyi divergence between posterior distributions. Examples are used to illustrate the results by using simulated and real data sets.

</details>

<details>

<summary>2021-02-17 19:37:35 - Using Distance Correlation for Efficient Bayesian Optimization</summary>

- *Takuya Kanazawa*

- `2102.08993v1` - [abs](http://arxiv.org/abs/2102.08993v1) - [pdf](http://arxiv.org/pdf/2102.08993v1)

> We propose a novel approach for Bayesian optimization, called $\textsf{GP-DC}$, which combines Gaussian processes with distance correlation. It balances exploration and exploitation automatically, and requires no manual parameter tuning. We evaluate $\textsf{GP-DC}$ on a number of benchmark functions and observe that it outperforms state-of-the-art methods such as $\textsf{GP-UCB}$ and max-value entropy search, as well as the classical expected improvement heuristic. We also apply $\textsf{GP-DC}$ to optimize sequential integral observations with a variable integration range and verify its empirical efficiency on both synthetic and real-world datasets.

</details>

<details>

<summary>2021-02-17 20:04:11 - BORE: Bayesian Optimization by Density-Ratio Estimation</summary>

- *Louis C. Tiao, Aaron Klein, Matthias Seeger, Edwin V. Bonilla, Cedric Archambeau, Fabio Ramos*

- `2102.09009v1` - [abs](http://arxiv.org/abs/2102.09009v1) - [pdf](http://arxiv.org/pdf/2102.09009v1)

> Bayesian optimization (BO) is among the most effective and widely-used blackbox optimization methods. BO proposes solutions according to an explore-exploit trade-off criterion encoded in an acquisition function, many of which are computed from the posterior predictive of a probabilistic surrogate model. Prevalent among these is the expected improvement (EI) function. The need to ensure analytical tractability of the predictive often poses limitations that can hinder the efficiency and applicability of BO. In this paper, we cast the computation of EI as a binary classification problem, building on the link between class-probability estimation and density-ratio estimation, and the lesser-known link between density-ratios and EI. By circumventing the tractability constraints, this reformulation provides numerous advantages, not least in terms of expressiveness, versatility, and scalability.

</details>

<details>

<summary>2021-02-18 00:53:44 - A Latent Space Model for Multilayer Network Data</summary>

- *Juan Sosa, Brenda Betancourt*

- `2102.09560v1` - [abs](http://arxiv.org/abs/2102.09560v1) - [pdf](http://arxiv.org/pdf/2102.09560v1)

> In this work, we propose a Bayesian statistical model to simultaneously characterize two or more social networks defined over a common set of actors. The key feature of the model is a hierarchical prior distribution that allows us to represent the entire system jointly, achieving a compromise between dependent and independent networks. Among others things, such a specification easily allows us to visualize multilayer network data in a low-dimensional Euclidean space, generate a weighted network that reflects the consensus affinity between actors, establish a measure of correlation between networks, assess cognitive judgements that subjects form about the relationships among actors, and perform clustering tasks at different social instances. Our model's capabilities are illustrated using several real-world data sets, taking into account different types of actors, sizes, and relations.

</details>

<details>

<summary>2021-02-18 09:05:14 - Bayesian Inference Forgetting</summary>

- *Shaopeng Fu, Fengxiang He, Yue Xu, Dacheng Tao*

- `2101.06417v2` - [abs](http://arxiv.org/abs/2101.06417v2) - [pdf](http://arxiv.org/pdf/2101.06417v2)

> The right to be forgotten has been legislated in many countries but the enforcement in machine learning would cause unbearable costs: companies may need to delete whole models learned from massive resources due to single individual requests. Existing works propose to remove the knowledge learned from the requested data via its influence function which is no longer naturally well-defined in Bayesian inference. This paper proposes a {\it Bayesian inference forgetting} (BIF) framework to realize the right to be forgotten in Bayesian inference. In the BIF framework, we develop forgetting algorithms for variational inference and Markov chain Monte Carlo. We show that our algorithms can provably remove the influence of single datums on the learned models. Theoretical analysis demonstrates that our algorithms have guaranteed generalizability. Experiments of Gaussian mixture models on the synthetic dataset and Bayesian neural networks on the real-world data verify the feasibility of our methods. The source code package is available at \url{https://github.com/fshp971/BIF}.

</details>

<details>

<summary>2021-02-18 09:26:39 - The Variational Bayesian Inference for Network Autoregression Models</summary>

- *Wei-Ting Lai, Ray-Bing Chen, Ying Chen, Thorsten Koch*

- `2102.09232v1` - [abs](http://arxiv.org/abs/2102.09232v1) - [pdf](http://arxiv.org/pdf/2102.09232v1)

> We develop a variational Bayesian (VB) approach for estimating large-scale dynamic network models in the network autoregression framework. The VB approach allows for the automatic identification of the dynamic structure of such a model and obtains a direct approximation of the posterior density. Compared to Markov Chain Monte Carlo (MCMC) based sampling approaches, the VB approach achieves enhanced computational efficiency without sacrificing estimation accuracy. In the simulation study conducted here, the proposed VB approach detects various types of proper active structures for dynamic network models. Compared to the alternative approach, the proposed method achieves similar or better accuracy, and its computational time is halved. In a real data analysis scenario of day-ahead natural gas flow prediction in the German gas transmission network with 51 nodes between October 2013 and September 2015, the VB approach delivers promising forecasting accuracy along with clearly detected structures in terms of dynamic dependence.

</details>

<details>

<summary>2021-02-18 12:19:12 - Analyzing count data using a time series model with an exponentially decaying covariance structure</summary>

- *Soudeep Deb*

- `2004.03130v2` - [abs](http://arxiv.org/abs/2004.03130v2) - [pdf](http://arxiv.org/pdf/2004.03130v2)

> Count data appears in various disciplines. In this work, a new method to analyze time series count data has been proposed. The method assumes exponentially decaying covariance structure, a special class of the Mat\'ern covariance function, for the latent variable in a Poisson regression model. It is implemented in a Bayesian framework, with the help of Gibbs sampling and ARMS sampling techniques. The proposed approach provides reliable estimates for the covariate effects and estimates the extent of variability explained by the temporally dependent process and the white noise process. The method is flexible, allows irregular spaced data, and can be extended naturally to bigger datasets. The Bayesian implementation helps us to compute the posterior predictive distribution and hence is more appropriate and attractive for count data forecasting problems. Two real life applications of different flavors are included in the paper. These two examples and a short simulation study establish that the proposed approach has good inferential and predictive abilities and performs better than the other competing models.

</details>

<details>

<summary>2021-02-18 15:29:05 - Personalized Dynamic Treatment Regimes in Continuous Time: A Bayesian Approach for Optimizing Clinical Decisions with Timing</summary>

- *William Hua, Hongyuan Mei, Sarah Zohar, Magali Giral, Yanxun Xu*

- `2007.04155v3` - [abs](http://arxiv.org/abs/2007.04155v3) - [pdf](http://arxiv.org/pdf/2007.04155v3)

> Accurate models of clinical actions and their impacts on disease progression are critical for estimating personalized optimal dynamic treatment regimes (DTRs) in medical/health research, especially in managing chronic conditions. Traditional statistical methods for DTRs usually focus on estimating the optimal treatment or dosage at each given medical intervention, but overlook the important question of "when this intervention should happen." We fill this gap by developing a two-step Bayesian approach to optimize clinical decisions with timing. In the first step, we build a generative model for a sequence of medical interventions-which are discrete events in continuous time-with a marked temporal point process (MTPP) where the mark is the assigned treatment or dosage. Then this clinical action model is embedded into a Bayesian joint framework where the other components model clinical observations including longitudinal medical measurements and time-to-event data conditional on treatment histories. In the second step, we propose a policy gradient method to learn the personalized optimal clinical decision that maximizes the patient survival by interacting the MTPP with the model on clinical observations while accounting for uncertainties in clinical observations learned from the posterior inference of the Bayesian joint model in the first step. A signature application of the proposed approach is to schedule follow-up visitations and assign a dosage at each visitation for patients after kidney transplantation. We evaluate our approach with comparison to alternative methods on both simulated and real-world datasets. In our experiments, the personalized decisions made by the proposed method are clinically useful: they are interpretable and successfully help improve patient survival.

</details>

<details>

<summary>2021-02-19 01:47:06 - Confidently Comparing Estimators with the c-value</summary>

- *Brian L. Trippe, Sameer K. Deshpande, Tamara Broderick*

- `2102.09705v1` - [abs](http://arxiv.org/abs/2102.09705v1) - [pdf](http://arxiv.org/pdf/2102.09705v1)

> Modern statistics provides an ever-expanding toolkit for estimating unknown parameters. Consequently, applied statisticians frequently face a difficult decision: retain a parameter estimate from a familiar method or replace it with an estimate from a newer or complex one. While it is traditional to compare estimators using risk, such comparisons are rarely conclusive in realistic settings. In response, we propose the "c-value" as a measure of confidence that a new estimate achieves smaller loss than an old estimate on a given dataset. We show that it is unlikely that a computed c-value is large and that the new estimate has larger loss than the old. Therefore, just as a small p-value provides evidence to reject a null hypothesis, a large c-value provides evidence to use a new estimate in place of the old. For a wide class of problems and estimators, we show how to compute a c-value by first constructing a data-dependent high-probability lower bound on the difference in loss. The c-value is frequentist in nature, but we show that it can provide a validation of Bayesian estimates in real data applications involving hierarchical models and Gaussian processes.

</details>

<details>

<summary>2021-02-19 04:03:21 - Nonparametric Bayesian Two-Level Clustering for Subject-Level Single-Cell Expression Data</summary>

- *Qiuyu Wu, Xiangyu Luo*

- `1912.08050v2` - [abs](http://arxiv.org/abs/1912.08050v2) - [pdf](http://arxiv.org/pdf/1912.08050v2)

> The advent of single-cell sequencing opens new avenues for personalized treatment. In this paper, we address a two-level clustering problem of simultaneous subject subgroup discovery (subject level) and cell type detection (cell level) for single-cell expression data from multiple subjects. However, current statistical approaches either cluster cells without considering the subject heterogeneity or group subjects without using the single-cell information. To bridge the gap between cell clustering and subject grouping, we develop a nonparametric Bayesian model, Subject and Cell clustering for Single-Cell expression data (SCSC) model, to achieve subject and cell grouping simultaneously. SCSC does not need to prespecify the subject subgroup number or the cell type number. It automatically induces subject subgroup structures and matches cell types across subjects. Moreover, it directly models the single-cell raw count data by deliberately considering the data's dropouts, library sizes, and over-dispersion. A blocked Gibbs sampler is proposed for the posterior inference. Simulation studies and the application to a multi-subject iPSC scRNA-seq dataset validate the ability of SCSC to simultaneously cluster subjects and cells.

</details>

<details>

<summary>2021-02-19 05:36:54 - Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels</summary>

- *Binxin Ru, Xingchen Wan, Xiaowen Dong, Michael Osborne*

- `2006.07556v2` - [abs](http://arxiv.org/abs/2006.07556v2) - [pdf](http://arxiv.org/pdf/2006.07556v2)

> Current neural architecture search (NAS) strategies focus only on finding a single, good, architecture. They offer little insight into why a specific network is performing well, or how we should modify the architecture if we want further improvements. We propose a Bayesian optimisation (BO) approach for NAS that combines the Weisfeiler-Lehman graph kernel with a Gaussian process surrogate. Our method optimises the architecture in a highly data-efficient manner: it is capable of capturing the topological structures of the architectures and is scalable to large graphs, thus making the high-dimensional and graph-like search spaces amenable to BO. More importantly, our method affords interpretability by discovering useful network features and their corresponding impact on the network performance. Indeed, we demonstrate empirically that our surrogate model is capable of identifying useful motifs which can guide the generation of new architectures. We finally show that our method outperforms existing NAS approaches to achieve the state of the art on both closed- and open-domain search spaces.

</details>

<details>

<summary>2021-02-19 19:47:14 - BayGo: Joint Bayesian Learning and Information-Aware Graph Optimization</summary>

- *Tamara Alshammari, Sumudu Samarakoon, Anis Elgabli, Mehdi Bennis*

- `2011.04345v2` - [abs](http://arxiv.org/abs/2011.04345v2) - [pdf](http://arxiv.org/pdf/2011.04345v2)

> This article deals with the problem of distributed machine learning, in which agents update their models based on their local datasets, and aggregate the updated models collaboratively and in a fully decentralized manner. In this paper, we tackle the problem of information heterogeneity arising in multi-agent networks where the placement of informative agents plays a crucial role in the learning dynamics. Specifically, we propose BayGo, a novel fully decentralized joint Bayesian learning and graph optimization framework with proven fast convergence over a sparse graph. Under our framework, agents are able to learn and communicate with the most informative agent to their own learning. Unlike prior works, our framework assumes no prior knowledge of the data distribution across agents nor does it assume any knowledge of the true parameter of the system. The proposed alternating minimization based framework ensures global connectivity in a fully decentralized way while minimizing the number of communication links. We theoretically show that by optimizing the proposed objective function, the estimation error of the posterior probability distribution decreases exponentially at each iteration. Via extensive simulations, we show that our framework achieves faster convergence and higher accuracy compared to fully-connected and star topology graphs.

</details>

<details>

<summary>2021-02-20 18:57:11 - Regression-Based Bayesian Estimation and Structure Learning for Nonparanormal Graphical Models</summary>

- *Jami J. Mulgrave, Subhashis Ghosal*

- `1812.04442v2` - [abs](http://arxiv.org/abs/1812.04442v2) - [pdf](http://arxiv.org/pdf/1812.04442v2)

> A nonparanormal graphical model is a semiparametric generalization of a Gaussian graphical model for continuous variables in which it is assumed that the variables follow a Gaussian graphical model only after some unknown smooth monotone transformations. We consider a Bayesian approach to inference in a nonparanormal graphical model in which we put priors on the unknown transformations through a random series based on B-splines. We use a regression formulation to construct the likelihood through the Cholesky decomposition on the underlying precision matrix of the transformed variables and put shrinkage priors on the regression coefficients. We apply a plug-in variational Bayesian algorithm for learning the sparse precision matrix and compare the performance to a posterior Gibbs sampling scheme in a simulation study. We finally apply the proposed methods to a real data set. KEYWORDS:

</details>

<details>

<summary>2021-02-21 11:04:10 - Inverse Gaussian Process regression for likelihood-free inference</summary>

- *Hongqiao Wang, Ziqiao Ao, Tengchao Yu, Jinglai Li*

- `2102.10583v1` - [abs](http://arxiv.org/abs/2102.10583v1) - [pdf](http://arxiv.org/pdf/2102.10583v1)

> In this work we consider Bayesian inference problems with intractable likelihood functions. We present a method to compute an approximate of the posterior with a limited number of model simulations. The method features an inverse Gaussian Process regression (IGPR), i.e., one from the output of a simulation model to the input of it. Within the method, we provide an adaptive algorithm with a tempering procedure to construct the approximations of the marginal posterior distributions. With examples we demonstrate that IGPR has a competitive performance compared to some commonly used algorithms, especially in terms of statistical stability and computational efficiency, while the price to pay is that it can only compute a weighted Gaussian approximation of the marginal posteriors.

</details>

<details>

<summary>2021-02-21 14:44:28 - Replica-exchange Nosé-Hoover dynamics for Bayesian learning on large datasets</summary>

- *Rui Luo, Qiang Zhang, Yaodong Yang, Jun Wang*

- `1905.12569v4` - [abs](http://arxiv.org/abs/1905.12569v4) - [pdf](http://arxiv.org/pdf/1905.12569v4)

> In this paper, we present a new practical method for Bayesian learning that can rapidly draw representative samples from complex posterior distributions with multiple isolated modes in the presence of mini-batch noise. This is achieved by simulating a collection of replicas in parallel with different temperatures and periodically swapping them. When evolving the replicas' states, the Nos\'e-Hoover dynamics is applied, which adaptively neutralizes the mini-batch noise. To perform proper exchanges, a new protocol is developed with a noise-aware test of acceptance, by which the detailed balance is reserved in an asymptotic way. While its efficacy on complex multimodal posteriors has been illustrated by testing over synthetic distributions, experiments with deep Bayesian neural networks on large-scale datasets have shown its significant improvements over strong baselines.

</details>

<details>

<summary>2021-02-21 18:24:11 - A Bayesian Time-Varying Autoregressive Model for Improved Short- and Long-Term Prediction</summary>

- *Christoph Berninger, Almond Stöcker, David Rügamer*

- `2006.05750v2` - [abs](http://arxiv.org/abs/2006.05750v2) - [pdf](http://arxiv.org/pdf/2006.05750v2)

> Motivated by the application to German interest rates, we propose a timevarying autoregressive model for short and long term prediction of time series that exhibit a temporary non-stationary behavior but are assumed to mean revert in the long run. We use a Bayesian formulation to incorporate prior assumptions on the mean reverting process in the model and thereby regularize predictions in the far future. We use MCMC-based inference by deriving relevant full conditional distributions and employ a Metropolis-Hastings within Gibbs Sampler approach to sample from the posterior (predictive) distribution. In combining data-driven short term predictions with long term distribution assumptions our model is competitive to the existing methods in the short horizon while yielding reasonable predictions in the long run. We apply our model to interest rate data and contrast the forecasting performance to the one of a 2-Additive-Factor Gaussian model as well as to the predictions of a dynamic Nelson-Siegel model.

</details>

<details>

<summary>2021-02-22 00:13:56 - Inference of dynamic systems from noisy and sparse data via manifold-constrained Gaussian processes</summary>

- *Shihao Yang, Samuel W. K. Wong, S. C. Kou*

- `2009.07444v3` - [abs](http://arxiv.org/abs/2009.07444v3) - [pdf](http://arxiv.org/pdf/2009.07444v3)

> Parameter estimation for nonlinear dynamic system models, represented by ordinary differential equations (ODEs), using noisy and sparse data is a vital task in many fields. We propose a fast and accurate method, MAGI (MAnifold-constrained Gaussian process Inference), for this task. MAGI uses a Gaussian process model over time-series data, explicitly conditioned on the manifold constraint that derivatives of the Gaussian process must satisfy the ODE system. By doing so, we completely bypass the need for numerical integration and achieve substantial savings in computational time. MAGI is also suitable for inference with unobserved system components, which often occur in real experiments. MAGI is distinct from existing approaches as we provide a principled statistical construction under a Bayesian framework, which incorporates the ODE system through the manifold constraint. We demonstrate the accuracy and speed of MAGI using realistic examples based on physical experiments.

</details>

<details>

<summary>2021-02-22 02:47:09 - Estimating SARS-CoV-2 Infections from Deaths, Confirmed Cases, Tests, and Random Surveys</summary>

- *Nicholas J. Irons, Adrian E. Raftery*

- `2102.10741v1` - [abs](http://arxiv.org/abs/2102.10741v1) - [pdf](http://arxiv.org/pdf/2102.10741v1)

> There are many sources of data giving information about the number of SARS-CoV-2 infections in the population, but all have major drawbacks, including biases and delayed reporting. For example, the number of confirmed cases largely underestimates the number of infections, deaths lag infections substantially, while test positivity rates tend to greatly overestimate prevalence. Representative random prevalence surveys, the only putatively unbiased source, are sparse in time and space, and the results come with a big delay. Reliable estimates of population prevalence are necessary for understanding the spread of the virus and the effects of mitigation strategies. We develop a simple Bayesian framework to estimate viral prevalence by combining the main available data sources. It is based on a discrete-time SIR model with time-varying reproductive parameter. Our model includes likelihood components that incorporate data of deaths due to the virus, confirmed cases, and the number of tests administered on each day. We anchor our inference with data from random sample testing surveys in Indiana and Ohio. We use the results from these two states to calibrate the model on positive test counts and proceed to estimate the infection fatality rate and the number of new infections on each day in each state in the USA. We estimate the extent to which reported COVID cases have underestimated true infection counts, which was large, especially in the first months of the pandemic. We explore the implications of our results for progress towards herd immunity.

</details>

<details>

<summary>2021-02-22 12:49:42 - Approximate Bayes factors for unit root testing</summary>

- *Magris Martin, Iosifidis Alexandros*

- `2102.10048v2` - [abs](http://arxiv.org/abs/2102.10048v2) - [pdf](http://arxiv.org/pdf/2102.10048v2)

> This paper introduces a feasible and practical Bayesian method for unit root testing in financial time series. We propose a convenient approximation of the Bayes factor in terms of the Bayesian Information Criterion as a straightforward and effective strategy for testing the unit root hypothesis. Our approximate approach relies on few assumptions, is of general applicability, and preserves a satisfactory error rate. Among its advantages, it does not require the prior distribution on model's parameters to be specified. Our simulation study and empirical application on real exchange rates show great accordance between the suggested simple approach and both Bayesian and non-Bayesian alternatives.

</details>

<details>

<summary>2021-02-22 15:38:02 - Estimating the effectiveness of permanent price reductions for competing products using multivariate Bayesian structural time series models</summary>

- *Fiammetta Menchetti, Iavor Bojinov*

- `2006.12269v4` - [abs](http://arxiv.org/abs/2006.12269v4) - [pdf](http://arxiv.org/pdf/2006.12269v4)

> The Florence branch of an Italian supermarket chain recently implemented a strategy that permanently lowered the price of numerous store brands in several product categories. To quantify the impact of such a policy change, researchers often use synthetic control methods for estimating causal effects when a subset of units receive a single persistent treatment, and the rest are unaffected by the change. In our applications, however, competitor brands not assigned to treatment are likely impacted by the intervention because of substitution effects; more broadly, this type of interference occurs whenever the treatment assignment of one unit affects the outcome of another. This paper extends the synthetic control methods to accommodate partial interference, allowing interference within predefined groups but not between them. Focusing on a class of causal estimands that capture the effect both on the treated and control units, we develop a multivariate Bayesian structural time series model for generating synthetic controls that would have occurred in the absence of an intervention enabling us to estimate our novel effects. In a simulation study, we explore our Bayesian procedure's empirical properties and show that it achieves good frequentists coverage even when the model is misspecified. We use our new methodology to make causal statements about the impact on sales of the affected store brands and their direct competitors. Our proposed approach is implemented in the CausalMBSTS R package.

</details>

<details>

<summary>2021-02-22 19:46:38 - Markovian Score Climbing: Variational Inference with KL(p||q)</summary>

- *Christian A. Naesseth, Fredrik Lindsten, David Blei*

- `2003.10374v2` - [abs](http://arxiv.org/abs/2003.10374v2) - [pdf](http://arxiv.org/pdf/2003.10374v2)

> Modern variational inference (VI) uses stochastic gradients to avoid intractable expectations, enabling large-scale probabilistic inference in complex models. VI posits a family of approximating distributions q and then finds the member of that family that is closest to the exact posterior p. Traditionally, VI algorithms minimize the "exclusive Kullback-Leibler (KL)" KL(q || p), often for computational convenience. Recent research, however, has also focused on the "inclusive KL" KL(p || q), which has good statistical properties that makes it more appropriate for certain inference problems. This paper develops a simple algorithm for reliably minimizing the inclusive KL using stochastic gradients with vanishing bias. This method, which we call Markovian score climbing (MSC), converges to a local optimum of the inclusive KL. It does not suffer from the systematic errors inherent in existing methods, such as Reweighted Wake-Sleep and Neural Adaptive Sequential Monte Carlo, which lead to bias in their final estimates. We illustrate convergence on a toy model and demonstrate the utility of MSC on Bayesian probit regression for classification as well as a stochastic volatility model for financial data.

</details>

<details>

<summary>2021-02-23 00:48:45 - Rapid Bayesian inference for expensive stochastic models</summary>

- *David J. Warne, Ruth E. Baker, Matthew J. Simpson*

- `1909.06540v3` - [abs](http://arxiv.org/abs/1909.06540v3) - [pdf](http://arxiv.org/pdf/1909.06540v3)

> Almost all fields of science rely upon statistical inference to estimate unknown parameters in theoretical and computational models. While the performance of modern computer hardware continues to grow, the computational requirements for the simulation of models are growing even faster. This is largely due to the increase in model complexity, often including stochastic dynamics, that is necessary to describe and characterize phenomena observed using modern, high resolution, experimental techniques. Such models are rarely analytically tractable, meaning that extremely large numbers of stochastic simulations are required for parameter inference. In such cases, parameter inference can be practically impossible. In this work, we present new computational Bayesian techniques that accelerate inference for expensive stochastic models by using computationally inexpensive approximations to inform feasible regions in parameter space, and through learning transforms that adjust the biased approximate inferences to closer represent the correct inferences under the expensive stochastic model. Using topical examples from ecology and cell biology, we demonstrate a speed improvement of an order of magnitude without any loss in accuracy. This represents a substantial improvement over current state-of-the-art methods for Bayesian computations when appropriate model approximations are available.

</details>

<details>

<summary>2021-02-23 02:28:22 - Causal Mediation Analysis for Sparse and Irregular Longitudinal Data</summary>

- *Shuxi Zeng, Stacy Rosenbaum, Elizabeth Archie, Susan Alberts, Fan Li*

- `2007.01796v3` - [abs](http://arxiv.org/abs/2007.01796v3) - [pdf](http://arxiv.org/pdf/2007.01796v3)

> Causal mediation analysis seeks to investigate how the treatment effect of an exposure on outcomes is mediated through intermediate variables. Although many applications involve longitudinal data, the existing methods are not directly applicable to settings where the mediator and outcome are measured on sparse and irregular time grids. We extend the existing causal mediation framework from a functional data analysis perspective, viewing the sparse and irregular longitudinal data as realizations of underlying smooth stochastic processes. We define causal estimands of direct and indirect effects accordingly and provide corresponding identification assumptions. For estimation and inference, we employ a functional principal component analysis approach for dimension reduction and use the first few functional principal components instead of the whole trajectories in the structural equation models. We adopt the Bayesian paradigm to accurately quantify the uncertainties. The operating characteristics of the proposed methods are examined via simulations. We apply the proposed methods to a longitudinal data set from a wild baboon population in Kenya to investigate the causal relationships between early adversity, strength of social bonds between animals, and adult glucocorticoid hormone concentrations. We find that early adversity has a significant direct effect (a 9-14% increase) on females' glucocorticoid concentrations across adulthood, but find little evidence that these effects were mediated by weak social bonds.

</details>

<details>

<summary>2021-02-23 02:40:09 - Bayesian Spatial Homogeneity Pursuit for Survival Data with an Application to the SEER Respiratory Cancer Data</summary>

- *Lijiang Geng, Guanyu Hu*

- `2003.03006v2` - [abs](http://arxiv.org/abs/2003.03006v2) - [pdf](http://arxiv.org/pdf/2003.03006v2)

> In this work, we propose a new Bayesian spatial homogeneity pursuit method for survival data under the proportional hazards model to detect spatially clustered patterns in baseline hazard and regression coefficients. Specially, regression coefficients and baseline hazard are assumed to have spatial homogeneity pattern over space. To capture such homogeneity, we develop a geographically weighted Chinese restaurant process prior to simultaneously estimate coefficients and baseline hazards and their uncertainty measures. An efficient Markov chain Monte Carlo (MCMC) algorithm is designed for our proposed methods. Performance is evaluated using simulated data, and further applied to a real data analysis of respiratory cancer in the state of Louisiana.

</details>

<details>

<summary>2021-02-23 12:06:51 - Inference in Stochastic Epidemic Models via Multinomial Approximations</summary>

- *Nick Whiteley, Lorenzo Rimella*

- `2006.13700v2` - [abs](http://arxiv.org/abs/2006.13700v2) - [pdf](http://arxiv.org/pdf/2006.13700v2)

> We introduce a new method for inference in stochastic epidemic models which uses recursive multinomial approximations to integrate over unobserved variables and thus circumvent likelihood intractability. The method is applicable to a class of discrete-time, finite-population compartmental models with partial, randomly under-reported or missing count observations. In contrast to state-of-the-art alternatives such as Approximate Bayesian Computation techniques, no forward simulation of the model is required and there are no tuning parameters. Evaluating the approximate marginal likelihood of model parameters is achieved through a computationally simple filtering recursion. The accuracy of the approximation is demonstrated through analysis of real and simulated data using a model of the 1995 Ebola outbreak in the Democratic Republic of Congo. We show how the method can be embedded within a Sequential Monte Carlo approach to estimating the time-varying reproduction number of COVID-19 in Wuhan, China, recently published by Kucharski et al. 2020.

</details>

<details>

<summary>2021-02-23 13:22:25 - Sparse Gaussian Processes Revisited: Bayesian Approaches to Inducing-Variable Approximations</summary>

- *Simone Rossi, Markus Heinonen, Edwin V. Bonilla, Zheyang Shen, Maurizio Filippone*

- `2003.03080v4` - [abs](http://arxiv.org/abs/2003.03080v4) - [pdf](http://arxiv.org/pdf/2003.03080v4)

> Variational inference techniques based on inducing variables provide an elegant framework for scalable posterior estimation in Gaussian process (GP) models. Besides enabling scalability, one of their main advantages over sparse approximations using direct marginal likelihood maximization is that they provide a robust alternative for point estimation of the inducing inputs, i.e. the location of the inducing variables. In this work we challenge the common wisdom that optimizing the inducing inputs in the variational framework yields optimal performance. We show that, by revisiting old model approximations such as the fully-independent training conditionals endowed with powerful sampling-based inference methods, treating both inducing locations and GP hyper-parameters in a Bayesian way can improve performance significantly. Based on stochastic gradient Hamiltonian Monte Carlo, we develop a fully Bayesian approach to scalable GP and deep GP models, and demonstrate its state-of-the-art performance through an extensive experimental campaign across several regression and classification problems.

</details>

<details>

<summary>2021-02-23 15:42:06 - A Simulation-Based Test of Identifiability for Bayesian Causal Inference</summary>

- *Sam Witty, David Jensen, Vikash Mansinghka*

- `2102.11761v1` - [abs](http://arxiv.org/abs/2102.11761v1) - [pdf](http://arxiv.org/pdf/2102.11761v1)

> This paper introduces a procedure for testing the identifiability of Bayesian models for causal inference. Although the do-calculus is sound and complete given a causal graph, many practical assumptions cannot be expressed in terms of graph structure alone, such as the assumptions required by instrumental variable designs, regression discontinuity designs, and within-subjects designs. We present simulation-based identifiability (SBI), a fully automated identification test based on a particle optimization scheme with simulated observations. This approach expresses causal assumptions as priors over functions in a structural causal model, including flexible priors using Gaussian processes. We prove that SBI is asymptotically sound and complete, and produces practical finite-sample bounds. We also show empirically that SBI agrees with known results in graph-based identification as well as with widely-held intuitions for designs in which graph-based methods are inconclusive.

</details>

<details>

<summary>2021-02-23 16:09:01 - Identifying Gene-environment interactions with robust marginal Bayesian variable selection</summary>

- *Xi Lu, Kun Fan, Jie Ren, Cen Wu*

- `2102.11772v1` - [abs](http://arxiv.org/abs/2102.11772v1) - [pdf](http://arxiv.org/pdf/2102.11772v1)

> In high-throughput genetics studies, an important aim is to identify gene-environment interactions associated with the clinical outcomes. Recently, multiple marginal penalization methods have been developed and shown to be effective in G$\times$E studies. However, within the Bayesian framework, marginal variable selection has not received much attention. In this study, we propose a novel marginal Bayesian variable selection method for G$\times$E studies. In particular, our marginal Bayesian method is robust to data contamination and outliers in the outcome variables. With the incorporation of spike-and-slab priors, we have implemented the Gibbs sampler based on MCMC. The proposed method outperforms a number of alternatives in extensive simulation studies. The utility of the marginal robust Bayesian variable selection method has been further demonstrated in the case studies using data from the Nurse Health Study (NHS). Some of the identified main and interaction effects from the real data analysis have important biological implications.

</details>

<details>

<summary>2021-02-23 18:16:36 - Generalized Posteriors in Approximate Bayesian Computation</summary>

- *Sebastian M Schmon, Patrick W Cannon, Jeremias Knoblauch*

- `2011.08644v2` - [abs](http://arxiv.org/abs/2011.08644v2) - [pdf](http://arxiv.org/pdf/2011.08644v2)

> Complex simulators have become a ubiquitous tool in many scientific disciplines, providing high-fidelity, implicit probabilistic models of natural and social phenomena. Unfortunately, they typically lack the tractability required for conventional statistical analysis. Approximate Bayesian computation (ABC) has emerged as a key method in simulation-based inference, wherein the true model likelihood and posterior are approximated using samples from the simulator. In this paper, we draw connections between ABC and generalized Bayesian inference (GBI). First, we re-interpret the accept/reject step in ABC as an implicitly defined error model. We then argue that these implicit error models will invariably be misspecified. While ABC posteriors are often treated as a necessary evil for approximating the standard Bayesian posterior, this allows us to re-interpret ABC as a potential robustification strategy. This leads us to suggest the use of GBI within ABC, a use case we explore empirically.

</details>

<details>

<summary>2021-02-24 05:09:41 - A Hybrid Approximation to the Marginal Likelihood</summary>

- *Eric Chuu, Debdeep Pati, Anirban Bhattacharya*

- `2102.12976v1` - [abs](http://arxiv.org/abs/2102.12976v1) - [pdf](http://arxiv.org/pdf/2102.12976v1)

> Computing the marginal likelihood or evidence is one of the core challenges in Bayesian analysis. While there are many established methods for estimating this quantity, they predominantly rely on using a large number of posterior samples obtained from a Markov Chain Monte Carlo (MCMC) algorithm. As the dimension of the parameter space increases, however, many of these methods become prohibitively slow and potentially inaccurate. In this paper, we propose a novel method in which we use the MCMC samples to learn a high probability partition of the parameter space and then form a deterministic approximation over each of these partition sets. This two-step procedure, which constitutes both a probabilistic and a deterministic component, is termed a Hybrid approximation to the marginal likelihood. We demonstrate its versatility in a plethora of examples with varying dimension and sample size, and we also highlight the Hybrid approximation's effectiveness in situations where there is either a limited number or only approximate MCMC samples available.

</details>

<details>

<summary>2021-02-24 11:48:07 - On Unbiased Estimation for Discretized Models</summary>

- *Jeremy Heng, Ajay Jasra, Kody J. H. Law, Alexander Tarakanov*

- `2102.12230v1` - [abs](http://arxiv.org/abs/2102.12230v1) - [pdf](http://arxiv.org/pdf/2102.12230v1)

> In this article, we consider computing expectations w.r.t. probability measures which are subject to discretization error. Examples include partially observed diffusion processes or inverse problems, where one may have to discretize time and/or space, in order to practically work with the probability of interest. Given access only to these discretizations, we consider the construction of unbiased Monte Carlo estimators of expectations w.r.t. such target probability distributions. It is shown how to obtain such estimators using a novel adaptation of randomization schemes and Markov simulation methods. Under appropriate assumptions, these estimators possess finite variance and finite expected cost. There are two important consequences of this approach: (i) unbiased inference is achieved at the canonical complexity rate, and (ii) the resulting estimators can be generated independently, thereby allowing strong scaling to arbitrarily many parallel processors. Several algorithms are presented, and applied to some examples of Bayesian inference problems, with both simulated and real observed data.

</details>

<details>

<summary>2021-02-24 16:33:04 - Variable Selection with ABC Bayesian Forests</summary>

- *Yi Liu, Veronika Ročková, Yuexi Wang*

- `1806.02304v4` - [abs](http://arxiv.org/abs/1806.02304v4) - [pdf](http://arxiv.org/pdf/1806.02304v4)

> Few problems in statistics are as perplexing as variable selection in the presence of very many redundant covariates. The variable selection problem is most familiar in parametric environments such as the linear model or additive variants thereof. In this work, we abandon the linear model framework, which can be quite detrimental when the covariates impact the outcome in a non-linear way, and turn to tree-based methods for variable selection. Such variable screening is traditionally done by pruning down large trees or by ranking variables based on some importance measure. Despite heavily used in practice, these ad-hoc selection rules are not yet well understood from a theoretical point of view. In this work, we devise a Bayesian tree-based probabilistic method and show that it is consistent for variable selection when the regression surface is a smooth mix of $p>n$ covariates. These results are the first model selection consistency results for Bayesian forest priors. Probabilistic assessment of variable importance is made feasible by a spike-and-slab wrapper around sum-of-trees priors. Sampling from posterior distributions over trees is inherently very difficult. As an alternative to MCMC, we propose ABC Bayesian Forests, a new ABC sampling method based on data-splitting that achieves higher ABC acceptance rate. We show that the method is robust and successful at finding variables with high marginal inclusion probabilities. Our ABC algorithm provides a new avenue towards approximating the median probability model in non-parametric setups where the marginal likelihood is intractable.

</details>

<details>

<summary>2021-02-24 23:48:39 - Efficient Debiased Evidence Estimation by Multilevel Monte Carlo Sampling</summary>

- *Kei Ishikawa, Takashi Goda*

- `2001.04676v2` - [abs](http://arxiv.org/abs/2001.04676v2) - [pdf](http://arxiv.org/pdf/2001.04676v2)

> In this paper, we propose a new stochastic optimization algorithm for Bayesian inference based on multilevel Monte Carlo (MLMC) methods. In Bayesian statistics, biased estimators of the model evidence have been often used as stochastic objectives because the existing debiasing techniques are computationally costly to apply. To overcome this issue, we apply an MLMC sampling technique to construct low-variance unbiased estimators both for the model evidence and its gradient. In the theoretical analysis, we show that the computational cost required for our proposed MLMC estimator to estimate the model evidence or its gradient with a given accuracy is an order of magnitude smaller than those of the previously known estimators. Our numerical experiments confirm considerable computational savings compared to the conventional estimators. Combining our MLMC estimator with gradient-based stochastic optimization results in a new scalable, efficient, debiased inference algorithm for Bayesian statistical models.

</details>

<details>

<summary>2021-02-25 09:05:35 - Time-Series Imputation with Wasserstein Interpolation for Optimal Look-Ahead-Bias and Variance Tradeoff</summary>

- *Jose Blanchet, Fernando Hernandez, Viet Anh Nguyen, Markus Pelger, Xuhui Zhang*

- `2102.12736v1` - [abs](http://arxiv.org/abs/2102.12736v1) - [pdf](http://arxiv.org/pdf/2102.12736v1)

> Missing time-series data is a prevalent practical problem. Imputation methods in time-series data often are applied to the full panel data with the purpose of training a model for a downstream out-of-sample task. For example, in finance, imputation of missing returns may be applied prior to training a portfolio optimization model. Unfortunately, this practice may result in a look-ahead-bias in the future performance on the downstream task. There is an inherent trade-off between the look-ahead-bias of using the full data set for imputation and the larger variance in the imputation from using only the training data. By connecting layers of information revealed in time, we propose a Bayesian posterior consensus distribution which optimally controls the variance and look-ahead-bias trade-off in the imputation. We demonstrate the benefit of our methodology both in synthetic and real financial data.

</details>

<details>

<summary>2021-02-25 09:10:48 - The Spectral Approach to Linear Rational Expectations Models</summary>

- *Majid M. Al-Sadoon*

- `2007.13804v3` - [abs](http://arxiv.org/abs/2007.13804v3) - [pdf](http://arxiv.org/pdf/2007.13804v3)

> This paper considers linear rational expectations models in the frequency domain under general conditions. The paper develops necessary and sufficient conditions for existence and uniqueness of particular and generic systems and characterizes the space of all solutions as an affine space in the frequency domain. It is demonstrated that solutions are not generally continuous with respect to the parameters of the models, invalidating mainstream frequentist and Bayesian methods. The ill-posedness of the problem motivates regularized solutions with theoretically guaranteed uniqueness, continuity, and even differentiability properties. Regularization is illustrated in an analysis of the limiting Gaussian likelihood functions of two analytically tractable models.

</details>

<details>

<summary>2021-02-25 10:11:56 - High-resolution Probabilistic Precipitation Prediction for use in Climate Simulations</summary>

- *Sherman Lo, Peter Watson, Peter Dueben, Ritabrata Dutta*

- `2012.09821v2` - [abs](http://arxiv.org/abs/2012.09821v2) - [pdf](http://arxiv.org/pdf/2012.09821v2)

> The accurate prediction of precipitation is important to allow for reliable warnings of flood or drought risk in a changing climate. However, to make trust-worthy predictions of precipitation, at a local scale, is one of the most difficult challenges for today's weather and climate models. This is because important features, such as individual clouds and high-resolution topography, cannot be resolved explicitly within simulations due to the significant computational cost of high-resolution simulations. Climate models are typically run at $\sim$50-100 km resolution which is insufficient to represent local precipitation events in satisfying detail. Here, we develop a method to make probabilistic precipitation predictions based on features that climate models can resolve well and that is not highly sensitive to the approximations used in individual models. To predict, we will use a temporal compound Poisson distribution dependent on the output of climate models at a location. We use the output of Earth System models at coarse resolution $\sim$50 km as input and train the statistical models towards precipitation observations over Wales at $\sim$10 km resolution. A Bayesian inferential scheme is provided so that the compound-Poisson model can be inferred using a Gibbs-within-Metropolis-Elliptic-Slice sampling scheme which enables us to quantify the uncertainty of our predictions. In addition, we use a Gaussian process regressor on the posterior samples of the model parameters, to infer a spatially coherent model and hence to produce spatially coherent rainfall prediction. We illustrate the prediction performance of our model by training over 5 years of the data up to 31st December 1999 and predicting precipitation for 20 years afterwards for Cardiff and Wales.

</details>

<details>

<summary>2021-02-25 11:28:46 - Mixed Variable Bayesian Optimization with Frequency Modulated Kernels</summary>

- *Changyong Oh, Efstratios Gavves, Max Welling*

- `2102.12792v1` - [abs](http://arxiv.org/abs/2102.12792v1) - [pdf](http://arxiv.org/pdf/2102.12792v1)

> The sample efficiency of Bayesian optimization(BO) is often boosted by Gaussian Process(GP) surrogate models. However, on mixed variable spaces, surrogate models other than GPs are prevalent, mainly due to the lack of kernels which can model complex dependencies across different types of variables. In this paper, we propose the frequency modulated (FM) kernel flexibly modeling dependencies among different types of variables, so that BO can enjoy the further improved sample efficiency. The FM kernel uses distances on continuous variables to modulate the graph Fourier spectrum derived from discrete variables. However, the frequency modulation does not always define a kernel with the similarity measure behavior which returns higher values for pairs of more similar points. Therefore, we specify and prove conditions for FM kernels to be positive definite and to exhibit the similarity measure behavior. In experiments, we demonstrate the improved sample efficiency of GP BO using FM kernels (BO-FM).On synthetic problems and hyperparameter optimization problems, BO-FM outperforms competitors consistently. Also, the importance of the frequency modulation principle is empirically demonstrated on the same problems. On joint optimization of neural architectures and SGD hyperparameters, BO-FM outperforms competitors including Regularized evolution(RE) and BOHB. Remarkably, BO-FM performs better even than RE and BOHB using three times as many evaluations.

</details>

<details>

<summary>2021-02-25 12:26:52 - Hyperparameter Transfer Learning with Adaptive Complexity</summary>

- *Samuel Horváth, Aaron Klein, Peter Richtárik, Cédric Archambeau*

- `2102.12810v1` - [abs](http://arxiv.org/abs/2102.12810v1) - [pdf](http://arxiv.org/pdf/2102.12810v1)

> Bayesian optimization (BO) is a sample efficient approach to automatically tune the hyperparameters of machine learning models. In practice, one frequently has to solve similar hyperparameter tuning problems sequentially. For example, one might have to tune a type of neural network learned across a series of different classification problems. Recent work on multi-task BO exploits knowledge gained from previous tuning tasks to speed up a new tuning task. However, previous approaches do not account for the fact that BO is a sequential decision making procedure. Hence, there is in general a mismatch between the number of evaluations collected in the current tuning task compared to the number of evaluations accumulated in all previously completed tasks. In this work, we enable multi-task BO to compensate for this mismatch, such that the transfer learning procedure is able to handle different data regimes in a principled way. We propose a new multi-task BO method that learns a set of ordered, non-linear basis functions of increasing complexity via nested drop-out and automatic relevance determination. Experiments on a variety of hyperparameter tuning problems show that our method improves the sample ef

</details>

<details>

<summary>2021-02-25 15:34:03 - On Posterior consistency of Bayesian Changepoint models</summary>

- *Nilabja Guha, Jyotishka Datta*

- `2102.12938v1` - [abs](http://arxiv.org/abs/2102.12938v1) - [pdf](http://arxiv.org/pdf/2102.12938v1)

> While there have been a lot of recent developments in the context of Bayesian model selection and variable selection for high dimensional linear models, there is not much work in the presence of change point in literature, unlike the frequentist counterpart. We consider a hierarchical Bayesian linear model where the active set of covariates that affects the observations through a mean model can vary between different time segments. Such structure may arise in social sciences/ economic sciences, such as sudden change of house price based on external economic factor, crime rate changes based on social and built-environment factors, and others. Using an appropriate adaptive prior, we outline the development of a hierarchical Bayesian methodology that can select the true change point as well as the true covariates, with high probability. We provide the first detailed theoretical analysis for posterior consistency with or without covariates, under suitable conditions. Gibbs sampling techniques provide an efficient computational strategy. We also consider small sample simulation study as well as application to crime forecasting applications.

</details>

<details>

<summary>2021-02-25 15:54:58 - Exact and computationally efficient Bayesian inference for generalized Markov modulated Poisson processes</summary>

- *Flavio B. Gonçalves, Livia M. Dutra, Roger W. C. Silva*

- `2006.09949v3` - [abs](http://arxiv.org/abs/2006.09949v3) - [pdf](http://arxiv.org/pdf/2006.09949v3)

> Statistical modeling of point patterns is an important and common problem in several areas. The Poisson process is the most common process used for this purpose, in particular, its generalization that considers the intensity function to be stochastic. This is called a Cox process and different choices to model the dynamics of the intensity gives rise to a wide range of models. We present a new class of unidimensional Cox process models in which the intensity function assumes parametric functional forms that switch among them according to a continuous-time Markov chain. A novel methodology is proposed to perform exact Bayesian inference based on MCMC algorithms. The term exact refers to the fact that no discrete time approximation is used and Monte Carlo error is the only source of inaccuracy. The reliability of the algorithms depends on a variety of specifications which are carefully addressed, resulting in a computationally efficient (in terms of computing time) algorithm and enabling its use with large data sets. Simulated and real examples are presented to illustrate the efficiency and applicability of the proposed methodology. A specific model to fit epidemic curves is proposed and used to analyze data from Dengue Fever in Brazil and COVID-19 in some countries.

</details>

<details>

<summary>2021-02-25 16:03:04 - Stein Variational Gradient Descent: many-particle and long-time asymptotics</summary>

- *Nikolas Nüsken, D. R. Michiel Renger*

- `2102.12956v1` - [abs](http://arxiv.org/abs/2102.12956v1) - [pdf](http://arxiv.org/pdf/2102.12956v1)

> Stein variational gradient descent (SVGD) refers to a class of methods for Bayesian inference based on interacting particle systems. In this paper, we consider the originally proposed deterministic dynamics as well as a stochastic variant, each of which represent one of the two main paradigms in Bayesian computational statistics: variational inference and Markov chain Monte Carlo. As it turns out, these are tightly linked through a correspondence between gradient flow structures and large-deviation principles rooted in statistical physics. To expose this relationship, we develop the cotangent space construction for the Stein geometry, prove its basic properties, and determine the large-deviation functional governing the many-particle limit for the empirical measure. Moreover, we identify the Stein-Fisher information (or kernelised Stein discrepancy) as its leading order contribution in the long-time and many-particle regime in the sense of $\Gamma$-convergence, shedding some light on the finite-particle properties of SVGD. Finally, we establish a comparison principle between the Stein-Fisher information and RKHS-norms that might be of independent interest.

</details>

<details>

<summary>2021-02-25 17:59:47 - Improving predictions of Bayesian neural nets via local linearization</summary>

- *Alexander Immer, Maciej Korzepa, Matthias Bauer*

- `2008.08400v3` - [abs](http://arxiv.org/abs/2008.08400v3) - [pdf](http://arxiv.org/pdf/2008.08400v3)

> The generalized Gauss-Newton (GGN) approximation is often used to make practical Bayesian deep learning approaches scalable by replacing a second order derivative with a product of first order derivatives. In this paper we argue that the GGN approximation should be understood as a local linearization of the underlying Bayesian neural network (BNN), which turns the BNN into a generalized linear model (GLM). Because we use this linearized model for posterior inference, we should also predict using this modified model instead of the original one. We refer to this modified predictive as "GLM predictive" and show that it effectively resolves common underfitting problems of the Laplace approximation. It extends previous results in this vein to general likelihoods and has an equivalent Gaussian process formulation, which enables alternative inference schemes for BNNs in function space. We demonstrate the effectiveness of our approach on several standard classification datasets as well as on out-of-distribution detection. We provide an implementation at https://github.com/AlexImmer/BNN-predictions.

</details>

<details>

<summary>2021-02-25 18:38:05 - Parameter estimation in dynamical systems via Statistical Learning: a reinterpretation of Approximate Bayesian Computation applied to COVID-19 spread</summary>

- *Diego Marcondes*

- `2007.14229v2` - [abs](http://arxiv.org/abs/2007.14229v2) - [pdf](http://arxiv.org/pdf/2007.14229v2)

> We propose a robust parameter estimation method for dynamical systems based on Statistical Learning techniques which aims to estimate a set of parameters that well fit the dynamics in order to obtain robust evidences about the qualitative behaviour of its trajectory. The method is quite general and flexible, since it does not rely on any specific property of the dynamical system, and represents a reinterpretation of Approximate Bayesian Computation methods through the lens of Statistical Learning. The method is specially useful for estimating parameters in epidemiological compartmental models in order to obtain qualitative properties of a disease evolution. We apply it to simulated and real data about COVID-19 spread in the US in order to evaluate qualitatively its evolution over time, showing how one may assess the effectiveness of measures implemented to slow the spread and some qualitative features of the disease current and future evolution.

</details>

<details>

<summary>2021-02-25 22:04:24 - Bayesian Coresets: Revisiting the Nonconvex Optimization Perspective</summary>

- *Jacky Y. Zhang, Rajiv Khanna, Anastasios Kyrillidis, Oluwasanmi Koyejo*

- `2007.00715v2` - [abs](http://arxiv.org/abs/2007.00715v2) - [pdf](http://arxiv.org/pdf/2007.00715v2)

> Bayesian coresets have emerged as a promising approach for implementing scalable Bayesian inference. The Bayesian coreset problem involves selecting a (weighted) subset of the data samples, such that the posterior inference using the selected subset closely approximates the posterior inference using the full dataset. This manuscript revisits Bayesian coresets through the lens of sparsity constrained optimization. Leveraging recent advances in accelerated optimization methods, we propose and analyze a novel algorithm for coreset selection. We provide explicit convergence rate guarantees and present an empirical evaluation on a variety of benchmark datasets to highlight our proposed algorithm's superior performance compared to state-of-the-art on speed and accuracy.

</details>

<details>

<summary>2021-02-25 22:34:25 - Optimizing Percentile Criterion Using Robust MDPs</summary>

- *Bahram Behzadian, Reazul Hasan Russel, Marek Petrik, Chin Pang Ho*

- `1910.10786v3` - [abs](http://arxiv.org/abs/1910.10786v3) - [pdf](http://arxiv.org/pdf/1910.10786v3)

> We address the problem of computing reliable policies in reinforcement learning problems with limited data. In particular, we compute policies that achieve good returns with high confidence when deployed. This objective, known as the \emph{percentile criterion}, can be optimized using Robust MDPs~(RMDPs). RMDPs generalize MDPs to allow for uncertain transition probabilities chosen adversarially from given ambiguity sets. We show that the RMDP solution's sub-optimality depends on the spans of the ambiguity sets along the value function. We then propose new algorithms that minimize the span of ambiguity sets defined by weighted $L_1$ and $L_\infty$ norms. Our primary focus is on Bayesian guarantees, but we also describe how our methods apply to frequentist guarantees and derive new concentration inequalities for weighted $L_1$ and $L_\infty$ norms. Experimental results indicate that our optimized ambiguity sets improve significantly on prior construction methods.

</details>

<details>

<summary>2021-02-25 23:51:23 - GANs with Conditional Independence Graphs: On Subadditivity of Probability Divergences</summary>

- *Mucong Ding, Constantinos Daskalakis, Soheil Feizi*

- `2003.00652v3` - [abs](http://arxiv.org/abs/2003.00652v3) - [pdf](http://arxiv.org/pdf/2003.00652v3)

> Generative Adversarial Networks (GANs) are modern methods to learn the underlying distribution of a data set. GANs have been widely used in sample synthesis, de-noising, domain transfer, etc. GANs, however, are designed in a model-free fashion where no additional information about the underlying distribution is available. In many applications, however, practitioners have access to the underlying independence graph of the variables, either as a Bayesian network or a Markov Random Field (MRF). We ask: how can one use this additional information in designing model-based GANs? In this paper, we provide theoretical foundations to answer this question by studying subadditivity properties of probability divergences, which establish upper bounds on the distance between two high-dimensional distributions by the sum of distances between their marginals over (local) neighborhoods of the graphical structure of the Bayes-net or the MRF. We prove that several popular probability divergences satisfy some notion of subadditivity under mild conditions. These results lead to a principled design of a model-based GAN that uses a set of simple discriminators on the neighborhoods of the Bayes-net/MRF, rather than a giant discriminator on the entire network, providing significant statistical and computational benefits. Our experiments on synthetic and real-world datasets demonstrate the benefits of our principled design of model-based GANs.

</details>

<details>

<summary>2021-02-26 00:31:19 - Data-Free Likelihood-Informed Dimension Reduction of Bayesian Inverse Problems</summary>

- *Tiangang Cui, Olivier Zahm*

- `2102.13245v1` - [abs](http://arxiv.org/abs/2102.13245v1) - [pdf](http://arxiv.org/pdf/2102.13245v1)

> Identifying a low-dimensional informed parameter subspace offers a viable path to alleviating the dimensionality challenge in the sampled-based solution to large-scale Bayesian inverse problems. This paper introduces a novel gradient-based dimension reduction method in which the informed subspace does not depend on the data. This permits an online-offline computational strategy where the expensive low-dimensional structure of the problem is detected in an offline phase, meaning before observing the data. This strategy is particularly relevant for multiple inversion problems as the same informed subspace can be reused. The proposed approach allows controlling the approximation error (in expectation over the data) of the posterior distribution. We also present sampling strategies that exploit the informed subspace to draw efficiently samples from the exact posterior distribution. The method is successfully illustrated on two numerical examples: a PDE-based inverse problem with a Gaussian process prior and a tomography problem with Poisson data and a Besov-$\mathcal{B}^2_{11}$ prior.

</details>

<details>

<summary>2021-02-26 03:08:50 - Exploring the space-time pattern of log-transformed infectious count of COVID-19: a clustering-segmented autoregressive sigmoid model</summary>

- *Xiaoping Shi, Meiqian Chen, Yucheng Dong*

- `2102.13287v1` - [abs](http://arxiv.org/abs/2102.13287v1) - [pdf](http://arxiv.org/pdf/2102.13287v1)

> At the end of April 20, 2020, there were only a few new COVID-19 cases remaining in China, whereas the rest of the world had shown increases in the number of new cases. It is of extreme importance to develop an efficient statistical model of COVID-19 spread, which could help in the global fight against the virus. We propose a clustering-segmented autoregressive sigmoid (CSAS) model to explore the space-time pattern of the log-transformed infectious count. Four key characteristics are included in this CSAS model, including unknown clusters, change points, stretched S-curves, and autoregressive terms, in order to understand how this outbreak is spreading in time and in space, to understand how the spread is affected by epidemic control strategies, and to apply the model to updated data from an extended period of time. We propose a nonparametric graph-based clustering method for discovering dissimilarity of the curve time series in space, which is justified with theoretical support to demonstrate how the model works under mild and easily verified conditions. We propose a very strict purity score that penalizes overestimation of clusters. Simulations show that our nonparametric graph-based clustering method is faster and more accurate than the parametric clustering method regardless of the size of data sets. We provide a Bayesian information criterion (BIC) to identify multiple change points and calculate a confidence interval for a mean response. By applying the CSAS model to the collected data, we can explain the differences between prevention and control policies in China and selected countries.

</details>

<details>

<summary>2021-02-26 09:26:20 - Oversampled Adaptive Sensing via a Predefined Codebook</summary>

- *Ali Bereyhi, Saba Asaad, Ralf R. Müller*

- `2102.13366v1` - [abs](http://arxiv.org/abs/2102.13366v1) - [pdf](http://arxiv.org/pdf/2102.13366v1)

> Oversampled adaptive sensing (OAS) is a Bayesian framework recently proposed for effective sensing of structured signals in a time-limited setting. In contrast to the conventional blind oversampling, OAS uses the prior information on the signal to construct posterior beliefs sequentially. These beliefs help in constructive oversampling which iteratively evolves through a sequence of time sub-frames.   The initial studies of OAS consider the idealistic assumption of full control on sensing coefficients which is not feasible in many applications. In this work, we extend the initial investigations on OAS to more realistic settings in which the sensing coefficients are selected from a predefined set of possible choices, referred to as the codebook. We extend the OAS framework to these settings and compare its performance with classical non-adaptive approaches.

</details>

<details>

<summary>2021-02-26 10:15:57 - Batch Bayesian Optimization on Permutations using Acquisition Weighted Kernels</summary>

- *Changyong Oh, Roberto Bondesan, Efstratios Gavves, Max Welling*

- `2102.13382v1` - [abs](http://arxiv.org/abs/2102.13382v1) - [pdf](http://arxiv.org/pdf/2102.13382v1)

> In this work we propose a batch Bayesian optimization method for combinatorial problems on permutations, which is well suited for expensive cost functions on permutations. We introduce LAW, a new efficient batch acquisition method based on the determinantal point process, using an acquisition weighted kernel. Relying on multiple parallel evaluations, LAW accelerates the search for the optimal permutation. We provide a regret analysis for our method to gain insight in its theoretical properties. We then apply the framework to permutation problems, which have so far received little attention in the Bayesian Optimization literature, despite their practical importance. We call this method LAW2ORDER. We evaluate the method on several standard combinatorial problems involving permutations such as quadratic assignment, flowshop scheduling and the traveling salesman, as well as on a structure learning task.

</details>

<details>

<summary>2021-02-26 11:02:58 - General Bayesian time-varying parameter VARs for predicting government bond yields</summary>

- *Manfred M. Fischer, Niko Hauzenberger, Florian Huber, Michael Pfarrhofer*

- `2102.13393v1` - [abs](http://arxiv.org/abs/2102.13393v1) - [pdf](http://arxiv.org/pdf/2102.13393v1)

> Time-varying parameter (TVP) regressions commonly assume that time-variation in the coefficients is determined by a simple stochastic process such as a random walk. While such models are capable of capturing a wide range of dynamic patterns, the true nature of time variation might stem from other sources, or arise from different laws of motion. In this paper, we propose a flexible TVP VAR that assumes the TVPs to depend on a panel of partially latent covariates. The latent part of these covariates differ in their state dynamics and thus capture smoothly evolving or abruptly changing coefficients. To determine which of these covariates are important, and thus to decide on the appropriate state evolution, we introduce Bayesian shrinkage priors to perform model selection. As an empirical application, we forecast the US term structure of interest rates and show that our approach performs well relative to a set of competing models. We then show how the model can be used to explain structural breaks in coefficients related to the US yield curve.

</details>

<details>

<summary>2021-02-26 12:10:50 - Learning Partially Known Stochastic Dynamics with Empirical PAC Bayes</summary>

- *Manuel Haussmann, Sebastian Gerwinn, Andreas Look, Barbara Rakitsch, Melih Kandemir*

- `2006.09914v3` - [abs](http://arxiv.org/abs/2006.09914v3) - [pdf](http://arxiv.org/pdf/2006.09914v3)

> Neural Stochastic Differential Equations model a dynamical environment with neural nets assigned to their drift and diffusion terms. The high expressive power of their nonlinearity comes at the expense of instability in the identification of the large set of free parameters. This paper presents a recipe to improve the prediction accuracy of such models in three steps: i) accounting for epistemic uncertainty by assuming probabilistic weights, ii) incorporation of partial knowledge on the state dynamics, and iii) training the resultant hybrid model by an objective derived from a PAC-Bayesian generalization bound. We observe in our experiments that this recipe effectively translates partial and noisy prior knowledge into an improved model fit.

</details>

<details>

<summary>2021-02-26 15:41:28 - Variational Full Bayes Lasso: Knots Selection in Regression Splines</summary>

- *Larissa Alves, Ronaldo Dias, Helio S. Migon*

- `2102.13548v1` - [abs](http://arxiv.org/abs/2102.13548v1) - [pdf](http://arxiv.org/pdf/2102.13548v1)

> We develop a fully automatic Bayesian Lasso via variational inference. This is a scalable procedure for approximating the posterior distribution. Special attention is driven to the knot selection in regression spline. In order to carry through our proposal, a full automatic variational Bayesian Lasso, a Jefferey's prior is proposed for the hyperparameters and a decision theoretical approach is introduced to decide if a knot is selected or not. Extensive simulation studies were developed to ensure the effectiveness of the proposed algorithms. The performance of the algorithms were also tested in some real data sets, including data from the world pandemic Covid-19. Again, the algorithms showed a very good performance in capturing the data structure.

</details>

<details>

<summary>2021-02-26 15:59:19 - VAT Compliance Incentives</summary>

- *Maria-Augusta Miceli*

- `2002.07862v3` - [abs](http://arxiv.org/abs/2002.07862v3) - [pdf](http://arxiv.org/pdf/2002.07862v3)

> In this work I clarify VAT evasion incentives through a game theoretical approach. Traditionally, evasion has been linked to the decreasing risk aversion in higher revenues (Allingham and Sandmo (1972), Cowell (1985) (1990)). I claim tax evasion to be a rational choice when compliance is stochastically more expensive than evading, even in absence of controls and sanctions. I create a framework able to measure the incentives for taxpayers to comply. The incentives here are deductions of specific VAT documented expenses from the income tax. The issue is very well known and deduction policies at work in many countries. The aim is to compute the right parameters for each precise class of taxpayers. VAT evasion is a collusive conduct between the two counterparts of the transaction. I therefore first explore the convenience for the two private counterparts to agree on the joint evasion and to form a coalition. Crucial is that compliance incentives break the agreement among the transaction participants' coalition about evading. The game solution leads to boundaries for marginal tax rates or deduction percentages, depending on parameters, able to create incentives to comply The stylized example presented here for VAT policies, already in use in many countries, is an attempt to establish a more general method for tax design, able to make compliance the "dominant strategy", satisfying the "outside option" constraint represented by evasion, even in absence of audit and sanctions. The theoretical results derived here can be easily applied to real data for precise tax design engineering.

</details>

<details>

<summary>2021-02-26 22:26:52 - Neural Empirical Bayes: Source Distribution Estimation and its Applications to Simulation-Based Inference</summary>

- *Maxime Vandegar, Michael Kagan, Antoine Wehenkel, Gilles Louppe*

- `2011.05836v2` - [abs](http://arxiv.org/abs/2011.05836v2) - [pdf](http://arxiv.org/pdf/2011.05836v2)

> We revisit empirical Bayes in the absence of a tractable likelihood function, as is typical in scientific domains relying on computer simulations. We investigate how the empirical Bayesian can make use of neural density estimators first to use all noise-corrupted observations to estimate a prior or source distribution over uncorrupted samples, and then to perform single-observation posterior inference using the fitted source distribution. We propose an approach based on the direct maximization of the log-marginal likelihood of the observations, examining both biased and de-biased estimators, and comparing to variational approaches. We find that, up to symmetries, a neural empirical Bayes approach recovers ground truth source distributions. With the learned source distribution in hand, we show the applicability to likelihood-free inference and examine the quality of the resulting posterior estimates. Finally, we demonstrate the applicability of Neural Empirical Bayes on an inverse problem from collider physics.

</details>

<details>

<summary>2021-02-27 20:36:05 - Time-Varying Coefficient Model Estimation Through Radial Basis Functions</summary>

- *Juan Sosa, Lina Buitrago*

- `2103.00315v1` - [abs](http://arxiv.org/abs/2103.00315v1) - [pdf](http://arxiv.org/pdf/2103.00315v1)

> In this paper we estimate the dynamic parameters of a time-varying coefficient model through radial kernel functions in the context of a longitudinal study. Our proposal is based on a linear combination of weighted kernel functions involving a bandwidth, centered around a given set of time points. In addition, we study different alternatives of estimation and inference including a Frequentist approach using weighted least squares along with bootstrap methods, and a Bayesian approach through both Markov chain Monte Carlo and variational methods. We compare the estimation strategies mention above with each other, and our radial kernel functions proposal with an expansion based on regression spline, by means of an extensive simulation study considering multiples scenarios in terms of sample size, number of repeated measurements, and subject-specific correlation. Our experiments show that the capabilities of our proposal based on radial kernel functions are indeed comparable with or even better than those obtained from regression splines. We illustrate our methodology by analyzing data from two AIDS clinical studies.

</details>

<details>

<summary>2021-02-27 22:47:22 - Accurate and Interpretable Machine Learning for Transparent Pricing of Health Insurance Plans</summary>

- *Rohun Kshirsagar, Li-Yen Hsu, Vatshank Chaturvedi, Charles H. Greenberg, Matthew McClelland, Anushadevi Mohan, Wideet Shende, Nicolas P. Tilmans, Renzo Frigato, Min Guo, Ankit Chheda, Meredith Trotter, Shonket Ray, Arnold Lee, Miguel Alvarado*

- `2009.10990v2` - [abs](http://arxiv.org/abs/2009.10990v2) - [pdf](http://arxiv.org/pdf/2009.10990v2)

> Health insurance companies cover half of the United States population through commercial employer-sponsored health plans and pay 1.2 trillion US dollars every year to cover medical expenses for their members. The actuary and underwriter roles at a health insurance company serve to assess which risks to take on and how to price those risks to ensure profitability of the organization. While Bayesian hierarchical models are the current standard in the industry to estimate risk, interest in machine learning as a way to improve upon these existing methods is increasing. Lumiata, a healthcare analytics company, ran a study with a large health insurance company in the United States. We evaluated the ability of machine learning models to predict the per member per month cost of employer groups in their next renewal period, especially those groups who will cost less than 95\% of what an actuarial model predicts (groups with "concession opportunities"). We developed a sequence of two models, an individual patient-level and an employer-group-level model, to predict the annual per member per month allowed amount for employer groups, based on a population of 14 million patients. Our models performed 20\% better than the insurance carrier's existing pricing model, and identified 84\% of the concession opportunities. This study demonstrates the application of a machine learning system to compute an accurate and fair price for health insurance products and analyzes how explainable machine learning models can exceed actuarial models' predictive accuracy while maintaining interpretability.

</details>

<details>

<summary>2021-02-28 04:26:09 - HALO: Learning to Prune Neural Networks with Shrinkage</summary>

- *Skyler Seto, Martin T. Wells, Wenyu Zhang*

- `2008.10183v3` - [abs](http://arxiv.org/abs/2008.10183v3) - [pdf](http://arxiv.org/pdf/2008.10183v3)

> Deep neural networks achieve state-of-the-art performance in a variety of tasks by extracting a rich set of features from unstructured data, however this performance is closely tied to model size. Modern techniques for inducing sparsity and reducing model size are (1) network pruning, (2) training with a sparsity inducing penalty, and (3) training a binary mask jointly with the weights of the network. We study different sparsity inducing penalties from the perspective of Bayesian hierarchical models and present a novel penalty called Hierarchical Adaptive Lasso (HALO) which learns to adaptively sparsify weights of a given network via trainable parameters. When used to train over-parametrized networks, our penalty yields small subnetworks with high accuracy without fine-tuning. Empirically, on image recognition tasks, we find that HALO is able to learn highly sparse network (only 5% of the parameters) with significant gains in performance over state-of-the-art magnitude pruning methods at the same level of sparsity. Code is available at https://github.com/skyler120/sparsity-halo.

</details>

<details>

<summary>2021-02-28 13:09:43 - Bayesian Nonparametric Space Partitions: A Survey</summary>

- *Xuhui Fan, Bin Li, Ling Luo, Scott A. Sisson*

- `2002.11394v2` - [abs](http://arxiv.org/abs/2002.11394v2) - [pdf](http://arxiv.org/pdf/2002.11394v2)

> Bayesian nonparametric space partition (BNSP) models provide a variety of strategies for partitioning a $D$-dimensional space into a set of blocks. In this way, the data points lie in the same block would share certain kinds of homogeneity. BNSP models can be applied to various areas, such as regression/classification trees, random feature construction, relational modeling, etc. In this survey, we investigate the current progress of BNSP research through the following three perspectives: models, which review various strategies for generating the partitions in the space and discuss their theoretical foundation `self-consistency'; applications, which cover the current mainstream usages of BNSP models and their potential future practises; and challenges, which identify the current unsolved problems and valuable future research topics. As there are no comprehensive reviews of BNSP literature before, we hope that this survey can induce further exploration and exploitation on this topic.

</details>

<details>

<summary>2021-02-28 16:05:20 - Scalable Constrained Bayesian Optimization</summary>

- *David Eriksson, Matthias Poloczek*

- `2002.08526v3` - [abs](http://arxiv.org/abs/2002.08526v3) - [pdf](http://arxiv.org/pdf/2002.08526v3)

> The global optimization of a high-dimensional black-box function under black-box constraints is a pervasive task in machine learning, control, and engineering. These problems are challenging since the feasible set is typically non-convex and hard to find, in addition to the curses of dimensionality and the heterogeneity of the underlying functions. In particular, these characteristics dramatically impact the performance of Bayesian optimization methods, that otherwise have become the de facto standard for sample-efficient optimization in unconstrained settings, leaving practitioners with evolutionary strategies or heuristics. We propose the scalable constrained Bayesian optimization (SCBO) algorithm that overcomes the above challenges and pushes the applicability of Bayesian optimization far beyond the state-of-the-art. A comprehensive experimental evaluation demonstrates that SCBO achieves excellent results on a variety of benchmarks. To this end, we propose two new control problems that we expect to be of independent value for the scientific community.

</details>

<details>

<summary>2021-02-28 17:47:28 - Random tree Besov priors -- Towards fractal imaging</summary>

- *Hanne Kekkonen, Matti Lassas, Eero Saksman, Samuli Siltanen*

- `2103.00574v1` - [abs](http://arxiv.org/abs/2103.00574v1) - [pdf](http://arxiv.org/pdf/2103.00574v1)

> We propose alternatives to Bayesian a priori distributions that are frequently used in the study of inverse problems. Our aim is to construct priors that have similar good edge-preserving properties as total variation or Mumford-Shah priors but correspond to well defined infinite-dimensional random variables, and can be approximated by finite-dimensional random variables. We introduce a new wavelet-based model, where the non zero coefficient are chosen in a systematic way so that prior draws have certain fractal behaviour. We show that realisations of this new prior take values in some Besov spaces and have singularities only on a small set $\tau$ that has a certain Hausdorff dimension. We also introduce an efficient algorithm for calculating the MAP estimator, arising from the the new prior, in denoising problem.

</details>

<details>

<summary>2021-02-28 19:38:00 - Controlling for sparsity in sparse factor analysis models: adaptive latent feature sharing for piecewise linear dimensionality reduction</summary>

- *Adam Farooq, Yordan P. Raykov, Petar Raykov, Max A. Little*

- `2006.12369v3` - [abs](http://arxiv.org/abs/2006.12369v3) - [pdf](http://arxiv.org/pdf/2006.12369v3)

> Ubiquitous linear Gaussian exploratory tools such as principle component analysis (PCA) and factor analysis (FA) remain widely used as tools for: exploratory analysis, pre-processing, data visualization and related tasks. However, due to their rigid assumptions including crowding of high dimensional data, they have been replaced in many settings by more flexible and still interpretable latent feature models. The Feature allocation is usually modelled using discrete latent variables assumed to follow either parametric Beta-Bernoulli distribution or Bayesian nonparametric prior. In this work we propose a simple and tractable parametric feature allocation model which can address key limitations of current latent feature decomposition techniques. The new framework allows for explicit control over the number of features used to express each point and enables a more flexible set of allocation distributions including feature allocations with different sparsity levels. This approach is used to derive a novel adaptive Factor analysis (aFA), as well as, an adaptive probabilistic principle component analysis (aPPCA) capable of flexible structure discovery and dimensionality reduction in a wide case of scenarios. We derive both standard Gibbs sampler, as well as, an expectation-maximization inference algorithms that converge orders of magnitude faster to a reasonable point estimate solution. The utility of the proposed aPPCA model is demonstrated for standard PCA tasks such as feature learning, data visualization and data whitening. We show that aPPCA and aFA can infer interpretable high level features both when applied on raw MNIST and when applied for interpreting autoencoder features. We also demonstrate an application of the aPPCA to more robust blind source separation for functional magnetic resonance imaging (fMRI).

</details>

<details>

<summary>2021-02-28 21:35:29 - A Hierarchical Spike-and-Slab Model for Pan-Cancer Survival Using Pan-Omic Data</summary>

- *Sarah Samorodnitsky, Katherine A. Hoadley, Eric F. Lock*

- `2103.00629v1` - [abs](http://arxiv.org/abs/2103.00629v1) - [pdf](http://arxiv.org/pdf/2103.00629v1)

> Pan-omics, pan-cancer analysis has advanced our understanding of the molecular heterogeneity of cancer, expanding what was known from single-cancer or single-omics studies. However, pan-cancer, pan-omics analyses have been limited in their ability to use information from multiple sources of data (e.g., omics platforms) and multiple sample sets (e.g., cancer types) to predict important clinical outcomes, like overall survival. We address the issue of prediction across multiple high-dimensional sources of data and multiple sample sets by using exploratory results from BIDIFAC+, a method for integrative dimension reduction of bidimensionally-linked matrices, in a predictive model. We apply a Bayesian hierarchical model that performs variable selection using spike-and-slab priors which are modified to allow for the borrowing of information across clustered data. This method is used to predict overall patient survival from the Cancer Genome Atlas (TCGA) using data from 29 cancer types and 4 omics sources. Our model selected patterns of variation identified by BIDIFAC+ that differentiate clinical tumor subtypes with markedly different survival outcomes. We also use simulations to evaluate the performance of the modified spike-and-slab prior in terms of its variable selection accuracy and prediction accuracy under different underlying data-generating frameworks. Software and code used for our analysis can be found at https://github.com/sarahsamorodnitsky/HierarchicalSS_PanCanPanOmics/ .

</details>

<details>

<summary>2021-02-28 23:00:34 - Feedback Coding for Active Learning</summary>

- *Gregory Canal, Matthieu Bloch, Christopher Rozell*

- `2103.00654v1` - [abs](http://arxiv.org/abs/2103.00654v1) - [pdf](http://arxiv.org/pdf/2103.00654v1)

> The iterative selection of examples for labeling in active machine learning is conceptually similar to feedback channel coding in information theory: in both tasks, the objective is to seek a minimal sequence of actions to encode information in the presence of noise. While this high-level overlap has been previously noted, there remain open questions on how to best formulate active learning as a communications system to leverage existing analysis and algorithms in feedback coding. In this work, we formally identify and leverage the structural commonalities between the two problems, including the characterization of encoder and noisy channel components, to design a new algorithm. Specifically, we develop an optimal transport-based feedback coding scheme called Approximate Posterior Matching (APM) for the task of active example selection and explore its application to Bayesian logistic regression, a popular model in active learning. We evaluate APM on a variety of datasets and demonstrate learning performance comparable to existing active learning methods, at a reduced computational cost. These results demonstrate the potential of directly deploying concepts from feedback channel coding to design efficient active learning strategies.

</details>


## 2021-03

<details>

<summary>2021-03-01 02:05:31 - Meta-learning representations for clustering with infinite Gaussian mixture models</summary>

- *Tomoharu Iwata*

- `2103.00694v1` - [abs](http://arxiv.org/abs/2103.00694v1) - [pdf](http://arxiv.org/pdf/2103.00694v1)

> For better clustering performance, appropriate representations are critical. Although many neural network-based metric learning methods have been proposed, they do not directly train neural networks to improve clustering performance. We propose a meta-learning method that train neural networks for obtaining representations such that clustering performance improves when the representations are clustered by the variational Bayesian (VB) inference with an infinite Gaussian mixture model. The proposed method can cluster unseen unlabeled data using knowledge meta-learned with labeled data that are different from the unlabeled data. For the objective function, we propose a continuous approximation of the adjusted Rand index (ARI), by which we can evaluate the clustering performance from soft clustering assignments. Since the approximated ARI and the VB inference procedure are differentiable, we can backpropagate the objective function through the VB inference procedure to train the neural networks. With experiments using text and image data sets, we demonstrate that our proposed method has a higher adjusted Rand index than existing methods do.

</details>

<details>

<summary>2021-03-01 03:05:37 - Beyond Marginal Uncertainty: How Accurately can Bayesian Regression Models Estimate Posterior Predictive Correlations?</summary>

- *Chaoqi Wang, Shengyang Sun, Roger Grosse*

- `2011.03178v2` - [abs](http://arxiv.org/abs/2011.03178v2) - [pdf](http://arxiv.org/pdf/2011.03178v2)

> While uncertainty estimation is a well-studied topic in deep learning, most such work focuses on marginal uncertainty estimates, i.e. the predictive mean and variance at individual input locations. But it is often more useful to estimate predictive correlations between the function values at different input locations. In this paper, we consider the problem of benchmarking how accurately Bayesian models can estimate predictive correlations. We first consider a downstream task which depends on posterior predictive correlations: transductive active learning (TAL). We find that TAL makes better use of models' uncertainty estimates than ordinary active learning, and recommend this as a benchmark for evaluating Bayesian models. Since TAL is too expensive and indirect to guide development of algorithms, we introduce two metrics which more directly evaluate the predictive correlations and which can be computed efficiently: meta-correlations (i.e. the correlations between the models correlation estimates and the true values), and cross-normalized likelihoods (XLL). We validate these metrics by demonstrating their consistency with TAL performance and obtain insights about the relative performance of current Bayesian neural net and Gaussian process models.

</details>

<details>

<summary>2021-03-01 04:04:32 - Educational Question Mining At Scale: Prediction, Analysis and Personalization</summary>

- *Zichao Wang, Sebastian Tschiatschek, Simon Woodhead, Jose Miguel Hernandez-Lobato, Simon Peyton Jones, Richard G. Baraniuk, Cheng Zhang*

- `2003.05980v2` - [abs](http://arxiv.org/abs/2003.05980v2) - [pdf](http://arxiv.org/pdf/2003.05980v2)

> Online education platforms enable teachers to share a large number of educational resources such as questions to form exercises and quizzes for students. With large volumes of available questions, it is important to have an automated way to quantify their properties and intelligently select them for students, enabling effective and personalized learning experiences. In this work, we propose a framework for mining insights from educational questions at scale. We utilize the state-of-the-art Bayesian deep learning method, in particular partial variational auto-encoders (p-VAE), to analyze real students' answers to a large collection of questions. Based on p-VAE, we propose two novel metrics that quantify question quality and difficulty, respectively, and a personalized strategy to adaptively select questions for students. We apply our proposed framework to a real-world dataset with tens of thousands of questions and tens of millions of answers from an online education platform. Our framework not only demonstrates promising results in terms of statistical metrics but also obtains highly consistent results with domain experts' evaluation.

</details>

<details>

<summary>2021-03-01 13:12:40 - Factor-augmented Bayesian treatment effects models for panel outcomes</summary>

- *Helga Wagner, Sylvia Frühwirth-Schnatter, Liana Jacobi*

- `2103.00977v1` - [abs](http://arxiv.org/abs/2103.00977v1) - [pdf](http://arxiv.org/pdf/2103.00977v1)

> We propose a new, flexible model for inference of the effect of a binary treatment on a continuous outcome observed over subsequent time periods. The model allows to seperate association due to endogeneity of treatment selection from additional longitudinal association of the outcomes and hence unbiased estimation of dynamic treatment effects.   We investigate the performance of the proposed method on simulated data and employ it to reanalyse data on the longitudinal effects of a long maternity leave   on mothers' earnings after their return to the labour market.

</details>

<details>

<summary>2021-03-01 17:50:51 - Information-geometry of physics-informed statistical manifolds and its use in data assimilation</summary>

- *Francesca Boso, Daniel M. Tartakovsky*

- `2103.01160v1` - [abs](http://arxiv.org/abs/2103.01160v1) - [pdf](http://arxiv.org/pdf/2103.01160v1)

> The data-aware method of distributions (DA-MD) is a low-dimension data assimilation procedure to forecast the behavior of dynamical systems described by differential equations. It combines sequential Bayesian update with the MD, such that the former utilizes available observations while the latter propagates the (joint) probability distribution of the uncertain system state(s). The core of DA-MD is the minimization of a distance between an observation and a prediction in distributional terms, with prior and posterior distributions constrained on a statistical manifold defined by the MD. We leverage the information-geometric properties of the statistical manifold to reduce predictive uncertainty via data assimilation. Specifically, we exploit the information geometric structures induced by two discrepancy metrics, the Kullback-Leibler divergence and the Wasserstein distance, which explicitly yield natural gradient descent. To further accelerate optimization, we build a deep neural network as a surrogate model for the MD that enables automatic differentiation. The manifold's geometry is quantified without sampling, yielding an accurate approximation of the gradient descent direction. Our numerical experiments demonstrate that accounting for the information-geometry of the manifold significantly reduces the computational cost of data assimilation by facilitating the calculation of gradients and by reducing the number of required iterations. Both storage needs and computational cost depend on the dimensionality of a statistical manifold, which is typically small by MD construction. When convergence is achieved, the Kullback-Leibler and $L_2$ Wasserstein metrics have similar performances, with the former being more sensitive to poor choices of the prior.

</details>

<details>

<summary>2021-03-01 18:33:04 - A tighter constraint on Earth-system sensitivity from long-term temperature and carbon-cycle observations</summary>

- *Tony E. Wong, Ying Cui, Dana L. Royer, Klaus Keller*

- `1910.11987v4` - [abs](http://arxiv.org/abs/1910.11987v4) - [pdf](http://arxiv.org/pdf/1910.11987v4)

> The long-term temperature response to a given change in CO2 forcing, or Earth-system sensitivity (ESS), is a key parameter quantifying our understanding about the relationship between changes in Earth's radiative forcing and the resulting long-term Earth-system response. Current ESS estimates are subject to sizable uncertainties. Long-term carbon cycle models can provide a useful avenue to constrain ESS, but previous efforts either use rather informal statistical approaches or focus on discrete paleoevents. Here, we improve on previous ESS estimates by using a Bayesian approach to fuse deep-time CO2 and temperature data over the last 420 Myrs with a long-term carbon cycle model. Our median ESS estimate of 3.4 deg C (2.6-4.7 deg C; 5-95% range) shows a narrower range than previous assessments. We show that weaker chemical weathering relative to the a priori model configuration via reduced weatherable land area yields better agreement with temperature records during the Cretaceous. Research into improving the understanding about these weathering mechanisms hence provides potentially powerful avenues to further constrain this fundamental Earth-system property.

</details>

<details>

<summary>2021-03-01 21:03:05 - Amortized Conditional Normalized Maximum Likelihood: Reliable Out of Distribution Uncertainty Estimation</summary>

- *Aurick Zhou, Sergey Levine*

- `2011.02696v2` - [abs](http://arxiv.org/abs/2011.02696v2) - [pdf](http://arxiv.org/pdf/2011.02696v2)

> While deep neural networks provide good performance for a range of challenging tasks, calibration and uncertainty estimation remain major challenges, especially under distribution shift. In this paper, we propose the amortized conditional normalized maximum likelihood (ACNML) method as a scalable general-purpose approach for uncertainty estimation, calibration, and out-of-distribution robustness with deep networks. Our algorithm builds on the conditional normalized maximum likelihood (CNML) coding scheme, which has minimax optimal properties according to the minimum description length principle, but is computationally intractable to evaluate exactly for all but the simplest of model classes. We propose to use approximate Bayesian inference technqiues to produce a tractable approximation to the CNML distribution. Our approach can be combined with any approximate inference algorithm that provides tractable posterior densities over model parameters. We demonstrate that ACNML compares favorably to a number of prior techniques for uncertainty estimation in terms of calibration on out-of-distribution inputs.

</details>

<details>

<summary>2021-03-01 22:11:42 - A practical tutorial on Variational Bayes</summary>

- *Minh-Ngoc Tran, Trong-Nghia Nguyen, Viet-Hung Dao*

- `2103.01327v1` - [abs](http://arxiv.org/abs/2103.01327v1) - [pdf](http://arxiv.org/pdf/2103.01327v1)

> This tutorial gives a quick introduction to Variational Bayes (VB), also called Variational Inference or Variational Approximation, from a practical point of view. The paper covers a range of commonly used VB methods and an attempt is made to keep the materials accessible to the wide community of data analysis practitioners. The aim is that the reader can quickly derive and implement their first VB algorithm for Bayesian inference with their data analysis problem. An end-user software package in Matlab together with the documentation can be found at https://vbayeslab.github.io/VBLabDocs/

</details>

<details>

<summary>2021-03-02 03:41:30 - Kernel Interpolation for Scalable Online Gaussian Processes</summary>

- *Samuel Stanton, Wesley J. Maddox, Ian Delbridge, Andrew Gordon Wilson*

- `2103.01454v1` - [abs](http://arxiv.org/abs/2103.01454v1) - [pdf](http://arxiv.org/pdf/2103.01454v1)

> Gaussian processes (GPs) provide a gold standard for performance in online settings, such as sample-efficient control and black box optimization, where we need to update a posterior distribution as we acquire data in a sequential fashion. However, updating a GP posterior to accommodate even a single new observation after having observed $n$ points incurs at least $O(n)$ computations in the exact setting. We show how to use structured kernel interpolation to efficiently recycle computations for constant-time $O(1)$ online updates with respect to the number of points $n$, while retaining exact inference. We demonstrate the promise of our approach in a range of online regression and classification settings, Bayesian optimization, and active sampling to reduce error in malaria incidence forecasting. Code is available at https://github.com/wjmaddox/online_gp.

</details>

<details>

<summary>2021-03-02 09:20:49 - Amortized Bayesian model comparison with evidential deep learning</summary>

- *Stefan T. Radev, Marco D'Alessandro, Ulf K. Mertens, Andreas Voss, Ullrich Köthe, Paul-Christian Bürkner*

- `2004.10629v4` - [abs](http://arxiv.org/abs/2004.10629v4) - [pdf](http://arxiv.org/pdf/2004.10629v4)

> Comparing competing mathematical models of complex natural processes is a shared goal among many branches of science. The Bayesian probabilistic framework offers a principled way to perform model comparison and extract useful metrics for guiding decisions. However, many interesting models are intractable with standard Bayesian methods, as they lack a closed-form likelihood function or the likelihood is computationally too expensive to evaluate. With this work, we propose a novel method for performing Bayesian model comparison using specialized deep learning architectures. Our method is purely simulation-based and circumvents the step of explicitly fitting all alternative models under consideration to each observed dataset. Moreover, it requires no hand-crafted summary statistics of the data and is designed to amortize the cost of simulation over multiple models and observable datasets. This makes the method particularly effective in scenarios where model fit needs to be assessed for a large number of datasets, so that per-dataset inference is practically infeasible.Finally, we propose a novel way to measure epistemic uncertainty in model comparison problems. We demonstrate the utility of our method on toy examples and simulated data from non-trivial models from cognitive science and single-cell neuroscience. We show that our method achieves excellent results in terms of accuracy, calibration, and efficiency across the examples considered in this work. We argue that our framework can enhance and enrich model-based analysis and inference in many fields dealing with computational models of natural processes. We further argue that the proposed measure of epistemic uncertainty provides a unique proxy to quantify absolute evidence even in a framework which assumes that the true data-generating model is within a finite set of candidate models.

</details>

<details>

<summary>2021-03-02 11:35:13 - Uncertainty quantification using martingales for misspecified Gaussian processes</summary>

- *Willie Neiswanger, Aaditya Ramdas*

- `2006.07368v2` - [abs](http://arxiv.org/abs/2006.07368v2) - [pdf](http://arxiv.org/pdf/2006.07368v2)

> We address uncertainty quantification for Gaussian processes (GPs) under misspecified priors, with an eye towards Bayesian Optimization (BO). GPs are widely used in BO because they easily enable exploration based on posterior uncertainty bands. However, this convenience comes at the cost of robustness: a typical function encountered in practice is unlikely to have been drawn from the data scientist's prior, in which case uncertainty estimates can be misleading, and the resulting exploration can be suboptimal. We present a frequentist approach to GP/BO uncertainty quantification. We utilize the GP framework as a working model, but do not assume correctness of the prior. We instead construct a confidence sequence (CS) for the unknown function using martingale techniques. There is a necessary cost to achieving robustness: if the prior was correct, posterior GP bands are narrower than our CS. Nevertheless, when the prior is wrong, our CS is statistically valid and empirically outperforms standard GP methods, in terms of both coverage and utility for BO. Additionally, we demonstrate that powered likelihoods provide robustness against model misspecification.

</details>

<details>

<summary>2021-03-02 13:44:56 - Concentration of posterior probabilities and normalized L0 criteria</summary>

- *David Rossell*

- `1806.04071v7` - [abs](http://arxiv.org/abs/1806.04071v7) - [pdf](http://arxiv.org/pdf/1806.04071v7)

> We study frequentist properties of Bayesian and $L_0$ model selection, with a focus on (potentially non-linear) high-dimensional regression. We propose a construction to study how posterior probabilities and normalized $L_0$ criteria concentrate on the (Kullback-Leibler) optimal model and other subsets of the model space. When such concentration occurs, one also bounds the frequentist probabilities of selecting the correct model, type I and type II errors. These results hold generally, and help validate the use of posterior probabilities and $L_0$ criteria to control frequentist error probabilities associated to model selection and hypothesis tests. Regarding regression, we help understand the effect of the sparsity imposed by the prior or the $L_0$ penalty, and of problem characteristics such as the sample size, signal-to-noise, dimension and true sparsity. A particular finding is that one may use less sparse formulations than would be asymptotically optimal, but still attain consistency and often also significantly better finite-sample performance. We also prove new results related to misspecifying the mean or covariance structures, and give tighter rates for certain non-local priors than currently available.

</details>

<details>

<summary>2021-03-02 15:27:49 - Oil and Gas Reservoirs Parameters Analysis Using Mixed Learning of Bayesian Networks</summary>

- *Irina Deeva, Anna Bubnova, Petr Andriushchenko, Anton Voskresenskiy, Nikita Bukhanov, Nikolay O. Nikitin, Anna V. Kalyuzhnaya*

- `2103.01804v1` - [abs](http://arxiv.org/abs/2103.01804v1) - [pdf](http://arxiv.org/pdf/2103.01804v1)

> In this paper, a multipurpose Bayesian-based method for data analysis, causal inference and prediction in the sphere of oil and gas reservoir development is considered. This allows analysing parameters of a reservoir, discovery dependencies among parameters (including cause and effects relations), checking for anomalies, prediction of expected values of missing parameters, looking for the closest analogues, and much more. The method is based on extended algorithm MixLearn@BN for structural learning of Bayesian networks. Key ideas of MixLearn@BN are following: (1) learning the network structure on homogeneous data subsets, (2) assigning a part of the structure by an expert, and (3) learning the distribution parameters on mixed data (discrete and continuous). Homogeneous data subsets are identified as various groups of reservoirs with similar features (analogues), where similarity measure may be based on several types of distances. The aim of the described technique of Bayesian network learning is to improve the quality of predictions and causal inference on such networks. Experimental studies prove that the suggested method gives a significant advantage in missing values prediction and anomalies detection accuracy. Moreover, the method was applied to the database of more than a thousand petroleum reservoirs across the globe and allowed to discover novel insights in geological parameters relationships.

</details>

<details>

<summary>2021-03-02 19:27:46 - Towards Trustworthy Predictions from Deep Neural Networks with Fast Adversarial Calibration</summary>

- *Christian Tomani, Florian Buettner*

- `2012.10923v2` - [abs](http://arxiv.org/abs/2012.10923v2) - [pdf](http://arxiv.org/pdf/2012.10923v2)

> To facilitate a wide-spread acceptance of AI systems guiding decision making in real-world applications, trustworthiness of deployed models is key. That is, it is crucial for predictive models to be uncertainty-aware and yield well-calibrated (and thus trustworthy) predictions for both in-domain samples as well as under domain shift. Recent efforts to account for predictive uncertainty include post-processing steps for trained neural networks, Bayesian neural networks as well as alternative non-Bayesian approaches such as ensemble approaches and evidential deep learning. Here, we propose an efficient yet general modelling approach for obtaining well-calibrated, trustworthy probabilities for samples obtained after a domain shift. We introduce a new training strategy combining an entropy-encouraging loss term with an adversarial calibration loss term and demonstrate that this results in well-calibrated and technically trustworthy predictions for a wide range of domain drifts. We comprehensively evaluate previously proposed approaches on different data modalities, a large range of data sets including sequence data, network architectures and perturbation strategies. We observe that our modelling approach substantially outperforms existing state-of-the-art approaches, yielding well-calibrated predictions under domain drift.

</details>

<details>

<summary>2021-03-02 21:04:33 - Sparse tree-based clustering of microbiome data to characterize microbiome heterogeneity in pancreatic cancer</summary>

- *Yushu Shi, Liangliang Zhang, Kim-Anh Do, Robert Jenq, Christine Peterson*

- `2007.15812v2` - [abs](http://arxiv.org/abs/2007.15812v2) - [pdf](http://arxiv.org/pdf/2007.15812v2)

> There is a keen interest in characterizing variation in the microbiome across cancer patients, given increasing evidence of its important role in determining treatment outcomes. Here our goal is to discover subgroups of patients with similar microbiome profiles. We propose a novel unsupervised clustering approach in the Bayesian framework that innovates over existing model-based clustering approaches, such as the Dirichlet multinomial mixture model, in three key respects: we incorporate feature selection, learn the appropriate number of clusters from the data, and integrate information on the tree structure relating the observed features. We compare the performance of our proposed method to existing methods on simulated data designed to mimic real microbiome data. We then illustrate results obtained for our motivating data set, a clinical study aimed at characterizing the tumor microbiome of pancreatic cancer patients.

</details>

<details>

<summary>2021-03-03 04:13:14 - Parsimonious Inference</summary>

- *Jed A. Duersch, Thomas A. Catanach*

- `2103.02165v1` - [abs](http://arxiv.org/abs/2103.02165v1) - [pdf](http://arxiv.org/pdf/2103.02165v1)

> Bayesian inference provides a uniquely rigorous approach to obtain principled justification for uncertainty in predictions, yet it is difficult to articulate suitably general prior belief in the machine learning context, where computational architectures are pure abstractions subject to frequent modifications by practitioners attempting to improve results. Parsimonious inference is an information-theoretic formulation of inference over arbitrary architectures that formalizes Occam's Razor; we prefer simple and sufficient explanations. Our universal hyperprior assigns plausibility to prior descriptions, encoded as sequences of symbols, by expanding on the core relationships between program length, Kolmogorov complexity, and Solomonoff's algorithmic probability. We then cast learning as information minimization over our composite change in belief when an architecture is specified, training data are observed, and model parameters are inferred. By distinguishing model complexity from prediction information, our framework also quantifies the phenomenon of memorization.   Although our theory is general, it is most critical when datasets are limited, e.g. small or skewed. We develop novel algorithms for polynomial regression and random forests that are suitable for such data, as demonstrated by our experiments. Our approaches combine efficient encodings with prudent sampling strategies to construct predictive ensembles without cross-validation, thus addressing a fundamental challenge in how to efficiently obtain predictions from data.

</details>

<details>

<summary>2021-03-03 11:57:42 - A Hamiltonian Monte Carlo Model for Imputation and Augmentation of Healthcare Data</summary>

- *Narges Pourshahrokhi, Samaneh Kouchaki, Kord M. Kober, Christine Miaskowski, Payam Barnaghi*

- `2103.02349v1` - [abs](http://arxiv.org/abs/2103.02349v1) - [pdf](http://arxiv.org/pdf/2103.02349v1)

> Missing values exist in nearly all clinical studies because data for a variable or question are not collected or not available. Inadequate handling of missing values can lead to biased results and loss of statistical power in analysis. Existing models usually do not consider privacy concerns or do not utilise the inherent correlations across multiple features to impute the missing values. In healthcare applications, we are usually confronted with high dimensional and sometimes small sample size datasets that need more effective augmentation or imputation techniques. Besides, imputation and augmentation processes are traditionally conducted individually. However, imputing missing values and augmenting data can significantly improve generalisation and avoid bias in machine learning models. A Bayesian approach to impute missing values and creating augmented samples in high dimensional healthcare data is proposed in this work. We propose folded Hamiltonian Monte Carlo (F-HMC) with Bayesian inference as a more practical approach to process the cross-dimensional relations by applying a random walk and Hamiltonian dynamics to adapt posterior distribution and generate large-scale samples. The proposed method is applied to a cancer symptom assessment dataset and confirmed to enrich the quality of data in precision, accuracy, recall, F1 score, and propensity metric.

</details>

<details>

<summary>2021-03-03 15:28:35 - Product Partition Dynamic Generalized Linear Models</summary>

- *Victor S. Comitti, Fábio N. Demarqui, Thiago R. dos Santos, Jéssica da Assunção Almeida*

- `2103.02470v1` - [abs](http://arxiv.org/abs/2103.02470v1) - [pdf](http://arxiv.org/pdf/2103.02470v1)

> Detection and modeling of change-points in time-series can be considerably challenging. In this paper we approach this problem by incorporating the class of Dynamic Generalized Linear Models (DGLM) into the well know class of Product Partition Models (PPM). This new methodology, that we call DGLM-PPM, extends the PPM to distributions within the Exponential Family while also retaining the flexibility of the DGLM class. It also provides a framework for Bayesian multiple change-point detection in dynamic regression models. Inference on the DGLM-PPM follow the steps of evolution and updating of the DGLM class. A Gibbs Sampler scheme with an Adaptive Rejection Metropolis Sampling (ARMS) step appended is used to compute posterior estimates of the relevant quantities. A simulation study shows that the proposed model provides reasonable estimates of the dynamic parameters and also assigns high change-point probabilities to the breaks introduced in the artificial data generated for this work. We also present a real life data example that highlights the superiority of the DGLM-PPM over the conventional DGLM in both in-sample and out-of-sample goodness of fit measures.

</details>

<details>

<summary>2021-03-03 22:14:01 - Importance Sampling with the Integrated Nested Laplace Approximation</summary>

- *Martin Outzen Berild, Sara Martino, Virgilio Gómez-Rubio, Håvard Rue*

- `2103.02721v1` - [abs](http://arxiv.org/abs/2103.02721v1) - [pdf](http://arxiv.org/pdf/2103.02721v1)

> The Integrated Nested Laplace Approximation (INLA) is a deterministic approach to Bayesian inference on latent Gaussian models (LGMs) and focuses on fast and accurate approximation of posterior marginals for the parameters in the models. Recently, methods have been developed to extend this class of models to those that can be expressed as conditional LGMs by fixing some of the parameters in the models to descriptive values. These methods differ in the manner descriptive values are chosen. This paper proposes to combine importance sampling with INLA (IS-INLA), and extends this approach with the more robust adaptive multiple importance sampling algorithm combined with INLA (AMIS-INLA).   This paper gives a comparison between these approaches and existing methods on a series of applications with simulated and observed datasets and evaluates their performance based on accuracy, efficiency, and robustness. The approaches are validated by exact posteriors in a simple bivariate linear model; then, they are applied to a Bayesian lasso model, a Bayesian imputation of missing covariate values, and lastly, in parametric Bayesian quantile regression. The applications show that the AMIS-INLA approach, in general, outperforms the other methods, but the IS-INLA algorithm could be considered for faster inference when good proposals are available.

</details>

<details>

<summary>2021-03-04 04:16:36 - Multimodal Bayesian Registration of Noisy Functions using Hamiltonian Monte Carlo</summary>

- *J. Derek Tucker, Lyndsay Shand, Kenny Chowdhary*

- `2005.14372v2` - [abs](http://arxiv.org/abs/2005.14372v2) - [pdf](http://arxiv.org/pdf/2005.14372v2)

> Functional data registration is a necessary processing step for many applications. The observed data can be inherently noisy, often due to measurement error or natural process uncertainty, which most functional alignment methods cannot handle. A pair of functions can also have multiple optimal alignment solutions, which is not addressed in current literature. In this paper, a flexible Bayesian approach to functional alignment is presented, which appropriately accounts for noise in the data without any pre-smoothing required. Additionally, by running parallel MCMC chains, the method can account for multiple optimal alignments via the multi-modal posterior distribution of the warping functions. To most efficiently sample the warping functions, the approach relies on a modification of the standard Hamiltonian Monte Carlo to be well-defined on the infinite-dimensional Hilbert space. This flexible Bayesian alignment method is applied to both simulated data and real data sets to show its efficiency in handling noisy functions and successfully accounting for multiple optimal alignments in the posterior; characterizing the uncertainty surrounding the warping functions.

</details>

<details>

<summary>2021-03-04 11:49:12 - Approximate Bayesian Conditional Copulas</summary>

- *Clara Grazian, Luciana Dalla Valle, Brunero Liseo*

- `2103.02974v1` - [abs](http://arxiv.org/abs/2103.02974v1) - [pdf](http://arxiv.org/pdf/2103.02974v1)

> Copula models are flexible tools to represent complex structures of dependence for multivariate random variables. According to Sklar's theorem (Sklar, 1959), any d-dimensional absolutely continuous density can be uniquely represented as the product of the marginal distributions and a copula function which captures the dependence structure among the vector components. In real data applications, the interest of the analyses often lies on specific functionals of the dependence, which quantify aspects of it in a few numerical values. A broad literature exists on such functionals, however extensions to include covariates are still limited. This is mainly due to the lack of unbiased estimators of the copula function, especially when one does not have enough information to select the copula model. Recent advances in computational methodologies and algorithms have allowed inference in the presence of complicated likelihood functions, especially in the Bayesian approach, whose methods, despite being computationally intensive, allow us to better evaluate the uncertainty of the estimates. In this work, we present several Bayesian methods to approximate the posterior distribution of functionals of the dependence, using nonparametric models which avoid the selection of the copula function. These methods are compared in simulation studies and in two realistic applications, from civil engineering and astrophysics.

</details>

<details>

<summary>2021-03-04 23:42:14 - Gaussian processes meet NeuralODEs: A Bayesian framework for learning the dynamics of partially observed systems from scarce and noisy data</summary>

- *Mohamed Aziz Bhouri, Paris Perdikaris*

- `2103.03385v1` - [abs](http://arxiv.org/abs/2103.03385v1) - [pdf](http://arxiv.org/pdf/2103.03385v1)

> This paper presents a machine learning framework (GP-NODE) for Bayesian systems identification from partial, noisy and irregular observations of nonlinear dynamical systems. The proposed method takes advantage of recent developments in differentiable programming to propagate gradient information through ordinary differential equation solvers and perform Bayesian inference with respect to unknown model parameters using Hamiltonian Monte Carlo sampling and Gaussian Process priors over the observed system states. This allows us to exploit temporal correlations in the observed data, and efficiently infer posterior distributions over plausible models with quantified uncertainty. Moreover, the use of sparsity-promoting priors such as the Finnish Horseshoe for free model parameters enables the discovery of interpretable and parsimonious representations for the underlying latent dynamics. A series of numerical studies is presented to demonstrate the effectiveness of the proposed GP-NODE method including predator-prey systems, systems biology, and a 50-dimensional human motion dynamical system. Taken together, our findings put forth a novel, flexible and robust workflow for data-driven model discovery under uncertainty. All code and data accompanying this manuscript are available online at \url{https://github.com/PredictiveIntelligenceLab/GP-NODEs}.

</details>

<details>

<summary>2021-03-05 00:11:56 - Gemini: Dynamic Bias Correction for Autonomous Experimentation and Molecular Simulation</summary>

- *Riley J. Hickman, Florian Häse, Loïc M. Roch, Alán Aspuru-Guzik*

- `2103.03391v1` - [abs](http://arxiv.org/abs/2103.03391v1) - [pdf](http://arxiv.org/pdf/2103.03391v1)

> Bayesian optimization has emerged as a powerful strategy to accelerate scientific discovery by means of autonomous experimentation. However, expensive measurements are required to accurately estimate materials properties, and can quickly become a hindrance to exhaustive materials discovery campaigns. Here, we introduce Gemini: a data-driven model capable of using inexpensive measurements as proxies for expensive measurements by correcting systematic biases between property evaluation methods. We recommend using Gemini for regression tasks with sparse data and in an autonomous workflow setting where its predictions of expensive to evaluate objectives can be used to construct a more informative acquisition function, thus reducing the number of expensive evaluations an optimizer needs to achieve desired target values. In a regression setting, we showcase the ability of our method to make accurate predictions of DFT calculated bandgaps of hybrid organic-inorganic perovskite materials. We further demonstrate the benefits that Gemini provides to autonomous workflows by augmenting the Bayesian optimizer Phoenics to yeild a scalable optimization framework leveraging multiple sources of measurement. Finally, we simulate an autonomous materials discovery platform for optimizing the activity of electrocatalysts for the oxygen evolution reaction. Realizing autonomous workflows with Gemini, we show that the number of measurements of a composition space comprising expensive and rare metals needed to achieve a target overpotential is significantly reduced when measurements from a proxy composition system with less expensive metals are available.

</details>

<details>

<summary>2021-03-05 05:16:43 - $γ$-ABC: Outlier-Robust Approximate Bayesian Computation Based on a Robust Divergence Estimator</summary>

- *Masahiro Fujisawa, Takeshi Teshima, Issei Sato, Masashi Sugiyama*

- `2006.07571v3` - [abs](http://arxiv.org/abs/2006.07571v3) - [pdf](http://arxiv.org/pdf/2006.07571v3)

> Approximate Bayesian computation (ABC) is a likelihood-free inference method that has been employed in various applications. However, ABC can be sensitive to outliers if a data discrepancy measure is chosen inappropriately. In this paper, we propose to use a nearest-neighbor-based $\gamma$-divergence estimator as a data discrepancy measure. We show that our estimator possesses a suitable theoretical robustness property called the redescending property. In addition, our estimator enjoys various desirable properties such as high flexibility, asymptotic unbiasedness, almost sure convergence, and linear-time computational complexity. Through experiments, we demonstrate that our method achieves significantly higher robustness than existing discrepancy measures.

</details>

<details>

<summary>2021-03-05 14:00:04 - On the Occasional Exactness of the Distributional Transform Approximation for Direct Gaussian Copula Models with Discrete Margins</summary>

- *John Hughes*

- `2103.03688v1` - [abs](http://arxiv.org/abs/2103.03688v1) - [pdf](http://arxiv.org/pdf/2103.03688v1)

> The direct Gaussian copula model with discrete marginal distributions is an appealing data-analytic tool but poses difficult computational challenges due to its intractable likelihood. A number of approximations/surrogates for the likelihood have been proposed, including the continuous extension-based approximation (CE) and the distributional transform-based approximation (DT). The continuous extension approach is exact up to Monte Carlo error but does not scale well computationally. The distributional transform approach permits efficient computation but offers no theoretical guarantee that it is exact. In practice, though, the distributional transform-based approximate likelihood is so very nearly exact for some variants of the model as to permit genuine maximum likelihood or Bayesian inference. We demonstrate the exactness of the distributional transform-based objective function for two interesting variants of the model, and propose a quantity that can be used to assess exactness for experimentally observed datasets. Said diagnostic will permit practitioners to determine whether genuine Bayesian inference or ordinary maximum likelihood inference using the DT-based likelihood is possible for a given dataset.

</details>

<details>

<summary>2021-03-05 14:31:05 - DOPE: D-Optimal Pooling Experimental design with application for SARS-CoV-2 screening</summary>

- *Yair Daon, Amit Huppert, Uri Obolski*

- `2103.03706v1` - [abs](http://arxiv.org/abs/2103.03706v1) - [pdf](http://arxiv.org/pdf/2103.03706v1)

> Testing individuals for the presence of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the pathogen causing the coronavirus disease 2019 (COVID-19), is crucial for curtailing transmission chains. Moreover, rapidly testing many potentially infected individuals is often a limiting factor in controlling COVID-19 outbreaks. Hence, pooling strategies, wherein individuals are grouped and tested simultaneously, are employed. We present a novel pooling strategy that implements D-Optimal Pooling Experimental design (DOPE). DOPE defines optimal pooled tests as those maximizing the mutual information between data and infection states. We estimate said mutual information via Monte-Carlo sampling and employ a discrete optimization heuristic for maximizing it. DOPE outperforms common pooling strategies both in terms of lower error rates and fewer tests utilized. DOPE holds several additional advantages: it provides posterior distributions of the probability of infection, rather than only binary classification outcomes; it naturally incorporates prior information of infection probabilities and test error rates; and finally, it can be easily extended to include other, newly discovered information regarding COVID-19. Hence, we believe that implementation of Bayesian D-optimal experimental design holds a great promise for the efforts of combating COVID-19 and other future pandemics.

</details>

<details>

<summary>2021-03-05 17:11:48 - A Bayesian Spatial Modeling Approach to Mortality Forecasting</summary>

- *Zhen Liu, Xiaoqian Sun, Yu-Bo Wang*

- `2102.11501v2` - [abs](http://arxiv.org/abs/2102.11501v2) - [pdf](http://arxiv.org/pdf/2102.11501v2)

> This paper extends Bayesian mortality projection models for multiple populations considering the stochastic structure and the effect of spatial autocorrelation among the observations. We explain high levels of overdispersion according to adjacent locations based on the conditional autoregressive model. In an empirical study, we compare different hierarchical projection models for the analysis of geographical diversity in mortality between the Japanese counties in multiple years, according to age. By a Markov chain Monte Carlo (MCMC) computation, results have demonstrated the flexibility and predictive performance of our proposed model.

</details>

<details>

<summary>2021-03-06 17:27:38 - A Bayesian - Deep Learning model for estimating Covid-19 evolution in Spain</summary>

- *Stefano Cabras*

- `2005.10335v2` - [abs](http://arxiv.org/abs/2005.10335v2) - [pdf](http://arxiv.org/pdf/2005.10335v2)

> This work proposes a semi-parametric approach to estimate Covid-19 (SARS-CoV-2) evolution in Spain. Considering the sequences of 14 days cumulative incidence of all Spanish regions, it combines modern Deep Learning (DL) techniques for analyzing sequences with the usual Bayesian Poisson-Gamma model for counts. DL model provides a suitable description of observed sequences but no reliable uncertainty quantification around it can be obtained. To overcome this we use the prediction from DL as an expert elicitation of the expected number of counts along with their uncertainty and thus obtaining the posterior predictive distribution of counts in an orthodox Bayesian analysis using the well known Poisson-Gamma model. The overall resulting model allows us to either predict the future evolution of the sequences on all regions, as well as, estimating the consequences of eventual scenarios.

</details>

<details>

<summary>2021-03-06 20:32:04 - A Diffusion Approximation Theory of Momentum SGD in Nonconvex Optimization</summary>

- *Tianyi Liu, Zhehui Chen, Enlu Zhou, Tuo Zhao*

- `1802.05155v5` - [abs](http://arxiv.org/abs/1802.05155v5) - [pdf](http://arxiv.org/pdf/1802.05155v5)

> Momentum Stochastic Gradient Descent (MSGD) algorithm has been widely applied to many nonconvex optimization problems in machine learning, e.g., training deep neural networks, variational Bayesian inference, and etc. Despite its empirical success, there is still a lack of theoretical understanding of convergence properties of MSGD. To fill this gap, we propose to analyze the algorithmic behavior of MSGD by diffusion approximations for nonconvex optimization problems with strict saddle points and isolated local optima. Our study shows that the momentum helps escape from saddle points, but hurts the convergence within the neighborhood of optima (if without the step size annealing or momentum annealing). Our theoretical discovery partially corroborates the empirical success of MSGD in training deep neural networks.

</details>

<details>

<summary>2021-03-06 21:14:23 - A Statistical Perspective on the Challenges in Molecular Microbial Biology</summary>

- *Pratheepa Jeganathan, Susan P. Holmes*

- `2103.04198v1` - [abs](http://arxiv.org/abs/2103.04198v1) - [pdf](http://arxiv.org/pdf/2103.04198v1)

> High throughput sequencing (HTS)-based technology enables identifying and quantifying non-culturable microbial organisms in all environments. Microbial sequences have enhanced our understanding of the human microbiome, the soil and plant environment, and the marine environment. All molecular microbial data pose statistical challenges due to contamination sequences from reagents, batch effects, unequal sampling, and undetected taxa. Technical biases and heteroscedasticity have the strongest effects, but different strains across subjects and environments also make direct differential abundance testing unwieldy. We provide an introduction to a few statistical tools that can overcome some of these difficulties and demonstrate those tools on an example. We show how standard statistical methods, such as simple hierarchical mixture and topic models, can facilitate inferences on latent microbial communities. We also review some nonparametric Bayesian approaches that combine visualization and uncertainty quantification. The intersection of molecular microbial biology and statistics is an exciting new venue. Finally, we list some of the important open problems that would benefit from more careful statistical method development.

</details>

<details>

<summary>2021-03-07 04:15:11 - The Determinants of Democracy Revisited: An Instrumental Variable Bayesian Model Averaging Approach</summary>

- *Sajad Rahimian*

- `2103.04255v1` - [abs](http://arxiv.org/abs/2103.04255v1) - [pdf](http://arxiv.org/pdf/2103.04255v1)

> Identifying the real causes of democracy is an ongoing debate. We contribute to the literature by examining the robustness of a comprehensive list of 42 potential determinants of democracy. We take a step forward and employ Instrumental Variable Bayesian Model Averaging (IVBMA) method to tackle endogeneity explicitly. Using the data of 111 countries, our IVBMA results mark arable land as the most persistent predictor of democracy with a posterior inclusion probability (PIP) of 0.961. Youth population (PIP: 0.893), life expectancy (PIP: 0.839), and GDP per capita (PIP: 0.758) are the next critical independent variables. In a subsample of 80 developing countries, in addition to arable land (PIP: 0.919), state fragility proves to be a significant determinant of democracy (PIP: 0.779).

</details>

<details>

<summary>2021-03-07 21:35:16 - Improving Bayesian estimation of Vaccine Efficacy</summary>

- *Mauro Gasparini*

- `2103.04462v1` - [abs](http://arxiv.org/abs/2103.04462v1) - [pdf](http://arxiv.org/pdf/2103.04462v1)

> A full Bayesian approach to the estimation of Vaccine Efficacy is presented, which is an improvement over the currently used exact method conditional on the total number of cases. As an example, we reconsider the statistical sections of the BioNTech/Pfizer protocol, which in 2020 has led to the first approved anti-Covid-19 vaccine.

</details>

<details>

<summary>2021-03-08 02:45:09 - Consistent Sparse Deep Learning: Theory and Computation</summary>

- *Yan Sun, Qifan Song, Faming Liang*

- `2102.13229v2` - [abs](http://arxiv.org/abs/2102.13229v2) - [pdf](http://arxiv.org/pdf/2102.13229v2)

> Deep learning has been the engine powering many successes of data science. However, the deep neural network (DNN), as the basic model of deep learning, is often excessively over-parameterized, causing many difficulties in training, prediction and interpretation. We propose a frequentist-like method for learning sparse DNNs and justify its consistency under the Bayesian framework: the proposed method could learn a sparse DNN with at most $O(n/\log(n))$ connections and nice theoretical guarantees such as posterior consistency, variable selection consistency and asymptotically optimal generalization bounds. In particular, we establish posterior consistency for the sparse DNN with a mixture Gaussian prior, show that the structure of the sparse DNN can be consistently determined using a Laplace approximation-based marginal posterior inclusion probability approach, and use Bayesian evidence to elicit sparse DNNs learned by an optimization method such as stochastic gradient descent in multiple runs with different initializations. The proposed method is computationally more efficient than standard Bayesian methods for large-scale sparse DNNs. The numerical results indicate that the proposed method can perform very well for large-scale network compression and high-dimensional nonlinear variable selection, both advancing interpretable machine learning.

</details>

<details>

<summary>2021-03-08 05:34:45 - Bayesian modelling of time-varying conditional heteroscedasticity</summary>

- *Sayar Karmakar, Arkaprava Roy*

- `2009.06007v2` - [abs](http://arxiv.org/abs/2009.06007v2) - [pdf](http://arxiv.org/pdf/2009.06007v2)

> Conditional heteroscedastic (CH) models are routinely used to analyze financial datasets. The classical models such as ARCH-GARCH with time-invariant coefficients are often inadequate to describe frequent changes over time due to market variability. However we can achieve significantly better insight by considering the time-varying analogues of these models. In this paper, we propose a Bayesian approach to the estimation of such models and develop computationally efficient MCMC algorithm based on Hamiltonian Monte Carlo (HMC) sampling. We also established posterior contraction rates with increasing sample size in terms of the average Hellinger metric. The performance of our method is compared with frequentist estimates and estimates from the time constant analogues. To conclude the paper we obtain time-varying parameter estimates for some popular Forex (currency conversion rate) and stock market datasets.

</details>

<details>

<summary>2021-03-08 12:55:17 - Robust decision analysis under severe uncertainty and ambiguous tradeoffs: an invasive species case study</summary>

- *Ullrika Sahlin, Matthias C. M. Troffaes, Lennart Edsman*

- `2103.04721v1` - [abs](http://arxiv.org/abs/2103.04721v1) - [pdf](http://arxiv.org/pdf/2103.04721v1)

> Bayesian decision analysis is a useful method for risk management decisions, but is limited in its ability to consider severe uncertainty in knowledge, and value ambiguity in management objectives. We study the use of robust Bayesian decision analysis to handle problems where one or both of these issues arise. The robust Bayesian approach models severe uncertainty through bounds on probability distributions, and value ambiguity through bounds on utility functions. To incorporate data, standard Bayesian updating is applied on the entire set of distributions. To elicit our expert's utility representing the value of different management objectives, we use a modified version of the swing weighting procedure that can cope with severe value ambiguity. We demonstrate these methods on an environmental management problem to eradicate an alien invasive marmorkrebs recently discovered in Sweden, which needed a rapid response despite substantial knowledge gaps if the species was still present (i.e. severe uncertainty) and the need for difficult tradeoffs and competing interests (i.e. value ambiguity). We identify that the decision alternatives to drain the system and remove individuals in combination with dredging and sieving with or without a degradable biocide, or increasing pH, are consistently bad under the entire range of probability and utility bounds. This case study shows how robust Bayesian decision analysis provides a transparent methodology for integrating information in risk management problems where little data are available and/or where the tradeoffs ambiguous.

</details>

<details>

<summary>2021-03-09 04:34:59 - Sequential Importance Sampling With Corrections For Partially Observed States</summary>

- *Valentina Di Marco, Jonathan Keith*

- `2103.05217v1` - [abs](http://arxiv.org/abs/2103.05217v1) - [pdf](http://arxiv.org/pdf/2103.05217v1)

> We consider an evolving system for which a sequence of observations is being made, with each observation revealing additional information about current and past states of the system. We suppose each observation is made without error, but does not fully determine the state of the system at the time it is made.   Our motivating example is drawn from invasive species biology, where it is common to know the precise location of invasive organisms that have been detected by a surveillance program, but at any time during the program there are invaders that have not been detected.   We propose a sequential importance sampling strategy to infer the state of the invasion under a Bayesian model of such a system. The strategy involves simulating multiple alternative states consistent with current knowledge of the system, as revealed by the observations. However, a difficult problem that arises is that observations made at a later time are invariably incompatible with previously simulated states. To solve this problem, we propose a two-step iterative process in which states of the system are alternately simulated in accordance with past observations, then corrected in light of new observations. We identify criteria under which such corrections can be made while maintaining appropriate importance weights.

</details>

<details>

<summary>2021-03-09 09:48:17 - Uncertainty-aware Sensitivity Analysis Using Rényi Divergences</summary>

- *Topi Paananen, Michael Riis Andersen, Aki Vehtari*

- `1910.07942v2` - [abs](http://arxiv.org/abs/1910.07942v2) - [pdf](http://arxiv.org/pdf/1910.07942v2)

> For nonlinear supervised learning models, assessing the importance of predictor variables or their interactions is not straightforward because it can vary in the domain of the variables. Importance can be assessed locally with sensitivity analysis using general methods that rely on the model's predictions or their derivatives. In this work, we extend derivative based sensitivity analysis to a Bayesian setting by differentiating the R\'enyi divergence of a model's predictive distribution. By utilising the predictive distribution instead of a point prediction, the model uncertainty is taken into account in a principled way. Our empirical results on simulated and real data sets demonstrate accurate and reliable identification of important variables and interaction effects compared to alternative methods.

</details>

<details>

<summary>2021-03-09 11:09:05 - Instance-Wise Minimax-Optimal Algorithms for Logistic Bandits</summary>

- *Marc Abeille, Louis Faury, Clément Calauzènes*

- `2010.12642v2` - [abs](http://arxiv.org/abs/2010.12642v2) - [pdf](http://arxiv.org/pdf/2010.12642v2)

> Logistic Bandits have recently attracted substantial attention, by providing an uncluttered yet challenging framework for understanding the impact of non-linearity in parametrized bandits. It was shown by Faury et al. (2020) that the learning-theoretic difficulties of Logistic Bandits can be embodied by a large (sometimes prohibitively) problem-dependent constant $\kappa$, characterizing the magnitude of the reward's non-linearity. In this paper we introduce a novel algorithm for which we provide a refined analysis. This allows for a better characterization of the effect of non-linearity and yields improved problem-dependent guarantees. In most favorable cases this leads to a regret upper-bound scaling as $\tilde{\mathcal{O}}(d\sqrt{T/\kappa})$, which dramatically improves over the $\tilde{\mathcal{O}}(d\sqrt{T}+\kappa)$ state-of-the-art guarantees. We prove that this rate is minimax-optimal by deriving a $\Omega(d\sqrt{T/\kappa})$ problem-dependent lower-bound. Our analysis identifies two regimes (permanent and transitory) of the regret, which ultimately re-conciliates Faury et al. (2020) with the Bayesian approach of Dong et al. (2019). In contrast to previous works, we find that in the permanent regime non-linearity can dramatically ease the exploration-exploitation trade-off. While it also impacts the length of the transitory phase in a problem-dependent fashion, we show that this impact is mild in most reasonable configurations.

</details>

<details>

<summary>2021-03-09 11:57:15 - A Bayesian construction of asymptotically unbiased estimators</summary>

- *Masayo Y. Hirose, Shuhei Mano*

- `2011.14747v2` - [abs](http://arxiv.org/abs/2011.14747v2) - [pdf](http://arxiv.org/pdf/2011.14747v2)

> A differential geometric framework to construct an asymptotically unbiased estimator of a function of a parameter is presented. The derived estimator asymptotically coincides with the uniformly minimum variance unbiased estimator, if a complete sufficient statistic exists. The framework is based on the maximum a posteriori estimation, where the prior is chosen such that the estimator is unbiased. The framework is demonstrated for the second-order asymptotic unbiasedness (unbiased up to $O(n^{-1})$ for a sample of size $n$). The condition of the asymptotic unbiasedness leads the choice of the prior such that the departure from a kind of harmonicity of the estimand is canceled out at each point of the model manifold. For a given estimand, the prior is given as an integral. On the other hand, for a given prior, we can address the bias of what estimator can be reduced by solving an elliptic partial differential equation. A family of invariant priors, which generalizes the Jeffreys prior, is mentioned as a specific example. Some illustrative examples of applications of the proposed framework are provided.

</details>

<details>

<summary>2021-03-09 12:13:33 - Unbiased inference for discretely observed hidden Markov model diffusions</summary>

- *Neil K. Chada, Jordan Franks, Ajay Jasra, Kody J. H. Law, Matti Vihola*

- `1807.10259v8` - [abs](http://arxiv.org/abs/1807.10259v8) - [pdf](http://arxiv.org/pdf/1807.10259v8)

> We develop a Bayesian inference method for diffusions observed discretely and with noise, which is free of discretisation bias. Unlike existing unbiased inference methods, our method does not rely on exact simulation techniques. Instead, our method uses standard time-discretised approximations of diffusions, such as the Euler--Maruyama scheme. Our approach is based on particle marginal Metropolis--Hastings, a particle filter, randomised multilevel Monte Carlo, and importance sampling type correction of approximate Markov chain Monte Carlo. The resulting estimator leads to inference without a bias from the time-discretisation as the number of Markov chain iterations increases. We give convergence results and recommend allocations for algorithm inputs. Our method admits a straightforward parallelisation, and can be computationally efficient. The user-friendly approach is illustrated on three examples, where the underlying diffusion is an Ornstein--Uhlenbeck process, a geometric Brownian motion, and a 2d non-reversible Langevin equation.

</details>

<details>

<summary>2021-03-09 12:29:13 - Inference in Bayesian Additive Vector Autoregressive Tree Models</summary>

- *Florian Huber, Luca Rossini*

- `2006.16333v2` - [abs](http://arxiv.org/abs/2006.16333v2) - [pdf](http://arxiv.org/pdf/2006.16333v2)

> Vector autoregressive (VAR) models assume linearity between the endogenous variables and their lags. This assumption might be overly restrictive and could have a deleterious impact on forecasting accuracy. As a solution, we propose combining VAR with Bayesian additive regression tree (BART) models. The resulting Bayesian additive vector autoregressive tree (BAVART) model is capable of capturing arbitrary non-linear relations between the endogenous variables and the covariates without much input from the researcher. Since controlling for heteroscedasticity is key for producing precise density forecasts, our model allows for stochastic volatility in the errors. We apply our model to two datasets. The first application shows that the BAVART model yields highly competitive forecasts of the US term structure of interest rates. In a second application, we estimate our model using a moderately sized Eurozone dataset to investigate the dynamic effects of uncertainty on the economy.

</details>

<details>

<summary>2021-03-09 13:43:29 - Optimal Transport of Information</summary>

- *Semyon Malamud, Anna Cieslak, Andreas Schrimpf*

- `2102.10909v4` - [abs](http://arxiv.org/abs/2102.10909v4) - [pdf](http://arxiv.org/pdf/2102.10909v4)

> We study the general problem of Bayesian persuasion (optimal information design) with continuous actions and continuous state space in arbitrary dimensions. First, we show that with a finite signal space, the optimal information design is always given by a partition. Second, we take the limit of an infinite signal space and characterize the solution in terms of a Monge-Kantorovich optimal transport problem with an endogenous information transport cost. We use our novel approach to: 1. Derive necessary and sufficient conditions for optimality based on Bregman divergences for non-convex functions. 2. Compute exact bounds for the Hausdorff dimension of the support of an optimal policy. 3. Derive a non-linear, second-order partial differential equation whose solutions correspond to regular optimal policies. We illustrate the power of our approach by providing explicit solutions to several non-linear, multidimensional Bayesian persuasion problems.

</details>

<details>

<summary>2021-03-09 16:09:21 - Time-varying auto-regressive models for count time-series</summary>

- *Arkaprava Roy, Sayar Karmakar*

- `2009.07634v2` - [abs](http://arxiv.org/abs/2009.07634v2) - [pdf](http://arxiv.org/pdf/2009.07634v2)

> Count-valued time series data are routinely collected in many application areas. We are particularly motivated to study the count time series of daily new cases, arising from COVID-19 spread. We propose two Bayesian models, a time-varying semiparametric AR(p) model for count and then a time-varying INGARCH model considering the rapid changes in the spread. We calculate posterior contraction rates of the proposed Bayesian methods with respect to average Hellinger metric. Our proposed structures of the models are amenable to Hamiltonian Monte Carlo (HMC) sampling for efficient computation. We substantiate our methods by simulations that show superiority compared to some of the close existing methods. Finally we analyze the daily time series data of newly confirmed cases to study its spread through different government interventions.

</details>

<details>

<summary>2021-03-09 17:05:25 - Bayesian inference of network structure from unreliable data</summary>

- *Jean-Gabriel Young, George T. Cantwell, M. E. J. Newman*

- `2008.03334v2` - [abs](http://arxiv.org/abs/2008.03334v2) - [pdf](http://arxiv.org/pdf/2008.03334v2)

> Most empirical studies of complex networks do not return direct, error-free measurements of network structure. Instead, they typically rely on indirect measurements that are often error-prone and unreliable. A fundamental problem in empirical network science is how to make the best possible estimates of network structure given such unreliable data. In this paper we describe a fully Bayesian method for reconstructing networks from observational data in any format, even when the data contain substantial measurement error and when the nature and magnitude of that error is unknown. The method is introduced through pedagogical case studies using real-world example networks, and specifically tailored to allow straightforward, computationally efficient implementation with a minimum of technical input. Computer code implementing the method is publicly available.

</details>

<details>

<summary>2021-03-09 22:40:55 - Asymptotic posterior normality of the generalized extreme value distribution</summary>

- *Likun Zhang, Benjamin A. Shaby*

- `2103.05747v1` - [abs](http://arxiv.org/abs/2103.05747v1) - [pdf](http://arxiv.org/pdf/2103.05747v1)

> The univariate generalized extreme value (GEV) distribution is the most commonly used tool for analysing the properties of rare events. The ever greater utilization of Bayesian methods for extreme value analysis warrants detailed theoretical investigation, which has thus far been underdeveloped. Even the most basic asymptotic results are difficult to obtain because the GEV fails to satisfy standard regularity conditions. Here, we prove that the posterior distribution of the GEV parameter vector, given an independent and identically distributed sequence of observations, converges to a normal distribution centred at the true parameter. The proof necessitates analysing integrals of the GEV likelihood function over the entire parameter space, which requires considerable care because the support of the GEV density depends on the parameters in complicated ways.

</details>

<details>

<summary>2021-03-09 22:46:52 - On Information Gain and Regret Bounds in Gaussian Process Bandits</summary>

- *Sattar Vakili, Kia Khezeli, Victor Picheny*

- `2009.06966v3` - [abs](http://arxiv.org/abs/2009.06966v3) - [pdf](http://arxiv.org/pdf/2009.06966v3)

> Consider the sequential optimization of an expensive to evaluate and possibly non-convex objective function $f$ from noisy feedback, that can be considered as a continuum-armed bandit problem. Upper bounds on the regret performance of several learning algorithms (GP-UCB, GP-TS, and their variants) are known under both a Bayesian (when $f$ is a sample from a Gaussian process (GP)) and a frequentist (when $f$ lives in a reproducing kernel Hilbert space) setting. The regret bounds often rely on the maximal information gain $\gamma_T$ between $T$ observations and the underlying GP (surrogate) model. We provide general bounds on $\gamma_T$ based on the decay rate of the eigenvalues of the GP kernel, whose specialisation for commonly used kernels, improves the existing bounds on $\gamma_T$, and subsequently the regret bounds relying on $\gamma_T$ under numerous settings. For the Mat\'ern family of kernels, where the lower bounds on $\gamma_T$, and regret under the frequentist setting, are known, our results close a huge polynomial in $T$ gap between the upper and lower bounds (up to logarithmic in $T$ factors).

</details>

<details>

<summary>2021-03-10 03:41:34 - Bayesian Poisson Mortality Projections with Incomplete Data</summary>

- *Rui Gong, Xiaoqian Sun, Leping Liu, Yu-Bo Wang*

- `2103.05856v1` - [abs](http://arxiv.org/abs/2103.05856v1) - [pdf](http://arxiv.org/pdf/2103.05856v1)

> The missing data problem pervasively exists in statistical applications. Even as simple as the count data in mortality projections, it may not be available for certain age-and-year groups due to the budget limitations or difficulties in tracing research units, resulting in the follow-up estimation and prediction inaccuracies. To circumvent this data-driven challenge, we extend the Poisson log-normal Lee-Carter model to accommodate a more flexible time structure, and develop the new sampling algorithm that improves the MCMC convergence when dealing with incomplete mortality data. Via the overdispersion term and Gibbs sampler, the extended model can be re-written as the dynamic linear model so that both Kalman and sequential Kalman filters can be incorporated into the sampling scheme. Additionally, our meticulous prior settings can avoid the re-scaling step in each MCMC iteration, and allow model selection simultaneously conducted with estimation and prediction. The proposed method is applied to the mortality data of Chinese males during the period 1995-2016 to yield mortality rate forecasts for 2017-2039. The results are comparable to those based on the imputed data set, suggesting that our approach could handle incomplete data well.

</details>

<details>

<summary>2021-03-10 05:36:32 - Time Fused Coefficient SIR Model with Application to COVID-19 Epidemic in the United States</summary>

- *Hou-Cheng Yang, Yishu Xue, Yuqing Pan, Qingyang Liu, Guanyu Hu*

- `2008.04284v3` - [abs](http://arxiv.org/abs/2008.04284v3) - [pdf](http://arxiv.org/pdf/2008.04284v3)

> In this paper, we propose a Susceptible-Infected-Removal (SIR) model with time fused coefficients. In particular, our proposed model discovers the underlying time homogeneity pattern for the SIR model's transmission rate and removal rate via Bayesian shrinkage priors. MCMC sampling for the proposed method is facilitated by the nimble package in R. Extensive simulation studies are carried out to examine the empirical performance of the proposed methods. We further apply the proposed methodology to analyze different levels of COVID-19 data in the United States.

</details>

<details>

<summary>2021-03-10 05:57:44 - A Bayesian-inspired, deep learning-based, semi-supervised domain adaptation technique for land cover mapping</summary>

- *Benjamin Lucas, Charlotte Pelletier, Daniel Schmidt, Geoffrey I. Webb, François Petitjean*

- `2005.11930v2` - [abs](http://arxiv.org/abs/2005.11930v2) - [pdf](http://arxiv.org/pdf/2005.11930v2)

> Land cover maps are a vital input variable to many types of environmental research and management. While they can be produced automatically by machine learning techniques, these techniques require substantial training data to achieve high levels of accuracy, which are not always available. One technique researchers use when labelled training data are scarce is domain adaptation (DA) -- where data from an alternate region, known as the source domain, are used to train a classifier and this model is adapted to map the study region, or target domain. The scenario we address in this paper is known as semi-supervised DA, where some labelled samples are available in the target domain. In this paper we present Sourcerer, a Bayesian-inspired, deep learning-based, semi-supervised DA technique for producing land cover maps from SITS data. The technique takes a convolutional neural network trained on a source domain and then trains further on the available target domain with a novel regularizer applied to the model weights. The regularizer adjusts the degree to which the model is modified to fit the target data, limiting the degree of change when the target data are few in number and increasing it as target data quantity increases. Our experiments on Sentinel-2 time series images compare Sourcerer with two state-of-the-art semi-supervised domain adaptation techniques and four baseline models. We show that on two different source-target domain pairings Sourcerer outperforms all other methods for any quantity of labelled target data available. In fact, the results on the more difficult target domain show that the starting accuracy of Sourcerer (when no labelled target data are available), 74.2%, is greater than the next-best state-of-the-art method trained on 20,000 labelled target instances.

</details>

<details>

<summary>2021-03-10 09:19:13 - Liberty or Depth: Deep Bayesian Neural Nets Do Not Need Complex Weight Posterior Approximations</summary>

- *Sebastian Farquhar, Lewis Smith, Yarin Gal*

- `2002.03704v4` - [abs](http://arxiv.org/abs/2002.03704v4) - [pdf](http://arxiv.org/pdf/2002.03704v4)

> We challenge the longstanding assumption that the mean-field approximation for variational inference in Bayesian neural networks is severely restrictive, and show this is not the case in deep networks. We prove several results indicating that deep mean-field variational weight posteriors can induce similar distributions in function-space to those induced by shallower networks with complex weight posteriors. We validate our theoretical contributions empirically, both through examination of the weight posterior using Hamiltonian Monte Carlo in small models and by comparing diagonal- to structured-covariance in large settings. Since complex variational posteriors are often expensive and cumbersome to implement, our results suggest that using mean-field variational inference in a deeper model is both a practical and theoretically justified alternative to structured approximations.

</details>

<details>

<summary>2021-03-10 10:01:27 - Adaptive MCMC for Generalized Method of Moments with Many Moment Conditions</summary>

- *Masahiro Tanaka*

- `1811.00722v5` - [abs](http://arxiv.org/abs/1811.00722v5) - [pdf](http://arxiv.org/pdf/1811.00722v5)

> A generalized method of moments (GMM) estimator is unreliable for a large number of moment conditions, that is, it is comparable, or larger than the sample size. While classical GMM literature proposes several provisions to this problem, its Bayesian counterpart (i.e., Bayesian inference using a GMM criterion as a quasi-likelihood) almost totally ignores it. This study bridges this gap by proposing an adaptive Markov Chain Monte Carlo (MCMC) approach to a GMM inference with many moment conditions. Particularly, this study focuses on the adaptive tuning of a weighting matrix on the fly. Our proposal consists of two elements. The first is the use of the nonparametric eigenvalue-regularized precision matrix estimator, which contributes to numerical stability. The second is the random update of a weighting matrix, which substantially reduces computational cost, while maintaining the accuracy of the estimation. We then present a simulation study and real data application to compare the performance of the proposed approach with existing approaches.

</details>

<details>

<summary>2021-03-10 10:23:34 - Fast-Rate Loss Bounds via Conditional Information Measures with Applications to Neural Networks</summary>

- *Fredrik Hellström, Giuseppe Durisi*

- `2010.11552v3` - [abs](http://arxiv.org/abs/2010.11552v3) - [pdf](http://arxiv.org/pdf/2010.11552v3)

> We present a framework to derive bounds on the test loss of randomized learning algorithms for the case of bounded loss functions. Drawing from Steinke & Zakynthinou (2020), this framework leads to bounds that depend on the conditional information density between the the output hypothesis and the choice of the training set, given a larger set of data samples from which the training set is formed. Furthermore, the bounds pertain to the average test loss as well as to its tail probability, both for the PAC-Bayesian and the single-draw settings. If the conditional information density is bounded uniformly in the size $n$ of the training set, our bounds decay as $1/n$. This is in contrast with the tail bounds involving conditional information measures available in the literature, which have a less benign $1/\sqrt{n}$ dependence. We demonstrate the usefulness of our tail bounds by showing that they lead to nonvacuous estimates of the test loss achievable with some neural network architectures trained on MNIST and Fashion-MNIST.

</details>

<details>

<summary>2021-03-10 16:05:41 - Bayesian sequential data assimilation for COVID-19 forecasting</summary>

- *Maria L. Daza-Torres, Marcos A. Capistrán, Antonio Capella, J. Andrés Christen*

- `2103.06152v1` - [abs](http://arxiv.org/abs/2103.06152v1) - [pdf](http://arxiv.org/pdf/2103.06152v1)

> We introduce a Bayesian sequential data assimilation method for COVID-19 forecasting. It is assumed that suitable transmission, epidemic and observation models are available and previously validated and the transmission and epidemic models are coded into a dynamical system. The observation model depends on the dynamical system state variables and parameters, and is cast as a likelihood function. We elicit prior distributions of the effective population size, the dynamical system initial conditions and infectious contact rate, and use Markov Chain Monte Carlo sampling to make inference and prediction of quantities of interest (QoI) at the onset of the epidemic outbreak. The forecast is sequentially updated over a sliding window of epidemic records as new data becomes available. Prior distributions for the state variables at the new forecasting time are assembled using the dynamical system, calibrated for the previous forecast. Moreover, changes in the contact rate and effective population size are naturally introduced through auto-regressive models on the corresponding parameters. We show our forecasting method's performance using a SEIR type model and COVID-19 data from several Mexican localities.

</details>

<details>

<summary>2021-03-10 16:20:03 - Bayesian Additive Regression Trees with Model Trees</summary>

- *Estevão B. Prado, Rafael A. Moral, Andrew C. Parnell*

- `2006.07493v5` - [abs](http://arxiv.org/abs/2006.07493v5) - [pdf](http://arxiv.org/pdf/2006.07493v5)

> Bayesian Additive Regression Trees (BART) is a tree-based machine learning method that has been successfully applied to regression and classification problems. BART assumes regularisation priors on a set of trees that work as weak learners and is very flexible for predicting in the presence of non-linearity and high-order interactions. In this paper, we introduce an extension of BART, called Model Trees BART (MOTR-BART), that considers piecewise linear functions at node levels instead of piecewise constants. In MOTR-BART, rather than having a unique value at node level for the prediction, a linear predictor is estimated considering the covariates that have been used as the split variables in the corresponding tree. In our approach, local linearities are captured more efficiently and fewer trees are required to achieve equal or better performance than BART. Via simulation studies and real data applications, we compare MOTR-BART to its main competitors. R code for MOTR-BART implementation is available at https://github.com/ebprado/MOTR-BART.

</details>

<details>

<summary>2021-03-10 16:35:34 - Active recursive Bayesian inference using Rényi information measures</summary>

- *Yeganeh M. Marghi, Aziz Kocanaogullari, Murat Akcakaya, Deniz Erdogmus*

- `2004.03139v2` - [abs](http://arxiv.org/abs/2004.03139v2) - [pdf](http://arxiv.org/pdf/2004.03139v2)

> Recursive Bayesian inference (RBI) provides optimal Bayesian latent variable estimates in real-time settings with streaming noisy observations. Active RBI attempts to effectively select queries that lead to more informative observations to rapidly reduce uncertainty until a confident decision is made. However, typically the optimality objectives of inference and query mechanisms are not jointly selected. Furthermore, conventional active querying methods stagger due to misleading prior information. Motivated by information theoretic approaches, we propose an active RBI framework with unified inference and query selection steps through Renyi entropy and $\alpha$-divergence. We also propose a new objective based on Renyi entropy and its changes called Momentum that encourages exploration for misleading prior cases. The proposed active RBI framework is applied to the trajectory of the posterior changes in the probability simplex that provides a coordinated active querying and decision making with specified confidence. Under certain assumptions, we analytically demonstrate that the proposed approach outperforms conventional methods such as mutual information by allowing the selections of unlikely events. We present empirical and experimental performance evaluations on two applications: restaurant recommendation and brain-computer interface (BCI) typing systems.

</details>

<details>

<summary>2021-03-10 18:56:36 - Bayesian adjustment for preferential testing in estimating the COVID-19 infection fatality rate</summary>

- *Harlan Campbell, Perry de Valpine, Lauren Maxwell, Valentijn MT de Jong, Thomas Debray, Thomas Jänisch, Paul Gustafson*

- `2005.08459v4` - [abs](http://arxiv.org/abs/2005.08459v4) - [pdf](http://arxiv.org/pdf/2005.08459v4)

> A key challenge in estimating the infection fatality rate (IFR) -- and its relation with various factors of interest -- is determining the total number of cases. The total number of cases is not known because not everyone is tested, but also, more importantly, because tested individuals are not representative of the population at large. We refer to the phenomenon whereby infected individuals are more likely to be tested than non-infected individuals, as "preferential testing." An open question is whether or not it is possible to reliably estimate the IFR without any specific knowledge about the degree to which the data are biased by preferential testing. In this paper we take a partial identifiability approach, formulating clearly where deliberate prior assumptions can be made and presenting a Bayesian model which pools information from different samples. When the model is fit to European data obtained from seroprevalence studies and national official COVID-19 statistics, we estimate the overall COVID-19 IFR for Europe to be 0.53%, 95% C.I. = [0.39%, 0.69%].

</details>

<details>

<summary>2021-03-10 19:51:28 - Transport Monte Carlo: High-Accuracy Posterior Approximation via Random Transport</summary>

- *Leo L. Duan*

- `1907.10448v6` - [abs](http://arxiv.org/abs/1907.10448v6) - [pdf](http://arxiv.org/pdf/1907.10448v6)

> In Bayesian applications, there is a huge interest in rapid and accurate estimation of the posterior distribution, particularly for high dimensional or hierarchical models. In this article, we propose to use optimization to solve for a joint distribution (random transport plan) between two random variables, $\theta$ from the posterior distribution and $\beta$ from the simple multivariate uniform. Specifically, we obtain an approximate estimate of the conditional distribution $\Pi(\beta\mid \theta)$ as an infinite mixture of simple location-scale changes; applying the Bayes' theorem, $\Pi(\theta\mid\beta)$ can be sampled as one of the reversed transforms from the uniform, with the weight proportional to the posterior density/mass function. This produces independent random samples with high approximation accuracy, as well as nice theoretic guarantees. Our method shows compelling advantages in performance and accuracy, compared to the state-of-the-art Markov chain Monte Carlo and approximations such as variational Bayes and normalizing flow. We illustrate this approach via several challenging applications, such as sampling from multi-modal distribution, estimating sparse signals in high dimension, and soft-thresholding of a graph with a prior on the degrees.

</details>

<details>

<summary>2021-03-10 22:23:06 - PoD-BIN: A Probability of Decision Bayesian Interval Design for Time-to-Event Dose-Finding Trials with Multiple Toxicity Grades</summary>

- *Meizi Liu, Yuan Ji, Ji Lin*

- `2103.06368v1` - [abs](http://arxiv.org/abs/2103.06368v1) - [pdf](http://arxiv.org/pdf/2103.06368v1)

> We consider a Bayesian framework based on "probability of decision" for dose-finding trial designs. The proposed PoD-BIN design evaluates the posterior predictive probabilities of up-and-down decisions. In PoD-BIN, multiple grades of toxicity, categorized as the mild toxicity (MT) and dose-limiting toxicity (DLT), are modeled simultaneously, and the primary outcome of interests is time-to-toxicity for both MT and DLT. This allows the possibility of enrolling new patients when previously enrolled patients are still being followed for toxicity, thus potentially shortening trial length. The Bayesian decision rules in PoD-BIN utilize the probability of decisions to balance the need to speed up the trial and the risk of exposing patients to overly toxic doses. We demonstrate via numerical examples the resulting balance of speed and safety of PoD-BIN and compare to existing designs.

</details>

<details>

<summary>2021-03-11 02:48:13 - Estimation of Conditional Mean Operator under the Bandable Covariance Structure</summary>

- *Kwangmin Lee, Kyoungjae Lee, Jaeyong Lee*

- `2103.06420v1` - [abs](http://arxiv.org/abs/2103.06420v1) - [pdf](http://arxiv.org/pdf/2103.06420v1)

> We consider high-dimensional multivariate linear regression models, where the joint distribution of covariates and response variables is a multivariate normal distribution with a bandable covariance matrix. The main goal of this paper is to estimate the regression coefficient matrix, which is a function of the bandable covariance matrix. Although the tapering estimator of covariance has the minimax optimal convergence rate for the class of bandable covariances, we show that it has a sub-optimal convergence rate for the regression coefficient; that is, a minimax estimator for the class of bandable covariances may not be a minimax estimator for its functionals. We propose the blockwise tapering estimator of the regression coefficient, which has the minimax optimal convergence rate for the regression coefficient under the bandable covariance assumption. We also propose a Bayesian procedure called the blockwise tapering post-processed posterior of the regression coefficient and show that the proposed Bayesian procedure has the minimax optimal convergence rate for the regression coefficient under the bandable covariance assumption. We show that the proposed methods outperform the existing methods via numerical studies.

</details>

<details>

<summary>2021-03-11 02:51:42 - BaySize: Bayesian Sample Size Planning for Phase I Dose-Finding Trials</summary>

- *Xiaolei Lin, Jiaying Lyu, Shijie Yuan, Sue-Jane Wang, Yuan Ji*

- `2103.06421v1` - [abs](http://arxiv.org/abs/2103.06421v1) - [pdf](http://arxiv.org/pdf/2103.06421v1)

> We propose BaySize, a sample size calculator for phase I clinical trials using Bayesian models. BaySize applies the concept of effect size in dose finding, assuming the MTD is defined based on an equivalence interval. Leveraging a decision framework that involves composite hypotheses, BaySize utilizes two prior distributions, the fitting prior (for model fitting) and sampling prior (for data generation), to conduct sample size calculation under desirable statistical power. Look-up tables are generated to facilitate practical applications. To our knowledge, BaySize is the first sample size tool that can be applied to a broad range of phase I trial designs.

</details>

<details>

<summary>2021-03-11 06:11:05 - Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies</summary>

- *Paul Pu Liang, Manzil Zaheer, Yuan Wang, Amr Ahmed*

- `2003.08197v4` - [abs](http://arxiv.org/abs/2003.08197v4) - [pdf](http://arxiv.org/pdf/2003.08197v4)

> Learning continuous representations of discrete objects such as text, users, movies, and URLs lies at the heart of many applications including language and user modeling. When using discrete objects as input to neural networks, we often ignore the underlying structures (e.g., natural groupings and similarities) and embed the objects independently into individual vectors. As a result, existing methods do not scale to large vocabulary sizes. In this paper, we design a simple and efficient embedding algorithm that learns a small set of anchor embeddings and a sparse transformation matrix. We call our method Anchor & Transform (ANT) as the embeddings of discrete objects are a sparse linear combination of the anchors, weighted according to the transformation matrix. ANT is scalable, flexible, and end-to-end trainable. We further provide a statistical interpretation of our algorithm as a Bayesian nonparametric prior for embeddings that encourages sparsity and leverages natural groupings among objects. By deriving an approximate inference algorithm based on Small Variance Asymptotics, we obtain a natural extension that automatically learns the optimal number of anchors instead of having to tune it as a hyperparameter. On text classification, language modeling, and movie recommendation benchmarks, we show that ANT is particularly suitable for large vocabulary sizes and demonstrates stronger performance with fewer parameters (up to 40x compression) as compared to existing compression baselines.

</details>

<details>

<summary>2021-03-11 09:16:45 - Recent advances in deep learning theory</summary>

- *Fengxiang He, Dacheng Tao*

- `2012.10931v2` - [abs](http://arxiv.org/abs/2012.10931v2) - [pdf](http://arxiv.org/pdf/2012.10931v2)

> Deep learning is usually described as an experiment-driven field under continuous criticizes of lacking theoretical foundations. This problem has been partially fixed by a large volume of literature which has so far not been well organized. This paper reviews and organizes the recent advances in deep learning theory. The literature is categorized in six groups: (1) complexity and capacity-based approaches for analyzing the generalizability of deep learning; (2) stochastic differential equations and their dynamic systems for modelling stochastic gradient descent and its variants, which characterize the optimization and generalization of deep learning, partially inspired by Bayesian inference; (3) the geometrical structures of the loss landscape that drives the trajectories of the dynamic systems; (4) the roles of over-parameterization of deep neural networks from both positive and negative perspectives; (5) theoretical foundations of several special structures in network architectures; and (6) the increasingly intensive concerns in ethics and security and their relationships with generalizability.

</details>

<details>

<summary>2021-03-11 18:14:11 - A hierarchical Bayesian model to find brain-behaviour associations in incomplete data sets</summary>

- *Fabio S. Ferreira, Agoston Mihalik, Rick A. Adams, John Ashburner, Janaina Mourao-Miranda*

- `2103.06845v1` - [abs](http://arxiv.org/abs/2103.06845v1) - [pdf](http://arxiv.org/pdf/2103.06845v1)

> Canonical Correlation Analysis (CCA) and its regularised versions have been widely used in the neuroimaging community to uncover multivariate associations between two data modalities (e.g., brain imaging and behaviour). However, these methods have inherent limitations: (1) statistical inferences about the associations are often not robust; (2) the associations within each data modality are not modelled; (3) missing values need to be imputed or removed. Group Factor Analysis (GFA) is a hierarchical model that addresses the first two limitations by providing Bayesian inference and modelling modality-specific associations. Here, we propose an extension of GFA that handles missing data, and highlight that GFA can be used as a predictive model. We applied GFA to synthetic and real data consisting of brain connectivity and non-imaging measures from the Human Connectome Project (HCP). In synthetic data, GFA uncovered the underlying shared and specific factors and predicted correctly the non-observed data modalities in complete and incomplete data sets. In the HCP data, we identified four relevant shared factors, capturing associations between mood, alcohol and drug use, cognition, demographics and psychopathological measures and the default mode, frontoparietal control, dorsal and ventral networks and insula, as well as two factors describing associations within brain connectivity. In addition, GFA predicted a set of non-imaging measures from brain connectivity. These findings were consistent in complete and incomplete data sets, and replicated previous findings in the literature. GFA is a promising tool that can be used to uncover associations between and within multiple data modalities in benchmark datasets (such as, HCP), and easily extended to more complex models to solve more challenging tasks.

</details>

<details>

<summary>2021-03-12 06:01:17 - Bayesian inference using synthetic likelihood: asymptotics and adjustments</summary>

- *David T. Frazier, David J. Nott, Christopher Drovandi, Robert Kohn*

- `1902.04827v4` - [abs](http://arxiv.org/abs/1902.04827v4) - [pdf](http://arxiv.org/pdf/1902.04827v4)

> Implementing Bayesian inference is often computationally challenging in applications involving complex models, and sometimes calculating the likelihood itself is difficult. Synthetic likelihood is one approach for carrying out inference when the likelihood is intractable, but it is straightforward to simulate from the model. The method constructs an approximate likelihood by taking a vector summary statistic as being multivariate normal, with the unknown mean and covariance matrix estimated by simulation for any given parameter value. Our article makes three contributions. The first shows that if the summary statistic satisfies a central limit theorem, then the synthetic likelihood posterior is asymptotically normal and yields credible sets with the correct level of frequentist coverage. This result is similar to that obtained by approximate Bayesian computation. The second contribution compares the computational efficiency of Bayesian synthetic likelihood and approximate Bayesian computation using the acceptance probability for rejection and importance sampling algorithms with a "good" proposal distribution. We show that Bayesian synthetic likelihood is computationally more efficient than approximate Bayesian computation, and behaves similarly to regression-adjusted approximate Bayesian computation. Based on the asymptotic results, the third contribution proposes using adjusted inference methods when a possibly misspecified form is assumed for the covariance matrix of the synthetic likelihood, such as diagonal or a factor model, to speed up the computation. The methodology is illustrated with some simulated and real examples.

</details>

<details>

<summary>2021-03-12 08:46:53 - Default Bayesian Model Selection of Constrained Multivariate Normal Linear Models</summary>

- *J. Mulder, H. Hoijtink, X. Gu*

- `1904.00679v2` - [abs](http://arxiv.org/abs/1904.00679v2) - [pdf](http://arxiv.org/pdf/1904.00679v2)

> The multivariate normal linear model is one of the most widely employed models for statistical inference in applied research. Special cases include (multivariate) t testing, (M)AN(C)OVA, (multivariate) multiple regression, and repeated measures analysis. Statistical procedures for model selection where the models may have equality and order constraints on the model parameters of interest are limited however. This paper presents a default Bayes factor for this model selection problem. The default Bayes factor is based on generalized fractional Bayes methodology where different fractions are used for different observations and where the default prior is centered on the boundary of the constrained space under investigation. First, the method is fully automatic and therefore can be applied when prior information is weak or completely unavailable. Second, using group specific fractions, the same amount of information is used from each group resulting in a minimally informative default prior having a matrix Cauchy distribution, resulting in a consistent default Bayes factor. Third, numerical computation can be done using parallelization which makes it computationally cheap. Fourth, the evidence can be updated in a relatively simple manner when observing new data. Fifth, the selection criterion can be applied relatively straightforwardly in the presence of missing data that are missing at random. Applications for the social and behavioral sciences are used for illustration.

</details>

<details>

<summary>2021-03-12 14:46:08 - Dynamic Feature Acquisition with Arbitrary Conditional Flows</summary>

- *Yang Li, Junier B. Oliva*

- `2006.07701v2` - [abs](http://arxiv.org/abs/2006.07701v2) - [pdf](http://arxiv.org/pdf/2006.07701v2)

> Many real-world situations allow for the acquisition of additional relevant information when making an assessment with limited or uncertain data. However, traditional ML approaches either require all features to be acquired beforehand or regard part of them as missing data that cannot be acquired. In this work, we propose models that dynamically acquire new features to further improve the prediction assessment. To trade off the improvement with the cost of acquisition, we leverage an information theoretic metric, conditional mutual information, to select the most informative feature to acquire. We leverage a generative model, arbitrary conditional flow (ACFlow), to learn the arbitrary conditional distributions required for estimating the information metric. We also learn a Bayesian network to accelerate the acquisition process. Our model demonstrates superior performance over baselines evaluated in multiple settings.

</details>

<details>

<summary>2021-03-12 16:23:57 - Value of information from vibration-based structural health monitoring extracted via Bayesian model updating</summary>

- *Antonios Kamariotis, Eleni Chatzi, Daniel Straub*

- `2103.07382v1` - [abs](http://arxiv.org/abs/2103.07382v1) - [pdf](http://arxiv.org/pdf/2103.07382v1)

> Quantifying the value of the information extracted from a structural health monitoring (SHM) system is an important step towards convincing decision makers to implement these systems. We quantify this value by adaptation of the Bayesian decision analysis framework. In contrast to previous works, we model in detail the entire process of data generation to processing, model updating and reliability calculation, and investigate it on a deteriorating bridge system. The framework assumes that dynamic response data are obtained in a sequential fashion from deployed accelerometers, subsequently processed by an output-only operational modal analysis scheme for identifying the system's modal characteristics. We employ a classical Bayesian model updating methodology to sequentially learn the deterioration and estimate the structural damage evolution over time. This leads to sequential updating of the structural reliability, which constitutes the basis for a preposterior Bayesian decision analysis. Alternative actions are defined and a heuristic-based approach is employed for the life-cycle optimization. By solving the preposterior Bayesian decision analysis, one is able to quantify the benefit of the availability of long-term SHM vibrational data. Numerical investigations show that this framework can provide quantitative measures on the optimality of an SHM system in a specific decision context.

</details>

<details>

<summary>2021-03-12 17:43:36 - Fast, Scalable Approximations to Posterior Distributions in Extended Latent Gaussian Models</summary>

- *Alex Stringer, Patrick Brown, Jamie Stafford*

- `2103.07425v1` - [abs](http://arxiv.org/abs/2103.07425v1) - [pdf](http://arxiv.org/pdf/2103.07425v1)

> We define a novel class of additive models called Extended Latent Gaussian Models and develop a fast, scalable approximate Bayesian inference methodology for this class. The new class covers a wide range of interesting models, and the new methodology is better suited to large samples than existing approaches. We discuss convergence theory for our posterior approximations. We then illustrate the computational aspects of our approach through a comparison to existing methods, and demonstrate its application in three challenging examples: the analysis of aggregated spatial point process data, the fitting of a Cox proportional hazards model with partial likelihood and a latent spatial point process, and an astrophysical model for estimating the mass of the Milky Way in the presence of multivariate measurement uncertainties. Computations make use of the publicly available aghq package in the R language and code for the examples in the paper is available from https://github.com/awstringer1/elgm-paper-code

</details>

<details>

<summary>2021-03-12 20:25:58 - Semiparametric analysis of clustered interval-censored survival data using Soft Bayesian Additive Regression Trees (SBART)</summary>

- *Piyali Basak, Antonio R. Linero, Debajyoti SInha, Stuart Lipsitz*

- `2005.02509v2` - [abs](http://arxiv.org/abs/2005.02509v2) - [pdf](http://arxiv.org/pdf/2005.02509v2)

> Popular parametric and semiparametric hazards regression models for clustered survival data are inappropriate and inadequate when the unknown effects of different covariates and clustering are complex. This calls for a flexible modeling framework to yield efficient survival prediction. Moreover, for some survival studies involving time to occurrence of some asymptomatic events, survival times are typically interval censored between consecutive clinical inspections. In this article, we propose a robust semiparametric model for clustered interval-censored survival data under a paradigm of Bayesian ensemble learning, called Soft Bayesian Additive Regression Trees or SBART (Linero and Yang, 2018), which combines multiple sparse (soft) decision trees to attain excellent predictive accuracy. We develop a novel semiparametric hazards regression model by modeling the hazard function as a product of a parametric baseline hazard function and a nonparametric component that uses SBART to incorporate clustering, unknown functional forms of the main effects, and interaction effects of various covariates. In addition to being applicable for left-censored, right-censored, and interval-censored survival data, our methodology is implemented using a data augmentation scheme which allows for existing Bayesian backfitting algorithms to be used. We illustrate the practical implementation and advantages of our method via simulation studies and an analysis of a prostate cancer surgery study where dependence on the experience and skill level of the physicians leads to clustering of survival times. We conclude by discussing our method's applicability in studies involving high dimensional data with complex underlying associations.

</details>

<details>

<summary>2021-03-12 20:35:33 - Hamiltonian Monte Carlo in Inverse Problems; Ill-Conditioning and Multi-Modality</summary>

- *Ian Langmore, Michael Dikovsky, Scott Geraedts, Peter Norgaard, Rob von Behren*

- `2103.07515v1` - [abs](http://arxiv.org/abs/2103.07515v1) - [pdf](http://arxiv.org/pdf/2103.07515v1)

> The Hamiltonian Monte Carlo (HMC) method allows sampling from continuous densities. Favorable scaling with dimension has led to wide adoption of HMC by the statistics community. Modern auto-differentiating software should allow more widespread usage in Bayesian inverse problems. This paper analyzes the two major difficulties encountered using HMC for inverse problems: poor conditioning and multi-modality. Novel results on preconditioning and replica exchange Monte Carlo parameter selection are presented in the context of spectroscopy. Recommendations are analyzed rigorously in the Gaussian case, and shown to generalize in a fusion plasma reconstruction.

</details>

<details>

<summary>2021-03-13 19:10:17 - Neuromodulated Neural Architectures with Local Error Signals for Memory-Constrained Online Continual Learning</summary>

- *Sandeep Madireddy, Angel Yanguas-Gil, Prasanna Balaprakash*

- `2007.08159v2` - [abs](http://arxiv.org/abs/2007.08159v2) - [pdf](http://arxiv.org/pdf/2007.08159v2)

> The ability to learn continuously from an incoming data stream without catastrophic forgetting is critical for designing intelligent systems. Many existing approaches to continual learning rely on stochastic gradient descent and its variants. However, these algorithms have to implement various strategies, such as memory buffers or replay, to overcome well-known shortcomings of stochastic gradient descent methods in terms of stability, greed, and short-term memory.   To that end, we develop a biologically-inspired light weight neural network architecture that incorporates local learning and neuromodulation to enable input processing over data streams and online learning. Next, we address the challenge of hyperparameter selection for tasks that are not known in advance by implementing transfer metalearning: using a Bayesian optimization to explore a design space spanning multiple local learning rules and their hyperparameters, we identify high performing configurations in classical single task online learning and we transfer them to continual learning tasks with task-similarity considerations.   We demonstrate the efficacy of our approach on both single task and continual learning setting. For the single task learning setting, we demonstrate superior performance over other local learning approaches on the MNIST, Fashion MNIST, and CIFAR-10 datasets. Using high performing configurations metalearned in the single task learning setting, we achieve superior continual learning performance on Split-MNIST, and Split-CIFAR-10 data as compared with other memory-constrained learning approaches, and match that of the state-of-the-art memory-intensive replay-based approaches.

</details>

<details>

<summary>2021-03-13 19:23:40 - Problem-fluent models for complex decision-making in autonomous materials research</summary>

- *Soojung Baek, Kristofer G. Reyes*

- `2103.07776v1` - [abs](http://arxiv.org/abs/2103.07776v1) - [pdf](http://arxiv.org/pdf/2103.07776v1)

> We review our recent work in the area of autonomous materials research, highlighting the coupling of machine learning methods and models and more problem-aware modeling. We review the general Bayesian framework for closed-loop design employed by many autonomous materials platforms. We then provide examples of our work on such platforms. We finally review our approaches to extend current statistical and ML models to better reflect problem-specific structure including the use of physics-based models and incorporation of operational considerations into the decision-making procedure.

</details>

<details>

<summary>2021-03-14 04:08:50 - Review on Ranking and Selection: A New Perspective</summary>

- *L. Jeff Hong, Weiwei Fan, Jun Luo*

- `2008.00249v3` - [abs](http://arxiv.org/abs/2008.00249v3) - [pdf](http://arxiv.org/pdf/2008.00249v3)

> In this paper, we briefly review the development of ranking-and-selection (R&S) in the past 70 years, especially the theoretical achievements and practical applications in the last 20 years. Different from the frequentist and Bayesian classifications adopted by Kim and Nelson (2006b) and Chick (2006) in their review articles, we categorize existing R&S procedures into fixed-precision and fixed-budget procedures, as in Hunter and Nelson (2017). We show that these two categories of procedures essentially differ in the underlying methodological formulations, i.e., they are built on hypothesis testing and dynamic-programming, respectively. In light of this variation, we review in detail some well-known procedures in the literature and show how they fit into these two formulations. In addition, we discuss the use of R&S procedures in solving various practical problems and propose what we think are the important research questions in the field.

</details>

<details>

<summary>2021-03-14 08:52:08 - Bayesian nonparametric estimation in the current status continuous mark model</summary>

- *Geurt Jongbloed, Frank van der Meulen, Lixue Pang*

- `1911.10387v2` - [abs](http://arxiv.org/abs/1911.10387v2) - [pdf](http://arxiv.org/pdf/1911.10387v2)

> In this paper we consider the current status continuous mark model where, if the event takes place before an inspection time $T$ a "continuous mark" variable is observed as well.   A Bayesian nonparametric method is introduced for estimating the distribution function of the joint distribution of the event time ($X$) and mark ($Y$). We consider a prior that is obtained by assigning a distribution on heights of cells, where cells are obtained from a partition of the support of the density of $(X, Y)$. As distribution on cell heights, we consider both a Dirichlet prior and a prior based on the graph-Laplacian on the specified partition. Our main result shows that under appropriate conditions, the posterior distribution function contracts pointwisely at rate $\left(n/\log n\right)^{-\frac{\rho}{3(\rho+2)}}$, where $\rho$ is the H\"older smoothness of the true density. In addition to our theoretical results, we provide computational methods for drawing from the posterior using probabilistic programming. The performance of our computational methods is illustrated in two examples.

</details>

<details>

<summary>2021-03-14 14:24:39 - Bayesian Temporal Factorization for Multidimensional Time Series Prediction</summary>

- *Xinyu Chen, Lijun Sun*

- `1910.06366v2` - [abs](http://arxiv.org/abs/1910.06366v2) - [pdf](http://arxiv.org/pdf/1910.06366v2)

> Large-scale and multidimensional spatiotemporal data sets are becoming ubiquitous in many real-world applications such as monitoring urban traffic and air quality. Making predictions on these time series has become a critical challenge due to not only the large-scale and high-dimensional nature but also the considerable amount of missing data. In this paper, we propose a Bayesian temporal factorization (BTF) framework for modeling multidimensional time series -- in particular spatiotemporal data -- in the presence of missing values. By integrating low-rank matrix/tensor factorization and vector autoregressive (VAR) process into a single probabilistic graphical model, this framework can characterize both global and local consistencies in large-scale time series data. The graphical model allows us to effectively perform probabilistic predictions and produce uncertainty estimates without imputing those missing values. We develop efficient Gibbs sampling algorithms for model inference and model updating for real-time prediction and test the proposed BTF framework on several real-world spatiotemporal data sets for both missing data imputation and multi-step rolling prediction tasks. The numerical experiments demonstrate the superiority of the proposed BTF approaches over existing state-of-the-art methods.

</details>

<details>

<summary>2021-03-14 20:28:51 - A Scalable Gradient-Free Method for Bayesian Experimental Design with Implicit Models</summary>

- *Jiaxin Zhang, Sirui Bi, Guannan Zhang*

- `2103.08026v1` - [abs](http://arxiv.org/abs/2103.08026v1) - [pdf](http://arxiv.org/pdf/2103.08026v1)

> Bayesian experimental design (BED) is to answer the question that how to choose designs that maximize the information gathering. For implicit models, where the likelihood is intractable but sampling is possible, conventional BED methods have difficulties in efficiently estimating the posterior distribution and maximizing the mutual information (MI) between data and parameters. Recent work proposed the use of gradient ascent to maximize a lower bound on MI to deal with these issues. However, the approach requires a sampling path to compute the pathwise gradient of the MI lower bound with respect to the design variables, and such a pathwise gradient is usually inaccessible for implicit models. In this paper, we propose a novel approach that leverages recent advances in stochastic approximate gradient ascent incorporated with a smoothed variational MI estimator for efficient and robust BED. Without the necessity of pathwise gradients, our approach allows the design process to be achieved through a unified procedure with an approximate gradient for implicit models. Several experiments show that our approach outperforms baseline methods, and significantly improves the scalability of BED in high-dimensional problems.

</details>

<details>

<summary>2021-03-14 21:10:03 - A Hybrid Gradient Method to Designing Bayesian Experiments for Implicit Models</summary>

- *Jiaxin Zhang, Sirui Bi, Guannan Zhang*

- `2103.08594v1` - [abs](http://arxiv.org/abs/2103.08594v1) - [pdf](http://arxiv.org/pdf/2103.08594v1)

> Bayesian experimental design (BED) aims at designing an experiment to maximize the information gathering from the collected data. The optimal design is usually achieved by maximizing the mutual information (MI) between the data and the model parameters. When the analytical expression of the MI is unavailable, e.g., having implicit models with intractable data distributions, a neural network-based lower bound of the MI was recently proposed and a gradient ascent method was used to maximize the lower bound. However, the approach in Kleinegesse et al., 2020 requires a pathwise sampling path to compute the gradient of the MI lower bound with respect to the design variables, and such a pathwise sampling path is usually inaccessible for implicit models. In this work, we propose a hybrid gradient approach that leverages recent advances in variational MI estimator and evolution strategies (ES) combined with black-box stochastic gradient ascent (SGA) to maximize the MI lower bound. This allows the design process to be achieved through a unified scalable procedure for implicit models without sampling path gradients. Several experiments demonstrate that our approach significantly improves the scalability of BED for implicit models in high-dimensional design space.

</details>

<details>

<summary>2021-03-15 00:12:39 - Learning Manifold Implicitly via Explicit Heat-Kernel Learning</summary>

- *Yufan Zhou, Changyou Chen, Jinhui Xu*

- `2010.01761v3` - [abs](http://arxiv.org/abs/2010.01761v3) - [pdf](http://arxiv.org/pdf/2010.01761v3)

> Manifold learning is a fundamental problem in machine learning with numerous applications. Most of the existing methods directly learn the low-dimensional embedding of the data in some high-dimensional space, and usually lack the flexibility of being directly applicable to down-stream applications. In this paper, we propose the concept of implicit manifold learning, where manifold information is implicitly obtained by learning the associated heat kernel. A heat kernel is the solution of the corresponding heat equation, which describes how "heat" transfers on the manifold, thus containing ample geometric information of the manifold. We provide both practical algorithm and theoretical analysis of our framework. The learned heat kernel can be applied to various kernel-based machine learning models, including deep generative models (DGM) for data generation and Stein Variational Gradient Descent for Bayesian inference. Extensive experiments show that our framework can achieve state-of-the-art results compared to existing methods for the two tasks.

</details>

<details>

<summary>2021-03-15 04:17:42 - Mixed Hamiltonian Monte Carlo for Mixed Discrete and Continuous Variables</summary>

- *Guangyao Zhou*

- `1909.04852v6` - [abs](http://arxiv.org/abs/1909.04852v6) - [pdf](http://arxiv.org/pdf/1909.04852v6)

> Hamiltonian Monte Carlo (HMC) has emerged as a powerful Markov Chain Monte Carlo (MCMC) method to sample from complex continuous distributions. However, a fundamental limitation of HMC is that it can not be applied to distributions with mixed discrete and continuous variables. In this paper, we propose mixed HMC (M-HMC) as a general framework to address this limitation. M-HMC is a novel family of MCMC algorithms that evolves the discrete and continuous variables in tandem, allowing more frequent updates of discrete variables while maintaining HMC's ability to suppress random-walk behavior. We establish M-HMC's theoretical properties, and present an efficient implementation with Laplace momentum that introduces minimal overhead compared to existing HMC methods. The superior performances of M-HMC over existing methods are demonstrated with numerical experiments on Gaussian mixture models (GMMs), variable selection in Bayesian logistic regression (BLR), and correlated topic models (CTMs).

</details>

<details>

<summary>2021-03-15 08:07:58 - Bayesian Model Averaging for Causality Estimation and its Approximation based on Gaussian Scale Mixture Distributions</summary>

- *Shunsuke Horii*

- `2103.08195v1` - [abs](http://arxiv.org/abs/2103.08195v1) - [pdf](http://arxiv.org/pdf/2103.08195v1)

> In the estimation of the causal effect under linear Structural Causal Models (SCMs), it is common practice to first identify the causal structure, estimate the probability distributions, and then calculate the causal effect. However, if the goal is to estimate the causal effect, it is not necessary to fix a single causal structure or probability distributions. In this paper, we first show from a Bayesian perspective that it is Bayes optimal to weight (average) the causal effects estimated under each model rather than estimating the causal effect under a fixed single model. This idea is also known as Bayesian model averaging. Although the Bayesian model averaging is optimal, as the number of candidate models increases, the weighting calculations become computationally hard. We develop an approximation to the Bayes optimal estimator by using Gaussian scale mixture distributions.

</details>

<details>

<summary>2021-03-15 08:09:32 - The $f$-Divergence Expectation Iteration Scheme</summary>

- *Kamélia Daudel, Randal Douc, François Portier, François Roueff*

- `1909.12239v2` - [abs](http://arxiv.org/abs/1909.12239v2) - [pdf](http://arxiv.org/pdf/1909.12239v2)

> This paper introduces the $f$-EI$(\phi)$ algorithm, a novel iterative algorithm which operates on measures and performs $f$-divergence minimisation in a Bayesian framework. We prove that for a rich family of values of $(f,\phi)$ this algorithm leads at each step to a systematic decrease in the $f$-divergence and show that we achieve an optimum. In the particular case where we consider a weighted sum of Dirac measures and the $\alpha$-divergence, we obtain that the calculations involved in the $f$-EI$(\phi)$ algorithm simplify to gradient-based computations. Empirical results support the claim that the $f$-EI$(\phi)$ algorithm serves as a powerful tool to assist Variational methods.

</details>

<details>

<summary>2021-03-15 15:59:08 - Efficient Variational Bayesian Structure Learning of Dynamic Graphical Models</summary>

- *Hang Yu, Songwei Wu, Justin Dauwels*

- `2009.07703v2` - [abs](http://arxiv.org/abs/2009.07703v2) - [pdf](http://arxiv.org/pdf/2009.07703v2)

> Estimating time-varying graphical models are of paramount importance in various social, financial, biological, and engineering systems, since the evolution of such networks can be utilized for example to spot trends, detect anomalies, predict vulnerability, and evaluate the impact of interventions. Existing methods require extensive tuning of parameters that control the graph sparsity and temporal smoothness. Furthermore, these methods are computationally burdensome with time complexity O(NP^3) for P variables and N time points. As a remedy, we propose a low-complexity tuning-free Bayesian approach, named BADGE. Specifically, we impose temporally-dependent spike-and-slab priors on the graphs such that they are sparse and varying smoothly across time. A variational inference algorithm is then derived to learn the graph structures from the data automatically. Owning to the pseudo-likelihood and the mean-field approximation, the time complexity of BADGE is only O(NP^2). Additionally, by identifying the frequency-domain resemblance to the time-varying graphical models, we show that BADGE can be extended to learning frequency-varying inverse spectral density matrices, and yields graphical models for multivariate stationary time series. Numerical results on both synthetic and real data show that that BADGE can better recover the underlying true graphs, while being more efficient than the existing methods, especially for high-dimensional cases.

</details>

<details>

<summary>2021-03-15 16:21:55 - Active Bayesian Assessment for Black-Box Classifiers</summary>

- *Disi Ji, Robert L. Logan IV, Padhraic Smyth, Mark Steyvers*

- `2002.06532v3` - [abs](http://arxiv.org/abs/2002.06532v3) - [pdf](http://arxiv.org/pdf/2002.06532v3)

> Recent advances in machine learning have led to increased deployment of black-box classifiers across a wide variety of applications. In many such situations there is a critical need to both reliably assess the performance of these pre-trained models and to perform this assessment in a label-efficient manner (given that labels may be scarce and costly to collect). In this paper, we introduce an active Bayesian approach for assessment of classifier performance to satisfy the desiderata of both reliability and label-efficiency. We begin by developing inference strategies to quantify uncertainty for common assessment metrics such as accuracy, misclassification cost, and calibration error. We then propose a general framework for active Bayesian assessment using inferred uncertainty to guide efficient selection of instances for labeling, enabling better performance assessment with fewer labels. We demonstrate significant gains from our proposed active Bayesian approach via a series of systematic empirical experiments assessing the performance of modern neural classifiers (e.g., ResNet and BERT) on several standard image and text classification datasets.

</details>

<details>

<summary>2021-03-15 17:11:08 - Variational State-Space Models for Localisation and Dense 3D Mapping in 6 DoF</summary>

- *Atanas Mirchev, Baris Kayalibay, Patrick van der Smagt, Justin Bayer*

- `2006.10178v3` - [abs](http://arxiv.org/abs/2006.10178v3) - [pdf](http://arxiv.org/pdf/2006.10178v3)

> We solve the problem of 6-DoF localisation and 3D dense reconstruction in spatial environments as approximate Bayesian inference in a deep state-space model. Our approach leverages both learning and domain knowledge from multiple-view geometry and rigid-body dynamics. This results in an expressive predictive model of the world, often missing in current state-of-the-art visual SLAM solutions. The combination of variational inference, neural networks and a differentiable raycaster ensures that our model is amenable to end-to-end gradient-based optimisation. We evaluate our approach on realistic unmanned aerial vehicle flight data, nearing the performance of state-of-the-art visual-inertial odometry systems. We demonstrate the applicability of the model to generative prediction and planning.

</details>

<details>

<summary>2021-03-15 18:28:05 - Estimation of parameters of the Gumbel type-II distribution under AT-II PHCS with an application of Covid-19 data</summary>

- *Subhankar Dutta, Suchandan Kayal*

- `2103.08641v1` - [abs](http://arxiv.org/abs/2103.08641v1) - [pdf](http://arxiv.org/pdf/2103.08641v1)

> In this paper, we investigate the classical and Bayesian estimation of unknown parameters of the Gumbel type-II distribution based on adaptive type-II progressive hybrid censored sample (AT-II PHCS). The maximum likelihood estimates (MLEs) and maximum product spacing estimates (MPSEs) are developed and computed numerically using Newton-Raphson method. Bayesian approaches are employed to estimate parameters under symmetric and asymmetric loss functions. Bayesian estimates are not in explicit forms. Thus, Bayesian estimates are obtained by using Markov chain Monte Carlo (MCMC) method along with the Metropolis-Hastings (MH) algorithm. Based on the normality property of MLEs the asymptotic confidence intervals are constructed. Also, bootstrap intervals and highest posterior density (HPD) credible intervals are constructed. Further a Monte Carlo simulation study is carried out. Finally, the data set based on the death rate due to Covid-19 in India is analyzed for illustration of the purpose.

</details>

<details>

<summary>2021-03-15 22:54:17 - Incorporating External Data into the Analysis of Clinical Trials via Bayesian Additive Regression Trees</summary>

- *Tianjian Zhou, Yuan Ji*

- `2103.08754v1` - [abs](http://arxiv.org/abs/2103.08754v1) - [pdf](http://arxiv.org/pdf/2103.08754v1)

> Most clinical trials involve the comparison of a new treatment to a control arm (e.g., the standard of care) and the estimation of a treatment effect. External data, including historical clinical trial data and real-world observational data, are commonly available for the control arm. Borrowing information from external data holds the promise of improving the estimation of relevant parameters and increasing the power of detecting a treatment effect if it exists. In this paper, we propose to use Bayesian additive regression trees (BART) for incorporating external data into the analysis of clinical trials, with a specific goal of estimating the conditional or population average treatment effect. BART naturally adjusts for patient-level covariates and captures potentially heterogeneous treatment effects across different data sources, achieving flexible borrowing. Simulation studies demonstrate that BART compares favorably to a hierarchical linear model and a normal-normal hierarchical model. We illustrate the proposed method with an acupuncture trial.

</details>

<details>

<summary>2021-03-16 12:28:26 - Gradient-Based Markov Chain Monte Carlo for Bayesian Inference With Non-Differentiable Priors</summary>

- *Jacob Vorstrup Goldman, Torben Sell, Sumeetpal Sidhu Singh*

- `2103.09017v1` - [abs](http://arxiv.org/abs/2103.09017v1) - [pdf](http://arxiv.org/pdf/2103.09017v1)

> The use of non-differentiable priors in Bayesian statistics has become increasingly popular, in particular in Bayesian imaging analysis. Current state of the art methods are approximate in the sense that they replace the posterior with a smooth approximation via Moreau-Yosida envelopes, and apply gradient-based discretized diffusions to sample from the resulting distribution. We characterize the error of the Moreau-Yosida approximation and propose a novel implementation using underdamped Langevin dynamics. In misson-critical cases, however, replacing the posterior with an approximation may not be a viable option. Instead, we show that Piecewise-Deterministic Markov Processes (PDMP) can be utilized for exact posterior inference from distributions satisfying almost everywhere differentiability. Furthermore, in contrast with diffusion-based methods, the suggested PDMP-based samplers place no assumptions on the prior shape, nor require access to a computationally cheap proximal operator, and consequently have a much broader scope of application. Through detailed numerical examples, including a non-differentiable circular distribution and a non-convex genomics model, we elucidate the relative strengths of these sampling methods on problems of moderate to high dimensions, underlining the benefits of PDMP-based methods when accurate sampling is decisive.

</details>

<details>

<summary>2021-03-16 20:26:34 - Optimal stratification of survival data via Bayesian nonparametric mixtures</summary>

- *Riccardo Corradin, Luis Enrique Nieto-Barajas, Bernardo Nipoti*

- `2103.09305v1` - [abs](http://arxiv.org/abs/2103.09305v1) - [pdf](http://arxiv.org/pdf/2103.09305v1)

> The stratified proportional hazards model represents a simple solution to account for heterogeneity within the data while keeping the multiplicative effect on the hazard function. Strata are typically defined a priori by resorting to the values taken by a categorical covariate. A general framework is proposed, which allows for the stratification of a generic accelerated life time model, including as a special case the Weibull proportional hazard model. The stratification is determined a posteriori by taking into account that strata might be characterized by different baseline survivals as well as different effects of the predictors. This is achieved by considering a Bayesian nonparametric mixture model and the posterior distribution it induces on the space of data partitions. The optimal stratification is then identified by means of the variation of information criterion and, in turn, stratum-specific inference is carried out. The performance of the proposed method and its robustness to the presence of right-censored observations are investigated by means of an extensive simulation study. A further illustration is provided by the analysis of a data set extracted from the University of Massachusetts AIDS Research Unit IMPACT Study.

</details>

<details>

<summary>2021-03-16 23:57:46 - Consistency of Empirical Bayes And Kernel Flow For Hierarchical Parameter Estimation</summary>

- *Yifan Chen, Houman Owhadi, Andrew M. Stuart*

- `2005.11375v2` - [abs](http://arxiv.org/abs/2005.11375v2) - [pdf](http://arxiv.org/pdf/2005.11375v2)

> Gaussian process regression has proven very powerful in statistics, machine learning and inverse problems. A crucial aspect of the success of this methodology, in a wide range of applications to complex and real-world problems, is hierarchical modeling and learning of hyperparameters. The purpose of this paper is to study two paradigms of learning hierarchical parameters: one is from the probabilistic Bayesian perspective, in particular, the empirical Bayes approach that has been largely used in Bayesian statistics; the other is from the deterministic and approximation theoretic view, and in particular the kernel flow algorithm that was proposed recently in the machine learning literature. Analysis of their consistency in the large data limit, as well as explicit identification of their implicit bias in parameter learning, are established in this paper for a Mat\'ern-like model on the torus. A particular technical challenge we overcome is the learning of the regularity parameter in the Mat\'ern-like field, for which consistency results have been very scarce in the spatial statistics literature. Moreover, we conduct extensive numerical experiments beyond the Mat\'ern-like model, comparing the two algorithms further. These experiments demonstrate learning of other hierarchical parameters, such as amplitude and lengthscale; they also illustrate the setting of model misspecification in which the kernel flow approach could show superior performance to the more traditional empirical Bayes approach.

</details>

<details>

<summary>2021-03-17 13:17:42 - Deriving Priors for Bayesian Prediction of Daily Response Propensity in Responsive Survey Design: Historical Data Analysis vs. Literature Review</summary>

- *Brady T. West, James Wagner, Stephanie Coffey, Michael R. Elliott*

- `1907.06560v4` - [abs](http://arxiv.org/abs/1907.06560v4) - [pdf](http://arxiv.org/pdf/1907.06560v4)

> Responsive Survey Design (RSD) aims to increase the efficiency of survey data collection via live monitoring of paradata and the introduction of protocol changes when survey errors and increased costs seem imminent. Daily predictions of response propensity for all active sampled cases are among the most important quantities for live monitoring of data collection outcomes, making sound predictions of these propensities essential for the success of RSD. Because it relies on real-time updates of prior beliefs about key design quantities, such as predicted response propensities, RSD stands to benefit from Bayesian approaches. However, empirical evidence of the merits of these approaches is lacking in the literature, and the derivation of informative prior distributions is required for these approaches to be effective. In this paper, we evaluate the ability of two approaches to deriving prior distributions for the coefficients defining daily response propensity models to improve predictions of daily response propensity in a real data collection employing RSD. The first approach involves analyses of historical data from the same survey, and the second approach involves literature review. We find that Bayesian methods based on these two approaches result in higher-quality predictions of response propensity than more standard approaches ignoring prior information. This is especially true during the early-to-middle periods of data collection when interventions are often considered in an RSD framework.

</details>

<details>

<summary>2021-03-17 16:42:47 - Goal-oriented adaptive sampling under random field modelling of response probability distributions</summary>

- *Athénaïs Gautier, David Ginsbourger, Guillaume Pirot*

- `2102.07612v2` - [abs](http://arxiv.org/abs/2102.07612v2) - [pdf](http://arxiv.org/pdf/2102.07612v2)

> In the study of natural and artificial complex systems, responses that are not completely determined by the considered decision variables are commonly modelled probabilistically, resulting in response distributions varying across decision space. We consider cases where the spatial variation of these response distributions does not only concern their mean and/or variance but also other features including for instance shape or uni-modality versus multi-modality. Our contributions build upon a non-parametric Bayesian approach to modelling the thereby induced fields of probability distributions, and in particular to a spatial extension of the logistic Gaussian model.   The considered models deliver probabilistic predictions of response distributions at candidate points, allowing for instance to perform (approximate) posterior simulations of probability density functions, to jointly predict multiple moments and other functionals of target distributions, as well as to quantify the impact of collecting new samples on the state of knowledge of the distribution field of interest. In particular, we introduce adaptive sampling strategies leveraging the potential of the considered random distribution field models to guide system evaluations in a goal-oriented way, with a view towards parsimoniously addressing calibration and related problems from non-linear (stochastic) inversion and global optimisation.

</details>

<details>

<summary>2021-03-17 16:52:01 - Mind the Gap when Conditioning Amortised Inference in Sequential Latent-Variable Models</summary>

- *Justin Bayer, Maximilian Soelch, Atanas Mirchev, Baris Kayalibay, Patrick van der Smagt*

- `2101.07046v2` - [abs](http://arxiv.org/abs/2101.07046v2) - [pdf](http://arxiv.org/pdf/2101.07046v2)

> Amortised inference enables scalable learning of sequential latent-variable models (LVMs) with the evidence lower bound (ELBO). In this setting, variational posteriors are often only partially conditioned. While the true posteriors depend, e.g., on the entire sequence of observations, approximate posteriors are only informed by past observations. This mimics the Bayesian filter -- a mixture of smoothing posteriors. Yet, we show that the ELBO objective forces partially-conditioned amortised posteriors to approximate products of smoothing posteriors instead. Consequently, the learned generative model is compromised. We demonstrate these theoretical findings in three scenarios: traffic flow, handwritten digits, and aerial vehicle dynamics. Using fully-conditioned approximate posteriors, performance improves in terms of generative modelling and multi-step prediction.

</details>

<details>

<summary>2021-03-17 17:43:38 - Bayesian Estimation of Attribute Disclosure Risks in Synthetic Data with the $\texttt{AttributeRiskCalculation}$ R Package</summary>

- *Ryan Hornby, Jingchen Hu*

- `2103.09805v1` - [abs](http://arxiv.org/abs/2103.09805v1) - [pdf](http://arxiv.org/pdf/2103.09805v1)

> Synthetic data is a promising approach to privacy protection in many contexts. A Bayesian synthesis model, also known as a synthesizer, simulates synthetic values of sensitive variables from their posterior predictive distributions. The resulting synthetic data can then be released in place of the confidential data. An important evaluation prior to synthetic data release is its level of privacy protection, which is often in the form of disclosure risks evaluation. Attribute disclosure, referring to an intruder correctly inferring the confidential values of synthetic records, is one type of disclosure that is challenging to be computationally evaluated. In this paper, we review and discuss in detail some Bayesian estimation approaches to attribute disclosure risks evaluation, with examples of commonly-used Bayesian synthesizers. We create the $\texttt{AttributeRiskCalculation}$ R package to facilitate its implementation, and demonstrate its functionality with examples of evaluating attribute disclosure risks in synthetic samples of the Consumer Expenditure Surveys.

</details>

<details>

<summary>2021-03-18 04:00:48 - Economic variable selection</summary>

- *Steven N. MacEachern, Koji Miyawaki*

- `1903.02136v4` - [abs](http://arxiv.org/abs/1903.02136v4) - [pdf](http://arxiv.org/pdf/1903.02136v4)

> Regression plays a key role in many research areas and its variable selection is a classic and major problem. This study emphasizes cost of predictors to be purchased for future use, when we select a subset of them. Its economic aspect is naturally formalized by the decision-theoretic approach. In addition, two Bayesian approaches are proposed to address uncertainty about model parameters and models: the restricted and extended approaches, which lead us to rethink about model averaging. From objective, rule-based, or robust Bayes point of view, the former is preferred. Proposed method is applied to three popular datasets for illustration.

</details>

<details>

<summary>2021-03-18 09:50:06 - Robust and Guided Bayesian Reconstruction of Single-Photon 3D Lidar Data: Application to Multispectral and Underwater Imaging</summary>

- *Abderrahim Halimi, Aurora Maccarone, Robert Lamb, Gerald S. Buller, Stephen McLaughlin*

- `2103.10122v1` - [abs](http://arxiv.org/abs/2103.10122v1) - [pdf](http://arxiv.org/pdf/2103.10122v1)

> 3D Lidar imaging can be a challenging modality when using multiple wavelengths, or when imaging in high noise environments (e.g., imaging through obscurants). This paper presents a hierarchical Bayesian algorithm for the robust reconstruction of multispectral single-photon Lidar data in such environments. The algorithm exploits multi-scale information to provide robust depth and reflectivity estimates together with their uncertainties to help with decision making. The proposed weight-based strategy allows the use of available guide information that can be obtained by using state-of-the-art learning based algorithms. The proposed Bayesian model and its estimation algorithm are validated on both synthetic and real images showing competitive results regarding the quality of the inferences and the computational complexity when compared to the state-of-the-art algorithms.

</details>

<details>

<summary>2021-03-18 11:26:59 - Getting a CLUE: A Method for Explaining Uncertainty Estimates</summary>

- *Javier Antorán, Umang Bhatt, Tameem Adel, Adrian Weller, José Miguel Hernández-Lobato*

- `2006.06848v2` - [abs](http://arxiv.org/abs/2006.06848v2) - [pdf](http://arxiv.org/pdf/2006.06848v2)

> Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty.

</details>

<details>

<summary>2021-03-18 11:34:08 - Bayesian Imaging With Data-Driven Priors Encoded by Neural Networks: Theory, Methods, and Algorithms</summary>

- *Matthew Holden, Marcelo Pereyra, Konstantinos C. Zygalakis*

- `2103.10182v1` - [abs](http://arxiv.org/abs/2103.10182v1) - [pdf](http://arxiv.org/pdf/2103.10182v1)

> This paper proposes a new methodology for performing Bayesian inference in imaging inverse problems where the prior knowledge is available in the form of training data. Following the manifold hypothesis and adopting a generative modelling approach, we construct a data-driven prior that is supported on a sub-manifold of the ambient space, which we can learn from the training data by using a variational autoencoder or a generative adversarial network. We establish the existence and well-posedness of the associated posterior distribution and posterior moments under easily verifiable conditions, providing a rigorous underpinning for Bayesian estimators and uncertainty quantification analyses. Bayesian computation is performed by using a parallel tempered version of the preconditioned Crank-Nicolson algorithm on the manifold, which is shown to be ergodic and robust to the non-convex nature of these data-driven models. In addition to point estimators and uncertainty quantification analyses, we derive a model misspecification test to automatically detect situations where the data-driven prior is unreliable, and explain how to identify the dimension of the latent space directly from the training data. The proposed approach is illustrated with a range of experiments with the MNIST dataset, where it outperforms alternative image reconstruction approaches from the state of the art. A model accuracy analysis suggests that the Bayesian probabilities reported by the data-driven models are also remarkably accurate under a frequentist definition of probability.

</details>

<details>

<summary>2021-03-18 13:01:58 - Identifying the Recurrence of Sleep Apnea Using a Harmonic Hidden Markov Model</summary>

- *Beniamino Hadj-Amar, Bärbel Finkenstädt, Mark Fiecas, Robert Huckstepp*

- `2001.01676v4` - [abs](http://arxiv.org/abs/2001.01676v4) - [pdf](http://arxiv.org/pdf/2001.01676v4)

> We propose to model time-varying periodic and oscillatory processes by means of a hidden Markov model where the states are defined through the spectral properties of a periodic regime. The number of states is unknown along with the relevant periodicities, the role and number of which may vary across states. We address this inference problem by a Bayesian nonparametric hidden Markov model assuming a sticky hierarchical Dirichlet process for the switching dynamics between different states while the periodicities characterizing each state are explored by means of a trans-dimensional Markov chain Monte Carlo sampling step. We develop the full Bayesian inference algorithm and illustrate the use of our proposed methodology for different simulation studies as well as an application related to respiratory research which focuses on the detection of apnea instances in human breathing traces.

</details>

<details>

<summary>2021-03-18 15:18:26 - Ordinal Probit Functional Outcome Regression with Application to Computer-Use Behavior in Rhesus Monkeys</summary>

- *Mark J. Meyer, Jeffrey S. Morris, Regina Paxton Gazes, Brent A. Coull*

- `1901.07976v2` - [abs](http://arxiv.org/abs/1901.07976v2) - [pdf](http://arxiv.org/pdf/1901.07976v2)

> Research in functional regression has made great strides in expanding to non-Gaussian functional outcomes, but exploration of ordinal functional outcomes remains limited. Motivated by a study of computer-use behavior in rhesus macaques (Macaca mulatta), we introduce the Ordinal Probit Functional Outcome Regression model (OPFOR). OPFOR models can be fit using one of several basis functions including penalized B-splines, wavelets, and O'Sullivan splines -- the last of which typically performs best. Simulation using a variety of underlying covariance patterns shows that the model performs reasonably well in estimation under multiple basis functions with near nominal coverage for joint credible intervals. Finally, in application, we use Bayesian model selection criteria adapted to functional outcome regression to best characterize the relation between several demographic factors of interest and the monkeys' computer use over the course of a year. In comparison with a standard ordinal longitudinal analysis, OPFOR outperforms a cumulative-link mixed-effects model in simulation and provides additional and more nuanced information on the nature of the monkeys' computer-use behavior.

</details>

<details>

<summary>2021-03-18 20:53:01 - Workflow Techniques for the Robust Use of Bayes Factors</summary>

- *Daniel J. Schad, Bruno Nicenboim, Paul-Christian Bürkner, Michael Betancourt, Shravan Vasishth*

- `2103.08744v2` - [abs](http://arxiv.org/abs/2103.08744v2) - [pdf](http://arxiv.org/pdf/2103.08744v2)

> Inferences about hypotheses are ubiquitous in the cognitive sciences. Bayes factors provide one general way to compare different hypotheses by their compatibility with the observed data. Those quantifications can then also be used to choose between hypotheses. While Bayes factors provide an immediate approach to hypothesis testing, they are highly sensitive to details of the data/model assumptions. Moreover it's not clear how straightforwardly this approach can be implemented in practice, and in particular how sensitive it is to the details of the computational implementation. Here, we investigate these questions for Bayes factor analyses in the cognitive sciences. We explain the statistics underlying Bayes factors as a tool for Bayesian inferences and discuss that utility functions are needed for principled decisions on hypotheses. Next, we study how Bayes factors misbehave under different conditions. This includes a study of errors in the estimation of Bayes factors. Importantly, it is unknown whether Bayes factor estimates based on bridge sampling are unbiased for complex analyses. We are the first to use simulation-based calibration as a tool to test the accuracy of Bayes factor estimates. Moreover, we study how stable Bayes factors are against different MCMC draws. We moreover study how Bayes factors depend on variation in the data. We also look at variability of decisions based on Bayes factors and how to optimize decisions using a utility function. We outline a Bayes factor workflow that researchers can use to study whether Bayes factors are robust for their individual analysis, and we illustrate this workflow using an example from the cognitive sciences. We hope that this study will provide a workflow to test the strengths and limitations of Bayes factors as a way to quantify evidence in support of scientific hypotheses. Reproducible code is available from https://osf.io/y354c/.

</details>

<details>

<summary>2021-03-18 22:25:55 - Inductive Inference in Supervised Classification</summary>

- *Ali Amiryousefi*

- `2103.10549v1` - [abs](http://arxiv.org/abs/2103.10549v1) - [pdf](http://arxiv.org/pdf/2103.10549v1)

> Inductive inference in supervised classification context constitutes to methods and approaches to assign some objects or items into different predefined classes using a formal rule that is derived from training data and possibly some additional auxiliary information. The optimality of such an assignment varies under different conditions due to intrinsic attributes of the objects being considered for such a task. One of these cases is when all the objects' features are discrete variables with a priori known categories. As another example, one can consider a modification of this case with a priori unknown categories. These two cases are the main focus of this thesis and based on Bayesian inductive theories, de Finetti type exchangeability is a suitable assumption that facilitates the derivation of classifiers in the former scenario. On the contrary, this type of exchangeability is not applicable in the latter case, instead, it is possible to utilise the partition exchangeability due to John Kingman. These two types of exchangeabilities are discussed and furthermore here I investigate inductive supervised classifiers based on both types of exchangeabilities. I further demonstrate that the classifiers based on de Finetti type exchangeability can optimally handle test items independently of each other in the presence of infinite amounts of training data while on the other hand, classifiers based on partition exchangeability still continue to benefit from joint labelling of all the test items. Additionally, it is shown that the inductive learning process for the simultaneous classifier saturates when the amount of test data tends to infinity.

</details>

<details>

<summary>2021-03-18 22:31:54 - Horseshoe Prior Bayesian Quantile Regression</summary>

- *David Kohns, Tibor Szendrei*

- `2006.07655v2` - [abs](http://arxiv.org/abs/2006.07655v2) - [pdf](http://arxiv.org/pdf/2006.07655v2)

> This paper extends the horseshoe prior of Carvalho et al. (2010) to Bayesian quantile regression (HS-BQR) and provides a fast sampling algorithm for computation in high dimensions. The performance of the proposed HS-BQR is evaluated on Monte Carlo simulations and a high dimensional Growth-at-Risk (GaR) forecasting application for the U.S. The Monte Carlo design considers several sparsity and error structures. Compared to alternative shrinkage priors, the proposed HS-BQR yields better (or at worst similar) performance in coefficient bias and forecast error. The HS-BQR is particularly potent in sparse designs and in estimating extreme quantiles. As expected, the simulations also highlight that identifying quantile specific location and scale effects for individual regressors in dense DGPs requires substantial data. In the GaR application, we forecast tail risks as well as complete forecast densities using the McCracken and Ng (2020) database. Quantile specific and density calibration score functions show that the HS-BQR provides the best performance, especially at short and medium run horizons. The ability to produce well calibrated density forecasts and accurate downside risk measures in large data contexts makes the HS-BQR a promising tool for nowcasting applications and recession modelling.

</details>

<details>

<summary>2021-03-19 02:58:07 - Semiparametric Bayesian Inference for Local Extrema of Functions in the Presence of Noise</summary>

- *Meng Li, Zejian Liu, Cheng-Han Yu, Marina Vannucci*

- `2103.10606v1` - [abs](http://arxiv.org/abs/2103.10606v1) - [pdf](http://arxiv.org/pdf/2103.10606v1)

> There is a wide range of applications where the local extrema of a function are the key quantity of interest. However, there is surprisingly little work on methods to infer local extrema with uncertainty quantification in the presence of noise. By viewing the function as an infinite-dimensional nuisance parameter, a semiparametric formulation of this problem poses daunting challenges, both methodologically and theoretically, as (i) the number of local extrema may be unknown, and (ii) the induced shape constraints associated with local extrema are highly irregular. In this article, we address these challenges by suggesting an encompassing strategy that eliminates the need to specify the number of local extrema, which leads to a remarkably simple, fast semiparametric Bayesian approach for inference on local extrema. We provide closed-form characterization of the posterior distribution and study its large sample behaviors under this encompassing regime. We show a multi-modal Bernstein-von Mises phenomenon in which the posterior measure converges to a mixture of Gaussians with the number of components matching the underlying truth, leading to posterior exploration that accounts for multi-modality. We illustrate the method through simulations and a real data application to event-related potential analysis.

</details>

<details>

<summary>2021-03-19 09:12:12 - Generalization Bounds via Information Density and Conditional Information Density</summary>

- *Fredrik Hellström, Giuseppe Durisi*

- `2005.08044v5` - [abs](http://arxiv.org/abs/2005.08044v5) - [pdf](http://arxiv.org/pdf/2005.08044v5)

> We present a general approach, based on an exponential inequality, to derive bounds on the generalization error of randomized learning algorithms. Using this approach, we provide bounds on the average generalization error as well as bounds on its tail probability, for both the PAC-Bayesian and single-draw scenarios. Specifically, for the case of subgaussian loss functions, we obtain novel bounds that depend on the information density between the training data and the output hypothesis. When suitably weakened, these bounds recover many of the information-theoretic available bounds in the literature. We also extend the proposed exponential-inequality approach to the setting recently introduced by Steinke and Zakynthinou (2020), where the learning algorithm depends on a randomly selected subset of the available training data. For this setup, we present bounds for bounded loss functions in terms of the conditional information density between the output hypothesis and the random variable determining the subset choice, given all training data. Through our approach, we recover the average generalization bound presented by Steinke and Zakynthinou (2020) and extend it to the PAC-Bayesian and single-draw scenarios. For the single-draw scenario, we also obtain novel bounds in terms of the conditional $\alpha$-mutual information and the conditional maximal leakage.

</details>

<details>

<summary>2021-03-19 13:12:53 - Fast Bayesian Record Linkage With Record-Specific Disagreement Parameters</summary>

- *Thomas Stringham*

- `2003.04238v2` - [abs](http://arxiv.org/abs/2003.04238v2) - [pdf](http://arxiv.org/pdf/2003.04238v2)

> Researchers are often interested in linking individuals between two datasets that lack a common unique identifier. Matching procedures often struggle to match records with common names, birthplaces or other field values. Computational feasibility is also a challenge, particularly when linking large datasets. We develop a Bayesian method for automated probabilistic record linkage and show it recovers more than 50% more true matches, holding accuracy constant, than comparable methods in a matching of military recruitment data to the 1900 US Census for which expert-labelled matches are available. Our approach, which builds on a recent state-of-the-art Bayesian method, refines the modelling of comparison data, allowing disagreement probability parameters conditional on non-match status to be record-specific in the smaller of the two datasets. This flexibility significantly improves matching when many records share common field values. We show that our method is computationally feasible in practice, despite the added complexity, with an R/C++ implementation that achieves significant improvement in speed over comparable recent methods. We also suggest a lightweight method for treatment of very common names and show how to estimate true positive rate and positive predictive value when true match status is unavailable.

</details>

<details>

<summary>2021-03-19 17:04:27 - ROCnReg: An R Package for Receiver Operating Characteristic Curve Inference with and without Covariate Information</summary>

- *Maria Xose Rodriguez-Alvarez, Vanda Inacio*

- `2003.13111v5` - [abs](http://arxiv.org/abs/2003.13111v5) - [pdf](http://arxiv.org/pdf/2003.13111v5)

> The receiver operating characteristic (ROC) curve is the most popular tool used to evaluate the discriminatory capability of diagnostic tests/biomarkers measured on a continuous scale when distinguishing between two alternative disease states (e.g, diseased and nondiseased). In some circumstances, the test's performance and its discriminatory ability may vary according to subject-specific characteristics or different test settings. In such cases, information-specific accuracy measures, such as the covariate-specific and the covariate-adjusted ROC curve are needed, as ignoring covariate information may lead to biased or erroneous results. This paper introduces the R package ROCnReg that allows estimating the pooled (unadjusted) ROC curve, the covariate-specific ROC curve, and the covariate-adjusted ROC curve by different methods, both from (semi) parametric and nonparametric perspectives and within Bayesian and frequentist paradigms. From the estimated ROC curve (pooled, covariate-specific or covariate-adjusted), several summary measures of accuracy, such as the (partial) area under the ROC curve and the Youden index, can be obtained. The package also provides functions to obtain ROC-based optimal threshold values using several criteria, namely, the Youden Index criterion and the criterion that sets a target value for the false positive fraction. For the Bayesian methods, we provide tools for assessing model fit via posterior predictive checks, while model choice can be carried out via several information criteria. Numerical and graphical outputs are provided for all methods. The package is illustrated through the analyses of data from an endocrine study where the aim is to assess the capability of the body mass index to detect the presence or absence of cardiovascular disease risk factors. The package is available from CRAN at https://CRAN.R-project.org/package=ROCnReg.

</details>

<details>

<summary>2021-03-19 17:16:22 - Copula Averaging for Tail Dependence in Insurance Claims Data</summary>

- *Sen Hu, Adrian O'Hagan*

- `2103.10912v1` - [abs](http://arxiv.org/abs/2103.10912v1) - [pdf](http://arxiv.org/pdf/2103.10912v1)

> Analysing dependent risks is an important task for insurance companies. A dependency is reflected in the fact that information about one random variable provides information about the likely distribution of values of another random variable. Insurance companies in particular must investigate such dependencies between different lines of business and the effects that an extreme loss event, such as an earthquake or hurricane, has across multiple lines of business simultaneously. Copulas provide a popular model-based approach to analysing the dependency between risks, and the coefficient of tail dependence is a measure of dependence for extreme losses. Besides commonly used empirical estimators for estimating the tail dependence coefficient, copula fitting can lead to estimation of such coefficients directly or can verify their existence. Generally, a range of copula models is available to fit a data set well, leading to multiple different tail dependence results; a method based on Bayesian model averaging is designed to obtain a unified estimate of tail dependence. In this article, this model-based coefficient estimation method is illustrated through a variety of copula fitting approaches and results are presented for several simulated data sets and also a real general insurance loss data set.

</details>

<details>

<summary>2021-03-19 20:13:29 - PHASE: PHysically-grounded Abstract Social Events for Machine Social Perception</summary>

- *Aviv Netanyahu, Tianmin Shu, Boris Katz, Andrei Barbu, Joshua B. Tenenbaum*

- `2103.01933v2` - [abs](http://arxiv.org/abs/2103.01933v2) - [pdf](http://arxiv.org/pdf/2103.01933v2)

> The ability to perceive and reason about social interactions in the context of physical environments is core to human social intelligence and human-machine cooperation. However, no prior dataset or benchmark has systematically evaluated physically grounded perception of complex social interactions that go beyond short actions, such as high-fiving, or simple group activities, such as gathering. In this work, we create a dataset of physically-grounded abstract social events, PHASE, that resemble a wide range of real-life social interactions by including social concepts such as helping another agent. PHASE consists of 2D animations of pairs of agents moving in a continuous space generated procedurally using a physics engine and a hierarchical planner. Agents have a limited field of view, and can interact with multiple objects, in an environment that has multiple landmarks and obstacles. Using PHASE, we design a social recognition task and a social prediction task. PHASE is validated with human experiments demonstrating that humans perceive rich interactions in the social events, and that the simulated agents behave similarly to humans. As a baseline model, we introduce a Bayesian inverse planning approach, SIMPLE (SIMulation, Planning and Local Estimation), which outperforms state-of-the-art feed-forward neural networks. We hope that PHASE can serve as a difficult new challenge for developing new models that can recognize complex social interactions.

</details>

<details>

<summary>2021-03-19 22:11:18 - Risk, Agricultural Production, and Weather Index Insurance in Village India</summary>

- *Jeffrey D. Michler, Frederi G. Viens, Gerald E. Shively*

- `2103.11047v1` - [abs](http://arxiv.org/abs/2103.11047v1) - [pdf](http://arxiv.org/pdf/2103.11047v1)

> We investigate the sources of variability in agricultural production and their relative importance in the context of weather index insurance for smallholder farmers in India. Using parcel-level panel data, multilevel modeling, and Bayesian methods we measure how large a role seasonal variation in weather plays in explaining yield variance. Seasonal variation in weather accounts for 19-20 percent of total variance in crop yields. Motivated by this result, we derive pricing and payout schedules for actuarially fair index insurance. These calculations shed light on the low uptake rates of index insurance and provide direction for designing more suitable index insurance.

</details>

<details>

<summary>2021-03-20 00:57:38 - Uncertainty quantification for fault slip inversion</summary>

- *J. Cricelio Montesinos-López, Antonio Capella, J. Andrés Christen, Josué Tago*

- `2012.05298v2` - [abs](http://arxiv.org/abs/2012.05298v2) - [pdf](http://arxiv.org/pdf/2012.05298v2)

> We propose an efficient Bayesian approach to infer a fault displacement from geodetic data in a slow slip event. Our physical model of the slip process reduces to a multiple linear regression subject to constraints. Assuming a Gaussian model for the geodetic data and considering a multivariate truncated normal prior distribution for the unknown fault slip, the resulting posterior distribution is also multivariate truncated normal. Regarding the posterior, we propose an algorithm based on Optimal Directional Gibbs that allows us to efficiently sample from the resulting high-dimensional posterior distribution of along dip and along strike movements of our fault grid division. A synthetic fault slip example illustrates the flexibility and accuracy of the proposed approach. The methodology is also applied to a real data set, for the 2006 Guerrero, Mexico, Slow Slip Event, where the objective is to recover the fault slip on a known interface that produces displacements observed at ground geodetic stations. As a by-product of our approach, we are able to estimate moment magnitude for the 2006 Guerrero Event with uncertainty quantification.

</details>

<details>

<summary>2021-03-20 10:27:42 - Bayesian Point Estimation and Predictive Density Estimation for the Binomial Distribution with a Restricted Probability Parameter</summary>

- *Yasuyuki Hamura*

- `2103.00518v2` - [abs](http://arxiv.org/abs/2103.00518v2) - [pdf](http://arxiv.org/pdf/2103.00518v2)

> In this paper, we consider Bayesian point estimation and predictive density estimation in the binomial case. After presenting preliminary results on these problems, we compare the risk functions of the Bayes estimators based on the truncated and untruncated beta priors and obtain dominance conditions when the probability parameter is less than or equal to a known constant. The case where there are both a lower bound restriction and an upper bound restriction is also treated. Then our problems are shown to be related to similar problems in the Poisson case. Finally, numerical studies are presented.

</details>

<details>

<summary>2021-03-20 12:54:19 - Modeling Heterogeneity and Missing Data of Multiple Longitudinal Outcomes in Electronic Health Records</summary>

- *Rebecca Anthopolos, Ying Wei, Qixuan Chen*

- `2103.11170v1` - [abs](http://arxiv.org/abs/2103.11170v1) - [pdf](http://arxiv.org/pdf/2103.11170v1)

> In electronic health records (EHRs), latent subgroups of patients may exhibit distinctive patterning in their longitudinal health trajectories. For such data, growth mixture models (GMMs) enable classifying patients into different latent classes based on individual trajectories and hypothesized risk factors. However, the application of GMMs is hindered by the special missing data problem in EHRs, which manifests two patient-led missing data processes: the visit process and the response process for an EHR variable conditional on a patient visiting the clinic. If either process is associated with the process generating the longitudinal outcomes, then valid inferences require accounting for a nonignorable missing data mechanism. We propose a Bayesian shared parameter model that links GMMs of multiple longitudinal health outcomes, the visit process, and the response process of each outcome given a visit using a discrete latent class variable. Our focus is on multiple longitudinal health outcomes for which there can be a clinically prescribed visit schedule. We demonstrate our model in EHR measurements on early childhood weight and height z-scores. Using data simulations, we illustrate the statistical properties of our method with respect to subgroup-specific or marginal inferences. We built the R package EHRMiss for model fitting, selection, and checking.

</details>

<details>

<summary>2021-03-21 14:16:28 - Posterior distributions for Hierarchical Spike and Slab Indian Buffet processes</summary>

- *Lancelot F. James, Juho Lee, Abhinav Pandey*

- `2103.11407v1` - [abs](http://arxiv.org/abs/2103.11407v1) - [pdf](http://arxiv.org/pdf/2103.11407v1)

> Bayesian nonparametric hierarchical priors are highly effective in providing flexible models for latent data structures exhibiting sharing of information between and across groups. Most prominent is the Hierarchical Dirichlet Process (HDP), and its subsequent variants, which model latent clustering between and across groups. The HDP, may be viewed as a more flexible extension of Latent Dirichlet Allocation models (LDA), and has been applied to, for example, topic modelling, natural language processing, and datasets arising in health-care. We focus on analogous latent feature allocation models, where the data structures correspond to multisets or unbounded sparse matrices. The fundamental development in this regard is the Hierarchical Indian Buffet process (HIBP), which utilizes a hierarchy of Beta processes over J groups, where each group generates binary random matrices, reflecting within group sharing of features, according to beta-Bernoulli IBP priors. To encompass HIBP versions of non-Bernoulli extensions of the IBP, we introduce hierarchical versions of general spike and slab IBP. We provide explicit novel descriptions of the marginal, posterior and predictive distributions of the HIBP and its generalizations which allow for exact sampling and simpler practical implementation. We highlight common structural properties of these processes and establish relationships to existing IBP type and related models arising in the literature. Examples of potential applications may involve topic models, Poisson factorization models, random count matrix priors and neural network models

</details>

<details>

<summary>2021-03-21 20:37:11 - Probabilistic morphisms and Bayesian nonparametrics</summary>

- *Jürgen Jost, Hông Vân Lê, Tat Dat Tran*

- `1905.11448v3` - [abs](http://arxiv.org/abs/1905.11448v3) - [pdf](http://arxiv.org/pdf/1905.11448v3)

> In this paper we develop a functorial language of probabilistic morphisms and apply it to some basic problems in Bayesian nonparametrics. First we extend and unify the Kleisli category of probabilistic morphisms proposed by Lawvere and Giry with the category of statistical models proposed by Chentsov and Morse-Sacksteder. Then we introduce the notion of a Bayesian statistical model that formalizes the notion of a parameter space with a given prior distribution in Bayesian statistics. {We revisit the existence of a posterior distribution, using probabilistic morphisms}. In particular, we give an explicit formula for posterior distributions of the Bayesian statistical model, assuming that the underlying parameter space is a Souslin space and the sample space is a subset in a complete connected finite dimensional Riemannian manifold. Then we give a new proof of the existence of Dirichlet measures over any measurable space using a functorial property of the Dirichlet map constructed by Sethuraman.

</details>

<details>

<summary>2021-03-22 06:00:42 - Adaptive Degradation Process with Deep Learning-Driven Trajectory</summary>

- *Li Yang*

- `2103.11598v1` - [abs](http://arxiv.org/abs/2103.11598v1) - [pdf](http://arxiv.org/pdf/2103.11598v1)

> Remaining useful life (RUL) estimation is a crucial component in the implementation of intelligent predictive maintenance and health management. Deep neural network (DNN) approaches have been proven effective in RUL estimation due to their capacity in handling high-dimensional non-linear degradation features. However, the applications of DNN in practice face two challenges: (a) online update of lifetime information is often unavailable, and (b) uncertainties in predicted values may not be analytically quantified. This paper addresses these issues by developing a hybrid DNN-based prognostic approach, where a Wiener-based-degradation model is enhanced with adaptive drift to characterize the system degradation. An LSTM-CNN encoder-decoder is developed to predict future degradation trajectories by jointly learning noise coefficients as well as drift coefficients, and adaptive drift is updated via Bayesian inference. A computationally efficient algorithm is proposed for the calculation of RUL distributions. Numerical experiments are presented using turbofan engines degradation data to demonstrate the superior accuracy of RUL prediction of our proposed approach.

</details>

<details>

<summary>2021-03-22 08:43:09 - A Bayesian semi-parametric approach for inference on the population partly conditional mean from longitudinal data with dropout</summary>

- *Maria Josefsson, Michael J. Daniels, Sara Pudas*

- `2011.12345v3` - [abs](http://arxiv.org/abs/2011.12345v3) - [pdf](http://arxiv.org/pdf/2011.12345v3)

> Studies of memory trajectories using longitudinal data often result in highly non-representative samples due to selective study enrollment and attrition. An additional bias comes from practice effects that result in improved or maintained performance due to familiarity with test content or context. These challenges may bias study findings and severely distort the ability to generalize to the target population. In this study we propose an approach for estimating the finite population mean of a longitudinal outcome conditioning on being alive at a specific time point. We develop a flexible Bayesian semi-parametric predictive estimator for population inference when longitudinal auxiliary information is known for the target population. We evaluate sensitivity of the results to untestable assumptions and further compare our approach to other methods used for population inference in a simulation study. The proposed approach is motivated by 15-year longitudinal data from the Betula longitudinal cohort study. We apply our approach to estimate lifespan trajectories in episodic memory, with the aim to generalize findings to a target population.

</details>

<details>

<summary>2021-03-22 10:53:27 - Probabilistic Grammars for Equation Discovery</summary>

- *Jure Brence, Ljupčo Todorovski, Sašo Džeroski*

- `2012.00428v2` - [abs](http://arxiv.org/abs/2012.00428v2) - [pdf](http://arxiv.org/pdf/2012.00428v2)

> Equation discovery, also known as symbolic regression, is a type of automated modeling that discovers scientific laws, expressed in the form of equations, from observed data and expert knowledge. Deterministic grammars, such as context-free grammars, have been used to limit the search spaces in equation discovery by providing hard constraints that specify which equations to consider and which not. In this paper, we propose the use of probabilistic context-free grammars in equation discovery. Such grammars encode soft constraints, specifying a prior probability distribution on the space of possible equations. We show that probabilistic grammars can be used to elegantly and flexibly formulate the parsimony principle, that favors simpler equations, through probabilities attached to the rules in the grammars. We demonstrate that the use of probabilistic, rather than deterministic grammars, in the context of a Monte-Carlo algorithm for grammar-based equation discovery, leads to more efficient equation discovery. Finally, by specifying prior probability distributions over equation spaces, the foundations are laid for Bayesian approaches to equation discovery.

</details>

<details>

<summary>2021-03-22 12:02:00 - Numerical comparisons between Bayesian and frequentist low-rank matrix completion: estimation accuracy and uncertainty quantification</summary>

- *The Tien Mai*

- `2103.11749v1` - [abs](http://arxiv.org/abs/2103.11749v1) - [pdf](http://arxiv.org/pdf/2103.11749v1)

> In this paper we perform a numerious numerical studies for the problem of low-rank matrix completion. We compare the Bayesain approaches and a recently introduced de-biased estimator which provides a useful way to build confidence intervals of interest. From a theoretical viewpoint, the de-biased estimator comes with a sharp minimax-optinmal rate of estimation error whereas the Bayesian approach reaches this rate with an additional logarithmic factor. Our simulation studies show originally interesting results that the de-biased estimator is just as good as the Bayesain estimators. Moreover, Bayesian approaches are much more stable and can outperform the de-biased estimator in the case of small samples. However, we also find that the length of the confidence intervals revealed by the de-biased estimator for an entry is absolutely shorter than the length of the considered credible interval. These suggest further theoretical studies on the estimation error and the concentration for Bayesian methods as they are being quite limited up to present.

</details>

<details>

<summary>2021-03-22 13:30:48 - Use of Historical Individual Patient Data in Analysis of Clinical Trials</summary>

- *Shirin Golchi*

- `2002.09910v2` - [abs](http://arxiv.org/abs/2002.09910v2) - [pdf](http://arxiv.org/pdf/2002.09910v2)

> Historical data from previous clinical trials, observational studies and health records may be utilized in analysis of clinical trials data to strengthen inference. Under the Bayesian framework incorporation of information obtained from any source other than the current data is facilitated through construction of an informative prior. The existing methodology for defining an informative prior based on historical data relies on measuring similarity to the current data at the study level and does not take advantage of individual patient data (IPD). This paper proposes a family of priors that utilize IPD to strengthen statistical inference. It is demonstrated that the proposed prior construction approach outperforms the existing methods where the historical data are partially exchangeable with the present data. The proposed method is applied to IPD from a set of trials in non-small cell lung cancer.

</details>

<details>

<summary>2021-03-22 16:34:11 - A Bayesian Model of Dose-Response for Cancer Drug Studies</summary>

- *Wesley Tansey, Christopher Tosh, David M. Blei*

- `1906.04072v3` - [abs](http://arxiv.org/abs/1906.04072v3) - [pdf](http://arxiv.org/pdf/1906.04072v3)

> Exploratory cancer drug studies test multiple tumor cell lines against multiple candidate drugs. The goal in each paired (cell line, drug) experiment is to map out the dose-response curve of the cell line as the dose level of the drug increases. We propose Bayesian Tensor Filtering (BTF), a hierarchical Bayesian model for dose-response modeling in multi-sample, multi-treatment cancer drug studies. BTF uses low-dimensional embeddings to share statistical strength between similar drugs and similar cell lines. Structured shrinkage priors in BTF encourage smoothness in the dose-response curves while remaining adaptive to sharp jumps when the data call for it. We focus on a pair of cancer drug studies exhibiting a particular pathology in their experimental design, leading us to a non-conjugate monotone mixture-of-Gammas likelihood. To perform posterior inference, we develop a variant of the elliptical slice sampling algorithm for sampling from linearly-constrained multivariate normal priors with non-conjugate likelihoods. In benchmarks, BTF outperforms state-of-the-art methods for covariance regression and dynamic Poisson matrix factorization. On the two cancer drug studies, BTF outperforms the current standard approach in biology and reveals potential new biomarkers of drug sensitivity in cancer. Code is available at https://github.com/tansey/functionalmf.

</details>

<details>

<summary>2021-03-22 18:00:19 - Modelling intransitivity in pairwise comparisons with application to baseball data</summary>

- *Harry Spearing, Jonathan Tawn, David Irons, Tim Paulden*

- `2103.12094v1` - [abs](http://arxiv.org/abs/2103.12094v1) - [pdf](http://arxiv.org/pdf/2103.12094v1)

> In most commonly used ranking systems, some level of underlying transitivity is assumed. If transitivity exists in a system then information about pairwise comparisons can be translated to other linked pairs. For example, if typically A beats B and B beats C, this could inform us about the expected outcome between A and C. We show that in the seminal Bradley-Terry model knowing the probabilities of A beating B and B beating C completely defines the probability of A beating C, with these probabilities determined by individual skill levels of A, B and C. Users of this model tend not to investigate the validity of this transitive assumption, nor that some skill levels may not be statistically significantly different from each other; the latter leading to false conclusions about rankings. We provide a novel extension to the Bradley-Terry model, which accounts for both of these features: the intransitive relationships between pairs of objects are dealt with through interaction terms that are specific to each pair; and by partitioning the $n$ skills into $A+1\leq n$ distinct clusters, any differences in the objects' skills become significant, given appropriate $A$. With $n$ competitors there are $n(n-1)/2$ interactions, so even in multiple round robin competitions this gives too many parameters to efficiently estimate. Therefore we separately cluster the $n(n-1)/2$ values of intransitivity into $K$ clusters, giving $(A,K)$ estimatable values respectively, typically with $A+K<n$. Using a Bayesian hierarchical model, $(A,K)$ are treated as unknown, and inference is conducted via a reversible jump Markov chain Monte Carlo (RJMCMC) algorithm. The model is shown to have an improved fit out of sample in both simulated data and when applied to American League baseball data.

</details>

<details>

<summary>2021-03-22 21:34:52 - Partitioned hybrid learning of Bayesian network structures</summary>

- *Jireh Huang, Qing Zhou*

- `2103.12188v1` - [abs](http://arxiv.org/abs/2103.12188v1) - [pdf](http://arxiv.org/pdf/2103.12188v1)

> We develop a novel hybrid method for Bayesian network structure learning called partitioned hybrid greedy search (pHGS), composed of three distinct yet compatible new algorithms: Partitioned PC (pPC) accelerates skeleton learning via a divide-and-conquer strategy, $p$-value adjacency thresholding (PATH) effectively accomplishes parameter tuning with a single execution, and hybrid greedy initialization (HGI) maximally utilizes constraint-based information to obtain a high-scoring and well-performing initial graph for greedy search. We establish structure learning consistency of our algorithms in the large-sample limit, and empirically validate our methods individually and collectively through extensive numerical comparisons. The combined merits of pPC and PATH achieve significant computational reductions compared to the PC algorithm without sacrificing the accuracy of estimated structures, and our generally applicable HGI strategy reliably improves the estimation structural accuracy of popular hybrid algorithms with negligible additional computational expense. Our empirical results demonstrate the superior empirical performance of pHGS against many state-of-the-art structure learning algorithms.

</details>

<details>

<summary>2021-03-23 10:03:31 - Any Part of Bayesian Network Structure Learning</summary>

- *Zhaolong Ling, Kui Yu, Hao Wang, Lin Liu, Jiuyong Li*

- `2103.13810v1` - [abs](http://arxiv.org/abs/2103.13810v1) - [pdf](http://arxiv.org/pdf/2103.13810v1)

> We study an interesting and challenging problem, learning any part of a Bayesian network (BN) structure. In this challenge, it will be computationally inefficient using existing global BN structure learning algorithms to find an entire BN structure to achieve the part of a BN structure in which we are interested. And local BN structure learning algorithms encounter the false edge orientation problem when they are directly used to tackle this challenging problem. In this paper, we first present a new concept of Expand-Backtracking to explain why local BN structure learning methods have the false edge orientation problem, then propose APSL, an efficient and accurate Any Part of BN Structure Learning algorithm. Specifically, APSL divides the V-structures in a Markov blanket (MB) into two types: collider V-structure and non-collider V-structure, then it starts from a node of interest and recursively finds both collider V-structures and non-collider V-structures in the found MBs, until the part of a BN structure in which we are interested are oriented. To improve the efficiency of APSL, we further design the APSL-FS algorithm using Feature Selection, APSL-FS. Using six benchmark BNs, the extensive experiments have validated the efficiency and accuracy of our methods.

</details>

<details>

<summary>2021-03-23 13:28:51 - A sampling criterion for constrained Bayesian optimization with uncertainties</summary>

- *Reda El Amri, Rodolphe Le Riche, Céline Helbert, Christophette Blanchet-Scalliet, Sébastien Da Veiga*

- `2103.05706v3` - [abs](http://arxiv.org/abs/2103.05706v3) - [pdf](http://arxiv.org/pdf/2103.05706v3)

> We consider the problem of chance constrained optimization where it is sought to optimize a function and satisfy constraints, both of which are affected by uncertainties. The real world declinations of this problem are particularly challenging because of their inherent computational cost.   To tackle such problems, we propose a new Bayesian optimization method. It applies to the situation where the uncertainty comes from some of the inputs, so that it becomes possible to define an acquisition criterion in the joint controlled-uncontrolled input space. The main contribution of this work is an acquisition criterion that accounts for both the average improvement in objective function and the constraint reliability. The criterion is derived following the Stepwise Uncertainty Reduction logic and its maximization provides both optimal controlled and uncontrolled parameters. Analytical expressions are given to efficiently calculate the criterion. Numerical studies on test functions are presented. It is found through experimental comparisons with alternative sampling criteria that the adequation between the sampling criterion and the problem contributes to the efficiency of the overall optimization. As a side result, an expression for the variance of the improvement is given.

</details>

<details>

<summary>2021-03-23 16:30:11 - Bayesian imputation of COVID-19 positive test counts for nowcasting under reporting lag</summary>

- *Radka Jersakova, James Lomax, James Hetherington, Brieuc Lehmann, George Nicholson, Mark Briers, Chris Holmes*

- `2103.12661v1` - [abs](http://arxiv.org/abs/2103.12661v1) - [pdf](http://arxiv.org/pdf/2103.12661v1)

> Obtaining up to date information on the number of UK COVID-19 regional infections is hampered by the reporting lag in positive test results for people with COVID-19 symptoms. In the UK, for "Pillar 2" swab tests for those showing symptoms, it can take up to five days for results to be collated. We make use of the stability of the under reporting process over time to motivate a statistical temporal model that infers the final total count given the partial count information as it arrives. We adopt a Bayesian approach that provides for subjective priors on parameters and a hierarchical structure for an underlying latent intensity process for the infection counts. This results in a smoothed time-series representation now-casting the expected number of daily counts of positive tests with uncertainty bands that can be used to aid decision making. Inference is performed using sequential Monte Carlo.

</details>

<details>

<summary>2021-03-23 16:38:19 - Nested Gaussian filters for recursive Bayesian inference and nonlinear tracking in state space models</summary>

- *Sara Pérez-Vieites, Joaquín Míguez*

- `2103.12666v1` - [abs](http://arxiv.org/abs/2103.12666v1) - [pdf](http://arxiv.org/pdf/2103.12666v1)

> We introduce a new sequential methodology to calibrate the fixed parameters and track the stochastic dynamical variables of a state-space system. The proposed method is based on the nested hybrid filtering (NHF) framework of [1], that combines two layers of filters, one inside the other, to compute the joint posterior probability distribution of the static parameters and the state variables. In particular, we explore the use of deterministic sampling techniques for Gaussian approximation in the first layer of the algorithm, instead of the Monte Carlo methods employed in the original procedure. The resulting scheme reduces the computational cost and so makes the algorithms potentially better-suited for high-dimensional state and parameter spaces. We describe a specific instance of the new method and then study its performance and efficiency of the resulting algorithms for a stochastic Lorenz 63 model with uncertain parameters.

</details>

<details>

<summary>2021-03-24 02:21:48 - Scalable Bayesian estimation in the multinomial probit model</summary>

- *Ruben Loaiza-Maya, Didier Nibbering*

- `2007.13247v2` - [abs](http://arxiv.org/abs/2007.13247v2) - [pdf](http://arxiv.org/pdf/2007.13247v2)

> The multinomial probit model is a popular tool for analyzing choice behaviour as it allows for correlation between choice alternatives. Because current model specifications employ a full covariance matrix of the latent utilities for the choice alternatives, they are not scalable to a large number of choice alternatives. This paper proposes a factor structure on the covariance matrix, which makes the model scalable to large choice sets. The main challenge in estimating this structure is that the model parameters require identifying restrictions. We identify the parameters by a trace-restriction on the covariance matrix, which is imposed through a reparametrization of the factor structure. We specify interpretable prior distributions on the model parameters and develop an MCMC sampler for parameter estimation. The proposed approach significantly improves performance in large choice sets relative to existing multinomial probit specifications. Applications to purchase data show the economic importance of including a large number of choice alternatives in consumer choice analysis.

</details>

<details>

<summary>2021-03-24 04:35:04 - Finding your feet: A Gaussian process model for estimating the abilities of batsmen in Test cricket</summary>

- *Oliver George Stevenson, Brendon James Brewer*

- `1908.11490v2` - [abs](http://arxiv.org/abs/1908.11490v2) - [pdf](http://arxiv.org/pdf/1908.11490v2)

> In the sport of cricket, player batting ability is traditionally measured using the batting average. However, the batting average fails to measure both short-term changes in ability that occur during an innings, and long-term changes that occur between innings, due to the likes of age and experience in various match conditions. We derive and fit a Bayesian parametric model that employs a Gaussian process to measure and predict how the batting abilities of cricket players vary and fluctuate over the course of entire playing careers. The results allow us to better quantify and predict player batting ability, compared with both traditional cricket statistics, such as the batting average, and more complex models, such as the official International Cricket Council ratings.

</details>

<details>

<summary>2021-03-24 12:13:46 - Bayesian Inference for Brain Activity from Functional Magnetic Resonance Imaging Collected at Two Spatial Resolutions</summary>

- *Andrew S. Whiteman, Andreas J. Bartsch, Jian Kang, Timothy D. Johnson*

- `2103.13131v1` - [abs](http://arxiv.org/abs/2103.13131v1) - [pdf](http://arxiv.org/pdf/2103.13131v1)

> Neuroradiologists and neurosurgeons increasingly opt to use functional magnetic resonance imaging (fMRI) to map functionally relevant brain regions for noninvasive presurgical planning and intraoperative neuronavigation. This application requires a high degree of spatial accuracy, but the fMRI signal-to-noise ratio (SNR) decreases as spatial resolution increases. In practice, fMRI scans can be collected at multiple spatial resolutions, and it is of interest to make more accurate inference on brain activity by combining data with different resolutions. To this end, we develop a new Bayesian model to leverage both better anatomical precision in high resolution fMRI and higher SNR in standard resolution fMRI. We assign a Gaussian process prior to the mean intensity function and develop an efficient, scalable posterior computation algorithm to integrate both sources of data. We draw posterior samples using an algorithm analogous to Riemann manifold Hamiltonian Monte Carlo in an expanded parameter space. We illustrate our method in analysis of presurgical fMRI data, and show in simulation that it infers the mean intensity more accurately than alternatives that use either the high or standard resolution fMRI data alone.

</details>

<details>

<summary>2021-03-24 13:22:45 - Bayesian Shape Invariant Model for Latent Growth Curve with Time-Invariant Covariates</summary>

- *Mohammad Alfrad Nobel Bhuiyan, Heidi Sucharew, Rhonda Szczesniak, Marepalli Rao, Jessica Woo, Jane Khoury, Md Monir Hossain*

- `1912.10842v2` - [abs](http://arxiv.org/abs/1912.10842v2) - [pdf](http://arxiv.org/pdf/1912.10842v2)

> In the attention-deficit hyperactivity disorder (ADHD) study, children are prescribed different stimulant medications. The height measurements are recorded longitudinally along with the medication time. Differences among the patients are captured by the parameters suggested the Superimposition by Translation and Rotation (SITAR) model using three subject-specific parameters to estimate their deviation from the mean growth curve. In this paper, we generalize the SITAR model in a Bayesian way with time-invariant covariates. The time-invariant model allows us to predict latent growth factors. Since patients suffer from a common disease, they usually exhibit a similar pattern, and it is natural to build a nonlinear model that is shaped invariant. The model is semi-parametric, where the population time curve is modeled with a natural cubic spline. The original shape invariant growth curve model, motivated by epidemiological research on the evolution of pubertal heights over time, fits the underlying shape function for height over age and estimates subject-specific deviations from this curve in terms of size, tempo, and velocity using maximum likelihood. The usefulness of the model is illustrated in the attention deficit hyperactivity disorder (ADHD) study. Further, we demonstrated the effect of stimulant medications on pubertal growth by gender.

</details>

<details>

<summary>2021-03-24 13:25:21 - Posterior Consistency of Semi-Supervised Regression on Graphs</summary>

- *Andrea L. Bertozzi, Bamdad Hosseini, Hao Li, Kevin Miller, Andrew M. Stuart*

- `2007.12809v2` - [abs](http://arxiv.org/abs/2007.12809v2) - [pdf](http://arxiv.org/pdf/2007.12809v2)

> Graph-based semi-supervised regression (SSR) is the problem of estimating the value of a function on a weighted graph from its values (labels) on a small subset of the vertices. This paper is concerned with the consistency of SSR in the context of classification, in the setting where the labels have small noise and the underlying graph weighting is consistent with well-clustered nodes. We present a Bayesian formulation of SSR in which the weighted graph defines a Gaussian prior, using a graph Laplacian, and the labeled data defines a likelihood. We analyze the rate of contraction of the posterior measure around the ground truth in terms of parameters that quantify the small label error and inherent clustering in the graph. We obtain bounds on the rates of contraction and illustrate their sharpness through numerical experiments. The analysis also gives insight into the choice of hyperparameters that enter the definition of the prior.

</details>

<details>

<summary>2021-03-24 14:18:35 - Consistency of Bayesian inference with Gaussian process priors for a parabolic inverse problem</summary>

- *Hanne Kekkonen*

- `2103.13213v1` - [abs](http://arxiv.org/abs/2103.13213v1) - [pdf](http://arxiv.org/pdf/2103.13213v1)

> We consider the statistical nonlinear inverse problem of recovering the absorption term $f>0$ in the heat equation $$ \partial_tu-\frac{1}{2}\Delta u+fu=0 \quad \text{on $\mathcal{O}\times(0,\textbf{T})$}\quad u = g \quad \text{on $\partial\mathcal{O}\times(0,\textbf{T})$}\quad u(\cdot,0)=u_0 \quad \text{on $\mathcal{O}$}, $$ where $\mathcal{O}\in\mathbb{R}^d$ is a bounded domain, $\textbf{T}<\infty$ is a fixed time, and $g,u_0$ are given sufficiently smooth functions describing boundary and initial values respectively. The data consists of $N$ discrete noisy point evaluations of the solution $u_f$ on $\mathcal{O}\times(0,\textbf{T})$. We study the statistical performance of Bayesian nonparametric procedures based on a large class of Gaussian process priors. We show that, as the number of measurements increases, the resulting posterior distributions concentrate around the true parameter generating the data, and derive a convergence rate for the reconstruction error of the associated posterior means. We also consider the optimality of the contraction rates and prove a lower bound for the minimax convergence rate for inferring $f$ from the data, and show that optimal rates can be achieved with truncated Gaussian priors.

</details>

<details>

<summary>2021-03-24 14:56:34 - Meta Analysis of Bayes Factors</summary>

- *Stavros Nikolakopoulos, Ioannis Ntzoufras*

- `2103.13236v1` - [abs](http://arxiv.org/abs/2103.13236v1) - [pdf](http://arxiv.org/pdf/2103.13236v1)

> Bayes Factors, the Bayesian tool for hypothesis testing, are receiving increasing attention in the literature. Compared to their frequentist rivals ($p$-values or test statistics), Bayes Factors have the conceptual advantage of providing evidence both for and against a null hypothesis and they can be calibrated so that they do not depend so heavily on the sample size. However, research on the synthesis of Bayes Factors arising from individual studies has received very limited attention. In this work we review and propose methods for combining Bayes Factors from multiple studies, depending on the level of information available. In the process, we provide insights with respect to the interplay between frequentist and Bayesian evidence. We also clarify why some intuitive suggestions in the literature can be misleading. We assess the performance of the methods discussed via a simulation study and apply the methods in an example from the field of psychology.

</details>

<details>

<summary>2021-03-24 15:34:04 - Estimation of causal effects with small data in the presence of trapdoor variables</summary>

- *Jouni Helske, Santtu Tikka, Juha Karvanen*

- `2003.03187v3` - [abs](http://arxiv.org/abs/2003.03187v3) - [pdf](http://arxiv.org/pdf/2003.03187v3)

> We consider the problem of estimating causal effects of interventions from observational data when well-known back-door and front-door adjustments are not applicable. We show that when an identifiable causal effect is subject to an implicit functional constraint that is not deducible from conditional independence relations, the estimator of the causal effect can exhibit bias in small samples. This bias is related to variables that we call trapdoor variables. We use simulated data to study different strategies to account for trapdoor variables and suggest how the related trapdoor bias might be minimized. The importance of trapdoor variables in causal effect estimation is illustrated with real data from the Life Course 1971-2002 study. Using this dataset, we estimate the causal effect of education on income in the Finnish context. Bayesian modelling allows us to take the parameter uncertainty into account and to present the estimated causal effects as posterior distributions.

</details>

<details>

<summary>2021-03-24 17:56:43 - Sequential pCN-MCMC, an efficient MCMC method for Bayesian inversion of high-dimensional multi-Gaussian priors</summary>

- *Sebastian Reuschen, Fabian Jobst, Wolfgang Nowak*

- `2103.13385v1` - [abs](http://arxiv.org/abs/2103.13385v1) - [pdf](http://arxiv.org/pdf/2103.13385v1)

> In geostatistics, Gaussian random fields are often used to model heterogeneities of soil or subsurface parameters. To give spatial approximations of these random fields, they are discretized. Then, different techniques of geostatistical inversion are used to condition them on measurement data. Among these techniques, Markov chain Monte Carlo (MCMC) techniques stand out, because they yield asymptotically unbiased conditional realizations. However, standard Markov Chain Monte Carlo (MCMC) methods suffer the curse of dimensionality when refining the discretization. This means that their efficiency decreases rapidly with an increasing number of discretization cells. Several MCMC approaches have been developed such that the MCMC efficiency does not depend on the discretization of the random field. The pre-conditioned Crank Nicolson Markov Chain Monte Carlo (pCN-MCMC) and the sequential Gibbs (or block-Gibbs) sampling are two examples. In this paper, we will present a combination of the pCN-MCMC and the sequential Gibbs sampling. Our algorithm, the sequential pCN-MCMC, will depend on two tuning-parameters: the correlation parameter $\beta$ of the pCN approach and the block size $\kappa$ of the sequential Gibbs approach. The original pCN-MCMC and the Gibbs sampling algorithm are special cases of our method. We present an algorithm that automatically finds the best tuning-parameter combination ($\kappa$ and $\beta$) during the burn-in-phase of the algorithm, thus choosing the best possible hybrid between the two methods. In our test cases, we achieve a speedup factors of $1-5.5$ over pCN and of $1-6.5$ over Gibbs. Furthermore, we provide the MATLAB implementation of our method as open-source code.

</details>

<details>

<summary>2021-03-25 07:51:43 - On the quantification of discretization uncertainty: comparison of two paradigms</summary>

- *Julien Bect, Souleymane Zio, Guillaume Perrin, Claire Cannamela, Emmanuel Vazquez*

- `2103.14559v1` - [abs](http://arxiv.org/abs/2103.14559v1) - [pdf](http://arxiv.org/pdf/2103.14559v1)

> Numerical models based on partial differential equations (PDE), or integro-differential equations, are ubiquitous in engineering and science, making it possible to understand or design systems for which physical experiments would be expensive-sometimes impossible-to carry out. Such models usually construct an approximate solution of the underlying continuous equations, using discretization methods such as finite differences or the finite elements method. The resulting discretization error introduces a form of uncertainty on the exact but unknown value of any quantity of interest (QoI), which affects the predictions of the numerical model alongside other sources of uncertainty such as parametric uncertainty or model inadequacy. The present article deals with the quantification of this discretization uncertainty.A first approach to this problem, now standard in the V\&V (Verification and Validation) literature, uses the grid convergence index (GCI) originally proposed by P. Roache in the field of computational fluid dynamics (CFD), which is based on the Richardson extrapolation technique. Another approach, based on Bayesian inference with Gaussian process models, was more recently introduced in the statistical literature. In this work we present and compare these two paradigms for the quantification of discretization uncertainty, which have been developped in different scientific communities, and assess the potential of the-younger-Bayesian approach to provide a replacement for the well-established GCI-based approach, with better probabilistic foundations. The methods are illustrated and evaluated on two standard test cases from the literature (lid-driven cavity and Timoshenko beam).

</details>

<details>

<summary>2021-03-25 08:40:18 - Brain Waves Analysis Via a Non-parametric Bayesian Mixture of Autoregressive Kernels</summary>

- *Guillermo Granados-Garcia, Mark Fiecas, Babak Shahbaba, Norbert Fortin, Hernando Ombao*

- `2102.11971v3` - [abs](http://arxiv.org/abs/2102.11971v3) - [pdf](http://arxiv.org/pdf/2102.11971v3)

> The standard approach to analyzing brain electrical activity is to examine the spectral density function (SDF) and identify predefined frequency bands that have the most substantial relative contributions to the overall variance of the signal. However, a limitation of this approach is that the precise frequency and bandwidth of oscillations vary with cognitive demands. Thus they should not be arbitrarily defined a priori in an experiment. In this paper, we develop a data-driven approach that identifies (i) the number of prominent peaks, (ii) the frequency peak locations, and (iii) their corresponding bandwidths (or spread of power around the peaks). We propose a Bayesian mixture auto-regressive decomposition method (BMARD), which represents the standardized SDFas a Dirichlet process mixture based on a kernel derived from second-order auto-regressive processes which completely characterize the location (peak)and scale (bandwidth) parameters. We present a Metropolis-Hastings within Gibbs algorithm to sample from the posterior distribution of the mixture parameters. Simulation studies demonstrate the robustness and performance of the BMARD method. Finally, we use the proposed BMARD method to analyze local field potential (LFP) activity from the hippocampus of laboratory rats across different conditions in a non-spatial sequence memory experiment to identify the most interesting frequency bands and examine the link between specific patterns of activity and trial-specific cognitive demands.

</details>

<details>

<summary>2021-03-25 11:13:15 - Parameter estimation and model selection for water sorption in a wood fibre material</summary>

- *Julien Berger, Thibaut Colinart, Bruna R. Loiola, Helcio R. B. Orlande*

- `2104.09609v1` - [abs](http://arxiv.org/abs/2104.09609v1) - [pdf](http://arxiv.org/pdf/2104.09609v1)

> The sorption curve is an essential feature for the modelling of heat and mass transfer in porous building materials. Several models have been proposed in the literature to represent the amount of moisture content in the material according to the water activity (or capillary pressure) level. These models are based on analytical expressions and few parameters that need to be estimated by inverse analysis. This article investigates the reliability of eight models through the accuracy of the estimated parameters. For this, experimental data for a wood fibre material are generated with special attention to the stop criterion to capture long time kinetic constants. Among five sets of measurements, the best estimate is computed. The reliability of the models is then discussed. After proving the theoretical identifiability of the unknown parameters for each model, the primary identifiability is analysed. It evaluates whether the parameters influence on the model output is sufficient to proceed the parameter estimation with accuracy. For this, a continuous derivative-based approach is adopted. Seven models have a low primary identifiability for at least one parameter. Indeed, when estimating the unknown parameters using the experimental observations, the parameters with low primary identifiability exhibit large uncertainties. Finally, an Approximation Bayesian Computation algorithm is used to simultaneously select the best model and estimate the parameters that best represent the experimental data. The thermodynamic and \textsc{Feng}--\textsc{Xing} models, together with a proposed model in this work, were the best ones selected by this algorithm.

</details>

<details>

<summary>2021-03-25 12:12:19 - Encoding the latent posterior of Bayesian Neural Networks for uncertainty quantification</summary>

- *Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, Isabelle Bloch*

- `2012.02818v2` - [abs](http://arxiv.org/abs/2012.02818v2) - [pdf](http://arxiv.org/pdf/2012.02818v2)

> Bayesian neural networks (BNNs) have been long considered an ideal, yet unscalable solution for improving the robustness and the predictive uncertainty of deep neural networks. While they could capture more accurately the posterior distribution of the network parameters, most BNN approaches are either limited to small networks or rely on constraining assumptions such as parameter independence. These drawbacks have enabled prominence of simple, but computationally heavy approaches such as Deep Ensembles, whose training and testing costs increase linearly with the number of networks. In this work we aim for efficient deep BNNs amenable to complex computer vision architectures, e.g. ResNet50 DeepLabV3+, and tasks, e.g. semantic segmentation, with fewer assumptions on the parameters. We achieve this by leveraging variational autoencoders (VAEs) to learn the interaction and the latent distribution of the parameters at each network layer. Our approach, Latent-Posterior BNN (LP-BNN), is compatible with the recent BatchEnsemble method, leading to highly efficient ({in terms of computation and} memory during both training and testing) ensembles. LP-BNN s attain competitive results across multiple metrics in several challenging benchmarks for image classification, semantic segmentation and out-of-distribution detection.

</details>

<details>

<summary>2021-03-25 12:50:14 - Active Structure Learning of Bayesian Networks in an Observational Setting</summary>

- *Noa Ben-David, Sivan Sabato*

- `2103.13796v1` - [abs](http://arxiv.org/abs/2103.13796v1) - [pdf](http://arxiv.org/pdf/2103.13796v1)

> We study active structure learning of Bayesian networks in an observational setting, in which there are external limitations on the number of variable values that can be observed from the same sample. Random samples are drawn from the joint distribution of the network variables, and the algorithm iteratively selects which variables to observe in the next sample. We propose a new active learning algorithm for this setting, that finds with a high probability a structure with a score that is $\epsilon$-close to the optimal score. We show that for a class of distributions that we term stable, a sample complexity reduction of up to a factor of $\widetilde{\Omega}(d^3)$ can be obtained, where $d$ is the number of network variables. We further show that in the worst case, the sample complexity of the active algorithm is guaranteed to be almost the same as that of a naive baseline algorithm. To supplement the theoretical results, we report experiments that compare the performance of the new active algorithm to the naive baseline and demonstrate the sample complexity improvements. Code for the algorithm and for the experiments is provided at https://github.com/noabdavid/activeBNSL.

</details>

<details>

<summary>2021-03-25 13:18:52 - A deep surrogate approach to efficient Bayesian inversion in PDE and integral equation models</summary>

- *Teo Deveney, Eike Mueller, Tony Shardlow*

- `1910.01547v4` - [abs](http://arxiv.org/abs/1910.01547v4) - [pdf](http://arxiv.org/pdf/1910.01547v4)

> We investigate a deep learning approach to efficiently perform Bayesian inference in partial differential equation (PDE) and integral equation models over potentially high-dimensional parameter spaces. The contributions of this paper are two-fold; the first is the introduction of a neural network approach to approximating the solutions of Fredholm and Volterra integral equations of the first and second kind. The second is the development of a new, efficient deep learning-based method for Bayesian inversion applied to problems that can be described by PDEs or integral equations. To achieve this we introduce a surrogate model, and demonstrate how this allows efficient sampling from a Bayesian posterior distribution in which the likelihood depends on the solutions of PDEs or integral equations. Our method relies on the direct approximation of parametric solutions by neural networks, without need of traditional numerical solves. This deep learning approach allows the accurate and efficient approximation of parametric solutions in significantly higher dimensions than is possible using classical discretisation schemes. Since the approximated solutions can be cheaply evaluated, the solutions of Bayesian inverse problems over large parameter spaces are efficient using Markov chain Monte Carlo. We demonstrate the performance of our method using two real-world examples; these include Bayesian inference in the PDE and integral equation case for an example from electrochemistry, and Bayesian inference of a function-valued heat-transfer parameter with applications in aviation.

</details>

<details>

<summary>2021-03-25 18:12:30 - Why optional stopping can be a problem for Bayesians</summary>

- *Rianne de Heide, Peter D. Grünwald*

- `1708.08278v5` - [abs](http://arxiv.org/abs/1708.08278v5) - [pdf](http://arxiv.org/pdf/1708.08278v5)

> Recently, optional stopping has been a subject of debate in the Bayesian psychology community. Rouder (2014) argues that optional stopping is no problem for Bayesians, and even recommends the use of optional stopping in practice, as do Wagenmakers et al. (2012). This article addresses the question whether optional stopping is problematic for Bayesian methods, and specifies under which circumstances and in which sense it is and is not. By slightly varying and extending Rouder's (2014) experiments, we illustrate that, as soon as the parameters of interest are equipped with default or pragmatic priors - which means, in most practical applications of Bayes factor hypothesis testing - resilience to optional stopping can break down. We distinguish between three types of default priors, each having their own specific issues with optional stopping, ranging from no-problem-at-all (Type 0 priors) to quite severe (Type II priors).

</details>

<details>

<summary>2021-03-25 18:52:01 - Learning landmark geodesics using Kalman ensembles</summary>

- *Andreas Bock, Colin J. Cotter*

- `2103.14076v1` - [abs](http://arxiv.org/abs/2103.14076v1) - [pdf](http://arxiv.org/pdf/2103.14076v1)

> We study the problem of diffeomorphometric geodesic landmark matching where the objective is to find a diffeomorphism that via its group action maps between two sets of landmarks. It is well-known that the motion of the landmarks, and thereby the diffeomorphism, can be encoded by an initial momentum leading to a formulation where the landmark matching problem can be solved as an optimisation problem over such momenta. The novelty of our work lies in the application of a derivative-free Bayesian inverse method for learning the optimal momentum encoding the diffeomorphic mapping between the template and the target. The method we apply is the ensemble Kalman filter, an extension of the Kalman filter to nonlinear observation operators. We describe an efficient implementation of the algorithm and show several numerical results for various target shapes.

</details>

<details>

<summary>2021-03-25 19:08:41 - PAC-Bayesian theory for stochastic LTI systems</summary>

- *Deividas Eringis, John Leth, Zheng-Hua Tan, Rafal Wisniewski, Alireza Fakhrizadeh Esfahani, Mihaly Petreczky*

- `2103.12866v2` - [abs](http://arxiv.org/abs/2103.12866v2) - [pdf](http://arxiv.org/pdf/2103.12866v2)

> In this paper we derive a PAC-Bayesian error bound for autonomous stochastic LTI state-space models. The motivation for deriving such error bounds is that they will allow deriving similar error bounds for more general dynamical systems, including recurrent neural networks. In turn, PACBayesian error bounds are known to be useful for analyzing machine learning algorithms and for deriving new ones.

</details>

<details>

<summary>2021-03-26 08:11:05 - Variable Selection Using Nearest Neighbor Gaussian Processes</summary>

- *Konstantin Posch, Maximilian Arbeiter, Martin Pleschberger, Juergen Pilz*

- `2103.14315v1` - [abs](http://arxiv.org/abs/2103.14315v1) - [pdf](http://arxiv.org/pdf/2103.14315v1)

> A novel Bayesian approach to the problem of variable selection using Gaussian process regression is proposed. The selection of the most relevant variables for a problem at hand often results in an increased interpretability and in many cases is an essential step in terms of model regularization. In detail, the proposed method relies on so-called nearest neighbor Gaussian processes, that can be considered as highly scalable approximations of classical Gaussian processes. To perform a variable selection the mean and the covariance function of the process are conditioned on a random set $\mathcal{A}$. This set holds the indices of variables that contribute to the model. While the specification of a priori beliefs regarding $\mathcal{A}$ allows to control the number of selected variables, so-called reference priors are assigned to the remaining model parameters. The application of the reference priors ensures that the process covariance matrix is (numerically) robust. For the model inference a Metropolis within Gibbs algorithm is proposed. Based on simulated data, an approximation problem from computer experiments and two real-world datasets, the performance of the new approach is evaluated.

</details>

<details>

<summary>2021-03-26 08:12:04 - Bayesian Clustering for Continuous-Time Hidden Markov Models</summary>

- *Yu Luo, David A. Stephens, David L. Buckeridge*

- `1906.10252v3` - [abs](http://arxiv.org/abs/1906.10252v3) - [pdf](http://arxiv.org/pdf/1906.10252v3)

> We develop clustering procedures for longitudinal trajectories based on a continuous-time hidden Markov model (CTHMM) and a generalized linear observation model. Specifically in this paper, we carry out finite and infinite mixture model-based clustering for a CTHMM and achieve inference using Markov chain Monte Carlo (MCMC). For a finite mixture model with prior on the number of components, we implement reversible-jump MCMC to facilitate the trans-dimensional move between different number of clusters. For a Dirichlet process mixture model, we utilize restricted Gibbs sampling split-merge proposals to expedite the MCMC algorithm. We employ proposed algorithms to the simulated data as well as a real data example, and the results demonstrate the desired performance of the new sampler.

</details>

<details>

<summary>2021-03-26 16:23:47 - A generalised and fully Bayesian framework for ensemble updating</summary>

- *Margrethe Kvale Loe, Håkon Tjelmeland*

- `2103.14565v1` - [abs](http://arxiv.org/abs/2103.14565v1) - [pdf](http://arxiv.org/pdf/2103.14565v1)

> We propose a generalised framework for the updating of a prior ensemble to a posterior ensemble, an essential yet challenging part in ensemble-based filtering methods. The proposed framework is based on a generalised and fully Bayesian view on the traditional ensemble Kalman filter (EnKF). In the EnKF, the updating of the ensemble is based on Gaussian assumptions, whereas in our general setup the updating may be based on another parametric family. In addition, we propose to formulate an optimality criterion and to find the optimal update with respect to this criterion. The framework is fully Bayesian in the sense that the parameters of the assumed forecast model are treated as random variables. As a consequence, a parameter vector is simulated, for each ensemble member, prior to the updating. In contrast to existing fully Bayesian approaches, where the parameters are simulated conditionally on all the forecast samples, the parameters are in our framework simulated conditionally on both the data and all the forecast samples, except the forecast sample which is to be updated. The proposed framework is studied in detail for two parametric families: the linear-Gaussian model and the finite state-space hidden Markov model. For both cases, we present simulation examples and compare the results with existing ensemble-based filtering methods. The results of the proposed approach indicate a promising performance. In particular, the filter based on the linear-Gaussian model gives a more realistic representation of the uncertainty than the traditional EnKF, and the effect of not conditioning on the forecast sample which is to be updated when simulating the parameters is remarkable.

</details>

<details>

<summary>2021-03-27 08:11:31 - COVID-19 mortality analysis from soft-data multivariate curve regression and machine learning</summary>

- *A. Torres-Signes, M. P. Frías, M. D. Ruiz-Medina*

- `2008.06344v3` - [abs](http://arxiv.org/abs/2008.06344v3) - [pdf](http://arxiv.org/pdf/2008.06344v3)

> A multiple objective space-time forecasting approach is presented involving cyclical curve log-regression, and multivariate time series spatial residual correlation analysis. Specifically, the mean quadratic loss function is minimized in the framework of trigonometric regression. While, in our subsequent spatial residual correlation analysis, maximization of the likelihood allows us to compute the posterior mode in a Bayesian multivariate time series soft-data framework. The presented approach is applied to the analysis of COVID-19 mortality in the first wave affecting the Spanish Communities, since March, 8, 2020 until May, 13, 2020. An empirical comparative study with Machine Learning (ML) regression, based on random k-fold cross-validation, and bootstrapping confidence interval and probability density estimation, is carried out. This empirical analysis also investigates the performance of ML regression models in a hard- and soft- data frameworks. The results could be extrapolated to other counts, countries, and posterior COVID-19 waves.

</details>

<details>

<summary>2021-03-27 08:51:47 - Measuring Uncertainty through Bayesian Learning of Deep Neural Network Structure</summary>

- *Zhijie Deng, Yucen Luo, Jun Zhu, Bo Zhang*

- `1911.09804v3` - [abs](http://arxiv.org/abs/1911.09804v3) - [pdf](http://arxiv.org/pdf/1911.09804v3)

> Bayesian neural networks (BNNs) augment deep networks with uncertainty quantification by Bayesian treatment of the network weights. However, such models face the challenge of Bayesian inference in a high-dimensional and usually over-parameterized space. This paper investigates a new line of Bayesian deep learning by performing Bayesian inference on network structure. Instead of building structure from scratch inefficiently, we draw inspirations from neural architecture search to represent the network structure. We then develop an efficient stochastic variational inference approach which unifies the learning of both network structure and weights. Empirically, our method exhibits competitive predictive performance while preserving the benefits of Bayesian principles across challenging scenarios. We also provide convincing experimental justification for our modeling choice.

</details>

<details>

<summary>2021-03-28 21:47:02 - Bayesian Optimal Experimental Design for Inferring Causal Structure</summary>

- *Michele Zemplenyi, Jeffrey W. Miller*

- `2103.15229v1` - [abs](http://arxiv.org/abs/2103.15229v1) - [pdf](http://arxiv.org/pdf/2103.15229v1)

> Inferring the causal structure of a system typically requires interventional data, rather than just observational data. Since interventional experiments can be costly, it is preferable to select interventions that yield the maximum amount of information about a system. We propose a novel Bayesian method for optimal experimental design by sequentially selecting interventions that minimize the expected posterior entropy as rapidly as possible. A key feature is that the method can be implemented by computing simple summaries of the current posterior, avoiding the computationally burdensome task of repeatedly performing posterior inference on hypothetical future datasets drawn from the posterior predictive. After deriving the method in a general setting, we apply it to the problem of inferring causal networks. We present a series of simulation studies in which we find that the proposed method performs favorably compared to existing alternative methods. Finally, we apply the method to real and simulated data from a protein-signaling network.

</details>

<details>

<summary>2021-03-29 04:11:34 - Bayesian Attention Networks for Data Compression</summary>

- *Michael Tetelman*

- `2103.15319v1` - [abs](http://arxiv.org/abs/2103.15319v1) - [pdf](http://arxiv.org/pdf/2103.15319v1)

> The lossless data compression algorithm based on Bayesian Attention Networks is derived from first principles. Bayesian Attention Networks are defined by introducing an attention factor per a training sample loss as a function of two sample inputs, from training sample and prediction sample. By using a sharpened Jensen's inequality we show that the attention factor is completely defined by a correlation function of the two samples w.r.t. the model weights. Due to the attention factor the solution for a prediction sample is mostly defined by a few training samples that are correlated with the prediction sample. Finding a specific solution per prediction sample couples together the training and the prediction. To make the approach practical we introduce a latent space to map each prediction sample to a latent space and learn all possible solutions as a function of the latent space along with learning attention as a function of the latent space and a training sample. The latent space plays a role of the context representation with a prediction sample defining a context and a learned context dependent solution used for the prediction.

</details>

<details>

<summary>2021-03-29 12:35:02 - Local Competition and Stochasticity for Adversarial Robustness in Deep Learning</summary>

- *Konstantinos P. Panousis, Sotirios Chatzis, Antonios Alexos, Sergios Theodoridis*

- `2101.01121v2` - [abs](http://arxiv.org/abs/2101.01121v2) - [pdf](http://arxiv.org/pdf/2101.01121v2)

> This work addresses adversarial robustness in deep learning by considering deep networks with stochastic local winner-takes-all (LWTA) activations. This type of network units result in sparse representations from each model layer, as the units are organized in blocks where only one unit generates a non-zero output. The main operating principle of the introduced units lies on stochastic arguments, as the network performs posterior sampling over competing units to select the winner. We combine these LWTA arguments with tools from the field of Bayesian non-parametrics, specifically the stick-breaking construction of the Indian Buffet Process, to allow for inferring the sub-part of each layer that is essential for modeling the data at hand. Then, inference is performed by means of stochastic variational Bayes. We perform a thorough experimental evaluation of our model using benchmark datasets. As we show, our method achieves high robustness to adversarial perturbations, with state-of-the-art performance in powerful adversarial attack schemes.

</details>

<details>

<summary>2021-03-29 14:14:06 - Machine learning based digital twin for stochastic nonlinear multi-degree of freedom dynamical system</summary>

- *Shailesh Garg, Ankush Gogoi, Souvik Chakraborty, Budhaditya Hazra*

- `2103.15636v1` - [abs](http://arxiv.org/abs/2103.15636v1) - [pdf](http://arxiv.org/pdf/2103.15636v1)

> The potential of digital twin technology is immense, specifically in the infrastructure, aerospace, and automotive sector. However, practical implementation of this technology is not at an expected speed, specifically because of lack of application-specific details. In this paper, we propose a novel digital twin framework for stochastic nonlinear multi-degree of freedom (MDOF) dynamical systems. The approach proposed in this paper strategically decouples the problem into two time-scales -- (a) a fast time-scale governing the system dynamics and (b) a slow time-scale governing the degradation in the system. The proposed digital twin has four components - (a) a physics-based nominal model (low-fidelity), (b) a Bayesian filtering algorithm a (c) a supervised machine learning algorithm and (d) a high-fidelity model for predicting future responses. The physics-based nominal model combined with Bayesian filtering is used combined parameter state estimation and the supervised machine learning algorithm is used for learning the temporal evolution of the parameters. While the proposed framework can be used with any choice of Bayesian filtering and machine learning algorithm, we propose to use unscented Kalman filter and Gaussian process. Performance of the proposed approach is illustrated using two examples. Results obtained indicate the applicability and excellent performance of the proposed digital twin framework.

</details>

<details>

<summary>2021-03-29 17:55:20 - Bayesian Bootstrap Spike-and-Slab LASSO</summary>

- *Lizhen Nie, Veronika Ročková*

- `2011.14279v2` - [abs](http://arxiv.org/abs/2011.14279v2) - [pdf](http://arxiv.org/pdf/2011.14279v2)

> The impracticality of posterior sampling has prevented the widespread adoption of spike-and-slab priors in high-dimensional applications. To alleviate the computational burden, optimization strategies have been proposed that quickly find local posterior modes. Trading off uncertainty quantification for computational speed, these strategies have enabled spike-and-slab deployments at scales that would be previously unfeasible. We build on one recent development in this strand of work: the Spike-and-Slab LASSO procedure of Ro\v{c}kov\'{a} and George (2018). Instead of optimization, however, we explore multiple avenues for posterior sampling, some traditional and some new. Intrigued by the speed of Spike-and-Slab LASSO mode detection, we explore the possibility of sampling from an approximate posterior by performing MAP optimization on many independently perturbed datasets. To this end, we explore Bayesian bootstrap ideas and introduce a new class of jittered Spike-and-Slab LASSO priors with random shrinkage targets. These priors are a key constituent of the Bayesian Bootstrap Spike-and-Slab LASSO (BB-SSL) method proposed here. BB-SSL turns fast optimization into approximate posterior sampling. Beyond its scalability, we show that BB-SSL has a strong theoretical support. Indeed, we find that the induced pseudo-posteriors contract around the truth at a near-optimal rate in sparse normal-means and in high-dimensional regression. We compare our algorithm to the traditional Stochastic Search Variable Selection (under Laplace priors) as well as many state-of-the-art methods for shrinkage priors. We show, both in simulations and on real data, that our method fares superbly in these comparisons, often providing substantial computational gains.

</details>

<details>

<summary>2021-03-29 19:54:25 - Modelling Heterogeneity Using Bayesian Structured Sparsity</summary>

- *Max Goplerud*

- `2103.15919v1` - [abs](http://arxiv.org/abs/2103.15919v1) - [pdf](http://arxiv.org/pdf/2103.15919v1)

> How to estimate heterogeneity, e.g. the effect of some variable differing across observations, is a key question in political science. Methods for doing so make simplifying assumptions about the underlying nature of the heterogeneity to draw reliable inferences. This paper allows a common way of simplifying complex phenomenon (placing observations with similar effects into discrete groups) to be integrated into regression analysis. The framework allows researchers to (i) use their prior knowledge to guide which groups are permissible and (ii) appropriately quantify uncertainty. The paper does this by extending work on "structured sparsity" from a traditional penalized likelihood approach to a Bayesian one by deriving new theoretical results and inferential techniques. It shows that this method outperforms state-of-the-art methods for estimating heterogeneous effects when the underlying heterogeneity is grouped and more effectively identifies groups of observations with different effects in observational data.

</details>

<details>

<summary>2021-03-30 05:37:53 - Horseshoe shrinkage methods for Bayesian fusion estimation</summary>

- *Sayantan Banerjee*

- `2102.07378v2` - [abs](http://arxiv.org/abs/2102.07378v2) - [pdf](http://arxiv.org/pdf/2102.07378v2)

> We consider the problem of estimation and structure learning of high dimensional signals via a normal sequence model, where the underlying parameter vector is piecewise constant, or has a block structure. We develop a Bayesian fusion estimation method by using the Horseshoe prior to induce a strong shrinkage effect on successive differences in the mean parameters, simultaneously imposing sufficient prior concentration for non-zero values of the same. The proposed method thus facilitates consistent estimation and structure recovery of the signal pieces. We provide theoretical justifications of our approach by deriving posterior convergence rates and establishing selection consistency under suitable assumptions. We also extend our proposed method to signal de-noising over arbitrary graphs and develop efficient computational methods along with providing theoretical guarantees. We demonstrate the superior performance of the Horseshoe based Bayesian fusion estimation method through extensive simulations and two real-life examples on signal de-noising in biological and geophysical applications. We also demonstrate the estimation performance of our method on a real-world large network for the graph signal de-noising problem.

</details>

<details>

<summary>2021-03-30 10:48:14 - An Improved and Extended Bayesian Synthetic Control</summary>

- *Sean Pinkney*

- `2103.16244v1` - [abs](http://arxiv.org/abs/2103.16244v1) - [pdf](http://arxiv.org/pdf/2103.16244v1)

> An improved and extended Bayesian synthetic control model is presented, expanding upon the latent factor model in Tuomaala 2019. The changes we make include 1) standardization of the data prior to model fit - which improves efficiency and generalization across different data sets; 2) adding time varying covariates; 3) adding the ability to have multiple treated units; 4) fitting the latent factors within the Bayesian model; and, 5) a sparsity inducing prior to automatically tune the number of latent factors. We demonstrate the similarity of estimates to two traditional synthetic control studies in Abadie, Diamond, and Hainmueller 2010 and Abadie, Diamond, and Hainmueller 2015 and extend to multiple target series with a new example of estimating digital website visitation from changes in data collection due to digital privacy laws.

</details>

<details>

<summary>2021-03-30 13:14:24 - Federated Generalized Bayesian Learning via Distributed Stein Variational Gradient Descent</summary>

- *Rahif Kassab, Osvaldo Simeone*

- `2009.06419v6` - [abs](http://arxiv.org/abs/2009.06419v6) - [pdf](http://arxiv.org/pdf/2009.06419v6)

> This paper introduces Distributed Stein Variational Gradient Descent (DSVGD), a non-parametric generalized Bayesian inference framework for federated learning. DSVGD maintains a number of non-random and interacting particles at a central server to represent the current iterate of the model global posterior. The particles are iteratively downloaded and updated by one of the agents with the end goal of minimizing the global free energy. By varying the number of particles, DSVGD enables a flexible trade-off between per-iteration communication load and number of communication rounds. DSVGD is shown to compare favorably to benchmark frequentist and Bayesian federated learning strategies, also scheduling a single device per iteration, in terms of accuracy and scalability with respect to the number of agents, while also providing well-calibrated, and hence trustworthy, predictions.

</details>

<details>

<summary>2021-03-30 13:35:14 - Neural Approximate Sufficient Statistics for Implicit Models</summary>

- *Yanzhi Chen, Dinghuai Zhang, Michael Gutmann, Aaron Courville, Zhanxing Zhu*

- `2010.10079v2` - [abs](http://arxiv.org/abs/2010.10079v2) - [pdf](http://arxiv.org/pdf/2010.10079v2)

> We consider the fundamental problem of how to automatically construct summary statistics for implicit generative models where the evaluation of the likelihood function is intractable, but sampling data from the model is possible. The idea is to frame the task of constructing sufficient statistics as learning mutual information maximizing representations of the data with the help of deep neural networks. The infomax learning procedure does not need to estimate any density or density ratio. We apply our approach to both traditional approximate Bayesian computation and recent neural likelihood methods, boosting their performance on a range of tasks.

</details>

<details>

<summary>2021-03-30 16:41:25 - Nonparametric estimations and the diffeological Fisher metric</summary>

- *Hông Vân Lê, Alexey A. Tuzhilin*

- `2011.13418v2` - [abs](http://arxiv.org/abs/2011.13418v2) - [pdf](http://arxiv.org/pdf/2011.13418v2)

> In this paper, first, we survey the concept of diffeological Fisher metric and its naturality, using functorial language of probability morphisms, and slightly extending L\^e's theory in \cite{Le2020} to include weakly $C^k$-diffeological statistical models. Then we introduce the resulting notions of the diffeological Fisher distance, the diffeological Hausdorff--Jeffrey measure and explain their role in classical and Bayesian nonparametric estimation problems in statistics.

</details>

<details>

<summary>2021-03-31 00:52:23 - Double spike Dirichlet priors for structured weighting</summary>

- *Huiming Lin, Meng Li*

- `2007.04387v2` - [abs](http://arxiv.org/abs/2007.04387v2) - [pdf](http://arxiv.org/pdf/2007.04387v2)

> Assigning weights to a large pool of objects is a fundamental task in a wide variety of applications. In this article, we introduce a concept of structured high-dimensional probability simplexes, whose most components are zero or near zero and the remaining ones are close to each other. Such structure is well motivated by 1) high-dimensional weights that are common in modern applications, and 2) ubiquitous examples in which equal weights -- despite their simplicity -- often achieve favorable or even state-of-the-art predictive performances. This particular structure, however, presents unique challenges both computationally and statistically. To address these challenges, we propose a new class of double spike Dirichlet priors to shrink a probability simplex to one with the desired structure. When applied to ensemble learning, such priors lead to a Bayesian method for structured high-dimensional ensembles that is useful for forecast combination and improving random forests, while enabling uncertainty quantification. We design efficient Markov chain Monte Carlo algorithms for easy implementation. Posterior contraction rates are established to provide theoretical support. We demonstrate the wide applicability and competitive performance of the proposed methods through simulations and two real data applications using the European Central Bank Survey of Professional Forecasters dataset and a UCI dataset.

</details>

<details>

<summary>2021-03-31 09:43:56 - BayesAdapter: Being Bayesian, Inexpensively and Reliably, via Bayesian Fine-tuning</summary>

- *Zhijie Deng, Hao Zhang, Xiao Yang, Yinpeng Dong, Jun Zhu*

- `2010.01979v4` - [abs](http://arxiv.org/abs/2010.01979v4) - [pdf](http://arxiv.org/pdf/2010.01979v4)

> Despite their theoretical appealingness, Bayesian neural networks (BNNs) are left behind in real-world adoption, due to persistent concerns on their scalability, accessibility, and reliability. In this work, we aim to relieve these concerns by developing the BayesAdapter framework for learning variational BNNs. In particular, we propose to adapt the pre-trained deterministic NNs to be BNNs via cost-effective Bayesian fine-tuning. To make BayesAdapter more practical, we technically contribute 1) a modularized, user-friendly implementation for the learning of variational BNNs under two representative variational distributions, 2) a generally applicable strategy for reducing the gradient variance in stochastic variational inference, 3) an explanation for the unreliability issue of BNNs' uncertainty estimates, and a corresponding prescription. Through extensive experiments on diverse benchmarks, we show that BayesAdapter can consistently induce posteriors with higher quality than the from-scratch variational inference and other competitive baselines, especially in large-scale settings, yet significantly reducing training overheads.

</details>

<details>

<summary>2021-03-31 10:01:55 - pivmet: Pivotal Methods for Bayesian Relabelling and k-Means Clustering</summary>

- *Leonardo Egidi, Roberta Pappadà, Francesco Pauli, Nicola Torelli*

- `2103.16948v1` - [abs](http://arxiv.org/abs/2103.16948v1) - [pdf](http://arxiv.org/pdf/2103.16948v1)

> The identification of groups' prototypes, i.e. elements of a dataset that represent different groups of data points, may be relevant to the tasks of clustering, classification and mixture modeling. The R package pivmet presented in this paper includes different methods for extracting pivotal units from a dataset. One of the main applications of pivotal methods is a Markov Chain Monte Carlo (MCMC) relabelling procedure to solve the label switching in Bayesian estimation of mixture models. Each method returns posterior estimates, and a set of graphical tools for visualizing the output. The package offers JAGS and Stan sampling procedures for Gaussian mixtures, and allows for user-defined priors' parameters. The package also provides functions to perform consensus clustering based on pivotal units, which may allow to improve classical techniques (e.g. k-means) by means of a careful seeding. The paper provides examples of applications to both real and simulated datasets.

</details>

<details>

<summary>2021-03-31 19:13:16 - Sample-Efficient Neural Architecture Search by Learning Action Space</summary>

- *Linnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, Yuandong Tian*

- `1906.06832v2` - [abs](http://arxiv.org/abs/1906.06832v2) - [pdf](http://arxiv.org/pdf/1906.06832v2)

> Neural Architecture Search (NAS) has emerged as a promising technique for automatic neural network design. However, existing MCTS based NAS approaches often utilize manually designed action space, which is not directly related to the performance metric to be optimized (e.g., accuracy), leading to sample-inefficient explorations of architectures. To improve the sample efficiency, this paper proposes Latent Action Neural Architecture Search (LaNAS), which learns actions to recursively partition the search space into good or bad regions that contain networks with similar performance metrics. During the search phase, as different action sequences lead to regions with different performance, the search efficiency can be significantly improved by biasing towards the good regions. On three NAS tasks, empirical results demonstrate that LaNAS is at least an order more sample efficient than baseline methods including evolutionary algorithms, Bayesian optimizations, and random search. When applied in practice, both one-shot and regular LaNAS consistently outperform existing results. Particularly, LaNAS achieves 99.0% accuracy on CIFAR-10 and 80.8% top1 accuracy at 600 MFLOPS on ImageNet in only 800 samples, significantly outperforming AmoebaNet with 33x fewer samples. Our code is publicly available at https://github.com/facebookresearch/LaMCTS.

</details>

<details>

<summary>2021-03-31 20:45:30 - Approximate Cross-validated Mean Estimates for Bayesian Hierarchical Regression Models</summary>

- *Amy X. Zhang, Le Bao, Michael J. Daniels*

- `2011.14238v2` - [abs](http://arxiv.org/abs/2011.14238v2) - [pdf](http://arxiv.org/pdf/2011.14238v2)

> We introduce a novel procedure for obtaining cross-validated predictive estimates for Bayesian hierarchical regression models (BHRMs). Bayesian hierarchical models are popular for their ability to model complex dependence structures and provide probabilistic uncertainty estimates, but can be computationally expensive to run. Cross-validation (CV) is therefore not a common practice to evaluate the predictive performance of BHRMs. Our method circumvents the need to re-run computationally costly estimation methods for each cross-validation fold and makes CV more feasible for large BHRMs. By conditioning on the variance-covariance parameters, we shift the CV problem from probability-based sampling to a simple and familiar optimization problem. In many cases, this produces estimates which are equivalent to full CV. We provide theoretical results and demonstrate its efficacy on publicly available data and in simulations.

</details>


## 2021-04

<details>

<summary>2021-04-01 07:12:50 - Edge-promoting adaptive Bayesian experimental design for X-ray imaging</summary>

- *Tapio Helin, Nuutti Hyvönen, Juha-Pekka Puska*

- `2104.00301v1` - [abs](http://arxiv.org/abs/2104.00301v1) - [pdf](http://arxiv.org/pdf/2104.00301v1)

> This work considers sequential edge-promoting Bayesian experimental design for (discretized) linear inverse problems, exemplified by X-ray tomography. The process of computing a total variation type reconstruction of the absorption inside the imaged body via lagged diffusivity iteration is interpreted in the Bayesian framework. Assuming a Gaussian additive noise model, this leads to an approximate Gaussian posterior with a covariance structure that contains information on the location of edges in the posterior mean. The next projection geometry is then chosen through A-optimal Bayesian design, which corresponds to minimizing the trace of the updated posterior covariance matrix that accounts for the new projection. Two and three-dimensional numerical examples based on simulated data demonstrate the functionality of the introduced approach.

</details>

<details>

<summary>2021-04-01 07:52:28 - Fast cross-validation for multi-penalty ridge regression</summary>

- *Mark A. van de Wiel, Mirrelijn M. van Nee, Armin Rauschenberger*

- `2005.09301v2` - [abs](http://arxiv.org/abs/2005.09301v2) - [pdf](http://arxiv.org/pdf/2005.09301v2)

> High-dimensional prediction with multiple data types needs to account for potentially strong differences in predictive signal. Ridge regression is a simple model for high-dimensional data that has challenged the predictive performance of many more complex models and learners, and that allows inclusion of data type specific penalties. The largest challenge for multi-penalty ridge is to optimize these penalties efficiently in a cross-validation (CV) setting, in particular for GLM and Cox ridge regression, which require an additional estimation loop by iterative weighted least squares (IWLS). Our main contribution is a computationally very efficient formula for the multi-penalty, sample-weighted hat-matrix, as used in the IWLS algorithm. As a result, nearly all computations are in low-dimensional space, rendering a speed-up of several orders of magnitude. We developed a flexible framework that facilitates multiple types of response, unpenalized covariates, several performance criteria and repeated CV. Extensions to paired and preferential data types are included and illustrated on several cancer genomics survival prediction problems. Moreover, we present similar computational shortcuts for maximum marginal likelihood and Bayesian probit regression. The corresponding R-package, multiridge, serves as a versatile standalone tool, but also as a fast benchmark for other more complex models and multi-view learners.

</details>

<details>

<summary>2021-04-01 09:53:12 - Robust M-Estimation Based Bayesian Cluster Enumeration for Real Elliptically Symmetric Distributions</summary>

- *Christian A. Schroth, Michael Muma*

- `2005.01404v3` - [abs](http://arxiv.org/abs/2005.01404v3) - [pdf](http://arxiv.org/pdf/2005.01404v3)

> Robustly determining the optimal number of clusters in a data set is an essential factor in a wide range of applications. Cluster enumeration becomes challenging when the true underlying structure in the observed data is corrupted by heavy-tailed noise and outliers. Recently, Bayesian cluster enumeration criteria have been derived by formulating cluster enumeration as maximization of the posterior probability of candidate models. This article generalizes robust Bayesian cluster enumeration so that it can be used with any arbitrary Real Elliptically Symmetric (RES) distributed mixture model. Our framework also covers the case of M-estimators that allow for mixture models, which are decoupled from a specific probability distribution. Examples of Huber's and Tukey's M-estimators are discussed. We derive a robust criterion for data sets with finite sample size, and also provide an asymptotic approximation to reduce the computational cost at large sample sizes. The algorithms are applied to simulated and real-world data sets, including radar-based person identification, and show a significant robustness improvement in comparison to existing methods.

</details>

<details>

<summary>2021-04-01 15:36:01 - Slamming the sham: A Bayesian model for adaptive adjustment with noisy control data</summary>

- *Andrew Gelman, Matthijs Vákár*

- `1905.09693v2` - [abs](http://arxiv.org/abs/1905.09693v2) - [pdf](http://arxiv.org/pdf/1905.09693v2)

> It is not always clear how to adjust for control data in causal inference, balancing the goals of reducing bias and variance. We show how, in a setting with repeated experiments, Bayesian hierarchical modeling yields an adaptive procedure that uses the data to determine how much adjustment to perform. The result is a novel analysis with increased statistical efficiency compared to the default analysis based on difference estimates. We demonstrate this procedure on two real examples, as well as on a series of simulated datasets. We show that the increased efficiency can have real-world consequences in terms of the conclusions that can be drawn from the experiments. We also discuss the relevance of this work to causal inference and statistical design and analysis more generally.

</details>

<details>

<summary>2021-04-01 17:42:15 - Bayesian Functional Principal Components Analysis via Variational Message Passing</summary>

- *Tui H. Nolan, Jeff Goldsmith, David Ruppert*

- `2104.00645v1` - [abs](http://arxiv.org/abs/2104.00645v1) - [pdf](http://arxiv.org/pdf/2104.00645v1)

> Functional principal components analysis is a popular tool for inference on functional data. Standard approaches rely on an eigendecomposition of a smoothed covariance surface in order to extract the orthonormal functions representing the major modes of variation. This approach can be a computationally intensive procedure, especially in the presence of large datasets with irregular observations. In this article, we develop a Bayesian approach, which aims to determine the Karhunen-Lo\`eve decomposition directly without the need to smooth and estimate a covariance surface. More specifically, we develop a variational Bayesian algorithm via message passing over a factor graph, which is more commonly referred to as variational message passing. Message passing algorithms are a powerful tool for compartmentalizing the algebra and coding required for inference in hierarchical statistical models. Recently, there has been much focus on formulating variational inference algorithms in the message passing framework because it removes the need for rederiving approximate posterior density functions if there is a change to the model. Instead, model changes are handled by changing specific computational units, known as fragments, within the factor graph. We extend the notion of variational message passing to functional principal components analysis. Indeed, this is the first article to address a functional data model via variational message passing. Our approach introduces two new fragments that are necessary for Bayesian functional principal components analysis. We present the computational details, a set of simulations for assessing accuracy and speed and an application to United States temperature data.

</details>

<details>

<summary>2021-04-01 17:46:11 - Preferential Bayesian optimisation with Skew Gaussian Processes</summary>

- *Alessio Benavoli, Dario Azzimonti, Dario Piga*

- `2008.06677v3` - [abs](http://arxiv.org/abs/2008.06677v3) - [pdf](http://arxiv.org/pdf/2008.06677v3)

> Preferential Bayesian optimisation (PBO) deals with optimisation problems where the objective function can only be accessed via preference judgments, such as "this is better than that" between two candidate solutions (like in A/B tests or recommender systems). The state-of-the-art approach to PBO uses a Gaussian process to model the preference function and a Bernoulli likelihood to model the observed pairwise comparisons. Laplace's method is then employed to compute posterior inferences and, in particular, to build an appropriate acquisition function. In this paper, we prove that the true posterior distribution of the preference function is a Skew Gaussian Process (SkewGP), with highly skewed pairwise marginals and, thus, show that Laplace's method usually provides a very poor approximation. We then derive an efficient method to compute the exact SkewGP posterior and use it as surrogate model for PBO employing standard acquisition functions (Upper Credible Bound, etc.). We illustrate the benefits of our exact PBO-SkewGP in a variety of experiments, by showing that it consistently outperforms PBO based on Laplace's approximation both in terms of convergence speed and computational time. We also show that our framework can be extended to deal with mixed preferential-categorical BO, where binary judgments (valid or non-valid) together with preference judgments are available.

</details>

<details>

<summary>2021-04-02 12:36:29 - Singular Value Shrinkage Priors for Bayesian Prediction</summary>

- *Takeru Matsuda, Fumiyasu Komaki*

- `1408.2951v3` - [abs](http://arxiv.org/abs/1408.2951v3) - [pdf](http://arxiv.org/pdf/1408.2951v3)

> We develop singular value shrinkage priors for the mean matrix parameters in the matrix-variate normal model with known covariance matrices. Our priors are superharmonic and put more weight on matrices with smaller singular values. They are a natural generalization of the Stein prior. Bayes estimators and Bayesian predictive densities based on our priors are minimax and dominate those based on the uniform prior in finite samples. In particular, our priors work well when the true value of the parameter has low rank.

</details>

<details>

<summary>2021-04-02 15:37:38 - Improved Max-value Entropy Search for Multi-objective Bayesian Optimization with Constraints</summary>

- *Daniel Fernández-Sánchez, Eduardo C. Garrido-Merchán, Daniel Hernández-Lobato*

- `2011.01150v2` - [abs](http://arxiv.org/abs/2011.01150v2) - [pdf](http://arxiv.org/pdf/2011.01150v2)

> We present MESMOC+, an improved version of Max-value Entropy search for Multi-Objective Bayesian optimization with Constraints (MESMOC). MESMOC+ can be used to solve constrained multi-objective problems when the objectives and the constraints are expensive to evaluate. MESMOC+ works by minimizing the entropy of the solution of the optimization problem in function space, i.e., the Pareto frontier, to guide the search for the optimum. The cost of MESMOC+ is linear in the number of objectives and constraints. Furthermore, it is often significantly smaller than the cost of alternative methods based on minimizing the entropy of the Pareto set. The reason for this is that it is easier to approximate the required computations in MESMOC+. Moreover, MESMOC+'s acquisition function is expressed as the sum of one acquisition per each black-box (objective or constraint). Thus, it can be used in a decoupled evaluation setting in which one chooses not only the next input location to evaluate, but also which black-box to evaluate there. We compare MESMOC+ with related methods in synthetic and real optimization problems. These experiments show that the entropy estimation provided by MESMOC+ is more accurate than that of previous methods. This leads to better optimization results. MESMOC+ is also competitive with other information-based methods for constrained multi-objective Bayesian optimization, but it is significantly faster.

</details>

<details>

<summary>2021-04-02 16:31:15 - Bayesian estimation of nonlinear Hawkes process</summary>

- *Deborah Sulem, Vincent Rivoirard, Judith Rousseau*

- `2103.17164v2` - [abs](http://arxiv.org/abs/2103.17164v2) - [pdf](http://arxiv.org/pdf/2103.17164v2)

> Multivariate point processes are widely applied to model event-type data such as natural disasters, online message exchanges, financial transactions or neuronal spike trains. One very popular point process model in which the probability of occurrences of new events depend on the past of the process is the Hawkes process. In this work we consider the nonlinear Hawkes process, which notably models excitation and inhibition phenomena between dimensions of the process. In a nonparametric Bayesian estimation framework, we obtain concentration rates of the posterior distribution on the parameters, under mild assumptions on the prior distribution and the model. These results also lead to convergence rates of Bayesian estimators. Another object of interest in event-data modelling is to recover the graph of interaction - or Granger connectivity graph - of the phenomenon. We provide consistency guarantees on Bayesian methods for estimating this quantity; in particular, we prove that the posterior distribution is consistent on the graph adjacency matrix of the process, as well as a Bayesian estimator based on an adequate loss function.

</details>

<details>

<summary>2021-04-03 16:33:25 - f-Divergence Variational Inference</summary>

- *Neng Wan, Dapeng Li, Naira Hovakimyan*

- `2009.13093v4` - [abs](http://arxiv.org/abs/2009.13093v4) - [pdf](http://arxiv.org/pdf/2009.13093v4)

> This paper introduces the $f$-divergence variational inference ($f$-VI) that generalizes variational inference to all $f$-divergences. Initiated from minimizing a crafty surrogate $f$-divergence that shares the statistical consistency with the $f$-divergence, the $f$-VI framework not only unifies a number of existing VI methods, e.g. Kullback-Leibler VI, R\'{e}nyi's $\alpha$-VI, and $\chi$-VI, but offers a standardized toolkit for VI subject to arbitrary divergences from $f$-divergence family. A general $f$-variational bound is derived and provides a sandwich estimate of marginal likelihood (or evidence). The development of the $f$-VI unfolds with a stochastic optimization scheme that utilizes the reparameterization trick, importance weighting and Monte Carlo approximation; a mean-field approximation scheme that generalizes the well-known coordinate ascent variational inference (CAVI) is also proposed for $f$-VI. Empirical examples, including variational autoencoders and Bayesian neural networks, are provided to demonstrate the effectiveness and the wide applicability of $f$-VI.

</details>

<details>

<summary>2021-04-05 14:02:00 - Adjusted composite likelihood for robust Bayesian meta-analysis</summary>

- *Michele Lambardi di San Miniato, Nicola Sartori*

- `2104.01920v1` - [abs](http://arxiv.org/abs/2104.01920v1) - [pdf](http://arxiv.org/pdf/2104.01920v1)

> A composite likelihood is a non-genuine likelihood function that allows to make inference on limited aspects of a model, such as marginal or conditional distributions. Composite likelihoods are not proper likelihoods and need therefore calibration for their use in inference, from both a frequentist and a Bayesian perspective. The maximizer to the composite likelihood can serve as an estimator and its variance is assessed by means of a suitably defined sandwich matrix. In the Bayesian setting, the composite likelihood can be adjusted by means of magnitude and curvature methods. Magnitude methods imply raising the likelihood to a constant, while curvature methods imply evaluating the likelihood at a different point by translating, rescaling and rotating the parameter vector. Some authors argue that curvature methods are more reliable in general, but others proved that magnitude methods are sufficient to recover, for instance, the null distribution of a test statistic. We propose a simple calibration for the marginal posterior distribution of a scalar parameter of interest which is invariant to monotonic and smooth transformations. This can be enough for instance in medical statistics, where a single scalar effect measure is often the target.

</details>

<details>

<summary>2021-04-05 16:51:19 - Nearly Consistent Finite Particle Estimates in Streaming Importance Sampling</summary>

- *Alec Koppel, Amrit Singh Bedi, Brian M. Sadler, Victor Elvira*

- `1909.10279v2` - [abs](http://arxiv.org/abs/1909.10279v2) - [pdf](http://arxiv.org/pdf/1909.10279v2)

> In Bayesian inference, we seek to compute information about random variables such as moments or quantiles on the basis of {available data} and prior information. When the distribution of random variables is {intractable}, Monte Carlo (MC) sampling is usually required. {Importance sampling is a standard MC tool that approximates this unavailable distribution with a set of weighted samples.} This procedure is asymptotically consistent as the number of MC samples (particles) go to infinity. However, retaining infinitely many particles is intractable. Thus, we propose a way to only keep a \emph{finite representative subset} of particles and their augmented importance weights that is \emph{nearly consistent}. To do so in {an online manner}, we (1) embed the posterior density estimate in a reproducing kernel Hilbert space (RKHS) through its kernel mean embedding; and (2) sequentially project this RKHS element onto a lower-dimensional subspace in RKHS using the maximum mean discrepancy, an integral probability metric. Theoretically, we establish that this scheme results in a bias determined by a compression parameter, which yields a tunable tradeoff between consistency and memory. In experiments, we observe the compressed estimates achieve comparable performance to the dense ones with substantial reductions in representational complexity.

</details>

<details>

<summary>2021-04-05 16:58:19 - Identification and Estimation in Many-to-one Two-sided Matching without Transfers</summary>

- *YingHua He, Shruti Sinha, Xiaoting Sun*

- `2104.02009v1` - [abs](http://arxiv.org/abs/2104.02009v1) - [pdf](http://arxiv.org/pdf/2104.02009v1)

> In a setting of many-to-one two-sided matching with non-transferable utilities, e.g., college admissions, we study conditions under which preferences of both sides are identified with data on one single market. The main challenge is that every agent's actual choice set is unobservable to the researcher. Assuming that the observed matching is stable, we show nonparametric and semiparametric identification of preferences of both sides under appropriate exclusion restrictions. Our identification arguments are constructive and thus directly provide a semiparametric estimator. In Monte Carlo simulations, the estimator can perform well but suffers from the curse of dimensionality. We thus adopt a parametric model and estimate it by a Bayesian approach with a Gibbs sampler, which works well in simulations. Finally, we apply our method to school admissions in Chile and conduct a counterfactual analysis of an affirmative action policy.

</details>

<details>

<summary>2021-04-05 18:19:49 - Objective Bayesian meta-analysis based on generalized multivariate random effects model</summary>

- *Olha Bodnar, Taras Bodnar*

- `2104.02105v1` - [abs](http://arxiv.org/abs/2104.02105v1) - [pdf](http://arxiv.org/pdf/2104.02105v1)

> Objective Bayesian inference procedures are derived for the parameters of the multivariate random effects model generalized to elliptically contoured distributions. The posterior for the overall mean vector and the between-study covariance matrix is deduced by assigning two noninformative priors to the model parameter, namely the Berger and Bernardo reference prior and the Jeffreys prior, whose analytical expressions are obtained under weak distributional assumptions. It is shown that the only condition needed for the posterior to be proper is that the sample size is larger than the dimension of the data-generating model, independently of the class of elliptically contoured distributions used in the definition of the generalized multivariate random effects model. The theoretical findings of the paper are applied to real data consisting of ten studies about the effectiveness of hypertension treatment for reducing blood pressure where the treatment effects on both the systolic blood pressure and diastolic blood pressure are investigated.

</details>

<details>

<summary>2021-04-06 05:37:04 - Manifold Optimization Assisted Gaussian Variational Approximation</summary>

- *Bingxin Zhou, Junbin Gao, Minh-Ngoc Tran, Richard Gerlach*

- `1902.03718v2` - [abs](http://arxiv.org/abs/1902.03718v2) - [pdf](http://arxiv.org/pdf/1902.03718v2)

> Gaussian variational approximation is a popular methodology to approximate posterior distributions in Bayesian inference especially in high dimensional and large data settings. To control the computational cost while being able to capture the correlations among the variables, the low rank plus diagonal structure was introduced in the previous literature for the Gaussian covariance matrix. For a specific Bayesian learning task, the uniqueness of the solution is usually ensured by imposing stringent constraints on the parameterized covariance matrix, which could break down during the optimization process. In this paper, we consider two special covariance structures by applying the Stiefel manifold and Grassmann manifold constraints, to address the optimization difficulty in such factorization architectures. To speed up the updating process with minimum hyperparameter-tuning efforts, we design two new schemes of Riemannian stochastic gradient descent methods and compare them with other existing methods of optimizing on manifolds. In addition to fixing the identification issue, results from both simulation and empirical experiments prove the ability of the proposed methods of obtaining competitive accuracy and comparable converge speed in both high-dimensional and large-scale learning tasks.

</details>

<details>

<summary>2021-04-06 09:19:42 - Forecasting intra-individual changes of affective states taking into account inter-individual differences using intensive longitudinal data from a university student drop out study in math</summary>

- *Augustin Kelava, Pascal Kilian, Judith Glaesser, Samuel Merk, Holger Brandt*

- `2104.02383v1` - [abs](http://arxiv.org/abs/2104.02383v1) - [pdf](http://arxiv.org/pdf/2104.02383v1)

> The longitudinal process that leads to university student drop out in STEM subjects can be described by referring to a) inter-individual differences (e.g., cognitive abilities) as well as b) intra-individual changes (e.g., affective states), c) (unobserved) heterogeneity of trajectories, and d) time-dependent variables. Large dynamic latent variable model frameworks for intensive longitudinal data (ILD) have been proposed which are (partially) capable of simultaneously separating the complex data structures (e.g., DLCA; Asparouhov, Hamaker, & Muth\'en, 2017; DSEM; Asparouhov, Hamaker, & Muth\'en, 2018; NDLC-SEM, Kelava & Brandt, 2019). From a methodological perspective, forecasting in dynamic frameworks allowing for real-time inferences on latent or observed variables based on ongoing data collection has not been an extensive research topic. From a practical perspective, there has been no empirical study on student drop out in math that integrates ILD, dynamic frameworks, and forecasting of critical states of the individuals allowing for real-time interventions. In this paper, we show how Bayesian forecasting of multivariate intra-individual variables and time-dependent class membership of individuals (affective states) can be performed in these dynamic frameworks. To illustrate our approach, we use an empirical example where we apply forecasting methodology to ILD from a large university student drop out study in math with multivariate observations collected over 50 measurement occasions from multiple students (N = 122). More specifically, we forecast emotions and behavior related to drop out. This allows us to model (i) just-in-time interventions, (ii) detection of heterogeneity in trajectories, and (iii) prediction of emerging dynamic states (e.g. critical stress levels or pre-decisional states).

</details>

<details>

<summary>2021-04-06 10:05:42 - Revisiting the empirical fundamental relationship of traffic flow for highways using a causal econometric approach</summary>

- *Anupriya, Daniel J. Graham, Daniel Hörcher, Prateek Bansal*

- `2104.02399v1` - [abs](http://arxiv.org/abs/2104.02399v1) - [pdf](http://arxiv.org/pdf/2104.02399v1)

> The fundamental relationship of traffic flow is empirically estimated by fitting a regression curve to a cloud of observations of traffic variables. Such estimates, however, may suffer from the confounding/endogeneity bias due to omitted variables such as driving behaviour and weather. To this end, this paper adopts a causal approach to obtain an unbiased estimate of the fundamental flow-density relationship using traffic detector data. In particular, we apply a Bayesian non-parametric spline-based regression approach with instrumental variables to adjust for the aforementioned confounding bias. The proposed approach is benchmarked against standard curve-fitting methods in estimating the flow-density relationship for three highway bottlenecks in the United States. Our empirical results suggest that the saturated (or hypercongested) regime of the estimated flow-density relationship using correlational curve fitting methods may be severely biased, which in turn leads to biased estimates of important traffic control inputs such as capacity and capacity-drop. We emphasise that our causal approach is based on the physical laws of vehicle movement in a traffic stream as opposed to a demand-supply framework adopted in the economics literature. By doing so, we also aim to conciliate the engineering and economics approaches to this empirical problem. Our results, thus, have important implications both for traffic engineers and transport economists.

</details>

<details>

<summary>2021-04-06 11:29:49 - Bayesian prediction of jumps in large panels of time series data</summary>

- *Angelos Alexopoulos, Petros Dellaportas, Omiros Papaspiliopoulos*

- `1904.05312v4` - [abs](http://arxiv.org/abs/1904.05312v4) - [pdf](http://arxiv.org/pdf/1904.05312v4)

> We take a new look at the problem of disentangling the volatility and jumps processes of daily stock returns. We first provide a computational framework for the univariate stochastic volatility model with Poisson-driven jumps that offers a competitive inference alternative to the existing tools. This methodology is then extended to a large set of stocks for which we assume that their unobserved jump intensities co-evolve in time through a dynamic factor model. To evaluate the proposed modelling approach we conduct out-of-sample forecasts and we compare the posterior predictive distributions obtained from the different models. We provide evidence that joint modelling of jumps improves the predictive ability of the stochastic volatility models.

</details>

<details>

<summary>2021-04-06 11:59:53 - Statistical Network Analysis with Bergm</summary>

- *Alberto Caimo, Lampros Bouranis, Robert Krause, Nial Friel*

- `2104.02444v1` - [abs](http://arxiv.org/abs/2104.02444v1) - [pdf](http://arxiv.org/pdf/2104.02444v1)

> Recent advances in computational methods for intractable models have made network data increasingly amenable to statistical analysis. Exponential random graph models (ERGMs) emerged as one of the main families of models capable of capturing the complex dependence structure of network data in a wide range of applied contexts. The Bergm package for R has become a popular package to carry out Bayesian parameter inference, missing data imputation, model selection and goodness-of-fit diagnostics for ERGMs. Over the last few years, the package has been considerably improved in terms of efficiency by adopting some of the state-of-the-art Bayesian computational methods for doubly-intractable distributions. Recently, version 5 of the package has been made available on CRAN having undergone a substantial makeover, which has made it more accessible and easy to use for practitioners. New functions include data augmentation procedures based on the approximate exchange algorithm for dealing with missing data, adjusted pseudo-likelihood and pseudo-posterior procedures, which allow for fast approximate inference of the ERGM parameter posterior and model evidence for networks on several thousands nodes.

</details>

<details>

<summary>2021-04-06 12:50:32 - Approximate Bayesian inference for analysis of spatio-temporal flood frequency data</summary>

- *Árni V. Johannesson, Stefan Siegert, Raphaël Huser, Haakon Bakka, Birgir Hrafnkelsson*

- `1907.04763v3` - [abs](http://arxiv.org/abs/1907.04763v3) - [pdf](http://arxiv.org/pdf/1907.04763v3)

> Extreme floods cause casualties, and widespread damage to property and vital civil infrastructure. We here propose a Bayesian approach for predicting extreme floods using the generalized extreme-value (GEV) distribution within gauged and ungauged catchments. A major methodological challenge is to find a suitable parametrization for the GEV distribution when covariates or latent spatial effects are involved. Other challenges involve balancing model complexity and parsimony using an appropriate model selection procedure, and making inference using a reliable and computationally efficient approach. Our approach relies on a latent Gaussian modeling framework with a novel multivariate link function designed to separate the interpretation of the parameters at the latent level and to avoid unreasonable estimates of the shape and time trend parameters. Structured additive regression models are proposed for the four parameters at the latent level. For computational efficiency with large datasets and richly parametrized models, we exploit an accurate and fast approximate Bayesian inference approach. We applied our proposed methodology to annual peak river flow data from 554 catchments across the United Kingdom (UK). Our model performed well in terms of flood predictions for both gauged and ungauged catchments. The results show that the spatial model components for the transformed location and scale parameters, and the time trend, are all important. Posterior estimates of the time trend parameters correspond to an average increase of about $1.5\%$ per decade and reveal a spatial structure across the UK. To estimate return levels for spatial aggregates, we further develop a novel copula-based post-processing approach of posterior predictive samples, in order to mitigate the effect of the conditional independence assumption at the data level, and we show that our approach provides accurate results.

</details>

<details>

<summary>2021-04-06 13:28:04 - Exploring Topic-Metadata Relationships with the STM: A Bayesian Approach</summary>

- *P. Schulze, S. Wiegrebe, P. W. Thurner, C. Heumann, M. Aßenmacher, S. Wankmüller*

- `2104.02496v1` - [abs](http://arxiv.org/abs/2104.02496v1) - [pdf](http://arxiv.org/pdf/2104.02496v1)

> Topic models such as the Structural Topic Model (STM) estimate latent topical clusters within text. An important step in many topic modeling applications is to explore relationships between the discovered topical structure and metadata associated with the text documents. Methods used to estimate such relationships must take into account that the topical structure is not directly observed, but instead being estimated itself. The authors of the STM, for instance, perform repeated OLS regressions of sampled topic proportions on metadata covariates by using a Monte Carlo sampling technique known as the method of composition. In this paper, we propose two improvements: first, we replace OLS with more appropriate Beta regression. Second, we suggest a fully Bayesian approach instead of the current blending of frequentist and Bayesian methods. We demonstrate our improved methodology by exploring relationships between Twitter posts by German members of parliament (MPs) and different metadata covariates.

</details>

<details>

<summary>2021-04-07 05:39:56 - Modeling a sequence of multinomial data with randomly varying probabilities</summary>

- *Soudeep Deb, Rishideep Roy, Shubhabrata Das*

- `2104.02924v1` - [abs](http://arxiv.org/abs/2104.02924v1) - [pdf](http://arxiv.org/pdf/2104.02924v1)

> We consider a sequence of variables having multinomial distribution with the number of trials corresponding to these variables being large and possibly different. The multinomial probabilities of the categories are assumed to vary randomly depending on batches. The proposed framework is interesting from the perspective of various applications in practice such as predicting the winner of an election, forecasting the market share of different brands etc. In this work, first we derive sufficient conditions of asymptotic normality of the estimates of the multinomial cell probabilities, and corresponding suitable transformations. Then, we consider a Bayesian setting to implement our model. We consider hierarchical priors using multivariate normal and inverse Wishart distributions, and establish the posterior consistency. Based on this result and following appropriate Gibbs sampling algorithms, we can infer about aggregate data. The methodology is illustrated in detail with two real life applications, in the contexts of political election and sales forecasting. Additional insights of effectiveness are also derived through a simulation study.

</details>

<details>

<summary>2021-04-07 08:37:40 - RADIOHEAD: Radiogenomic Analysis Incorporating Tumor Heterogeneity in Imaging Through Densities</summary>

- *Shariq Mohammed, Karthik Bharath, Sebastian Kurtek, Arvind Rao, Veerabhadran Baladandayuthapani*

- `2104.00510v2` - [abs](http://arxiv.org/abs/2104.00510v2) - [pdf](http://arxiv.org/pdf/2104.00510v2)

> Recent technological advancements have enabled detailed investigation of associations between the molecular architecture and tumor heterogeneity, through multi-source integration of radiological imaging and genomic (radiogenomic) data. In this paper, we integrate and harness radiogenomic data in patients with lower grade gliomas (LGG), a type of brain cancer, in order to develop a regression framework called RADIOHEAD (RADIOgenomic analysis incorporating tumor HEterogeneity in imAging through Densities) to identify radiogenomic associations. Imaging data is represented through voxel intensity probability density functions of tumor sub-regions obtained from multimodal magnetic resonance imaging, and genomic data through molecular signatures in the form of pathway enrichment scores corresponding to their gene expression profiles. Employing a Riemannian-geometric framework for principal component analysis on the set of probability densities functions, we map each probability density to a vector of principal component scores, which are then included as predictors in a Bayesian regression model with the pathway enrichment scores as the response. Variable selection compatible with the grouping structure amongst the predictors induced through the tumor sub-regions is carried out under a group spike-and-slab prior. A Bayesian false discovery rate mechanism is then used to infer significant associations based on the posterior distribution of the regression coefficients. Our analyses reveal several pathways relevant to LGG etiology (such as synaptic transmission, nerve impulse and neurotransmitter pathways), to have significant associations with the corresponding imaging-based predictors.

</details>

<details>

<summary>2021-04-07 13:27:40 - UltraNest -- a robust, general purpose Bayesian inference engine</summary>

- *Johannes Buchner*

- `2101.09604v2` - [abs](http://arxiv.org/abs/2101.09604v2) - [pdf](http://arxiv.org/pdf/2101.09604v2)

> UltraNest is a general-purpose Bayesian inference package for parameter estimation and model comparison. It allows fitting arbitrary models specified as likelihood functions written in Python, C, C++, Fortran, Julia or R. With a focus on correctness and speed (in that order), UltraNest is especially useful for multi-modal or non-Gaussian parameter spaces, computational expensive models, in robust pipelines. Parallelisation to computing clusters and resuming incomplete runs is available.

</details>

<details>

<summary>2021-04-07 15:34:26 - Inference for partially observed Riemannian Ornstein--Uhlenbeck diffusions of covariance matrices</summary>

- *Mai Ngoc Bui, Yvo Pokern, Petros Dellaportas*

- `2104.03193v1` - [abs](http://arxiv.org/abs/2104.03193v1) - [pdf](http://arxiv.org/pdf/2104.03193v1)

> We construct a generalization of the Ornstein--Uhlenbeck processes on the cone of covariance matrices endowed with the Log-Euclidean and the Affine-Invariant metrics. Our development exploits the Riemannian geometric structure of symmetric positive definite matrices viewed as a differential manifold. We then provide Bayesian inference for discretely observed diffusion processes of covariance matrices based on an MCMC algorithm built with the help of a novel diffusion bridge sampler accounting for the geometric structure. Our proposed algorithm is illustrated with a real data financial application.

</details>

<details>

<summary>2021-04-07 23:56:02 - Informative Path Planning for Extreme Anomaly Detection in Environment Exploration and Monitoring</summary>

- *Antoine Blanchard, Themistoklis Sapsis*

- `2005.10040v3` - [abs](http://arxiv.org/abs/2005.10040v3) - [pdf](http://arxiv.org/pdf/2005.10040v3)

> An unmanned autonomous vehicle (UAV) is sent on a mission to explore and reconstruct an unknown environment from a series of measurements collected by Bayesian optimization. The success of the mission is judged by the UAV's ability to faithfully reconstruct any anomalous features present in the environment, with emphasis on the extremes (e.g., extreme topographic depressions or abnormal chemical concentrations). We show that the criteria commonly used for determining which locations the UAV should visit are ill-suited for this task. We introduce a number of novel criteria that guide the UAV towards regions of strong anomalies by leveraging previously collected information in a mathematically elegant and computationally tractable manner. We demonstrate superiority of the proposed approach in several applications, including reconstruction of seafloor topography from real-world bathymetry data, as well as tracking of dynamic anomalies. A particularly attractive property of our approach is its ability to overcome adversarial conditions, that is, situations in which prior beliefs about the locations of the extremes are imprecise or erroneous.

</details>

<details>

<summary>2021-04-08 00:07:48 - Synthetic Likelihood in Misspecified Models: Consequences and Corrections</summary>

- *David T. Frazier, Christopher Drovandi, David J. Nott*

- `2104.03436v1` - [abs](http://arxiv.org/abs/2104.03436v1) - [pdf](http://arxiv.org/pdf/2104.03436v1)

> We analyse the behaviour of the synthetic likelihood (SL) method when the model generating the simulated data differs from the actual data generating process. One of the most common methods to obtain SL-based inferences is via the Bayesian posterior distribution, with this method often referred to as Bayesian synthetic likelihood (BSL). We demonstrate that when the model is misspecified, the BSL posterior can be poorly behaved, placing significant posterior mass on values of the model parameters that do not represent the true features observed in the data. Theoretical results demonstrate that in misspecified models the BSL posterior can display a wide range of behaviours depending on the level of model misspecification, including being asymptotically non-Gaussian. Our results suggest that a recently proposed robust BSL approach can ameliorate this behavior and deliver reliable posterior inference under model misspecification. We document all theoretical results using a simple running example.

</details>

<details>

<summary>2021-04-08 10:39:22 - Reflection on modern methods: competing risks versus multi-state models</summary>

- *Fran Llopis-Cardona, Carmen Armero, Gabriel Sanfélix-Gimeno*

- `2104.03671v1` - [abs](http://arxiv.org/abs/2104.03671v1) - [pdf](http://arxiv.org/pdf/2104.03671v1)

> Survival competing risks models are very useful for studying the incidence of diseases whose occurrence competes with other possible diseases or health conditions. These models perform properly when working with terminal events, such as death, that imply the conclusion of the corresponding study. But they do not allow the treatment of scenarios with non-terminal competing events that may occur sequentially. Multi-state models are complex survival models. They focus on pathways defined by the temporal and sequential occurrence of numerous events of interest and thus they are suitable for connecting competing non-terminal events as well as to manage other survival scenarios with higher complexity. We discuss competing risks within the framework of multi-state models and clarify the usefulness of both models for analysing epidemiological data. We highlight the power of multi-state models through a real-world study of recurrent hip fracture from Bayesian inferential methodology.

</details>

<details>

<summary>2021-04-08 12:10:32 - Bayesian analysis of seasonally cointegrated VAR model</summary>

- *Justyna Wróblewska*

- `2012.14820v2` - [abs](http://arxiv.org/abs/2012.14820v2) - [pdf](http://arxiv.org/pdf/2012.14820v2)

> The paper aims at developing the Bayesian seasonally cointegrated model for quarterly data. We propose the prior structure, derive the set of full conditional posterior distributions, and propose the sampling scheme. The identification of cointegrating spaces is obtained \emph{via} orthonormality restrictions imposed on vectors spanning them. In the case of annual frequency, the cointegrating vectors are complex, which should be taken into account when identifying them. The point estimation of the cointegrating spaces is also discussed. The presented methods are illustrated by a simulation experiment and are employed in the analysis of money and prices in the Polish economy.

</details>

<details>

<summary>2021-04-08 12:57:46 - Residual Gaussian Process: A Tractable Nonparametric Bayesian Emulator for Multi-fidelity Simulations</summary>

- *Wei W. Xing, Akeel A. Shah, Peng Wang, Shandian Zhe Qian Fu, Robert. M. Kirby*

- `2104.03743v1` - [abs](http://arxiv.org/abs/2104.03743v1) - [pdf](http://arxiv.org/pdf/2104.03743v1)

> Challenges in multi-fidelity modeling relate to accuracy, uncertainty estimation and high-dimensionality. A novel additive structure is introduced in which the highest fidelity solution is written as a sum of the lowest fidelity solution and residuals between the solutions at successive fidelity levels, with Gaussian process priors placed over the low fidelity solution and each of the residuals. The resulting model is equipped with a closed-form solution for the predictive posterior, making it applicable to advanced, high-dimensional tasks that require uncertainty estimation. Its advantages are demonstrated on univariate benchmarks and on three challenging multivariate problems. It is shown how active learning can be used to enhance the model, especially with a limited computational budget. Furthermore, error bounds are derived for the mean prediction in the univariate case.

</details>

<details>

<summary>2021-04-08 15:18:35 - Bayesian Variational Federated Learning and Unlearning in Decentralized Networks</summary>

- *Jinu Gong, Osvaldo Simeone, Joonhyuk Kang*

- `2104.03834v1` - [abs](http://arxiv.org/abs/2104.03834v1) - [pdf](http://arxiv.org/pdf/2104.03834v1)

> Federated Bayesian learning offers a principled framework for the definition of collaborative training algorithms that are able to quantify epistemic uncertainty and to produce trustworthy decisions. Upon the completion of collaborative training, an agent may decide to exercise her legal "right to be forgotten", which calls for her contribution to the jointly trained model to be deleted and discarded. This paper studies federated learning and unlearning in a decentralized network within a Bayesian framework. It specifically develops federated variational inference (VI) solutions based on the decentralized solution of local free energy minimization problems within exponential-family models and on local gossip-driven communication. The proposed protocols are demonstrated to yield efficient unlearning mechanisms.

</details>

<details>

<summary>2021-04-08 17:38:02 - Approximate Bayesian inference from noisy likelihoods with Gaussian process emulated MCMC</summary>

- *Marko Järvenpää, Jukka Corander*

- `2104.03942v1` - [abs](http://arxiv.org/abs/2104.03942v1) - [pdf](http://arxiv.org/pdf/2104.03942v1)

> We present an efficient approach for doing approximate Bayesian inference when only a limited number of noisy likelihood evaluations can be obtained due to computational constraints, which is becoming increasingly common for applications of complex models. Our main methodological innovation is to model the log-likelihood function using a Gaussian process (GP) in a local fashion and apply this model to emulate the progression that an exact Metropolis-Hastings (MH) algorithm would take if it was applicable. New log-likelihood evaluation locations are selected using sequential experimental design strategies such that each MH accept/reject decision is done within a pre-specified error tolerance. The resulting approach is conceptually simple and sample-efficient as it takes full advantage of the GP model. It is also more robust to violations of GP modelling assumptions and better suited for the typical situation where the posterior is substantially more concentrated than the prior, compared with various existing inference methods based on global GP surrogate modelling. We discuss the probabilistic interpretations and central theoretical aspects of our approach, and we then demonstrate the benefits of the resulting algorithm in the context of likelihood-free inference for simulator-based statistical models.

</details>

<details>

<summary>2021-04-08 20:09:13 - A Hierarchical Bayesian Model for Stochastic Spatiotemporal SIR Modeling and Prediction of COVID-19 Cases and Hospitalizations</summary>

- *Curtis B. Storlie, Ricardo L. Rojas, Gabriel O. Demuth, Benjamin D. Pollock, Patrick W. Johnson, Patrick M. Wilson, Ethan P. Heinzen, Hongfang Liu, Rickey E. Carter, Sean C. Dowdy, Shannon M. Dunlay, Elizabeth B. Habermann, Daryl J. Kor, Matthew R. Neville, Andrew H. Limper, Katherine H. Noe, Mohamad Bydon, Pablo Moreno Franco, Priya Sampathkumar, Nilay D. Shah, Henry H. Ting*

- `2104.04033v1` - [abs](http://arxiv.org/abs/2104.04033v1) - [pdf](http://arxiv.org/pdf/2104.04033v1)

> Most COVID-19 predictive modeling efforts use statistical or mathematical models to predict national- and state-level COVID-19 cases or deaths in the future. These approaches assume parameters such as reproduction time, test positivity rate, hospitalization rate, and social intervention effectiveness (masking, distancing, and mobility) are constant. However, the one certainty with the COVID-19 pandemic is that these parameters change over time, as well as vary across counties and states. In fact, the rate of spread over region, hospitalization rate, hospital length of stay and mortality rate, the proportion of the population that is susceptible, test positivity rate, and social behaviors can all change significantly over time. Thus, the quantification of uncertainty becomes critical in making meaningful and accurate forecasts of the future. Bayesian approaches are a natural way to fully represent this uncertainty in mathematical models and have become particularly popular in physics and engineering models. The explicit integration time varying parameters and uncertainty quantification into a hierarchical Bayesian forecast model differentiates the Mayo COVID-19 model from other forecasting models. By accounting for all sources of uncertainty in both parameter estimation as well as future trends with a Bayesian approach, the Mayo COVID-19 model accurately forecasts future cases and hospitalizations, as well as the degree of uncertainty. This approach has been remarkably accurate and a linchpin in Mayo Clinic's response to managing the COVID-19 pandemic. The model accurately predicted timing and extent of the summer and fall surges at Mayo Clinic sites, allowing hospital leadership to manage resources effectively to provide a successful pandemic response. This model has also proven to be very useful to the state of Minnesota to help guide difficult policy decisions.

</details>

<details>

<summary>2021-04-08 20:21:01 - Output-Weighted Optimal Sampling for Bayesian Experimental Design and Uncertainty Quantification</summary>

- *Antoine Blanchard, Themistoklis Sapsis*

- `2006.12394v3` - [abs](http://arxiv.org/abs/2006.12394v3) - [pdf](http://arxiv.org/pdf/2006.12394v3)

> We introduce a class of acquisition functions for sample selection that leads to faster convergence in applications related to Bayesian experimental design and uncertainty quantification. The approach follows the paradigm of active learning, whereby existing samples of a black-box function are utilized to optimize the next most informative sample. The proposed method aims to take advantage of the fact that some input directions of the black-box function have a larger impact on the output than others, which is important especially for systems exhibiting rare and extreme events. The acquisition functions introduced in this work leverage the properties of the likelihood ratio, a quantity that acts as a probabilistic sampling weight and guides the active-learning algorithm towards regions of the input space that are deemed most relevant. We demonstrate superiority of the proposed approach in the uncertainty quantification of a hydrological system as well as the probabilistic quantification of rare events in dynamical systems and the identification of their precursors.

</details>

<details>

<summary>2021-04-09 01:20:21 - Stopping Criterion for Active Learning Based on Error Stability</summary>

- *Hideaki Ishibashi, Hideitsu Hino*

- `2104.01836v2` - [abs](http://arxiv.org/abs/2104.01836v2) - [pdf](http://arxiv.org/pdf/2104.01836v2)

> Active learning is a framework for supervised learning to improve the predictive performance by adaptively annotating a small number of samples. To realize efficient active learning, both an acquisition function that determines the next datum and a stopping criterion that determines when to stop learning should be considered. In this study, we propose a stopping criterion based on error stability, which guarantees that the change in generalization error upon adding a new sample is bounded by the annotation cost and can be applied to any Bayesian active learning. We demonstrate that the proposed criterion stops active learning at the appropriate timing for various learning models and real datasets.

</details>

<details>

<summary>2021-04-09 02:44:02 - Latent Network Estimation and Variable Selection for Compositional Data via Variational EM</summary>

- *Nathan Osborne, Christine B. Peterson, Marina Vannucci*

- `2010.13229v2` - [abs](http://arxiv.org/abs/2010.13229v2) - [pdf](http://arxiv.org/pdf/2010.13229v2)

> Network estimation and variable selection have been extensively studied in the statistical literature, but only recently have those two challenges been addressed simultaneously. In this paper, we seek to develop a novel method to simultaneously estimate network interactions and associations to relevant covariates for count data, and specifically for compositional data, which have a fixed sum constraint. We use a hierarchical Bayesian model with latent layers and employ spike-and-slab priors for both edge and covariate selection. For posterior inference, we develop a novel variational inference scheme with an expectation maximization step, to enable efficient estimation. Through simulation studies, we demonstrate that the proposed model outperforms existing methods in its accuracy of network recovery. We show the practical utility of our model via an application to microbiome data. The human microbiome has been shown to contribute to many of the functions of the human body, and also to be linked with a number of diseases. In our application, we seek to better understand the interaction between microbes and relevant covariates, as well as the interaction of microbes with each other. We provide a Python implementation of our algorithm, called SINC (Simultaneous Inference for Networks and Covariates), available online.

</details>

<details>

<summary>2021-04-09 15:07:54 - Benchmarking Simulation-Based Inference</summary>

- *Jan-Matthis Lueckmann, Jan Boelts, David S. Greenberg, Pedro J. Gonçalves, Jakob H. Macke*

- `2101.04653v2` - [abs](http://arxiv.org/abs/2101.04653v2) - [pdf](http://arxiv.org/pdf/2101.04653v2)

> Recent advances in probabilistic modelling have led to a large number of simulation-based inference algorithms which do not require numerical evaluation of likelihoods. However, a public benchmark with appropriate performance metrics for such 'likelihood-free' algorithms has been lacking. This has made it difficult to compare algorithms and identify their strengths and weaknesses. We set out to fill this gap: We provide a benchmark with inference tasks and suitable performance metrics, with an initial selection of algorithms including recent approaches employing neural networks and classical Approximate Bayesian Computation methods. We found that the choice of performance metric is critical, that even state-of-the-art algorithms have substantial room for improvement, and that sequential estimation improves sample efficiency. Neural network-based approaches generally exhibit better performance, but there is no uniformly best algorithm. We provide practical advice and highlight the potential of the benchmark to diagnose problems and improve algorithms. The results can be explored interactively on a companion website. All code is open source, making it possible to contribute further benchmark tasks and inference algorithms.

</details>

<details>

<summary>2021-04-09 16:06:15 - Optimal Epidemic Control in Equilibrium with Imperfect Testing and Enforcement</summary>

- *Thomas Phelan, Alexis Akira Toda*

- `2104.04455v1` - [abs](http://arxiv.org/abs/2104.04455v1) - [pdf](http://arxiv.org/pdf/2104.04455v1)

> We analyze equilibrium behavior and optimal policy within a Susceptible-Infected-Recovered epidemic model augmented with potentially undiagnosed agents who infer their health status and a social planner with imperfect enforcement of social distancing. We define and prove the existence of a perfect Bayesian Markov competitive equilibrium and contrast it with the efficient allocation subject to the same informational constraints. We identify two externalities, static (individual actions affect current risk of infection) and dynamic (individual actions affect future disease prevalence), and study how they are affected by limitations on testing and enforcement. We prove that a planner with imperfect enforcement will always wish to curtail activity, but that its incentives vanish as testing becomes perfect. When a vaccine arrives far into the future, the planner with perfect enforcement may encourage activity before herd immunity. We find that lockdown policies have modest welfare gains, whereas quarantine policies are effective even with imperfect testing.

</details>

<details>

<summary>2021-04-10 15:18:58 - What Makes an Effective Scalarising Function for Multi-Objective Bayesian Optimisation?</summary>

- *Clym Stock-Williams, Tinkle Chugh, Alma Rahat, Wei Yu*

- `2104.04790v1` - [abs](http://arxiv.org/abs/2104.04790v1) - [pdf](http://arxiv.org/pdf/2104.04790v1)

> Performing multi-objective Bayesian optimisation by scalarising the objectives avoids the computation of expensive multi-dimensional integral-based acquisition functions, instead of allowing one-dimensional standard acquisition functions\textemdash such as Expected Improvement\textemdash to be applied. Here, two infill criteria based on hypervolume improvement\textemdash one recently introduced and one novel\textemdash are compared with the multi-surrogate Expected Hypervolume Improvement. The reasons for the disparities in these methods' effectiveness in maximising the hypervolume of the acquired Pareto Front are investigated. In addition, the effect of the surrogate model mean function on exploration and exploitation is examined: careful choice of data normalisation is shown to be preferable to the exploration parameter commonly used with the Expected Improvement acquisition function. Finally, the effectiveness of all the methodological improvements defined here is demonstrated on a real-world problem: the optimisation of a wind turbine blade aerofoil for both aerodynamic performance and structural stiffness. With effective scalarisation, Bayesian optimisation finds a large number of new aerofoil shapes that strongly dominate standard designs.

</details>

<details>

<summary>2021-04-11 12:14:04 - ALT-MAS: A Data-Efficient Framework for Active Testing of Machine Learning Algorithms</summary>

- *Huong Ha, Sunil Gupta, Santu Rana, Svetha Venkatesh*

- `2104.04999v1` - [abs](http://arxiv.org/abs/2104.04999v1) - [pdf](http://arxiv.org/pdf/2104.04999v1)

> Machine learning models are being used extensively in many important areas, but there is no guarantee a model will always perform well or as its developers intended. Understanding the correctness of a model is crucial to prevent potential failures that may have significant detrimental impact in critical application areas. In this paper, we propose a novel framework to efficiently test a machine learning model using only a small amount of labeled test data. The idea is to estimate the metrics of interest for a model-under-test using Bayesian neural network (BNN). We develop a novel data augmentation method helping to train the BNN to achieve high accuracy. We also devise a theoretic information based sampling strategy to sample data points so as to achieve accurate estimations for the metrics of interest. Finally, we conduct an extensive set of experiments to test various machine learning models for different types of metrics. Our experiments show that the metrics estimations by our method are significantly better than existing baselines.

</details>

<details>

<summary>2021-04-11 15:16:21 - Prediction of Future Failures for Heterogeneous Reliability Field Data</summary>

- *Colin Lewis-Beck, Qinglong Tian, William Q. Meeker*

- `2011.03140v3` - [abs](http://arxiv.org/abs/2011.03140v3) - [pdf](http://arxiv.org/pdf/2011.03140v3)

> This article introduces methods for constructing prediction bounds or intervals for the number of future failures from heterogeneous reliability field data. We focus on within-sample prediction where early data from a failure-time process is used to predict future failures from the same process. Early data from high-reliability products, however, often have limited information due to some combination of small sample sizes, censoring, and truncation. In such cases, we use a Bayesian hierarchical model to model jointly multiple lifetime distributions arising from different subpopulations of similar products. By borrowing information across subpopulations, our method enables stable estimation and the computation of corresponding prediction intervals, even in cases where there are few observed failures. Three applications are provided to illustrate this methodology, and a simulation study is used to validate the coverage performance of the prediction intervals.

</details>

<details>

<summary>2021-04-11 17:58:49 - Local-HDP: Interactive Open-Ended 3D Object Categorization in Real-Time Robotic Scenarios</summary>

- *H. Ayoobi, H. Kasaei, M. Cao, R. Verbrugge, B. Verheij*

- `2009.01152v3` - [abs](http://arxiv.org/abs/2009.01152v3) - [pdf](http://arxiv.org/pdf/2009.01152v3)

> We introduce a non-parametric hierarchical Bayesian approach for open-ended 3D object categorization, named the Local Hierarchical Dirichlet Process (Local-HDP). This method allows an agent to learn independent topics for each category incrementally and to adapt to the environment in time. Hierarchical Bayesian approaches like Latent Dirichlet Allocation (LDA) can transform low-level features to high-level conceptual topics for 3D object categorization. However, the efficiency and accuracy of LDA-based approaches depend on the number of topics that is chosen manually. Moreover, fixing the number of topics for all categories can lead to overfitting or underfitting of the model. In contrast, the proposed Local-HDP can autonomously determine the number of topics for each category. Furthermore, the online variational inference method has been adapted for fast posterior approximation in the Local-HDP model. Experiments show that the proposed Local-HDP method outperforms other state-of-the-art approaches in terms of accuracy, scalability, and memory efficiency by a large margin. Moreover, two robotic experiments have been conducted to show the applicability of the proposed approach in real-time applications.

</details>

<details>

<summary>2021-04-11 21:15:50 - Bayesian exponential random graph models for populations of networks</summary>

- *Brieuc Lehmann, Simon White*

- `2104.05110v1` - [abs](http://arxiv.org/abs/2104.05110v1) - [pdf](http://arxiv.org/pdf/2104.05110v1)

> The collection of data on populations of networks is becoming increasingly common, where each data point can be seen as a realisation of a network-valued random variable. A canonical example is that of brain networks: a typical neuroimaging study collects one or more brain scans across multiple individuals, each of which can be modelled as a network with nodes corresponding to distinct brain regions and edges corresponding to structural or functional connections between these regions. Most statistical network models, however, were originally proposed to describe a single underlying relational structure, although recent years have seen a drive to extend these models to populations of networks. Here, we propose one such extension: a multilevel framework for populations of networks based on exponential random graph models. By pooling information across the individual networks, this framework provides a principled approach to characterise the relational structure for an entire population. To perform inference, we devise a novel exchange-within-Gibbs MCMC algorithm that generates samples from the doubly-intractable posterior. To illustrate our framework, we use it to assess group-level variations in networks derived from fMRI scans, enabling the inference of age-related differences in the topological structure of the brain's functional connectivity.

</details>

<details>

<summary>2021-04-11 23:06:17 - Couplings for Multinomial Hamiltonian Monte Carlo</summary>

- *Kai Xu, Tor Erlend Fjelde, Charles Sutton, Hong Ge*

- `2104.05134v1` - [abs](http://arxiv.org/abs/2104.05134v1) - [pdf](http://arxiv.org/pdf/2104.05134v1)

> Hamiltonian Monte Carlo (HMC) is a popular sampling method in Bayesian inference. Recently, Heng & Jacob (2019) studied Metropolis HMC with couplings for unbiased Monte Carlo estimation, establishing a generic parallelizable scheme for HMC. However, in practice a different HMC method, multinomial HMC, is considered as the go-to method, e.g. as part of the no-U-turn sampler. In multinomial HMC, proposed states are not limited to end-points as in Metropolis HMC; instead points along the entire trajectory can be proposed. In this paper, we establish couplings for multinomial HMC, based on optimal transport for multinomial sampling in its transition. We prove an upper bound for the meeting time - the time it takes for the coupled chains to meet - based on the notion of local contractivity. We evaluate our methods using three targets: 1,000 dimensional Gaussians, logistic regression and log-Gaussian Cox point processes. Compared to Heng & Jacob (2019), coupled multinomial HMC generally attains a smaller meeting time, and is more robust to choices of step sizes and trajectory lengths, which allows re-use of existing adaptation methods for HMC. These improvements together paves the way for a wider and more practical use of coupled HMC methods.

</details>

<details>

<summary>2021-04-12 04:08:29 - Inference from Non-Random Samples Using Bayesian Machine Learning</summary>

- *Yutao Liu, Andrew Gelman, Qixuan Chen*

- `2104.05192v1` - [abs](http://arxiv.org/abs/2104.05192v1) - [pdf](http://arxiv.org/pdf/2104.05192v1)

> We consider inference from non-random samples in data-rich settings where high-dimensional auxiliary information is available both in the sample and the target population, with survey inference being a special case. We propose a regularized prediction approach that predicts the outcomes in the population using a large number of auxiliary variables such that the ignorability assumption is reasonable while the Bayesian framework is straightforward for quantification of uncertainty. Besides the auxiliary variables, inspired by Little & An (2004), we also extend the approach by estimating the propensity score for a unit to be included in the sample and also including it as a predictor in the machine learning models. We show through simulation studies that the regularized predictions using soft Bayesian additive regression trees yield valid inference for the population means and coverage rates close to the nominal levels. We demonstrate the application of the proposed methods using two different real data applications, one in a survey and one in an epidemiology study.

</details>

<details>

<summary>2021-04-12 10:23:42 - Convolutional Neural Networks as Summary Statistics for Approximate Bayesian Computation</summary>

- *Mattias Åkesson, Prashant Singh, Fredrik Wrede, Andreas Hellander*

- `2001.11760v5` - [abs](http://arxiv.org/abs/2001.11760v5) - [pdf](http://arxiv.org/pdf/2001.11760v5)

> Approximate Bayesian Computation is widely used in systems biology for inferring parameters in stochastic gene regulatory network models. Its performance hinges critically on the ability to summarize high-dimensional system responses such as time series into a few informative, low-dimensional summary statistics. The quality of those statistics acutely impacts the accuracy of the inference task. Existing methods to select the best subset out of a pool of candidate statistics do not scale well with large pools of several tens to hundreds of candidate statistics. Since high quality statistics are imperative for good performance, this becomes a serious bottleneck when performing inference on complex and high-dimensional problems. This paper proposes a convolutional neural network architecture for automatically learning informative summary statistics of temporal responses. We show that the proposed network can effectively circumvent the statistics selection problem of the preprocessing step for ABC inference. The proposed approach is demonstrated on two benchmark problem and one challenging inference problem learning parameters in a high-dimensional stochastic genetic oscillator. We also study the impact of experimental design on network performance by comparing different data richness and data acquisition strategies.

</details>

<details>

<summary>2021-04-12 17:41:18 - GPflux: A Library for Deep Gaussian Processes</summary>

- *Vincent Dutordoir, Hugh Salimbeni, Eric Hambro, John McLeod, Felix Leibfried, Artem Artemev, Mark van der Wilk, James Hensman, Marc P. Deisenroth, ST John*

- `2104.05674v1` - [abs](http://arxiv.org/abs/2104.05674v1) - [pdf](http://arxiv.org/pdf/2104.05674v1)

> We introduce GPflux, a Python library for Bayesian deep learning with a strong emphasis on deep Gaussian processes (DGPs). Implementing DGPs is a challenging endeavour due to the various mathematical subtleties that arise when dealing with multivariate Gaussian distributions and the complex bookkeeping of indices. To date, there are no actively maintained, open-sourced and extendable libraries available that support research activities in this area. GPflux aims to fill this gap by providing a library with state-of-the-art DGP algorithms, as well as building blocks for implementing novel Bayesian and GP-based hierarchical models and inference schemes. GPflux is compatible with and built on top of the Keras deep learning eco-system. This enables practitioners to leverage tools from the deep learning community for building and training customised Bayesian models, and create hierarchical models that consist of Bayesian and standard neural network layers in a single coherent framework. GPflux relies on GPflow for most of its GP objects and operations, which makes it an efficient, modular and extensible library, while having a lean codebase.

</details>

<details>

<summary>2021-04-12 22:02:51 - Nonparametric Gaussian Mixture Models for the Multi-Armed Contextual Bandit</summary>

- *Iñigo Urteaga, Chris H. Wiggins*

- `1808.02932v3` - [abs](http://arxiv.org/abs/1808.02932v3) - [pdf](http://arxiv.org/pdf/1808.02932v3)

> We here adopt Bayesian nonparametric mixture models to extend multi-armed bandits in general, and Thompson sampling in particular, to scenarios where there is reward model uncertainty. In the stochastic multi-armed bandit, where an agent must learn a policy that maximizes long term payoff, the reward for the selected action is generated from an unknown distribution. Thompson sampling is a generative and interpretable multi-armed bandit algorithm that has been shown both to perform well in practice, and to enjoy optimality properties for certain reward functions. Nevertheless, Thompson sampling requires knowledge of the true reward model, for calculation of expected rewards and sampling from its parameter posterior. In this work, we extend Thompson sampling to complex scenarios where there is model uncertainty, by adopting a very flexible set of reward distributions: Bayesian nonparametric Gaussian mixture models. The generative process of Bayesian nonparametric mixtures naturally aligns with the Bayesian modeling of multi-armed bandits: the nonparametric model autonomously determines its complexity as new rewards are observed for the played arms. By characterizing each arm's reward distribution with independent nonparametric mixture models, the proposed method sequentially learns the model that best approximates the true underlying reward distribution, achieving successful performance in complex -- not in the exponential family -- bandits. Our contribution is valuable for practical scenarios, as it avoids stringent case-by-case model specifications and hyperparameter tuning, yet attains reduced regret in diverse bandit settings.

</details>

<details>

<summary>2021-04-13 01:44:44 - Approximate Bayesian Computation of Bézier Simplices</summary>

- *Akinori Tanaka, Akiyoshi Sannai, Ken Kobayashi, Naoki Hamada*

- `2104.04679v2` - [abs](http://arxiv.org/abs/2104.04679v2) - [pdf](http://arxiv.org/pdf/2104.04679v2)

> B\'ezier simplex fitting algorithms have been recently proposed to approximate the Pareto set/front of multi-objective continuous optimization problems. These new methods have shown to be successful at approximating various shapes of Pareto sets/fronts when sample points exactly lie on the Pareto set/front. However, if the sample points scatter away from the Pareto set/front, those methods often likely suffer from over-fitting. To overcome this issue, in this paper, we extend the B\'ezier simplex model to a probabilistic one and propose a new learning algorithm of it, which falls into the framework of approximate Bayesian computation (ABC) based on the Wasserstein distance. We also study the convergence property of the Wasserstein ABC algorithm. An extensive experimental evaluation on publicly available problem instances shows that the new algorithm converges on a finite sample. Moreover, it outperforms the deterministic fitting methods on noisy instances.

</details>

<details>

<summary>2021-04-13 09:53:50 - Least Squares Approximation for a Distributed System</summary>

- *Xuening Zhu, Feng Li, Hansheng Wang*

- `1908.04904v4` - [abs](http://arxiv.org/abs/1908.04904v4) - [pdf](http://arxiv.org/pdf/1908.04904v4)

> In this work, we develop a distributed least squares approximation (DLSA) method that is able to solve a large family of regression problems (e.g., linear regression, logistic regression, and Cox's model) on a distributed system. By approximating the local objective function using a local quadratic form, we are able to obtain a combined estimator by taking a weighted average of local estimators. The resulting estimator is proved to be statistically as efficient as the global estimator. Moreover, it requires only one round of communication. We further conduct a shrinkage estimation based on the DLSA estimation using an adaptive Lasso approach. The solution can be easily obtained by using the LARS algorithm on the master node. It is theoretically shown that the resulting estimator possesses the oracle property and is selection consistent by using a newly designed distributed Bayesian information criterion (DBIC). The finite sample performance and computational efficiency are further illustrated by an extensive numerical study and an airline dataset. The airline dataset is 52 GB in size. The entire methodology has been implemented in Python for a {\it de-facto} standard Spark system. The proposed DLSA algorithm on the Spark system takes 26 minutes to obtain a logistic regression estimator, which is more efficient and memory friendly than conventional methods.

</details>

<details>

<summary>2021-04-13 09:56:12 - Gibbs posterior inference on multivariate quantiles</summary>

- *Indrabati Bhattacharya, Ryan Martin*

- `2002.01052v3` - [abs](http://arxiv.org/abs/2002.01052v3) - [pdf](http://arxiv.org/pdf/2002.01052v3)

> Bayesian and other likelihood-based methods require specification of a statistical model and may not be fully satisfactory for inference on quantities, such as quantiles, that are not naturally defined as model parameters. In this paper, we construct a direct and model-free Gibbs posterior distribution for multivariate quantiles. Being model-free means that inferences drawn from the Gibbs posterior are not subject to model misspecification bias, and being direct means that no priors for or marginalization over nuisance parameters are required. We show here that the Gibbs posterior enjoys a root-$n$ convergence rate and a Bernstein--von Mises property, i.e., for large n, the Gibbs posterior distribution can be approximated by a Gaussian. Moreover, we present numerical results showing the validity and efficiency of credible sets derived from a suitably scaled Gibbs posterior.

</details>

<details>

<summary>2021-04-13 16:11:14 - Optimal transport couplings of Gibbs samplers on partitions for unbiased estimation</summary>

- *Brian L. Trippe, Tin D. Nguyen, Tamara Broderick*

- `2104.04514v2` - [abs](http://arxiv.org/abs/2104.04514v2) - [pdf](http://arxiv.org/pdf/2104.04514v2)

> Computational couplings of Markov chains provide a practical route to unbiased Monte Carlo estimation that can utilize parallel computation. However, these approaches depend crucially on chains meeting after a small number of transitions. For models that assign data into groups, e.g. mixture models, the obvious approaches to couple Gibbs samplers fail to meet quickly. This failure owes to the so-called "label-switching" problem; semantically equivalent relabelings of the groups contribute well-separated posterior modes that impede fast mixing and cause large meeting times. We here demonstrate how to avoid label switching by considering chains as exploring the space of partitions rather than labelings. Using a metric on this space, we employ an optimal transport coupling of the Gibbs conditionals. This coupling outperforms alternative couplings that rely on labelings and, on a real dataset, provides estimates more precise than usual ergodic averages in the limited time regime. Code is available at github.com/tinnguyen96/coupling-Gibbs-partition.

</details>

<details>

<summary>2021-04-14 11:03:01 - Discrete sticky couplings of functional autoregressive processes</summary>

- *Alain Durmus, Andreas Eberle, Aurélien Enfroy, Arnaud Guillin, Pierre Monmarché*

- `2104.06771v1` - [abs](http://arxiv.org/abs/2104.06771v1) - [pdf](http://arxiv.org/pdf/2104.06771v1)

> In this paper, we provide bounds in Wasserstein and total variation distances between the distributions of the successive iterates of two functional autoregressive processes with isotropic Gaussian noise of the form $Y_{k+1} = \mathrm{T}_\gamma(Y_k) + \sqrt{\gamma\sigma^2} Z_{k+1}$ and $\tilde{Y}_{k+1} = \tilde{\mathrm{T}}_\gamma(\tilde{Y}_k) + \sqrt{\gamma\sigma^2} \tilde{Z}_{k+1}$. More precisely, we give non-asymptotic bounds on $\rho(\mathcal{L}(Y_{k}),\mathcal{L}(\tilde{Y}_k))$, where $\rho$ is an appropriate weighted Wasserstein distance or a $V$-distance, uniformly in the parameter $\gamma$, and on $\rho(\pi_{\gamma},\tilde{\pi}_{\gamma})$, where $\pi_{\gamma}$ and $\tilde{\pi}_{\gamma}$ are the respective stationary measures of the two processes. The class of considered processes encompasses the Euler-Maruyama discretization of Langevin diffusions and its variants. The bounds we derive are of order $\gamma$ as $\gamma \to 0$. To obtain our results, we rely on the construction of a discrete sticky Markov chain $(W_k^{(\gamma)})_{k \in \mathbb{N}}$ which bounds the distance between an appropriate coupling of the two processes. We then establish stability and quantitative convergence results for this process uniformly on $\gamma$. In addition, we show that it converges in distribution to the continuous sticky process studied in previous work. Finally, we apply our result to Bayesian inference of ODE parameters and numerically illustrate them on two particular problems.

</details>

<details>

<summary>2021-04-14 11:58:54 - Forecasting COVID-19 Counts At A Single Hospital: A Hierarchical Bayesian Approach</summary>

- *Alexandra Hope Lee, Panagiotis Lymperopoulos, Joshua T. Cohen, John B. Wong, Michael C. Hughes*

- `2104.09327v1` - [abs](http://arxiv.org/abs/2104.09327v1) - [pdf](http://arxiv.org/pdf/2104.09327v1)

> We consider the problem of forecasting the daily number of hospitalized COVID-19 patients at a single hospital site, in order to help administrators with logistics and planning. We develop several candidate hierarchical Bayesian models which directly capture the count nature of data via a generalized Poisson likelihood, model time-series dependencies via autoregressive and Gaussian process latent processes, and share statistical strength across related sites. We demonstrate our approach on public datasets for 8 hospitals in Massachusetts, U.S.A. and 10 hospitals in the United Kingdom. Further prospective evaluation compares our approach favorably to baselines currently used by stakeholders at 3 related hospitals to forecast 2-week-ahead demand by rescaling state-level forecasts.

</details>

<details>

<summary>2021-04-14 12:38:27 - Short-term bus travel time prediction for transfer synchronization with intelligent uncertainty handling</summary>

- *Niklas Christoffer Petersen, Anders Parslov, Filipe Rodrigues*

- `2104.06819v1` - [abs](http://arxiv.org/abs/2104.06819v1) - [pdf](http://arxiv.org/pdf/2104.06819v1)

> This paper presents two novel approaches for uncertainty estimation adapted and extended for the multi-link bus travel time problem. The uncertainty is modeled directly as part of recurrent artificial neural networks, but using two fundamentally different approaches: one based on Deep Quantile Regression (DQR) and the other on Bayesian Recurrent Neural Networks (BRNN). Both models predict multiple time steps into the future, but handle the time-dependent uncertainty estimation differently. We present a sampling technique in order to aggregate quantile estimates for link level travel time to yield the multi-link travel time distribution needed for a vehicle to travel from its current position to a specific downstream stop point or transfer site.   To motivate the relevance of uncertainty-aware models in the domain, we focus on the connection assurance application as a case study: An expert system to determine whether a bus driver should hold and wait for a connecting service, or break the connection and reduce its own delay. Our results show that the DQR-model performs overall best for the 80%, 90% and 95% prediction intervals, both for a 15 minute time horizon into the future (t + 1), but also for the 30 and 45 minutes time horizon (t + 2 and t + 3), with a constant, but very small underestimation of the uncertainty interval (1-4 pp.). However, we also show, that the BRNN model still can outperform the DQR for specific cases. Lastly, we demonstrate how a simple decision support system can take advantage of our uncertainty-aware travel time models to prioritize the difference in travel time uncertainty for bus holding at strategic points, thus reducing the introduced delay for the connection assurance application.

</details>

<details>

<summary>2021-04-14 15:07:19 - A hybrid Gibbs sampler for edge-preserving tomographic reconstruction with uncertain view angles</summary>

- *Felipe Uribe, Johnathan M. Bardsley, Yiqiu Dong, Per Christian Hansen, Nicolai A. B. Riis*

- `2104.06919v1` - [abs](http://arxiv.org/abs/2104.06919v1) - [pdf](http://arxiv.org/pdf/2104.06919v1)

> In computed tomography, data consist of measurements of the attenuation of X-rays passing through an object. The goal is to reconstruct the linear attenuation coefficient of the object's interior. For each position of the X-ray source, characterized by its angle with respect to a fixed coordinate system, one measures a set of data referred to as a view. A common assumption is that these view angles are known, but in some applications they are known with imprecision. We propose a framework to solve a Bayesian inverse problem that jointly estimates the view angles and an image of the object's attenuation coefficient. We also include a few hyperparameters that characterize the likelihood and the priors. Our approach is based on a Gibbs sampler where the associated conditional densities are simulated using different sampling schemes - hence the term hybrid. In particular, the conditional distribution associated with the reconstruction is nonlinear in the image pixels, non-Gaussian and high-dimensional. We approach this distribution by constructing a Laplace approximation that represents the target conditional locally at each Gibbs iteration. This enables sampling of the attenuation coefficients in an efficient manner using iterative reconstruction algorithms. The numerical results show that our algorithm is able to jointly identify the image and the view angles, while also providing uncertainty estimates of both. We demonstrate our method with 2D X-ray computed tomography problems using fan beam configurations.

</details>

<details>

<summary>2021-04-14 17:56:06 - Modeling the Heterogeneity in COVID-19's Reproductive Number and its Impact on Predictive Scenarios</summary>

- *Claire Donnat, Susan Holmes*

- `2004.05272v4` - [abs](http://arxiv.org/abs/2004.05272v4) - [pdf](http://arxiv.org/pdf/2004.05272v4)

> The correct evaluation of the reproductive number $R$ for COVID-19 -- which characterizes the average number of secondary cases generated by each typical primary case -- is central in the quantification of the potential scope of the pandemic and the selection of an appropriate course of action. In most models, $R$ is modeled as a universal constant for the virus across outbreak clusters and individuals -- effectively averaging out the inherent variability of the transmission process due to varying individual contact rates, population densities, demographics, or temporal factors amongst many. Yet, due to the exponential nature of epidemic growth, the error due to this simplification can be rapidly amplified and lead to inaccurate predictions and/or risk evaluation. From the statistical modeling perspective, the magnitude of the impact of this averaging remains an open question: how can this intrinsic variability be percolated into epidemic models, and how can its impact on uncertainty quantification and predictive scenarios be better quantified? In this paper, we propose to study this question through a Bayesian perspective, creating a bridge between the agent-based and compartmental approaches commonly used in the literature. After deriving a Bayesian model that captures at scale the heterogeneity of a population and environmental conditions, we simulate the spread of the epidemic as well as the impact of different social distancing strategies, and highlight the strong impact of this added variability on the reported results. We base our discussion on both synthetic experiments -- thereby quantifying of the reliability and the magnitude of the effects -- and real COVID-19 data.

</details>

<details>

<summary>2021-04-14 20:30:34 - Statistical guarantees for Bayesian uncertainty quantification in non-linear inverse problems with Gaussian process priors</summary>

- *François Monard, Richard Nickl, Gabriel P. Paternain*

- `2007.15892v2` - [abs](http://arxiv.org/abs/2007.15892v2) - [pdf](http://arxiv.org/pdf/2007.15892v2)

> Bayesian inference and uncertainty quantification in a general class of non-linear inverse regression models is considered. Analytic conditions on the regression model $\{\mathscr G(\theta): \theta \in \Theta\}$ and on Gaussian process priors for $\theta$ are provided such that semi-parametrically efficient inference is possible for a large class of linear functionals of $\theta$. A general semi-parametric Bernstein-von Mises theorem is proved that shows that the (non-Gaussian) posterior distributions are approximated by certain Gaussian measures centred at the posterior mean. As a consequence posterior-based credible sets are valid and optimal from a frequentist point of view. The theory is illustrated with two applications with PDEs that arise in non-linear tomography problems: an elliptic inverse problem for a Schr\"odinger equation, and inversion of non-Abelian X-ray transforms. New analytical techniques are deployed to show that the relevant Fisher information operators are invertible between suitable function spaces

</details>

<details>

<summary>2021-04-15 00:17:41 - COVID-19 Clinical footprint to infer about mortality</summary>

- *Carlos E. Rodríguez, Ramsés H. Mena*

- `2104.07172v1` - [abs](http://arxiv.org/abs/2104.07172v1) - [pdf](http://arxiv.org/pdf/2104.07172v1)

> Information of 1.6 million patients identified as SARS-CoV-2 positive in Mexico is used to understand the relationship between comorbidities, symptoms, hospitalizations and deaths due to the COVID-19 disease. Using the presence or absence of these latter variables a clinical footprint for each patient is created. The risk, expected mortality and the prediction of death outcomes, among other relevant quantities, are obtained and analyzed by means of a multivariate Bernoulli distribution. The proposal considers all possible footprint combinations resulting in a robust model suitable for Bayesian inference.

</details>

<details>

<summary>2021-04-15 07:59:25 - A Review of Spatiotemporal Models for Count Data in R Packages. A Case Study of COVID-19 Data</summary>

- *María Victoria Ibáñez, Marina Martínez-Garcia, Amelia Simó*

- `2103.04697v2` - [abs](http://arxiv.org/abs/2103.04697v2) - [pdf](http://arxiv.org/pdf/2103.04697v2)

> Spatio-temporal models for count data are required in a wide range of scientific fields and they have become particularly crucial nowadays because of their ability to analyse COVID-19-related data. Models for count data are needed when the variable of interest take only non-negative integer values and these integers arise from counting occurrences. Several R-packages are currently available to deal with spatiotemporal areal count data. Each package focuses on different models and/or statistical methodologies. Unfortunately, the results generated by these models are rarely comparable due to differences in notation and methods. The main objective of this paper is to present a review describing the most important approaches that can be used to model and analyse count data when questions of scientific interest concern both their spatial and their temporal behaviour and we monitor their performance under the same data set.   For this review, we focus on the three R-packages that can be used for this purpose and the different models assessed are representative of the two most widespread methodologies used to analyse spatiotemporal count data: the classical approach (based on Penalised Likelihood or Estimating Equations) and the Bayesian point of view.   A case study is analysed as an illustration of these different methodologies. In this case study, these packages are used to model and predict daily hospitalisations from COVID-19 in 24 health regions within the Valencian Community (Spain), with data corresponding to the period from 28 June to 13 December 2020. Because of the current urgent need for monitoring and predicting data in the COVID-19 pandemic, this case study is, in itself, of particular importance and can be considered the secondary objective of this work. Satisfactory and promising results have been obtained in this second goal.

</details>

<details>

<summary>2021-04-15 16:50:25 - Integrated organic inference (IOI): A reconciliation of statistical paradigms</summary>

- *Russell J. Bowater*

- `2002.07966v3` - [abs](http://arxiv.org/abs/2002.07966v3) - [pdf](http://arxiv.org/pdf/2002.07966v3)

> It is recognised that the Bayesian approach to inference can not adequately cope with all the types of pre-data beliefs about population quantities of interest that are commonly held in practice. In particular, it generally encounters difficulty when there is a lack of such beliefs over some or all the parameters of a model, or within certain partitions of the parameter space concerned. To address this issue, a fairly comprehensive theory of inference is put forward called integrated organic inference that is based on a fusion of Fisherian and Bayesian reasoning. Depending on the pre-data knowledge that is held about any given model parameter, inferences are made about the parameter conditional on all other parameters using one of three methods of inference, namely organic fiducial inference, bispatial inference and Bayesian inference. The full conditional post-data densities that result from doing this are then combined using a framework that allows a joint post-data density for all the parameters to be sensibly formed without requiring these full conditional densities to be compatible. Various examples of the application of this theory are presented. Finally, the theory is defended against possible criticisms partially in terms of what was previously defined as generalised subjective probability.

</details>

<details>

<summary>2021-04-15 22:26:54 - Fully Bayesian inference for spatiotemporal data with the multi-resolution approximation</summary>

- *Luc Villandré, Jean-François Plante, Thierry Duchesne, Patrick Brown*

- `2004.10101v3` - [abs](http://arxiv.org/abs/2004.10101v3) - [pdf](http://arxiv.org/pdf/2004.10101v3)

> Large spatiotemporal datasets are a challenge for conventional Bayesian models because of the cubic computational complexity of the algorithms for obtaining the Cholesky decomposition of the covariance matrix in the multivariate normal density. Moreover, standard numerical algorithms for posterior estimation, such as Markov Chain Monte Carlo (MCMC), are intractable in this context, as they require thousands, if not millions, of costly likelihood evaluations. To overcome those limitations, we propose IS-MRA (Importance sampling - Multi-Resolution Approximation), which takes advantage of the sparse inverse covariance structure produced by the Multi-Resolution Approximation (MRA) approach. IS-MRA is fully Bayesian and facilitates the approximation of the hyperparameter marginal posterior distributions. We apply IS-MRA to large MODIS Level 3 Land Surface Temperature (LST) datasets, sampled between May 18 and May 31, 2012 in the western part of the state of Maharashtra, India. We find that IS-MRA can produce realistic prediction surfaces over regions where concentrated missingness, caused by sizable cloud cover, is observed. Through a validation analysis and simulation study, we also find that predictions tend to be very accurate.

</details>

<details>

<summary>2021-04-15 22:31:32 - The SIR-P Model: An Illustration of the Screening Paradox</summary>

- *Jacques Balayla*

- `2104.07806v1` - [abs](http://arxiv.org/abs/2104.07806v1) - [pdf](http://arxiv.org/pdf/2104.07806v1)

> In previous work by this author, the screening paradox - the loss of predictive power of screening tests over time $t$ - was mathematically formalized using Bayesian theory. Where $J$ is Youden's statistic, $b$ is the specificity of the screening test and $\phi$ is the prevalence of disease, the ratio of positive predictive values at subsequent time $k$, $\rho(\phi_{k})$, over the original $\rho(\phi_{0})$ at $t_0$ is given by:   $\zeta(\phi_{0},k) = \frac{\rho(\phi_{k})}{\rho(\phi_{0})} =\frac{\phi_k(1-b)+J\phi_0\phi_k}{\phi_0(1-b)+J\phi_0\phi_k}$   Herein, we modify the traditional Kermack-McKendrick SIR Model to include the fluctuation of the positive predictive value $\rho(\phi)$ (PPV) of a screening test over time as a function of the prevalence threshold $\phi_e$. We term this modified model the SIR-P model. Where a = sensitivity, b = specificity, $S$ = number susceptible, $I$ = number infected, $R$ = number recovered/dead, $\beta$ = infectious rate, $\gamma$ = recovery rate, and $N$ is the total number in the population, the predictive value $\rho(\phi,t)$ over time $t$ is given by:   $\rho(\phi,t) = \frac{a[\frac{\beta IS}{N}-\gamma I]}{ a[\frac{\beta IS}{N}-\gamma I]+(1-b)(1-[\frac{\beta IS}{N}-\gamma I])}$   Otherwise stated:   $\rho(\phi,t) = \frac{a\frac{dI}{dt}}{ a\frac{dI}{dt}+(1-b)(1-\frac{dI}{dt})}$ where $\frac{dI}{dt}$ is the fluctuation of infected individuals over time $t$.

</details>

<details>

<summary>2021-04-16 15:03:23 - Fast ABC with joint generative modelling and subset simulation</summary>

- *Eliane Maalouf, David Ginsbourger, Niklas Linde*

- `2104.08156v1` - [abs](http://arxiv.org/abs/2104.08156v1) - [pdf](http://arxiv.org/pdf/2104.08156v1)

> We propose a novel approach for solving inverse-problems with high-dimensional inputs and an expensive forward mapping. It leverages joint deep generative modelling to transfer the original problem spaces to a lower dimensional latent space. By jointly modelling input and output variables and endowing the latent with a prior distribution, the fitted probabilistic model indirectly gives access to the approximate conditional distributions of interest. Since model error and observational noise with unknown distributions are common in practice, we resort to likelihood-free inference with Approximate Bayesian Computation (ABC). Our method calls on ABC by Subset Simulation to explore the regions of the latent space with dissimilarities between generated and observed outputs below prescribed thresholds. We diagnose the diversity of approximate posterior solutions by monitoring the probability content of these regions as a function of the threshold. We further analyze the curvature of the resulting diagnostic curve to propose an adequate ABC threshold. When applied to a cross-borehole tomography example from geophysics, our approach delivers promising performance without using prior knowledge of the forward nor of the noise distribution.

</details>

<details>

<summary>2021-04-16 17:24:20 - Introducing prior information in Weighted Likelihood Bootstrap with applications to model misspecification</summary>

- *Emilia Pompe*

- `2103.14445v2` - [abs](http://arxiv.org/abs/2103.14445v2) - [pdf](http://arxiv.org/pdf/2103.14445v2)

> We propose Posterior Bootstrap, a set of algorithms extending Weighted Likelihood Bootstrap, to properly incorporate prior information and address the problem of model misspecification in Bayesian inference. We consider two approaches to incorporating prior knowledge: the first is based on penalization of the Weighted Likelihood Bootstrap objective function, and the second uses pseudo-samples from the prior predictive distribution. We also propose methodology for hierarchical models, which was not previously known for methods based on Weighted Likelihood Bootstrap. Edgeworth expansions guide the development of our methodology and allow us to provide greater insight on properties of Weighted Likelihood Bootstrap than were previously known. Our experiments confirm the theoretical results and show a reduction in the impact of model misspecification against Bayesian inference in the misspecified setting.

</details>

<details>

<summary>2021-04-16 23:17:53 - Stability of Gibbs Posteriors from the Wasserstein Loss for Bayesian Full Waveform Inversion</summary>

- *Matthew M. Dunlop, Yunan Yang*

- `2004.03730v2` - [abs](http://arxiv.org/abs/2004.03730v2) - [pdf](http://arxiv.org/pdf/2004.03730v2)

> Recently, the Wasserstein loss function has been proven to be effective when applied to deterministic full-waveform inversion (FWI) problems. We consider the application of this loss function in Bayesian FWI so that the uncertainty can be captured in the solution. Other loss functions that are commonly used in practice are also considered for comparison. Existence and stability of the resulting Gibbs posteriors are shown on function space under weak assumptions on the prior and model. In particular, the distribution arising from the Wasserstein loss is shown to be quite stable with respect to high-frequency noise in the data. We then illustrate the difference between the resulting distributions numerically, using Laplace approximations to estimate the unknown velocity field and uncertainty associated with the estimates.

</details>

<details>

<summary>2021-04-19 08:56:20 - Bayesian Estimation of Two-Part Joint Models for a Longitudinal Semicontinuous Biomarker and a Terminal Event with R-INLA: Interests for Cancer Clinical Trial Evaluation</summary>

- *Denis Rustand, Janet van Niekerk, Haavard Rue, Christophe Tournigand, Virginie Rondeau, Laurent Briollais*

- `2010.13704v2` - [abs](http://arxiv.org/abs/2010.13704v2) - [pdf](http://arxiv.org/pdf/2010.13704v2)

> Two-part joint models for a longitudinal semicontinuous biomarker and a terminal event have been recently introduced based on frequentist estimation. The biomarker distribution is decomposed into a probability of positive value and the expected value among positive values. Shared random effects can represent the association structure between the biomarker and the terminal event. The computational burden increases compared to standard joint models with a single regression model for the biomarker. In this context, the frequentist estimation implemented in the R package frailtypack can be challenging for complex models (i.e., large number of parameters and dimension of the random effects). As an alternative, we propose a Bayesian estimation of two-part joint models based on the Integrated Nested Laplace Approximation (INLA) algorithm to alleviate the computational burden and fit more complex models. Our simulation studies show that R-INLA reduces the computation time substantially as well as the variability of the parameter estimates and improves the model convergence compared to frailtypack. We contrast the Bayesian and frequentist approaches in the analysis of two randomized cancer clinical trials (GERCOR and PRIME studies), where R-INLA suggests a stronger association between the biomarker and the risk of event. Moreover, the Bayesian approach was able to characterize subgroups of patients associated with different responses to treatment in the PRIME study while frailtypack had convergence issues. Our study suggests that the Bayesian approach using R-INLA algorithm enables broader applications of the two-part joint model to clinical applications.

</details>

<details>

<summary>2021-04-19 10:03:17 - MCMC computations for Bayesian mixture models using repulsive point processes</summary>

- *Mario Beraha, Raffaele Argiento, Jesper Møller, Alessandra Guglielmi*

- `2011.06444v2` - [abs](http://arxiv.org/abs/2011.06444v2) - [pdf](http://arxiv.org/pdf/2011.06444v2)

> Repulsive mixture models have recently gained popularity for Bayesian cluster detection. Compared to more traditional mixture models, repulsive mixture models produce a smaller number of well separated clusters. The most commonly used methods for posterior inference either require to fix a priori the number of components or are based on reversible jump MCMC computation. We present a general framework for mixture models, when the prior of the `cluster centres' is a finite repulsive point process depending on a hyperparameter, specified by a density which may depend on an intractable normalizing constant. By investigating the posterior characterization of this class of mixture models, we derive a MCMC algorithm which avoids the well-known difficulties associated to reversible jump MCMC computation. In particular, we use an ancillary variable method, which eliminates the problem of having intractable normalizing constants in the Hastings ratio. The ancillary variable method relies on a perfect simulation algorithm, and we demonstrate this is fast because the number of components is typically small. In several simulation studies and an application on sociological data, we illustrate the advantage of our new methodology over existing methods, and we compare the use of a determinantal or a repulsive Gibbs point process prior model.

</details>

<details>

<summary>2021-04-19 10:19:14 - Mixtures of Gaussian Processes for regression under multiple prior distributions</summary>

- *Sarem Seitz*

- `2104.09185v1` - [abs](http://arxiv.org/abs/2104.09185v1) - [pdf](http://arxiv.org/pdf/2104.09185v1)

> When constructing a Bayesian Machine Learning model, we might be faced with multiple different prior distributions and thus are required to properly consider them in a sensible manner in our model. While this situation is reasonably well explored for classical Bayesian Statistics, it appears useful to develop a corresponding method for complex Machine Learning problems. Given their underlying Bayesian framework and their widespread popularity, Gaussian Processes are a good candidate to tackle this task. We therefore extend the idea of Mixture models for Gaussian Process regression in order to work with multiple prior beliefs at once - both a analytical regression formula and a Sparse Variational approach are considered. In addition, we consider the usage of our approach to additionally account for the problem of prior misspecification in functional regression problems.

</details>

<details>

<summary>2021-04-19 11:43:02 - A Quantile-based Approach for Hyperparameter Transfer Learning</summary>

- *David Salinas, Huibin Shen, Valerio Perrone*

- `1909.13595v2` - [abs](http://arxiv.org/abs/1909.13595v2) - [pdf](http://arxiv.org/pdf/1909.13595v2)

> Bayesian optimization (BO) is a popular methodology to tune the hyperparameters of expensive black-box functions. Traditionally, BO focuses on a single task at a time and is not designed to leverage information from related functions, such as tuning performance objectives of the same algorithm across multiple datasets. In this work, we introduce a novel approach to achieve transfer learning across different \emph{datasets} as well as different \emph{objectives}. The main idea is to regress the mapping from hyperparameter to objective quantiles with a semi-parametric Gaussian Copula distribution, which provides robustness against different scales or outliers that can occur in different tasks. We introduce two methods to leverage this mapping: a Thompson sampling strategy as well as a Gaussian Copula process using such quantile estimate as a prior. We show that these strategies can combine the estimation of multiple objectives such as latency and accuracy, steering the hyperparameters optimization toward faster predictions for the same level of accuracy. Extensive experiments demonstrate significant improvements over state-of-the-art methods for both hyperparameter optimization and neural architecture search.

</details>

<details>

<summary>2021-04-19 12:41:23 - High-dimensional variable selection via low-dimensional adaptive learning</summary>

- *Christian Staerk, Maria Kateri, Ioannis Ntzoufras*

- `1905.00105v3` - [abs](http://arxiv.org/abs/1905.00105v3) - [pdf](http://arxiv.org/pdf/1905.00105v3)

> A stochastic search method, the so-called Adaptive Subspace (AdaSub) method, is proposed for variable selection in high-dimensional linear regression models. The method aims at finding the best model with respect to a certain model selection criterion and is based on the idea of adaptively solving low-dimensional sub-problems in order to provide a solution to the original high-dimensional problem. Any of the usual $\ell_0$-type model selection criteria can be used, such as Akaike's Information Criterion (AIC), the Bayesian Information Criterion (BIC) or the Extended BIC (EBIC), with the last being particularly suitable for high-dimensional cases. The limiting properties of the new algorithm are analysed and it is shown that, under certain conditions, AdaSub converges to the best model according to the considered criterion. In a simulation study, the performance of AdaSub is investigated in comparison to alternative methods. The effectiveness of the proposed method is illustrated via various simulated datasets and a high-dimensional real data example.

</details>

<details>

<summary>2021-04-19 14:29:46 - Bayesian Optimization with a Prior for the Optimum</summary>

- *Artur Souza, Luigi Nardi, Leonardo B. Oliveira, Kunle Olukotun, Marius Lindauer, Frank Hutter*

- `2006.14608v4` - [abs](http://arxiv.org/abs/2006.14608v4) - [pdf](http://arxiv.org/pdf/2006.14608v4)

> While Bayesian Optimization (BO) is a very popular method for optimizing expensive black-box functions, it fails to leverage the experience of domain experts. This causes BO to waste function evaluations on bad design choices (e.g., machine learning hyperparameters) that the expert already knows to work poorly. To address this issue, we introduce Bayesian Optimization with a Prior for the Optimum (BOPrO). BOPrO allows users to inject their knowledge into the optimization process in the form of priors about which parts of the input space will yield the best performance, rather than BO's standard priors over functions, which are much less intuitive for users. BOPrO then combines these priors with BO's standard probabilistic model to form a pseudo-posterior used to select which points to evaluate next. We show that BOPrO is around 6.67x faster than state-of-the-art methods on a common suite of benchmarks, and achieves a new state-of-the-art performance on a real-world hardware design application. We also show that BOPrO converges faster even if the priors for the optimum are not entirely accurate and that it robustly recovers from misleading priors.

</details>

<details>

<summary>2021-04-20 02:56:59 - Critical Window Variable Selection for Mixtures: Estimating the Impact of Multiple Air Pollutants on Stillbirth</summary>

- *Joshua L. Warren, Howard H. Chang, Lauren K. Warren, Matthew J. Strickland, Lyndsey A. Darrow, James A. Mulholland*

- `2104.09730v1` - [abs](http://arxiv.org/abs/2104.09730v1) - [pdf](http://arxiv.org/pdf/2104.09730v1)

> Understanding the role of time-varying pollution mixtures on human health is critical as people are simultaneously exposed to multiple pollutants during their lives. For vulnerable sub-populations who have well-defined exposure periods (e.g., pregnant women), questions regarding critical windows of exposure to these mixtures are important for mitigating harm. We extend Critical Window Variable Selection (CWVS) to the multipollutant setting by introducing CWVS for Mixtures (CWVSmix), a hierarchical Bayesian method that combines smoothed variable selection and temporally correlated weight parameters to (i) identify critical windows of exposure to mixtures of time-varying pollutants, (ii) estimate the time-varying relative importance of each individual pollutant and their first order interactions within the mixture, and (iii) quantify the impact of the mixtures on health. Through simulation, we show that CWVSmix offers the best balance of performance in each of these categories in comparison to competing methods. Using these approaches, we investigate the impact of exposure to multiple ambient air pollutants on the risk of stillbirth in New Jersey, 2005-2014. We find consistent elevated risk in gestational weeks 2, 16-17, and 20 for non-Hispanic Black mothers, with pollution mixtures dominated by ammonium (weeks 2, 17, 20), nitrate (weeks 2, 17), nitrogen oxides (weeks 2, 16), PM2.5 (week 2), and sulfate (week 20). The method is available in the R package CWVSmix.

</details>

<details>

<summary>2021-04-20 14:22:44 - Bayesian model inversion using stochastic spectral embedding</summary>

- *P. -R. Wagner, S. Marelli, B. Sudret*

- `2005.07380v2` - [abs](http://arxiv.org/abs/2005.07380v2) - [pdf](http://arxiv.org/pdf/2005.07380v2)

> In this paper we propose a new sampling-free approach to solve Bayesian model inversion problems that is an extension of the previously proposed spectral likelihood expansions (SLE) method. Our approach, called stochastic spectral likelihood embedding (SSLE), uses the recently presented stochastic spectral embedding (SSE) method for local spectral expansion refinement to approximate the likelihood function at the core of Bayesian inversion problems. We show that, similar to SLE, this approach results in analytical expressions for key statistics of the Bayesian posterior distribution, such as evidence, posterior moments and posterior marginals, by direct post-processing of the expansion coefficients. Because SSLE and SSE rely on the direct approximation of the likelihood function, they are in a way independent of the computational/mathematical complexity of the forward model. We further enhance the efficiency of SSLE by introducing a likelihood specific adaptive sample enrichment scheme. To showcase the performance of the proposed SSLE, we solve three problems that exhibit different kinds of complexity in the likelihood function: multimodality, high posterior concentration and high nominal dimensionality. We demonstrate how SSLE significantly improves on SLE, and present it as a promising alternative to existing inversion frameworks.

</details>

<details>

<summary>2021-04-20 16:32:46 - Partial Correlation Graphical LASSO</summary>

- *Jack Storror Carter, David Rossell, Jim Q. Smith*

- `2104.10099v1` - [abs](http://arxiv.org/abs/2104.10099v1) - [pdf](http://arxiv.org/pdf/2104.10099v1)

> Standard likelihood penalties to learn Gaussian graphical models are based on regularising the off-diagonal entries of the precision matrix. Such methods, and their Bayesian counterparts, are not invariant to scalar multiplication of the variables, unless one standardises the observed data to unit sample variances. We show that such standardisation can have a strong effect on inference and introduce a new family of penalties based on partial correlations. We show that the latter, as well as the maximum likelihood, $L_0$ and logarithmic penalties are scale invariant. We illustrate the use of one such penalty, the partial correlation graphical LASSO, which sets an $L_{1}$ penalty on partial correlations. The associated optimization problem is no longer convex, but is conditionally convex. We show via simulated examples and in two real datasets that, besides being scale invariant, there can be important gains in terms of inference.

</details>

<details>

<summary>2021-04-20 19:42:46 - Constrained Bayesian Hierarchical Models for Gaussian Data: A Model Selection Criterion Approach</summary>

- *Qingying Zong, Jonathan R. Bradley*

- `2104.10222v1` - [abs](http://arxiv.org/abs/2104.10222v1) - [pdf](http://arxiv.org/pdf/2104.10222v1)

> Consider the setting where there are B>1 candidate statistical models, and one is interested in model selection. Two common approaches to solve this problem are to select a single model or to combine the candidate models through model averaging. Instead, we select a subset of the combined parameter space associated with the models. Specifically, a model averaging perspective is used to increase the parameter space, and a model selection criterion is used to select a subset of this expanded parameter space. We account for the variability of the criterion by adapting Yekutieli (2012)'s method to Bayesian model averaging (BMA). Yekutieli (2012)'s method treats model selection as a truncation problem. We truncate the joint support of the data and the parameter space to only include small values of the covariance penalized error (CPE) criterion. The CPE is a general expression that contains several information criteria as special cases. Simulation results show that as long as the truncated set does not have near zero probability, we tend to obtain lower mean squared error than BMA. Additional theoretical results are provided that provide the foundation for these observations. We apply our approach to a dataset consisting of American Community Survey (ACS) period estimates to illustrate that this perspective can lead to improvements of a single model.

</details>

<details>

<summary>2021-04-21 10:48:22 - Hierarchical clustering with discrete latent variable models and the integrated classification likelihood</summary>

- *Etienne Côme, Nicolas Jouvin, Pierre Latouche, Charles Bouveyron*

- `2002.11577v3` - [abs](http://arxiv.org/abs/2002.11577v3) - [pdf](http://arxiv.org/pdf/2002.11577v3)

> Finding a set of nested partitions of a dataset is useful to uncover relevant structure at different scales, and is often dealt with a data-dependent methodology. In this paper, we introduce a general two-step methodology for model-based hierarchical clustering. Considering the integrated classification likelihood criterion as an objective function, this work applies to every discrete latent variable models (DLVMs) where this quantity is tractable. The first step of the methodology involves maximizing the criterion with respect to the partition. Addressing the known problem of sub-optimal local maxima found by greedy hill climbing heuristics, we introduce a new hybrid algorithm based on a genetic algorithm efficiently exploring the space of solutions. The resulting algorithm carefully combines and merges different solutions, and allows the joint inference of the number $K$ of clusters as well as the clusters themselves. Starting from this natural partition, the second step of the methodology is based on a bottom-up greedy procedure to extract a hierarchy of clusters. In a Bayesian context, this is achieved by considering the Dirichlet cluster proportion prior parameter $\alpha$ as a regularization term controlling the granularity of the clustering. A new approximation of the criterion is derived as a log-linear function of $\alpha$, enabling a simple functional form of the merge decision criterion. This second step allows the exploration of the clustering at coarser scales. The proposed approach is compared with existing strategies on simulated as well as real settings, and its results are shown to be particularly relevant. A reference implementation of this work is available in the R package greed accompanying the paper.

</details>

<details>

<summary>2021-04-21 12:23:41 - Asymmetric linear double autoregression</summary>

- *Songhua Tan, Qianqian Zhu*

- `2007.09649v3` - [abs](http://arxiv.org/abs/2007.09649v3) - [pdf](http://arxiv.org/pdf/2007.09649v3)

> This paper proposes the asymmetric linear double autoregression, which jointly models the conditional mean and conditional heteroscedasticity characterized by asymmetric effects. A sufficient condition is established for the existence of a strictly stationary solution. With a quasi-maximum likelihood estimation (QMLE) procedure introduced, a Bayesian information criterion (BIC) and its modified version are proposed for model selection. To detect asymmetric effects in the volatility, the Wald, Lagrange multiplier and quasi-likelihood ratio test statistics are put forward, and their limiting distributions are established under both null and local alternative hypotheses. Moreover, a mixed portmanteau test is constructed to check the adequacy of the fitted model. All asymptotic properties of inference tools including QMLE, BICs, asymmetric tests and the mixed portmanteau test, are established without any moment condition on the data process, which makes the new model and its inference tools applicable for heavy-tailed data. Simulation studies indicate that the proposed methods perform well in finite samples, and an empirical application to S\&P500 Index illustrates the usefulness of the new model.

</details>

<details>

<summary>2021-04-22 07:36:40 - A tractable Bayesian joint model for longitudinal and survival data</summary>

- *Danilo Alvares, Francisco Javier Rubio*

- `2104.10906v1` - [abs](http://arxiv.org/abs/2104.10906v1) - [pdf](http://arxiv.org/pdf/2104.10906v1)

> We introduce a numerically tractable formulation of Bayesian joint models for longitudinal and survival data. The longitudinal process is modelled using generalised linear mixed models, while the survival process is modelled using a parametric general hazard structure. The two processes are linked by sharing fixed and random effects, separating the effects that play a role at the time scale from those that affect the hazard scale. This strategy allows for the inclusion of non-linear and time-dependent effects while avoiding the need for numerical integration, which facilitates the implementation of the proposed joint model. We explore the use of flexible parametric distributions for modelling the baseline hazard function which can capture the basic shapes of interest in practice. We discuss prior elicitation based on the interpretation of the parameters. We present an extensive simulation study, where we analyse the inferential properties of the proposed models, and illustrate the trade-off between flexibility, sample size, and censoring. We also apply our proposal to two real data applications in order to demonstrate the adaptability of our formulation both in univariate time-to-event data and in a competing risks framework. The methodology is implemented in rstan.

</details>

<details>

<summary>2021-04-22 17:24:00 - Variational Bayesian Supertrees</summary>

- *Michael Karcher, Cheng Zhang, Frederick A Matsen IV*

- `2104.11191v1` - [abs](http://arxiv.org/abs/2104.11191v1) - [pdf](http://arxiv.org/pdf/2104.11191v1)

> Given overlapping subsets of a set of taxa (e.g. species), and posterior distributions on phylogenetic tree topologies for each of these taxon sets, how can we infer a posterior distribution on phylogenetic tree topologies for the entire taxon set? Although the equivalent problem for in the non-Bayesian case has attracted substantial research, the Bayesian case has not attracted the attention it deserves. In this paper we develop a variational Bayes approach to this problem and demonstrate its effectiveness.

</details>

<details>

<summary>2021-04-23 07:48:42 - Bayesian Neural Architecture Search using A Training-Free Performance Metric</summary>

- *Andrés Camero, Hao Wang, Enrique Alba, Thomas Bäck*

- `2001.10726v2` - [abs](http://arxiv.org/abs/2001.10726v2) - [pdf](http://arxiv.org/pdf/2001.10726v2)

> Recurrent neural networks (RNNs) are a powerful approach for time series prediction. However, their performance is strongly affected by their architecture and hyperparameter settings. The architecture optimization of RNNs is a time-consuming task, where the search space is typically a mixture of real, integer and categorical values. To allow for shrinking and expanding the size of the network, the representation of architectures often has a variable length. In this paper, we propose to tackle the architecture optimization problem with a variant of the Bayesian Optimization (BO) algorithm. To reduce the evaluation time of candidate architectures the Mean Absolute Error Random Sampling (MRS), a training-free method to estimate the network performance, is adopted as the objective function for BO. Also, we propose three fixed-length encoding schemes to cope with the variable-length architecture representation. The result is a new perspective on accurate and efficient design of RNNs, that we validate on three problems. Our findings show that 1) the BO algorithm can explore different network architectures using the proposed encoding schemes and successfully designs well-performing architectures, and 2) the optimization time is significantly reduced by using MRS, without compromising the performance as compared to the architectures obtained from the actual training procedure.

</details>

<details>

<summary>2021-04-23 09:26:19 - Efficient Bernoulli factory MCMC for intractable posteriors</summary>

- *Dootika Vats, Flávio Gonçalves, Krzysztof Łatuszyński, Gareth O. Roberts*

- `2004.07471v3` - [abs](http://arxiv.org/abs/2004.07471v3) - [pdf](http://arxiv.org/pdf/2004.07471v3)

> Accept-reject based Markov chain Monte Carlo (MCMC) algorithms have traditionally utilised acceptance probabilities that can be explicitly written as a function of the ratio of the target density at the two contested points. This feature is rendered almost useless in Bayesian posteriors with unknown functional forms. We introduce a new family of MCMC acceptance probabilities that has the distinguishing feature of not being a function of the ratio of the target density at the two points. We present two stable Bernoulli factories that generate events within this class of acceptance probabilities. The efficiency of our methods rely on obtaining reasonable local upper or lower bounds on the target density and we present two classes of problems where such bounds are viable: Bayesian inference for diffusions and MCMC on constrained spaces. The resulting portkey Barker's algorithms are exact and computationally more efficient that the current state-of-the-art.

</details>

<details>

<summary>2021-04-23 16:43:01 - A Gaussian Process Model of Cross-Category Dynamics in Brand Choice</summary>

- *Ryan Dew, Yuhao Fan*

- `2104.11702v1` - [abs](http://arxiv.org/abs/2104.11702v1) - [pdf](http://arxiv.org/pdf/2104.11702v1)

> Understanding individual customers' sensitivities to prices, promotions, brand, and other aspects of the marketing mix is fundamental to a wide swath of marketing problems, including targeting and pricing. Companies that operate across many product categories have a unique opportunity, insofar as they can use purchasing data from one category to augment their insights in another. Such cross-category insights are especially crucial in situations where purchasing data may be rich in one category, and scarce in another. An important aspect of how consumers behave across categories is dynamics: preferences are not stable over time, and changes in individual-level preference parameters in one category may be indicative of changes in other categories, especially if those changes are driven by external factors. Yet, despite the rich history of modeling cross-category preferences, the marketing literature lacks a framework that flexibly accounts for \textit{correlated dynamics}, or the cross-category interlinkages of individual-level sensitivity dynamics. In this work, we propose such a framework, leveraging individual-level, latent, multi-output Gaussian processes to build a nonparametric Bayesian choice model that allows information sharing of preference parameters across customers, time, and categories. We apply our model to grocery purchase data, and show that our model detects interesting dynamics of customers' price sensitivities across multiple categories. Managerially, we show that capturing correlated dynamics yields substantial predictive gains, relative to benchmarks. Moreover, we find that capturing correlated dynamics can have implications for understanding changes in consumers preferences over time, and developing targeted marketing strategies based on those dynamics.

</details>

<details>

<summary>2021-04-23 19:47:41 - Error-guided likelihood-free MCMC</summary>

- *Volodimir Begy, Erich Schikuta*

- `2010.06735v3` - [abs](http://arxiv.org/abs/2010.06735v3) - [pdf](http://arxiv.org/pdf/2010.06735v3)

> This work presents a novel posterior inference method for models with intractable evidence and likelihood functions. Error-guided likelihood-free MCMC, or EG-LF-MCMC in short, has been developed for scientific applications, where a researcher is interested in obtaining approximate posterior densities over model parameters, while avoiding the need for expensive training of component estimators on full observational data or the tedious design of expressive summary statistics, as in related approaches. Our technique is based on two phases. In the first phase, we draw samples from the prior, simulate respective observations and record their errors $\epsilon$ in relation to the true observation. We train a classifier to distinguish between corresponding and non-corresponding $(\epsilon, \boldsymbol{\theta})$-tuples. In the second stage the said classifier is conditioned on the smallest recorded $\epsilon$ value from the training set and employed for the calculation of transition probabilities in a Markov Chain Monte Carlo sampling procedure. By conditioning the MCMC on specific $\epsilon$ values, our method may also be used in an amortized fashion to infer posterior densities for observations, which are located a given distance away from the observed data. We evaluate the proposed method on benchmark problems with semantically and structurally different data and compare its performance against the state of the art approximate Bayesian computation (ABC).

</details>

<details>

<summary>2021-04-23 22:43:16 - High-dimensional near-optimal experiment design for drug discovery via Bayesian sparse sampling</summary>

- *Hannes Eriksson, Christos Dimitrakakis, Lars Carlsson*

- `2104.11834v1` - [abs](http://arxiv.org/abs/2104.11834v1) - [pdf](http://arxiv.org/pdf/2104.11834v1)

> We study the problem of performing automated experiment design for drug screening through Bayesian inference and optimisation. In particular, we compare and contrast the behaviour of linear-Gaussian models and Gaussian processes, when used in conjunction with upper confidence bound algorithms, Thompson sampling, or bounded horizon tree search. We show that non-myopic sophisticated exploration techniques using sparse tree search have a distinct advantage over methods such as Thompson sampling or upper confidence bounds in this setting. We demonstrate the significant superiority of the approach over existing and synthetic datasets of drug toxicity.

</details>

<details>

<summary>2021-04-25 04:51:36 - Contraction of a quasi-Bayesian model with shrinkage priors in precision matrix estimation</summary>

- *Ruoyang Zhang, Yisha Yao, Malay Ghosh*

- `2104.12060v1` - [abs](http://arxiv.org/abs/2104.12060v1) - [pdf](http://arxiv.org/pdf/2104.12060v1)

> Currently several Bayesian approaches are available to estimate large sparse precision matrices, including Bayesian graphical Lasso (Wang, 2012), Bayesian structure learning (Banerjee and Ghosal, 2015), and graphical horseshoe (Li et al., 2019). Although these methods have exhibited nice empirical performances, in general they are computationally expensive. Moreover, we have limited knowledge about the theoretical properties, e.g., posterior contraction rate, of graphical Bayesian Lasso and graphical horseshoe. In this paper, we propose a new method that integrates some commonly used continuous shrinkage priors into a quasi-Bayesian framework featured by a pseudo-likelihood. Under mild conditions, we establish an optimal posterior contraction rate for the proposed method. Compared to existing approaches, our method has two main advantages. First, our method is computationally more efficient while achieving similar error rate; second, our framework is more amenable to theoretical analysis. Extensive simulation experiments and the analysis on a real data set are supportive of our theoretical results.

</details>

<details>

<summary>2021-04-25 05:40:32 - Bayesian Analysis on Limiting the Student-$t$ Linear Regression Model</summary>

- *Yoshiko Hayashi*

- `2008.04522v3` - [abs](http://arxiv.org/abs/2008.04522v3) - [pdf](http://arxiv.org/pdf/2008.04522v3)

> For the outlier problem in linear regression models, the Student-$t$ linear regression model is one of the common methods for robust modeling and is widely adopted in the literature. However, most of them applies it without careful theoretical consideration. This study provides the practically useful and quite simple conditions to ensure that the Student-$t$ linear regression model is robust against an outlier in the $y$-direction using regular variation theory.

</details>

<details>

<summary>2021-04-25 11:41:18 - Variational Approximation of Factor Stochastic Volatility Models</summary>

- *David Gunawan, Robert Kohn, David Nott*

- `2010.06738v2` - [abs](http://arxiv.org/abs/2010.06738v2) - [pdf](http://arxiv.org/pdf/2010.06738v2)

> Estimation and prediction in high dimensional multivariate factor stochastic volatility models is an important and active research area because such models allow a parsimonious representation of multivariate stochastic volatility. Bayesian inference for factor stochastic volatility models is usually done by Markov chain Monte Carlo methods, often by particle Markov chain Monte Carlo, which are usually slow for high dimensional or long time series because of the large number of parameters and latent states involved. Our article makes two contributions. The first is to propose fast and accurate variational Bayes methods to approximate the posterior distribution of the states and parameters in factor stochastic volatility models. The second contribution is to extend this batch methodology to develop fast sequential variational updates for prediction as new observations arrive. The methods are applied to simulated and real datasets and shown to produce good approximate inference and prediction compared to the latest particle Markov chain Monte Carlo approaches, but are much faster.

</details>

<details>

<summary>2021-04-25 19:09:38 - Variational Inference in high-dimensional linear regression</summary>

- *Sumit Mukherjee, Subhabrata Sen*

- `2104.12232v1` - [abs](http://arxiv.org/abs/2104.12232v1) - [pdf](http://arxiv.org/pdf/2104.12232v1)

> We study high-dimensional Bayesian linear regression with product priors. Using the nascent theory of non-linear large deviations (Chatterjee and Dembo,2016), we derive sufficient conditions for the leading-order correctness of the naive mean-field approximation to the log-normalizing constant of the posterior distribution. Subsequently, assuming a true linear model for the observed data, we derive a limiting infinite dimensional variational formula for the log normalizing constant of the posterior. Furthermore, we establish that under an additional "separation" condition, the variational problem has a unique optimizer, and this optimizer governs the probabilistic properties of the posterior distribution. We provide intuitive sufficient conditions for the validity of this "separation" condition. Finally, we illustrate our results on concrete examples with specific design matrices.

</details>

<details>

<summary>2021-04-25 20:33:17 - Rate-optimal refinement strategies for local approximation MCMC</summary>

- *Andrew D. Davis, Youssef Marzouk, Aaron Smith, Natesh Pillai*

- `2006.00032v2` - [abs](http://arxiv.org/abs/2006.00032v2) - [pdf](http://arxiv.org/pdf/2006.00032v2)

> Many Bayesian inference problems involve target distributions whose density functions are computationally expensive to evaluate. Replacing the target density with a local approximation based on a small number of carefully chosen density evaluations can significantly reduce the computational expense of Markov chain Monte Carlo (MCMC) sampling. Moreover, continual refinement of the local approximation can guarantee asymptotically exact sampling. We devise a new strategy for balancing the decay rate of the bias due to the approximation with that of the MCMC variance. We prove that the error of the resulting local approximation MCMC (LA-MCMC) algorithm decays at roughly the expected $1/\sqrt{T}$ rate, and we demonstrate this rate numerically. We also introduce an algorithmic parameter that guarantees convergence given very weak tail bounds, significantly strengthening previous convergence results. Finally, we apply LA-MCMC to a computationally intensive Bayesian inverse problem arising in groundwater hydrology.

</details>

<details>

<summary>2021-04-25 23:23:22 - Stopping Criterion Design for Recursive Bayesian Classification: Analysis and Decision Geometry</summary>

- *Aziz Kocanaogullari, Murat Akcakaya, Deniz Erdogmus*

- `2007.15568v2` - [abs](http://arxiv.org/abs/2007.15568v2) - [pdf](http://arxiv.org/pdf/2007.15568v2)

> Systems that are based on recursive Bayesian updates for classification limit the cost of evidence collection through certain stopping/termination criteria and accordingly enforce decision making. Conventionally, two termination criteria based on pre-defined thresholds over (i) the maximum of the state posterior distribution; and (ii) the state posterior uncertainty are commonly used. In this paper, we propose a geometric interpretation over the state posterior progression and accordingly we provide a point-by-point analysis over the disadvantages of using such conventional termination criteria. For example, through the proposed geometric interpretation we show that confidence thresholds defined over maximum of the state posteriors suffer from stiffness that results in unnecessary evidence collection whereas uncertainty based thresholding methods are fragile to number of categories and terminate prematurely if some state candidates are already discovered to be unfavorable. Moreover, both types of termination methods neglect the evolution of posterior updates. We then propose a new stopping/termination criterion with a geometrical insight to overcome the limitations of these conventional methods and provide a comparison in terms of decision accuracy and speed. We validate our claims using simulations and using real experimental data obtained through a brain computer interfaced typing system.

</details>

<details>

<summary>2021-04-26 04:54:40 - Model-based Differentially Private Data Synthesis and Statistical Inference in Multiply Synthetic Differentially Private Data</summary>

- *Fang Liu*

- `1606.08052v3` - [abs](http://arxiv.org/abs/1606.08052v3) - [pdf](http://arxiv.org/pdf/1606.08052v3)

> We propose the approach of model-based differentially private synthesis (modips) in the Bayesian framework for releasing individual-level surrogate/synthetic datasets with privacy guarantees given the original data. The modips technique integrates the concept of differential privacy into model-based data synthesis. We introduce several variants for the general modips approach and different procedures to obtaining privacy-preserving posterior samples, a key step in modips. The uncertainty from the sanitization and synthetic process in modips can be accounted for by releasing multiple synthetic datasets and quantified via an inferential combination rule that is proposed in this paper. We run empirical studies to examine the impacts of the number of synthetic sets and the privacy budget allocation schemes on the inference based on synthetic data.

</details>

<details>

<summary>2021-04-26 07:40:57 - Identification of high-energy astrophysical point sources via hierarchical Bayesian nonparametric clustering</summary>

- *Andrea Sottosanti, Mauro Bernardi, Alessandra R. Brazzale, Alex Geringer-Sameth, David C. Stenning, Roberto Trotta, David A. van Dyk*

- `2104.11492v2` - [abs](http://arxiv.org/abs/2104.11492v2) - [pdf](http://arxiv.org/pdf/2104.11492v2)

> The light we receive from distant astrophysical objects carries information about their origins and the physical mechanisms that power them. The study of these signals, however, is complicated by the fact that observations are often a mixture of the light emitted by multiple localized sources situated in a spatially-varying background. A general algorithm to achieve robust and accurate source identification in this case remains an open question in astrophysics.   This paper focuses on high-energy light (such as X-rays and gamma-rays), for which observatories can detect individual photons (quanta of light), measuring their incoming direction, arrival time, and energy. Our proposed Bayesian methodology uses both the spatial and energy information to identify point sources, that is, separate them from the spatially-varying background, to estimate their number, and to compute the posterior probabilities that each photon originated from each identified source. This is accomplished via a Dirichlet process mixture while the background is simultaneously reconstructed via a flexible Bayesian nonparametric model based on B-splines. Our proposed method is validated with a suite of simulation studies and illustrated with an application to a complex region of the sky observed by the \emph{Fermi} Gamma-ray Space Telescope.

</details>

<details>

<summary>2021-04-26 09:06:02 - Predicting Depressive Symptom Severity through Individuals' Nearby Bluetooth Devices Count Data Collected by Mobile Phones: A Preliminary Longitudinal Study</summary>

- *Yuezhou Zhang, Amos A Folarin, Shaoxiong Sun, Nicholas Cummins, Yatharth Ranjan, Zulqarnain Rashid, Pauline Conde, Callum Stewart, Petroula Laiou, Faith Matcham, Carolin Oetzmann, Femke Lamers, Sara Siddi, Sara Simblett, Aki Rintala, David C Mohr, Inez Myin-Germeys, Til Wykes, Josep Maria Haro, Brenda WJH Pennix, Vaibhav A Narayan, Peter Annas, Matthew Hotopf, Richard JB Dobson*

- `2104.12407v1` - [abs](http://arxiv.org/abs/2104.12407v1) - [pdf](http://arxiv.org/pdf/2104.12407v1)

> The Bluetooth sensor embedded in mobile phones provides an unobtrusive, continuous, and cost-efficient means to capture individuals' proximity information, such as the nearby Bluetooth devices count (NBDC). The continuous NBDC data can partially reflect individuals' behaviors and status, such as social connections and interactions, working status, mobility, and social isolation and loneliness, which were found to be significantly associated with depression by previous survey-based studies. This paper aims to explore the NBDC data's value in predicting depressive symptom severity as measured via the 8-item Patient Health Questionnaire (PHQ-8). The data used in this paper included 2,886 bi-weekly PHQ-8 records collected from 316 participants recruited from three study sites in the Netherlands, Spain, and the UK as part of the EU RADAR-CNS study. From the NBDC data two weeks prior to each PHQ-8 score, we extracted 49 Bluetooth features, including statistical features and nonlinear features for measuring periodicity and regularity of individuals' life rhythms. Linear mixed-effect models were used to explore associations between Bluetooth features and the PHQ-8 score. We then applied hierarchical Bayesian linear regression models to predict the PHQ-8 score from the extracted Bluetooth features. A number of significant associations were found between Bluetooth features and depressive symptom severity. Compared with commonly used machine learning models, the proposed hierarchical Bayesian linear regression model achieved the best prediction metrics, R2= 0.526, and root mean squared error (RMSE) of 3.891. Bluetooth features can explain an extra 18.8% of the variance in the PHQ-8 score relative to the baseline model without Bluetooth features (R2=0.338, RMSE = 4.547).

</details>

<details>

<summary>2021-04-26 13:02:43 - Bayesian predictive inference without a prior</summary>

- *Patrizia Berti, Emanuela Dreassi, Fabrizio Leisen, Pietro Rigo, Luca Pratelli*

- `2104.11643v2` - [abs](http://arxiv.org/abs/2104.11643v2) - [pdf](http://arxiv.org/pdf/2104.11643v2)

> Let $(X_n:n\ge 1)$ be a sequence of random observations. Let $\sigma_n(\cdot)=P\bigl(X_{n+1}\in\cdot\mid X_1,\ldots,X_n\bigr)$ be the $n$-th predictive distribution and $\sigma_0(\cdot)=P(X_1\in\cdot)$ the marginal distribution of $X_1$. In a Bayesian framework, to make predictions on $(X_n)$, one only needs the collection $\sigma=(\sigma_n:n\ge 0)$. Because of the Ionescu-Tulcea theorem, $\sigma$ can be assigned directly, without passing through the usual prior/posterior scheme. One main advantage is that no prior probability has to be selected. In this paper, $\sigma$ is subjected to two requirements: (i) The resulting sequence $(X_n)$ is conditionally identically distributed, in the sense of Berti, Pratelli and Rigo (2004); (ii) Each $\sigma_{n+1}$ is a simple recursive update of $\sigma_n$. Various new $\sigma$ satisfying (i)-(ii) are introduced and investigated. For such $\sigma$, the asymptotics of $\sigma_n$, as $n\rightarrow\infty$, is determined. In some cases, the probability distribution of $(X_n)$ is also evaluated.

</details>

<details>

<summary>2021-04-26 16:47:43 - Online parameter inference for the simulation of a Bunsen flame using heteroscedastic Bayesian neural network ensembles</summary>

- *Maximilian L. Croci, Ushnish Sengupta, Matthew P. Juniper*

- `2104.13201v1` - [abs](http://arxiv.org/abs/2104.13201v1) - [pdf](http://arxiv.org/pdf/2104.13201v1)

> This paper proposes a Bayesian data-driven machine learning method for the online inference of the parameters of a G-equation model of a ducted, premixed flame. Heteroscedastic Bayesian neural network ensembles are trained on a library of 1.7 million flame fronts simulated in LSGEN2D, a G-equation solver, to learn the Bayesian posterior distribution of the model parameters given observations. The ensembles are then used to infer the parameters of Bunsen flame experiments so that the dynamics of these can be simulated in LSGEN2D. This allows the surface area variation of the flame edge, a proxy for the heat release rate, to be calculated. The proposed method provides cheap and online parameter and uncertainty estimates matching results obtained with the ensemble Kalman filter, at less computational cost. This enables fast and reliable simulation of the combustion process.

</details>

<details>

<summary>2021-04-26 17:24:29 - Invariant polynomials and machine learning</summary>

- *Ward Haddadin*

- `2104.12733v1` - [abs](http://arxiv.org/abs/2104.12733v1) - [pdf](http://arxiv.org/pdf/2104.12733v1)

> We present an application of invariant polynomials in machine learning. Using the methods developed in previous work, we obtain two types of generators of the Lorentz- and permutation-invariant polynomials in particle momenta; minimal algebra generators and Hironaka decompositions. We discuss and prove some approximation theorems to make use of these invariant generators in machine learning algorithms in general and in neural networks specifically. By implementing these generators in neural networks applied to regression tasks, we test the improvements in performance under a wide range of hyperparameter choices and find a reduction of the loss on training data and a significant reduction of the loss on validation data. For a different approach on quantifying the performance of these neural networks, we treat the problem from a Bayesian inference perspective and employ nested sampling techniques to perform model comparison. Beyond a certain network size, we find that networks utilising Hironaka decompositions perform the best.

</details>

<details>

<summary>2021-04-26 19:20:37 - Latent Map Gaussian Processes for Mixed Variable Metamodeling</summary>

- *Nicholas Oune, Ramin Bostanabad*

- `2102.03935v2` - [abs](http://arxiv.org/abs/2102.03935v2) - [pdf](http://arxiv.org/pdf/2102.03935v2)

> Gaussian processes (GPs) are ubiquitously used in sciences and engineering as metamodels. Standard GPs, however, can only handle numerical or quantitative variables. In this paper, we introduce latent map Gaussian processes (LMGPs) that inherit the attractive properties of GPs and are also applicable to mixed data which have both quantitative and qualitative inputs. The core idea behind LMGPs is to learn a continuous, low-dimensional latent space or manifold which encodes all qualitative inputs. To learn this manifold, we first assign a unique prior vector representation to each combination of qualitative inputs. We then use a low-rank linear map to project these priors on a manifold that characterizes the posterior representations. As the posteriors are quantitative, they can be directly used in any standard correlation function such as the Gaussian or Matern. Hence, the optimal map and the corresponding manifold, along with other hyperparameters of the correlation function, can be systematically learned via maximum likelihood estimation. Through a wide range of analytic and real-world examples, we demonstrate the advantages of LMGPs over state-of-the-art methods in terms of accuracy and versatility. In particular, we show that LMGPs can handle variable-length inputs, have an explainable neural network interpretation, and provide insights into how qualitative inputs affect the response or interact with each other. We also employ LMGPs in Bayesian optimization and illustrate that they can discover optimal compound compositions more efficiently than conventional methods that convert compositions to qualitative variables via manual featurization.

</details>

<details>

<summary>2021-04-27 00:23:56 - A Comprehensive Survey of Inverse Uncertainty Quantification of Physical Model Parameters in Nuclear System Thermal-Hydraulics Codes</summary>

- *Xu Wu, Ziyu Xie, Farah Alsafadi, Tomasz Kozlowski*

- `2104.12919v1` - [abs](http://arxiv.org/abs/2104.12919v1) - [pdf](http://arxiv.org/pdf/2104.12919v1)

> Uncertainty Quantification (UQ) is an essential step in computational model validation because assessment of the model accuracy requires a concrete, quantifiable measure of uncertainty in the model predictions. The concept of UQ in the nuclear community generally means forward UQ (FUQ), in which the information flow is from the inputs to the outputs. Inverse UQ (IUQ), in which the information flow is from the model outputs and experimental data to the inputs, is an equally important component of UQ but has been significantly underrated until recently. FUQ requires knowledge in the input uncertainties which has been specified by expert opinion or user self-evaluation. IUQ is defined as the process to inversely quantify the input uncertainties based on experimental data. This review paper aims to provide a comprehensive and comparative discussion of the major aspects of the IUQ methodologies that have been used on the physical models in system thermal-hydraulics codes. IUQ methods can be categorized by three main groups: frequentist (deterministic), Bayesian (probabilistic), and empirical (design-of-experiments). We used eight metrics to evaluate an IUQ method, including solidity, complexity, accessibility, independence, flexibility, comprehensiveness, transparency, and tractability. Twelve IUQ methods are reviewed, compared, and evaluated based on these eight metrics. Such comparative evaluation will provide a good guidance for users to select a proper IUQ method based on the IUQ problem under investigation.

</details>

<details>

<summary>2021-04-27 02:28:41 - The SPDE Approach to Matérn Fields: Graph Representations</summary>

- *Daniel Sanz-Alonso, Ruiyi Yang*

- `2004.08000v3` - [abs](http://arxiv.org/abs/2004.08000v3) - [pdf](http://arxiv.org/pdf/2004.08000v3)

> This paper investigates Gaussian Markov random field approximations to nonstationary Gaussian fields using graph representations of stochastic partial differential equations. We establish approximation error guarantees building on the theory of spectral convergence of graph Laplacians. The proposed graph representations provide a generalization of the Mat\'ern model to unstructured point clouds, and facilitate inference and sampling using linear algebra methods for sparse matrices. In addition, they bridge and unify several models in Bayesian inverse problems, spatial statistics and graph-based machine learning. We demonstrate through examples in these three disciplines that the unity revealed by graph representations facilitates the exchange of ideas across them.

</details>

<details>

<summary>2021-04-27 09:19:13 - The Family of Alpha,[a,b] Stochastic Orders: Risk vs. Expected Value</summary>

- *Bar Light, Andres Perlroth*

- `1908.06398v5` - [abs](http://arxiv.org/abs/1908.06398v5) - [pdf](http://arxiv.org/pdf/1908.06398v5)

> In this paper we provide a novel family of stochastic orders that generalizes second order stochastic dominance, which we call the $\alpha,[a,b]$-concave stochastic orders.   These stochastic orders are generated by a novel set of "very" concave functions where $\alpha$ parameterizes the degree of concavity. The $\alpha,[a,b]$-concave stochastic orders allow us to derive novel comparative statics results for important applications in economics that cannot be derived using previous stochastic orders. In particular, our comparative statics results are useful when an increase in a lottery's riskiness changes the agent's optimal action in the opposite direction to an increase in the lottery's expected value. For this kind of situation, we provide a tool to determine which of these two forces dominates -- riskiness or expected value. We apply our results in consumption-savings problems, self-protection problems, and in a Bayesian game.

</details>

<details>

<summary>2021-04-27 14:33:30 - A statistical theory of cold posteriors in deep neural networks</summary>

- *Laurence Aitchison*

- `2008.05912v2` - [abs](http://arxiv.org/abs/2008.05912v2) - [pdf](http://arxiv.org/pdf/2008.05912v2)

> To get Bayesian neural networks to perform comparably to standard neural networks it is usually necessary to artificially reduce uncertainty using a "tempered" or "cold" posterior. This is extremely concerning: if the prior is accurate, Bayes inference/decision theory is optimal, and any artificial changes to the posterior should harm performance. While this suggests that the prior may be at fault, here we argue that in fact, BNNs for image classification use the wrong likelihood. In particular, standard image benchmark datasets such as CIFAR-10 are carefully curated. We develop a generative model describing curation which gives a principled Bayesian account of cold posteriors, because the likelihood under this new generative model closely matches the tempered likelihoods used in past work.

</details>

<details>

<summary>2021-04-27 23:28:11 - Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors</summary>

- *Nikhil Mehta, Kevin J Liang, Vinay K Verma, Lawrence Carin*

- `2004.10098v3` - [abs](http://arxiv.org/abs/2004.10098v3) - [pdf](http://arxiv.org/pdf/2004.10098v3)

> Naively trained neural networks tend to experience catastrophic forgetting in sequential task settings, where data from previous tasks are unavailable. A number of methods, using various model expansion strategies, have been proposed recently as possible solutions. However, determining how much to expand the model is left to the practitioner, and often a constant schedule is chosen for simplicity, regardless of how complex the incoming task is. Instead, we propose a principled Bayesian nonparametric approach based on the Indian Buffet Process (IBP) prior, letting the data determine how much to expand the model complexity. We pair this with a factorization of the neural network's weight matrices. Such an approach allows the number of factors of each weight matrix to scale with the complexity of the task, while the IBP prior encourages sparse weight factor selection and factor reuse, promoting positive knowledge transfer between tasks. We demonstrate the effectiveness of our method on a number of continual learning benchmarks and analyze how weight factors are allocated and reused throughout the training.

</details>

<details>

<summary>2021-04-27 23:28:44 - Latent Causal Invariant Model</summary>

- *Xinwei Sun, Botong Wu, Xiangyu Zheng, Chang Liu, Wei Chen, Tao Qin, Tie-yan Liu*

- `2011.02203v4` - [abs](http://arxiv.org/abs/2011.02203v4) - [pdf](http://arxiv.org/pdf/2011.02203v4)

> Current supervised learning can learn spurious correlation during the data-fitting process, imposing issues regarding interpretability, out-of-distribution (OOD) generalization, and robustness. To avoid spurious correlation, we propose a Latent Causal Invariance Model (LaCIM) which pursues causal prediction. Specifically, we introduce latent variables that are separated into (a) output-causative factors and (b) others that are spuriously correlated to the output via confounders, to model the underlying causal factors. We further assume the generating mechanisms from latent space to observed data to be causally invariant. We give the identifiable claim of such invariance, particularly the disentanglement of output-causative factors from others, as a theoretical guarantee for precise inference and avoiding spurious correlation. We propose a Variational-Bayesian-based method for estimation and to optimize over the latent space for prediction. The utility of our approach is verified by improved interpretability, prediction power on various OOD scenarios (including healthcare) and robustness on security.

</details>

<details>

<summary>2021-04-28 12:58:19 - Greed is Good: Exploration and Exploitation Trade-offs in Bayesian Optimisation</summary>

- *George De Ath, Richard M. Everson, Alma A. M. Rahat, Jonathan E. Fieldsend*

- `1911.12809v2` - [abs](http://arxiv.org/abs/1911.12809v2) - [pdf](http://arxiv.org/pdf/1911.12809v2)

> The performance of acquisition functions for Bayesian optimisation to locate the global optimum of continuous functions is investigated in terms of the Pareto front between exploration and exploitation. We show that Expected Improvement (EI) and the Upper Confidence Bound (UCB) always select solutions to be expensively evaluated on the Pareto front, but Probability of Improvement is not guaranteed to do so and Weighted Expected Improvement does so only for a restricted range of weights.   We introduce two novel $\epsilon$-greedy acquisition functions. Extensive empirical evaluation of these together with random search, purely exploratory, and purely exploitative search on 10 benchmark problems in 1 to 10 dimensions shows that $\epsilon$-greedy algorithms are generally at least as effective as conventional acquisition functions (e.g., EI and UCB), particularly with a limited budget. In higher dimensions $\epsilon$-greedy approaches are shown to have improved performance over conventional approaches. These results are borne out on a real world computational fluid dynamics optimisation problem and a robotics active learning problem. Our analysis and experiments suggest that the most effective strategy, particularly in higher dimensions, is to be mostly greedy, occasionally selecting a random exploratory solution.

</details>

<details>

<summary>2021-04-28 13:37:10 - Distributional Gaussian Process Layers for Outlier Detection in Image Segmentation</summary>

- *Sebastian G. Popescu, David J. Sharp, James H. Cole, Konstantinos Kamnitsas, Ben Glocker*

- `2104.13756v1` - [abs](http://arxiv.org/abs/2104.13756v1) - [pdf](http://arxiv.org/pdf/2104.13756v1)

> We propose a parameter efficient Bayesian layer for hierarchical convolutional Gaussian Processes that incorporates Gaussian Processes operating in Wasserstein-2 space to reliably propagate uncertainty. This directly replaces convolving Gaussian Processes with a distance-preserving affine operator on distributions. Our experiments on brain tissue-segmentation show that the resulting architecture approaches the performance of well-established deterministic segmentation algorithms (U-Net), which has never been achieved with previous hierarchical Gaussian Processes. Moreover, by applying the same segmentation model to out-of-distribution data (i.e., images with pathology such as brain tumors), we show that our uncertainty estimates result in out-of-distribution detection that outperforms the capabilities of previous Bayesian networks and reconstruction-based approaches that learn normative distributions.

</details>

<details>

<summary>2021-04-28 20:29:49 - BayesSUR: An R package for high-dimensional multivariate Bayesian variable and covariance selection in linear regression</summary>

- *Zhi Zhao, Marco Banterle, Leonardo Bottolo, Sylvia Richardson, Alex Lewin, Manuela Zucknick*

- `2104.14008v1` - [abs](http://arxiv.org/abs/2104.14008v1) - [pdf](http://arxiv.org/pdf/2104.14008v1)

> In molecular biology, advances in high-throughput technologies have made it possible to study complex multivariate phenotypes and their simultaneous associations with high-dimensional genomic and other omics data, a problem that can be studied with high-dimensional multi-response regression, where the response variables are potentially highly correlated. To this purpose, we recently introduced several multivariate Bayesian variable and covariance selection models, e.g., Bayesian estimation methods for sparse seemingly unrelated regression for variable and covariance selection. Several variable selection priors have been implemented in this context, in particular the hotspot detection prior for latent variable inclusion indicators, which results in sparse variable selection for associations between predictors and multiple phenotypes. We also propose an alternative, which uses a Markov random field (MRF) prior for incorporating prior knowledge about the dependence structure of the inclusion indicators. Inference of Bayesian seemingly unrelated regression (SUR) by Markov chain Monte Carlo methods is made computationally feasible by factorisation of the covariance matrix amongst the response variables. In this paper we present BayesSUR, an R package, which allows the user to easily specify and run a range of different Bayesian SUR models, which have been implemented in C++ for computational efficiency. The R package allows the specification of the models in a modular way, where the user chooses the priors for variable selection and for covariance selection separately. We demonstrate the performance of sparse SUR models with the hotspot prior and spike-and-slab MRF prior on synthetic and real data sets representing eQTL or mQTL studies and in vitro anti-cancer drug screening studies as examples for typical applications.

</details>

<details>

<summary>2021-04-28 20:44:10 - Simplified Kalman filter for online rating: one-fits-all approach</summary>

- *Leszek Szczecinski, Raphaëlle Tihon*

- `2104.14012v1` - [abs](http://arxiv.org/abs/2104.14012v1) - [pdf](http://arxiv.org/pdf/2104.14012v1)

> In this work, we deal with the problem of rating in sports, where the skills of the players/teams are inferred from the observed outcomes of the games. Our focus is on the online rating algorithms which estimate the skills after each new game by exploiting the probabilistic models of the relationship between the skills and the game outcome. We propose a Bayesian approach which may be seen as an approximate Kalman filter and which is generic in the sense that it can be used with any skills-outcome model and can be applied in the individual -- as well as in the group-sports. We show how the well-know algorithms (such as the Elo, the Glicko, and the TrueSkill algorithms) may be seen as instances of the one-fits-all approach we propose. In order to clarify the conditions under which the gains of the Bayesian approach over the simpler solutions can actually materialize, we critically compare the known and the new algorithms by means of numerical examples using the synthetic as well as the empirical data.

</details>

<details>

<summary>2021-04-28 22:05:54 - A Study of the Mathematics of Deep Learning</summary>

- *Anirbit Mukherjee*

- `2104.14033v1` - [abs](http://arxiv.org/abs/2104.14033v1) - [pdf](http://arxiv.org/pdf/2104.14033v1)

> "Deep Learning"/"Deep Neural Nets" is a technological marvel that is now increasingly deployed at the cutting-edge of artificial intelligence tasks. This dramatic success of deep learning in the last few years has been hinged on an enormous amount of heuristics and it has turned out to be a serious mathematical challenge to be able to rigorously explain them. In this thesis, submitted to the Department of Applied Mathematics and Statistics, Johns Hopkins University we take several steps towards building strong theoretical foundations for these new paradigms of deep-learning. In chapter 2 we show new circuit complexity theorems for deep neural functions and prove classification theorems about these function spaces which in turn lead to exact algorithms for empirical risk minimization for depth 2 ReLU nets. We also motivate a measure of complexity of neural functions to constructively establish the existence of high-complexity neural functions. In chapter 3 we give the first algorithm which can train a ReLU gate in the realizable setting in linear time in an almost distribution free set up. In chapter 4 we give rigorous proofs towards explaining the phenomenon of autoencoders being able to do sparse-coding. In chapter 5 we give the first-of-its-kind proofs of convergence for stochastic and deterministic versions of the widely used adaptive gradient deep-learning algorithms, RMSProp and ADAM. This chapter also includes a detailed empirical study on autoencoders of the hyper-parameter values at which modern algorithms have a significant advantage over classical acceleration based methods. In the last chapter 6 we give new and improved PAC-Bayesian bounds for the risk of stochastic neural nets. This chapter also includes an experimental investigation revealing new geometric properties of the paths in weight space that are traced out by the net during the training.

</details>

<details>

<summary>2021-04-29 04:31:24 - Bayesian Neural Networks with Soft Evidence</summary>

- *Edward Yu*

- `2010.09570v2` - [abs](http://arxiv.org/abs/2010.09570v2) - [pdf](http://arxiv.org/pdf/2010.09570v2)

> Bayes's rule deals with hard evidence, that is, we can calculate the probability of event $A$ occuring given that event $B$ has occurred. Soft evidence, on the other hand, involves a degree of uncertainty about whether event $B$ has actually occurred or not. Jeffrey's rule of conditioning provides a way to update beliefs in the case of soft evidence. We provide a framework to learn a probability distribution on the weights of a neural network trained using soft evidence by way of two simple algorithms for approximating Jeffrey conditionalization. We propose an experimental protocol for benchmarking these algorithms on empirical datasets and find that Jeffrey based methods are competitive or better in terms of accuracy yet show improvements in calibration metrics upwards of 20% in some cases, even when the data contains mislabeled points.

</details>

<details>

<summary>2021-04-29 14:32:21 - Learning Continuous-Time Dynamics by Stochastic Differential Networks</summary>

- *Yingru Liu, Yucheng Xing, Xuewen Yang, Xin Wang, Jing Shi, Di Jin, Zhaoyue Chen*

- `2006.06145v3` - [abs](http://arxiv.org/abs/2006.06145v3) - [pdf](http://arxiv.org/pdf/2006.06145v3)

> Learning continuous-time stochastic dynamics is a fundamental and essential problem in modeling sporadic time series, whose observations are irregular and sparse in both time and dimension. For a given system whose latent states and observed data are high-dimensional, it is generally impossible to derive a precise continuous-time stochastic process to describe the system behaviors. To solve the above problem, we apply Variational Bayesian method and propose a flexible continuous-time stochastic recurrent neural network named Variational Stochastic Differential Networks (VSDN), which embeds the complicated dynamics of the sporadic time series by neural Stochastic Differential Equations (SDE). VSDNs capture the stochastic dependency among latent states and observations by deep neural networks. We also incorporate two differential Evidence Lower Bounds to efficiently train the models. Through comprehensive experiments, we show that VSDNs outperform state-of-the-art continuous-time deep learning models and achieve remarkable performance on prediction and interpolation tasks for sporadic time series.

</details>

<details>

<summary>2021-04-29 15:38:46 - What Are Bayesian Neural Network Posteriors Really Like?</summary>

- *Pavel Izmailov, Sharad Vikram, Matthew D. Hoffman, Andrew Gordon Wilson*

- `2104.14421v1` - [abs](http://arxiv.org/abs/2104.14421v1) - [pdf](http://arxiv.org/pdf/2104.14421v1)

> The posterior over Bayesian neural network (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC). To investigate foundational questions in Bayesian deep learning, we instead use full-batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can achieve significant performance gains over standard training and deep ensembles; (2) a single long HMC chain can provide a comparable representation of the posterior to multiple shorter chains; (3) in contrast to recent studies, we find posterior tempering is not needed for near-optimal performance, with little evidence for a "cold posterior" effect, which we show is largely an artifact of data augmentation; (4) BMA performance is robust to the choice of prior scale, and relatively similar for diagonal Gaussian, mixture of Gaussian, and logistic priors; (5) Bayesian neural networks show surprisingly poor generalization under domain shift; (6) while cheaper alternatives such as deep ensembles and SGMCMC methods can provide good generalization, they provide distinct predictive distributions from HMC. Notably, deep ensemble predictive distributions are similarly close to HMC as standard SGLD, and closer than standard variational inference.

</details>

<details>

<summary>2021-04-30 02:16:41 - Bayesian Information Criterion for Linear Mixed-effects Models</summary>

- *Nan Shen, Bárbara González*

- `2104.14725v1` - [abs](http://arxiv.org/abs/2104.14725v1) - [pdf](http://arxiv.org/pdf/2104.14725v1)

> The use of Bayesian information criterion (BIC) in the model selection procedure is under the assumption that the observations are independent and identically distributed (i.i.d.). However, in practice, we do not always have i.i.d. samples. For example, clustered observations tend to be more similar within the same group, and longitudinal data is collected by measuring the same subject repeatedly. In these scenarios, the assumption in BIC is not satisfied. The concept of effective sample size is brought up and improved BIC is defined by replacing the sample size in the original BIC expression with the effective sample size, which will give us a better theoretical foundation in the circumstance that mixed-effects models involve. Numerical experiment results are also given by comparing the performance of our new BIC with other widely used BICs.

</details>

<details>

<summary>2021-04-30 05:37:21 - On Uninformative Optimal Policies in Adaptive LQR with Unknown B-Matrix</summary>

- *Ingvar Ziemann, Henrik Sandberg*

- `2011.09288v3` - [abs](http://arxiv.org/abs/2011.09288v3) - [pdf](http://arxiv.org/pdf/2011.09288v3)

> This paper presents local asymptotic minimax regret lower bounds for adaptive Linear Quadratic Regulators (LQR). We consider affinely parametrized $B$-matrices and known $A$-matrices and aim to understand when logarithmic regret is impossible even in the presence of structural side information. After defining the intrinsic notion of an uninformative optimal policy in terms of a singularity condition for Fisher information we obtain local minimax regret lower bounds for such uninformative instances of LQR by appealing to van Trees' inequality (Bayesian Cram\'er-Rao) and a representation of regret in terms of a quadratic form (Bellman error). It is shown that if the parametrization induces an uninformative optimal policy, logarithmic regret is impossible and the rate is at least order square root in the time horizon. We explicitly characterize the notion of an uninformative optimal policy in terms of the nullspaces of system-theoretic quantities and the particular instance parametrization.

</details>

<details>

<summary>2021-04-30 06:57:13 - Learning Multiple Defaults for Machine Learning Algorithms</summary>

- *Florian Pfisterer, Jan N. van Rijn, Philipp Probst, Andreas Müller, Bernd Bischl*

- `1811.09409v3` - [abs](http://arxiv.org/abs/1811.09409v3) - [pdf](http://arxiv.org/pdf/1811.09409v3)

> The performance of modern machine learning methods highly depends on their hyperparameter configurations. One simple way of selecting a configuration is to use default settings, often proposed along with the publication and implementation of a new algorithm. Those default values are usually chosen in an ad-hoc manner to work good enough on a wide variety of datasets. To address this problem, different automatic hyperparameter configuration algorithms have been proposed, which select an optimal configuration per dataset. This principled approach usually improves performance but adds additional algorithmic complexity and computational costs to the training procedure. As an alternative to this, we propose learning a set of complementary default values from a large database of prior empirical results. Selecting an appropriate configuration on a new dataset then requires only a simple, efficient and embarrassingly parallel search over this set. We demonstrate the effectiveness and efficiency of the approach we propose in comparison to random search and Bayesian Optimization.

</details>

<details>

<summary>2021-04-30 10:18:43 - DeBayes: a Bayesian Method for Debiasing Network Embeddings</summary>

- *Maarten Buyl, Tijl De Bie*

- `2002.11442v3` - [abs](http://arxiv.org/abs/2002.11442v3) - [pdf](http://arxiv.org/pdf/2002.11442v3)

> As machine learning algorithms are increasingly deployed for high-impact automated decision making, ethical and increasingly also legal standards demand that they treat all individuals fairly, without discrimination based on their age, gender, race or other sensitive traits. In recent years much progress has been made on ensuring fairness and reducing bias in standard machine learning settings. Yet, for network embedding, with applications in vulnerable domains ranging from social network analysis to recommender systems, current options remain limited both in number and performance. We thus propose DeBayes: a conceptually elegant Bayesian method that is capable of learning debiased embeddings by using a biased prior. Our experiments show that these representations can then be used to perform link prediction that is significantly more fair in terms of popular metrics such as demographic parity and equalized opportunity.

</details>

<details>

<summary>2021-04-30 15:11:58 - Inference and model determination for Temperature-Driven non-linear Ecological Models</summary>

- *Marios Kondakis, Nikolaos Demiris, Ioannis Ntzoufras, Nikos E. Papanikolaou*

- `2104.15043v1` - [abs](http://arxiv.org/abs/2104.15043v1) - [pdf](http://arxiv.org/pdf/2104.15043v1)

> This paper is concerned with a contemporary Bayesian approach to the effect of temperature on developmental rates. We develop statistical methods using recent computational tools to model four commonly used ecological non-linear mathematical curves that describe arthropods' developmental rates. Such models address the effect of temperature fluctuations on the developmental rate of arthropods. In addition to the widely used Gaussian distributional assumption, we also explore Inverse Gamma--based alternatives, which naturally accommodate adaptive variance fluctuation with temperature. Moreover, to overcome the associated parameter indeterminacy in the case of no development, we suggest the Zero Inflated Inverse Gamma model. The ecological models are compared graphically via posterior predictive plots and quantitatively via Marginal likelihood estimates and Information criteria values. Inference is performed using the Stan software and we investigate the statistical and computational efficiency of its Hamiltonian Monte Carlo and Variational Inference methods. We explore model uncertainty and use Bayesian Model Averaging framework for robust estimation of the key ecological parameters

</details>

<details>

<summary>2021-04-30 15:16:35 - Dynamic Slate Recommendation with Gated Recurrent Units and Thompson Sampling</summary>

- *Simen Eide, David S. Leslie, Arnoldo Frigessi*

- `2104.15046v1` - [abs](http://arxiv.org/abs/2104.15046v1) - [pdf](http://arxiv.org/pdf/2104.15046v1)

> We consider the problem of recommending relevant content to users of an internet platform in the form of lists of items, called slates. We introduce a variational Bayesian Recurrent Neural Net recommender system that acts on time series of interactions between the internet platform and the user, and which scales to real world industrial situations. The recommender system is tested both online on real users, and on an offline dataset collected from a Norwegian web-based marketplace, FINN.no, that is made public for research. This is one of the first publicly available datasets which includes all the slates that are presented to users as well as which items (if any) in the slates were clicked on. Such a data set allows us to move beyond the common assumption that implicitly assumes that users are considering all possible items at each interaction. Instead we build our likelihood using the items that are actually in the slate, and evaluate the strengths and weaknesses of both approaches theoretically and in experiments. We also introduce a hierarchical prior for the item parameters based on group memberships. Both item parameters and user preferences are learned probabilistically. Furthermore, we combine our model with bandit strategies to ensure learning, and introduce `in-slate Thompson Sampling' which makes use of the slates to maximise explorative opportunities. We show experimentally that explorative recommender strategies perform on par or above their greedy counterparts. Even without making use of exploration to learn more effectively, click rates increase simply because of improved diversity in the recommended slates.

</details>


## 2021-05

<details>

<summary>2021-05-01 03:41:40 - Approximate Bayesian Computations to fit and compare insurance loss models</summary>

- *Pierre-Olivier Goffard, Patrick J. Laub*

- `2007.03833v2` - [abs](http://arxiv.org/abs/2007.03833v2) - [pdf](http://arxiv.org/pdf/2007.03833v2)

> Approximate Bayesian Computation (ABC) is a statistical learning technique to calibrate and select models by comparing observed data to simulated data. This technique bypasses the use of the likelihood and requires only the ability to generate synthetic data from the models of interest. We apply ABC to fit and compare insurance loss models using aggregated data. A state-of-the-art ABC implementation in Python is proposed. It uses sequential Monte Carlo to sample from the posterior distribution and the Wasserstein distance to compare the observed and synthetic data.

</details>

<details>

<summary>2021-05-01 10:23:22 - Autoregressive Hidden Markov Models with partial knowledge on latent space applied to aero-engines prognostics</summary>

- *Pablo Juesas, Emmanuel Ramasso, Sébastien Drujont, Vincent Placet*

- `2105.00211v1` - [abs](http://arxiv.org/abs/2105.00211v1) - [pdf](http://arxiv.org/pdf/2105.00211v1)

> [This paper was initially published in PHME conference in 2016, selected for further publication in International Journal of Prognostics and Health Management.]   This paper describes an Autoregressive Partially-hidden Markov model (ARPHMM) for fault detection and prognostics of equipments based on sensors' data. It is a particular dynamic Bayesian network that allows to represent the dynamics of a system by means of a Hidden Markov Model (HMM) and an autoregressive (AR) process. The Markov chain assumes that the system is switching back and forth between internal states while the AR process ensures a temporal coherence on sensor measurements. A sound learning procedure of standard ARHMM based on maximum likelihood allows to iteratively estimate all parameters simultaneously. This paper suggests a modification of the learning procedure considering that one may have prior knowledge about the structure which becomes partially hidden. The integration of the prior is based on the Theory of Weighted Distributions which is compatible with the Expectation-Maximization algorithm in the sense that the convergence properties are still satisfied. We show how to apply this model to estimate the remaining useful life based on health indicators. The autoregressive parameters can indeed be used for prediction while the latent structure can be used to get information about the degradation level. The interest of the proposed method for prognostics and health assessment is demonstrated on CMAPSS datasets.

</details>

<details>

<summary>2021-05-01 11:39:25 - Bayesian Inference of a Dependent Competing Risk Data</summary>

- *Debashis Samanta, Debasis Kundu*

- `2105.00224v1` - [abs](http://arxiv.org/abs/2105.00224v1) - [pdf](http://arxiv.org/pdf/2105.00224v1)

> Analysis of competing risks data plays an important role in the lifetime data analysis. Recently Feizjavadian and Hashemi (Computational Statistics and Data Analysis, vol. 82, 19-34, 2015) provided a classical inference of a competing risks data set using four-parameter Marshall-Olkin bivariate Weibull distribution when the failure of an unit at a particular time point can happen due to more than one cause. The aim of this paper is to provide the Bayesian analysis of the same model based on a very flexible Gamma-Dirichlet prior on the scale parameters. It is observed that the Bayesian inference has certain advantages over the classical inference in this case. We provide the Bayes estimates of the unknown parameters and the associated highest posterior density credible intervals based on Gibbs sampling technique. We further consider the Bayesian inference of the model parameters assuming partially ordered Gamma-Dirichlet prior on the scale parameters when one cause is more severe than the other cause. We have extended the results for different censoring schemes also.

</details>

<details>

<summary>2021-05-02 00:33:27 - A simple consistent Bayes factor for testing the Kendall rank correlation coefficient</summary>

- *Shen Zhang, Keying Ye, Min Wang*

- `2105.00364v1` - [abs](http://arxiv.org/abs/2105.00364v1) - [pdf](http://arxiv.org/pdf/2105.00364v1)

> In this paper, we propose a simple and easy-to-implement Bayesian hypothesis test for the presence of an association, described by Kendall's tau coefficient, between two variables measured on at least an ordinal scale. Owing to the absence of the likelihood functions for the data, we employ the asymptotic sampling distributions of the test statistic as the working likelihoods and then specify a truncated normal prior distribution on the noncentrality parameter of the alternative hypothesis, which results in the Bayes factor available in closed form in terms of the cumulative distribution function of the standard normal distribution. Investigating the asymptotic behavior of the Bayes factor we find the conditions of the priors so that it is consistent to whichever the hypothesis is true. Simulation studies and a real-data application are used to illustrate the effectiveness of the proposed Bayes factor. It deserves mentioning that the proposed method can be easily covered in undergraduate and graduate courses in nonparametric statistics with an emphasis on students' Bayesian thinking for data analysis.

</details>

<details>

<summary>2021-05-02 12:30:39 - A model of inter-organizational network formation</summary>

- *Shweta Gaonkar, Angelo Mele*

- `2105.00458v1` - [abs](http://arxiv.org/abs/2105.00458v1) - [pdf](http://arxiv.org/pdf/2105.00458v1)

> How do inter-organizational networks emerge? Accounting for interdependence among ties while studying tie formation is one of the key challenges in this area of research. We address this challenge using an equilibrium framework where firms' decisions to form links with other firms are modeled as a strategic game. In this game, firms weigh the costs and benefits of establishing a relationship with other firms and form ties if their net payoffs are positive. We characterize the equilibrium networks as exponential random graphs (ERGM), and we estimate the firms' payoffs using a Bayesian approach. To demonstrate the usefulness of our approach, we apply the framework to a co-investment network of venture capital firms in the medical device industry. The equilibrium framework allows researchers to draw economic interpretation from parameter estimates of the ERGM Model. We learn that firms rely on their joint partners (transitivity) and prefer to form ties with firms similar to themselves (homophily). These results hold after controlling for the interdependence among ties. Another, critical advantage of a structural approach is that it allows us to simulate the effects of economic shocks or policy counterfactuals. We test two such policy shocks, namely, firm entry and regulatory change. We show how new firms' entry or a regulatory shock of minimum capital requirements increase the co-investment network's density and clustering.

</details>

<details>

<summary>2021-05-02 14:42:32 - Bayesian structure learning and sampling of Bayesian networks with the R package BiDAG</summary>

- *Polina Suter, Jack Kuipers, Giusi Moffa, Niko Beerenwinkel*

- `2105.00488v1` - [abs](http://arxiv.org/abs/2105.00488v1) - [pdf](http://arxiv.org/pdf/2105.00488v1)

> The R package BiDAG implements Markov chain Monte Carlo (MCMC) methods for structure learning and sampling of Bayesian networks. The package includes tools to search for a maximum a posteriori (MAP) graph and to sample graphs from the posterior distribution given the data. A new hybrid approach to structure learning enables inference in large graphs. In the first step, we define a reduced search space by means of the PC algorithm or based on prior knowledge. In the second step, an iterative order MCMC scheme proceeds to optimize within the restricted search space and estimate the MAP graph. Sampling from the posterior distribution is implemented using either order or partition MCMC. The models and algorithms can handle both discrete and continuous data. The BiDAG package also provides an implementation of MCMC schemes for structure learning and sampling of dynamic Bayesian networks.

</details>

<details>

<summary>2021-05-02 21:30:24 - Towards Improving the Predictive Capability of Computer Simulations by Integrating Inverse Uncertainty Quantification and Quantitative Validation with Bayesian Hypothesis Testing</summary>

- *Ziyu Xie, Farah Alsafadi, Xu Wu*

- `2105.00553v1` - [abs](http://arxiv.org/abs/2105.00553v1) - [pdf](http://arxiv.org/pdf/2105.00553v1)

> The Best Estimate plus Uncertainty (BEPU) approach for nuclear systems modeling and simulation requires that the prediction uncertainty must be quantified in order to prove that the investigated design stays within acceptance criteria. A rigorous Uncertainty Quantification (UQ) process should simultaneously consider multiple sources of quantifiable uncertainties: (1) parameter uncertainty due to randomness or lack of knowledge; (2) experimental uncertainty due to measurement noise; (3) model uncertainty caused by missing/incomplete physics and numerical approximation errors, and (4) code uncertainty when surrogate models are used. In this paper, we propose a comprehensive framework to integrate results from inverse UQ and quantitative validation to provide robust predictions so that all these sources of uncertainties can be taken into consideration.   Inverse UQ quantifies the parameter uncertainties based on experimental data while taking into account uncertainties from model, code and measurement. In the validation step, we use a quantitative validation metric based on Bayesian hypothesis testing. The resulting metric, called the Bayes factor, is then used to form weighting factors to combine the prior and posterior knowledge of the parameter uncertainties in a Bayesian model averaging process. In this way, model predictions will be able to integrate the results from inverse UQ and validation to account for all available sources of uncertainties. This framework is a step towards addressing the ANS Nuclear Grand Challenge on "Simulation/Experimentation" by bridging the gap between models and data.

</details>

<details>

<summary>2021-05-02 22:14:53 - Gibbs sampler and coordinate ascent variational inference: a set-theoretical review</summary>

- *Se Yoon Lee*

- `2008.01006v7` - [abs](http://arxiv.org/abs/2008.01006v7) - [pdf](http://arxiv.org/pdf/2008.01006v7)

> One of the fundamental problems in Bayesian statistics is the approximation of the posterior distribution. Gibbs sampler and coordinate ascent variational inference are renownedly utilized approximation techniques that rely on stochastic and deterministic approximations. In this paper, we define fundamental sets of densities frequently used in Bayesian inference. We shall be concerned with the clarification of the two schemes from the set-theoretical point of view. This new way provides an alternative mechanism for analyzing the two schemes endowed with pedagogical insights.

</details>

<details>

<summary>2021-05-03 02:07:36 - A Bayesian Method for Estimating Uncertainty in Excavated Material</summary>

- *Mehala Balamurali*

- `2105.00600v1` - [abs](http://arxiv.org/abs/2105.00600v1) - [pdf](http://arxiv.org/pdf/2105.00600v1)

> This paper proposes a method to probabilistically quantify the moments (mean and variance) of excavated material during excavation by aggregating the prior moments of the grade blocks around the given bucket dig location. By modelling the moments as random probability density functions (pdf) at sampled locations, a formulation of the sums of Gaussian based uncertainty estimation is presented that jointly estimates the location pdfs, as well as the prior values for uncertainty coming from ore body knowledge (obk) sub block models. The moments calculated at each random location is a single Gaussian and they are the components of Gaussian mixture distribution. The overall uncertainty of the excavated material at the given bucket location is represented by the Gaussian Mixture Model (GMM) and therefore moment matching method is proposed to estimate the moments of the reduced GMM. The method was tested in a region at a Pilbara iron ore deposit situated in the Brockman Iron Formation of the Hamersley Province, Western Australia, and suggests a frame work to quantify the uncertainty in the excavated material that hasn't been studied anywhere in the literature yet.

</details>

<details>

<summary>2021-05-03 09:13:08 - Analysis of zero inflated dichotomous variables from a Bayesian perspective: Application to occupational health</summary>

- *David Moriña, Pedro Puig, Albert Navarro*

- `2105.00700v1` - [abs](http://arxiv.org/abs/2105.00700v1) - [pdf](http://arxiv.org/pdf/2105.00700v1)

> This work proposes a new methodology to fit zero inflated Bernoulli data from a Bayesian approach, able to distinguish between two potential sources of zeros (structurals and non-structurals). Its usage is illustrated by means of a real example from the field of occupational health as the phenomenon of sickness presenteeism, in which it is reasonable to think that some individuals will never be at risk of suffering it because they have not been sick in the period of study (structural zeros). Without separating structural and non-structural zeros one would one would be studying jointly the general health status and the presenteeism itself, and therefore obtaining potentially biased estimates as the phenomenon is being implicitly underestimated by diluting it into the general health status. The proposed methodology performance has been evaluated through a comprehensive simulation study, and it has been compiled as an R package freely available to the community.

</details>

<details>

<summary>2021-05-03 13:22:08 - Bayesian tests of symmetry for the generalized von Mises distribution</summary>

- *Sara Salvador, Riccardo Gatto*

- `2105.00833v1` - [abs](http://arxiv.org/abs/2105.00833v1) - [pdf](http://arxiv.org/pdf/2105.00833v1)

> Bayesian tests on the symmetry of the generalized von Mises model for planar directions (Gatto and Jammalamadaka, 2007) are introduced. The generalized von Mises distribution is a flexible model that can be axially symmetric or asymmetric, unimodal or bimodal. A characterization of axial symmetry is provided and taken as null hypothesis for one of the proposed Bayesian tests. The Bayesian tests are obtained by the technique of probability perturbation. The prior probability measure is perturbed so to give a positive prior probability to the null hypothesis, which would be null otherwise. This allows for the derivation of simple computational formulae for the Bayes factors. Numerical results reveal that, whenever the simulation scheme of the samples supports the null hypothesis, the null posterior probabilities appear systematically larger than their prior counterpart.

</details>

<details>

<summary>2021-05-03 13:26:26 - Bayesian Numerical Methods for Nonlinear Partial Differential Equations</summary>

- *Junyang Wang, Jon Cockayne, Oksana Chkrebtii, T. J. Sullivan, Chris. J. Oates*

- `2104.12587v2` - [abs](http://arxiv.org/abs/2104.12587v2) - [pdf](http://arxiv.org/pdf/2104.12587v2)

> The numerical solution of differential equations can be formulated as an inference problem to which formal statistical approaches can be applied. However, nonlinear partial differential equations (PDEs) pose substantial challenges from an inferential perspective, most notably the absence of explicit conditioning formula. This paper extends earlier work on linear PDEs to a general class of initial value problems specified by nonlinear PDEs, motivated by problems for which evaluations of the right-hand-side, initial conditions, or boundary conditions of the PDE have a high computational cost. The proposed method can be viewed as exact Bayesian inference under an approximate likelihood, which is based on discretisation of the nonlinear differential operator. Proof-of-concept experimental results demonstrate that meaningful probabilistic uncertainty quantification for the unknown solution of the PDE can be performed, while controlling the number of times the right-hand-side, initial and boundary conditions are evaluated. A suitable prior model for the solution of the PDE is identified using novel theoretical analysis of the sample path properties of Mat\'{e}rn processes, which may be of independent interest.

</details>

<details>

<summary>2021-05-03 14:28:11 - How Bayesian Should Bayesian Optimisation Be?</summary>

- *George De Ath, Richard Everson, Jonathan Fieldsend*

- `2105.00894v1` - [abs](http://arxiv.org/abs/2105.00894v1) - [pdf](http://arxiv.org/pdf/2105.00894v1)

> Bayesian optimisation (BO) uses probabilistic surrogate models - usually Gaussian processes (GPs) - for the optimisation of expensive black-box functions. At each BO iteration, the GP hyperparameters are fit to previously-evaluated data by maximising the marginal likelihood. However, this fails to account for uncertainty in the hyperparameters themselves, leading to overconfident model predictions. This uncertainty can be accounted for by taking the Bayesian approach of marginalising out the model hyperparameters.   We investigate whether a fully-Bayesian treatment of the Gaussian process hyperparameters in BO (FBBO) leads to improved optimisation performance. Since an analytic approach is intractable, we compare FBBO using three approximate inference schemes to the maximum likelihood approach, using the Expected Improvement (EI) and Upper Confidence Bound (UCB) acquisition functions paired with ARD and isotropic Matern kernels, across 15 well-known benchmark problems for 4 observational noise settings. FBBO using EI with an ARD kernel leads to the best performance in the noise-free setting, with much less difference between combinations of BO components when the noise is increased. FBBO leads to over-exploration with UCB, but is not detrimental with EI. Therefore, we recommend that FBBO using EI with an ARD kernel as the default choice for BO.

</details>

<details>

<summary>2021-05-03 15:47:55 - Homotopy Sampling, with an Application to Particle Filters</summary>

- *Juan M. Restrepo, Jorge M. Ramirez*

- `2105.01576v1` - [abs](http://arxiv.org/abs/2105.01576v1) - [pdf](http://arxiv.org/pdf/2105.01576v1)

> We propose a homotopy sampling procedure, loosely based on importance sampling. Starting from a known probability distribution, the homotopy procedure generates the unknown normalization of a target distribution. In the context of stationary distributions that are associated with physical systems the method is an alternative way to estimate an unknown microcanonical ensemble. The process is iterative and also generates samples from the target distribution. In practice, the homotopy procedure does not circumvent using sample averages in the estimation of the normalization constant. The error in the procedure depends on the errors incurred in sample averaging and the number of stages used in the computational implementation of the process. However, we show that it is possible to exchange the number of homotopy stages and the total number of samples needed at each stage in order to enhance the computational efficiency of the implemented algorithm. Estimates of the error as a function of stages and sample averages are derived. These could guide computational efficiency decisions on how the calculation would be mapped to a given computer architecture. Consideration is given to how the procedure can be adapted to Bayesian estimation problems, both stationary and non-stationary. Emphasis is placed on the non-stationary problems, and in particular, on a sequential estimation technique known as particle filtering. It is shown that a modification of the particle filter framework to include the homotopy process can improve the computational robustness of particle filters. The homotopy process can ameliorate particle filter collapse, a common challenge to using particle filters when the sample dimension is small compared with the state space dimensions.

</details>

<details>

<summary>2021-05-03 20:13:34 - What can the millions of random treatments in nonexperimental data reveal about causes?</summary>

- *Andre F. Ribeiro, Frank Neffke, Ricardo Hausmann*

- `2105.01152v1` - [abs](http://arxiv.org/abs/2105.01152v1) - [pdf](http://arxiv.org/pdf/2105.01152v1)

> We propose a new method to estimate causal effects from nonexperimental data. Each pair of sample units is first associated with a stochastic 'treatment' - differences in factors between units - and an effect - a resultant outcome difference. It is then proposed that all such pairs can be combined to provide more accurate estimates of causal effects in observational data, provided a statistical model connecting combinatorial properties of treatments to the accuracy and unbiasedness of their effects. The article introduces one such model and a Bayesian approach to combine the $O(n^2)$ pairwise observations typically available in nonexperimnetal data. This also leads to an interpretation of nonexperimental datasets as incomplete, or noisy, versions of ideal factorial experimental designs.   This approach to causal effect estimation has several advantages: (1) it expands the number of observations, converting thousands of individuals into millions of observational treatments; (2) starting with treatments closest to the experimental ideal, it identifies noncausal variables that can be ignored in the future, making estimation easier in each subsequent iteration while departing minimally from experiment-like conditions; (3) it recovers individual causal effects in heterogeneous populations. We evaluate the method in simulations and the National Supported Work (NSW) program, an intensively studied program whose effects are known from randomized field experiments. We demonstrate that the proposed approach recovers causal effects in common NSW samples, as well as in arbitrary subpopulations and an order-of-magnitude larger supersample with the entire national program data, outperforming Statistical, Econometrics and Machine Learning estimators in all cases...

</details>

<details>

<summary>2021-05-04 07:33:28 - From the Expectation Maximisation Algorithm to Autoencoded Variational Bayes</summary>

- *Graham W. Pulford*

- `2010.13551v2` - [abs](http://arxiv.org/abs/2010.13551v2) - [pdf](http://arxiv.org/pdf/2010.13551v2)

> Although the expectation maximisation (EM) algorithm was introduced in 1970, it remains somewhat inaccessible to machine learning practitioners due to its obscure notation, terse proofs and lack of concrete links to modern machine learning techniques like autoencoded variational Bayes. This has resulted in gaps in the AI literature concerning the meaning of such concepts like "latent variables" and "variational lower bound," which are frequently used but often not clearly explained. The roots of these ideas lie in the EM algorithm. We first give a tutorial presentation of the EM algorithm for estimating the parameters of a $K$-component mixture density. The Gaussian mixture case is presented in detail using $K$-ary scalar hidden (or latent) variables rather than the more traditional binary valued $K$-dimenional vectors. This presentation is motivated by mixture modelling from the target tracking literature. In a similar style to Bishop's 2009 book, we present variational Bayesian inference as a generalised EM algorithm stemming from the variational (or evidential) lower bound, as well as the technique of mean field approximation (or product density transform). We continue the evolution from EM to variational autoencoders, developed by Kingma & Welling in 2014. In so doing, we establish clear links between the EM algorithm and its variational counterparts, hence clarifying the meaning of "latent variables." We provide a detailed coverage of the "reparametrisation trick" and focus on how the AEVB differs from conventional variational Bayesian inference. Throughout the tutorial, consistent notational conventions are used. This unifies the narrative and clarifies the concepts. Some numerical examples are given to further illustrate the algorithms.

</details>

<details>

<summary>2021-05-04 09:48:59 - Field dynamics inference for local and causal interactions</summary>

- *Philipp Frank, Reimar Leike, Torsten A. Enßlin*

- `1902.02624v3` - [abs](http://arxiv.org/abs/1902.02624v3) - [pdf](http://arxiv.org/pdf/1902.02624v3)

> Inference of fields defined in space and time from observational data is a core discipline in many scientific areas. This work approaches the problem in a Bayesian framework. The proposed method is based on statistically homogeneous random fields defined in space and time and demonstrates how to reconstruct the field together with its prior correlation structure from data. The prior model of the correlation structure is described in a non-parametric fashion and solely builds on fundamental physical assumptions such as space-time homogeneity, locality, and causality. These assumptions are sufficient to successfully infer the field and its prior correlation structure from noisy and incomplete data of a single realization of the process as demonstrated via multiple numerical examples.

</details>

<details>

<summary>2021-05-04 13:58:44 - The Lévy combination test</summary>

- *Daniel J. Wilson*

- `2105.01501v1` - [abs](http://arxiv.org/abs/2105.01501v1) - [pdf](http://arxiv.org/pdf/2105.01501v1)

> A novel class of methods for combining $p$-values to perform aggregate hypothesis tests has emerged that exploit the properties of heavy-tailed Stable distributions. These methods offer important practical advantages including robustness to dependence and better-than-Bonferroni scaleability, and they reveal theoretical connections between Bayesian and classical hypothesis tests. The harmonic mean $p$-value (HMP) procedure is based on the convergence of summed inverse $p$-values to the Landau distribution, while the Cauchy combination test (CCT) is based on the self-similarity of summed Cauchy-transformed $p$-values. The CCT has the advantage that it is analytic and exact. The HMP has the advantage that it emulates a model-averaged Bayes factor, is insensitive to $p$-values near 1, and offers multilevel testing via a closed testing procedure. Here I investigate whether other Stable combination tests can combine these benefits, and identify a new method, the L\'evy combination test (LCT). The LCT exploits the self-similarity of sums of L\'evy random variables transformed from $p$-values. Under arbitrary dependence, the LCT possesses better robustness than the CCT and HMP, with two-fold worst-case inflation at small significance thresholds. It controls the strong-sense familywise error rate through a multilevel test uniformly more powerful than Bonferroni. Simulations show that the LCT behaves like Simes' test in some respects, with power intermediate between the HMP and Bonferroni. The LCT represents an interesting and attractive addition to combined testing methods based on heavy-tailed distributions.

</details>

<details>

<summary>2021-05-04 15:19:42 - Bayesian Conditional Auto-Regressive LASSO Models to Learn Sparse Microbial Networks with Predictors</summary>

- *Yunyi Shen, Claudia Solis-Lemus*

- `2012.08397v5` - [abs](http://arxiv.org/abs/2012.08397v5) - [pdf](http://arxiv.org/pdf/2012.08397v5)

> Microbiome data analyses require statistical models that can simultaneously decode microbes' reactions to the environment and interactions among microbes. While a multiresponse linear regression model seems like a straightforward solution, we argue that treating it as a graphical model is flawed given that the regression coefficient matrix does not encode the conditional dependence structure between response and predictor nodes because it does not represent the adjacency matrix. This observation is especially important in biological settings when we have prior knowledge on the edges from specific experimental interventions that can only be properly encoded under a conditional dependence model. Here, we propose a chain graph model with two sets of nodes (predictors and responses) whose solution yields a graph with edges that indeed represent conditional dependence and thus, agrees with the experimenter's intuition on the average behavior of nodes under treatment. The solution to our model is sparse via Bayesian LASSO and is also guaranteed to be the sparse solution to a Conditional Auto-Regressive (CAR) model. In addition, we propose an adaptive extension so that different shrinkage can be applied to different edges to incorporate edge-specific prior knowledge. Our model is computationally inexpensive through an efficient Gibbs sampling algorithm and can account for binary, counting, and compositional responses via appropriate hierarchical structure. We apply our model to a human gut and a soil microbial compositional datasets and we highlight that CAR-LASSO can estimate biologically meaningful network structures in the data. The CAR-LASSO software is available as an R package at https://github.com/YunyiShen/CAR-LASSO.

</details>

<details>

<summary>2021-05-04 15:24:03 - Dynamic Quantile Function Models</summary>

- *Wilson Ye Chen, Gareth W. Peters, Richard H. Gerlach, Scott A. Sisson*

- `1707.02587v5` - [abs](http://arxiv.org/abs/1707.02587v5) - [pdf](http://arxiv.org/pdf/1707.02587v5)

> Motivated by the need for effectively summarising, modelling, and forecasting the distributional characteristics of intra-daily returns, as well as the recent work on forecasting histogram-valued time-series in the area of symbolic data analysis, we develop a time-series model for forecasting quantile-function-valued (QF-valued) daily summaries for intra-daily returns. We call this model the dynamic quantile function (DQF) model. Instead of a histogram, we propose to use a $g$-and-$h$ quantile function to summarise the distribution of intra-daily returns. We work with a Bayesian formulation of the DQF model in order to make statistical inference while accounting for parameter uncertainty; an efficient MCMC algorithm is developed for sampling-based posterior inference. Using ten international market indices and approximately 2,000 days of out-of-sample data from each market, the performance of the DQF model compares favourably, in terms of forecasting VaR of intra-daily returns, against the interval-valued and histogram-valued time-series models. Additionally, we demonstrate that the QF-valued forecasts can be used to forecast VaR measures at the daily timescale via a simple quantile regression model on daily returns (QR-DQF). In certain markets, the resulting QR-DQF model is able to provide competitive VaR forecasts for daily returns.

</details>

<details>

<summary>2021-05-04 15:29:36 - Occam Factor for Gaussian Models With Unknown Variance Structure</summary>

- *Zachary M. Pisano, Daniel Q. Naiman, Carey E. Priebe*

- `2105.01566v1` - [abs](http://arxiv.org/abs/2105.01566v1) - [pdf](http://arxiv.org/pdf/2105.01566v1)

> We discuss model selection to determine whether the variance-covariance matrix of a multivariate Gaussian model with known mean should be considered to be a constant diagonal, a non-constant diagonal, or an arbitrary positive definite matrix. Of particular interest is the relationship between Bayesian evidence and the flexibility penalty due to Priebe and Rougier. For the case of an exponential family in canonical form equipped with a conjugate prior for the canonical parameter, flexibility may be exactly decomposed into the usual BIC likelihood penalty and a $O_p(1)$ term, the latter of which we explicitly compute. We also investigate the asymptotics of Bayes factors for linearly nested canonical exponential families equipped with conjugate priors; in particular, we find the exact rates at which Bayes factors correctly diverge in favor of the correct model: linearly and logarithmically in the number of observations when the full and nested models are true, respectively. Such theoretical considerations for the general case permit us to fully express the asymptotic behavior of flexibility and Bayes factors for the variance-covariance structure selection problem when we assume that the prior for the model precision is a member of the gamma/Wishart family of distributions or is uninformative. Simulations demonstrate evidence's immediate and superior performance in model selection compared to approximate criteria such as the BIC. We extend the framework to the multivariate Gaussian linear model with three data-driven examples.

</details>

<details>

<summary>2021-05-04 16:21:00 - Using machine learning to identify nontraditional spatial dependence in occupancy data</summary>

- *Narmadha M. Mohankumar, Trevor J. Hefley*

- `2006.09983v2` - [abs](http://arxiv.org/abs/2006.09983v2) - [pdf](http://arxiv.org/pdf/2006.09983v2)

> Spatial models for occupancy data are used to estimate and map the true presence of a species, which may depend on biotic and abiotic factors as well as spatial autocorrelation. Traditionally researchers have accounted for spatial autocorrelation in occupancy data by using a correlated normally distributed site-level random effect, which might be incapable of identifying nontraditional spatial dependence such as discontinuities and abrupt transitions. Machine learning approaches have the potential to identify and model nontraditional spatial dependence, but these approaches do not account for observer errors such as false absences. By combining the flexibility of Bayesian hierarchal modeling and machine learning approaches, we present a general framework to model occupancy data that accounts for both traditional and nontraditional spatial dependence as well as false absences. We demonstrate our framework using six synthetic occupancy data sets and two real data sets. Our results demonstrate how to identify and model both traditional and nontraditional spatial dependence in occupancy data which enables a broader class of spatial occupancy models that can be used to improve predictive accuracy and model adequacy.

</details>

<details>

<summary>2021-05-05 06:49:24 - Hierarchical Bayesian propulsion power models for marine vessels</summary>

- *Antti Solonen, Ramona Maraia, Sebastian Springer, Heikki Haario, Marko Laine, Olle Räty, Jukka-Pekka Jalkanen, Matti Antola*

- `2004.11267v2` - [abs](http://arxiv.org/abs/2004.11267v2) - [pdf](http://arxiv.org/pdf/2004.11267v2)

> Assessing the magnitude of fuel consumption of marine traffic is a challenging task. The consumption can be reduced by the ways the vessels are operated, to achieve both improved cost efficiency and reduced CO2 emissions. Mathematical models for predicting ships' consumption are in a central role in both of these tasks. Nowadays, many ships are equipped with data collection systems, which enable data-based calibration of the consumption models. Typically this calibration procedure is carried out independently for each particular ship, using only data collected from the ship in question. In this paper, we demonstrate a hierarchical Bayesian modeling approach, where we fit a single model over many vessels, with the assumption that the parameters of vessels of same type and similar characteristics (e.g. vessel size) are likely close to each other. The benefits of such an approach are two-fold; 1) we can borrow information about parameters that are not well informed by the vessel-specific data using data from similar ships, and 2) we can use the final hierarchical model to predict the behavior of a vessel from which we don't have any data, based only on its characteristics. In this paper, we discuss the basic concept and present a first simple version of the model. We apply the Stan statistical modeling tool for the model fitting and use real data from 64 cruise ships collected via the widely used commercial Eniram platform. By using Bayesian statistical methods we obtain uncertainties for the model predictions, too. The prediction accuracy of the model is compared to an existing data-free modeling approach.

</details>

<details>

<summary>2021-05-05 10:16:34 - On the mathematical axiomatization of approximate Bayesian computation. A robust set for estimating mechanistic network models through optimal transport</summary>

- *Marco Tarsia, Antonietta Mira, Daniele Cassani*

- `2105.01962v1` - [abs](http://arxiv.org/abs/2105.01962v1) - [pdf](http://arxiv.org/pdf/2105.01962v1)

> We research relations between optimal transport theory (OTT) and approximate Bayesian computation (ABC) possibly connected to relevant metrics defined on probability measures. Those of ABC are computational methods based on Bayesian statistics and applicable to a given generative model to estimate its a posteriori distribution in case the likelihood function is intractable. The idea is therefore to simulate sets of synthetic data from the model with respect to assigned parameters and, rather than comparing prospects of these data with the corresponding observed values as typically ABC requires, to employ just a distance between a chosen distribution associated to the synthetic data and another of the observed values. Our focus lies in theoretical and methodological aspects, although there would exist a remarkable part of algorithmic implementation, and more precisely issues regarding mathematical foundation and asymptotic properties are carefully analysed, inspired by an in-depth study of what is then our main bibliographic reference, that is Bernton et al. (2019), carrying out what follows: a rigorous formulation of the set-up for the ABC rejection algorithm, also to regain a transparent and general result of convergence as the ABC threshold goes to zero whereas the number n of samples from the prior stays fixed; general technical proposals about distances leaning on OTT; weak assumptions which lead to lower bounds for small values of threshold and as n goes to infinity, ultimately showing a reasonable possibility of lack of concentration which is contrary to what is proposed in Bernton et al. (2019) itself.

</details>

<details>

<summary>2021-05-05 15:47:33 - A Bayesian latent allocation model for clustering compositional data with application to the Great Barrier Reef</summary>

- *Luiza Piancastelli, Nial Friel, Julie Vercelloni, Kerrie Mengersen, Antonietta Mira*

- `2105.02140v1` - [abs](http://arxiv.org/abs/2105.02140v1) - [pdf](http://arxiv.org/pdf/2105.02140v1)

> Relative abundance is a common metric to estimate the composition of species in ecological surveys reflecting patterns of commonness and rarity of biological assemblages. Measurements of coral reef compositions formed by four communities along Australia's Great Barrier Reef (GBR) gathered between 2012 and 2017 are the focus of this paper. We undertake the task of finding clusters of transect locations with similar community composition and investigate changes in clustering dynamics over time. During these years, an unprecedented sequence of extreme weather events (cyclones and coral bleaching) impacted the 58 surveyed locations. The dependence between constituent parts of a composition presents a challenge for existing multivariate clustering approaches. In this paper, we introduce a finite mixture of Dirichlet distributions with group-specific parameters, where cluster memberships are dictated by unobserved latent variables. The inference is carried in a Bayesian framework, where MCMC strategies are outlined to sample from the posterior model. Simulation studies are presented to illustrate the performance of the model in a controlled setting. The application of the model to the 2012 coral reef data reveals that clusters were spatially distributed in similar ways across reefs which indicates a potential influence of wave exposure at the origin of coral reef community composition. The number of clusters estimated by the model decreased from four in 2012 to two from 2014 until 2017. Posterior probabilities of transect allocations to the same cluster substantially increase through time showing a potential homogenization of community composition across the whole GBR. The Bayesian model highlights the diversity of coral reef community composition within a coral reef and rapid changes across large spatial scales that may contribute to undermining the future of the GBR's biodiversity.

</details>

<details>

<summary>2021-05-05 17:28:08 - Bayesian Dynamic Estimation of Mortality Schedules in Small Areas</summary>

- *Guilherme Lopes de Oliveira, Rosangela Helena Loschi, Renato Martins Assunção*

- `2105.02203v1` - [abs](http://arxiv.org/abs/2105.02203v1) - [pdf](http://arxiv.org/pdf/2105.02203v1)

> The determination of the shapes of mortality curves, the estimation and projection of mortality patterns over time, and the investigation of differences in mortality patterns across different small underdeveloped populations have received special attention in recent years. The challenges involved in this type of problems are the common sparsity and the unstable behavior of observed death counts in small areas (populations). These features impose many dificulties in the estimation of reasonable mortality schedules. In this chapter, we present a discussion about this problem and we introduce the use of relational Bayesian dynamic models for estimating and smoothing mortality schedules by age and sex. Preliminary results are presented, including a comparison with a methodology recently proposed in the literature. The analyzes are based on simulated data as well as mortality data observed in some Brazilian municipalities.

</details>

<details>

<summary>2021-05-05 22:52:10 - Uncertain Neighbors: Bayesian Propensity Score Matching For Causal Inference</summary>

- *R. Michael Alvarez, Ines Levin*

- `2105.02362v1` - [abs](http://arxiv.org/abs/2105.02362v1) - [pdf](http://arxiv.org/pdf/2105.02362v1)

> We compare the performance of standard nearest-neighbor propensity score matching with that of an analogous Bayesian propensity score matching procedure. We show that the Bayesian approach makes better use of available information, as it makes less arbitrary decisions about which observations to drop and which ones to keep in the matched sample. We conduct a simulation study to evaluate the performance of standard and Bayesian nearest-neighbor matching when matching is done with and without replacement. We then use both methods to replicate a recent study about the impact of land reform on guerrilla activity in Colombia.

</details>

<details>

<summary>2021-05-06 06:37:40 - Semi-Exact Control Functionals From Sard's Method</summary>

- *Leah F. South, Toni Karvonen, Chris Nemeth, Mark Girolami, Chris. J. Oates*

- `2002.00033v4` - [abs](http://arxiv.org/abs/2002.00033v4) - [pdf](http://arxiv.org/pdf/2002.00033v4)

> The numerical approximation of posterior expected quantities of interest is considered. A novel control variate technique is proposed for post-processing of Markov chain Monte Carlo output, based both on Stein's method and an approach to numerical integration due to Sard. The resulting estimators are proven to be polynomially exact in the Gaussian context, while empirical results suggest the estimators approximate a Gaussian cubature method near the Bernstein-von-Mises limit. The main theoretical result establishes a bias-correction property in settings where the Markov chain does not leave the posterior invariant. Empirical results are presented across a selection of Bayesian inference tasks. All methods used in this paper are available in the R package ZVCV.

</details>

<details>

<summary>2021-05-06 13:18:26 - Chance-Constrained Active Inference</summary>

- *Thijs van de Laar, Ismail Senoz, Ayça Özçelikkale, Henk Wymeersch*

- `2102.08792v2` - [abs](http://arxiv.org/abs/2102.08792v2) - [pdf](http://arxiv.org/pdf/2102.08792v2)

> Active Inference (ActInf) is an emerging theory that explains perception and action in biological agents, in terms of minimizing a free energy bound on Bayesian surprise. Goal-directed behavior is elicited by introducing prior beliefs on the underlying generative model. In contrast to prior beliefs, which constrain all realizations of a random variable, we propose an alternative approach through chance constraints, which allow for a (typically small) probability of constraint violation, and demonstrate how such constraints can be used as intrinsic drivers for goal-directed behavior in ActInf. We illustrate how chance-constrained ActInf weights all imposed (prior) constraints on the generative model, allowing e.g., for a trade-off between robust control and empirical chance constraint violation. Secondly, we interpret the proposed solution within a message passing framework. Interestingly, the message passing interpretation is not only relevant to the context of ActInf, but also provides a general purpose approach that can account for chance constraints on graphical models. The chance constraint message updates can then be readily combined with other pre-derived message update rules, without the need for custom derivations. The proposed chance-constrained message passing framework thus accelerates the search for workable models in general, and can be used to complement message-passing formulations on generative neural models.

</details>

<details>

<summary>2021-05-06 15:13:56 - Bayesian Sequential Joint Detection and Estimation under Multiple Hypotheses</summary>

- *Dominik Reinhard, Michael Fauß, Abdelhak M. Zoubir*

- `2003.12405v3` - [abs](http://arxiv.org/abs/2003.12405v3) - [pdf](http://arxiv.org/pdf/2003.12405v3)

> We consider the problem of jointly testing multiple hypotheses and estimating a random parameter of the underlying distribution. This problem is investigated in a sequential setup under mild assumptions on the underlying random process. The optimal method minimizes the expected number of samples while ensuring that the average detection/estimation errors do not exceed a certain level. After converting the constrained problem to an unconstrained one, we characterize the general solution by a non-linear Bellman equation, which is parametrized by a set of cost coefficients. A strong connection between the derivatives of the cost function with respect to the coefficients and the detection/estimation errors of the sequential procedure is derived. Based on this fundamental property, we further show that for suitably chosen cost coefficients the solutions of the constrained and the unconstrained problem coincide. We present two approaches to finding the optimal coefficients. For the first approach, the final optimization problem is converted into a linear program, whereas the second approach solves it with a projected gradient ascent. To illustrate the theoretical results, we consider two problems for which the optimal schemes are designed numerically. Using Monte Carlo simulations, it is validated that the numerical results agree with the theory.

</details>

<details>

<summary>2021-05-06 16:41:04 - Practical and Rigorous Uncertainty Bounds for Gaussian Process Regression</summary>

- *Christian Fiedler, Carsten W. Scherer, Sebastian Trimpe*

- `2105.02796v1` - [abs](http://arxiv.org/abs/2105.02796v1) - [pdf](http://arxiv.org/pdf/2105.02796v1)

> Gaussian Process Regression is a popular nonparametric regression method based on Bayesian principles that provides uncertainty estimates for its predictions. However, these estimates are of a Bayesian nature, whereas for some important applications, like learning-based control with safety guarantees, frequentist uncertainty bounds are required. Although such rigorous bounds are available for Gaussian Processes, they are too conservative to be useful in applications. This often leads practitioners to replacing these bounds by heuristics, thus breaking all theoretical guarantees. To address this problem, we introduce new uncertainty bounds that are rigorous, yet practically useful at the same time. In particular, the bounds can be explicitly evaluated and are much less conservative than state of the art results. Furthermore, we show that certain model misspecifications lead to only graceful degradation. We demonstrate these advantages and the usefulness of our results for learning-based control with numerical examples.

</details>

<details>

<summary>2021-05-07 08:23:12 - A Discriminative Gaussian Mixture Model with Sparsity</summary>

- *Hideaki Hayashi, Seiichi Uchida*

- `1911.06028v2` - [abs](http://arxiv.org/abs/1911.06028v2) - [pdf](http://arxiv.org/pdf/1911.06028v2)

> In probabilistic classification, a discriminative model based on the softmax function has a potential limitation in that it assumes unimodality for each class in the feature space. The mixture model can address this issue, although it leads to an increase in the number of parameters. We propose a sparse classifier based on a discriminative GMM, referred to as a sparse discriminative Gaussian mixture (SDGM). In the SDGM, a GMM-based discriminative model is trained via sparse Bayesian learning. Using this sparse learning framework, we can simultaneously remove redundant Gaussian components and reduce the number of parameters used in the remaining components during learning; this learning method reduces the model complexity, thereby improving the generalization capability. Furthermore, the SDGM can be embedded into neural networks (NNs), such as convolutional NNs, and can be trained in an end-to-end manner. Experimental results demonstrated that the proposed method outperformed the existing softmax-based discriminative models.

</details>

<details>

<summary>2021-05-07 08:25:17 - Laplace Matching for fast Approximate Inference in Generalized Linear Models</summary>

- *Marius Hobbhahn, Philipp Hennig*

- `2105.03109v1` - [abs](http://arxiv.org/abs/2105.03109v1) - [pdf](http://arxiv.org/pdf/2105.03109v1)

> Bayesian inference in generalized linear models (GLMs), i.e.~Gaussian regression with non-Gaussian likelihoods, is generally non-analytic and requires computationally expensive approximations, such as sampling or variational inference. We propose an approximate inference framework primarily designed to be computationally cheap while still achieving high approximation quality. The concept, which we call \emph{Laplace Matching}, involves closed-form, approximate, bi-directional transformations between the parameter spaces of exponential families. These are constructed from Laplace approximations under custom-designed basis transformations. The mappings can then be leveraged to effectively turn a latent Gaussian distribution into a conjugate prior for a rich class of observable variables. This effectively turns inference in GLMs into conjugate inference (with small approximation errors). We empirically evaluate the method in two different GLMs, showing approximation quality comparable to state-of-the-art approximate inference techniques at a drastic reduction in computational cost. More specifically, our method has a cost comparable to the \emph{very first} step of the iterative optimization usually employed in standard GLM inference.

</details>

<details>

<summary>2021-05-07 10:27:43 - Inferring food intake from multiple biomarkers using a latent variable model</summary>

- *Silvia D'Angelo, Lorraine Brennan, Isobel Claire Gormley*

- `2006.02995v3` - [abs](http://arxiv.org/abs/2006.02995v3) - [pdf](http://arxiv.org/pdf/2006.02995v3)

> Metabolomic based approaches have gained much attention in recent years due to their promising potential to deliver objective tools for assessment of food intake. In particular, multiple biomarkers have emerged for single foods. However, there is a lack of statistical tools available for combining multiple biomarkers to infer food intake. Furthermore, there is a paucity of approaches for estimating the uncertainty around biomarker based prediction of intake.   Here, to facilitate inference on the relationship between multiple metabolomic biomarkers and food intake in an intervention study conducted under the A-DIET research programme, a latent variable model, multiMarker, is proposed. The proposed model draws on factor analytic and mixture of experts models, describing intake as a continuous latent variable whose value gives raise to the observed biomarker values. We employ a mixture of Gaussian distributions to flexibly model the latent variable. A Bayesian hierarchical modelling framework provides flexibility to adapt to different biomarker distributions and facilitates prediction of the latent intake along with its associated uncertainty.   Simulation studies are conducted to assess the performance of the proposed multiMarker framework, prior to its application to the motivating application of quantifying apple intake.

</details>

<details>

<summary>2021-05-07 13:48:33 - Bayesian spatio-temporal model for high-resolution short-term forecasting of precipitation fields</summary>

- *Stephen Richard Johnson, Sarah Elizabeth Heaps, Kevin James Wilson, Darren James Wilkinson*

- `2105.03269v1` - [abs](http://arxiv.org/abs/2105.03269v1) - [pdf](http://arxiv.org/pdf/2105.03269v1)

> With extreme weather events becoming more common, the risk posed by surface water flooding is ever increasing. In this work we propose a model, and associated Bayesian inference scheme, for generating probabilistic (high-resolution short-term) forecasts of localised precipitation. The parametrisation of our underlying hierarchical dynamic spatio-temporal model is motivated by a forward-time, centred-space finite difference solution to a collection of stochastic partial differential equations, where the main driving forces are advection and diffusion. Observations from both weather radar and ground based rain gauges provide information from which we can learn about the likely values of the (latent) precipitation field in addition to other unknown model parameters. Working in the Bayesian paradigm provides a coherent framework for capturing uncertainty both in the underlying model parameters and also in our forecasts. Further, appealing to simulation based (MCMC) sampling yields a straightforward solution to handling zeros, treated as censored observations, via data augmentation. Both the underlying state and the observations are of moderately large dimension ($\mathcal{O}(10^4)$ and $\mathcal{O}(10^3)$ respectively) and this renders standard inference approaches computationally infeasible. Our solution is to embed the ensemble Kalman smoother within a Gibbs sampling scheme to facilitate approximate Bayesian inference in reasonable time. Both the methodology and the effectiveness of our posterior sampling scheme are demonstrated via simulation studies and also by a case study of real data from the Urban Observatory project based in Newcastle upon Tyne, UK.

</details>

<details>

<summary>2021-05-07 16:39:14 - Robust and integrative Bayesian neural networks for likelihood-free parameter inference</summary>

- *Fredrik Wrede, Robin Eriksson, Richard Jiang, Linda Petzold, Stefan Engblom, Andreas Hellander, Prashant Singh*

- `2102.06521v2` - [abs](http://arxiv.org/abs/2102.06521v2) - [pdf](http://arxiv.org/pdf/2102.06521v2)

> State-of-the-art neural network-based methods for learning summary statistics have delivered promising results for simulation-based likelihood-free parameter inference. Existing approaches require density estimation as a post-processing step building upon deterministic neural networks, and do not take network prediction uncertainty into account. This work proposes a robust integrated approach that learns summary statistics using Bayesian neural networks, and directly estimates the posterior density using categorical distributions. An adaptive sampling scheme selects simulation locations to efficiently and iteratively refine the predictive posterior of the network conditioned on observations. This allows for more efficient and robust convergence on comparatively large prior spaces. We demonstrate our approach on benchmark examples and compare against related methods.

</details>

<details>

<summary>2021-05-07 18:36:47 - Bayesian Estimation of Attribute and Identification Disclosure Risks in Synthetic Data</summary>

- *Jingchen Hu*

- `1804.02784v3` - [abs](http://arxiv.org/abs/1804.02784v3) - [pdf](http://arxiv.org/pdf/1804.02784v3)

> The synthetic data approach to data confidentiality has been actively researched on, and for the past decade or so, a good number of high quality work on developing innovative synthesizers, creating appropriate utility measures and risk measures, among others, have been published. Comparing to a large volume of work on synthesizers development and utility measures creation, measuring risks has overall received less attention. This paper focuses on the detailed construction of some Bayesian methods proposed for estimating disclosure risks in synthetic data. In the processes of presenting attribute and identification disclosure risks evaluation methods, we highlight key steps, emphasize Bayesian thinking, illustrate with real application examples, and discuss challenges and future research directions. We hope to give the readers a comprehensive view of the Bayesian estimation procedures, enable synthetic data researchers and producers to use these procedures to evaluate disclosure risks, and encourage more researchers to work in this important growing field.

</details>

<details>

<summary>2021-05-08 01:56:42 - Spike-and-Slab Meets LASSO: A Review of the Spike-and-Slab LASSO</summary>

- *Ray Bai, Veronika Rockova, Edward I. George*

- `2010.06451v4` - [abs](http://arxiv.org/abs/2010.06451v4) - [pdf](http://arxiv.org/pdf/2010.06451v4)

> High-dimensional data sets have become ubiquitous in the past few decades, often with many more covariates than observations. In the frequentist setting, penalized likelihood methods are the most popular approach for variable selection and estimation in high-dimensional data. In the Bayesian framework, spike-and-slab methods are commonly used as probabilistic constructs for high-dimensional modeling. Within the context of linear regression, Rockova and George (2018) introduced the spike-and-slab LASSO (SSL), an approach based on a prior which provides a continuum between the penalized likelihood LASSO and the Bayesian point-mass spike-and-slab formulations. Since its inception, the spike-and-slab LASSO has been extended to a variety of contexts, including generalized linear models, factor analysis, graphical models, and nonparametric regression. The goal of this paper is to survey the landscape surrounding spike-and-slab LASSO methodology. First we elucidate the attractive properties and the computational tractability of SSL priors in high dimensions. We then review methodological developments of the SSL and outline several theoretical developments. We illustrate the methodology on both simulated and real datasets.

</details>

<details>

<summary>2021-05-08 09:28:42 - Spline-Based Bayesian Emulators for Large Scale Spatial Inverse Problems</summary>

- *Anirban Mondal, Bani Mallick*

- `2105.03651v1` - [abs](http://arxiv.org/abs/2105.03651v1) - [pdf](http://arxiv.org/pdf/2105.03651v1)

> A Bayesian approach to nonlinear inverse problems is considered where the unknown quantity (input) is a random spatial field. The forward model is complex and non-linear, therefore computationally expensive. An emulator-based methodology is developed, where the Bayesian multivariate adaptive regression splines (BMARS) are used to model the function that maps the inputs to the outputs. Discrete cosine transformation (DCT) is used for dimension reduction of the input spatial field. The posterior sampling is carried out using trans-dimensional Markov Chain Monte Carlo (MCMC) methods. Numerical results are presented by analyzing simulated as well as real data on hydrocarbon reservoir characterization.

</details>

<details>

<summary>2021-05-08 13:31:37 - A defective cure rate quantile regression model for male breast cancer data</summary>

- *Agatha Rodrigues, Patrick Borges, Bruno Santos*

- `2105.03699v1` - [abs](http://arxiv.org/abs/2105.03699v1) - [pdf](http://arxiv.org/pdf/2105.03699v1)

> In this article, we particularly address the problem of assessing the impact of clinical stage and age on the specific survival times of men with breast cancer when cure is a possibility, where there is also the interest of explaining this impact on different quantiles of the survival times. To this end, we developed a quantile regression model for survival data in the presence of long-term survivors based on the generalized distribution of Gompertz in a defective version, which is conveniently reparametrized in terms of the q-th quantile and then linked to covariates via a logarithm link function. This proposal allows us to obtain how each variable affects the survival times in different quantiles. In addition, we are able to study the effects of covariates on the cure rate as well. We consider Markov Chain Monte Carlo (MCMC) methods to develop a Bayesian analysis in the proposed model and we evaluate its performance through a Monte Carlo simulation study. Finally, we illustrate the advantages of our model in a data set about male breast cancer from Brazil.

</details>

<details>

<summary>2021-05-08 13:39:59 - Shrinkage priors for nonparametric Bayesian prediction of nonhomogeneous Poisson processes</summary>

- *Fumiyasu Komaki*

- `2006.04052v2` - [abs](http://arxiv.org/abs/2006.04052v2) - [pdf](http://arxiv.org/pdf/2006.04052v2)

> We consider nonparametric Bayesian estimation and prediction for nonhomogeneous Poisson process models with unknown intensity functions. We propose a class of improper priors for intensity functions. Nonparametric Bayesian inference with kernel mixture based on the class improper priors is shown to be useful, although improper priors have not been widely used for nonparametric Bayes problems. Several theorems corresponding to those for finite-dimensional independent Poisson models hold for nonhomogeneous Poisson process models with infinite-dimensional parameter spaces. Bayesian estimation and prediction based on the improper priors are shown to be admissible under the Kullback--Leibler loss. Numerical methods for Bayesian inference based on the priors are investigated.

</details>

<details>

<summary>2021-05-09 05:31:11 - Mean-Field Approximation to Gaussian-Softmax Integral with Application to Uncertainty Estimation</summary>

- *Zhiyun Lu, Eugene Ie, Fei Sha*

- `2006.07584v2` - [abs](http://arxiv.org/abs/2006.07584v2) - [pdf](http://arxiv.org/pdf/2006.07584v2)

> Many methods have been proposed to quantify the predictive uncertainty associated with the outputs of deep neural networks. Among them, ensemble methods often lead to state-of-the-art results, though they require modifications to the training procedures and are computationally costly for both training and inference. In this paper, we propose a new single-model based approach. The main idea is inspired by the observation that we can "simulate" an ensemble of models by drawing from a Gaussian distribution, with a form similar to those from the asymptotic normality theory, infinitesimal Jackknife, Laplacian approximation to Bayesian neural networks, and trajectories in stochastic gradient descents. However, instead of using each model in the "ensemble" to predict and then aggregating their predictions, we integrate the Gaussian distribution and the softmax outputs of the neural networks. We use a mean-field approximation formula to compute this analytically intractable integral. The proposed approach has several appealing properties: it functions as an ensemble without requiring multiple models, and it enables closed-form approximate inference using only the first and second moments of the Gaussian. Empirically, the proposed approach performs competitively when compared to state-of-the-art methods, including deep ensembles, temperature scaling, dropout and Bayesian NNs, on standard uncertainty estimation tasks. It also outperforms many methods on out-of-distribution detection.

</details>

<details>

<summary>2021-05-09 18:56:10 - Statistical Assessment of Replicability via Bayesian Model Criticism</summary>

- *Yi Zhao, Xiaoquan Wen*

- `2105.03993v1` - [abs](http://arxiv.org/abs/2105.03993v1) - [pdf](http://arxiv.org/pdf/2105.03993v1)

> Assessment of replicability is critical to ensure the quality and rigor of scientific research. In this paper, we discuss inference and modeling principles for replicability assessment. Targeting distinct application scenarios, we propose two types of Bayesian model criticism approaches to identify potentially irreproducible results in scientific experiments. They are motivated by established Bayesian prior and posterior predictive model-checking procedures and generalize many existing replicability assessment methods. Finally, we discuss the statistical properties of the proposed replicability assessment approaches and illustrate their usages by simulations and examples of real data analysis, including the data from the Reproducibility Project: Psychology and a systematic review of impacts of pre-existing cardiovascular disease on COVID-19 outcomes.

</details>

<details>

<summary>2021-05-09 19:21:43 - Bayesian Kernelised Test of (In)dependence with Mixed-type Variables</summary>

- *Alessio Benavoli, Cassio de Campos*

- `2105.04001v1` - [abs](http://arxiv.org/abs/2105.04001v1) - [pdf](http://arxiv.org/pdf/2105.04001v1)

> A fundamental task in AI is to assess (in)dependence between mixed-type variables (text, image, sound). We propose a Bayesian kernelised correlation test of (in)dependence using a Dirichlet process model. The new measure of (in)dependence allows us to answer some fundamental questions: Based on data, are (mixed-type) variables independent? How likely is dependence/independence to hold? How high is the probability that two mixed-type variables are more than just weakly dependent? We theoretically show the properties of the approach, as well as algorithms for fast computation with it. We empirically demonstrate the effectiveness of the proposed method by analysing its performance and by comparing it with other frequentist and Bayesian approaches on a range of datasets and tasks with mixed-type variables.

</details>

<details>

<summary>2021-05-09 21:38:24 - Hierarchical Bayesian Nearest Neighbor Co-Kriging Gaussian Process Models; An Application to Intersatellite Calibration</summary>

- *Si Cheng, Bledar A. Konomi, Jessica L. Matthews, Georgios Karagiannis, Emily L. Kang*

- `2004.01341v3` - [abs](http://arxiv.org/abs/2004.01341v3) - [pdf](http://arxiv.org/pdf/2004.01341v3)

> Recent advancements in remote sensing technology and the increasing size of satellite constellations allows massive geophysical information to be gathered daily on a global scale by numerous platforms of different fidelity. The auto-regressive co-kriging model is a suitable framework to analyse such data sets because it accounts for cross-dependencies among different fidelity satellite outputs. However, its implementation in multifidelity large spatial data-sets is practically infeasible because its computational complexity increases cubically with the total number of observations. In this paper, we propose a nearest neighbour co-kriging Gaussian process that couples the auto-regressive model and nearest neighbour GP by using augmentation ideas; reducing the computational complexity to be linear with the total number of spatial observed locations. The latent process of the nearest neighbour GP is augmented in a manner which allows the specification of semi-conjugate priors. This facilitates the design of an efficient MCMC sampler involving mostly direct sampling updates which can be implemented in parallel computational environments. The good predictive performance of the proposed method is demonstrated in a simulation study. We use the proposed method to analyze High-resolution Infrared Radiation Sounder data gathered from two NOAA polar orbiting satellites.

</details>

<details>

<summary>2021-05-10 13:06:55 - A Bayesian Approach for Predicting Food and Beverage Sales in Staff Canteens and Restaurants</summary>

- *Konstantin Posch, Christian Truden, Philipp Hungerländer, Jürgen Pilz*

- `2005.12647v3` - [abs](http://arxiv.org/abs/2005.12647v3) - [pdf](http://arxiv.org/pdf/2005.12647v3)

> Accurate demand forecasting is one of the key aspects for successfully managing restaurants and staff canteens. In particular, properly predicting future sales of menu items allows a precise ordering of food stock. From an environmental point of view, this ensures maintaining a low level of pre-consumer food waste, while from the managerial point of view, this is critical to guarantee the profitability of the restaurant. Hence, we are interested in predicting future values of the daily sold quantities of given menu items. The corresponding time series show multiple strong seasonalities, trend changes, data gaps, and outliers. We propose a forecasting approach that is solely based on the data retrieved from Point of Sales systems and allows for a straightforward human interpretation. Therefore, we propose two generalized additive models for predicting the future sales. In an extensive evaluation, we consider two data sets collected at a casual restaurant and a large staff canteen consisting of multiple time series, that cover a period of 20 months, respectively. We show that the proposed models fit the features of the considered restaurant data. Moreover, we compare the predictive performance of our method against the performance of other well-established forecasting approaches.

</details>

<details>

<summary>2021-05-10 13:07:44 - Bayesian Optimistic Optimisation with Exponentially Decaying Regret</summary>

- *Hung Tran-The, Sunil Gupta, Santu Rana, Svetha Venkatesh*

- `2105.04332v1` - [abs](http://arxiv.org/abs/2105.04332v1) - [pdf](http://arxiv.org/pdf/2105.04332v1)

> Bayesian optimisation (BO) is a well-known efficient algorithm for finding the global optimum of expensive, black-box functions. The current practical BO algorithms have regret bounds ranging from $\mathcal{O}(\frac{logN}{\sqrt{N}})$ to $\mathcal O(e^{-\sqrt{N}})$, where $N$ is the number of evaluations. This paper explores the possibility of improving the regret bound in the noiseless setting by intertwining concepts from BO and tree-based optimistic optimisation which are based on partitioning the search space. We propose the BOO algorithm, a first practical approach which can achieve an exponential regret bound with order $\mathcal O(N^{-\sqrt{N}})$ under the assumption that the objective function is sampled from a Gaussian process with a Mat\'ern kernel with smoothness parameter $\nu > 4 +\frac{D}{2}$, where $D$ is the number of dimensions. We perform experiments on optimisation of various synthetic functions and machine learning hyperparameter tuning tasks and show that our algorithm outperforms baselines.

</details>

<details>

<summary>2021-05-10 13:59:25 - Gradient-based Bayesian Experimental Design for Implicit Models using Mutual Information Lower Bounds</summary>

- *Steven Kleinegesse, Michael U. Gutmann*

- `2105.04379v1` - [abs](http://arxiv.org/abs/2105.04379v1) - [pdf](http://arxiv.org/pdf/2105.04379v1)

> We introduce a framework for Bayesian experimental design (BED) with implicit models, where the data-generating distribution is intractable but sampling from it is still possible. In order to find optimal experimental designs for such models, our approach maximises mutual information lower bounds that are parametrised by neural networks. By training a neural network on sampled data, we simultaneously update network parameters and designs using stochastic gradient-ascent. The framework enables experimental design with a variety of prominent lower bounds and can be applied to a wide range of scientific tasks, such as parameter estimation, model discrimination and improving future predictions. Using a set of intractable toy models, we provide a comprehensive empirical comparison of prominent lower bounds applied to the aforementioned tasks. We further validate our framework on a challenging system of stochastic differential equations from epidemiology.

</details>

<details>

<summary>2021-05-10 15:16:18 - Scaffolding Simulations with Deep Learning for High-dimensional Deconvolution</summary>

- *Anders Andreassen, Patrick T. Komiske, Eric M. Metodiev, Benjamin Nachman, Adi Suresh, Jesse Thaler*

- `2105.04448v1` - [abs](http://arxiv.org/abs/2105.04448v1) - [pdf](http://arxiv.org/pdf/2105.04448v1)

> A common setting for scientific inference is the ability to sample from a high-fidelity forward model (simulation) without having an explicit probability density of the data. We propose a simulation-based maximum likelihood deconvolution approach in this setting called OmniFold. Deep learning enables this approach to be naturally unbinned and (variable-, and) high-dimensional. In contrast to model parameter estimation, the goal of deconvolution is to remove detector distortions in order to enable a variety of down-stream inference tasks. Our approach is the deep learning generalization of the common Richardson-Lucy approach that is also called Iterative Bayesian Unfolding in particle physics. We show how OmniFold can not only remove detector distortions, but it can also account for noise processes and acceptance effects.

</details>

<details>

<summary>2021-05-10 15:28:10 - Search Algorithms and Loss Functions for Bayesian Clustering</summary>

- *David B. Dahl, Devin J. Johnson, Peter Mueller*

- `2105.04451v1` - [abs](http://arxiv.org/abs/2105.04451v1) - [pdf](http://arxiv.org/pdf/2105.04451v1)

> We propose a randomized greedy search algorithm to find a point estimate for a random partition based on a loss function and posterior Monte Carlo samples. Given the large size and awkward discrete nature of the search space, the minimization of the posterior expected loss is challenging. Our approach is a stochastic search based on a series of greedy optimizations performed in a random order and is embarrassingly parallel. We consider several loss functions, including Binder loss and variation of information. We note that criticisms of Binder loss are the result of using equal penalties of misclassification and we show an efficient means to compute Binder loss with potentially unequal penalties. Furthermore, we extend the original variation of information to allow for unequal penalties and show no increased computational costs. We provide a reference implementation of our algorithm. Using a variety of examples, we show that our method produces clustering estimates that better minimize the expected loss and are obtained faster than existing methods.

</details>

<details>

<summary>2021-05-11 06:05:45 - Sketching in Bayesian High Dimensional Regression With Big Data Using Gaussian Scale Mixture Priors</summary>

- *Rajarshi Guhaniyogi, Aaron Scheffler*

- `2105.04795v1` - [abs](http://arxiv.org/abs/2105.04795v1) - [pdf](http://arxiv.org/pdf/2105.04795v1)

> Bayesian computation of high dimensional linear regression models with a popular Gaussian scale mixture prior distribution using Markov Chain Monte Carlo (MCMC) or its variants can be extremely slow or completely prohibitive due to the heavy computational cost that grows in the cubic order of p, with p as the number of features. Although a few recently developed algorithms make the computation efficient in presence of a small to moderately large sample size (with the complexity growing in the cubic order of n), the computation becomes intractable when sample size n is also large. In this article we adopt the data sketching approach to compress the n original samples by a random linear transformation to m<<n samples in p dimensions, and compute Bayesian regression with Gaussian scale mixture prior distributions with the randomly compressed response vector and feature matrix. Our proposed approach yields computational complexity growing in the cubic order of m. Another important motivation for this compression procedure is that it anonymizes the data by revealing little information about the original data in the course of analysis. Our detailed empirical investigation with the Horseshoe prior from the class of Gaussian scale mixture priors shows closely similar inference and a massive reduction in per iteration computation time of the proposed approach compared to the regression with the full sample. One notable contribution of this article is to derive posterior contraction rate for high dimensional predictor coefficient with a general class of shrinkage priors on them under data compression/sketching. In particular, we characterize the dimension of the compressed response vector m as a function of the sample size, number of predictors and sparsity in the regression to guarantee accurate estimation of predictor coefficients asymptotically, even after data compression.

</details>

<details>

<summary>2021-05-11 09:58:12 - On Unbiased Score Estimation for Partially Observed Diffusions</summary>

- *Jeremy Heng, Jeremie Houssineau, Ajay Jasra*

- `2105.04912v1` - [abs](http://arxiv.org/abs/2105.04912v1) - [pdf](http://arxiv.org/pdf/2105.04912v1)

> We consider the problem of statistical inference for a class of partially-observed diffusion processes, with discretely-observed data and finite-dimensional parameters. We construct unbiased estimators of the score function, i.e. the gradient of the log-likelihood function with respect to parameters, with no time-discretization bias. These estimators can be straightforwardly employed within stochastic gradient methods to perform maximum likelihood estimation or Bayesian inference. As our proposed methodology only requires access to a time-discretization scheme such as the Euler-Maruyama method, it is applicable to a wide class of diffusion processes and observation models. Our approach is based on a representation of the score as a smoothing expectation using Girsanov theorem, and a novel adaptation of the randomization schemes developed in Mcleish [2011], Rhee and Glynn [2015], Jacob et al. [2020a]. This allows one to remove the time-discretization bias and burn-in bias when computing smoothing expectations using the conditional particle filter of Andrieu et al. [2010]. Central to our approach is the development of new couplings of multiple conditional particle filters. We prove under assumptions that our estimators are unbiased and have finite variance. The methodology is illustrated on several challenging applications from population ecology and neuroscience.

</details>

<details>

<summary>2021-05-11 12:00:24 - Bayesian Selective Inference: Non-informative Priors</summary>

- *Daniel G. Rasines, G. Alastair Young*

- `2008.04584v2` - [abs](http://arxiv.org/abs/2008.04584v2) - [pdf](http://arxiv.org/pdf/2008.04584v2)

> We discuss Bayesian inference for parameters selected using the data. First, we provide a critical analysis of the existing positions in the literature regarding the correct Bayesian approach under selection. Second, we propose two types of non-informative priors for selection models. These priors may be employed to produce a posterior distribution in the absence of prior information as well as to provide well-calibrated frequentist inference for the selected parameter. We test the proposed priors empirically in several scenarios.

</details>

<details>

<summary>2021-05-11 13:46:51 - Bayesian Analysis of Glucose Dynamics during the Oral Glucose Tolerance Test (OGTT)</summary>

- *Hugo Flores-Arguedas, Marcos A. Capistrán*

- `2012.05108v2` - [abs](http://arxiv.org/abs/2012.05108v2) - [pdf](http://arxiv.org/pdf/2012.05108v2)

> In this paper, we propose a model of the dynamics of the blood glucose level during an Oral Glucose Tolerance Test (OGTT). This dynamic includes the action of insulin and glucagon in the glucose homeostasis process as the reaction of an oral stimulus. We propose a Bayesian approach in the inference of five parameters related to insulin secretion, glucagon secretion, gastrointestinal emptying, and basal glucose level. Two insulin indicators related to the glucose level in blood and in the gastrointestinal tract allow us to suggest a classification for patients with impaired insulin sensitivity.

</details>

<details>

<summary>2021-05-11 14:38:06 - Phylogenetically informed Bayesian truncated copula graphical models for microbial association networks</summary>

- *Hee Cheol Chung, Irina Gaynanova, Yang Ni*

- `2105.05082v1` - [abs](http://arxiv.org/abs/2105.05082v1) - [pdf](http://arxiv.org/pdf/2105.05082v1)

> Microorganisms play a critical role in host health. The advancement of high-throughput sequencing technology provides opportunities for a deeper understanding of microbial interactions. However, due to the limitations of 16S ribosomal RNA sequencing, microbiome data are zero-inflated, and a quantitative comparison of microbial abundances cannot be made across subjects. By leveraging a recent microbiome profiling technique that quantifies 16S ribosomal RNA microbial counts, we propose a novel Bayesian graphical model that incorporates microorganisms' evolutionary history through a phylogenetic tree prior and explicitly accounts for zero-inflation using the truncated Gaussian copula. Our simulation study reveals that the evolutionary information substantially improves the network estimation accuracy. We apply the proposed model to the quantitative gut microbiome data of 106 healthy subjects, and identify three distinct microbial communities that are not determined by existing microbial network estimation models. We further find that these communities are discriminated based on microorganisms' ability to utilize oxygen as an energy source.

</details>

<details>

<summary>2021-05-11 16:02:39 - A Twin Neural Model for Uplift</summary>

- *Mouloud Belbahri, Olivier Gandouet, Alejandro Murua, Vahid Partovi Nia*

- `2105.05146v1` - [abs](http://arxiv.org/abs/2105.05146v1) - [pdf](http://arxiv.org/pdf/2105.05146v1)

> Uplift is a particular case of conditional treatment effect modeling. Such models deal with cause-and-effect inference for a specific factor, such as a marketing intervention or a medical treatment. In practice, these models are built on individual data from randomized clinical trials where the goal is to partition the participants into heterogeneous groups depending on the uplift. Most existing approaches are adaptations of random forests for the uplift case. Several split criteria have been proposed in the literature, all relying on maximizing heterogeneity. However, in practice, these approaches are prone to overfitting. In this work, we bring a new vision to uplift modeling. We propose a new loss function defined by leveraging a connection with the Bayesian interpretation of the relative risk. Our solution is developed for a specific twin neural network architecture allowing to jointly optimize the marginal probabilities of success for treated and control individuals. We show that this model is a generalization of the uplift logistic interaction model. We modify the stochastic gradient descent algorithm to allow for structured sparse solutions. This helps training our uplift models to a great extent. We show our proposed method is competitive with the state-of-the-art in simulation setting and on real data from large scale randomized experiments.

</details>

<details>

<summary>2021-05-11 18:26:31 - Gaussian graphical models with graph constraints for magnetic moment interaction in high entropy alloys</summary>

- *Xinrui Liu, Yifeng Wu, Douglas L. Irving, Meng Li*

- `2105.05280v1` - [abs](http://arxiv.org/abs/2105.05280v1) - [pdf](http://arxiv.org/pdf/2105.05280v1)

> This article is motivated by studying the interaction of magnetic moments in high entropy alloys (HEAs), which plays an important role in guiding HEA designs in materials science. While first principles simulations can capture magnetic moments of individual atoms, explicit models are required to analyze their interactions. This is essentially an inverse covariance matrix estimation problem. Unlike most of the literature on graphical models, the inverse covariance matrix possesses inherent structural constraints encompassing node types, topological distance of nodes, and partially specified conditional dependence patterns. The present article is, to our knowledge, the first to consider such intricate structures in graphical models. In particular, we introduce graph constraints to formulate these structures that arise from domain knowledge and are critical for interpretability, which leads to a Bayesian conditional autoregressive model with graph constraints (CARGO) for structured inverse covariance matrices. The CARGO method enjoys efficient implementation with a modified Gauss-Seidel scheme through proximity operators for closed-form posterior exploration. We establish algorithmic convergence for the proposed algorithm under a verifiable stopping criterion. Simulations show competitive performance of CARGO relative to several other methods and confirm our convergence analysis. In a novel real data application to HEAs, the proposed methods lead to data-driven quantification and interpretation of magnetic moment interactions with high tractability and transferability.

</details>

<details>

<summary>2021-05-11 23:09:14 - Real-time Ionospheric Imaging of S4 Scintillation from Limited Data with Parallel Kalman Filters and Smoothness</summary>

- *Alexandra Koulouri*

- `2105.05360v1` - [abs](http://arxiv.org/abs/2105.05360v1) - [pdf](http://arxiv.org/pdf/2105.05360v1)

> In this paper, we propose a Bayesian framework to create two dimensional ionospheric images of high spatio-temporal resolution to monitor ionospheric irregularities as measured by the S4 index. Here, we recast the standard Bayesian recursive filtering for a linear Gaussian state-space model, also referred to as the Kalman filter, first by augmenting the (pierce point) observation model with connectivity information stemming from the insight and assumptions/standard modeling about the spatial distribution of the scintillation activity on the ionospheric shell at 350 km altitude. Thus, we achieve to handle the limited spatio-temporal observations. Then, by introducing a set of Kalman filters running in parallel, we mitigate the uncertainty related to a tuning parameter of the proposed augmented model. The output images are a weighted average of the state estimates of the individual filters. We demonstrate our approach by rendering two dimensional real-time ionospheric images of S4 amplitude scintillation at 350 km over South America with temporal resolution of one minute. Furthermore, we employ extra S4 data that was not used in producing these ionospheric images, to check and verify the ability of our images to predict this extra data in particular ionospheric pierce points. Our results show that in areas with a network of ground receivers with a relatively good coverage (e.g. within a couple of kilometers distance) the produced images can provide reliable real-time results. Our proposed algorithmic framework can be readily used to visualize real-time ionospheric images taking as inputs the available scintillation data provided from freely available web-servers.

</details>

<details>

<summary>2021-05-11 23:20:18 - A Langevinized Ensemble Kalman Filter for Large-Scale Static and Dynamic Learning</summary>

- *Peiyi Zhang, Qifan Song, Faming Liang*

- `2105.05363v1` - [abs](http://arxiv.org/abs/2105.05363v1) - [pdf](http://arxiv.org/pdf/2105.05363v1)

> The Ensemble Kalman Filter (EnKF) has achieved great successes in data assimilation in atmospheric and oceanic sciences, but its failure in convergence to the right filtering distribution precludes its use for uncertainty quantification. We reformulate the EnKF under the framework of Langevin dynamics, which leads to a new particle filtering algorithm, the so-called Langevinized EnKF. The Langevinized EnKF inherits the forecast-analysis procedure from the EnKF and the use of mini-batch data from the stochastic gradient Langevin-type algorithms, which make it scalable with respect to both the dimension and sample size. We prove that the Langevinized EnKF converges to the right filtering distribution in Wasserstein distance under the big data scenario that the dynamic system consists of a large number of stages and has a large number of samples observed at each stage. We reformulate the Bayesian inverse problem as a dynamic state estimation problem based on the techniques of subsampling and Langevin diffusion process. We illustrate the performance of the Langevinized EnKF using a variety of examples, including the Lorenz-96 model, high-dimensional variable selection, Bayesian deep learning, and Long Short Term Memory (LSTM) network learning with dynamic data.

</details>

<details>

<summary>2021-05-12 04:37:16 - A Unified Evaluation of Two-Candidate Ballot-Polling Election Auditing Methods</summary>

- *Zhuoqun Huang, Ronald L. Rivest, Philip B. Stark, Vanessa Teague, Damjan Vukcevic*

- `2008.08536v2` - [abs](http://arxiv.org/abs/2008.08536v2) - [pdf](http://arxiv.org/pdf/2008.08536v2)

> Counting votes is complex and error-prone. Several statistical methods have been developed to assess election accuracy by manually inspecting randomly selected physical ballots. Two 'principled' methods are risk-limiting audits (RLAs) and Bayesian audits (BAs). RLAs use frequentist statistical inference while BAs are based on Bayesian inference. Until recently, the two have been thought of as fundamentally different.   We present results that unify and shed light upon 'ballot-polling' RLAs and BAs (which only require the ability to sample uniformly at random from all cast ballot cards) for two-candidate plurality contests, which are building blocks for auditing more complex social choice functions, including some preferential voting systems. We highlight the connections between the methods and explore their performance.   First, building on a previous demonstration of the mathematical equivalence of classical and Bayesian approaches, we show that BAs, suitably calibrated, are risk-limiting. Second, we compare the efficiency of the methods across a wide range of contest sizes and margins, focusing on the distribution of sample sizes required to attain a given risk limit. Third, we outline several ways to improve performance and show how the mathematical equivalence explains the improvements.

</details>

<details>

<summary>2021-05-12 07:51:47 - Multiscale Invertible Generative Networks for High-Dimensional Bayesian Inference</summary>

- *Shumao Zhang, Pengchuan Zhang, Thomas Y. Hou*

- `2105.05489v1` - [abs](http://arxiv.org/abs/2105.05489v1) - [pdf](http://arxiv.org/pdf/2105.05489v1)

> We propose a Multiscale Invertible Generative Network (MsIGN) and associated training algorithm that leverages multiscale structure to solve high-dimensional Bayesian inference. To address the curse of dimensionality, MsIGN exploits the low-dimensional nature of the posterior, and generates samples from coarse to fine scale (low to high dimension) by iteratively upsampling and refining samples. MsIGN is trained in a multi-stage manner to minimize the Jeffreys divergence, which avoids mode dropping in high-dimensional cases. On two high-dimensional Bayesian inverse problems, we show superior performance of MsIGN over previous approaches in posterior approximation and multiple mode capture. On the natural image synthesis task, MsIGN achieves superior performance in bits-per-dimension over baseline models and yields great interpret-ability of its neurons in intermediate layers.

</details>

<details>

<summary>2021-05-12 08:57:39 - Differentially Private Bayesian Inference for Generalized Linear Models</summary>

- *Tejas Kulkarni, Joonas Jälkö, Antti Koskela, Samuel Kaski, Antti Honkela*

- `2011.00467v3` - [abs](http://arxiv.org/abs/2011.00467v3) - [pdf](http://arxiv.org/pdf/2011.00467v3)

> Generalized linear models (GLMs) such as logistic regression are among the most widely used arms in data analyst's repertoire and often used on sensitive datasets. A large body of prior works that investigate GLMs under differential privacy (DP) constraints provide only private point estimates of the regression coefficients, and are not able to quantify parameter uncertainty. In this work, with logistic and Poisson regression as running examples, we introduce a generic noise-aware DP Bayesian inference method for a GLM at hand, given a noisy sum of summary statistics. Quantifying uncertainty allows us to determine which of the regression coefficients are statistically significantly different from zero. We provide a previously unknown tight privacy analysis and experimentally demonstrate that the posteriors obtained from our model, while adhering to strong privacy guarantees, are close to the non-private posteriors.

</details>

<details>

<summary>2021-05-12 10:11:32 - Robust Learning of Fixed-Structure Bayesian Networks in Nearly-Linear Time</summary>

- *Yu Cheng, Honghao Lin*

- `2105.05555v1` - [abs](http://arxiv.org/abs/2105.05555v1) - [pdf](http://arxiv.org/pdf/2105.05555v1)

> We study the problem of learning Bayesian networks where an $\epsilon$-fraction of the samples are adversarially corrupted. We focus on the fully-observable case where the underlying graph structure is known. In this work, we present the first nearly-linear time algorithm for this problem with a dimension-independent error guarantee. Previous robust algorithms with comparable error guarantees are slower by at least a factor of $(d/\epsilon)$, where $d$ is the number of variables in the Bayesian network and $\epsilon$ is the fraction of corrupted samples.   Our algorithm and analysis are considerably simpler than those in previous work. We achieve this by establishing a direct connection between robust learning of Bayesian networks and robust mean estimation. As a subroutine in our algorithm, we develop a robust mean estimation algorithm whose runtime is nearly-linear in the number of nonzeros in the input samples, which may be of independent interest.

</details>

<details>

<summary>2021-05-12 12:44:54 - Smooth Variational Graph Embeddings for Efficient Neural Architecture Search</summary>

- *Jovita Lukasik, David Friede, Arber Zela, Frank Hutter, Margret Keuper*

- `2010.04683v3` - [abs](http://arxiv.org/abs/2010.04683v3) - [pdf](http://arxiv.org/pdf/2010.04683v3)

> Neural architecture search (NAS) has recently been addressed from various directions, including discrete, sampling-based methods and efficient differentiable approaches. While the former are notoriously expensive, the latter suffer from imposing strong constraints on the search space. Architecture optimization from a learned embedding space for example through graph neural network based variational autoencoders builds a middle ground and leverages advantages from both sides. Such approaches have recently shown good performance on several benchmarks. Yet, their stability and predictive power heavily depends on their capacity to reconstruct networks from the embedding space. In this paper, we propose a two-sided variational graph autoencoder, which allows to smoothly encode and accurately reconstruct neural architectures from various search spaces. We evaluate the proposed approach on neural architectures defined by the ENAS approach, the NAS-Bench-101 and the NAS-Bench-201 search space and show that our smooth embedding space allows to directly extrapolate the performance prediction to architectures outside the seen domain (e.g. with more operations). Thus, it facilitates to predict good network architectures even without expensive Bayesian optimization or reinforcement learning.

</details>

<details>

<summary>2021-05-12 21:16:24 - SPUX Framework: a Scalable Package for Bayesian Uncertainty Quantification and Propagation</summary>

- *Jonas Šukys, Marco Bacci*

- `2105.05969v1` - [abs](http://arxiv.org/abs/2105.05969v1) - [pdf](http://arxiv.org/pdf/2105.05969v1)

> We present SPUX - a modular framework for Bayesian inference enabling uncertainty quantification and propagation in linear and nonlinear, deterministic and stochastic models, and supporting Bayesian model selection. SPUX can be coupled to any serial or parallel application written in any programming language, (e.g. including Python, R, Julia, C/C++, Fortran, Java, or a binary executable), scales effortlessly from serial runs on a personal computer to parallel high performance computing clusters, and aims to provide a platform particularly suited to support and foster reproducibility in computational science. We illustrate SPUX capabilities for a simple yet representative random walk model, describe how to couple different types of user applications, and showcase several readily available examples from environmental sciences. In addition to available state-of-the-art numerical inference algorithms including EMCEE, PMCMC (PF) and SABC, the open source nature of the SPUX framework and the explicit description of the hierarchical parallel SPUX executors should also greatly simplify the implementation and usage of other inference and optimization techniques.

</details>

<details>

<summary>2021-05-13 01:19:08 - A Bayesian Long Short-Term Memory Model for Value at Risk and Expected Shortfall Joint Forecasting</summary>

- *Zhengkun Li, Minh-Ngoc Tran, Chao Wang, Richard Gerlach, Junbin Gao*

- `2001.08374v2` - [abs](http://arxiv.org/abs/2001.08374v2) - [pdf](http://arxiv.org/pdf/2001.08374v2)

> Value-at-Risk (VaR) and Expected Shortfall (ES) are widely used in the financial sector to measure the market risk and manage the extreme market movement. The recent link between the quantile score function and the Asymmetric Laplace density has led to a flexible likelihood-based framework for joint modelling of VaR and ES. It is of high interest in financial applications to be able to capture the underlying joint dynamics of these two quantities. We address this problem by developing a hybrid model that is based on the Asymmetric Laplace quasi-likelihood and employs the Long Short-Term Memory (LSTM) time series modelling technique from Machine Learning to capture efficiently the underlying dynamics of VaR and ES. We refer to this model as LSTM-AL. We adopt the adaptive Markov chain Monte Carlo (MCMC) algorithm for Bayesian inference in the LSTM-AL model. Empirical results show that the proposed LSTM-AL model can improve the VaR and ES forecasting accuracy over a range of well-established competing models.

</details>

<details>

<summary>2021-05-13 16:09:59 - Testing Conditional Independence in Supervised Learning Algorithms</summary>

- *David S. Watson, Marvin N. Wright*

- `1901.09917v5` - [abs](http://arxiv.org/abs/1901.09917v5) - [pdf](http://arxiv.org/pdf/1901.09917v5)

> We propose the conditional predictive impact (CPI), a consistent and unbiased estimator of the association between one or several features and a given outcome, conditional on a reduced feature set. Building on the knockoff framework of Cand\`es et al. (2018), we develop a novel testing procedure that works in conjunction with any valid knockoff sampler, supervised learning algorithm, and loss function. The CPI can be efficiently computed for high-dimensional data without any sparsity constraints. We demonstrate convergence criteria for the CPI and develop statistical inference procedures for evaluating its magnitude, significance, and precision. These tests aid in feature and model selection, extending traditional frequentist and Bayesian techniques to general supervised learning tasks. The CPI may also be applied in causal discovery to identify underlying multivariate graph structures. We test our method using various algorithms, including linear regression, neural networks, random forests, and support vector machines. Empirical results show that the CPI compares favorably to alternative variable importance measures and other nonparametric tests of conditional independence on a diverse array of real and simulated datasets. Simulations confirm that our inference procedures successfully control Type I error and achieve nominal coverage probability. Our method has been implemented in an R package, cpi, which can be downloaded from https://github.com/dswatson/cpi.

</details>

<details>

<summary>2021-05-13 22:23:51 - PICAR: An Efficient Extendable Approach for Fitting Hierarchical Spatial Models</summary>

- *Ben Seiyon Lee, Murali Haran*

- `1912.02382v2` - [abs](http://arxiv.org/abs/1912.02382v2) - [pdf](http://arxiv.org/pdf/1912.02382v2)

> Hierarchical spatial models are very flexible and popular for a vast array of applications in areas such as ecology, social science, public health, and atmospheric science. It is common to carry out Bayesian inference for these models via Markov chain Monte Carlo (MCMC). Each iteration of the MCMC algorithm is computationally expensive due to costly matrix operations. In addition, the MCMC algorithm needs to be run for more iterations because the strong cross-correlations among the spatial latent variables result in slow mixing Markov chains. To address these computational challenges, we propose a projection-based intrinsic conditional autoregression (PICAR) approach, which is a discretized and dimension-reduced representation of the underlying spatial random field using empirical basis functions on a triangular mesh. Our approach exhibits fast mixing as well as a considerable reduction in computational cost per iteration. PICAR is computationally efficient and scales well to high dimensions. It is also automated and easy to implement for a wide array of user-specified hierarchical spatial models. We show, via simulation studies, that our approach performs well in terms of parameter inference and prediction. We provide several examples to illustrate the applicability of our method, including (i) a high-dimensional cloud cover dataset that showcases its computational efficiency, (ii) a spatially varying coefficient model that demonstrates the ease of implementation of PICAR in the probabilistic programming languages stan and nimble, and (iii) a watershed survey example that illustrates how PICAR applies to models that are not amenable to efficient inference via existing methods.

</details>

<details>

<summary>2021-05-14 14:31:36 - Improving the Performance of Bayesian Logistic Regression Model with Overdose Control in Oncology Dose-Finding Studies</summary>

- *Hongtao Zhang, Alan Y Chiang, Jixian Wang*

- `2105.06855v1` - [abs](http://arxiv.org/abs/2105.06855v1) - [pdf](http://arxiv.org/pdf/2105.06855v1)

> An accurately identified maximum tolerated dose (MTD) serves as the cornerstone of successful subsequent phases in oncology drug development. Bayesian logistic regression model (BLRM) is a popular and versatile model-based dose-finding design. However, BLRM with original overdose control strategy has been reported to be safe but "excessively conservative". In this manuscript, we investigate the reason for conservativeness and point out that a major reason could be the lack of appropriate underdose control. We propose designs that balance overdose and underdose control to improve the performance over original BLRM. Simulation results reveal that the new designs have better accuracy and treat more patients at MTD.

</details>

<details>

<summary>2021-05-14 15:48:22 - Global sensitivity analysis informed model reduction and selection applied to a Valsalva maneuver model</summary>

- *E. Benjamin Randall, Nicholas Z. Randolph, Alen Alexanderian, Mette S. Olufsen*

- `2005.12879v3` - [abs](http://arxiv.org/abs/2005.12879v3) - [pdf](http://arxiv.org/pdf/2005.12879v3)

> In this study, we develop a methodology for model reduction and selection informed by global sensitivity analysis (GSA) methods. We apply these techniques to a control model that takes systolic blood pressure and thoracic tissue pressure data as inputs and predicts heart rate in response to the Valsalva maneuver (VM). The study compares four GSA methods based on Sobol' indices (SIs) quantifying the parameter influence on the difference between the model output and the heart rate data. The GSA methods include standard scalar SIs determining the average parameter influence over the time interval studied and three time-varying methods analyzing how parameter influence changes over time. The time-varying methods include a new technique, termed limited-memory SIs, predicting parameter influence using a moving window approach. Using the limited-memory SIs, we perform model reduction and selection to analyze the necessity of modeling both the aortic and carotid baroreceptor regions in response to the VM. We compare the original model to three systematically reduced models including (i) the aortic and carotid regions, (ii) the aortic region only, and (iii) the carotid region only. Model selection is done quantitatively using the Akaike and Bayesian Information Criteria and qualitatively by comparing the neurological predictions. Results show that it is necessary to incorporate both the aortic and carotid regions to model the VM.

</details>

<details>

<summary>2021-05-14 17:11:04 - BNNpriors: A library for Bayesian neural network inference with different prior distributions</summary>

- *Vincent Fortuin, Adrià Garriga-Alonso, Mark van der Wilk, Laurence Aitchison*

- `2105.06964v1` - [abs](http://arxiv.org/abs/2105.06964v1) - [pdf](http://arxiv.org/pdf/2105.06964v1)

> Bayesian neural networks have shown great promise in many applications where calibrated uncertainty estimates are crucial and can often also lead to a higher predictive performance. However, it remains challenging to choose a good prior distribution over their weights. While isotropic Gaussian priors are often chosen in practice due to their simplicity, they do not reflect our true prior beliefs well and can lead to suboptimal performance. Our new library, BNNpriors, enables state-of-the-art Markov Chain Monte Carlo inference on Bayesian neural networks with a wide range of predefined priors, including heavy-tailed ones, hierarchical ones, and mixture priors. Moreover, it follows a modular approach that eases the design and implementation of new custom priors. It has facilitated foundational discoveries on the nature of the cold posterior effect in Bayesian neural networks and will hopefully catalyze future research as well as practical applications in this area.

</details>

<details>

<summary>2021-05-14 18:41:45 - Bayesian inference under model misspecification using transport-Lagrangian distances: an application to seismic inversion</summary>

- *Andrea Scarinci, Michael Fehler, Youssef Marzouk*

- `2105.07027v1` - [abs](http://arxiv.org/abs/2105.07027v1) - [pdf](http://arxiv.org/pdf/2105.07027v1)

> Model misspecification constitutes a major obstacle to reliable inference in many inverse problems. Inverse problems in seismology, for example, are particularly affected by misspecification of wave propagation velocities. In this paper, we focus on a specific seismic inverse problem - full-waveform moment tensor inversion - and develop a Bayesian framework that seeks robustness to velocity misspecification. A novel element of our framework is the use of transport-Lagrangian (TL) distances between observed and model predicted waveforms to specify a loss function, and the use of this loss to define a generalized belief update via a Gibbs posterior. The TL distance naturally disregards certain features of the data that are more sensitive to model misspecification, and therefore produces less biased or dispersed posterior distributions in this setting. To make the latter notion precise, we use several diagnostics to assess the quality of inference and uncertainty quantification, i.e., continuous rank probability scores and rank histograms. We interpret these diagnostics in the Bayesian setting and compare the results to those obtained using more typical Gaussian noise models and squared-error loss, under various scenarios of misspecification. Finally, we discuss potential generalizability of the proposed framework to a broader class of inverse problems affected by model misspecification.

</details>

<details>

<summary>2021-05-14 23:40:49 - Hypothetical Beliefs Identify Information</summary>

- *Jonathan Libgober*

- `2105.07097v1` - [abs](http://arxiv.org/abs/2105.07097v1) - [pdf](http://arxiv.org/pdf/2105.07097v1)

> After observing the outcome of a Blackwell experiment, a Bayesian decisionmaker can form (a) posterior beliefs over the state, as well as (b) posterior beliefs she would observe any given signal (assuming an independent draw from the same experiment). I call the latter her contingent hypothetical beliefs. I show geometrically how contingent hypothetical beliefs relate to information structures. Specifically, the information structure can (generically) be derived by regressing contingent hypothetical beliefs on posterior beliefs over the state. Her prior is the unit eigenvector of a matrix determined from her posterior beliefs over the state and her contingent hypothetical beliefs. Thus, all aspects of a decisionmaker's information acquisition problem can be determined using ex-post data (i.e., beliefs after having received signals). I compare my results to similar ones obtained in cases where information is modeled deterministically; the focus on single-agent stochastic information distinguishes my work.

</details>

<details>

<summary>2021-05-15 02:43:53 - Shrinkage-based random local clocks with scalable inference</summary>

- *Alexander A. Fisher, Xiang Ji, Akihiko Nishimura, Philippe Lemey, Marc A. Suchard*

- `2105.07119v1` - [abs](http://arxiv.org/abs/2105.07119v1) - [pdf](http://arxiv.org/pdf/2105.07119v1)

> Local clock models propose that the rate of molecular evolution is constant within phylogenetic sub-trees. Current local clock inference procedures scale poorly to large taxa problems, impose model misspecification, or require a priori knowledge of the existence and location of clocks. To overcome these challenges, we present an autocorrelated, Bayesian model of heritable clock rate evolution that leverages heavy-tailed priors with mean zero to shrink increments of change between branch-specific clocks. We further develop an efficient Hamiltonian Monte Carlo sampler that exploits closed form gradient computations to scale our model to large trees. Inference under our shrinkage-clock exhibits an over 3-fold speed increase compared to the popular random local clock when estimating branch-specific clock rates on a simulated dataset. We further show our shrinkage-clock recovers known local clocks within a rodent and mammalian phylogeny. Finally, in a problem that once appeared computationally impractical, we investigate the heritable clock structure of various surface glycoproteins of influenza A virus in the absence of prior knowledge about clock placement.

</details>

<details>

<summary>2021-05-15 07:54:30 - Statistical Models for the Analysis of Optimization Algorithms with Benchmark Functions</summary>

- *David Issa Mattos, Jan Bosch, Helena Holmström Olsson*

- `2010.03783v4` - [abs](http://arxiv.org/abs/2010.03783v4) - [pdf](http://arxiv.org/pdf/2010.03783v4)

> Frequentist statistical methods, such as hypothesis testing, are standard practice in papers that provide benchmark comparisons. Unfortunately, these methods have often been misused, e.g., without testing for their statistical test assumptions or without controlling for family-wise errors in multiple group comparisons, among several other problems. Bayesian Data Analysis (BDA) addresses many of the previously mentioned shortcomings but its use is not widely spread in the analysis of empirical data in the evolutionary computing community. This paper provides three main contributions. First, we motivate the need for utilizing Bayesian data analysis and provide an overview of this topic. Second, we discuss the practical aspects of BDA to ensure that our models are valid and the results transparent. Finally, we provide five statistical models that can be used to answer multiple research questions. The online appendix provides a step-by-step guide on how to perform the analysis of the models discussed in this paper, including the code for the statistical models, the data transformations and the discussed tables and figures.

</details>

<details>

<summary>2021-05-16 11:12:22 - Ordinal Bayesian incentive compatibility in random assignment model</summary>

- *Sulagna Dasgupta, Debasis Mishra*

- `2009.13104v2` - [abs](http://arxiv.org/abs/2009.13104v2) - [pdf](http://arxiv.org/pdf/2009.13104v2)

> We explore the consequences of weakening the notion of incentive compatibility from strategy-proofness to ordinal Bayesian incentive compatibility (OBIC) in the random assignment model. If the common prior of the agents is a uniform prior, then a large class of random mechanisms are OBIC with respect to this prior -- this includes the probabilistic serial mechanism. We then introduce a robust version of OBIC: a mechanism is locally robust OBIC if it is OBIC with respect all independent priors in some neighborhood of a given independent prior. We show that every locally robust OBIC mechanism satisfying a mild property called elementary monotonicity is strategy-proof. This leads to a strengthening of the impossibility result in Bogomolnaia and Moulin (2001): if there are at least four agents, there is no locally robust OBIC and ordinally efficient mechanism satisfying equal treatment of equals.

</details>

<details>

<summary>2021-05-16 12:05:10 - Bayesian reconstruction of memories stored in neural networks from their connectivity</summary>

- *Sebastian Goldt, Florent Krzakala, Lenka Zdeborová, Nicolas Brunel*

- `2105.07416v1` - [abs](http://arxiv.org/abs/2105.07416v1) - [pdf](http://arxiv.org/pdf/2105.07416v1)

> The advent of comprehensive synaptic wiring diagrams of large neural circuits has created the field of connectomics and given rise to a number of open research questions. One such question is whether it is possible to reconstruct the information stored in a recurrent network of neurons, given its synaptic connectivity matrix. Here, we address this question by determining when solving such an inference problem is theoretically possible in specific attractor network models and by providing a practical algorithm to do so. The algorithm builds on ideas from statistical physics to perform approximate Bayesian inference and is amenable to exact analysis. We study its performance on three different models and explore the limitations of reconstructing stored patterns from synaptic connectivity.

</details>

<details>

<summary>2021-05-17 07:23:31 - Temporal Gaussian Process Regression in Logarithmic Time</summary>

- *Adrien Corenflos, Zheng Zhao, Simo Särkkä*

- `2102.09964v4` - [abs](http://arxiv.org/abs/2102.09964v4) - [pdf](http://arxiv.org/pdf/2102.09964v4)

> The aim of this article is to present a novel parallelization method for temporal Gaussian process (GP) regression problems. The method allows for solving GP regression problems in logarithmic O(log N) time, where N is the number of time steps. Our approach uses the state-space representation of GPs which in its original form allows for linear O(N) time GP regression by leveraging the Kalman filtering and smoothing methods. By using a recently proposed parallelization method for Bayesian filters and smoothers, we are able to reduce the linear computational complexity of the temporal GP regression problems into logarithmic span complexity. This ensures logarithmic time complexity when run on parallel hardware such as a graphics processing unit (GPU). We experimentally demonstrate the computational benefits on simulated and real datasets via our open-source implementation leveraging the GPflow framework.

</details>

<details>

<summary>2021-05-17 10:01:00 - Laplacian-P-splines for Bayesian inference in the mixture cure model</summary>

- *Oswaldo Gressani, Christel Faes, Niel Hens*

- `2103.01526v2` - [abs](http://arxiv.org/abs/2103.01526v2) - [pdf](http://arxiv.org/pdf/2103.01526v2)

> The mixture cure model for analyzing survival data is characterized by the assumption that the population under study is divided into a group of subjects who will experience the event of interest over some finite time horizon and another group of cured subjects who will never experience the event irrespective of the duration of follow-up. When using the Bayesian paradigm for inference in survival models with a cure fraction, it is common practice to rely on Markov chain Monte Carlo (MCMC) methods to sample from posterior distributions. Although computationally feasible, the iterative nature of MCMC often implies long sampling times to explore the target space with chains that may suffer from slow convergence and poor mixing. An alternative strategy for fast and flexible sampling-free Bayesian inference in the mixture cure model is suggested in this paper by combining Laplace approximations and penalized B-splines. A logistic regression model is assumed for the cure proportion and a Cox proportional hazards model with a P-spline approximated baseline hazard is used to specify the conditional survival function of susceptible subjects. Laplace approximations to the conditional latent vector are based on analytical formulas for the gradient and Hessian of the log-likelihood, resulting in a substantial speed-up in approximating posterior distributions. The statistical performance and computational efficiency of the proposed Laplacian-P-splines mixture cure (LPSMC) model is assessed in a simulation study. Results show that LPSMC is an appealing alternative to classic MCMC for approximate Bayesian inference in standard mixture cure models. Finally, the novel LPSMC approach is illustrated on three applications involving real survival data.

</details>

<details>

<summary>2021-05-17 17:27:03 - GPU-Accelerated Hierarchical Bayesian Inference with Application to Modeling Cosmic Populations: CUDAHM</summary>

- *János M. Szalai-Gindl, Thomas J. Loredo, Brandon C. Kelly, István Csabai, Tamás Budavári, László Dobos*

- `2105.08026v1` - [abs](http://arxiv.org/abs/2105.08026v1) - [pdf](http://arxiv.org/pdf/2105.08026v1)

> We describe a computational framework for hierarchical Bayesian inference with simple (typically single-plate) parametric graphical models that uses graphics processing units (GPUs) to accelerate computations, enabling deployment on very large datasets. Its C++ implementation, CUDAHM (CUDA for Hierarchical Models) exploits conditional independence between instances of a plate, facilitating massively parallel exploration of the replication parameter space using the single instruction, multiple data architecture of GPUs. It provides support for constructing Metropolis-within-Gibbs samplers that iterate between GPU-accelerated robust adaptive Metropolis sampling of plate-level parameters conditional on upper-level parameters, and Metropolis-Hastings sampling of upper-level parameters on the host processor conditional on the GPU results. CUDAHM is motivated by demographic problems in astronomy, where density estimation and linear and nonlinear regression problems must be addressed for populations of thousands to millions of objects whose features are measured with possibly complex uncertainties. We describe a thinned latent point process framework for modeling such demographic data. We demonstrate accurate GPU-accelerated parametric conditional density deconvolution for simulated populations of up to 300,000 objects in ~1 hour using a single NVIDIA Tesla K40c GPU. Supplementary material provides details about the CUDAHM API and the demonstration problem.

</details>

<details>

<summary>2021-05-17 21:29:07 - Bayesian Aggregation</summary>

- *Yuling Yao*

- `1912.11218v2` - [abs](http://arxiv.org/abs/1912.11218v2) - [pdf](http://arxiv.org/pdf/1912.11218v2)

> A general challenge in statistics is prediction in the presence of multiple candidate models or learning algorithms. Model aggregation tries to combine all predictive distributions from individual models, which is more stable and flexible than single model selection. In this article we describe when and how to aggregate models under the lens of Bayesian decision theory. Among two widely used methods, Bayesian model averaging (BMA) and Bayesian stacking, we compare their predictive performance, and review their theoretical optimality, probabilistic interpretation, practical implementation, and extensions in complex models.

</details>

<details>

<summary>2021-05-17 22:21:20 - Bayesian Inverse Uncertainty Quantification of a MOOSE-based Melt Pool Model for Additive Manufacturing Using Experimental Data</summary>

- *Ziyu Xie, Wen Jiang, Congjian Wang, Xu Wu*

- `2105.05370v2` - [abs](http://arxiv.org/abs/2105.05370v2) - [pdf](http://arxiv.org/pdf/2105.05370v2)

> Additive manufacturing (AM) technology is being increasingly adopted in a wide variety of application areas due to its ability to rapidly produce, prototype, and customize designs. AM techniques afford significant opportunities in regard to nuclear materials, including an accelerated fabrication process and reduced cost. High-fidelity modeling and simulation (M\&S) of AM processes is being developed in Idaho National Laboratory (INL)'s Multiphysics Object-Oriented Simulation Environment (MOOSE) to support AM process optimization and provide a fundamental understanding of the various physical interactions involved. In this paper, we employ Bayesian inverse uncertainty quantification (UQ) to quantify the input uncertainties in a MOOSE-based melt pool model for AM. Inverse UQ is the process of inversely quantifying the input uncertainties while keeping model predictions consistent with the measurement data. The inverse UQ process takes into account uncertainties from the model, code, and data while simultaneously characterizing the uncertain distributions in the input parameters--rather than merely providing best-fit point estimates. We employ measurement data on melt pool geometry (lengths and depths) to quantify the uncertainties in several melt pool model parameters. Simulation results using the posterior uncertainties have shown improved agreement with experimental data, as compared to those using the prior nominal values. The resulting parameter uncertainties can be used to replace expert opinions in future uncertainty, sensitivity, and validation studies.

</details>

<details>

<summary>2021-05-18 11:45:07 - Bayesian Levy-Dynamic Spatio-Temporal Process: Towards Big Data Analysis</summary>

- *Sourabh Bhattacharya*

- `2105.08451v1` - [abs](http://arxiv.org/abs/2105.08451v1) - [pdf](http://arxiv.org/pdf/2105.08451v1)

> In this era of big data, all scientific disciplines are evolving fast to cope up with the enormity of the available information. So is statistics, the queen of science. Big data are particularly relevant to spatio-temporal statistics, thanks to much-improved technology in satellite based remote sensing and Geographical Information Systems. However, none of the existing approaches seem to meet the simultaneous demand of reality emulation and cheap computation. In this article, with the Levy random fields as the starting point, e construct a new Bayesian nonparametric, nonstationary and nonseparable dynamic spatio- temporal model with the additional realistic property that the lagged spatio-temporal correlations converge to zero as the lag tends to infinity. Although our Bayesian model seems to be intricately structured and is variable-dimensional with respect to each time index, we are able to devise a fast and efficient parallel Markov Chain Monte Carlo (MCMC) algorithm for Bayesian inference. Our simulation experiment brings out quite encouraging performance from our Bayesian Levy-dynamic approach. We finally apply our Bayesian Levy-dynamic model and methods to a sea surface temperature dataset consisting of 139,300 data points in space and time. Although not big data in the true sense, this is a large and highly structured data by any standard. Even for this large and complex data, our parallel MCMC algorithm, implemented on 80 processors, generated 110,000 MCMC realizations from the Levy-dynamic posterior within a single day, and the resultant Bayesian posterior predictive analysis turned out to be encouraging. Thus, it is not unreasonable to expect that with significantly more computing resources, it is feasible to analyse terabytes of spatio-temporal data with our new model and methods.

</details>

<details>

<summary>2021-05-18 13:15:42 - Quantifying the Effects of Contact Tracing, Testing, and Containment Measures in the Presence of Infection Hotspots</summary>

- *Lars Lorch, Heiner Kremer, William Trouleau, Stratis Tsirtsis, Aron Szanto, Bernhard Schölkopf, Manuel Gomez-Rodriguez*

- `2004.07641v5` - [abs](http://arxiv.org/abs/2004.07641v5) - [pdf](http://arxiv.org/pdf/2004.07641v5)

> Multiple lines of evidence strongly suggest that infection hotspots, where a single individual infects many others, play a key role in the transmission dynamics of COVID-19. However, most of the existing epidemiological models fail to capture this aspect by neither representing the sites visited by individuals explicitly nor characterizing disease transmission as a function of individual mobility patterns. In this work, we introduce a temporal point process modeling framework that specifically represents visits to the sites where individuals get in contact and infect each other. Under our model, the number of infections caused by an infectious individual naturally emerges to be overdispersed. Using an efficient sampling algorithm, we demonstrate how to apply Bayesian optimization with longitudinal case data to estimate the transmission rate of infectious individuals at the sites they visit and in their households. Simulations using fine-grained and publicly available demographic data and site locations from Bern, Switzerland showcase the flexibility of our framework. To facilitate research and analyses of other cities and regions, we release an open-source implementation of our framework.

</details>

<details>

<summary>2021-05-18 15:38:20 - eBASCS: Disentangling Overlapping Astronomical Sources II, using Spatial, Spectral, and Temporal Information</summary>

- *Antoine D. Meyer, David A. van Dyk, Vinay L. Kashyap, Luis F. Campos, David E. Jones, Aneta Siemiginowska, Andreas Zezas*

- `2105.08606v1` - [abs](http://arxiv.org/abs/2105.08606v1) - [pdf](http://arxiv.org/pdf/2105.08606v1)

> The analysis of individual X-ray sources that appear in a crowded field can easily be compromised by the misallocation of recorded events to their originating sources. Even with a small number of sources, that nonetheless have overlapping point spread functions, the allocation of events to sources is a complex task that is subject to uncertainty. We develop a Bayesian method designed to sift high-energy photon events from multiple sources with overlapping point spread functions, leveraging the differences in their spatial, spectral, and temporal signatures. The method probabilistically assigns each event to a given source. Such a disentanglement allows more detailed spectral or temporal analysis to focus on the individual component in isolation, free of contamination from other sources or the background. We are also able to compute source parameters of interest like their locations, relative brightness, and background contamination, while accounting for the uncertainty in event assignments. Simulation studies that include event arrival time information demonstrate that the temporal component improves event disambiguation beyond using only spatial and spectral information. The proposed methods correctly allocate up to 65% more events than the corresponding algorithms that ignore event arrival time information. We apply our methods to two stellar X-ray binaries, UV Cet and HBC515 A, observed with Chandra. We demonstrate that our methods are capable of removing the contamination due to a strong flare on UV Cet B in its companion approximately 40 times weaker during that event, and that evidence for spectral variability at timescales of a few ks can be determined in HBC515 Aa and HBC515 Ab.

</details>

<details>

<summary>2021-05-18 16:34:09 - The Neural Moving Average Model for Scalable Variational Inference of State Space Models</summary>

- *Tom Ryder, Dennis Prangle, Andrew Golightly, Isaac Matthews*

- `1910.00879v2` - [abs](http://arxiv.org/abs/1910.00879v2) - [pdf](http://arxiv.org/pdf/1910.00879v2)

> Variational inference has had great success in scaling approximate Bayesian inference to big data by exploiting mini-batch training. To date, however, this strategy has been most applicable to models of independent data. We propose an extension to state space models of time series data based on a novel generative model for latent temporal states: the neural moving average model. This permits a subsequence to be sampled without drawing from the entire distribution, enabling training iterations to use mini-batches of the time series at low computational cost. We illustrate our method on autoregressive, Lotka-Volterra, FitzHugh-Nagumo and stochastic volatility models, achieving accurate parameter estimation in a short time.

</details>

<details>

<summary>2021-05-18 17:16:23 - Flexible Bayesian Modeling of Counts: Constructing Penalized Complexity Priors</summary>

- *Mahsa Nadifar, Hossein Baghishani, Thomas Kneib, Afshin Fallah*

- `2105.08686v1` - [abs](http://arxiv.org/abs/2105.08686v1) - [pdf](http://arxiv.org/pdf/2105.08686v1)

> Many of the data, particularly in medicine and disease mapping are count. Indeed, the under or overdispersion problem in count data distrusts the performance of the classical Poisson model. For taking into account this problem, in this paper, we introduce a new Bayesian structured additive regression model, called gamma count, with enough flexibility in modeling dispersion. Setting convenient prior distributions on the model parameters is a momentous issue in Bayesian statistics that characterize the nature of our uncertainty parameters. Relying on a recently proposed class of penalized complexity priors, motivated from a general set of construction principles, we derive the prior structure. The model can be formulated as a latent Gaussian model, and consequently, we can carry out the fast computation by using the integrated nested Laplace approximation method. We investigate the proposed methodology simulation study. Different expropriate prior distribution are examined to provide reasonable sensitivity analysis. To explain the applicability of the proposed model, we analyzed two real-world data sets related to the larynx mortality cancer in Germany and the handball champions league.

</details>

<details>

<summary>2021-05-18 17:56:58 - A flexible Bayesian framework to estimate age- and cause-specific child mortality over time from sample registration data</summary>

- *Austin E Schumacher, Tyler H McCormick, Jon Wakefield, Yue Chu, Jamie Perin, Francisco Villavicencio, Noah Simon, Li Liu*

- `2003.00401v3` - [abs](http://arxiv.org/abs/2003.00401v3) - [pdf](http://arxiv.org/pdf/2003.00401v3)

> In order to implement disease-specific interventions in young age groups, policy makers in low- and middle-income countries require timely and accurate estimates of age- and cause-specific child mortality. High quality data is not available in settings where these interventions are most needed, but there is a push to create sample registration systems that collect detailed mortality information. Current methods that estimate mortality from this data employ multistage frameworks without rigorous statistical justification that separately estimate all-cause and cause-specific mortality and are not sufficiently adaptable to capture important features of the data. We propose a flexible Bayesian modeling framework to estimate age- and cause-specific child mortality from sample registration data. We provide a theoretical justification for the framework, explore its properties via simulation, and use it to estimate mortality trends using data from the Maternal and Child Health Surveillance System in China.

</details>

<details>

<summary>2021-05-18 18:49:06 - Measuring performance for end-of-life care</summary>

- *Sebastien Haneuse, Deborah Schrag, Francesca Dominici, Sharon-Lise Normand, Kyu Ha Lee*

- `2105.08776v1` - [abs](http://arxiv.org/abs/2105.08776v1) - [pdf](http://arxiv.org/pdf/2105.08776v1)

> Although not without controversy, readmission is entrenched as a hospital quality metric, with statistical analyses generally based on fitting a logistic-Normal generalized linear mixed model. Such analyses, however, ignore death as a competing risk, although doing so for clinical conditions with high mortality can have profound effects; a hospitals seemingly good performance for readmission may be an artifact of it having poor performance for mortality. In this paper we propose novel multivariate hospital-level performance measures for readmission and mortality, that derive from framing the analysis as one of cluster-correlated semi-competing risks data. We also consider a number of profiling-related goals, including the identification of extreme performers and a bivariate classification of whether the hospital has higher-/lower-than-expected readmission and mortality rates, via a Bayesian decision-theoretic approach that characterizes hospitals on the basis of minimizing the posterior expected loss for an appropriate loss function. In some settings, particularly if the number of hospitals is large, the computational burden may be prohibitive. To resolve this, we propose a series of analysis strategies that will be useful in practice. Throughout the methods are illustrated with data from CMS on N=17,685 patients diagnosed with pancreatic cancer between 2000-2012 at one of J=264 hospitals in California.

</details>

<details>

<summary>2021-05-18 18:54:54 - Quantifying sources of uncertainty in drug discovery predictions with probabilistic models</summary>

- *Stanley E. Lazic, Dominic P. Williams*

- `2105.09474v1` - [abs](http://arxiv.org/abs/2105.09474v1) - [pdf](http://arxiv.org/pdf/2105.09474v1)

> Knowing the uncertainty in a prediction is critical when making expensive investment decisions and when patient safety is paramount, but machine learning (ML) models in drug discovery typically provide only a single best estimate and ignore all sources of uncertainty. Predictions from these models may therefore be over-confident, which can put patients at risk and waste resources when compounds that are destined to fail are further developed. Probabilistic predictive models (PPMs) can incorporate uncertainty in both the data and model, and return a distribution of predicted values that represents the uncertainty in the prediction. PPMs not only let users know when predictions are uncertain, but the intuitive output from these models makes communicating risk easier and decision making better. Many popular machine learning methods have a PPM or Bayesian analogue, making PPMs easy to fit into current workflows. We use toxicity prediction as a running example, but the same principles apply for all prediction models used in drug discovery. The consequences of ignoring uncertainty and how PPMs account for uncertainty are also described. We aim to make the discussion accessible to a broad non-mathematical audience. Equations are provided to make ideas concrete for mathematical readers (but can be skipped without loss of understanding) and code is available for computational researchers (https://github.com/stanlazic/ML_uncertainty_quantification).

</details>

<details>

<summary>2021-05-19 06:49:00 - Bayesian Model Selection for High-Dimensional Ising Models, With Applications to Educational Data</summary>

- *Jaewoo Park, Ick Hoon Jin, Michael Schweinberger*

- `1911.07142v2` - [abs](http://arxiv.org/abs/1911.07142v2) - [pdf](http://arxiv.org/pdf/1911.07142v2)

> Doubly-intractable posterior distributions arise in many applications of statistics concerned with discrete and dependent data, including physics, spatial statistics, machine learning, the social sciences, and other fields. A specific example is psychometrics, which has adapted high-dimensional Ising models from machine learning, with a view to studying the interactions among binary item responses in educational assessments. To estimate high-dimensional Ising models from educational assessment data, $\ell_1$-penalized nodewise logistic regressions have been used. Theoretical results in high-dimensional statistics show that $\ell_1$-penalized nodewise logistic regressions can recover the true interaction structure with high probability, provided that certain assumptions are satisfied. Those assumptions are hard to verify in practice and may be violated, and quantifying the uncertainty about the estimated interaction structure and parameter estimators is challenging. We propose a Bayesian approach that helps quantify the uncertainty about the interaction structure and parameters without requiring strong assumptions, and can be applied to Ising models with thousands of parameters. We demonstrate the advantages of the proposed Bayesian approach compared with $\ell_1$-penalized nodewise logistic regressions by simulation studies and applications to small and large educational data sets with up to 2,485 parameters. Among other things, the simulation studies suggest that the Bayesian approach is more robust against model misspecification due to omitted covariates than $\ell_1$-penalized nodewise logistic regressions.

</details>

<details>

<summary>2021-05-19 08:34:55 - A Simple Model of Monetary Policy under Phillips-Curve Causal Disagreements</summary>

- *Ran Spiegler*

- `2105.08988v1` - [abs](http://arxiv.org/abs/2105.08988v1) - [pdf](http://arxiv.org/pdf/2105.08988v1)

> I study a static textbook model of monetary policy and relax the conventional assumption that the private sector has rational expectations. Instead, the private sector forms inflation forecasts according to a misspecified subjective model that disagrees with the central bank's (true) model over the causal underpinnings of the Phillips Curve. Following the AI/Statistics literature on Bayesian Networks, I represent the private sector's model by a direct acyclic graph (DAG). I show that when the private sector's model reverses the direction of causality between inflation and output, the central bank's optimal policy can exhibit an attenuation effect that is sensitive to the noisiness of the true inflation-output equations.

</details>

<details>

<summary>2021-05-19 09:17:16 - Multi-fidelity Neural Architecture Search with Knowledge Distillation</summary>

- *Ilya Trofimov, Nikita Klyuchnikov, Mikhail Salnikov, Alexander Filippov, Evgeny Burnaev*

- `2006.08341v2` - [abs](http://arxiv.org/abs/2006.08341v2) - [pdf](http://arxiv.org/pdf/2006.08341v2)

> Neural architecture search (NAS) targets at finding the optimal architecture of a neural network for a problem or a family of problems. Evaluations of neural architectures are very time-consuming. One of the possible ways to mitigate this issue is to use low-fidelity evaluations, namely training on a part of a dataset, fewer epochs, with fewer channels, etc. In this paper, we propose a bayesian multi-fidelity method for neural architecture search: MF-KD. The method relies on a new approach to low-fidelity evaluations of neural architectures by training for a few epochs using a knowledge distillation. Knowledge distillation adds to a loss function a term forcing a network to mimic some teacher network. We carry out experiments on CIFAR-10, CIFAR-100, and ImageNet-16-120. We show that training for a few epochs with such a modified loss function leads to a better selection of neural architectures than training for a few epochs with a logistic loss. The proposed method outperforms several state-of-the-art baselines.

</details>

<details>

<summary>2021-05-19 15:54:08 - Estimating heterogeneous survival treatment effect in observational data using machine learning</summary>

- *Liangyuan Hu, Jiayi Ji, Fan Li*

- `2008.07044v4` - [abs](http://arxiv.org/abs/2008.07044v4) - [pdf](http://arxiv.org/pdf/2008.07044v4)

> Methods for estimating heterogeneous treatment effect in observational data have largely focused on continuous or binary outcomes, and have been relatively less vetted with survival outcomes. Using flexible machine learning methods in the counterfactual framework is a promising approach to address challenges due to complex individual characteristics, to which treatments need to be tailored. To evaluate the operating characteristics of recent survival machine learning methods for the estimation of treatment effect heterogeneity and inform better practice, we carry out a comprehensive simulation study presenting a wide range of settings describing confounded heterogeneous survival treatment effects and varying degrees of covariate overlap. Our results suggest that the nonparametric Bayesian Additive Regression Trees within the framework of accelerated failure time model (AFT-BART-NP) consistently yields the best performance, in terms of bias, precision and expected regret. Moreover, the credible interval estimators from AFT-BART-NP provide close to nominal frequentist coverage for the individual survival treatment effect when the covariate overlap is at least moderate. Including a non-parametrically estimated propensity score as an additional fixed covariate in the AFT-BART-NP model formulation can further improve its efficiency and frequentist coverage. Finally, we demonstrate the application of flexible causal machine learning estimators through a comprehensive case study examining the heterogeneous survival effects of two radiotherapy approaches for localized high-risk prostate cancer.

</details>

<details>

<summary>2021-05-19 16:07:02 - Improving Adaptive Seamless Designs through Bayesian optimization</summary>

- *Jakob Richter, Tim Friede, Jörg Rahnenführer*

- `2105.09223v1` - [abs](http://arxiv.org/abs/2105.09223v1) - [pdf](http://arxiv.org/pdf/2105.09223v1)

> We propose to use Bayesian optimization (BO) to improve the efficiency of the design selection process in clinical trials. BO is a method to optimize expensive black-box functions, by using a regression as a surrogate to guide the search. In clinical trials, planning test procedures and sample sizes is a crucial task. A common goal is to maximize the test power, given a set of treatments, corresponding effect sizes, and a total number of samples. From a wide range of possible designs we aim to select the best one in a short time to allow quick decisions. The standard approach to simulate the power for each single design can become too time-consuming. When the number of possible designs becomes very large, either large computational resources are required or an exhaustive exploration of all possible designs takes too long. Here, we propose to use BO to quickly find a clinical trial design with high power from a large number of candidate designs. We demonstrate the effectiveness of our approach by optimizing the power of adaptive seamless designs for different sets of treatment effect sizes. Comparing BO with an exhaustive evaluation of all candidate designs shows that BO finds competitive designs in a fraction of the time.

</details>

<details>

<summary>2021-05-20 09:20:35 - Nonlinear Hawkes Process with Gaussian Process Self Effects</summary>

- *Noa Malem-Shinitski, Cesar Ojeda, Manfred Opper*

- `2105.09618v1` - [abs](http://arxiv.org/abs/2105.09618v1) - [pdf](http://arxiv.org/pdf/2105.09618v1)

> Traditionally, Hawkes processes are used to model time--continuous point processes with history dependence. Here we propose an extended model where the self--effects are of both excitatory and inhibitory type and follow a Gaussian Process. Whereas previous work either relies on a less flexible parameterization of the model, or requires a large amount of data, our formulation allows for both a flexible model and learning when data are scarce. We continue the line of work of Bayesian inference for Hawkes processes, and our approach dispenses with the necessity of estimating a branching structure for the posterior, as we perform inference on an aggregated sum of Gaussian Processes. Efficient approximate Bayesian inference is achieved via data augmentation, and we describe a mean--field variational inference approach to learn the model parameters. To demonstrate the flexibility of the model we apply our methodology on data from three different domains and compare it to previously reported results.

</details>

<details>

<summary>2021-05-20 11:37:38 - Improved Neuronal Ensemble Inference with Generative Model and MCMC</summary>

- *Shun Kimura, Keisuke Ota, Koujin Takeda*

- `2105.09679v1` - [abs](http://arxiv.org/abs/2105.09679v1) - [pdf](http://arxiv.org/pdf/2105.09679v1)

> Neuronal ensemble inference is a significant problem in the study of biological neural networks. Various methods have been proposed for ensemble inference from experimental data of neuronal activity. Among them, Bayesian inference approach with generative model was proposed recently. However, this method requires large computational cost for appropriate inference. In this work, we give an improved Bayesian inference algorithm by modifying update rule in Markov chain Monte Carlo method and introducing the idea of simulated annealing for hyperparameter control. We compare the performance of ensemble inference between our algorithm and the original one, and discuss the advantage of our method.

</details>

<details>

<summary>2021-05-20 15:17:27 - Lookahead Acquisition Functions for Finite-Horizon Time-Dependent Bayesian Optimization and Application to Quantum Optimal Control</summary>

- *S. Ashwin Renganathan, Jeffrey Larson, Stefan M. Wild*

- `2105.09824v1` - [abs](http://arxiv.org/abs/2105.09824v1) - [pdf](http://arxiv.org/pdf/2105.09824v1)

> We propose a novel Bayesian method to solve the maximization of a time-dependent expensive-to-evaluate stochastic oracle. We are interested in the decision that maximizes the oracle at a finite time horizon, given a limited budget of noisy evaluations of the oracle that can be performed before the horizon. Our recursive two-step lookahead acquisition function for Bayesian optimization makes nonmyopic decisions at every stage by maximizing the expected utility at the specified time horizon. Specifically, we propose a generalized two-step lookahead framework with a customizable \emph{value} function that allows users to define the utility. We illustrate how lookahead versions of classic acquisition functions such as the expected improvement, probability of improvement, and upper confidence bound can be obtained with this framework. We demonstrate the utility of our proposed approach on several carefully constructed synthetic cases and a real-world quantum optimal control problem.

</details>

<details>

<summary>2021-05-20 16:55:28 - A flexible Bayesian non-confounding spatial model for analysis of dispersed count data in clinical studies</summary>

- *Mahsa Nadifar, Hossein Baghishani, Afshin Fallah*

- `2105.09893v1` - [abs](http://arxiv.org/abs/2105.09893v1) - [pdf](http://arxiv.org/pdf/2105.09893v1)

> In employing spatial regression models for counts, we usually meet two issues. First, ignoring the inherent collinearity between covariates and the spatial effect would lead to causal inferences. Second, real count data usually reveal over or under-dispersion where the classical Poisson model is not appropriate to use. We propose a flexible Bayesian hierarchical modeling approach by joining non-confounding spatial methodology and a newly reconsidered dispersed count modeling from the renewal theory to control the issues. Specifically, we extend the methodology for analyzing spatial count data based on the gamma distribution assumption for waiting times. The model can be formulated as a latent Gaussian model, and consequently, we can carry out the fast computation using the integrated nested Laplace approximation method. We also examine different popular approaches for handling spatial confounding and compare their performances in the presence of dispersion. We use the proposed methodology to analyze a clinical dataset related to stomach cancer incidence in Slovenia and perform a simulation study to understand the proposed approach's merits better.

</details>

<details>

<summary>2021-05-20 18:25:43 - Data-driven discovery of interpretable causal relations for deep learning material laws with uncertainty propagation</summary>

- *Xiao Sun, Bahador Bahmani, Nikolaos N. Vlassis, WaiChing Sun, Yanxun Xu*

- `2105.09980v1` - [abs](http://arxiv.org/abs/2105.09980v1) - [pdf](http://arxiv.org/pdf/2105.09980v1)

> This paper presents a computational framework that generates ensemble predictive mechanics models with uncertainty quantification (UQ). We first develop a causal discovery algorithm to infer causal relations among time-history data measured during each representative volume element (RVE) simulation through a directed acyclic graph (DAG). With multiple plausible sets of causal relationships estimated from multiple RVE simulations, the predictions are propagated in the derived causal graph while using a deep neural network equipped with dropout layers as a Bayesian approximation for uncertainty quantification. We select two representative numerical examples (traction-separation laws for frictional interfaces, elastoplasticity models for granular assembles) to examine the accuracy and robustness of the proposed causal discovery method for the common material law predictions in civil engineering applications.

</details>

<details>

<summary>2021-05-20 22:14:30 - Bayesian hierarchical stacking: Some models are (somewhere) useful</summary>

- *Yuling Yao, Gregor Pirš, Aki Vehtari, Andrew Gelman*

- `2101.08954v2` - [abs](http://arxiv.org/abs/2101.08954v2) - [pdf](http://arxiv.org/pdf/2101.08954v2)

> Stacking is a widely used model averaging technique that asymptotically yields optimal predictions among linear averages. We show that stacking is most effective when model predictive performance is heterogeneous in inputs, and we can further improve the stacked mixture with a hierarchical model. We generalize stacking to Bayesian hierarchical stacking. The model weights are varying as a function of data, partially-pooled, and inferred using Bayesian inference. We further incorporate discrete and continuous inputs, other structured priors, and time series and longitudinal data. To verify the performance gain of the proposed method, we derive theory bounds, and demonstrate on several applied problems.

</details>

<details>

<summary>2021-05-21 12:37:17 - Power Calculations for Replication Studies</summary>

- *Charlotte Micheloud, Leonhard Held*

- `2004.10814v4` - [abs](http://arxiv.org/abs/2004.10814v4) - [pdf](http://arxiv.org/pdf/2004.10814v4)

> The reproducibility crisis has led to an increasing number of replication studies being conducted. Sample sizes for replication studies are often calculated using conditional power based on the effect estimate from the original study. However, this approach is not well suited as it ignores the uncertainty of the original result. Bayesian methods are used in clinical trials to incorporate prior information into power calculations. We propose to adapt this methodology to the replication framework and promote the use of predictive instead of conditional power in the design of replication studies. Moreover, we describe how extensions of the methodology to sequential clinical trials can be tailored to replication studies. Conditional and predictive power calculated at an interim analysis are compared and we argue that predictive power is a useful tool to decide whether to stop a replication study prematurely. A recent project on the replicability of social sciences is used to illustrate the properties of the different methods.

</details>

<details>

<summary>2021-05-21 18:42:53 - Bayesian/Graphoid intersection property for factorisation spaces</summary>

- *Grégoire Sergeant-Perthuis*

- `1903.06026v2` - [abs](http://arxiv.org/abs/1903.06026v2) - [pdf](http://arxiv.org/pdf/1903.06026v2)

> We remark that Pearl's Graphoid intersection property, also called intersection property in Bayesian networks, is a particular case of a general intersection property, in the sense of intersection of coverings, for factorisation spaces, also coined as factorisation models, factor graphs or by Lauritzen in his reference book 'Graphical Models' as hierarchical model subspaces. A particular case of this intersection property appears in Lauritzen's book as a consequence of the decomposition into interaction subspaces; the novel proof that we give of this result allows us to extend it in the most general setting. It also allows us to give a direct and new proof of the Hammersley-Clifford theorem transposing and reducing it to a corresponding statement for graphs, justifying formally the geometric intuition of independency, and extending it to non finite graphs. This intersection property is the starting point for a generalization of the decomposition into interaction subspaces to collections of vector spaces.

</details>

<details>

<summary>2021-05-21 19:22:17 - Understanding Uncertainty in Bayesian Deep Learning</summary>

- *Cooper Lorsung*

- `2106.13055v1` - [abs](http://arxiv.org/abs/2106.13055v1) - [pdf](http://arxiv.org/pdf/2106.13055v1)

> Neural Linear Models (NLM) are deep Bayesian models that produce predictive uncertainty by learning features from the data and then performing Bayesian linear regression over these features. Despite their popularity, few works have focused on formally evaluating the predictive uncertainties of these models. Furthermore, existing works point out the difficulties of encoding domain knowledge in models like NLMs, making them unsuitable for applications where interpretability is required. In this work, we show that traditional training procedures for NLMs can drastically underestimate uncertainty in data-scarce regions. We identify the underlying reasons for this behavior and propose a novel training method that can both capture useful predictive uncertainties as well as allow for incorporation of domain knowledge.

</details>

<details>

<summary>2021-05-22 11:08:34 - A partial graphical model with a structural prior on the direct links between predictors and responses</summary>

- *Eunice Okome Obiang, Pascal Jézéquel, Frédéric Proïa*

- `2003.11869v2` - [abs](http://arxiv.org/abs/2003.11869v2) - [pdf](http://arxiv.org/pdf/2003.11869v2)

> This paper is devoted to the estimation of a partial graphical model with a structural Bayesian penalization. Precisely, we are interested in the linear regression setting where the estimation is made through the direct links between potentially high-dimensional predictors and multiple responses, since it is known that Gaussian graphical models enable to exhibit direct links only, whereas coefficients in linear regressions contain both direct and indirect relations (due \textit{e.g.} to strong correlations among the variables). A smooth penalty reflecting a generalized Gaussian Bayesian prior on the covariates is added, either enforcing patterns (like row structures) in the direct links or regulating the joint influence of predictors. We give a theoretical guarantee for our method, taking the form of an upper bound on the estimation error arising with high probability, provided that the model is suitably regularized. Empirical studies on synthetic data and a real dataset are conducted.

</details>

<details>

<summary>2021-05-22 15:48:54 - Intersection property and interaction decomposition</summary>

- *Grégoire Sergeant-Perthuis*

- `1904.09017v2` - [abs](http://arxiv.org/abs/1904.09017v2) - [pdf](http://arxiv.org/pdf/1904.09017v2)

> The decomposition into interaction subspaces is a hierarchical decomposition of the spaces of cylindrical functions of a finite product space, also called factor spaces. It is an important construction in graphical models and a standard way to prove the Hammersley-Clifford theorem that relates Markov fields to Gibbs fields and plays a central role in Kellerer's result for the linearized marginal problem. We define an intersection of sum property, or simply intersection property, and show that it characterizes collections of vector subspaces over a poset that can be hierarchically decomposed into direct sums, giving therefore a general setting for such construction to hold. We will call this generalization the interaction decomposition. The intersection property is the Bayesian intersection property when specified to factor spaces which, under this new perspective on the interaction decomposition, appears to be a structure property. An application is the extension of the decomposition into interaction subspaces for any product of any set.

</details>

<details>

<summary>2021-05-23 09:02:46 - Bayesian Effect Selection for Additive Quantile Regression with an Analysis to Air Pollution Thresholds</summary>

- *Nadja Klein, Jorge Mateu*

- `2105.10890v1` - [abs](http://arxiv.org/abs/2105.10890v1) - [pdf](http://arxiv.org/pdf/2105.10890v1)

> Statistical techniques used in air pollution modelling usually lack the possibility to understand which predictors affect air pollution in which functional form; and are not able to regress on exceedances over certain thresholds imposed by authorities directly. The latter naturally induce conditional quantiles and reflect the seriousness of particular events. In the present paper we focus on this important aspect by developing quantile regression models further. We propose a general Bayesian effect selection approach for additive quantile regression within a highly interpretable framework. We place separate normal beta prime spike and slab priors on the scalar importance parameters of effect parts and implement a fast Gibbs sampling scheme. Specifically, it enables to study quantile-specific covariate effects, allows these covariates to be of general functional form using additive predictors, and facilitates the analysts' decision whether an effect should be included linearly, non-linearly or not at all in the quantiles of interest. In a detailed analysis on air pollution data in Madrid (Spain) we find the added value of modelling extreme nitrogen dioxide (NO2) concentrations and how thresholds are driven differently by several climatological variables and traffic as a spatial proxy. Our results underpin the need of enhanced statistical models to support short-term decisions and enable local authorities to mitigate or even prevent exceedances of NO2 concentration limits.

</details>

<details>

<summary>2021-05-23 21:18:35 - Robust Bayesian Nonparametric Variable Selection for Linear Regression</summary>

- *Alberto Cabezas, Marco Battiston, Christopher Nemeth*

- `2105.11022v1` - [abs](http://arxiv.org/abs/2105.11022v1) - [pdf](http://arxiv.org/pdf/2105.11022v1)

> Spike-and-slab and horseshoe regression are arguably the most popular Bayesian variable selection approaches for linear regression models. However, their performance can deteriorate if outliers and heteroskedasticity are present in the data, which are common features in many real-world statistics and machine learning applications. In this work, we propose a Bayesian nonparametric approach to linear regression that performs variable selection while accounting for outliers and heteroskedasticity. Our proposed model is an instance of a Dirichlet process scale mixture model with the advantage that we can derive the full conditional distributions of all parameters in closed form, hence producing an efficient Gibbs sampler for posterior inference. Moreover, we present how to extend the model to account for heavy-tailed response variables. The performance of the model is tested against competing algorithms on synthetic and real-world datasets.

</details>

<details>

<summary>2021-05-24 04:08:33 - Random concave functions</summary>

- *Peter Baxendale, Ting-Kam Leonard Wong*

- `1910.13668v3` - [abs](http://arxiv.org/abs/1910.13668v3) - [pdf](http://arxiv.org/pdf/1910.13668v3)

> Spaces of convex and concave functions appear naturally in theory and applications. For example, convex regression and log-concave density estimation are important topics in nonparametric statistics. In stochastic portfolio theory, concave functions on the unit simplex measure the concentration of capital, and their gradient maps define novel investment strategies. The gradient maps may also be regarded as optimal transport maps on the simplex. In this paper we construct and study probability measures supported on spaces of concave functions. These measures may serve as prior distributions in Bayesian statistics and Cover's universal portfolio, and induce distribution-valued random variables via optimal transport. The random concave functions are constructed on the unit simplex by taking a suitably scaled (mollified, or soft) minimum of random hyperplanes. Depending on the regime of the parameters, we show that as the number of hyperplanes tends to infinity there are several possible limiting behaviors. In particular, there is a transition from a deterministic almost sure limit to a non-trivial limiting distribution that can be characterized using convex duality and Poisson point processes.

</details>

<details>

<summary>2021-05-24 08:52:07 - On Lower Bounds for Standard and Robust Gaussian Process Bandit Optimization</summary>

- *Xu Cai, Jonathan Scarlett*

- `2008.08757v2` - [abs](http://arxiv.org/abs/2008.08757v2) - [pdf](http://arxiv.org/pdf/2008.08757v2)

> In this paper, we consider algorithm-independent lower bounds for the problem of black-box optimization of functions having a bounded norm is some Reproducing Kernel Hilbert Space (RKHS), which can be viewed as a non-Bayesian Gaussian process bandit problem. In the standard noisy setting, we provide a novel proof technique for deriving lower bounds on the regret, with benefits including simplicity, versatility, and an improved dependence on the error probability. In a robust setting in which every sampled point may be perturbed by a suitably-constrained adversary, we provide a novel lower bound for deterministic strategies, demonstrating an inevitable joint dependence of the cumulative regret on the corruption level and the time horizon, in contrast with existing lower bounds that only characterize the individual dependencies. Furthermore, in a distinct robust setting in which the final point is perturbed by an adversary, we strengthen an existing lower bound that only holds for target success probabilities very close to one, by allowing for arbitrary success probabilities above $\frac{2}{3}$.

</details>

<details>

<summary>2021-05-24 09:53:10 - Approximating the Operating Characteristics of Bayesian Uncertainty Directed Trial Designs</summary>

- *Marta Bonsaglio, Sandra Fortini, Steffen Ventz, Lorenzo Trippa*

- `2105.11177v1` - [abs](http://arxiv.org/abs/2105.11177v1) - [pdf](http://arxiv.org/pdf/2105.11177v1)

> Bayesian response adaptive clinical trials are currently evaluating experimental therapies for several diseases. Adaptive decisions, such as pre-planned variations of the randomization probabilities, attempt to accelerate the development of new treatments. The design of response adaptive trials, in most cases, requires time consuming simulation studies to describe operating characteristics, such as type I/II error rates, across plausible scenarios. We investigate large sample approximations of pivotal operating characteristics in Bayesian Uncertainty directed trial Designs (BUDs). A BUD trial utilizes an explicit metric u to quantify the information accrued during the study on parameters of interest, for example the treatment effects. The randomization probabilities vary during time to minimize the uncertainty summary u at completion of the study. We provide an asymptotic analysis (i) of the allocation of patients to treatment arms and (ii) of the randomization probabilities. For BUDs with outcome distributions belonging to the natural exponential family with quadratic variance function, we illustrate the asymptotic normality of the number of patients assigned to each arm and of the randomization probabilities. We use these results to approximate relevant operating characteristics such as the power of the BUD. We evaluate the accuracy of the approximations through simulations under several scenarios for binary, time-to-event and continuous outcome models.

</details>

<details>

<summary>2021-05-24 10:15:04 - Vector autoregression models with skewness and heavy tails</summary>

- *Sune Karlsson, Stepan Mazur, Hoang Nguyen*

- `2105.11182v1` - [abs](http://arxiv.org/abs/2105.11182v1) - [pdf](http://arxiv.org/pdf/2105.11182v1)

> With uncertain changes of the economic environment, macroeconomic downturns during recessions and crises can hardly be explained by a Gaussian structural shock. There is evidence that the distribution of macroeconomic variables is skewed and heavy tailed. In this paper, we contribute to the literature by extending a vector autoregression (VAR) model to account for a more realistic assumption of the multivariate distribution of the macroeconomic variables. We propose a general class of generalized hyperbolic skew Student's t distribution with stochastic volatility for the error term in the VAR model that allows us to take into account skewness and heavy tails. Tools for Bayesian inference and model selection using a Gibbs sampler are provided. In an empirical study, we present evidence of skewness and heavy tails for monthly macroeconomic variables. The analysis also gives a clear message that skewness should be taken into account for better predictions during recessions and crises.

</details>

<details>

<summary>2021-05-24 17:54:28 - Uncertainty Quantification for Bayesian CART</summary>

- *Ismael Castillo, Veronika Rockova*

- `1910.07635v2` - [abs](http://arxiv.org/abs/1910.07635v2) - [pdf](http://arxiv.org/pdf/1910.07635v2)

> This work affords new insights into Bayesian CART in the context of structured wavelet shrinkage. The main thrust is to develop a formal inferential framework for Bayesian tree-based regression. We reframe Bayesian CART as a g-type prior which departs from the typical wavelet product priors by harnessing correlation induced by the tree topology. The practically used Bayesian CART priors are shown to attain adaptive near rate-minimax posterior concentration in the supremum norm in regression models. For the fundamental goal of uncertainty quantification, we construct adaptive confidence bands for the regression function with uniform coverage under self-similarity. In addition, we show that tree-posteriors enable optimal inference in the form of efficient confidence sets for smooth functionals of the regression function.

</details>

<details>

<summary>2021-05-25 09:33:34 - Inferring Hierarchical Mixture Structures: A Bayesian Nonparametric Approach</summary>

- *Weipeng Huang, Nishma Laitonjam, Guangyuan Piao, Neil Hurley*

- `1905.05022v6` - [abs](http://arxiv.org/abs/1905.05022v6) - [pdf](http://arxiv.org/pdf/1905.05022v6)

> This paper focuses on the problem of hierarchical non-overlapping clustering of a dataset. In such a clustering, each data item is associated with exactly one leaf node and each internal node is associated with all the data items stored in the sub-tree beneath it, so that each level of the hierarchy corresponds to a partition of the dataset. We develop a novel Bayesian nonparametric method combining the nested Chinese Restaurant Process (nCRP) and the Hierarchical Dirichlet Process (HDP). Compared with other existing Bayesian approaches, our solution tackles data with complex latent mixture features which has not been previously explored in the literature. We discuss the details of the model and the inference procedure. Furthermore, experiments on three datasets show that our method achieves solid empirical results in comparison with existing algorithms.

</details>

<details>

<summary>2021-05-25 10:09:45 - HINT: Hierarchical Invertible Neural Transport for Density Estimation and Bayesian Inference</summary>

- *Jakob Kruse, Gianluca Detommaso, Ullrich Köthe, Robert Scheichl*

- `1905.10687v4` - [abs](http://arxiv.org/abs/1905.10687v4) - [pdf](http://arxiv.org/pdf/1905.10687v4)

> Many recent invertible neural architectures are based on coupling block designs where variables are divided in two subsets which serve as inputs of an easily invertible (usually affine) triangular transformation. While such a transformation is invertible, its Jacobian is very sparse and thus may lack expressiveness. This work presents a simple remedy by noting that subdivision and (affine) coupling can be repeated recursively within the resulting subsets, leading to an efficiently invertible block with dense, triangular Jacobian. By formulating our recursive coupling scheme via a hierarchical architecture, HINT allows sampling from a joint distribution p(y,x) and the corresponding posterior p(x|y) using a single invertible network. We evaluate our method on some standard data sets and benchmark its full power for density estimation and Bayesian inference on a novel data set of 2D shapes in Fourier parameterization, which enables consistent visualization of samples for different dimensionalities.

</details>

<details>

<summary>2021-05-25 10:20:06 - Efficient Bayesian model selection for coupled hidden Markov models with application to infectious diseases</summary>

- *Jake Carson, Trevelyan J. McKinley, Peter Neal, Simon E. F. Spencer*

- `2105.11807v1` - [abs](http://arxiv.org/abs/2105.11807v1) - [pdf](http://arxiv.org/pdf/2105.11807v1)

> Performing model selection for coupled hidden Markov models (CHMMs) is highly challenging, owing to the large dimension of the hidden state process. Whilst in principle the hidden state process can be marginalized out via forward filtering, in practice the computational cost of doing so increases exponentially with the number of coupled Markov chains, making this approach infeasible in most applications. Monte Carlo methods can be utilized, but despite many remarkable developments in model selection methodology, generic approaches continue to be ill-suited for such high-dimensional problems. Here we develop specialized solutions for CHMMs with weak inter-chain dependencies. Specifically we construct effective proposal distributions for the hidden state process that remain computationally viable as the number of chains increases, and that require little user input or tuning. This methodology is particularly applicable to individual-level infectious disease models characterized as CHMMs, in which each chain represents an individual, and the coupling represents contact between individuals. Since the only significant contacts are between susceptible and infectious individuals, and since multiple infection pathways are often possible, the resulting CHMMs naturally have low inter-chain dependencies. We demonstrate the utility of our methodology with an application to a study of highly pathogenic avian influenza in chickens.

</details>

<details>

<summary>2021-05-25 17:19:09 - Trajectory Modeling via Random Utility Inverse Reinforcement Learning</summary>

- *Anselmo R. Pitombeira-Neto, Helano P. Santos, Ticiana L. Coelho da Silva, José Antonio F. de Macedo*

- `2105.12092v1` - [abs](http://arxiv.org/abs/2105.12092v1) - [pdf](http://arxiv.org/pdf/2105.12092v1)

> We consider the problem of modeling trajectories of drivers in a road network from the perspective of inverse reinforcement learning. As rational agents, drivers are trying to maximize some reward function unknown to an external observer as they make up their trajectories. We apply the concept of random utility from microeconomic theory to model the unknown reward function as a function of observable features plus an error term which represents features known only to the driver. We develop a parameterized generative model for the trajectories based on a random utility Markov decision process formulation of drivers decisions. We show that maximum entropy inverse reinforcement learning is a particular case of our proposed formulation when we assume a Gumbel density function for the unobserved reward error terms. We illustrate Bayesian inference on model parameters through a case study with real trajectory data from a large city obtained from sensors placed on sparsely distributed points on the street network.

</details>

<details>

<summary>2021-05-26 10:11:57 - An Equivalence between Bayesian Priors and Penalties in Variational Inference</summary>

- *Pierre Wolinski, Guillaume Charpiat, Yann Ollivier*

- `2002.00178v2` - [abs](http://arxiv.org/abs/2002.00178v2) - [pdf](http://arxiv.org/pdf/2002.00178v2)

> In machine learning, it is common to optimize the parameters of a probabilistic model, modulated by an ad hoc regularization term that penalizes some values of the parameters. Regularization terms appear naturally in Variational Inference (VI), a tractable way to approximate Bayesian posteriors: the loss to optimize contains a Kullback--Leibler divergence term between the approximate posterior and a Bayesian prior. We fully characterize which regularizers can arise this way, and provide a systematic way to compute the corresponding prior. This viewpoint also provides a prediction for useful values of the regularization factor in neural networks. We apply this framework to regularizers such as L2, L1 or group-Lasso.

</details>

<details>

<summary>2021-05-26 13:02:25 - Exact Monte Carlo likelihood-based inference for jump-diffusion processes</summary>

- *Flávio B. Gonçalves, Krzysztof G. Łatuszyński, Gareth O. Roberts*

- `1707.00332v4` - [abs](http://arxiv.org/abs/1707.00332v4) - [pdf](http://arxiv.org/pdf/1707.00332v4)

> Statistical inference for discretely observed jump-diffusion processes is a complex problem which motivates new methodological challenges. Thus existing approaches invariably resort to time-discretisations which inevitably lead to approximations in inference. In this paper, we give the first general collection of methodologies for exact (in this context meaning discretisation-free) likelihood-based inference for discretely observed finite activity jump-diffusions. The only sources of error involved are Monte Carlo error and convergence of EM or MCMC algorithms. We shall introduce both frequentist and Bayesian approaches, illustrating the methodology through simulated and real examples.

</details>

<details>

<summary>2021-05-26 15:13:59 - An Empirical Bayes Method for Chi-Squared Data</summary>

- *Lilun Du, Inchi Hu*

- `1903.00776v2` - [abs](http://arxiv.org/abs/1903.00776v2) - [pdf](http://arxiv.org/pdf/1903.00776v2)

> In a thought-provoking paper, Efron (2011) investigated the merit and limitation of an empirical Bayes method to correct selection bias based on Tweedie's formula first reported by \cite{Robbins:1956}. The exceptional virtue of Tweedie's formula for the normal distribution lies in its representation of selection bias as a simple function of the derivative of log marginal likelihood. Since the marginal likelihood and its derivative can be estimated from the data directly without invoking prior information, bias correction can be carried out conveniently. We propose a Bayesian hierarchical model for chi-squared data such that the resulting Tweedie's formula has the same virtue as that of the normal distribution. Because the family of noncentral chi-squared distributions, the common alternative distributions for chi-squared tests, does not constitute an exponential family, our results cannot be obtained by extending existing results. Furthermore, the corresponding Tweedie's formula manifests new phenomena quite different from those of the normal distribution and suggests new ways of analyzing chi-squared data.

</details>

<details>

<summary>2021-05-26 15:55:47 - Bayes Factor Asymptotics for Variable Selection in the Gaussian Process Framework</summary>

- *Minerva Mukhopadhyay, Sourabh Bhattacharya*

- `1810.09909v3` - [abs](http://arxiv.org/abs/1810.09909v3) - [pdf](http://arxiv.org/pdf/1810.09909v3)

> Although variable selection is one of the most popular areas of modern statistical research, much of its development has taken place in the classical paradigm compared to the Bayesian counterpart. Somewhat surprisingly, both the paradigms have focussed almost completely on linear models, in spite of the vast scope offered by the model liberation movement brought about by modern advancements in studying real, complex phenomena.   In this article, we investigate general Bayesian variable selection in models driven by Gaussian processes, which allows us to treat linear, non-linear and nonparametric models, in conjunction with even dependent setups, in the same vein. We consider the Bayes factor route to variable selection, and develop a general asymptotic theory for the Gaussian process framework in the "large p, large n" settings even with p>>n, establishing almost sure exponential convergence of the Bayes factor under appropriately mild conditions. The fixed p setup is included as a special case.   To illustrate, we apply our general result to variable selection in linear regression, Gaussian process model with squared exponential covariance function accommodating the covariates, and a first order autoregressive process with time-varying covariates. We also follow up our theoretical investigations with ample simulation experiments in the above regression contexts and variable selection in a real, riboflavin data consisting of 71 observations but 4088 covariates. For implementation of variable selection using Bayes factors, we develop a novel and effective general-purpose transdimensional, transformation based Markov chain Monte Carlo algorithm, which has played a crucial role in our simulated and real data applications.

</details>

<details>

<summary>2021-05-26 18:59:42 - Ideal Bayesian Spatial Adaptation</summary>

- *Veronika Rockova, Judith Rousseau*

- `2105.12793v1` - [abs](http://arxiv.org/abs/2105.12793v1) - [pdf](http://arxiv.org/pdf/2105.12793v1)

> Many real-life applications involve estimation of curves that exhibit complicated shapes including jumps or varying-frequency oscillations. Practical methods have been devised that can adapt to a locally varying complexity of an unknown function (e.g. variable-knot splines, sparse wavelet reconstructions, kernel methods or trees/forests). However, the overwhelming majority of existing asymptotic minimaxity theory is predicated on homogeneous smoothness assumptions. Focusing on locally Holderian functions, we provide new locally adaptive posterior concentration rate results under the supremum loss for widely used Bayesian machine learning techniques in white noise and non-parametric regression. In particular, we show that popular spike-and-slab priors and Bayesian CART are uniformly locally adaptive. In addition, we propose a new class of repulsive partitioning priors which relate to variable knot splines and which are exact-rate adaptive. For uncertainty quantification, we construct locally adaptive confidence bands whose width depends on the local smoothness and which achieve uniform asymptotic coverage under local self-similarity. To illustrate that spatial adaptation is not at all automatic, we provide lower-bound results showing that popular hierarchical Gaussian process priors fall short of spatial adaptation.

</details>

<details>

<summary>2021-05-26 19:13:09 - Bayesian Origin-Destination Estimation in Networked Transit Systems using Nodal In- and Outflow Counts</summary>

- *Steffen O. P. Blume, Francesco Corman, Giovanni Sansavini*

- `2105.12798v1` - [abs](http://arxiv.org/abs/2105.12798v1) - [pdf](http://arxiv.org/pdf/2105.12798v1)

> We propose a Bayesian inference approach for static Origin-Destination (OD)-estimation in large-scale networked transit systems. The approach finds posterior distribution estimates of the OD-coefficients, which describe the relative proportions of passengers travelling between origin and destination locations, via a Hamiltonian Monte Carlo sampling procedure. We suggest two different inference model formulations: the instantaneous-balance and average-delay model. We discuss both models' sensitivity to various count observation properties, and establish that the average-delay model is generally more robust in determining the coefficient posteriors. The instantaneous-balance model, however, requires lower resolution count observations and produces comparably accurate estimates as the average-delay model, pending that count observations are only moderately interfered by trend fluctuations or the truncation of the observation window, and sufficient number of dispersed data records are available. We demonstrate that the Bayesian posterior distribution estimates provide quantifiable measures of the estimation uncertainty and prediction quality of the model, whereas the point estimates obtained from an alternative constrained quadratic programming optimisation approach only provide the residual errors between the predictions and observations. Moreover, the Bayesian approach proves more robust in scaling to high-dimensional underdetermined problems. The Bayesian instantaneous-balance OD-coefficient posteriors are determined for the New York City (NYC) subway network, based on several years of entry and exit count observations recorded at station turnstiles across the network. The average-delay model proves intractable on the real-world test scenario, given its computational time complexity and the incompleteness as well as coarseness of the turnstile records.

</details>

<details>

<summary>2021-05-26 21:22:16 - Flexible Bayesian modelling of concomitant covariate effects in mixture models</summary>

- *Marco Berrettini, Giuliano Galimberti, Saverio Ranciati, Thomas Brendan Murphy*

- `2105.12852v1` - [abs](http://arxiv.org/abs/2105.12852v1) - [pdf](http://arxiv.org/pdf/2105.12852v1)

> Mixture models provide a useful tool to account for unobserved heterogeneity and are at the basis of many model-based clustering methods. In order to gain additional flexibility, some model parameters can be expressed as functions of concomitant covariates. In particular, component weights of the mixture can be linked to the covariates through a multinomial logistic regression model, where each component weight is a function of the linear predictor involving one or more covariates. The proposed contribution extends this approach by replacing the linear predictor, used for the component weights, with an additive structure, where each term is a smooth function of the covariates considered. An estimation procedure within the Bayesian paradigm is suggested. In particular, a data augmentation scheme based on differenced random utility models is exploited, and smoothness of the covariate effects is controlled by suitable choices for the prior distributions of the spline coefficients. The performance of the proposed methodology is investigated via simulation experiments, and an application to an original dataset about UK parliamentary votes on Brexit is discussed.

</details>

<details>

<summary>2021-05-26 22:58:27 - Combined parameter and state inference with automatically calibrated ABC</summary>

- *Anthony Ebert, Pierre Pudlo, Kerrie Mengersen, Paul Wu, Christopher Drovandi*

- `1910.14227v2` - [abs](http://arxiv.org/abs/1910.14227v2) - [pdf](http://arxiv.org/pdf/1910.14227v2)

> State space models contain time-indexed parameters, termed states, as well as static parameters, simply termed parameters. The problem of inferring both static parameters as well as states simultaneously, based on time-indexed observations, is the subject of much recent literature. This problem is compounded once we consider models with intractable likelihoods. In these situations, some emerging approaches have incorporated existing likelihood-free techniques for static parameters, such as approximate Bayesian computation (ABC) into likelihood-based algorithms for combined inference of parameters and states. These emerging approaches currently require extensive manual calibration of a time-indexed tuning parameter: the acceptance threshold.   We design an SMC$^2$ algorithm (Chopin et al., 2013, JRSS B) for likelihood-free approximation with automatically tuned thresholds. We prove consistency of the algorithm and discuss the proposed calibration. We demonstrate this algorithm's performance with three examples. We begin with two examples of state space models. The first example is a toy example, with an emission distribution that is a skew normal distribution. The second example is a stochastic volatility model involving an intractable stable distribution. The last example is the most challenging; it deals with an inhomogeneous Hawkes process.

</details>

<details>

<summary>2021-05-27 01:32:07 - Bayesian Inference for Population Attributable Measures from Under-identified Models</summary>

- *Sarah Pirikahu, Geoffrey Jones, Martin Hazelton*

- `2105.12901v1` - [abs](http://arxiv.org/abs/2105.12901v1) - [pdf](http://arxiv.org/pdf/2105.12901v1)

> Population attributable risk (PAR) is used in epidemiology to predict the impact of removing a risk factor from the population. Until recently, no standard approach for calculating confidence intervals or the variance for PAR was available in the literature. Pirikahu et al. (2016) outlined a fully Bayesian approach to provide credible intervals for the PAR from a cross-sectional study, where the data was presented in the form of a 2 x 2 table. However, extensions to cater for other frequently used study designs were not provided. In this paper we provide methodology to calculate credible intervals for the PAR for case-control and cohort studies. Additionally, we extend the cross-sectional example to allow for the incorporation of uncertainty that arises when an imperfect diagnostic test is used. In all these situations the model becomes over-parameterised, or non-identifiable, which can result in standard "off-the-shelf" Markov chain Monte Carlo updaters taking a long time to converge or even failing altogether. We adapt an importance sampling methodology to overcome this problem, and propose some novel MCMC samplers that take into consideration the shape of the posterior ridge to aid in the convergence of the Markov chain.

</details>

<details>

<summary>2021-05-27 08:51:07 - Errors-in-Variables for deep learning: rethinking aleatoric uncertainty</summary>

- *Jörg Martin, Clemens Elster*

- `2105.09095v2` - [abs](http://arxiv.org/abs/2105.09095v2) - [pdf](http://arxiv.org/pdf/2105.09095v2)

> We present a Bayesian treatment for deep regression using an Errors-in-Variables model which accounts for the uncertainty associated with the input to the employed neural network. It is shown how the treatment can be combined with already existing approaches for uncertainty quantification that are based on variational inference. Our approach yields a decomposition of the predictive uncertainty into an aleatoric and epistemic part that is more complete and, in many cases, more consistent from a statistical perspective. We illustrate and discuss the approach along various toy and real world examples.

</details>

<details>

<summary>2021-05-27 11:17:24 - Deep Distributional Time Series Models and the Probabilistic Forecasting of Intraday Electricity Prices</summary>

- *Nadja Klein, Michael Stanley Smith, David J. Nott*

- `2010.01844v2` - [abs](http://arxiv.org/abs/2010.01844v2) - [pdf](http://arxiv.org/pdf/2010.01844v2)

> Recurrent neural networks (RNNs) with rich feature vectors of past values can provide accurate point forecasts for series that exhibit complex serial dependence. We propose two approaches to constructing deep time series probabilistic models based on a variant of RNN called an echo state network (ESN). The first is where the output layer of the ESN has stochastic disturbances and a shrinkage prior for additional regularization. The second approach employs the implicit copula of an ESN with Gaussian disturbances, which is a deep copula process on the feature space. Combining this copula with a non-parametrically estimated marginal distribution produces a deep distributional time series model. The resulting probabilistic forecasts are deep functions of the feature vector and also marginally calibrated. In both approaches, Bayesian Markov chain Monte Carlo methods are used to estimate the models and compute forecasts. The proposed models are suitable for the complex task of forecasting intraday electricity prices. Using data from the Australian National Electricity Market, we show that our deep time series models provide accurate short term probabilistic price forecasts, with the copula model dominating. Moreover, the models provide a flexible framework for incorporating probabilistic forecasts of electricity demand as additional features, which increases upper tail forecast accuracy from the copula model significantly.

</details>

<details>

<summary>2021-05-27 15:43:09 - Bayesian Optimisation for Constrained Problems</summary>

- *Juan Ungredda, Juergen Branke*

- `2105.13245v1` - [abs](http://arxiv.org/abs/2105.13245v1) - [pdf](http://arxiv.org/pdf/2105.13245v1)

> Many real-world optimisation problems such as hyperparameter tuning in machine learning or simulation-based optimisation can be formulated as expensive-to-evaluate black-box functions. A popular approach to tackle such problems is Bayesian optimisation (BO), which builds a response surface model based on the data collected so far, and uses the mean and uncertainty predicted by the model to decide what information to collect next. In this paper, we propose a novel variant of the well-known Knowledge Gradient acquisition function that allows it to handle constraints. We empirically compare the new algorithm with four other state-of-the-art constrained Bayesian optimisation algorithms and demonstrate its superior performance. We also prove theoretical convergence in the infinite budget limit.

</details>

<details>

<summary>2021-05-27 19:46:24 - On the Impossibility of Statistically Improving Empirical Optimization: A Second-Order Stochastic Dominance Perspective</summary>

- *Henry Lam*

- `2105.13419v1` - [abs](http://arxiv.org/abs/2105.13419v1) - [pdf](http://arxiv.org/pdf/2105.13419v1)

> When the underlying probability distribution in a stochastic optimization is observed only through data, various data-driven formulations have been studied to obtain approximate optimal solutions. We show that no such formulations can, in a sense, theoretically improve the statistical quality of the solution obtained from empirical optimization. We argue this by proving that the first-order behavior of the optimality gap against the oracle best solution, which includes both the bias and variance, for any data-driven solution is second-order stochastically dominated by empirical optimization, as long as suitable smoothness holds with respect to the underlying distribution. We demonstrate this impossibility of improvement in a range of examples including regularized optimization, distributionally robust optimization, parametric optimization and Bayesian generalizations. We also discuss the connections of our results to semiparametric statistical inference and other perspectives in the data-driven optimization literature.

</details>

<details>

<summary>2021-05-27 19:48:23 - Model Selection for Production System via Automated Online Experiments</summary>

- *Zhenwen Dai, Praveen Chandar, Ghazal Fazelnia, Ben Carterette, Mounia Lalmas-Roelleke*

- `2105.13420v1` - [abs](http://arxiv.org/abs/2105.13420v1) - [pdf](http://arxiv.org/pdf/2105.13420v1)

> A challenge that machine learning practitioners in the industry face is the task of selecting the best model to deploy in production. As a model is often an intermediate component of a production system, online controlled experiments such as A/B tests yield the most reliable estimation of the effectiveness of the whole system, but can only compare two or a few models due to budget constraints. We propose an automated online experimentation mechanism that can efficiently perform model selection from a large pool of models with a small number of online experiments. We derive the probability distribution of the metric of interest that contains the model uncertainty from our Bayesian surrogate model trained using historical logs. Our method efficiently identifies the best model by sequentially selecting and deploying a list of models from the candidate set that balance exploration-exploitation. Using simulations based on real data, we demonstrate the effectiveness of our method on two different tasks.

</details>

<details>

<summary>2021-05-27 22:09:42 - Statistical Inference for Diagnostic Test Accuracy Studies with Multiple Comparisons</summary>

- *Max Westphal, Antonia Zapf*

- `2105.13469v1` - [abs](http://arxiv.org/abs/2105.13469v1) - [pdf](http://arxiv.org/pdf/2105.13469v1)

> Diagnostic accuracy studies assess sensitivity and specificity of a new index test in relation to an established comparator or the reference standard. The development and selection of the index test is usually assumed to be conducted prior to the accuracy study. In practice, this is often violated, for instance if the choice of the (apparently) best biomarker, model or cutpoint is based on the same data that is used later for validation purposes. In this work, we investigate several multiple comparison procedures which provide family-wise error rate control for the emerging multiple testing problem. Due to the nature of the co-primary hypothesis problem, conventional approaches for multiplicity adjustment are too conservative for the specific problem and thus need to be adapted. In an extensive simulation study, five multiple comparison procedures are compared with regards to statistical error rates in least-favorable and realistic scenarios. This covers parametric and nonparamtric methods and one Bayesian approach. All methods have been implemented in the new open-source R package DTAmc which allows to reproduce all simulation results. Based on our numerical results, we conclude that the parametric approaches (maxT, Bonferroni) are easy to apply but can have inflated type I error rates for small sample sizes. The two investigated Bootstrap procedures, in particular the so-called pairs Bootstrap, allow for a family-wise error rate control in finite samples and in addition have a competitive statistical power.

</details>

<details>

<summary>2021-05-27 22:35:02 - Non-parametric Bayesian Causal Modeling of the SARS-CoV-2 Viral Load Distribution vs. Patient's Age</summary>

- *Matteo Guardiani, Philipp Frank, Andrija Kostić, Gordian Edenhofer, Jakob Roth, Berit Uhlmann, Torsten Enßlin*

- `2105.13483v1` - [abs](http://arxiv.org/abs/2105.13483v1) - [pdf](http://arxiv.org/pdf/2105.13483v1)

> The viral load of patients infected with SARS-CoV-2 varies on logarithmic scales and possibly with age. Controversial claims have been made in the literature regarding whether the viral load distribution actually depends on the age of the patients. Such a dependence would have implications for the COVID-19 spreading mechanism, the age-dependent immune system reaction, and thus for policymaking. We hereby develop a method to analyze viral-load distribution data as a function of the patients' age within a flexible, non-parametric, hierarchical, Bayesian, and causal model. This method can be applied to other contexts as well, and for this purpose, it is made freely available. The developed reconstruction method also allows testing for bias in the data. This could be due to, e.g., bias in patient-testing and data collection or systematic errors in the measurement of the viral load. We perform these tests by calculating the Bayesian evidence for each implied possible causal direction. When applying these tests to publicly available age and SARS-CoV-2 viral load data, we find a statistically significant increase in the viral load with age, but only for one of the two analyzed datasets. If we consider this dataset, and based on the current understanding of viral load's impact on patients' infectivity, we expect a non-negligible difference in the infectivity of different age groups. This difference is nonetheless too small to justify considering any age group as noninfectious.

</details>

<details>

<summary>2021-05-28 03:20:57 - Pseudo-marginal Inference for CTMCs on Infinite Spaces via Monotonic Likelihood Approximations</summary>

- *Miguel Biron-Lattes, Alexandre Bouchard-Côté, Trevor Campbell*

- `2105.13566v1` - [abs](http://arxiv.org/abs/2105.13566v1) - [pdf](http://arxiv.org/pdf/2105.13566v1)

> Bayesian inference for Continuous-Time Markov Chains (CTMCs) on countably infinite spaces is notoriously difficult because evaluating the likelihood exactly is intractable. One way to address this challenge is to first build a non-negative and unbiased estimate of the likelihood -- involving the matrix exponential of finite truncations of the true rate matrix -- and then to use the estimates in a pseudo-marginal inference method. In this work, we show that we can dramatically increase the efficiency of this approach by avoiding the computation of exact matrix exponentials. In particular, we develop a general methodology for constructing an unbiased, non-negative estimate of the likelihood using doubly-monotone matrix exponential approximations. We further develop a novel approximation in this family -- the skeletoid -- as well as theory regarding its approximation error and how that relates to the variance of the estimates used in pseudo-marginal inference. Experimental results show that our approach yields more efficient posterior inference for a wide variety of CTMCs.

</details>

<details>

<summary>2021-05-28 04:46:49 - Empowering Differential Networks Using Bayesian Analysis</summary>

- *Jarod Smith, Mohammad Arashi, Andriette Bekker*

- `2105.13584v1` - [abs](http://arxiv.org/abs/2105.13584v1) - [pdf](http://arxiv.org/pdf/2105.13584v1)

> Differential networks (DN) are important tools for modeling the changes in conditional dependencies between multiple samples. A Bayesian approach for estimating DNs, from the classical viewpoint, is introduced with a computationally efficient threshold selection for graphical model determination. The algorithm separately estimates the precision matrices of the DN using the Bayesian adaptive graphical lasso procedure. Synthetic experiments illustrate that the Bayesian DN performs exceptionally well in numerical accuracy and graphical structure determination in comparison to state-of-the-art methods. The proposed method is applied to South African COVID-$19$ data to investigate the change in DN structure between various phases of the pandemic.

</details>

<details>

<summary>2021-05-28 06:26:22 - Can visualization alleviate dichotomous thinking? Effects of visual representations on the cliff effect</summary>

- *Jouni Helske, Satu Helske, Matthew Cooper, Anders Ynnerman, Lonni Besançon*

- `2002.07671v4` - [abs](http://arxiv.org/abs/2002.07671v4) - [pdf](http://arxiv.org/pdf/2002.07671v4)

> Common reporting styles for statistical results in scientific articles, such as p-values and confidence intervals (CI), have been reported to be prone to dichotomous interpretations, especially with respect to the null hypothesis significance testing framework. For example when the p-value is small enough or the CIs of the mean effects of a studied drug and a placebo are not overlapping, scientists tend to claim significant differences while often disregarding the magnitudes and absolute differences in the effect sizes. This type of reasoning has been shown to be potentially harmful to science. Techniques relying on the visual estimation of the strength of evidence have been recommended to reduce such dichotomous interpretations but their effectiveness has also been challenged. We ran two experiments on researchers with expertise in statistical analysis to compare several alternative representations of confidence intervals and used Bayesian multilevel models to estimate the effects of the representation styles on differences in researchers' subjective confidence in the results. We also asked the respondents' opinions and preferences in representation styles. Our results suggest that adding visual information to classic CI representation can decrease the tendency towards dichotomous interpretations - measured as the `cliff effect': the sudden drop in confidence around p-value 0.05 - compared with classic CI visualization and textual representation of the CI with p-values. All data and analyses are publicly available at https://github.com/helske/statvis.

</details>

<details>

<summary>2021-05-28 06:38:31 - bssm: Bayesian Inference of Non-linear and Non-Gaussian State Space Models in R</summary>

- *Jouni Helske, Matti Vihola*

- `2101.08492v2` - [abs](http://arxiv.org/abs/2101.08492v2) - [pdf](http://arxiv.org/pdf/2101.08492v2)

> We present an R package bssm for Bayesian non-linear/non-Gaussian state space modelling. Unlike the existing packages, bssm allows for easy-to-use approximate inference based on Gaussian approximations such as the Laplace approximation and the extended Kalman filter. The package accommodates also discretely observed latent diffusion processes. The inference is based on fully automatic, adaptive Markov chain Monte Carlo (MCMC) on the hyperparameters, with optional importance sampling post-correction to eliminate any approximation bias. The package implements also a direct pseudo-marginal MCMC and a delayed acceptance pseudo-marginal MCMC using intermediate approximations. The package offers an easy-to-use interface to define models with linear-Gaussian state dynamics with non-Gaussian observation models, and has an Rcpp interface for specifying custom non-linear and diffusion models.

</details>

<details>

<summary>2021-05-28 13:37:24 - Sequential design of multi-fidelity computer experiments: maximizing the rate of stepwise uncertainty reduction</summary>

- *Rémi Stroh, Julien Bect, Séverine Demeyer, Nicolas Fischer, Damien Marquis, Emmanuel Vazquez*

- `2007.13553v2` - [abs](http://arxiv.org/abs/2007.13553v2) - [pdf](http://arxiv.org/pdf/2007.13553v2)

> This article deals with the sequential design of experiments for (deterministic or stochastic) multi-fidelity numerical simulators, that is, simulators that offer control over the accuracy of simulation of the physical phenomenon or system under study. Very often, accurate simulations correspond to high computational efforts whereas coarse simulations can be obtained at a smaller cost. In this setting, simulation results obtained at several levels of fidelity can be combined in order to estimate quantities of interest (the optimal value of the output, the probability that the output exceeds a given threshold...) in an efficient manner. To do so, we propose a new Bayesian sequential strategy called Maximal Rate of Stepwise Uncertainty Reduction (MR-SUR), that selects additional simulations to be performed by maximizing the ratio between the expected reduction of uncertainty and the cost of simulation. This generic strategy unifies several existing methods, and provides a principled approach to develop new ones. We assess its performance on several examples, including a computationally intensive problem of fire safety analysis where the quantity of interest is the probability of exceeding a tenability threshold during a building fire.

</details>

<details>

<summary>2021-05-28 20:04:48 - Detecting Adversarial Examples with Bayesian Neural Network</summary>

- *Yao Li, Tongyi Tang, Cho-Jui Hsieh, Thomas C. M. Lee*

- `2105.08620v2` - [abs](http://arxiv.org/abs/2105.08620v2) - [pdf](http://arxiv.org/pdf/2105.08620v2)

> In this paper, we propose a new framework to detect adversarial examples motivated by the observations that random components can improve the smoothness of predictors and make it easier to simulate output distribution of deep neural network. With these observations, we propose a novel Bayesian adversarial example detector, short for BATer, to improve the performance of adversarial example detection. In specific, we study the distributional difference of hidden layer output between natural and adversarial examples, and propose to use the randomness of Bayesian neural network (BNN) to simulate hidden layer output distribution and leverage the distribution dispersion to detect adversarial examples. The advantage of BNN is that the output is stochastic while neural networks without random components do not have such characteristics. Empirical results on several benchmark datasets against popular attacks show that the proposed BATer outperforms the state-of-the-art detectors in adversarial example detection.

</details>

<details>

<summary>2021-05-28 22:47:31 - Gryffin: An algorithm for Bayesian optimization of categorical variables informed by expert knowledge</summary>

- *Florian Häse, Matteo Aldeghi, Riley J. Hickman, Loïc M. Roch, Alán Aspuru-Guzik*

- `2003.12127v2` - [abs](http://arxiv.org/abs/2003.12127v2) - [pdf](http://arxiv.org/pdf/2003.12127v2)

> Designing functional molecules and advanced materials requires complex design choices: tuning continuous process parameters such as temperatures or flow rates, while simultaneously selecting catalysts or solvents. To date, the development of data-driven experiment planning strategies for autonomous experimentation has largely focused on continuous process parameters despite the urge to devise efficient strategies for the selection of categorical variables. Here, we introduce Gryffin, a general purpose optimization framework for the autonomous selection of categorical variables driven by expert knowledge. Gryffin augments Bayesian optimization based on kernel density estimation with smooth approximations to categorical distributions. Leveraging domain knowledge in the form of physicochemical descriptors, Gryffin can significantly accelerate the search for promising molecules and materials. Gryffin can further highlight relevant correlations between the provided descriptors to inspire physical insights and foster scientific intuition. In addition to comprehensive benchmarks, we demonstrate the capabilities and performance of Gryffin on three examples in materials science and chemistry: (i) the discovery of non-fullerene acceptors for organic solar cells, (ii) the design of hybrid organic-inorganic perovskites for light harvesting, and (iii) the identification of ligands and process parameters for Suzuki-Miyaura reactions. Our results suggest that Gryffin, in its simplest form, is competitive with state-of-the-art categorical optimization algorithms. However, when leveraging domain knowledge provided via descriptors, Gryffin outperforms other approaches while simultaneously refining this domain knowledge to promote scientific understanding.

</details>

<details>

<summary>2021-05-29 09:50:18 - Estimating Spatial Econometrics Models with Integrated Nested Laplace Approximation</summary>

- *Virgilio Gomez-Rubio, Roger S. Bivand, Håvard Rue*

- `1703.01273v2` - [abs](http://arxiv.org/abs/1703.01273v2) - [pdf](http://arxiv.org/pdf/1703.01273v2)

> Integrated Nested Laplace Approximation provides a fast and effective method for marginal inference on Bayesian hierarchical models. This methodology has been implemented in the R-INLA package which permits INLA to be used from within R statistical software. Although INLA is implemented as a general methodology, its use in practice is limited to the models implemented in the R-INLA package.   Spatial autoregressive models are widely used in spatial econometrics but have until now been missing from the R-INLA package. In this paper, we describe the implementation and application of a new class of latent models in INLA made available through R-INLA. This new latent class implements a standard spatial lag model, which is widely used and that can be used to build more complex models in spatial econometrics.   The implementation of this latent model in R-INLA also means that all the other features of INLA can be used for model fitting, model selection and inference in spatial econometrics, as will be shown in this paper. Finally, we will illustrate the use of this new latent model and its applications with two datasets based on Gaussian and binary outcomes.

</details>

<details>

<summary>2021-05-29 10:26:23 - Information Directed Sampling for Sparse Linear Bandits</summary>

- *Botao Hao, Tor Lattimore, Wei Deng*

- `2105.14267v1` - [abs](http://arxiv.org/abs/2105.14267v1) - [pdf](http://arxiv.org/pdf/2105.14267v1)

> Stochastic sparse linear bandits offer a practical model for high-dimensional online decision-making problems and have a rich information-regret structure. In this work we explore the use of information-directed sampling (IDS), which naturally balances the information-regret trade-off. We develop a class of information-theoretic Bayesian regret bounds that nearly match existing lower bounds on a variety of problem instances, demonstrating the adaptivity of IDS. To efficiently implement sparse IDS, we propose an empirical Bayesian approach for sparse posterior sampling using a spike-and-slab Gaussian-Laplace prior. Numerical results demonstrate significant regret reductions by sparse IDS relative to several baselines.

</details>

<details>

<summary>2021-05-29 21:52:38 - Safe-Bayesian Generalized Linear Regression</summary>

- *Rianne de Heide, Alisa Kirichenko, Nishant Mehta, Peter Grünwald*

- `1910.09227v3` - [abs](http://arxiv.org/abs/1910.09227v3) - [pdf](http://arxiv.org/pdf/1910.09227v3)

> We study generalized Bayesian inference under misspecification, i.e. when the model is 'wrong but useful'. Generalized Bayes equips the likelihood with a learning rate $\eta$. We show that for generalized linear models (GLMs), $\eta$-generalized Bayes concentrates around the best approximation of the truth within the model for specific $\eta \neq 1$, even under severely misspecified noise, as long as the tails of the true distribution are exponential. We derive MCMC samplers for generalized Bayesian lasso and logistic regression and give examples of both simulated and real-world data in which generalized Bayes substantially outperforms standard Bayes.

</details>

<details>

<summary>2021-05-30 00:16:18 - Divide-and-Conquer Bayesian Inference in Hidden Markov Models</summary>

- *Chunlei Wang, Sanvesh Srivastava*

- `2105.14395v1` - [abs](http://arxiv.org/abs/2105.14395v1) - [pdf](http://arxiv.org/pdf/2105.14395v1)

> Divide-and-conquer Bayesian methods consist of three steps: dividing the data into smaller computationally manageable subsets, running a sampling algorithm in parallel on all the subsets, and combining parameter draws from all the subsets. The combined parameter draws are used for efficient posterior inference in massive data settings. A major restriction of existing divide-and-conquer methods is that their first two steps assume that the observations are independent. We address this problem by developing a divide-and-conquer method for Bayesian inference in parametric hidden Markov models, where the state space is known and finite. Our main contributions are two-fold. First, after partitioning the data into smaller blocks of consecutive observations, we modify the likelihood for performing posterior computations on the subsets such that the posterior variances of the subset and true posterior distributions have the same asymptotic order. Second, if the number of subsets is chosen appropriately depending on the mixing properties of the hidden Markov chain, then we show that the subset posterior distributions defined using the modified likelihood are asymptotically normal as the subset sample size tends to infinity. The latter result also implies that we can use any existing combination algorithm in the third step. We show that the combined posterior distribution obtained using one such algorithm is close to the true posterior distribution in 1-Wasserstein distance under widely used regularity assumptions. Our numerical results show that the proposed method provides an accurate approximation of the true posterior distribution than its competitors in diverse simulation studies and a real data analysis.

</details>

<details>

<summary>2021-05-30 09:06:32 - Autonomous Tracking and State Estimation with Generalised Group Lasso</summary>

- *Rui Gao, Simo Särkkä, Rubén Claveria-Vega, Simon Godsill*

- `2007.11573v3` - [abs](http://arxiv.org/abs/2007.11573v3) - [pdf](http://arxiv.org/pdf/2007.11573v3)

> We address the problem of autonomous tracking and state estimation for marine vessels, autonomous vehicles, and other dynamic signals under a (structured) sparsity assumption. The aim is to improve the tracking and estimation accuracy with respect to classical Bayesian filters and smoothers. We formulate the estimation problem as a dynamic generalised group Lasso problem and develop a class of smoothing-and-splitting methods to solve it. The Levenberg--Marquardt iterated extended Kalman smoother-based multi-block alternating direction method of multipliers (LM-IEKS-mADMM) algorithms are based on the alternating direction method of multipliers (ADMM) framework. This leads to minimisation subproblems with an inherent structure to which three new augmented recursive smoothers are applied. Our methods can deal with large-scale problems without pre-processing for dimensionality reduction. Moreover, the methods allow one to solve nonsmooth nonconvex optimisation problems. We then prove that under mild conditions, the proposed methods converge to a stationary point of the optimisation problem. By simulated and real-data experiments including multi-sensor range measurement problems, marine vessel tracking, autonomous vehicle tracking, and audio signal restoration, we show the practical effectiveness of the proposed methods.

</details>

<details>

<summary>2021-05-30 12:23:26 - Deep kernel processes</summary>

- *Laurence Aitchison, Adam X. Yang, Sebastian W. Ober*

- `2010.01590v2` - [abs](http://arxiv.org/abs/2010.01590v2) - [pdf](http://arxiv.org/pdf/2010.01590v2)

> We define deep kernel processes in which positive definite Gram matrices are progressively transformed by nonlinear kernel functions and by sampling from (inverse) Wishart distributions. Remarkably, we find that deep Gaussian processes (DGPs), Bayesian neural networks (BNNs), infinite BNNs, and infinite BNNs with bottlenecks can all be written as deep kernel processes. For DGPs the equivalence arises because the Gram matrix formed by the inner product of features is Wishart distributed, and as we show, standard isotropic kernels can be written entirely in terms of this Gram matrix -- we do not need knowledge of the underlying features. We define a tractable deep kernel process, the deep inverse Wishart process, and give a doubly-stochastic inducing-point variational inference scheme that operates on the Gram matrices, not on the features, as in DGPs. We show that the deep inverse Wishart process gives superior performance to DGPs and infinite BNNs on standard fully-connected baselines.

</details>

<details>

<summary>2021-05-30 14:49:10 - BABA: Beta Approximation for Bayesian Active Learning</summary>

- *Jae Oh Woo*

- `2105.14559v1` - [abs](http://arxiv.org/abs/2105.14559v1) - [pdf](http://arxiv.org/pdf/2105.14559v1)

> This paper introduces a new acquisition function under the Bayesian active learning framework, namely BABA. It is motivated by previously well-established works BALD, and BatchBALD which capture the mutual information between the model parameters and the predictive outputs of the data. Our proposed measure, BABA, endeavors to quantify the normalized mutual information by approximating the stochasticity of predictive probabilities using Beta distributions. BABA outperforms the well-known family of acquisition functions, including BALD and BatchBALD. We demonstrate this by showing extensive experimental results obtained from MNIST and EMNIST datasets.

</details>

<details>

<summary>2021-05-31 03:09:40 - Regularized Sparse Gaussian Processes</summary>

- *Rui Meng, Herbert Lee, Soper Braden, Priyadip Ray*

- `1910.05843v2` - [abs](http://arxiv.org/abs/1910.05843v2) - [pdf](http://arxiv.org/pdf/1910.05843v2)

> Gaussian processes are a flexible Bayesian nonparametric modelling approach that has been widely applied but poses computational challenges. To address the poor scaling of exact inference methods, approximation methods based on sparse Gaussian processes (SGP) are attractive. An issue faced by SGP, especially in latent variable models, is the inefficient learning of the inducing inputs, which leads to poor model prediction. We propose a regularization approach by balancing the reconstruction performance of data and the approximation performance of the model itself. This regularization improves both inference and prediction performance. We extend this regularization approach into latent variable models with SGPs and show that performing variational inference (VI) on those models is equivalent to performing VI on a related empirical Bayes model.

</details>

<details>

<summary>2021-05-31 07:26:05 - Lecture notes on ridge regression</summary>

- *Wessel N. van Wieringen*

- `1509.09169v7` - [abs](http://arxiv.org/abs/1509.09169v7) - [pdf](http://arxiv.org/pdf/1509.09169v7)

> The linear regression model cannot be fitted to high-dimensional data, as the high-dimensionality brings about empirical non-identifiability. Penalized regression overcomes this non-identifiability by augmentation of the loss function by a penalty (i.e. a function of regression coefficients). The ridge penalty is the sum of squared regression coefficients, giving rise to ridge regression. Here many aspect of ridge regression are reviewed e.g. moments, mean squared error, its equivalence to constrained estimation, and its relation to Bayesian regression. Finally, its behaviour and use are illustrated in simulation and on omics data. Subsequently, ridge regression is generalized to allow for a more general penalty. The ridge penalization framework is then translated to logistic regression and its properties are shown to carry over. To contrast ridge penalized estimation, the final chapter introduces its lasso counterpart.

</details>

<details>

<summary>2021-05-31 11:12:44 - Radial Bayesian Neural Networks: Beyond Discrete Support In Large-Scale Bayesian Deep Learning</summary>

- *Sebastian Farquhar, Michael Osborne, Yarin Gal*

- `1907.00865v4` - [abs](http://arxiv.org/abs/1907.00865v4) - [pdf](http://arxiv.org/pdf/1907.00865v4)

> We propose Radial Bayesian Neural Networks (BNNs): a variational approximate posterior for BNNs which scales well to large models while maintaining a distribution over weight-space with full support. Other scalable Bayesian deep learning methods, like MC dropout or deep ensembles, have discrete support-they assign zero probability to almost all of the weight-space. Unlike these discrete support methods, Radial BNNs' full support makes them suitable for use as a prior for sequential inference. In addition, they solve the conceptual challenges with the a priori implausibility of weight distributions with discrete support. The Radial BNN is motivated by avoiding a sampling problem in 'mean-field' variational inference (MFVI) caused by the so-called 'soap-bubble' pathology of multivariate Gaussians. We show that, unlike MFVI, Radial BNNs are robust to hyperparameters and can be efficiently applied to a challenging real-world medical application without needing ad-hoc tweaks and intensive tuning. In fact, in this setting Radial BNNs out-perform discrete-support methods like MC dropout. Lastly, by using Radial BNNs as a theoretically principled, robust alternative to MFVI we make significant strides in a Bayesian continual learning evaluation.

</details>

<details>

<summary>2021-05-31 13:09:09 - Online Bayesian inference for multiple changepoints and risk assessment</summary>

- *Olivier Sorba, C Geissler*

- `2106.05834v1` - [abs](http://arxiv.org/abs/2106.05834v1) - [pdf](http://arxiv.org/pdf/2106.05834v1)

> The aim of the present study is to detect abrupt trend changes in the mean of a multidimensional sequential signal. Directly inspired by papers of Fernhead and Liu ([4] and [5]), this work describes the signal in a hierarchical manner : the change dates of a time segmentation process trigger the renewal of a piece-wise constant emission law. Bayesian posterior information on the change dates and emission parameters is obtained. These estimations can be revised online, i.e. as new data arrive. This paper proposes explicit formulations corresponding to various emission laws, as well as a generalization to the case where only partially observed data are available. Practical applications include the returns of partially observed multi-asset investment strategies, when only scant prior knowledge of the movers of the returns is at hand, limited to some statistical assumptions. This situation is different from the study of trend changes in the returns of individual assets, where fundamental exogenous information (news, earnings announcements, controversies, etc.) can be used.

</details>

<details>

<summary>2021-05-31 14:46:54 - Multiscale Bayesian Survival Analysis</summary>

- *Ismaël Castillo, Stéphanie van der Pas*

- `2005.02889v3` - [abs](http://arxiv.org/abs/2005.02889v3) - [pdf](http://arxiv.org/pdf/2005.02889v3)

> We consider Bayesian nonparametric inference in the right-censoring survival model, where modeling is made at the level of the hazard rate. We derive posterior limiting distributions for linear functionals of the hazard, and then for `many' functionals simultaneously in appropriate multiscale spaces. As an application, we derive Bernstein-von Mises theorems for the cumulative hazard and survival functions, which lead to asymptotically efficient confidence bands for these quantities. Further, we show optimal posterior contraction rates for the hazard in terms of the supremum norm. In medical studies, a popular approach is to model hazards a priori as random histograms with possibly dependent heights. This and more general classes of arbitrarily smooth prior distributions are considered as applications of our theory. A sampler is provided for possibly dependent histogram posteriors. Its finite sample properties are investigated on both simulated and real data experiments.

</details>

<details>

<summary>2021-05-31 17:33:06 - Learning to infer in recurrent biological networks</summary>

- *Ari S. Benjamin, Konrad P. Kording*

- `2006.10811v2` - [abs](http://arxiv.org/abs/2006.10811v2) - [pdf](http://arxiv.org/pdf/2006.10811v2)

> A popular theory of perceptual processing holds that the brain learns both a generative model of the world and a paired recognition model using variational Bayesian inference. Most hypotheses of how the brain might learn these models assume that neurons in a population are conditionally independent given their common inputs. This simplification is likely not compatible with the type of local recurrence observed in the brain. Seeking an alternative that is compatible with complex inter-dependencies yet consistent with known biology, we argue here that the cortex may learn with an adversarial algorithm. Many observable symptoms of this approach would resemble known neural phenomena, including wake/sleep cycles and oscillations that vary in magnitude with surprise, and we describe how further predictions could be tested. We illustrate the idea on recurrent neural networks trained to model image and video datasets. This framework for learning brings variational inference closer to neuroscience and yields multiple testable hypotheses.

</details>

<details>

<summary>2021-05-31 22:00:03 - NOMU: Neural Optimization-based Model Uncertainty</summary>

- *Jakob Heiss, Jakob Weissteiner, Hanna Wutte, Sven Seuken, Josef Teichmann*

- `2102.13640v3` - [abs](http://arxiv.org/abs/2102.13640v3) - [pdf](http://arxiv.org/pdf/2102.13640v3)

> We study methods for estimating model uncertainty for neural networks (NNs). To isolate the effect of model uncertainty, we focus on a noiseless setting with scarce training data. We introduce five important desiderata regarding model uncertainty that any method should satisfy. However, we find that established benchmarks often fail to reliably capture some of these desiderata, even those that are required by Bayesian theory. To address this, we introduce a new approach for capturing model uncertainty for NNs, which we call Neural Optimization-based Model Uncertainty (NOMU). The main idea of NOMU is to design a network architecture consisting of two connected sub-NNs, one for model prediction and one for model uncertainty, and to train it using a carefully-designed loss function. Importantly, our design enforces that NOMU satisfies our five desiderata. Due to its modular architecture, NOMU can provide model uncertainty for any given (previously trained) NN if given access to its training data. We first experimentally study noiseless regression with scarce training data to highlight the deficiencies of the established benchmarks. Finally, we study the important task of Bayesian optimization (BO) with costly evaluations, where good model uncertainty estimates are essential. Our results show that NOMU performs as well or better than state-of-the-art benchmarks.

</details>


## 2021-06

<details>

<summary>2021-06-01 00:01:32 - Multi-Robot Gaussian Process Estimation and Coverage: A Deterministic Sequencing Algorithm and Regret Analysis</summary>

- *Lai Wei, Andrew McDonald, Vaibhav Srivastava*

- `2101.04306v3` - [abs](http://arxiv.org/abs/2101.04306v3) - [pdf](http://arxiv.org/pdf/2101.04306v3)

> We study the problem of distributed multi-robot coverage over an unknown, nonuniform sensory field. Modeling the sensory field as a realization of a Gaussian Process and using Bayesian techniques, we devise a policy which aims to balance the tradeoff between learning the sensory function and covering the environment. We propose an adaptive coverage algorithm called Deterministic Sequencing of Learning and Coverage (DSLC) that schedules learning and coverage epochs such that its emphasis gradually shifts from exploration to exploitation while never fully ceasing to learn. Using a novel definition of coverage regret which characterizes overall coverage performance of a multi-robot team over a time horizon $T$, we analyze DSLC to provide an upper bound on expected cumulative coverage regret. Finally, we illustrate the empirical performance of the algorithm through simulations of the coverage task over an unknown distribution of wildfires.

</details>

<details>

<summary>2021-06-01 09:55:29 - Bayesian Reasoning with Trained Neural Networks</summary>

- *Jakob Knollmüller, Torsten Enßlin*

- `2001.11031v3` - [abs](http://arxiv.org/abs/2001.11031v3) - [pdf](http://arxiv.org/pdf/2001.11031v3)

> We showed how to use trained neural networks to perform Bayesian reasoning in order to solve tasks outside their initial scope. Deep generative models provide prior knowledge, and classification/regression networks impose constraints. The tasks at hand were formulated as Bayesian inference problems, which we approximately solved through variational or sampling techniques. The approach built on top of already trained networks, and the addressable questions grew super-exponentially with the number of available networks. In its simplest form, the approach yielded conditional generative models. However, multiple simultaneous constraints constitute elaborate questions. We compared the approach to specifically trained generators, showed how to solve riddles, and demonstrated its compatibility with state-of-the-art architectures.

</details>

<details>

<summary>2021-06-01 14:43:47 - Transformation Models for Flexible Posteriors in Variational Bayes</summary>

- *Sefan Hörtling, Daniel Dold, Oliver Dürr, Beate Sick*

- `2106.00528v1` - [abs](http://arxiv.org/abs/2106.00528v1) - [pdf](http://arxiv.org/pdf/2106.00528v1)

> The main challenge in Bayesian models is to determine the posterior for the model parameters. Already, in models with only one or few parameters, the analytical posterior can only be determined in special settings. In Bayesian neural networks, variational inference is widely used to approximate difficult-to-compute posteriors by variational distributions. Usually, Gaussians are used as variational distributions (Gaussian-VI) which limits the quality of the approximation due to their limited flexibility. Transformation models on the other hand are flexible enough to fit any distribution. Here we present transformation model-based variational inference (TM-VI) and demonstrate that it allows to accurately approximate complex posteriors in models with one parameter and also works in a mean-field fashion for multi-parameter models like neural networks.

</details>

<details>

<summary>2021-06-01 15:39:17 - Efficient adaptive MCMC implementation for Pseudo-Bayesian quantum tomography</summary>

- *The Tien Mai*

- `2106.00577v1` - [abs](http://arxiv.org/abs/2106.00577v1) - [pdf](http://arxiv.org/pdf/2106.00577v1)

> We revisit the Pseudo-Bayesian approach to the problem of estimating density matrix in quantum state tomography in this paper. Pseudo-Bayesian inference has been shown to offer a powerful paradign for quantum tomography with attractive theoretical and empirical results. However, the computation of (Pseudo-)Bayesian estimators, due to sampling from complex and high-dimensional distribution, pose significant challenges that hampers their usages in practical settings. To overcome this problem, we present an efficient adaptive MCMC sampling method for the Pseudo-Bayesian estimator. We show in simulations that our approach is substantially faster than the previous implementation by at least two orders of magnitude which is significant for practical quantum tomography.

</details>

<details>

<summary>2021-06-01 16:50:01 - Characteristic and Necessary Minutiae in Fingerprints</summary>

- *Johannes Wieditz, Yvo Pokern, Dominic Schuhmacher, Stephan Huckemann*

- `2009.07910v3` - [abs](http://arxiv.org/abs/2009.07910v3) - [pdf](http://arxiv.org/pdf/2009.07910v3)

> Fingerprints feature a ridge pattern with moderately varying ridge frequency (RF), following an orientation field (OF), which usually features some singularities. Additionally at some points, called minutiae, ridge lines end or fork and this point pattern is usually used for fingerprint identification and authentication. Whenever the OF features divergent ridge lines (e.g. near singularities), a nearly constant RF necessitates the generation of more ridge lines, originating at minutiae. We call these the necessary minutiae. It turns out that fingerprints feature additional minutiae which occur at rather arbitrary locations. We call these the random minutiae or, since they may convey fingerprint individuality beyond the OF, the characteristic minutiae. In consequence, the minutiae point pattern is assumed to be a realization of the superposition of two stochastic point processes: a Strauss point process (whose activity function is given by the divergence field) with an additional hard core, and a homogeneous Poisson point process, modelling the necessary and the characteristic minutiae, respectively. We perform Bayesian inference using an MCMC-based minutiae separating algorithm (MiSeal). In simulations, it provides good mixing and good estimation of underlying parameters. In application to fingerprints, we can separate the two minutiae patterns and verify by example of two different prints with similar OF that characteristic minutiae convey fingerprint individuality.

</details>

<details>

<summary>2021-06-01 20:11:27 - Parametrization invariant interpretation of priors and posteriors</summary>

- *Jesus Cerquides*

- `2105.08304v2` - [abs](http://arxiv.org/abs/2105.08304v2) - [pdf](http://arxiv.org/pdf/2105.08304v2)

> In this paper we leverage on probability over Riemannian manifolds to rethink the interpretation of priors and posteriors in Bayesian inference. The main mindshift is to move away from the idea that "a prior distribution establishes a probability distribution over the parameters of our model" to the idea that "a prior distribution establishes a probability distribution over probability distributions". To do that we assume that our probabilistic model is a Riemannian manifold with the Fisher metric. Under this mindset, any distribution over probability distributions should be "intrinsic", that is, invariant to the specific parametrization which is selected for the manifold. We exemplify our ideas through a simple analysis of distributions over the manifold of Bernoulli distributions. One of the major shortcomings of maximum a posteriori estimates is that they depend on the parametrization. Based on the understanding developed here, we can define the maximum a posteriori estimate which is independent of the parametrization.

</details>

<details>

<summary>2021-06-02 08:06:01 - Approximate Laplace approximations for scalable model selection</summary>

- *David Rossell, Oriol Abril, Anirban Bhattacharya*

- `2012.07429v2` - [abs](http://arxiv.org/abs/2012.07429v2) - [pdf](http://arxiv.org/pdf/2012.07429v2)

> We propose the approximate Laplace approximation (ALA) to evaluate integrated likelihoods, a bottleneck in Bayesian model selection. The Laplace approximation (LA) is a popular tool that speeds up such computation and equips strong model selection properties. However, when the sample size is large or one considers many models the cost of the required optimizations becomes impractical. ALA reduces the cost to that of solving a least-squares problem for each model. Further, it enables efficient computation across models such as sharing pre-computed sufficient statistics and certain operations in matrix decompositions. We prove that in generalized (possibly non-linear) models ALA achieves a strong form of model selection consistency for a suitably-defined optimal model, at the same functional rates as exact computation. We consider fixed- and high-dimensional problems, group and hierarchical constraints, and the possibility that all models are misspecified. We also obtain ALA rates for Gaussian regression under non-local priors, an important example where the LA can be costly and does not consistently estimate the integrated likelihood. Our examples include non-linear regression, logistic, Poisson and survival models. We implement the methodology in the R package mombf.

</details>

<details>

<summary>2021-06-02 09:16:36 - ABC Learning of Hawkes Processes with Missing or Noisy Event Times</summary>

- *Isabella Deutsch, Gordon J. Ross*

- `2006.09015v3` - [abs](http://arxiv.org/abs/2006.09015v3) - [pdf](http://arxiv.org/pdf/2006.09015v3)

> The self-exciting Hawkes process is widely used to model events which occur in bursts. However, many real world data sets contain missing events and/or noisily observed event times, which we refer to as data distortion. The presence of such distortion can severely bias the learning of the Hawkes process parameters. To circumvent this, we propose modeling the distortion function explicitly. This leads to a model with an intractable likelihood function which makes it difficult to deploy standard parameter estimation techniques. As such, we develop the ABC-Hawkes algorithm which is a novel approach to estimation based on Approximate Bayesian Computation (ABC) and Markov Chain Monte Carlo. This allows the parameters of the Hawkes process to be learned in settings where conventional methods induce substantial bias or are inapplicable. The proposed approach is shown to perform well on both real and simulated data.

</details>

<details>

<summary>2021-06-02 16:01:25 - Interpretable Biomanufacturing Process Risk and Sensitivity Analyses for Quality-by-Design and Stability Control</summary>

- *Wei Xie, Bo Wang, Cheng Li, Dongming Xie, Jared Auclair*

- `1909.04261v4` - [abs](http://arxiv.org/abs/1909.04261v4) - [pdf](http://arxiv.org/pdf/1909.04261v4)

> While biomanufacturing plays a significant role in supporting the economy and ensuring public health, it faces critical challenges, including complexity, high variability, lengthy lead time, and very limited process data, especially for personalized new cell and gene biotherapeutics. Driven by these challenges, we propose an interpretable semantic bioprocess probabilistic knowledge graph and develop a game theory based risk and sensitivity analyses for production process to facilitate quality-by-design and stability control. Specifically, by exploring the causal relationships and interactions of critical process parameters and quality attributes (CPPs/CQAs), we create a Bayesian network based probabilistic knowledge graph characterizing the complex causal interdependencies of all factors. Then, we introduce a Shapley value based sensitivity analysis, which can correctly quantify the variation contribution from each input factor on the outputs (i.e., productivity, product quality). Since the bioprocess model coefficients are learned from limited process observations, we derive the Bayesian posterior distribution to quantify model uncertainty and further develop the Shapley value based sensitivity analysis to evaluate the impact of estimation uncertainty from each set of model coefficients. Therefore, the proposed bioprocess risk and sensitivity analyses can identify the bottlenecks, guide the reliable process specifications and the most "informative" data collection, and improve production stability.

</details>

<details>

<summary>2021-06-02 19:27:44 - A Constraint-Based Algorithm for the Structural Learning of Continuous-Time Bayesian Networks</summary>

- *Alessandro Bregoli, Marco Scutari, Fabio Stella*

- `2007.03248v3` - [abs](http://arxiv.org/abs/2007.03248v3) - [pdf](http://arxiv.org/pdf/2007.03248v3)

> Dynamic Bayesian networks have been well explored in the literature as discrete-time models: however, their continuous-time extensions have seen comparatively little attention. In this paper, we propose the first constraint-based algorithm for learning the structure of continuous-time Bayesian networks. We discuss the different statistical tests and the underlying hypotheses used by our proposal to establish conditional independence. Furthermore, we analyze and discuss the computational complexity of the best and worst cases for the proposed algorithm. Finally, we validate its performance using synthetic data, and we discuss its strengths and limitations comparing it with the score-based structure learning algorithm from Nodelman et al. (2003). We find the latter to be more accurate in learning networks with binary variables, while our constraint-based approach is more accurate with variables assuming more than two values. Numerical experiments confirm that score-based and constraint-based algorithms are comparable in terms of computation time.

</details>

<details>

<summary>2021-06-03 01:08:27 - Thirty Years of The Network Scale up Method</summary>

- *Ian Laga, Le Bao, Xiaoyue Niu*

- `2011.12516v2` - [abs](http://arxiv.org/abs/2011.12516v2) - [pdf](http://arxiv.org/pdf/2011.12516v2)

> Estimating the size of hard-to-reach populations is an important problem for many fields. The Network Scale-up Method (NSUM) is a relatively new approach to estimate the size of these hard-to-reach populations by asking respondents the question, "How many X's do you know," where X is the population of interest (e.g. "How many female sex workers do you know?"). The answers to these questions form Aggregated Relational Data (ARD). The NSUM has been used to estimate the size of a variety of subpopulations, including female sex workers, drug users, and even children who have been hospitalized for choking. Within the Network Scale-up methodology, there are a multitude of estimators for the size of the hidden population, including direct estimators, maximum likelihood estimators, and Bayesian estimators. In this article, we first provide an in-depth analysis of ARD properties and the techniques to collect the data. Then, we comprehensively review different estimation methods in terms of the assumptions behind each model, the relationships between the estimators, and the practical considerations of implementing the methods. Finally, we provide a summary of the dominant methods and an extensive list of the applications, and discuss the open problems and potential research directions in this area.

</details>

<details>

<summary>2021-06-03 14:44:02 - Stochastic tree ensembles for regularized nonlinear regression</summary>

- *Jingyu He, P. Richard Hahn*

- `2002.03375v4` - [abs](http://arxiv.org/abs/2002.03375v4) - [pdf](http://arxiv.org/pdf/2002.03375v4)

> This paper develops a novel stochastic tree ensemble method for nonlinear regression, which we refer to as XBART, short for Accelerated Bayesian Additive Regression Trees. By combining regularization and stochastic search strategies from Bayesian modeling with computationally efficient techniques from recursive partitioning approaches, the new method attains state-of-the-art performance: in many settings it is both faster and more accurate than the widely-used XGBoost algorithm. Via careful simulation studies, we demonstrate that our new approach provides accurate point-wise estimates of the mean function and does so faster than popular alternatives, such as BART, XGBoost and neural networks (using Keras). We also prove a number of basic theoretical results about the new algorithm, including consistency of the single tree version of the model and stationarity of the Markov chain produced by the ensemble version. Furthermore, we demonstrate that initializing standard Bayesian additive regression trees Markov chain Monte Carlo (MCMC) at XBART-fitted trees considerably improves credible interval coverage and reduces total run-time.

</details>

<details>

<summary>2021-06-03 15:44:39 - What are the most important statistical ideas of the past 50 years?</summary>

- *Andrew Gelman, Aki Vehtari*

- `2012.00174v5` - [abs](http://arxiv.org/abs/2012.00174v5) - [pdf](http://arxiv.org/pdf/2012.00174v5)

> We review the most important statistical ideas of the past half century, which we categorize as: counterfactual causal inference, bootstrapping and simulation-based inference, overparameterized models and regularization, Bayesian multilevel models, generic computation algorithms, adaptive decision analysis, robust inference, and exploratory data analysis. We discuss key contributions in these subfields, how they relate to modern computing and big data, and how they might be developed and extended in future decades. The goal of this article is to provoke thought and discussion regarding the larger themes of research in statistics and data science.

</details>

<details>

<summary>2021-06-03 20:53:41 - Bayesian inference on high-dimensional multivariate binary data</summary>

- *Antik Chakraborty, Rihui Ou, David B. Dunson*

- `2106.02127v1` - [abs](http://arxiv.org/abs/2106.02127v1) - [pdf](http://arxiv.org/pdf/2106.02127v1)

> It has become increasingly common to collect high-dimensional binary data; for example, with the emergence of new sampling techniques in ecology. In smaller dimensions, multivariate probit (MVP) models are routinely used for inferences. However, algorithms for fitting such models face issues in scaling up to high dimensions due to the intractability of the likelihood, involving an integral over a multivariate normal distribution having no analytic form. Although a variety of algorithms have been proposed to approximate this intractable integral, these approaches are difficult to implement and/or inaccurate in high dimensions. We propose a two-stage Bayesian approach for inference on model parameters while taking care of the uncertainty propagation between the stages. We use the special structure of latent Gaussian models to reduce the highly expensive computation involved in joint parameter estimation to focus inference on marginal distributions of model parameters. This essentially makes the method embarrassingly parallel for both stages. We illustrate performance in simulations and applications to joint species distribution modeling in ecology.

</details>

<details>

<summary>2021-06-03 21:56:54 - Interactive Communication in Bilateral Trade</summary>

- *Jieming Mao, Renato Paes Leme, Kangning Wang*

- `2106.02150v1` - [abs](http://arxiv.org/abs/2106.02150v1) - [pdf](http://arxiv.org/pdf/2106.02150v1)

> We define a model of interactive communication where two agents with private types can exchange information before a game is played. The model contains Bayesian persuasion as a special case of a one-round communication protocol. We define message complexity corresponding to the minimum number of interactive rounds necessary to achieve the best possible outcome. Our main result is that for bilateral trade, agents don't stop talking until they reach an efficient outcome: Either agents achieve an efficient allocation in finitely many rounds of communication; or the optimal communication protocol has infinite number of rounds. We show an important class of bilateral trade settings where efficient allocation is achievable with a small number of rounds of communication.

</details>

<details>

<summary>2021-06-03 23:04:16 - Bayesian Models Applied to Cyber Security Anomaly Detection Problems</summary>

- *José A. Perusquía, Jim E. Griffin, Cristiano Villa*

- `2003.10360v4` - [abs](http://arxiv.org/abs/2003.10360v4) - [pdf](http://arxiv.org/pdf/2003.10360v4)

> Cyber security is an important concern for all individuals, organisations and governments globally. Cyber attacks have become more sophisticated, frequent and dangerous than ever, and traditional anomaly detection methods have been proved to be less effective when dealing with these new classes of cyber threats. In order to address this, both classical and Bayesian models offer a valid and innovative alternative to the traditional signature-based methods, motivating the increasing interest in statistical research that it has been observed in recent years. In this review we provide a description of some typical cyber security challenges, typical types of data and statistical methods, paying special attention to Bayesian approaches for these problems.

</details>

<details>

<summary>2021-06-04 05:36:19 - Estimating parking occupancy using smart meter transaction data</summary>

- *Daniel Jordon, Robert Hampshire, Tayo Fabusuyi*

- `2106.02270v1` - [abs](http://arxiv.org/abs/2106.02270v1) - [pdf](http://arxiv.org/pdf/2106.02270v1)

> The excessive search for parking, known as cruising, generates pollution and congestion. Cities are looking for approaches that will reduce the negative impact associated with searching for parking. However, adequately measuring the number of vehicles in search of parking is difficult and requires sensing technologies. In this paper, we develop an approach that eliminates the need for sensing technology by using parking meter payment transactions to estimate parking occupancy and the number of cars searching for parking. The estimation scheme is based on Particle Markov Chain Monte Carlo. We validate the performance of the Particle Markov Chain Monte Carlo approach using data simulated from a GI/GI/s queue. We show that the approach generates asymptotically unbiased Bayesian estimates of the parking occupancy and underlying model parameters such as arrival rates, average parking time, and the payment compliance rate. Finally, we estimate parking occupancy and cruising using parking meter data from SFpark, a large scale parking experiment and subsequently, compare the Particle Markov Chain Monte Carlo parking occupancy estimates against the ground truth data from the parking sensors. Our approach is easily replicated and scalable given that it only requires using data that cities already possess, namely historical parking payment transactions.

</details>

<details>

<summary>2021-06-04 13:20:28 - On the Effects of Quantisation on Model Uncertainty in Bayesian Neural Networks</summary>

- *Martin Ferianc, Partha Maji, Matthew Mattina, Miguel Rodrigues*

- `2102.11062v2` - [abs](http://arxiv.org/abs/2102.11062v2) - [pdf](http://arxiv.org/pdf/2102.11062v2)

> Bayesian neural networks (BNNs) are making significant progress in many research areas where decision-making needs to be accompanied by uncertainty estimation. Being able to quantify uncertainty while making decisions is essential for understanding when the model is over-/under-confident, and hence BNNs are attracting interest in safety-critical applications, such as autonomous driving, healthcare, and robotics. Nevertheless, BNNs have not been as widely used in industrial practice, mainly because of their increased memory and compute costs. In this work, we investigate quantisation of BNNs by compressing 32-bit floating-point weights and activations to their integer counterparts, that has already been successful in reducing the compute demand in standard pointwise neural networks. We study three types of quantised BNNs, we evaluate them under a wide range of different settings, and we empirically demonstrate that a uniform quantisation scheme applied to BNNs does not substantially decrease their quality of uncertainty estimation.

</details>

<details>

<summary>2021-06-04 19:52:14 - Ensemble Markov chain Monte Carlo with teleporting walkers</summary>

- *Michael Lindsey, Jonathan Weare, Anna Zhang*

- `2106.02686v1` - [abs](http://arxiv.org/abs/2106.02686v1) - [pdf](http://arxiv.org/pdf/2106.02686v1)

> We introduce an ensemble Markov chain Monte Carlo approach to sampling from a probability density with known likelihood. This method upgrades an underlying Markov chain by allowing an ensemble of such chains to interact via a process in which one chain's state is cloned as another's is deleted. This effective teleportation of states can overcome issues of metastability in the underlying chain, as the scheme enjoys rapid mixing once the modes of the target density have been populated. We derive a mean-field limit for the evolution of the ensemble. We analyze the global and local convergence of this mean-field limit, showing asymptotic convergence independent of the spectral gap of the underlying Markov chain, and moreover we interpret the limiting evolution as a gradient flow. We explain how interaction can be applied selectively to a subset of state variables in order to maintain advantage on very high-dimensional problems. Finally we present the application of our methodology to Bayesian hyperparameter estimation for Gaussian process regression.

</details>

<details>

<summary>2021-06-04 21:23:04 - Statistical summaries of unlabelled evolutionary trees and ranked hierarchical clustering trees</summary>

- *Samyak Rajanala, Julia A. Palacios*

- `2106.02724v1` - [abs](http://arxiv.org/abs/2106.02724v1) - [pdf](http://arxiv.org/pdf/2106.02724v1)

> Rooted and ranked binary trees are mathematical objects of great importance used to model hierarchical data and evolutionary relationships with applications in many fields including evolutionary biology and genetic epidemiology. Bayesian phylogenetic inference usually explore the posterior distribution of trees via Markov Chain Monte Carlo methods, however assessing uncertainty and summarizing distributions or samples of such trees remains challenging. While labelled phylogenetic trees have been extensively studied, relatively less literature exists for unlabelled trees which are increasingly useful, for example when one seeks to summarize samples of trees obtained with different methods, or from different samples and environments, and wishes to assess stability and generalizability of these summaries. In our paper, we exploit recently proposed distance metrics of unlabelled ranked binary trees and unlabelled ranked genealogies (equipped with branch lengths) to define the Frechet mean and variance as summaries of these tree distributions. We provide an efficient combinatorial optimization algorithm for computing the Frechet mean from a sample of or distribution on unlabelled ranked tree shapes and unlabelled ranked genealogies. We show the applicability of our summary statistics for studying popular tree distributions and for comparing the SARS-CoV-2 evolutionary trees across different locations during the COVID-19 epidemic in 2020.

</details>

<details>

<summary>2021-06-05 15:22:40 - Spike-and-Slab Group Lasso for Consistent Estimation and Variable Selection in Non-Gaussian Generalized Additive Models</summary>

- *Ray Bai*

- `2007.07021v5` - [abs](http://arxiv.org/abs/2007.07021v5) - [pdf](http://arxiv.org/pdf/2007.07021v5)

> We study estimation and variable selection in non-Gaussian Bayesian generalized additive models (GAMs) under a spike-and-slab prior for grouped variables. Our framework subsumes GAMs for logistic regression, Poisson regression, negative binomial regression, and gamma regression, and encompasses both canonical and non-canonical link functions. Under mild conditions, we establish posterior contraction rates and model selection consistency when $p \gg n$. For computation, we propose an EM algorithm for obtaining MAP estimates in our model, which is available in the R package sparseGAM. We illustrate our method on both synthetic and real data sets.

</details>

<details>

<summary>2021-06-06 09:22:32 - Seemingly Unrelated Multi-State processes: a Bayesian semiparametric approach</summary>

- *Andrea Cremaschi, Raffele Argiento, Maria De Iorio, Cai Shirong, Yap Seng Chong, Michael J. Meaney, Michelle Z. L. Kee*

- `2106.03072v1` - [abs](http://arxiv.org/abs/2106.03072v1) - [pdf](http://arxiv.org/pdf/2106.03072v1)

> Many applications in medical statistics as well as in other fields can be described by transitions between multiple states (e.g. from health to disease) experienced by individuals over time. In this context, multi-state models are a popular statistical technique, in particular when the exact transition times are not observed. The key quantities of interest are the transition rates, capturing the instantaneous risk of moving from one state to another. The main contribution of this work is to propose a joint semiparametric model for several possibly related multi-state processes (Seemingly Unrelated Multi-State, SUMS, processes), assuming a Markov structure for the transitions over time. The dependence between different processes is captured by specifying a joint random effect distribution on the transition rates of each process. We assume a flexible random effect distribution, which allows for clustering of the individuals, overdispersion and outliers. Moreover, we employ a graph structure to describe the dependence among processes, exploiting tools from the Gaussian Graphical model literature. It is also possible to include covariate effects. We use our approach to model disease progression in mental health. Posterior inference is performed through a specially devised MCMC algorithm.

</details>

<details>

<summary>2021-06-06 18:50:53 - Calibration of imperfect mathematical models by multiple sources of data with measurement bias</summary>

- *Mengyang Gu, Kyle Anderson, Erika McPhillips*

- `1810.11664v2` - [abs](http://arxiv.org/abs/1810.11664v2) - [pdf](http://arxiv.org/pdf/1810.11664v2)

> Model calibration involves using experimental or field data to estimate the unknown parameters of a mathematical model. This task is complicated by discrepancy between the model and reality, and by possible bias in the data. We consider model calibration in the presence of both model discrepancy and measurement bias using multiple sources of data. Model discrepancy is often estimated using a Gaussian stochastic process (GaSP), but it has been observed in many studies that the calibrated mathematical model can be far from the reality. Here we show that modeling the discrepancy function via a GaSP often leads to an inconsistent estimation of the calibration parameters even if one has an infinite number of repeated experiments and infinite number of observations in a fixed input domain in each experiment. We introduce the scaled Gaussian stochastic process (S-GaSP) to model the discrepancy function. Unlike the GaSP, the S-GaSP utilizes a non-increasing scaling function which assigns more probability mass on the smaller $L_2$ loss between the mathematical model and reality, preventing the calibrated mathematical model from deviating too much from reality.   We apply our technique to the calibration of a geophysical model of K\={\i}lauea Volcano, Hawai`i, using multiple radar satellite interferograms. We compare the use of models calibrated using multiple data sets simultaneously with results obtained using stacks (averages). We derive distributions for the maximum likelihood estimator and Bayesian inference, both implemented in the "RobustCalibration" package available on CRAN. Analysis of both simulated and real data confirm that our approach can identify the measurement bias and model discrepancy using multiple sources of data, and provide better estimates of model parameters.

</details>

<details>

<summary>2021-06-07 10:14:18 - BayesIMP: Uncertainty Quantification for Causal Data Fusion</summary>

- *Siu Lun Chau, Jean-François Ton, Javier González, Yee Whye Teh, Dino Sejdinovic*

- `2106.03477v1` - [abs](http://arxiv.org/abs/2106.03477v1) - [pdf](http://arxiv.org/pdf/2106.03477v1)

> While causal models are becoming one of the mainstays of machine learning, the problem of uncertainty quantification in causal inference remains challenging. In this paper, we study the causal data fusion problem, where datasets pertaining to multiple causal graphs are combined to estimate the average treatment effect of a target variable. As data arises from multiple sources and can vary in quality and quantity, principled uncertainty quantification becomes essential. To that end, we introduce Bayesian Interventional Mean Processes, a framework which combines ideas from probabilistic integration and kernel mean embeddings to represent interventional distributions in the reproducing kernel Hilbert space, while taking into account the uncertainty within each causal graph. To demonstrate the utility of our uncertainty estimation, we apply our method to the Causal Bayesian Optimisation task and show improvements over state-of-the-art methods.

</details>

<details>

<summary>2021-06-07 16:01:45 - A multivariate Gaussian random field prior against spatial confounding</summary>

- *Isa Marques, Thomas Kneib, Nadja Klein*

- `2106.03737v1` - [abs](http://arxiv.org/abs/2106.03737v1) - [pdf](http://arxiv.org/pdf/2106.03737v1)

> Spatial models are used in a variety research areas, such as environmental sciences, epidemiology, or physics. A common phenomenon in many spatial regression models is spatial confounding. This phenomenon takes place when spatially indexed covariates modeling the mean of the response are correlated with the spatial random effect. As a result, estimates for regression coefficients of the covariates can be severely biased and interpretation of these is no longer valid. Recent literature has shown that typical solutions for reducing spatial confounding can lead to misleading and counterintuitive results. In this paper, we develop a computationally efficient spatial model in a Bayesian framework integrating novel prior structure that reduces spatial confounding. Starting from the univariate case, we extend our prior structure to case of multiple spatially confounded covariates. In a simulation study, we show that our novel model flexibly detects and reduces spatial confounding in spatial datasets, and it performs better than typically used methods such as restricted spatial regression. These results are promising for any applied researcher who wishes to interpret covariate effects in spatial regression models. As a real data illustration, we study the effect of elevation and temperature on the mean of daily precipitation in Germany.

</details>

<details>

<summary>2021-06-08 02:41:51 - Speedy Performance Estimation for Neural Architecture Search</summary>

- *Binxin Ru, Clare Lyle, Lisa Schut, Miroslav Fil, Mark van der Wilk, Yarin Gal*

- `2006.04492v2` - [abs](http://arxiv.org/abs/2006.04492v2) - [pdf](http://arxiv.org/pdf/2006.04492v2)

> Reliable yet efficient evaluation of generalisation performance of a proposed architecture is crucial to the success of neural architecture search (NAS). Traditional approaches face a variety of limitations: training each architecture to completion is prohibitively expensive, early stopped validation accuracy may correlate poorly with fully trained performance, and model-based estimators require large training sets. We instead propose to estimate the final test performance based on a simple measure of training speed. Our estimator is theoretically motivated by the connection between generalisation and training speed, and is also inspired by the reformulation of a PAC-Bayes bound under the Bayesian setting. Our model-free estimator is simple, efficient, and cheap to implement, and does not require hyperparameter-tuning or surrogate training before deployment. We demonstrate on various NAS search spaces that our estimator consistently outperforms other alternatives in achieving better correlation with the true test performance rankings. We further show that our estimator can be easily incorporated into both query-based and one-shot NAS methods to improve the speed or quality of the search.

</details>

<details>

<summary>2021-06-08 09:14:39 - Seismic Inverse Modeling Method based on Generative Adversarial Network</summary>

- *Pengfei Xie, YanShu Yin, JiaGen Hou, Mei Chen, Lixin Wang*

- `2106.04197v1` - [abs](http://arxiv.org/abs/2106.04197v1) - [pdf](http://arxiv.org/pdf/2106.04197v1)

> Seismic inverse modeling is a common method in reservoir prediction and it plays a vital role in the exploration and development of oil and gas. Conventional seismic inversion method is difficult to combine with complicated and abstract knowledge on geological mode and its uncertainty is difficult to be assessed. The paper proposes an inversion modeling method based on GAN consistent with geology, well logs, seismic data. GAN is a the most promising generation model algorithm that extracts spatial structure and abstract features of training images. The trained GAN can reproduce the models with specific mode. In our test, 1000 models were generated in 1 second. Based on the trained GAN after assessment, the optimal result of models can be calculated through Bayesian inversion frame. Results show that inversion models conform to observation data and have a low uncertainty under the premise of fast generation. This seismic inverse modeling method increases the efficiency and quality of inversion iteration. It is worthy of studying and applying in fusion of seismic data and geological knowledge.

</details>

<details>

<summary>2021-06-08 13:46:46 - Reinforced Few-Shot Acquisition Function Learning for Bayesian Optimization</summary>

- *Bing-Jing Hsieh, Ping-Chun Hsieh, Xi Liu*

- `2106.04335v1` - [abs](http://arxiv.org/abs/2106.04335v1) - [pdf](http://arxiv.org/pdf/2106.04335v1)

> Bayesian optimization (BO) conventionally relies on handcrafted acquisition functions (AFs) to sequentially determine the sample points. However, it has been widely observed in practice that the best-performing AF in terms of regret can vary significantly under different types of black-box functions. It has remained a challenge to design one AF that can attain the best performance over a wide variety of black-box functions. This paper aims to attack this challenge through the perspective of reinforced few-shot AF learning (FSAF). Specifically, we first connect the notion of AFs with Q-functions and view a deep Q-network (DQN) as a surrogate differentiable AF. While it serves as a natural idea to combine DQN and an existing few-shot learning method, we identify that such a direct combination does not perform well due to severe overfitting, which is particularly critical in BO due to the need of a versatile sampling policy. To address this, we present a Bayesian variant of DQN with the following three features: (i) It learns a distribution of Q-networks as AFs based on the Kullback-Leibler regularization framework. This inherently provides the uncertainty required in sampling for BO and mitigates overfitting. (ii) For the prior of the Bayesian DQN, we propose to use a demo policy induced by an off-the-shelf AF for better training stability. (iii) On the meta-level, we leverage the meta-loss of Bayesian model-agnostic meta-learning, which serves as a natural companion to the proposed FSAF. Moreover, with the proper design of the Q-networks, FSAF is general-purpose in that it is agnostic to the dimension and the cardinality of the input domain. Through extensive experiments, we demonstrate that the FSAF achieves comparable or better regrets than the state-of-the-art benchmarks on a wide variety of synthetic and real-world test functions.

</details>

<details>

<summary>2021-06-08 16:21:12 - Bayesian hierarchical modeling and analysis for physical activity trajectories using actigraph data</summary>

- *Pierfrancesco Alaimo Di Loro, Marco Mingione, Jonah Lipsitt, Christina M. Batteate, Michael Jerrett, Sudipto Banerjee*

- `2101.01624v3` - [abs](http://arxiv.org/abs/2101.01624v3) - [pdf](http://arxiv.org/pdf/2101.01624v3)

> Rapid developments in streaming data technologies are continuing to generate increased interest in monitoring human activity. Wearable devices, such as wrist-worn sensors that monitor gross motor activity (actigraphy), have become prevalent. An actigraph unit continually records the activity level of an individual, producing a very large amount of data at a high-resolution that can be immediately downloaded and analyzed. While this kind of \textit{big data} includes both spatial and temporal information, the variation in such data seems to be more appropriately modeled by considering stochastic evolution through time while accounting for spatial information separately. We propose a comprehensive Bayesian hierarchical modeling and inferential framework for actigraphy data reckoning with the massive sizes of such databases while attempting to offer full inference. Building upon recent developments in this field, we construct Nearest Neighbour Gaussian Processes (NNGPs) for actigraphy data to compute at large temporal scales. More specifically, we construct a temporal NNGP and we focus on the optimized implementation of the collapsed algorithm in this specific context. This approach permits improved model scaling while also offering full inference. We test and validate our methods on simulated data and subsequently apply and verify their predictive ability on an original dataset concerning a health study conducted by the Fielding School of Public Health of the University of California, Los Angeles.

</details>

<details>

<summary>2021-06-08 18:50:11 - Automatically Differentiable Random Coefficient Logistic Demand Estimation</summary>

- *Andrew Chia*

- `2106.04636v1` - [abs](http://arxiv.org/abs/2106.04636v1) - [pdf](http://arxiv.org/pdf/2106.04636v1)

> We show how the random coefficient logistic demand (BLP) model can be phrased as an automatically differentiable moment function, including the incorporation of numerical safeguards proposed in the literature. This allows gradient-based frequentist and quasi-Bayesian estimation using the Continuously Updating Estimator (CUE). Drawing from the machine learning literature, we outline hitherto under-utilized best practices in both frequentist and Bayesian estimation techniques. Our Monte Carlo experiments compare the performance of CUE, 2S-GMM, and LTE estimation. Preliminary findings indicate that the CUE estimated using LTE and frequentist optimization has a lower bias but higher MAE compared to the traditional 2-Stage GMM (2S-GMM) approach. We also find that using credible intervals from MCMC sampling for the non-linear parameters together with frequentist analytical standard errors for the concentrated out linear parameters provides empirical coverage closest to the nominal level. The accompanying admest Python package provides a platform for replication and extensibility.

</details>

<details>

<summary>2021-06-08 19:35:46 - Modelling for Poisson process intensities over irregular spatial domains</summary>

- *Chunyi Zhao, Athanasios Kottas*

- `2106.04654v1` - [abs](http://arxiv.org/abs/2106.04654v1) - [pdf](http://arxiv.org/pdf/2106.04654v1)

> We develop nonparametric Bayesian modelling approaches for Poisson processes, using weighted combinations of structured beta densities to represent the point process intensity function. For a regular spatial domain, such as the unit square, the model construction implies a Bernstein-Dirichlet prior for the Poisson process density, which supports general inference for point process functionals. The key contribution of the methodology is two classes of flexible and computationally efficient models for spatial Poisson process intensities over irregular domains. We address the choice or estimation of the number of beta basis densities, and develop methods for prior specification and posterior simulation for full inference about functionals of the point process. The methodology is illustrated with both synthetic and real data sets.

</details>

<details>

<summary>2021-06-08 20:47:21 - Bayesian Optimization over Hybrid Spaces</summary>

- *Aryan Deshwal, Syrine Belakaria, Janardhan Rao Doppa*

- `2106.04682v1` - [abs](http://arxiv.org/abs/2106.04682v1) - [pdf](http://arxiv.org/pdf/2106.04682v1)

> We consider the problem of optimizing hybrid structures (mixture of discrete and continuous input variables) via expensive black-box function evaluations. This problem arises in many real-world applications. For example, in materials design optimization via lab experiments, discrete and continuous variables correspond to the presence/absence of primitive elements and their relative concentrations respectively. The key challenge is to accurately model the complex interactions between discrete and continuous variables. In this paper, we propose a novel approach referred as Hybrid Bayesian Optimization (HyBO) by utilizing diffusion kernels, which are naturally defined over continuous and discrete variables. We develop a principled approach for constructing diffusion kernels over hybrid spaces by utilizing the additive kernel formulation, which allows additive interactions of all orders in a tractable manner. We theoretically analyze the modeling strength of additive hybrid kernels and prove that it has the universal approximation property. Our experiments on synthetic and six diverse real-world benchmarks show that HyBO significantly outperforms the state-of-the-art methods.

</details>

<details>

<summary>2021-06-08 21:34:37 - Black-box density function estimation using recursive partitioning</summary>

- *Erik Bodin, Zhenwen Dai, Neill D. F. Campbell, Carl Henrik Ek*

- `2010.13632v2` - [abs](http://arxiv.org/abs/2010.13632v2) - [pdf](http://arxiv.org/pdf/2010.13632v2)

> We present a novel approach to Bayesian inference and general Bayesian computation that is defined through a sequential decision loop. Our method defines a recursive partitioning of the sample space. It neither relies on gradients nor requires any problem-specific tuning, and is asymptotically exact for any density function with a bounded domain. The output is an approximation to the whole density function including the normalisation constant, via partitions organised in efficient data structures. Such approximations may be used for evidence estimation or fast posterior sampling, but also as building blocks to treat a larger class of estimation problems. The algorithm shows competitive performance to recent state-of-the-art methods on synthetic and real-world problems including parameter inference for gravitational-wave physics.

</details>

<details>

<summary>2021-06-09 07:04:02 - Bias-Robust Bayesian Optimization via Dueling Bandits</summary>

- *Johannes Kirschner, Andreas Krause*

- `2105.11802v2` - [abs](http://arxiv.org/abs/2105.11802v2) - [pdf](http://arxiv.org/pdf/2105.11802v2)

> We consider Bayesian optimization in settings where observations can be adversarially biased, for example by an uncontrolled hidden confounder. Our first contribution is a reduction of the confounded setting to the dueling bandit model. Then we propose a novel approach for dueling bandits based on information-directed sampling (IDS). Thereby, we obtain the first efficient kernelized algorithm for dueling bandits that comes with cumulative regret guarantees. Our analysis further generalizes a previously proposed semi-parametric linear bandit model to non-linear reward functions, and uncovers interesting links to doubly-robust estimation.

</details>

<details>

<summary>2021-06-09 07:06:05 - Nonlinear Hawkes Processes in Time-Varying System</summary>

- *Feng Zhou, Quyu Kong, Yixuan Zhang, Cheng Feng, Jun Zhu*

- `2106.04844v1` - [abs](http://arxiv.org/abs/2106.04844v1) - [pdf](http://arxiv.org/pdf/2106.04844v1)

> Hawkes processes are a class of point processes that have the ability to model the self- and mutual-exciting phenomena. Although the classic Hawkes processes cover a wide range of applications, their expressive ability is limited due to three key hypotheses: parametric, linear and homogeneous. Recent work has attempted to address these limitations separately. This work aims to overcome all three assumptions simultaneously by proposing the flexible state-switching Hawkes processes: a flexible, nonlinear and nonhomogeneous variant where a state process is incorporated to interact with the point processes. The proposed model empowers Hawkes processes to be applied to time-varying systems. For inference, we utilize the latent variable augmentation technique to design two efficient Bayesian inference algorithms: Gibbs sampler and mean-field variational inference, with analytical iterative updates to estimate the posterior. In experiments, our model achieves superior performance compared to the state-of-the-art competitors.

</details>

<details>

<summary>2021-06-09 07:16:04 - Dynamic mechanism design: An elementary introduction</summary>

- *Kiho Yoon*

- `2106.04850v1` - [abs](http://arxiv.org/abs/2106.04850v1) - [pdf](http://arxiv.org/pdf/2106.04850v1)

> This paper introduces dynamic mechanism design in an elementary fashion. We first examine optimal dynamic mechanisms: We find necessary and sufficient conditions for perfect Bayesian incentive compatibility and formulate the optimal dynamic mechanism problem. We next examine efficient dynamic mechanisms: We establish the uniqueness of Groves mechanism and investigate budget balance of the dynamic pivot mechanism in some detail for a bilateral trading environment. This introduction reveals that many results and techniques of static mechanism design can be straightforwardly extended and adapted to the analysis of dynamic settings.

</details>

<details>

<summary>2021-06-09 07:40:00 - Bayesian Boosting for Linear Mixed Models</summary>

- *Boyao Zhang, Colin Griesbach, Cora Kim, Nadia Müller-Voggel, Elisabeth Bergherr*

- `2106.04862v1` - [abs](http://arxiv.org/abs/2106.04862v1) - [pdf](http://arxiv.org/pdf/2106.04862v1)

> Boosting methods are widely used in statistical learning to deal with high-dimensional data due to their variable selection feature. However, those methods lack straightforward ways to construct estimators for the precision of the parameters such as variance or confidence interval, which can be achieved by conventional statistical methods like Bayesian inference. In this paper, we propose a new inference method "BayesBoost" that combines boosting and Bayesian for linear mixed models to make the uncertainty estimation for the random effects possible on the one hand. On the other hand, the new method overcomes the shortcomings of Bayesian inference in giving precise and unambiguous guidelines for the selection of covariates by benefiting from boosting techniques. The implementation of Bayesian inference leads to the randomness of model selection criteria like the conditional AIC (cAIC), so we also propose a cAIC-based model selection criteria that focus on the stabilized regions instead of the global minimum. The effectiveness of the new approach can be observed via simulation and in a data example from the field of neurophysiology focussing on the mechanisms in the brain while listening to unpleasant sounds.

</details>

<details>

<summary>2021-06-09 10:44:15 - Sparse Algorithms for Markovian Gaussian Processes</summary>

- *William J. Wilkinson, Arno Solin, Vincent Adam*

- `2103.10710v3` - [abs](http://arxiv.org/abs/2103.10710v3) - [pdf](http://arxiv.org/pdf/2103.10710v3)

> Approximate Bayesian inference methods that scale to very large datasets are crucial in leveraging probabilistic models for real-world time series. Sparse Markovian Gaussian processes combine the use of inducing variables with efficient Kalman filter-like recursions, resulting in algorithms whose computational and memory requirements scale linearly in the number of inducing points, whilst also enabling parallel parameter updates and stochastic optimisation. Under this paradigm, we derive a general site-based approach to approximate inference, whereby we approximate the non-Gaussian likelihood with local Gaussian terms, called sites. Our approach results in a suite of novel sparse extensions to algorithms from both the machine learning and signal processing literature, including variational inference, expectation propagation, and the classical nonlinear Kalman smoothers. The derived methods are suited to large time series, and we also demonstrate their applicability to spatio-temporal data, where the model has separate inducing points in both time and space.

</details>

<details>

<summary>2021-06-09 13:38:24 - Spatial modelling of COVID-19 incident cases using Richards' curve: an application to the Italian regions</summary>

- *Marco Mingione, Pierfrancesco Alaimo Di Loro, Alessio Farcomeni, Fabio Divino, Gianfranco Lovison, Giovanna Jona Lasinio, Antonello Maruotti*

- `2106.05067v1` - [abs](http://arxiv.org/abs/2106.05067v1) - [pdf](http://arxiv.org/pdf/2106.05067v1)

> We introduce an extended generalised logistic growth model for discrete outcomes, in which a network structure can be specified to deal with spatial dependence and time dependence is dealt with using an Auto-Regressive approach. A major challenge concerns the specification of the network structure, crucial to consistently estimate the canonical parameters of the generalised logistic curve, e.g. peak time and height. Parameters are estimated under the Bayesian framework, using the {\texttt{ Stan}} probabilistic programming language. The proposed approach is motivated by the analysis of the first and second wave of COVID-19 in Italy, i.e. from February 2020 to July 2020 and from July 2020 to December 2020, respectively. We analyse data at the regional level and, interestingly enough, prove that substantial spatial and temporal dependence occurred in both waves, although strong restrictive measures were implemented during the first wave. Accurate predictions are obtained, improving those of the model where independence across regions is assumed.

</details>

<details>

<summary>2021-06-09 15:18:20 - Bayesian Modeling for Exposure Response Curve via Gaussian Processes: Causal Effects of Exposure to Air Pollution on Health Outcomes</summary>

- *Boyu Ren, Xiao Wu, Danielle Braun, Natesh Pillai, Francesca Dominici*

- `2105.03454v2` - [abs](http://arxiv.org/abs/2105.03454v2) - [pdf](http://arxiv.org/pdf/2105.03454v2)

> Motivated by environmental health research on air pollution, we address the challenge of estimation and uncertainty quantification of causal exposure-response function (CERF). The CERF describes the relationship between a continuously varying exposure (or treatment) and its causal effect on a outcome. We propose a new Bayesian approach that relies on a Gaussian process (GP) model to estimate the CERF. We parametrize the covariance (kernel) function of the GP to mimic matching via a Generalized Propensity Score (GPS). The tuning parameters of the matching function are chosen to optimize covariate balance. Our approach achieves automatic uncertainty evaluation of the CERF with high computational efficiency, enables change point detection through inference on derivatives of the CERF, and yields the desired separation of design and analysis phases for causal estimation. We provide theoretical results showing the correspondence between our Bayesian GP framework and traditional approaches in causal inference for estimating causal effects of a continuous exposure. We apply the methods to 520,711 ZIP-code-level observations to estimate the causal effect of long-term exposures to PM2.5 on all-cause mortality among Medicare enrollees in the United States.

</details>

<details>

<summary>2021-06-09 15:26:10 - Bayesian model averaging for analysis of lattice field theory results</summary>

- *William I. Jay, Ethan T. Neil*

- `2008.01069v3` - [abs](http://arxiv.org/abs/2008.01069v3) - [pdf](http://arxiv.org/pdf/2008.01069v3)

> Statistical modeling is a key component in the extraction of physical results from lattice field theory calculations. Although the general models used are often strongly motivated by physics, many model variations can frequently be considered for the same lattice data. Model averaging, which amounts to a probability-weighted average over all model variations, can incorporate systematic errors associated with model choice without being overly conservative. We discuss the framework of model averaging from the perspective of Bayesian statistics, and give useful formulae and approximations for the particular case of least-squares fitting, commonly used in modeling lattice results. In addition, we frame the common problem of data subset selection (e.g. choice of minimum and maximum time separation for fitting a two-point correlation function) as a model selection problem and study model averaging as a straightforward alternative to manual selection of fit ranges. Numerical examples involving both mock and real lattice data are given.

</details>

<details>

<summary>2021-06-09 17:43:40 - Probabilistic Deep Learning with Probabilistic Neural Networks and Deep Probabilistic Models</summary>

- *Daniel T. Chang*

- `2106.00120v3` - [abs](http://arxiv.org/abs/2106.00120v3) - [pdf](http://arxiv.org/pdf/2106.00120v3)

> Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neural network components which capture complex non-linear stochastic relationships between the random variables. We discuss some major examples of each approach including Bayesian neural networks and mixture density networks (for probabilistic neural networks), and variational autoencoders, deep Gaussian processes and deep mixed effects models (for deep probabilistic models). TensorFlow Probability is a library for probabilistic modeling and inference which can be used for both approaches of probabilistic deep learning. We include its code examples for illustration.

</details>

<details>

<summary>2021-06-09 17:46:22 - Bayesian Attention Belief Networks</summary>

- *Shujian Zhang, Xinjie Fan, Bo Chen, Mingyuan Zhou*

- `2106.05251v1` - [abs](http://arxiv.org/abs/2106.05251v1) - [pdf](http://arxiv.org/pdf/2106.05251v1)

> Attention-based neural networks have achieved state-of-the-art results on a wide range of tasks. Most such models use deterministic attention while stochastic attention is less explored due to the optimization difficulties or complicated model design. This paper introduces Bayesian attention belief networks, which construct a decoder network by modeling unnormalized attention weights with a hierarchy of gamma distributions, and an encoder network by stacking Weibull distributions with a deterministic-upward-stochastic-downward structure to approximate the posterior. The resulting auto-encoding networks can be optimized in a differentiable way with a variational lower bound. It is simple to convert any models with deterministic attention, including pretrained ones, to the proposed Bayesian attention belief networks. On a variety of language understanding tasks, we show that our method outperforms deterministic attention and state-of-the-art stochastic attention in accuracy, uncertainty estimation, generalization across domains, and robustness to adversarial attacks. We further demonstrate the general applicability of our method on neural machine translation and visual question answering, showing great potential of incorporating our method into various attention-related tasks.

</details>

<details>

<summary>2021-06-09 18:37:20 - Calculating the Likelihood Ratio for Multiple Pieces of Evidence</summary>

- *Norman Fenton, Martin Neil*

- `2106.05328v1` - [abs](http://arxiv.org/abs/2106.05328v1) - [pdf](http://arxiv.org/pdf/2106.05328v1)

> When presenting forensic evidence, such as a DNA match, experts often use the Likelihood ratio (LR) to explain the impact of evidence . The LR measures the probative value of the evidence with respect to a single hypothesis such as 'DNA comes from the suspect', and is defined as the probability of the evidence if the hypothesis is true divided by the probability of the evidence if the hypothesis is false. The LR is a valid measure of probative value because, by Bayes Theorem, the higher the LR is, the more our belief in the probability the hypothesis is true increases after observing the evidence. The LR is popular because it measures the probative value of evidence without having to make any explicit assumptions about the prior probability of the hypothesis. However, whereas the LR can in principle be easily calculated for a distinct single piece of evidence that relates directly to a specific hypothesis, in most realistic situations 'the evidence' is made up of multiple dependent components that impact multiple different hypotheses. In such situations the LR cannot be calculated . However, once the multiple pieces of evidence and hypotheses are modelled as a causal Bayesian network (BN), any relevant LR can be automatically derived using any BN software application.

</details>

<details>

<summary>2021-06-10 00:43:30 - Loss function based second-order Jensen inequality and its application to particle variational inference</summary>

- *Futoshi Futami, Tomoharu Iwata, Naonori Ueda, Issei Sato, Masashi Sugiyama*

- `2106.05010v2` - [abs](http://arxiv.org/abs/2106.05010v2) - [pdf](http://arxiv.org/pdf/2106.05010v2)

> Bayesian model averaging, obtained as the expectation of a likelihood function by a posterior distribution, has been widely used for prediction, evaluation of uncertainty, and model selection. Various approaches have been developed to efficiently capture the information in the posterior distribution; one such approach is the optimization of a set of models simultaneously with interaction to ensure the diversity of the individual models in the same way as ensemble learning. A representative approach is particle variational inference (PVI), which uses an ensemble of models as an empirical approximation for the posterior distribution. PVI iteratively updates each model with a repulsion force to ensure the diversity of the optimized models. However, despite its promising performance, a theoretical understanding of this repulsion and its association with the generalization ability remains unclear. In this paper, we tackle this problem in light of PAC-Bayesian analysis. First, we provide a new second-order Jensen inequality, which has the repulsion term based on the loss function. Thanks to the repulsion term, it is tighter than the standard Jensen inequality. Then, we derive a novel generalization error bound and show that it can be reduced by enhancing the diversity of models. Finally, we derive a new PVI that optimizes the generalization error bound directly. Numerical experiments demonstrate that the performance of the proposed PVI compares favorably with existing methods in the experiment.

</details>

<details>

<summary>2021-06-10 04:10:59 - Think Global and Act Local: Bayesian Optimisation over High-Dimensional Categorical and Mixed Search Spaces</summary>

- *Xingchen Wan, Vu Nguyen, Huong Ha, Binxin Ru, Cong Lu, Michael A. Osborne*

- `2102.07188v2` - [abs](http://arxiv.org/abs/2102.07188v2) - [pdf](http://arxiv.org/pdf/2102.07188v2)

> High-dimensional black-box optimisation remains an important yet notoriously challenging problem. Despite the success of Bayesian optimisation methods on continuous domains, domains that are categorical, or that mix continuous and categorical variables, remain challenging. We propose a novel solution -- we combine local optimisation with a tailored kernel design, effectively handling high-dimensional categorical and mixed search spaces, whilst retaining sample efficiency. We further derive convergence guarantee for the proposed approach. Finally, we demonstrate empirically that our method outperforms the current baselines on a variety of synthetic and real-world tasks in terms of performance, computational costs, or both.

</details>

<details>

<summary>2021-06-10 05:54:46 - High-Dimensional Bayesian Optimization with Sparse Axis-Aligned Subspaces</summary>

- *David Eriksson, Martin Jankowiak*

- `2103.00349v2` - [abs](http://arxiv.org/abs/2103.00349v2) - [pdf](http://arxiv.org/pdf/2103.00349v2)

> Bayesian optimization (BO) is a powerful paradigm for efficient optimization of black-box objective functions. High-dimensional BO presents a particular challenge, in part because the curse of dimensionality makes it difficult to define -- as well as do inference over -- a suitable class of surrogate models. We argue that Gaussian process surrogate models defined on sparse axis-aligned subspaces offer an attractive compromise between flexibility and parsimony. We demonstrate that our approach, which relies on Hamiltonian Monte Carlo for inference, can rapidly identify sparse subspaces relevant to modeling the unknown objective function, enabling sample-efficient high-dimensional BO. In an extensive suite of experiments comparing to existing methods for high-dimensional BO we demonstrate that our algorithm, Sparse Axis-Aligned Subspace BO (SAASBO), achieves excellent performance on several synthetic and real-world problems without the need to set problem-specific hyperparameters.

</details>

<details>

<summary>2021-06-10 06:56:01 - An Interpretable Neural Network for Parameter Inference</summary>

- *Johann Pfitzinger*

- `2106.05536v1` - [abs](http://arxiv.org/abs/2106.05536v1) - [pdf](http://arxiv.org/pdf/2106.05536v1)

> Adoption of deep neural networks in fields such as economics or finance has been constrained by the lack of interpretability of model outcomes. This paper proposes a generative neural network architecture - the parameter encoder neural network (PENN) - capable of estimating local posterior distributions for the parameters of a regression model. The parameters fully explain predictions in terms of the inputs and permit visualization, interpretation and inference in the presence of complex heterogeneous effects and feature dependencies. The use of Bayesian inference techniques offers an intuitive mechanism to regularize local parameter estimates towards a stable solution, and to reduce noise-fitting in settings of limited data availability. The proposed neural network is particularly well-suited to applications in economics and finance, where parameter inference plays an important role. An application to an asset pricing problem demonstrates how the PENN can be used to explore nonlinear risk dynamics in financial markets, and to compare empirical nonlinear effects to behavior posited by financial theory.

</details>

<details>

<summary>2021-06-10 07:05:13 - Development and Realization of Validation Benchmarks</summary>

- *Farid Mohammadi*

- `2011.13216v2` - [abs](http://arxiv.org/abs/2011.13216v2) - [pdf](http://arxiv.org/pdf/2011.13216v2)

> In the field of modeling, the word validation refers to simple comparisons between model outputs and experimental data. Usually, this comparison constitutes plotting the model results against data on the same axes to provide a visual assessment of agreement or lack thereof. However, there are a number of concerns with such naive comparisons. First, these comparisons tend to provide qualitative rather than quantitative assessments and are clearly insufficient for making decisions regarding model validity. Second, they often disregard or only partly account for existing uncertainties in the experimental observations or the model input parameters. Third, such comparisons can not reveal whether the model is appropriate for the intended purposes, as they mainly focus on the agreement in the observable quantities.   These pitfalls give rise to the need for an uncertainty-aware framework that includes a validation metric. This metric shall provide a measure for comparison of the system response quantities of an experiment with the ones from a computational model, while accounting for uncertainties in both. To address this need, we have developed a statistical framework that incorporates a probabilistic modeling technique using a fully Bayesian approach. A Bayesian perspective yields an optimal bias-variance trade-off against the experimental data and provide an integrative metric for model validation that incorporates parameter and conceptual uncertainty. Additionally, to accelerate the analysis for computationally demanding flow and transport models in porous media, the framework is equipped with a model reduction technique, namely Bayesian Sparse Polynomial Chaos Expansion. We demonstrate the capabilities of the aforementioned Bayesian validation framework by applying it to an application for validation as well as uncertainty quantification of fluid flow in fractured porous media.

</details>

<details>

<summary>2021-06-10 08:21:00 - Learning Nonparametric Volterra Kernels with Gaussian Processes</summary>

- *Magnus Ross, Michael T. Smith, Mauricio A. Álvarez*

- `2106.05582v1` - [abs](http://arxiv.org/abs/2106.05582v1) - [pdf](http://arxiv.org/pdf/2106.05582v1)

> This paper introduces a method for the nonparametric Bayesian learning of nonlinear operators, through the use of the Volterra series with kernels represented using Gaussian processes (GPs), which we term the nonparametric Volterra kernels model (NVKM). When the input function to the operator is unobserved and has a GP prior, the NVKM constitutes a powerful method for both single and multiple output regression, and can be viewed as a nonlinear and nonparametric latent force model. When the input function is observed, the NVKM can be used to perform Bayesian system identification. We use recent advances in efficient sampling of explicit functions from GPs to map process realisations through the Volterra series without resorting to numerical integration, allowing scalability through doubly stochastic variational inference, and avoiding the need for Gaussian approximations of the output processes. We demonstrate the performance of the model for both multiple output regression and system identification using standard benchmarks.

</details>

<details>

<summary>2021-06-10 09:06:42 - Bayesian Quadrature on Riemannian Data Manifolds</summary>

- *Christian Fröhlich, Alexandra Gessner, Philipp Hennig, Bernhard Schölkopf, Georgios Arvanitidis*

- `2102.06645v2` - [abs](http://arxiv.org/abs/2102.06645v2) - [pdf](http://arxiv.org/pdf/2102.06645v2)

> Riemannian manifolds provide a principled way to model nonlinear geometric structure inherent in data. A Riemannian metric on said manifolds determines geometry-aware shortest paths and provides the means to define statistical models accordingly. However, these operations are typically computationally demanding. To ease this computational burden, we advocate probabilistic numerical methods for Riemannian statistics. In particular, we focus on Bayesian quadrature (BQ) to numerically compute integrals over normal laws on Riemannian manifolds learned from data. In this task, each function evaluation relies on the solution of an expensive initial value problem. We show that by leveraging both prior knowledge and an active exploration scheme, BQ significantly reduces the number of required evaluations and thus outperforms Monte Carlo methods on a wide range of integration problems. As a concrete application, we highlight the merits of adopting Riemannian geometry with our proposed framework on a nonlinear dataset from molecular dynamics.

</details>

<details>

<summary>2021-06-10 15:37:38 - Bayesian semi-parametric inference for diffusion processes using splines</summary>

- *Paul A. Jenkins, Murray Pollock, Gareth O. Roberts*

- `2106.05820v1` - [abs](http://arxiv.org/abs/2106.05820v1) - [pdf](http://arxiv.org/pdf/2106.05820v1)

> We introduce a semi-parametric method to simultaneously infer both the drift and volatility functions of a discretely observed scalar diffusion. We introduce spline bases to represent these functions and develop a Markov chain Monte Carlo algorithm to infer, a posteriori, the coefficients of these functions in the spline basis. A key innovation is that we use spline bases to model transformed versions of the drift and volatility functions rather than the functions themselves. The output of the algorithm is a posterior sample of plausible drift and volatility functions that are not constrained to any particular parametric family. The flexibility of this approach provides practitioners a powerful investigative tool, allowing them to posit parametric models to better capture the underlying dynamics of their processes of interest. We illustrate the versatility of our method by applying it to challenging datasets from finance, paleoclimatology, and astrophysics. In view of the parametric diffusion models widely employed in the literature for those examples, some of our results are surprising since they call into question some aspects of these models.

</details>

<details>

<summary>2021-06-10 21:49:23 - RNN with Particle Flow for Probabilistic Spatio-temporal Forecasting</summary>

- *Soumyasundar Pal, Liheng Ma, Yingxue Zhang, Mark Coates*

- `2106.06064v1` - [abs](http://arxiv.org/abs/2106.06064v1) - [pdf](http://arxiv.org/pdf/2106.06064v1)

> Spatio-temporal forecasting has numerous applications in analyzing wireless, traffic, and financial networks. Many classical statistical models often fall short in handling the complexity and high non-linearity present in time-series data. Recent advances in deep learning allow for better modelling of spatial and temporal dependencies. While most of these models focus on obtaining accurate point forecasts, they do not characterize the prediction uncertainty. In this work, we consider the time-series data as a random realization from a nonlinear state-space model and target Bayesian inference of the hidden states for probabilistic forecasting. We use particle flow as the tool for approximating the posterior distribution of the states, as it is shown to be highly effective in complex, high-dimensional settings. Thorough experimentation on several real world time-series datasets demonstrates that our approach provides better characterization of uncertainty while maintaining comparable accuracy to the state-of-the art point forecasting methods.

</details>

<details>

<summary>2021-06-10 22:44:37 - A Nonmyopic Approach to Cost-Constrained Bayesian Optimization</summary>

- *Eric Hans Lee, David Eriksson, Valerio Perrone, Matthias Seeger*

- `2106.06079v1` - [abs](http://arxiv.org/abs/2106.06079v1) - [pdf](http://arxiv.org/pdf/2106.06079v1)

> Bayesian optimization (BO) is a popular method for optimizing expensive-to-evaluate black-box functions. BO budgets are typically given in iterations, which implicitly assumes each evaluation has the same cost. In fact, in many BO applications, evaluation costs vary significantly in different regions of the search space. In hyperparameter optimization, the time spent on neural network training increases with layer size; in clinical trials, the monetary cost of drug compounds vary; and in optimal control, control actions have differing complexities. Cost-constrained BO measures convergence with alternative cost metrics such as time, money, or energy, for which the sample efficiency of standard BO methods is ill-suited. For cost-constrained BO, cost efficiency is far more important than sample efficiency. In this paper, we formulate cost-constrained BO as a constrained Markov decision process (CMDP), and develop an efficient rollout approximation to the optimal CMDP policy that takes both the cost and future iterations into account. We validate our method on a collection of hyperparameter optimization problems as well as a sensor set selection application.

</details>

<details>

<summary>2021-06-11 00:20:27 - Sparse Bayesian Learning via Stepwise Regression</summary>

- *Sebastian Ament, Carla Gomes*

- `2106.06095v1` - [abs](http://arxiv.org/abs/2106.06095v1) - [pdf](http://arxiv.org/pdf/2106.06095v1)

> Sparse Bayesian Learning (SBL) is a powerful framework for attaining sparsity in probabilistic models. Herein, we propose a coordinate ascent algorithm for SBL termed Relevance Matching Pursuit (RMP) and show that, as its noise variance parameter goes to zero, RMP exhibits a surprising connection to Stepwise Regression. Further, we derive novel guarantees for Stepwise Regression algorithms, which also shed light on RMP. Our guarantees for Forward Regression improve on deterministic and probabilistic results for Orthogonal Matching Pursuit with noise. Our analysis of Backward Regression on determined systems culminates in a bound on the residual of the optimal solution to the subset selection problem that, if satisfied, guarantees the optimality of the result. To our knowledge, this bound is the first that can be computed in polynomial time and depends chiefly on the smallest singular value of the matrix. We report numerical experiments using a variety of feature selection algorithms. Notably, RMP and its limiting variant are both efficient and maintain strong performance with correlated features.

</details>

<details>

<summary>2021-06-11 06:10:06 - Active Learning of Continuous-time Bayesian Networks through Interventions</summary>

- *Dominik Linzner, Heinz Koeppl*

- `2105.14742v2` - [abs](http://arxiv.org/abs/2105.14742v2) - [pdf](http://arxiv.org/pdf/2105.14742v2)

> We consider the problem of learning structures and parameters of Continuous-time Bayesian Networks (CTBNs) from time-course data under minimal experimental resources. In practice, the cost of generating experimental data poses a bottleneck, especially in the natural and social sciences. A popular approach to overcome this is Bayesian optimal experimental design (BOED). However, BOED becomes infeasible in high-dimensional settings, as it involves integration over all possible experimental outcomes. We propose a novel criterion for experimental design based on a variational approximation of the expected information gain. We show that for CTBNs, a semi-analytical expression for this criterion can be calculated for structure and parameter learning. By doing so, we can replace sampling over experimental outcomes by solving the CTBNs master-equation, for which scalable approximations exist. This alleviates the computational burden of sampling possible experimental outcomes in high-dimensions. We employ this framework in order to recommend interventional sequences. In this context, we extend the CTBN model to conditional CTBNs in order to incorporate interventions. We demonstrate the performance of our criterion on synthetic and real-world data.

</details>

<details>

<summary>2021-06-11 07:21:29 - Continuous-Time Model-Based Reinforcement Learning</summary>

- *Çağatay Yıldız, Markus Heinonen, Harri Lähdesmäki*

- `2102.04764v3` - [abs](http://arxiv.org/abs/2102.04764v3) - [pdf](http://arxiv.org/pdf/2102.04764v3)

> Model-based reinforcement learning (MBRL) approaches rely on discrete-time state transition models whereas physical systems and the vast majority of control tasks operate in continuous-time. To avoid time-discretization approximation of the underlying process, we propose a continuous-time MBRL framework based on a novel actor-critic method. Our approach also infers the unknown state evolution differentials with Bayesian neural ordinary differential equations (ODE) to account for epistemic uncertainty. We implement and test our method on a new ODE-RL suite that explicitly solves continuous-time control systems. Our experiments illustrate that the model is robust against irregular and noisy data, is sample-efficient, and can solve control problems which pose challenges to discrete-time MBRL methods.

</details>

<details>

<summary>2021-06-11 08:31:26 - Noise and Fluctuation of Finite Learning Rate Stochastic Gradient Descent</summary>

- *Kangqiao Liu, Liu Ziyin, Masahito Ueda*

- `2012.03636v4` - [abs](http://arxiv.org/abs/2012.03636v4) - [pdf](http://arxiv.org/pdf/2012.03636v4)

> In the vanishing learning rate regime, stochastic gradient descent (SGD) is now relatively well understood. In this work, we propose to study the basic properties of SGD and its variants in the non-vanishing learning rate regime. The focus is on deriving exactly solvable results and discussing their implications. The main contributions of this work are to derive the stationary distribution for discrete-time SGD in a quadratic loss function with and without momentum; in particular, one implication of our result is that the fluctuation caused by discrete-time dynamics takes a distorted shape and is dramatically larger than a continuous-time theory could predict. Examples of applications of the proposed theory considered in this work include the approximation error of variants of SGD, the effect of minibatch noise, the optimal Bayesian inference, the escape rate from a sharp minimum, and the stationary covariance of a few second-order methods including damped Newton's method, natural gradient descent, and Adam.

</details>

<details>

<summary>2021-06-11 08:55:00 - Model Selection for Bayesian Autoencoders</summary>

- *Ba-Hien Tran, Simone Rossi, Dimitrios Milios, Pietro Michiardi, Edwin V. Bonilla, Maurizio Filippone*

- `2106.06245v1` - [abs](http://arxiv.org/abs/2106.06245v1) - [pdf](http://arxiv.org/pdf/2106.06245v1)

> We develop a novel method for carrying out model selection for Bayesian autoencoders (BAEs) by means of prior hyper-parameter optimization. Inspired by the common practice of type-II maximum likelihood optimization and its equivalence to Kullback-Leibler divergence minimization, we propose to optimize the distributional sliced-Wasserstein distance (DSWD) between the output of the autoencoder and the empirical data distribution. The advantages of this formulation are that we can estimate the DSWD based on samples and handle high-dimensional problems. We carry out posterior estimation of the BAE parameters via stochastic gradient Hamiltonian Monte Carlo and turn our BAE into a generative model by fitting a flexible Dirichlet mixture model in the latent space. Consequently, we obtain a powerful alternative to variational autoencoders, which are the preferred choice in modern applications of autoencoders for representation learning with uncertainty. We evaluate our approach qualitatively and quantitatively using a vast experimental campaign on a number of unsupervised learning tasks and show that, in small-data regimes where priors matter, our approach provides state-of-the-art results, outperforming multiple competitive baselines.

</details>

<details>

<summary>2021-06-11 11:15:36 - Asynchronous ε-Greedy Bayesian Optimisation</summary>

- *George De Ath, Richard M. Everson, Jonathan E. Fieldsend*

- `2010.07615v4` - [abs](http://arxiv.org/abs/2010.07615v4) - [pdf](http://arxiv.org/pdf/2010.07615v4)

> Batch Bayesian optimisation (BO) is a successful technique for the optimisation of expensive black-box functions. Asynchronous BO can reduce wallclock time by starting a new evaluation as soon as another finishes, thus maximising resource utilisation. To maximise resource allocation, we develop a novel asynchronous BO method, AEGiS (Asynchronous $\epsilon$-Greedy Global Search) that combines greedy search, exploiting the surrogate's mean prediction, with Thompson sampling and random selection from the approximate Pareto set describing the trade-off between exploitation (surrogate mean prediction) and exploration (surrogate posterior variance). We demonstrate empirically the efficacy of AEGiS on synthetic benchmark problems, meta-surrogate hyperparameter tuning problems and real-world problems, showing that AEGiS generally outperforms existing methods for asynchronous BO. When a single worker is available performance is no worse than BO using expected improvement.

</details>

<details>

<summary>2021-06-11 12:14:38 - Structured Bayesian variable selection for multiple correlated response variables and high-dimensional predictors</summary>

- *Zhi Zhao, Marco Banterle, Alex Lewin, Manuela Zucknick*

- `2101.05899v2` - [abs](http://arxiv.org/abs/2101.05899v2) - [pdf](http://arxiv.org/pdf/2101.05899v2)

> It is becoming increasingly common to study complex associations between multiple phenotypes and high-dimensional genomic features in biomedicine. However, it requires flexible and efficient joint statistical models if there are correlations between multiple response variables and between high-dimensional predictors. We propose a structured multivariate Bayesian variable selection model to identify sparse predictors associated with multiple correlated response variables. The approach makes use of known structure information between the multiple response variables and high-dimensional predictors via a Markov random field (MRF) prior for the latent indicator variables of the coefficient matrix of a sparse seemingly unrelated regressions (SSUR). The structure information included in the MRF prior can improve the model performance (i.e., variable selection and response prediction) compared to other common priors. In addition, we employ random effects to capture heterogeneity of grouped samples. The proposed approach is validated by simulation studies and applied to a pharmacogenomic study which includes pharmacological profiling and multi-omics data (i.e., gene expression, copy number variation and mutation) from in vitro anti-cancer drug sensitivity screening.

</details>

<details>

<summary>2021-06-11 12:18:18 - Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design</summary>

- *Adam Foster, Desi R. Ivanova, Ilyas Malik, Tom Rainforth*

- `2103.02438v2` - [abs](http://arxiv.org/abs/2103.02438v2) - [pdf](http://arxiv.org/pdf/2103.02438v2)

> We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of adaptive Bayesian experimental design that allows experiments to be run in real-time. Traditional sequential Bayesian optimal experimental design approaches require substantial computation at each stage of the experiment. This makes them unsuitable for most real-world applications, where decisions must typically be made quickly. DAD addresses this restriction by learning an amortized design network upfront and then using this to rapidly run (multiple) adaptive experiments at deployment time. This network represents a design policy which takes as input the data from previous steps, and outputs the next design using a single forward pass; these design decisions can be made in milliseconds during the live experiment. To train the network, we introduce contrastive information bounds that are suitable objectives for the sequential setting, and propose a customized network architecture that exploits key symmetries. We demonstrate that DAD successfully amortizes the process of experimental design, outperforming alternative strategies on a number of problems.

</details>

<details>

<summary>2021-06-11 15:37:17 - Modeling the Marked Presence-only Data: A Case Study of Estimating the Female Sex Worker Size in Malawi</summary>

- *Ian Laga, Xiaoyue Niu, Le Bao*

- `1912.07099v3` - [abs](http://arxiv.org/abs/1912.07099v3) - [pdf](http://arxiv.org/pdf/1912.07099v3)

> Certain subpopulations like female sex workers (FSW), men who have sex with men (MSM), and people who inject drugs (PWID) often have higher prevalence of HIV/AIDS and are difficult to map directly due to stigma, discrimination, and criminalization. Fine-scale mapping of those populations contributes to the progress towards reducing the inequalities and ending the AIDS epidemic. In 2016 and 2017, the PLACE surveys were conducted at 3,290 venues in 20 out of the total 28 districts in Malawi to estimate the FSW sizes. These venues represent a presence-only data set where, instead of knowing both where people live and do not live (presence-absence data), only information about visited locations is available. In this study, we develop a Bayesian model for presence-only data and utilize the PLACE data to estimate the FSW size and uncertainty interval at a $1.5 \times 1.5$-km resolution for all of Malawi. The estimates can also be aggregated to any desirable level (city/district/region) for implementing targeted HIV prevention and treatment programs in FSW communities, which have been successful in lowering the incidence of HIV and other sexually transmitted infections.

</details>

<details>

<summary>2021-06-11 17:19:34 - Divide-and-Conquer MCMC for Multivariate Binary Data</summary>

- *Suchit Mehrotra, Halley Brantley, Peter Onglao, Patricia Bata, Roland Romero, Jacob Westman, Lauren Bangerter, Arnab Maity*

- `2102.09008v3` - [abs](http://arxiv.org/abs/2102.09008v3) - [pdf](http://arxiv.org/pdf/2102.09008v3)

> The analysis of large scale medical claims data has the potential to improve quality of care by generating insights which can be used to create tailored medical programs. In particular, the multivariate probit model can be used to investigate the correlation between multiple binary responses of interest in such data, e.g. the presence of multiple chronic conditions. Bayesian modeling is well suited to such analyses because of the automatic uncertainty quantification provided by the posterior distribution. A complicating factor is that large medical claims datasets often do not fit in memory, which renders the estimation of the posterior using traditional Markov Chain Monte Carlo (MCMC) methods computationally infeasible. To address this challenge, we extend existing divide-and-conquer MCMC algorithms to the multivariate probit model, demonstrating, via simulation, that they should be preferred over mean-field variational inference when the estimation of the latent correlation structure between binary responses is of primary interest. We apply this algorithm to a large database of de-identified Medicare Advantage claims from a single large US health insurance provider, where we find medically meaningful groupings of common chronic conditions and asses the impact of the urban-rural health gap by identifying underutilized provider specialties in rural areas.

</details>

<details>

<summary>2021-06-11 17:31:02 - Simple and Efficient Hard Label Black-box Adversarial Attacks in Low Query Budget Regimes</summary>

- *Satya Narayan Shukla, Anit Kumar Sahu, Devin Willmott, J. Zico Kolter*

- `2007.07210v2` - [abs](http://arxiv.org/abs/2007.07210v2) - [pdf](http://arxiv.org/pdf/2007.07210v2)

> We focus on the problem of black-box adversarial attacks, where the aim is to generate adversarial examples for deep learning models solely based on information limited to output label~(hard label) to a queried data input. We propose a simple and efficient Bayesian Optimization~(BO) based approach for developing black-box adversarial attacks. Issues with BO's performance in high dimensions are avoided by searching for adversarial examples in a structured low-dimensional subspace. We demonstrate the efficacy of our proposed attack method by evaluating both $\ell_\infty$ and $\ell_2$ norm constrained untargeted and targeted hard label black-box attacks on three standard datasets - MNIST, CIFAR-10 and ImageNet. Our proposed approach consistently achieves 2x to 10x higher attack success rate while requiring 10x to 20x fewer queries compared to the current state-of-the-art black-box adversarial attacks.

</details>

<details>

<summary>2021-06-12 06:13:39 - Variational Auto-Regressive Gaussian Processes for Continual Learning</summary>

- *Sanyam Kapoor, Theofanis Karaletsos, Thang D. Bui*

- `2006.05468v3` - [abs](http://arxiv.org/abs/2006.05468v3) - [pdf](http://arxiv.org/pdf/2006.05468v3)

> Through sequential construction of posteriors on observing data online, Bayes' theorem provides a natural framework for continual learning. We develop Variational Auto-Regressive Gaussian Processes (VAR-GPs), a principled posterior updating mechanism to solve sequential tasks in continual learning. By relying on sparse inducing point approximations for scalable posteriors, we propose a novel auto-regressive variational distribution which reveals two fruitful connections to existing results in Bayesian inference, expectation propagation and orthogonal inducing points. Mean predictive entropy estimates show VAR-GPs prevent catastrophic forgetting, which is empirically supported by strong performance on modern continual learning benchmarks against competitive baselines. A thorough ablation study demonstrates the efficacy of our modeling choices.

</details>

<details>

<summary>2021-06-12 12:59:06 - Quantifying Uncertainty in Deep Spatiotemporal Forecasting</summary>

- *Dongxia Wu, Liyao Gao, Xinyue Xiong, Matteo Chinazzi, Alessandro Vespignani, Yi-An Ma, Rose Yu*

- `2105.11982v2` - [abs](http://arxiv.org/abs/2105.11982v2) - [pdf](http://arxiv.org/pdf/2105.11982v2)

> Deep learning is gaining increasing popularity for spatiotemporal forecasting. However, prior works have mostly focused on point estimates without quantifying the uncertainty of the predictions. In high stakes domains, being able to generate probabilistic forecasts with confidence intervals is critical to risk assessment and decision making. Hence, a systematic study of uncertainty quantification (UQ) methods for spatiotemporal forecasting is missing in the community. In this paper, we describe two types of spatiotemporal forecasting problems: regular grid-based and graph-based. Then we analyze UQ methods from both the Bayesian and the frequentist point of view, casting in a unified framework via statistical decision theory. Through extensive experiments on real-world road network traffic, epidemics, and air quality forecasting tasks, we reveal the statistical and computational trade-offs for different UQ methods: Bayesian methods are typically more robust in mean prediction, while confidence levels obtained from frequentist methods provide more extensive coverage over data variations. Computationally, quantile regression type methods are cheaper for a single confidence interval but require re-training for different intervals. Sampling based methods generate samples that can form multiple confidence intervals, albeit at a higher computational cost.

</details>

<details>

<summary>2021-06-12 14:41:28 - Graph-based Prior and Forward Models for Inverse Problems on Manifolds with Boundaries</summary>

- *John Harlim, Shixiao Jiang, Hwanwoo Kim, Daniel Sanz-Alonso*

- `2106.06787v1` - [abs](http://arxiv.org/abs/2106.06787v1) - [pdf](http://arxiv.org/pdf/2106.06787v1)

> This paper develops manifold learning techniques for the numerical solution of PDE-constrained Bayesian inverse problems on manifolds with boundaries. We introduce graphical Mat\'ern-type Gaussian field priors that enable flexible modeling near the boundaries, representing boundary values by superposition of harmonic functions with appropriate Dirichlet boundary conditions. We also investigate the graph-based approximation of forward models from PDE parameters to observed quantities. In the construction of graph-based prior and forward models, we leverage the ghost point diffusion map algorithm to approximate second-order elliptic operators with classical boundary conditions. Numerical results validate our graph-based approach and demonstrate the need to design prior covariance models that account for boundary conditions.

</details>

<details>

<summary>2021-06-12 15:50:02 - Event Outlier Detection in Continuous Time</summary>

- *Siqi Liu, Milos Hauskrecht*

- `1912.09522v3` - [abs](http://arxiv.org/abs/1912.09522v3) - [pdf](http://arxiv.org/pdf/1912.09522v3)

> Continuous-time event sequences represent discrete events occurring in continuous time. Such sequences arise frequently in real-life. Usually we expect the sequences to follow some regular pattern over time. However, sometimes these patterns may be interrupted by unexpected absence or occurrences of events. Identification of these unexpected cases can be very important as they may point to abnormal situations that need human attention. In this work, we study and develop methods for detecting outliers in continuous-time event sequences, including unexpected absence and unexpected occurrences of events. Since the patterns that event sequences tend to follow may change in different contexts, we develop outlier detection methods based on point processes that can take context information into account. Our methods are based on Bayesian decision theory and hypothesis testing with theoretical guarantees. To test the performance of the methods, we conduct experiments on both synthetic data and real-world clinical data and show the effectiveness of the proposed methods.

</details>

<details>

<summary>2021-06-13 00:09:16 - Librispeech Transducer Model with Internal Language Model Prior Correction</summary>

- *Albert Zeyer, André Merboldt, Wilfried Michel, Ralf Schlüter, Hermann Ney*

- `2104.03006v2` - [abs](http://arxiv.org/abs/2104.03006v2) - [pdf](http://arxiv.org/pdf/2104.03006v2)

> We present our transducer model on Librispeech. We study variants to include an external language model (LM) with shallow fusion and subtract an estimated internal LM. This is justified by a Bayesian interpretation where the transducer model prior is given by the estimated internal LM. The subtraction of the internal LM gives us over 14% relative improvement over normal shallow fusion. Our transducer has a separate probability distribution for the non-blank labels which allows for easier combination with the external LM, and easier estimation of the internal LM. We additionally take care of including the end-of-sentence (EOS) probability of the external LM in the last blank probability which further improves the performance. All our code and setups are published.

</details>

<details>

<summary>2021-06-13 01:47:22 - A Large Deviation Approach to Posterior Consistency in Dynamical Systems</summary>

- *Langxuan Su, Sayan Mukherjee*

- `2106.06894v1` - [abs](http://arxiv.org/abs/2106.06894v1) - [pdf](http://arxiv.org/pdf/2106.06894v1)

> In this paper, we provide asymptotic results concerning (generalized) Bayesian inference for certain dynamical systems based on a large deviation approach. Given a sequence of observations $y$, a class of model processes parameterized by $\theta \in \Theta$ which can be characterized as a stochastic process $X^\theta$ or a measure $\mu_\theta$, and a loss function $L$ which measures the error between $y$ and a realization of $X^\theta$, we specify the generalized posterior distribution $\pi_t(\theta \mid y)$. The goal of this paper is to study the asymptotic behavior of $\pi_t(\theta \mid y)$ as $t \to \infty.$ In particular, we state conditions on the model family $\{\mu_\theta\}_{\theta \in \Theta}$ and the loss function $L$ such that the posterior distribution converges. The two conditions we require are: (1) a conditional large deviation behavior for a single $X^\theta$, and (2) an exponential continuity condition over the model family for the map from the parameter $\theta$ to the loss incurred between $X^\theta$ and the observation sequence $y$. The proposed framework is quite general, we apply it to two very different classes of dynamical systems: continuous time hypermixing processes and Gibbs processes on shifts of finite type. We also show that the generalized posterior distribution concentrates asymptotically on those parameters that minimize the expected loss and a divergence term, hence proving posterior consistency.

</details>

<details>

<summary>2021-06-13 03:01:31 - Unlabeled Data Help in Graph-Based Semi-Supervised Learning: A Bayesian Nonparametrics Perspective</summary>

- *Daniel Sanz-Alonso, Ruiyi Yang*

- `2008.11809v3` - [abs](http://arxiv.org/abs/2008.11809v3) - [pdf](http://arxiv.org/pdf/2008.11809v3)

> In this paper we analyze the graph-based approach to semi-supervised learning under a manifold assumption. We adopt a Bayesian perspective and demonstrate that, for a suitable choice of prior constructed with sufficiently many unlabeled data, the posterior contracts around the truth at a rate that is minimax optimal up to a logarithmic factor. Our theory covers both regression and classification.

</details>

<details>

<summary>2021-06-13 09:11:58 - Bayesian Inference Gaussian Process Multiproxy Alignment of Continuous Signals (BIGMACS): Applications for Paleoceanography</summary>

- *Taehee Lee, Lorraine E. Lisiecki, Devin Rand, Geoffrey Gebbie, Charles E. Lawrence*

- `1907.08738v4` - [abs](http://arxiv.org/abs/1907.08738v4) - [pdf](http://arxiv.org/pdf/1907.08738v4)

> We first introduce a novel profile-based alignment algorithm, the multiple continuous Signal Alignment algorithm with Gaussian Process Regression profiles (SA-GPR). SA-GPR addresses the limitations of currently available signal alignment methods by adopting a hybrid of the particle smoothing and Markov-chain Monte Carlo (MCMC) algorithms to align signals, and by applying the Gaussian process regression to construct profiles to be aligned continuously. SA-GPR shares all the strengths of the existing alignment algorithms that depend on profiles but is more exact in the sense that profiles do not need to be discretized as sequential bins. The uncertainty of performance over the resolution of such bins is thereby eliminated. This methodology produces alignments that are consistent, that regularize extreme cases, and that properly reflect the inherent uncertainty.   Then we extend SA-GPR to a specific problem in the field of paleoceanography with a method called Bayesian Inference Gaussian Process Multiproxy Alignment of Continuous Signals (BIGMACS). The goal of BIGMACS is to infer continuous ages for ocean sediment cores using two classes of age proxies: proxies that explicitly return calendar ages (e.g., radiocarbon) and those used to synchronize ages in multiple marine records (e.g., an oxygen isotope based marine proxy known as benthic ${\delta}^{18}{\rm O}$). BIGMACS integrates these two proxies by iteratively performing two steps: profile construction from benthic ${\delta}^{18}{\rm O}$ age models and alignment of each core to the profile also reflecting radiocarbon dates. We use BIGMACS to construct a new Deep Northeastern Atlantic stack (i.e., a profile from a particular benthic ${\delta}^{18}{\rm O}$ records) of five ocean sediment cores. We conclude by constructing multiproxy age models for two additional cores from the same region by aligning them to the stack.

</details>

<details>

<summary>2021-06-13 09:20:25 - Bayesian Neural Networks for Virtual Flow Metering: An Empirical Study</summary>

- *Bjarne Grimstad, Mathilde Hotvedt, Anders T. Sandnes, Odd Kolbjørnsen, Lars S. Imsland*

- `2102.01391v3` - [abs](http://arxiv.org/abs/2102.01391v3) - [pdf](http://arxiv.org/pdf/2102.01391v3)

> Recent works have presented promising results from the application of machine learning (ML) to the modeling of flow rates in oil and gas wells. Encouraging results and advantageous properties of ML models, such as computationally cheap evaluation and ease of calibration to new data, have sparked optimism for the development of data-driven virtual flow meters (VFMs). Data-driven VFMs are developed in the small data regime, where it is important to question the uncertainty and robustness of models. The modeling of uncertainty may help to build trust in models, which is a prerequisite for industrial applications. The contribution of this paper is the introduction of a probabilistic VFM based on Bayesian neural networks. Uncertainty in the model and measurements is described, and the paper shows how to perform approximate Bayesian inference using variational inference. The method is studied by modeling on a large and heterogeneous dataset, consisting of 60 wells across five different oil and gas assets. The predictive performance is analyzed on historical and future test data, where an average error of 4-6% and 8-13% is achieved for the 50% best performing models, respectively. Variational inference appears to provide more robust predictions than the reference approach on future data. Prediction performance and uncertainty calibration is explored in detail and discussed in light of four data challenges. The findings motivate the development of alternative strategies to improve the robustness of data-driven VFMs.

</details>

<details>

<summary>2021-06-13 13:53:27 - Post-hoc loss-calibration for Bayesian neural networks</summary>

- *Meet P. Vadera, Soumya Ghosh, Kenney Ng, Benjamin M. Marlin*

- `2106.06997v1` - [abs](http://arxiv.org/abs/2106.06997v1) - [pdf](http://arxiv.org/pdf/2106.06997v1)

> Bayesian decision theory provides an elegant framework for acting optimally under uncertainty when tractable posterior distributions are available. Modern Bayesian models, however, typically involve intractable posteriors that are approximated with, potentially crude, surrogates. This difficulty has engendered loss-calibrated techniques that aim to learn posterior approximations that favor high-utility decisions. In this paper, focusing on Bayesian neural networks, we develop methods for correcting approximate posterior predictive distributions encouraging them to prefer high-utility decisions. In contrast to previous work, our approach is agnostic to the choice of the approximate inference algorithm, allows for efficient test time decision making through amortization, and empirically produces higher quality decisions. We demonstrate the effectiveness of our approach through controlled experiments spanning a diversity of tasks and datasets.

</details>

<details>

<summary>2021-06-13 16:24:44 - Deep Bayesian Unsupervised Lifelong Learning</summary>

- *Tingting Zhao, Zifeng Wang, Aria Masoomi, Jennifer Dy*

- `2106.07035v1` - [abs](http://arxiv.org/abs/2106.07035v1) - [pdf](http://arxiv.org/pdf/2106.07035v1)

> Lifelong Learning (LL) refers to the ability to continually learn and solve new problems with incremental available information over time while retaining previous knowledge. Much attention has been given lately to Supervised Lifelong Learning (SLL) with a stream of labelled data. In contrast, we focus on resolving challenges in Unsupervised Lifelong Learning (ULL) with streaming unlabelled data when the data distribution and the unknown class labels evolve over time. Bayesian framework is natural to incorporate past knowledge and sequentially update the belief with new data. We develop a fully Bayesian inference framework for ULL with a novel end-to-end Deep Bayesian Unsupervised Lifelong Learning (DBULL) algorithm, which can progressively discover new clusters without forgetting the past with unlabelled data while learning latent representations. To efficiently maintain past knowledge, we develop a novel knowledge preservation mechanism via sufficient statistics of the latent representation for raw data. To detect the potential new clusters on the fly, we develop an automatic cluster discovery and redundancy removal strategy in our inference inspired by Nonparametric Bayesian statistics techniques. We demonstrate the effectiveness of our approach using image and text corpora benchmark datasets in both LL and batch settings.

</details>

<details>

<summary>2021-06-13 17:36:38 - Wide Mean-Field Variational Bayesian Neural Networks Ignore the Data</summary>

- *Beau Coker, Weiwei Pan, Finale Doshi-Velez*

- `2106.07052v1` - [abs](http://arxiv.org/abs/2106.07052v1) - [pdf](http://arxiv.org/pdf/2106.07052v1)

> Variational inference enables approximate posterior inference of the highly over-parameterized neural networks that are popular in modern machine learning. Unfortunately, such posteriors are known to exhibit various pathological behaviors. We prove that as the number of hidden units in a single-layer Bayesian neural network tends to infinity, the function-space posterior mean under mean-field variational inference actually converges to zero, completely ignoring the data. This is in contrast to the true posterior, which converges to a Gaussian process. Our work provides insight into the over-regularization of the KL divergence in variational inference.

</details>

<details>

<summary>2021-06-14 01:23:52 - Bayesian Spatial Homogeneity Pursuit of Functional Data: an Application to the U.S. Income Distribution</summary>

- *Guanyu Hu, Junxian Geng, Yishu Xue, Huiyan Sang*

- `2002.06663v3` - [abs](http://arxiv.org/abs/2002.06663v3) - [pdf](http://arxiv.org/pdf/2002.06663v3)

> An income distribution describes how an entity's total wealth is distributed amongst its population. A problem of interest to regional economics researchers is to understand the spatial homogeneity of income distributions among different regions. In economics, the Lorenz curve is a well-known functional representation of income distribution. In this article, we propose a mixture of finite mixtures (MFM) model as well as a Markov random field constrained mixture of finite mixtures (MRFC-MFM) model in the context of spatial functional data analysis to capture spatial homogeneity of Lorenz curves. We design efficient Markov chain Monte Carlo (MCMC) algorithms to simultaneously infer the posterior distributions of the number of clusters and the clustering configuration of spatial functional data. Extensive simulation studies are carried out to show the effectiveness of the proposed methods compared with existing methods. We apply the proposed spatial functional clustering method to state level income Lorenz curves from the American Community Survey Public Use Microdata Sample (PUMS) data. The results reveal a number of important clustering patterns of state-level income distributions across US.

</details>

<details>

<summary>2021-06-14 02:53:25 - Modelling Animal-Vehicle Collision Counts across Large Networks Using a Bayesian Hierarchical Model with Time-Varying Parameters</summary>

- *Krishna Murthy Gurumurthy, Zili Li, Kara M. Kockelman, Prateek Bansal*

- `2104.02289v2` - [abs](http://arxiv.org/abs/2104.02289v2) - [pdf](http://arxiv.org/pdf/2104.02289v2)

> Animal-vehicle collisions (AVCs) are common around the world and result in considerable loss of animal and human life, as well as significant property damage and regular insurance claims. Understanding their occurrence in relation to various contributing factors and being able to identify locations of high risk are valuable to AVC prevention, yielding economic, social and environmental cost savings. However, many challenges exist in the study of AVC datasets. These include seasonality of animal activity, unknown exposure (i.e., the number of animal crossings), very low AVC counts across most sections of extensive roadway networks, and computational burdens that come with discrete response analysis using large datasets. To overcome these challenges, a Bayesian hierarchical model is proposed where the exposure is modeled with nonparametric Dirichlet process, and the number of segment-level AVCs is assumed to follow a Binomial distribution. A P\'olya-Gamma augmented Gibbs sampler is derived to estimate the proposed model. By using the AVC data of multiple years across about 100,000 segments of state-controlled highways in Texas, U.S., it is demonstrated that the model is scalable to large datasets, with a preponderance of zeros and clear monthly seasonality in counts, while identifying high-risk locations (for application of design treatments, like separated animal crossings with fencing) and key explanatory factors based on segment-specific factors (such as changes in speed limit) can be done within the modelling framework, which provide useful information for policy-making purposes.

</details>

<details>

<summary>2021-06-14 12:16:04 - Conformal Bayesian Computation</summary>

- *Edwin Fong, Chris Holmes*

- `2106.06137v2` - [abs](http://arxiv.org/abs/2106.06137v2) - [pdf](http://arxiv.org/pdf/2106.06137v2)

> We develop scalable methods for producing conformal Bayesian predictive intervals with finite sample calibration guarantees. Bayesian posterior predictive distributions, $p(y \mid x)$, characterize subjective beliefs on outcomes of interest, $y$, conditional on predictors, $x$. Bayesian prediction is well-calibrated when the model is true, but the predictive intervals may exhibit poor empirical coverage when the model is misspecified, under the so called ${\cal{M}}$-open perspective. In contrast, conformal inference provides finite sample frequentist guarantees on predictive confidence intervals without the requirement of model fidelity. Using 'add-one-in' importance sampling, we show that conformal Bayesian predictive intervals are efficiently obtained from re-weighted posterior samples of model parameters. Our approach contrasts with existing conformal methods that require expensive refitting of models or data-splitting to achieve computational efficiency. We demonstrate the utility on a range of examples including extensions to partially exchangeable settings such as hierarchical models.

</details>

<details>

<summary>2021-06-14 14:39:55 - High-resolution population estimation using household survey data and building footprints</summary>

- *Gianluca Boo, Edith Darin, Douglas R Leasure, Claire A Dooley, Heather R Chamberlain, Attila N Lázár, Kevin Tschirhart, Cyrus Sinai, Nicole A Hoff, Trevon Fuller, Kamy Musene, Arly Batumbo, Anne W Rimoin, Andrew J Tatem*

- `2106.07461v1` - [abs](http://arxiv.org/abs/2106.07461v1) - [pdf](http://arxiv.org/pdf/2106.07461v1)

> The national census is an essential data source to support decision-making in many areas of public interest. However, this data may become outdated during the intercensal period, which can stretch up to several decades. We developed a Bayesian hierarchical model leveraging recent household surveys with probabilistic sampling designs and building footprints to produce up-to-date population estimates. We estimated population totals and age and sex breakdowns with associated uncertainty measures within grid cells of approximately 100m in five provinces of the Democratic Republic of the Congo, a country where the last census was completed in 1984. The model exhibited a very good fit, with an R^2 value of 0.79 for out-of-sample predictions of population totals at the microcensus-cluster level and 1.00 for age and sex proportions at the province level. The results confirm the benefits of combining household surveys and building footprints for high-resolution population estimation in countries with outdated censuses.

</details>

<details>

<summary>2021-06-14 15:06:24 - Modeling satellite-based open water fraction via flexible Beta regression: An application to wetlands in the north-western Pacific coast of Mexico</summary>

- *Inder Tecuapetla-Gómez, Julia Trinidad Reyes*

- `2106.07478v1` - [abs](http://arxiv.org/abs/2106.07478v1) - [pdf](http://arxiv.org/pdf/2106.07478v1)

> Carbon sequestration and water filtering are two examples of the several ecosystem services provided by wetlands. Open water mapping is an effective means to measure any wetland extension as these are comprised of many open water bodies. An economical, though indirect, approach towards mapping open water bodies is through applying geo-computational methods to satellite images. In this work we propose the flexible Beta regression (FBR) model to predict open water fraction from measurements of a water index. We focus on observations derived from two MODIS images acquired during the dry season of 2008 in Marismas Nacionales, a wetland located in the north-western Pacific coast of Mexico. A Bayesian estimation procedure is presented to estimate the FBR model; in particular, we provide details of a nested Metropolis-Hastings and Gibbs sampling algorithm to carry out parameter estimation. Our results show that the FBR model produces valid predictors of water fraction unlike the standard model. Our work is complemented by software developed in the R language and available through a GitHub repository.

</details>

<details>

<summary>2021-06-14 16:18:59 - Maximum Probability Theorem: A Framework for Probabilistic Learning</summary>

- *Amir Emad Marvasti, Ehsan Emad Marvasti, Ulas Bagci, Hassan Foroosh*

- `1910.09417v5` - [abs](http://arxiv.org/abs/1910.09417v5) - [pdf](http://arxiv.org/pdf/1910.09417v5)

> We present a theoretical framework of probabilistic learning derived by Maximum Probability (MP) Theorem shown in the current paper. In this probabilistic framework, a model is defined as an event in the probability space, and a model or the associated event -- either the true underlying model or the parameterized model -- have a quantified probability measure. This quantification of a model's probability measure is derived by the MP Theorem, in which we have shown that an event's probability measure has an upper-bound given its conditional distribution on an arbitrary random variable. Through this alternative framework, the notion of model parameters is encompassed in the definition of the model or the associated event. Therefore, this framework deviates from the conventional approach of assuming a prior on the model parameters. Instead, the regularizing effects of assuming prior over parameters is seen through maximizing probabilities of models or according to information theory, minimizing the information content of a model. The probability of a model in our framework is invariant to reparameterization and is solely dependent on the model's likelihood function. Also, rather than maximizing the posterior in a conventional Bayesian setting, the objective function in our alternative framework is defined as the probability of set operations (e.g. intersection) on the event of the true underlying model and the event of the model at hand. Our theoretical framework, as a derivation of MP theorem, adds clarity to probabilistic learning through solidifying the definition of probabilistic models, quantifying their probabilities, and providing a visual understanding of objective functions.

</details>

<details>

<summary>2021-06-14 17:31:30 - Bayesian hierarchical nonlinear modelling of intra-abdominal volume during pneumoperitoneum for laparoscopic surgery</summary>

- *Gabriel Calvo, Carmen Armero, Virgilio Gómez-Rubio, Guido Mazzinari*

- `2106.07626v1` - [abs](http://arxiv.org/abs/2106.07626v1) - [pdf](http://arxiv.org/pdf/2106.07626v1)

> Laparoscopy is an operation carried out in the abdomen or pelvis through small incisions with external visual control by a camera. This technique needs the abdomen to be insufflated with carbon dioxide to obtain a working space for surgical instruments' manipulation. Identifying the critical point at which insufflation should be limited is crucial to maximizing surgical working space and minimizing injurious effects. Bayesian nonlinear growth mixed-effects models are applied to data coming from a repeated measures design. This study allows to assess the relationship between the insufflation pressure and the intra--abdominal volume.

</details>

<details>

<summary>2021-06-14 17:52:49 - Variational Causal Networks: Approximate Bayesian Inference over Causal Structures</summary>

- *Yashas Annadani, Jonas Rothfuss, Alexandre Lacoste, Nino Scherrer, Anirudh Goyal, Yoshua Bengio, Stefan Bauer*

- `2106.07635v1` - [abs](http://arxiv.org/abs/2106.07635v1) - [pdf](http://arxiv.org/pdf/2106.07635v1)

> Learning the causal structure that underlies data is a crucial step towards robust real-world decision making. The majority of existing work in causal inference focuses on determining a single directed acyclic graph (DAG) or a Markov equivalence class thereof. However, a crucial aspect to acting intelligently upon the knowledge about causal structure which has been inferred from finite data demands reasoning about its uncertainty. For instance, planning interventions to find out more about the causal mechanisms that govern our data requires quantifying epistemic uncertainty over DAGs. While Bayesian causal inference allows to do so, the posterior over DAGs becomes intractable even for a small number of variables. Aiming to overcome this issue, we propose a form of variational inference over the graphs of Structural Causal Models (SCMs). To this end, we introduce a parametric variational family modelled by an autoregressive distribution over the space of discrete DAGs. Its number of parameters does not grow exponentially with the number of variables and can be tractably learned by maximising an Evidence Lower Bound (ELBO). In our experiments, we demonstrate that the proposed variational posterior is able to provide a good approximation of the true posterior.

</details>

<details>

<summary>2021-06-14 18:20:21 - Variational Inference with Continuously-Indexed Normalizing Flows</summary>

- *Anthony Caterini, Rob Cornish, Dino Sejdinovic, Arnaud Doucet*

- `2007.05426v2` - [abs](http://arxiv.org/abs/2007.05426v2) - [pdf](http://arxiv.org/pdf/2007.05426v2)

> Continuously-indexed flows (CIFs) have recently achieved improvements over baseline normalizing flows on a variety of density estimation tasks. CIFs do not possess a closed-form marginal density, and so, unlike standard flows, cannot be plugged in directly to a variational inference (VI) scheme in order to produce a more expressive family of approximate posteriors. However, we show here how CIFs can be used as part of an auxiliary VI scheme to formulate and train expressive posterior approximations in a natural way. We exploit the conditional independence structure of multi-layer CIFs to build the required auxiliary inference models, which we show empirically yield low-variance estimators of the model evidence. We then demonstrate the advantages of CIFs over baseline flows in VI problems when the posterior distribution of interest possesses a complicated topology, obtaining improved results in both the Bayesian inference and surrogate maximum likelihood settings.

</details>

<details>

<summary>2021-06-14 23:12:58 - Embracing Uncertainty in "Small Data" Problems: Estimating Earthquakes from Historical Anecdotes</summary>

- *Justin A. Krometis, Hayden Ringer, Jared P. Whitehead, Nathan E. Glatt-Holtz, Ronald A. Harris*

- `2106.07797v1` - [abs](http://arxiv.org/abs/2106.07797v1) - [pdf](http://arxiv.org/pdf/2106.07797v1)

> We apply the Bayesian inversion process to make principled estimates of the magnitude and location of a pre-instrumental earthquake in Eastern Indonesia in the mid 19th century, by combining anecdotal historical accounts of the resultant tsunami with our modern understanding of the geology of the region. Quantifying the seismic record prior to modern instrumentation is critical to a more thorough understanding of the current risks in Eastern Indonesia. In particular, the occurrence of such a major earthquake in the 1850s provides evidence that this region is susceptible to future seismic hazards on the same order of magnitude. More importantly, the approach taken here gives evidence that even "small data" that is limited in scope and extremely uncertain can still be used to yield information on past seismic events, which is key to an increased understanding of the current seismic state. Moreover, sensitivity bounds indicate that the results obtained here are robust despite the inherent uncertainty in the observations.

</details>

<details>

<summary>2021-06-15 06:40:32 - Machine learning-based conditional mean filter: a generalization of the ensemble Kalman filter for nonlinear data assimilation</summary>

- *Truong-Vinh Hoang, Sebastian Krumscheid, Hermann G. Matthies, Raúl Tempone*

- `2106.07908v1` - [abs](http://arxiv.org/abs/2106.07908v1) - [pdf](http://arxiv.org/pdf/2106.07908v1)

> Filtering is a data assimilation technique that performs the sequential inference of dynamical systems states from noisy observations. Herein, we propose a machine learning-based ensemble conditional mean filter (ML-EnCMF) for tracking possibly high-dimensional non-Gaussian state models with nonlinear dynamics based on sparse observations. The proposed filtering method is developed based on the conditional expectation and numerically implemented using machine learning (ML) techniques combined with the ensemble method. The contribution of this work is twofold. First, we demonstrate that the ensembles assimilated using the ensemble conditional mean filter (EnCMF) provide an unbiased estimator of the Bayesian posterior mean, and their variance matches the expected conditional variance. Second, we implement the EnCMF using artificial neural networks, which have a significant advantage in representing nonlinear functions over high-dimensional domains such as the conditional mean. Finally, we demonstrate the effectiveness of the ML-EnCMF for tracking the states of Lorenz-63 and Lorenz-96 systems under the chaotic regime. Numerical results show that the ML-EnCMF outperforms the ensemble Kalman filter.

</details>

<details>

<summary>2021-06-15 08:27:24 - Embarrassingly parallel MCMC using deep invertible transformations</summary>

- *Diego Mesquita, Paul Blomstedt, Samuel Kaski*

- `1903.04556v2` - [abs](http://arxiv.org/abs/1903.04556v2) - [pdf](http://arxiv.org/pdf/1903.04556v2)

> While MCMC methods have become a main work-horse for Bayesian inference, scaling them to large distributed datasets is still a challenge. Embarrassingly parallel MCMC strategies take a divide-and-conquer stance to achieve this by writing the target posterior as a product of subposteriors, running MCMC for each of them in parallel and subsequently combining the results. The challenge then lies in devising efficient aggregation strategies. Current strategies trade-off between approximation quality, and costs of communication and computation. In this work, we introduce a novel method that addresses these issues simultaneously. Our key insight is to introduce a deep invertible transformation to approximate each of the subposteriors. These approximations can be made accurate even for complex distributions and serve as intermediate representations, keeping the total communication cost limited. Moreover, they enable us to sample from the product of the subposteriors using an efficient and stable importance sampling scheme. We demonstrate the approach outperforms available state-of-the-art methods in a range of challenging scenarios, including high-dimensional and heterogeneous subposteriors.

</details>

<details>

<summary>2021-06-15 11:07:51 - Comparisons of Australian Mental Health Distributions</summary>

- *David Gunawan, William Griffiths, Duangkamon Chotikapanich*

- `2106.08047v1` - [abs](http://arxiv.org/abs/2106.08047v1) - [pdf](http://arxiv.org/pdf/2106.08047v1)

> Bayesian nonparametric estimates of Australian mental health distributions are obtained to assess how the mental health status of the population has changed over time and to compare the mental health status of female/male and indigenous/non-indigenous population subgroups. First- and second-order stochastic dominance are used to compare distributions, with results presented in terms of the posterior probability of dominance and the posterior probability of no dominance. Our results suggest mental health has deteriorated in recent years, that males mental health status is better than that of females, and non-indigenous health status is better than that of the indigenous population.

</details>

<details>

<summary>2021-06-15 15:22:36 - Treed distributed lag nonlinear models</summary>

- *Daniel Mork, Ander Wilson*

- `2010.06147v3` - [abs](http://arxiv.org/abs/2010.06147v3) - [pdf](http://arxiv.org/pdf/2010.06147v3)

> In studies of maternal exposure to air pollution a children's health outcome is regressed on exposures observed during pregnancy. The distributed lag nonlinear model (DLNM) is a statistical method commonly implemented to estimate an exposure-time-response function when it is postulated the exposure effect is nonlinear. Previous implementations of the DLNM estimate an exposure-time-response surface parameterized with a bivariate basis expansion. However, basis functions such as splines assume smoothness across the entire exposure-time-response surface, which may be unrealistic in settings where the exposure is associated with the outcome only in a specific time window. We propose a framework for estimating the DLNM based on Bayesian additive regression trees. Our method operates using a set of regression trees that each assume piecewise constant relationships across the exposure-time space. In a simulation, we show that our model outperforms spline-based models when the exposure-time surface is not smooth, while both methods perform similarly in settings where the true surface is smooth. Importantly, the proposed approach is lower variance and more precisely identifies critical windows during which exposure is associated with a future health outcome. We apply our method to estimate the association between maternal exposure to PM$_{2.5}$ and birth weight in a Colorado USA birth cohort.

</details>

<details>

<summary>2021-06-15 16:41:16 - A Bayesian adaptive design for dual-agent phase I-II cancer clinical trials combining efficacy data across stages</summary>

- *José L. Jiménez, Haiyan Zheng*

- `2106.08277v1` - [abs](http://arxiv.org/abs/2106.08277v1) - [pdf](http://arxiv.org/pdf/2106.08277v1)

> Integrated phase I-II clinical trial designs are efficient approaches to accelerate drug development. In cases where efficacy cannot be ascertained in a short period of time, two-stage approaches are usually employed. When different patient populations are involved across stages, it is worth of discussion about the use of efficacy data collected from both stages. In this paper, we focus on a two-stage design that aims to estimate safe dose combinations with a certain level of efficacy. In stage I, conditional escalation with overdose control (EWOC) is used to allocate successive cohorts of patients. The maximum tolerated dose (MTD) curve is estimated based on a Bayesian dose-toxicity model. In stage II, we consider an adaptive allocation of patients to drug combinations that have a high probability of being efficacious along the obtained MTD curve. A robust Bayesian hierarchical model is proposed to allow sharing of information on the efficacy parameters across stages assuming the related parameters are either exchangeable or nonexchangeable. Under the assumption of exchangeability, a random-effects distribution is specified for the main effects parameters to capture uncertainty about the between-stage differences. The proposed methodology is assessed with extensive simulations motivated by a real phase I-II drug combination trial using continuous doses.

</details>

<details>

<summary>2021-06-15 17:27:21 - Markov Equivalence of Max-Linear Bayesian Networks</summary>

- *Carlos Améndola, Ben Hollering, Seth Sullivant, Ngoc Tran*

- `2106.08305v1` - [abs](http://arxiv.org/abs/2106.08305v1) - [pdf](http://arxiv.org/pdf/2106.08305v1)

> Max-linear Bayesian networks have emerged as highly applicable models for causal inference via extreme value data. However, conditional independence (CI) for max-linear Bayesian networks behaves differently than for classical Gaussian Bayesian networks. We establish the parallel between the two theories via tropicalization, and establish the surprising result that the Markov equivalence classes for max-linear Bayesian networks coincide with the ones obtained by regular CI. Our paper opens up many problems at the intersection of extreme value statistics, causal inference and tropical geometry.

</details>

<details>

<summary>2021-06-15 19:09:42 - Optimizing Biomanufacturing Harvesting Decisions under Limited Historical Data</summary>

- *Bo Wang, Wei Xie, Tugce Martagan, Alp Akcay, Bram van Ravenstein*

- `2101.03735v3` - [abs](http://arxiv.org/abs/2101.03735v3) - [pdf](http://arxiv.org/pdf/2101.03735v3)

> In biopharmaceutical manufacturing, fermentation processes play a critical role on productivity and profit. A fermentation process uses living cells with complex biological mechanisms, and this leads to high variability in the process outputs. By building on the biological mechanisms of protein and impurity growth, we introduce a stochastic model to characterize the accumulation of the protein and impurity levels in the fermentation process. However, a common challenge in industry is the availability of only very limited amount of data especially in the development and early stage of production. This adds an additional layer of uncertainty, referred to as model risk, due to the difficulty of estimating the model parameters with limited data. In this paper, we study the harvesting decision for a fermentation process under model risk. In particular, we adopt a Bayesian approach to update the unknown parameters of the growth-rate distributions, and use the resulting posterior distributions to characterize the impact of model risk on fermentation output variability. The harvesting problem is formulated as a Markov decision process model with knowledge states that summarize the posterior distributions and hence incorporate the model risk in decision-making. The resulting model is solved by using a reinforcement learning algorithm based on Bayesian sparse sampling. We provide analytical results on the structure of the optimal policy and its objective function, and explicitly study the impact of model risk on harvesting decisions. Our case studies at MSD Animal Health demonstrate that the proposed model and solution approach improve the harvesting decisions in real life by achieving substantially higher average output from a fermentation batch along with lower batch-to-batch variability.

</details>

<details>

<summary>2021-06-15 21:32:28 - CODA: Constructivism Learning for Instance-Dependent Dropout Architecture Construction</summary>

- *Xiaoli Li*

- `2106.08444v1` - [abs](http://arxiv.org/abs/2106.08444v1) - [pdf](http://arxiv.org/pdf/2106.08444v1)

> Dropout is attracting intensive research interest in deep learning as an efficient approach to prevent overfitting. Recently incorporating structural information when deciding which units to drop out produced promising results comparing to methods that ignore the structural information. However, a major issue of the existing work is that it failed to differentiate among instances when constructing the dropout architecture. This can be a significant deficiency for many applications. To solve this issue, we propose Constructivism learning for instance-dependent Dropout Architecture (CODA), which is inspired from a philosophical theory, constructivism learning. Specially, based on the theory we have designed a better drop out technique, Uniform Process Mixture Models, using a Bayesian nonparametric method Uniform process. We have evaluated our proposed method on 5 real-world datasets and compared the performance with other state-of-the-art dropout techniques. The experimental results demonstrated the effectiveness of CODA.

</details>

<details>

<summary>2021-06-16 01:19:03 - Stochastic Convergence Rates and Applications of Adaptive Quadrature in Bayesian Inference</summary>

- *Blair Bilodeau, Alex Stringer, Yanbo Tang*

- `2102.06801v2` - [abs](http://arxiv.org/abs/2102.06801v2) - [pdf](http://arxiv.org/pdf/2102.06801v2)

> We provide the first stochastic convergence rates for a family of adaptive quadrature rules used to normalize the posterior distribution in Bayesian models. Our results apply to the uniform relative error in the approximate posterior density, the coverage probabilities of approximate credible sets, and approximate moments and quantiles, therefore guaranteeing fast asymptotic convergence of approximate summary statistics used in practice. The family of quadrature rules includes adaptive Gauss-Hermite quadrature, and we apply this rule in two challenging low-dimensional examples. Further, we demonstrate how adaptive quadrature can be used as a crucial component of a modern approximate Bayesian inference procedure for high-dimensional additive models. The method is implemented and made publicly available in the aghq package for the R language, available on CRAN.

</details>

<details>

<summary>2021-06-16 05:58:52 - Characterization of equilibrium existence and purification in general Bayesian games</summary>

- *Wei He, Xiang Sun, Yeneng Sun, Yishu Zeng*

- `2106.08563v1` - [abs](http://arxiv.org/abs/2106.08563v1) - [pdf](http://arxiv.org/pdf/2106.08563v1)

> This paper studies Bayesian games with general action spaces, correlated types and interdependent payoffs. We introduce the condition of ``decomposable coarser payoff-relevant information'', and show that this condition is both sufficient and necessary for the existence of pure-strategy equilibria and purification from behavioral strategies. As a consequence of our purification method, a new existence result on pure-strategy equilibria is also obtained for discontinuous Bayesian games. Illustrative applications of our results to oligopolistic competitions and all-pay auctions are provided.

</details>

<details>

<summary>2021-06-16 13:08:56 - Optimized Auxiliary Particle Filters: adapting mixture proposals via convex optimization</summary>

- *Nicola Branchini, Víctor Elvira*

- `2011.09317v2` - [abs](http://arxiv.org/abs/2011.09317v2) - [pdf](http://arxiv.org/pdf/2011.09317v2)

> Auxiliary particle filters (APFs) are a class of sequential Monte Carlo (SMC) methods for Bayesian inference in state-space models. In their original derivation, APFs operate in an extended state space using an auxiliary variable to improve inference. In this work, we propose optimized auxiliary particle filters, a framework where the traditional APF auxiliary variables are interpreted as weights in an importance sampling mixture proposal. Under this interpretation, we devise a mechanism for proposing the mixture weights that is inspired by recent advances in multiple and adaptive importance sampling. In particular, we propose to select the mixture weights by formulating a convex optimization problem, with the aim of approximating the filtering posterior at each timestep. Further, we propose a weighting scheme that generalizes previous results on the APF (Pitt et al. 2012), proving unbiasedness and consistency of our estimators. Our framework demonstrates significantly improved estimates on a range of metrics compared to state-of-the-art particle filters at similar computational complexity in challenging and widely used dynamical models.

</details>

<details>

<summary>2021-06-16 15:42:52 - The semi-hierarchical Dirichlet Process and its application to clustering homogeneous distributions</summary>

- *Mario Beraha, Alessandra Guglielmi, Fernando A. Quintana*

- `2005.10287v4` - [abs](http://arxiv.org/abs/2005.10287v4) - [pdf](http://arxiv.org/pdf/2005.10287v4)

> Assessing homogeneity of distributions is an old problem that has received considerable attention, especially in the nonparametric Bayesian literature. To this effect, we propose the semi-hierarchical Dirichlet process, a novel hierarchical prior that extends the hierarchical Dirichlet process of Teh et al. (2006) and that avoids the degeneracy issues of nested processes recently described by Camerlenghi et al. (2019a). We go beyond the simple yes/no answer to the homogeneity question and embed the proposed prior in a random partition model; this procedure allows us to give a more comprehensive response to the above question and in fact find groups of populations that are internally homogeneous when I greater or equal than 2 such populations are considered. We study theoretical properties of the semi-hierarchical Dirichlet process and of the Bayes factor for the homogeneity test when I = 2. Extensive simulation studies and applications to educational data are also discussed.

</details>

<details>

<summary>2021-06-16 16:44:31 - Thompson Sampling with Information Relaxation Penalties</summary>

- *Seungki Min, Costis Maglaras, Ciamac C. Moallemi*

- `1902.04251v2` - [abs](http://arxiv.org/abs/1902.04251v2) - [pdf](http://arxiv.org/pdf/1902.04251v2)

> We consider a finite-horizon multi-armed bandit (MAB) problem in a Bayesian setting, for which we propose an information relaxation sampling framework. With this framework, we define an intuitive family of control policies that include Thompson sampling (TS) and the Bayesian optimal policy as endpoints. Analogous to TS, which, at each decision epoch pulls an arm that is best with respect to the randomly sampled parameters, our algorithms sample entire future reward realizations and take the corresponding best action. However, this is done in the presence of "penalties" that seek to compensate for the availability of future information.   We develop several novel policies and performance bounds for MAB problems that vary in terms of improving performance and increasing computational complexity between the two endpoints. Our policies can be viewed as natural generalizations of TS that simultaneously incorporate knowledge of the time horizon and explicitly consider the exploration-exploitation trade-off. We prove associated structural results on performance bounds and suboptimality gaps. Numerical experiments suggest that this new class of policies perform well, in particular in settings where the finite time horizon introduces significant exploration-exploitation tension into the problem. Finally, inspired by the finite-horizon Gittins index, we propose an index policy that builds on our framework that particularly outperforms the state-of-the-art algorithms in our numerical experiments.

</details>

<details>

<summary>2021-06-16 23:35:07 - On Construction and Estimation of Stationary Mixture Transition Distribution Models</summary>

- *Xiaotian Zheng, Athanasios Kottas, Bruno Sansó*

- `2010.12696v2` - [abs](http://arxiv.org/abs/2010.12696v2) - [pdf](http://arxiv.org/pdf/2010.12696v2)

> Mixture transition distribution time series models build high-order dependence through a weighted combination of first-order transition densities for each one of a specified number of lags. We present a framework to construct stationary transition mixture distribution models that extend beyond linear, Gaussian dynamics. We study conditions for first-order strict stationarity which allow for different constructions with either continuous or discrete families for the first-order transition densities given a pre-specified family for the marginal density, and with general forms for the resulting conditional expectations. Inference and prediction are developed under the Bayesian framework with particular emphasis on flexible, structured priors for the mixture weights. Model properties are investigated both analytically and through synthetic data examples. Finally, Poisson and Lomax examples are illustrated through real data applications.

</details>

<details>

<summary>2021-06-17 07:27:53 - Investigating Methods to Improve Language Model Integration for Attention-based Encoder-Decoder ASR Models</summary>

- *Mohammad Zeineldeen, Aleksandr Glushko, Wilfried Michel, Albert Zeyer, Ralf Schlüter, Hermann Ney*

- `2104.05544v2` - [abs](http://arxiv.org/abs/2104.05544v2) - [pdf](http://arxiv.org/pdf/2104.05544v2)

> Attention-based encoder-decoder (AED) models learn an implicit internal language model (ILM) from the training transcriptions. The integration with an external LM trained on much more unpaired text usually leads to better performance. A Bayesian interpretation as in the hybrid autoregressive transducer (HAT) suggests dividing by the prior of the discriminative acoustic model, which corresponds to this implicit LM, similarly as in the hybrid hidden Markov model approach. The implicit LM cannot be calculated efficiently in general and it is yet unclear what are the best methods to estimate it. In this work, we compare different approaches from the literature and propose several novel methods to estimate the ILM directly from the AED model. Our proposed methods outperform all previous approaches. We also investigate other methods to suppress the ILM mainly by decreasing the capacity of the AED model, limiting the label context, and also by training the AED model together with a pre-existing LM.

</details>

<details>

<summary>2021-06-17 08:24:48 - A Two-Stage Bayesian Semiparametric Model for Novelty Detection with Robust Prior Information</summary>

- *Francesco Denti, Andrea Cappozzo, Francesca Greselin*

- `2006.09012v3` - [abs](http://arxiv.org/abs/2006.09012v3) - [pdf](http://arxiv.org/pdf/2006.09012v3)

> Novelty detection methods aim at partitioning the test units into already observed and previously unseen patterns. However, two significant issues arise: there may be considerable interest in identifying specific structures within the novelty, and contamination in the known classes could completely blur the actual separation between manifest and new groups. Motivated by these problems, we propose a two-stage Bayesian semiparametric novelty detector, building upon prior information robustly extracted from a set of complete learning units. We devise a general-purpose multivariate methodology that we also extend to handle functional data objects. We provide insights on the model behavior by investigating the theoretical properties of the associated semiparametric prior. From the computational point of view, we propose a suitable $\boldsymbol{\xi}$-sequence to construct an independent slice-efficient sampler that takes into account the difference between manifest and novelty components. We showcase our model performance through an extensive simulation study and applications on both multivariate and functional datasets, in which diverse and distinctive unknown patterns are discovered.

</details>

<details>

<summary>2021-06-17 08:36:16 - Inferring UK COVID-19 fatal infection trajectories from daily mortality data: were infections already in decline before the UK lockdowns?</summary>

- *Simon N. Wood*

- `2005.02090v8` - [abs](http://arxiv.org/abs/2005.02090v8) - [pdf](http://arxiv.org/pdf/2005.02090v8)

> The number of new infections per day is a key quantity for effective epidemic management. It can be estimated relatively directly by testing of random population samples. Without such direct epidemiological measurement, other approaches are required to infer whether the number of new cases is likely to be increasing or decreasing: for example, estimating the pathogen effective reproduction number, R, using data gathered from the clinical response to the disease. For Covid-19 (SARS-CoV-2) such R estimation is heavily dependent on modelling assumptions, because the available clinical case data are opportunistic observational data subject to severe temporal confounding. Given this difficulty it is useful to retrospectively reconstruct the time course of infections from the least compromised available data, using minimal prior assumptions. A Bayesian inverse problem approach applied to UK data on first wave Covid-19 deaths and the disease duration distribution suggests that fatal infections were in decline before full UK lockdown (24 March 2020), and that fatal infections in Sweden started to decline only a day or two later. An analysis of UK data using the model of Flaxman et al. (2020, Nature 584) gives the same result under relaxation of its prior assumptions on R, suggesting an enhanced role for non pharmaceutical interventions (NPIs) short of full lock down in the UK context. Similar patterns appear to have occurred in the subsequent two lockdowns. Estimates from the main UK Covid statistical surveillance surveys, available since original publication, support these results. Replication code for the paper is available in the supporting information of doi/10.1111/biom.13462.

</details>

<details>

<summary>2021-06-17 10:50:28 - Differentially Private Hamiltonian Monte Carlo</summary>

- *Ossi Räisä, Antti Koskela, Antti Honkela*

- `2106.09376v1` - [abs](http://arxiv.org/abs/2106.09376v1) - [pdf](http://arxiv.org/pdf/2106.09376v1)

> Markov chain Monte Carlo (MCMC) algorithms have long been the main workhorses of Bayesian inference. Among them, Hamiltonian Monte Carlo (HMC) has recently become very popular due to its efficiency resulting from effective use of the gradients of the target distribution. In privacy-preserving machine learning, differential privacy (DP) has become the gold standard in ensuring that the privacy of data subjects is not violated. Existing DP MCMC algorithms either use random-walk proposals, or do not use the Metropolis--Hastings (MH) acceptance test to ensure convergence without decreasing their step size to zero. We present a DP variant of HMC using the MH acceptance test that builds on a recently proposed DP MCMC algorithm called the penalty algorithm, and adds noise to the gradient evaluations of HMC. We prove that the resulting algorithm converges to the correct distribution, and is ergodic. We compare DP-HMC with the existing penalty, DP-SGLD and DP-SGNHT algorithms, and find that DP-HMC has better or equal performance than the penalty algorithm, and performs more consistently than DP-SGLD or DP-SGNHT.

</details>

<details>

<summary>2021-06-17 15:29:25 - Hierarchical surrogate-based Approximate Bayesian Computation for an electric motor test bench</summary>

- *David N. John, Livia Stohrer, Claudia Schillings, Michael Schick, Vincent Heuveline*

- `2106.09597v1` - [abs](http://arxiv.org/abs/2106.09597v1) - [pdf](http://arxiv.org/pdf/2106.09597v1)

> Inferring parameter distributions of complex industrial systems from noisy time series data requires methods to deal with the uncertainty of the underlying data and the used simulation model. Bayesian inference is well suited for these uncertain inverse problems. Standard methods used to identify uncertain parameters are Markov Chain Monte Carlo (MCMC) methods with explicit evaluation of a likelihood function. However, if the likelihood is very complex, such that its evaluation is computationally expensive, or even unknown in its explicit form, Approximate Bayesian Computation (ABC) methods provide a promising alternative. In this work both methods are first applied to artificially generated data and second on a real world problem, by using data of an electric motor test bench. We show that both methods are able to infer the distribution of varying parameters with a Bayesian hierarchical approach. But the proposed ABC method is computationally much more efficient in order to achieve results with similar accuracy. We suggest to use summary statistics in order to reduce the dimension of the data which significantly increases the efficiency of the algorithm. Further the simulation model is replaced by a Polynomial Chaos Expansion (PCE) surrogate to speed up model evaluations. We proof consistency for the proposed surrogate-based ABC method with summary statistics under mild conditions on the (approximated) forward model.

</details>

<details>

<summary>2021-06-17 17:35:29 - PAC-Bayes, MAC-Bayes and Conditional Mutual Information: Fast rate bounds that handle general VC classes</summary>

- *Peter Grünwald, Thomas Steinke, Lydia Zakynthinou*

- `2106.09683v1` - [abs](http://arxiv.org/abs/2106.09683v1) - [pdf](http://arxiv.org/pdf/2106.09683v1)

> We give a novel, unified derivation of conditional PAC-Bayesian and mutual information (MI) generalization bounds. We derive conditional MI bounds as an instance, with special choice of prior, of conditional MAC-Bayesian (Mean Approximately Correct) bounds, itself derived from conditional PAC-Bayesian bounds, where `conditional' means that one can use priors conditioned on a joint training and ghost sample. This allows us to get nontrivial PAC-Bayes and MI-style bounds for general VC classes, something recently shown to be impossible with standard PAC-Bayesian/MI bounds. Second, it allows us to get faster rates of order $O \left(({\text{KL}}/n)^{\gamma}\right)$ for $\gamma > 1/2$ if a Bernstein condition holds and for exp-concave losses (with $\gamma=1$), which is impossible with both standard PAC-Bayes generalization and MI bounds. Our work extends the recent work by Steinke and Zakynthinou [2020] who handle MI with VC but neither PAC-Bayes nor fast rates, the recent work of Hellstr\"om and Durisi [2020] who extend the latter to the PAC-Bayes setting via a unifying exponential inequality, and Mhammedi et al. [2019] who initiated fast rate PAC-Bayes generalization error bounds but handle neither MI nor general VC classes.

</details>

<details>

<summary>2021-06-17 19:23:23 - Variational Combinatorial Sequential Monte Carlo Methods for Bayesian Phylogenetic Inference</summary>

- *Antonio Khalil Moretti, Liyi Zhang, Christian A. Naesseth, Hadiah Venner, David Blei, Itsik Pe'er*

- `2106.00075v2` - [abs](http://arxiv.org/abs/2106.00075v2) - [pdf](http://arxiv.org/pdf/2106.00075v2)

> Bayesian phylogenetic inference is often conducted via local or sequential search over topologies and branch lengths using algorithms such as random-walk Markov chain Monte Carlo (MCMC) or Combinatorial Sequential Monte Carlo (CSMC). However, when MCMC is used for evolutionary parameter learning, convergence requires long runs with inefficient exploration of the state space. We introduce Variational Combinatorial Sequential Monte Carlo (VCSMC), a powerful framework that establishes variational sequential search to learn distributions over intricate combinatorial structures. We then develop nested CSMC, an efficient proposal distribution for CSMC and prove that nested CSMC is an exact approximation to the (intractable) locally optimal proposal. We use nested CSMC to define a second objective, VNCSMC which yields tighter lower bounds than VCSMC. We show that VCSMC and VNCSMC are computationally efficient and explore higher probability spaces than existing methods on a range of tasks.

</details>

<details>

<summary>2021-06-17 20:25:38 - Wide stochastic networks: Gaussian limit and PAC-Bayesian training</summary>

- *Eugenio Clerico, George Deligiannidis, Arnaud Doucet*

- `2106.09798v1` - [abs](http://arxiv.org/abs/2106.09798v1) - [pdf](http://arxiv.org/pdf/2106.09798v1)

> The limit of infinite width allows for substantial simplifications in the analytical study of overparameterized neural networks. With a suitable random initialization, an extremely large network is well approximated by a Gaussian process, both before and during training. In the present work, we establish a similar result for a simple stochastic architecture whose parameters are random variables. The explicit evaluation of the output distribution allows for a PAC-Bayesian training procedure that directly optimizes the generalization bound. For a large but finite-width network, we show empirically on MNIST that this training approach can outperform standard PAC-Bayesian methods.

</details>

<details>

<summary>2021-06-17 22:26:24 - Euclidean Representation of Low-Rank Matrices and Its Statistical Applications</summary>

- *Fangzheng Xie*

- `2103.04220v2` - [abs](http://arxiv.org/abs/2103.04220v2) - [pdf](http://arxiv.org/pdf/2103.04220v2)

> Low-rank matrices are pervasive throughout statistics, machine learning, signal processing, optimization, and applied mathematics. In this paper, we propose a novel and user-friendly Euclidean representation framework for low-rank matrices. Correspondingly, we establish a collection of technical and theoretical tools for analyzing the intrinsic perturbation of low-rank matrices in which the underlying referential matrix and the perturbed matrix both live on the same low-rank matrix manifold. Our analyses show that, locally around the referential matrix, the sine-theta distance between subspaces is equivalent to the Euclidean distance between two appropriately selected orthonormal basis, circumventing the orthogonal Procrustes analysis. We also establish the regularity of the proposed Euclidean representation function, which has a profound statistical impact and a meaningful geometric interpretation. These technical devices are applicable to a broad range of statistical problems. Specific applications considered in detail include Bayesian sparse spiked covariance model with non-intrinsic loss, efficient estimation in stochastic block models where the block probability matrix may be degenerate, and least-squares estimation in biclustering problems. Both the intrinsic perturbation analysis of low-rank matrices and the regularity theorem may be of independent interest.

</details>

<details>

<summary>2021-06-18 07:08:24 - PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees</summary>

- *Jonas Rothfuss, Vincent Fortuin, Martin Josifoski, Andreas Krause*

- `2002.05551v5` - [abs](http://arxiv.org/abs/2002.05551v5) - [pdf](http://arxiv.org/pdf/2002.05551v5)

> Meta-learning can successfully acquire useful inductive biases from data. Yet, its generalization properties to unseen learning tasks are poorly understood. Particularly if the number of meta-training tasks is small, this raises concerns about overfitting. We provide a theoretical analysis using the PAC-Bayesian framework and derive novel generalization bounds for meta-learning. Using these bounds, we develop a class of PAC-optimal meta-learning algorithms with performance guarantees and a principled meta-level regularization. Unlike previous PAC-Bayesian meta-learners, our method results in a standard stochastic optimization problem which can be solved efficiently and scales well. When instantiating our PAC-optimal hyper-posterior (PACOH) with Gaussian processes and Bayesian Neural Networks as base learners, the resulting methods yield state-of-the-art performance, both in terms of predictive accuracy and the quality of uncertainty estimates. Thanks to their principled treatment of uncertainty, our meta-learners can also be successfully employed for sequential decision problems.

</details>

<details>

<summary>2021-06-18 11:13:05 - Bayesian Cox Regression for Population-scale Inference in Electronic Health Records</summary>

- *Alexander W. Jung, Moritz Gerstung*

- `2106.10057v1` - [abs](http://arxiv.org/abs/2106.10057v1) - [pdf](http://arxiv.org/pdf/2106.10057v1)

> The Cox model is an indispensable tool for time-to-event analysis, particularly in biomedical research. However, medicine is undergoing a profound transformation, generating data at an unprecedented scale, which opens new frontiers to study and understand diseases. With the wealth of data collected, new challenges for statistical inference arise, as datasets are often high dimensional, exhibit an increasing number of measurements at irregularly spaced time points, and are simply too large to fit in memory. Many current implementations for time-to-event analysis are ill-suited for these problems as inference is computationally demanding and requires access to the full data at once. Here we propose a Bayesian version for the counting process representation of Cox's partial likelihood for efficient inference on large-scale datasets with millions of data points and thousands of time-dependent covariates. Through the combination of stochastic variational inference and a reweighting of the log-likelihood, we obtain an approximation for the posterior distribution that factorizes over subsamples of the data, enabling the analysis in big data settings. Crucially, the method produces viable uncertainty estimates for large-scale and high-dimensional datasets. We show the utility of our method through a simulation study and an application to myocardial infarction in the UK Biobank.

</details>

<details>

<summary>2021-06-18 11:48:16 - MARS: Masked Automatic Ranks Selection in Tensor Decompositions</summary>

- *Maxim Kodryan, Dmitry Kropotov, Dmitry Vetrov*

- `2006.10859v2` - [abs](http://arxiv.org/abs/2006.10859v2) - [pdf](http://arxiv.org/pdf/2006.10859v2)

> Tensor decomposition methods are known to be efficient for compressing and accelerating neural networks. However, the problem of optimal decomposition structure determination is still not well studied while being quite important. Specifically, decomposition ranks present the crucial parameter controlling the compression-accuracy trade-off. In this paper, we introduce MARS -- a new efficient method for the automatic selection of ranks in general tensor decompositions. During training, the procedure learns binary masks over decomposition cores that "select" the optimal tensor structure. The learning is performed via relaxed maximum a posteriori (MAP) estimation in a specific Bayesian model. The proposed method achieves better results compared to previous works in various tasks.

</details>

<details>

<summary>2021-06-18 12:59:04 - Assessing an Alternative for `Negative Variance Components': A Gentle Introduction to Bayesian Covariance Structure Modelling for Negative Associations Among Patients with Personalized Treatments</summary>

- *Jean-Paul Fox, Wouter Smink*

- `2106.10107v1` - [abs](http://arxiv.org/abs/2106.10107v1) - [pdf](http://arxiv.org/pdf/2106.10107v1)

> The multilevel model (MLM) is the popular approach to describe dependences of hierarchically clustered observations. A main feature is the capability to estimate (cluster-specific) random effect parameters, while their distribution describes the variation across clusters. However, the MLM can only model positive associations among clustered observations, and it is not suitable for small sample sizes. The limitation of the MLM becomes apparent when estimation methods produce negative estimates for random effect variances, which can be seen as an indication that observations are negatively correlated. A gentle introduction to Bayesian Covariance Structure Modelling (BCSM) is given, which makes it possible to model also negatively correlated observations. The BCSM does not model dependences through random (cluster-specific) effects, but through a covariance matrix. We show that this makes the BCSM particularly useful for small data samples. We draw specific attention to detect effects of a personalized intervention. The effect of a personalized treatment can differ across individuals, and this can lead to negative associations among measurements of individuals who are treated by the same therapist. It is shown that the BCSM enables the modeling of negative associations among clustered measurements and aids in the interpretation of negative clustering effects. Through a simulation study and by analysis of a real data example, we discuss the suitability of the BCSM for small data sets and for exploring effects of individualized treatments, specifically when (standard) MLM software produces negative or zero variance estimates.

</details>

<details>

<summary>2021-06-18 14:21:51 - DG-LMC: A Turn-key and Scalable Synchronous Distributed MCMC Algorithm via Langevin Monte Carlo within Gibbs</summary>

- *Vincent Plassier, Maxime Vono, Alain Durmus, Eric Moulines*

- `2106.06300v2` - [abs](http://arxiv.org/abs/2106.06300v2) - [pdf](http://arxiv.org/pdf/2106.06300v2)

> Performing reliable Bayesian inference on a big data scale is becoming a keystone in the modern era of machine learning. A workhorse class of methods to achieve this task are Markov chain Monte Carlo (MCMC) algorithms and their design to handle distributed datasets has been the subject of many works. However, existing methods are not completely either reliable or computationally efficient. In this paper, we propose to fill this gap in the case where the dataset is partitioned and stored on computing nodes within a cluster under a master/slaves architecture. We derive a user-friendly centralised distributed MCMC algorithm with provable scaling in high-dimensional settings. We illustrate the relevance of the proposed methodology on both synthetic and real data experiments.

</details>

<details>

<summary>2021-06-18 19:41:09 - Amazon SageMaker Automatic Model Tuning: Scalable Gradient-Free Optimization</summary>

- *Valerio Perrone, Huibin Shen, Aida Zolic, Iaroslav Shcherbatyi, Amr Ahmed, Tanya Bansal, Michele Donini, Fela Winkelmolen, Rodolphe Jenatton, Jean Baptiste Faddoul, Barbara Pogorzelska, Miroslav Miladinovic, Krishnaram Kenthapadi, Matthias Seeger, Cédric Archambeau*

- `2012.08489v2` - [abs](http://arxiv.org/abs/2012.08489v2) - [pdf](http://arxiv.org/pdf/2012.08489v2)

> Tuning complex machine learning systems is challenging. Machine learning typically requires to set hyperparameters, be it regularization, architecture, or optimization parameters, whose tuning is critical to achieve good predictive performance. To democratize access to machine learning systems, it is essential to automate the tuning. This paper presents Amazon SageMaker Automatic Model Tuning (AMT), a fully managed system for gradient-free optimization at scale. AMT finds the best version of a trained machine learning model by repeatedly evaluating it with different hyperparameter configurations. It leverages either random search or Bayesian optimization to choose the hyperparameter values resulting in the best model, as measured by the metric chosen by the user. AMT can be used with built-in algorithms, custom algorithms, and Amazon SageMaker pre-built containers for machine learning frameworks. We discuss the core functionality, system architecture, our design principles, and lessons learned. We also describe more advanced features of AMT, such as automated early stopping and warm-starting, showing in experiments their benefits to users.

</details>

<details>

<summary>2021-06-18 19:46:07 - Fair Bayesian Optimization</summary>

- *Valerio Perrone, Michele Donini, Muhammad Bilal Zafar, Robin Schmucker, Krishnaram Kenthapadi, Cédric Archambeau*

- `2006.05109v3` - [abs](http://arxiv.org/abs/2006.05109v3) - [pdf](http://arxiv.org/pdf/2006.05109v3)

> Given the increasing importance of machine learning (ML) in our lives, several algorithmic fairness techniques have been proposed to mitigate biases in the outcomes of the ML models. However, most of these techniques are specialized to cater to a single family of ML models and a specific definition of fairness, limiting their adaptibility in practice. We introduce a general constrained Bayesian optimization (BO) framework to optimize the performance of any ML model while enforcing one or multiple fairness constraints. BO is a model-agnostic optimization method that has been successfully applied to automatically tune the hyperparameters of ML models. We apply BO with fairness constraints to a range of popular models, including random forests, gradient boosting, and neural networks, showing that we can obtain accurate and fair solutions by acting solely on the hyperparameters. We also show empirically that our approach is competitive with specialized techniques that enforce model-specific fairness constraints, and outperforms preprocessing methods that learn fair representations of the input data. Moreover, our method can be used in synergy with such specialized fairness techniques to tune their hyperparameters. Finally, we study the relationship between fairness and the hyperparameters selected by BO. We observe a correlation between regularization and unbiased models, explaining why acting on the hyperparameters leads to ML models that generalize well and are fair.

</details>

<details>

<summary>2021-06-18 23:03:39 - Scalable Bayesian change point detection with spike and slab priors</summary>

- *Lorenzo Cappello, Oscar Hernan Madrid Padilla, Julia A. Palacios*

- `2106.10383v1` - [abs](http://arxiv.org/abs/2106.10383v1) - [pdf](http://arxiv.org/pdf/2106.10383v1)

> We study the use of spike and slab priors for consistent estimation of the number of change points and their locations. Leveraging recent results in the variable selection literature, we show that an estimator based on spike and slab priors achieves optimal localization rate in the multiple offline change point detection problem. Based on this estimator, we propose a Bayesian change point detection method, which is one of the fastest Bayesian methodologies, and it is more robust to misspecification of the error terms than the competing methods. We demonstrate through empirical work the good performance of our approach vis-a-vis some state-of-the-art benchmarks.

</details>

<details>

<summary>2021-06-19 06:22:47 - Overcoming Catastrophic Forgetting by Generative Regularization</summary>

- *Patrick H. Chen, Wei Wei, Cho-jui Hsieh, Bo Dai*

- `1912.01238v3` - [abs](http://arxiv.org/abs/1912.01238v3) - [pdf](http://arxiv.org/pdf/1912.01238v3)

> In this paper, we propose a new method to overcome catastrophic forgetting by adding generative regularization to Bayesian inference framework. Bayesian method provides a general framework for continual learning. We could further construct a generative regularization term for all given classification models by leveraging energy-based models and Langevin-dynamic sampling to enrich the features learned in each task. By combining discriminative and generative loss together, we empirically show that the proposed method outperforms state-of-the-art methods on a variety of tasks, avoiding catastrophic forgetting in continual learning. In particular, the proposed method outperforms baseline methods over 15% on the Fashion-MNIST dataset and 10% on the CUB dataset

</details>

<details>

<summary>2021-06-19 07:48:51 - Posterior Impropriety of some Sparse Bayesian Learning Models</summary>

- *Anand Dixit, Vivekananda Roy*

- `2008.00242v2` - [abs](http://arxiv.org/abs/2008.00242v2) - [pdf](http://arxiv.org/pdf/2008.00242v2)

> Sparse Bayesian learning models are typically used for prediction in datasets with significantly greater number of covariates than observations. Such models often take a reproducing kernel Hilbert space (RKHS) approach to carry out the task of prediction and can be implemented using either proper or improper priors. In this article we show that a few sparse Bayesian learning models in the literature, when implemented using improper priors, lead to improper posteriors.

</details>

<details>

<summary>2021-06-19 14:33:04 - Robust Hierarchical Modeling of Counts under Zero-inflation and Outliers</summary>

- *Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa*

- `2106.10503v1` - [abs](http://arxiv.org/abs/2106.10503v1) - [pdf](http://arxiv.org/pdf/2106.10503v1)

> Count data with zero inflation and large outliers are ubiquitous in many scientific applications. However, the posterior analysis under a standard statistical model such as Poisson or negative binomial distribution is sensitive to such contamination. This paper introduces a novel framework for Bayesian modeling of counts robust to both zeros inflation and large outliers. In doing so, we introduce the rescaled beta distribution and adopt it to absorb undesirable effects from zero and outlying counts. The proposed approach has two appealing features: the efficiency of the posterior computation via a custom Gibbs sampling algorithm, and the theoretical posterior robustness, where the extreme outliers are automatically removed from the posterior distribution. We demonstrate the usefulness of the proposed method through simulation and real data applications.

</details>

<details>

<summary>2021-06-19 20:59:22 - Geographic and Racial Disparities in the Incidence of Low Birthweight in Pennsylvania</summary>

- *Guangzi Song, Loni Philip Tabb, Harrison Quick*

- `2106.10571v1` - [abs](http://arxiv.org/abs/2106.10571v1) - [pdf](http://arxiv.org/pdf/2106.10571v1)

> Babies born with low and very low birthweights -- i.e., birthweights below 2,500 and 1,500 grams, respectively -- have an increased risk of complications compared to other babies, and the proportion of babies with a low birthweight is a common metric used when evaluating public health in a population. While many factors increase the risk of a baby having a low birthweight, many can be linked to the mother's socioeconomic status, which in turn contributes to large racial disparities in the incidence of low weight births. Here, we employ Bayesian statistical models to analyze the proportion of babies with low birthweight in Pennsylvania counties by race/ethnicity. Due to the small number of births -- and low weight births -- in many Pennsylvania counties when stratified by race/ethnicity, our methods must walk a fine line. On one hand, leveraging spatial structure can help improve the precision of our estimates. On the other hand, we must be cautious to avoid letting the model overwhelm the information in the data and produce spurious conclusions. As such, we first develop a framework by which we can measure (and control) the informativeness of our spatial model. After demonstrating the properties of our framework via simulation, we analyze the low birthweight data from Pennsylvania and examine the extent to which the commonly used conditional autoregressive model can lead to oversmoothing. We then reanalyze the data using our proposed framework and highlight its ability to detect (or not detect) evidence of racial disparities in the incidence of low birthweight.

</details>

<details>

<summary>2021-06-20 03:41:19 - Discrepancies in Epidemiological Modeling of Aggregated Heterogeneous Data</summary>

- *Anna L. Trella, Peniel N. Argaw, Michelle M. Li, James A. Hay*

- `2106.10610v1` - [abs](http://arxiv.org/abs/2106.10610v1) - [pdf](http://arxiv.org/pdf/2106.10610v1)

> Within epidemiological modeling, the majority of analyses assume a single epidemic process for generating ground-truth data. However, this assumed data generation process can be unrealistic, since data sources for epidemics are often aggregated across geographic regions and communities. As a result, state-of-the-art models for estimating epidemiological parameters, e.g.~transmission rates, can be inappropriate when faced with complex systems. Our work empirically demonstrates some limitations of applying epidemiological models to aggregated datasets. We generate three complex outbreak scenarios by combining incidence curves from multiple epidemics that are independently simulated via SEIR models with different sets of parameters. Using these scenarios, we assess the robustness of a state-of-the-art Bayesian inference method that estimates the epidemic trajectory from viral load surveillance data. We evaluate two data-generating models within this Bayesian inference framework: a simple exponential growth model and a highly flexible Gaussian process prior model. Our results show that both models generate accurate transmission rate estimates for the combined incidence curve at the cost of generating biased estimates for each underlying epidemic, reflecting highly heterogeneous underlying population dynamics. The exponential growth model, while interpretable, is unable to capture the complexity of the underlying epidemics. With sufficient surveillance data, the Gaussian process prior model captures the shape of complex trajectories, but is imprecise for periods of low data coverage. Thus, our results highlight the potential pitfalls of neglecting complexity and heterogeneity in the data generation process, which can mask underlying location- and population-specific epidemic dynamics.

</details>

<details>

<summary>2021-06-20 08:26:58 - A Comparative Study of Imputation Methods for Multivariate Ordinal Data</summary>

- *Chayut Wongkamthong, Olanrewaju Akande*

- `2010.10471v4` - [abs](http://arxiv.org/abs/2010.10471v4) - [pdf](http://arxiv.org/pdf/2010.10471v4)

> Missing data remains a very common problem in large datasets, including survey and census data containing many ordinal responses, such as political polls and opinion surveys. Multiple imputation (MI) is usually the go-to approach for analyzing such incomplete datasets, and there are indeed several implementations of MI, including methods using generalized linear models, tree-based models, and Bayesian non-parametric models. However, there is limited research on the statistical performance of these methods for multivariate ordinal data. In this article, we perform an empirical evaluation of several MI methods, including MI by chained equations (MICE) using multinomial logistic regression models, MICE using proportional odds logistic regression models, MICE using classification and regression trees, MICE using random forest, MI using Dirichlet process (DP) mixtures of products of multinomial distributions, and MI using DP mixtures of multivariate normal distributions. We evaluate the methods using simulation studies based on ordinal variables selected from the 2018 American Community Survey (ACS). Under our simulation settings, the results suggest that MI using proportional odds logistic regression models, classification and regression trees and DP mixtures of multinomial distributions generally outperform the other methods. In certain settings, MI using multinomial logistic regression models is able to achieve comparable performance, depending on the missing data mechanism and amount of missing data.

</details>

<details>

<summary>2021-06-20 09:13:55 - Bayesian inference for continuous-time hidden Markov models with an unknown number of states</summary>

- *Yu Luo, David A. Stephens*

- `2106.10660v1` - [abs](http://arxiv.org/abs/2106.10660v1) - [pdf](http://arxiv.org/pdf/2106.10660v1)

> We consider the modeling of data generated by a latent continuous-time Markov jump process with a state space of finite but unknown dimensions. Typically in such models, the number of states has to be pre-specified, and Bayesian inference for a fixed number of states has not been studied until recently. In addition, although approaches to address the problem for discrete-time models have been developed, no method has been successfully implemented for the continuous-time case. We focus on reversible jump Markov chain Monte Carlo which allows the trans-dimensional move among different numbers of states in order to perform Bayesian inference for the unknown number of states. Specifically, we propose an efficient split-combine move which can facilitate the exploration of the parameter space, and demonstrate that it can be implemented effectively at scale. Subsequently, we extend this algorithm to the context of model-based clustering, allowing numbers of states and clusters both determined during the analysis. The model formulation, inference methodology, and associated algorithm are illustrated by simulation studies. Finally, We apply this method to real data from a Canadian healthcare system in Quebec.

</details>

<details>

<summary>2021-06-20 13:35:00 - Life-cycle assessment for flutter probability of a long-span suspension bridge based on field monitoring data</summary>

- *Xiaolei Chu, Hung Nguyen Sinh, Wei Cui, Lin Zhao, Yaojun Ge*

- `2106.10694v1` - [abs](http://arxiv.org/abs/2106.10694v1) - [pdf](http://arxiv.org/pdf/2106.10694v1)

> Assessment of structural safety status is of paramount importance for existing bridges, where accurate evaluation of flutter probability is essential for long-span bridges. In current engineering practice, at the design stage, flutter critical wind speed is usually estimated by the wind tunnel test, which is sensitive to modal frequencies and damping ratios. After construction, structural properties of existing structures will change with time due to various factors, such as structural deteriorations and periodic environments. The structural dynamic properties, such as modal frequencies and damping ratios, cannot be considered as the same values as the initial ones, and the deteriorations should be included when estimating the life-cycle flutter probability. This paper proposes an evaluation framework to assess the life-cycle flutter probability of long-span bridges considering the deteriorations of structural properties, based on field monitoring data. The Bayesian approach is employed for modal identification of a suspension bridge with the main span of 1650 m, and the field monitoring data during 2010-2015 is analyzed to determine the deterioration functions of modal frequencies and damping ratios, as well as their inter-seasonal fluctuations. According to the historical trend, the long-term structural properties can be predicted, and the probability distributions of flutter critical wind speed for each year in the long term are calculated. Consequently, the life-cycle flutter probability is estimated, based on the predicted modal frequencies and damping ratios.

</details>

<details>

<summary>2021-06-20 22:06:44 - Generalization in the Face of Adaptivity: A Bayesian Perspective</summary>

- *Moshe Shenfeld, Katrina Ligett*

- `2106.10761v1` - [abs](http://arxiv.org/abs/2106.10761v1) - [pdf](http://arxiv.org/pdf/2106.10761v1)

> Repeated use of a data sample via adaptively chosen queries can rapidly lead to overfitting, wherein the issued queries yield answers on the sample that differ wildly from the values of those queries on the underlying data distribution. Differential privacy provides a tool to ensure generalization despite adaptively-chosen queries, but its worst-case nature means that it cannot, for example, yield improved results for low-variance queries. In this paper, we give a simple new characterization that illuminates the core problem of adaptive data analysis. We show explicitly that the harms of adaptivity come from the covariance between the behavior of future queries and a Bayes factor-based measure of how much information about the data sample was encoded in the responses given to past queries. We leverage this intuition to introduce a new stability notion; we then use it to prove new generalization results for the most basic noise-addition mechanisms (Laplace and Gaussian noise addition), with guarantees that scale with the variance of the queries rather than the square of their range. Our characterization opens the door to new insights and new algorithms for the fundamental problem of achieving generalization in adaptive data analysis.

</details>

<details>

<summary>2021-06-21 09:07:17 - Long short-term relevance learning</summary>

- *Bram van de Weg, Lars Greve, Bojana Rosic*

- `2106.12694v1` - [abs](http://arxiv.org/abs/2106.12694v1) - [pdf](http://arxiv.org/pdf/2106.12694v1)

> To incorporate prior knowledge as well as measurement uncertainties in the traditional long short term memory (LSTM) neural networks, an efficient sparse Bayesian training algorithm is introduced to the network architecture. The proposed scheme automatically determines relevant neural connections and adapts accordingly, in contrast to the classical LSTM solution. Due to its flexibility, the new LSTM scheme is less prone to overfitting, and hence can approximate time dependent solutions by use of a smaller data set. On a structural nonlinear finite element application we show that the self-regulating framework does not require prior knowledge of a suitable network architecture and size, while ensuring satisfying accuracy at reasonable computational cost.

</details>

<details>

<summary>2021-06-21 09:24:40 - Tumor Radiogenomics with Bayesian Layered Variable Selection</summary>

- *Shariq Mohammed, Sebastian Kurtek, Karthik Bharath, Arvind Rao, Veerabhadran Baladandayuthapani*

- `2106.10941v1` - [abs](http://arxiv.org/abs/2106.10941v1) - [pdf](http://arxiv.org/pdf/2106.10941v1)

> We propose a statistical framework to integrate radiological magnetic resonance imaging (MRI) and genomic data to identify the underlying radiogenomic associations in lower grade gliomas (LGG). We devise a novel imaging phenotype by dividing the tumor region into concentric spherical layers that mimics the tumor evolution process. MRI data within each layer is represented by voxel--intensity-based probability density functions which capture the complete information about tumor heterogeneity. Under a Riemannian-geometric framework these densities are mapped to a vector of principal component scores which act as imaging phenotypes. Subsequently, we build Bayesian variable selection models for each layer with the imaging phenotypes as the response and the genomic markers as predictors. Our novel hierarchical prior formulation incorporates the interior-to-exterior structure of the layers, and the correlation between the genomic markers. We employ a computationally-efficient Expectation--Maximization-based strategy for estimation. Simulation studies demonstrate the superior performance of our approach compared to other approaches. With a focus on the cancer driver genes in LGG, we discuss some biologically relevant findings. Genes implicated with survival and oncogenesis are identified as being associated with the spherical layers, which could potentially serve as early-stage diagnostic markers for disease monitoring, prior to routine invasive approaches.

</details>

<details>

<summary>2021-06-21 09:40:51 - Diffusion Approximations for a Class of Sequential Testing Problems</summary>

- *Victor F. Araman, Rene Caldentey*

- `2102.07030v3` - [abs](http://arxiv.org/abs/2102.07030v3) - [pdf](http://arxiv.org/pdf/2102.07030v3)

> We consider a decision maker who must choose an action in order to maximize a reward function that depends also on an unknown parameter {\Theta}. The decision maker can delay taking the action in order to experiment and gather additional information on {\Theta}. We model the decision maker's problem using a Bayesian sequential experimentation framework and use dynamic programming and diffusion-asymptotic analysis to solve it. For that, we scale our problem in a way that both the average number of experiments that is conducted per unit of time is large and the informativeness of each individual experiment is low. Under such regime, we derive a diffusion approximation for the sequential experimentation problem, which provides a number of important insights about the nature of the problem and its solution. Our solution method also shows that the complexity of the problem grows only quadratically with the cardinality of the set of actions from which the decision maker can choose. We illustrate our methodology and results using a concrete application in the context of assortment selection and new product introduction. Specifically, we study the problem of a seller who wants to select an optimal assortment of products to launch into the marketplace and is uncertain about consumers' preferences. Motivated by emerging practices in e-commerce, we assume that the seller is able to use a crowdvoting system to learn these preferences before a final assortment decision is made. In this context, we undertake an extensive numerical analysis to assess the value of learning and demonstrate the effectiveness and robustness of the heuristics derived from the diffusion approximation.

</details>

<details>

<summary>2021-06-21 10:15:20 - Addressing Catastrophic Forgetting in Few-Shot Problems</summary>

- *Pauching Yap, Hippolyt Ritter, David Barber*

- `2005.00146v3` - [abs](http://arxiv.org/abs/2005.00146v3) - [pdf](http://arxiv.org/pdf/2005.00146v3)

> Neural networks are known to suffer from catastrophic forgetting when trained on sequential datasets. While there have been numerous attempts to solve this problem in large-scale supervised classification, little has been done to overcome catastrophic forgetting in few-shot classification problems. We demonstrate that the popular gradient-based model-agnostic meta-learning algorithm (MAML) indeed suffers from catastrophic forgetting and introduce a Bayesian online meta-learning framework that tackles this problem. Our framework utilises Bayesian online learning and meta-learning along with Laplace approximation and variational inference to overcome catastrophic forgetting in few-shot classification problems. The experimental evaluations demonstrate that our framework can effectively achieve this goal in comparison with various baselines. As an additional utility, we also demonstrate empirically that our framework is capable of meta-learning on sequentially arriving few-shot tasks from a stationary task distribution.

</details>

<details>

<summary>2021-06-21 13:11:52 - Analytically Tractable Bayesian Deep Q-Learning</summary>

- *Luong Ha, Nguyen, James-A. Goulet*

- `2106.11086v1` - [abs](http://arxiv.org/abs/2106.11086v1) - [pdf](http://arxiv.org/pdf/2106.11086v1)

> Reinforcement learning (RL) has gained increasing interest since the demonstration it was able to reach human performance on video game benchmarks using deep Q-learning (DQN). The current consensus for training neural networks on such complex environments is to rely on gradient-based optimization. Although alternative Bayesian deep learning methods exist, most of them still rely on gradient-based optimization, and they typically do not scale on benchmarks such as the Atari game environment. Moreover none of these approaches allow performing the analytical inference for the weights and biases defining the neural network. In this paper, we present how we can adapt the temporal difference Q-learning framework to make it compatible with the tractable approximate Gaussian inference (TAGI), which allows learning the parameters of a neural network using a closed-form analytical method. Throughout the experiments with on- and off-policy reinforcement learning approaches, we demonstrate that TAGI can reach a performance comparable to backpropagation-trained networks while using fewer hyperparameters, and without relying on gradient-based optimization.

</details>

<details>

<summary>2021-06-21 13:59:47 - Deep Gaussian Processes: A Survey</summary>

- *Kalvik Jakkala*

- `2106.12135v1` - [abs](http://arxiv.org/abs/2106.12135v1) - [pdf](http://arxiv.org/pdf/2106.12135v1)

> Gaussian processes are one of the dominant approaches in Bayesian learning. Although the approach has been applied to numerous problems with great success, it has a few fundamental limitations. Multiple methods in literature have addressed these limitations. However, there has not been a comprehensive survey of the topics as of yet. Most existing surveys focus on only one particular variant of Gaussian processes and their derivatives. This survey details the core motivations for using Gaussian processes, their mathematical formulations, limitations, and research themes that have flourished over the years to address said limitations. Furthermore, one particular research area is Deep Gaussian Processes (DGPs), it has improved substantially in the past decade. The significant publications that advanced the forefront of this research area are outlined in their survey. Finally, a brief discussion on open problems and research directions for future work is presented at the end.

</details>

<details>

<summary>2021-06-21 14:48:53 - Bayesian Inference for Gamma Models</summary>

- *Jingyu He, Nicholas Polson, Jianeng Xu*

- `2106.01906v2` - [abs](http://arxiv.org/abs/2106.01906v2) - [pdf](http://arxiv.org/pdf/2106.01906v2)

> We use the theory of normal variance-mean mixtures to derive a data augmentation scheme for models that include gamma functions. Our methodology applies to many situations in statistics and machine learning, including Multinomial-Dirichlet distributions, Negative binomial regression, Poisson-Gamma hierarchical models, Extreme value models, to name but a few. All of those models include a gamma function which does not admit a natural conjugate prior distribution providing a significant challenge to inference and prediction. To provide a data augmentation strategy, we construct and develop the theory of the class of Exponential Reciprocal Gamma distributions. This allows scalable EM and MCMC algorithms to be developed. We illustrate our methodology on a number of examples, including gamma shape inference, negative binomial regression and Dirichlet allocation. Finally, we conclude with directions for future research.

</details>

<details>

<summary>2021-06-21 15:09:31 - On the definition of likelihood function</summary>

- *Flávio B. Gonçalves, Pedro Franklin*

- `1906.10733v4` - [abs](http://arxiv.org/abs/1906.10733v4) - [pdf](http://arxiv.org/pdf/1906.10733v4)

> We discuss a general definition of likelihood function in terms of Radon-Nikod\'{y}m derivatives. The definition is validated by the Likelihood Principle once we establish a result regarding the proportionality of likelihood functions under different dominating measures. This general framework is particularly useful when there exists no or more than one obvious choice for a dominating measure as in some infinite-dimensional models. We discuss the importance of considering continuous versions of densities and how these are related to the Likelihood Principle and the basic concept of likelihood. We also discuss the use of the predictive measure as a dominating measure in the Bayesian approach. Finally, some examples illustrate the general definition of likelihood function and the importance of choosing particular dominating measures in some cases.

</details>

<details>

<summary>2021-06-21 16:25:55 - Why flatness does and does not correlate with generalization for deep neural networks</summary>

- *Shuofeng Zhang, Isaac Reid, Guillermo Valle Pérez, Ard Louis*

- `2103.06219v2` - [abs](http://arxiv.org/abs/2103.06219v2) - [pdf](http://arxiv.org/pdf/2103.06219v2)

> The intuition that local flatness of the loss landscape is correlated with better generalization for deep neural networks (DNNs) has been explored for decades, spawning many different flatness measures. Recently, this link with generalization has been called into question by a demonstration that many measures of flatness are vulnerable to parameter re-scaling which arbitrarily changes their value without changing neural network outputs.   Here we show that, in addition, some popular variants of SGD such as Adam and Entropy-SGD, can also break the flatness-generalization correlation. As an alternative to flatness measures, we use a function based picture and propose using the log of Bayesian prior upon initialization, $\log P(f)$, as a predictor of the generalization when a DNN converges on function $f$ after training to zero error. The prior is directly proportional to the Bayesian posterior for functions that give zero error on a test set. For the case of image classification, we show that $\log P(f)$ is a significantly more robust predictor of generalization than flatness measures are.   Whilst local flatness measures fail under parameter re-scaling, the prior/posterior, which is global quantity, remains invariant under re-scaling. Moreover, the correlation with generalization as a function of data complexity remains good for different variants of SGD.

</details>

<details>

<summary>2021-06-21 17:26:41 - Implementing Approximate Bayesian Inference using Adaptive Quadrature: the aghq Package</summary>

- *Alex Stringer*

- `2101.04468v3` - [abs](http://arxiv.org/abs/2101.04468v3) - [pdf](http://arxiv.org/pdf/2101.04468v3)

> The aghq package for implementing approximate Bayesian inference using adaptive quadrature is introduced. The method and software are described, and use of the package in making approximate Bayesian inferences in several challenging low- and high-dimensional models is illustrated. Examples include an infectious disease model; an astrostatistical model for estimating the mass of the Milky Way; two examples in non-Gaussian model-based geostatistics including one incorporating zero-inflation which is not easily fit using other methods; and a model for zero-inflated, overdispersed count data. The aghq package is especially compatible with the popular TMB interface for automatic differentiation and Laplace approximation, and existing users of that software can make approximate Bayesian inferences with aghq using very little additional code. The aghq package is available from CRAN and complete code for all examples in this paper can be found at https://github.com/awstringer1/aghq-software-paper-code.

</details>

<details>

<summary>2021-06-21 17:35:43 - The Tajima heterochronous n-coalescent: inference from heterochronously sampled molecular data</summary>

- *Lorenzo Cappello, Amandine Veber, Julia A. Palacios*

- `2004.06826v2` - [abs](http://arxiv.org/abs/2004.06826v2) - [pdf](http://arxiv.org/pdf/2004.06826v2)

> The observed sequence variation at a locus informs about the evolutionary history of the sample and past population size dynamics. The Kingman coalescent is used in a generative model of molecular sequence variation to infer evolutionary parameters. However, it is well understood that inference under this model does not scale well with sample size. Here, we build on recent work based on a lower resolution coalescent process, the Tajima coalescent, to model longitudinal samples. While the Kingman coalescent models the ancestry of labeled individuals, the heterochronous Tajima coalescent models the ancestry of individuals labeled by their sampling time. We propose a new inference scheme for the reconstruction of effective population size trajectories based on this model with the potential to improve computational efficiency. Modeling of longitudinal samples is necessary for applications (e.g. ancient DNA and RNA from rapidly evolving pathogens like viruses) and statistically desirable (variance reduction and parameter identifiability). We propose an efficient algorithm to calculate the likelihood and employ a Bayesian nonparametric procedure to infer the population size trajectory. We provide a new MCMC sampler to explore the space of heterochronous Tajima's genealogies and model parameters. We compare our procedure with state-of-the-art methodologies in simulations and applications.

</details>

<details>

<summary>2021-06-21 18:10:42 - Bayesian Neural Network via Stochastic Gradient Descent</summary>

- *Abhinav Sagar*

- `2006.08453v4` - [abs](http://arxiv.org/abs/2006.08453v4) - [pdf](http://arxiv.org/pdf/2006.08453v4)

> The goal of bayesian approach used in variational inference is to minimize the KL divergence between variational distribution and unknown posterior distribution. This is done by maximizing the Evidence Lower Bound (ELBO). A neural network is used to parametrize these distributions using Stochastic Gradient Descent. This work extends the work done by others by deriving the variational inference models. We show how SGD can be applied on bayesian neural networks by gradient estimation techniques. For validation, we have tested our model on 5 UCI datasets and the metrics chosen for evaluation are Root Mean Square Error (RMSE) error and negative log likelihood. Our work considerably beats the previous state of the art approaches for regression using bayesian neural networks.

</details>

<details>

<summary>2021-06-21 18:12:29 - Stochastic Bayesian Neural Networks</summary>

- *Abhinav Sagar*

- `2008.07587v3` - [abs](http://arxiv.org/abs/2008.07587v3) - [pdf](http://arxiv.org/pdf/2008.07587v3)

> Bayesian neural networks perform variational inference over the weights however calculation of the posterior distribution remains a challenge. Our work builds on variational inference techniques for bayesian neural networks using the original Evidence Lower Bound. In this paper, we present a stochastic bayesian neural network in which we maximize Evidence Lower Bound using a new objective function which we name as Stochastic Evidence Lower Bound. We evaluate our network on 5 publicly available UCI datasets using test RMSE and log likelihood as the evaluation metrics. We demonstrate that our work not only beats the previous state of the art algorithms but is also scalable to larger datasets.

</details>

<details>

<summary>2021-06-21 22:08:17 - Local convexity of the TAP free energy and AMP convergence for Z2-synchronization</summary>

- *Michael Celentano, Zhou Fan, Song Mei*

- `2106.11428v1` - [abs](http://arxiv.org/abs/2106.11428v1) - [pdf](http://arxiv.org/pdf/2106.11428v1)

> We study mean-field variational Bayesian inference using the TAP approach, for Z2-synchronization as a prototypical example of a high-dimensional Bayesian model. We show that for any signal strength $\lambda > 1$ (the weak-recovery threshold), there exists a unique local minimizer of the TAP free energy functional near the mean of the Bayes posterior law. Furthermore, the TAP free energy in a local neighborhood of this minimizer is strongly convex. Consequently, a natural-gradient/mirror-descent algorithm achieves linear convergence to this minimizer from a local initialization, which may be obtained by a finite number of iterates of Approximate Message Passing (AMP). This provides a rigorous foundation for variational inference in high dimensions via minimization of the TAP free energy.   We also analyze the finite-sample convergence of AMP, showing that AMP is asymptotically stable at the TAP minimizer for any $\lambda > 1$, and is linearly convergent to this minimizer from a spectral initialization for sufficiently large $\lambda$. Such a guarantee is stronger than results obtainable by state evolution analyses, which only describe a fixed number of AMP iterations in the infinite-sample limit.   Our proofs combine the Kac-Rice formula and Sudakov-Fernique Gaussian comparison inequality to analyze the complexity of critical points that satisfy strong convexity and stability conditions within their local neighborhoods.

</details>

<details>

<summary>2021-06-21 22:24:23 - Learning versus Unlearning: An Experiment on Retractions</summary>

- *Duarte Gonçalves, Jonathan Libgober, Jack Willis*

- `2106.11433v1` - [abs](http://arxiv.org/abs/2106.11433v1) - [pdf](http://arxiv.org/pdf/2106.11433v1)

> Widely discredited ideas nevertheless persist. Why do people fail to ``unlearn''? We study one explanation: beliefs are resistant to retractions (the revoking of earlier information). Our experimental design identifies unlearning -- i.e., updating from retractions -- and enables its comparison with learning from equivalent new information. Across different kinds of retractions -- for instance, those consistent or contradictory with the prior, or those occurring when prior beliefs are either extreme or moderate -- subjects do not fully unlearn from retractions and update less from them than from equivalent new information. This phenomenon is not explained by most of the well-studied violations of Bayesian updating, which yield differing predictions in our design. However, it is consistent with difficulties in conditional reasoning, which have been documented in other domains and circumstances.

</details>

<details>

<summary>2021-06-22 07:34:16 - Bayesian models for prediction of the set-difference in volleyball</summary>

- *Ioannis Ntzoufras, Vasilis Palaskas, Sotiris Drikos*

- `1911.04541v5` - [abs](http://arxiv.org/abs/1911.04541v5) - [pdf](http://arxiv.org/pdf/1911.04541v5)

> The aim of this paper is to study and develop Bayesian models for the analysis of volleyball match outcomes as recorded by the set-difference. Due to the peculiarity of the outcome variable (set-difference) which takes discrete values from $-3$ to $3$, we cannot consider standard models based on the usual Poisson or binomial assumptions used for other sports such as football/soccer. Hence, the first and foremost challenge was to build models appropriate for the set-differences of each volleyball match. Here we consider two major approaches:   a) an ordered multinomial logistic regression model and   b) a model based on a truncated version of the Skellam distribution.   For the first model, we consider the set-difference as an ordinal response variable within the framework of multinomial logistic regression models. Concerning the second model, we adjust the Skellam distribution in order to account for the volleyball rules. We fit and compare both models with the same covariate structure as in Karlis & Ntzoufras (2003). Both models are fitted, illustrated and compared within Bayesian framework using data from both the regular season and the play-offs of the season 2016/17 of the Greek national men's volleyball league A1.

</details>

<details>

<summary>2021-06-22 07:53:17 - On Stein Variational Neural Network Ensembles</summary>

- *Francesco D'Angelo, Vincent Fortuin, Florian Wenzel*

- `2106.10760v2` - [abs](http://arxiv.org/abs/2106.10760v2) - [pdf](http://arxiv.org/pdf/2106.10760v2)

> Ensembles of deep neural networks have achieved great success recently, but they do not offer a proper Bayesian justification. Moreover, while they allow for averaging of predictions over several hypotheses, they do not provide any guarantees for their diversity, leading to redundant solutions in function space. In contrast, particle-based inference methods, such as Stein variational gradient descent (SVGD), offer a Bayesian framework, but rely on the choice of a kernel to measure the similarity between ensemble members. In this work, we study different SVGD methods operating in the weight space, function space, and in a hybrid setting. We compare the SVGD approaches to other ensembling-based methods in terms of their theoretical properties and assess their empirical performance on synthetic and real-world tasks. We find that SVGD using functional and hybrid kernels can overcome the limitations of deep ensembles. It improves on functional diversity and uncertainty estimation and approaches the true Bayesian posterior more closely. Moreover, we show that using stochastic SVGD updates, as opposed to the standard deterministic ones, can further improve the performance.

</details>

<details>

<summary>2021-06-22 07:58:26 - Rank-normalization, folding, and localization: An improved $\widehat{R}$ for assessing convergence of MCMC</summary>

- *Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, Paul-Christian Bürkner*

- `1903.08008v5` - [abs](http://arxiv.org/abs/1903.08008v5) - [pdf](http://arxiv.org/pdf/1903.08008v5)

> Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic $\widehat{R}$ of Gelman and Rubin (1992) has serious flaws. Traditional $\widehat{R}$ will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.

</details>

<details>

<summary>2021-06-22 11:40:00 - Simulation-Driven COVID-19 Epidemiological Modeling with Social Media</summary>

- *Jose Storopoli, Andre Luis Marques Ferreira dos Santos, Alessandra Cristina Guedes Pellini, Breck Baldwin*

- `2106.11686v1` - [abs](http://arxiv.org/abs/2106.11686v1) - [pdf](http://arxiv.org/pdf/2106.11686v1)

> Modern Bayesian approaches and workflows emphasize in how simulation is important in the context of model developing. Simulation can help researchers understand how the model behaves in a controlled setting and can be used to stress the model in different ways before it is exposed to any real data. This improved understanding could be beneficial in epidemiological models, specially when dealing with COVID-19. Unfortunately, few researchers perform any simulations. We present a simulation algorithm that implements a simple agent-based model for disease transmission that works with a standard compartment epidemiological model for COVID-19. Our algorithm can be applied in different parameterizations to reflect several plausible epidemic scenarios. Additionally, we also model how social media information in the form of daily symptom mentions can be incorporate into COVID-19 epidemiological models. We test our social media COVID-19 model with two experiments. The first using simulated data from our agent-based simulation algorithm and the second with real data using a machine learning tweet classifier to identify tweets that mention symptoms from noise. Our results shows how a COVID-19 model can be (1) used to incorporate social media data and (2) assessed and evaluated with simulated and real data.

</details>

<details>

<summary>2021-06-22 12:41:19 - A Deep Latent Space Model for Graph Representation Learning</summary>

- *Hanxuan Yang, Qingchao Kong, Wenji Mao*

- `2106.11721v1` - [abs](http://arxiv.org/abs/2106.11721v1) - [pdf](http://arxiv.org/pdf/2106.11721v1)

> Graph representation learning is a fundamental problem for modeling relational data and benefits a number of downstream applications. Traditional Bayesian-based graph models and recent deep learning based GNN either suffer from impracticability or lack interpretability, thus combined models for undirected graphs have been proposed to overcome the weaknesses. As a large portion of real-world graphs are directed graphs (of which undirected graphs are special cases), in this paper, we propose a Deep Latent Space Model (DLSM) for directed graphs to incorporate the traditional latent variable based generative model into deep learning frameworks. Our proposed model consists of a graph convolutional network (GCN) encoder and a stochastic decoder, which are layer-wise connected by a hierarchical variational auto-encoder architecture. By specifically modeling the degree heterogeneity using node random factors, our model possesses better interpretability in both community structure and degree heterogeneity. For fast inference, the stochastic gradient variational Bayes (SGVB) is adopted using a non-iterative recognition model, which is much more scalable than traditional MCMC-based methods. The experiments on real-world datasets show that the proposed model achieves the state-of-the-art performances on both link prediction and community detection tasks while learning interpretable node embeddings. The source code is available at https://github.com/upperr/DLSM.

</details>

<details>

<summary>2021-06-22 13:39:01 - Global inducing point variational posteriors for Bayesian neural networks and deep Gaussian processes</summary>

- *Sebastian W. Ober, Laurence Aitchison*

- `2005.08140v5` - [abs](http://arxiv.org/abs/2005.08140v5) - [pdf](http://arxiv.org/pdf/2005.08140v5)

> We consider the optimal approximate posterior over the top-layer weights in a Bayesian neural network for regression, and show that it exhibits strong dependencies on the lower-layer weights. We adapt this result to develop a correlated approximate posterior over the weights at all layers in a Bayesian neural network. We extend this approach to deep Gaussian processes, unifying inference in the two model classes. Our approximate posterior uses learned "global" inducing points, which are defined only at the input layer and propagated through the network to obtain inducing inputs at subsequent layers. By contrast, standard, "local", inducing point methods from the deep Gaussian process literature optimise a separate set of inducing inputs at every layer, and thus do not model correlations across layers. Our method gives state-of-the-art performance for a variational Bayesian method, without data augmentation or tempering, on CIFAR-10 of 86.7%, which is comparable to SGMCMC without tempering but with data augmentation (88% in Wenzel et al. 2020).

</details>

<details>

<summary>2021-06-22 19:59:27 - A Compartment Model of Human Mobility and Early Covid-19 Dynamics in NYC</summary>

- *Ian Frankenburg, Sudipto Banerjee*

- `2102.01821v2` - [abs](http://arxiv.org/abs/2102.01821v2) - [pdf](http://arxiv.org/pdf/2102.01821v2)

> In this paper, we build a mechanistic system to understand the relation between a reduction in human mobility and Covid-19 spread dynamics within New York City. To this end, we propose a multivariate compartmental model that jointly models smartphone mobility data and case counts during the first 90 days of the epidemic. Parameter calibration is achieved through the formulation of a general Bayesian hierarchical model to provide uncertainty quantification of resulting estimates. The open-source probabilistic programming language Stan is used for the requisite computation. Through sensitivity analysis and out-of-sample forecasting, we find our simple and interpretable model provides evidence that reductions in human mobility altered case dynamics.

</details>

<details>

<summary>2021-06-22 20:28:21 - Meta-Learning Divergences of Variational Inference</summary>

- *Ruqi Zhang, Yingzhen Li, Christopher De Sa, Sam Devlin, Cheng Zhang*

- `2007.02912v2` - [abs](http://arxiv.org/abs/2007.02912v2) - [pdf](http://arxiv.org/pdf/2007.02912v2)

> Variational inference (VI) plays an essential role in approximate Bayesian inference due to its computational efficiency and broad applicability. Crucial to the performance of VI is the selection of the associated divergence measure, as VI approximates the intractable distribution by minimizing this divergence. In this paper we propose a meta-learning algorithm to learn the divergence metric suited for the task of interest, automating the design of VI methods. In addition, we learn the initialization of the variational parameters without additional cost when our method is deployed in the few-shot learning scenarios. We demonstrate our approach outperforms standard VI on Gaussian mixture distribution approximation, Bayesian neural network regression, image generation with variational autoencoders and recommender systems with a partial variational autoencoder.

</details>

<details>

<summary>2021-06-23 10:28:00 - Physics-constrained Bayesian inference of state functions in classical density-functional theory</summary>

- *Peter Yatsyshin, Serafim Kalliadasis, Andrew B. Duncan*

- `2010.03374v4` - [abs](http://arxiv.org/abs/2010.03374v4) - [pdf](http://arxiv.org/pdf/2010.03374v4)

> We develop a novel data-driven approach to the inverse problem of classical statistical mechanics: given experimental data on the collective motion of a classical many-body system, how does one characterise the free energy landscape of that system? By combining non-parametric Bayesian inference with physically-motivated constraints, we develop an efficient learning algorithm which automates the construction of approximate free energy functionals. In contrast to optimisation-based machine learning approaches, which seek to minimise a cost function, the central idea of the proposed Bayesian inference is to propagate a set of prior assumptions through the model, derived from physical principles. The experimental data is used to probabilistically weigh the possible model predictions. This naturally leads to humanly interpretable algorithms with full uncertainty quantification of predictions. In our case, the output of the learning algorithm is a probability distribution over a family of free energy functionals, consistent with the observed particle data. We find that surprisingly small data samples contain sufficient information for inferring highly accurate analytic expressions of the underlying free energy functionals, making our algorithm highly data efficient. We consider excluded volume particle interactions, which are ubiquitous in nature, whilst being highly challenging for modelling in terms of free energy. To validate our approach we consider the paradigmatic case of one-dimensional fluid and develop inference algorithms for the canonical and grand-canonical statistical-mechanical ensembles. Extensions to higher-dimensional systems are conceptually straightforward, whilst standard coarse-graining techniques allow one to easily incorporate attractive interactions.

</details>

<details>

<summary>2021-06-23 16:54:55 - Bayesian Deep Learning Hyperparameter Search for Robust Function Mapping to Polynomials with Noise</summary>

- *Nidhin Harilal, Udit Bhatia, Auroop R. Ganguly*

- `2106.12532v1` - [abs](http://arxiv.org/abs/2106.12532v1) - [pdf](http://arxiv.org/pdf/2106.12532v1)

> Advances in neural architecture search, as well as explainability and interpretability of connectionist architectures, have been reported in the recent literature. However, our understanding of how to design Bayesian Deep Learning (BDL) hyperparameters, specifically, the depth, width and ensemble size, for robust function mapping with uncertainty quantification, is still emerging. This paper attempts to further our understanding by mapping Bayesian connectionist representations to polynomials of different orders with varying noise types and ratios. We examine the noise-contaminated polynomials to search for the combination of hyperparameters that can extract the underlying polynomial signals while quantifying uncertainties based on the noise attributes. Specifically, we attempt to study the question that an appropriate neural architecture and ensemble configuration can be found to detect a signal of any n-th order polynomial contaminated with noise having different distributions and signal-to-noise (SNR) ratios and varying noise attributes. Our results suggest the possible existence of an optimal network depth as well as an optimal number of ensembles for prediction skills and uncertainty quantification, respectively. However, optimality is not discernible for width, even though the performance gain reduces with increasing width at high values of width. Our experiments and insights can be directional to understand theoretical properties of BDL representations and to design practical solutions.

</details>

<details>

<summary>2021-06-23 17:25:43 - Approximate Bayesian Computation with Path Signatures</summary>

- *Joel Dyer, Patrick Cannon, Sebastian M Schmon*

- `2106.12555v1` - [abs](http://arxiv.org/abs/2106.12555v1) - [pdf](http://arxiv.org/pdf/2106.12555v1)

> Simulation models of scientific interest often lack a tractable likelihood function, precluding standard likelihood-based statistical inference. A popular likelihood-free method for inferring simulator parameters is approximate Bayesian computation, where an approximate posterior is sampled by comparing simulator output and observed data. However, effective measures of closeness between simulated and observed data are generally difficult to construct, particularly for time series data which are often high-dimensional and structurally complex. Existing approaches typically involve manually constructing summary statistics, requiring substantial domain expertise and experimentation, or rely on unrealistic assumptions such as iid data. Others are inappropriate in more complex settings like multivariate or irregularly sampled time series data. In this paper, we introduce the use of path signatures as a natural candidate feature set for constructing distances between time series data for use in approximate Bayesian computation algorithms. Our experiments show that such an approach can generate more accurate approximate Bayesian posteriors than existing techniques for time series models.

</details>

<details>

<summary>2021-06-23 19:24:39 - Blang: Bayesian declarative modelling of general data structures and inference via algorithms based on distribution continua</summary>

- *Alexandre Bouchard-Côté, Kevin Chern, Davor Cubranic, Sahand Hosseini, Justin Hume, Matteo Lepur, Zihui Ouyang, Giorgio Sgarbi*

- `1912.10396v2` - [abs](http://arxiv.org/abs/1912.10396v2) - [pdf](http://arxiv.org/pdf/1912.10396v2)

> Consider a Bayesian inference problem where a variable of interest does not take values in a Euclidean space. These "non-standard" data structures are in reality fairly common. They are frequently used in problems involving latent discrete factor models, networks, and domain specific problems such as sequence alignments and reconstructions, pedigrees, and phylogenies. In principle, Bayesian inference should be particularly well-suited in such scenarios, as the Bayesian paradigm provides a principled way to obtain confidence assessment for random variables of any type. However, much of the recent work on making Bayesian analysis more accessible and computationally efficient has focused on inference in Euclidean spaces.   In this paper, we introduce Blang, a domain specific language and library aimed at bridging this gap. Blang allows users to perform Bayesian analysis on arbitrary data types while using a declarative syntax similar to BUGS. Blang is augmented with intuitive language additions to create data types of the user's choosing. To perform inference at scale on such arbitrary state spaces, Blang leverages recent advances in sequential Monte Carlo and non-reversible Markov chain Monte Carlo methods.

</details>

<details>

<summary>2021-06-24 02:17:16 - A Non-ergodic Effective Amplitude Ground-Motion Model for California</summary>

- *Grigorios Lavrentiadis, Norman A. Abrahamson, Nicolas M. Kuehn*

- `2106.07834v2` - [abs](http://arxiv.org/abs/2106.07834v2) - [pdf](http://arxiv.org/pdf/2106.07834v2)

> A new non-ergodic ground-motion model (GMM) for effective amplitude spectral ($EAS$) values for California is presented in this study. $EAS$, which is defined in Goulet et al. (2018), is a smoothed rotation-independent Fourier amplitude spectrum of the two horizontal components of an acceleration time history. The main motivation for developing a non-ergodic $EAS$ GMM, rather than a spectral acceleration GMM, is that the scaling of $EAS$ does not depend on spectral shape, and therefore, the more frequent small magnitude events can be used in the estimation of the non-ergodic terms.   The model is developed using the California subset of the NGAWest2 dataset Ancheta et al. (2013). The Bayless and Abrahamson (2019b) (BA18) ergodic $EAS$ GMM was used as backbone to constrain the average source, path, and site scaling. The non-ergodic GMM is formulated as a Bayesian hierarchical model: the non-ergodic source and site terms are modeled as spatially varying coefficients following the approach of Landwehr et al. (2016), and the non-ergodic path effects are captured by the cell-specific anelastic attenuation attenuation following the approach of Dawood and Rodriguez-Marek (2013). Close to stations and past events, the mean values of the non-ergodic terms deviate from zero to capture the systematic effects and their epistemic uncertainty is small. In areas with sparse data, the epistemic uncertainty of the non-ergodic terms is large, as the systematic effects cannot be determined.   The non-ergodic total aleatory standard deviation is approximately $30$ to $40\%$ smaller than the total aleatory standard deviation of BA18. This reduction in the aleatory variability has a significant impact on hazard calculations at large return periods. The epistemic uncertainty of the ground motion predictions is small in areas close to stations and past events.

</details>

<details>

<summary>2021-06-24 17:19:58 - MIxBN: library for learning Bayesian networks from mixed data</summary>

- *Anna V. Bubnova, Irina Deeva, Anna V. Kalyuzhnaya*

- `2106.13194v1` - [abs](http://arxiv.org/abs/2106.13194v1) - [pdf](http://arxiv.org/pdf/2106.13194v1)

> This paper describes a new library for learning Bayesian networks from data containing discrete and continuous variables (mixed data). In addition to the classical learning methods on discretized data, this library proposes its algorithm that allows structural learning and parameters learning from mixed data without discretization since data discretization leads to information loss. This algorithm based on mixed MI score function for structural learning, and also linear regression and Gaussian distribution approximation for parameters learning. The library also offers two algorithms for enumerating graph structures - the greedy Hill-Climbing algorithm and the evolutionary algorithm. Thus the key capabilities of the proposed library are as follows: (1) structural and parameters learning of a Bayesian network on discretized data, (2) structural and parameters learning of a Bayesian network on mixed data using the MI mixed score function and Gaussian approximation, (3) launching learning algorithms on one of two algorithms for enumerating graph structures - Hill-Climbing and the evolutionary algorithm. Since the need for mixed data representation comes from practical necessity, the advantages of our implementations are evaluated in the context of solving approximation and gap recovery problems on synthetic data and real datasets.

</details>

<details>

<summary>2021-06-25 12:18:29 - Multivariate hierarchical analysis of car crashes data considering a spatial network lattice</summary>

- *Andrea Gilardi, Jorge Mateu, Riccardo Borgoni, Robin Lovelace*

- `2011.12595v2` - [abs](http://arxiv.org/abs/2011.12595v2) - [pdf](http://arxiv.org/pdf/2011.12595v2)

> Road traffic casualties represent a hidden global epidemic, demanding evidence-based interventions. This paper demonstrates a network lattice approach for identifying road segments of particular concern, based on a case study of a major city (Leeds, UK), in which 5,862 crashes of different severities were recorded over an eight-year period (2011-2018). We consider a family of Bayesian hierarchical models that include spatially structured and unstructured random effects, to capture the dependencies between the severity levels. Results highlight roads that are more prone to collisions, relative to estimated traffic volumes, in the northwest and south of city-centre. We analyse the Modifiable Areal Unit Problem (MAUP), proposing a novel procedure to investigate the presence of MAUP on a network lattice. We conclude that our methods enable a reliable estimation of road safety levels to help identify "hotspots" on the road network and to inform effective local interventions.

</details>

<details>

<summary>2021-06-26 07:10:58 - Functional Classwise Principal Component Analysis: A Novel Classification Framework</summary>

- *Avishek Chatterjee, Satyaki Mazumder, Koel Das*

- `2106.13959v1` - [abs](http://arxiv.org/abs/2106.13959v1) - [pdf](http://arxiv.org/pdf/2106.13959v1)

> In recent times, functional data analysis (FDA) has been successfully applied in the field of high dimensional data classification. In this paper, we present a novel classification framework using functional data and classwise Principal Component Analysis (PCA). Our proposed method can be used in high dimensional time series data which typically suffers from small sample size problem. Our method extracts a piece wise linear functional feature space and is particularly suitable for hard classification problems.The proposed framework converts time series data into functional data and uses classwise functional PCA for feature extraction followed by classification using a Bayesian linear classifier. We demonstrate the efficacy of our proposed method by applying it to both synthetic data sets and real time series data from diverse fields including but not limited to neuroscience, food science, medical sciences and chemometrics.

</details>

<details>

<summary>2021-06-26 15:28:38 - The mbsts package: Multivariate Bayesian Structural Time Series Models in R</summary>

- *Ning Ning, Jinwen Qiu*

- `2106.14045v1` - [abs](http://arxiv.org/abs/2106.14045v1) - [pdf](http://arxiv.org/pdf/2106.14045v1)

> The multivariate Bayesian structural time series (MBSTS) model \citep{qiu2018multivariate,Jammalamadaka2019Predicting} as a generalized version of many structural time series models, deals with inference and prediction for multiple correlated time series, where one also has the choice of using a different candidate pool of contemporaneous predictors for each target series. The MBSTS model has wide applications and is ideal for feature selection, time series forecasting, nowcasting, inferring causal impact, and others. This paper demonstrates how to use the R package \pkg{mbsts} for MBSTS modeling, establishing a bridge between user-friendly and developer-friendly functions in package and the corresponding methodology. A simulated dataset and object-oriented functions in the \pkg{mbsts} package are explained in the way that enables users to flexibly add or deduct some components, as well as to simplify or complicate some settings.

</details>

<details>

<summary>2021-06-26 17:01:49 - Bayesian Joint Chance Constrained Optimization: Approximations and Statistical Consistency</summary>

- *Prateek Jaiswal, Harsha Honnappa, Vinayak A. Rao*

- `2106.12199v2` - [abs](http://arxiv.org/abs/2106.12199v2) - [pdf](http://arxiv.org/pdf/2106.12199v2)

> This paper considers data-driven chance-constrained stochastic optimization problems in a Bayesian framework. Bayesian posteriors afford a principled mechanism to incorporate data and prior knowledge into stochastic optimization problems. However, the computation of Bayesian posteriors is typically an intractable problem, and has spawned a large literature on approximate Bayesian computation. Here, in the context of chance-constrained optimization, we focus on the question of statistical consistency (in an appropriate sense) of the optimal value, computed using an approximate posterior distribution. To this end, we rigorously prove a frequentist consistency result demonstrating the convergence of the optimal value to the optimal value of a fixed, parameterized constrained optimization problem. We augment this by also establishing a probabilistic rate of convergence of the optimal value. We also prove the convex feasibility of the approximate Bayesian stochastic optimization problem. Finally, we demonstrate the utility of our approach on an optimal staffing problem for an M/M/c queueing model.

</details>

<details>

<summary>2021-06-26 20:14:31 - Bayesian Time-Varying Tensor Vector Autoregressive Models for Dynamic Effective Connectivity</summary>

- *Wei Zhang, Ivor Cribben, sonia Petrone, Michele Guindani*

- `2106.14083v1` - [abs](http://arxiv.org/abs/2106.14083v1) - [pdf](http://arxiv.org/pdf/2106.14083v1)

> Recent developments in functional magnetic resonance imaging (fMRI) investigate how some brain regions directly influence the activity of other regions of the brain {\it dynamically} throughout the course of an experiment, namely dynamic effective connectivity. Time-varying vector autoregressive (TV-VAR) models have been employed to draw inferencesfor this purpose, but they are very computationally intensive, since the number of parameters to be estimated increases quadratically with the number of time series. In this paper, we propose a computationally efficient Bayesian time-varying VAR approach for modeling high-dimensional time series. The proposed framework employs a tensor decomposition for the VAR coefficient matrices at different lags. Dynamically varying connectivity patterns are captured by assuming that at any given time only a subset of components in the tensor decomposition is active. Latent binary time series select the active components at each time via a convenient Ising prior specification. The proposed prior structure encourages sparsity in the tensor structure and allows to ascertain model complexity through the posterior distribution. More specifically, sparsity-inducing priors are employed to allow for global-local shrinkage of the coefficients, to determine automatically the rank of the tensor decomposition and to guide the selection of the lags of the auto-regression. We show the performances of our model formulation via simulation studies and data from a real fMRI study involving a book reading experiment.

</details>

<details>

<summary>2021-06-26 20:22:54 - Deep Learning Partial Least Squares</summary>

- *Nicholas Polson, Vadim Sokolov, Jianeng Xu*

- `2106.14085v1` - [abs](http://arxiv.org/abs/2106.14085v1) - [pdf](http://arxiv.org/pdf/2106.14085v1)

> High dimensional data reduction techniques are provided by using partial least squares within deep learning. Our framework provides a nonlinear extension of PLS together with a disciplined approach to feature selection and architecture design in deep learning. This leads to a statistical interpretation of deep learning that is tailor made for predictive problems. We can use the tools of PLS, such as scree-plot, bi-plot to provide model diagnostics. Posterior predictive uncertainty is available using MCMC methods at the last layer. Thus we achieve the best of both worlds: scalability and fast predictive rule construction together with uncertainty quantification. Our key construct is to employ deep learning within PLS by predicting the output scores as a deep learner of the input scores. As with PLS our X-scores are constructed using SVD and applied to both regression and classification problems and are fast and scalable. Following Frank and Friedman 1993, we provide a Bayesian shrinkage interpretation of our nonlinear predictor. We introduce a variety of new partial least squares models: PLS-ReLU, PLS-Autoencoder, PLS-Trees and PLS-GP. To illustrate our methodology, we use simulated examples and the analysis of preferences of orange juice and predicting wine quality as a function of input characteristics. We also illustrate Brillinger's estimation procedure to provide the feature selection and data dimension reduction. Finally, we conclude with directions for future research.

</details>

<details>

<summary>2021-06-27 05:09:57 - An Approach to Causal Inference over Stochastic Networks</summary>

- *Duncan A. Clark, Mark S. Handcock*

- `2106.14145v1` - [abs](http://arxiv.org/abs/2106.14145v1) - [pdf](http://arxiv.org/pdf/2106.14145v1)

> Claiming causal inferences in network settings necessitates careful consideration of the often complex dependency between outcomes for actors. Of particular importance are treatment spillover or outcome interference effects. We consider causal inference when the actors are connected via an underlying network structure. Our key contribution is a model for causality when the underlying network is unobserved and the actor covariates evolve stochastically over time. We develop a joint model for the relational and covariate generating process that avoids restrictive separability assumptions and deterministic network assumptions that do not hold in the majority of social network settings of interest. Our framework utilizes the highly general class of Exponential-family Random Network models (ERNM) of which Markov Random Fields (MRF) and Exponential-family Random Graph models (ERGM) are special cases. We present potential outcome based inference within a Bayesian framework, and propose a simple modification to the exchange algorithm to allow for sampling from ERNM posteriors. We present results of a simulation study demonstrating the validity of the approach. Finally, we demonstrate the value of the framework in a case-study of smoking over time in the context of adolescent friendship networks.

</details>

<details>

<summary>2021-06-27 16:16:19 - Learning Gaussian Networks</summary>

- *Dan Geiger, David Heckerman*

- `1302.6808v3` - [abs](http://arxiv.org/abs/1302.6808v3) - [pdf](http://arxiv.org/pdf/1302.6808v3)

> We describe algorithms for learning Bayesian networks from a combination of user knowledge and statistical data. The algorithms have two components: a scoring metric and a search procedure. The scoring metric takes a network structure, statistical data, and a user's prior knowledge, and returns a score proportional to the posterior probability of the network structure given the data. The search procedure generates networks for evaluation by the scoring metric. Previous work has concentrated on metrics for domains containing only discrete variables, under the assumption that data represents a multinomial sample. In this paper, we extend this work, developing scoring metrics for domains containing all continuous variables or a mixture of discrete and continuous variables, under the assumption that continuous data is sampled from a multivariate normal distribution. Our work extends traditional statistical approaches for identifying vanishing regression coefficients in that we identify two important assumptions, called event equivalence and parameter modularity, that when combined allow the construction of prior distributions for multivariate normal parameters from a single prior Bayesian network specified by a user.

</details>

<details>

<summary>2021-06-28 12:11:37 - BNPqte: A Bayesian Nonparametric Approach to Causal Inference on Quantiles in R</summary>

- *Chuji Luo, Michael J. Daniels*

- `2106.14599v1` - [abs](http://arxiv.org/abs/2106.14599v1) - [pdf](http://arxiv.org/pdf/2106.14599v1)

> In this article, we introduce the BNPqte R package which implements the Bayesian nonparametric approach of Xu, Daniels and Winterstein (2018) for estimating quantile treatment effects in observational studies. This approach provides flexible modeling of the distributions of potential outcomes, so it is capable of capturing a variety of underlying relationships among the outcomes, treatments and confounders and estimating multiple quantile treatment effects simultaneously. Specifically, this approach uses a Bayesian additive regression trees (BART) model to estimate the propensity score and a Dirichlet process mixture (DPM) of multivariate normals model to estimate the conditional distribution of the potential outcome given the estimated propensity score. The BNPqte R package provides a fast implementation for this approach by designing efficient R functions for the DPM of multivariate normals model in joint and conditional density estimation. These R functions largely improve the efficiency of the DPM model in density estimation, compared to the popular DPpackage. BART-related R functions in the BNPqte R package are inherited from the BART R package with two modifications on variable importance and split probability. To maximize computational efficiency, the actual sampling and computation for each model are carried out in C++ code. The Armadillo C++ library is also used for fast linear algebra calculations.

</details>

<details>

<summary>2021-06-28 12:47:28 - A Comparison of Prior Elicitation Aggregation using the Classical Method and SHELF</summary>

- *Cameron J. Williams, Kevin J. Wilson, Nina Wilson*

- `2001.11365v3` - [abs](http://arxiv.org/abs/2001.11365v3) - [pdf](http://arxiv.org/pdf/2001.11365v3)

> Subjective Bayesian prior distributions elicited from experts can be aggregated together to form group priors. This paper compares aggregated priors formed by Equal Weight Aggregation, the Classical Method, and the Sheffield Elicitation Framework to each other and individual expert priors, using an expert elicitation carried out for a clinical trial. Aggregation methods and individual expert prior distributions are compared using proper scoring rules to compare the informativeness and calibration of the distributions. The three aggregation methods outperform the individual experts, and the Sheffield Elicitation Framework performs best amongst them.

</details>

<details>

<summary>2021-06-28 15:37:48 - Nested sampling with any prior you like</summary>

- *Justin Alsing, Will Handley*

- `2102.12478v3` - [abs](http://arxiv.org/abs/2102.12478v3) - [pdf](http://arxiv.org/pdf/2102.12478v3)

> Nested sampling is an important tool for conducting Bayesian analysis in Astronomy and other fields, both for sampling complicated posterior distributions for parameter inference, and for computing marginal likelihoods for model comparison. One technical obstacle to using nested sampling in practice is the requirement (for most common implementations) that prior distributions be provided in the form of transformations from the unit hyper-cube to the target prior density. For many applications - particularly when using the posterior from one experiment as the prior for another - such a transformation is not readily available. In this letter we show that parametric bijectors trained on samples from a desired prior density provide a general-purpose method for constructing transformations from the uniform base density to a target prior, enabling the practical use of nested sampling under arbitrary priors. We demonstrate the use of trained bijectors in conjunction with nested sampling on a number of examples from cosmology.

</details>

<details>

<summary>2021-06-28 20:54:41 - Fast Bayesian Variable Selection in Binomial and Negative Binomial Regression</summary>

- *Martin Jankowiak*

- `2106.14981v1` - [abs](http://arxiv.org/abs/2106.14981v1) - [pdf](http://arxiv.org/pdf/2106.14981v1)

> Bayesian variable selection is a powerful tool for data analysis, as it offers a principled method for variable selection that accounts for prior information and uncertainty. However, wider adoption of Bayesian variable selection has been hampered by computational challenges, especially in difficult regimes with a large number of covariates or non-conjugate likelihoods. Generalized linear models for count data, which are prevalent in biology, ecology, economics, and beyond, represent an important special case. Here we introduce an efficient MCMC scheme for variable selection in binomial and negative binomial regression that exploits Tempered Gibbs Sampling (Zanella and Roberts, 2019) and that includes logistic regression as a special case. In experiments we demonstrate the effectiveness of our approach, including on cancer data with seventeen thousand covariates.

</details>

<details>

<summary>2021-06-29 08:29:57 - Local field reconstruction from rotating coil measurements in particle accelerator magnets</summary>

- *Ion Gabriel Ion, Melvin Liebsch, Abele Simona, Dimitrios Loukrezis, Carlo Petrone, Stephan Russenschuck, Herbert De Gersem, Sebastian Schöps*

- `2106.15168v1` - [abs](http://arxiv.org/abs/2106.15168v1) - [pdf](http://arxiv.org/pdf/2106.15168v1)

> In this paper a general approach to reconstruct three dimensional field solutions in particle accelerator magnets from distributed magnetic measurements is presented. To exploit the locality of the measurement operation a special discretization of the Laplace equation is used. Extracting the coefficients of the field representations yields an inverse problem which is solved by Bayesian inversion. This allows not only to pave the way for uncertainty quantification, but also to derive a suitable regularization. The approach is applied to rotating coil measurements and can be extended to any other measurement procedure.

</details>

<details>

<summary>2021-06-29 17:45:37 - On the Optimal Configuration of a Square Array Group Testing Algorithm</summary>

- *Ugnė Čižikovienė, Viktor Skorniakov*

- `2106.15603v1` - [abs](http://arxiv.org/abs/2106.15603v1) - [pdf](http://arxiv.org/pdf/2106.15603v1)

> Up to date, only lower and upper bounds for the optimal configuration of a Square Array (A2) Group Testing (GT) algorithm are known. We establish exact analytical formulae and provide a couple of applications of our result. First, we compare the A2 GT scheme to several other classical GT schemes in terms of the gain per specimen attained at optimal configuration. Second, operating under objective Bayesian framework with the loss designed to attain minimum at optimal GT configuration, we suggest the preferred choice of the group size under natural minimal assumptions: the prior information regarding the prevalence suggests that grouping and application of A2 is better than individual testing. The same suggestion is provided for the Minimax strategy.

</details>

<details>

<summary>2021-06-29 19:42:53 - Likelihoods and Parameter Priors for Bayesian Networks</summary>

- *David Heckerman, Dan Geiger*

- `2105.06241v2` - [abs](http://arxiv.org/abs/2105.06241v2) - [pdf](http://arxiv.org/pdf/2105.06241v2)

> We develop simple methods for constructing likelihoods and parameter priors for learning about the parameters and structure of a Bayesian network. In particular, we introduce several assumptions that permit the construction of likelihoods and parameter priors for a large number of Bayesian-network structures from a small set of assessments. The most notable assumption is that of likelihood equivalence, which says that data can not help to discriminate network structures that encode the same assertions of conditional independence. We describe the constructions that follow from these assumptions, and also present a method for directly computing the marginal likelihood of a random sample with no missing observations. Also, we show how these assumptions lead to a general framework for characterizing parameter priors of multivariate distributions.

</details>

<details>

<summary>2021-06-29 22:25:53 - Fast Game Content Adaptation Through Bayesian-based Player Modelling</summary>

- *Miguel González-Duque, Rasmus Berg Palm, Sebastian Risi*

- `2105.08484v2` - [abs](http://arxiv.org/abs/2105.08484v2) - [pdf](http://arxiv.org/pdf/2105.08484v2)

> In games, as well as many user-facing systems, adapting content to users' preferences and experience is an important challenge. This paper explores a novel method to realize this goal in the context of dynamic difficulty adjustment (DDA). Here the aim is to constantly adapt the content of a game to the skill level of the player, keeping them engaged by avoiding states that are either too difficult or too easy. Current systems for DDA rely on expensive data mining, or on hand-crafted rules designed for particular domains, and usually adapts to keep players in the flow, leaving no room for the designer to present content that is purposefully easy or difficult. This paper presents Fast Bayesian Content Adaption (FBCA), a system for DDA that is agnostic to the domain and that can target particular difficulties. We deploy this framework in two different domains: the puzzle game Sudoku, and a simple Roguelike game. By modifying the acquisition function's optimization, we are reliably able to present a content with a bespoke difficulty for players with different skill levels in less than five iterations for Sudoku and fifteen iterations for the simple Roguelike. Our method significantly outperforms simpler DDA heuristics with the added benefit of maintaining a model of the user. These results point towards a promising alternative for content adaption in a variety of different domains.

</details>

<details>

<summary>2021-06-30 11:00:24 - Variational Refinement for Importance Sampling Using the Forward Kullback-Leibler Divergence</summary>

- *Ghassen Jerfel, Serena Wang, Clara Fannjiang, Katherine A. Heller, Yian Ma, Michael I. Jordan*

- `2106.15980v1` - [abs](http://arxiv.org/abs/2106.15980v1) - [pdf](http://arxiv.org/pdf/2106.15980v1)

> Variational Inference (VI) is a popular alternative to asymptotically exact sampling in Bayesian inference. Its main workhorse is optimization over a reverse Kullback-Leibler divergence (RKL), which typically underestimates the tail of the posterior leading to miscalibration and potential degeneracy. Importance sampling (IS), on the other hand, is often used to fine-tune and de-bias the estimates of approximate Bayesian inference procedures. The quality of IS crucially depends on the choice of the proposal distribution. Ideally, the proposal distribution has heavier tails than the target, which is rarely achievable by minimizing the RKL. We thus propose a novel combination of optimization and sampling techniques for approximate Bayesian inference by constructing an IS proposal distribution through the minimization of a forward KL (FKL) divergence. This approach guarantees asymptotic consistency and a fast convergence towards both the optimal IS estimator and the optimal variational approximation. We empirically demonstrate on real data that our method is competitive with variational boosting and MCMC.

</details>

<details>

<summary>2021-06-30 15:19:10 - Bayesian Spanning Tree: Estimating the Backbone of the Dependence Graph</summary>

- *Leo L. Duan, David B. Dunson*

- `2106.16120v1` - [abs](http://arxiv.org/abs/2106.16120v1) - [pdf](http://arxiv.org/pdf/2106.16120v1)

> In multivariate data analysis, it is often important to estimate a graph characterizing dependence among (p) variables. A popular strategy uses the non-zero entries in a (p\times p) covariance or precision matrix, typically requiring restrictive modeling assumptions for accurate graph recovery. To improve model robustness, we instead focus on estimating the {\em backbone} of the dependence graph. We use a spanning tree likelihood, based on a minimalist graphical model that is purposely overly-simplified. Taking a Bayesian approach, we place a prior on the space of trees and quantify uncertainty in the graphical model. In both theory and experiments, we show that this model does not require the population graph to be a spanning tree or the covariance to satisfy assumptions beyond positive-definiteness. The model accurately recovers the backbone of the population graph at a rate competitive with existing approaches but with better robustness. We show combinatorial properties of the spanning tree, which may be of independent interest, and develop an efficient Gibbs sampler for Bayesian inference. Analyzing electroencephalography data using a Hidden Markov Model with each latent state modeled by a spanning tree, we show that results are much more interpretable compared with popular alternatives.

</details>

<details>

<summary>2021-06-30 23:16:21 - Bayesian semiparametric long memory models for discretized event data</summary>

- *Antik Chakraborty, Otso Ovaskainen, David B. Dunson*

- `2004.08309v2` - [abs](http://arxiv.org/abs/2004.08309v2) - [pdf](http://arxiv.org/pdf/2004.08309v2)

> We introduce a new class of semiparametric latent variable models for long memory discretized event data. The proposed methodology is motivated by a study of bird vocalizations in the Amazon rain forest; the timings of vocalizations exhibit self-similarity and long range dependence ruling out models based on Poisson processes. The proposed class of FRActional Probit (FRAP) models is based on thresholding of a latent process consisting of an additive expansion of a smooth Gaussian process with a fractional Brownian motion. We develop a Bayesian approach to inference using Markov chain Monte Carlo, and show good performance in simulation studies. Applying the methods to the Amazon bird vocalization data, we find substantial evidence for self-similarity and non-Markovian/Poisson dynamics. To accommodate the bird vocalization data, in which there are many different species of birds exhibiting their own vocalization dynamics, a hierarchical expansion of FRAP is provided in Supplementary Materials.

</details>


## 2021-07

<details>

<summary>2021-07-01 04:55:52 - How many samples are needed to reliably approximate the best linear estimator for a linear inverse problem?</summary>

- *Gernot Holler*

- `2107.00215v1` - [abs](http://arxiv.org/abs/2107.00215v1) - [pdf](http://arxiv.org/pdf/2107.00215v1)

> The linear minimum mean squared error (LMMSE) estimator is the best linear estimator for a Bayesian linear inverse problem with respect to the mean squared error. It arises as the solution operator to a Tikhonov-type regularized inverse problem with a particular quadratic discrepancy term and a particular quadratic regularization operator. To be able to evaluate the LMMSE estimator, one must know the forward operator and the first two statistical moments of both the prior and the noise. If such knowledge is not available, one may approximate the LMMSE estimator based on given samples. In this work, it is investigated, in a finite-dimensional setting, how many samples are needed to reliably approximate the LMMSE estimator, in the sense that, with high probability, the mean squared error of the approximation is smaller than a given multiple of the mean squared error of the LMMSE estimator.

</details>

<details>

<summary>2021-07-01 08:10:38 - Managing driving modes in automated driving systems</summary>

- *David Ríos Insua, William N. Caballero, Roi Naveiro*

- `2107.00280v1` - [abs](http://arxiv.org/abs/2107.00280v1) - [pdf](http://arxiv.org/pdf/2107.00280v1)

> Current technologies are unable to produce massively deployable, fully autonomous vehicles that do not require human intervention. Such technological limitations are projected to persist for decades. Therefore, roadway scenarios requiring a driver to regain control of a vehicle, and vice versa, will remain critical to the safe operation of semi-autonomous vehicles for the foreseeable future. Herein, we adopt a comprehensive perspective on this problem taking into account the operational design domain, driver and environment monitoring, trajectory planning, and driver intervention performance assessment. Leveraging decision analysis and Bayesian forecasting, both the support of driving mode management decisions and the issuing of early warnings to the driver are addressed. A statistical modeling framework is created and a suite of algorithms are developed to manage driving modes and issue relevant warnings in accordance with the management by exception principle. The efficacy of these developed methods are then illustrated and examined via a simulated case study.

</details>

<details>

<summary>2021-07-01 11:22:55 - A semiparametric Bayesian approach to epidemics, with application to the spread of the coronavirus MERS in South Korea in 2015</summary>

- *Michael Schweinberger, Rashmi P. Bomiriya, Sergii Babkin*

- `2107.00375v1` - [abs](http://arxiv.org/abs/2107.00375v1) - [pdf](http://arxiv.org/pdf/2107.00375v1)

> We consider incomplete observations of stochastic processes governing the spread of infectious diseases through finite populations by way of contact. We propose a flexible semiparametric modeling framework with at least three advantages. First, it enables researchers to study the structure of a population contact network and its impact on the spread of infectious diseases. Second, it can accommodate short- and long-tailed degree distributions and detect potential superspreaders, who represent an important public health concern. Third, it addresses the important issue of incomplete data. Starting from first principles, we show when the incomplete-data generating process is ignorable for the purpose of Bayesian inference for the parameters of the population model. We demonstrate the semiparametric modeling framework by simulations and an application to the partially observed MERS epidemic in South Korea in 2015. We conclude with an extended discussion of open questions and directions for future research.

</details>

<details>

<summary>2021-07-01 11:45:39 - A statistical model to assess risk for supporting SARS-CoV-2 quarantine decisions</summary>

- *Sonja Jäckle, Elias Röger, Volker Dicken, Benjamin Geisler, Jakob Schumacher, Max Westphal*

- `2010.15677v4` - [abs](http://arxiv.org/abs/2010.15677v4) - [pdf](http://arxiv.org/pdf/2010.15677v4)

> In February 2020 the first human infection with SARS-CoV-2 was reported in Germany. Since then the local public health offices have been responsible to monitor and react to the dynamics of the pandemic. One of their major tasks is to contain the spread of the virus after potential spreading events, for example when one or multiple participants have a positive test result after a group meeting (e.g. at school, at a sports event or at work). In this case, contacts of the infected person have to be traced and potentially are quarantined (at home) for a period of time. When all relevant contact persons obtain a negative polymerase chain reaction (PCR) test result, the quarantine may be stopped. However, tracing and testing of all contacts is time-consuming, costly and (thus) not always feasible. This motivates our work, in which we present a statistical model for the probability that no transmission of Sars-CoV-2 occurred given an arbitrary number of test results at potentially different timepoints. Hereby, the time-dependent sensitivity and specificity of the conducted PCR test are taken in account. We employ a parametric Bayesian model which can be adopted to different situations when specific prior knowledge is available. This is illustrated for group events in German school classes and applied to exemplary real-world data from this context. Our approach has the potential to support important quarantine decisions with the goal to achieve a better balance between necessary containment of the pandemic and preservation of social and economic life. The focus of future work should be on further refinement and evaluation of quarantine decisions based on our statistical model.

</details>

<details>

<summary>2021-07-01 14:29:30 - Parallel Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints</summary>

- *Eduardo C. Garrido-Merchán, Daniel Hernández-Lobato*

- `2004.00601v2` - [abs](http://arxiv.org/abs/2004.00601v2) - [pdf](http://arxiv.org/pdf/2004.00601v2)

> Real-world problems often involve the optimization of several objectives under multiple constraints. An example is the hyper-parameter tuning problem of machine learning algorithms. In particular, the minimization of the estimation of the generalization error of a deep neural network and at the same time the minimization of its prediction time. We may also consider as a constraint that the deep neural network must be implemented in a chip with an area below some size. Here, both the objectives and the constraint are black boxes, i.e., functions whose analytical expressions are unknown and are expensive to evaluate. Bayesian optimization (BO) methodologies have given state-of-the-art results for the optimization of black-boxes. Nevertheless, most BO methods are sequential and evaluate the objectives and the constraints at just one input location, iteratively. Sometimes, however, we may have resources to evaluate several configurations in parallel. Notwithstanding, no parallel BO method has been proposed to deal with the optimization of multiple objectives under several constraints. If the expensive evaluations can be carried out in parallel (as when a cluster of computers is available), sequential evaluations result in a waste of resources. This article introduces PPESMOC, Parallel Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints, an information-based batch method for the simultaneous optimization of multiple expensive-to-evaluate black-box functions under the presence of several constraints. Iteratively, PPESMOC selects a batch of input locations at which to evaluate the black-boxes so as to maximally reduce the entropy of the Pareto set of the optimization problem. We present empirical evidence in the form of synthetic, benchmark and real-world experiments that illustrate the effectiveness of PPESMOC.

</details>

<details>

<summary>2021-07-01 14:49:19 - A sparse Bayesian hierarchical vector autoregressive model for microbial dynamics in a wastewater treatment plant</summary>

- *Naomi E. Hannaford, Sarah E. Heaps, Tom M. W. Nye, Thomas P. Curtis, Ben Allen, Andrew Golightly, Darren J. Wilkinson*

- `2107.00502v1` - [abs](http://arxiv.org/abs/2107.00502v1) - [pdf](http://arxiv.org/pdf/2107.00502v1)

> Proper function of a wastewater treatment plant (WWTP) relies on maintaining a delicate balance between a multitude of competing microorganisms. Gaining a detailed understanding of the complex network of interactions therein is essential to maximising not only current operational efficiencies, but also for the effective design of new treatment technologies. Metagenomics offers an insight into these dynamic systems through the analysis of the microbial DNA sequences present. Unique taxa are inferred through sequence clustering to form operational taxonomic units (OTUs), with per-taxa abundance estimates obtained from corresponding sequence counts. The data in this study comprise weekly OTU counts from an activated sludge (AS) tank of a WWTP. To model the OTU dynamics, we develop a Bayesian hierarchical vector autoregressive model, which is a linear approximation to the commonly used generalised Lotka-Volterra (gLV) model. To tackle the high dimensionality and sparsity of the data, they are first clustered into 12 "bins" using a seasonal phase-based approach. The autoregressive coefficient matrix is assumed to be sparse, so we explore different shrinkage priors by analysing simulated data sets before selecting the regularised horseshoe prior for the biological application. We find that ammonia and chemical oxygen demand have a positive relationship with several bins and pH has a positive relationship with one bin. These results are supported by findings in the biological literature. We identify several negative interactions, which suggests OTUs in different bins may be competing for resources and that these relationships are complex. We also identify two positive interactions. Although simpler than a gLV model, our vector autoregression offers valuable insight into the microbial dynamics of the WWTP.

</details>

<details>

<summary>2021-07-01 14:59:07 - A Fully Bayesian Gradient-Free Supervised Dimension Reduction Method using Gaussian Processes</summary>

- *Raphael Gautier, Piyush Pandita, Sayan Ghosh, Dimitri Mavris*

- `2008.03534v2` - [abs](http://arxiv.org/abs/2008.03534v2) - [pdf](http://arxiv.org/pdf/2008.03534v2)

> Modern day engineering problems are ubiquitously characterized by sophisticated computer codes that map parameters or inputs to an underlying physical process. In other situations, experimental setups are used to model the physical process in a laboratory, ensuring high precision while being costly in materials and logistics. In both scenarios, only limited amount of data can be generated by querying the expensive information source at a finite number of inputs or designs. This problem is compounded further in the presence of a high-dimensional input space. State-of-the-art parameter space dimension reduction methods, such as active subspace, aim to identify a subspace of the original input space that is sufficient to explain the output response. These methods are restricted by their reliance on gradient evaluations or copious data, making them inadequate to expensive problems without direct access to gradients. The proposed methodology is gradient-free and fully Bayesian, as it quantifies uncertainty in both the low-dimensional subspace and the surrogate model parameters. This enables a full quantification of epistemic uncertainty and robustness to limited data availability. It is validated on multiple datasets from engineering and science and compared to two other state-of-the-art methods based on four aspects: a) recovery of the active subspace, b) deterministic prediction accuracy, c) probabilistic prediction accuracy, and d) training time. The comparison shows that the proposed method improves the active subspace recovery and predictive accuracy, in both the deterministic and probabilistic sense, when only few model observations are available for training, at the cost of increased training time.

</details>

<details>

<summary>2021-07-01 16:41:52 - Adaptive Bayesian density estimation in sup-norm</summary>

- *Zacharie Naulet*

- `1805.05816v3` - [abs](http://arxiv.org/abs/1805.05816v3) - [pdf](http://arxiv.org/pdf/1805.05816v3)

> We investigate the problem of deriving adaptive posterior rates of contraction on $\mathbb{L}^{\infty}$ balls in density estimation. Although it is known that log-density priors can achieve optimal rates when the true density is sufficiently smooth, adaptive rates were still to be proven. Here we establish that the so-called spike-and-slab prior can achieve adaptive and optimal posterior contraction rates. Along the way, we prove a generic $\mathbb{L}^{\infty}$ contraction result for log-density priors with independent wavelet coefficients. Interestingly, our approach is different from previous works on $\mathbb{L}^{\infty}$ contraction and is reminiscent of the classical test-based approach used in Bayesian nonparametrics. Moreover, we require no lower bound on the smoothness of the true density, albeit the rates are deteriorated by an extra $\log(n)$ factor in the case of low smoothness.

</details>

<details>

<summary>2021-07-01 18:49:04 - Estimating Perinatal Critical Windows of Susceptibility to Environmental Mixtures via Structured Bayesian Regression Tree Pairs</summary>

- *Daniel Mork, Ander Wilson*

- `2102.09071v3` - [abs](http://arxiv.org/abs/2102.09071v3) - [pdf](http://arxiv.org/pdf/2102.09071v3)

> Maternal exposure to environmental chemicals during pregnancy can alter birth and children's health outcomes. Research seeks to identify critical windows, time periods when the exposures can change future health outcomes, and estimate the exposure-response relationship. Existing statistical approaches focus on estimation of the association between maternal exposure to a single environmental chemical observed at high-temporal resolution, such as weekly throughout pregnancy, and children's health outcomes. Extending to multiple chemicals observed at high temporal resolution poses a dimensionality problem and statistical methods are lacking. We propose a tree-based model for mixtures of exposures that are observed at high temporal resolution. The proposed approach uses an additive ensemble of structured tree-pairs that define structured main effects and interactions between time-resolved predictors and variable selection to select out of the model predictors not correlated with the outcome. We apply our method in a simulation and the analysis of the relationship between five exposures measured weekly throughout pregnancy and resulting birth weight in a Denver, Colorado birth cohort. We identified critical windows during which fine particulate matter, sulfur dioxide, and temperature are negatively associated with birth weight and an interaction between fine particulate matter and temperature. Software is made available in the R package dlmtree.

</details>

<details>

<summary>2021-07-01 21:09:06 - q-Paths: Generalizing the Geometric Annealing Path using Power Means</summary>

- *Vaden Masrani, Rob Brekelmans, Thang Bui, Frank Nielsen, Aram Galstyan, Greg Ver Steeg, Frank Wood*

- `2107.00745v1` - [abs](http://arxiv.org/abs/2107.00745v1) - [pdf](http://arxiv.org/pdf/2107.00745v1)

> Many common machine learning methods involve the geometric annealing path, a sequence of intermediate densities between two distributions of interest constructed using the geometric average. While alternatives such as the moment-averaging path have demonstrated performance gains in some settings, their practical applicability remains limited by exponential family endpoint assumptions and a lack of closed form energy function. In this work, we introduce $q$-paths, a family of paths which is derived from a generalized notion of the mean, includes the geometric and arithmetic mixtures as special cases, and admits a simple closed form involving the deformed logarithm function from nonextensive thermodynamics. Following previous analysis of the geometric path, we interpret our $q$-paths as corresponding to a $q$-exponential family of distributions, and provide a variational representation of intermediate densities as minimizing a mixture of $\alpha$-divergences to the endpoints. We show that small deviations away from the geometric path yield empirical gains for Bayesian inference using Sequential Monte Carlo and generative model evaluation using Annealed Importance Sampling.

</details>

<details>

<summary>2021-07-02 02:49:21 - Variational Inference with Vine Copulas: An efficient Approach for Bayesian Computer Model Calibration</summary>

- *Vojtech Kejzlar, Tapabrata Maiti*

- `2003.12890v2` - [abs](http://arxiv.org/abs/2003.12890v2) - [pdf](http://arxiv.org/pdf/2003.12890v2)

> With the advancements of computer architectures, the use of computational models proliferates to solve complex problems in many scientific applications such as nuclear physics and climate research. However, the potential of such models is often hindered because they tend to be computationally expensive and consequently ill-fitting for uncertainty quantification. Furthermore, they are usually not calibrated with real-time observations. We develop a computationally efficient algorithm based on variational Bayes inference (VBI) for calibration of computer models with Gaussian processes. Unfortunately, the speed and scalability of VBI diminishes when applied to the calibration framework with dependent data. To preserve the efficiency of VBI, we adopt a pairwise decomposition of the data likelihood using vine copulas that separate the information on dependence structure in data from their marginal distributions. We provide both theoretical and empirical evidence for the computational scalability of our methodology and describe all the necessary details for an efficient implementation of the proposed algorithm. We also demonstrate the opportunities given by our method for practitioners on a real data example through calibration of the Liquid Drop Model of nuclear binding energies.

</details>

<details>

<summary>2021-07-02 07:05:11 - Reconsidering Dependency Networks from an Information Geometry Perspective</summary>

- *Kazuya Takabatake, Shotaro Akaho*

- `2107.00871v1` - [abs](http://arxiv.org/abs/2107.00871v1) - [pdf](http://arxiv.org/pdf/2107.00871v1)

> Dependency networks (Heckerman et al., 2000) are potential probabilistic graphical models for systems comprising a large number of variables. Like Bayesian networks, the structure of a dependency network is represented by a directed graph, and each node has a conditional probability table. Learning and inference are realized locally on individual nodes; therefore, computation remains tractable even with a large number of variables. However, the dependency network's learned distribution is the stationary distribution of a Markov chain called pseudo-Gibbs sampling and has no closed-form expressions. This technical disadvantage has impeded the development of dependency networks. In this paper, we consider a certain manifold for each node. Then, we can interpret pseudo-Gibbs sampling as iterative m-projections onto these manifolds. This interpretation provides a theoretical bound for the location where the stationary distribution of pseudo-Gibbs sampling exists in distribution space. Furthermore, this interpretation involves structure and parameter learning algorithms as optimization problems. In addition, we compare dependency and Bayesian networks experimentally. The results demonstrate that the dependency network and the Bayesian network have roughly the same performance in terms of the accuracy of their learned distributions. The results also show that the dependency network can learn much faster than the Bayesian network.

</details>

<details>

<summary>2021-07-02 07:41:31 - Geometric variational inference</summary>

- *Philipp Frank, Reimar Leike, Torsten A. Enßlin*

- `2105.10470v2` - [abs](http://arxiv.org/abs/2105.10470v2) - [pdf](http://arxiv.org/pdf/2105.10470v2)

> Efficiently accessing the information contained in non-linear and high dimensional probability distributions remains a core challenge in modern statistics. Traditionally, estimators that go beyond point estimates are either categorized as Variational Inference (VI) or Markov-Chain Monte-Carlo (MCMC) techniques. While MCMC methods that utilize the geometric properties of continuous probability distributions to increase their efficiency have been proposed, VI methods rarely use the geometry. This work aims to fill this gap and proposes geometric Variational Inference (geoVI), a method based on Riemannian geometry and the Fisher information metric. It is used to construct a coordinate transformation that relates the Riemannian manifold associated with the metric to Euclidean space. The distribution, expressed in the coordinate system induced by the transformation, takes a particularly simple form that allows for an accurate variational approximation by a normal distribution. Furthermore, the algorithmic structure allows for an efficient implementation of geoVI which is demonstrated on multiple examples, ranging from low-dimensional illustrative ones to non-linear, hierarchical Bayesian inverse problems in thousands of dimensions.

</details>

<details>

<summary>2021-07-02 14:22:22 - Bayesian Hyperparameter Optimization with BoTorch, GPyTorch and Ax</summary>

- *Daniel T. Chang*

- `1912.05686v2` - [abs](http://arxiv.org/abs/1912.05686v2) - [pdf](http://arxiv.org/pdf/1912.05686v2)

> Deep learning models are full of hyperparameters, which are set manually before the learning process can start. To find the best configuration for these hyperparameters in such a high dimensional space, with time-consuming and expensive model training / validation, is not a trivial challenge. Bayesian optimization is a powerful tool for the joint optimization of hyperparameters, efficiently trading off exploration and exploitation of the hyperparameter space. In this paper, we discuss Bayesian hyperparameter optimization, including hyperparameter optimization, Bayesian optimization, and Gaussian processes. We also review BoTorch, GPyTorch and Ax, the new open-source frameworks that we use for Bayesian optimization, Gaussian process inference and adaptive experimentation, respectively. For experimentation, we apply Bayesian hyperparameter optimization, for optimizing group weights, to weighted group pooling, which couples unsupervised tiered graph autoencoders learning and supervised graph prediction learning for molecular graphs. We find that Ax, BoTorch and GPyTorch together provide a simple-to-use but powerful framework for Bayesian hyperparameter optimization, using Ax's high-level API that constructs and runs a full optimization loop and returns the best hyperparameter configuration.

</details>

<details>

<summary>2021-07-02 16:15:19 - Stratified sampling and bootstrapping for approximate Bayesian computation</summary>

- *Umberto Picchini, Richard G. Everitt*

- `1905.07976v4` - [abs](http://arxiv.org/abs/1905.07976v4) - [pdf](http://arxiv.org/pdf/1905.07976v4)

> Approximate Bayesian computation (ABC) is computationally intensive for complex model simulators. To exploit expensive simulations, data-resampling via bootstrapping can be employed to obtain many artificial datasets at little cost. However, when using this approach within ABC, the posterior variance is inflated, thus resulting in biased posterior inference. Here we use stratified Monte Carlo to considerably reduce the bias induced by data resampling. We also show empirically that it is possible to obtain reliable inference using a larger than usual ABC threshold. Finally, we show that with stratified Monte Carlo we obtain a less variable ABC likelihood. Ultimately we show how our approach improves the computational efficiency of the ABC samplers. We construct several ABC samplers employing our methodology, such as rejection and importance ABC samplers, and ABC-MCMC samplers. We consider simulation studies for static (Gaussian, g-and-k distribution, Ising model, astronomical model) and dynamic models (Lotka-Volterra). We compare against state-of-art sequential Monte Carlo ABC samplers, synthetic likelihoods, and likelihood-free Bayesian optimization. For a computationally expensive Lotka-Volterra case study, we found that our strategy leads to a more than 10-fold computational saving, compared to a sampler that does not use our novel approach.

</details>

<details>

<summary>2021-07-02 20:57:22 - Bayesian two-interval test</summary>

- *Nicolas Meyer, Erik-André Sauleau*

- `2107.01271v1` - [abs](http://arxiv.org/abs/2107.01271v1) - [pdf](http://arxiv.org/pdf/2107.01271v1)

> The null hypothesis test (NHT) is widely used for validating scientific hypotheses but is actually highly criticized. Although Bayesian tests overcome several criticisms, some limits remain. We propose a Bayesian two-interval test (2IT) in which two hypotheses on an effect being present or absent are expressed as prespecified joint or disjoint intervals and their posterior probabilities are computed. The same formalism can be applied for superiority, non-inferiority, or equivalence tests. The 2IT was studied for three real examples and three sets of simulations (comparison of a proportion and a mean to a reference and comparison of two proportions). Several scenarios were created (with different sample sizes), and simulations were conducted to compute the probabilities of the parameter of interest being in the interval corresponding to either hypothesis given the data generated under one of the hypotheses. Posterior estimates were obtained using conjugacy with a low-informative prior. Bias was also estimated. The probability of accepting a hypothesis when that hypothesis is true progressively increases the sample size, tending towards 1, while the probability of accepting the other hypothesis is always very low (less than 5%) and tends towards 0. The speed of convergence varies with the gap between the hypotheses and with their width. In the case of a mean, the bias is low and rapidly becomes negligible. We propose a Bayesian test that follows a scientifically sound process, in which two interval hypotheses are explicitly used and tested. The proposed test has almost none of the limitations of the NHT and suggests new features, such as a rationale for serendipity or a justification for a "trend in data". The conceptual framework of the 2-IT also allows the calculation of a sample size and the use of sequential methods in numerous contexts.

</details>

<details>

<summary>2021-07-02 22:35:21 - Prequential MDL for Causal Structure Learning with Neural Networks</summary>

- *Jorg Bornschein, Silvia Chiappa, Alan Malek, Rosemary Nan Ke*

- `2107.05481v1` - [abs](http://arxiv.org/abs/2107.05481v1) - [pdf](http://arxiv.org/pdf/2107.05481v1)

> Learning the structure of Bayesian networks and causal relationships from observations is a common goal in several areas of science and technology. We show that the prequential minimum description length principle (MDL) can be used to derive a practical scoring function for Bayesian networks when flexible and overparametrized neural networks are used to model the conditional probability distributions between observed variables. MDL represents an embodiment of Occam's Razor and we obtain plausible and parsimonious graph structures without relying on sparsity inducing priors or other regularizers which must be tuned. Empirically we demonstrate competitive results on synthetic and real-world data. The score often recovers the correct structure even in the presence of strongly nonlinear relationships between variables; a scenario were prior approaches struggle and usually fail. Furthermore we discuss how the the prequential score relates to recent work that infers causal structure from the speed of adaptation when the observations come from a source undergoing distributional shift.

</details>

<details>

<summary>2021-07-02 23:06:11 - A Tutorial on Sparse Gaussian Processes and Variational Inference</summary>

- *Felix Leibfried, Vincent Dutordoir, ST John, Nicolas Durrande*

- `2012.13962v11` - [abs](http://arxiv.org/abs/2012.13962v11) - [pdf](http://arxiv.org/pdf/2012.13962v11)

> Gaussian processes (GPs) provide a framework for Bayesian inference that can offer principled uncertainty estimates for a large range of problems. For example, if we consider regression problems with Gaussian likelihoods, a GP model enjoys a posterior in closed form. However, identifying the posterior GP scales cubically with the number of training examples and requires to store all examples in memory. In order to overcome these obstacles, sparse GPs have been proposed that approximate the true posterior GP with pseudo-training examples. Importantly, the number of pseudo-training examples is user-defined and enables control over computational and memory complexity. In the general case, sparse GPs do not enjoy closed-form solutions and one has to resort to approximate inference. In this context, a convenient choice for approximate inference is variational inference (VI), where the problem of Bayesian inference is cast as an optimization problem -- namely, to maximize a lower bound of the log marginal likelihood. This paves the way for a powerful and versatile framework, where pseudo-training examples are treated as optimization arguments of the approximate posterior that are jointly identified together with hyperparameters of the generative model (i.e. prior and likelihood). The framework can naturally handle a wide scope of supervised learning problems, ranging from regression with heteroscedastic and non-Gaussian likelihoods to classification problems with discrete labels, but also multilabel problems. The purpose of this tutorial is to provide access to the basic matter for readers without prior knowledge in both GPs and VI. A proper exposition to the subject enables also access to more recent advances (like importance-weighted VI as well as interdomain, multioutput and deep GPs) that can serve as an inspiration for new research ideas.

</details>

<details>

<summary>2021-07-03 09:58:03 - Gaussian graphical modeling for spectrometric data analysis</summary>

- *Laura Codazzi, Alessandro Colombi, Matteo Gianella, Raffaele Argiento, Lucia Paci, Alessia Pini*

- `2103.11666v2` - [abs](http://arxiv.org/abs/2103.11666v2) - [pdf](http://arxiv.org/pdf/2103.11666v2)

> Motivated by the analysis of spectrometric data, we introduce a Gaussian graphical model for learning the dependence structure among frequency bands of the infrared absorbance spectrum. The spectra are modeled as continuous functional data through a B-spline basis expansion and a Gaussian graphical model is assumed as a prior specification for the smoothing coefficients to induce sparsity in their precision matrix. Bayesian inference is carried out to simultaneously smooth the curves and to estimate the conditional independence structure between portions of the functional domain. The proposed model is applied to the analysis of infrared absorbance spectra of strawberry purees.

</details>

<details>

<summary>2021-07-03 15:13:26 - Predictions with dynamic Bayesian predictive synthesis are exact minimax</summary>

- *Kōsaku Takanashi, Kenichiro McAlinn*

- `1911.08662v4` - [abs](http://arxiv.org/abs/1911.08662v4) - [pdf](http://arxiv.org/pdf/1911.08662v4)

> We analyze the combination of multiple predictive distributions for time series data when all forecasts are misspecified. We show that a specific dynamic form of Bayesian predictive synthesis -- a general and coherent Bayesian framework for ensemble methods -- produces exact minimax predictive densities with regard to Kullback-Leibler loss, providing theoretical support for finite sample predictive performance over existing ensemble methods. A simulation study that highlights this theoretical result is presented, showing that dynamic Bayesian predictive synthesis is superior to other ensemble methods using multiple metrics.

</details>

<details>

<summary>2021-07-03 23:17:26 - Bayesian decision-making under misspecified priors with applications to meta-learning</summary>

- *Max Simchowitz, Christopher Tosh, Akshay Krishnamurthy, Daniel Hsu, Thodoris Lykouris, Miroslav Dudík, Robert E. Schapire*

- `2107.01509v1` - [abs](http://arxiv.org/abs/2107.01509v1) - [pdf](http://arxiv.org/pdf/2107.01509v1)

> Thompson sampling and other Bayesian sequential decision-making algorithms are among the most popular approaches to tackle explore/exploit trade-offs in (contextual) bandits. The choice of prior in these algorithms offers flexibility to encode domain knowledge but can also lead to poor performance when misspecified. In this paper, we demonstrate that performance degrades gracefully with misspecification. We prove that the expected reward accrued by Thompson sampling (TS) with a misspecified prior differs by at most $\tilde{\mathcal{O}}(H^2 \epsilon)$ from TS with a well specified prior, where $\epsilon$ is the total-variation distance between priors and $H$ is the learning horizon. Our bound does not require the prior to have any parametric form. For priors with bounded support, our bound is independent of the cardinality or structure of the action space, and we show that it is tight up to universal constants in the worst case.   Building on our sensitivity analysis, we establish generic PAC guarantees for algorithms in the recently studied Bayesian meta-learning setting and derive corollaries for various families of priors. Our results generalize along two axes: (1) they apply to a broader family of Bayesian decision-making algorithms, including a Monte-Carlo implementation of the knowledge gradient algorithm (KG), and (2) they apply to Bayesian POMDPs, the most general Bayesian decision-making setting, encompassing contextual bandits as a special case. Through numerical simulations, we illustrate how prior misspecification and the deployment of one-step look-ahead (as in KG) can impact the convergence of meta-learning in multi-armed and contextual bandits with structured and correlated priors.

</details>

<details>

<summary>2021-07-04 10:46:23 - Deep Gaussian Process Emulation using Stochastic Imputation</summary>

- *Deyu Ming, Daniel Williamson, Serge Guillas*

- `2107.01590v1` - [abs](http://arxiv.org/abs/2107.01590v1) - [pdf](http://arxiv.org/pdf/2107.01590v1)

> We propose a novel deep Gaussian process (DGP) inference method for computer model emulation using stochastic imputation. By stochastically imputing the latent layers, the approach transforms the DGP into the linked GP, a state-of-the-art surrogate model formed by linking a system of feed-forward coupled GPs. This transformation renders a simple while efficient DGP training procedure that only involves optimizations of conventional stationary GPs. In addition, the analytically tractable mean and variance of the linked GP allows one to implement predictions from DGP emulators in a fast and accurate manner. We demonstrate the method in a series of synthetic examples and real-world applications, and show that it is a competitive candidate for efficient DGP surrogate modeling in comparison to the variational inference and the fully-Bayesian approach. A $\texttt{Python}$ package $\texttt{dgpsi}$ implementing the method is also produced and available at https://github.com/mingdeyu/DGP.

</details>

<details>

<summary>2021-07-04 12:33:04 - A similarity-based Bayesian mixture-of-experts model</summary>

- *Tianfang Zhang, Rasmus Bokrantz, Jimmy Olsson*

- `2012.02130v3` - [abs](http://arxiv.org/abs/2012.02130v3) - [pdf](http://arxiv.org/pdf/2012.02130v3)

> We present a new nonparametric mixture-of-experts model for multivariate regression problems, inspired by the probabilistic $k$-nearest neighbors algorithm. Using a conditionally specified model, predictions for out-of-sample inputs are based on similarities to each observed data point, yielding predictive distributions represented by Gaussian mixtures. Posterior inference is performed on the parameters of the mixture components as well as the distance metric using a mean-field variational Bayes algorithm accompanied with a stochastic gradient-based optimization procedure. The proposed method is especially advantageous in settings where inputs are of relatively high dimension in comparison to the data size, where input--output relationships are complex, and where predictive distributions may be skewed or multimodal. Computational studies on two synthetic datasets and one dataset comprising dose statistics of radiation therapy treatment plans show that our mixture-of-experts method performs similarly or better than a conditional Dirichlet process mixture model both in terms of validation metrics and visual inspection.

</details>

<details>

<summary>2021-07-04 15:04:02 - Learning Bayesian Networks through Birkhoff Polytope: A Relaxation Method</summary>

- *Aramayis Dallakyan, Mohsen Pourahmadi*

- `2107.01658v1` - [abs](http://arxiv.org/abs/2107.01658v1) - [pdf](http://arxiv.org/pdf/2107.01658v1)

> We establish a novel framework for learning a directed acyclic graph (DAG) when data are generated from a Gaussian, linear structural equation model. It consists of two parts: (1) introduce a permutation matrix as a new parameter within a regularized Gaussian log-likelihood to represent variable ordering; and (2) given the ordering, estimate the DAG structure through sparse Cholesky factor of the inverse covariance matrix. For permutation matrix estimation, we propose a relaxation technique that avoids the NP-hard combinatorial problem of order estimation. Given an ordering, a sparse Cholesky factor is estimated using a cyclic coordinatewise descent algorithm which decouples row-wise. Our framework recovers DAGs without the need for an expensive verification of the acyclicity constraint or enumeration of possible parent sets. We establish numerical convergence of the algorithm, and consistency of the Cholesky factor estimator when the order of variables is known. Through several simulated and macro-economic datasets, we study the scope and performance of the proposed methodology.

</details>

<details>

<summary>2021-07-04 17:19:24 - Calibrating generalized predictive distributions</summary>

- *Pei-Shien Wu, Ryan Martin*

- `2107.01688v1` - [abs](http://arxiv.org/abs/2107.01688v1) - [pdf](http://arxiv.org/pdf/2107.01688v1)

> In prediction problems, it is common to model the data-generating process and then use a model-based procedure, such as a Bayesian predictive distribution, to quantify uncertainty about the next observation. However, if the posited model is misspecified, then its predictions may not be calibrated -- that is, the predictive distribution's quantiles may not be nominal frequentist prediction upper limits, even asymptotically. Rather than abandoning the comfort of a model-based formulation for a more complicated non-model-based approach, here we propose a strategy in which the data itself helps determine if the assumed model-based solution should be adjusted to account for model misspecification. This is achieved through a generalized Bayes formulation where a learning rate parameter is tuned, via the proposed generalized predictive calibration (GPrC) algorithm, to make the predictive distribution calibrated, even under model misspecification. Extensive numerical experiments are presented, under a variety of settings, demonstrating the proposed GPrC algorithm's validity, efficiency, and robustness.

</details>

<details>

<summary>2021-07-05 13:16:19 - Block-Term Tensor Decomposition Model Selection and Computation: The Bayesian Way</summary>

- *Paris V. Giampouras, Athanasios A. Rontogiannis, Eleftherios Kofidis*

- `2101.02931v2` - [abs](http://arxiv.org/abs/2101.02931v2) - [pdf](http://arxiv.org/pdf/2101.02931v2)

> The so-called block-term decomposition (BTD) tensor model, especially in its rank-$(L_r,L_r,1)$ version, has been recently receiving increasing attention due to its enhanced ability of representing systems and signals that are composed of \emph{blocks} of rank higher than one, a scenario encountered in numerous and diverse applications. Uniqueness conditions and fitting methods have thus been thoroughly studied. Nevertheless, the challenging problem of estimating the BTD model structure, namely the number of block terms, $R$, and their individual ranks, $L_r$, has only recently started to attract significant attention, mainly through regularization-based approaches which entail the need to tune the regularization parameter(s). In this work, we build on ideas of sparse Bayesian learning (SBL) and put forward a fully automated Bayesian approach. Through a suitably crafted multi-level \emph{hierarchical} probabilistic model, which gives rise to heavy-tailed prior distributions for the BTD factors, structured sparsity is \emph{jointly} imposed. Ranks are then estimated from the numbers of blocks ($R$) and columns ($L_r$) of non-negligible energy. Approximate posterior inference is implemented, within the variational inference framework. The resulting iterative algorithm completely avoids hyperparameter tuning, which is a significant defect of regularization-based methods. Alternative probabilistic models are also explored and the connections with their regularization-based counterparts are brought to light with the aid of the associated maximum a-posteriori (MAP) estimators. We report simulation results with both synthetic and real-word data, which demonstrate the merits of the proposed method in terms of both rank estimation and model fitting as compared to state-of-the-art relevant methods.

</details>

<details>

<summary>2021-07-05 15:03:07 - Antithetic Riemannian Manifold And Quantum-Inspired Hamiltonian Monte Carlo</summary>

- *Wilson Tsakane Mongwe, Rendani Mbuvha, Tshilidzi Marwala*

- `2107.02070v1` - [abs](http://arxiv.org/abs/2107.02070v1) - [pdf](http://arxiv.org/pdf/2107.02070v1)

> Markov Chain Monte Carlo inference of target posterior distributions in machine learning is predominately conducted via Hamiltonian Monte Carlo and its variants. This is due to Hamiltonian Monte Carlo based samplers ability to suppress random-walk behaviour. As with other Markov Chain Monte Carlo methods, Hamiltonian Monte Carlo produces auto-correlated samples which results in high variance in the estimators, and low effective sample size rates in the generated samples. Adding antithetic sampling to Hamiltonian Monte Carlo has been previously shown to produce higher effective sample rates compared to vanilla Hamiltonian Monte Carlo. In this paper, we present new algorithms which are antithetic versions of Riemannian Manifold Hamiltonian Monte Carlo and Quantum-Inspired Hamiltonian Monte Carlo. The Riemannian Manifold Hamiltonian Monte Carlo algorithm improves on Hamiltonian Monte Carlo by taking into account the local geometry of the target, which is beneficial for target densities that may exhibit strong correlations in the parameters. Quantum-Inspired Hamiltonian Monte Carlo is based on quantum particles that can have random mass. Quantum-Inspired Hamiltonian Monte Carlo uses a random mass matrix which results in better sampling than Hamiltonian Monte Carlo on spiky and multi-modal distributions such as jump diffusion processes. The analysis is performed on jump diffusion process using real world financial market data, as well as on real world benchmark classification tasks using Bayesian logistic regression.

</details>

<details>

<summary>2021-07-05 15:26:09 - Analyzing Relevance Vector Machines using a single penalty approach</summary>

- *Anand Dixit, Vivekananda Roy*

- `2107.02085v1` - [abs](http://arxiv.org/abs/2107.02085v1) - [pdf](http://arxiv.org/pdf/2107.02085v1)

> Relevance vector machine (RVM) is a popular sparse Bayesian learning model typically used for prediction. Recently it has been shown that improper priors assumed on multiple penalty parameters in RVM may lead to an improper posterior. Currently in the literature, the sufficient conditions for posterior propriety of RVM do not allow improper priors over the multiple penalty parameters. In this article, we propose a single penalty relevance vector machine (SPRVM) model in which multiple penalty parameters are replaced by a single penalty and we consider a semi Bayesian approach for fitting the SPRVM. The necessary and sufficient conditions for posterior propriety of SPRVM are more liberal than those of RVM and allow for several improper priors over the penalty parameter. Additionally, we also prove the geometric ergodicity of the Gibbs sampler used to analyze the SPRVM model and hence can estimate the asymptotic standard errors associated with the Monte Carlo estimate of the means of the posterior predictive distribution. Such a Monte Carlo standard error cannot be computed in the case of RVM, since the rate of convergence of the Gibbs sampler used to analyze RVM is not known. The predictive performance of RVM and SPRVM is compared by analyzing three real life datasets.

</details>

<details>

<summary>2021-07-06 02:55:42 - Neuronized Priors for Bayesian Sparse Linear Regression</summary>

- *Minsuk Shin, Jun S Liu*

- `1810.00141v3` - [abs](http://arxiv.org/abs/1810.00141v3) - [pdf](http://arxiv.org/pdf/1810.00141v3)

> Although Bayesian variable selection methods have been intensively studied, their routine use in practice has not caught up with their non-Bayesian counterparts such as Lasso, likely due to difficulties in both computations and flexibilities of prior choices. To ease these challenges, we propose the neuronized priors to unify and extend some popular shrinkage priors, such as Laplace, Cauchy, horseshoe, and spike-and-slab priors. A neuronized prior can be written as the product of a Gaussian weight variable and a scale variable transformed from Gaussian via an activation function. Compared with classic spike-and-slab priors, the neuronized priors achieve the same explicit variable selection without employing any latent indicator variables, which results in both more efficient and flexible posterior sampling and more effective posterior modal estimation. Theoretically, we provide specific conditions on the neuronized formulation to achieve the optimal posterior contraction rate, and show that a broadly applicable MCMC algorithm achieves an exponentially fast convergence rate under the neuronized formulation. We also examine various simulated and real data examples and demonstrate that using the neuronization representation is computationally more or comparably efficient than its standard counterpart in all well-known cases. An R package NPrior is provided in the CRAN for using neuronized priors in Bayesian linear regression.

</details>

<details>

<summary>2021-07-06 10:11:59 - Predicting Exporters with Machine Learning</summary>

- *Francesca Micocci, Armando Rungi*

- `2107.02512v1` - [abs](http://arxiv.org/abs/2107.02512v1) - [pdf](http://arxiv.org/pdf/2107.02512v1)

> In this contribution, we exploit machine learning techniques to predict out-of-sample firms' ability to export based on the financial accounts of both exporters and non-exporters. Therefore, we show how forecasts can be used as exporting scores, i.e., to measure the distance of non-exporters from export status. For our purpose, we train and test various algorithms on the financial reports of 57,021 manufacturing firms in France in 2010-2018. We find that a Bayesian Additive Regression Tree with Missingness In Attributes (BART-MIA) performs better than other techniques with a prediction accuracy of up to $0.90$. Predictions are robust to changes in definitions of exporters and in the presence of discontinuous exporters. Eventually, we argue that exporting scores can be helpful for trade promotion, trade credit, and to assess firms' competitiveness. For example, back-of-the-envelope estimates show that a representative firm with just below-average exporting scores needs up to $44\%$ more cash resources and up to $2.5$ times more capital expenses to reach full export status.

</details>

<details>

<summary>2021-07-06 11:39:53 - An empirical study of Linespots: A novel past-fault algorithm</summary>

- *Maximilian Scholz, Richard Torkar*

- `2007.09394v3` - [abs](http://arxiv.org/abs/2007.09394v3) - [pdf](http://arxiv.org/pdf/2007.09394v3)

> This paper proposes the novel past-faults fault prediction algorithm Linespots, based on the Bugspots algorithm. We analyze the predictive performance and runtime of Linespots compared to Bugspots with an empirical study using the most significant self-built dataset as of now, including high-quality samples for validation. As a novelty in fault prediction, we use Bayesian data analysis and Directed Acyclic Graphs to model the effects. We found consistent improvements in the predictive performance of Linespots over Bugspots for all seven evaluation metrics. We conclude that Linespots should be used over Bugspots in all cases where no real-time performance is necessary.

</details>

<details>

<summary>2021-07-06 16:14:41 - Bayesian clustering using random effects models and predictive projections</summary>

- *Yinan Mao, David J. Nott*

- `2106.15847v2` - [abs](http://arxiv.org/abs/2106.15847v2) - [pdf](http://arxiv.org/pdf/2106.15847v2)

> Linear mixed models are widely used for analyzing hierarchically structured data involving missingness and unbalanced study designs. We consider a Bayesian clustering method that combines linear mixed models and predictive projections. For each observation, we consider a predictive replicate in which only a subset of the random effects is shared between the observation and its replicate, with the remainder being integrated out using the conditional prior. Predictive projections are then defined in which the number of distinct values taken by the shared random effects is finite, in order to obtain different clusters. Integrating out some of the random effects acts as a noise filter, allowing the clustering to be focused on only certain chosen features of the data. The method is inspired by methods for Bayesian model checking, in which simulated data replicates from a fitted model are used for model criticism by examining their similarity to the observed data in relevant ways. Here the predictive replicates are used to define similarity between observations in relevant ways for clustering. To illustrate the way our method reveals aspects of the data at different scales, we consider fitting temporal trends in longitudinal data using Fourier cosine bases with a random effect for each basis function, and different clusterings defined by shared random effects for replicates of low or high frequency terms. The method is demonstrated in a series of real examples.

</details>

<details>

<summary>2021-07-06 17:56:46 - Bayesian Algorithm Execution: Estimating Computable Properties of Black-box Functions Using Mutual Information</summary>

- *Willie Neiswanger, Ke Alexander Wang, Stefano Ermon*

- `2104.09460v2` - [abs](http://arxiv.org/abs/2104.09460v2) - [pdf](http://arxiv.org/pdf/2104.09460v2)

> In many real-world problems, we want to infer some property of an expensive black-box function $f$, given a budget of $T$ function evaluations. One example is budget constrained global optimization of $f$, for which Bayesian optimization is a popular method. Other properties of interest include local optima, level sets, integrals, or graph-structured information induced by $f$. Often, we can find an algorithm $\mathcal{A}$ to compute the desired property, but it may require far more than $T$ queries to execute. Given such an $\mathcal{A}$, and a prior distribution over $f$, we refer to the problem of inferring the output of $\mathcal{A}$ using $T$ evaluations as Bayesian Algorithm Execution (BAX). To tackle this problem, we present a procedure, InfoBAX, that sequentially chooses queries that maximize mutual information with respect to the algorithm's output. Applying this to Dijkstra's algorithm, for instance, we infer shortest paths in synthetic and real-world graphs with black-box edge costs. Using evolution strategies, we yield variants of Bayesian optimization that target local, rather than global, optima. On these problems, InfoBAX uses up to 500 times fewer queries to $f$ than required by the original algorithm. Our method is closely connected to other Bayesian optimal experimental design procedures such as entropy search methods and optimal sensor placement using Gaussian processes.

</details>

<details>

<summary>2021-07-06 20:53:26 - Scalable Bayesian inference for time series via divide-and-conquer</summary>

- *Rihui Ou, Deborshee Sen, David Dunson*

- `2106.11043v2` - [abs](http://arxiv.org/abs/2106.11043v2) - [pdf](http://arxiv.org/pdf/2106.11043v2)

> Bayesian computational algorithms tend to scale poorly as data size increases. This had led to the development of divide-and-conquer-based approaches for scalable inference. These divide the data into subsets, perform inference for each subset in parallel, and then combine these inferences. While appealing theoretical properties and practical performance have been demonstrated for independent observations, scalable inference for dependent data remains challenging. In this work, we study the problem of Bayesian inference from very long time series. The literature in this area focuses mainly on approximate approaches that lack any theoretical guarantees and may provide arbitrarily poor accuracy in practice. We propose a simple and scalable divide-and-conquer method, and provide accuracy guarantees. Numerical simulations and real data applications demonstrate the effectiveness of our approach.

</details>

<details>

<summary>2021-07-06 22:23:27 - Solution of Physics-based Bayesian Inverse Problems with Deep Generative Priors</summary>

- *Dhruv V Patel, Deep Ray, Assad A Oberai*

- `2107.02926v1` - [abs](http://arxiv.org/abs/2107.02926v1) - [pdf](http://arxiv.org/pdf/2107.02926v1)

> Inverse problems are notoriously difficult to solve because they can have no solutions, multiple solutions, or have solutions that vary significantly in response to small perturbations in measurements. Bayesian inference, which poses an inverse problem as a stochastic inference problem, addresses these difficulties and provides quantitative estimates of the inferred field and the associated uncertainty. However, it is difficult to employ when inferring vectors of large dimensions, and/or when prior information is available through previously acquired samples. In this paper, we describe how deep generative adversarial networks can be used to represent the prior distribution in Bayesian inference and overcome these challenges. We apply these ideas to inverse problems that are diverse in terms of the governing physical principles, sources of prior knowledge, type of measurement, and the extent of available information about measurement noise. In each case we apply the proposed approach to infer the most likely solution and quantitative estimates of uncertainty.

</details>

<details>

<summary>2021-07-07 03:57:22 - Harnessing Heterogeneity: Learning from Decomposed Feedback in Bayesian Modeling</summary>

- *Kai Wang, Bryan Wilder, Sze-chuan Suen, Bistra Dilkina, Milind Tambe*

- `2107.03003v1` - [abs](http://arxiv.org/abs/2107.03003v1) - [pdf](http://arxiv.org/pdf/2107.03003v1)

> There is significant interest in learning and optimizing a complex system composed of multiple sub-components, where these components may be agents or autonomous sensors. Among the rich literature on this topic, agent-based and domain-specific simulations can capture complex dynamics and subgroup interaction, but optimizing over such simulations can be computationally and algorithmically challenging. Bayesian approaches, such as Gaussian processes (GPs), can be used to learn a computationally tractable approximation to the underlying dynamics but typically neglect the detailed information about subgroups in the complicated system. We attempt to find the best of both worlds by proposing the idea of decomposed feedback, which captures group-based heterogeneity and dynamics. We introduce a novel decomposed GP regression to incorporate the subgroup decomposed feedback. Our modified regression has provably lower variance -- and thus a more accurate posterior -- compared to previous approaches; it also allows us to introduce a decomposed GP-UCB optimization algorithm that leverages subgroup feedback. The Bayesian nature of our method makes the optimization algorithm trackable with a theoretical guarantee on convergence and no-regret property. To demonstrate the wide applicability of this work, we execute our algorithm on two disparate social problems: infectious disease control in a heterogeneous population and allocation of distributed weather sensors. Experimental results show that our new method provides significant improvement compared to the state-of-the-art.

</details>

<details>

<summary>2021-07-07 05:03:42 - Exact Learning Augmented Naive Bayes Classifier</summary>

- *Shouta Sugahara, Maomi Ueno*

- `2107.03018v1` - [abs](http://arxiv.org/abs/2107.03018v1) - [pdf](http://arxiv.org/pdf/2107.03018v1)

> Earlier studies have shown that classification accuracies of Bayesian networks (BNs) obtained by maximizing the conditional log likelihood (CLL) of a class variable, given the feature variables, were higher than those obtained by maximizing the marginal likelihood (ML). However, differences between the performances of the two scores in the earlier studies may be attributed to the fact that they used approximate learning algorithms, not exact ones. This paper compares the classification accuracies of BNs with approximate learning using CLL to those with exact learning using ML. The results demonstrate that the classification accuracies of BNs obtained by maximizing the ML are higher than those obtained by maximizing the CLL for large data. However, the results also demonstrate that the classification accuracies of exact learning BNs using the ML are much worse than those of other methods when the sample size is small and the class variable has numerous parents. To resolve the problem, we propose an exact learning augmented naive Bayes classifier (ANB), which ensures a class variable with no parents. The proposed method is guaranteed to asymptotically estimate the identical class posterior to that of the exactly learned BN. Comparison experiments demonstrated the superior performance of the proposed method.

</details>

<details>

<summary>2021-07-07 10:43:45 - Probabilistic feature extraction, dose statistic prediction and dose mimicking for automated radiation therapy treatment planning</summary>

- *Tianfang Zhang, Rasmus Bokrantz, Jimmy Olsson*

- `2102.12569v3` - [abs](http://arxiv.org/abs/2102.12569v3) - [pdf](http://arxiv.org/pdf/2102.12569v3)

> Purpose: We propose a general framework for quantifying predictive uncertainties of dose-related quantities and leveraging this information in a dose mimicking problem in the context of automated radiation therapy treatment planning.   Methods: A three-step pipeline, comprising feature extraction, dose statistic prediction and dose mimicking, is employed. In particular, the features are produced by a convolutional variational autoencoder and used as inputs in a previously developed nonparametric Bayesian statistical method, estimating the multivariate predictive distribution of a collection of predefined dose statistics. Specially developed objective functions are then used to construct a probabilistic dose mimicking problem based on the produced distributions, creating deliverable treatment plans.   Results: The numerical experiments are performed using a dataset of 94 retrospective treatment plans of prostate cancer patients. We show that the features extracted by the variational autoencoder capture geometric information of substantial relevance to the dose statistic prediction problem and are related to dose statistics in a more regularized fashion than hand-crafted features. The estimated predictive distributions are reasonable and outperforms a non-input-dependent benchmark method, and the deliverable plans produced by the probabilistic dose mimicking agree better with their clinical counterparts than for a non-probabilistic formulation.   Conclusions: We demonstrate that prediction of dose-related quantities may be extended to include uncertainty estimation and that such probabilistic information may be leveraged in a dose mimicking problem. The treatment plans produced by the proposed pipeline resemble their original counterparts well, illustrating the merits of a holistic approach to automated planning based on probabilistic modeling.

</details>

<details>

<summary>2021-07-07 11:41:16 - Hierarchical log Gaussian Cox process for regeneration in uneven-aged forests</summary>

- *Mikko Kuronen, Aila Särkkä, Matti Vihola, Mari Myllymäki*

- `2005.01962v3` - [abs](http://arxiv.org/abs/2005.01962v3) - [pdf](http://arxiv.org/pdf/2005.01962v3)

> We propose a hierarchical log Gaussian Cox process (LGCP) for point patterns, where a set of points x affects another set of points y but not vice versa. We use the model to investigate the effect of large trees to the locations of seedlings. In the model, every point in x has a parametric influence kernel or signal, which together form an influence field. Conditionally on the parameters, the influence field acts as a spatial covariate in the intensity of the model, and the intensity itself is a non-linear function of the parameters. Points outside the observation window may affect the influence field inside the window. We propose an edge correction to account for this missing data. The parameters of the model are estimated in a Bayesian framework using Markov chain Monte Carlo (MCMC) where a Laplace approximation is used for the Gaussian field of the LGCP model. The proposed model is used to analyze the effect of large trees on the success of regeneration in uneven-aged forest stands in Finland.

</details>

<details>

<summary>2021-07-07 12:32:29 - A Closed-Form Approximation to the Conjugate Prior of the Dirichlet and Beta Distributions</summary>

- *Kaspar Thommen*

- `2107.03183v1` - [abs](http://arxiv.org/abs/2107.03183v1) - [pdf](http://arxiv.org/pdf/2107.03183v1)

> We derive the conjugate prior of the Dirichlet and beta distributions and explore it with numerical examples to gain an intuitive understanding of the distribution itself, its hyperparameters, and conditions concerning its convergence. Due to the prior's intractability, we proceed to define and analyze a closed-form approximation. Finally, we provide an algorithm implementing this approximation that enables fully tractable Bayesian conjugate treatment of Dirichlet and beta likelihoods without the need for Monte Carlo simulations.

</details>

<details>

<summary>2021-07-07 15:42:13 - The Promises and Pitfalls of Deep Kernel Learning</summary>

- *Sebastian W. Ober, Carl E. Rasmussen, Mark van der Wilk*

- `2102.12108v2` - [abs](http://arxiv.org/abs/2102.12108v2) - [pdf](http://arxiv.org/pdf/2102.12108v2)

> Deep kernel learning (DKL) and related techniques aim to combine the representational power of neural networks with the reliable uncertainty estimates of Gaussian processes. One crucial aspect of these models is an expectation that, because they are treated as Gaussian process models optimized using the marginal likelihood, they are protected from overfitting. However, we identify situations where this is not the case. We explore this behavior, explain its origins and consider how it applies to real datasets. Through careful experimentation on the UCI, CIFAR-10, and the UTKFace datasets, we find that the overfitting from overparameterized maximum marginal likelihood, in which the model is "somewhat Bayesian", can in certain scenarios be worse than that from not being Bayesian at all. We explain how and when DKL can still be successful by investigating optimization dynamics. We also find that failures of DKL can be rectified by a fully Bayesian treatment, which leads to the desired performance improvements over standard neural networks and Gaussian processes.

</details>

<details>

<summary>2021-07-07 15:56:22 - Probabilistic semi-nonnegative matrix factorization: a Skellam-based framework</summary>

- *Benoit Fuentes, Gaël Richard*

- `2107.03317v1` - [abs](http://arxiv.org/abs/2107.03317v1) - [pdf](http://arxiv.org/pdf/2107.03317v1)

> We present a new probabilistic model to address semi-nonnegative matrix factorization (SNMF), called Skellam-SNMF. It is a hierarchical generative model consisting of prior components, Skellam-distributed hidden variables and observed data. Two inference algorithms are derived: Expectation-Maximization (EM) algorithm for maximum \emph{a posteriori} estimation and Variational Bayes EM (VBEM) for full Bayesian inference, including the estimation of parameters prior distribution. From this Skellam-based model, we also introduce a new divergence $\mathcal{D}$ between a real-valued target data $x$ and two nonnegative parameters $\lambda_{0}$ and $\lambda_{1}$ such that $\mathcal{D}\left(x\mid\lambda_{0},\lambda_{1}\right)=0\Leftrightarrow x=\lambda_{0}-\lambda_{1}$, which is a generalization of the Kullback-Leibler (KL) divergence. Finally, we conduct experimental studies on those new algorithms in order to understand their behavior and prove that they can outperform the classic SNMF approach on real data in a task of automatic clustering.

</details>

<details>

<summary>2021-07-07 21:02:21 - Variable selection with missing data in both covariates and outcomes: Imputation and machine learning</summary>

- *Liangyuan Hu, Jung-Yi Joyce Lin, Jiayi Ji*

- `2104.02769v2` - [abs](http://arxiv.org/abs/2104.02769v2) - [pdf](http://arxiv.org/pdf/2104.02769v2)

> The missing data issue is ubiquitous in health studies. Variable selection in the presence of both missing covariates and outcomes is an important statistical research topic but has been less studied. Existing literature focuses on parametric regression techniques that provide direct parameter estimates of the regression model. Flexible nonparametric machine learning methods considerably mitigate the reliance on the parametric assumptions, but do not provide as naturally defined variable importance measure as the covariate effect native to parametric models. We investigate a general variable selection approach when both the covariates and outcomes can be missing at random and have general missing data patterns. This approach exploits the flexibility of machine learning modeling techniques and bootstrap imputation, which is amenable to nonparametric methods in which the covariate effects are not directly available. We conduct expansive simulations investigating the practical operating characteristics of the proposed variable selection approach, when combined with four tree-based machine learning methods, XGBoost, Random Forests, Bayesian Additive Regression Trees (BART) and Conditional Random Forests, and two commonly used parametric methods, lasso and backward stepwise selection. Numeric results suggest that when combined with bootstrap imputation, XGBoost and BART have the overall best variable selection performance with respect to the $F_1$ score and Type I error across various settings. In general, there is no significant difference in the variable selection performance due to imputation methods. We further demonstrate the methods via a case study of risk factors for 3-year incidence of metabolic syndrome with data from the Study of Women's Health Across the Nation.

</details>

<details>

<summary>2021-07-08 01:16:04 - Frequentist Consistency of Variational Bayes</summary>

- *Yixin Wang, David M. Blei*

- `1705.03439v3` - [abs](http://arxiv.org/abs/1705.03439v3) - [pdf](http://arxiv.org/pdf/1705.03439v3)

> A key challenge for modern Bayesian statistics is how to perform scalable inference of posterior distributions. To address this challenge, variational Bayes (VB) methods have emerged as a popular alternative to the classical Markov chain Monte Carlo (MCMC) methods. VB methods tend to be faster while achieving comparable predictive performance. However, there are few theoretical results around VB. In this paper, we establish frequentist consistency and asymptotic normality of VB methods. Specifically, we connect VB methods to point estimates based on variational approximations, called frequentist variational approximations, and we use the connection to prove a variational Bernstein-von Mises theorem. The theorem leverages the theoretical characterizations of frequentist variational approximations to understand asymptotic properties of VB. In summary, we prove that (1) the VB posterior converges to the Kullback-Leibler (KL) minimizer of a normal distribution, centered at the truth and (2) the corresponding variational expectation of the parameter is consistent and asymptotically normal. As applications of the theorem, we derive asymptotic properties of VB posteriors in Bayesian mixture models, Bayesian generalized linear mixed models, and Bayesian stochastic block models. We conduct a simulation study to illustrate these theoretical results.

</details>

<details>

<summary>2021-07-08 02:37:02 - FFORMPP: Feature-based forecast model performance prediction</summary>

- *Thiyanga S. Talagala, Feng Li, Yanfei Kang*

- `1908.11500v3` - [abs](http://arxiv.org/abs/1908.11500v3) - [pdf](http://arxiv.org/pdf/1908.11500v3)

> This paper introduces a novel meta-learning algorithm for time series forecast model performance prediction. We model the forecast error as a function of time series features calculated from the historical time series with an efficient Bayesian multivariate surface regression approach. The minimum predicted forecast error is then used to identify an individual model or a combination of models to produce the final forecasts. It is well-known that the performance of most meta-learning models depends on the representativeness of the reference dataset used for training. In such circumstances, we augment the reference dataset with a feature-based time series simulation approach, namely GRATIS, in generating a rich and representative time series collection. The proposed framework is tested using the M4 competition data and is compared against commonly used forecasting approaches. Our approach provides comparable performances to other model selection/combination approaches but at a lower computational cost and a higher degree of interpretability, which is important for supporting decisions. We also provide useful insights regarding which forecasting models are expected to work better for particular types of time series, the intrinsic mechanisms of the meta-learners and how the forecasting performances are affected by various factors.

</details>

<details>

<summary>2021-07-08 05:45:15 - Traffic prediction at signalised intersections using Integrated Nested Laplace Approximation</summary>

- *D. Townsend, C. Nel*

- `2107.03617v1` - [abs](http://arxiv.org/abs/2107.03617v1) - [pdf](http://arxiv.org/pdf/2107.03617v1)

> A Bayesian approach to predicting traffic flows at signalised intersections is considered using the the INLA framework. INLA is a deterministic, computationally efficient alternative to MCMC for estimating a posterior distribution. It is designed for latent Gaussian models where the parameters follow a joint Gaussian distribution. An assumption which naturally evolves from an LGM is that of a Gaussian Markov Random Field (GMRF). It can be shown that a traffic prediction model based in both space and time satisfies this assumption, and as such the INLA algorithm provides accurate prediction when space, time, and other relevant covariants are included in the model.

</details>

<details>

<summary>2021-07-08 05:53:37 - Validation and Inference of Agent Based Models</summary>

- *D. Townsend*

- `2107.03619v1` - [abs](http://arxiv.org/abs/2107.03619v1) - [pdf](http://arxiv.org/pdf/2107.03619v1)

> Agent Based Modelling (ABM) is a computational framework for simulating the behaviours and interactions of autonomous agents. As Agent Based Models are usually representative of complex systems, obtaining a likelihood function of the model parameters is nearly always intractable. There is a necessity to conduct inference in a likelihood free context in order to understand the model output. Approximate Bayesian Computation is a suitable approach for this inference. It can be applied to an Agent Based Model to both validate the simulation and infer a set of parameters to describe the model. Recent research in ABC has yielded increasingly efficient algorithms for calculating the approximate likelihood. These are investigated and compared using a pedestrian model in the Hamilton CBD.

</details>

<details>

<summary>2021-07-08 10:49:46 - Flexible Variational Bayes based on a Copula of a Mixture of Normals</summary>

- *David Gunawan, Robert Kohn, David Nott*

- `2106.14392v2` - [abs](http://arxiv.org/abs/2106.14392v2) - [pdf](http://arxiv.org/pdf/2106.14392v2)

> Variational Bayes methods approximate the posterior density by a family of tractable distributions and use optimisation to estimate the unknown parameters of the approximation. Variational approximation is useful when exact inference is intractable or very costly. Our article develops a flexible variational approximation based on a copula of a mixture of normals, which is implemented using the natural gradient and a variance reduction method. The efficacy of the approach is illustrated by using simulated and real datasets to approximate multimodal, skewed and heavy-tailed posterior distributions, including an application to Bayesian deep feedforward neural network regression models. Each example shows that the proposed variational approximation is much more accurate than the corresponding Gaussian copula and a mixture of normals variational approximations.

</details>

<details>

<summary>2021-07-08 11:11:25 - Analytically Tractable Hidden-States Inference in Bayesian Neural Networks</summary>

- *Luong-Ha Nguyen, James-A. Goulet*

- `2107.03759v1` - [abs](http://arxiv.org/abs/2107.03759v1) - [pdf](http://arxiv.org/pdf/2107.03759v1)

> With few exceptions, neural networks have been relying on backpropagation and gradient descent as the inference engine in order to learn the model parameters, because the closed-form Bayesian inference for neural networks has been considered to be intractable. In this paper, we show how we can leverage the tractable approximate Gaussian inference's (TAGI) capabilities to infer hidden states, rather than only using it for inferring the network's parameters. One novel aspect it allows is to infer hidden states through the imposition of constraints designed to achieve specific objectives, as illustrated through three examples: (1) the generation of adversarial-attack examples, (2) the usage of a neural network as a black-box optimization method, and (3) the application of inference on continuous-action reinforcement learning. These applications showcase how tasks that were previously reserved to gradient-based optimization approaches can now be approached with analytically tractable inference

</details>

<details>

<summary>2021-07-08 13:32:41 - On the symmetric and skew-symmetric K-distributions</summary>

- *Stylianos E. Trevlakis, Nestor D. Chatzidiamantis, George K. Karagiannidis*

- `2107.02092v2` - [abs](http://arxiv.org/abs/2107.02092v2) - [pdf](http://arxiv.org/pdf/2107.02092v2)

> We propose a family of four-parameter distributions that contain the K-distribution as special case. The family is derived as a mixture distribution that uses the three-parameter reflected Gamma distribution as parental and the two-parameter Gamma distribution as prior. Properties of the proposed family are investigated as well; these include probability density function, cumulative distribution function, moments, and cumulants. The family is termed symmetric K-distribution (SKD) based on its resemblance to the K-distribution as well as its symmetric nature. The standard form of the SKD, which often proves to be an adequate model, is also discussed. Moreover, an order statistics analysis is provided as well as the distributions of the product and ratio of two independent and identical SKD random variables are derived. Finally, a generalisation of the proposed family, which enables non-zero skewness values, is investigated, while both the SKD and the skew-SKD are proven capable of describing the complex dynamics of machine learning, Bayesian analysis and other fields through simplified expressions with high accuracy.

</details>

<details>

<summary>2021-07-08 18:23:59 - Scaling Gaussian Processes with Derivative Information Using Variational Inference</summary>

- *Misha Padidar, Xinran Zhu, Leo Huang, Jacob R. Gardner, David Bindel*

- `2107.04061v1` - [abs](http://arxiv.org/abs/2107.04061v1) - [pdf](http://arxiv.org/pdf/2107.04061v1)

> Gaussian processes with derivative information are useful in many settings where derivative information is available, including numerous Bayesian optimization and regression tasks that arise in the natural sciences. Incorporating derivative observations, however, comes with a dominating $O(N^3D^3)$ computational cost when training on $N$ points in $D$ input dimensions. This is intractable for even moderately sized problems. While recent work has addressed this intractability in the low-$D$ setting, the high-$N$, high-$D$ setting is still unexplored and of great value, particularly as machine learning problems increasingly become high dimensional. In this paper, we introduce methods to achieve fully scalable Gaussian process regression with derivatives using variational inference. Analogous to the use of inducing values to sparsify the labels of a training set, we introduce the concept of inducing directional derivatives to sparsify the partial derivative information of a training set. This enables us to construct a variational posterior that incorporates derivative information but whose size depends neither on the full dataset size $N$ nor the full dimensionality $D$. We demonstrate the full scalability of our approach on a variety of tasks, ranging from a high dimensional stellarator fusion regression task to training graph convolutional neural networks on Pubmed using Bayesian optimization. Surprisingly, we find that our approach can improve regression performance even in settings where only label data is available.

</details>

<details>

<summary>2021-07-08 21:26:29 - PAC Bayesian Performance Guarantees for Deep (Stochastic) Networks in Medical Imaging</summary>

- *Anthony Sicilia, Xingchen Zhao, Anastasia Sosnovskikh, Seong Jae Hwang*

- `2104.05600v2` - [abs](http://arxiv.org/abs/2104.05600v2) - [pdf](http://arxiv.org/pdf/2104.05600v2)

> Application of deep neural networks to medical imaging tasks has in some sense become commonplace. Still, a "thorn in the side" of the deep learning movement is the argument that deep networks are prone to overfitting and are thus unable to generalize well when datasets are small (as is common in medical imaging tasks). One way to bolster confidence is to provide mathematical guarantees, or bounds, on network performance after training which explicitly quantify the possibility of overfitting. In this work, we explore recent advances using the PAC-Bayesian framework to provide bounds on generalization error for large (stochastic) networks. While previous efforts focus on classification in larger natural image datasets (e.g., MNIST and CIFAR-10), we apply these techniques to both classification and segmentation in a smaller medical imagining dataset: the ISIC 2018 challenge set. We observe the resultant bounds are competitive compared to a simpler baseline, while also being more explainable and alleviating the need for holdout sets.

</details>

<details>

<summary>2021-07-08 21:57:07 - Many Objective Bayesian Optimization</summary>

- *Lucia Asencio Martín, Eduardo C. Garrido-Merchán*

- `2107.04126v1` - [abs](http://arxiv.org/abs/2107.04126v1) - [pdf](http://arxiv.org/pdf/2107.04126v1)

> Some real problems require the evaluation of expensive and noisy objective functions. Moreover, the analytical expression of these objective functions may be unknown. These functions are known as black-boxes, for example, estimating the generalization error of a machine learning algorithm and computing its prediction time in terms of its hyper-parameters. Multi-objective Bayesian optimization (MOBO) is a set of methods that has been successfully applied for the simultaneous optimization of black-boxes. Concretely, BO methods rely on a probabilistic model of the objective functions, typically a Gaussian process. This model generates a predictive distribution of the objectives. However, MOBO methods have problems when the number of objectives in a multi-objective optimization problem are 3 or more, which is the many objective setting. In particular, the BO process is more costly as more objectives are considered, computing the quality of the solution via the hyper-volume is also more costly and, most importantly, we have to evaluate every objective function, wasting expensive computational, economic or other resources. However, as more objectives are involved in the optimization problem, it is highly probable that some of them are redundant and not add information about the problem solution. A measure that represents how similar are GP predictive distributions is proposed. We also propose a many objective Bayesian optimization algorithm that uses this metric to determine whether two objectives are redundant. The algorithm stops evaluating one of them if the similarity is found, saving resources and not hurting the performance of the multi-objective BO algorithm. We show empirical evidence in a set of toy, synthetic, benchmark and real experiments that GPs predictive distributions of the effectiveness of the metric and the algorithm.

</details>

<details>

<summary>2021-07-09 04:56:18 - From Many to One: Consensus Inference in a MIP</summary>

- *Noel Cressie, Michael Bertolacci, Andrew Zammit-Mangion*

- `2107.04208v1` - [abs](http://arxiv.org/abs/2107.04208v1) - [pdf](http://arxiv.org/pdf/2107.04208v1)

> A Model Intercomparison Project (MIP) consists of teams who each estimate the same underlying quantity (e.g., temperature projections to the year 2070), and the spread of the estimates indicates their uncertainty. It recognizes that a community of scientists will not agree completely but that there is value in looking for a consensus and information in the range of disagreement. A simple average of the teams' outputs gives a consensus estimate, but it does not recognize that some outputs are more variable than others. Statistical analysis of variance (ANOVA) models offer a way to obtain a weighted consensus estimate of outputs with a variance that is the smallest possible and hence the tightest possible 'one-sigma' and 'two-sigma' intervals. Modulo dependence between MIP outputs, the ANOVA approach weights a team's output inversely proportional to its variation. When external verification data are available for evaluating the fidelity of each MIP output, ANOVA weights can also provide a prior distribution for Bayesian Model Averaging to yield a consensus estimate. We use a MIP of carbon dioxide flux inversions to illustrate the ANOVA-based weighting and subsequent consensus inferences.

</details>

<details>

<summary>2021-07-09 10:47:48 - A Bayesian Semiparametric Vector Multiplicative Error Model</summary>

- *Nicola Donelli, Stefano Peluso, Antonietta Mira*

- `2107.04354v1` - [abs](http://arxiv.org/abs/2107.04354v1) - [pdf](http://arxiv.org/pdf/2107.04354v1)

> Interactions among multiple time series of positive random variables are crucial in diverse financial applications, from spillover effects to volatility interdependence. A popular model in this setting is the vector Multiplicative Error Model (vMEM) which poses a linear iterative structure on the dynamics of the conditional mean, perturbed by a multiplicative innovation term. A main limitation of vMEM is however its restrictive assumption on the distribution of the random innovation term. A Bayesian semiparametric approach that models the innovation vector as an infinite location-scale mixture of multidimensional kernels with support on the positive orthant is used to address this major shortcoming of vMEM. Computational complications arising from the constraints to the positive orthant are avoided through the formulation of a slice sampler on the parameter-extended unconstrained version of the model. The method is applied to simulated and real data and a flexible specification is obtained that outperforms the classical ones in terms of fitting and predictive power.

</details>

<details>

<summary>2021-07-09 16:27:23 - Entropy, Information, and the Updating of Probabilities</summary>

- *Ariel Caticha*

- `2107.04529v1` - [abs](http://arxiv.org/abs/2107.04529v1) - [pdf](http://arxiv.org/pdf/2107.04529v1)

> This paper is a review of a particular approach to the method of maximum entropy as a general framework for inference. The discussion emphasizes the pragmatic elements in the derivation. An epistemic notion of information is defined in terms of its relation to the Bayesian beliefs of ideally rational agents. The method of updating from a prior to a posterior probability distribution is designed through an eliminative induction process. The logarithmic relative entropy is singled out as the unique tool for updating that (a) is of universal applicability; (b) that recognizes the value of prior information; and (c) that recognizes the privileged role played by the notion of independence in science. The resulting framework -- the ME method -- can handle arbitrary priors and arbitrary constraints. It includes MaxEnt and Bayes' rule as special cases and, therefore, it unifies entropic and Bayesian methods into a single general inference scheme. The ME method goes beyond the mere selection of a single posterior, but also addresses the question of how much less probable other distributions might be, which provides a direct bridge to the theories of fluctuations and large deviations.

</details>

<details>

<summary>2021-07-09 16:50:57 - How Likely are Ride-share Drivers to Earn a Living Wage? Large-scale Spatio-temporal Density Smoothing with the Graph-fused Elastic Net</summary>

- *Mauricio Tec, Natalia Zuniga-Garcia, Randy B. Machemehl, James G. Scott*

- `1911.08106v3` - [abs](http://arxiv.org/abs/1911.08106v3) - [pdf](http://arxiv.org/pdf/1911.08106v3)

> Ride-sourcing or transportation network companies (TNCs) provide on-demand transportation service for compensation, connecting drivers of personal vehicles with passengers through smartphone applications. In this study, we consider the problem of estimating a spatiotemporally varying probability distribution for the productivity of a TNC driver, using data on more than 1.2 million TNC trips in Austin, Texas. We propose a graph-based smoothing approach that allows for distinct spatial and temporal dynamics, including different degrees of smoothness, spatio-temporal interactions, and interpolation in regions with little or no data. For such a goal, we introduce the Graph-fused Elastic Net (GFEN) and use it in combination with a dyadic tree decomposition for density estimation. In addition, we present an optimization-driven approach for fast point estimates scalable to massive graphs. Bayesian inference and uncertainty quantification with MCMC are also illustrated. The main results demonstrate that the optimization strategy is an effective exploration tool for selecting adequate regularization schemes using Bayesian optimization of the cross-validation loss. Two key empirical findings made possible by our method include: 1) the probability that a TNC driver can expect to earn a living wage in Austin exhibits high variability in space and time, from as low as 25% to as high as 85%; and 2) some drivers suffer considerable "tail risk", with the bottom 10% of the earnings distribution falling below $10 per hour -- grossly below a living wage in Austin for a single adult -- for specific times and locations. All code and data for the paper are publicly available, as a Shiny app for visualizing the results and a software package in Julia for implementing the GFEN.

</details>

<details>

<summary>2021-07-09 18:26:31 - Use of Variational Inference in Music Emotion Recognition</summary>

- *Nathalie Deziderio, Hugo Tremonte de Carvalho*

- `2106.14323v2` - [abs](http://arxiv.org/abs/2106.14323v2) - [pdf](http://arxiv.org/pdf/2106.14323v2)

> This work was developed aiming to employ Statistical techniques to the field of Music Emotion Recognition, a well-recognized area within the Signal Processing world, but hardly explored from the statistical point of view. Here, we opened several possibilities within the field, applying modern Bayesian Statistics techniques and developing efficient algorithms, focusing on the applicability of the results obtained. Although the motivation for this project was the development of a emotion-based music recommendation system, its main contribution is a highly adaptable multivariate model that can be useful interpreting any database where there is an interest in applying regularization in an efficient manner. Broadly speaking, we will explore what role a sound theoretical statistical analysis can play in the modeling of an algorithm that is able to understand a well-known database and what can be gained with this kind of approach.

</details>

<details>

<summary>2021-07-09 20:41:23 - Gaussian Process Subspace Regression for Model Reduction</summary>

- *Ruda Zhang, Simon Mak, David Dunson*

- `2107.04668v1` - [abs](http://arxiv.org/abs/2107.04668v1) - [pdf](http://arxiv.org/pdf/2107.04668v1)

> Subspace-valued functions arise in a wide range of problems, including parametric reduced order modeling (PROM). In PROM, each parameter point can be associated with a subspace, which is used for Petrov-Galerkin projections of large system matrices. Previous efforts to approximate such functions use interpolations on manifolds, which can be inaccurate and slow. To tackle this, we propose a novel Bayesian nonparametric model for subspace prediction: the Gaussian Process Subspace regression (GPS) model. This method is extrinsic and intrinsic at the same time: with multivariate Gaussian distributions on the Euclidean space, it induces a joint probability model on the Grassmann manifold, the set of fixed-dimensional subspaces. The GPS adopts a simple yet general correlation structure, and a principled approach for model selection. Its predictive distribution admits an analytical form, which allows for efficient subspace prediction over the parameter space. For PROM, the GPS provides a probabilistic prediction at a new parameter point that retains the accuracy of local reduced models, at a computational complexity that does not depend on system dimension, and thus is suitable for online computation. We give four numerical examples to compare our method to subspace interpolation, as well as two methods that interpolate local reduced models. Overall, GPS is the most data efficient, more computationally efficient than subspace interpolation, and gives smooth predictions with uncertainty quantification.

</details>

<details>

<summary>2021-07-10 03:03:51 - Coupling-based convergence assessment of some Gibbs samplers for high-dimensional Bayesian regression with shrinkage priors</summary>

- *Niloy Biswas, Anirban Bhattacharya, Pierre E. Jacob, James E. Johndrow*

- `2012.04798v3` - [abs](http://arxiv.org/abs/2012.04798v3) - [pdf](http://arxiv.org/pdf/2012.04798v3)

> We consider Markov chain Monte Carlo (MCMC) algorithms for Bayesian high-dimensional regression with continuous shrinkage priors. A common challenge with these algorithms is the choice of the number of iterations to perform. This is critical when each iteration is expensive, as is the case when dealing with modern data sets, such as genome-wide association studies with thousands of rows and up to hundred of thousands of columns. We develop coupling techniques tailored to the setting of high-dimensional regression with shrinkage priors, which enable practical, non-asymptotic diagnostics of convergence without relying on traceplots or long-run asymptotics. By establishing geometric drift and minorization conditions for the algorithm under consideration, we prove that the proposed couplings have finite expected meeting time. Focusing on a class of shrinkage priors which includes the 'Horseshoe', we empirically demonstrate the scalability of the proposed couplings. A highlight of our findings is that less than 1000 iterations can be enough for a Gibbs sampler to reach stationarity in a regression on 100,000 covariates. The numerical results also illustrate the impact of the prior on the computational efficiency of the coupling, and suggest the use of priors where the local precisions are Half-t distributed with degree of freedom larger than one.

</details>

<details>

<summary>2021-07-10 09:44:06 - A Bayesian Hurdle Quantile Regression Model for Citation Analysis with Mass Points at Lower Values</summary>

- *Marzieh Shahmandi, Paul Wilson, Mike Thelwall*

- `2102.04481v2` - [abs](http://arxiv.org/abs/2102.04481v2) - [pdf](http://arxiv.org/pdf/2102.04481v2)

> Quantile regression presents a complete picture of the effects on the location, scale, and shape of the dependent variable at all points, not just the mean. We focus on two challenges for citation count analysis by quantile regression: discontinuity and substantial mass points at lower counts. A Bayesian hurdle quantile regression model for count data with a substantial mass point at zero was proposed by King and Song (2019). It uses quantile regression for modeling the nonzero data and logistic regression for modeling the probability of zeros versus nonzeros. We show that substantial mass points for low citation counts will nearly certainly also affect parameter estimation in the quantile regression part of the model, similar to a mass point at zero. We update the King and Song model by shifting the hurdle point past the main mass points. This model delivers more accurate quantile regression for moderately to highly cited articles, especially at quantiles corresponding to values just beyond the mass points, and enables estimates of the extent to which factors influence the chances that an article will be low cited. To illustrate the potential of this method, it is applied to simulated citation counts and data from Scopus.

</details>

<details>

<summary>2021-07-10 17:46:00 - A bayesian reanalysis of the phase III aducanumab (ADU) trial</summary>

- *Tommaso Costa, Franco Cauda*

- `2107.03686v2` - [abs](http://arxiv.org/abs/2107.03686v2) - [pdf](http://arxiv.org/pdf/2107.03686v2)

> In this article we have conducted a reanalysis of the phase III aducanumab (ADU) summary statistics announced by Biogen, in particular the result of the Clinical Dementia Rating-Sum of Boxes (CDR-SB). The results showed that the evidence on the efficacy of the drug is very low and a more clearer view of the results of clinical trials are presented in the Bayesian framework that can be useful for future development and research in the field.

</details>

<details>

<summary>2021-07-11 03:56:54 - Lugsail lag windows for estimating time-average covariance matrices</summary>

- *Dootika Vats, James M. Flegal*

- `1809.04541v3` - [abs](http://arxiv.org/abs/1809.04541v3) - [pdf](http://arxiv.org/pdf/1809.04541v3)

> Lag windows are commonly used in time series, econometrics, steady-state simulation, and Markov chain Monte Carlo to estimate time-average covariance matrices. In the presence of positive correlation of the underlying process, estimators of this matrix almost always exhibit significant negative bias, leading to undesirable finite-sample properties. We propose a new family of lag windows specifically designed to improve finite-sample performance by offsetting this negative bias. Any existing lag window can be adapted into a lugsail equivalent with no additional assumptions. We use these lag windows within spectral variance estimators and demonstrate its advantages in a linear regression model with autocorrelated and heteroskedastic residuals. We further employ the lugsail lag windows in weighted batch means estimators due to their computational efficiency on large simulation output. We obtain bias and variance results for these multivariate estimators and significantly weaken the mixing condition on the process. Superior finite-sample properties are illustrated in a vector autoregressive process and a Bayesian logistic regression model.

</details>

<details>

<summary>2021-07-11 19:28:28 - A Joint introduction to Gaussian Processes and Relevance Vector Machines with Connections to Kalman filtering and other Kernel Smoothers</summary>

- *Luca Martino, Jesse Read*

- `2009.09217v4` - [abs](http://arxiv.org/abs/2009.09217v4) - [pdf](http://arxiv.org/pdf/2009.09217v4)

> The expressive power of Bayesian kernel-based methods has led them to become an important tool across many different facets of artificial intelligence, and useful to a plethora of modern application domains, providing both power and interpretability via uncertainty analysis. This article introduces and discusses two methods which straddle the areas of probabilistic Bayesian schemes and kernel methods for regression: Gaussian Processes and Relevance Vector Machines. Our focus is on developing a common framework with which to view these methods, via intermediate methods a probabilistic version of the well-known kernel ridge regression, and drawing connections among them, via dual formulations, and discussion of their application in the context of major tasks: regression, smoothing, interpolation, and filtering. Overall, we provide understanding of the mathematical concepts behind these models, and we summarize and discuss in depth different interpretations and highlight the relationship to other methods, such as linear kernel smoothers, Kalman filtering and Fourier approximations. Throughout, we provide numerous figures to promote understanding, and we make numerous recommendations to practitioners. Benefits and drawbacks of the different techniques are highlighted. To our knowledge, this is the most in-depth study of its kind to date focused on these two methods, and will be relevant to theoretical understanding and practitioners throughout the domains of data-science, signal processing, machine learning, and artificial intelligence in general.

</details>

<details>

<summary>2021-07-12 13:34:28 - Cohesion and Repulsion in Bayesian Distance Clustering</summary>

- *Abhinav Natarajan, Maria De Iorio, Andreas Heinecke, Emanuel Mayer, Simon Glenn*

- `2107.05414v1` - [abs](http://arxiv.org/abs/2107.05414v1) - [pdf](http://arxiv.org/pdf/2107.05414v1)

> Clustering in high-dimensions poses many statistical challenges. While traditional distance-based clustering methods are computationally feasible, they lack probabilistic interpretation and rely on heuristics for estimation of the number of clusters. On the other hand, probabilistic model-based clustering techniques often fail to scale and devising algorithms that are able to effectively explore the posterior space is an open problem. Based on recent developments in Bayesian distance-based clustering, we propose a hybrid solution that entails defining a likelihood on pairwise distances between observations. The novelty of the approach consists in including both cohesion and repulsion terms in the likelihood, which allows for cluster identifiability. This implies that clusters are composed of objects which have small "dissimilarities" among themselves (cohesion) and similar dissimilarities to observations in other clusters (repulsion). We show how this modelling strategy has interesting connection with existing proposals in the literature as well as a decision-theoretic interpretation. The proposed method is computationally efficient and applicable to a wide variety of scenarios. We demonstrate the approach in a simulation study and an application in digital numismatics.

</details>

<details>

<summary>2021-07-12 15:32:15 - Recent advances in Bayesian optimization with applications to parameter reconstruction in optical nano-metrology</summary>

- *Matthias Plock, Sven Burger, Philipp-Immanuel Schneider*

- `2107.05499v1` - [abs](http://arxiv.org/abs/2107.05499v1) - [pdf](http://arxiv.org/pdf/2107.05499v1)

> Parameter reconstruction is a common problem in optical nano metrology. It generally involves a set of measurements, to which one attempts to fit a numerical model of the measurement process. The model evaluation typically involves to solve Maxwell's equations and is thus time consuming. This makes the reconstruction computationally demanding. Several methods exist for fitting the model to the measurements. On the one hand, Bayesian optimization methods for expensive black-box optimization enable an efficient reconstruction by training a machine learning model of the squared sum of deviations. On the other hand, curve fitting algorithms, such as the Levenberg-Marquardt method, take the deviations between all model outputs and corresponding measurement values into account which enables a fast local convergence. In this paper we present a Bayesian Target Vector Optimization scheme which combines these two approaches. We compare the performance of the presented method against a standard Levenberg-Marquardt-like algorithm, a conventional Bayesian optimization scheme, and the L-BFGS-B and Nelder-Mead simplex algorithms. As a stand-in for problems from nano metrology, we employ a non-linear least-square problem from the NIST Standard Reference Database. We find that the presented method generally uses fewer calls of the model function than any of the competing schemes to achieve similar reconstruction performance.

</details>

<details>

<summary>2021-07-12 21:18:32 - Bayesian Meta-Prior Learning Using Empirical Bayes</summary>

- *Sareh Nabi, Houssam Nassif, Joseph Hong, Hamed Mamani, Guido Imbens*

- `2002.01129v3` - [abs](http://arxiv.org/abs/2002.01129v3) - [pdf](http://arxiv.org/pdf/2002.01129v3)

> Adding domain knowledge to a learning system is known to improve results. In multi-parameter Bayesian frameworks, such knowledge is incorporated as a prior. On the other hand, various model parameters can have different learning rates in real-world problems, especially with skewed data. Two often-faced challenges in Operation Management and Management Science applications are the absence of informative priors, and the inability to control parameter learning rates. In this study, we propose a hierarchical Empirical Bayes approach that addresses both challenges, and that can generalize to any Bayesian framework. Our method learns empirical meta-priors from the data itself and uses them to decouple the learning rates of first-order and second-order features (or any other given feature grouping) in a Generalized Linear Model. As the first-order features are likely to have a more pronounced effect on the outcome, focusing on learning first-order weights first is likely to improve performance and convergence time. Our Empirical Bayes method clamps features in each group together and uses the deployed model's observed data to empirically compute a hierarchical prior in hindsight. We report theoretical results for the unbiasedness, strong consistency, and optimal frequentist cumulative regret properties of our meta-prior variance estimator. We apply our method to a standard supervised learning optimization problem, as well as an online combinatorial optimization problem in a contextual bandit setting implemented in an Amazon production system. Both during simulations and live experiments, our method shows marked improvements, especially in cases of small traffic. Our findings are promising, as optimizing over sparse data is often a challenge.

</details>

<details>

<summary>2021-07-12 22:17:30 - Bayesian mitigation of spatial coarsening for a Hawkes model applied to gunfire, wildfire and viral contagion</summary>

- *Andrew J. Holbrook, Xiang Ji, Marc A. Suchard*

- `2010.02994v2` - [abs](http://arxiv.org/abs/2010.02994v2) - [pdf](http://arxiv.org/pdf/2010.02994v2)

> Self-exciting spatiotemporal Hawkes processes have found increasing use in the study of large-scale public health threats ranging from gun violence and earthquakes to wildfires and viral contagion. Whereas many such applications feature locational uncertainty, i.e., the exact spatial positions of individual events are unknown, most Hawkes model analyses to date have ignored spatial coarsening present in the data. Three particular 21st century public health crises -- urban gun violence, rural wildfires and global viral spread -- present qualitatively and quantitatively varying uncertainty regimes that exhibit (a) different collective magnitudes of spatial coarsening, (b) uniform and mixed magnitude coarsening, (c) differently shaped uncertainty regions and -- less orthodox -- (d) locational data distributed within the `wrong' effective space. We explicitly model such uncertainties in a Bayesian manner and jointly infer unknown locations together with all parameters of a reasonably flexible Hawkes model, obtaining results that are practically and statistically distinct from those obtained while ignoring spatial coarsening. This work also features two different secondary contributions: first, to facilitate Bayesian inference of locations and background rate parameters, we make a subtle yet crucial change to an established kernel-based rate model; and second, to facilitate the same Bayesian inference at scale, we develop a massively parallel implementation of the model's log-likelihood gradient with respect to locations and thus avoid its quadratic computational cost in the context of Hamiltonian Monte Carlo. Our examples involve thousands of observations and allow us to demonstrate practicality at moderate scales.

</details>

<details>

<summary>2021-07-13 01:31:20 - A higher-order singular value decomposition tensor emulator for spatio-temporal simulators</summary>

- *Giri Gopalan, Christopher K. Wikle*

- `2010.03985v2` - [abs](http://arxiv.org/abs/2010.03985v2) - [pdf](http://arxiv.org/pdf/2010.03985v2)

> We introduce methodology to construct an emulator for environmental and ecological spatio-temporal processes that uses the higher order singular value decomposition (HOSVD) as an extension of singular value decomposition (SVD) approaches to emulation. Some important advantages of the method are that it allows for the use of a combination of supervised learning methods (e.g., random forests and Gaussian process regression) and also allows for the prediction of process values at spatial locations and time points that were not used in the training sample. The method is demonstrated with two applications: the first is a periodic solution to a shallow ice approximation partial differential equation from glaciology, and second is an agent-based model of collective animal movement. In both cases, we demonstrate the value of combining different machine learning models for accurate emulation. In addition, in the agent-based model case we demonstrate the ability of the tensor emulator to successfully capture individual behavior in space and time. We demonstrate via a real data example the ability to perform Bayesian inference in order to learn parameters governing collective animal behavior.

</details>

<details>

<summary>2021-07-13 01:44:23 - A Bayesian hierarchical modeling approach to combining multiple data sources: A case study in size estimation</summary>

- *Jacob Parsons, Xiaoyue Niu, Le Bao*

- `2012.05346v2` - [abs](http://arxiv.org/abs/2012.05346v2) - [pdf](http://arxiv.org/pdf/2012.05346v2)

> To combat the HIV/AIDS pandemic effectively, targeted interventions among certain key populations play a critical role. Examples of such key populations include sex workers, people who inject drugs, and men who have sex with men. While having accurate estimates for the size of these key populations is important, any attempt to directly contact or count members of these populations is difficult. As a result, indirect methods are used to produce size estimates. Multiple approaches for estimating the size of such populations have been suggested but often give conflicting results. It is therefore necessary to have a principled way to combine and reconcile these estimates. To this end, we present a Bayesian hierarchical model for estimating the size of key populations that combines multiple estimates from different sources of information. The proposed model makes use of multiple years of data and explicitly models the systematic error in the data sources used. We use the model to estimate the size of people who inject drugs in Ukraine. We evaluate the appropriateness of the model and compare the contribution of each data source to the final estimates.

</details>

<details>

<summary>2021-07-13 01:46:16 - Heterogeneous Effects in the Built Environment</summary>

- *Adam Peterson, Emma Sanchez-Vaznaugh, Brisa Sanchez*

- `2107.05805v1` - [abs](http://arxiv.org/abs/2107.05805v1) - [pdf](http://arxiv.org/pdf/2107.05805v1)

> We present an approach to estimate distance-dependent heterogeneous associations between point-referenced exposures to built environment characteristics and health outcomes. By estimating associations that depend non-linearly on distance between subjects and point-referenced exposures, this method addresses the modifiable area-unit problem that is pervasive in the built environment literature. Additionally, by estimating heterogeneous effects, the method also addresses the uncertain geographic context problem. The key innovation of our method is to combine ideas from the non-parametric function estimation literature and the Bayesian Dirichlet process literature. The former is used to estimate nonlinear associations between subject's outcomes and proximate built environment features, and the latter identifies clusters within the population that have different effects. We study this method in simulations and apply our model to study heterogeneity in the association between fast food restaurant availability and weight status of children attending schools in Los Angeles, California.

</details>

<details>

<summary>2021-07-13 09:10:15 - Outcome-guided Bayesian Clustering for Disease Subtype Discovery Using High-dimensional Transcriptomic Data</summary>

- *Lingsong Meng, Zhiguang Huo*

- `2107.05933v1` - [abs](http://arxiv.org/abs/2107.05933v1) - [pdf](http://arxiv.org/pdf/2107.05933v1)

> The discovery of disease subtypes is an essential step for developing precision medicine, and disease subtyping via omics data has become a popular approach. While promising, subtypes obtained from conventional approaches may not be necessarily associated with clinical outcomes. The collection of rich clinical data along with omics data has provided an unprecedented opportunity to facilitate the disease subtyping process and to discovery clinically meaningful disease subtypes. Thus, we developed an outcome-guided Bayesian clustering (GuidedBayesianClustering) method to fully integrate the clinical data and the high-dimensional omics data. A Gaussian mixed model framework was applied to perform sample clustering; a spike-and-slab prior was utilized to perform gene selection; a mixture model prior was employed to incorporate the guidance from a clinical outcome variable; and a decision framework was adopted to infer the false discovery rate of the selected genes. We deployed conjugate priors to facilitate efficient Gibbs sampling. Our proposed full Bayesian method is capable of simultaneously (i) obtaining sample clustering (disease subtype discovery); (ii) performing feature selection (select genes related to the disease subtype); and (iii) utilizing clinical outcome variable to guide the disease subtype discovery. The superior performance of the GuidedBayesianClustering was demonstrated through simulations and applications of breast cancer expression data.

</details>

<details>

<summary>2021-07-13 13:46:31 - Nested Sampling Methods</summary>

- *Johannes Buchner*

- `2101.09675v2` - [abs](http://arxiv.org/abs/2101.09675v2) - [pdf](http://arxiv.org/pdf/2101.09675v2)

> Nested sampling (NS) computes parameter posterior distributions and makes Bayesian model comparison computationally feasible. Its strengths are the unsupervised navigation of complex, potentially multi-modal posteriors until a well-defined termination point. A systematic literature review of nested sampling algorithms and variants is presented. We focus on complete algorithms, including solutions to likelihood-restricted prior sampling, parallelisation, termination and diagnostics. The relation between number of live points, dimensionality and computational cost is studied for two complete algorithms. A new formulation of NS is presented, which casts the parameter space exploration as a search on a tree. Previously published ways of obtaining robust error estimates and dynamic variations of the number of live points are presented as special cases of this formulation. A new on-line diagnostic test is presented based on previous insertion rank order work. The survey of nested sampling methods concludes with outlooks for future research.

</details>

<details>

<summary>2021-07-13 19:18:18 - Bayesian Nonparametric Density Autoregression with Lag Selection</summary>

- *Matthew Heiner, Athanasios Kottas*

- `2003.09759v2` - [abs](http://arxiv.org/abs/2003.09759v2) - [pdf](http://arxiv.org/pdf/2003.09759v2)

> We develop a Bayesian nonparametric autoregressive model applied to flexibly estimate general transition densities exhibiting nonlinear lag dependence. Our approach is related to Bayesian density regression using Dirichlet process mixtures, with the Markovian likelihood defined through the conditional distribution obtained from the mixture. This results in a Bayesian nonparametric extension of a mixtures-of-experts model formulation. We address computational challenges to posterior sampling that arise from the Markovian structure in the likelihood. The base model is illustrated with synthetic data from a classical model for population dynamics, as well as a series of waiting times between eruptions of Old Faithful Geyser. We study inferences available through the base model before extending the methodology to include automatic relevance detection among a pre-specified set of lags. Inference for global and local lag selection is explored with additional simulation studies, and the methods are illustrated through analysis of an annual time series of pink salmon abundance in a stream in Alaska. We further explore and compare transition density estimation performance for alternative configurations of the proposed model.

</details>

<details>

<summary>2021-07-13 21:29:12 - Infinite Mixtures of Infinite Factor Analysers</summary>

- *Keefe Murphy, Cinzia Viroli, Isobel Claire Gormley*

- `1701.07010v6` - [abs](http://arxiv.org/abs/1701.07010v6) - [pdf](http://arxiv.org/pdf/1701.07010v6)

> Factor-analytic Gaussian mixture models are often employed as a model-based approach to clustering high-dimensional data. Typically, the numbers of clusters and latent factors must be specified in advance of model fitting, and remain fixed. The pair which optimises some model selection criterion is then chosen. For computational reasons, models in which the number of latent factors differ across clusters are rarely considered. Here the infinite mixture of infinite factor analysers (IMIFA) model is introduced. IMIFA employs a Pitman-Yor process prior to facilitate automatic inference of the number of clusters using the stick-breaking construction and a slice sampler. Furthermore, IMIFA employs multiplicative gamma process shrinkage priors to allow cluster-specific numbers of factors, automatically inferred via an adaptive Gibbs sampler. IMIFA is presented as the flagship of a family of factor-analytic mixture models, providing flexible approaches to clustering high-dimensional data. Applications to a benchmark data set, metabolomic spectral data, and a manifold learning handwritten digit example illustrate the IMIFA model and its advantageous features. These include obviating the need for model selection criteria, reducing the computational burden associated with the search of the model space, improving clustering performance by allowing cluster-specific numbers of factors, and quantifying uncertainty in the numbers of clusters and cluster-specific factors.

</details>

<details>

<summary>2021-07-13 23:23:06 - For high-dimensional hierarchical models, consider exchangeability of effects across covariates instead of across datasets</summary>

- *Brian L. Trippe, Hilary K. Finucane, Tamara Broderick*

- `2107.06428v1` - [abs](http://arxiv.org/abs/2107.06428v1) - [pdf](http://arxiv.org/pdf/2107.06428v1)

> Hierarchical Bayesian methods enable information sharing across multiple related regression problems. While standard practice is to model regression parameters (effects) as (1) exchangeable across datasets and (2) correlated to differing degrees across covariates, we show that this approach exhibits poor statistical performance when the number of covariates exceeds the number of datasets. For instance, in statistical genetics, we might regress dozens of traits (defining datasets) for thousands of individuals (responses) on up to millions of genetic variants (covariates). When an analyst has more covariates than datasets, we argue that it is often more natural to instead model effects as (1) exchangeable across covariates and (2) correlated to differing degrees across datasets. To this end, we propose a hierarchical model expressing our alternative perspective. We devise an empirical Bayes estimator for learning the degree of correlation between datasets. We develop theory that demonstrates that our method outperforms the classic approach when the number of covariates dominates the number of datasets, and corroborate this result empirically on several high-dimensional multiple regression and classification problems.

</details>

<details>

<summary>2021-07-14 07:32:07 - Reverse-Bayes methods for evidence assessment and research synthesis</summary>

- *Leonhard Held, Robert Matthews, Manuela Ott, Samuel Pawel*

- `2102.13443v2` - [abs](http://arxiv.org/abs/2102.13443v2) - [pdf](http://arxiv.org/pdf/2102.13443v2)

> It is now widely accepted that the standard inferential toolkit used by the scientific research community -- null-hypothesis significance testing (NHST) -- is not fit for purpose. Yet despite the threat posed to the scientific enterprise, there is no agreement concerning alternative approaches for evidence assessment. This lack of consensus reflects long-standing issues concerning Bayesian methods, the principal alternative to NHST. We report on recent work that builds on an approach to inference put forward over 70 years ago to address the well-known "Problem of Priors" in Bayesian analysis, by reversing the conventional prior-likelihood-posterior ("forward") use of Bayes's Theorem. Such Reverse-Bayes analysis allows priors to be deduced from the likelihood by requiring that the posterior achieve a specified level of credibility. We summarise the technical underpinning of this approach, and show how it opens up new approaches to common inferential challenges, such as assessing the credibility of scientific findings, setting them in appropriate context, estimating the probability of successful replications, and extracting more insight from NHST while reducing the risk of misinterpretation. We argue that Reverse-Bayes methods have a key role to play in making Bayesian methods more accessible and attractive for evidence assessment and research synthesis. As a running example we consider a recently published meta-analysis from several randomized controlled clinical trials investigating the association between corticosteroids and mortality in hospitalized patients with COVID-19.

</details>

<details>

<summary>2021-07-14 08:11:30 - Bayesian Lifetime Regression with Multi-type Group-shared Latent Heterogeneity</summary>

- *Xuxue Sun, Mingyang Li*

- `2107.06539v1` - [abs](http://arxiv.org/abs/2107.06539v1) - [pdf](http://arxiv.org/pdf/2107.06539v1)

> Products manufactured from the same batch or utilized in the same region often exhibit correlated lifetime observations due to the latent heterogeneity caused by the influence of shared but unobserved covariates. The unavailable group-shared covariates involve multiple different types (e.g., discrete, continuous, or mixed-type) and induce different structures of indispensable group-shared latent heterogeneity. Without carefully capturing such latent heterogeneity, the lifetime modeling accuracy will be significantly undermined. In this work, we propose a generic Bayesian lifetime modeling approach by comprehensively investigating the structures of group-shared latent heterogeneity caused by different types of group-shared unobserved covariates. The proposed approach is flexible to characterize multi-type group-shared latent heterogeneity in lifetime data. Besides, it can handle the case of lack of group membership information and address the issue of limited sample size. Bayesian sampling algorithm with data augmentation technique is further developed to jointly quantify the influence of observed covariates and group-shared latent heterogeneity. Further, we conduct comprehensive numerical study to demonstrate the improved performance of proposed modeling approach via comparison with alternative models. We also present empirical study results to investigate the impacts of group number and sample size per group on estimating the group-shared latent heterogeneity and to demonstrate model identifiability of proposed approach for different structures of unobserved group-shared covariates. We also present a real case study to illustrate the effectiveness of proposed approach.

</details>

<details>

<summary>2021-07-14 14:53:31 - Spatiotemporal wildfire modeling through point processes with moderate and extreme marks</summary>

- *Jonathan Koh, François Pimont, Jean-Luc Dupuy, Thomas Opitz*

- `2105.08004v2` - [abs](http://arxiv.org/abs/2105.08004v2) - [pdf](http://arxiv.org/pdf/2105.08004v2)

> Accurate spatiotemporal modeling of conditions leading to moderate and large wildfires provides better understanding of mechanisms driving fire-prone ecosystems and improves risk management. We here develop a joint model for the occurrence intensity and the wildfire size distribution by combining extreme-value theory and point processes within a novel Bayesian hierarchical model, and use it to study daily summer wildfire data for the French Mediterranean basin during 1995--2018. The occurrence component models wildfire ignitions as a spatiotemporal log-Gaussian Cox process. Burnt areas are numerical marks attached to points and are considered as extreme if they exceed a high threshold. The size component is a two-component mixture varying in space and time that jointly models moderate and extreme fires. We capture non-linear influence of covariates (Fire Weather Index, forest cover) through component-specific smooth functions, which may vary with season. We propose estimating shared random effects between model components to reveal and interpret common drivers of different aspects of wildfire activity. This leads to increased parsimony and reduced estimation uncertainty with better predictions. Specific stratified subsampling of zero counts is implemented to cope with large observation vectors. We compare and validate models through predictive scores and visual diagnostics. Our methodology provides a holistic approach to explaining and predicting the drivers of wildfire activity and associated uncertainties.

</details>

<details>

<summary>2021-07-14 15:08:12 - Compound Sequential Change-point Detection in Parallel Data Streams</summary>

- *Yunxiao Chen, Xiaoou Li*

- `1909.05903v2` - [abs](http://arxiv.org/abs/1909.05903v2) - [pdf](http://arxiv.org/pdf/1909.05903v2)

> We consider sequential change-point detection in parallel data streams, where each stream has its own change point. Once a change is detected in a data stream, this stream is deactivated permanently. The goal is to maximize the normal operation of the pre-change streams, while controlling the proportion of post-change streams among the active streams at all time points. Taking a Bayesian formulation, we develop a compound decision framework for this problem. A procedure is proposed that is uniformly optimal among all sequential procedures which control the expected proportion of postchange streams at all time points. We also investigate the asymptotic behavior of the proposed method when the number of data streams grows large. Numerical examples are provided to illustrate the use and performance of the proposed method.

</details>

<details>

<summary>2021-07-14 15:22:05 - Fundamental limits and algorithms for sparse linear regression with sublinear sparsity</summary>

- *Lan V. Truong*

- `2101.11156v4` - [abs](http://arxiv.org/abs/2101.11156v4) - [pdf](http://arxiv.org/pdf/2101.11156v4)

> We establish exact asymptotic expressions for the normalized mutual information and minimum mean-square-error (MMSE) of sparse linear regression in the sub-linear sparsity regime. Our result is achieved by a generalization of the adaptive interpolation method in Bayesian inference for linear regimes to sub-linear ones. A modification of the well-known approximate message passing algorithm to approach the MMSE fundamental limit is also proposed, and its state evolution is rigorously analyzed. Our results show that the traditional linear assumption between the signal dimension and number of observations in the replica and adaptive interpolation methods is not necessary for sparse signals. They also show how to modify the existing well-known AMP algorithms for linear regimes to sub-linear ones.

</details>

<details>

<summary>2021-07-14 18:01:46 - Nonlinear State Space Modeling and Control of the Impact of Patients' Modifiable Lifestyle Behaviors on the Emergence of Multiple Chronic Conditions</summary>

- *Syed Hasib Akhter Faruqui, Adel Alaeddini, Jing Wang, Susan P Fisher-Hoch, Joseph B Mccormic*

- `2107.13394v1` - [abs](http://arxiv.org/abs/2107.13394v1) - [pdf](http://arxiv.org/pdf/2107.13394v1)

> The emergence and progression of multiple chronic conditions (MCC) over time often form a dynamic network that depends on patient's modifiable risk factors and their interaction with non-modifiable risk factors and existing conditions. Continuous time Bayesian networks (CTBNs) are effective methods for modeling the complex network of MCC relationships over time. However, CTBNs are not able to effectively formulate the dynamic impact of patient's modifiable risk factors on the emergence and progression of MCC. Considering a functional CTBN (FCTBN) to represent the underlying structure of the MCC relationships with respect to individuals' risk factors and existing conditions, we propose a nonlinear state-space model based on Extended Kalman filter (EKF) to capture the dynamics of the patients' modifiable risk factors and existing conditions on the MCC evolution over time. We also develop a tensor control chart to dynamically monitor the effect of changes in the modifiable risk factors of individual patients on the risk of new chronic conditions emergence. We validate the proposed approach based on a combination of simulation and real data from a dataset of 385 patients from Cameron County Hispanic Cohort (CCHC) over multiple years. The dataset examines the emergence of 5 chronic conditions (Diabetes, Obesity, Cognitive Impairment, Hyperlipidemia, and Hypertension) based on 4 modifiable risk factors representing lifestyle behaviors (Diet, Exercise, Smoking Habit, and Drinking Habit) and 3 non-modifiable risk factors, including demographic information (Age, Gender, Education). The results demonstrate the effectiveness of the proposed methodology for dynamic prediction and monitoring of the risk of MCC emergence in individual patients.

</details>

<details>

<summary>2021-07-14 21:03:44 - A Functional Model for Structure Learning and Parameter Estimation in Continuous Time Bayesian Network: An Application in Identifying Patterns of Multiple Chronic Conditions</summary>

- *Syed Hasib Akhter Faruqui, Adel Alaeddini, Jing Wang, Carlos A. Jaramillo*

- `2007.15847v2` - [abs](http://arxiv.org/abs/2007.15847v2) - [pdf](http://arxiv.org/pdf/2007.15847v2)

> Bayesian networks are powerful statistical models to study the probabilistic relationships among set random variables with major applications in disease modeling and prediction. Here, we propose a continuous time Bayesian network with conditional dependencies, represented as Poisson regression, to model the impact of exogenous variables on the conditional dependencies of the network. We also propose an adaptive regularization method with an intuitive early stopping feature based on density based clustering for efficient learning of the structure and parameters of the proposed network. Using a dataset of patients with multiple chronic conditions extracted from electronic health records of the Department of Veterans Affairs we compare the performance of the proposed approach with some of the existing methods in the literature for both short-term (one-year ahead) and long-term (multi-year ahead) predictions. The proposed approach provides a sparse intuitive representation of the complex functional relationships between multiple chronic conditions. It also provides the capability of analyzing multiple disease trajectories over time given any combination of prior conditions.

</details>

<details>

<summary>2021-07-14 21:25:53 - Hybrid Bayesian Neural Networks with Functional Probabilistic Layers</summary>

- *Daniel T. Chang*

- `2107.07014v1` - [abs](http://arxiv.org/abs/2107.07014v1) - [pdf](http://arxiv.org/pdf/2107.07014v1)

> Bayesian neural networks provide a direct and natural way to extend standard deep neural networks to support probabilistic deep learning through the use of probabilistic layers that, traditionally, encode weight (and bias) uncertainty. In particular, hybrid Bayesian neural networks utilize standard deterministic layers together with few probabilistic layers judicially positioned in the networks for uncertainty estimation. A major aspect and benefit of Bayesian inference is that priors, in principle, provide the means to encode prior knowledge for use in inference and prediction. However, it is difficult to specify priors on weights since the weights have no intuitive interpretation. Further, the relationships of priors on weights to the functions computed by networks are difficult to characterize. In contrast, functions are intuitive to interpret and are direct since they map inputs to outputs. Therefore, it is natural to specify priors on functions to encode prior knowledge, and to use them in inference and prediction based on functions. To support this, we propose hybrid Bayesian neural networks with functional probabilistic layers that encode function (and activation) uncertainty. We discuss their foundations in functional Bayesian inference, functional variational inference, sparse Gaussian processes, and sparse variational Gaussian processes. We further perform few proof-of-concept experiments using GPflus, a new library that provides Gaussian process layers and supports their use with deterministic Keras layers to form hybrid neural network and Gaussian process models.

</details>

<details>

<summary>2021-07-15 05:32:44 - Posterior Covariance Information Criterion</summary>

- *Yukito Iba, Keisuke Yano*

- `2106.13694v3` - [abs](http://arxiv.org/abs/2106.13694v3) - [pdf](http://arxiv.org/pdf/2106.13694v3)

> We introduce an information criterion, PCIC, for predictive evaluation based on quasi-posterior distributions. It is regarded as a natural generalisation of the widely applicable information criterion (WAIC) and can be computed via a single Markov chain Monte Carlo run. PCIC is useful in a variety of predictive settings that are not well dealt with in WAIC, including weighted likelihood inference and quasi-Bayesian prediction

</details>

<details>

<summary>2021-07-15 09:39:14 - Decentralized Bayesian Learning with Metropolis-Adjusted Hamiltonian Monte Carlo</summary>

- *Vyacheslav Kungurtsev, Adam Cobb, Tara Javidi, Brian Jalaian*

- `2107.07211v1` - [abs](http://arxiv.org/abs/2107.07211v1) - [pdf](http://arxiv.org/pdf/2107.07211v1)

> Federated learning performed by a decentralized networks of agents is becoming increasingly important with the prevalence of embedded software on autonomous devices. Bayesian approaches to learning benefit from offering more information as to the uncertainty of a random quantity, and Langevin and Hamiltonian methods are effective at realizing sampling from an uncertain distribution with large parameter dimensions. Such methods have only recently appeared in the decentralized setting, and either exclusively use stochastic gradient Langevin and Hamiltonian Monte Carlo approaches that require a diminishing stepsize to asymptotically sample from the posterior and are known in practice to characterize uncertainty less faithfully than constant step-size methods with a Metropolis adjustment, or assume strong convexity properties of the potential function. We present the first approach to incorporating constant stepsize Metropolis-adjusted HMC in the decentralized sampling framework, show theoretical guarantees for consensus and probability distance to the posterior stationary distribution, and demonstrate their effectiveness numerically on standard real world problems, including decentralized learning of neural networks which is known to be highly non-convex.

</details>

<details>

<summary>2021-07-15 12:19:10 - Input Dependent Sparse Gaussian Processes</summary>

- *Bahram Jafrasteh, Carlos Villacampa-Calvo, Daniel Hernández-Lobato*

- `2107.07281v1` - [abs](http://arxiv.org/abs/2107.07281v1) - [pdf](http://arxiv.org/pdf/2107.07281v1)

> Gaussian Processes (GPs) are Bayesian models that provide uncertainty estimates associated to the predictions made. They are also very flexible due to their non-parametric nature. Nevertheless, GPs suffer from poor scalability as the number of training instances N increases. More precisely, they have a cubic cost with respect to $N$. To overcome this problem, sparse GP approximations are often used, where a set of $M \ll N$ inducing points is introduced during training. The location of the inducing points is learned by considering them as parameters of an approximate posterior distribution $q$. Sparse GPs, combined with variational inference for inferring $q$, reduce the training cost of GPs to $\mathcal{O}(M^3)$. Critically, the inducing points determine the flexibility of the model and they are often located in regions of the input space where the latent function changes. A limitation is, however, that for some learning tasks a large number of inducing points may be required to obtain a good prediction performance. To address this limitation, we propose here to amortize the computation of the inducing points locations, as well as the parameters of the variational posterior approximation q. For this, we use a neural network that receives the observed data as an input and outputs the inducing points locations and the parameters of $q$. We evaluate our method in several experiments, showing that it performs similar or better than other state-of-the-art sparse variational GP approaches. However, with our method the number of inducing points is reduced drastically due to their dependency on the input data. This makes our method scale to larger datasets and have faster training and prediction times.

</details>

<details>

<summary>2021-07-15 12:48:16 - Tensor-train approximation of the chemical master equation and its application for parameter inference</summary>

- *Ion Gabriel Ion, Christian Wildner, Dimitrios Loukrezis, Heinz Koeppl, Herbert De Gersem*

- `2106.15188v2` - [abs](http://arxiv.org/abs/2106.15188v2) - [pdf](http://arxiv.org/pdf/2106.15188v2)

> In this work, we perform Bayesian inference tasks for the chemical master equation in the tensor-train format. The tensor-train approximation has been proven to be very efficient in representing high dimensional data arising from the explicit representation of the chemical master equation solution. An additional advantage of representing the probability mass function in the tensor train format is that parametric dependency can be easily incorporated by introducing a tensor product basis expansion in the parameter space. Time is treated as an additional dimension of the tensor and a linear system is derived to solve the chemical master equation in time. We exemplify the tensor-train method by performing inference tasks such as smoothing and parameter inference using the tensor-train framework. A very high compression ratio is observed for storing the probability mass function of the solution. Since all linear algebra operations are performed in the tensor-train format, a significant reduction of the computational time is observed as well.

</details>

<details>

<summary>2021-07-15 14:35:48 - Efficient Möbius Transformations and their applications to Dempster-Shafer Theory: Clarification and implementation</summary>

- *Maxime Chaveroche, Franck Davoine, Véronique Cherfaoui*

- `2107.07359v1` - [abs](http://arxiv.org/abs/2107.07359v1) - [pdf](http://arxiv.org/pdf/2107.07359v1)

> Dempster-Shafer Theory (DST) generalizes Bayesian probability theory, offering useful additional information, but suffers from a high computational burden. A lot of work has been done to reduce the complexity of computations used in information fusion with Dempster's rule. The main approaches exploit either the structure of Boolean lattices or the information contained in belief sources. Each has its merits depending on the situation. In this paper, we propose sequences of graphs for the computation of the zeta and M\"obius transformations that optimally exploit both the structure of distributive semilattices and the information contained in belief sources. We call them the Efficient M\"obius Transformations (EMT). We show that the complexity of the EMT is always inferior to the complexity of algorithms that consider the whole lattice, such as the Fast M\"obius Transform (FMT) for all DST transformations. We then explain how to use them to fuse two belief sources. More generally, our EMTs apply to any function in any finite distributive lattice, focusing on a meet-closed or join-closed subset. This article extends our work published at the international conference on Scalable Uncertainty Management (SUM). It clarifies it, brings some minor corrections and provides implementation details such as data structures and algorithms applied to DST.

</details>

<details>

<summary>2021-07-15 15:49:51 - Consistent Online Gaussian Process Regression Without the Sample Complexity Bottleneck</summary>

- *Alec Koppel, Hrusikesha Pradhan, Ketan Rajawat*

- `2004.11094v2` - [abs](http://arxiv.org/abs/2004.11094v2) - [pdf](http://arxiv.org/pdf/2004.11094v2)

> Gaussian processes provide a framework for nonlinear nonparametric Bayesian inference widely applicable across science and engineering. Unfortunately, their computational burden scales cubically with the training sample size, which in the case that samples arrive in perpetuity, approaches infinity. This issue necessitates approximations for use with streaming data, which to date mostly lack convergence guarantees. Thus, we develop the first online Gaussian process approximation that preserves convergence to the population posterior, i.e., asymptotic posterior consistency, while ameliorating its intractable complexity growth with the sample size. We propose an online compression scheme that, following each a posteriori update, fixes an error neighborhood with respect to the Hellinger metric centered at the current posterior, and greedily tosses out past kernel dictionary elements until its boundary is hit. We call the resulting method Parsimonious Online Gaussian Processes (POG). For diminishing error radius, exact asymptotic consistency is preserved (Theorem 1(i)) at the cost of unbounded memory in the limit. On the other hand, for constant error radius, POG converges to a neighborhood of the population posterior (Theorem 1(ii))but with finite memory at-worst determined by the metric entropy of the feature space (Theorem 2). Experimental results are presented on several nonlinear regression problems which illuminates the merits of this approach as compared with alternatives that fix the subspace dimension defining the history of past points.

</details>

<details>

<summary>2021-07-15 18:48:42 - Multivariate Conway-Maxwell-Poisson Distribution: Sarmanov Method and Doubly-Intractable Bayesian Inference</summary>

- *Luiza S. C. Piancastelli, Nial Friel, Wagner Barreto-Souza, Hernando Ombao*

- `2107.07561v1` - [abs](http://arxiv.org/abs/2107.07561v1) - [pdf](http://arxiv.org/pdf/2107.07561v1)

> In this paper, a multivariate count distribution with Conway-Maxwell (COM)-Poisson marginals is proposed. To do this, we develop a modification of the Sarmanov method for constructing multivariate distributions. Our multivariate COM-Poisson (MultCOMP) model has desirable features such as (i) it admits a flexible covariance matrix allowing for both negative and positive non-diagonal entries; (ii) it overcomes the limitation of the existing bivariate COM-Poisson distributions in the literature that do not have COM-Poisson marginals; (iii) it allows for the analysis of multivariate counts and is not just limited to bivariate counts. Inferential challenges are presented by the likelihood specification as it depends on a number of intractable normalizing constants involving the model parameters. These obstacles motivate us to propose a Bayesian inferential approach where the resulting doubly-intractable posterior is dealt with via the exchange algorithm and the Grouped Independence Metropolis-Hastings algorithm. Numerical experiments based on simulations are presented to illustrate the proposed Bayesian approach. We analyze the potential of the MultCOMP model through a real data application on the numbers of goals scored by the home and away teams in the Premier League from 2018 to 2021. Here, our interest is to assess the effect of a lack of crowds during the COVID-19 pandemic on the well-known home team advantage. A MultCOMP model fit shows that there is evidence of a decreased number of goals scored by the home team, not accompanied by a reduced score from the opponent. Hence, our analysis suggests a smaller home team advantage in the absence of crowds, which agrees with the opinion of several football experts.

</details>

<details>

<summary>2021-07-16 10:15:32 - Subspace Shrinkage in Conjugate Bayesian Vector Autoregressions</summary>

- *Florian Huber, Gary Koop*

- `2107.07804v1` - [abs](http://arxiv.org/abs/2107.07804v1) - [pdf](http://arxiv.org/pdf/2107.07804v1)

> Macroeconomists using large datasets often face the choice of working with either a large Vector Autoregression (VAR) or a factor model. In this paper, we develop methods for combining the two using a subspace shrinkage prior. Subspace priors shrink towards a class of functions rather than directly forcing the parameters of a model towards some pre-specified location. We develop a conjugate VAR prior which shrinks towards the subspace which is defined by a factor model. Our approach allows for estimating the strength of the shrinkage as well as the number of factors. After establishing the theoretical properties of our proposed prior, we carry out simulations and apply it to US macroeconomic data. Using simulations we show that our framework successfully detects the number of factors. In a forecasting exercise involving a large macroeconomic data set we find that combining VARs with factor models using our prior can lead to forecast improvements.

</details>

<details>

<summary>2021-07-16 12:54:53 - Statistical Models of Repeated Categorical Ratings: The R package rater</summary>

- *Jeffrey Pullin, Lyle Gurrin, Damjan Vukcevic*

- `2010.09335v3` - [abs](http://arxiv.org/abs/2010.09335v3) - [pdf](http://arxiv.org/pdf/2010.09335v3)

> A common occurrence in many disciplines is the need to assign a set of items into categories or classes with known labels. This is often done by one or more expert raters, or sometimes by an automated process. If these assignments, or 'ratings', are difficult to do, a common tactic is to repeat them by different raters, or even by the same rater multiple times on different occasions.   We present an R package, rater, available on CRAN, that implements Bayesian versions of several statistical models that allow analysis of repeated categorical rating data. Inference is possible for the true underlying (latent) class of each item, as well as the accuracy of each rater.   The models are based on, and include, the Dawid-Skene model. We use the Stan probabilistic programming language as the main computational engine.   We illustrate usage of rater through a few examples. We also discuss in detail the techniques of marginalisation and conditioning, which are necessary for these models but also apply more generally to other models implemented in Stan.

</details>

<details>

<summary>2021-07-16 14:38:50 - Accelerating Bayesian Structure Learning in Sparse Gaussian Graphical Models</summary>

- *Reza Mohammadi, Helene Massam, Gerard Letac*

- `1706.04416v3` - [abs](http://arxiv.org/abs/1706.04416v3) - [pdf](http://arxiv.org/pdf/1706.04416v3)

> Gaussian graphical models are relevant tools to learn conditional independence structure between variables. In this class of models, Bayesian structure learning is often done by search algorithms over the graph space. The conjugate prior for the precision matrix satisfying graphical constraints is the well-known G-Wishart. With this prior, the transition probabilities in the search algorithms necessitate evaluating the ratios of the prior normalizing constants of G-Wishart. In moderate to high-dimensions, this ratio is often approximated using sampling-based methods as computationally expensive updates in the search algorithm. Calculating this ratio so far has been a major computational bottleneck. We overcome this issue by representing a search algorithm in which the ratio of normalizing constant is carried out by an explicit closed-form approximation. Using this approximation within our search algorithm yields significant improvement in the scalability of structure learning without sacrificing structure learning accuracy. We study the conditions under which the approximation is valid. We also evaluate the efficacy of our method with simulation studies. We show that the new search algorithm with our approximation outperforms state-of-the-art methods in both computational efficiency and accuracy. The implementation of our work is available in the R package BDgraph.

</details>

<details>

<summary>2021-07-16 16:40:36 - Efficient Bayesian Sampling Using Normalizing Flows to Assist Markov Chain Monte Carlo Methods</summary>

- *Marylou Gabrié, Grant M. Rotskoff, Eric Vanden-Eijnden*

- `2107.08001v1` - [abs](http://arxiv.org/abs/2107.08001v1) - [pdf](http://arxiv.org/pdf/2107.08001v1)

> Normalizing flows can generate complex target distributions and thus show promise in many applications in Bayesian statistics as an alternative or complement to MCMC for sampling posteriors. Since no data set from the target posterior distribution is available beforehand, the flow is typically trained using the reverse Kullback-Leibler (KL) divergence that only requires samples from a base distribution. This strategy may perform poorly when the posterior is complicated and hard to sample with an untrained normalizing flow. Here we explore a distinct training strategy, using the direct KL divergence as loss, in which samples from the posterior are generated by (i) assisting a local MCMC algorithm on the posterior with a normalizing flow to accelerate its mixing rate and (ii) using the data generated this way to train the flow. The method only requires a limited amount of \textit{a~priori} input about the posterior, and can be used to estimate the evidence required for model validation, as we illustrate on examples.

</details>

<details>

<summary>2021-07-16 16:43:16 - The Attraction Indian Buffet Distribution</summary>

- *Richard L. Warr, David B. Dahl, Jeremy M. Meyer, Arthur Lui*

- `2106.05403v2` - [abs](http://arxiv.org/abs/2106.05403v2) - [pdf](http://arxiv.org/pdf/2106.05403v2)

> We propose the attraction Indian buffet distribution (AIBD), a distribution for binary feature matrices influenced by pairwise similarity information. Binary feature matrices are used in Bayesian models to uncover latent variables (i.e., features) that explain observed data. The Indian buffet process (IBP) is a popular exchangeable prior distribution for latent feature matrices. In the presence of additional information, however, the exchangeability assumption is not reasonable or desirable. The AIBD can incorporate pairwise similarity information, yet it preserves many properties of the IBP, including the distribution of the total number of features. Thus, much of the interpretation and intuition that one has for the IBP directly carries over to the AIBD. A temperature parameter controls the degree to which the similarity information affects feature-sharing between observations. Unlike other nonexchangeable distributions for feature allocations, the probability mass function of the AIBD has a tractable normalizing constant, making posterior inference on hyperparameters straight-forward using standard MCMC methods. A novel posterior sampling algorithm is proposed for the IBP and the AIBD. We demonstrate the feasibility of the AIBD as a prior distribution in feature allocation models and compare the performance of competing methods in simulations and an application.

</details>

<details>

<summary>2021-07-16 16:56:01 - Bayesian Crowdsourcing with Constraints</summary>

- *Panagiotis A. Traganitis, Georgios B. Giannakis*

- `2012.11048v2` - [abs](http://arxiv.org/abs/2012.11048v2) - [pdf](http://arxiv.org/pdf/2012.11048v2)

> Crowdsourcing has emerged as a powerful paradigm for efficiently labeling large datasets and performing various learning tasks, by leveraging crowds of human annotators. When additional information is available about the data, semi-supervised crowdsourcing approaches that enhance the aggregation of labels from human annotators are well motivated. This work deals with semi-supervised crowdsourced classification, under two regimes of semi-supervision: a) label constraints, that provide ground-truth labels for a subset of data; and b) potentially easier to obtain instance-level constraints, that indicate relationships between pairs of data. Bayesian algorithms based on variational inference are developed for each regime, and their quantifiably improved performance, compared to unsupervised crowdsourcing, is analytically and empirically validated on several crowdsourcing datasets.

</details>

<details>

<summary>2021-07-16 22:58:50 - Markov Blanket Discovery using Minimum Message Length</summary>

- *Yang Li, Kevin B Korb, Lloyd Allison*

- `2107.08140v1` - [abs](http://arxiv.org/abs/2107.08140v1) - [pdf](http://arxiv.org/pdf/2107.08140v1)

> Causal discovery automates the learning of causal Bayesian networks from data and has been of active interest from their beginning. With the sourcing of large data sets off the internet, interest in scaling up to very large data sets has grown. One approach to this is to parallelize search using Markov Blanket (MB) discovery as a first step, followed by a process of combining MBs in a global causal model. We develop and explore three new methods of MB discovery using Minimum Message Length (MML) and compare them empirically to the best existing methods, whether developed specifically as MB discovery or as feature selection. Our best MML method is consistently competitive and has some advantageous features.

</details>

<details>

<summary>2021-07-17 16:32:29 - A Bayesian Hierarchical Score for Structure Learning from Related Data Sets</summary>

- *Laura Azzimonti, Giorgio Corani, Marco Scutari*

- `2008.01683v3` - [abs](http://arxiv.org/abs/2008.01683v3) - [pdf](http://arxiv.org/pdf/2008.01683v3)

> Score functions for learning the structure of Bayesian networks in the literature assume that data are a homogeneous set of observations; whereas it is often the case that they comprise different related, but not homogeneous, data sets collected in different ways. In this paper we propose a new Bayesian Dirichlet score, which we call Bayesian Hierarchical Dirichlet (BHD). The proposed score is based on a hierarchical model that pools information across data sets to learn a single encompassing network structure, while taking into account the differences in their probabilistic structures. We derive a closed-form expression for BHD using a variational approximation of the marginal likelihood, we study the associated computational cost and we evaluate its performance using simulated data. We find that, when data comprise multiple related data sets, BHD outperforms the Bayesian Dirichlet equivalent uniform (BDeu) score in terms of reconstruction accuracy as measured by the Structural Hamming distance, and that it is as accurate as BDeu when data are homogeneous. This improvement is particularly clear when either the number of variables in the network or the number of observations is large. Moreover, the estimated networks are sparser and therefore more interpretable than those obtained with BDeu thanks to a lower number of false positive arcs.

</details>

<details>

<summary>2021-07-17 17:12:48 - A Reproducing Kernel Hilbert Space Approach to Functional Calibration of Computer Models</summary>

- *Rui Tuo, Shiyuan He, Arash Pourhabib, Yu Ding, Jianhua Z. Huang*

- `2107.08288v1` - [abs](http://arxiv.org/abs/2107.08288v1) - [pdf](http://arxiv.org/pdf/2107.08288v1)

> This paper develops a frequentist solution to the functional calibration problem, where the value of a calibration parameter in a computer model is allowed to vary with the value of control variables in the physical system. The need of functional calibration is motivated by engineering applications where using a constant calibration parameter results in a significant mismatch between outputs from the computer model and the physical experiment. Reproducing kernel Hilbert spaces (RKHS) are used to model the optimal calibration function, defined as the functional relationship between the calibration parameter and control variables that gives the best prediction. This optimal calibration function is estimated through penalized least squares with an RKHS-norm penalty and using physical data. An uncertainty quantification procedure is also developed for such estimates. Theoretical guarantees of the proposed method are provided in terms of prediction consistency and consistency of estimating the optimal calibration function. The proposed method is tested using both real and synthetic data and exhibits more robust performance in prediction and uncertainty quantification than the existing parametric functional calibration method and a state-of-art Bayesian method.

</details>

<details>

<summary>2021-07-18 07:09:07 - Stick-breaking processes with exchangeable length variables</summary>

- *María F. Gil-Leyva, Ramsés H. Mena*

- `2008.04475v2` - [abs](http://arxiv.org/abs/2008.04475v2) - [pdf](http://arxiv.org/pdf/2008.04475v2)

> Our object of study is the general class of stick-breaking processes with exchangeable length variables. These generalize well-known Bayesian non-parametric priors in an unexplored direction. We give conditions to assure the respective species sampling process is proper and the corresponding prior has full support. For a rich sub-class we explain how, by tuning a single $[0,1]$-valued parameter, the stochastic ordering of the weights can be modulated, and Dirichlet and Geometric priors can be recovered. A general formula for the distribution of the latent allocation variables is derived and an MCMC algorithm is proposed for density estimation purposes.

</details>

<details>

<summary>2021-07-18 13:22:14 - A Peek into the Unobservable: Hidden States and Bayesian Inference for the Bitcoin and Ether Price Series</summary>

- *Constandina Koki, Stefanos Leonardos, Georgios Piliouras*

- `1909.10957v2` - [abs](http://arxiv.org/abs/1909.10957v2) - [pdf](http://arxiv.org/pdf/1909.10957v2)

> Conventional financial models fail to explain the economic and monetary properties of cryptocurrencies due to the latter's dual nature: their usage as financial assets on the one side and their tight connection to the underlying blockchain structure on the other. In an effort to examine both components via a unified approach, we apply a recently developed Non-Homogeneous Hidden Markov (NHHM) model with an extended set of financial and blockchain specific covariates on the Bitcoin (BTC) and Ether (ETH) price data. Based on the observable series, the NHHM model offers a novel perspective on the underlying microstructure of the cryptocurrency market and provides insight on unobservable parameters such as the behavior of investors, traders and miners. The algorithm identifies two alternating periods (hidden states) of inherently different activity -- fundamental versus uninformed or noise traders -- in the Bitcoin ecosystem and unveils differences in both the short/long run dynamics and in the financial characteristics of the two states, such as significant explanatory variables, extreme events and varying series autocorrelation. In a somewhat unexpected result, the Bitcoin and Ether markets are found to be influenced by markedly distinct indicators despite their perceived correlation. The current approach backs earlier findings that cryptocurrencies are unlike any conventional financial asset and makes a first step towards understanding cryptocurrency markets via a more comprehensive lens.

</details>

<details>

<summary>2021-07-18 14:32:04 - Compressed Monte Carlo with application in particle filtering</summary>

- *Luca Martino, Víctor Elvira*

- `2107.08459v1` - [abs](http://arxiv.org/abs/2107.08459v1) - [pdf](http://arxiv.org/pdf/2107.08459v1)

> Bayesian models have become very popular over the last years in several fields such as signal processing, statistics, and machine learning. Bayesian inference requires the approximation of complicated integrals involving posterior distributions. For this purpose, Monte Carlo (MC) methods, such as Markov Chain Monte Carlo and importance sampling algorithms, are often employed. In this work, we introduce the theory and practice of a Compressed MC (C-MC) scheme to compress the statistical information contained in a set of random samples. In its basic version, C-MC is strictly related to the stratification technique, a well-known method used for variance reduction purposes. Deterministic C-MC schemes are also presented, which provide very good performance. The compression problem is strictly related to the moment matching approach applied in different filtering techniques, usually called as Gaussian quadrature rules or sigma-point methods. C-MC can be employed in a distributed Bayesian inference framework when cheap and fast communications with a central processor are required. Furthermore, C-MC is useful within particle filtering and adaptive IS algorithms, as shown by three novel schemes introduced in this work. Six numerical results confirm the benefits of the introduced schemes, outperforming the corresponding benchmark methods. A related code is also provided.

</details>

<details>

<summary>2021-07-18 14:37:07 - Differentially Private Bayesian Neural Networks on Accuracy, Privacy and Reliability</summary>

- *Qiyiwen Zhang, Zhiqi Bu, Kan Chen, Qi Long*

- `2107.08461v1` - [abs](http://arxiv.org/abs/2107.08461v1) - [pdf](http://arxiv.org/pdf/2107.08461v1)

> Bayesian neural network (BNN) allows for uncertainty quantification in prediction, offering an advantage over regular neural networks that has not been explored in the differential privacy (DP) framework. We fill this important gap by leveraging recent development in Bayesian deep learning and privacy accounting to offer a more precise analysis of the trade-off between privacy and accuracy in BNN. We propose three DP-BNNs that characterize the weight uncertainty for the same network architecture in distinct ways, namely DP-SGLD (via the noisy gradient method), DP-BBP (via changing the parameters of interest) and DP-MC Dropout (via the model architecture). Interestingly, we show a new equivalence between DP-SGD and DP-SGLD, implying that some non-Bayesian DP training naturally allows for uncertainty quantification. However, the hyperparameters such as learning rate and batch size, can have different or even opposite effects in DP-SGD and DP-SGLD.   Extensive experiments are conducted to compare DP-BNNs, in terms of privacy guarantee, prediction accuracy, uncertainty quantification, calibration, computation speed, and generalizability to network architecture. As a result, we observe a new tradeoff between the privacy and the reliability. When compared to non-DP and non-Bayesian approaches, DP-SGLD is remarkably accurate under strong privacy guarantee, demonstrating the great potential of DP-BNN in real-world tasks.

</details>

<details>

<summary>2021-07-18 14:45:23 - Compressed particle methods for expensive models with application in Astronomy and Remote Sensing</summary>

- *Luca Martino, Víctor Elvira, Javier López-Santiago, Gustau Camps-Valls*

- `2107.08465v1` - [abs](http://arxiv.org/abs/2107.08465v1) - [pdf](http://arxiv.org/pdf/2107.08465v1)

> In many inference problems, the evaluation of complex and costly models is often required. In this context, Bayesian methods have become very popular in several fields over the last years, in order to obtain parameter inversion, model selection or uncertainty quantification. Bayesian inference requires the approximation of complicated integrals involving (often costly) posterior distributions. Generally, this approximation is obtained by means of Monte Carlo (MC) methods. In order to reduce the computational cost of the corresponding technique, surrogate models (also called emulators) are often employed. Another alternative approach is the so-called Approximate Bayesian Computation (ABC) scheme. ABC does not require the evaluation of the costly model but the ability to simulate artificial data according to that model. Moreover, in ABC, the choice of a suitable distance between real and artificial data is also required. In this work, we introduce a novel approach where the expensive model is evaluated only in some well-chosen samples. The selection of these nodes is based on the so-called compressed Monte Carlo (CMC) scheme. We provide theoretical results supporting the novel algorithms and give empirical evidence of the performance of the proposed method in several numerical experiments. Two of them are real-world applications in astronomy and satellite remote sensing.

</details>

<details>

<summary>2021-07-18 17:22:33 - Decoupling Shrinkage and Selection for the Bayesian Quantile Regression</summary>

- *David Kohns, Tibor Szendrei*

- `2107.08498v1` - [abs](http://arxiv.org/abs/2107.08498v1) - [pdf](http://arxiv.org/pdf/2107.08498v1)

> This paper extends the idea of decoupling shrinkage and sparsity for continuous priors to Bayesian Quantile Regression (BQR). The procedure follows two steps: In the first step, we shrink the quantile regression posterior through state of the art continuous priors and in the second step, we sparsify the posterior through an efficient variant of the adaptive lasso, the signal adaptive variable selection (SAVS) algorithm. We propose a new variant of the SAVS which automates the choice of penalisation through quantile specific loss-functions that are valid in high dimensions. We show in large scale simulations that our selection procedure decreases bias irrespective of the true underlying degree of sparsity in the data, compared to the un-sparsified regression posterior. We apply our two-step approach to a high dimensional growth-at-risk (GaR) exercise. The prediction accuracy of the un-sparsified posterior is retained while yielding interpretable quantile specific variable selection results. Our procedure can be used to communicate to policymakers which variables drive downside risk to the macro economy.

</details>

<details>

<summary>2021-07-19 10:01:50 - Assessing competitive balance in the English Premier League for over forty seasons using a stochastic block model</summary>

- *Francesca Basini, Vasiliki Tsouli, Ioannis Ntzoufras, Nial Friel*

- `2107.08732v1` - [abs](http://arxiv.org/abs/2107.08732v1) - [pdf](http://arxiv.org/pdf/2107.08732v1)

> Competitive balance is a desirable feature in any professional sports league and encapsulates the notion that there is unpredictability in the outcome of games as opposed to an imbalanced league in which the outcome of some games are more predictable than others, for example, when an apparent strong team plays against a weak team. In this paper, we develop a model-based clustering approach to provide an assessment of the balance between teams in a league. We propose a novel Bayesian model to represent the results of a football season as a dense network with nodes identified by teams and categorical edges representing the outcome of each game. The resulting stochastic block model facilitates the probabilistic clustering of teams to assess whether there are competitive imbalances in a league. A key question then is to assess the uncertainty around the number of clusters or blocks and consequently estimation of the partition or allocation of teams to blocks. To do this, we develop an MCMC algorithm that allows the joint estimation of the number of blocks and the allocation of teams to blocks. We apply our model to each season in the English premier league from $1978/79$ to $2019/20$. A key finding of this analysis is evidence which suggests a structural change from a reasonably balanced league to a two-tier league which occurred around the early 2000's.

</details>

<details>

<summary>2021-07-19 21:13:50 - BICNet: A Bayesian Approach for Estimating Task Effects on Intrinsic Connectivity Networks in fMRI Data</summary>

- *Meini Tang, Chee-Ming Ting, Hernando Ombao*

- `2107.09160v1` - [abs](http://arxiv.org/abs/2107.09160v1) - [pdf](http://arxiv.org/pdf/2107.09160v1)

> Intrinsic connectivity networks (ICNs) are specific dynamic functional brain networks that are consistently found under various conditions including rest and task. Studies have shown that some stimuli actually activate intrinsic connectivity through either suppression, excitation, moderation or modification. Nevertheless, the structure of ICNs and task-related effects on ICNs are not yet fully understood. In this paper, we propose a Bayesian Intrinsic Connectivity Network (BICNet) model to identify the ICNs and quantify the task-related effects on the ICN dynamics. Using an extended Bayesian dynamic sparse latent factor model, the proposed BICNet has the following advantages: (1) it simultaneously identifies the individual ICNs and group-level ICN spatial maps; (2) it robustly identifies ICNs by jointly modeling resting-state functional magnetic resonance imaging (rfMRI) and task-related functional magnetic resonance imaging (tfMRI); (3) compared to independent component analysis (ICA)-based methods, it can quantify the difference of ICNs amplitudes across different states; (4) it automatically performs feature selection through the sparsity of the ICNs rather than ad-hoc thresholding. The proposed BICNet was applied to the rfMRI and language tfMRI data from the Human Connectome Project (HCP) and the analysis identified several ICNs related to distinct language processing functions.

</details>

<details>

<summary>2021-07-19 21:36:01 - Optimal Bayesian Smoothing of Functional Observations over a Large Graph</summary>

- *Arkaprava Roy, Shubhashis Ghosal*

- `2104.10335v3` - [abs](http://arxiv.org/abs/2104.10335v3) - [pdf](http://arxiv.org/pdf/2104.10335v3)

> In modern contexts, some types of data are observed in high-resolution, essentially continuously in time. Such data units are best described as taking values in a space of functions. Subject units carrying the observations may have intrinsic relations among themselves, and are best described by the nodes of a large graph. It is often sensible to think that the underlying signals in these functional observations vary smoothly over the graph, in that neighboring nodes have similar underlying signals. This qualitative information allows borrowing of strength over neighboring nodes and consequently leads to more accurate inference. In this paper, we consider a model with Gaussian functional observations and adopt a Bayesian approach to smoothing over the nodes of the graph. We characterize the minimax rate of estimation in terms of the regularity of the signals and their variation across nodes quantified in terms of the graph Laplacian. We show that an appropriate prior constructed from the graph Laplacian can attain the minimax bound, while using a mixture prior, the minimax rate up to a logarithmic factor can be attained simultaneously for all possible values of functional and graphical smoothness. We also show that in the fixed smoothness setting, an optimal sized credible region has arbitrarily high frequentist coverage. A simulation experiment demonstrates that the method performs better than potential competing methods like the random forest. The method is also applied to a dataset on daily temperatures measured at several weather stations in the US state of North Carolina.

</details>

<details>

<summary>2021-07-19 22:36:57 - Independent finite approximations for Bayesian nonparametric inference</summary>

- *Tin D. Nguyen, Jonathan Huggins, Lorenzo Masoero, Lester Mackey, Tamara Broderick*

- `2009.10780v3` - [abs](http://arxiv.org/abs/2009.10780v3) - [pdf](http://arxiv.org/pdf/2009.10780v3)

> Bayesian nonparametric priors based on completely random measures (CRMs) offer a flexible modeling approach when the number of latent components in a dataset is unknown. However, managing the infinite dimensionality of CRMs typically requires practitioners to derive ad-hoc algorithms, preventing the use of general-purpose inference methods and often leading to long compute times. We propose a general but explicit recipe to construct a simple finite-dimensional approximation that can replace the infinite-dimensional CRMs. Our independent finite approximation (IFA) is a generalization of important cases that are used in practice. The independence of atom weights in our approximation (i) makes the construction well-suited for parallel and distributed computation and (ii) facilitates more convenient inference schemes. We quantify the approximation error between IFAs and the target nonparametric prior. We compare IFAs with an alternative approximation scheme -- truncated finite approximations (TFAs), where the atom weights are constructed sequentially. We prove that, for worst-case choices of observation likelihoods, TFAs are a more efficient approximation than IFAs. However, in real-data experiments with image denoising and topic modeling, we find that IFAs perform very similarly to TFAs in terms of task-specific accuracy metrics.

</details>

<details>

<summary>2021-07-20 07:52:35 - Variational Laplace for Bayesian neural networks</summary>

- *Ali Unlu, Laurence Aitchison*

- `2103.00222v3` - [abs](http://arxiv.org/abs/2103.00222v3) - [pdf](http://arxiv.org/pdf/2103.00222v3)

> We develop variational Laplace for Bayesian neural networks (BNNs) which exploits a local approximation of the curvature of the likelihood to estimate the ELBO without the need for stochastic sampling of the neural-network weights. The Variational Laplace objective is simple to evaluate, as it is (in essence) the log-likelihood, plus weight-decay, plus a squared-gradient regularizer. Variational Laplace gave better test performance and expected calibration errors than maximum a-posteriori inference and standard sampling-based variational inference, despite using the same variational approximate posterior. Finally, we emphasise care needed in benchmarking standard VI as there is a risk of stopping before the variance parameters have converged. We show that early-stopping can be avoided by increasing the learning rate for the variance parameters.

</details>

<details>

<summary>2021-07-20 09:24:13 - JAGS, NIMBLE, Stan: a detailed comparison among Bayesian MCMC software</summary>

- *Mario Beraha, Daniele Falco, Alessandra Guglielmi*

- `2107.09357v1` - [abs](http://arxiv.org/abs/2107.09357v1) - [pdf](http://arxiv.org/pdf/2107.09357v1)

> The aim of this work is the comparison of the performance of the three popular software platforms JAGS, NIMBLE and Stan. These probabilistic programming languages are able to automatically generate samples from the posterior distribution of interest using MCMC algorithms, starting from the specification of a Bayesian model, i.e. the likelihood and the prior. The final goal is to present a detailed analysis of their strengths and weaknesses to statisticians or applied scientists. In this way, we wish to contribute to make them fully aware of the pros and cons of this software. We carry out a systematic comparison of the three platforms on a wide class of models, prior distributions, and data generating mechanisms. Our extensive simulation studies evaluate the quality of the MCMC chains produced, the efficiency of the software and the goodness of fit of the output. We also consider the efficiency of the parallelization made by the three platforms.

</details>

<details>

<summary>2021-07-20 10:38:39 - Diagnosis of model-structural errors with a sliding time-window Bayesian analysis</summary>

- *Han-Fang Hsueh, Anneli Guthke, Thomas Wöhling, Wolfgang Nowak*

- `2107.09399v1` - [abs](http://arxiv.org/abs/2107.09399v1) - [pdf](http://arxiv.org/pdf/2107.09399v1)

> Deterministic hydrological models with uncertain, but inferred-to-be-time-invariant parameters typically show time-dependent model structural errors. Such errors can occur if a hydrological process is active in certain time periods in nature, but is not resolved by the model. Such missing processes could become visible during calibration as time-dependent best-fit values of model parameters. We propose a formal time-windowed Bayesian analysis to diagnose this type of model error, formalizing the question \In which period of the calibration time-series does the model statistically disqualify itself as quasi-true?" Using Bayesian model evidence (BME) as model performance metric, we determine how much the data in time windows of the calibration time-series support or refute the model. Then, we track BME over sliding time windows to obtain a dynamic, time-windowed BME (tBME) and search for sudden decreases that indicate an onset of model error. tBME also allows us to perform a formal, sliding likelihood-ratio test of the model against the data. Our proposed approach is designed to detect error occurrence on various temporal scales, which is especially useful in hydrological modelling. We illustrate this by applying our proposed method to soil moisture modeling. We test tBME as model error indicator on several synthetic and real-world test cases that we designed to vary in error sources and error time scales. Results prove the usefulness of the framework for detecting structural errors in dynamic models. Moreover, the time sequence of posterior parameter distributions helps to investigate the reasons for model error and provide guidance for model improvement.

</details>

<details>

<summary>2021-07-20 11:18:08 - Bayesian beta nonlinear models with constrained parameters to describe ruminal degradation kinetics</summary>

- *Diego Salmerón*

- `2006.04461v2` - [abs](http://arxiv.org/abs/2006.04461v2) - [pdf](http://arxiv.org/pdf/2006.04461v2)

> The models used to describe the kinetics of ruminal degradation are usually nonlinear models where the dependent variable is the proportion of degraded food. The method of least squares is the standard approach used to estimate the unknown parameters but this method can lead to unacceptable predictions. To solve this issue, a beta nonlinear model and the Bayesian perspective is proposed in this article. The application of standard methodologies to obtain prior distributions, such as the Jeffreys prior or the reference priors, involves serious difficulties here because this model is a nonlinear non-normal regression model, and the constrained parameters appear in the log-likelihood function through the Gamma function. This paper proposes an objective method to obtain the prior distribution, which can be applied to other models with similar complexity, can be easily implemented in OpenBUGS, and solves the problem of unacceptable predictions. The model is generalized to a larger class of models. The methodology was applied to real data with three models that were compared using the Deviance Information Criterion and the root mean square prediction error. A simulation study was performed to evaluate the coverage of the credible intervals.

</details>

<details>

<summary>2021-07-20 13:39:15 - On some information-theoretic aspects of non-linear statistical inverse problems</summary>

- *Richard Nickl, Gabriel Paternain*

- `2107.09488v1` - [abs](http://arxiv.org/abs/2107.09488v1) - [pdf](http://arxiv.org/pdf/2107.09488v1)

> Results by van der Vaart (1991) from semi-parametric statistics about the existence of a non-zero Fisher information are reviewed in an infinite-dimensional non-linear Gaussian regression setting. Information-theoretically optimal inference on aspects of the unknown parameter is possible if and only if the adjoint of the linearisation of the regression map satisfies a certain range condition. It is shown that this range condition may fail in a commonly studied elliptic inverse problem with a divergence form equation, and that a large class of smooth linear functionals of the conductivity parameter cannot be estimated efficiently in this case. In particular, Gaussian `Bernstein von Mises'-type approximations for Bayesian posterior distributions do not hold in this setting.

</details>

<details>

<summary>2021-07-20 22:05:15 - Investigating the performance of multi-objective optimization when learning Bayesian Networks</summary>

- *Paolo Cazzaniga, Marco S. Nobile, Daniele Ramazzotti*

- `1808.01345v2` - [abs](http://arxiv.org/abs/1808.01345v2) - [pdf](http://arxiv.org/pdf/1808.01345v2)

> Bayesian Networks have been widely used in the last decades in many fields, to describe statistical dependencies among random variables. In general, learning the structure of such models is a problem with considerable theoretical interest that poses many challenges. On the one hand, it is a well-known NP-complete problem, practically hardened by the huge search space of possible solutions. On the other hand, the phenomenon of I-equivalence, i.e., different graphical structures underpinning the same set of statistical dependencies, may lead to multimodal fitness landscapes further hindering maximum likelihood approaches to solve the task. In particular, we exploit the NSGA-II multi-objective optimization procedure in order to explicitly account for both the likelihood of a solution and the number of selected arcs, by setting these as the two objective functions of the method. The aim of this work is to investigate the behavior of NSGA-II and analyse the quality of its solutions. We thus thoroughly examined the optimization results obtained on a wide set of simulated data, by considering both the goodness of the inferred solutions in terms of the objective functions values achieved, and by comparing the retrieved structures with the ground truth, i.e., the networks used to generate the target data. Our results show that NSGA-II can converge to solutions characterized by better likelihood and less arcs than classic approaches, although paradoxically characterized in many cases by a lower similarity with the target network.

</details>

<details>

<summary>2021-07-20 23:49:21 - Accelerating sequential Monte Carlo with surrogate likelihoods</summary>

- *Joshua J Bon, Anthony Lee, Christopher Drovandi*

- `2009.03699v2` - [abs](http://arxiv.org/abs/2009.03699v2) - [pdf](http://arxiv.org/pdf/2009.03699v2)

> Delayed-acceptance is a technique for reducing computational effort for Bayesian models with expensive likelihoods. Using a delayed-acceptance kernel for Markov chain Monte Carlo can reduce the number of expensive likelihoods evaluations required to approximate a posterior expectation. Delayed-acceptance uses a surrogate, or approximate, likelihood to avoid evaluation of the expensive likelihood when possible. Within the sequential Monte Carlo framework, we utilise the history of the sampler to adaptively tune the surrogate likelihood to yield better approximations of the expensive likelihood, and use a surrogate first annealing schedule to further increase computational efficiency. Moreover, we propose a framework for optimising computation time whilst avoiding particle degeneracy, which encapsulates existing strategies in the literature. Overall, we develop a novel algorithm for computationally efficient SMC with expensive likelihood functions. The method is applied to static Bayesian models, which we demonstrate on toy and real examples, code for which is available at https://github.com/bonStats/smcdar.

</details>

<details>

<summary>2021-07-21 02:51:19 - EMG Pattern Recognition via Bayesian Inference with Scale Mixture-Based Stochastic Generative Models</summary>

- *Akira Furui, Takuya Igaue, Toshio Tsuji*

- `2107.09853v1` - [abs](http://arxiv.org/abs/2107.09853v1) - [pdf](http://arxiv.org/pdf/2107.09853v1)

> Electromyogram (EMG) has been utilized to interface signals for prosthetic hands and information devices owing to its ability to reflect human motion intentions. Although various EMG classification methods have been introduced into EMG-based control systems, they do not fully consider the stochastic characteristics of EMG signals. This paper proposes an EMG pattern classification method incorporating a scale mixture-based generative model. A scale mixture model is a stochastic EMG model in which the EMG variance is considered as a random variable, enabling the representation of uncertainty in the variance. This model is extended in this study and utilized for EMG pattern classification. The proposed method is trained by variational Bayesian learning, thereby allowing the automatic determination of the model complexity. Furthermore, to optimize the hyperparameters of the proposed method with a partial discriminative approach, a mutual information-based determination method is introduced. Simulation and EMG analysis experiments demonstrated the relationship between the hyperparameters and classification accuracy of the proposed method as well as the validity of the proposed method. The comparison using public EMG datasets revealed that the proposed method outperformed the various conventional classifiers. These results indicated the validity of the proposed method and its applicability to EMG-based control systems. In EMG pattern recognition, a classifier based on a generative model that reflects the stochastic characteristics of EMG signals can outperform the conventional general-purpose classifier.

</details>

<details>

<summary>2021-07-21 09:04:03 - Fast Hyperparameter Optimization of Deep Neural Networks via Ensembling Multiple Surrogates</summary>

- *Yang Li, Jiawei Jiang, Yingxia Shao, Bin Cui*

- `1811.02319v3` - [abs](http://arxiv.org/abs/1811.02319v3) - [pdf](http://arxiv.org/pdf/1811.02319v3)

> The performance of deep neural networks crucially depends on good hyperparameter configurations. Bayesian optimization is a powerful framework for optimizing the hyperparameters of DNNs. These methods need sufficient evaluation data to approximate and minimize the validation error function of hyperparameters. However, the expensive evaluation cost of DNNs leads to very few evaluation data within a limited time, which greatly reduces the efficiency of Bayesian optimization. Besides, the previous researches focus on using the complete evaluation data to conduct Bayesian optimization, and ignore the intermediate evaluation data generated by early stopping methods. To alleviate the insufficient evaluation data problem, we propose a fast hyperparameter optimization method, HOIST, that utilizes both the complete and intermediate evaluation data to accelerate the hyperparameter optimization of DNNs. Specifically, we train multiple basic surrogates to gather information from the mixed evaluation data, and then combine all basic surrogates using weighted bagging to provide an accurate ensemble surrogate. Our empirical studies show that HOIST outperforms the state-of-the-art approaches on a wide range of DNNs, including feed forward neural networks, convolutional neural networks, recurrent neural networks, and variational autoencoder.

</details>

<details>

<summary>2021-07-21 11:46:11 - Scalable Control Variates for Monte Carlo Methods via Stochastic Optimization</summary>

- *Shijing Si, Chris. J. Oates, Andrew B. Duncan, Lawrence Carin, François-Xavier Briol*

- `2006.07487v2` - [abs](http://arxiv.org/abs/2006.07487v2) - [pdf](http://arxiv.org/pdf/2006.07487v2)

> Control variates are a well-established tool to reduce the variance of Monte Carlo estimators. However, for large-scale problems including high-dimensional and large-sample settings, their advantages can be outweighed by a substantial computational cost. This paper considers control variates based on Stein operators, presenting a framework that encompasses and generalizes existing approaches that use polynomials, kernels and neural networks. A learning strategy based on minimising a variational objective through stochastic optimization is proposed, leading to scalable and effective control variates. Novel theoretical results are presented to provide insight into the variance reduction that can be achieved, and an empirical assessment, including applications to Bayesian inference, is provided in support.

</details>

<details>

<summary>2021-07-21 14:40:14 - Fed-ensemble: Improving Generalization through Model Ensembling in Federated Learning</summary>

- *Naichen Shi, Fan Lai, Raed Al Kontar, Mosharaf Chowdhury*

- `2107.10663v1` - [abs](http://arxiv.org/abs/2107.10663v1) - [pdf](http://arxiv.org/pdf/2107.10663v1)

> In this paper we propose Fed-ensemble: a simple approach that bringsmodel ensembling to federated learning (FL). Instead of aggregating localmodels to update a single global model, Fed-ensemble uses random permutations to update a group of K models and then obtains predictions through model averaging. Fed-ensemble can be readily utilized within established FL methods and does not impose a computational overhead as it only requires one of the K models to be sent to a client in each communication round. Theoretically, we show that predictions on newdata from all K models belong to the same predictive posterior distribution under a neural tangent kernel regime. This result in turn sheds light onthe generalization advantages of model averaging. We also illustrate thatFed-ensemble has an elegant Bayesian interpretation. Empirical results show that our model has superior performance over several FL algorithms,on a wide range of data sets, and excels in heterogeneous settings often encountered in FL applications.

</details>

<details>

<summary>2021-07-21 14:44:19 - Tracking the Transmission Dynamics of COVID-19 with a Time-Varying Coefficient State-Space Model</summary>

- *Joshua P. Keller, Tianjian Zhou, Andee Kaplan, G. Brooke Anderson, Wen Zhou*

- `2107.10118v1` - [abs](http://arxiv.org/abs/2107.10118v1) - [pdf](http://arxiv.org/pdf/2107.10118v1)

> The spread of COVID-19 has been greatly impacted by regulatory policies and behavior patterns that vary across counties, states, and countries. Population-level dynamics of COVID-19 can generally be described using a set of ordinary differential equations, but these deterministic equations are insufficient for modeling the observed case rates, which can vary due to local testing and case reporting policies and non-homogeneous behavior among individuals. To assess the impact of population mobility on the spread of COVID-19, we have developed a novel Bayesian time-varying coefficient state-space model for infectious disease transmission. The foundation of this model is a time-varying coefficient compartment model to recapitulate the dynamics among susceptible, exposed, undetected infectious, detected infectious, undetected removed, detected non-infectious, detected recovered, and detected deceased individuals. The infectiousness and detection parameters are modeled to vary by time, and the infectiousness component in the model incorporates information on multiple sources of population mobility. Along with this compartment model, a multiplicative process model is introduced to allow for deviation from the deterministic dynamics. We apply this model to observed COVID-19 cases and deaths in several US states and Colorado counties. We find that population mobility measures are highly correlated with transmission rates and can explain complicated temporal variation in infectiousness in these regions. Additionally, the inferred connections between mobility and epidemiological parameters, varying across locations, have revealed the heterogeneous effects of different policies on the dynamics of COVID-19.

</details>

<details>

<summary>2021-07-21 16:01:13 - Bayesian iterative screening in ultra-high dimensional settings</summary>

- *Run Wang, Somak Dutta, Vivekananda Roy*

- `2107.10175v1` - [abs](http://arxiv.org/abs/2107.10175v1) - [pdf](http://arxiv.org/pdf/2107.10175v1)

> Variable selection in ultra-high dimensional linear regression is often preceded by a screening step to significantly reduce the dimension. Here a Bayesian variable screening method (BITS) is developed. BITS can successfully integrate prior knowledge, if any, on effect sizes, and the number of true variables. BITS iteratively includes potential variables with the highest posterior probability accounting for the already selected variables. It is implemented by a fast Cholesky update algorithm and is shown to have the screening consistency property. BITS is built based on a model with Gaussian errors, yet, the screening consistency is proved to hold under more general tail conditions. The notion of posterior screening consistency allows the resulting model to provide a good starting point for further Bayesian variable selection methods. A new screening consistent stopping rule based on posterior probability is developed. Simulation studies and real data examples are used to demonstrate scalability and fine screening performance.

</details>

<details>

<summary>2021-07-21 17:22:58 - Inner spike and slab Bayesian nonparametric models</summary>

- *Antonio Canale, Antonio Lijoi, Bernardo Nipoti, Igor Prünster*

- `2107.10223v1` - [abs](http://arxiv.org/abs/2107.10223v1) - [pdf](http://arxiv.org/pdf/2107.10223v1)

> Discrete Bayesian nonparametric models whose expectation is a convex linear combination of a point mass at some point of the support and a diffuse probability distribution allow to incorporate strong prior information, while still being extremely flexible. Recent contributions in the statistical literature have successfully implemented such a modelling strategy in a variety of applications, including density estimation, nonparametric regression and model-based clustering. We provide a thorough study of a large class of nonparametric models we call inner spike and slab hNRMI models, which are obtained by considering homogeneous normalized random measures with independent increments (hNRMI) with base measure given by a convex linear combination of a point mass and a diffuse probability distribution. In this paper we investigate the distributional properties of these models and our results include: i) the exchangeable partition probability function they induce, ii) the distribution of the number of distinct values in an exchangeable sample, iii) the posterior predictive distribution, and iv) the distribution of the number of elements that coincide with the only point of the support with positive probability. Our findings are the main building block for an actual implementation of Bayesian inner spike and slab hNRMI models by means of a generalized P\'olya urn scheme.

</details>

<details>

<summary>2021-07-22 04:17:46 - Learning Sparse Fixed-Structure Gaussian Bayesian Networks</summary>

- *Arnab Bhattacharyya, Davin Choo, Rishikesh Gajjala, Sutanu Gayen, Yuhao Wang*

- `2107.10450v1` - [abs](http://arxiv.org/abs/2107.10450v1) - [pdf](http://arxiv.org/pdf/2107.10450v1)

> Gaussian Bayesian networks (a.k.a. linear Gaussian structural equation models) are widely used to model causal interactions among continuous variables. In this work, we study the problem of learning a fixed-structure Gaussian Bayesian network up to a bounded error in total variation distance. We analyze the commonly used node-wise least squares regression (LeastSquares) and prove that it has a near-optimal sample complexity. We also study a couple of new algorithms for the problem:   - BatchAvgLeastSquares takes the average of several batches of least squares solutions at each node, so that one can interpolate between the batch size and the number of batches. We show that BatchAvgLeastSquares also has near-optimal sample complexity.   - CauchyEst takes the median of solutions to several batches of linear systems at each node. We show that the algorithm specialized to polytrees, CauchyEstTree, has near-optimal sample complexity.   Experimentally, we show that for uncontaminated, realizable data, the LeastSquares algorithm performs best, but in the presence of contamination or DAG misspecification, CauchyEst/CauchyEstTree and BatchAvgLeastSquares respectively perform better.

</details>

<details>

<summary>2021-07-22 09:39:23 - Methods for the inclusion of real world evidence in network meta-analysis</summary>

- *David Jenkins, Humaira Hussein, Reynaldo Martina, Pascale Dequen-O'Byrne, Keith R Abrams, Sylwia Bujkiewicz*

- `1805.06839v2` - [abs](http://arxiv.org/abs/1805.06839v2) - [pdf](http://arxiv.org/pdf/1805.06839v2)

> Background: Network Meta-Analysis (NMA) is a key component of submissions to reimbursement agencies world-wide, especially when there is limited direct head-to-head evidence for multiple technologies from randomised controlled trials (RCTs). Many NMAs include only data from RCTs. However, real-world evidence (RWE) is also becoming widely recognised as a valuable source of clinical data. We investigate methods for the inclusion of RWE in NMA and its impact on the uncertainty around the effectiveness estimates.   Methods: A range of methods for inclusion of RWE in evidence synthesis, including Bayesian hierarchical and power prior models, were investigated by applying them to an example in relapsing remitting multiple sclerosis. The effect of the inclusion of RWE was investigated by varying the degree of down weighting of this part of evidence by the use of a power prior.   Results: Whilst the inclusion of the RWE led to an increase in the level of uncertainty surrounding effect estimates in this example, this depended on the method of inclusion adopted for the RWE. Power prior NMA model resulted in stable effect estimates for fingolimod yet increasing the width of the credible intervals with increasing weight given to RWE data. The hierarchical NMA models were effective in allowing for heterogeneity between study designs; however, this also increased the level of uncertainty.   Conclusion: The power prior approach for the inclusion of RWE in NMAs indicates that the degree to which RWE is taken into account can have a significant impact on the overall level of uncertainty. The hierarchical modelling approach further allowed for accommodating differences between study types. Consequently, further work investigating both empirical evidence for biases associated with individual RWE studies and methods of elicitation from experts on the extent of such biases is warranted.

</details>

<details>

<summary>2021-07-22 15:51:20 - Learning Bayesian Networks from Incomplete Data with the Node-Average Likelihood</summary>

- *Tjebbe Bodewes, Marco Scutari*

- `2004.14441v5` - [abs](http://arxiv.org/abs/2004.14441v5) - [pdf](http://arxiv.org/pdf/2004.14441v5)

> Bayesian network (BN) structure learning from complete data has been extensively studied in the literature. However, fewer theoretical results are available for incomplete data, and most are related to the Expectation-Maximisation (EM) algorithm. Balov (2013) proposed an alternative approach called Node-Average Likelihood (NAL) that is competitive with EM but computationally more efficient; and he proved its consistency and model identifiability for discrete BNs.   In this paper, we give general sufficient conditions for the consistency of NAL; and we prove consistency and identifiability for conditional Gaussian BNs, which include discrete and Gaussian BNs as special cases. Furthermore, we confirm our results and the results in Balov (2013) with an independent simulation study. Hence we show that NAL has a much wider applicability than originally implied in Balov (2013), and that it is competitive with EM for conditional Gaussian BNs as well.

</details>

<details>

<summary>2021-07-23 10:27:01 - COVID-19 and the gig economy in Poland</summary>

- *Maciej Beręsewicz, Dagmara Nikulin*

- `2107.11124v1` - [abs](http://arxiv.org/abs/2107.11124v1) - [pdf](http://arxiv.org/pdf/2107.11124v1)

> We use a dataset covering nearly the entire target population based on passively collected data from smartphones to measure the impact of the first COVID-19 wave on the gig economy in Poland. In particular, we focus on transportation (Uber, Bolt) and delivery (Wolt, Takeaway, Glover, DeliGoo) apps, which make it possible to distinguish between the demand and supply part of this market. Based on Bayesian structural time-series models, we estimate the causal impact of the first COVID-19 wave on the number of active drivers and couriers. We show a significant relative increase for Wolt and Glover (15% and 24%) and a slight relative decrease for Uber and Bolt (-3% and -7%) in comparison to a counterfactual control. The change for Uber and Bolt can be partially explained by the prospect of a new law (the so-called Uber Lex), which was already announced in 2019 and is intended to regulate the work of platform drivers.

</details>

<details>

<summary>2021-07-23 12:00:14 - Estimation of sparse linear dynamic networks using the stable spline horseshoe prior</summary>

- *Gianluigi Pillonetto*

- `2107.11155v1` - [abs](http://arxiv.org/abs/2107.11155v1) - [pdf](http://arxiv.org/pdf/2107.11155v1)

> Identification of the so-called dynamic networks is one of the most challenging problems appeared recently in control literature. Such systems consist of large-scale interconnected systems, also called modules. To recover full networks dynamics the two crucial steps are topology detection, where one has to infer from data which connections are active, and modules estimation. Since a small percentage of connections are effective in many real systems, the problem finds also fundamental connections with group-sparse estimation. In particular, in the linear setting modules correspond to unknown impulse responses expected to have null norm but in a small fraction of samples. This paper introduces a new Bayesian approach for linear dynamic networks identification where impulse responses are described through the combination of two particular prior distributions. The first one is a block version of the horseshoe prior, a model possessing important global-local shrinkage features. The second one is the stable spline prior, that encodes information on smooth-exponential decay of the modules. The resulting model is called stable spline horseshoe (SSH) prior. It implements aggressive shrinkage of small impulse responses while larger impulse responses are conveniently subject to stable spline regularization. Inference is performed by a Markov Chain Monte Carlo scheme, tailored to the dynamic context and able to efficiently return the posterior of the modules in sampled form. We include numerical studies that show how the new approach can accurately reconstruct sparse networks dynamics also when thousands of unknown impulse response coefficients must be inferred from data sets of relatively small size.

</details>

<details>

<summary>2021-07-23 16:29:53 - Diagnostics for Conditional Density Models and Bayesian Inference Algorithms</summary>

- *David Zhao, Niccolò Dalmasso, Rafael Izbicki, Ann B. Lee*

- `2102.10473v6` - [abs](http://arxiv.org/abs/2102.10473v6) - [pdf](http://arxiv.org/pdf/2102.10473v6)

> There has been growing interest in the AI community for precise uncertainty quantification. Conditional density models f(y|x), where x represents potentially high-dimensional features, are an integral part of uncertainty quantification in prediction and Bayesian inference. However, it is challenging to assess conditional density estimates and gain insight into modes of failure. While existing diagnostic tools can determine whether an approximated conditional density is compatible overall with a data sample, they lack a principled framework for identifying, locating, and interpreting the nature of statistically significant discrepancies over the entire feature space. In this paper, we present rigorous and easy-to-interpret diagnostics such as (i) the "Local Coverage Test" (LCT), which distinguishes an arbitrarily misspecified model from the true conditional density of the sample, and (ii) "Amortized Local P-P plots" (ALP) which can quickly provide interpretable graphical summaries of distributional differences at any location x in the feature space. Our validation procedures scale to high dimensions and can potentially adapt to any type of data at hand. We demonstrate the effectiveness of LCT and ALP through a simulated experiment and applications to prediction and parameter inference for image data.

</details>

<details>

<summary>2021-07-23 19:13:43 - Geometric convergence of elliptical slice sampling</summary>

- *Viacheslav Natarovskii, Daniel Rudolf, Björn Sprungk*

- `2105.03308v3` - [abs](http://arxiv.org/abs/2105.03308v3) - [pdf](http://arxiv.org/pdf/2105.03308v3)

> For Bayesian learning, given likelihood function and Gaussian prior, the elliptical slice sampler, introduced by Murray, Adams and MacKay 2010, provides a tool for the construction of a Markov chain for approximate sampling of the underlying posterior distribution. Besides of its wide applicability and simplicity its main feature is that no tuning is necessary. Under weak regularity assumptions on the posterior density we show that the corresponding Markov chain is geometrically ergodic and therefore yield qualitative convergence guarantees. We illustrate our result for Gaussian posteriors as they appear in Gaussian process regression, as well as in a setting of a multi-modal distribution. Remarkably, our numerical experiments indicate a dimension-independent performance of elliptical slice sampling even in situations where our ergodicity result does not apply.

</details>

<details>

<summary>2021-07-24 00:27:43 - Aggregative Efficiency of Bayesian Learning in Networks</summary>

- *Krishna Dasaratha, Kevin He*

- `1911.10116v6` - [abs](http://arxiv.org/abs/1911.10116v6) - [pdf](http://arxiv.org/pdf/1911.10116v6)

> When individuals in a social network learn about an unknown state from private signals and neighbors' actions, the network structure often causes information loss. We consider rational agents and Gaussian signals in the canonical sequential social-learning problem and ask how the network changes the efficiency of signal aggregation. Rational actions in our model are a log-linear function of observations and admit a signal-counting interpretation of accuracy. This generates a fine-grained ranking of networks based on their aggregative efficiency index. Networks where agents observe multiple neighbors but not their common predecessors confound information, and we show confounding can make learning very inefficient. In a class of networks where agents move in generations and observe the previous generation, aggregative efficiency is a simple function of network parameters: increasing in observations and decreasing in confounding. Generations after the first contribute very little additional information due to confounding, even when generations are arbitrarily large.

</details>

<details>

<summary>2021-07-24 11:43:53 - Effect of the COVID-19 pandemic on bike-sharing demand and hire time: Evidence from Santander Cycles in London</summary>

- *Shahram Heydari, Garyfallos Konstantinoudis, Abdul Wahid Behsoodi*

- `2107.11589v1` - [abs](http://arxiv.org/abs/2107.11589v1) - [pdf](http://arxiv.org/pdf/2107.11589v1)

> The COVID-19 pandemic has been influencing travel behaviour in many urban areas around the world since the beginning of 2020. As a consequence, bike-sharing schemes have been affected partly due to the change in travel demand and behaviour as well as a shift from public transit. This study estimates the varying effect of the COVID-19 pandemic on the London bike-sharing system (Santander Cycles) over the period March-December 2020. We employed a Bayesian second-order random walk time-series model to account for temporal correlation in the data. We compared the observed number of cycle hires and hire time with their respective counterfactuals (what would have been if the pandemic had not happened) to estimate the magnitude of the change caused by the pandemic. The results indicated that following a reduction in cycle hires in March and April 2020, the demand rebounded from May 2020, remaining in the expected range of what would have been if the pandemic had not occurred. This could indicate the resiliency of Santander Cycles. With respect to hire time, an important increase occurred in April, May, and June 2020, indicating that bikes were hired for longer trips, perhaps partly due to a shift from public transit.

</details>

<details>

<summary>2021-07-24 14:06:00 - Automatic tempered posterior distributions for Bayesian inversion problems</summary>

- *L. Martino, F. Llorente, E. Curbelo, J. Lopez-Santiago, J. Miguez*

- `2107.11614v1` - [abs](http://arxiv.org/abs/2107.11614v1) - [pdf](http://arxiv.org/pdf/2107.11614v1)

> We propose a novel adaptive importance sampling scheme for Bayesian inversion problems where the inference of the variables of interest and the power of the data noise is split. More specifically, we consider a Bayesian analysis for the variables of interest (i.e., the parameters of the model to invert), whereas we employ a maximum likelihood approach for the estimation of the noise power. The whole technique is implemented by means of an iterative procedure, alternating sampling and optimization steps. Moreover, the noise power is also used as a tempered parameter for the posterior distribution of the the variables of interest. Therefore, a sequence of tempered posterior densities is generated, where the tempered parameter is automatically selected according to the actual estimation of the noise power. A complete Bayesian study over the model parameters and the scale parameter can be also performed. Numerical experiments show the benefits of the proposed approach.

</details>

<details>

<summary>2021-07-25 11:23:43 - Sensitivity and robustness analysis in Bayesian networks with the bnmonitor R package</summary>

- *Manuele Leonelli, Ramsiya Ramanathan, Rachel L. Wilkerson*

- `2107.11785v1` - [abs](http://arxiv.org/abs/2107.11785v1) - [pdf](http://arxiv.org/pdf/2107.11785v1)

> Bayesian networks are a class of models that are widely used for risk assessment of complex operational systems. There are now multiple approaches, as well as implemented software, that guide their construction via data learning or expert elicitation. However, a constructed Bayesian network needs to be validated before it can be used for practical risk assessment. Here, we illustrate the usage of the bnmonitor R package: the first comprehensive software for the validation of a Bayesian network. An applied data analysis using bnmonitor is carried out over a medical dataset to illustrate the use of its wide array of functions.

</details>

<details>

<summary>2021-07-25 14:13:28 - Probabilistic selection of inducing points in sparse Gaussian processes</summary>

- *Anders Kirk Uhrenholt, Valentin Charvet, Bjørn Sand Jensen*

- `2010.09370v4` - [abs](http://arxiv.org/abs/2010.09370v4) - [pdf](http://arxiv.org/pdf/2010.09370v4)

> Sparse Gaussian processes and various extensions thereof are enabled through inducing points, that simultaneously bottleneck the predictive capacity and act as the main contributor towards model complexity. However, the number of inducing points is generally not associated with uncertainty which prevents us from applying the apparatus of Bayesian reasoning for identifying an appropriate trade-off. In this work we place a point process prior on the inducing points and approximate the associated posterior through stochastic variational inference. By letting the prior encourage a moderate number of inducing points, we enable the model to learn which and how many points to utilise. We experimentally show that fewer inducing points are preferred by the model as the points become less informative, and further demonstrate how the method can be employed in deep Gaussian processes and latent variable modelling.

</details>

<details>

<summary>2021-07-25 15:29:42 - Active multi-fidelity Bayesian online changepoint detection</summary>

- *Gregory W. Gundersen, Diana Cai, Chuteng Zhou, Barbara E. Engelhardt, Ryan P. Adams*

- `2103.14224v2` - [abs](http://arxiv.org/abs/2103.14224v2) - [pdf](http://arxiv.org/pdf/2103.14224v2)

> Online algorithms for detecting changepoints, or abrupt shifts in the behavior of a time series, are often deployed with limited resources, e.g., to edge computing settings such as mobile phones or industrial sensors. In these scenarios it may be beneficial to trade the cost of collecting an environmental measurement against the quality or "fidelity" of this measurement and how the measurement affects changepoint estimation. For instance, one might decide between inertial measurements or GPS to determine changepoints for motion. A Bayesian approach to changepoint detection is particularly appealing because we can represent our posterior uncertainty about changepoints and make active, cost-sensitive decisions about data fidelity to reduce this posterior uncertainty. Moreover, the total cost could be dramatically lowered through active fidelity switching, while remaining robust to changes in data distribution. We propose a multi-fidelity approach that makes cost-sensitive decisions about which data fidelity to collect based on maximizing information gain with respect to changepoints. We evaluate this framework on synthetic, video, and audio data and show that this information-based approach results in accurate predictions while reducing total cost.

</details>

<details>

<summary>2021-07-25 21:42:51 - Repulsive Deep Ensembles are Bayesian</summary>

- *Francesco D'Angelo, Vincent Fortuin*

- `2106.11642v2` - [abs](http://arxiv.org/abs/2106.11642v2) - [pdf](http://arxiv.org/pdf/2106.11642v2)

> Deep ensembles have recently gained popularity in the deep learning community for their conceptual simplicity and efficiency. However, maintaining functional diversity between ensemble members that are independently trained with gradient descent is challenging. This can lead to pathologies when adding more ensemble members, such as a saturation of the ensemble performance, which converges to the performance of a single model. Moreover, this does not only affect the quality of its predictions, but even more so the uncertainty estimates of the ensemble, and thus its performance on out-of-distribution data. We hypothesize that this limitation can be overcome by discouraging different ensemble members from collapsing to the same function. To this end, we introduce a kernelized repulsive term in the update rule of the deep ensembles. We show that this simple modification not only enforces and maintains diversity among the members but, even more importantly, transforms the maximum a posteriori inference into proper Bayesian inference. Namely, we show that the training dynamics of our proposed repulsive ensembles follow a Wasserstein gradient flow of the KL divergence with the true posterior. We study repulsive terms in weight and function space and empirically compare their performance to standard ensembles and Bayesian baselines on synthetic and real-world prediction tasks.

</details>

<details>

<summary>2021-07-26 07:17:37 - A Novel Bivariate Generalized Weibull Distribution with Properties and Applications</summary>

- *Ashok Kumar Pathak, Mohd. Arshad, Qazi J. Azhad, Mukti Khetan, Arvind Pandey*

- `2107.11998v1` - [abs](http://arxiv.org/abs/2107.11998v1) - [pdf](http://arxiv.org/pdf/2107.11998v1)

> Univariate Weibull distribution is a well-known lifetime distribution and has been widely used in reliability and survival analysis. In this paper, we introduce a new family of bivariate generalized Weibull (BGW) distributions, whose univariate marginals are exponentiated Weibull distribution. Different statistical quantiles like marginals, conditional distribution, conditional expectation, product moments, correlation and a measure component reliability are derived. Various measures of dependence and statistical properties along with ageing properties are examined. Further, the copula associated with BGW distribution and its various important properties are also considered. The methods of maximum likelihood and Bayesian estimation are employed to estimate unknown parameters of the model. A Monte Carlo simulation and real data study are carried out to demonstrate the performance of the estimators and results have proven the effectiveness of the distribution in real-life situations

</details>

<details>

<summary>2021-07-26 10:50:39 - Bias in Zipf's Law Estimators</summary>

- *Charlie Pilgrim, Thomas T Hills*

- `2008.00903v4` - [abs](http://arxiv.org/abs/2008.00903v4) - [pdf](http://arxiv.org/pdf/2008.00903v4)

> The prevailing maximum likelihood estimators for inferring power law models from rank-frequency data are biased. The source of this bias is an inappropriate likelihood function. The correct likelihood function is derived and shown to be computationally intractable. A more computationally efficient method of approximate Bayesian computation (ABC) is explored. This method is shown to have less bias for data generated from idealised rank-frequency Zipfian distributions. However, the existing estimators and the ABC estimator described here assume that words are drawn from a simple probability distribution, while language is a much more complex process. We show that this false assumption leads to continued biases when applying any of these methods to natural language to estimate Zipf exponents. We recommend that researchers be aware of these biases when investigating power laws in rank-frequency data.

</details>

<details>

<summary>2021-07-26 14:53:14 - Are Bayesian neural networks intrinsically good at out-of-distribution detection?</summary>

- *Christian Henning, Francesco D'Angelo, Benjamin F. Grewe*

- `2107.12248v1` - [abs](http://arxiv.org/abs/2107.12248v1) - [pdf](http://arxiv.org/pdf/2107.12248v1)

> The need to avoid confident predictions on unfamiliar data has sparked interest in out-of-distribution (OOD) detection. It is widely assumed that Bayesian neural networks (BNN) are well suited for this task, as the endowed epistemic uncertainty should lead to disagreement in predictions on outliers. In this paper, we question this assumption and provide empirical evidence that proper Bayesian inference with common neural network architectures does not necessarily lead to good OOD detection. To circumvent the use of approximate inference, we start by studying the infinite-width case, where Bayesian inference can be exact considering the corresponding Gaussian process. Strikingly, the kernels induced under common architectural choices lead to uncertainties that do not reflect the underlying data generating process and are therefore unsuited for OOD detection. Finally, we study finite-width networks using HMC, and observe OOD behavior that is consistent with the infinite-width case. Overall, our study discloses fundamental problems when naively using BNNs for OOD detection and opens interesting avenues for future research.

</details>

<details>

<summary>2021-07-26 17:52:46 - Wasserstein-Splitting Gaussian Process Regression for Heterogeneous Online Bayesian Inference</summary>

- *Michael E. Kepler, Alec Koppel, Amrit Singh Bedi, Daniel J. Stilwell*

- `2107.12797v1` - [abs](http://arxiv.org/abs/2107.12797v1) - [pdf](http://arxiv.org/pdf/2107.12797v1)

> Gaussian processes (GPs) are a well-known nonparametric Bayesian inference technique, but they suffer from scalability problems for large sample sizes, and their performance can degrade for non-stationary or spatially heterogeneous data. In this work, we seek to overcome these issues through (i) employing variational free energy approximations of GPs operating in tandem with online expectation propagation steps; and (ii) introducing a local splitting step which instantiates a new GP whenever the posterior distribution changes significantly as quantified by the Wasserstein metric over posterior distributions. Over time, then, this yields an ensemble of sparse GPs which may be updated incrementally, and adapts to locality, heterogeneity, and non-stationarity in training data.

</details>

<details>

<summary>2021-07-27 03:14:09 - Identification of parameters in the torsional dynamics of a drilling process through Bayesian statistics</summary>

- *Mario Germán Sandoval, Americo Cunha Jr, Rubens Sampaio*

- `2107.13535v1` - [abs](http://arxiv.org/abs/2107.13535v1) - [pdf](http://arxiv.org/pdf/2107.13535v1)

> This work presents the estimation of the parameters of an experimental setup, which is modeled as a system with three degrees of freedom, composed by a shaft, two rotors, and a DC motor, that emulates a drilling process. A Bayesian technique is used in the estimation process, to take into account the uncertainties and variabilities intrinsic to the measurement taken, which are modeled as a noise of Gaussian nature. With this procedure it is expected to check the reliability of the nominal values of the physical parameters of the test rig. An estimation process assuming that nine parameters of the experimental apparatus are unknown is conducted, and the results show that for some quantities the relative deviation with respect to the nominal values is very high. This deviation evidentiates a strong deficiency in the mathematical model used to describe the dynamic behavior of the experimental apparatus.

</details>

<details>

<summary>2021-07-27 07:36:52 - Communication, Renegotiation and Coordination with Private Values (Extended Version)</summary>

- *Yuval Heller, Christoph Kuzmics*

- `2005.05713v3` - [abs](http://arxiv.org/abs/2005.05713v3) - [pdf](http://arxiv.org/pdf/2005.05713v3)

> An equilibrium is communication-proof if it is unaffected by new opportunities to communicate and renegotiate. We characterize the set of equilibria of coordination games with pre-play communication in which players have private preferences over the feasible coordinated outcomes. Communication-proof equilibria provide a narrow selection from the large set of qualitatively diverse Bayesian Nash equilibria in such games. Under a communication-proof equilibrium, players never miscoordinate, play their jointly preferred outcome whenever there is one, and communicate only the ordinal part of their preferences. Moreover, such equilibria are robust to changes in players' beliefs, interim Pareto efficient, and evolutionarily stable.

</details>

<details>

<summary>2021-07-27 11:42:27 - Technical properties of Ranked Nodes Method</summary>

- *Pekka Laitila, Kai Virtanen*

- `2107.12747v1` - [abs](http://arxiv.org/abs/2107.12747v1) - [pdf](http://arxiv.org/pdf/2107.12747v1)

> This paper presents analytical and experimental results on the ranked nodes method (RNM) that is used to construct conditional probability tables for Bayesian networks by expert elicitation. The majority of the results are focused on a setting in which RNM is applied to a child node and parent nodes that all have the same amount discrete ordinal states. The results indicate on RNM properties that can be used to support its future elaboration and development.

</details>

<details>

<summary>2021-07-27 15:14:09 - Efficient inference of interventional distributions</summary>

- *Arnab Bhattacharyya, Sutanu Gayen, Saravanan Kandasamy, Vedant Raval, N. V. Vinodchandran*

- `2107.11712v2` - [abs](http://arxiv.org/abs/2107.11712v2) - [pdf](http://arxiv.org/pdf/2107.11712v2)

> We consider the problem of efficiently inferring interventional distributions in a causal Bayesian network from a finite number of observations. Let $\mathcal{P}$ be a causal model on a set $\mathbf{V}$ of observable variables on a given causal graph $G$. For sets $\mathbf{X},\mathbf{Y}\subseteq \mathbf{V}$, and setting ${\bf x}$ to $\mathbf{X}$, let $P_{\bf x}(\mathbf{Y})$ denote the interventional distribution on $\mathbf{Y}$ with respect to an intervention ${\bf x}$ to variables ${\bf x}$. Shpitser and Pearl (AAAI 2006), building on the work of Tian and Pearl (AAAI 2001), gave an exact characterization of the class of causal graphs for which the interventional distribution $P_{\bf x}({\mathbf{Y}})$ can be uniquely determined. We give the first efficient version of the Shpitser-Pearl algorithm. In particular, under natural assumptions, we give a polynomial-time algorithm that on input a causal graph $G$ on observable variables $\mathbf{V}$, a setting ${\bf x}$ of a set $\mathbf{X} \subseteq \mathbf{V}$ of bounded size, outputs succinct descriptions of both an evaluator and a generator for a distribution $\hat{P}$ that is $\varepsilon$-close (in total variation distance) to $P_{\bf x}({\mathbf{Y}})$ where $Y=\mathbf{V}\setminus \mathbf{X}$, if $P_{\bf x}(\mathbf{Y})$ is identifiable. We also show that when $\mathbf{Y}$ is an arbitrary set, there is no efficient algorithm that outputs an evaluator of a distribution that is $\varepsilon$-close to $P_{\bf x}({\mathbf{Y}})$ unless all problems that have statistical zero-knowledge proofs, including the Graph Isomorphism problem, have efficient randomized algorithms.

</details>

<details>

<summary>2021-07-27 17:14:09 - School neighbourhood and compliance with WHO-recommended annual NO2 guideline: a case study of Greater London</summary>

- *Niloofar Shoari, Shahram Heydari, Marta Blangiardo*

- `2107.12952v1` - [abs](http://arxiv.org/abs/2107.12952v1) - [pdf](http://arxiv.org/pdf/2107.12952v1)

> Despite several national and local policies towards cleaner air in England, many schools in London breach the WHO-recommended concentrations of air pollutants such as NO2 and PM2.5. This is while, previous studies highlight significant adverse health effects of air pollutants on children's health. In this paper we adopted a Bayesian spatial hierarchical model to investigate factors that affect the odds of schools exceeding the WHO-recommended concentration of NO2 (i.e., 40 ug/m3 annual mean) in Greater London (UK). We considered a host of variables including schools' characteristics as well as their neighbourhoods' attributes from household, socioeconomic, transport-related, land use, built and natural environment characteristics perspectives. The results indicated that transport-related factors including the number of traffic lights and bus stops in the immediate vicinity of schools, and borough-level bus fuel consumption are determinant factors that increase the likelihood of non-compliance with the WHO guideline. In contrast, distance from roads, river transport, and underground stations, vehicle speed (an indicator of traffic congestion), the proportion of borough-level green space, and the area of green space at schools reduce the likelihood of exceeding the WHO recommended concentration of NO2. As a sensitivity analysis, we repeated our analysis under a hypothetical scenario in which the recommended concentration of NO2 is 35 ug/m3, instead of 40 ug/m3. Our results underscore the importance of adopting clean fuel technologies on buses, installing green barriers, and reducing motorised traffic around schools in reducing exposure to NO2 concentrations in proximity to schools. This study would be useful for local authority decision making with the aim of improving air quality for school-aged children in urban settings.

</details>

<details>

<summary>2021-07-27 19:31:13 - Numerical issues in maximum likelihood parameter estimation for Gaussian process interpolation</summary>

- *Subhasish Basak, Sébastien Petit, Julien Bect, Emmanuel Vazquez*

- `2101.09747v2` - [abs](http://arxiv.org/abs/2101.09747v2) - [pdf](http://arxiv.org/pdf/2101.09747v2)

> This article investigates the origin of numerical issues in maximum likelihood parameter estimation for Gaussian process (GP) interpolation and investigates simple but effective strategies for improving commonly used open-source software implementations. This work targets a basic problem but a host of studies, particularly in the literature of Bayesian optimization, rely on off-the-shelf GP implementations. For the conclusions of these studies to be reliable and reproducible, robust GP implementations are critical.

</details>

<details>

<summary>2021-07-28 05:13:20 - Unbiased MLMC stochastic gradient-based optimization of Bayesian experimental designs</summary>

- *Takashi Goda, Tomohiko Hironaka, Wataru Kitade, Adam Foster*

- `2005.08414v3` - [abs](http://arxiv.org/abs/2005.08414v3) - [pdf](http://arxiv.org/pdf/2005.08414v3)

> In this paper we propose an efficient stochastic optimization algorithm to search for Bayesian experimental designs such that the expected information gain is maximized. The gradient of the expected information gain with respect to experimental design parameters is given by a nested expectation, for which the standard Monte Carlo method using a fixed number of inner samples yields a biased estimator. In this paper, applying the idea of randomized multilevel Monte Carlo (MLMC) methods, we introduce an unbiased Monte Carlo estimator for the gradient of the expected information gain with finite expected squared $\ell_2$-norm and finite expected computational cost per sample. Our unbiased estimator can be combined well with stochastic gradient descent algorithms, which results in our proposal of an optimization algorithm to search for an optimal Bayesian experimental design. Numerical experiments confirm that our proposed algorithm works well not only for a simple test problem but also for a more realistic pharmacokinetic problem.

</details>

<details>

<summary>2021-07-28 08:35:25 - Soft Tensor Regression</summary>

- *Georgia Papadogeorgou, Zhengwu Zhang, David B. Dunson*

- `1910.09699v2` - [abs](http://arxiv.org/abs/1910.09699v2) - [pdf](http://arxiv.org/pdf/1910.09699v2)

> Statistical methods relating tensor predictors to scalar outcomes in a regression model generally vectorize the tensor predictor and estimate the coefficients of its entries employing some form of regularization, use summaries of the tensor covariate, or use a low dimensional approximation of the coefficient tensor. However, low rank approximations of the coefficient tensor can suffer if the true rank is not small. We propose a tensor regression framework which assumes a soft version of the parallel factors (PARAFAC) approximation. In contrast to classic PARAFAC, where each entry of the coefficient tensor is the sum of products of row-specific contributions across the tensor modes, the soft tensor regression (Softer) framework allows the row-specific contributions to vary around an overall mean. We follow a Bayesian approach to inference, and show that softening the PARAFAC increases model flexibility, leads to improved estimation of coefficient tensors, more accurate identification of important predictor entries, and more precise predictions, even for a low approximation rank. From a theoretical perspective, we show that employing Softer leads to a weakly consistent posterior distribution of the coefficient tensor, irrespective of the true or approximation tensor rank, a result that is not true when employing the classic PARAFAC for tensor regression. In the context of our motivating application, we adapt Softer to symmetric and semi-symmetric tensor predictors and analyze the relationship between brain network characteristics and human traits.soft

</details>

<details>

<summary>2021-07-28 11:51:35 - Bayesian Autoencoders: Analysing and Fixing the Bernoulli likelihood for Out-of-Distribution Detection</summary>

- *Bang Xiang Yong, Tim Pearce, Alexandra Brintrup*

- `2107.13304v1` - [abs](http://arxiv.org/abs/2107.13304v1) - [pdf](http://arxiv.org/pdf/2107.13304v1)

> After an autoencoder (AE) has learnt to reconstruct one dataset, it might be expected that the likelihood on an out-of-distribution (OOD) input would be low. This has been studied as an approach to detect OOD inputs. Recent work showed this intuitive approach can fail for the dataset pairs FashionMNIST vs MNIST. This paper suggests this is due to the use of Bernoulli likelihood and analyses why this is the case, proposing two fixes: 1) Compute the uncertainty of likelihood estimate by using a Bayesian version of the AE. 2) Use alternative distributions to model the likelihood.

</details>

<details>

<summary>2021-07-28 15:51:01 - Approximate Bayesian Computation for an Explicit-Duration Hidden Markov Model of COVID-19 Hospital Trajectories</summary>

- *Gian Marco Visani, Alexandra Hope Lee, Cuong Nguyen, David M. Kent, John B. Wong, Joshua T. Cohen, Michael C. Hughes*

- `2105.00773v2` - [abs](http://arxiv.org/abs/2105.00773v2) - [pdf](http://arxiv.org/pdf/2105.00773v2)

> We address the problem of modeling constrained hospital resources in the midst of the COVID-19 pandemic in order to inform decision-makers of future demand and assess the societal value of possible interventions. For broad applicability, we focus on the common yet challenging scenario where patient-level data for a region of interest are not available. Instead, given daily admissions counts, we model aggregated counts of observed resource use, such as the number of patients in the general ward, in the intensive care unit, or on a ventilator. In order to explain how individual patient trajectories produce these counts, we propose an aggregate count explicit-duration hidden Markov model, nicknamed the ACED-HMM, with an interpretable, compact parameterization. We develop an Approximate Bayesian Computation approach that draws samples from the posterior distribution over the model's transition and duration parameters given aggregate counts from a specific location, thus adapting the model to a region or individual hospital site of interest. Samples from this posterior can then be used to produce future forecasts of any counts of interest. Using data from the United States and the United Kingdom, we show our mechanistic approach provides competitive probabilistic forecasts for the future even as the dynamics of the pandemic shift. Furthermore, we show how our model provides insight about recovery probabilities or length of stay distributions, and we suggest its potential to answer challenging what-if questions about the societal value of possible interventions.

</details>

<details>

<summary>2021-07-28 20:53:56 - Bayesian Variable Selection for Cox Regression Model with Spatially Varying Coefficients with Applications to Louisiana Respiratory Cancer Data</summary>

- *Jinjian Mu, Qingyang Liu, Lynn Kuo, Guanyu Hu*

- `2008.00615v2` - [abs](http://arxiv.org/abs/2008.00615v2) - [pdf](http://arxiv.org/pdf/2008.00615v2)

> The Cox regression model is a commonly used model in survival analysis. In public health studies, clinical data are often collected from medical service providers of different locations. There are large geographical variations in the covariate effects on survival rates from particular diseases. In this paper, we focus on the variable selection issue for the Cox regression model with spatially varying coefficients. We propose a Bayesian hierarchical model which incorporates a horseshoe prior for sparsity and a point mass mixture prior to determine whether a regression coefficient is spatially varying. An efficient two-stage computational method is used for posterior inference and variable selection. It essentially applies the existing method for maximizing the partial likelihood for the Cox model by site independently first, and then applying an MCMC algorithm for variable selection based on results of the first stage. Extensive simulation studies are carried out to examine the empirical performance of the proposed method. Finally, we apply the proposed methodology to analyzing a real data set on respiratory cancer in Louisiana from the SEER program.

</details>

<details>

<summary>2021-07-29 00:34:16 - Learning Gaussian DAGs from Network Data</summary>

- *Hangjian Li, Oscar Hernan Madrid Padilla, Qing Zhou*

- `1905.10848v2` - [abs](http://arxiv.org/abs/1905.10848v2) - [pdf](http://arxiv.org/pdf/1905.10848v2)

> Structural learning of directed acyclic graphs (DAGs) or Bayesian networks has been studied extensively under the assumption that data are independent. We propose a new Gaussian DAG model for dependent data which assumes the observations are correlated according to an undirected network. Under this model, we develop a method to estimate the DAG structure given a topological ordering of the nodes. The proposed method jointly estimates the Bayesian network and the correlations among observations by optimizing a scoring function based on penalized likelihood. We show that under some mild conditions, the proposed method produces consistent estimators after one iteration. Extensive numerical experiments also demonstrate that by jointly estimating the DAG structure and the sample correlation, our method achieves much higher accuracy in structure learning. When the node ordering is unknown, through experiments on synthetic and real data, we show that our algorithm can be used to estimate the correlations between samples, with which we can de-correlate the dependent data to significantly improve the performance of classical DAG learning methods.

</details>

<details>

<summary>2021-07-29 06:17:35 - CARlasso: An R package for the estimation of sparse microbial networks with predictors</summary>

- *Yunyi Shen, Claudia Solis-Lemus*

- `2107.13763v1` - [abs](http://arxiv.org/abs/2107.13763v1) - [pdf](http://arxiv.org/pdf/2107.13763v1)

> Microbiome data analyses require statistical tools that can simultaneously decode microbes' reactions to the environment and interactions among microbes. We introduce CARlasso, the first user-friendly open-source and publicly available R package to fit a chain graph model for the inference of sparse microbial networks that represent both interactions among nodes and effects of a set of predictors. Unlike in standard regression approaches, the edges represent the correct conditional structure among responses and predictors that allows the incorporation of prior knowledge from controlled experiments. In addition, CARlasso 1) enforces sparsity in the network via LASSO; 2) allows for an adaptive extension to include different shrinkage to different edges; 3) is computationally inexpensive through an efficient Gibbs sampling algorithm so it can equally handle small and big data; 4) allows for continuous, binary, counting and compositional responses via proper hierarchical structure, and 5) has a similar syntax to lm for ease of use. The package also supports Bayesian graphical LASSO and several of its hierarchical models as well as lower level one-step sampling functions of the CAR-LASSO model for users to extend.

</details>

<details>

<summary>2021-07-29 06:49:34 - Bayesian Optimization for Min Max Optimization</summary>

- *Dorina Weichert, Alexander Kister*

- `2107.13772v1` - [abs](http://arxiv.org/abs/2107.13772v1) - [pdf](http://arxiv.org/pdf/2107.13772v1)

> A solution that is only reliable under favourable conditions is hardly a safe solution. Min Max Optimization is an approach that returns optima that are robust against worst case conditions. We propose algorithms that perform Min Max Optimization in a setting where the function that should be optimized is not known a priori and hence has to be learned by experiments. Therefore we extend the Bayesian Optimization setting, which is tailored to maximization problems, to Min Max Optimization problems. While related work extends the two acquisition functions Expected Improvement and Gaussian Process Upper Confidence Bound; we extend the two acquisition functions Entropy Search and Knowledge Gradient. These acquisition functions are able to gain knowledge about the optimum instead of just looking for points that are supposed to be optimal. In our evaluation we show that these acquisition functions allow for better solutions - converging faster to the optimum than the benchmark settings.

</details>

<details>

<summary>2021-07-29 07:25:35 - Efficiently resolving rotational ambiguity in Bayesian matrix sampling with matching</summary>

- *Evan Poworoznek, Federico Ferrari, David Dunson*

- `2107.13783v1` - [abs](http://arxiv.org/abs/2107.13783v1) - [pdf](http://arxiv.org/pdf/2107.13783v1)

> A wide class of Bayesian models involve unidentifiable random matrices that display rotational ambiguity, with the Gaussian factor model being a typical example. A rich variety of Markov chain Monte Carlo (MCMC) algorithms have been proposed for sampling the parameters of these models. However, without identifiability constraints, reliable posterior summaries of the parameters cannot be obtained directly from the MCMC output. As an alternative, we propose a computationally efficient post-processing algorithm that allows inference on non-identifiable parameters. We first orthogonalize the posterior samples using Varimax and then tackle label and sign switching with a greedy matching algorithm. We compare the performance and computational complexity with other methods using a simulation study and chemical exposures data. The algorithm implementation is available in the infinitefactor R package on CRAN.

</details>

<details>

<summary>2021-07-29 10:57:04 - Neural Variational Gradient Descent</summary>

- *Lauro Langosco di Langosco, Vincent Fortuin, Heiko Strathmann*

- `2107.10731v2` - [abs](http://arxiv.org/abs/2107.10731v2) - [pdf](http://arxiv.org/pdf/2107.10731v2)

> Particle-based approximate Bayesian inference approaches such as Stein Variational Gradient Descent (SVGD) combine the flexibility and convergence guarantees of sampling methods with the computational benefits of variational inference. In practice, SVGD relies on the choice of an appropriate kernel function, which impacts its ability to model the target distribution -- a challenging problem with only heuristic solutions. We propose Neural Variational Gradient Descent (NVGD), which is based on parameterizing the witness function of the Stein discrepancy by a deep neural network whose parameters are learned in parallel to the inference, mitigating the necessity to make any kernel choices whatsoever. We empirically evaluate our method on popular synthetic inference problems, real-world Bayesian linear regression, and Bayesian neural network inference.

</details>

<details>

<summary>2021-07-29 14:47:00 - A Bayesian change point model for spatio-temporal data</summary>

- *Candace Berrett, Brianne Gurney, David Arthur, Todd Moon, Gus P. Williams*

- `2105.10637v2` - [abs](http://arxiv.org/abs/2105.10637v2) - [pdf](http://arxiv.org/pdf/2105.10637v2)

> Urbanization of an area is known to increase the temperature of the surrounding area. This phenomenon -- a so-called urban heat island (UHI) -- occurs at a local level over a period of time and has lasting impacts for historical data analysis. We propose a methodology to examine if long-term changes in temperature increases and decreases across time exist (and to what extent) at the local level for a given set of temperature readings at various locations. Specifically, we propose a Bayesian change point model for spatio-temporally dependent data where we select the number of change points at each location using a "forwards" selection process using deviance information criteria (DIC). We then fit the selected model and examine the linear slopes across time to quantify changes in long-term temperature behavior. We show the utility of this model and method using a synthetic data set and temperature measurements from eight stations in Utah consisting of daily temperature data for 60 years.

</details>

<details>

<summary>2021-07-29 17:04:31 - Coordinating users of shared facilities via data-driven predictive assistants and game theory</summary>

- *Philipp Geiger, Michel Besserve, Justus Winkelmann, Claudius Proissl, Bernhard Schölkopf*

- `1803.06247v6` - [abs](http://arxiv.org/abs/1803.06247v6) - [pdf](http://arxiv.org/pdf/1803.06247v6)

> We study data-driven assistants that provide congestion forecasts to users of shared facilities (roads, cafeterias, etc.), to support coordination between them, and increase efficiency of such collective systems. Key questions are: (1) when and how much can (accurate) predictions help for coordination, and (2) which assistant algorithms reach optimal predictions?   First we lay conceptual ground for this setting where user preferences are a priori unknown and predictions influence outcomes. Addressing (1), we establish conditions under which self-fulfilling prophecies, i.e., "perfect" (probabilistic) predictions of what will happen, solve the coordination problem in the game-theoretic sense of selecting a Bayesian Nash equilibrium (BNE). Next we prove that such prophecies exist even in large-scale settings where only aggregated statistics about users are available. This entails a new (nonatomic) BNE existence result. Addressing (2), we propose two assistant algorithms that sequentially learn from users' reactions, together with optimality/convergence guarantees. We validate one of them in a large real-world experiment.

</details>

<details>

<summary>2021-07-30 07:25:07 - Trusted-Maximizers Entropy Search for Efficient Bayesian Optimization</summary>

- *Quoc Phong Nguyen, Zhaoxuan Wu, Bryan Kian Hsiang Low, Patrick Jaillet*

- `2107.14465v1` - [abs](http://arxiv.org/abs/2107.14465v1) - [pdf](http://arxiv.org/pdf/2107.14465v1)

> Information-based Bayesian optimization (BO) algorithms have achieved state-of-the-art performance in optimizing a black-box objective function. However, they usually require several approximations or simplifying assumptions (without clearly understanding their effects on the BO performance) and/or their generalization to batch BO is computationally unwieldy, especially with an increasing batch size. To alleviate these issues, this paper presents a novel trusted-maximizers entropy search (TES) acquisition function: It measures how much an input query contributes to the information gain on the maximizer over a finite set of trusted maximizers, i.e., inputs optimizing functions that are sampled from the Gaussian process posterior belief of the objective function. Evaluating TES requires either only a stochastic approximation with sampling or a deterministic approximation with expectation propagation, both of which are investigated and empirically evaluated using synthetic benchmark objective functions and real-world optimization problems, e.g., hyperparameter tuning of a convolutional neural network and synthesizing 'physically realizable' faces to fool a black-box face recognition system. Though TES can naturally be generalized to a batch variant with either approximation, the latter is amenable to be scaled to a much larger batch size in our experiments.

</details>

<details>

<summary>2021-07-30 12:02:30 - Multiple exposure distributed lag models with variable selection</summary>

- *Joseph Antonelli, Ander Wilson, Brent Coull*

- `2107.14567v1` - [abs](http://arxiv.org/abs/2107.14567v1) - [pdf](http://arxiv.org/pdf/2107.14567v1)

> Distributed lag models are useful in environmental epidemiology as they allow the user to investigate critical windows of exposure, defined as the time period during which exposure to a pollutant adversely affects health outcomes. Recent studies have focused on estimating the health effects of a large number of environmental exposures, or an environmental mixture, on health outcomes. In such settings, it is important to understand which environmental exposures affect a particular outcome, while acknowledging the possibility that different exposures have different critical windows. Further, in the studies of environmental mixtures, it is important to identify interactions among exposures, and to account for the fact that this interaction may occur between two exposures having different critical windows. Exposure to one exposure early in time could cause an individual to be more or less susceptible to another exposure later in time. We propose a Bayesian model to estimate the temporal effects of a large number of exposures on an outcome. We use spike-and-slab priors and semiparametric distributed lag curves to identify important exposures and exposure interactions, and discuss extensions with improved power to detect harmful exposures. We then apply these methods to estimate the effects of exposure to multiple air pollutants during pregnancy on birthweight from vital records in Colorado.

</details>

<details>

<summary>2021-07-30 14:02:18 - On the accuracy and precision of correlation functions and field-level inference in cosmology</summary>

- *Florent Leclercq, Alan Heavens*

- `2103.04158v2` - [abs](http://arxiv.org/abs/2103.04158v2) - [pdf](http://arxiv.org/pdf/2103.04158v2)

> We present a comparative study of the accuracy and precision of correlation function methods and full-field inference in cosmological data analysis. To do so, we examine a Bayesian hierarchical model that predicts log-normal fields and their two-point correlation function. Although a simplified analytic model, the log-normal model produces fields that share many of the essential characteristics of the present-day non-Gaussian cosmological density fields. We use three different statistical techniques: (i) a standard likelihood-based analysis of the two-point correlation function; (ii) a likelihood-free (simulation-based) analysis of the two-point correlation function; (iii) a field-level analysis, made possible by the more sophisticated data assimilation technique. We find that (a) standard assumptions made to write down a likelihood for correlation functions can cause significant biases, a problem that is alleviated with simulation-based inference; and (b) analysing the entire field offers considerable advantages over correlation functions, through higher accuracy, higher precision, or both. The gains depend on the degree of non-Gaussianity, but in all cases, including for weak non-Gaussianity, the advantage of analysing the full field is substantial.

</details>

<details>

<summary>2021-07-30 14:22:58 - Fast Bayesian inference for large occupancy data sets, using the Polya-Gamma scheme</summary>

- *Alex Diana, Emily Dennis, Eleni Matechou, Byron Morgan*

- `2107.14656v1` - [abs](http://arxiv.org/abs/2107.14656v1) - [pdf](http://arxiv.org/pdf/2107.14656v1)

> In recent years, the study of species' occurrence has benefited from the increased availability of large-scale citizen-science data. Whilst abundance data from standardized monitoring schemes are biased towards well-studied taxa and locations, opportunistic data are available for many taxonomic groups, from a large number of locations and across long timescales. Hence, these data provide opportunities to measure species' changes in occurrence, particularly through the use of occupancy models, which account for imperfect detection. However, existing Bayesian occupancy models are extremely slow when applied to large citizen-science data sets. In this paper, we propose a novel framework for fast Bayesian inference in occupancy models that account for both spatial and temporal autocorrelation. We express the occupancy and detection processes within a logistic regression framework, which enables us to use the Polya-Gamma scheme to perform inference quickly and efficiently, even for very large data sets. Spatial and temporal random effects are modelled using Gaussian processes, allowing us to infer the strength of spatio-temporal autocorrelation from the data. We apply our model to data on two UK butterfly species, one common and widespread and one rare, using records from the Butterflies for the New Millennium database, producing occupancy indices spanning 45 years. Our framework can be applied to a wide range of taxa, providing measures of variation in species' occurrence, which are used to assess biodiversity change.

</details>

<details>

<summary>2021-07-30 20:07:09 - A New Bayesian Optimization Algorithm for Complex High-Dimensional Disease Epidemic Systems</summary>

- *Yuyang Chen, Kaiming Bi, Chih-Hang J. Wu, David Ben-Arieh, Ashesh Sinha*

- `2108.00062v1` - [abs](http://arxiv.org/abs/2108.00062v1) - [pdf](http://arxiv.org/pdf/2108.00062v1)

> This paper presents an Improved Bayesian Optimization (IBO) algorithm to solve complex high-dimensional epidemic models' optimal control solution. Evaluating the total objective function value for disease control models with hundreds of thousands of control time periods is a high computational cost. In this paper, we improve the conventional Bayesian Optimization (BO) approach from two parts. The existing BO methods optimize the minimizer step for once time during each acquisition function update process. To find a better solution for each acquisition function update, we do more local minimization steps to tune the algorithm. When the model is high dimensions, and the objective function is complicated, only some update iterations of the acquisition function may not find the global optimal solution. The IBO algorithm adds a series of Adam-based steps at the final stage of the algorithm to increase the solution's accuracy. Comparative simulation experiments using different kernel functions and acquisition functions have shown that the Improved Bayesian Optimization algorithm is effective and suitable for handing large-scale and complex epidemic models under study. The IBO algorithm is then compared with four other global optimization algorithms on three well-known synthetic test functions. The effectiveness and robustness of the IBO algorithm are also demonstrated through some simulation experiments to compare with the Particle Swarm Optimization algorithm and Random Search algorithm. With its reliable convergence behaviors and straightforward implementation, the IBO algorithm has a great potential to solve other complex optimal control problems with high dimensionality.

</details>

<details>

<summary>2021-07-30 20:15:52 - OPFython: A Python-Inspired Optimum-Path Forest Classifier</summary>

- *Gustavo Henrique de Rosa, João Paulo Papa, Alexandre Xavier Falcão*

- `2001.10420v3` - [abs](http://arxiv.org/abs/2001.10420v3) - [pdf](http://arxiv.org/pdf/2001.10420v3)

> Machine learning techniques have been paramount throughout the last years, being applied in a wide range of tasks, such as classification, object recognition, person identification, and image segmentation. Nevertheless, conventional classification algorithms, e.g., Logistic Regression, Decision Trees, and Bayesian classifiers, might lack complexity and diversity, not suitable when dealing with real-world data. A recent graph-inspired classifier, known as the Optimum-Path Forest, has proven to be a state-of-the-art technique, comparable to Support Vector Machines and even surpassing it in some tasks. This paper proposes a Python-based Optimum-Path Forest framework, denoted as OPFython, where all of its functions and classes are based upon the original C language implementation. Additionally, as OPFython is a Python-based library, it provides a more friendly environment and a faster prototyping workspace than the C language.

</details>

<details>

<summary>2021-07-30 23:37:19 - Dependent Modeling of Temporal Sequences of Random Partitions</summary>

- *Garritt L. Page, Fernando A. Quintana, David B. Dahl*

- `1912.11542v3` - [abs](http://arxiv.org/abs/1912.11542v3) - [pdf](http://arxiv.org/pdf/1912.11542v3)

> We consider the task of modeling a dependent sequence of random partitions. It is well-known that a random measure in Bayesian nonparametrics induces a distribution over random partitions. The community has therefore assumed that the best approach to obtain a dependent sequence of random partitions is through modeling dependent random measures. We argue that this approach is problematic and show that the random partition model induced by dependent Bayesian nonparametric priors exhibit counter-intuitive dependence among partitions even though the dependence for the sequence of random probability measures is intuitive. Because of this, we advocate instead to model the sequence of random partitions directly when clustering is of principal interest. To this end, we develop a class of dependent random partition models that explicitly models dependence in a sequence of partitions. We derive conditional and marginal properties of the joint partition model and devise computational strategies when employing the method in Bayesian modeling. In the case of temporal dependence, we demonstrate through simulation how the methodology produces partitions that evolve gently and naturally over time. We further illustrate the utility of the method by applying it to an environmental data set that exhibits spatio-temporal dependence.

</details>

<details>

<summary>2021-07-31 00:29:53 - Bayesian Variable Selection in Multivariate Nonlinear Regression with Graph Structures</summary>

- *Yabo Niu, Nilabja Guha, Debkumar De, Anindya Bhadra, Veerabhadran Baladandayuthapani, Bani K. Mallick*

- `2010.14638v2` - [abs](http://arxiv.org/abs/2010.14638v2) - [pdf](http://arxiv.org/pdf/2010.14638v2)

> Gaussian graphical models (GGMs) are well-established tools for probabilistic exploration of dependence structures using precision matrices. We develop a Bayesian method to incorporate covariate information in this GGMs setup in a nonlinear seemingly unrelated regression framework. We propose a joint predictor and graph selection model and develop an efficient collapsed Gibbs sampler algorithm to search the joint model space. Furthermore, we investigate its theoretical variable selection properties. We demonstrate our method on a variety of simulated data, concluding with a real data set from the TCPA project.

</details>

<details>

<summary>2021-07-31 02:48:03 - Rationally Inattentive Utility Maximization for Interpretable Deep Image Classification</summary>

- *Kunal Pattanayak, Vikram Krishnamurthy*

- `2102.04594v3` - [abs](http://arxiv.org/abs/2102.04594v3) - [pdf](http://arxiv.org/pdf/2102.04594v3)

> Are deep convolutional neural networks (CNNs) for image classification explainable by utility maximization with information acquisition costs? We demonstrate that deep CNNs behave equivalently (in terms of necessary and sufficient conditions) to rationally inattentive utility maximizers, a generative model used extensively in economics for human decision making. Our claim is based by extensive experiments on 200 deep CNNs from 5 popular architectures. The parameters of our interpretable model are computed efficiently via convex feasibility algorithms. As an application, we show that our economics-based interpretable model can predict the classification performance of deep CNNs trained with arbitrary parameters with accuracy exceeding 94% . This eliminates the need to re-train the deep CNNs for image classification. The theoretical foundation of our approach lies in Bayesian revealed preference studied in micro-economics. All our results are on GitHub and completely reproducible.

</details>

<details>

<summary>2021-07-31 14:36:33 - Bayesian analysis of the prevalence bias: learning and predicting from imbalanced data</summary>

- *Loic Le Folgoc, Vasileios Baltatzis, Amir Alansary, Sujal Desai, Anand Devaraj, Sam Ellis, Octavio E. Martinez Manzanera, Fahdi Kanavati, Arjun Nair, Julia Schnabel, Ben Glocker*

- `2108.00250v1` - [abs](http://arxiv.org/abs/2108.00250v1) - [pdf](http://arxiv.org/pdf/2108.00250v1)

> Datasets are rarely a realistic approximation of the target population. Say, prevalence is misrepresented, image quality is above clinical standards, etc. This mismatch is known as sampling bias. Sampling biases are a major hindrance for machine learning models. They cause significant gaps between model performance in the lab and in the real world. Our work is a solution to prevalence bias. Prevalence bias is the discrepancy between the prevalence of a pathology and its sampling rate in the training dataset, introduced upon collecting data or due to the practioner rebalancing the training batches. This paper lays the theoretical and computational framework for training models, and for prediction, in the presence of prevalence bias. Concretely a bias-corrected loss function, as well as bias-corrected predictive rules, are derived under the principles of Bayesian risk minimization. The loss exhibits a direct connection to the information gain. It offers a principled alternative to heuristic training losses and complements test-time procedures based on selecting an operating point from summary curves. It integrates seamlessly in the current paradigm of (deep) learning using stochastic backpropagation and naturally with Bayesian models.

</details>


## 2021-08

<details>

<summary>2021-08-01 21:34:02 - Bayesian nonparametric tests for multivariate locations</summary>

- *Indrabati Bhattacharya, Subhashis Ghosal*

- `2007.00568v2` - [abs](http://arxiv.org/abs/2007.00568v2) - [pdf](http://arxiv.org/pdf/2007.00568v2)

> In this paper, we propose novel, fully Bayesian non-parametric tests for one-sample and two-sample multivariate location problems. We model the underlying distribution using a Dirichlet process prior, and develop a testing procedure based on the posterior credible region for the spatial median functional of the distribution. For the one-sample problem, we fail to reject the null hypothesis if the credible set contains the null value. For the two-sample problem, we form a credible set for the difference of the spatial medians of the two samples and we fail to reject the null hypothesis of equality if the credible set contains zero. We derive the local asymptotic power of the tests under shrinking alternatives, and also present a simulation study to compare the finite-sample performance of our testing procedures with existing parametric and non-parametric tests.

</details>

<details>

<summary>2021-08-02 08:51:59 - Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning</summary>

- *Samuel Kessler, Vu Nguyen, Stefan Zohren, Stephen Roberts*

- `1912.02290v5` - [abs](http://arxiv.org/abs/1912.02290v5) - [pdf](http://arxiv.org/pdf/1912.02290v5)

> We place an Indian Buffet process (IBP) prior over the structure of a Bayesian Neural Network (BNN), thus allowing the complexity of the BNN to increase and decrease automatically. We further extend this model such that the prior on the structure of each hidden layer is shared globally across all layers, using a Hierarchical-IBP (H-IBP). We apply this model to the problem of resource allocation in Continual Learning (CL) where new tasks occur and the network requires extra resources. Our model uses online variational inference with reparameterisation of the Bernoulli and Beta distributions, which constitute the IBP and H-IBP priors. As we automatically learn the number of weights in each layer of the BNN, overfitting and underfitting problems are largely overcome. We show empirically that our approach offers a competitive edge over existing methods in CL.

</details>

<details>

<summary>2021-08-02 13:55:57 - Bayesian Nonparametric Classification for Incomplete Data With a High Missing Rate: an Application to Semiconductor Manufacturing Data</summary>

- *Sewon Park, Kyeongwon Lee, Da-Eun Jeong, Heung-Kook Ko, Jaeyong Lee*

- `2107.14409v2` - [abs](http://arxiv.org/abs/2107.14409v2) - [pdf](http://arxiv.org/pdf/2107.14409v2)

> During the semiconductor manufacturing process, predicting the yield of the semiconductor is an important problem. Early detection of defective product production in the manufacturing process can save huge production cost. The data generated from the semiconductor manufacturing process have characteristics of highly non-normal distributions, complicated missing patterns and high missing rate, which complicate the prediction of the yield. We propose Dirichlet process - naive Bayes model (DPNB), a classification method based on the mixtures of Dirichlet process and naive Bayes model. Since the DPNB is based on the mixtures of Dirichlet process and learns the joint distribution of all variables involved, it can handle highly non-normal data and can make predictions for the test dataset with any missing patterns. The DPNB also performs well for high missing rates since it uses all information of observed components. Experiments on various real datasets including semiconductor manufacturing data show that the DPNB has better performance than MICE and MissForest in terms of predicting missing values as percentage of missing values increases.

</details>

<details>

<summary>2021-08-02 15:17:09 - End-to-end Robustness for Sensing-Reasoning Machine Learning Pipelines</summary>

- *Zhuolin Yang, Zhikuan Zhao, Hengzhi Pei, Boxin Wang, Bojan Karlas, Ji Liu, Heng Guo, Bo Li, Ce Zhang*

- `2003.00120v3` - [abs](http://arxiv.org/abs/2003.00120v3) - [pdf](http://arxiv.org/pdf/2003.00120v3)

> As machine learning (ML) being applied to many mission-critical scenarios, certifying ML model robustness becomes increasingly important. Many previous works focuses on the robustness of independent ML and ensemble models, and can only certify a very small magnitude of the adversarial perturbation. In this paper, we take a different viewpoint and improve learning robustness by going beyond independent ML and ensemble models. We aim at promoting the generic Sensing-Reasoning machine learning pipeline which contains both the sensing (e.g. deep neural networks) and reasoning (e.g. Markov logic networks (MLN)) components enriched with domain knowledge. Can domain knowledge help improve learning robustness? Can we formally certify the end-to-end robustness of such an ML pipeline? We first theoretically analyze the computational complexity of checking the provable robustness in the reasoning component. We then derive the provable robustness bound for several concrete reasoning components. We show that for reasoning components such as MLN and a specific family of Bayesian networks it is possible to certify the robustness of the whole pipeline even with a large magnitude of perturbation which cannot be certified by existing work. Finally, we conduct extensive real-world experiments on large scale datasets to evaluate the certified robustness for Sensing-Reasoning ML pipelines.

</details>

<details>

<summary>2021-08-02 17:02:14 - Bayesian Sample Size Calculations for SMART Studies</summary>

- *Armando Turchetta, Erica E. M. Moodie, David A. Stephens, Sylvie D. Lambert*

- `2108.01041v1` - [abs](http://arxiv.org/abs/2108.01041v1) - [pdf](http://arxiv.org/pdf/2108.01041v1)

> In the management of most chronic conditions characterized by the lack of universally effective treatments, adaptive treatment strategies (ATSs) have been growing in popularity as they offer a more individualized approach, and sequential multiple assignment randomized trials (SMARTs) have gained attention as the most suitable clinical trial design to formalize the study of these strategies. While the number of SMARTs has increased in recent years, their design has remained limited to the frequentist setting, which may not fully or appropriately account for uncertainty in design parameters and hence not yield appropriate sample size recommendations. Specifically, standard frequentist formulae rely on several assumptions that can be easily misspecified. The Bayesian framework offers a straightforward path to alleviate some of these concerns. In this paper, we provide calculations in a Bayesian setting to allow more realistic and robust estimates that account for uncertainty in inputs through the `two priors' approach. Additionally, compared to the standard formulae, this methodology allows us to rely on fewer assumptions, integrate pre-trial knowledge, and switch the focus from the standardized effect size to the minimal detectable difference. The proposed methodology is evaluated in a thorough simulation study and is implemented to estimate the sample size for a full-scale SMART of an Internet-Based Adaptive Stress Management intervention based on a pilot SMART conducted on cardiovascular disease patients from two Canadian provinces.

</details>

<details>

<summary>2021-08-02 18:50:12 - Efficacy of Statistical and Artificial Intelligence-based False Information Cyberattack Detection Models for Connected Vehicles</summary>

- *Sakib Mahmud Khan, Gurcan Comert, Mashrur Chowdhury*

- `2108.01124v1` - [abs](http://arxiv.org/abs/2108.01124v1) - [pdf](http://arxiv.org/pdf/2108.01124v1)

> Connected vehicles (CVs), because of the external connectivity with other CVs and connected infrastructure, are vulnerable to cyberattacks that can instantly compromise the safety of the vehicle itself and other connected vehicles and roadway infrastructure. One such cyberattack is the false information attack, where an external attacker injects inaccurate information into the connected vehicles and eventually can cause catastrophic consequences by compromising safety-critical applications like the forward collision warning. The occurrence and target of such attack events can be very dynamic, making real-time and near-real-time detection challenging. Change point models, can be used for real-time anomaly detection caused by the false information attack. In this paper, we have evaluated three change point-based statistical models; Expectation Maximization, Cumulative Summation, and Bayesian Online Change Point Algorithms for cyberattack detection in the CV data. Also, data-driven artificial intelligence (AI) models, which can be used to detect known and unknown underlying patterns in the dataset, have the potential of detecting a real-time anomaly in the CV data. We have used six AI models to detect false information attacks and compared the performance for detecting the attacks with our developed change point models. Our study shows that change points models performed better in real-time false information attack detection compared to the performance of the AI models. Change point models having the advantage of no training requirements can be a feasible and computationally efficient alternative to AI models for false information attack detection in connected vehicles.

</details>

<details>

<summary>2021-08-03 05:54:27 - The G-Wishart Weighted Proposal Algorithm: Efficient Posterior Computation for Gaussian Graphical Models</summary>

- *Willem van den Boom, Alexandros Beskos, Maria De Iorio*

- `2108.01308v1` - [abs](http://arxiv.org/abs/2108.01308v1) - [pdf](http://arxiv.org/pdf/2108.01308v1)

> Gaussian graphical models can capture complex dependency structures amongst variables. For such models, Bayesian inference is attractive as it provides principled ways to incorporate prior information and to quantify uncertainty through the posterior distribution. However, posterior computation under the conjugate G-Wishart prior distribution on the precision matrix is expensive for general non-decomposable graphs. We therefore propose a new Markov chain Monte Carlo (MCMC) method named the G-Wishart weighted proposal algorithm (WWA). WWA's distinctive features include delayed acceptance MCMC, Gibbs updates for the precision matrix and an informed proposal distribution on the graph space that enables embarrassingly parallel computations. Compared to existing approaches, WWA reduces the frequency of the relatively expensive sampling from the G-Wishart distribution. This results in faster MCMC convergence, improved MCMC mixing and reduced computation time. Numerical studies on simulated and real data show that WWA provides a more efficient tool for posterior inference than competing state-of-the-art MCMC algorithms.

</details>

<details>

<summary>2021-08-03 09:55:38 - Bayesian inference on the number of recurrent events: A joint model of recurrence and survival</summary>

- *Willem van den Boom, Maria De Iorio, Marta Tallarita*

- `2005.06819v2` - [abs](http://arxiv.org/abs/2005.06819v2) - [pdf](http://arxiv.org/pdf/2005.06819v2)

> The number of recurrent events before a terminating event is often of interest. For instance, death terminates an individual's process of rehospitalizations and the number of rehospitalizations is an important indicator of economic cost. We propose a model in which the number of recurrences before termination is a random variable of interest, enabling inference and prediction on it. Then, conditionally on this number, we specify a joint distribution for recurrence and survival. This novel conditional approach induces dependence between recurrence and survival, which is often present, for instance due to frailty that affects both. Additional dependence between recurrence and survival is introduced by the specification of a joint distribution on their respective frailty terms. Moreover, through the introduction of an autoregressive model, our approach is able to capture the temporal dependence in the recurrent events trajectory. A non-parametric random effects distribution for the frailty terms accommodates population heterogeneity and allows for data-driven clustering of the subjects. A tailored Gibbs sampler involving reversible jump and slice sampling steps implements posterior inference. We illustrate our model on colorectal cancer data, compare its performance with existing approaches and provide appropriate inference on the number of recurrent events.

</details>

<details>

<summary>2021-08-03 13:07:16 - Variational Bayes on Manifolds</summary>

- *Minh-Ngoc Tran, Dang H. Nguyen, Duy Nguyen*

- `1908.03097v3` - [abs](http://arxiv.org/abs/1908.03097v3) - [pdf](http://arxiv.org/pdf/1908.03097v3)

> Variational Bayes (VB) has become a widely-used tool for Bayesian inference in statistics and machine learning. Nonetheless, the development of the existing VB algorithms is so far generally restricted to the case where the variational parameter space is Euclidean, which hinders the potential broad application of VB methods. This paper extends the scope of VB to the case where the variational parameter space is a Riemannian manifold. We develop an efficient manifold-based VB algorithm that exploits both the geometric structure of the constraint parameter space and the information geometry of the manifold of VB approximating probability distributions. Our algorithm is provably convergent and achieves a convergence rate of order $\mathcal O(1/\sqrt{T})$ and $\mathcal O(1/T^{2-2\epsilon})$ for a non-convex evidence lower bound function and a strongly retraction-convex evidence lower bound function, respectively. We develop in particular two manifold VB algorithms, Manifold Gaussian VB and Manifold Neural Net VB, and demonstrate through numerical experiments that the proposed algorithms are stable, less sensitive to initialization and compares favourably to existing VB methods.

</details>

<details>

<summary>2021-08-03 13:28:38 - Bayesian spectral density approach for identification and uncertainty quantification of bridge section's flutter derivatives operated in turbulent flow</summary>

- *Xiaolei Chu, Wei Cui, Peng Liu, Lin Zhao, Yaojun Ge*

- `2108.01494v1` - [abs](http://arxiv.org/abs/2108.01494v1) - [pdf](http://arxiv.org/pdf/2108.01494v1)

> This study presents a Bayesian spectral density approach for identification and uncertainty quantification of flutter derivatives of bridge sections utilizing buffeting displacement responses, where the wind tunnel test is conducted in turbulent flow. Different from traditional time-domain approaches (e.g., least square method and stochastic subspace identification), the newly-proposed approach is operated in frequency domain. Based on the affine invariant ensemble sampler algorithm, Markov chain Monte-Carlo sampling is employed to accomplish the Bayesian inference. The probability density function of flutter derivatives is modeled based on complex Wishart distribution, where probability serves as the measure. By the Bayesian spectral density approach, the most probable values and corresponding posterior distributions (namely identification uncertainty here) of each flutter derivative can be obtained at the same time. Firstly, numerical simulations are conducted and the identified results are accurate. Secondly, thin plate model, flutter derivatives of which have theoretical solutions, is chosen to be tested in turbulent flow for the sake of verification. The identified results of thin plate model are consistent with the theoretical solutions. Thirdly, the center-slotted girder model, which is widely-utilized long-span bridge sections in current engineering practice, is employed to investigate the applicability of the proposed approach on a general bridge section. For the center-slotted girder model, the flutter derivatives are also extracted by least square method in uniform flow to cross validate the newly-proposed approach. The identified results by two different approaches are compatible.

</details>

<details>

<summary>2021-08-04 02:20:40 - Gaussian Process Regression and Classification using International Classification of Disease Codes as Covariates</summary>

- *Sanvesh Srivastava, Zongyi Xu, Yunyi Li, W. Nick Street, Stephanie Gilbertson-White*

- `2108.01813v1` - [abs](http://arxiv.org/abs/2108.01813v1) - [pdf](http://arxiv.org/pdf/2108.01813v1)

> International Classification of Disease (ICD) codes are widely used for encoding diagnoses in electronic health records (EHR). Automated methods have been developed over the years for predicting biomedical responses using EHR that borrow information among diagnostically similar patients. Relatively less attention has been paid to developing patient similarity measures that model the structure of ICD codes and the presence of multiple chronic conditions, where a chronic condition is defined as a set of ICD codes. Motivated by this problem, we first develop a type of string kernel function for measuring similarity between a pair of subsets of ICD codes, which uses the definition of chronic conditions. Second, we extend this similarity measure to define a family of covariance functions on subsets of ICD codes. Using this family, we develop Gaussian process (GP) priors for Bayesian nonparametric regression and classification using diagnoses and other demographic information as covariates. Markov chain Monte Carlo (MCMC) algorithms are used for posterior inference and predictions. The proposed methods are free of any tuning parameters and are well-suited for automated prediction of continuous and categorical biomedical responses that depend on chronic conditions. We evaluate the practical performance of our method on EHR data collected from 1660 patients at the University of Iowa Hospitals and Clinics (UIHC) with six different primary cancer sites. Our method has better sensitivity and specificity than its competitors in classifying different primary cancer sites and estimates the marginal associations between chronic conditions and primary cancer sites.

</details>

<details>

<summary>2021-08-04 12:20:47 - Staged trees and asymmetry-labeled DAGs</summary>

- *Gherardo Varando, Federico Carli, Manuele Leonelli*

- `2108.01994v1` - [abs](http://arxiv.org/abs/2108.01994v1) - [pdf](http://arxiv.org/pdf/2108.01994v1)

> Bayesian networks are a widely-used class of probabilistic graphical models capable of representing symmetric conditional independence between variables of interest using the topology of the underlying graph. They can be seen as a special case of the much more general class of models called staged trees, which can represent any type of non-symmetric conditional independence. Here we formalize the relationship between these two models and introduce a minimal Bayesian network representation of the staged tree, which can be used to read conditional independences in an intuitive way. Furthermore, we define a new labeled graph, termed asymmetry-labeled directed acyclic graph, whose edges are labeled to denote the type of dependence existing between any two random variables. Various datasets are used to illustrate the methodology, highlighting the need to construct models which more flexibly encode and represent non-symmetric structures.

</details>

<details>

<summary>2021-08-04 21:21:17 - High dimensional Bayesian Optimization Algorithm for Complex System in Time Series</summary>

- *Yuyang Chen, Kaiming Bi, Chih-Hang J. Wu, David Ben-Arieh, Ashesh Sinha*

- `2108.02289v1` - [abs](http://arxiv.org/abs/2108.02289v1) - [pdf](http://arxiv.org/pdf/2108.02289v1)

> At present, high-dimensional global optimization problems with time-series models have received much attention from engineering fields. Since it was proposed, Bayesian optimization has quickly become a popular and promising approach for solving global optimization problems. However, the standard Bayesian optimization algorithm is insufficient to solving the global optimal solution when the model is high-dimensional. Hence, this paper presents a novel high dimensional Bayesian optimization algorithm by considering dimension reduction and different dimension fill-in strategies. Most existing literature about Bayesian optimization algorithms did not discuss the sampling strategies to optimize the acquisition function. This study proposed a new sampling method based on both the multi-armed bandit and random search methods while optimizing the acquisition function. Besides, based on the time-dependent or dimension-dependent characteristics of the model, the proposed algorithm can reduce the dimension evenly. Then, five different dimension fill-in strategies were discussed and compared in this study. Finally, to increase the final accuracy of the optimal solution, the proposed algorithm adds a local search based on a series of Adam-based steps at the final stage. Our computational experiments demonstrated that the proposed Bayesian optimization algorithm could achieve reasonable solutions with excellent performances for high dimensional global optimization problems with a time-series optimal control model.

</details>

<details>

<summary>2021-08-05 10:33:05 - Shape Modeling with Spline Partitions</summary>

- *Shufei Ge, Shijia Wang, Lloyd Elliott*

- `2108.02507v1` - [abs](http://arxiv.org/abs/2108.02507v1) - [pdf](http://arxiv.org/pdf/2108.02507v1)

> Shape modelling (with methods that output shapes) is a new and important task in Bayesian nonparametrics and bioinformatics. In this work, we focus on Bayesian nonparametric methods for capturing shapes by partitioning a space using curves. In related work, the classical Mondrian process is used to partition spaces recursively with axis-aligned cuts, and is widely applied in multi-dimensional and relational data. The Mondrian process outputs hyper-rectangles. Recently, the random tessellation process was introduced as a generalization of the Mondrian process, partitioning a domain with non-axis aligned cuts in an arbitrary dimensional space, and outputting polytopes. Motivated by these processes, in this work, we propose a novel parallelized Bayesian nonparametric approach to partition a domain with curves, enabling complex data-shapes to be acquired. We apply our method to HIV-1-infected human macrophage image dataset, and also simulated datasets sets to illustrate our approach. We compare to support vector machines, random forests and state-of-the-art computer vision methods such as simple linear iterative clustering super pixel image segmentation. We develop an R package that is available at \url{https://github.com/ShufeiGe/Shape-Modeling-with-Spline-Partitions}.

</details>

<details>

<summary>2021-08-05 13:03:20 - A variational Bayesian spatial interaction model for estimating revenue and demand at business facilities</summary>

- *Shanaka Perera, Virginia Aglietti, Theodoros Damoulas*

- `2108.02594v1` - [abs](http://arxiv.org/abs/2108.02594v1) - [pdf](http://arxiv.org/pdf/2108.02594v1)

> We study the problem of estimating potential revenue or demand at business facilities and understanding its generating mechanism. This problem arises in different fields such as operation research or urban science, and more generally, it is crucial for businesses' planning and decision making. We develop a Bayesian spatial interaction model, henceforth BSIM, which provides probabilistic predictions about revenues generated by a particular business location provided their features and the potential customers' characteristics in a given region. BSIM explicitly accounts for the competition among the competitive facilities through a probability value determined by evaluating a store-specific Gaussian distribution at a given customer location. We propose a scalable variational inference framework that, while being significantly faster than competing Markov Chain Monte Carlo inference schemes, exhibits comparable performances in terms of parameters identification and uncertainty quantification. We demonstrate the benefits of BSIM in various synthetic settings characterised by an increasing number of stores and customers. Finally, we construct a real-world, large spatial dataset for pub activities in London, UK, which includes over 1,500 pubs and 150,000 customer regions. We demonstrate how BSIM outperforms competing approaches on this large dataset in terms of prediction performances while providing results that are both interpretable and consistent with related indicators observed for the London region.

</details>

<details>

<summary>2021-08-05 15:04:50 - Covariance Estimation and its Application in Large-Scale Online Controlled Experiments</summary>

- *Tao Xiong, Yihan Bao, Penglei Zhao, Yong Wang*

- `2108.02668v1` - [abs](http://arxiv.org/abs/2108.02668v1) - [pdf](http://arxiv.org/pdf/2108.02668v1)

> During the last few decades, online controlled experiments (also known as A/B tests) have been adopted as a golden standard for measuring business improvements in industry. In our company, there are more than a billion users participating in thousands of experiments simultaneously, and with statistical inference and estimations conducted to thousands of online metrics in those experiments routinely, computational costs would become a large concern. In this paper we propose a novel algorithm for estimating the covariance of online metrics, which introduces more flexibility to the trade-off between computational costs and precision in covariance estimation. This covariance estimation method reduces computational cost of metric calculation in large-scale setting, which facilitates further application in both online controlled experiments and adaptive experiments scenarios like variance reduction, continuous monitoring, Bayesian optimization, etc., and it can be easily implemented in engineering practice.

</details>

<details>

<summary>2021-08-05 16:14:32 - Quasi-likelihood analysis for marked point processes and application to marked Hawkes processes</summary>

- *Simon Clinet*

- `2001.11624v2` - [abs](http://arxiv.org/abs/2001.11624v2) - [pdf](http://arxiv.org/pdf/2001.11624v2)

> We develop a quasi-likelihood analysis procedure for a general class of multivariate marked point processes. As a by-product of the general method, we establish under stability and ergodicity conditions the local asymptotic normality of the quasi-log likelihood, along with the convergence of moments of quasi-likelihood and quasi-Bayesian estimators. To illustrate the general approach, we then turn our attention to a class of multivariate marked Hawkes processes with generalized exponential kernels, comprising among others the so-called Erlang kernels. We provide explicit conditions on the kernel functions and the mark dynamics under which a certain transformation of the original process is Markovian and $V$-geometrically ergodic. We finally prove that the latter result, which is of interest in its own right, constitutes the key ingredient to show that the generalized exponential Hawkes process falls under the scope of application of the quasi-likelihood analysis.

</details>

<details>

<summary>2021-08-05 20:14:07 - Discussion on "Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects" by Hahn, Murray and Carvalho</summary>

- *Liangyuan Hu*

- `2108.02836v1` - [abs](http://arxiv.org/abs/2108.02836v1) - [pdf](http://arxiv.org/pdf/2108.02836v1)

> Hahn et al. (2020) offers an extensive study to explicate and evaluate the performance of the BCF model in different settings and provides a detailed discussion about its utility in causal inference. It is a welcomed addition to the causal machine learning literature. I will emphasize the contribution of the BCF model to the field of causal inference through discussions on two topics: 1) the difference between the PS in the BCF model and the Bayesian PS in a Bayesian updating approach, 2) an alternative exposition of the role of the PS in outcome modeling based methods for the estimation of causal effects. I will conclude with comments on avenues for future research involving BCF that will be important and much needed in the era of Big data.

</details>

<details>

<summary>2021-08-06 13:45:56 - A rigorous introduction for linear models</summary>

- *Jun Lu*

- `2105.04240v3` - [abs](http://arxiv.org/abs/2105.04240v3) - [pdf](http://arxiv.org/pdf/2105.04240v3)

> This survey is meant to provide an introduction to linear models and the theories behind them. Our goal is to give a rigorous introduction to the readers with prior exposure to ordinary least squares. In machine learning, the output is usually a nonlinear function of the input. Deep learning even aims to find a nonlinear dependence with many layers which require a large amount of computation. However, most of these algorithms build upon simple linear models. We then describe linear models from different views and find the properties and theories behind the models. The linear model is the main technique in regression problems and the primary tool for it is the least squares approximation which minimizes a sum of squared errors. This is a natural choice when we're interested in finding the regression function which minimizes the corresponding expected squared error. This survey is primarily a summary of purpose, significance of important theories behind linear models, e.g., distribution theory, minimum variance estimator. We first describe ordinary least squares from three different points of view upon which we disturb the model with random noise and Gaussian noise. By Gaussian noise, the model gives rise to the likelihood so that we introduce a maximum likelihood estimator. It also develops some distribution theories via this Gaussian disturbance. The distribution theory of least squares will help us answer various questions and introduce related applications. We then prove least squares is the best unbiased linear model in the sense of mean squared error and most importantly, it actually approaches the theoretical limit. We end up with linear models with the Bayesian approach and beyond.

</details>

<details>

<summary>2021-08-07 05:11:24 - A Nonparametric Bayesian Framework for Uncertainty Quantification in Stochastic Simulation</summary>

- *Wei Xie, Cheng Li, Yuefeng Wu, Pu Zhang*

- `1910.03766v2` - [abs](http://arxiv.org/abs/1910.03766v2) - [pdf](http://arxiv.org/pdf/1910.03766v2)

> When we use simulation to assess the performance of stochastic systems, the input models used to drive simulation experiments are often estimated from finite real-world data. There exist both input model and simulation estimation uncertainties in the system performance estimates. Without strong prior information on the input models and the system mean response surface, in this paper, we propose a Bayesian nonparametric framework to quantify the impact from both sources of uncertainty. Specifically, since the real-world data often represent the variability caused by various latent sources of uncertainty, Dirichlet Processes Mixtures (DPM) based nonparametric input models are introduced to model a mixture of heterogeneous distributions, which can faithfully capture the important features of real-world data, such as multi-modality and skewness. Bayesian posteriors of flexible input models characterize the input model estimation uncertainty, which automatically accounts for both model selection and parameter value uncertainty. Then, input model estimation uncertainty is propagated to outputs by using direct simulation. Thus, under very general conditions, our framework delivers an empirical credible interval accounting for both input and simulation uncertainties. A variance decomposition is further developed to quantify the relative contributions from both sources of uncertainty. Our approach is supported by rigorous theoretical and empirical study.

</details>

<details>

<summary>2021-08-07 15:07:32 - Bayesian $L_{\frac{1}{2}}$ regression</summary>

- *Xiongwen Ke, Yanan Fan*

- `2108.03464v1` - [abs](http://arxiv.org/abs/2108.03464v1) - [pdf](http://arxiv.org/pdf/2108.03464v1)

> It is well known that bridge regression enjoys superior theoretical properties than traditional LASSO. However, the current latent variable representation of its Bayesian counterpart, based on the exponential power prior, is computationally expensive in higher dimensions. In this paper, we show that the exponential power prior has a closed-form scale mixture of normal decomposition for $\alpha=(\frac{1}{2})^\gamma, \gamma \in \mathbb{N}^+$. We develop a partially collapsed Gibbs sampling scheme, which outperforms existing Markov chain Monte Carlo strategies, we also study theoretical properties under this prior when $p>n$. In addition, we introduce a non-separable bridge penalty function inspired by the fully Bayesian formulation and a novel, efficient, coordinate-descent algorithm. We prove the algorithm's convergence and show that the local minimizer from our optimization algorithm has an oracle property. Finally, simulation studies were carried out to illustrate the performance of the new algorithms.

</details>

<details>

<summary>2021-08-08 17:37:53 - Model choice and parameter inference in controlled branching processes</summary>

- *Miguel González, Carmen Minuesa, Inés del Puerto*

- `2108.03691v1` - [abs](http://arxiv.org/abs/2108.03691v1) - [pdf](http://arxiv.org/pdf/2108.03691v1)

> Our purpose is to estimate the posterior distribution of the parameters of interest for controlled branching processes (CBPs) without prior knowledge of the maximum number of offspring that an individual can give birth to and without explicit likelihood calculations. We consider that only the population sizes at each generation and at least the number of progenitors of the last generation are observed, but the number of offspring produced by any individual at any generation is unknown. The proposed approach is two-fold. Firstly, to estimate the maximum progeny per individual we make use of an approximate Bayesian computation (ABC) algorithm for model choice and based on sequential importance sampling with the raw data. Secondly, given such an estimate and taking advantage of the simulated values of the previous stage, we approximate the posterior distribution of the main parameters of a CBP by applying the rejection ABC algorithm with an appropriate summary statistic and a post-processing adjustment. The accuracy of the proposed method is illustrated by means of simulated examples developed with the statistical software R. Moreover, we apply the methodology to two real datasets describing populations with logistic growth. To this end, different population growth models based on CBPs are proposed for the first time.

</details>

<details>

<summary>2021-08-08 18:41:12 - A survey of statistical learning techniques as applied to inexpensive pediatric Obstructive Sleep Apnea data</summary>

- *Emily T. Winn, Marilyn Vazquez, Prachi Loliencar, Kaisa Taipale, Xu Wang, Giseon Heo*

- `2002.07873v3` - [abs](http://arxiv.org/abs/2002.07873v3) - [pdf](http://arxiv.org/pdf/2002.07873v3)

> Pediatric obstructive sleep apnea affects an estimated 1-5% of elementary-school aged children and can lead to other detrimental health problems. Swift diagnosis and treatment are critical to a child's growth and development, but the variability of symptoms and the complexity of the available data make this a challenge. We take a first step in streamlining the process by focusing on inexpensive data from questionnaires and craniofacial measurements. We apply correlation networks, the Mapper algorithm from topological data analysis, and singular value decomposition in a process of exploratory data analysis. We then apply a variety of supervised and unsupervised learning techniques from statistics, machine learning, and topology, ranging from support vector machines to Bayesian classifiers and manifold learning. Finally, we analyze the results of each of these methods and discuss the implications for a multi-data-sourced algorithm moving forward.

</details>

<details>

<summary>2021-08-08 19:23:29 - A Theoretical Analysis of Logistic Regression and Bayesian Classifiers</summary>

- *Roman V. Kirin*

- `2108.03715v1` - [abs](http://arxiv.org/abs/2108.03715v1) - [pdf](http://arxiv.org/pdf/2108.03715v1)

> This study aims to show the fundamental difference between logistic regression and Bayesian classifiers in the case of exponential and unexponential families of distributions, yielding the following findings. First, the logistic regression is a less general representation of a Bayesian classifier. Second, one should suppose distributions of classes for the correct specification of logistic regression equations. Third, in specific cases, there is no difference between predicted probabilities from correctly specified generative Bayesian classifier and discriminative logistic regression.

</details>

<details>

<summary>2021-08-08 20:57:22 - Discussion: "Bayesian Optimal Design of Experiments for Inferring the Statistical Expectation of Expensive Black-Box Functions" (Pandita, P., Bilionis, I., and Panchal, J., 2019. ASME. J. Mech. Des. 141(10): 101404)</summary>

- *Xianliang Gong, Yulin Pan*

- `2108.03732v1` - [abs](http://arxiv.org/abs/2108.03732v1) - [pdf](http://arxiv.org/pdf/2108.03732v1)

> The authors of the discussed paper simplified the information-based acquisition on estimating statistical expectation and developed analytical computation for each involved quantity under uniform input distribution. In this discussion, we show that (1) the last three terms of the acquisition always add up to zero, leaving a concise form with a much more intuitive interpretation of the acquisition; (2) the analytical computation of the acquisition can be generalized to arbitrary input distribution, greatly broadening the application of the developed framework.

</details>

<details>

<summary>2021-08-08 23:43:12 - Variational Bayes method for ordinary differential equation models</summary>

- *Hyunjoo Yang, Jaeyong Lee*

- `2011.09718v4` - [abs](http://arxiv.org/abs/2011.09718v4) - [pdf](http://arxiv.org/pdf/2011.09718v4)

> Ordinary differential equations (ODEs) are a mathematical model used in many application areas such as climatology, bioinformatics, and chemical engineering with its intuitive appeal to modeling. Despite ODE's wide usage in modeling, the frequent absence of their analytic solutions makes it challenging to estimate ODE parameters from the data, especially when the model has lots of variables and parameters. This paper proposes a Bayesian ODE parameter estimating algorithm which is fast and accurate even for models with many parameters. The proposed method approximates an ODE model with a state-space model based on equations of a numeric solver. It allows fast estimation by avoiding computations of a complete numerical solution in the likelihood. The posterior is obtained by a variational Bayes method, more specifically, the approximate Riemannian conjugate gradient method (Honkela et al. 2010), which avoids samplings based on Markov chain Monte Carlo (MCMC). In simulation studies, we compared the speed and performance of the proposed method with existing methods. The proposed method showed the best performance in the reproduction of the true ODE curve with strong stability as well as the fastest computation, especially in a large model with more than 30 parameters. As a real-world data application, a SIR model with time-varying parameters was fitted to the COVID-19 data. Taking advantage of the proposed algorithm, more than 50 parameters were adequately estimated for each country.

</details>

<details>

<summary>2021-08-09 01:40:42 - Generalization of the power-law rating curve using hydrodynamic theory and Bayesian hierarchical modeling</summary>

- *Birgir Hrafnkelsson, Helgi Sigurdarson, Sölvi Rögnvaldsson, Axel Ö. Jansson, Rafael D. Vias, Sigurdur M. Gardarsson*

- `2010.04769v2` - [abs](http://arxiv.org/abs/2010.04769v2) - [pdf](http://arxiv.org/pdf/2010.04769v2)

> The power-law rating curve has been used extensively in hydraulic practice and hydrology. It is given by $Q(h)=a(h-c)^b$, where $Q$ is discharge, $h$ is water elevation, $a$, $b$ and $c$ are unknown parameters. We propose a novel extension of the power-law rating curve, referred to as the generalized power-law rating curve. It is constructed by linking the physics of open channel flow to a model of the form $Q(h)=a(h-c)^{f(h)}$. The function $f(h)$ is referred to as the power-law exponent and it depends on the water elevation. The proposed model and the power-law model are fitted within the framework of Bayesian hierarchical models. By exploring the properties of the proposed rating curve and its power-law exponent, we find that cross sectional shapes that are likely to be found in nature are such that the power-law exponent $f(h)$ will usually be in the interval $[1.0,2.67]$. This fact is utilized for the construction of prior densities for the model parameters. An efficient Markov chain Monte Carlo sampling scheme, that utilizes the lognormal distributional assumption at the data level and Gaussian assumption at the latent level, is proposed for the two models. The two statistical models were applied to four datasets. In the case of three datasets the generalized power-law rating curve gave a better fit than the power-law rating curve while in the fourth case the two models fitted equally well and the generalized power-law rating curve mimicked the power-law rating curve.

</details>

<details>

<summary>2021-08-09 01:58:15 - A Test for Independence Via Bayesian Nonparametric Estimation of Mutual Information</summary>

- *Luai Al-Labadi, Forough Fazeli Asl, Zahra Saberi*

- `2002.03490v2` - [abs](http://arxiv.org/abs/2002.03490v2) - [pdf](http://arxiv.org/pdf/2002.03490v2)

> Mutual information is a well-known tool to measure the mutual dependence between variables. In this paper, a Bayesian nonparametric estimation of mutual information is established by means of the Dirichlet process and the $k$-nearest neighbor distance. As a direct outcome of the estimation, an easy-to-implement test of independence is introduced through the relative belief ratio. Several theoretical properties of the approach are presented. The procedure is investigated through various examples where the results are compared to its frequentist counterpart and demonstrate a good performance.

</details>

<details>

<summary>2021-08-09 02:06:12 - A Bayesian Nonparametric Estimation of Mutual Information</summary>

- *Luai Al-Labadi, Forough Fazeli-Asl, Zahra Saberi*

- `2108.03780v1` - [abs](http://arxiv.org/abs/2108.03780v1) - [pdf](http://arxiv.org/pdf/2108.03780v1)

> Mutual information is a widely-used information theoretic measure to quantify the amount of association between variables. It is used extensively in many applications such as image registration, diagnosis of failures in electrical machines, pattern recognition, data mining and tests of independence. The main goal of this paper is to provide an efficient estimator of the mutual information based on the approach of Al Labadi et. al. (2021). The estimator is explored through various examples and is compared to its frequentist counterpart due to Berrett et al. (2019). The results show the good performance of the procedure by having a smaller mean squared error.

</details>

<details>

<summary>2021-08-09 09:38:03 - Bayesian decision-theoretic design of experiments under an alternative model</summary>

- *Antony M. Overstall, James M. McGree*

- `1909.12570v4` - [abs](http://arxiv.org/abs/1909.12570v4) - [pdf](http://arxiv.org/pdf/1909.12570v4)

> Traditionally Bayesian decision-theoretic design of experiments proceeds by choosing a design to minimise expectation of a given loss function over the space of all designs. The loss function encapsulates the aim of the experiment, and the expectation is taken with respect to the joint distribution of all unknown quantities implied by the statistical model that will be fitted to observed responses. In this paper, an extended framework is proposed whereby the expectation of the loss is taken with respect to a joint distribution implied by an alternative statistical model. Motivation for this includes promoting robustness, ensuring computational feasibility and for allowing realistic prior specification when deriving a design. To aid in exploring the new framework, an asymptotic approximation to the expected loss under an alternative model is derived, and the properties of different loss functions are established. The framework is then demonstrated on a linear regression versus full-treatment model scenario, on estimating parameters of a non-linear model under model discrepancy and a cubic spline model under an unknown number of basis functions.

</details>

<details>

<summary>2021-08-09 13:19:53 - Identification in Bayesian Estimation of the Skewness Matrix in a Multivariate Skew-Elliptical Distribution</summary>

- *Sakae Oya, Teruo Nakatsuma*

- `2108.04019v1` - [abs](http://arxiv.org/abs/2108.04019v1) - [pdf](http://arxiv.org/pdf/2108.04019v1)

> Harvey et al. (2010) extended the Bayesian estimation method by Sahu et al. (2003) to a multivariate skew-elliptical distribution with a general skewness matrix, and applied it to Bayesian portfolio optimization with higher moments. Although their method is epochal in the sense that it can handle the skewness dependency among asset returns and incorporate higher moments into portfolio optimization, it cannot identify all elements in the skewness matrix due to label switching in the Gibbs sampler. To deal with this identification issue, we propose to modify their sampling algorithm by imposing a positive lower-triangular constraint on the skewness matrix of the multivariate skew- elliptical distribution and improved interpretability. Furthermore, we propose a Bayesian sparse estimation of the skewness matrix with the horseshoe prior to further improve the accuracy. In the simulation study, we demonstrate that the proposed method with the identification constraint can successfully estimate the true structure of the skewness dependency while the existing method suffers from the identification issue.

</details>

<details>

<summary>2021-08-09 14:54:11 - Bayesian Analysis of Sparse Multivariate Matched Proportions</summary>

- *Mark J. Meyer, Tong Li, Katherine Hobbs Knutson*

- `2108.04096v1` - [abs](http://arxiv.org/abs/2108.04096v1) - [pdf](http://arxiv.org/pdf/2108.04096v1)

> Multivariate matched proportions (MMP) data appears in a variety of contexts---including post-market surveillance of adverse events in pharmaceuticals, disease classification, and agreement between care providers---and consists of multiple sets of paired binary measurements taken on the same subject. While recent work proposes non-Bayesian methods to address the complexities of MMP data, the issue of sparsity, where no or very few responses are recorded for one or more sets, is unaddressed. However, the presence of sparse sets results in underestimates of variance, loss of coverage, and bias in existing methods. Additionally, Bayesian methods have not previously been considered for MMP data. In this work, we propose a Bayesian marginal probability model for MMP data with robust t-priors that adjusts for sparsity using targeted informative priors on the variance components of sparse sets and half-Cauchy priors on non-sparse sets. In simulation, we demonstrate our method's ability to handle sparsity and show that it outperforms existing methods in terms of coverage and bias, while maintaining comparable power. Finally, we analyze data from a study of care coordination within a System of Care framework and provide additional insights that the original univariate analysis missed.

</details>

<details>

<summary>2021-08-10 08:04:11 - Advanced Dropout: A Model-free Methodology for Bayesian Dropout Optimization</summary>

- *Jiyang Xie, Zhanyu Ma, and Jianjun Lei, Guoqiang Zhang, Jing-Hao Xue, Zheng-Hua Tan, Jun Guo*

- `2010.05244v2` - [abs](http://arxiv.org/abs/2010.05244v2) - [pdf](http://arxiv.org/pdf/2010.05244v2)

> Due to lack of data, overfitting ubiquitously exists in real-world applications of deep neural networks (DNNs). We propose advanced dropout, a model-free methodology, to mitigate overfitting and improve the performance of DNNs. The advanced dropout technique applies a model-free and easily implemented distribution with parametric prior, and adaptively adjusts dropout rate. Specifically, the distribution parameters are optimized by stochastic gradient variational Bayes in order to carry out an end-to-end training. We evaluate the effectiveness of the advanced dropout against nine dropout techniques on seven computer vision datasets (five small-scale datasets and two large-scale datasets) with various base models. The advanced dropout outperforms all the referred techniques on all the datasets.We further compare the effectiveness ratios and find that advanced dropout achieves the highest one on most cases. Next, we conduct a set of analysis of dropout rate characteristics, including convergence of the adaptive dropout rate, the learned distributions of dropout masks, and a comparison with dropout rate generation without an explicit distribution. In addition, the ability of overfitting prevention is evaluated and confirmed. Finally, we extend the application of the advanced dropout to uncertainty inference, network pruning, text classification, and regression. The proposed advanced dropout is also superior to the corresponding referred methods. Codes are available at https://github.com/PRIS-CV/AdvancedDropout.

</details>

<details>

<summary>2021-08-10 13:20:59 - Variational Laplace for Bayesian neural networks</summary>

- *Ali Unlu, Laurence Aitchison*

- `2011.10443v2` - [abs](http://arxiv.org/abs/2011.10443v2) - [pdf](http://arxiv.org/pdf/2011.10443v2)

> We develop variational Laplace for Bayesian neural networks (BNNs) which exploits a local approximation of the curvature of the likelihood to estimate the ELBO without the need for stochastic sampling of the neural-network weights. The Variational Laplace objective is simple to evaluate, as it is (in essence) the log-likelihood, plus weight-decay, plus a squared-gradient regularizer. Variational Laplace gave better test performance and expected calibration errors than maximum a-posteriori inference and standard sampling-based variational inference, despite using the same variational approximate posterior. Finally, we emphasise care needed in benchmarking standard VI as there is a risk of stopping before the variance parameters have converged. We show that early-stopping can be avoided by increasing the learning rate for the variance parameters.

</details>

<details>

<summary>2021-08-10 13:50:35 - An Extreme Value Bayesian Lasso for the Conditional Left and Right Tails</summary>

- *Miguel de Carvalho, Soraia Pereira, Paula Pereira, Patrícia de Zea Bermudez*

- `2010.07164v2` - [abs](http://arxiv.org/abs/2010.07164v2) - [pdf](http://arxiv.org/pdf/2010.07164v2)

> We introduce a novel regression model for the conditional left and right tail of a possibly heavy-tailed response. The proposed model can be used to learn the effect of covariates on an extreme value setting via a Lasso-type specification based on a Lagrangian restriction. Our model can be used to track if some covariates are significant for the lower values, but not for the (right) tail---and vice-versa; in addition to this, the proposed model bypasses the need for conditional threshold selection in an extreme value theory framework. We assess the finite-sample performance of the proposed methods through a simulation study that reveals that our method recovers the true conditional distribution over a variety of simulation scenarios, along with being accurate on variable selection. Rainfall data are used to showcase how the proposed method can learn to distinguish between key drivers of moderate rainfall, against those of extreme rainfall.

</details>

<details>

<summary>2021-08-10 14:00:26 - MCMC for a hyperbolic Bayesian inverse problem in traffic flow modelling</summary>

- *Jeremie Coullon, Yvo Pokern*

- `2001.02013v2` - [abs](http://arxiv.org/abs/2001.02013v2) - [pdf](http://arxiv.org/pdf/2001.02013v2)

> As a Bayesian approach to fitting motorway traffic flow models remains rare in the literature, we explore empirically the sampling challenges this approach offers which have to do with the strong correlations and multi-modality of the posterior distribution. In particular, we provide a unified statistical model to estimate using motorway data both boundary conditions and fundamental diagram parameters in LWR, a well known motorway traffic flow model. This allows us to provide a traffic flow density estimation method that is shown to be superior to two methods found in the traffic flow literature. To sample from this challenging posterior distribution we use a state-of-the-art gradient-free function space sampler augmented with parallel tempering.

</details>

<details>

<summary>2021-08-10 16:17:56 - Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation</summary>

- *Abhinav Sagar*

- `2008.07588v2` - [abs](http://arxiv.org/abs/2008.07588v2) - [pdf](http://arxiv.org/pdf/2008.07588v2)

> Deep learning motivated by convolutional neural networks has been highly successful in a range of medical imaging problems like image classification, image segmentation, image synthesis etc. However for validation and interpretability, not only do we need the predictions made by the model but also how confident it is while making those predictions. This is important in safety critical applications for the people to accept it. In this work, we used an encoder decoder architecture based on variational inference techniques for segmenting brain tumour images. We evaluate our work on the publicly available BRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over Union (IOU) as the evaluation metrics. Our model is able to segment brain tumours while taking into account both aleatoric uncertainty and epistemic uncertainty in a principled bayesian manner.

</details>

<details>

<summary>2021-08-10 19:49:51 - Robust posterior inference for Youden's index cutoff</summary>

- *Nicholas Syring*

- `2108.04898v1` - [abs](http://arxiv.org/abs/2108.04898v1) - [pdf](http://arxiv.org/pdf/2108.04898v1)

> Youden's index cutoff is a classifier mapping a patient's diagnostic test outcome and available covariate information to a diagnostic category. Typically the cutoff is estimated indirectly by first modeling the conditional distributions of test outcomes given diagnosis and then choosing the optimal cutoff for the estimated distributions. Here we present a Gibbs posterior distribution for direct inference on the cutoff. Our approach makes incorporating prior information about the cutoff much easier compared to existing methods, and does so without specifying probability models for the data, which may be misspecified. The proposed Gibbs posterior distribution is robust with respect to data distributions, is supported by large-sample theory, and performs well in simulations compared to alternative Bayesian and bootstrap-based methods. In addition, two real data sets are examined which illustrate the flexibility of the Gibbs posterior approach and its ability to utilize direct prior information about the cutoff.

</details>

<details>

<summary>2021-08-11 05:09:50 - Bayesian functional graphical models</summary>

- *Lin Zhang, Veera Baladandayuthapani, Quinton Neville, Karina Quevedo, Jeffrey S. Morris*

- `2108.05034v1` - [abs](http://arxiv.org/abs/2108.05034v1) - [pdf](http://arxiv.org/pdf/2108.05034v1)

> We develop a Bayesian graphical modeling framework for functional data for correlated multivariate random variables observed over a continuous domain. Our method leads to graphical Markov models for functional data which allows the graphs to vary over the functional domain. The model involves estimation of graphical models that evolve functionally in a nonparametric fashion while accounting for within-functional correlations and borrowing strength across functional positions so contiguous locations are encouraged but not forced to have similar graph structure and edge strength. We utilize a strategy that combines nonparametric basis function modeling with modified Bayesian graphical regularization techniques, which induces a new class of hypoexponential normal scale mixture distributions that not only leads to adaptively shrunken estimators of the conditional cross-covariance but also facilitates a thorough theoretical investigation of the shrinkage properties. Our approach scales up to large functional datasets collected on a fine grid. We show through simulations and real data analysis that the Bayesian functional graphical model can efficiently reconstruct the functionally-evolving graphical models by accounting for within-function correlations.

</details>

<details>

<summary>2021-08-11 10:09:45 - Bayesian local exchangeability design for phase II basket trials</summary>

- *Yilin Liu, Michael Kane, Denise Esserman, Daniel Zelterman, Wei Wei*

- `2108.05127v1` - [abs](http://arxiv.org/abs/2108.05127v1) - [pdf](http://arxiv.org/pdf/2108.05127v1)

> We propose an information borrowing strategy for the design and monitoring of phase II basket trials based on the local multisource exchangeability assumption between baskets (disease types). We construct a flexible statistical design using the proposed strategy. Our approach partitions potentially heterogeneous baskets into non-exchangeable blocks. Information borrowing is only allowed to occur locally, i.e., among similar baskets within the same block. The amount of borrowing is determined by between-basket similarities. The number of blocks and block memberships are inferred from data based on the posterior probability of each partition. The proposed method is compared to the multisource exchangeability model and Simon's two-stage design, respectively. In a variety of simulation scenarios, we demonstrate the proposed method is able to maintain the type I error rate and have desirable basket-wise power. In addition, our method is computationally efficient compared to existing Bayesian methods in that the posterior profiles of interest can be derived explicitly without the need for sampling algorithms.

</details>

<details>

<summary>2021-08-11 16:35:11 - Dimension free convergence rates for Gibbs samplers for Bayesian linear mixed models</summary>

- *Zhumengmeng Jin, James P. Hobert*

- `2103.06324v2` - [abs](http://arxiv.org/abs/2103.06324v2) - [pdf](http://arxiv.org/pdf/2103.06324v2)

> The emergence of big data has led to a growing interest in so-called convergence complexity analysis, which is the study of how the convergence rate of a Monte Carlo Markov chain (for an intractable Bayesian posterior distribution) scales as the underlying data set grows in size. Convergence complexity analysis of practical Monte Carlo Markov chains on continuous state spaces is quite challenging, and there have been very few successful analyses of such chains. One fruitful analysis was recently presented by Qin and Hobert (2021b), who studied a Gibbs sampler for a simple Bayesian random effects model. These authors showed that, under regularity conditions, the geometric convergence rate of this Gibbs sampler converges to zero as the data set grows in size. It is shown herein that similar behavior is exhibited by Gibbs samplers for more general Bayesian models that possess both random effects and traditional continuous covariates, the so-called mixed models. The analysis employs the Wasserstein-based techniques introduced by Qin and Hobert (2021b).

</details>

<details>

<summary>2021-08-11 19:12:09 - Kurtosis control in wavelet shrinkage with generalized secant hyperbolic prior</summary>

- *Alex Rodrigo dos Santos Sousa*

- `2108.05418v1` - [abs](http://arxiv.org/abs/2108.05418v1) - [pdf](http://arxiv.org/pdf/2108.05418v1)

> The present paper proposes a bayesian approach for wavelet shrinkage with the use of a shrinkage prior based on the generalized secant hyperbolic distribution symmetric around zero in a nonparemetric regression problem. This shrinkage prior allows the control of the kurtosis of the coefficients, which impacts on the level of shrinkage on its extreme values. Statistical properties such as bias, variance, classical and bayesian risks of the rule are analyzed and performances of the proposed rule are obtained in simulations studies involving the Donoho-Johnstone test functions. Application of the proposed shrinker in denoising Brazilian stock market dataset is also provided.

</details>

<details>

<summary>2021-08-11 23:15:40 - Gamblers Learn from Experience</summary>

- *Joshua E. Blumenstock, Matthew Olckers*

- `2011.00432v2` - [abs](http://arxiv.org/abs/2011.00432v2) - [pdf](http://arxiv.org/pdf/2011.00432v2)

> Mobile phone-based sports betting has exploded in popularity in many African countries. Commentators worry that low-ability gamblers will not learn from experience, and may rely on debt to gamble. Using data on financial transactions for over 50 000 Kenyan smartphone users, we find that gamblers do learn from experience. Gamblers are less likely to bet following poor results and more likely to bet following good results. The reaction to positive and negative feedback is of equal magnitude and is consistent with a model of Bayesian updating. Using an instrumental variables strategy, we find no evidence that increased gambling leads to increased debt.

</details>

<details>

<summary>2021-08-12 00:12:59 - Hierarchical Stochastic Block Model for Community Detection in Multiplex Networks</summary>

- *Arash A. Amini, Marina S. Paez, Lizhen Lin*

- `1904.05330v2` - [abs](http://arxiv.org/abs/1904.05330v2) - [pdf](http://arxiv.org/pdf/1904.05330v2)

> Multiplex networks have become increasingly more prevalent in many fields, and have emerged as a powerful tool for modeling the complexity of real networks. There is a critical need for developing inference models for multiplex networks that can take into account potential dependencies across different layers, particularly when the aim is community detection. We add to a limited literature by proposing a novel and efficient Bayesian model for community detection in multiplex networks. A key feature of our approach is the ability to model varying communities at different network layers. In contrast, many existing models assume the same communities for all layers. Moreover, our model automatically picks up the necessary number of communities at each layer (as validated by real data examples). This is appealing, since deciding the number of communities is a challenging aspect of community detection, and especially so in the multiplex setting, if one allows the communities to change across layers. Borrowing ideas from hierarchical Bayesian modeling, we use a hierarchical Dirichlet prior to model community labels across layers, allowing dependency in their structure. Given the community labels, a stochastic block model (SBM) is assumed for each layer. We develop an efficient slice sampler for sampling the posterior distribution of the community labels as well as the link probabilities between communities. In doing so, we address some unique challenges posed by coupling the complex likelihood of SBM with the hierarchical nature of the prior on the labels. An extensive empirical validation is performed on simulated and real data, demonstrating the superior performance of the model over single-layer alternatives, as well as the ability to uncover interesting structures in real networks.

</details>

<details>

<summary>2021-08-12 13:22:06 - Bayesian inference of a non-local proliferation model</summary>

- *Zuzanna Szymańska, Jakub Skrzeczkowski, Błażej Miasojedow, Piotr Gwiazda*

- `2106.05955v3` - [abs](http://arxiv.org/abs/2106.05955v3) - [pdf](http://arxiv.org/pdf/2106.05955v3)

> From a systems biology perspective the majority of cancer models, although interesting and providing a qualitative explanation of some problems, have a major disadvantage in that they usually miss a genuine connection with experimental data. Having this in mind, in this paper, we aim at contributing to the improvement of many cancer models which contain a proliferation term. To this end, we propose a new non-local model of cell proliferation. We select data which are suitable to perform a Bayesian inference for unknown parameters and we provide a discussion on the range of applicability of the model. Furthermore, we provide proof of the stability of a posteriori distributions in total variation norm which exploits the theory of spaces of measures equipped with the weighted flat norm. In a companion paper, we provide a detailed proof of the well-posedness of the problem and we investigate the convergence of the EBT algorithm applied to solve the equation.

</details>

<details>

<summary>2021-08-12 16:38:46 - City-wide modeling of Vehicle-to-Grid Economics to Understand Effects of Battery Performance</summary>

- *Heta A. Gandhi, Andrew D. White*

- `2108.05837v1` - [abs](http://arxiv.org/abs/2108.05837v1) - [pdf](http://arxiv.org/pdf/2108.05837v1)

> Vehicle-to-grid (V2G) is a promising approach to solve the problem of grid-level intermittent supply and demand mismatch, caused due to renewable energy resources, because it uses the existing resource of electric vehicle (EV) batteries as the energy storage medium. EV battery design together with an impetus on profitability for participating EV owners is pivotal for V2G success. To better understand what battery device parameters are most important for V2G adoption, we model the economics of V2G process under realistic conditions. Most previous studies that perform V2G economic analysis, assume ideal driving conditions, use linear battery degradation models, or only consider V2G for ancillary services. Our model accounts realistic battery degradation, empirical charging efficiencies, for randomness in commute behavior, and historic hourly electricity prices in six cities in the United States. We model user behavior with Bayesian optimization to provide a best-case scenario for V2G. Across all cities, we find that charging rate and efficiency are the most important factors that determine EV users' profits. Surprisingly, EV battery cost and thus degradation due to cycling has little effect. These findings should help focus research on figures of merit that better reflect real usage of batteries in a V2G economy.

</details>

<details>

<summary>2021-08-12 18:39:39 - A modified Susceptible-Infected-Recovered model for observed under-reported incidence data</summary>

- *Imelda Trejo, Nicolas Hengartner*

- `2012.05294v2` - [abs](http://arxiv.org/abs/2012.05294v2) - [pdf](http://arxiv.org/pdf/2012.05294v2)

> Fitting Susceptible-Infected-Recovered (SIR) models to incidence data is problematic when not all infected individuals are reported. Assuming an underlying SIR model with general but known distribution for the time to recovery, this paper derives the implied differential-integral equations for observed incidence data when a fixed fraction of newly infected individuals are not observed. The parameters of the resulting system of differential equations are identifiable. Using these differential equations, we develop a stochastic model for the conditional distribution of current disease incidence given the entire past history of reported cases. We estimate the model parameters using Bayesian Markov Chain Monte-Carlo sampling of the posterior distribution. We use our model to estimate the transmission rate and fraction of asymptomatic individuals for the current Coronavirus 2019 outbreak in eight American Countries: the United States of America, Brazil, Mexico, Argentina, Chile, Colombia, Peru, and Panama, from January 2020 to May 2021. Our analysis reveals that consistently, about 40-60% of the infections were not observed in the American outbreaks. The two exception are Mexico and Peru, with acute under-reporting in Mexico.

</details>

<details>

<summary>2021-08-12 18:52:21 - Efficient reduced-rank methods for Gaussian processes with eigenfunction expansions</summary>

- *Philip Greengard, Michael O'Neil*

- `2108.05924v1` - [abs](http://arxiv.org/abs/2108.05924v1) - [pdf](http://arxiv.org/pdf/2108.05924v1)

> In this work we introduce a reduced-rank algorithm for Gaussian process regression. Our numerical scheme converts a Gaussian process on a user-specified interval to its Karhunen-Lo\`eve expansion, the $L^2$-optimal reduced-rank representation. Numerical evaluation of the Karhunen-Lo\`eve expansion is performed once during precomputation and involves computing a numerical eigendecomposition of an integral operator whose kernel is the covariance function of the Gaussian process. The Karhunen-Lo\`eve expansion is independent of observed data and depends only on the covariance kernel and the size of the interval on which the Gaussian process is defined. The scheme of this paper does not require translation invariance of the covariance kernel. We also introduce a class of fast algorithms for Bayesian fitting of hyperparameters, and demonstrate the performance of our algorithms with numerical experiments in one and two dimensions. Extensions to higher dimensions are mathematically straightforward but suffer from the standard curses of high dimensions.

</details>

<details>

<summary>2021-08-12 19:07:50 - Learning Temporal Evolution of Spatial Dependence with Generalized Spatiotemporal Gaussian Process Models</summary>

- *Shiwei Lan*

- `1901.04030v6` - [abs](http://arxiv.org/abs/1901.04030v6) - [pdf](http://arxiv.org/pdf/1901.04030v6)

> A large number of scientific studies and engineering problems involve high-dimensional spatiotemporal data with complicated relationships. In this paper, we focus on a type of space-time interaction named \emph{temporal evolution of spatial dependence (TESD)}, which is a zero time-lag spatiotemporal covariance. For this purpose, we propose a novel Bayesian nonparametric method based on non-stationary spatiotemporal Gaussian process (STGP). The classic STGP has a covariance kernel separable in space and time, failed to characterize TESD. More recent works on non-separable STGP treat location and time together as a joint variable, which is unnecessarily inefficient. We generalize STGP (gSTGP) to introduce the time-dependence to the spatial kernel by varying its eigenvalues over time in the Mercer's representation. The resulting non-stationary non-separable covariance model bares a quasi Kronecker sum structure. Finally, a hierarchical Bayesian model for the joint covariance is proposed to allow for full flexibility in learning TESD. A simulation study and a longitudinal neuroimaging analysis on Alzheimer's patients demonstrate that the proposed methodology is (statistically) effective and (computationally) efficient in characterizing TESD. Theoretic properties of gSTGP including posterior contraction (for covariance) are also studied.

</details>

<details>

<summary>2021-08-12 21:05:47 - Scalable3-BO: Big Data meets HPC - A scalable asynchronous parallel high-dimensional Bayesian optimization framework on supercomputers</summary>

- *Anh Tran*

- `2108.05969v1` - [abs](http://arxiv.org/abs/2108.05969v1) - [pdf](http://arxiv.org/pdf/2108.05969v1)

> Bayesian optimization (BO) is a flexible and powerful framework that is suitable for computationally expensive simulation-based applications and guarantees statistical convergence to the global optimum. While remaining as one of the most popular optimization methods, its capability is hindered by the size of data, the dimensionality of the considered problem, and the nature of sequential optimization. These scalability issues are intertwined with each other and must be tackled simultaneously. In this work, we propose the Scalable$^3$-BO framework, which employs sparse GP as the underlying surrogate model to scope with Big Data and is equipped with a random embedding to efficiently optimize high-dimensional problems with low effective dimensionality. The Scalable$^3$-BO framework is further leveraged with asynchronous parallelization feature, which fully exploits the computational resource on HPC within a computational budget. As a result, the proposed Scalable$^3$-BO framework is scalable in three independent perspectives: with respect to data size, dimensionality, and computational resource on HPC. The goal of this work is to push the frontiers of BO beyond its well-known scalability issues and minimize the wall-clock waiting time for optimizing high-dimensional computationally expensive applications. We demonstrate the capability of Scalable$^3$-BO with 1 million data points, 10,000-dimensional problems, with 20 concurrent workers in an HPC environment.

</details>

<details>

<summary>2021-08-13 08:56:58 - A Bayesian Approach to In-Game Win Probability in Soccer</summary>

- *Pieter Robberechts, Jan Van Haaren, Jesse Davis*

- `1906.05029v2` - [abs](http://arxiv.org/abs/1906.05029v2) - [pdf](http://arxiv.org/pdf/1906.05029v2)

> In-game win probability models, which provide a sports team's likelihood of winning at each point in a game based on historical observations, are becoming increasingly popular. In baseball, basketball and American football, they have become important tools to enhance fan experience, to evaluate in-game decision-making, and to inform coaching decisions. While equally relevant in soccer, the adoption of these models is held back by technical challenges arising from the low-scoring nature of the sport.   In this paper, we introduce an in-game win probability model for soccer that addresses the shortcomings of existing models. First, we demonstrate that in-game win probability models for other sports struggle to provide accurate estimates for soccer, especially towards the end of a game. Second, we introduce a novel Bayesian statistical framework that estimates running win, tie and loss probabilities by leveraging a set of contextual game state features. An empirical evaluation on eight seasons of data for the top-five soccer leagues demonstrates that our framework provides well-calibrated probabilities. Furthermore, two use cases show its ability to enhance fan experience and to evaluate performance in crucial game situations.

</details>

<details>

<summary>2021-08-13 12:42:19 - Efficient Bayesian inference of fully stochastic epidemiological models with applications to COVID-19</summary>

- *Yuting I. Li, Günther Turk, Paul B. Rohrbach, Patrick Pietzonka, Julian Kappler, Rajesh Singh, Jakub Dolezal, Timothy Ekeh, Lukas Kikuchi, Joseph D. Peterson, Austen Bolitho, Hideki Kobayashi, Michael E. Cates, R. Adhikari, Robert L. Jack*

- `2010.11783v2` - [abs](http://arxiv.org/abs/2010.11783v2) - [pdf](http://arxiv.org/pdf/2010.11783v2)

> Epidemiological forecasts are beset by uncertainties about the underlying epidemiological processes, and the surveillance process through which data are acquired. We present a Bayesian inference methodology that quantifies these uncertainties, for epidemics that are modelled by (possibly) non-stationary, continuous-time, Markov population processes. The efficiency of the method derives from a functional central limit theorem approximation of the likelihood, valid for large populations. We demonstrate the methodology by analysing the early stages of the COVID-19 pandemic in the UK, based on age-structured data for the number of deaths. This includes maximum a posteriori estimates, MCMC sampling of the posterior, computation of the model evidence, and the determination of parameter sensitivities via the Fisher information matrix. Our methodology is implemented in PyRoss, an open-source platform for analysis of epidemiological compartment models.

</details>

<details>

<summary>2021-08-13 15:12:53 - Random Subspace Mixture Models for Interpretable Anomaly Detection</summary>

- *Cetin Savkli, Catherine Schwartz*

- `2108.06283v1` - [abs](http://arxiv.org/abs/2108.06283v1) - [pdf](http://arxiv.org/pdf/2108.06283v1)

> We present a new subspace-based method to construct probabilistic models for high-dimensional data and highlight its use in anomaly detection. The approach is based on a statistical estimation of probability density using densities of random subspaces combined with geometric averaging. In selecting random subspaces, equal representation of each attribute is used to ensure correct statistical limits. Gaussian mixture models (GMMs) are used to create the probability densities for each subspace with techniques included to mitigate singularities allowing for the ability to handle both numerical and categorial attributes. The number of components for each GMM is determined automatically through Bayesian information criterion to prevent overfitting. The proposed algorithm attains competitive AUC scores compared with prominent algorithms against benchmark anomaly detection datasets with the added benefits of being simple, scalable, and interpretable.

</details>

<details>

<summary>2021-08-13 18:50:51 - A flexible sensitivity analysis approach for unmeasured confounding with multiple treatments and a binary outcome with application to SEER-Medicare lung cancer data</summary>

- *Liangyuan Hu, Jungang Zou, Chenyang Gu, Jiayi Ji, Michael Lopez, Minal Kale*

- `2012.06093v4` - [abs](http://arxiv.org/abs/2012.06093v4) - [pdf](http://arxiv.org/pdf/2012.06093v4)

> In the absence of a randomized experiment, a key assumption for drawing causal inference about treatment effects is the ignorable treatment assignment. Violations of the ignorability assumption may lead to biased treatment effect estimates. Sensitivity analysis helps gauge how causal conclusions will be altered in response to the potential magnitude of departure from the ignorability assumption. However, sensitivity analysis approaches for unmeasured confounding in the context of multiple treatments and binary outcomes are scarce. We propose a flexible Monte Carlo sensitivity analysis approach for causal inference in such settings. We first derive the general form of the bias introduced by unmeasured confounding, with emphasis on theoretical properties uniquely relevant to multiple treatments. We then propose methods to encode the impact of unmeasured confounding on potential outcomes and adjust the estimates of causal effects in which the presumed unmeasured confounding is removed. Our proposed methods embed nested multiple imputation within the Bayesian framework, which allow for seamless integration of the uncertainty about the values of the sensitivity parameters and the sampling variability, as well as use of the Bayesian Additive Regression Trees for modeling flexibility. Expansive simulations validate our methods and gain insight into sensitivity analysis with multiple treatments. We use the SEER-Medicare data to demonstrate sensitivity analysis using three treatments for early stage non-small cell lung cancer. The methods developed in this work are readily available in the R package SAMTx.

</details>

<details>

<summary>2021-08-13 19:53:36 - A Generalization of the Ornstein-Uhlenbeck Process: Theoretical Results, Simulations and Parameter Estimation</summary>

- *J. Stein, S. R. C. Lopes, A. V. Medino*

- `2108.06374v1` - [abs](http://arxiv.org/abs/2108.06374v1) - [pdf](http://arxiv.org/pdf/2108.06374v1)

> In this work, we study the class of stochastic process that generalizes the Ornstein-Uhlenbeck processes, hereafter called by \emph{Generalized Ornstein-Uhlenbeck Type Process} and denoted by GOU type process. We consider them driven by the class of noise processes such as Brownian motion, symmetric $\alpha$-stable L\'evy process, a L\'evy process, and even a Poisson process. We give necessary and sufficient conditions under the memory kernel function for the time-stationary and the Markov properties for these processes. When the GOU type process is driven by a L\'evy noise we prove that it is infinitely divisible showing its generating triplet. Several examples derived from the GOU type process are illustrated showing some of their basic properties as well as some time series realizations. These examples also present their theoretical and empirical autocorrelation or normalized codifference functions depending on whether the process has a finite or infinite second moment. We also present the maximum likelihood estimation as well as the Bayesian estimation procedures for the so-called \emph{Cosine process}, a particular process in the class of GOU type processes. For the Bayesian estimation method, we consider the power series representation of Fox's H-function to better approximate the density function of a random variable $\alpha$-stable distributed. We consider four goodness-of-fit tests for helping to decide which \emph{Cosine process} (driven by a Gaussian or an $\alpha$-stable noise) best fit real data sets. Two applications of GOU type model are presented: one based on the Apple company stock market price data and the other based on the cardiovascular mortality in Los Angeles County data.

</details>

<details>

<summary>2021-08-13 21:45:42 - Bayesian Pseudo Posterior Mechanism under Asymptotic Differential Privacy</summary>

- *Terrance D. Savitsky, Matthew R. Williams, Jingchen Hu*

- `1909.11796v8` - [abs](http://arxiv.org/abs/1909.11796v8) - [pdf](http://arxiv.org/pdf/1909.11796v8)

> We propose a Bayesian pseudo posterior mechanism to generate record-level synthetic databases equipped with an $(\epsilon,\delta)-$ probabilistic differential privacy (pDP) guarantee, where $\delta$ denotes the probability that any observed database exceeds $\epsilon$. The pseudo posterior mechanism employs a data record-indexed, risk-based weight vector with weight values $\in [0, 1]$ that surgically downweight the likelihood contributions for high-risk records for model estimation and the generation of record-level synthetic data for public release. The pseudo posterior synthesizer constructs a weight for each data record using the Lipschitz bound for that record under a log-pseudo likelihood utility function that generalizes the exponential mechanism (EM) used to construct a formally private data generating mechanism. By selecting weights to remove likelihood contributions with non-finite log-likelihood values, we guarantee a finite local privacy guarantee for our pseudo posterior mechanism at every sample size. Our results may be applied to \emph{any} synthesizing model envisioned by the data disseminator in a computationally tractable way that only involves estimation of a pseudo posterior distribution for parameters, $\theta$, unlike recent approaches that use naturally-bounded utility functions implemented through the EM. We specify mild conditions that guarantee the asymptotic contraction of $\delta$ to $0$ over the space of databases. We illustrate our pseudo posterior mechanism on the sensitive family income variable from the Consumer Expenditure Surveys database published by the U.S. Bureau of Labor Statistics. We show that utility is better preserved in the synthetic data for our pseudo posterior mechanism as compared to the EM, both estimated using the same non-private synthesizer, due to our use of targeted downweighting.

</details>

<details>

<summary>2021-08-14 02:20:49 - A fast asynchronous MCMC sampler for sparse Bayesian inference</summary>

- *Yves Atchadé, Liwei Wang*

- `2108.06446v1` - [abs](http://arxiv.org/abs/2108.06446v1) - [pdf](http://arxiv.org/pdf/2108.06446v1)

> We propose a very fast approximate Markov Chain Monte Carlo (MCMC) sampling framework that is applicable to a large class of sparse Bayesian inference problems, where the computational cost per iteration in several models is of order $O(ns)$, where $n$ is the sample size, and $s$ the underlying sparsity of the model. This cost can be further reduced by data sub-sampling when stochastic gradient Langevin dynamics are employed. The algorithm is an extension of the asynchronous Gibbs sampler of Johnson et al. (2013), but can be viewed from a statistical perspective as a form of Bayesian iterated sure independent screening (Fan et al. (2009)). We show that in high-dimensional linear regression problems, the Markov chain generated by the proposed algorithm admits an invariant distribution that recovers correctly the main signal with high probability under some statistical assumptions. Furthermore we show that its mixing time is at most linear in the number of regressors. We illustrate the algorithm with several models.

</details>

<details>

<summary>2021-08-14 05:01:46 - Inference on the Sharpe ratio via the upsilon distribution</summary>

- *Steven E. Pav*

- `1505.00829v3` - [abs](http://arxiv.org/abs/1505.00829v3) - [pdf](http://arxiv.org/pdf/1505.00829v3)

> The upsilon distribution, the sum of independent chi random variates and a normal, is introduced. As a special case, the upsilon distribution includes Lecoutre's lambda-prime distribution. The upsilon distribution finds application in Frequentist inference on the Sharpe ratio, including hypothesis tests on independent samples, confidence intervals, and prediction intervals, as well as their Bayesian counterparts. These tests are extended to the case of factor models of returns.

</details>

<details>

<summary>2021-08-14 14:47:57 - Dimensionality Reduction and State Space Systems: Forecasting the US Treasury Yields Using Frequentist and Bayesian VARs</summary>

- *Sudiksha Joshi*

- `2108.06553v1` - [abs](http://arxiv.org/abs/2108.06553v1) - [pdf](http://arxiv.org/pdf/2108.06553v1)

> Using a state-space system, I forecasted the US Treasury yields by employing frequentist and Bayesian methods after first decomposing the yields of varying maturities into its unobserved term structure factors. Then, I exploited the structure of the state-space model to forecast the Treasury yields and compared the forecast performance of each model using mean squared forecast error. Among the frequentist methods, I applied the two-step Diebold-Li, two-step principal components, and one-step Kalman filter approaches. Likewise, I imposed the five different priors in Bayesian VARs: Diffuse, Minnesota, natural conjugate, the independent normal inverse: Wishart, and the stochastic search variable selection priors. After forecasting the Treasury yields for 9 different forecast horizons, I found that the BVAR with Minnesota prior generally minimizes the loss function. I augmented the above BVARs by including macroeconomic variables and constructed impulse response functions with a recursive ordering identification scheme. Finally, I fitted a sign-restricted BVAR with dummy observations.

</details>

<details>

<summary>2021-08-15 08:45:22 - Bayesian Learning for Deep Neural Network Adaptation</summary>

- *Xurong Xie, Xunying Liu, Tan Lee, Lan Wang*

- `2012.07460v3` - [abs](http://arxiv.org/abs/2012.07460v3) - [pdf](http://arxiv.org/pdf/2012.07460v3)

> A key task for speech recognition systems is to reduce the mismatch between training and evaluation data that is often attributable to speaker differences. Speaker adaptation techniques play a vital role to reduce the mismatch. Model-based speaker adaptation approaches often require sufficient amounts of target speaker data to ensure robustness. When the amount of speaker level data is limited, speaker adaptation is prone to overfitting and poor generalization. To address the issue, this paper proposes a full Bayesian learning based DNN speaker adaptation framework to model speaker-dependent (SD) parameter uncertainty given limited speaker specific adaptation data. This framework is investigated in three forms of model based DNN adaptation techniques: Bayesian learning of hidden unit contributions (BLHUC), Bayesian parameterized activation functions (BPAct), and Bayesian hidden unit bias vectors (BHUB). In the three methods, deterministic SD parameters are replaced by latent variable posterior distributions for each speaker, whose parameters are efficiently estimated using a variational inference based approach. Experiments conducted on 300-hour speed perturbed Switchboard corpus trained LF-MMI TDNN/CNN-TDNN systems suggest the proposed Bayesian adaptation approaches consistently outperform the deterministic adaptation on the NIST Hub5'00 and RT03 evaluation sets. When using only the first five utterances from each speaker as adaptation data, significant word error rate reductions up to 1.4% absolute (7.2% relative) were obtained on the CallHome subset. The efficacy of the proposed Bayesian adaptation techniques is further demonstrated in a comparison against the state-of-the-art performance obtained on the same task using the most recent systems reported in the literature.

</details>

<details>

<summary>2021-08-15 17:10:46 - Evolutionarily Stable (Mis)specifications: Theory and Applications</summary>

- *Kevin He, Jonathan Libgober*

- `2012.15007v2` - [abs](http://arxiv.org/abs/2012.15007v2) - [pdf](http://arxiv.org/pdf/2012.15007v2)

> We introduce an evolutionary framework to evaluate competing (mis)specifications in strategic situations, focusing on which misspecifications can persist over correct specifications. Agents with heterogeneous specifications coexist in a society and repeatedly play a stage game against random opponents, drawing Bayesian inferences about the environment based on personal experience. One specification is evolutionarily stable against another if, whenever sufficiently prevalent, its adherents obtain higher average payoffs than their counterparts. Agents' equilibrium beliefs are constrained but not wholly determined by specifications. Endogenous belief formation through the learning channel generates novel stability phenomena compared to frameworks where single beliefs are the heritable units of cultural transmission. In linear-quadratic-normal games where players receive correlated signals but possibly misperceive the information structure, the correct specification is evolutionarily unstable against a correlational error whose direction depends on social interaction structure. We also endogenize coarse thinking in games and show how its prevalence varies with game parameters.

</details>

<details>

<summary>2021-08-15 23:18:39 - Bayesian Parameter Estimations for Grey System Models in Online Traffic Speed Predictions</summary>

- *Gurcan Comert, Negash Begashaw, Negash G. Medhin*

- `2108.06839v1` - [abs](http://arxiv.org/abs/2108.06839v1) - [pdf](http://arxiv.org/pdf/2108.06839v1)

> This paper presents Bayesian parameter estimation for first order Grey system models' parameters (or sometimes referred to as hyperparameters). There are different forms of first-order Grey System Models. These include $GM(1,1)$, $GM(1,1| \cos(\omega t)$, $GM(1,1| \sin(\omega t)$, and $GM(1,1| \cos(\omega t), \sin(\omega t)$. The whitenization equation of these models is a first-order linear differential equation of the form \[ \frac{dx}{dt} + a x = f(t) \] where $a$ is a parameter and $f(t) = b$ in $GM(1,1|)$ , $f(t) = b_1\cos(\omega t) + b_2$ in $GM(1,1| cos(\omega t)$, $f(t) = b_1\sin(\omega t)+b_2$ in $GM(1,1| \sin(\omega t)$, $f(t) = b_1\sin(\omega t) + b_2\cos(\omega t) + b_3$ in $GM(1,1| \cos(\omega t), \sin(\omega t)$, $f(t) = b x^2$ in Grey Verhulst model (GVM),   and where $b, b_1, b_2$, and $b_3$ are parameters. The results from Bayesian estimations are compared to the least square estimated models with fixed $\omega$. We found that using rolling Bayesian estimations for GM parameters can allow us to estimate the parameters in all possible forms. Based on the data used for the comparison, the numerical results showed that models with Bayesian parameter estimations are up to 45\% more accurate in mean squared errors.

</details>

<details>

<summary>2021-08-16 10:42:39 - Efficient Hyperparameter Optimization under Multi-Source Covariate Shift</summary>

- *Masahiro Nomura, Yuta Saito*

- `2006.10600v2` - [abs](http://arxiv.org/abs/2006.10600v2) - [pdf](http://arxiv.org/pdf/2006.10600v2)

> A typical assumption in supervised machine learning is that the train (source) and test (target) datasets follow completely the same distribution. This assumption is, however, often violated in uncertain real-world applications, which motivates the study of learning under covariate shift. In this setting, the naive use of adaptive hyperparameter optimization methods such as Bayesian optimization does not work as desired since it does not address the distributional shift among different datasets. In this work, we consider a novel hyperparameter optimization problem under the multi-source covariate shift whose goal is to find the optimal hyperparameters for a target task of interest using only unlabeled data in a target task and labeled data in multiple source tasks. To conduct efficient hyperparameter optimization for the target task, it is essential to estimate the target objective using only the available information. To this end, we construct the variance reduced estimator that unbiasedly approximates the target objective with a desirable variance property. Building on the proposed estimator, we provide a general and tractable hyperparameter optimization procedure, which works preferably in our setting with a no-regret guarantee. The experiments demonstrate that the proposed framework broadens the applications of automated hyperparameter optimization.

</details>

<details>

<summary>2021-08-16 13:19:26 - Bayesian sequential composite hypothesis testing in discrete time</summary>

- *Erik Ekström, Yuqiong Wang*

- `2108.10866v1` - [abs](http://arxiv.org/abs/2108.10866v1) - [pdf](http://arxiv.org/pdf/2108.10866v1)

> We study the sequential testing problem of two alternative hypotheses regarding an unknown parameter in an exponential family when observations are costly. In a Bayesian setting, the problem can be embedded in a Markovian framework. Using the conditional probability of one of the hypotheses as the underlying spatial variable, we show that the cost function is concave and that the posterior distribution becomes more concentrated as time goes on. Moreover, we study time monotonicity of the value function. For a large class of model specifications, the cost function is non-decreasing in time, and the optimal stopping boundaries are thus monotone.

</details>

<details>

<summary>2021-08-16 13:23:50 - Confidence sets in a sparse stochastic block model with two communities of unknown sizes</summary>

- *B. J. K. Kleijn, J. van Waaij*

- `2108.07078v1` - [abs](http://arxiv.org/abs/2108.07078v1) - [pdf](http://arxiv.org/pdf/2108.07078v1)

> In a sparse stochastic block model with two communities of unequal sizes we derive two posterior concentration inequalities, that imply (1) posterior (almost-)exact recovery of the community structure under sparsity bounds comparable to well-known sharp bounds in the planted bi-section model; (2) a construction of confidence sets for the community assignment from credible sets, with finite graph sizes. The latter enables exact frequentist uncertain quantification with Bayesian credible sets at non-asymptotic graph sizes, where posteriors can be simulated well. There turns out to be no proportionality between credible and confidence levels: for given edge probabilities and a desired confidence level, there exists a critical graph size where the required credible level drops sharply from close to one to close to zero. At such graph sizes the frequentist decides to include not most of the posterior support for the construction of his confidence set, but only a small subset of community assignments containing the highest amounts of posterior probability (like the maximum-a-posteriori estimator). It is argued that for the proposed construction of confidence sets, a form of early stopping applies to MCMC sampling of the posterior, which would enable the computation of confidence sets at larger graph sizes.

</details>

<details>

<summary>2021-08-16 13:40:35 - Accounting for multiple imputation-induced variability for differential analysis in mass spectrometry-based label-free quantitative proteomics</summary>

- *Marie Chion, Christine Carapito, Frédéric Bertrand*

- `2108.07086v1` - [abs](http://arxiv.org/abs/2108.07086v1) - [pdf](http://arxiv.org/pdf/2108.07086v1)

> Imputing missing values is common practice in label-free quantitative proteomics. Imputation aims at replacing a missing value with a user-defined one. However, the imputation itself may not be optimally considered downstream of the imputation process, as imputed datasets are often considered as if they had always been complete. Hence, the uncertainty due to the imputation is not adequately taken into account. We provide a rigorous multiple imputation strategy, leading to a less biased estimation of the parameters' variability thanks to Rubin's rules. The imputation-based peptide's intensities' variance estimator is then moderated using Bayesian hierarchical models. This estimator is finally included in moderated t-test statistics to provide differential analyses results. This workflow can be used both at peptide and protein-level in quantification datasets. For protein-level results based on peptide-level quantification data, an aggregation step is also included. Our methodology, named mi4p, was compared to the state-of-the-art limma workflow implemented in the DAPAR R package, both on simulated and real datasets. We observed a trade-off between sensitivity and specificity, while the overall performance of mi4p outperforms DAPAR in terms of F-Score.

</details>

<details>

<summary>2021-08-16 13:58:01 - How many data clusters are in the Galaxy data set? Bayesian cluster analysis in action</summary>

- *Bettina Grün, Gertraud Malsiner-Walli, Sylvia Frühwirth-Schnatter*

- `2101.12686v2` - [abs](http://arxiv.org/abs/2101.12686v2) - [pdf](http://arxiv.org/pdf/2101.12686v2)

> In model-based clustering, the Galaxy data set is often used as a benchmark data set to study the performance of different modeling approaches. Aitkin (2001) compares maximum likelihood and Bayesian analyses of the Galaxy data set and expresses reservations about the Bayesian approach due to the fact that the prior assumptions imposed remain rather obscure while playing a major role in the results obtained and conclusions drawn.   The aim of the paper is to address Aitkin's concerns about the Bayesian approach by shedding light on how the specified priors influence the number of estimated clusters. We perform a sensitivity analysis of different prior specifications for the mixtures of finite mixture model, i.e., the mixture model where a prior on the number of components is included. We use an extensive set of different prior specifications in a full factorial design and assess their impact on the estimated number of clusters for the Galaxy data set. Results highlight the interaction effects of the prior specifications and provide insights into which prior specifications are recommended to obtain a sparse clustering solution. A simulation study with artificial data provides further empirical evidence to support the recommendations.   A clear understanding of the impact of the prior specifications removes restraints preventing the use of Bayesian methods due to the complexity of selecting suitable priors. Also, the regularizing properties of the priors may be intentionally exploited to obtain a suitable clustering solution meeting prior expectations and needs of the application.

</details>

<details>

<summary>2021-08-16 16:32:13 - Hierarchical Infinite Relational Model</summary>

- *Feras A. Saad, Vikash K. Mansinghka*

- `2108.07208v1` - [abs](http://arxiv.org/abs/2108.07208v1) - [pdf](http://arxiv.org/pdf/2108.07208v1)

> This paper describes the hierarchical infinite relational model (HIRM), a new probabilistic generative model for noisy, sparse, and heterogeneous relational data. Given a set of relations defined over a collection of domains, the model first infers multiple non-overlapping clusters of relations using a top-level Chinese restaurant process. Within each cluster of relations, a Dirichlet process mixture is then used to partition the domain entities and model the probability distribution of relation values. The HIRM generalizes the standard infinite relational model and can be used for a variety of data analysis tasks including dependence detection, clustering, and density estimation. We present new algorithms for fully Bayesian posterior inference via Gibbs sampling. We illustrate the efficacy of the method on a density estimation benchmark of twenty object-attribute datasets with up to 18 million cells and use it to discover relational structure in real-world datasets from politics and genomics.

</details>

<details>

<summary>2021-08-16 16:58:57 - Multimodal Information Gain in Bayesian Design of Experiments</summary>

- *Quan Long*

- `2108.07224v1` - [abs](http://arxiv.org/abs/2108.07224v1) - [pdf](http://arxiv.org/pdf/2108.07224v1)

> One of the well-known challenges in optimal experimental design is how to efficiently estimate the nested integrations of the expected information gain. The Gaussian approximation and associated importance sampling have been shown to be effective at reducing the numerical costs. However, they may fail due to the non-negligible biases and the numerical instabilities. A new approach is developed to compute the expected information gain, when the posterior distribution is multimodal - a situation previously ignored by the methods aiming at accelerating the nested numerical integrations. Specifically, the posterior distribution is approximated using a mixture distribution constructed by multiple runs of global search for the modes and weighted local Laplace approximations. Under any given probability of capturing all the modes, we provide an estimation of the number of runs of searches, which is dimension independent. It is shown that the novel global-local multimodal approach can be significantly more accurate and more efficient than the other existing approaches, especially when the number of modes is large. The methods can be applied to the designs of experiments with both calibrated and uncalibrated observation noises.

</details>

<details>

<summary>2021-08-16 17:01:20 - Revisiting Empirical Bayes Methods and Applications to Special Types of Data</summary>

- *Xiuwen Duan*

- `2108.07227v1` - [abs](http://arxiv.org/abs/2108.07227v1) - [pdf](http://arxiv.org/pdf/2108.07227v1)

> Empirical Bayes methods have been around for a long time and have a wide range of applications. These methods provide a way in which historical data can be aggregated to provide estimates of the posterior mean. This thesis revisits some of the empirical Bayesian methods and develops new applications. We first look at a linear empirical Bayes estimator and apply it on ranking and symbolic data. Next, we consider Tweedie's formula and show how it can be applied to analyze a microarray dataset. The application of the formula is simplified with the Pearson system of distributions. Saddlepoint approximations enable us to generalize several results in this direction. The results show that the proposed methods perform well in applications to real data sets.

</details>

<details>

<summary>2021-08-17 08:13:42 - Bayesian Synthetic Likelihood Estimation for Underreported Non-Stationary Time Series: Covid-19 Incidence in Spain</summary>

- *David Moriña, Amanda Fernández-Fontelo, Alejandra Cabaña, Argimiro Arratia, Pedro Puig*

- `2104.07575v2` - [abs](http://arxiv.org/abs/2104.07575v2) - [pdf](http://arxiv.org/pdf/2104.07575v2)

> The problem of dealing with misreported data is very common in a wide range of contexts for different reasons. The current situation caused by the Covid-19 worldwide pandemic is a clear example, where the data provided by official sources were not always reliable due to data collection issues and to the high proportion of asymptomatic cases. In this work, we explore the performance of Bayesian Synthetic Likelihood to estimate the parameters of a model capable of dealing with misreported information and to reconstruct the most likely evolution of the phenomenon. The performance of the proposed methodology is evaluated through a comprehensive simulation study and illustrated by reconstructing the weekly Covid-19 incidence in each Spanish Autonomous Community in 2020.

</details>

<details>

<summary>2021-08-17 20:11:05 - Randomized maximum likelihood based posterior sampling</summary>

- *Yuming Ba, Jana de Wiljes, Dean S. Oliver, Sebastian Reich*

- `2101.03612v2` - [abs](http://arxiv.org/abs/2101.03612v2) - [pdf](http://arxiv.org/pdf/2101.03612v2)

> Minimization of a stochastic cost function is commonly used for approximate sampling in high-dimensional Bayesian inverse problems with Gaussian prior distributions and multimodal posterior distributions. The density of the samples generated by minimization is not the desired target density, unless the observation operator is linear, but the distribution of samples is useful as a proposal density for importance sampling or for Markov chain Monte Carlo methods. In this paper, we focus on applications to sampling from multimodal posterior distributions in high dimensions. We first show that sampling from multimodal distributions is improved by computing all critical points instead of only minimizers of the objective function. For applications to high-dimensional geoscience problems, we demonstrate an efficient approximate weighting that uses a low-rank Gauss-Newton approximation of the determinant of the Jacobian. The method is applied to two toy problems with known posterior distributions and a Darcy flow problem with multiple modes in the posterior.

</details>

<details>

<summary>2021-08-17 20:23:56 - Measurement Error in Meta-Analysis (MEMA) -- a Bayesian framework for continuous outcome data</summary>

- *Harlan Campbell, Valentijn M. T. de Jong, Lauren Maxwell, Thomas P. A. Debray, Thomas Jaenisch, Paul Gustafson*

- `2011.07186v2` - [abs](http://arxiv.org/abs/2011.07186v2) - [pdf](http://arxiv.org/pdf/2011.07186v2)

> Ideally, a meta-analysis will summarize data from several unbiased studies. Here we consider the less than ideal situation in which contributing studies may be compromised by measurement error. Measurement error affects every study design, from randomized controlled trials to retrospective observational studies. We outline a flexible Bayesian framework for continuous outcome data which allows one to obtain appropriate point and interval estimates with varying degrees of prior knowledge about the magnitude of the measurement error. We also demonstrate how, if individual-participant data (IPD) are available, the Bayesian meta-analysis model can adjust for multiple participant-level covariates, measured with or without measurement error.

</details>

<details>

<summary>2021-08-18 13:19:25 - Estimation of Li-ion degradation test sample sizes required to understand cell-to-cell variability</summary>

- *Philipp Dechent, Samuel Greenbank, Felix Hildenbrand, Saad Jbabdi, Dirk Uwe Sauer, David A. Howey*

- `2107.07881v2` - [abs](http://arxiv.org/abs/2107.07881v2) - [pdf](http://arxiv.org/pdf/2107.07881v2)

> Ageing of lithium-ion batteries results in irreversible reduction in performance. Intrinsic variability between cells, caused by manufacturing differences, occurs throughout life and increases with age. Researchers need to know the minimum number of cells they should test to give an accurate representation of population variability, since testing many cells is expensive. In this paper, empirical capacity versus time ageing models were fitted to various degradation datasets for commercially available cells assuming the model parameters could be drawn from a larger population distribution. Using a hierarchical Bayesian approach, we estimated the number of cells required to be tested. Depending on the complexity, ageing models with 1, 2 or 3 parameters respectively required data from at least 9, 11 or 13 cells for a consistent fit. This implies researchers will need to test at least these numbers of cells at each test point in their experiment to capture manufacturing variability.

</details>

<details>

<summary>2021-08-18 17:11:29 - Geometry-informed irreversible perturbations for accelerated convergence of Langevin dynamics</summary>

- *Benjamin J. Zhang, Youssef M. Marzouk, Konstantinos Spiliopoulos*

- `2108.08247v1` - [abs](http://arxiv.org/abs/2108.08247v1) - [pdf](http://arxiv.org/pdf/2108.08247v1)

> We introduce a novel geometry-informed irreversible perturbation that accelerates convergence of the Langevin algorithm for Bayesian computation. It is well documented that there exist perturbations to the Langevin dynamics that preserve its invariant measure while accelerating its convergence. Irreversible perturbations and reversible perturbations (such as Riemannian manifold Langevin dynamics (RMLD)) have separately been shown to improve the performance of Langevin samplers. We consider these two perturbations simultaneously by presenting a novel form of irreversible perturbation for RMLD that is informed by the underlying geometry. Through numerical examples, we show that this new irreversible perturbation can improve performance of the estimator over reversible perturbations that do not take the geometry into account. Moreover we demonstrate that irreversible perturbations generally can be implemented in conjunction with the stochastic gradient version of the Langevin algorithm. Lastly, while continuous-time irreversible perturbations cannot impair the performance of a Langevin estimator, the situation can sometimes be more complicated when discretization is considered. To this end, we describe a discrete-time example in which irreversibility increases both the bias and variance of the resulting estimator.

</details>

<details>

<summary>2021-08-19 02:11:38 - Bayesian Semiparametric Hidden Markov Tensor Partition Models for Longitudinal Data with Local Variable Selection</summary>

- *Giorgio Paulon, Peter Müller, Abhra Sarkar*

- `2108.08439v1` - [abs](http://arxiv.org/abs/2108.08439v1) - [pdf](http://arxiv.org/pdf/2108.08439v1)

> We present a flexible Bayesian semiparametric mixed model for longitudinal data analysis in the presence of potentially high-dimensional categorical covariates. Building on a novel hidden Markov tensor decomposition technique, our proposed method allows the fixed effects components to vary between dependent random partitions of the covariate space at different time points. The mechanism not only allows different sets of covariates to be included in the model at different time points but also allows the selected predictors' influences to vary flexibly over time. Smooth time-varying additive random effects are used to capture subject specific heterogeneity. We establish posterior convergence guarantees for both function estimation and variable selection. We design a Markov chain Monte Carlo algorithm for posterior computation. We evaluate the method's empirical performances through synthetic experiments and demonstrate its practical utility through real world applications.

</details>

<details>

<summary>2021-08-19 12:27:58 - Bayesian Characterization of Uncertainties Surrounding Fluvial Flood Hazard Estimates</summary>

- *Sanjib Sharma, Ganesh Raj Ghimire, Rocky Talchabhadel, Jeeban Panthi, Benjamin Seiyon Lee, Fengyun Sun, Rupesh Baniya, Tirtha Raj Adhikari*

- `2010.04789v3` - [abs](http://arxiv.org/abs/2010.04789v3) - [pdf](http://arxiv.org/pdf/2010.04789v3)

> Fluvial floods drive severe risk to riverine communities. There is a strong evidence of increasing flood hazards in many regions around the world. The choice of methods and assumptions used in flood hazard estimates can impact the design of risk management strategies. In this study, we characterize the expected flood hazards conditioned on the uncertain model structures, model parameters and prior distributions of the parameters. We construct a Bayesian framework for river stage return level estimation using a nonstationary statistical model that relies exclusively on Indian Ocean Dipole Index. We show that ignoring uncertainties can lead to biased estimation of expected flood hazards. We find that the considered model parametric uncertainty is more influential than model structures and model priors. Our results highlight the importance of incorporating uncertainty in river stage estimates, and are of practical use for informing water infrastructure designs in a changing climate.

</details>

<details>

<summary>2021-08-19 14:22:17 - Teaching Uncertainty Quantification in Machine Learning through Use Cases</summary>

- *Matias Valdenegro-Toro*

- `2108.08712v1` - [abs](http://arxiv.org/abs/2108.08712v1) - [pdf](http://arxiv.org/pdf/2108.08712v1)

> Uncertainty in machine learning is not generally taught as general knowledge in Machine Learning course curricula. In this paper we propose a short curriculum for a course about uncertainty in machine learning, and complement the course with a selection of use cases, aimed to trigger discussion and let students play with the concepts of uncertainty in a programming setting. Our use cases cover the concept of output uncertainty, Bayesian neural networks and weight distributions, sources of uncertainty, and out of distribution detection. We expect that this curriculum and set of use cases motivates the community to adopt these important concepts into courses for safety in AI.

</details>

<details>

<summary>2021-08-19 20:59:07 - Calibration and Uncertainty Quantification of Convective Parameters in an Idealized GCM</summary>

- *Oliver R. A. Dunbar, Alfredo Garbuno-Inigo, Tapio Schneider, Andrew M. Stuart*

- `2012.13262v3` - [abs](http://arxiv.org/abs/2012.13262v3) - [pdf](http://arxiv.org/pdf/2012.13262v3)

> Parameters in climate models are usually calibrated manually, exploiting only small subsets of the available data. This precludes both optimal calibration and quantification of uncertainties. Traditional Bayesian calibration methods that allow uncertainty quantification are too expensive for climate models; they are also not robust in the presence of internal climate variability. For example, Markov chain Monte Carlo (MCMC) methods typically require $O(10^5)$ model runs and are sensitive to internal variability noise, rendering them infeasible for climate models. Here we demonstrate an approach to model calibration and uncertainty quantification that requires only $O(10^2)$ model runs and can accommodate internal climate variability. The approach consists of three stages: (i) a calibration stage uses variants of ensemble Kalman inversion to calibrate a model by minimizing mismatches between model and data statistics; (ii) an emulation stage emulates the parameter-to-data map with Gaussian processes (GP), using the model runs in the calibration stage for training; (iii) a sampling stage approximates the Bayesian posterior distributions by sampling the GP emulator with MCMC. We demonstrate the feasibility and computational efficiency of this calibrate-emulate-sample (CES) approach in a perfect-model setting. Using an idealized general circulation model, we estimate parameters in a simple convection scheme from synthetic data generated with the model. The CES approach generates probability distributions of the parameters that are good approximations of the Bayesian posteriors, at a fraction of the computational cost usually required to obtain them. Sampling from this approximate posterior allows the generation of climate predictions with quantified parametric uncertainties.

</details>

<details>

<summary>2021-08-20 04:35:11 - A Copula-based Fully Bayesian Nonparametric Evaluation of Cardiovascular Risk Markers in the Mexico City Diabetes Study</summary>

- *Claudia Wehrhahn, Ruth Fuentes-García, Ramsés H. Mena, Fabrizio Leisen, Maria Elena González-Villalpando, Clicerio González-Villalpando*

- `2007.11700v3` - [abs](http://arxiv.org/abs/2007.11700v3) - [pdf](http://arxiv.org/pdf/2007.11700v3)

> Cardiovascular disease lead the cause of death world wide and several studies have been carried out to understand and explore cardiovascular risk markers in normoglycemic and diabetic populations. In this work, we explore the association structure between hyperglycemic markers and cardiovascular risk markers controlled by triglycerides, body mass index, age and gender, for the normoglycemic population in The Mexico City Diabetes Study. Understanding the association structure could contribute to the assessment of additional cardiovascular risk markers in this low income urban population with a high prevalence of classic cardiovascular risk biomarkers. The association structure is measured by conditional Kendall's tau, defined through conditional copula functions. The latter are in turn modeled under a fully Bayesian nonparametric approach, which allows the complete shape of the copula function to vary for different values of the controlled covariates.

</details>

<details>

<summary>2021-08-20 13:23:17 - A survey on Bayesian inference for Gaussian mixture model</summary>

- *Jun Lu*

- `2108.11753v1` - [abs](http://arxiv.org/abs/2108.11753v1) - [pdf](http://arxiv.org/pdf/2108.11753v1)

> Clustering has become a core technology in machine learning, largely due to its application in the field of unsupervised learning, clustering, classification, and density estimation. A frequentist approach exists to hand clustering based on mixture model which is known as the EM algorithm where the parameters of the mixture model are usually estimated into a maximum likelihood estimation framework. Bayesian approach for finite and infinite Gaussian mixture model generates point estimates for all variables as well as associated uncertainty in the form of the whole estimates' posterior distribution.   The sole aim of this survey is to give a self-contained introduction to concepts and mathematical tools in Bayesian inference for finite and infinite Gaussian mixture model in order to seamlessly introduce their applications in subsequent sections. However, we clearly realize our inability to cover all the useful and interesting results concerning this field and given the paucity of scope to present this discussion, e.g., the separated analysis of the generation of Dirichlet samples by stick-breaking and Polya's Urn approaches. We refer the reader to literature in the field of the Dirichlet process mixture model for a much detailed introduction to the related fields. Some excellent examples include (Frigyik et al., 2010; Murphy, 2012; Gelman et al., 2014; Hoff, 2009).   This survey is primarily a summary of purpose, significance of important background and techniques for Gaussian mixture model, e.g., Dirichlet prior, Chinese restaurant process, and most importantly the origin and complexity of the methods which shed light on their modern applications. The mathematical prerequisite is a first course in probability. Other than this modest background, the development is self-contained, with rigorous proofs provided throughout.

</details>

<details>

<summary>2021-08-21 08:46:48 - Post-Processed Posteriors for Sparse Covariances and Its Application to Global Minimum Variance Portfolio</summary>

- *Kwangmin Lee, Jaeyong Lee*

- `2108.09462v1` - [abs](http://arxiv.org/abs/2108.09462v1) - [pdf](http://arxiv.org/pdf/2108.09462v1)

> We consider Bayesian inference of sparse covariance matrices and propose a post-processed posterior. This method consists of two steps. In the first step, posterior samples are obtained from the conjugate inverse-Wishart posterior without considering the sparse structural assumption. The posterior samples are transformed in the second step to satisfy the sparse structural assumption through the hard-thresholding function. This non-traditional Bayesian procedure is justified by showing that the post-processed posterior attains the optimal minimax rates. We also investigate the application of the post-processed posterior to the estimation of the global minimum variance portfolio. We show that the post-processed posterior for the global minimum variance portfolio also attains the optimal minimax rate under the sparse covariance assumption. The advantages of the post-processed posterior for the global minimum variance portfolio are demonstrated by a simulation study and a real data analysis with S&P 400 data.

</details>

<details>

<summary>2021-08-21 10:52:20 - On Bayesian Estimation of Densities and Sampling Distributions: the Posterior Predictive Distribution as the Bayes Estimator</summary>

- *A. G. Nogales*

- `2008.00683v3` - [abs](http://arxiv.org/abs/2008.00683v3) - [pdf](http://arxiv.org/pdf/2008.00683v3)

> Optimality results for two outstanding Bayesian estimation problems are given in this paper: the estimation of the sampling distribution for the squared total variation function and the estimation of the density for the $L^1$-squared loss function. The posterior predictive distribution provides the solution to these problems. Some examples are presented to illustrate it. The Bayesian estimation problem of a distribution function is also addressed. Consistency of the estimator of the density is proved.

</details>

<details>

<summary>2021-08-21 12:21:01 - A Sparse Structure Learning Algorithm for Bayesian Network Identification from Discrete High-Dimensional Data</summary>

- *Nazanin Shajoonnezhad, Amin Nikanjam*

- `2108.09501v1` - [abs](http://arxiv.org/abs/2108.09501v1) - [pdf](http://arxiv.org/pdf/2108.09501v1)

> This paper addresses the problem of learning a sparse structure Bayesian network from high-dimensional discrete data. Compared to continuous Bayesian networks, learning a discrete Bayesian network is a challenging problem due to the large parameter space. Although many approaches have been developed for learning continuous Bayesian networks, few approaches have been proposed for the discrete ones. In this paper, we address learning Bayesian networks as an optimization problem and propose a score function that satisfies the sparsity and the DAG property simultaneously. Besides, we implement a block-wised stochastic coordinate descent algorithm to optimize the score function. Specifically, we use a variance reducing method in our optimization algorithm to make the algorithm work efficiently in high-dimensional data. The proposed approach is applied to synthetic data from well-known benchmark networks. The quality, scalability, and robustness of the constructed network are measured. Compared to some competitive approaches, the results reveal that our algorithm outperforms the others in evaluation metrics.

</details>

<details>

<summary>2021-08-21 12:31:49 - Safeguarded Dynamic Label Regression for Generalized Noisy Supervision</summary>

- *Jiangchao Yao, Ya Zhang, Ivor W. Tsang, Jun Sun*

- `1903.02152v2` - [abs](http://arxiv.org/abs/1903.02152v2) - [pdf](http://arxiv.org/pdf/1903.02152v2)

> Learning with noisy labels, which aims to reduce expensive labors on accurate annotations, has become imperative in the Big Data era. Previous noise transition based method has achieved promising results and presented a theoretical guarantee on performance in the case of class-conditional noise. However, this type of approaches critically depend on an accurate pre-estimation of the noise transition, which is usually impractical. Subsequent improvement adapts the pre-estimation along with the training progress via a Softmax layer. However, the parameters in the Softmax layer are highly tweaked for the fragile performance due to the ill-posed stochastic approximation. To address these issues, we propose a Latent Class-Conditional Noise model (LCCN) that naturally embeds the noise transition under a Bayesian framework. By projecting the noise transition into a Dirichlet-distributed space, the learning is constrained on a simplex based on the whole dataset, instead of some ad-hoc parametric space. We then deduce a dynamic label regression method for LCCN to iteratively infer the latent labels, to stochastically train the classifier and to model the noise. Our approach safeguards the bounded update of the noise transition, which avoids previous arbitrarily tuning via a batch of samples. We further generalize LCCN for open-set noisy labels and the semi-supervised setting. We perform extensive experiments with the controllable noise data sets, CIFAR-10 and CIFAR-100, and the agnostic noise data sets, Clothing1M and WebVision17. The experimental results have demonstrated that the proposed model outperforms several state-of-the-art methods.

</details>

<details>

<summary>2021-08-22 02:59:03 - Information Geometry and Classical Cramér-Rao Type Inequalities</summary>

- *Kumar Vijay Mishra, M. Ashok Kumar*

- `2104.01061v3` - [abs](http://arxiv.org/abs/2104.01061v3) - [pdf](http://arxiv.org/pdf/2104.01061v3)

> We examine the role of information geometry in the context of classical Cram\'er-Rao (CR) type inequalities. In particular, we focus on Eguchi's theory of obtaining dualistic geometric structures from a divergence function and then applying Amari-Nagoaka's theory to obtain a CR type inequality. The classical deterministic CR inequality is derived from Kullback-Leibler (KL)-divergence. We show that this framework could be generalized to other CR type inequalities through four examples: $\alpha$-version of CR inequality, generalized CR inequality, Bayesian CR inequality, and Bayesian $\alpha$-CR inequality. These are obtained from, respectively, $I_\alpha$-divergence (or relative $\alpha$-entropy), generalized Csisz\'ar divergence, Bayesian KL divergence, and Bayesian $I_\alpha$-divergence.

</details>

<details>

<summary>2021-08-23 08:52:52 - A Bayesian Nonparametric Analysis of the 2003 Outbreak of Highly Pathogenic Avian Influenza in the Netherlands</summary>

- *R. G. Seymour, T. Kypraios, P. D. O'Neill, T. J. Hagenaars*

- `2009.04137v2` - [abs](http://arxiv.org/abs/2009.04137v2) - [pdf](http://arxiv.org/pdf/2009.04137v2)

> Infectious diseases on farms pose both public and animal health risks, so understanding how they spread between farms is crucial for developing disease control strategies to prevent future outbreaks. We develop novel Bayesian nonparametric methodology to fit spatial stochastic transmission models in which the infection rate between any two farms is a function that depends on the distance between them, but without assuming a specified parametric form. Making nonparametric inference in this context is challenging since the likelihood function of the observed data is intractable because the underlying transmission process is unobserved. We adopt a fully Bayesian approach by assigning a transformed Gaussian Process prior distribution to the infection rate function, and then develop an efficient data augmentation Markov Chain Monte Carlo algorithm to perform Bayesian inference. We use the posterior predictive distribution to simulate the effect of different disease control methods and their economic impact. We analyse a large outbreak of Avian Influenza in the Netherlands and infer the between-farm infection rate, as well as the unknown infection status of farms which were pre-emptively culled. We use our results to analyse ring-culling strategies, and conclude that although effective, ring-culling has limited impact in high density areas.

</details>

<details>

<summary>2021-08-23 09:10:49 - Mutational signatures and transmissibility of SARS-CoV-2 Gamma and Lambda variants</summary>

- *Karen Y. Oróstica, Sebastian Contreras, Sebastian B. Mohr, Jonas Dehning, Simon Bauer, David Medina-Ortiz, Emil N. Iftekhar, Karen Mujica, Paulo C. Covarrubias, Soledad Ulloa, Andrés E. Castillo, Ricardo A. Verdugo, Jorge Fernández, Álvaro Olivera-Nappa, Viola Priesemann*

- `2108.10018v1` - [abs](http://arxiv.org/abs/2108.10018v1) - [pdf](http://arxiv.org/pdf/2108.10018v1)

> The emergence of SARS-CoV-2 variants of concern endangers the long-term control of COVID-19, especially in countries with limited genomic surveillance. In this work, we explored genomic drivers of contagion in Chile. We sequenced 3443 SARS-CoV-2 genomes collected between January and July 2021, where the Gamma (P.1), Lambda (C.37), Alpha (B.1.1.7), B.1.1.348, and B.1.1 lineages were predominant. Using a Bayesian model tailored for limited genomic surveillance, we found that Lambda and Gamma variants' reproduction numbers were about 5% and 16% larger than Alpha's, respectively. We observed an overabundance of mutations in the Spike gene, strongly correlated with the variant's transmissibility. Furthermore, the variants' mutational signatures featured a breakpoint concurrent with the beginning of vaccination (mostly CoronaVac, an inactivated virus vaccine), indicating an additional putative selective pressure. Thus, our work provides a reliable method for quantifying novel variants' transmissibility under subsampling (as newly-reported Delta, B.1.617.2) and highlights the importance of continuous genomic surveillance.

</details>

<details>

<summary>2021-08-23 09:50:00 - The sceptical Bayes factor for the assessment of replication success</summary>

- *Samuel Pawel, Leonhard Held*

- `2009.01520v2` - [abs](http://arxiv.org/abs/2009.01520v2) - [pdf](http://arxiv.org/pdf/2009.01520v2)

> Replication studies are increasingly conducted but there is no established statistical criterion for replication success. We propose a novel approach combining reverse-Bayes analysis with Bayesian hypothesis testing: a sceptical prior is determined for the effect size such that the original finding is no longer convincing in terms of a Bayes factor. This prior is then contrasted to an advocacy prior (the reference posterior of the effect size based on the original study), and replication success is declared if the replication data favour the advocacy over the sceptical prior at a higher level than the original data favoured the sceptical prior over the null hypothesis. The sceptical Bayes factor is the highest level where replication success can be declared. A comparison to existing methods reveals that the sceptical Bayes factor combines several notions of replicability: it ensures that both studies show sufficient evidence against the null and penalises incompatibility of their effect estimates. Analysis of asymptotic properties and error rates, as well as case studies from the Social Sciences Replication Project show the advantages of the method for the assessment of replicability.

</details>

<details>

<summary>2021-08-23 16:09:07 - Estimating a Latent Tree for Extremes</summary>

- *Ngoc Mai Tran, Johannes Buck, Claudia Klüppelberg*

- `2102.06197v2` - [abs](http://arxiv.org/abs/2102.06197v2) - [pdf](http://arxiv.org/pdf/2102.06197v2)

> The Latent River Problem has emerged as a flagship problem for causal discovery in extreme value statistics. This paper gives QTree, a simple and efficient algorithm to solve the Latent River Problem that outperforms existing methods. QTree returns a directed graph and achieves almost perfect recovery on the Upper Danube, the existing benchmark dataset, as well as on new data from the Lower Colorado River in Texas. It can handle missing data, has an automated parameter tuning procedure, and runs in time $O(n |V|^2)$, where $n$ is the number of observations and $|V|$ the number of nodes in the graph. In addition, under a Bayesian network model for extreme values with propagating noise, we show that the QTree estimator returns for $n\to\infty$ a.s. the correct tree.

</details>

<details>

<summary>2021-08-23 18:09:41 - Explaining Bayesian Neural Networks</summary>

- *Kirill Bykov, Marina M. -C. Höhne, Adelaida Creosteanu, Klaus-Robert Müller, Frederick Klauschen, Shinichi Nakajima, Marius Kloft*

- `2108.10346v1` - [abs](http://arxiv.org/abs/2108.10346v1) - [pdf](http://arxiv.org/pdf/2108.10346v1)

> To make advanced learning machines such as Deep Neural Networks (DNNs) more transparent in decision making, explainable AI (XAI) aims to provide interpretations of DNNs' predictions. These interpretations are usually given in the form of heatmaps, each one illustrating relevant patterns regarding the prediction for a given instance. Bayesian approaches such as Bayesian Neural Networks (BNNs) so far have a limited form of transparency (model transparency) already built-in through their prior weight distribution, but notably, they lack explanations of their predictions for given instances. In this work, we bring together these two perspectives of transparency into a holistic explanation framework for explaining BNNs. Within the Bayesian framework, the network weights follow a probability distribution. Hence, the standard (deterministic) prediction strategy of DNNs extends in BNNs to a predictive distribution, and thus the standard explanation extends to an explanation distribution. Exploiting this view, we uncover that BNNs implicitly employ multiple heterogeneous prediction strategies. While some of these are inherited from standard DNNs, others are revealed to us by considering the inherent uncertainty in BNNs. Our quantitative and qualitative experiments on toy/benchmark data and real-world data from pathology show that the proposed approach of explaining BNNs can lead to more effective and insightful explanations.

</details>

<details>

<summary>2021-08-24 01:42:13 - Bayesian Inference for Generalized Linear Model with Linear Inequality Constraints</summary>

- *Rahul Ghosal, Sujit K. Ghosh*

- `2108.10472v1` - [abs](http://arxiv.org/abs/2108.10472v1) - [pdf](http://arxiv.org/pdf/2108.10472v1)

> Bayesian statistical inference for Generalized Linear Models (GLMs) with parameters lying on a constrained space is of general interest (e.g., in monotonic or convex regression), but often constructing valid prior distributions supported on a subspace spanned by a set of linear inequality constraints can be challenging, especially when some of the constraints might be binding leading to a lower dimensional subspace. For the general case with canonical link, it is shown that a generalized truncated multivariate normal supported on a desired subspace can be used. Moreover, it is shown that such prior distribution facilitates the construction of a general purpose product slice sampling method to obtain (approximate) samples from corresponding posterior distribution, making the inferential method computationally efficient for a wide class of GLMs with an arbitrary set of linear inequality constraints. The proposed product slice sampler is shown to be uniformly ergodic, having a geometric convergence rate under a set of mild regularity conditions satisfied by many popular GLMs (e.g., logistic and Poisson regressions with constrained coefficients). One of the primary advantages of the proposed Bayesian estimation method over classical methods is that uncertainty of parameter estimates is easily quantified by using the samples simulated from the path of the Markov Chain of the slice sampler. Numerical illustrations using simulated data sets are presented to illustrate the superiority of the proposed methods compared to some existing methods in terms of sampling bias and variances. In addition, real case studies are presented using data sets for fertilizer-crop production and estimating the SCRAM rate in nuclear power plants.

</details>

<details>

<summary>2021-08-24 04:02:45 - Uncertainty Quantification of the 4th kind; optimal posterior accuracy-uncertainty tradeoff with the minimum enclosing ball</summary>

- *Hamed Hamze Bajgiran, Pau Batlle Franch, Houman Owhadi, Clint Scovel, Mahdy Shirdel, Michael Stanley, Peyman Tavallali*

- `2108.10517v1` - [abs](http://arxiv.org/abs/2108.10517v1) - [pdf](http://arxiv.org/pdf/2108.10517v1)

> There are essentially three kinds of approaches to Uncertainty Quantification (UQ): (A) robust optimization, (B) Bayesian, (C) decision theory. Although (A) is robust, it is unfavorable with respect to accuracy and data assimilation. (B) requires a prior, it is generally brittle and posterior estimations can be slow. Although (C) leads to the identification of an optimal prior, its approximation suffers from the curse of dimensionality and the notion of risk is one that is averaged with respect to the distribution of the data. We introduce a 4th kind which is a hybrid between (A), (B), (C), and hypothesis testing. It can be summarized as, after observing a sample $x$, (1) defining a likelihood region through the relative likelihood and (2) playing a minmax game in that region to define optimal estimators and their risk. The resulting method has several desirable properties (a) an optimal prior is identified after measuring the data, and the notion of risk is a posterior one, (b) the determination of the optimal estimate and its risk can be reduced to computing the minimum enclosing ball of the image of the likelihood region under the quantity of interest map (which is fast and not subject to the curse of dimensionality). The method is characterized by a parameter in $ [0,1]$ acting as an assumed lower bound on the rarity of the observed data (the relative likelihood). When that parameter is near $1$, the method produces a posterior distribution concentrated around a maximum likelihood estimate with tight but low confidence UQ estimates. When that parameter is near $0$, the method produces a maximal risk posterior distribution with high confidence UQ estimates. In addition to navigating the accuracy-uncertainty tradeoff, the proposed method addresses the brittleness of Bayesian inference by navigating the robustness-accuracy tradeoff associated with data assimilation.

</details>

<details>

<summary>2021-08-24 08:55:46 - State estimation for aoristic models</summary>

- *M. N. M. van Lieshout, R. L. Markwitz*

- `2108.10584v1` - [abs](http://arxiv.org/abs/2108.10584v1) - [pdf](http://arxiv.org/pdf/2108.10584v1)

> Aoristic data can be described by a marked point process in time in which the points cannot be observed directly but are known to lie in observable intervals, the marks. We consider Bayesian state estimation for the latent points when the marks are modelled in terms of an alternating renewal process in equilibrium and the prior is a Markov point point process. We derive the posterior distribution, estimate its parameters and present some examples that illustrate the influence of the prior distribution.

</details>

<details>

<summary>2021-08-24 18:01:33 - Stationarity and inference in multistate promoter models of stochastic gene expression via stick-breaking measures</summary>

- *William Lippitt, Sunder Sethuraman, Xueying Tang*

- `2108.10896v1` - [abs](http://arxiv.org/abs/2108.10896v1) - [pdf](http://arxiv.org/pdf/2108.10896v1)

> In a general stochastic multistate promoter model of dynamic mRNA/protein interactions, we identify the stationary joint distribution of the promoter state, mRNA, and protein levels through an explicit `stick-breaking' construction of interest in itself. This derivation is a constructive advance over previous work where the stationary distribution is solved only in restricted cases. Moreover, the stick-breaking construction allows to sample directly from the stationary distribution, permitting inference procedures and model selection. In this context, we discuss numerical Bayesian experiments to illustrate the results.

</details>

<details>

<summary>2021-08-24 19:11:45 - Bayesian Inference using the Proximal Mapping: Uncertainty Quantification under Varying Dimensionality</summary>

- *Maoran Xu, Hua Zhou, Yujie Hu, Leo L. Duan*

- `2108.04851v2` - [abs](http://arxiv.org/abs/2108.04851v2) - [pdf](http://arxiv.org/pdf/2108.04851v2)

> In statistical applications, it is common to encounter parameters supported on a varying or unknown dimensional space. Examples include the fused lasso regression, the matrix recovery under an unknown low rank, etc. Despite the ease of obtaining a point estimate via the optimization, it is much more challenging to quantify their uncertainty -- in the Bayesian framework, a major difficulty is that if assigning the prior associated with a $p$-dimensional measure, then there is zero posterior probability on any lower-dimensional subset with dimension $d<p$; to avoid this caveat, one needs to choose another dimension-selection prior on $d$, which often involves a highly combinatorial problem. To significantly reduce the modeling burden, we propose a new generative process for the prior: starting from a continuous random variable such as multivariate Gaussian, we transform it into a varying-dimensional space using the proximal mapping.   This leads to a large class of new Bayesian models that can directly exploit the popular frequentist regularizations and their algorithms, such as the nuclear norm penalty and the alternating direction method of multipliers, while providing a principled and probabilistic uncertainty estimation.   We show that this framework is well justified in the geometric measure theory, and enjoys a convenient posterior computation via the standard Hamiltonian Monte Carlo. We demonstrate its use in the analysis of the dynamic flow network data.

</details>

<details>

<summary>2021-08-24 19:58:00 - Pseudo Bayesian Mixed Models under Informative Sampling</summary>

- *Terrance D. Savitsky, Matthew R. Williams*

- `1904.07680v5` - [abs](http://arxiv.org/abs/1904.07680v5) - [pdf](http://arxiv.org/pdf/1904.07680v5)

> When random effects are correlated with sample design variables, the usual approach of employing individual survey weights (constructed to be inversely proportional to the unit survey inclusion probabilities) to form a pseudo-likelihood no longer produces asymptotically unbiased inference. We construct a weight-exponentiated formulation for the random effects distribution that achieves unbiased inference for generating hyperparameters of the random effects. We contrast our approach with frequentist methods that rely on numerical integration to reveal that only the Bayesian method achieves both unbiased estimation with respect to the sampling design distribution and consistency with respect to the population generating distribution. Our simulations and real data example for a survey of business establishments demonstrate the utility of our approach across different modeling formulations and sampling designs. This work serves as a capstone for recent developmental efforts that combine traditional survey estimation approaches with the Bayesian modeling paradigm and provides a bridge across the two rich but disparate sub-fields.

</details>

<details>

<summary>2021-08-24 23:29:59 - Fully Bayesian Estimation under Dependent and Informative Cluster Sampling</summary>

- *Luis G. Leon-Novelo, Terrance D. Savitsky*

- `2101.06237v2` - [abs](http://arxiv.org/abs/2101.06237v2) - [pdf](http://arxiv.org/pdf/2101.06237v2)

> Survey data are often collected under multistage sampling designs where units are binned to clusters that are sampled in a first stage. The unit-indexed population variables of interest are typically dependent within cluster. We propose a Fully Bayesian method that constructs an exact likelihood for the observed sample to incorporate unit-level marginal sampling weights for performing unbiased inference for population parameters while simultaneously accounting for the dependence induced by sampling clusters of units to produce correct uncertainty quantification. Our approach parameterizes cluster-indexed random effects in both a marginal model for the response and a conditional model for published, unit-level sampling weights. We compare our method to plug-in Bayesian and frequentist alternatives in a simulation study and demonstrate that our method most closely achieves correct uncertainty quantification for model parameters, including the generating variances for cluster-indexed random effects. We demonstrate our method in an application with NHANES data.

</details>

<details>

<summary>2021-08-25 00:48:07 - Layer Adaptive Node Selection in Bayesian Neural Networks: Statistical Guarantees and Implementation Details</summary>

- *Sanket Jantre, Shrijita Bhattacharya, Tapabrata Maiti*

- `2108.11000v1` - [abs](http://arxiv.org/abs/2108.11000v1) - [pdf](http://arxiv.org/pdf/2108.11000v1)

> Sparse deep neural networks have proven to be efficient for predictive model building in large-scale studies. Although several works have studied theoretical and numerical properties of sparse neural architectures, they have primarily focused on the edge selection. Sparsity through edge selection might be intuitively appealing; however, it does not necessarily reduce the structural complexity of a network. Instead pruning excessive nodes in each layer leads to a structurally sparse network which would have lower computational complexity and memory footprint. We propose a Bayesian sparse solution using spike-and-slab Gaussian priors to allow for node selection during training. The use of spike-and-slab prior alleviates the need of an ad-hoc thresholding rule for pruning redundant nodes from a network. In addition, we adopt a variational Bayes approach to circumvent the computational challenges of traditional Markov Chain Monte Carlo (MCMC) implementation. In the context of node selection, we establish the fundamental result of variational posterior consistency together with the characterization of prior parameters. In contrast to the previous works, our theoretical development relaxes the assumptions of the equal number of nodes and uniform bounds on all network weights, thereby accommodating sparse networks with layer-dependent node structures or coefficient bounds. With a layer-wise characterization of prior inclusion probabilities, we also discuss optimal contraction rates of the variational posterior. Finally, we provide empirical evidence to substantiate that our theoretical work facilitates layer-wise optimal node recovery together with competitive predictive performance.

</details>

<details>

<summary>2021-08-25 23:02:06 - Bayesian Estimation of the Hydroxyl Radical Diffusion Coefficient at Low Temperature and High Pressure from Atomistic Molecular Dynamics</summary>

- *Carter T. Butts, Rachel W. Martin*

- `2108.11511v1` - [abs](http://arxiv.org/abs/2108.11511v1) - [pdf](http://arxiv.org/pdf/2108.11511v1)

> The hydroxyl radical is the primary reactive oxygen species produced by the radiolysis of water, and is a significant source of radiation damage to living organisms. Mobility of the hydroxyl radical at low temperatures and/or high pressures is hence a potentially important factor in determining the challenges facing psychrophilic and/or barophilic organisms in high-radiation environments (e.g., ice-interface or undersea environments in which radiative heating is a potential heat and energy source). Here, we estimate the diffusion coefficient for the hydroxyl radical in aqueous solution, using a hierarchical Bayesian model based on atomistic molecular dynamics trajectories in TIP4P/2005 water over a range of temperatures and pressures.

</details>

<details>

<summary>2021-08-25 23:40:32 - Pathogen.jl: Infectious Disease Transmission Network Modelling with Julia</summary>

- *Justin Angevaare, Zeny Feng, Rob Deardon*

- `2002.05850v3` - [abs](http://arxiv.org/abs/2002.05850v3) - [pdf](http://arxiv.org/pdf/2002.05850v3)

> We introduce Pathogen.jl for simulation and inference of transmission network individual level models (TN-ILMs) of infectious disease spread in continuous time. TN-ILMs can be used to jointly infer transmission networks, event times, and model parameters within a Bayesian framework via Markov chain Monte Carlo (MCMC). We detail our specific strategies for conducting MCMC for TN-ILMs, and our implementation of these strategies in the Julia package, Pathogen.jl, which leverages key features of the Julia language. We provide an example using Pathogen.jl to simulate an epidemic following a susceptible-infectious-removed (SIR) TN-ILM, and then perform inference using observations that were generated from that epidemic. We also demonstrate the functionality of Pathogen.jl with an application of TN-ILMs to data from a measles outbreak that occurred in Hagelloch, Germany in 1861(Pfeilsticker 1863; Oesterle 1992).

</details>

<details>

<summary>2021-08-26 05:00:27 - Modeling Item Response Theory with Stochastic Variational Inference</summary>

- *Mike Wu, Richard L. Davis, Benjamin W. Domingue, Chris Piech, Noah Goodman*

- `2108.11579v1` - [abs](http://arxiv.org/abs/2108.11579v1) - [pdf](http://arxiv.org/pdf/2108.11579v1)

> Item Response Theory (IRT) is a ubiquitous model for understanding human behaviors and attitudes based on their responses to questions. Large modern datasets offer opportunities to capture more nuances in human behavior, potentially improving psychometric modeling leading to improved scientific understanding and public policy. However, while larger datasets allow for more flexible approaches, many contemporary algorithms for fitting IRT models may also have massive computational demands that forbid real-world application. To address this bottleneck, we introduce a variational Bayesian inference algorithm for IRT, and show that it is fast and scalable without sacrificing accuracy. Applying this method to five large-scale item response datasets from cognitive science and education yields higher log likelihoods and higher accuracy in imputing missing data than alternative inference algorithms. Using this new inference approach we then generalize IRT with expressive Bayesian models of responses, leveraging recent advances in deep learning to capture nonlinear item characteristic curves (ICC) with neural networks. Using an eigth-grade mathematics test from TIMSS, we show our nonlinear IRT models can capture interesting asymmetric ICCs. The algorithm implementation is open-source, and easily usable.

</details>

<details>

<summary>2021-08-26 06:40:31 - Distributed Soft Bayesian Additive Regression Trees</summary>

- *Hao Ran, Yang Bai*

- `2108.11600v1` - [abs](http://arxiv.org/abs/2108.11600v1) - [pdf](http://arxiv.org/pdf/2108.11600v1)

> Bayesian Additive Regression Trees(BART) is a Bayesian nonparametric approach which has been shown to be competitive with the best modern predictive methods such as random forest and Gradient Boosting Decision Tree.The sum of trees structure combined with a Bayesian inferential framework provide a accurate and robust statistic method.BART variant named SBART using randomized decision trees has been developed and show practical benefits compared to BART. The primary bottleneck of SBART is the speed to compute the sufficient statistics and the publicly avaiable implementation of the SBART algorithm in the R package is very slow.In this paper we show how the SBART algorithm can be modified and computed using single program,multiple data(SPMD) distributed computation with the Message Passing Interface(MPI) library.This approach scales nearly linearly in the number of processor cores, enabling the practitioner to perform statistical inference on massive datasets. Our approach can also handle datasets too massive to fit on any single data repository.We have made modification to this algorithm to make it capable to handle classfication problem which can not be done with the original R package.With data experiments we show the advantage of distributed SBART for classfication problem compared to BART.

</details>

<details>

<summary>2021-08-26 06:53:09 - On Soft Bayesian Additive Regression Trees and asynchronous longitudinal regression analysis</summary>

- *Hao Ran, Yang Bai*

- `2108.11603v1` - [abs](http://arxiv.org/abs/2108.11603v1) - [pdf](http://arxiv.org/pdf/2108.11603v1)

> In many longitudinal studies, the covariate and response are often intermittently observed at irregular, mismatched and subject-specific times. How to deal with such data when covariate and response are observed asynchronously is an often raised problem. Bayesian Additive Regression Trees(BART) is a Bayesian non-Parametric approach which has been shown to be competitive with the best modern predictive methods such as random forest and boosted decision trees. The sum of trees structure combined with a Bayesian inferential framework provide a accurate and robust statistic method. BART variant soft Bayesian Additive Regression Trees(SBART) constructed using randomized decision trees was developed and substantial theoretical and practical benefits were shown. In this paper, we propose a weighted SBART model solution for asynchronous longitudinal data. In comparison to other methods, the current methods are valid under with little assumptions on the covariate process. Extensive simulation studies provide numerical support for this solution. And data from an HIV study is used to illustrate our methodology

</details>

<details>

<summary>2021-08-26 08:18:03 - Online Optimization of Stimulation Speed in an Auditory Brain-Computer Interface under Time Constraints</summary>

- *Jan Sosulski, David Hübner, Aaron Klein, Michael Tangermann*

- `2109.06011v1` - [abs](http://arxiv.org/abs/2109.06011v1) - [pdf](http://arxiv.org/pdf/2109.06011v1)

> The decoding of brain signals recorded via, e.g., an electroencephalogram, using machine learning is key to brain-computer interfaces (BCIs). Stimulation parameters or other experimental settings of the BCI protocol typically are chosen according to the literature. The decoding performance directly depends on the choice of parameters, as they influence the elicited brain signals and optimal parameters are subject-dependent. Thus a fast and automated selection procedure for experimental parameters could greatly improve the usability of BCIs.   We evaluate a standalone random search and a combined Bayesian optimization with random search in a closed-loop auditory event-related potential protocol. We aimed at finding the individually best stimulation speed -- also known as stimulus onset asynchrony (SOA) -- that maximizes the classification performance of a regularized linear discriminant analysis. To make the Bayesian optimization feasible under noise and the time pressure posed by an online BCI experiment, we first used offline simulations to initialize and constrain the internal optimization model. Then we evaluated our approach online with 13 healthy subjects.   We could show that for 8 out of 13 subjects, the proposed approach using Bayesian optimization succeeded to select the individually optimal SOA out of multiple evaluated SOA values. Our data suggests, however, that subjects were influenced to very different degrees by the SOA parameter. This makes the automatic parameter selection infeasible for subjects where the influence is limited.   Our work proposes an approach to exploit the benefits of individualized experimental protocols and evaluated it in an auditory BCI. When applied to other experimental parameters our approach could enhance the usability of BCI for different target groups -- specifically if an individual disease progress may prevent the use of standard parameters.

</details>

<details>

<summary>2021-08-26 15:54:59 - Nonparametric Bayesian volatility estimation for gamma-driven stochastic differential equations</summary>

- *Denis Belomestny, Shota Gugushvili, Moritz Schauer, Peter Spreij*

- `2011.08321v2` - [abs](http://arxiv.org/abs/2011.08321v2) - [pdf](http://arxiv.org/pdf/2011.08321v2)

> We study a nonparametric Bayesian approach to estimation of the volatility function of a stochastic differential equation driven by a gamma process. The volatility function is modelled a priori as piecewise constant, and we specify a gamma prior on its values. This leads to a straightforward procedure for posterior inference via an MCMC procedure. We give theoretical performance guarantees (contraction rates for the posterior) for the Bayesian estimate in terms of the regularity of the unknown volatility function. We illustrate the method on synthetic and real data examples.

</details>

<details>

<summary>2021-08-26 16:29:07 - Active Learning for Deep Gaussian Process Surrogates</summary>

- *Annie Sauer, Robert B. Gramacy, David Higdon*

- `2012.08015v2` - [abs](http://arxiv.org/abs/2012.08015v2) - [pdf](http://arxiv.org/pdf/2012.08015v2)

> Deep Gaussian processes (DGPs) are increasingly popular as predictive models in machine learning (ML) for their non-stationary flexibility and ability to cope with abrupt regime changes in training data. Here we explore DGPs as surrogates for computer simulation experiments whose response surfaces exhibit similar characteristics. In particular, we transport a DGP's automatic warping of the input space and full uncertainty quantification (UQ), via a novel elliptical slice sampling (ESS) Bayesian posterior inferential scheme, through to active learning (AL) strategies that distribute runs non-uniformly in the input space -- something an ordinary (stationary) GP could not do. Building up the design sequentially in this way allows smaller training sets, limiting both expensive evaluation of the simulator code and mitigating cubic costs of DGP inference. When training data sizes are kept small through careful acquisition, and with parsimonious layout of latent layers, the framework can be both effective and computationally tractable. Our methods are illustrated on simulation data and two real computer experiments of varying input dimensionality. We provide an open source implementation in the "deepgp" package on CRAN.

</details>

<details>

<summary>2021-08-26 18:59:14 - Contaminated Gibbs-type priors</summary>

- *Federico Camerlenghi, Riccardo Corradin, Andrea Ongaro*

- `2108.11997v1` - [abs](http://arxiv.org/abs/2108.11997v1) - [pdf](http://arxiv.org/pdf/2108.11997v1)

> Gibbs-type priors are widely used as key components in several Bayesian nonparametric models. By virtue of their flexibility and mathematical tractability, they turn out to be predominant priors in species sampling problems, clustering and mixture modelling. We introduce a new family of processes which extend the Gibbs-type one, by including a contaminant component in the model to account for the presence of anomalies (outliers) or an excess of observations with frequency one. We first investigate the induced random partition, the associated predictive distribution and we characterize the asymptotic behaviour of the number of clusters. All the results we obtain are in closed form and easily interpretable, as a noteworthy example we focus on the contaminated version of the Pitman-Yor process. Finally we pinpoint the advantage of our construction in different applied problems: we show how the contaminant component helps to perform outlier detection for an astronomical clustering problem and to improve predictive inference in a species-related dataset, exhibiting a high number of species with frequency one.

</details>

<details>

<summary>2021-08-26 19:46:03 - Decentralized Stochastic Gradient Langevin Dynamics and Hamiltonian Monte Carlo</summary>

- *Mert Gürbüzbalaban, Xuefeng Gao, Yuanhan Hu, Lingjiong Zhu*

- `2007.00590v4` - [abs](http://arxiv.org/abs/2007.00590v4) - [pdf](http://arxiv.org/pdf/2007.00590v4)

> Stochastic gradient Langevin dynamics (SGLD) and stochastic gradient Hamiltonian Monte Carlo (SGHMC) are two popular Markov Chain Monte Carlo (MCMC) algorithms for Bayesian inference that can scale to large datasets, allowing to sample from the posterior distribution of the parameters of a statistical model given the input data and the prior distribution over the model parameters. However, these algorithms do not apply to the decentralized learning setting, when a network of agents are working collaboratively to learn the parameters of a statistical model without sharing their individual data due to privacy reasons or communication constraints. We study two algorithms: Decentralized SGLD (DE-SGLD) and Decentralized SGHMC (DE-SGHMC) which are adaptations of SGLD and SGHMC methods that allow scaleable Bayesian inference in the decentralized setting for large datasets. We show that when the posterior distribution is strongly log-concave and smooth, the iterates of these algorithms converge linearly to a neighborhood of the target distribution in the 2-Wasserstein distance if their parameters are selected appropriately. We illustrate the efficiency of our algorithms on decentralized Bayesian linear regression and Bayesian logistic regression problems.

</details>

<details>

<summary>2021-08-26 21:54:15 - Selection of inverse gamma and half-t priors for hierarchical models: sensitivity and recommendations</summary>

- *Zachary Brehm, Aaron Wagner, Erik VonKaenel, David Burton, Samuel J. Weisenthal, Martin Cole, Yiping Pang, Sally W. Thurston*

- `2108.12045v1` - [abs](http://arxiv.org/abs/2108.12045v1) - [pdf](http://arxiv.org/pdf/2108.12045v1)

> While the importance of prior selection is well understood, establishing guidelines for selecting priors in hierarchical models has remained an active, and sometimes contentious, area of Bayesian methodology research. Choices of hyperparameters for individual families of priors are often discussed in the literature, but rarely are different families of priors compared under similar models and hyperparameters. Using simulated data, we evaluate the performance of inverse gamma and half-$t$ priors for estimating the standard deviation of random effects in three hierarchical models: the 8-schools model, a random intercepts longitudinal model, and a simple multiple outcomes model. We compare the performance of the two prior families using a range of prior hyperparameters, some of which have been suggested in the literature, and others that allow for a direct comparison of pairs of half-$t$ and inverse-gamma priors. Estimation of very small values of the random effect standard deviation led to convergence issues especially for the half-$t$ priors. For most settings, we found that the posterior distribution of the standard deviation had smaller bias under half-$t$ priors than under their inverse-gamma counterparts. Inverse gamma priors generally gave similar coverage but had smaller interval lengths than their half-$t$ prior counterparts. Our results for these two prior families will inform prior specification for hierarchical models, allowing practitioners to better align their priors with their respective models and goals.

</details>

<details>

<summary>2021-08-27 07:50:34 - The ICSCREAM methodology: Identification of penalizing configurations in computer experiments using screening and metamodel -- Applications in thermal-hydraulics</summary>

- *A. Marrel, Bertrand Iooss, V Chabridon*

- `2004.04663v3` - [abs](http://arxiv.org/abs/2004.04663v3) - [pdf](http://arxiv.org/pdf/2004.04663v3)

> In the framework of risk assessment in nuclear accident analysis, best-estimatecomputer codes, associated to a probabilistic modeling of the uncertain input variables,are used to estimate safety margins. A first step in such uncertainty quantificationstudies is often to identify the critical configurations (or penalizing, in thesense of a prescribed safety margin) of several input parameters (called ``scenarioinputs''), under the uncertainty on the other input parameters. However, the largeCPU-time cost of most of the computer codes used in nuclear engineering, as theones related to thermal-hydraulic accident scenario simulations, involve to develophighly efficient strategies. This work focuses on machine learning algorithms bythe way of the metamodel-based approach (i.e., a mathematical model which is fittedon a small-size sample of simulations). To achieve it with a very large numberof inputs, a specific and original methodology, called ICSCREAM (Identificationof penalizing Configurations using SCREening And Metamodel), is proposed. Thescreening of influential inputs is based on an advanced global sensitivity analysistool (HSIC importance measures). A Gaussian process metamodel is then sequentiallybuilt and used to estimate, within a Bayesian framework, the conditionalprobabilities of exceeding a high-level threshold, according to the scenario inputs.The efficiency of this methodology is illustrated on two high-dimensional (arounda hundred inputs) thermal-hydraulic industrial cases simulating an accident of primarycoolant loss in a pressurized water reactor. For both use cases, the studyfocuses on the peak cladding temperature (PCT) and critical configurations aredefined by exceeding the 90%-quantile of PCT. In both cases, the ICSCREAMmethodology allows to estimate, by using only around one thousand of code simulations,the impact of the scenario inputs and their critical areas of values.

</details>

<details>

<summary>2021-08-27 10:26:16 - Bayesian Surrogate Analysis and Uncertainty Propagation</summary>

- *Sascha Ranftl, Wolfgang von der Linden*

- `2101.04038v2` - [abs](http://arxiv.org/abs/2101.04038v2) - [pdf](http://arxiv.org/pdf/2101.04038v2)

> The quantification of uncertainties of computer simulations due to input parameter uncertainties is paramount to assess a model's credibility. For computationally expensive simulations, this is often feasible only via surrogate models that are learned from a small set of simulation samples. The surrogate models are commonly chosen and deemed trustworthy based on heuristic measures, and substituted for the simulation in order to approximately propagate the simulation input uncertainties to the simulation output. In the process, the contribution of the uncertainties of the surrogate itself to the simulation output uncertainties are usually neglected. In this work, we specifically address the case of doubtful surrogate trustworthiness, i.e. non-negligible surrogate uncertainties. We find that Bayesian probability theory yields a natural measure of surrogate trustworthiness, and that surrogate uncertainties can easily be included in simulation output uncertainties. For a Gaussian likelihood for the simulation data, with unknown surrogate variance and given a generalized linear surrogate model, the resulting formulas reduce to simple matrix multiplications. The framework contains Polynomial Chaos Expansions as a special case, and is easily extended to Gaussian Process Regression. Additionally, we show a simple way to implicitly include spatio-temporal correlations. Lastly, we demonstrate a numerical example where surrogate uncertainties are in part negligible and in part non-negligible.

</details>

<details>

<summary>2021-08-27 17:06:38 - Bayesian Sparse Blind Deconvolution Using MCMC Methods Based on Normal-Inverse-Gamma Prior</summary>

- *Burak Cevat Civek, Emre Ertin*

- `2108.12398v1` - [abs](http://arxiv.org/abs/2108.12398v1) - [pdf](http://arxiv.org/pdf/2108.12398v1)

> Bayesian estimation methods for sparse blind deconvolution problems conventionally employ Bernoulli-Gaussian (BG) prior for modeling sparse sequences and utilize Markov Chain Monte Carlo (MCMC) methods for the estimation of unknowns. However, the discrete nature of the BG model creates computational bottlenecks, preventing efficient exploration of the probability space even with the recently proposed enhanced sampler schemes. To address this issue, we propose an alternative MCMC method by modeling the sparse sequences using the Normal-Inverse-Gamma (NIG) prior. We derive effective Gibbs samplers for this prior and illustrate that the computational burden associated with the BG model can be eliminated by transferring the problem into a completely continuous-valued framework. In addition to sparsity, we also incorporate time and frequency domain constraints on the convolving sequences. We demonstrate the effectiveness of the proposed methods via extensive simulations and characterize computational gains relative to the existing methods that utilize BG modeling.

</details>

<details>

<summary>2021-08-27 20:22:33 - Local estimators and Bayesian inverse problems with non-unique solutions</summary>

- *Jiguang Sun*

- `2105.09141v2` - [abs](http://arxiv.org/abs/2105.09141v2) - [pdf](http://arxiv.org/pdf/2105.09141v2)

> The Bayesian approach is effective for inverse problems. The posterior density distribution provides useful information of the unknowns. However, for problems with non-unique solutions, the classical estimators such as the maximum a posterior (MAP) and conditional mean (CM) are not enough. We introduce two new estimators, the local maximum a posterior (LMAP) and local conditional mean (LCM). Their applications are demonstrated by three inverse problems: an inverse spectral problem, an inverse source problem, and an inverse medium problem.

</details>

<details>

<summary>2021-08-28 01:17:55 - Bayesian Clustered Coefficients Regression with Auxiliary Covariates Assistant Random Effects</summary>

- *Guanyu Hu, Yishu Xue, Zhihua Ma*

- `2004.12022v2` - [abs](http://arxiv.org/abs/2004.12022v2) - [pdf](http://arxiv.org/pdf/2004.12022v2)

> In regional economics research, a problem of interest is to detect similarities between regions, and estimate their shared coefficients in economics models. In this article, we propose a mixture of finite mixtures (MFM) clustered regression model with auxiliary covariates that account for similarities in demographic or economic characteristics over a spatial domain. Our Bayesian construction provides both inference for number of clusters and clustering configurations, and estimation for parameters for each cluster. Empirical performance of the proposed model is illustrated through simulation experiments, and further applied to a study of influential factors for monthly housing cost in Georgia.

</details>

<details>

<summary>2021-08-29 04:13:46 - Bayesian Non-parametric Quantile Process Regression and Estimation of Marginal Quantile Effects</summary>

- *Steven G. Xu, Brian J. Reich*

- `2102.11309v3` - [abs](http://arxiv.org/abs/2102.11309v3) - [pdf](http://arxiv.org/pdf/2102.11309v3)

> Flexible estimation of multiple conditional quantiles is of interest in numerous applications, such as studying the effect of pregnancy-related factors on low and high birth weight. We propose a Bayesian non-parametric method to simultaneously estimate non-crossing, non-linear quantile curves. We expand the conditional distribution function of the response in I-spline basis functions where the covariate-dependent coefficients are modeled using neural networks. By leveraging the approximation power of splines and neural networks, our model can approximate any continuous quantile function. Compared to existing models, our model estimates all rather than a finite subset of quantiles, scales well to high dimensions, and accounts for estimation uncertainty. While the model is arbitrarily flexible, interpretable marginal quantile effects are estimated using accumulative local effect plots and variable importance measures. A simulation study shows that our model can better recover quantiles of the response distribution when the data is sparse, and an analysis of birth weight data is presented.

</details>

<details>

<summary>2021-08-29 11:56:12 - Inequality in Education: A Comparison of Australian Indigenous and Nonindigenous Populations</summary>

- *David Gunawan, William Griffiths, Duangkamon Chotikapanich*

- `2108.12830v1` - [abs](http://arxiv.org/abs/2108.12830v1) - [pdf](http://arxiv.org/pdf/2108.12830v1)

> Educational achievement distributions for Australian indigenous and nonindigenous populations in the years 2001, 2006, 2014 and 2017 are considered. Bayesian inference is used to analyse how these ordinal categorical distributions have changed over time and to compare indigenous and nonindigenous distributions. Both the level of educational achievement and inequality in educational achievement are considered. To compare changes in levels over time, as well as inequality between the two populations, first order stochastic dominance and an index of educational poverty are used. To examine changes in inequality over time, two inequality indices and generalised Lorenz dominance are considered. Results are presented in terms of posterior densities for the indices and posterior probabilities for dominance for the dominance comparisons. We find some evidence of improvement over time, especially in the lower parts of the indigenous distribution and that inequality has significantly increased from 2001 to 2017.

</details>

<details>

<summary>2021-08-29 18:59:50 - Scalable computation for Bayesian hierarchical models</summary>

- *Omiros Papaspiliopoulos, Timothée Stumpf-Fétizon, Giacomo Zanella*

- `2103.10875v2` - [abs](http://arxiv.org/abs/2103.10875v2) - [pdf](http://arxiv.org/pdf/2103.10875v2)

> The article is about algorithms for learning Bayesian hierarchical models, the computational complexity of which scales linearly with the number of observations and the number of parameters in the model. It focuses on crossed random effect and nested multilevel models, which are used ubiquitously in applied sciences, and illustrates the methodology on two challenging real data analyses on predicting electoral results and real estate prices respectively. The posterior dependence in both classes is sparse: in crossed random effects models it resembles a random graph, whereas in nested multilevel models it is tree-structured. For each class we develop a framework for scalable computation. We provide a number of negative (for crossed) and positive (for nested) results for the scalability (or lack thereof) of methods based on sparse linear algebra, which are relevant also to Laplace approximation methods for such models. Our numerical experiments compare with off-the-shelf variational approximations and Hamiltonian Monte Carlo. Our theoretical results, although partial, are useful in suggesting interesting methodologies and lead to conclusions that our numerics suggest to hold well beyond the scope of the underlying assumptions.

</details>

<details>

<summary>2021-08-30 01:03:55 - Keeping the Listener Engaged: a Dynamic Model of Bayesian Persuasion</summary>

- *Yeon-Koo Che, Kyungmin Kim, Konrad Mierendorff*

- `2003.07338v3` - [abs](http://arxiv.org/abs/2003.07338v3) - [pdf](http://arxiv.org/pdf/2003.07338v3)

> We consider a dynamic model of Bayesian persuasion in which information takes time and is costly for the sender to generate and for the receiver to process, and neither player can commit to their future actions. Persuasion may totally collapse in a Markov perfect equilibrium (MPE) of this game. However, for persuasion costs sufficiently small, a version of a folk theorem holds: outcomes that approximate Kamenica and Gentzkow (2011)'s sender-optimal persuasion as well as full revelation and everything in between are obtained in MPE, as the cost vanishes.

</details>

<details>

<summary>2021-08-30 12:42:48 - Covid-19 Belgium: Extended SEIR-QD model with nursing homes and long-term scenarios-based forecasts</summary>

- *Nicolas Franco*

- `2009.03450v4` - [abs](http://arxiv.org/abs/2009.03450v4) - [pdf](http://arxiv.org/pdf/2009.03450v4)

> Following the spread of the COVID-19 pandemic and pending the establishment of vaccination campaigns, several non pharmaceutical interventions such as partial and full lockdown, quarantine and measures of physical distancing have been imposed in order to reduce the spread of the disease and to lift the pressure on healthcare system. Mathematical models are important tools for estimating the impact of these interventions, for monitoring the current evolution of the epidemic at a national level and for estimating the potential long-term consequences of relaxation of measures. In this paper, we model the evolution of the COVID-19 epidemic in Belgium with a deterministic age-structured extended compartmental model. Our model takes special consideration for nursing homes which are modelled as separate entities from the general population in order to capture the specific delay and dynamics within these entities. The model integrates social contact data and is fitted on hospitalisations data (admission and discharge), on the daily number of COVID-19 deaths (with a distinction between general population and nursing home related deaths) and results from serological studies, with a sensitivity analysis based on a Bayesian approach. We present the situation as in November 2020 with the estimation of some characteristics of the COVID-19 deduced from the model. We also present several mid-term and long-term projections based on scenarios of reinforcement or relaxation of social contacts for different general sectors, with a lot of uncertainties remaining.

</details>

<details>

<summary>2021-08-30 12:51:22 - Adaptive treatment allocation and selection in multi-arm clinical trials: a Bayesian perspective</summary>

- *Elja Arjas, Dario Gasbarra*

- `2104.03398v2` - [abs](http://arxiv.org/abs/2104.03398v2) - [pdf](http://arxiv.org/pdf/2104.03398v2)

> Clinical trials are an instrument for making informed decisions based on evidence from well-designed experiments. Here we consider adaptive designs mainly from the perspective of multi-arm Phase II clinical trials, in which one or more experimental treatments are compared to a control. Treatment allocation of individual trial participants is assumed to take place according to a fixed block randomization, albeit with an important twist: The performance of each treatment arm is assessed after every measured outcome, in terms of the posterior distribution of a corresponding model parameter. Different treatments arms are then compared to each other, according to pre-defined criteria and using the joint posterior as the basis for such assessment. If a treatment is found to be sufficiently clearly inferior to the currently best candidate, it can be closed off either temporarily or permanently from further participant accrual. The latter possibility provides a method for adaptive treatment selection, including early stopping of the trial. The main development in the paper is in terms of binary outcomes, but some extensions, notably for handling time-to-event data, are discussed as well. The presentation is to a large extent comparative and expository.

</details>

<details>

<summary>2021-08-30 14:58:35 - Multi-Resolution Spatio-Temporal Prediction with Application to Wind Power Generation</summary>

- *Shixiang Zhu, Hanyu Zhang, Yao Xie, Pascal Van Hentenryck*

- `2108.13285v1` - [abs](http://arxiv.org/abs/2108.13285v1) - [pdf](http://arxiv.org/pdf/2108.13285v1)

> This paper proposes a spatio-temporal model for wind speed prediction which can be run at different resolutions. The model assumes that the wind prediction of a cluster is correlated to its upstream influences in recent history, and the correlation between clusters is represented by a directed dynamic graph. A Bayesian approach is also described in which prior beliefs about the predictive errors at different data resolutions are represented in a form of Gaussian processes. The joint framework enhances the predictive performance by combining results from predictions at different data resolution and provides reasonable uncertainty quantification. The model is evaluated on actual wind data from the Midwest U.S. and shows a superior performance compared to traditional baselines.

</details>

<details>

<summary>2021-08-30 14:59:10 - Bayesian Sensitivity Analysis for Missing Data Using the E-value</summary>

- *Wu Xue, Abbas Zaidi*

- `2108.13286v1` - [abs](http://arxiv.org/abs/2108.13286v1) - [pdf](http://arxiv.org/pdf/2108.13286v1)

> Sensitivity Analysis is a framework to assess how conclusions drawn from missing outcome data may be vulnerable to departures from untestable underlying assumptions. We extend the E-value, a popular metric for quantifying robustness of causal conclusions, to the setting of missing outcomes. With motivating examples from partially-observed Facebook conversion events, we present methodology for conducting Sensitivity Analysis at scale with three contributions. First, we develop a method for the Bayesian estimation of sensitivity parameters leveraging noisy benchmarks(e.g., aggregated reports for protecting unit-level privacy); both empirically derived subjective and objective priors are explored. Second, utilizing the Bayesian estimation of the sensitivity parameters we propose a mechanism for posterior inference of the E-value via simulation. Finally, closed form distributions of the E-value are constructed to make direct inference possible when posterior simulation is infeasible due to computational constraints. We demonstrate gains in performance over asymptotic inference of the E-value using data-based simulations, supplemented by a case-study of Facebook conversion events.

</details>

<details>

<summary>2021-08-30 17:48:18 - Dependent Bayesian nonparametric modeling of compositional data using random Bernstein polynomials</summary>

- *Claudia Wehrhahn, Andrés F. Barrientos, Alejandro Jara*

- `2108.13403v1` - [abs](http://arxiv.org/abs/2108.13403v1) - [pdf](http://arxiv.org/pdf/2108.13403v1)

> We discuss Bayesian nonparametric procedures for the regression analysis of compositional responses, that is, data supported on a multivariate simplex. The procedures are based on a modified class of multivariate Bernstein polynomials and on the use of dependent stick-breaking processes. A general model and two simplified versions of the general model are discussed. Appealing theoretical properties such as continuity, association structure, support, and consistency of the posterior distribution are established. Additionally, we exploit the use of spike-and-slab priors for choosing the version of the model that best adapts to the complexity of the underlying true data-generating distribution. The performance of the proposed model is illustrated in a simulation study and in an application to solid waste data from Colombia.

</details>

<details>

<summary>2021-08-30 19:30:24 - Bayesian Inference of Globular Cluster Properties Using Distribution Functions</summary>

- *Gwendolyn M. Eadie, Jeremy J. Webb, Jeffrey S. Rosenthal*

- `2108.13491v1` - [abs](http://arxiv.org/abs/2108.13491v1) - [pdf](http://arxiv.org/pdf/2108.13491v1)

> We present a Bayesian inference approach to estimating the cumulative mass profile and mean squared velocity profile of a globular cluster given the spatial and kinematic information of its stars. Mock globular clusters with a range of sizes and concentrations are generated from lowered isothermal dynamical models, from which we test the reliability of the Bayesian method to estimate model parameters through repeated statistical simulation. We find that given unbiased star samples, we are able to reconstruct the cluster parameters used to generate the mock cluster and the cluster's cumulative mass and mean velocity squared profiles with good accuracy. We further explore how strongly biased sampling, which could be the result of observing constraints, may affect this approach. Our tests indicate that if we instead have biased samples, then our estimates can be off in certain ways that are dependent on cluster morphology. Overall, our findings motivate obtaining samples of stars that are as unbiased as possible. This may be achieved by combining information from multiple telescopes (e.g., Hubble and Gaia), but will require careful modeling of the measurement uncertainties through a hierarchical model, which we plan to pursue in future work.

</details>

<details>

<summary>2021-08-31 02:24:25 - Multivariate Lévy Adaptive B-Spline Regression</summary>

- *Sewon Park, Jaeyong Lee*

- `2108.11863v3` - [abs](http://arxiv.org/abs/2108.11863v3) - [pdf](http://arxiv.org/pdf/2108.11863v3)

> We develop a fully Bayesian nonparametric regression model based on a L\'evy process prior named MLABS (Multivariate L\'evy Adaptive B-Spline regression) model, a multivariate version of the LARK (L\'evy Adaptive Regression Kernels) models, for estimating unknown functions with either varying degrees of smoothness or high interaction orders. L\'evy process priors have advantages of encouraging sparsity in the expansions and providing automatic selection over the number of basis functions. The unknown regression function is expressed as a weighted sum of tensor product of B-spline basis functions as the elements of an overcomplete system, which can deal with multi-dimensional data. The B-spline basis can express systematically functions with varying degrees of smoothness. By changing a set of degrees of the tensor product basis function, MLABS can adapt the smoothness of target functions due to the nice properties of B-spline bases. The local support of the B-spline basis enables the MLABS to make more delicate predictions than other existing methods in the two-dimensional surface data. Experiments on various simulated and real-world datasets illustrate that the MLABS model has comparable performance on regression and classification problems. We also show that the MLABS model has more stable and accurate predictive abilities than state-of-the-art nonparametric regression models in relatively low-dimensional data.

</details>

<details>

<summary>2021-08-31 08:11:02 - Self-Bounding Majority Vote Learning Algorithms by the Direct Minimization of a Tight PAC-Bayesian C-Bound</summary>

- *Paul Viallard, Pascal Germain, Amaury Habrard, Emilie Morvant*

- `2104.13626v2` - [abs](http://arxiv.org/abs/2104.13626v2) - [pdf](http://arxiv.org/pdf/2104.13626v2)

> In the PAC-Bayesian literature, the C-Bound refers to an insightful relation between the risk of a majority vote classifier (under the zero-one loss) and the first two moments of its margin (i.e., the expected margin and the voters' diversity). Until now, learning algorithms developed in this framework minimize the empirical version of the C-Bound, instead of explicit PAC-Bayesian generalization bounds. In this paper, by directly optimizing PAC-Bayesian guarantees on the C-Bound, we derive self-bounding majority vote learning algorithms. Moreover, our algorithms based on gradient descent are scalable and lead to accurate predictors paired with non-vacuous guarantees.

</details>

<details>

<summary>2021-08-31 16:37:43 - srMO-BO-3GP: A sequential regularized multi-objective constrained Bayesian optimization for design applications</summary>

- *Anh Tran, Mike Eldred, Scott McCann, Yan Wang*

- `2007.03502v3` - [abs](http://arxiv.org/abs/2007.03502v3) - [pdf](http://arxiv.org/pdf/2007.03502v3)

> Bayesian optimization (BO) is an efficient and flexible global optimization framework that is applicable to a very wide range of engineering applications. To leverage the capability of the classical BO, many extensions, including multi-objective, multi-fidelity, parallelization, latent-variable model, have been proposed to improve the limitation of the classical BO framework. In this work, we propose a novel multi-objective (MO) extension, called srMO-BO-3GP, to solve the MO optimization problems in a sequential setting. Three different Gaussian processes (GPs) are stacked together, where each of the GP is assigned with a different task: the first GP is used to approximate the single-objective function, the second GP is used to learn the unknown constraints, and the third GP is used to learn the uncertain Pareto frontier. At each iteration, a MO augmented Tchebycheff function converting MO to single-objective is adopted and extended with a regularized ridge term, where the regularization is introduced to smoothen the single-objective function. Finally, we couple the third GP along with the classical BO framework to promote the richness and diversity of the Pareto frontier by the exploitation and exploration acquisition function. The proposed framework is demonstrated using several numerical benchmark functions, as well as a thermomechanical finite element model for flip-chip package design optimization.

</details>

<details>

<summary>2021-08-31 17:30:08 - Bayesian learning of forest and tree graphical models</summary>

- *Edmund Jones*

- `2108.13992v1` - [abs](http://arxiv.org/abs/2108.13992v1) - [pdf](http://arxiv.org/pdf/2108.13992v1)

> In Bayesian learning of Gaussian graphical model structure, it is common to restrict attention to certain classes of graphs and approximate the posterior distribution by repeatedly moving from one graph to another, using MCMC or methods such as stochastic shotgun search (SSS). I give two corrected versions of an algorithm for non-decomposable graphs and discuss random graph distributions, in particular as prior distributions. The main topic of the thesis is Bayesian structure-learning with forests or trees. Restricting attention to these graphs can be justified using theorems on random graphs. I describe how to use the Chow$\unicode{x2013}$Liu algorithm and the Matrix Tree Theorem to find the MAP forest and certain quantities in the posterior distribution on trees. I give adapted versions of MCMC and SSS for approximating the posterior distribution for forests and trees, and systems for storing these graphs so that it is easy to choose moves to neighbouring graphs. Experiments show that SSS with trees does well when the true graph is a tree or sparse graph. SSS with trees or forests does better than SSS with decomposable graphs in certain cases. Graph priors improve detection of hubs but need large ranges of probabilities. MCMC on forests fails to mix well and MCMC on trees is slower than SSS. (For a longer abstract see the thesis.)

</details>

<details>

<summary>2021-08-31 17:34:53 - Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning: Analysis of the Black-Box Optimization Challenge 2020</summary>

- *Ryan Turner, David Eriksson, Michael McCourt, Juha Kiili, Eero Laaksonen, Zhen Xu, Isabelle Guyon*

- `2104.10201v2` - [abs](http://arxiv.org/abs/2104.10201v2) - [pdf](http://arxiv.org/pdf/2104.10201v2)

> This paper presents the results and insights from the black-box optimization (BBO) challenge at NeurIPS 2020 which ran from July-October, 2020. The challenge emphasized the importance of evaluating derivative-free optimizers for tuning the hyperparameters of machine learning models. This was the first black-box optimization challenge with a machine learning emphasis. It was based on tuning (validation set) performance of standard machine learning models on real datasets. This competition has widespread impact as black-box optimization (e.g., Bayesian optimization) is relevant for hyperparameter tuning in almost every machine learning project as well as many applications outside of machine learning. The final leaderboard was determined using the optimization performance on held-out (hidden) objective functions, where the optimizers ran without human intervention. Baselines were set using the default settings of several open-source black-box optimization packages as well as random search.

</details>

<details>

<summary>2021-08-31 18:02:18 - Preferential Batch Bayesian Optimization</summary>

- *Eero Siivola, Akash Kumar Dhaka, Michael Riis Andersen, Javier Gonzalez, Pablo Garcia Moreno, Aki Vehtari*

- `2003.11435v3` - [abs](http://arxiv.org/abs/2003.11435v3) - [pdf](http://arxiv.org/pdf/2003.11435v3)

> Most research in Bayesian optimization (BO) has focused on \emph{direct feedback} scenarios, where one has access to exact values of some expensive-to-evaluate objective. This direction has been mainly driven by the use of BO in machine learning hyper-parameter configuration problems. However, in domains such as modelling human preferences, A/B tests, or recommender systems, there is a need for methods that can replace direct feedback with \emph{preferential feedback}, obtained via rankings or pairwise comparisons. In this work, we present preferential batch Bayesian optimization (PBBO), a new framework that allows finding the optimum of a latent function of interest, given any type of parallel preferential feedback for a group of two or more points. We do so by using a Gaussian process model with a likelihood specially designed to enable parallel and efficient data collection mechanisms, which are key in modern machine learning. We show how the acquisitions developed under this framework generalize and augment previous approaches in Bayesian optimization, expanding the use of these techniques to a wider range of domains. An extensive simulation study shows the benefits of this approach, both with simulated functions and four real data sets.

</details>

<details>

<summary>2021-08-31 18:30:07 - Approximate Bayesian Optimisation for Neural Networks</summary>

- *Nadhir Hassen, Irina Rish*

- `2108.12461v2` - [abs](http://arxiv.org/abs/2108.12461v2) - [pdf](http://arxiv.org/pdf/2108.12461v2)

> A body of work has been done to automate machine learning algorithm to highlight the importance of model choice. Automating the process of choosing the best forecasting model and its corresponding parameters can result to improve a wide range of real-world applications. Bayesian optimisation (BO) uses a blackbox optimisation methods to propose solutions according to an exploration-exploitation trade-off criterion through acquisition functions. BO framework imposes two key ingredients: a probabilistic surrogate model that consist of prior belief of the unknown objective function(data-dependant) and an objective function that describes how optimal is the model-fit. Choosing the best model and its associated hyperparameters can be very expensive, and is typically fit using Gaussian processes (GPs) and at some extends applying approximate inference due its intractability. However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations. In addition, most real-dataset are non-stationary which make idealistic assumptions on surrogate models. The necessity to solve the analytical tractability and the computational feasibility in a stochastic fashion enables to ensure the efficiency and the applicability of Bayesian optimisation. In this paper we explore the use of neural networks as an alternative to GPs to model distributions over functions, we provide a link between density-ratio estimation and class probability estimation based on approximate inference, this reformulation provides algorithm efficiency and tractability.

</details>


## 2021-09

<details>

<summary>2021-09-01 00:21:09 - Empirical Framework for Cournot Oligopoly with Private Information</summary>

- *Gaurab Aryal, Federico Zincenko*

- `2106.15035v2` - [abs](http://arxiv.org/abs/2106.15035v2) - [pdf](http://arxiv.org/pdf/2106.15035v2)

> We propose an empirical framework for Cournot oligopoly with private information about costs. First, considering a linear demand with a random intercept, we characterize the Bayesian Cournot-Nash equilibrium and determine its testable implications. Then we establish nonparametric identification of the joint distribution of demand and market-specific technology shock, and then firm-specific cost distributions. Following the identification steps, we propose a likelihood-based estimation method, and for illustration, apply it to the global upstream market for crude oil. We also extend the baseline model to include either conduct parameters, nonlinear demand, or selective entry.

</details>

<details>

<summary>2021-09-01 12:05:06 - Variational Inference and Sparsity in High-Dimensional Deep Gaussian Mixture Models</summary>

- *Lucas Kock, Nadja Klein, David J. Nott*

- `2105.01496v2` - [abs](http://arxiv.org/abs/2105.01496v2) - [pdf](http://arxiv.org/pdf/2105.01496v2)

> Gaussian mixture models are a popular tool for model-based clustering, and mixtures of factor analyzers are Gaussian mixture models having parsimonious factor covariance structure for mixture components. There are several recent extensions of mixture of factor analyzers to deep mixtures, where the Gaussian model for the latent factors is replaced by a mixture of factor analyzers. This construction can be iterated to obtain a model with many layers. These deep models are challenging to fit, and we consider Bayesian inference using sparsity priors to further regularize the estimation. A scalable natural gradient variational inference algorithm is developed for fitting the model, and we suggest computationally efficient approaches to the architecture choice using overfitted mixtures where unnecessary components drop out in the estimation. In a number of simulated and two real examples, we demonstrate the versatility of our approach for high-dimensional problems, and demonstrate that the use of sparsity inducing priors can be helpful for obtaining improved clustering results.

</details>

<details>

<summary>2021-09-01 13:30:58 - A truncated mean-parameterised Conway-Maxwell-Poisson model for the analysis of Test match bowlers</summary>

- *Pete Philipson*

- `2109.00378v1` - [abs](http://arxiv.org/abs/2109.00378v1) - [pdf](http://arxiv.org/pdf/2109.00378v1)

> Assessing the relative merits of sportsmen and women whose careers took place far apart in time via a suitable statistical model is a complex task as any comparison is compromised by fundamental changes to the sport and society and often handicapped by the popularity of inappropriate traditional metrics. In this work we focus on cricket and the ranking of Test match bowlers using bowling data from the first Test in 1877 onwards. A truncated, mean-parameterised Conway-Maxwell-Poisson model is developed to handle the under- and overdispersed nature of the data, which are in the form of small counts, and to extract the innate ability of individual bowlers. Inferences are made using a Bayesian approach by deploying a Markov Chain Monte Carlo algorithm to obtain parameter estimates and confidence intervals. The model offers a good fit and indicates that the commonly used bowling average is a flawed measure.

</details>

<details>

<summary>2021-09-01 16:09:55 - Bayesian data combination model with Gaussian process latent variable model for mixed observed variables under NMAR missingness</summary>

- *Masaki Mitsuhiro, Takahiro Hoshino*

- `2109.00462v1` - [abs](http://arxiv.org/abs/2109.00462v1) - [pdf](http://arxiv.org/pdf/2109.00462v1)

> In the analysis of observational data in social sciences and businesses, it is difficult to obtain a "(quasi) single-source dataset" in which the variables of interest are simultaneously observed. Instead, multiple-source datasets are typically acquired for different individuals or units. Various methods have been proposed to investigate the relationship between the variables in each dataset, e.g., matching and latent variable modeling. It is necessary to utilize these datasets as a single-source dataset with missing variables. Existing methods assume that the datasets to be integrated are acquired from the same population or that the sampling depends on covariates. This assumption is referred to as missing at random (MAR) in terms of missingness. However, as will been shown in application studies, it is likely that this assumption does not hold in actual data analysis and the results obtained may be biased. We propose a data fusion method that does not assume that datasets are homogenous. We use a Gaussian process latent variable model for non-MAR missing data. This model assumes that the variables of concern and the probability of being missing depend on latent variables. A simulation study and real-world data analysis show that the proposed method with a missing-data mechanism and the latent Gaussian process yields valid estimates, whereas an existing method provides severely biased estimates. This is the first study in which non-random assignment to datasets is considered and resolved under resonable assumptions in data fusion problem.

</details>

<details>

<summary>2021-09-01 16:29:21 - Physics-integrated hybrid framework for model form error identification in nonlinear dynamical systems</summary>

- *Shailesh Garg, Souvik Chakraborty, Budhaditya Hazra*

- `2109.00538v1` - [abs](http://arxiv.org/abs/2109.00538v1) - [pdf](http://arxiv.org/pdf/2109.00538v1)

> For real-life nonlinear systems, the exact form of nonlinearity is often not known and the known governing equations are often based on certain assumptions and approximations. Such representation introduced model-form error into the system. In this paper, we propose a novel gray-box modeling approach that not only identifies the model-form error but also utilizes it to improve the predictive capability of the known but approximate governing equation. The primary idea is to treat the unknown model-form error as a residual force and estimate it using duel Bayesian filter based joint input-state estimation algorithms. For improving the predictive capability of the underlying physics, we first use machine learning algorithm to learn a mapping between the estimated state and the input (model-form error) and then introduce it into the governing equation as an additional term. This helps in improving the predictive capability of the governing physics and allows the model to generalize to unseen environment. Although in theory, any machine learning algorithm can be used within the proposed framework, we use Gaussian process in this work. To test the performance of proposed framework, case studies discussing four different dynamical systems are discussed; results for which indicate that the framework is applicable to a wide variety of systems and can produce reliable estimates of original system's states.

</details>

<details>

<summary>2021-09-01 22:05:56 - A Bayesian Evaluation Framework for Subjectively Annotated Visual Recognition Tasks</summary>

- *Derek S. Prijatelj, Mel McCurrie, Walter J. Scheirer*

- `2007.06711v2` - [abs](http://arxiv.org/abs/2007.06711v2) - [pdf](http://arxiv.org/pdf/2007.06711v2)

> An interesting development in automatic visual recognition has been the emergence of tasks where it is not possible to assign objective labels to images, yet still feasible to collect annotations that reflect human judgements about them. Machine learning-based predictors for these tasks rely on supervised training that models the behavior of the annotators, i.e., what would the average person's judgement be for an image? A key open question for this type of work, especially for applications where inconsistency with human behavior can lead to ethical lapses, is how to evaluate the epistemic uncertainty of trained predictors, i.e., the uncertainty that comes from the predictor's model. We propose a Bayesian framework for evaluating black box predictors in this regime, agnostic to the predictor's internal structure. The framework specifies how to estimate the epistemic uncertainty that comes from the predictor with respect to human labels by approximating a conditional distribution and producing a credible interval for the predictions and their measures of performance. The framework is successfully applied to four image classification tasks that use subjective human judgements: facial beauty assessment, social attribute assignment, apparent age estimation, and ambiguous scene labeling.

</details>

<details>

<summary>2021-09-02 01:12:44 - Bayesian Detectability of Induced Polarisation in Airborne Electromagnetic Data using Reversible Jump Sequential Monte Carlo</summary>

- *Laurence Davies, Alan Yusen Ley-Cooper, Matthew Sutton, Christopher Drovandi*

- `2109.00661v1` - [abs](http://arxiv.org/abs/2109.00661v1) - [pdf](http://arxiv.org/pdf/2109.00661v1)

> Detection of induced polarisation (IP) effects in airborne electromagnetic (AEM) measurements does not yet have an established methodology. This contribution develops a Bayesian approach to the IP-detectability problem using decoupled transdimensional layered models, and applies an approach novel to geophysics whereby transdimensional proposals are used within the embarrassingly parallelisable and robust static Sequential Monte Carlo (SMC) class of algorithms for the simultaneous inference of parameters and models. Henceforth referring to this algorithm as Reversible Jump Sequential Monte Carlo (RJSMC), the statistical methodological contributions to the algorithm account for adaptivity considerations for multiple models and proposal types, especially surrounding particle impoverishment in unlikely models. Methodological contributions to solid Earth geophysics include the decoupled model approach and proposal of a statistic that use posterior model odds for IP detectability. A case study is included investigating detectability of IP effects in AEM data at a broad scale.

</details>

<details>

<summary>2021-09-02 05:30:57 - Multi-agent Bayesian Learning with Best Response Dynamics: Convergence and Stability</summary>

- *Manxi Wu, Saurabh Amin, Asuman Ozdaglar*

- `2109.00719v1` - [abs](http://arxiv.org/abs/2109.00719v1) - [pdf](http://arxiv.org/pdf/2109.00719v1)

> We study learning dynamics induced by strategic agents who repeatedly play a game with an unknown payoff-relevant parameter. In this dynamics, a belief estimate of the parameter is repeatedly updated given players' strategies and realized payoffs using Bayes's rule. Players adjust their strategies by accounting for best response strategies given the belief. We show that, with probability 1, beliefs and strategies converge to a fixed point, where the belief consistently estimates the payoff distribution for the strategy, and the strategy is an equilibrium corresponding to the belief. However, learning may not always identify the unknown parameter because the belief estimate relies on the game outcomes that are endogenously generated by players' strategies. We obtain sufficient and necessary conditions, under which learning leads to a globally stable fixed point that is a complete information Nash equilibrium. We also provide sufficient conditions that guarantee local stability of fixed point beliefs and strategies.

</details>

<details>

<summary>2021-09-02 16:53:04 - Bayesian mixture autoregressive model with Student's t innovations</summary>

- *Davide Ravagli, Georgi N. Boshnakov*

- `2109.01083v1` - [abs](http://arxiv.org/abs/2109.01083v1) - [pdf](http://arxiv.org/pdf/2109.01083v1)

> This paper introduces a fully Bayesian analysis of mixture autoregressive models with Student t components. With the capacity of capturing the behaviour in the tails of the distribution, the Student t MAR model provides a more flexible modelling framework than its Gaussian counterpart, leading to fitted models with fewer parameters and of easier interpretation. The degrees of freedom are also treated as random variables, and hence are included in the estimation process.

</details>

<details>

<summary>2021-09-02 16:58:52 - Compromise, Don't Optimize: Generalizing Perfect Bayesian Equilibrium to Allow for Ambiguity</summary>

- *Karl Schlag, Andriy Zapechelnyuk*

- `2003.02539v2` - [abs](http://arxiv.org/abs/2003.02539v2) - [pdf](http://arxiv.org/pdf/2003.02539v2)

> We introduce a solution concept for extensive-form games of incomplete information in which players need not assign likelihoods to what they do not know about the game. This is embedded in a model in which players can hold multiple priors. Players make choices by looking for compromises that yield a good performance under each of their updated priors. Our solution concept is called perfect compromise equilibrium. It generalizes perfect Bayesian equilibrium. We show how it deals with ambiguity in Cournot and Bertrand markets, public good provision, Spence's job market signaling, bilateral trade with common value, and forecasting.

</details>

<details>

<summary>2021-09-02 21:25:51 - Robust confidence distributions from proper scoring rules</summary>

- *Erlis Ruli, Laura Ventura, Monica Musio*

- `2109.01219v1` - [abs](http://arxiv.org/abs/2109.01219v1) - [pdf](http://arxiv.org/pdf/2109.01219v1)

> A confidence distribution is a distribution for a parameter of interest based on a parametric statistical model. As such, it serves the same purpose for frequentist statisticians as a posterior distribution for Bayesians, since it allows to reach point estimates, to assess their precision, to set up tests along with measures of evidence, to derive confidence intervals, comparing the parameter of interest with other parameters from other studies, etc. A general recipe for deriving confidence distributions is based on classical pivotal quantities and their exact or approximate distributions.   However, in the presence of model misspecifications or outlying values in the observed data, classical pivotal quantities, and thus confidence distributions, may be inaccurate. The aim of this paper is to discuss the derivation and application of robust confidence distributions. In particular, we discuss a general approach based on the Tsallis scoring rule in order to compute a robust confidence distribution. Examples and simulation results are discussed for some problems often encountered in practice, such as the two-sample heteroschedastic comparison, the receiver operating characteristic curves and regression models.

</details>

<details>

<summary>2021-09-03 14:18:43 - Variational Bayes algorithm and posterior consistency of Ising model parameter estimation</summary>

- *Minwoo Kim, Shrijita Bhattacharya, Tapabrata Maiti*

- `2109.01548v1` - [abs](http://arxiv.org/abs/2109.01548v1) - [pdf](http://arxiv.org/pdf/2109.01548v1)

> Ising models originated in statistical physics and are widely used in modeling spatial data and computer vision problems. However, statistical inference of this model remains challenging due to intractable nature of the normalizing constant in the likelihood. Here, we use a pseudo-likelihood instead to study the Bayesian estimation of two-parameter, inverse temperature, and magnetization, Ising model with a fully specified coupling matrix. We develop a computationally efficient variational Bayes procedure for model estimation. Under the Gaussian mean-field variational family, we derive posterior contraction rates of the variational posterior obtained under the pseudo-likelihood. We also discuss the loss incurred due to variational posterior over true posterior for the pseudo-likelihood approach. Extensive simulation studies validate the efficacy of mean-field Gaussian and bivariate Gaussian families as the possible choices of the variational family for inference of Ising model parameters.

</details>

<details>

<summary>2021-09-03 16:23:33 - Affine-Mapping based Variational Ensemble Kalman Filter</summary>

- *Linjie Wen, Jinglai Li*

- `2103.06315v4` - [abs](http://arxiv.org/abs/2103.06315v4) - [pdf](http://arxiv.org/pdf/2103.06315v4)

> We propose an affine-mapping based variational Ensemble Kalman filter for sequential Bayesian filtering problems with generic observation models. Specifically, the proposed method is formulated as to construct an affine mapping from the prior ensemble to the posterior one, and the affine mapping is computed via a variational Bayesian formulation, i.e., by minimizing the Kullback-Leibler divergence between the transformed distribution through the affine mapping and the actual posterior. Some theoretical properties of resulting optimization problem are studied and a gradient descent scheme is proposed to solve the resulting optimization problem. With numerical examples we demonstrate that the method has competitive performance against existing methods.

</details>

<details>

<summary>2021-09-03 16:25:09 - Simultaneous quantification and changepoint detection of point source gas emissions using recursive Bayesian inference</summary>

- *Amir Montazeri, Xiaochi Zhou, John D. Albertson*

- `2109.01603v1` - [abs](http://arxiv.org/abs/2109.01603v1) - [pdf](http://arxiv.org/pdf/2109.01603v1)

> Recent findings suggest that abnormal operating conditions of equipment in the oil and gas supply chain represent a large fraction of anthropogenic methane emissions. Thus, effective mitigation of emissions necessitates rapid identification and repair of sources caused by faulty equipment. In addition to advances in sensing technology that allow for more frequent surveillance, prompt and cost-effective identification of sources requires computational frameworks that provide automatic fault detection. Here, we present a changepoint detection algorithm based on a recursive Bayesian scheme that allows for simultaneous emission rate estimation and fault detection. The proposed algorithm is tested on a series of near-field controlled release mobile experiments, with promising results demonstrating successful detection (>90% success rate) of changes in the leak rate when the emission rate is tripled after an abrupt change. Moreover, we show that the statistics of the measurements, such as the coefficient of variation and range are good predictors of the performance of the algorithm. Finally, we describe how this methodology can be easily adapted to suit time-averaged concentration data measured by stationary sensors, thus showcasing its flexibility.

</details>

<details>

<summary>2021-09-03 20:51:13 - Bayesian Estimation of the Degrees of Freedom Parameter of the Student-$t$ Distribution---A Beneficial Re-parameterization</summary>

- *Darjus Hosszejni*

- `2109.01726v1` - [abs](http://arxiv.org/abs/2109.01726v1) - [pdf](http://arxiv.org/pdf/2109.01726v1)

> In this paper, conditional data augmentation (DA) is investigated for the degrees of freedom parameter $\nu$ of a Student-$t$ distribution. Based on a restricted version of the expected augmented Fisher information, it is conjectured that the ancillarity DA is progressively more efficient for MCMC estimation than the sufficiency DA as $\nu$ increases; with the break even point lying at as low as $\nu\approx4$. The claim is examined further and generalized through a large simulation study and a application to U.S. macroeconomic time series. Finally, the ancillarity-sufficiency interweaving strategy is empirically shown to combine the benefits of both DAs. The proposed algorithm may set a new standard for estimating $\nu$ as part of any model.

</details>

<details>

<summary>2021-09-03 23:18:35 - Using BART to Perform Pareto Optimization and Quantify its Uncertainties</summary>

- *Akira Horiguchi, Thomas J. Santner, Ying Sun, Matthew T. Pratola*

- `2101.02558v2` - [abs](http://arxiv.org/abs/2101.02558v2) - [pdf](http://arxiv.org/pdf/2101.02558v2)

> Techniques to reduce the energy burden of an industrial ecosystem often require solving a multiobjective optimization problem. However, collecting experimental data can often be either expensive or time-consuming. In such cases, statistical methods can be helpful. This article proposes Pareto Front (PF) and Pareto Set (PS) estimation methods using Bayesian Additive Regression Trees (BART), which is a non-parametric model whose assumptions are typically less restrictive than popular alternatives, such as Gaussian Processes (GPs). These less restrictive assumptions allow BART to handle scenarios (e.g. high-dimensional input spaces, nonsmooth responses, large datasets) that GPs find difficult. The performance of our BART-based method is compared to a GP-based method using analytic test functions, demonstrating convincing advantages. Finally, our BART-based methodology is applied to a motivating engineering problem. Supplementary materials, which include a theorem proof, algorithms, and R code, for this article are available online.

</details>

<details>

<summary>2021-09-04 03:38:49 - Schr{ö}dinger-F{ö}llmer Sampler: Sampling without Ergodicity</summary>

- *Jian Huang, Yuling Jiao, Lican Kang, Xu Liao, Jin Liu, Yanyan Liu*

- `2106.10880v3` - [abs](http://arxiv.org/abs/2106.10880v3) - [pdf](http://arxiv.org/pdf/2106.10880v3)

> Sampling from probability distributions is an important problem in statistics and machine learning, specially in Bayesian inference when integration with respect to posterior distribution is intractable and sampling from the posterior is the only viable option for inference. In this paper, we propose Schr\"{o}dinger-F\"{o}llmer sampler (SFS), a novel approach for sampling from possibly unnormalized distributions. The proposed SFS is based on the Schr\"{o}dinger-F\"{o}llmer diffusion process on the unit interval with a time dependent drift term, which transports the degenerate distribution at time zero to the target distribution at time one. Comparing with the existing Markov chain Monte Carlo samplers that require ergodicity, no such requirement is needed for SFS. Computationally, SFS can be easily implemented using the Euler-Maruyama discretization. In theoretical analysis, we establish non-asymptotic error bounds for the sampling distribution of SFS in the Wasserstein distance under suitable conditions. We conduct numerical experiments to evaluate the performance of SFS and demonstrate that it is able to generate samples with better quality than several existing methods.

</details>

<details>

<summary>2021-09-04 14:51:26 - A Bayesian semi-parametric approach for modeling memory decay in dynamic social networks</summary>

- *Giuseppe Arena, Joris Mulder, Roger Th. A. J. Leenders*

- `2109.01881v1` - [abs](http://arxiv.org/abs/2109.01881v1) - [pdf](http://arxiv.org/pdf/2109.01881v1)

> In relational event networks, the tendency for actors to interact with each other depends greatly on the past interactions between the actors in a social network. Both the quantity of past interactions and the time that elapsed since the past interactions occurred affect the actors' decision-making to interact with other actors in the network. Recently occurred events generally have a stronger influence on current interaction behavior than past events that occurred a long time ago--a phenomenon known as "memory decay". Previous studies either predefined a short-run and long-run memory or fixed a parametric exponential memory using a predefined half-life period. In real-life relational event networks however it is generally unknown how the memory of actors about the past events fades as time goes by. For this reason it is not recommendable to fix this in an ad hoc manner, but instead we should learn the shape of memory decay from the observed data. In this paper, a novel semi-parametric approach based on Bayesian Model Averaging is proposed for learning the shape of the memory decay without requiring any parametric assumptions. The method is applied to relational event history data among socio-political actors in India.

</details>

<details>

<summary>2021-09-05 03:19:02 - Bayesian Time Varying Coefficient Model with Applications to Marketing Mix Modeling</summary>

- *Edwin Ng, Zhishi Wang, Athena Dai*

- `2106.03322v3` - [abs](http://arxiv.org/abs/2106.03322v3) - [pdf](http://arxiv.org/pdf/2106.03322v3)

> Both Bayesian and varying coefficient models are very useful tools in practice as they can be used to model parameter heterogeneity in a generalizable way. Motivated by the need of enhancing Marketing Mix Modeling at Uber, we propose a Bayesian Time Varying Coefficient model, equipped with a hierarchical Bayesian structure. This model is different from other time varying coefficient models in the sense that the coefficients are weighted over a set of local latent variables following certain probabilistic distributions. Stochastic Variational Inference is used to approximate the posteriors of latent variables and dynamic coefficients. The proposed model also helps address many challenges faced by traditional MMM approaches. We used simulations as well as real world marketing datasets to demonstrate our model superior performance in terms of both accuracy and interpretability.

</details>

<details>

<summary>2021-09-05 09:33:34 - An empirical Bayes Approach to stochastic blockmodels and graphons: shrinkage estimation and model selection</summary>

- *Zhanhao Peng, Qing Zhou*

- `2006.07435v2` - [abs](http://arxiv.org/abs/2006.07435v2) - [pdf](http://arxiv.org/pdf/2006.07435v2)

> The graphon (W-graph), including the stochastic block model as a special case, has been widely used in modeling and analyzing network data. This random graph model is well-characterized by its graphon function, and estimation of the graphon function has gained a lot of recent research interests. Most existing works focus on community detection in the latent space of the model, while adopting simple maximum likelihood or Bayesian estimates for the graphon or connectivity parameters given the identified communities. In this work, we propose a hierarchical Binomial model and develop a novel empirical Bayes estimate of the connectivity matrix of a stochastic block model to approximate the graphon function. Based on the likelihood of our hierarchical model, we further introduce a model selection criterion for choosing the number of communities. Numerical results on extensive simulations and two well-annotated social networks demonstrate the superiority of our approach in terms of estimation accuracy and model selection.

</details>

<details>

<summary>2021-09-05 19:11:33 - Robust Importance Sampling for Error Estimation in the Context of Optimal Bayesian Transfer Learning</summary>

- *Omar Maddouri, Xiaoning Qian, Francis J. Alexander, Edward R. Dougherty, Byung-Jun Yoon*

- `2109.02150v1` - [abs](http://arxiv.org/abs/2109.02150v1) - [pdf](http://arxiv.org/pdf/2109.02150v1)

> Classification has been a major task for building intelligent systems as it enables decision-making under uncertainty. Classifier design aims at building models from training data for representing feature-label distributions--either explicitly or implicitly. In many scientific or clinical settings, training data are typically limited, which makes designing accurate classifiers and evaluating their classification error extremely challenging. While transfer learning (TL) can alleviate this issue by incorporating data from relevant source domains to improve learning in a different target domain, it has received little attention for performance assessment, notably in error estimation. In this paper, we fill this gap by investigating knowledge transferability in the context of classification error estimation within a Bayesian paradigm. We introduce a novel class of Bayesian minimum mean-square error (MMSE) estimators for optimal Bayesian transfer learning (OBTL), which enables rigorous evaluation of classification error under uncertainty in a small-sample setting. Using Monte Carlo importance sampling, we employ the proposed estimator to evaluate the classification accuracy of a broad family of classifiers that span diverse learning capabilities. Experimental results based on both synthetic data as well as real-world RNA sequencing (RNA-seq) data show that our proposed OBTL error estimation scheme clearly outperforms standard error estimators, especially in a small-sample setting, by tapping into the data from other relevant domains.

</details>

<details>

<summary>2021-09-05 21:52:01 - Dimensional Analysis in Statistical Modelling</summary>

- *Tae Yoon Lee, James V. Zidek, Nancy Heckman*

- `2002.11259v2` - [abs](http://arxiv.org/abs/2002.11259v2) - [pdf](http://arxiv.org/pdf/2002.11259v2)

> Building on recent work in statistical science, the paper presents a theory for modelling natural phenomena that unifies physical and statistical paradigms based on the underlying principle that a model must be nondimensionalizable. After all, such phenomena cannot depend on how the experimenter chooses to assess them. Yet the model itself must be comprised of quantities that can be determined theoretically or empirically. Hence, the underlying principle requires that the model represents these natural processes correctly no matter what scales and units of measurement are selected. This goal was realized for physical modelling through the celebrated theories of Buckingham and Bridgman and for statistical modellers through the invariance principle of Hunt and Stein. Building on recent research in statistical science, the paper shows how the latter can embrace and extend the former. The invariance principle is extended to encompass the Bayesian paradigm, thereby enabling an assessment of model uncertainty. The paper covers topics not ordinarily seen in statistical science regarding dimensions, scales, and units of quantities in statistical modelling. It shows the special difficulties that can arise when models involve transcendental functions, such as the logarithm which is used e.g. in likelihood analysis and is a singularity in the family of Box-Cox family of transformations. Further, it demonstrates the importance of the scale of measurement, in particular how differently modellers must handle ratio- and interval-scales

</details>

<details>

<summary>2021-09-05 23:00:18 - The problem of perfect predictors in statistical spike train models</summary>

- *Sahand Farhoodi, Uri Eden*

- `2102.00574v3` - [abs](http://arxiv.org/abs/2102.00574v3) - [pdf](http://arxiv.org/pdf/2102.00574v3)

> Generalized Linear Models (GLMs) have been used extensively in statistical models of spike train data. However, the maximum likelihood estimates of the model parameters and their uncertainty, can be challenging to compute in situations where response and non-response can be separated by a single predictor or a linear combination of multiple predictors. Such situations are likely to arise in many neural systems due to properties such as refractoriness and incomplete sampling of the signals that influence spiking. In this paper, we describe multiple classes of approaches to address this problem: using an optimization algorithm with a fixed iteration limit, computing the maximum likelihood solution in the limit, Bayesian estimation, regularization, change of basis, and modifying the search parameters. We demonstrate a specific application of each of these methods to spiking data from rat somatosensory cortex and discuss the advantages and disadvantages of each. We also provide an example of a roadmap for selecting a method based on the problem's particular analysis issues and scientific goals.

</details>

<details>

<summary>2021-09-06 06:39:44 - Adaptive semiparametric Bayesian differential equations via sequential Monte Carlo</summary>

- *Shijia Wang, Shufei Ge, Renny Doig, Liangliang Wang*

- `2002.02571v2` - [abs](http://arxiv.org/abs/2002.02571v2) - [pdf](http://arxiv.org/pdf/2002.02571v2)

> Nonlinear differential equations (DEs) are used in a wide range of scientific problems to model complex dynamic systems. The differential equations often contain unknown parameters that are of scientific interest, which have to be estimated from noisy measurements of the dynamic system. Generally, there is no closed-form solution for nonlinear DEs, and the likelihood surface for the parameter of interest is multi-modal and very sensitive to different parameter values. We propose a Bayesian framework for nonlinear DE systems. A flexible nonparametric function is used to represent the dynamic process such that expensive numerical solvers can be avoided. A sequential Monte Carlo algorithm in the annealing framework is proposed to conduct Bayesian inference for parameters in DEs. In our numerical experiments, we use examples of ordinary differential equations and delay differential equations to demonstrate the effectiveness of the proposed algorithm. We developed an R package that is available at \url{https://github.com/shijiaw/smcDE}.

</details>

<details>

<summary>2021-09-06 07:36:54 - Post-Processing of MCMC</summary>

- *Leah F. South, Marina Riabiz, Onur Teymur, Chris. J. Oates*

- `2103.16048v3` - [abs](http://arxiv.org/abs/2103.16048v3) - [pdf](http://arxiv.org/pdf/2103.16048v3)

> Markov chain Monte Carlo (MCMC) is the engine of modern Bayesian statistics, being used to approximate the posterior and derived quantities of interest. Despite this, the issue of how the output from a Markov chain is post-processed and reported is often overlooked. Convergence diagnostics can be used to control bias via burn-in removal, but these do not account for (common) situations where a limited computational budget engenders a bias-variance trade-off. The aim of this article is to review state-of-the-art techniques for post-processing Markov chain output. Our review covers methods based on discrepancy minimisation, which directly address the bias-variance trade-off, as well as general-purpose control variate methods for approximating expected quantities of interest.

</details>

<details>

<summary>2021-09-06 08:32:39 - Selection of Summary Statistics for Network Model Choice with Approximate Bayesian Computation</summary>

- *Louis Raynal, Jukka-Pekka Onnela*

- `2101.07766v2` - [abs](http://arxiv.org/abs/2101.07766v2) - [pdf](http://arxiv.org/pdf/2101.07766v2)

> Approximate Bayesian Computation (ABC) now serves as one of the major strategies to perform model choice and parameter inference on models with intractable likelihoods. An essential component of ABC involves comparing a large amount of simulated data with the observed data through summary statistics. To avoid the curse of dimensionality, summary statistic selection is of prime importance, and becomes even more critical when applying ABC to mechanistic network models. Indeed, while many summary statistics can be used to encode network structures, their computational complexity can be highly variable. For large networks, computation of summary statistics can quickly create a bottleneck, making the use of ABC difficult. To reduce this computational burden and make the analysis of mechanistic network models more practical, we investigated two questions in a model choice framework. First, we studied the utility of cost-based filter selection methods to account for different summary costs during the selection process. Second, we performed selection using networks generated with a smaller number of nodes to reduce the time required for the selection step. Our findings show that computationally inexpensive summary statistics can be efficiently selected with minimal impact on classification accuracy. Furthermore, we found that networks with a smaller number of nodes can only be employed to eliminate a moderate number of summaries. While this latter finding is network specific, the former is general and can be adapted to any ABC application.

</details>

<details>

<summary>2021-09-06 13:49:15 - Deep State-Space Gaussian Processes</summary>

- *Zheng Zhao, Muhammad Emzir, Simo Särkkä*

- `2008.04733v3` - [abs](http://arxiv.org/abs/2008.04733v3) - [pdf](http://arxiv.org/pdf/2008.04733v3)

> This paper is concerned with a state-space approach to deep Gaussian process (DGP) regression. We construct the DGP by hierarchically putting transformed Gaussian process (GP) priors on the length scales and magnitudes of the next level of Gaussian processes in the hierarchy. The idea of the state-space approach is to represent the DGP as a non-linear hierarchical system of linear stochastic differential equations (SDEs), where each SDE corresponds to a conditional GP. The DGP regression problem then becomes a state estimation problem, and we can estimate the state efficiently with sequential methods by using the Markov property of the state-space DGP. The computational complexity scales linearly with respect to the number of measurements. Based on this, we formulate state-space MAP as well as Bayesian filtering and smoothing solutions to the DGP regression problem. We demonstrate the performance of the proposed models and methods on synthetic non-stationary signals and apply the state-space DGP to detection of the gravitational waves from LIGO measurements.

</details>

<details>

<summary>2021-09-06 14:00:47 - Spike and slab variational Bayes for high dimensional logistic regression</summary>

- *Kolyan Ray, Botond Szabo, Gabriel Clara*

- `2010.11665v2` - [abs](http://arxiv.org/abs/2010.11665v2) - [pdf](http://arxiv.org/pdf/2010.11665v2)

> Variational Bayes (VB) is a popular scalable alternative to Markov chain Monte Carlo for Bayesian inference. We study a mean-field spike and slab VB approximation of widely used Bayesian model selection priors in sparse high-dimensional logistic regression. We provide non-asymptotic theoretical guarantees for the VB posterior in both $\ell_2$ and prediction loss for a sparse truth, giving optimal (minimax) convergence rates. Since the VB algorithm does not depend on the unknown truth to achieve optimality, our results shed light on effective prior choices. We confirm the improved performance of our VB algorithm over common sparse VB approaches in a numerical study.

</details>

<details>

<summary>2021-09-06 14:18:41 - What is the probability that a vaccinated person is shielded from Covid-19? A Bayesian MCMC based reanalysis of published data with emphasis on what should be reported as 'efficacy'</summary>

- *Giulio D'Agostini, Alfredo Esposito*

- `2102.11022v3` - [abs](http://arxiv.org/abs/2102.11022v3) - [pdf](http://arxiv.org/pdf/2102.11022v3)

> Based on the information communicated in press releases, and finally published towards the end of 2020 by Pfizer, Moderna and AstraZeneca, we have built up a simple Bayesian model, in which the main quantity of interest plays the role of {\em vaccine efficacy} (`$\epsilon$'). The resulting Bayesian Network is processed by a Markov Chain Monte Carlo (MCMC), implemented in JAGS interfaced to R via rjags. As outcome, we get several probability density functions (pdf's) of $\epsilon$, each conditioned on the data provided by the three pharma companies. The result is rather stable against large variations of the number of people participating in the trials and it is `somehow' in good agreement with the results provided by the companies, in the sense that their values correspond to the most probable value (`mode') of the pdf's resulting from MCMC, thus reassuring us about the validity of our simple model. However we maintain that the number to be reported as `vaccine efficacy' should be the mean of the distribution, rather than the mode, as it was already very clear to Laplace about 250 years ago (its `rule of succession' follows from the simplest problem of the kind). This is particularly important in the case in which the number of successes equals the numbers of trials, as it happens with the efficacy against `severe forms' of infection, claimed by Moderna to be 100%. The implication of the various uncertainties on the predicted number of vaccinated infectees is also shown, using both MCMC and approximated formulae.

</details>

<details>

<summary>2021-09-06 16:53:53 - Estimating the course of the COVID-19 pandemic in Germany via spline-based hierarchical modelling of death counts</summary>

- *Tobias Wistuba, Andreas Mayr, Christian Staerk*

- `2109.02599v1` - [abs](http://arxiv.org/abs/2109.02599v1) - [pdf](http://arxiv.org/pdf/2109.02599v1)

> The effective reproduction number is a key figure to monitor the course of the COVID-19 pandemic. In this study we consider a retrospective modelling approach for estimating the effective reproduction number based on death counts during the first year of the pandemic in Germany. The proposed Bayesian hierarchical model incorporates splines to estimate reproduction numbers flexibly over time while adjusting for varying effective infection fatality rates. The approach also provides estimates of dark figures regarding undetected infections over time. Results for Germany illustrate that estimated reproduction numbers based on death counts are often similar to classical estimates based on confirmed cases. However, considering death counts proves to be more robust against shifts in testing policies: during the second wave of infections, classical estimation of the reproduction number suggests a flattening/ decreasing trend of infections following the "lockdown light" in November 2020, while our results indicate that true numbers of infections continued to rise until the "second lockdown" in December 2020. This observation is associated with more stringent testing criteria introduced concurrently with the "lockdown light", which is reflected in subsequently increasing dark figures of infections estimated by our model. These findings illustrate that the retrospective viewpoint can provide additional insights regarding the course of the pandemic. In light of progressive vaccinations, shifting the focus from modelling confirmed cases to reported deaths with the possibility to incorporate effective infection fatality rates might be of increasing relevance for the future surveillance of the pandemic.

</details>

<details>

<summary>2021-09-06 20:32:32 - Screening the Discrepancy Function of a Computer Model</summary>

- *Pierre Barbillon, Anabel Forte, Rui Paulo*

- `2109.02726v1` - [abs](http://arxiv.org/abs/2109.02726v1) - [pdf](http://arxiv.org/pdf/2109.02726v1)

> Screening traditionally refers to the problem of detecting active inputs in the computer model. In this paper, we develop methodology that applies to screening, but the main focus is on detecting active inputs not in the computer model itself but rather on the discrepancy function that is introduced to account for model inadequacy when linking the computer model with field observations. We contend this is an important problem as it informs the modeler which are the inputs that are potentially being mishandled in the model, but also along which directions it may be less recommendable to use the model for prediction. The methodology is Bayesian and is inspired by the continuous spike and slab prior popularized by the literature on Bayesian variable selection. In our approach, and in contrast with previous proposals, a single MCMC sample from the full model allows us to compute the posterior probabilities of all the competing models, resulting in a methodology that is computationally very fast. The approach hinges on the ability to obtain posterior inclusion probabilities of the inputs, which are very intuitive and easy to interpret quantities, as the basis for selecting active inputs. For that reason, we name the methodology PIPS -- posterior inclusion probability screening.

</details>

<details>

<summary>2021-09-07 09:17:29 - Combining data assimilation and machine learning to estimate parameters of a convective-scale model</summary>

- *Stefanie Legler, Tijana Janjic*

- `2109.02953v1` - [abs](http://arxiv.org/abs/2109.02953v1) - [pdf](http://arxiv.org/pdf/2109.02953v1)

> Errors in the representation of clouds in convection-permitting numerical weather prediction models can be introduced by different sources. These can be the forcing and boundary conditions, the representation of orography, the accuracy of the numerical schemes determining the evolution of humidity and temperature, but large contributions are due to the parametrization of microphysics and the parametrization of processes in the surface and boundary layers. These schemes typically contain several tunable parameters that are either not physical or only crudely known, leading to model errors. Traditionally, the numerical values of these model parameters are chosen by manual model tuning. More objectively, they can be estimated from observations by the augmented state approach during the data assimilation. Alternatively, in this work, we look at the problem of parameter estimation through an artificial intelligence lens by training two types of artificial neural networks (ANNs) to estimate several parameters of the one-dimensional modified shallow-water model as a function of the observations or analysis of the atmospheric state. Through perfect model experiments, we show that Bayesian neural networks (BNNs) and Bayesian approximations of point estimate neural networks (NNs) are able to estimate model parameters and their relevant statistics. The estimation of parameters combined with data assimilation for the state decreases the initial state errors even when assimilating sparse and noisy observations. The sensitivity to the number of ensemble members, observation coverage, and neural network size is shown. Additionally, we use the method of layer-wise relevance propagation to gain insight into how the ANNs are learning and discover that they naturally select only a few gridpoints that are subject to strong winds and rain to make their predictions of chosen parameters.

</details>

<details>

<summary>2021-09-07 11:47:32 - Semiparametric Bayesian Networks</summary>

- *David Atienza, Concha Bielza, Pedro Larrañaga*

- `2109.03008v1` - [abs](http://arxiv.org/abs/2109.03008v1) - [pdf](http://arxiv.org/pdf/2109.03008v1)

> We introduce semiparametric Bayesian networks that combine parametric and nonparametric conditional probability distributions. Their aim is to incorporate the advantages of both components: the bounded complexity of parametric models and the flexibility of nonparametric ones. We demonstrate that semiparametric Bayesian networks generalize two well-known types of Bayesian networks: Gaussian Bayesian networks and kernel density estimation Bayesian networks. For this purpose, we consider two different conditional probability distributions required in a semiparametric Bayesian network. In addition, we present modifications of two well-known algorithms (greedy hill-climbing and PC) to learn the structure of a semiparametric Bayesian network from data. To realize this, we employ a score function based on cross-validation. In addition, using a validation dataset, we apply an early-stopping criterion to avoid overfitting. To evaluate the applicability of the proposed algorithm, we conduct an exhaustive experiment on synthetic data sampled by mixing linear and nonlinear functions, multivariate normal data sampled from Gaussian Bayesian networks, real data from the UCI repository, and bearings degradation data. As a result of this experiment, we conclude that the proposed algorithm accurately learns the combination of parametric and nonparametric components, while achieving a performance comparable with those provided by state-of-the-art methods.

</details>

<details>

<summary>2021-09-08 15:14:22 - Self-explaining variational posterior distributions for Gaussian Process models</summary>

- *Sarem Seitz*

- `2109.03708v1` - [abs](http://arxiv.org/abs/2109.03708v1) - [pdf](http://arxiv.org/pdf/2109.03708v1)

> Bayesian methods have become a popular way to incorporate prior knowledge and a notion of uncertainty into machine learning models. At the same time, the complexity of modern machine learning makes it challenging to comprehend a model's reasoning process, let alone express specific prior assumptions in a rigorous manner. While primarily interested in the former issue, recent developments intransparent machine learning could also broaden the range of prior information that we can provide to complex Bayesian models. Inspired by the idea of self-explaining models, we introduce a corresponding concept for variational GaussianProcesses. On the one hand, our contribution improves transparency for these types of models. More importantly though, our proposed self-explaining variational posterior distribution allows to incorporate both general prior knowledge about a target function as a whole and prior knowledge about the contribution of individual features.

</details>

<details>

<summary>2021-09-08 16:48:28 - Grid-Uniform Copulas and Rectangle Exchanges: Bayesian Model and Inference for a Rich Class of Copula Functions</summary>

- *Nicolás Kuschinski, Alejandro Jara*

- `2109.03768v1` - [abs](http://arxiv.org/abs/2109.03768v1) - [pdf](http://arxiv.org/pdf/2109.03768v1)

> Copula-based models provide a great deal of flexibility in modelling multivariate distributions, allowing for the specifications of models for the marginal distributions separately from the dependence structure (copula) that links them to form a joint distribution. Choosing a class of copula models is not a trivial task and its misspecification can lead to wrong conclusions. We introduce a novel class of grid-uniform copula functions, which is dense in the space of all continuous copula functions in a Hellinger sense. We propose a Bayesian model based on this class and develop an automatic Markov chain Monte Carlo algorithm for exploring the corresponding posterior distribution. The methodology is illustrated by means of simulated data and compared to the main existing approach.

</details>

<details>

<summary>2021-09-08 16:56:43 - Confidence Intervals for Nonparametric Empirical Bayes Analysis</summary>

- *Nikolaos Ignatiadis, Stefan Wager*

- `1902.02774v4` - [abs](http://arxiv.org/abs/1902.02774v4) - [pdf](http://arxiv.org/pdf/1902.02774v4)

> In an empirical Bayes analysis, we use data from repeated sampling to imitate inferences made by an oracle Bayesian with extensive knowledge of the data-generating distribution. Existing results provide a comprehensive characterization of when and why empirical Bayes point estimates accurately recover oracle Bayes behavior. In this paper, we develop flexible and practical confidence intervals that provide asymptotic frequentist coverage of empirical Bayes estimands, such as the posterior mean or the local false sign rate. The coverage statements hold even when the estimands are only partially identified or when empirical Bayes point estimates converge very slowly.

</details>

<details>

<summary>2021-09-08 18:07:24 - Persuasion with Coarse Communication</summary>

- *Yunus C. Aybas, Eray Turkel*

- `1910.13547v5` - [abs](http://arxiv.org/abs/1910.13547v5) - [pdf](http://arxiv.org/pdf/1910.13547v5)

> We study games of Bayesian persuasion where communication is coarse. This model captures interactions between a sender and a receiver, where the sender is unable to fully describe the state or recommend all possible actions. The sender always weakly benefits from more signals, as it increases their ability to persuade. However, more signals do not always lead to more information being sent, and the receiver might prefer outcomes with coarse communication. As a motivating example, we study advertising where a larger signal space corresponds to better targeting ability for the advertiser, and show that customers may prefer less targeting. In a class of games where the sender's utility is independent from the state, we show that an additional signal is more valuable to the sender when the receiver is more difficult to persuade. More generally, we characterize optimal ways to send information using limited signals, show that the sender's optimization problem can be solved by searching within a finite set, and prove an upper bound on the marginal value of a signal. Finally, we show how our approach can be applied to settings with cheap talk and heterogeneous priors.

</details>

<details>

<summary>2021-09-08 19:57:05 - Bayesian data selection</summary>

- *Eli N. Weinstein, Jeffrey W. Miller*

- `2109.02712v2` - [abs](http://arxiv.org/abs/2109.02712v2) - [pdf](http://arxiv.org/pdf/2109.02712v2)

> Insights into complex, high-dimensional data can be obtained by discovering features of the data that match or do not match a model of interest. To formalize this task, we introduce the "data selection" problem: finding a lower-dimensional statistic - such as a subset of variables - that is well fit by a given parametric model of interest. A fully Bayesian approach to data selection would be to parametrically model the value of the statistic, nonparametrically model the remaining "background" components of the data, and perform standard Bayesian model selection for the choice of statistic. However, fitting a nonparametric model to high-dimensional data tends to be highly inefficient, statistically and computationally. We propose a novel score for performing both data selection and model selection, the "Stein volume criterion", that takes the form of a generalized marginal likelihood with a kernelized Stein discrepancy in place of the Kullback-Leibler divergence. The Stein volume criterion does not require one to fit or even specify a nonparametric background model, making it straightforward to compute - in many cases it is as simple as fitting the parametric model of interest with an alternative objective function. We prove that the Stein volume criterion is consistent for both data selection and model selection, and we establish consistency and asymptotic normality (Bernstein-von Mises) of the corresponding generalized posterior on parameters. We validate our method in simulation and apply it to the analysis of single-cell RNA sequencing datasets using probabilistic principal components analysis and a spin glass model of gene regulation.

</details>

<details>

<summary>2021-09-09 06:07:55 - Dependent Dirichlet Processes for Analysis of a Generalized Shared Frailty Model</summary>

- *Chong Zhong, Zhihua Ma, Junshan Shen, Catherine Liu*

- `2109.03713v2` - [abs](http://arxiv.org/abs/2109.03713v2) - [pdf](http://arxiv.org/pdf/2109.03713v2)

> Bayesian paradigm takes advantage of well fitting complicated survival models and feasible computing in survival analysis owing to the superiority in tackling the complex censoring scheme, compared with the frequentist paradigm. In this chapter, we aim to display the latest tendency in Bayesian computing, in the sense of automating the posterior sampling, through Bayesian analysis of survival modeling for multivariate survival outcomes with complicated data structure. Motivated by relaxing the strong assumption of proportionality and the restriction of a common baseline population, we propose a generalized shared frailty model which includes both parametric and nonparametric frailty random effects so as to incorporate both treatment-wise and temporal variation for multiple events. We develop a survival-function version of ANOVA dependent Dirichlet process to model the dependency among the baseline survival functions. The posterior sampling is implemented by the No-U-Turn sampler in Stan, a contemporary Bayesian computing tool, automatically. The proposed model is validated by analysis of the bladder cancer recurrences data. The estimation is consistent with existing results. Our model and Bayesian inference provide evidence that the Bayesian paradigm fosters complex modeling and feasible computing in survival analysis and Stan relaxes the posterior inference.

</details>

<details>

<summary>2021-09-09 10:06:39 - A Bayesian framework for case-cohort Cox regression: application to dietary epidemiology</summary>

- *Andrew Yiu, Robert J. B. Goudie, Stephen J. Sharp, Paul J. Newcombe, Brian D. M. Tom*

- `2007.12974v2` - [abs](http://arxiv.org/abs/2007.12974v2) - [pdf](http://arxiv.org/pdf/2007.12974v2)

> The case-cohort study design bypasses resource constraints by collecting certain expensive covariates for only a small subset of the full cohort. Weighted Cox regression is the most widely used approach for analysing case-cohort data within the Cox model, but is inefficient. Alternative approaches based on multiple imputation and nonparametric maximum likelihood suffer from incompatibility and computational issues respectively. We introduce a novel Bayesian framework for case-cohort Cox regression that avoids the aforementioned problems. Users can include auxiliary variables to help predict the unmeasured expensive covariates with a prediction model of their choice, while the models for the nuisance parameters are nonparametrically specified and integrated out. Posterior sampling can be carried out using procedures based on the pseudo-marginal MCMC algorithm. The method scales effectively to large, complex datasets, as demonstrated in our application: investigating the associations between saturated fatty acids and type 2 diabetes using the EPIC-Norfolk study. As part of our analysis, we also develop a new approach for handling compositional data in the Cox model, leading to more reliable and interpretable results compared to previous studies. The performance of our method is illustrated with extensive simulations. The code used to produce the results in this paper can be found at https://github.com/andrewyiu/bayes_cc .

</details>

<details>

<summary>2021-09-09 17:46:00 - Modeling Massive Spatial Datasets Using a Conjugate Bayesian Linear Regression Framework</summary>

- *Sudipto Banerjee*

- `2109.04447v1` - [abs](http://arxiv.org/abs/2109.04447v1) - [pdf](http://arxiv.org/pdf/2109.04447v1)

> Geographic Information Systems (GIS) and related technologies have generated substantial interest among statisticians with regard to scalable methodologies for analyzing large spatial datasets. A variety of scalable spatial process models have been proposed that can be easily embedded within a hierarchical modeling framework to carry out Bayesian inference. While the focus of statistical research has mostly been directed toward innovative and more complex model development, relatively limited attention has been accorded to approaches for easily implementable scalable hierarchical models for the practicing scientist or spatial analyst. This article discusses how point-referenced spatial process models can be cast as a conjugate Bayesian linear regression that can rapidly deliver inference on spatial processes. The approach allows exact sampling directly (avoids iterative algorithms such as Markov chain Monte Carlo) from the joint posterior distribution of regression parameters, the latent process and the predictive random variables, and can be easily implemented on statistical programming environments such as R.

</details>

<details>

<summary>2021-09-09 19:07:11 - On the Statistical Consistency of Risk-Sensitive Bayesian Decision-Making</summary>

- *Prateek Jaiswal, Harsha Honnappa, Vinayak A. Rao*

- `1903.05220v4` - [abs](http://arxiv.org/abs/1903.05220v4) - [pdf](http://arxiv.org/pdf/1903.05220v4)

> We study data-driven decision-making problems in the Bayesian framework, where the expectation in the Bayes risk is replaced by a risk-sensitive entropic risk measure. We focus on problems where calculating the posterior distribution is intractable, a typical situation in modern applications with large datasets and complex data generating models. We leverage a dual representation of the entropic risk measure to introduce a novel risk-sensitive variational Bayesian (RSVB) framework for jointly computing a risk-sensitive posterior approximation and the corresponding decision rule. The proposed RSVB framework can be used to extract computational methods for doing risk-sensitive approximate Bayesian inference. We show that our general framework includes two well-known computational methods for doing approximate Bayesian inference viz. naive VB and loss-calibrated VB. We also study the impact of these computational approximations on the predictive performance of the inferred decision rules and values. We compute the convergence rates of the RSVB approximate posterior and also of the corresponding optimal value and decision rules. We illustrate our theoretical findings in both parametric and nonparametric settings with the help of three examples: the single and multi-product newsvendor model and Gaussian process classification.

</details>

<details>

<summary>2021-09-10 07:53:59 - Implicit Copulas: An Overview</summary>

- *Michael Stanley Smith*

- `2109.04718v1` - [abs](http://arxiv.org/abs/2109.04718v1) - [pdf](http://arxiv.org/pdf/2109.04718v1)

> Implicit copulas are the most common copula choice for modeling dependence in high dimensions. This broad class of copulas is introduced and surveyed, including elliptical copulas, skew $t$ copulas, factor copulas, time series copulas and regression copulas. The common auxiliary representation of implicit copulas is outlined, and how this makes them both scalable and tractable for statistical modeling. Issues such as parameter identification, extended likelihoods for discrete or mixed data, parsimony in high dimensions, and simulation from the copula model are considered. Bayesian approaches to estimate the copula parameters, and predict from an implicit copula model, are outlined. Particular attention is given to implicit copula processes constructed from time series and regression models, which is at the forefront of current research. Two econometric applications -- one from macroeconomic time series and the other from financial asset pricing -- illustrate the advantages of implicit copula models.

</details>

<details>

<summary>2021-09-10 15:25:37 - Sharp regret bounds for empirical Bayes and compound decision problems</summary>

- *Yury Polyanskiy, Yihong Wu*

- `2109.03943v2` - [abs](http://arxiv.org/abs/2109.03943v2) - [pdf](http://arxiv.org/pdf/2109.03943v2)

> We consider the classical problems of estimating the mean of an $n$-dimensional normally (with identity covariance matrix) or Poisson distributed vector under the squared loss. In a Bayesian setting the optimal estimator is given by the prior-dependent conditional mean. In a frequentist setting various shrinkage methods were developed over the last century. The framework of empirical Bayes, put forth by Robbins (1956), combines Bayesian and frequentist mindsets by postulating that the parameters are independent but with an unknown prior and aims to use a fully data-driven estimator to compete with the Bayesian oracle that knows the true prior. The central figure of merit is the regret, namely, the total excess risk over the Bayes risk in the worst case (over the priors). Although this paradigm was introduced more than 60 years ago, little is known about the asymptotic scaling of the optimal regret in the nonparametric setting.   We show that for the Poisson model with compactly supported and subexponential priors, the optimal regret scales as $\Theta((\frac{\log n}{\log\log n})^2)$ and $\Theta(\log^3 n)$, respectively, both attained by the original estimator of Robbins. For the normal mean model, the regret is shown to be at least $\Omega((\frac{\log n}{\log\log n})^2)$ and $\Omega(\log^2 n)$ for compactly supported and subgaussian priors, respectively, the former of which resolves the conjecture of Singh (1979) on the impossibility of achieving bounded regret; before this work, the best regret lower bound was $\Omega(1)$. In addition to the empirical Bayes setting, these results are shown to hold in the compound setting where the parameters are deterministic. As a side application, the construction in this paper also leads to improved or new lower bounds for density estimation of Gaussian and Poisson mixtures.

</details>

<details>

<summary>2021-09-10 16:32:23 - Foreseeing the Benefits of Incidental Supervision</summary>

- *Hangfeng He, Mingyuan Zhang, Qiang Ning, Dan Roth*

- `2006.05500v2` - [abs](http://arxiv.org/abs/2006.05500v2) - [pdf](http://arxiv.org/pdf/2006.05500v2)

> Real-world applications often require improved models by leveraging a range of cheap incidental supervision signals. These could include partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotations -- all having statistical associations with gold annotations but not exactly the same. However, we currently lack a principled way to measure the benefits of these signals to a given target task, and the common practice of evaluating these benefits is through exhaustive experiments with various models and hyperparameters. This paper studies whether we can, in a single framework, quantify the benefits of various types of incidental signals for a given target task without going through combinatorial experiments. We propose a unified PAC-Bayesian motivated informativeness measure, PABI, that characterizes the uncertainty reduction provided by incidental supervision signals. We demonstrate PABI's effectiveness by quantifying the value added by various types of incidental signals to sequence tagging tasks. Experiments on named entity recognition (NER) and question answering (QA) show that PABI's predictions correlate well with learning performance, providing a promising way to determine, ahead of learning, which supervision signals would be beneficial.

</details>

<details>

<summary>2021-09-10 20:11:22 - From viral evolution to spatial contagion: a biologically modulated Hawkes model</summary>

- *Andrew J. Holbrook, Xiang Ji, Marc A. Suchard*

- `2103.03348v3` - [abs](http://arxiv.org/abs/2103.03348v3) - [pdf](http://arxiv.org/pdf/2103.03348v3)

> Mutations sometimes increase contagiousness for evolving pathogens. During an epidemic, scientists use viral genome data to infer a shared evolutionary history and connect this history to geographic spread. We propose a model that directly relates a pathogen's evolution to its spatial contagion dynamics -- effectively combining the two epidemiological paradigms of phylogenetic inference and self-exciting process modeling -- and apply this \emph{phylogenetic Hawkes process} to a Bayesian analysis of 23,422 viral cases from the 2014-2016 Ebola outbreak in West Africa. The proposed model is able to detect individual viruses with significantly elevated rates of spatiotemporal propagation for a subset of 1,610 samples that provide genome data. Finally, to facilitate model application in big data settings, we develop massively parallel implementations for the gradient and Hessian of the log-likelihood and apply our high performance computing framework within an adaptively preconditioned Hamiltonian Monte Carlo routine.

</details>

<details>

<summary>2021-09-10 20:31:46 - Bayesian inference of PolII dynamics over the exclusion process</summary>

- *Massimo Cavallaro, Yuexuan Wang, Daniel Hebenstreit, Ritabrata Dutta*

- `2109.05100v1` - [abs](http://arxiv.org/abs/2109.05100v1) - [pdf](http://arxiv.org/pdf/2109.05100v1)

> Transcription is a complex phenomenon that permits the conversion of genetic information into phenotype by means of an enzyme called PolII, which erratically moves along and scans the DNA template. We perform Bayesian inference over a paradigmatic mechanistic model of non-equilibrium statistical physics, i.e., the asymmetric exclusion processes in the hydrodynamic limit, assuming a Gaussian process prior for the PolII progression rate as a latent variable. Our framework allows us to infer the speed of PolIIs during transcription given their spatial distribution, whilst avoiding the explicit inversion of the system's dynamics. The results may have implications for the understanding of gene expression.

</details>

<details>

<summary>2021-09-11 06:09:26 - DynaNet: Neural Kalman Dynamical Model for Motion Estimation and Prediction</summary>

- *Changhao Chen, Chris Xiaoxuan Lu, Bing Wang, Niki Trigoni, Andrew Markham*

- `1908.03918v3` - [abs](http://arxiv.org/abs/1908.03918v3) - [pdf](http://arxiv.org/pdf/1908.03918v3)

> Dynamical models estimate and predict the temporal evolution of physical systems. State Space Models (SSMs) in particular represent the system dynamics with many desirable properties, such as being able to model uncertainty in both the model and measurements, and optimal (in the Bayesian sense) recursive formulations e.g. the Kalman Filter. However, they require significant domain knowledge to derive the parametric form and considerable hand-tuning to correctly set all the parameters. Data driven techniques e.g. Recurrent Neural Networks have emerged as compelling alternatives to SSMs with wide success across a number of challenging tasks, in part due to their ability to extract relevant features from rich inputs. They however lack interpretability and robustness to unseen conditions. In this work, we present DynaNet, a hybrid deep learning and time-varying state-space model which can be trained end-to-end. Our neural Kalman dynamical model allows us to exploit the relative merits of each approach. We demonstrate state-of-the-art estimation and prediction on a number of physically challenging tasks, including visual odometry, sensor fusion for visual-inertial navigation and pendulum control. In addition we show how DynaNet can indicate failures through investigation of properties such as the rate of innovation (Kalman Gain).

</details>

<details>

<summary>2021-09-11 16:40:43 - Bayesian Topic Regression for Causal Inference</summary>

- *Maximilian Ahrens, Julian Ashwin, Jan-Peter Calliess, Vu Nguyen*

- `2109.05317v1` - [abs](http://arxiv.org/abs/2109.05317v1) - [pdf](http://arxiv.org/pdf/2109.05317v1)

> Causal inference using observational text data is becoming increasingly popular in many research areas. This paper presents the Bayesian Topic Regression (BTR) model that uses both text and numerical information to model an outcome variable. It allows estimation of both discrete and continuous treatment effects. Furthermore, it allows for the inclusion of additional numerical confounding factors next to text data. To this end, we combine a supervised Bayesian topic model with a Bayesian regression framework and perform supervised representation learning for the text features jointly with the regression parameter training, respecting the Frisch-Waugh-Lovell theorem. Our paper makes two main contributions. First, we provide a regression framework that allows causal inference in settings when both text and numerical confounders are of relevance. We show with synthetic and semi-synthetic datasets that our joint approach recovers ground truth with lower bias than any benchmark model, when text and numerical features are correlated. Second, experiments on two real-world datasets demonstrate that a joint and supervised learning strategy also yields superior prediction results compared to strategies that estimate regression weights for text and non-text features separately, being even competitive with more complex deep neural networks.

</details>

<details>

<summary>2021-09-12 15:42:07 - On The Radon-Nikodym Spectral Approach With Optimal Clustering</summary>

- *Vladislav Gennadievich Malyshkin*

- `1906.00460v17` - [abs](http://arxiv.org/abs/1906.00460v17) - [pdf](http://arxiv.org/pdf/1906.00460v17)

> Problems of interpolation, classification, and clustering are considered. In the tenets of Radon--Nikodym approach $\langle f(\mathbf{x})\psi^2 \rangle / \langle\psi^2\rangle$, where the $\psi(\mathbf{x})$ is a linear function on input attributes, all the answers are obtained from a generalized eigenproblem $|f|\psi^{[i]}\rangle = \lambda^{[i]} |\psi^{[i]}\rangle$. The solution to the interpolation problem is a regular Radon-Nikodym derivative. The solution to the classification problem requires prior and posterior probabilities that are obtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian approach new observations change only outcome probabilities, in the Radon-Nikodym approach not only outcome probabilities but also the probability space $|\psi^{[i]}\rangle$ change with new observations. This is a remarkable feature of the approach: both the probabilities and the probability space are constructed from the data. The Lebesgue quadrature technique can be also applied to the optimal clustering problem. The problem is solved by constructing a Gaussian quadrature on the Lebesgue measure. A distinguishing feature of the Radon-Nikodym approach is the knowledge of the invariant group: all the answers are invariant relatively any non-degenerated linear transform of input vector $\mathbf{x}$ components. A software product implementing the algorithms of interpolation, classification, and optimal clustering is available from the authors.

</details>

<details>

<summary>2021-09-12 16:44:46 - Context-aware surrogate modeling for balancing approximation and sampling costs in multi-fidelity importance sampling and Bayesian inverse problems</summary>

- *Terrence Alsup, Benjamin Peherstorfer*

- `2010.11708v2` - [abs](http://arxiv.org/abs/2010.11708v2) - [pdf](http://arxiv.org/pdf/2010.11708v2)

> Multi-fidelity methods leverage low-cost surrogate models to speed up computations and make occasional recourse to expensive high-fidelity models to establish accuracy guarantees. Because surrogate and high-fidelity models are used together, poor predictions by surrogate models can be compensated with frequent recourse to high-fidelity models. Thus, there is a trade-off between investing computational resources to improve the accuracy of surrogate models versus simply making more frequent recourse to expensive high-fidelity models; however, this trade-off is ignored by traditional modeling methods that construct surrogate models that are meant to replace high-fidelity models rather than being used together with high-fidelity models. This work considers multi-fidelity importance sampling and theoretically and computationally trades off increasing the fidelity of surrogate models for constructing more accurate biasing densities and the numbers of samples that are required from the high-fidelity models to compensate poor biasing densities. Numerical examples demonstrate that such context-aware surrogate models for multi-fidelity importance sampling have lower fidelity than what typically is set as tolerance in traditional model reduction, leading to runtime speedups of up to one order of magnitude in the presented examples.

</details>

<details>

<summary>2021-09-13 11:20:45 - Function-on-function partial quantile regression</summary>

- *Ufuk Beyaztas, Han Lin Shang, Aylin Alin*

- `2109.05874v1` - [abs](http://arxiv.org/abs/2109.05874v1) - [pdf](http://arxiv.org/pdf/2109.05874v1)

> In this paper, a functional partial quantile regression approach, a quantile regression analog of the functional partial least squares regression, is proposed to estimate the function-on-function linear quantile regression model. A partial quantile covariance function is first used to extract the functional partial quantile regression basis functions. The extracted basis functions are then used to obtain the functional partial quantile regression components and estimate the final model. In our proposal, the functional forms of the discretely observed random variables are first constructed via a finite-dimensional basis function expansion method. The functional partial quantile regression constructed using the functional random variables is approximated via the partial quantile regression constructed using the basis expansion coefficients. The proposed method uses an iterative procedure to extract the partial quantile regression components. A Bayesian information criterion is used to determine the optimum number of retained components. The proposed functional partial quantile regression model allows for more than one functional predictor in the model. However, the true form of the proposed model is unspecified, as the relevant predictors for the model are unknown in practice. Thus, a forward variable selection procedure is used to determine the significant predictors for the proposed model. Moreover, a case-sampling-based bootstrap procedure is used to construct pointwise prediction intervals for the functional response. The predictive performance of the proposed method is evaluated using several Monte Carlo experiments under different data generation processes and error distributions. Through an empirical data example, air quality data are analyzed to demonstrate the effectiveness of the proposed method.

</details>

<details>

<summary>2021-09-13 12:05:12 - Bayesian Estimation of the ETAS Model for Earthquake Occurrences</summary>

- *Gordon J Ross*

- `2109.05894v1` - [abs](http://arxiv.org/abs/2109.05894v1) - [pdf](http://arxiv.org/pdf/2109.05894v1)

> The Epidemic Type Aftershock Sequence (ETAS) model is one of the most widely-used approaches to seismic forecasting. However most studies of ETAS use point estimates for the model parameters, which ignores the inherent uncertainty that arises from estimating these from historical earthquake catalogs, resulting in misleadingly optimistic forecasts. In contrast, Bayesian statistics allows parameter uncertainty to be explicitly represented, and fed into the forecast distribution. Despite its growing popularity in seismology, the application of Bayesian statistics to the ETAS model has been limited by the complex nature of the resulting posterior distribution which makes it infeasible to apply on catalogs containing more than a few hundred earthquakes. To combat this, we develop a new framework for estimating the ETAS model in a fully Bayesian manner, which can be efficiently scaled up to large catalogs containing thousands of earthquakes. We also provide easy-to-use software which implements our method.

</details>

<details>

<summary>2021-09-13 13:10:01 - Convergence of Likelihood Ratios and Estimators for Selection in non-neutral Wright-Fisher Diffusions</summary>

- *Jaromir Sant, Paul A. Jenkins, Jere Koskela, Dario Spano*

- `2001.03527v3` - [abs](http://arxiv.org/abs/2001.03527v3) - [pdf](http://arxiv.org/pdf/2001.03527v3)

> A number of discrete time, finite population size models in genetics describing the dynamics of allele frequencies are known to converge (subject to suitable scaling) to a diffusion process in the infinite population limit, termed the Wright-Fisher diffusion. In this article we show that the diffusion is ergodic uniformly in the selection and mutation parameters, and that the measures induced by the solution to the stochastic differential equation are uniformly locally asymptotically normal. Subsequently these two results are used to analyse the statistical properties of the Maximum Likelihood and Bayesian estimators for the selection parameter, when both selection and mutation are acting on the population. In particular, it is shown that these estimators are uniformly over compact sets consistent, display uniform in the selection parameter asymptotic normality and convergence of moments over compact sets, and are asymptotically efficient for a suitable class of loss functions.

</details>

<details>

<summary>2021-09-13 16:25:04 - Wavelet Shrinkage in Nonparametric Regression Models with Positive Noise</summary>

- *Alex Rodrigo dos Santos Sousa, Nancy Lopes Garcia*

- `2109.06102v1` - [abs](http://arxiv.org/abs/2109.06102v1) - [pdf](http://arxiv.org/pdf/2109.06102v1)

> Wavelet shrinkage estimators are widely applied in several fields of science for denoising data in wavelet domain by reducing the magnitudes of empirical coefficients. In nonparametric regression problem, most of the shrinkage rules are derived from models composed by an unknown function with additive gaussian noise. Although gaussian noise assumption is reasonable in several real data analysis, mainly for large sample sizes, it is not general. Contaminated data with positive noise can occur in practice and nonparametric regression models with positive noise bring challenges in wavelet shrinkage point of view. This work develops bayesian shrinkage rules to estimate wavelet coefficients from a nonparametric regression framework with additive and strictly positive noise under exponential and lognormal distributions. Computational aspects are discussed and simulation studies to analyse the performances of the proposed shrinkage rules and compare them with standard techniques are done. An application in winning times Boston Marathon dataset is also provided.

</details>

<details>

<summary>2021-09-13 18:00:41 - Predicting Credit Default Probabilities Using Bayesian Statistics and Monte Carlo Simulations</summary>

- *Dominic Joseph*

- `2108.03389v2` - [abs](http://arxiv.org/abs/2108.03389v2) - [pdf](http://arxiv.org/pdf/2108.03389v2)

> Banks and financial institutions all over the world manage portfolios containing tens of thousands of customers. Not all customers are high credit-worthy, and many possess varying degrees of risk to the Bank or financial institutions that lend money to these customers. Hence assessment of credit risk is paramount in the field of credit risk management. This paper discusses the use of Bayesian principles and simulation-techniques to estimate and calibrate the default probability of credit ratings. The methodology is a two-phase approach where, in the first phase, a posterior density of default rate parameter is estimated based the default history data. In the second phase of the approach, an estimate of true default rate parameter is obtained through simulations

</details>

<details>

<summary>2021-09-13 18:04:55 - Stochastic geometry to generalize the Mondrian Process</summary>

- *Eliza O'Reilly, Ngoc Tran*

- `2002.00797v3` - [abs](http://arxiv.org/abs/2002.00797v3) - [pdf](http://arxiv.org/pdf/2002.00797v3)

> The stable under iterated tessellation (STIT) process is a stochastic process that produces a recursive partition of space with cut directions drawn independently from a distribution over the sphere. The case of random axis-aligned cuts is known as the Mondrian process. Random forests and Laplace kernel approximations built from the Mondrian process have led to efficient online learning methods and Bayesian optimization. In this work, we utilize tools from stochastic geometry to resolve some fundamental questions concerning STIT processes in machine learning. First, we show that a STIT process with cut directions drawn from a discrete distribution can be efficiently simulated by lifting to a higher dimensional axis-aligned Mondrian process. Second, we characterize all possible kernels that stationary STIT processes and their mixtures can approximate. We also give a uniform convergence rate for the approximation error of the STIT kernels to the targeted kernels, generalizing the work of [3] for the Mondrian case. Third, we obtain consistency results for STIT forests in density estimation and regression. Finally, we give a formula for the density estimator arising from an infinite STIT random forest. This allows for precise comparisons between the Mondrian forest, the Mondrian kernel and the Laplace kernel in density estimation. Our paper calls for further developments at the novel intersection of stochastic geometry and machine learning.

</details>

<details>

<summary>2021-09-13 18:42:08 - Parametric Modeling Approach to COVID-19 Pandemic Data</summary>

- *N. I. Badmus, O. Faweya, S. A. Ige*

- `2109.06254v1` - [abs](http://arxiv.org/abs/2109.06254v1) - [pdf](http://arxiv.org/pdf/2109.06254v1)

> The problem of skewness is common among clinical trials and survival data which has being the research focus derivation and proposition of different flexible distributions. Thus, a new distribution called Extended Rayleigh Lomax distribution is constructed from Rayleigh Lomax distribution to capture the excessiveness of some survival data. We derive the new distribution by using beta logit function proposed by Jones (2004). Some statistical properties of the distribution such as probability density function, cumulative density function, reliability rate, hazard rate, reverse hazard rate, moment generating functions, likelihood functions, skewness, kurtosis and coefficient of variation are obtained. We also performed the expected estimation of model parameters by maximum likelihood; goodness of fit and model selection criteria including Anderson Darling (AD), CramerVon Misses (CVM), Kolmogorov Smirnov (KS), Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC) and Consistent Akaike Information Criterion (CAIC) are employed to select the better distribution from those models considered in the work. The results from the statistics criteria show that the proposed distribution performs better with better representation of the States in Nigeria COVID-19 death cases data than other competing models.

</details>

<details>

<summary>2021-09-13 20:55:00 - Bayesian inference-driven model parameterization and model selection for 2CLJQ fluid models</summary>

- *Owen C. Madin, Simon Boothroyd, Richard A. Messerly, John D. Chodera, Josh Fass, Michael R. Shirts*

- `2105.07863v2` - [abs](http://arxiv.org/abs/2105.07863v2) - [pdf](http://arxiv.org/pdf/2105.07863v2)

> A high level of physical detail in a molecular model improves its ability to perform high accuracy simulations, but can also significantly affect its complexity and computational cost. In some situations, it is worthwhile to add additional complexity to a model to capture properties of interest; in others, additional complexity is unnecessary and can make simulations computationally infeasible. In this work we demonstrate the use of Bayes factors for molecular model selection, using Monte Carlo sampling techniques to evaluate the evidence for different levels of complexity in the two-centered Lennard-Jones + quadrupole (2CLJQ) fluid model. Examining three levels of nested model complexity, we demonstrate that the use of variable quadrupole and bond length parameters in this model framework is justified only sometimes. We also explore the effect of the Bayesian prior distribution on the Bayes factors, as well as ways to propose meaningful prior distributions. This Bayesian Markov Chain Monte Carlo (MCMC) process is enabled by the use of analytical surrogate models that accurately approximate the physical properties of interest. This work paves the way for further atomistic model selection work via Bayesian inference and surrogate modeling

</details>

<details>

<summary>2021-09-13 23:42:57 - Truncated Simulation and Inference in Edge-Exchangeable Networks</summary>

- *Xinglong Li, Trevor Campbell*

- `2005.08136v3` - [abs](http://arxiv.org/abs/2005.08136v3) - [pdf](http://arxiv.org/pdf/2005.08136v3)

> Edge-exchangeable probabilistic network models generate edges as an i.i.d.~sequence from a discrete measure, providing a simple means for statistical inference of latent network properties. The measure is often constructed using the self-product of a realization from a Bayesian nonparametric (BNP) discrete prior; but unlike in standard BNP models, the self-product measure prior is not conjugate the likelihood, hindering the development of exact simulation and inference algorithms. Approximation via finite truncation of the discrete measure is a straightforward alternative, but incurs an unknown approximation error. In this paper, we develop methods for forward simulation and posterior inference in random self-product-measure models based on truncation, and provide theoretical guarantees on the quality of the results as a function of the truncation level. The techniques we present are general and extend to the broader class of discrete Bayesian nonparametric models.

</details>

<details>

<summary>2021-09-14 10:12:53 - Bayesian model-based outlier detection in network meta-analysis</summary>

- *Silvia Metelli, Dimitris Mavridis, Perrine Créquit, Anna Chaimani*

- `2109.06559v1` - [abs](http://arxiv.org/abs/2109.06559v1) - [pdf](http://arxiv.org/pdf/2109.06559v1)

> In a network meta-analysis, some of the collected studies may deviate markedly from the others, for example having very unusual effect sizes. These deviating studies can be regarded as outlying with respect to the rest of the network and can be influential on the pooled results. Thus, it could be inappropriate to synthesize those studies without further investigation. In this paper, we propose two Bayesian methods to detect outliers in a network meta-analysis via: (a) a mean-shifted outlier model and (b), posterior predictive p-values constructed from ad-hoc discrepancy measures. The former method uses Bayes factors to formally test each study against outliers while the latter provides a score of outlyingness for each study in the network, which allows to numerically quantify the uncertainty associated with being outlier. Furthermore, we present a simple method based on informative priors as part of the network meta-analysis model to down-weight the detected outliers. We conduct extensive simulations to evaluate the effectiveness of the proposed methodology while comparing it to some alternative, available outlier diagnostic tools. Two real networks of interventions are then used to demonstrate our methods in practice.

</details>

<details>

<summary>2021-09-14 10:26:55 - Gibbs posterior inference on a Levy density under discrete sampling</summary>

- *Zhe Wang, Ryan Martin*

- `2109.06567v1` - [abs](http://arxiv.org/abs/2109.06567v1) - [pdf](http://arxiv.org/pdf/2109.06567v1)

> In mathematical finance, Levy processes are widely used for their ability to model both continuous variation and abrupt, discontinuous jumps. These jumps are practically relevant, so reliable inference on the feature that controls jump frequencies and magnitudes, namely, the Levy density, is of critical importance. A specific obstacle to carrying out model-based (e.g., Bayesian) inference in such problems is that, for general Levy processes, the likelihood is intractable. To overcome this obstacle, here we adopt a Gibbs posterior framework that updates a prior distribution using a suitable loss function instead of a likelihood. We establish asymptotic posterior concentration rates for the proposed Gibbs posterior. In particular, in the most interesting and practically relevant case, we give conditions under which the Gibbs posterior concentrates at (nearly) the minimax optimal rate, adaptive to the unknown smoothness of the true Levy density.

</details>

<details>

<summary>2021-09-14 11:24:48 - Quasi-Measurable Spaces</summary>

- *Patrick Forré*

- `2109.11631v1` - [abs](http://arxiv.org/abs/2109.11631v1) - [pdf](http://arxiv.org/pdf/2109.11631v1)

> We introduce the categories of quasi-measurable spaces, which are slight generalizations of the category of quasi-Borel spaces, where we now allow for general sample spaces and less restrictive random variables, spaces and maps. We show that each category of quasi-measurable spaces is bi-complete and cartesian closed. We also introduce several different strong probability monads. Together these constructions provide convenient categories for higher probability theory that also support semantics of higher-order probabilistic programming languages in the same way as the category of quasi-Borel spaces does.   An important special case is the category of quasi-universal spaces, where the sample space is the set of the real numbers together with the sigma-algebra of all universally measurable subsets. The induced sigma-algebras on those quasi-universal spaces then have explicit descriptions in terms of intersections of Lebesgue-complete sigma-algebras. A central role is then played by countably separated and universal quasi-universal spaces, which replace the role of standard Borel spaces. We prove in this setting a Fubini theorem, a disintegration theorem for Markov kernels, a Kolmogorov extension theorem and a conditional de Finetti theorem. We also translate our findings into properties of the corresponding Markov category of Markov kernels between universal quasi-universal spaces.   Furthermore, we formalize causal Bayesian networks in terms of quasi-universal spaces and prove a global Markov property. For this we translate the notion of transitional conditional independence into this setting and study its (asymmetric) separoid rules. Altogether we are now able to reason about conditional independence relations between variables and causal mechanisms on equal footing. Finally, we also highlight how one can use exponential objects and random functions for counterfactual reasoning.

</details>

<details>

<summary>2021-09-14 12:49:29 - A Wasserstein index of dependence for random measures</summary>

- *Marta Catalano, Hugo Lavenant, Antonio Lijoi, Igor Prünster*

- `2109.06646v1` - [abs](http://arxiv.org/abs/2109.06646v1) - [pdf](http://arxiv.org/pdf/2109.06646v1)

> Nonparametric latent structure models provide flexible inference on distinct, yet related, groups of observations. Each component of a vector of $d \ge 2$ random measures models the distribution of a group of exchangeable observations, while their dependence structure regulates the borrowing of information across different groups. Recent work has quantified the dependence between random measures in terms of Wasserstein distance from the maximally dependent scenario when $d=2$. By solving an intriguing max-min problem we are now able to define a Wasserstein index of dependence $I_\mathcal{W}$ with the following properties: (i) it simultaneously quantifies the dependence of $d \ge 2$ random measures; (ii) it takes values in [0,1]; (iii) it attains the extreme values $\{0,1\}$ under independence and complete dependence, respectively; (iv) since it is defined in terms of the underlying L\'evy measures, it is possible to evaluate it numerically in many Bayesian nonparametric models for partially exchangeable data.

</details>

<details>

<summary>2021-09-14 13:13:36 - Anytime Parallel Tempering</summary>

- *A. Marie d'Avigneau, S. S. Singh, L. M. Murray*

- `2006.14875v3` - [abs](http://arxiv.org/abs/2006.14875v3) - [pdf](http://arxiv.org/pdf/2006.14875v3)

> Developing efficient MCMC algorithms is indispensable in Bayesian inference. In parallel tempering, multiple interacting MCMC chains run to more efficiently explore the state space and improve performance. The multiple chains advance independently through local moves, and the performance enhancement steps are exchange moves, where the chains pause to exchange their current sample amongst each other. To accelerate the independent local moves, they may be performed simultaneously on multiple processors. Another problem is then encountered: depending on the MCMC implementation and inference problem, local moves can take a varying and random amount of time to complete. There may also be infrastructure-induced variations, such as competing jobs on the same processors, which arises in cloud computing. Before exchanges can occur, all chains must complete the local moves they are engaged in to avoid introducing a potentially substantial bias (Proposition 2.1). To solve this issue of randomly varying local move completion times in multi-processor parallel tempering, we adopt the Anytime Monte Carlo framework of Murray et al. (2016): we impose real-time deadlines on the parallel local moves and perform exchanges at these deadlines without any processor idling. We show our methodology for exchanges at real-time deadlines does not introduce a bias and leads to significant performance enhancements over the na\"ive approach of idling until every processor's local moves complete. The methodology is then applied in an ABC setting, where an Anytime ABC parallel tempering algorithm is derived for the difficult task of estimating the parameters of a Lotka-Volterra predator-prey model, and similar efficiency enhancements are observed.

</details>

<details>

<summary>2021-09-14 15:18:46 - Bayesian I-optimal designs for choice experiments with mixtures</summary>

- *Mario Becerra, Peter Goos*

- `2108.01748v2` - [abs](http://arxiv.org/abs/2108.01748v2) - [pdf](http://arxiv.org/pdf/2108.01748v2)

> Discrete choice experiments are frequently used to quantify consumer preferences by having respondents choose between different alternatives. Choice experiments involving mixtures of ingredients have been largely overlooked in the literature, even though many products and services can be described as mixtures of ingredients. As a consequence, little research has been done on the optimal design of choice experiments involving mixtures. The only existing research has focused on D-optimal designs, which means that an estimation-based approach was adopted. However, in experiments with mixtures, it is crucial to obtain models that yield precise predictions for any combination of ingredient proportions. This is because the goal of mixture experiments generally is to find the mixture that optimizes the respondents' utility. As a result, the I-optimality criterion is more suitable for designing choice experiments with mixtures than the D-optimality criterion because the I-optimality criterion focuses on getting precise predictions with the estimated statistical model. In this paper, we study Bayesian I-optimal designs, compare them with their Bayesian D-optimal counterparts, and show that the former designs perform substantially better than the latter in terms of the variance of the predicted utility.

</details>

<details>

<summary>2021-09-14 15:25:36 - Bayesian hierarchical analysis of a multifaceted program against extreme poverty</summary>

- *Louis Charlot*

- `2109.06759v1` - [abs](http://arxiv.org/abs/2109.06759v1) - [pdf](http://arxiv.org/pdf/2109.06759v1)

> The evaluation of a multifaceted program against extreme poverty in different developing countries gave encouraging results, but with important heterogeneity between countries. This master thesis proposes to study this heterogeneity with a Bayesian hierarchical analysis. The analysis we carry out with two different hierarchical models leads to a very low amount of pooling of information between countries, indicating that this observed heterogeneity should be interpreted mostly as true heterogeneity, and not as sampling error. We analyze the first order behavior of our hierarchical models, in order to understand what leads to this very low amount of pooling. We try to give to this work a didactic approach, with an introduction of Bayesian analysis and an explanation of the different modeling and computational choices of our analysis.

</details>

<details>

<summary>2021-09-14 15:26:25 - Using Clinical Experts Beliefs to Compare Survival Models in Health Technology Assessment</summary>

- *J. W. Stevens, M. Orr*

- `2109.06760v1` - [abs](http://arxiv.org/abs/2109.06760v1) - [pdf](http://arxiv.org/pdf/2109.06760v1)

> Objectives: The aim of this paper is to contrast the retrospective and prospective use of experts beliefs in choosing between survival models in economic evaluations. Methods: The use of experts retrospective (posterior) beliefs is discussed. A process for prospectively quantifying prior beliefs about model parameters in five standard models is described. Statistical criterion for comparing models, and the interpretation and computation of model probabilities is discussed. A case study is provided. Results: Experts have little difficulty in expressing their posterior beliefs. Information criterion is an approximation to Bayesian model evidence and is based on data alone. In contrast, Bayes factors measure evidence in the data and makes use of prior information. When model averaging is of interest, there is no unique way to specify prior ignorance about model probabilities. Formulating and interpreting weights of similar models should acknowledge the dilution phenomenon such that highly correlated models are given smaller weights than those with low correlation. Conclusion: The retrospective use of experts beliefs to validate a model is potentially misleading, may not achieve its intended objective and is an inefficient use of information. Experts beliefs should be elicited prospectively as probability distributions to strengthen inferences, facilitate the choice of model, and mitigate the impact of dilution on model probabilities in situations when model averaging is of interest.

</details>

<details>

<summary>2021-09-14 15:51:32 - Improving performances of MCMC for Nearest Neighbor Gaussian Process models with full data augmentation</summary>

- *Sébastien Coube-Sisqueille, Benoît Liquet*

- `2010.00896v3` - [abs](http://arxiv.org/abs/2010.00896v3) - [pdf](http://arxiv.org/pdf/2010.00896v3)

> Even though Nearest Neighbor Gaussian Processes (NNGP) alleviate considerably MCMC implementation of Bayesian space-time models, they do not solve the convergence problems caused by high model dimension. Frugal alternatives such as response or collapsed algorithms are an answer.gree Our approach is to keep full data augmentation but to try and make it more efficient. We present two strategies to do so. The first scheme is to pay a particular attention to the seemingly trivial fixed effects of the model. We show empirically that re-centering the latent field on the intercept critically improves chain behavior. We extend this approach to other fixed effects that may interfere with a coherent spatial field. We propose a simple method that requires no tuning while remaining affordable thanks to NNGP's sparsity. The second scheme accelerates the sampling of the random field using Chromatic samplers. This method makes long sequential simulation boil down to group-parallelized or group-vectorized sampling. The attractive possibility to parallelize NNGP likelihood can therefore be carried over to field sampling. We present a R implementation of our methods for Gaussian fields in the public repository https://github.com/SebastienCoube/Improving_NNGP_full_augmentation . An extensive vignette is provided. We run our implementation on two synthetic toy examples along with the state of the art package spNNGP. Finally, we apply our method on a real data set of lead contamination in the United States of America mainland.

</details>

<details>

<summary>2021-09-14 20:18:35 - Learning trends of COVID-19 using semi-supervised clustering</summary>

- *Semhar Michael, Xuwen Zhu, Volodymyr Melnykov*

- `2109.06955v1` - [abs](http://arxiv.org/abs/2109.06955v1) - [pdf](http://arxiv.org/pdf/2109.06955v1)

> A finite mixture model is used to learn trends from the currently available data on coronavirus (COVID-19). Data on the number of confirmed COVID-19 related cases and deaths for European countries and the United States (US) are explored. A semi-supervised clustering approach with positive equivalence constraints is used to incorporate country and state information into the model. The analysis of trends in the rates of cases and deaths is carried out jointly using a mixture of multivariate Gaussian non-linear regression models with a mean trend specified using a generalized logistic function. The optimal number of clusters is chosen using the Bayesian information criterion. The resulting clusters provide insight into different mitigation strategies adopted by US states and European countries. The obtained results help identify the current relative standing of individual states and show a possible future if they continue with the chosen mitigation technique

</details>

<details>

<summary>2021-09-15 04:30:14 - Sequential prediction under log-loss and misspecification</summary>

- *Meir Feder, Yury Polyanskiy*

- `2102.00050v3` - [abs](http://arxiv.org/abs/2102.00050v3) - [pdf](http://arxiv.org/pdf/2102.00050v3)

> We consider the question of sequential prediction under the log-loss in terms of cumulative regret. Namely, given a hypothesis class of distributions, learner sequentially predicts the (distribution of the) next letter in sequence and its performance is compared to the baseline of the best constant predictor from the hypothesis class. The well-specified case corresponds to an additional assumption that the data-generating distribution belongs to the hypothesis class as well. Here we present results in the more general misspecified case. Due to special properties of the log-loss, the same problem arises in the context of competitive-optimality in density estimation, and model selection. For the $d$-dimensional Gaussian location hypothesis class, we show that cumulative regrets in the well-specified and misspecified cases asymptotically coincide. In other words, we provide an $o(1)$ characterization of the distribution-free (or PAC) regret in this case -- the first such result as far as we know. We recall that the worst-case (or individual-sequence) regret in this case is larger by an additive constant ${d\over 2} + o(1)$. Surprisingly, neither the traditional Bayesian estimators, nor the Shtarkov's normalized maximum likelihood achieve the PAC regret and our estimator requires special "robustification" against heavy-tailed data. In addition, we show two general results for misspecified regret: the existence and uniqueness of the optimal estimator, and the bound sandwiching the misspecified regret between well-specified regrets with (asymptotically) close hypotheses classes.

</details>

<details>

<summary>2021-09-15 08:50:55 - D3p -- A Python Package for Differentially-Private Probabilistic Programming</summary>

- *Lukas Prediger, Niki Loppi, Samuel Kaski, Antti Honkela*

- `2103.11648v2` - [abs](http://arxiv.org/abs/2103.11648v2) - [pdf](http://arxiv.org/pdf/2103.11648v2)

> We present d3p, a software package designed to help fielding runtime efficient widely-applicable Bayesian inference under differential privacy guarantees. d3p achieves general applicability to a wide range of probabilistic modelling problems by implementing the differentially private variational inference algorithm, allowing users to fit any parametric probabilistic model with a differentiable density function. d3p adopts the probabilistic programming paradigm as a powerful way for the user to flexibly define such models. We demonstrate the use of our software on a hierarchical logistic regression example, showing the expressiveness of the modelling approach as well as the ease of running the parameter inference. We also perform an empirical evaluation of the runtime of the private inference on a complex model and find a $\sim$10 fold speed-up compared to an implementation using TensorFlow Privacy.

</details>

<details>

<summary>2021-09-15 09:04:03 - Bayesian testing of linear versus nonlinear effects using Gaussian process priors</summary>

- *Joris Mulder*

- `2109.07166v1` - [abs](http://arxiv.org/abs/2109.07166v1) - [pdf](http://arxiv.org/pdf/2109.07166v1)

> A Bayes factor is proposed for testing whether the effect of a key predictor variable on the dependent variable is linear or nonlinear, possibly while controlling for certain covariates. The test can be used (i) when one is interested in quantifying the relative evidence in the data of a linear versus a nonlinear relationship and (ii) to quantify the evidence in the data in favor of a linear relationship (useful when building linear models based on transformed variables). Under the nonlinear model, a Gaussian process prior is employed using a parameterization similar to Zellner's $g$ prior resulting in a scale-invariant test. Moreover a Bayes factor is proposed for one-sided testing of whether the nonlinear effect is consistently positive, consistently negative, or neither. Applications are provides from various fields including social network research and education.

</details>

<details>

<summary>2021-09-15 13:32:12 - Exact Bayesian inference for diffusion driven Cox processes</summary>

- *Flavio B. Gonçalves, Krzysztof G. Łatuszyński, Gareth O. Roberts*

- `2007.05812v2` - [abs](http://arxiv.org/abs/2007.05812v2) - [pdf](http://arxiv.org/pdf/2007.05812v2)

> In this paper, we present a novel methodology to perform Bayesian inference for Cox processes in which the intensity function is driven by a diffusion process. The novelty lies in the fact that no discretization error is involved, despite the non-tractability of both the likelihood function and the transition density of the diffusion. The methodology is based on an MCMC algorithm and its exactness is built on retrospective sampling techniques. The efficiency of the methodology is investigated in some simulated examples and its applicability is illustrated in some real data analyzes.

</details>

<details>

<summary>2021-09-15 16:03:21 - EM-Based Smooth Graphon Estimation Using Bayesian and Spline-Based Approaches</summary>

- *Benjamin Sischka, Göran Kauermann*

- `1903.06936v4` - [abs](http://arxiv.org/abs/1903.06936v4) - [pdf](http://arxiv.org/pdf/1903.06936v4)

> This paper proposes the estimation of a smooth graphon model for network data analysis using principles of the EM algorithm. The approach considers both variability with respect to ordering the nodes of a network and smooth estimation of the graphon by nonparametric regression. To do so, (linear) B-splines are used, which allow for smooth estimation of the graphon, conditional on the node ordering. This provides the M-step. The true ordering of the nodes arising from the graphon model remains unobserved and Bayesian ideas are employed to obtain posterior samples given the network data. This yields the E-step. Combining both steps gives an EM-based approach for smooth graphon estimation. Unlike common other methods, this procedure does not require the restriction of a monotonic marginal function. The proposed graphon estimate allows to explore node-ordering strategies and therefore to compare the common degree-based node ranking with the ordering conditional on the network. Variability and uncertainty are taken into account using MCMC techniques. Examples and simulation studies support the applicability of the approach.

</details>

<details>

<summary>2021-09-15 18:00:04 - Shrinkage with shrunken shoulders: Gibbs sampling shrinkage model posteriors with guaranteed convergence rates</summary>

- *Akihiko Nishimura, Marc A. Suchard*

- `1911.02160v5` - [abs](http://arxiv.org/abs/1911.02160v5) - [pdf](http://arxiv.org/pdf/1911.02160v5)

> Use of continuous shrinkage priors -- with a "spike" near zero and heavy-tails towards infinity -- is an increasingly popular approach to induce sparsity in parameter estimates. When the parameters are only weakly identified by the likelihood, however, the posterior may end up with tails as heavy as the prior, jeopardizing robustness of inference. A natural solution is to "shrink the shoulders" of a shrinkage prior by lightening up its tails beyond a reasonable parameter range, yielding a regularized version of the prior. We develop a regularization approach which, unlike previous proposals, preserves computationally attractive structures of original shrinkage priors. We study theoretical properties of the Gibbs sampler on resulting posterior distributions, with emphasis on convergence rates of the P{\'o}lya-Gamma Gibbs sampler for sparse logistic regression. Our analysis shows that the proposed regularization leads to geometric ergodicity under a broad range of global-local shrinkage priors. Essentially, the only requirement is for the prior $\pi_{\rm local}$ on the local scale $\lambda$ to satisfy $\pi_{\rm local}(0) < \infty$. If $\pi_{\rm local}(\cdot)$ further satisfies $\lim_{\lambda \to 0} \pi_{\rm local}(\lambda) / \lambda^a < \infty$ for $a > 0$, as in the case of Bayesian bridge priors, we show the sampler to be uniformly ergodic.

</details>

<details>

<summary>2021-09-15 20:17:25 - Bivariate Hierarchical Bayesian Model for Combining Summary Measures and their Uncertainties from Multiple Sources</summary>

- *Yujing Yao, R. Todd Ogden, Chubing Zeng, Qixuan Chen*

- `2109.07560v1` - [abs](http://arxiv.org/abs/2109.07560v1) - [pdf](http://arxiv.org/pdf/2109.07560v1)

> It is often of interest to combine available estimates of a similar quantity from multiple data sources. When the corresponding variances of each estimate are also available, a model should take into account the uncertainty of the estimates themselves as well as the uncertainty in the estimation of variances. In addition, if there exists a strong association between estimates and their variances, the correlation between these two quantities should also be considered. In this paper, we propose a bivariate hierarchical Bayesian model that jointly models the estimates and their estimated variances assuming a correlation between these two measures. We conduct simulations to explore the performance of the proposed bivariate Bayesian model and compare it to other commonly used methods under different correlation scenarios. The proposed bivariate Bayesian model has a wide range of applications. We illustrate its application in three very different areas: PET brain imaging studies, meta-analysis, and small area estimation.

</details>

<details>

<summary>2021-09-15 20:22:09 - Non-smooth Bayesian Optimization in Tuning Problems</summary>

- *Hengrui Luo, James W. Demmel, Younghyun Cho, Xiaoye S. Li, Yang Liu*

- `2109.07563v1` - [abs](http://arxiv.org/abs/2109.07563v1) - [pdf](http://arxiv.org/pdf/2109.07563v1)

> Building surrogate models is one common approach when we attempt to learn unknown black-box functions. Bayesian optimization provides a framework which allows us to build surrogate models based on sequential samples drawn from the function and find the optimum. Tuning algorithmic parameters to optimize the performance of large, complicated "black-box" application codes is a specific important application, which aims at finding the optima of black-box functions. Within the Bayesian optimization framework, the Gaussian process model produces smooth or continuous sample paths. However, the black-box function in the tuning problem is often non-smooth. This difficult tuning problem is worsened by the fact that we usually have limited sequential samples from the black-box function. Motivated by these issues encountered in tuning, we propose a novel additive Gaussian process model called clustered Gaussian process (cGP), where the additive components are induced by clustering. In the examples we studied, the performance can be improved by as much as 90% among repetitive experiments. By using this surrogate model, we want to capture the non-smoothness of the black-box function. In addition to an algorithm for constructing this model, we also apply the model to several artificial and real applications to evaluate it.

</details>

<details>

<summary>2021-09-15 20:40:18 - Smoothing and adaptation of shifted Pólya Tree ensembles</summary>

- *Thibault Randrianarisoa*

- `2010.12299v2` - [abs](http://arxiv.org/abs/2010.12299v2) - [pdf](http://arxiv.org/pdf/2010.12299v2)

> Recently, S. Arlot and R. Genuer have shown that a model of random forests outperforms its single-tree counterpart in the estimation of $\alpha-$H\"older functions, $\alpha\leq2$. This backs up the idea that ensembles of tree estimators are smoother estimators than single trees. On the other hand, most positive optimality results on Bayesian tree-based methods assume that $\alpha\leq1$. Naturally, one wonders whether Bayesian counterparts of forest estimators are optimal on smoother classes, just like it has been observed for frequentist estimators for $\alpha\leq 2$. We dwell on the problem of density estimation and introduce an ensemble estimator from the classical (truncated) P\'olya tree construction in Bayesian nonparametrics. The resulting Bayesian forest estimator is shown to lead to optimal posterior contraction rates, up to logarithmic terms, for the Hellinger and $L^1$ distances on probability density functions on $[0;1)$ for arbitrary H\"older regularity $\alpha>0$. This improves upon previous results for constructions related to the P\'olya tree prior whose optimality was only proven in the case $\alpha\leq 1$. Also, we introduce an adaptive version of this new prior in the sense that it does not require the knowledge of $\alpha$ to be defined and attain optimality.

</details>

<details>

<summary>2021-09-16 00:03:44 - How trustworthy is your tree? Bayesian phylogenetic effective sample size through the lens of Monte Carlo error</summary>

- *Andrew F. Magee, Michael D. Karcher, Frederick A. Matsen IV, Vladimir N. Minin*

- `2109.07629v1` - [abs](http://arxiv.org/abs/2109.07629v1) - [pdf](http://arxiv.org/pdf/2109.07629v1)

> Bayesian inference is a popular and widely-used approach to infer phylogenies (evolutionary trees). However, despite decades of widespread application, it remains difficult to judge how well a given Bayesian Markov chain Monte Carlo (MCMC) run explores the space of phylogenetic trees. In this paper, we investigate the Monte Carlo error of phylogenies, focusing on high-dimensional summaries of the posterior distribution, including variability in estimated edge/branch (known in phylogenetics as "split") probabilities and tree probabilities, and variability in the estimated summary tree. Specifically, we ask if there is any measure of effective sample size (ESS) applicable to phylogenetic trees which is capable of capturing the Monte Carlo error of these three summary measures. We find that there are some ESS measures capable of capturing the error inherent in using MCMC samples to approximate the posterior distributions on phylogenies. We term these tree ESS measures, and identify a set of three which are useful in practice for assessing the Monte Carlo error. Lastly, we present visualization tools that can improve comparisons between multiple independent MCMC runs by accounting for the Monte Carlo error present in each chain. Our results indicate that common post-MCMC workflows are insufficient to capture the inherent Monte Carlo error of the tree, and highlight the need for both within-chain mixing and between-chain convergence assessments.

</details>

<details>

<summary>2021-09-16 08:26:37 - Statistical Inference for Bayesian Risk Minimization via Exponentially Tilted Empirical Likelihood</summary>

- *Rong Tang, Yun Yang*

- `2109.07792v1` - [abs](http://arxiv.org/abs/2109.07792v1) - [pdf](http://arxiv.org/pdf/2109.07792v1)

> The celebrated Bernstein von-Mises theorem ensures that credible regions from Bayesian posterior are well-calibrated when the model is correctly-specified, in the frequentist sense that their coverage probabilities tend to the nominal values as data accrue. However, this conventional Bayesian framework is known to lack robustness when the model is misspecified or only partly specified, such as in quantile regression, risk minimization based supervised/unsupervised learning and robust estimation. To overcome this difficulty, we propose a new Bayesian inferential approach that substitutes the (misspecified or partly specified) likelihoods with proper exponentially tilted empirical likelihoods plus a regularization term. Our surrogate empirical likelihood is carefully constructed by using the first order optimality condition of the empirical risk minimization as the moment condition. We show that the Bayesian posterior obtained by combining this surrogate empirical likelihood and the prior is asymptotically close to a normal distribution centering at the empirical risk minimizer with covariance matrix taking an appropriate sandwiched form. Consequently, the resulting Bayesian credible regions are automatically calibrated to deliver valid uncertainty quantification. Computationally, the proposed method can be easily implemented by Markov Chain Monte Carlo sampling algorithms. Our numerical results show that the proposed method tends to be more accurate than existing state-of-the-art competitors.

</details>

<details>

<summary>2021-09-16 09:03:25 - Estimation of Population Size with Heterogeneous Catchability and Behavioural Dependence: Applications to Air and Water Borne Disease Surveillance</summary>

- *Kiranmoy Chatterjee, Prajamitra Bhuyan*

- `2105.08679v5` - [abs](http://arxiv.org/abs/2105.08679v5) - [pdf](http://arxiv.org/pdf/2105.08679v5)

> Population size estimation based on the capture-recapture experiment is an interesting problem in various fields including epidemiology, criminology, demography, etc. In many real-life scenarios, there exists inherent heterogeneity among the individuals and dependency between capture and recapture attempts. A novel trivariate Bernoulli model is considered to incorporate these features, and the Bayesian estimation of the model parameters is suggested using data augmentation. Simulation results show robustness under model misspecification and the superiority of the performance of the proposed method over existing competitors. The method is applied to analyse real case studies on epidemiological surveillance. The results provide interesting insight on the heterogeneity and dependence involved in the capture-recapture mechanism. The methodology proposed can assist in effective decision-making and policy formulation.

</details>

<details>

<summary>2021-09-16 11:04:38 - Generalized mixtures of finite mixtures and telescoping sampling</summary>

- *Sylvia Frühwirth-Schnatter, Gertraud Malsiner-Walli, Bettina Grün*

- `2005.09918v3` - [abs](http://arxiv.org/abs/2005.09918v3) - [pdf](http://arxiv.org/pdf/2005.09918v3)

> Within a Bayesian framework, a comprehensive investigation of mixtures of finite mixtures (MFMs), i.e., finite mixtures with a prior on the number of components, is performed. This model class has applications in model-based clustering as well as for semi-parametric density estimation and requires suitable prior specifications and inference methods to exploit its full potential. We contribute by considering a generalized class of MFMs where the hyperparameter $\gamma_K$ of a symmetric Dirichlet prior on the weight distribution depends on the number of components. We show that this model class may be regarded as a Bayesian non-parametric mixture outside the class of Gibbs-type priors. We emphasize the distinction between the number of components $K$ of a mixture and the number of clusters $K_+$, i.e., the number of filled components given the data. In the MFM model, $K_+$ is a random variable and its prior depends on the prior on $K$ and on the hyperparameter $\gamma_K$. We employ a flexible prior distribution for the number of components $K$ and derive the corresponding prior on the number of clusters $K_+$ for generalized MFMs. For posterior inference, we propose the novel telescoping sampler which allows Bayesian inference for mixtures with arbitrary component distributions without resorting to reversible jump Markov chain Monte Carlo (MCMC) methods. The telescoping sampler explicitly samples the number of components, but otherwise requires only the usual MCMC steps of a finite mixture model. The ease of its application using different component distributions is demonstrated on several data sets.

</details>

<details>

<summary>2021-09-16 14:04:15 - Identification of taxon through classification with partial reject options</summary>

- *Måns Karlsson, Ola Hössjer*

- `1906.04538v4` - [abs](http://arxiv.org/abs/1906.04538v4) - [pdf](http://arxiv.org/pdf/1906.04538v4)

> Identification of taxa can significantly be assisted by statistical classification based on trait measurements in two major ways; either individually or by phylogenetic (clustering) methods. In this paper we present a general Bayesian approach for classifying species individually based on measurements of a mixture of continuous and ordinal traits as well as any type of covariates. It is assumed that the trait vector is derived from a latent variable with a multivariate Gaussian distribution. Decision rules based on supervised learning are presented that estimate model parameters through blockwise Gibbs sampling. These decision regions allow for uncertainty (partial rejection), so that not necessarily one specific category (taxon) is output when new subjects are classified, but rather a set of categories including the most probable taxa. This type of discriminant analysis employs reward functions with a set-valued input argument, so that an optimal Bayes classifier can be defined. We also present a way of safeguarding against outlying new observations, using an analogue of a $p$-value within our Bayesian setting. Our method is illustrated on an original ornithological data set of birds. We also incorporate model selection through cross-validation, examplified on another original data set of birds.

</details>

<details>

<summary>2021-09-16 15:47:50 - Multimodal Data Fusion in High-Dimensional Heterogeneous Datasets via Generative Models</summary>

- *Yasin Yilmaz, Mehmet Aktukmak, Alfred O. Hero*

- `2108.12445v2` - [abs](http://arxiv.org/abs/2108.12445v2) - [pdf](http://arxiv.org/pdf/2108.12445v2)

> The commonly used latent space embedding techniques, such as Principal Component Analysis, Factor Analysis, and manifold learning techniques, are typically used for learning effective representations of homogeneous data. However, they do not readily extend to heterogeneous data that are a combination of numerical and categorical variables, e.g., arising from linked GPS and text data. In this paper, we are interested in learning probabilistic generative models from high-dimensional heterogeneous data in an unsupervised fashion. The learned generative model provides latent unified representations that capture the factors common to the multiple dimensions of the data, and thus enable fusing multimodal data for various machine learning tasks. Following a Bayesian approach, we propose a general framework that combines disparate data types through the natural parameterization of the exponential family of distributions. To scale the model inference to millions of instances with thousands of features, we use the Laplace-Bernstein approximation for posterior computations involving nonlinear link functions. The proposed algorithm is presented in detail for the commonly encountered heterogeneous datasets with real-valued (Gaussian) and categorical (multinomial) features. Experiments on two high-dimensional and heterogeneous datasets (NYC Taxi and MovieLens-10M) demonstrate the scalability and competitive performance of the proposed algorithm on different machine learning tasks such as anomaly detection, data imputation, and recommender systems.

</details>

<details>

<summary>2021-09-16 16:11:40 - The Reciprocal Bayesian LASSO</summary>

- *Himel Mallick, Rahim Alhamzawi, Erina Paul, Vladimir Svetnik*

- `2001.08327v4` - [abs](http://arxiv.org/abs/2001.08327v4) - [pdf](http://arxiv.org/pdf/2001.08327v4)

> A reciprocal LASSO (rLASSO) regularization employs a decreasing penalty function as opposed to conventional penalization approaches that use increasing penalties on the coefficients, leading to stronger parsimony and superior model selection relative to traditional shrinkage methods. Here we consider a fully Bayesian formulation of the rLASSO problem, which is based on the observation that the rLASSO estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters are assigned independent inverse Laplace priors. Bayesian inference from this posterior is possible using an expanded hierarchy motivated by a scale mixture of double Pareto or truncated normal distributions. On simulated and real datasets, we show that the Bayesian formulation outperforms its classical cousin in estimation, prediction, and variable selection across a wide range of scenarios while offering the advantage of posterior inference. Finally, we discuss other variants of this new approach and provide a unified framework for variable selection using flexible reciprocal penalties. All methods described in this paper are publicly available as an R package at: https://github.com/himelmallick/BayesRecipe.

</details>

<details>

<summary>2021-09-16 20:32:58 - Improving Regression Uncertainty Estimation Under Statistical Change</summary>

- *Tony Tohme, Kevin Vanslette, Kamal Youcef-Toumi*

- `2109.08213v1` - [abs](http://arxiv.org/abs/2109.08213v1) - [pdf](http://arxiv.org/pdf/2109.08213v1)

> While deep neural networks are highly performant and successful in a wide range of real-world problems, estimating their predictive uncertainty remains a challenging task. To address this challenge, we propose and implement a loss function for regression uncertainty estimation based on the Bayesian Validation Metric (BVM) framework while using ensemble learning. A series of experiments on in-distribution data show that the proposed method is competitive with existing state-of-the-art methods. In addition, experiments on out-of-distribution data show that the proposed method is robust to statistical change and exhibits superior predictive capability.

</details>

<details>

<summary>2021-09-17 12:19:56 - Bayesian model selection in additive partial linear models via locally adaptive splines</summary>

- *Seonghyun Jeong, Taeyoung Park, David A. van Dyk*

- `2008.06213v2` - [abs](http://arxiv.org/abs/2008.06213v2) - [pdf](http://arxiv.org/pdf/2008.06213v2)

> We provide a flexible framework for selecting among a class of additive partial linear models that allows both linear and nonlinear additive components. In practice, it is challenging to determine which additive components should be excluded from the model while simultaneously determining whether nonzero additive components should be represented as linear or non-linear components in the final model. In this paper, we propose a Bayesian model selection method that is facilitated by a carefully specified class of models, including the choice of a prior distribution and the nonparametric model used for the nonlinear additive components. We employ a series of latent variables that determine the effect of each variable among the three possibilities (no effect, linear effect, and nonlinear effect) and that simultaneously determine the knots of each spline for a suitable penalization of smooth functions. The use of a pseudo-prior distribution along with a collapsing scheme enables us to deploy well-behaved Markov chain Monte Carlo samplers, both for model selection and for fitting the preferred model. Our method and algorithm are deployed on a suite of numerical studies and are applied to a nutritional epidemiology study. The numerical results show that the proposed methodology outperforms previously available methods in terms of effective sample sizes of the Markov chain samplers and the overall misclassification rates.

</details>

<details>

<summary>2021-09-17 17:17:45 - BDNNSurv: Bayesian deep neural networks for survival analysis using pseudo values</summary>

- *Dai Feng, Lili Zhao*

- `2101.03170v2` - [abs](http://arxiv.org/abs/2101.03170v2) - [pdf](http://arxiv.org/pdf/2101.03170v2)

> There has been increasing interest in modeling survival data using deep learning methods in medical research. In this paper, we proposed a Bayesian hierarchical deep neural networks model for modeling and prediction of survival data. Compared with previously studied methods, the new proposal can provide not only point estimate of survival probability but also quantification of the corresponding uncertainty, which can be of crucial importance in predictive modeling and subsequent decision making. The favorable statistical properties of point and uncertainty estimates were demonstrated by simulation studies and real data analysis. The Python code implementing the proposed approach was provided.

</details>

<details>

<summary>2021-09-18 17:07:24 - Asynchronous and Distributed Data Augmentation for Massive Data Settings</summary>

- *Jiayuan Zhou, Kshitij Khare, Sanvesh Srivastava*

- `2109.08969v1` - [abs](http://arxiv.org/abs/2109.08969v1) - [pdf](http://arxiv.org/pdf/2109.08969v1)

> Data augmentation (DA) algorithms are widely used for Bayesian inference due to their simplicity. In massive data settings, however, DA algorithms are prohibitively slow because they pass through the full data in any iteration, imposing serious restrictions on their usage despite the advantages. Addressing this problem, we develop a framework for extending any DA that exploits asynchronous and distributed computing. The extended DA algorithm is indexed by a parameter $r \in (0, 1)$ and is called Asynchronous and Distributed (AD) DA with the original DA as its parent. Any ADDA starts by dividing the full data into $k$ smaller disjoint subsets and storing them on $k$ processes, which could be machines or processors. Every iteration of ADDA augments only an $r$-fraction of the $k$ data subsets with some positive probability and leaves the remaining $(1-r)$-fraction of the augmented data unchanged. The parameter draws are obtained using the $r$-fraction of new and $(1-r)$-fraction of old augmented data. For many choices of $k$ and $r$, the fractional updates of ADDA lead to a significant speed-up over the parent DA in massive data settings, and it reduces to the distributed version of its parent DA when $r=1$. We show that the ADDA Markov chain is Harris ergodic with the desired stationary distribution under mild conditions on the parent DA algorithm. We demonstrate the numerical advantages of the ADDA in three representative examples corresponding to different kinds of massive data settings encountered in applications. In all these examples, our DA generalization is significantly faster than its parent DA algorithm for all the choices of $k$ and $r$. We also establish geometric ergodicity of the ADDA Markov chain for all three examples, which in turn yields asymptotically valid standard errors for estimates of desired posterior quantities.

</details>

<details>

<summary>2021-09-19 08:58:46 - Constrained School Choice with Incomplete Information</summary>

- *Hugo Gimbert, Claire Mathieu, Simon Mauras*

- `2109.09089v1` - [abs](http://arxiv.org/abs/2109.09089v1) - [pdf](http://arxiv.org/pdf/2109.09089v1)

> School choice is the two-sided matching market where students (on one side) are to be matched with schools (on the other side) based on their mutual preferences. The classical algorithm to solve this problem is the celebrated deferred acceptance procedure, proposed by Gale and Shapley. After both sides have revealed their mutual preferences, the algorithm computes an optimal stable matching. Most often in practice, notably when the process is implemented by a national clearinghouse and thousands of schools enter the market, there is a quota on the number of applications that a student can submit: students have to perform a partial revelation of their preferences, based on partial information on the market. We model this situation by drawing each student type from a publicly known distribution and study Nash equilibria of the corresponding Bayesian game. We focus on symmetric equilibria, in which all students play the same strategy. We show existence of these equilibria in the general case, and provide two algorithms to compute such equilibria under additional assumptions, including the case where schools have identical preferences over students.

</details>

<details>

<summary>2021-09-20 01:28:14 - Scalable Multi-Task Gaussian Processes with Neural Embedding of Coregionalization</summary>

- *Haitao Liu, Jiaqi Ding, Xinyu Xie, Xiaomo Jiang, Yusong Zhao, Xiaofang Wang*

- `2109.09261v1` - [abs](http://arxiv.org/abs/2109.09261v1) - [pdf](http://arxiv.org/pdf/2109.09261v1)

> Multi-task regression attempts to exploit the task similarity in order to achieve knowledge transfer across related tasks for performance improvement. The application of Gaussian process (GP) in this scenario yields the non-parametric yet informative Bayesian multi-task regression paradigm. Multi-task GP (MTGP) provides not only the prediction mean but also the associated prediction variance to quantify uncertainty, thus gaining popularity in various scenarios. The linear model of coregionalization (LMC) is a well-known MTGP paradigm which exploits the dependency of tasks through linear combination of several independent and diverse GPs. The LMC however suffers from high model complexity and limited model capability when handling complicated multi-task cases. To this end, we develop the neural embedding of coregionalization that transforms the latent GPs into a high-dimensional latent space to induce rich yet diverse behaviors. Furthermore, we use advanced variational inference as well as sparse approximation to devise a tight and compact evidence lower bound (ELBO) for higher quality of scalable model inference. Extensive numerical experiments have been conducted to verify the higher prediction quality and better generalization of our model, named NSVLMC, on various real-world multi-task datasets and the cross-fluid modeling of unsteady fluidized bed.

</details>

<details>

<summary>2021-09-20 01:55:43 - Computationally Efficient High-Dimensional Bayesian Optimization via Variable Selection</summary>

- *Yihang Shen, Carl Kingsford*

- `2109.09264v1` - [abs](http://arxiv.org/abs/2109.09264v1) - [pdf](http://arxiv.org/pdf/2109.09264v1)

> Bayesian Optimization (BO) is a method for globally optimizing black-box functions. While BO has been successfully applied to many scenarios, developing effective BO algorithms that scale to functions with high-dimensional domains is still a challenge. Optimizing such functions by vanilla BO is extremely time-consuming. Alternative strategies for high-dimensional BO that are based on the idea of embedding the high-dimensional space to the one with low dimension are sensitive to the choice of the embedding dimension, which needs to be pre-specified. We develop a new computationally efficient high-dimensional BO method that exploits variable selection. Our method is able to automatically learn axis-aligned sub-spaces, i.e. spaces containing selected variables, without the demand of any pre-specified hyperparameters. We theoretically analyze the computational complexity of our algorithm and derive the regret bound. We empirically show the efficacy of our method on several synthetic and real problems.

</details>

<details>

<summary>2021-09-20 07:43:52 - Estimation of Measures for Two-Way Contingency Tables Using the Bayesian Estimators</summary>

- *Tomotaka Momozaki, Koji Cho, Tomoyuki Nakagawa, Sadao Tomizawa*

- `2109.09339v1` - [abs](http://arxiv.org/abs/2109.09339v1) - [pdf](http://arxiv.org/pdf/2109.09339v1)

> In the analysis of two-way contingency tables, the measures for representing the degree of departure from independence, symmetry or asymmetry are often used. These measures in contingency tables are expressed as functions of the probability structure of the tables. Hence, the value of a measure is estimated. Plug-in estimators of measures with sample proportions are used to estimate the measures, but without sufficient sample size, the bias and mean squared error (MSE) of the estimators become large. This study proposes an estimator that can reduce the bias and MSE, even without a sufficient sample size, using the Bayesian estimators of cell probabilities. We asymptotically evaluate the MSE of the estimator of the measure plugging in the posterior means of the cell probabilities when the prior distribution of the cell probabilities is the Dirichlet distribution. As a result, we can derive the Dirichlet parameter that asymptotically minimizes the MSE of the estimator. Numerical experiments show that the proposed estimator has a smaller bias and MSE than the plug-in estimator with sample proportions, uniform prior, and Jeffreys prior. Another advantage of our approach is the construction of credible intervals for measures using Monte Carlo simulations.

</details>

<details>

<summary>2021-09-20 12:48:57 - Coefficients of factor score determinacy for mean plausible values of Bayesian factor analysis</summary>

- *André Beauducel, Norbert Hilger*

- `2109.09494v1` - [abs](http://arxiv.org/abs/2109.09494v1) - [pdf](http://arxiv.org/pdf/2109.09494v1)

> In the context of Bayesian factor analysis, it is possible to compute mean plausible values, which might be used as covariates or predictors or in order to provide individual scores for the Bayesian latent variables. Previous simulation studies ascertained the validity of the plausible values by the mean squared difference of the plausible values and the generating factor scores. However, the generating factor scores are unknown in empirical studies so that an indicator that is solely based on model parameters is needed in order to evaluate the validity of factor score estimates in empirical studies. The coefficient of determinacy is based on model parameters and can be computed whenever Bayesian factor analysis is performed in empirical settings. Therefore, the central aim of the present simulation study was to compare the coefficient of determinacy based on model parameters with the correlation of mean plausible values with the generating factors. It was found that the coefficient of determinacy yields an acceptable estimate for the validity of mean plausible values. As for small sample sizes and a small salient loading size the coefficient of determinacy overestimates the validity, it is recommended to report the coefficient of determinacy together with a bias-correction in order to estimate the validity of mean plausible values in empirical settings.

</details>

<details>

<summary>2021-09-20 13:21:39 - Deep Bayesian Estimation for Dynamic Treatment Regimes with a Long Follow-up Time</summary>

- *Adi Lin, Jie Lu, Junyu Xuan, Fujin Zhu, Guangquan Zhang*

- `2109.11929v1` - [abs](http://arxiv.org/abs/2109.11929v1) - [pdf](http://arxiv.org/pdf/2109.11929v1)

> Causal effect estimation for dynamic treatment regimes (DTRs) contributes to sequential decision making. However, censoring and time-dependent confounding under DTRs are challenging as the amount of observational data declines over time due to a reducing sample size but the feature dimension increases over time. Long-term follow-up compounds these challenges. Another challenge is the highly complex relationships between confounders, treatments, and outcomes, which causes the traditional and commonly used linear methods to fail. We combine outcome regression models with treatment models for high dimensional features using uncensored subjects that are small in sample size and we fit deep Bayesian models for outcome regression models to reveal the complex relationships between confounders, treatments, and outcomes. Also, the developed deep Bayesian models can model uncertainty and output the prediction variance which is essential for the safety-aware applications, such as self-driving cars and medical treatment design. The experimental results on medical simulations of HIV treatment show the ability of the proposed method to obtain stable and accurate dynamic causal effect estimation from observational data, especially with long-term follow-up. Our technique provides practical guidance for sequential decision making, and policy-making.

</details>

<details>

<summary>2021-09-20 13:40:05 - Bayesian Paired-Comparison with the bpcs Package</summary>

- *David Issa Mattos, Érika Martins Silva Ramos*

- `2101.11227v4` - [abs](http://arxiv.org/abs/2101.11227v4) - [pdf](http://arxiv.org/pdf/2101.11227v4)

> This article introduces the bpcs R package (Bayesian Paired Comparison in Stan) and the statistical models implemented in the package. This package aims to facilitate the use of Bayesian models for paired comparison data in behavioral research. Bayesian analysis of paired comparison data allows parameter estimation even in conditions where the maximum likelihood does not exist, allows easy extension of paired comparison models, provide straightforward interpretation of the results with credible intervals, have better control of type I error, have more robust evidence towards the null hypothesis, allows propagation of uncertainties, includes prior information, and perform well when handling models with many parameters and latent variables. The bpcs package provides a consistent interface for R users and several functions to evaluate the posterior distribution of all parameters, to estimate the posterior distribution of any contest between items, and to obtain the posterior distribution of the ranks. Three reanalyses of recent studies that used the frequentist Bradley-Terry model are presented. These reanalyses are conducted with the Bayesian models of the bpcs package, and all the code used to fit the models, generate the figures, and the tables are available in the online appendix.

</details>

<details>

<summary>2021-09-20 22:33:20 - Overinference from Weak Signals, Underinference from Strong Signals, and the Psychophysics of Interpreting Information</summary>

- *Michael Thaler*

- `2109.09871v1` - [abs](http://arxiv.org/abs/2109.09871v1) - [pdf](http://arxiv.org/pdf/2109.09871v1)

> Numerous experiments have found that when people receive signals that would lead a Bayesian to substantially revise beliefs, they underinfer. This paper experimentally considers inference from a wider range of signal strengths and finds that subjects overinfer when signals are sufficiently weak. A model of cognitive imprecision adapted to study misperceptions about signal strength explains the data well. As the theory predicts, subjects with more experience, greater cognitive sophistication, and lower variance in answers, infer less from weak signals and infer more from strong signals. The results also relate misperceptions to demand for information and inference from multiple signals.

</details>

<details>

<summary>2021-09-21 04:12:02 - A Bayesian Hidden Semi-Markov Model with Covariate-Dependent State Duration Parameters for High-Frequency Environmental Data</summary>

- *Shirley Rojas-Salazar, Erin M. Schliep, Christopher K. Wikle, Emily H. Stanley, Stephen R. Carpenter, Noah R. Lottig*

- `2109.09949v1` - [abs](http://arxiv.org/abs/2109.09949v1) - [pdf](http://arxiv.org/pdf/2109.09949v1)

> Environmental time series data observed at high frequencies can be studied with approaches such as hidden Markov and semi-Markov models (HMM and HSMM). HSMMs extend the HMM by explicitly modeling the time spent in each state. In a discrete-time HSMM, the duration in each state can be modeled with a zero-truncated Poisson distribution, where the duration parameter may be state-specific but constant in time. We extend the HSMM by allowing the state-specific duration parameters to vary in time and model them as a function of known covariates observed over a period of time leading up to a state transition. In addition, we propose a data subsampling approach given that high-frequency data can violate the conditional independence assumption of the HSMM. We apply the model to high-frequency data collected by an instrumented buoy in Lake Mendota. We model the phycocyanin concentration, which is used in aquatic systems to estimate the relative abundance of blue-green algae, and identify important time-varying effects associated with the duration in each state.

</details>

<details>

<summary>2021-09-21 17:30:14 - A Bayesian hierarchical model for disease mapping that accounts for scaling and heavy-tailed latent effects</summary>

- *Victoire Michal, Laís Picinini Freitas, Alexandra M. Schmidt*

- `2109.10330v1` - [abs](http://arxiv.org/abs/2109.10330v1) - [pdf](http://arxiv.org/pdf/2109.10330v1)

> In disease mapping, the relative risk of a disease is commonly estimated across different areas within a region of interest. The number of cases in an area is often assumed to follow a Poisson distribution whose mean is decomposed as the product between an offset and the logarithm of the disease's relative risk. The log risk may be written as the sum of fixed effects and latent random effects. The commonly used BYM model further decomposes the latent effects into a sum of independent effects and spatial effects to account for potential overdispersion and a spatial correlation structure among the counts. However, this model suffers from an identifiably issue. The BYM2 model reparametrises the latter by decomposing each latent effect into a weighted sum of independent and spatial effects. We build on the BYM2 model to allow for heavy-tailed latent effects and accommodate potentially outlying risks, after accounting for the fixed effects. We assume a scale mixture structure wherein the variance of the latent process changes across areas and allows for outlier identification. We explore two prior specifications of this scale mixture structure in simulation studies and in the analysis of Zika cases from the 2015-2016 epidemic in Rio de Janeiro. The simulation studies show that, in terms of WAIC and outlier detection, the two parametrisations always perform well compared to commonly used models. Our analysis of Zika cases finds 19 districts of Rio as potential outliers, after accounting for the socio-development index, which may help prioritise interventions.

</details>

<details>

<summary>2021-09-21 20:56:32 - Active inference, Bayesian optimal design, and expected utility</summary>

- *Noor Sajid, Lancelot Da Costa, Thomas Parr, Karl Friston*

- `2110.04074v1` - [abs](http://arxiv.org/abs/2110.04074v1) - [pdf](http://arxiv.org/pdf/2110.04074v1)

> Active inference, a corollary of the free energy principle, is a formal way of describing the behavior of certain kinds of random dynamical systems that have the appearance of sentience. In this chapter, we describe how active inference combines Bayesian decision theory and optimal Bayesian design principles under a single imperative to minimize expected free energy. It is this aspect of active inference that allows for the natural emergence of information-seeking behavior. When removing prior outcomes preferences from expected free energy, active inference reduces to optimal Bayesian design, i.e., information gain maximization. Conversely, active inference reduces to Bayesian decision theory in the absence of ambiguity and relative risk, i.e., expected utility maximization. Using these limiting cases, we illustrate how behaviors differ when agents select actions that optimize expected utility, expected information gain, and expected free energy. Our T-maze simulations show optimizing expected free energy produces goal-directed information-seeking behavior while optimizing expected utility induces purely exploitive behavior and maximizing information gain engenders intrinsically motivated behavior.

</details>

<details>

<summary>2021-09-22 01:47:29 - Kernel Machine and Distributed Lag Models for Assessing Windows of Susceptibility to Environmental Mixtures in Children's Health Studies</summary>

- *Ander Wilson, Hsiao-Hsien Leon Hsu, Yueh-Hsiu Mathilda Chiu, Robert O. Wright, Rosalind J. Wright, Brent A. Coull*

- `1904.12417v7` - [abs](http://arxiv.org/abs/1904.12417v7) - [pdf](http://arxiv.org/pdf/1904.12417v7)

> Exposures to environmental chemicals during gestation can alter health status later in life. Most studies of maternal exposure to chemicals during pregnancy have focused on a single chemical exposure observed at high temporal resolution. Recent research has turned to focus on exposure to mixtures of multiple chemicals, generally observed at a single time point. We consider statistical methods for analyzing data on chemical mixtures that are observed at a high temporal resolution. As motivation, we analyze the association between exposure to four ambient air pollutants observed weekly throughout gestation and birth weight in a Boston-area prospective birth cohort. To explore patterns in the data, we first apply methods for analyzing data on (1) a single chemical observed at high temporal resolution, and (2) a mixture measured at a single point in time. We highlight the shortcomings of these approaches for temporally-resolved data on exposure to chemical mixtures. Second, we propose a novel method, a Bayesian kernel machine regression distributed lag model (BKMR-DLM), that simultaneously accounts for nonlinear associations and interactions among time-varying measures of exposure to mixtures. BKMR-DLM uses a functional weight for each exposure that parameterizes the window of susceptibility corresponding to that exposure within a kernel machine framework that captures non-linear and interaction effects of the multivariate exposure on the outcome. In a simulation study, we show that the proposed method can better estimate the exposure-response function and, in high signal settings, can identify critical windows in time during which exposure has an increased association with the outcome. Applying the proposed method to the Boston birth cohort data, we find evidence of a negative association between organic carbon and birth weight and that nitrate modifies the organic carbon, ...

</details>

<details>

<summary>2021-09-22 09:47:47 - Two-level Bayesian interaction analysis for survival data incorporating pathway information</summary>

- *Xing Qin, Shuangge Ma, Mengyun Wu*

- `2109.10621v1` - [abs](http://arxiv.org/abs/2109.10621v1) - [pdf](http://arxiv.org/pdf/2109.10621v1)

> Genetic interactions play an important role in the progression of complex diseases, providing explanation of variations in disease phenotype missed by main genetic effects. Comparatively, there are fewer investigations on prognostic survival time, given its challenging characteristics such as censoring. In recent biomedical research, two-level analysis of both genes and their involved pathways has received much attention and been demonstrated to be more effective than single-level analysis, however such analysis is limited to main effects. Pathways are not isolated and their interactions have also been suggested to have important contributions to the prognosis of complex diseases. In this article, we develop a novel two-level Bayesian interaction analysis approach for survival data. This approach is the first to conduct the analysis of lower-level gene-gene interactions and higher-level pathway-pathway interactions simultaneously. Significantly advancing from existing Bayesian studies based on the Markov Chain Monte Carlo (MCMC) technique, we propose a variational inference framework based on the accelerated failure time model with favourable priors to account for two-level selection as well as censoring. The computational efficiency is much desirable for high dimensional interaction analysis. We examine performance of the proposed approach using extensive simulation. Application to TCGA melanoma and lung adenocarcinoma data leads to biologically sensible findings with satisfactory prediction accuracy and selection stability.

</details>

<details>

<summary>2021-09-22 12:59:45 - On Resource-Efficient Bayesian Network Classifiers and Deep Neural Networks</summary>

- *Wolfgang Roth, Günther Schindler, Holger Fröning, Franz Pernkopf*

- `2010.11773v2` - [abs](http://arxiv.org/abs/2010.11773v2) - [pdf](http://arxiv.org/pdf/2010.11773v2)

> We present two methods to reduce the complexity of Bayesian network (BN) classifiers. First, we introduce quantization-aware training using the straight-through gradient estimator to quantize the parameters of BNs to few bits. Second, we extend a recently proposed differentiable tree-augmented naive Bayes (TAN) structure learning approach by also considering the model size. Both methods are motivated by recent developments in the deep learning community, and they provide effective means to trade off between model size and prediction accuracy, which is demonstrated in extensive experiments. Furthermore, we contrast quantized BN classifiers with quantized deep neural networks (DNNs) for small-scale scenarios which have hardly been investigated in the literature. We show Pareto optimal models with respect to model size, number of operations, and test error and find that both model classes are viable options.

</details>

<details>

<summary>2021-09-22 14:27:19 - Tighter risk certificates for neural networks</summary>

- *María Pérez-Ortiz, Omar Rivasplata, John Shawe-Taylor, Csaba Szepesvári*

- `2007.12911v3` - [abs](http://arxiv.org/abs/2007.12911v3) - [pdf](http://arxiv.org/pdf/2007.12911v3)

> This paper presents an empirical study regarding training probabilistic neural networks using training objectives derived from PAC-Bayes bounds. In the context of probabilistic neural networks, the output of training is a probability distribution over network weights. We present two training objectives, used here for the first time in connection with training neural networks. These two training objectives are derived from tight PAC-Bayes bounds. We also re-implement a previously used training objective based on a classical PAC-Bayes bound, to compare the properties of the predictors learned using the different training objectives. We compute risk certificates for the learnt predictors, based on part of the data used to learn the predictors. We further experiment with different types of priors on the weights (both data-free and data-dependent priors) and neural network architectures. Our experiments on MNIST and CIFAR-10 show that our training methods produce competitive test set errors and non-vacuous risk bounds with much tighter values than previous results in the literature, showing promise not only to guide the learning algorithm through bounding the risk but also for model selection. These observations suggest that the methods studied here might be good candidates for self-certified learning, in the sense of using the whole data set for learning a predictor and certifying its risk on any unseen data (from the same distribution as the training data) potentially without the need for holding out test data.

</details>

<details>

<summary>2021-09-22 17:13:32 - On Reparameterization Invariant Bayesian Point Estimates and Credible Regions</summary>

- *Aki Vehtari*

- `2109.10843v1` - [abs](http://arxiv.org/abs/2109.10843v1) - [pdf](http://arxiv.org/pdf/2109.10843v1)

> This paper considers reparameterization invariant Bayesian point estimates and credible regions of model parameters for scientific inference and communication. The effect of intrinsic loss function choice in Bayesian intrinsic estimates and regions is studied with the following findings. A particular intrinsic loss function, using Kullback-Leibler divergence from the full model to the restricted model, has strong connection to a Bayesian predictive criterion, which produces point estimates with the best predictive performance. An alternative intrinsic loss function, using Kullback-Leibler divergence from the restricted model to the full model, produces estimates with interesting frequency properties for at least some commonly used distributions, that is, unbiased minimum variance estimates of the location and scale parameters.

</details>

<details>

<summary>2021-09-22 17:51:22 - Greater Than the Sum of its Parts: Computationally Flexible Bayesian Hierarchical Modeling</summary>

- *Devin S. Johnson, Brian M. Brost, Mevin B. Hooten*

- `2010.12568v2` - [abs](http://arxiv.org/abs/2010.12568v2) - [pdf](http://arxiv.org/pdf/2010.12568v2)

> We propose a multistage method for making inference at all levels of a Bayesian hierarchical model (BHM) using natural data partitions to increase efficiency by allowing computations to take place in parallel form using software that is most appropriate for each data partition. The full hierarchical model is then approximated by the product of independent normal distributions for the data component of the model. In the second stage, the Bayesian maximum {\it a posteriori} (MAP) estimator is found by maximizing the approximated posterior density with respect to the parameters. If the parameters of the model can be represented as normally distributed random effects then the second stage optimization is equivalent to fitting a multivariate normal linear mixed model. This method can be extended to account for common fixed parameters shared between data partitions, as well as parameters that are distinct between partitions. In the case of distinct parameter estimation, we consider a third stage that re-estimates the distinct parameters for each data partition based on the results of the second stage. This allows more information from the entire data set to properly inform the posterior distributions of the distinct parameters. The method is demonstrated with two ecological data sets and models, a random effects GLM and an Integrated Population Model (IPM). The multistage results were compared to estimates from models fit in single stages to the entire data set. Both examples demonstrate that multistage point and posterior standard deviation estimates closely approximate those obtained from fitting the models with all data simultaneously and can therefore be considered for fitting hierarchical Bayesian models when it is computationally prohibitive to do so in one step.

</details>

<details>

<summary>2021-09-22 17:59:05 - A Robust Asymmetric Kernel Function for Bayesian Optimization, with Application to Image Defect Detection in Manufacturing Systems</summary>

- *Areej AlBahar, Inyoung Kim, Xiaowei Yue*

- `2109.10898v1` - [abs](http://arxiv.org/abs/2109.10898v1) - [pdf](http://arxiv.org/pdf/2109.10898v1)

> Some response surface functions in complex engineering systems are usually highly nonlinear, unformed, and expensive-to-evaluate. To tackle this challenge, Bayesian optimization, which conducts sequential design via a posterior distribution over the objective function, is a critical method used to find the global optimum of black-box functions. Kernel functions play an important role in shaping the posterior distribution of the estimated function. The widely used kernel function, e.g., radial basis function (RBF), is very vulnerable and susceptible to outliers; the existence of outliers is causing its Gaussian process surrogate model to be sporadic. In this paper, we propose a robust kernel function, Asymmetric Elastic Net Radial Basis Function (AEN-RBF). Its validity as a kernel function and computational complexity are evaluated. When compared to the baseline RBF kernel, we prove theoretically that AEN-RBF can realize smaller mean squared prediction error under mild conditions. The proposed AEN-RBF kernel function can also realize faster convergence to the global optimum. We also show that the AEN-RBF kernel function is less sensitive to outliers, and hence improves the robustness of the corresponding Bayesian optimization with Gaussian processes. Through extensive evaluations carried out on synthetic and real-world optimization problems, we show that AEN-RBF outperforms existing benchmark kernel functions.

</details>

<details>

<summary>2021-09-22 18:37:30 - Bayesian Nonparametric Modelling of Conditional Multidimensional Dependence Structures</summary>

- *Rosario Barone, Luciana Dalla Valle*

- `2109.10969v1` - [abs](http://arxiv.org/abs/2109.10969v1) - [pdf](http://arxiv.org/pdf/2109.10969v1)

> In recent years, conditional copulas, that allow dependence between variables to vary according to the values of one or more covariates, have attracted increasing attention. In high dimension, vine copulas offer greater flexibility compared to multivariate copulas, since they are constructed using bivariate copulas as building blocks. In this paper we present a novel inferential approach for multivariate distributions, which combines the flexibility of vine constructions with the advantages of Bayesian nonparametrics, not requiring the specification of parametric families for each pair copula. Expressing multivariate copulas using vines allows us to easily account for covariate specifications driving the dependence between response variables. More precisely, we specify the vine copula density as an infinite mixture of Gaussian copulas, defining a Dirichlet process (DP) prior on the mixing measure, and we perform posterior inference via Markov chain Monte Carlo (MCMC) sampling. Our approach is successful as for clustering as well as for density estimation. We carry out intensive simulation studies and apply the proposed approach to investigate the impact of natural disasters on financial development. Our results show that the methodology is able to capture the heterogeneity in the dataset and to reveal different behaviours of different country clusters in relation to natural disasters.

</details>

<details>

<summary>2021-09-22 19:34:47 - High-dimensional structure learning of sparse vector autoregressive models using fractional marginal pseudo-likelihood</summary>

- *Kimmo Suotsalo, Yingying Xu, Jukka Corander, Johan Pensar*

- `2011.01484v2` - [abs](http://arxiv.org/abs/2011.01484v2) - [pdf](http://arxiv.org/pdf/2011.01484v2)

> Learning vector autoregressive models from multivariate time series is conventionally approached through least squares or maximum likelihood estimation. These methods typically assume a fully connected model which provides no direct insight to the model structure and may lead to highly noisy estimates of the parameters. Because of these limitations, there has been an increasing interest towards methods that produce sparse estimates through penalized regression. However, such methods are computationally intensive and may become prohibitively time-consuming when the number of variables in the model increases. In this paper we adopt an approximate Bayesian approach to the learning problem by combining fractional marginal likelihood and pseudo-likelihood. We propose a novel method, PLVAR, that is both faster and produces more accurate estimates than the state-of-the-art methods based on penalized regression. We prove the consistency of the PLVAR estimator and demonstrate the attractive performance of the method on both simulated and real-world data.

</details>

<details>

<summary>2021-09-23 10:12:50 - On the parametrization of epidemiologic models -- lessons from modelling COVID-19 epidemic</summary>

- *Yuri Kheifetz, Holger Kirsten, Markus Scholz*

- `2109.11916v1` - [abs](http://arxiv.org/abs/2109.11916v1) - [pdf](http://arxiv.org/pdf/2109.11916v1)

> A plethora of prediction models of SARS-CoV-2 pandemic were proposed in the past. Prediction performances not only depend on the structure and features of the model, but also on its parametrization. Official databases are often biased due to lag in reporting of cases, changing testing policy or incompleteness of data. Moreover, model parametrization is time-dependent e.g. due to changing age-structures, new emerging virus variants, non-pharmaceutical interventions and ongoing vaccination programs. To cover these aspects, we develop a principled approach to parametrize SIR-type epidemiologic models of different complexities by embedding the model structure as a hidden layer into a general Input-Output Non-Linear Dynamical System (IO-NLDS). Non-explicitly modelled impacts on the system are imposed as inputs of the system. Observable data are coupled to hidden states of the model by appropriate data models considering possible biases of the data. We estimate model parameters including their time-dependence by a Bayesian knowledge synthesis process considering parameter ranges derived from external studies as prior information. We applied this approach on a SIR-type model and data of Germany and Saxony demonstrating good prediction performances. By our approach, we can estimate and compare for example the relative effectiveness of non-pharmaceutical interventions and can provide scenarios of the future course of the epidemic under specified conditions. Our method of parameter estimation can be translated to other data sets, i.e. other countries and other SIR-type models even for other disease contexts.

</details>

<details>

<summary>2021-09-23 10:54:43 - Properties of using Fisher information gain for Bayesian design of experiments</summary>

- *Antony M. Overstall*

- `2003.07315v4` - [abs](http://arxiv.org/abs/2003.07315v4) - [pdf](http://arxiv.org/pdf/2003.07315v4)

> The Bayesian decision-theoretic approach to design of experiments involves specifying a design (values of all controllable variables) to maximise the expected utility function (expectation with respect to the distribution of responses and parameters). For most common utility functions, the expected utility is rarely available in closed form and requires a computationally expensive approximation which then needs to be maximised over the space of all possible designs. This hinders practical use of the Bayesian approach to find experimental designs. However, recently, a new utility called Fisher information gain has been proposed. The resulting expected Fisher information gain reduces to the prior expectation of the trace of the Fisher information matrix. Since the Fisher information is often available in closed form, this significantly simplifies approximation and subsequent identification of optimal designs. In this paper, it is shown that for exponential family models, maximising the expected Fisher information gain is equivalent to maximising an alternative objective function over a reduced-dimension space, simplifying even further the identification of optimal designs. However, if this function does not have enough global maxima, then designs that maximise the expected Fisher information gain lead to non-identifiablility.

</details>

<details>

<summary>2021-09-23 11:06:06 - A numerically stable algorithm for integrating Bayesian models using Markov melding</summary>

- *Andrew A. Manderson, Robert J. B. Goudie*

- `2001.08038v2` - [abs](http://arxiv.org/abs/2001.08038v2) - [pdf](http://arxiv.org/pdf/2001.08038v2)

> When statistical analyses consider multiple data sources, Markov melding provides a method for combining the source-specific Bayesian models. Markov melding joins together submodels that have a common quantity. One challenge is that the prior for this quantity can be implicit, and its prior density must be estimated. We show that error in this density estimate makes the two-stage Markov chain Monte Carlo sampler employed by Markov melding unstable and unreliable. We propose a robust two-stage algorithm that estimates the required prior marginal self-density ratios using weighted samples, dramatically improving accuracy in the tails of the distribution. The stabilised version of the algorithm is pragmatic and provides reliable inference. We demonstrate our approach using an evidence synthesis for inferring HIV prevalence, and an evidence synthesis of A/H1N1 influenza.

</details>

<details>

<summary>2021-09-23 18:13:34 - Bayesian inference of an uncertain generalized diffusion operator</summary>

- *Teresa Portone, Robert D. Moser*

- `2105.01807v2` - [abs](http://arxiv.org/abs/2105.01807v2) - [pdf](http://arxiv.org/pdf/2105.01807v2)

> This paper defines a novel Bayesian inverse problem to infer an infinite-dimensional uncertain operator appearing in a differential equation, whose action on an observable state variable affects its dynamics. Inference is made tractable by parametrizing the operator using its eigendecomposition. The plausibility of operator inference in the sparse data regime is explored in terms of an uncertain, generalized diffusion operator appearing in an evolution equation for a contaminant's transport through a heterogeneous porous medium. Sparse data are augmented with prior information through the imposition of deterministic constraints on the eigendecomposition and the use of qualitative information about the system in the definition of the prior distribution. Limited observations of the state variable's evolution are used as data for inference, and the dependence on the solution of the inverse problem is studied as a function of the frequency of observations, as well as on whether or not the data is collected as a spatial or time series.

</details>

<details>

<summary>2021-09-23 20:46:38 - An introduction to the determination of the probability of a successful trial: Frequentist and Bayesian approaches</summary>

- *Madan G. Kundu, Sandipan Samanta, Shoubhik Mondal*

- `2102.13550v2` - [abs](http://arxiv.org/abs/2102.13550v2) - [pdf](http://arxiv.org/pdf/2102.13550v2)

> Determination of posterior probability for go-no-go decision and predictive power are becoming increasingly common for resource optimization in clinical investigation. There are vast published literature on these topics; however, the terminologies are not consistently used across the literature. Further, there is a lack of consolidated presentation of various concepts of the probability of success. We attempted to fill this gap. This paper first provides a detailed derivation of these probability of success measures under the frequentist and Bayesian paradigms in a general setting. Subsequently, we have presented the analytical formula for these probability of success measures for continuous, binary, and time-to-event endpoints separately. This paper can be used as a single point reference to determine the following measures: (a) the conditional power (CP) based on interim results, (b) the predictive power of success (PPoS) based on interim results with or without prior distribution, and (d) the probability of success (PoS) for a prospective trial at the design stage. We have discussed both clinical success and trial success. This paper's discussion is mostly based on the normal approximation for prior distribution and the estimate of the parameter of interest. Besides, predictive power using the beta prior for the binomial case is also presented. Some examples are given for illustration. R functions to calculate CP and PPoS are available through the LongCART package. An R shiny app is also available at https://ppos.herokuapp.com/.

</details>

<details>

<summary>2021-09-24 01:43:58 - Dimension-Grouped Mixed Membership Models for Multivariate Categorical Data</summary>

- *Yuqi Gu, Elena A. Erosheva, Gongjun Xu, David B. Dunson*

- `2109.11705v1` - [abs](http://arxiv.org/abs/2109.11705v1) - [pdf](http://arxiv.org/pdf/2109.11705v1)

> Mixed Membership Models (MMMs) are a popular family of latent structure models for complex multivariate data. Instead of forcing each subject to belong to a single cluster, MMMs incorporate a vector of subject-specific weights characterizing partial membership across clusters. With this flexibility come challenges in uniquely identifying, estimating, and interpreting the parameters. In this article, we propose a new class of Dimension-Grouped MMMs (Gro-M$^3$s) for multivariate categorical data, which improve parsimony and interpretability. In Gro-M$^3$s, observed variables are partitioned into groups such that the latent membership is constant across variables within a group but can differ across groups. Traditional latent class models are obtained when all variables are in one group, while traditional MMMs are obtained when each variable is in its own group. The new model corresponds to a novel decomposition of probability tensors. Theoretically, we propose transparent identifiability conditions for both the unknown grouping structure and the associated model parameters in general settings. Methodologically, we propose a Bayesian approach for Dirichlet Gro-M$^3$s to inferring the variable grouping structure and estimating model parameters. Simulation results demonstrate good computational performance and empirically confirm the identifiability results. We illustrate the new methodology through an application to a functional disability dataset.

</details>

<details>

<summary>2021-09-24 07:48:34 - Regularization Guarantees Generalization in Bayesian Reinforcement Learning through Algorithmic Stability</summary>

- *Aviv Tamar, Daniel Soudry, Ev Zisselman*

- `2109.11792v1` - [abs](http://arxiv.org/abs/2109.11792v1) - [pdf](http://arxiv.org/pdf/2109.11792v1)

> In the Bayesian reinforcement learning (RL) setting, a prior distribution over the unknown problem parameters -- the rewards and transitions -- is assumed, and a policy that optimizes the (posterior) expected return is sought. A common approximation, which has been recently popularized as meta-RL, is to train the agent on a sample of $N$ problem instances from the prior, with the hope that for large enough $N$, good generalization behavior to an unseen test instance will be obtained. In this work, we study generalization in Bayesian RL under the probably approximately correct (PAC) framework, using the method of algorithmic stability. Our main contribution is showing that by adding regularization, the optimal policy becomes stable in an appropriate sense. Most stability results in the literature build on strong convexity of the regularized loss -- an approach that is not suitable for RL as Markov decision processes (MDPs) are not convex. Instead, building on recent results of fast convergence rates for mirror descent in regularized MDPs, we show that regularized MDPs satisfy a certain quadratic growth criterion, which is sufficient to establish stability. This result, which may be of independent interest, allows us to study the effect of regularization on generalization in the Bayesian RL setting.

</details>

<details>

<summary>2021-09-24 07:59:07 - Scalable Bayesian high-dimensional local dependence learning</summary>

- *Kyoungjae Lee, Lizhen Lin*

- `2109.11795v1` - [abs](http://arxiv.org/abs/2109.11795v1) - [pdf](http://arxiv.org/pdf/2109.11795v1)

> In this work, we propose a scalable Bayesian procedure for learning the local dependence structure in a high-dimensional model where the variables possess a natural ordering. The ordering of variables can be indexed by time, the vicinities of spatial locations, and so on, with the natural assumption that variables far apart tend to have weak correlations. Applications of such models abound in a variety of fields such as finance, genome associations analysis and spatial modeling. We adopt a flexible framework under which each variable is dependent on its neighbors or predecessors, and the neighborhood size can vary for each variable. It is of great interest to reveal this local dependence structure by estimating the covariance or precision matrix while yielding a consistent estimate of the varying neighborhood size for each variable. The existing literature on banded covariance matrix estimation, which assumes a fixed bandwidth cannot be adapted for this general setup. We employ the modified Cholesky decomposition for the precision matrix and design a flexible prior for this model through appropriate priors on the neighborhood sizes and Cholesky factors. The posterior contraction rates of the Cholesky factor are derived which are nearly or exactly minimax optimal, and our procedure leads to consistent estimates of the neighborhood size for all the variables. Another appealing feature of our procedure is its scalability to models with large numbers of variables due to efficient posterior inference without resorting to MCMC algorithms. Numerical comparisons are carried out with competitive methods, and applications are considered for some real datasets.

</details>

<details>

<summary>2021-09-24 10:28:46 - Quantification of empirical determinacy: the impact of likelihood weighting on posterior location and spread in Bayesian meta-analysis estimated with JAGS and INLA</summary>

- *Sona Hunanyan, Håvard Rue, Martyn Plummer, Małgorzata Roos*

- `2109.11870v1` - [abs](http://arxiv.org/abs/2109.11870v1) - [pdf](http://arxiv.org/pdf/2109.11870v1)

> The popular Bayesian meta-analysis expressed by Bayesian normal-normal hierarchical model (NNHM) synthesizes knowledge from several studies and is highly relevant in practice. Moreover, NNHM is the simplest Bayesian hierarchical model (BHM), which illustrates problems typical in more complex BHMs. Until now, it has been unclear to what extent the data determines the marginal posterior distributions of the parameters in NNHM. To address this issue we computed the second derivative of the Bhattacharyya coefficient with respect to the weighted likelihood, defined the total empirical determinacy (TED), the proportion of the empirical determinacy of location to TED (pEDL), and the proportion of the empirical determinacy of spread to TED (pEDS). We implemented this method in the R package \texttt{ed4bhm} and considered two case studies and one simulation study. We quantified TED, pEDL and pEDS under different modeling conditions such as model parametrization, the primary outcome, and the prior. This clarified to what extent the location and spread of the marginal posterior distributions of the parameters are determined by the data. Although these investigations focused on Bayesian NNHM, the method proposed is applicable more generally to complex BHMs.

</details>

<details>

<summary>2021-09-24 14:36:41 - Deep Generative Models for Reject Inference in Credit Scoring</summary>

- *Rogelio A. Mancisidor, Michael Kampffmeyer, Kjersti Aas, Robert Jenssen*

- `1904.11376v2` - [abs](http://arxiv.org/abs/1904.11376v2) - [pdf](http://arxiv.org/pdf/1904.11376v2)

> Credit scoring models based on accepted applications may be biased and their consequences can have a statistical and economic impact. Reject inference is the process of attempting to infer the creditworthiness status of the rejected applications. In this research, we use deep generative models to develop two new semi-supervised Bayesian models for reject inference in credit scoring, in which we model the data generating process to be dependent on a Gaussian mixture. The goal is to improve the classification accuracy in credit scoring models by adding reject applications. Our proposed models infer the unknown creditworthiness of the rejected applications by exact enumeration of the two possible outcomes of the loan (default or non-default). The efficient stochastic gradient optimization technique used in deep generative models makes our models suitable for large data sets. Finally, the experiments in this research show that our proposed models perform better than classical and alternative machine learning models for reject inference in credit scoring.

</details>

<details>

<summary>2021-09-24 17:27:46 - Nishimori meets Bethe: a spectral method for node classification in sparse weighted graphs</summary>

- *Lorenzo Dall'Amico, Romain Couillet, Nicolas Tremblay*

- `2103.03561v2` - [abs](http://arxiv.org/abs/2103.03561v2) - [pdf](http://arxiv.org/pdf/2103.03561v2)

> This article unveils a new relation between the Nishimori temperature parametrizing a distribution P and the Bethe free energy on random Erdos-Renyi graphs with edge weights distributed according to P. Estimating the Nishimori temperature being a task of major importance in Bayesian inference problems, as a practical corollary of this new relation, a numerical method is proposed to accurately estimate the Nishimori temperature from the eigenvalues of the Bethe Hessian matrix of the weighted graph. The algorithm, in turn, is used to propose a new spectral method for node classification in weighted (possibly sparse) graphs. The superiority of the method over competing state-of-the-art approaches is demonstrated both through theoretical arguments and real-world data experiments.

</details>

<details>

<summary>2021-09-24 19:59:53 - Bayesian non-parametric non-negative matrix factorization for pattern identification in environmental mixtures</summary>

- *Elizabeth A. Gibson, Sebastian T. Rowland, Jeff Goldsmith, John Paisley, Julie B. Herbstman, Marianthi-Anna Kiourmourtzoglou*

- `2109.12164v1` - [abs](http://arxiv.org/abs/2109.12164v1) - [pdf](http://arxiv.org/pdf/2109.12164v1)

> Environmental health researchers may aim to identify exposure patterns that represent sources, product use, or behaviors that give rise to mixtures of potentially harmful environmental chemical exposures. We present Bayesian non-parametric non-negative matrix factorization (BN^2MF) as a novel method to identify patterns of chemical exposures when the number of patterns is not known a priori. We placed non-negative continuous priors on pattern loadings and individual scores to enhance interpretability and used a clever non-parametric sparse prior to estimate the pattern number. We further derived variational confidence intervals around estimates; this is a critical development because it quantifies the model's confidence in estimated patterns. These unique features contrast with existing pattern recognition methods employed in this field which are limited by user-specified pattern number, lack of interpretability of patterns in terms of human understanding, and lack of uncertainty quantification.

</details>

<details>

<summary>2021-09-25 03:18:38 - Methods to Compute Prediction Intervals: A Review and New Results</summary>

- *Qinglong Tian, Daniel J. Nordman, William Q. Meeker*

- `2011.03065v2` - [abs](http://arxiv.org/abs/2011.03065v2) - [pdf](http://arxiv.org/pdf/2011.03065v2)

> This paper reviews two main types of prediction interval methods under a parametric framework. First, we describe methods based on an (approximate) pivotal quantity. Examples include the plug-in, pivotal, and calibration methods. Then we describe methods based on a predictive distribution (sometimes derived based on the likelihood). Examples include Bayesian, fiducial, and direct-bootstrap methods. Several examples involving continuous distributions along with simulation studies to evaluate coverage probability properties are provided. We provide specific connections among different prediction interval methods for the (log-)location-scale family of distributions. This paper also discusses general prediction interval methods for discrete data, using the binomial and Poisson distributions as examples. We also overview methods for dependent data, with application to time series, spatial data, and Markov random fields, for example.

</details>

<details>

<summary>2021-09-25 04:30:59 - Regression model selection via log-likelihood ratio and constrained minimum criterion</summary>

- *Min Tsao*

- `2107.08529v2` - [abs](http://arxiv.org/abs/2107.08529v2) - [pdf](http://arxiv.org/pdf/2107.08529v2)

> Although the log-likelihood is widely used in model selection, the log-likelihood ratio has had few applications in this area. We develop a log-likelihood ratio based method for selecting regression models by focusing on the set of models deemed plausible by the likelihood ratio test. We show that when the sample size is large and the significance level of the test is small, there is a high probability that the smallest model in the set is the true model; thus, we select this smallest model. The significance level of the test serves as a parameter for this method. We consider three levels of this parameter in a simulation study and compare this method with the Akaike Information Criterion and Bayesian Information Criterion to demonstrate its excellent accuracy and adaptability to different sample sizes. We also apply this method to select a logistic regression model for a South African heart disease dataset.

</details>

<details>

<summary>2021-09-25 23:02:47 - Contributions to Large Scale Bayesian Inference and Adversarial Machine Learning</summary>

- *Víctor Gallego*

- `2109.13232v1` - [abs](http://arxiv.org/abs/2109.13232v1) - [pdf](http://arxiv.org/pdf/2109.13232v1)

> The rampant adoption of ML methodologies has revealed that models are usually adopted to make decisions without taking into account the uncertainties in their predictions. More critically, they can be vulnerable to adversarial examples. Thus, we believe that developing ML systems that take into account predictive uncertainties and are robust against adversarial examples is a must for critical, real-world tasks. We start with a case study in retailing. We propose a robust implementation of the Nerlove-Arrow model using a Bayesian structural time series model. Its Bayesian nature facilitates incorporating prior information reflecting the manager's views, which can be updated with relevant data. However, this case adopted classical Bayesian techniques, such as the Gibbs sampler. Nowadays, the ML landscape is pervaded with neural networks and this chapter also surveys current developments in this sub-field. Then, we tackle the problem of scaling Bayesian inference to complex models and large data regimes. In the first part, we propose a unifying view of two different Bayesian inference algorithms, Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) and Stein Variational Gradient Descent (SVGD), leading to improved and efficient novel sampling schemes. In the second part, we develop a framework to boost the efficiency of Bayesian inference in probabilistic models by embedding a Markov chain sampler within a variational posterior approximation. After that, we present an alternative perspective on adversarial classification based on adversarial risk analysis, and leveraging the scalable Bayesian approaches from chapter 2. In chapter 4 we turn to reinforcement learning, introducing Threatened Markov Decision Processes, showing the benefits of accounting for adversaries in RL while the agent learns.

</details>

<details>

<summary>2021-09-26 12:10:30 - Density Estimation via Bayesian Inference Engines</summary>

- *M. P. Wand, J. C. F. Yu*

- `2009.06182v4` - [abs](http://arxiv.org/abs/2009.06182v4) - [pdf](http://arxiv.org/pdf/2009.06182v4)

> We explain how effective automatic probability density function estimates can be constructed using contemporary Bayesian inference engines such as those based on no-U-turn sampling and expectation propagation. Extensive simulation studies demonstrate that the proposed density estimates have excellent comparative performance and scale well to very large sample sizes due to a binning strategy. Moreover, the approach is fully Bayesian and all estimates are accompanied by pointwise credible intervals. An accompanying package in the R language facilitates easy use of the new density estimates.

</details>

<details>

<summary>2021-09-27 04:55:16 - Bayesian Matrix Completion for Hypothesis Testing</summary>

- *Bora Jin, David B. Dunson, Julia E. Rager, David Reif, Stephanie M. Engel, Amy H. Herring*

- `2009.08405v5` - [abs](http://arxiv.org/abs/2009.08405v5) - [pdf](http://arxiv.org/pdf/2009.08405v5)

> High-throughput screening (HTS) is a well-established technology that rapidly and efficiently screens thousands of chemicals for potential toxicity. Massive testing using HTS primarily aims to differentiate active vs inactive chemicals for different types of biological endpoints. However, even using high-throughput technology, it is not feasible to test all possible combinations of chemicals and assay endpoints, resulting in a majority of missing combinations. Our goal is to derive posterior probabilities of activity for each chemical by assay endpoint combination, addressing the sparsity of HTS data. We propose a Bayesian hierarchical framework, which borrows information across different chemicals and assay endpoints in a low-dimensional latent space. This framework facilitates out-of-sample prediction of bioactivity potential for new chemicals not yet tested. Furthermore, this paper makes a novel attempt in toxicology to simultaneously model heteroscedastic errors as well as a nonparametric mean function. It leads to a broader definition of activity whose need has been suggested by toxicologists. Simulation studies demonstrate that our approach shows superior performance with more realistic inferences on activity than current standard methods. Application to an HTS data set identifies chemicals that are most likely active for two disease outcomes: neurodevelopmental disorders and obesity. Code is available on Github.

</details>

<details>

<summary>2021-09-27 11:17:41 - The Relative Importance of Ability, Luck and Motivation in Team Sports: a Bayesian Model of Performance in Rugby</summary>

- *Fernando Delbianco, Federico Fioravanti, Fernando Tohmé*

- `2110.00001v1` - [abs](http://arxiv.org/abs/2110.00001v1) - [pdf](http://arxiv.org/pdf/2110.00001v1)

> Results in contact sports like Rugby are mainly interpreted in terms of the ability and/or luck of teams. But this neglects the important role of the {\em motivation} of players, reflected in the effort exerted in the game. Here we present a Bayesian hierarchical model to infer the main features that explain score differences in rugby matches of the English Premiership Rugby 2020/2021 season. The main result is that, indeed, {\em effort} (seen as a ratio between the number of tries and the scoring kick attempts) is highly relevant to explain outcomes in those matches.

</details>

<details>

<summary>2021-09-27 15:01:03 - A Bayesian Approach to Linking Data Without Unique Identifiers</summary>

- *Edwin Farley, Roee Gutman*

- `2012.00601v2` - [abs](http://arxiv.org/abs/2012.00601v2) - [pdf](http://arxiv.org/pdf/2012.00601v2)

> Existing file linkage methods may produce sub-optimal results because they consider neither the interactions between different pairs of matched records nor relationships between variables that are exclusive to one of the files. In addition, many of the current methods fail to address the uncertainty in the linkage, which may result in overly precise estimates of relationships between variables that are exclusive to one of the files. Bayesian methods for record linkage can reduce the bias in the estimation of scientific relationships of interest and provide interval estimates that account for the uncertainty in the linkage; however, implementation of these methods can often be complex and computationally intensive. This article presents the gfs_sampler package for the Python programming language that utilizes a Bayesian approach for file linkage. The linking procedure implemented in gfs_sampler samples from the joint posterior distribution of model parameters and the linking permutations. The algorithm approaches file linkage as a missing data problem and generates multiple linked data sets. For computational efficiency, only the linkage permutations are stored and multiple analyses are performed using each of the permutations separately. This implementation reduces the computational complexity of the linking process and the expertise required of researchers analyzing linked data sets. We describe the algorithm implemented in the gfs_sampler package and its statistical basis, and demonstrate its use on a sample data set.

</details>

<details>

<summary>2021-09-27 15:58:54 - Bayesian Nonparametric Dimensionality Reduction of Categorical Data for Predicting Severity of COVID-19 in Pregnant Women</summary>

- *Marzieh Ajirak, Cassandra Heiselman, Anna Fuchs, Mia Heiligenstein, Kimberly Herrera, Diana Garretto, Petar Djuric*

- `2011.03715v2` - [abs](http://arxiv.org/abs/2011.03715v2) - [pdf](http://arxiv.org/pdf/2011.03715v2)

> The coronavirus disease (COVID-19) has rapidly spread throughout the world and while pregnant women present the same adverse outcome rates, they are underrepresented in clinical research. We collected clinical data of 155 test-positive COVID-19 pregnant women at Stony Brook University Hospital. Many of these collected data are of multivariate categorical type, where the number of possible outcomes grows exponentially as the dimension of data increases. We modeled the data within the unsupervised Bayesian framework and mapped them into a lower-dimensional space using latent Gaussian processes. The latent features in the lower dimensional space were further used for predicting if a pregnant woman would be admitted to a hospital due to COVID-19 or would remain with mild symptoms. We compared the prediction accuracy with the dummy/one-hot encoding of categorical data and found that the latent Gaussian process had better accuracy.

</details>

<details>

<summary>2021-09-27 22:32:38 - Statistical Inference of Minimally Complex Models</summary>

- *Clélia de Mulatier, Paolo P. Mazza, Matteo Marsili*

- `2008.00520v2` - [abs](http://arxiv.org/abs/2008.00520v2) - [pdf](http://arxiv.org/pdf/2008.00520v2)

> Finding the model that best describes a high dimensional dataset is a daunting task. For binary data, we show that this becomes feasible when restricting the search to a family of simple models, that we call Minimally Complex Models (MCMs). These are spin models, with interactions of arbitrary order, that are composed of independent components of minimal complexity (Beretta et al., 2018). They tend to be simple in information theoretic terms, which means that they are well-fitted to specific types of data, and are therefore easy to falsify. We show that Bayesian model selection restricted to these models is computationally feasible and has many other advantages. First, their evidence, which trades off goodness-of-fit against model complexity, can be computed easily without any parameter fitting. This allows selecting the best MCM among all, even though the number of models is astronomically large. Furthermore, MCMs can be inferred and sampled from without any computational effort. Finally, model selection among MCMs is invariant with respect to changes in the representation of the data. MCMs portray the structure of dependencies among variables in a simple way, as illustrated in several examples, and thus provide robust predictions on dependencies in the data. MCMs contain interactions of any order between variables, and thus may reveal the presence of interactions of order higher than pairwise.

</details>

<details>

<summary>2021-09-28 10:30:15 - Modeling animal movement with directional persistence and attractive points</summary>

- *Gianluca Mastrantonio*

- `2012.03248v2` - [abs](http://arxiv.org/abs/2012.03248v2) - [pdf](http://arxiv.org/pdf/2012.03248v2)

> GPS technology is currently easily accessible to researchers, and many animal movement datasets are available. Two of the main features that a model which describes an animal's path can possess are directional persistence and attraction to a point in space. In this work, we propose a new approach that can have both characteristics. Our proposal is a hidden Markov model with a new emission distribution. The emission distribution models the two aforementioned characteristics, while the latent state of the hidden Markov model is needed to account for the behavioral modes. We show that the model is easy to implement in a Bayesian framework. We estimate our proposal on the motivating data that represent GPS locations of a Maremma Sheepdog recorded in Australia. The obtained results are easily interpretable and we show that our proposal outperforms the main competitive model.

</details>

<details>

<summary>2021-09-28 15:47:34 - Perturbation theory for killed Markov processes and quasi-stationary distributions</summary>

- *Daniel Rudolf, Andi Q. Wang*

- `2109.13819v1` - [abs](http://arxiv.org/abs/2109.13819v1) - [pdf](http://arxiv.org/pdf/2109.13819v1)

> We investigate the stability of quasi-stationary distributions of killed Markov processes to perturbations of the generator. In the first setting, we consider a general bounded self-adjoint perturbation operator, and after that, study a particular unbounded perturbation corresponding to the truncation of the killing rate. In both scenarios, we quantify the difference between eigenfunctions of the smallest eigenvalue of the perturbed and unperturbed generator in a Hilbert space norm. As a consequence, $\mathcal{L}^1$-norm estimates of the difference of the resulting quasi-stationary distributions in terms of the perturbation are provided. These results are particularly relevant to the recently-proposed class of quasi-stationary Monte Carlo methods, designed for scalable exact Bayesian inference.

</details>

<details>

<summary>2021-09-28 17:35:57 - A PAC-Bayesian Analysis of Distance-Based Classifiers: Why Nearest-Neighbour works!</summary>

- *Thore Graepel, Ralf Herbrich*

- `2109.13889v1` - [abs](http://arxiv.org/abs/2109.13889v1) - [pdf](http://arxiv.org/pdf/2109.13889v1)

> Abstract We present PAC-Bayesian bounds for the generalisation error of the K-nearest-neighbour classifier (K-NN). This is achieved by casting the K-NN classifier into a kernel space framework in the limit of vanishing kernel bandwidth. We establish a relation between prior measures over the coefficients in the kernel expansion and the induced measure on the weight vectors in kernel space. Defining a sparse prior over the coefficients allows the application of a PAC-Bayesian folk theorem that leads to a generalisation bound that is a function of the number of redundant training examples: those that can be left out without changing the solution. The presented bound requires to quantify a prior belief in the sparseness of the solution and is evaluated after learning when the actual redundancy level is known. Even for small sample size (m ~ 100) the bound gives non-trivial results when both the expected sparseness and the actual redundancy are high.

</details>

<details>

<summary>2021-09-29 05:14:02 - An efficient pseudo marginal method for state space models</summary>

- *David Gunawan, Pratiti Chatterjee, Robert Kohn*

- `2109.14194v1` - [abs](http://arxiv.org/abs/2109.14194v1) - [pdf](http://arxiv.org/pdf/2109.14194v1)

> Pseudo Marginal Metropolis-Hastings (PMMH) is a general approach to carry out Bayesian inference when the likelihood is intractable but can be estimated unbiasedly. Our article develops an efficient PMMH method for estimating the parameters of complex and high-dimensional state-space models and has the following features. First, it runs multiple particle filters in parallel and uses their averaged unbiased likelihood estimate. Second, it combines block and correlated PMMH sampling. The first two features enable our sampler to scale up better to longer time series and higher dimensional state vectors than previous approaches. Third, the article develops an efficient auxiliary disturbance particle filter, which is necessary when the bootstrap filter is inefficient, but the state transition density cannot be expressed in closed form. Fourth, it uses delayed acceptance to make the make the sampler more efficient. The performance of the sampler is investigated empirically by applying it to Dynamic Stochastic General Equilibrium models with relatively high state dimensions and with intractable state transition densities. Although our focus is on applying the method to state-space models, the approach will be useful in a wide range of applications such as large panel data models and stochastic differential equation models with mixed effects.

</details>

<details>

<summary>2021-09-29 15:19:51 - Variational Inference for Continuous-Time Switching Dynamical Systems</summary>

- *Lukas Köhs, Bastian Alt, Heinz Koeppl*

- `2109.14492v1` - [abs](http://arxiv.org/abs/2109.14492v1) - [pdf](http://arxiv.org/pdf/2109.14492v1)

> Switching dynamical systems provide a powerful, interpretable modeling framework for inference in time-series data in, e.g., the natural sciences or engineering applications. Since many areas, such as biology or discrete-event systems, are naturally described in continuous time, we present a model based on an Markov jump process modulating a subordinated diffusion process. We provide the exact evolution equations for the prior and posterior marginal densities, the direct solutions of which are however computationally intractable. Therefore, we develop a new continuous-time variational inference algorithm, combining a Gaussian process approximation on the diffusion level with posterior inference for Markov jump processes. By minimizing the path-wise Kullback-Leibler divergence we obtain (i) Bayesian latent state estimates for arbitrary points on the real axis and (ii) point estimates of unknown system parameters, utilizing variational expectation maximization. We extensively evaluate our algorithm under the model assumption and for real-world examples.

</details>

<details>

<summary>2021-09-29 19:25:10 - Adaptive Bayesian Sum of Trees Model for Covariate Dependent Spectral Analysis</summary>

- *Yakun Wang, Zeda Li, Scott A. Bruce*

- `2109.14677v1` - [abs](http://arxiv.org/abs/2109.14677v1) - [pdf](http://arxiv.org/pdf/2109.14677v1)

> This article introduces a flexible and adaptive nonparametric method for estimating the association between multiple covariates and power spectra of multiple time series. The proposed approach uses a Bayesian sum of trees model to capture complex dependencies and interactions between covariates and the power spectrum, which are often observed in studies of biomedical time series. Local power spectra corresponding to terminal nodes within trees are estimated nonparametrically using Bayesian penalized linear splines. The trees are considered to be random and fit using a Bayesian backfitting Markov chain Monte Carlo (MCMC) algorithm that sequentially considers tree modifications via reversible-jump MCMC techniques. For high-dimensional covariates, a sparsity-inducing Dirichlet hyperprior on tree splitting proportions is considered, which provides sparse estimation of covariate effects and efficient variable selection. By averaging over the posterior distribution of trees, the proposed method can recover both smooth and abrupt changes in the power spectrum across multiple covariates. Empirical performance is evaluated via simulations to demonstrate the proposed method's ability to accurately recover complex relationships and interactions. The proposed methodology is used to study gait maturation in young children by evaluating age-related changes in power spectra of stride interval time series in the presence of other covariates.

</details>

<details>

<summary>2021-09-30 09:21:37 - Revisiting Bayesian Optimization in the light of the COCO benchmark</summary>

- *Rodolphe Le Riche, Victor Picheny*

- `2103.16649v4` - [abs](http://arxiv.org/abs/2103.16649v4) - [pdf](http://arxiv.org/pdf/2103.16649v4)

> It is commonly believed that Bayesian optimization (BO) algorithms are highly efficient for optimizing numerically costly functions. However, BO is not often compared to widely different alternatives, and is mostly tested on narrow sets of problems (multimodal, low-dimensional functions), which makes it difficult to assess where (or if) they actually achieve state-of-the-art performance. Moreover, several aspects in the design of these algorithms vary across implementations without a clear recommendation emerging from current practices, and many of these design choices are not substantiated by authoritative test campaigns. This article reports a large investigation about the effects on the performance of (Gaussian process based) BO of common and less common design choices. The experiments are carried out with the established COCO (COmparing Continuous Optimizers) software. It is found that a small initial budget, a quadratic trend, high-quality optimization of the acquisition criterion bring consistent progress. Using the GP mean as an occasional acquisition contributes to a negligible additional improvement. Warping degrades performance. The Mat\'ern 5/2 kernel is a good default but it may be surpassed by the exponential kernel on irregular functions. Overall, the best EGO variants are competitive or improve over state-of-the-art algorithms in dimensions less or equal to 5 for multimodal functions. The code developed for this study makes the new version (v2.1.1) of the R package DiceOptim available on CRAN. The structure of the experiments by function groups allows to define priorities for future research on Bayesian optimization.

</details>

<details>

<summary>2021-09-30 09:53:45 - Bayesian Multi-Species N-Mixture Models for Unmarked Animal Communities</summary>

- *Niamh Mimnagh, Andrew Parnell, Estevao Prado, Rafael de Andrade Moral*

- `2109.14966v1` - [abs](http://arxiv.org/abs/2109.14966v1) - [pdf](http://arxiv.org/pdf/2109.14966v1)

> We propose an extension of the N-mixture model which allows for the estimation of both abundances of multiple species simultaneously and their inter-species correlations. We also propose further extensions to this multi-species N-mixture model, one of which permits us to examine data which has an excess of zero counts, and another which allows us to relax the assumption of closure inherent in N-mixture models through the incorporation of an AR term in the abundance. The inclusion of a multivariate normal distribution as prior on the random effect in the abundance facilitates the estimation of a matrix of interspecies correlations. Each model is also fitted to avian point data collected as part of the NABBS 2010-2019. Results of simulation studies reveal that these models produce accurate estimates of abundance, inter-species correlations and detection probabilities at both small and large sample sizes, in scenarios with small, large and no zero inflation. Results of model-fitting to the North American Breeding Bird Survey data reveal an increase in Bald Eagle population size in southeastern Alaska in the decade examined.Our novel multi-species N-mixture model accounts for full communities, allowing us to examine abundances of every species present in a study area and, as these species do not exist in a vacuum, allowing us to estimate correlations between species' abundances.While previous multi-species abundance models have allowed for the estimation of abundance and detection probability, ours is the first to address the estimation of both positive and negative inter-species correlations, which allows us to begin to make inferences as to the effect that these species' abundances have on one another. Our modelling approach provides a method of quantifying the strength of association between species' population sizes, and is of practical use to population and conservation ecologists.

</details>

<details>

<summary>2021-09-30 10:07:56 - A multi-level model for estimating region-age-time-type specific male circumcision coverage from household survey and health system data in South Africa</summary>

- *Matthew L. Thomas, Khangelani Zuma, Dayanund Loykissoonlal, Bridget Dube, Peter Vranken, Sarah E. Porter, Katharine Kripke, Thapelo Seatlhodi, Gesine Meyer-Rath, Leigh F. Johnson, Jeffrey W. Eaton*

- `2108.09142v2` - [abs](http://arxiv.org/abs/2108.09142v2) - [pdf](http://arxiv.org/pdf/2108.09142v2)

> Voluntary medical male circumcision (VMMC) reduces the risk of male HIV acquisition by 60%. Programmes to provide male circumcision (MC) to prevent HIV infection have been introduced in sub-Saharan African countries with high HIV burden. While large-scale provision of MMC is recent, traditional MC has long been conducted as part of male coming-of-age practices. How and at what age traditional MC occurs varies by ethnic groups within countries. Accurate estimates of MC coverage by age and type of circumcision (traditional or medical) over time at sub-national levels are essential for planning and delivering VMMCs to meet targets and evaluating their impacts on HIV incidence. In this paper, we developed a Bayesian competing risks time-to-event model to produce region-age-time-type specific probabilities and coverage of MC with probabilistic uncertainty. The model jointly synthesises data from household surveys and health system data on the number of VMMCs conducted. We demonstrated the model using data from five household surveys and VMMC programme data to produce estimates of MC coverage for 52 districts in South Africa between 2008 and 2019. Nationally in 2008, 24.1% (CI: 23.4-24.8%) of men aged 15-49 were traditionally circumcised and 19.4% (CI: 18.9-20.0%) were medically circumcised. Between 2008 and 2019, five million VMMCs were conducted, and MC coverage among men aged 15-49 increased to 64.0% (CI: 63.2-64.9%) and medical MC coverage to 42% (CI: 41.3-43.0%). MC coverage varied widely across districts, ranging from 13.4-86.3%. The average age of traditional MC ranged between 13 to 19 years, depending on local cultural practices.

</details>

<details>

<summary>2021-09-30 11:28:38 - Fast and Flexible Bayesian Inference in Time-varying Parameter Regression Models</summary>

- *Niko Hauzenberger, Florian Huber, Gary Koop, Luca Onorante*

- `1910.10779v4` - [abs](http://arxiv.org/abs/1910.10779v4) - [pdf](http://arxiv.org/pdf/1910.10779v4)

> In this paper, we write the time-varying parameter (TVP) regression model involving K explanatory variables and T observations as a constant coefficient regression model with KT explanatory variables. In contrast with much of the existing literature which assumes coefficients to evolve according to a random walk, a hierarchical mixture model on the TVPs is introduced. The resulting model closely mimics a random coefficients specification which groups the TVPs into several regimes. These flexible mixtures allow for TVPs that feature a small, moderate or large number of structural breaks. We develop computationally efficient Bayesian econometric methods based on the singular value decomposition of the KT regressors. In artificial data, we find our methods to be accurate and much faster than standard approaches in terms of computation time. In an empirical exercise involving inflation forecasting using a large number of predictors, we find our models to forecast better than alternative approaches and document different patterns of parameter change than are found with approaches which assume random walk evolution of parameters.

</details>

<details>

<summary>2021-09-30 13:03:18 - Bayesian workflow for disease transmission modeling in Stan</summary>

- *Léo Grinsztajn, Elizaveta Semenova, Charles C. Margossian, Julien Riou*

- `2006.02985v3` - [abs](http://arxiv.org/abs/2006.02985v3) - [pdf](http://arxiv.org/pdf/2006.02985v3)

> This tutorial shows how to build, fit, and criticize disease transmission models in Stan, and should be useful to researchers interested in modeling the SARS-CoV-2 pandemic and other infectious diseases in a Bayesian framework. Bayesian modeling provides a principled way to quantify uncertainty and incorporate both data and prior knowledge into the model estimates. Stan is an expressive probabilistic programming language that abstracts the inference and allows users to focus on the modeling. As a result, Stan code is readable and easily extensible, which makes the modeler's work more transparent. Furthermore, Stan's main inference engine, Hamiltonian Monte Carlo sampling, is amiable to diagnostics, which means the user can verify whether the obtained inference is reliable. In this tutorial, we demonstrate how to formulate, fit, and diagnose a compartmental transmission model in Stan, first with a simple Susceptible-Infected-Recovered (SIR) model, then with a more elaborate transmission model used during the SARS-CoV-2 pandemic. We also cover advanced topics which can further help practitioners fit sophisticated models; notably, how to use simulations to probe the model and priors, and computational techniques to scale-up models based on ordinary differential equations.

</details>


## 2021-10

<details>

<summary>2021-10-01 03:45:10 - Long-term prediction intervals with many covariates</summary>

- *Sayar Karmakar, Marek Chudy, Wei Biao Wu*

- `2012.08223v2` - [abs](http://arxiv.org/abs/2012.08223v2) - [pdf](http://arxiv.org/pdf/2012.08223v2)

> Accurate forecasting is one of the fundamental focus in the literature of econometric time-series. Often practitioners and policy makers want to predict outcomes of an entire time horizon in the future instead of just a single $k$-step ahead prediction. These series, apart from their own possible non-linear dependence, are often also influenced by many external predictors. In this paper, we construct prediction intervals of time-aggregated forecasts in a high-dimensional regression setting. Our approach is based on quantiles of residuals obtained by the popular LASSO routine. We allow for general heavy-tailed, long-memory, and nonlinear stationary error process and stochastic predictors. Through a series of systematically arranged consistency results we provide theoretical guarantees of our proposed quantile-based method in all of these scenarios. After validating our approach using simulations we also propose a novel bootstrap based method that can boost the coverage of the theoretical intervals. Finally analyzing the EPEX Spot data, we construct prediction intervals for hourly electricity prices over horizons spanning 17 weeks and contrast them to selected Bayesian and bootstrap interval forecasts.

</details>

<details>

<summary>2021-10-01 09:04:26 - TyXe: Pyro-based Bayesian neural nets for Pytorch</summary>

- *Hippolyt Ritter, Theofanis Karaletsos*

- `2110.00276v1` - [abs](http://arxiv.org/abs/2110.00276v1) - [pdf](http://arxiv.org/pdf/2110.00276v1)

> We introduce TyXe, a Bayesian neural network library built on top of Pytorch and Pyro. Our leading design principle is to cleanly separate architecture, prior, inference and likelihood specification, allowing for a flexible workflow where users can quickly iterate over combinations of these components. In contrast to existing packages TyXe does not implement any layer classes, and instead relies on architectures defined in generic Pytorch code. TyXe then provides modular choices for canonical priors, variational guides, inference techniques, and layer selections for a Bayesian treatment of the specified architecture. Sampling tricks for variance reduction, such as local reparameterization or flipout, are implemented as effect handlers, which can be applied independently of other specifications. We showcase the ease of use of TyXe to explore Bayesian versions of popular models from various libraries: toy regression with a pure Pytorch neural network; large-scale image classification with torchvision ResNets; graph neural networks based on DGL; and Neural Radiance Fields built on top of Pytorch3D. Finally, we provide convenient abstractions for variational continual learning. In all cases the change from a deterministic to a Bayesian neural network comes with minimal modifications to existing code, offering a broad range of researchers and practitioners alike practical access to uncertainty estimation techniques. The library is available at https://github.com/TyXe-BDL/TyXe.

</details>

<details>

<summary>2021-10-01 10:02:39 - Social Learning in Nonatomic Routing Games</summary>

- *Emilien Macault, Marco Scarsini, Tristan Tomala*

- `2009.11580v3` - [abs](http://arxiv.org/abs/2009.11580v3) - [pdf](http://arxiv.org/pdf/2009.11580v3)

> We consider a discrete-time nonatomic routing game with variable demand and uncertain costs. Given a routing network with single origin and destination, the cost function of each edge depends on some uncertain persistent state parameter. At every period, a random traffic demand is routed through the network according to a Wardrop equilibrium. The realized costs are publicly observed and the public Bayesian belief about the state parameter is updated. We say that there is strong learning when beliefs converge to the truth and weak learning when the equilibrium flow converges to the complete-information flow. We characterize the networks for which learning occurs. We prove that these networks have a series-parallel structure and provide a counterexample to show that learning may fail in non-series-parallel networks.

</details>

<details>

<summary>2021-10-01 14:20:37 - A Bayesian approach to location estimation of mobile devices from mobile network operator data</summary>

- *Martijn Tennekes, Yvonne A. P. M. Gootzen*

- `2110.00439v1` - [abs](http://arxiv.org/abs/2110.00439v1) - [pdf](http://arxiv.org/pdf/2110.00439v1)

> Mobile network operator (MNO) data are a rich data source for official statistics, such as present population, mobility, migration, and tourism. Estimating the geographic location of mobile devices is an essential step for statistical inference. Most studies use the Voronoi tessellation for this, which is based on the assumption that mobile devices are always connected to the nearest radio cell. This paper uses a modular Bayesian approach, allowing for different modules of prior knowledge about where devices are expected to be, and different modules for the likelihood of connection given a geographic location. We discuss and compare the use of several prior modules, including one that is based on land use. We show that the Voronoi tessellation can be used as a likelihood module. Alternatively, we propose a signal strength model using radio cell properties such as antenna height, propagation direction, and power. Using Bayes' rule, we derive a posterior probability distribution that is an estimate for the geographic location, which can be used for further statistical inference. We describe the method and provide illustrations of a fictional example that resembles a real-world situation. The method has been implemented in the R packages mobloc and mobvis, which are briefly described.

</details>

<details>

<summary>2021-10-01 18:03:07 - Conditional Deep Gaussian Processes: multi-fidelity kernel learning</summary>

- *Chi-Ken Lu, Patrick Shafto*

- `2002.02826v3` - [abs](http://arxiv.org/abs/2002.02826v3) - [pdf](http://arxiv.org/pdf/2002.02826v3)

> Deep Gaussian Processes (DGPs) were proposed as an expressive Bayesian model capable of a mathematically grounded estimation of uncertainty. The expressivity of DPGs results from not only the compositional character but the distribution propagation within the hierarchy. Recently, [1] pointed out that the hierarchical structure of DGP well suited modeling the multi-fidelity regression, in which one is provided sparse observations with high precision and plenty of low fidelity observations. We propose the conditional DGP model in which the latent GPs are directly supported by the fixed lower fidelity data. Then the moment matching method in [2] is applied to approximate the marginal prior of conditional DGP with a GP. The obtained effective kernels are implicit functions of the lower-fidelity data, manifesting the expressivity contributed by distribution propagation within the hierarchy. The hyperparameters are learned via optimizing the approximate marginal likelihood. Experiments with synthetic and high dimensional data show comparable performance against other multi-fidelity regression methods, variational inference, and multi-output GP. We conclude that, with the low fidelity data and the hierarchical DGP structure, the effective kernel encodes the inductive bias for true function allowing the compositional freedom discussed in [3,4].

</details>

<details>

<summary>2021-10-01 18:30:39 - Towards Compact Neural Networks via End-to-End Training: A Bayesian Tensor Approach with Automatic Rank Determination</summary>

- *Cole Hawkins, Xing Liu, Zheng Zhang*

- `2010.08689v3` - [abs](http://arxiv.org/abs/2010.08689v3) - [pdf](http://arxiv.org/pdf/2010.08689v3)

> While post-training model compression can greatly reduce the inference cost of a deep neural network, uncompressed training still consumes a huge amount of hardware resources, run-time and energy. It is highly desirable to directly train a compact neural network from scratch with low memory and low computational cost. Low-rank tensor decomposition is one of the most effective approaches to reduce the memory and computing requirements of large-size neural networks. However, directly training a low-rank tensorized neural network is a very challenging task because it is hard to determine a proper tensor rank {\it a priori}, which controls the model complexity and compression ratio in the training process. This paper presents a novel end-to-end framework for low-rank tensorized training of neural networks. We first develop a flexible Bayesian model that can handle various low-rank tensor formats (e.g., CP, Tucker, tensor train and tensor-train matrix) that compress neural network parameters in training. This model can automatically determine the tensor ranks inside a nonlinear forward model, which is beyond the capability of existing Bayesian tensor methods. We further develop a scalable stochastic variational inference solver to estimate the posterior density of large-scale problems in training. Our work provides the first general-purpose rank-adaptive framework for end-to-end tensorized training. Our numerical results on various neural network architectures show orders-of-magnitude parameter reduction and little accuracy loss (or even better accuracy) in the training process. Specifically, on a very large deep learning recommendation system with over $4.2\times 10^9$ model parameters, our method can reduce the variables to only $1.6\times 10^5$ automatically in the training process (i.e., by $2.6\times 10^4$ times) while achieving almost the same accuracy.

</details>

<details>

<summary>2021-10-01 21:30:19 - A non-parametric Bayesian approach for adjusting partial compliance in sequential decision making</summary>

- *Indrabati Bhattacharya, Brent A. Johnson, William Artman, Andrew Wilson, Kevin G. Lynch, James R. McKay, Ashkan Ertefaie*

- `2110.00659v1` - [abs](http://arxiv.org/abs/2110.00659v1) - [pdf](http://arxiv.org/pdf/2110.00659v1)

> Existing methods in estimating the mean outcome under a given dynamic treatment regime rely on intention-to-treat analyses which estimate the effect of following a certain dynamic treatment regime regardless of compliance behavior of patients. There are two major concerns with intention-to-treat analyses: (1) the estimated effects are often biased toward the null effect; (2) the results are not generalizable and reproducible due to the potential differential compliance behavior. These are particularly problematic in settings with high level of non-compliance such as substance use disorder treatments. Our work is motivated by the Adaptive Treatment for Alcohol and Cocaine Dependence study (ENGAGE), which is a multi-stage trial that aimed to construct optimal treatment strategies to engage patients in therapy. Due to the relatively low level of compliance in this trial, intention-to-treat analyses essentially estimate the effect of being randomized to a certain treatment sequence which is not of interest. We fill this important gap by defining the target parameter as the mean outcome under a dynamic treatment regime given potential compliance strata. We propose a flexible non-parametric Bayesian approach, which consists of a Gaussian copula model for the potential compliances, and a Dirichlet process mixture model for the potential outcomes. Our simulations highlight the need for and usefulness of this approach in practice and illustrate the robustness of our estimator in non-linear and non-Gaussian settings.

</details>

<details>

<summary>2021-10-01 23:22:03 - Challenges in Markov chain Monte Carlo for Bayesian neural networks</summary>

- *Theodore Papamarkou, Jacob Hinkle, M. Todd Young, David Womble*

- `1910.06539v6` - [abs](http://arxiv.org/abs/1910.06539v6) - [pdf](http://arxiv.org/pdf/1910.06539v6)

> Markov chain Monte Carlo (MCMC) methods have not been broadly adopted in Bayesian neural networks (BNNs). This paper initially reviews the main challenges in sampling from the parameter posterior of a neural network via MCMC. Such challenges culminate to lack of convergence to the parameter posterior. Nevertheless, this paper shows that a non-converged Markov chain, generated via MCMC sampling from the parameter space of a neural network, can yield via Bayesian marginalization a valuable posterior predictive distribution of the output of the neural network. Classification examples based on multilayer perceptrons showcase highly accurate posterior predictive distributions. The postulate of limited scope for MCMC developments in BNNs is partially valid; an asymptotically exact parameter posterior seems less plausible, yet an accurate posterior predictive distribution is a tenable research avenue.

</details>

<details>

<summary>2021-10-02 20:10:40 - Feel-Good Thompson Sampling for Contextual Bandits and Reinforcement Learning</summary>

- *Tong Zhang*

- `2110.00871v1` - [abs](http://arxiv.org/abs/2110.00871v1) - [pdf](http://arxiv.org/pdf/2110.00871v1)

> Thompson Sampling has been widely used for contextual bandit problems due to the flexibility of its modeling power. However, a general theory for this class of methods in the frequentist setting is still lacking. In this paper, we present a theoretical analysis of Thompson Sampling, with a focus on frequentist regret bounds. In this setting, we show that the standard Thompson Sampling is not aggressive enough in exploring new actions, leading to suboptimality in some pessimistic situations. A simple modification called Feel-Good Thompson Sampling, which favors high reward models more aggressively than the standard Thompson Sampling, is proposed to remedy this problem. We show that the theoretical framework can be used to derive Bayesian regret bounds for standard Thompson Sampling, and frequentist regret bounds for Feel-Good Thompson Sampling. It is shown that in both cases, we can reduce the bandit regret problem to online least squares regression estimation. For the frequentist analysis, the online least squares regression bound can be directly obtained using online aggregation techniques which have been well studied. The resulting bandit regret bound matches the minimax lower bound in the finite action case. Moreover, the analysis can be generalized to handle a class of linearly embeddable contextual bandit problems (which generalizes the popular linear contextual bandit model). The obtained result again matches the minimax lower bound. Finally we illustrate that the analysis can be extended to handle some MDP problems.

</details>

<details>

<summary>2021-10-03 04:52:33 - Bayesian probabilistic models for corporate context, with an application to internal audit activities</summary>

- *Francesco Toraldo, Fabio S. Priuli*

- `2007.06084v2` - [abs](http://arxiv.org/abs/2007.06084v2) - [pdf](http://arxiv.org/pdf/2007.06084v2)

> In this paper we present a business case carried out in Poste Italiane, in the context of fair performance evaluations of human resources engaged in internal audit activities. In addition to the development of a Bayesian network supporting the goal of the Internal Audit unit of Poste Italiane, the work has led to the development of a methodological approach to advanced analytics in corporate context, whose usefulness goes well beyond the specific use case described here. We thus present the different stages of such analytical strategy, from feature selection, to model structure inference and model selection, as a general toolbox that allows a completely transparent and explainable process to support data-driven decisions in business environments.

</details>

<details>

<summary>2021-10-03 07:29:57 - Kalman Bayesian Neural Networks for Closed-form Online Learning</summary>

- *Philipp Wagner, Xinyang Wu, Marco F. Huber*

- `2110.00944v1` - [abs](http://arxiv.org/abs/2110.00944v1) - [pdf](http://arxiv.org/pdf/2110.00944v1)

> Compared to point estimates calculated by standard neural networks, Bayesian neural networks (BNN) provide probability distributions over the output predictions and model parameters, i.e., the weights. Training the weight distribution of a BNN, however, is more involved due to the intractability of the underlying Bayesian inference problem and thus, requires efficient approximations. In this paper, we propose a novel approach for BNN learning via closed-form Bayesian inference. For this purpose, the calculation of the predictive distribution of the output and the update of the weight distribution are treated as Bayesian filtering and smoothing problems, where the weights are modeled as Gaussian random variables. This allows closed-form expressions for training the network's parameters in a sequential/online fashion without gradient descent. We demonstrate our method on several UCI datasets and compare it to the state of the art.

</details>

<details>

<summary>2021-10-03 19:27:24 - Bayesian Model-Averaged Meta-Analysis in Medicine</summary>

- *František Bartoš, Quentin F. Gronau, Bram Timmers, Willem M. Otte, Alexander Ly, Eric-Jan Wagenmakers*

- `2110.01076v1` - [abs](http://arxiv.org/abs/2110.01076v1) - [pdf](http://arxiv.org/pdf/2110.01076v1)

> We outline a Bayesian model-averaged meta-analysis for standardized mean differences in order to quantify evidence for both treatment effectiveness $\delta$ and across-study heterogeneity $\tau$. We construct four competing models by orthogonally combining two present-absent assumptions, one for the treatment effect and one for across-study heterogeneity. To inform the choice of prior distributions for the model parameters, we used 50% of the Cochrane Database of Systematic Reviews to specify rival prior distributions for $\delta$ and $\tau$. The relative predictive performance of the competing models and rival prior distributions was assessed using the remaining 50\% of the Cochrane Database. On average, $\mathcal{H}_1^r$ -- the model that assumes the presence of a treatment effect as well as across-study heterogeneity -- outpredicted the other models, but not by a large margin. Within $\mathcal{H}_1^r$, predictive adequacy was relatively constant across the rival prior distributions. We propose specific empirical prior distributions, both for the field in general and for each of 46 specific medical subdisciplines. An example from oral health demonstrates how the proposed prior distributions can be used to conduct a Bayesian model-averaged meta-analysis in the open-source software R and JASP. The preregistered analysis plan is available at https://osf.io/zs3df/.

</details>

<details>

<summary>2021-10-03 21:41:40 - Data Integration in Causal Inference</summary>

- *Xu Shi, Ziyang Pan, Wang Miao*

- `2110.01106v1` - [abs](http://arxiv.org/abs/2110.01106v1) - [pdf](http://arxiv.org/pdf/2110.01106v1)

> Integrating data from multiple heterogeneous sources has become increasingly popular to achieve a large sample size and diverse study population. This paper reviews development in causal inference methods that combines multiple datasets collected by potentially different designs from potentially heterogeneous populations. We summarize recent advances on combining randomized clinical trial with external information from observational studies or historical controls, combining samples when no single sample has all relevant variables with application to two-sample Mendelian randomization, distributed data setting under privacy concerns for comparative effectiveness and safety research using real-world data, Bayesian causal inference, and causal discovery methods.

</details>

<details>

<summary>2021-10-03 23:58:06 - Hierarchical Causal Analysis of Natural Languages on a Chain Event Graph</summary>

- *Xuewen Yu, Jim Q. Smith*

- `2110.01129v1` - [abs](http://arxiv.org/abs/2110.01129v1) - [pdf](http://arxiv.org/pdf/2110.01129v1)

> Various graphical models are widely used in reliability to provide a qualitative description of domain experts hypotheses about how a system might fail. Here we argue that the semantics developed within standard causal Bayesian networks are not rich enough to fully capture the intervention calculus needed for this domain and a more tree-based approach is necessary. We instead construct a Bayesian hierarchical model with a chain event graph at its lowest level so that typical interventions made in reliability models are supported by a bespoke causal calculus. We then demonstrate how we can use this framework to automate the process of causal discovery from maintenance logs, extracting natural language information describing hypothesised causes of failures. Through our customised causal algebra we are then able to make predictive inferences about the effects of a variety of types of remedial interventions. The proposed methodology is illustrated throughout with examples drawn from real maintenance logs.

</details>

<details>

<summary>2021-10-04 12:20:02 - Posterior predictive model checking using formal methods in a spatio-temporal model</summary>

- *Laura Vana, Ennio Visconti, Laura Nenzi, Annalisa Cadonna, Gregor Kastner*

- `2110.01360v1` - [abs](http://arxiv.org/abs/2110.01360v1) - [pdf](http://arxiv.org/pdf/2110.01360v1)

> We propose an interdisciplinary framework, Bayesian formal predictive model checking (Bayes FPMC), which combines Bayesian predictive inference, a well established tool in statistics, with formal verification methods rooting in the computer science community.   Bayesian predictive inference allows for coherently incorporating uncertainty about unknown quantities by making use of methods or models that produce predictive distributions which in turn inform decision problems. By formalizing these problems and the corresponding properties, we can use spatio-temporal reach and escape logic to probabilistically assess their satisfaction. This way, competing models can directly be ranked according to how well they solve the actual problem at hand.   The approach is illustrated on an urban mobility application, where the crowdedness in the center of Milan is proxied by aggregated mobile phone traffic data. We specify several desirable spatio-temporal properties related to city crowdedness such as a fault tolerant network or the reachability of hospitals. After verifying these properties on draws from the posterior predictive distributions, we compare several spatio-temporal Bayesian models based on their overall and property-based predictive performance.

</details>

<details>

<summary>2021-10-04 15:28:53 - Longitudinal surface-based spatial Bayesian GLM reveals complex trajectories of motor neurodegeneration in ALS</summary>

- *Amanda F. Mejia, Vincent Koppelmans, Laura Jelsone-Swain, Sanjay Kalra, Robert C. Welsh*

- `2110.01510v1` - [abs](http://arxiv.org/abs/2110.01510v1) - [pdf](http://arxiv.org/pdf/2110.01510v1)

> Longitudinal fMRI datasets hold great promise for the study of neurodegenerative diseases, but realizing their potential depends on extracting accurate fMRI-based brain measures in individuals over time. This is especially true for rare, heterogeneous and/or rapidly progressing diseases, which often involve small samples whose functional features may vary dramatically across subjects and over time, making traditional group-difference analyses of limited utility. One such disease is ALS, which results in extreme motor function loss and eventual death. Here, we analyze a rich longitudinal dataset containing 190 motor task fMRI scans from 16 ALS patients and 22 age-matched HCs. We propose a novel longitudinal extension to our cortical surface-based spatial Bayesian GLM, which has high power and precision to detect activations in individuals. Using a series of longitudinal mixed-effects models to subsequently study the relationship between activation and disease progression, we observe an inverted U-shaped trajectory: at relatively mild disability we observe enlarging activations, while at higher disability we observe severely diminished activation, reflecting progression toward complete motor function loss. We observe distinct trajectories depending on clinical progression rate, with faster progressors exhibiting more extreme hyper-activation and subsequent hypo-activation. These differential trajectories suggest that initial hyper-activation is likely attributable to loss of inhibitory neurons. By contrast, earlier studies employing more limited sampling designs and using traditional group-difference analysis approaches were only able to observe the initial hyper-activation, which was assumed to be due to a compensatory process. This study provides a first example of how surface-based spatial Bayesian modeling furthers scientific understanding of neurodegenerative disease.

</details>

<details>

<summary>2021-10-05 02:25:39 - Density Forecasts in Panel Data Models: A Semiparametric Bayesian Perspective</summary>

- *Laura Liu*

- `1805.04178v3` - [abs](http://arxiv.org/abs/1805.04178v3) - [pdf](http://arxiv.org/pdf/1805.04178v3)

> This paper constructs individual-specific density forecasts for a panel of firms or households using a dynamic linear model with common and heterogeneous coefficients as well as cross-sectional heteroskedasticity. The panel considered in this paper features a large cross-sectional dimension N but short time series T. Due to the short T, traditional methods have difficulty in disentangling the heterogeneous parameters from the shocks, which contaminates the estimates of the heterogeneous parameters. To tackle this problem, I assume that there is an underlying distribution of heterogeneous parameters, model this distribution nonparametrically allowing for correlation between heterogeneous parameters and initial conditions as well as individual-specific regressors, and then estimate this distribution by combining information from the whole panel. Theoretically, I prove that in cross-sectional homoskedastic cases, both the estimated common parameters and the estimated distribution of the heterogeneous parameters achieve posterior consistency, and that the density forecasts asymptotically converge to the oracle forecast. Methodologically, I develop a simulation-based posterior sampling algorithm specifically addressing the nonparametric density estimation of unobserved heterogeneous parameters. Monte Carlo simulations and an empirical application to young firm dynamics demonstrate improvements in density forecasts relative to alternative approaches.

</details>

<details>

<summary>2021-10-05 10:10:38 - Fully differentiable model discovery</summary>

- *Gert-Jan Both, Remy Kusters*

- `2106.04886v2` - [abs](http://arxiv.org/abs/2106.04886v2) - [pdf](http://arxiv.org/pdf/2106.04886v2)

> Model discovery aims at autonomously discovering differential equations underlying a dataset. Approaches based on Physics Informed Neural Networks (PINNs) have shown great promise, but a fully-differentiable model which explicitly learns the equation has remained elusive. In this paper we propose such an approach by integrating neural network-based surrogates with Sparse Bayesian Learning (SBL). This combination yields a robust model discovery algorithm, which we showcase on various datasets. We then identify a connection with multitask learning, and build on it to construct a Physics Informed Normalizing Flow (PINF). We present a proof-of-concept using a PINF to directly learn a density model from single particle data. Our work expands PINNs to various types of neural network architectures, and connects neural network-based surrogates to the rich field of Bayesian parameter inference.

</details>

<details>

<summary>2021-10-05 11:20:24 - Likelihood-Free Inference with Deep Gaussian Processes</summary>

- *Alexander Aushev, Henri Pesonen, Markus Heinonen, Jukka Corander, Samuel Kaski*

- `2006.10571v2` - [abs](http://arxiv.org/abs/2006.10571v2) - [pdf](http://arxiv.org/pdf/2006.10571v2)

> In recent years, surrogate models have been successfully used in likelihood-free inference to decrease the number of simulator evaluations. The current state-of-the-art performance for this task has been achieved by Bayesian Optimization with Gaussian Processes (GPs). While this combination works well for unimodal target distributions, it is restricting the flexibility and applicability of Bayesian Optimization for accelerating likelihood-free inference more generally. We address this problem by proposing a Deep Gaussian Process (DGP) surrogate model that can handle more irregularly behaved target distributions. Our experiments show how DGPs can outperform GPs on objective functions with multimodal distributions and maintain a comparable performance in unimodal cases. This confirms that DGPs as surrogate models can extend the applicability of Bayesian Optimization for likelihood-free inference (BOLFI), while adding computational overhead that remains negligible for computationally intensive simulators.

</details>

<details>

<summary>2021-10-05 12:49:00 - Evaluating the impact of local tracing partnerships on the performance of contact tracing for COVID-19 in England</summary>

- *Pantelis Samartsidis, Shaun R. Seaman, Abbie Harrison, Angelos Alexopoulos, Gareth J. Hughes, Christopher Rawlinson, Charlotte Anderson, Andre Charlett, Isabel Oliver, Daniela De Angelis*

- `2110.02005v1` - [abs](http://arxiv.org/abs/2110.02005v1) - [pdf](http://arxiv.org/pdf/2110.02005v1)

> Assessing the impact of an intervention using time-series observational data on multiple units and outcomes is a frequent problem in many fields of scientific research. In this paper, we present a novel method to estimate intervention effects in such a setting by generalising existing approaches based on the factor analysis model and developing a Bayesian algorithm for inference. Our method is one of the few that can simultaneously: deal with outcomes of mixed type (continuous, binomial, count); increase efficiency in the estimates of the causal effects by jointly modelling multiple outcomes affected by the intervention; easily provide uncertainty quantification for all causal estimands of interest. We use the proposed approach to evaluate the impact that local tracing partnerships (LTP) had on the effectiveness of England's Test and Trace (TT) programme for COVID-19. Our analyses suggest that, overall, LTPs had a small positive impact on TT. However, there is considerable heterogeneity in the estimates of the causal effects over units and time.

</details>

<details>

<summary>2021-10-05 18:14:00 - Adaptive Group Testing with Mismatched Models</summary>

- *Mingzhou Fan, Byung-Jun Yoon, Francis J. Alexander, Edward R. Dougherty, Xiaoning Qian*

- `2110.02265v1` - [abs](http://arxiv.org/abs/2110.02265v1) - [pdf](http://arxiv.org/pdf/2110.02265v1)

> Accurate detection of infected individuals is one of the critical steps in stopping any pandemic. When the underlying infection rate of the disease is low, testing people in groups, instead of testing each individual in the population, can be more efficient. In this work, we consider noisy adaptive group testing design with specific test sensitivity and specificity that select the optimal group given previous test results based on pre-selected utility function. As in prior studies on group testing, we model this problem as a sequential Bayesian Optimal Experimental Design (BOED) to adaptively design the groups for each test. We analyze the required number of group tests when using the updated posterior on the infection status and the corresponding Mutual Information (MI) as our utility function for selecting new groups. More importantly, we study how the potential bias on the ground-truth noise of group tests may affect the group testing sample complexity.

</details>

<details>

<summary>2021-10-05 19:00:32 - On the Correspondence between Gaussian Processes and Geometric Harmonics</summary>

- *Felix Dietrich, Juan M. Bello-Rivas, Ioannis G. Kevrekidis*

- `2110.02296v1` - [abs](http://arxiv.org/abs/2110.02296v1) - [pdf](http://arxiv.org/pdf/2110.02296v1)

> We discuss the correspondence between Gaussian process regression and Geometric Harmonics, two similar kernel-based methods that are typically used in different contexts. Research communities surrounding the two concepts often pursue different goals. Results from both camps can be successfully combined, providing alternative interpretations of uncertainty in terms of error estimation, or leading towards accelerated Bayesian Optimization due to dimensionality reduction.

</details>

<details>

<summary>2021-10-05 19:33:42 - Approximate Message Passing for orthogonally invariant ensembles: Multivariate non-linearities and spectral initialization</summary>

- *Xinyi Zhong, Tianhao Wang, Zhou Fan*

- `2110.02318v1` - [abs](http://arxiv.org/abs/2110.02318v1) - [pdf](http://arxiv.org/pdf/2110.02318v1)

> We study a class of Approximate Message Passing (AMP) algorithms for symmetric and rectangular spiked random matrix models with orthogonally invariant noise. The AMP iterates have fixed dimension $K \geq 1$, a multivariate non-linearity is applied in each AMP iteration, and the algorithm is spectrally initialized with $K$ super-critical sample eigenvectors. We derive the forms of the Onsager debiasing coefficients and corresponding AMP state evolution, which depend on the free cumulants of the noise spectral distribution. This extends previous results for such models with $K=1$ and an independent initialization.   Applying this approach to Bayesian principal components analysis, we introduce a Bayes-OAMP algorithm that uses as its non-linearity the posterior mean conditional on all preceding AMP iterates. We describe a practical implementation of this algorithm, where all debiasing and state evolution parameters are estimated from the observed data, and we illustrate the accuracy and stability of this approach in simulations.

</details>

<details>

<summary>2021-10-05 23:38:02 - Adaptive design for identifying maximum tolerated dose early to accelerate dose-finding trial</summary>

- *Masahiro Kojima*

- `2110.02413v1` - [abs](http://arxiv.org/abs/2110.02413v1) - [pdf](http://arxiv.org/pdf/2110.02413v1)

> Purpose: The early identification of maximum tolerated dose (MTD) in phase I trial leads to faster progression to a phase II trial or an expansion cohort to confirm efficacy. Methods: We propose a novel adaptive design for identifying MTD early to accelerate dose-finding trials. The early identification of MTD is determined adaptively by dose-retainment probability using a trial data via Bayesian analysis. We applied the early identification design to an actual trial. A simulation study evaluates the performance of the early identification design. Results: In the actual study, we confirmed the MTD could be early identified and the study period was shortened. In the simulation study, the percentage of the correct MTD selection in the early identification Keyboard and early identification Bayesian optimal interval (BOIN) designs was almost same from the non-early identification version. The early identification Keyboard and BOIN designs reduced the study duration by about 50% from the model-assisted designs. In addition, the early identification Keyboard and BOIN designs reduced the study duration by about 20% from time-to-event model-assisted designs. Conclusion: We proposed the early identification of MTD maintaining the accuracy to be able to short the study period.

</details>

<details>

<summary>2021-10-06 00:57:10 - A Bayesian Modified Ising Model for Identifying Spatially Variable Genes from Spatial Transcriptomics Data</summary>

- *Xi Jiang, Qiwei Li, Guanghua Xiao*

- `2104.13957v3` - [abs](http://arxiv.org/abs/2104.13957v3) - [pdf](http://arxiv.org/pdf/2104.13957v3)

> A recent technology breakthrough in spatial molecular profiling has enabled the comprehensive molecular characterizations of single cells while preserving spatial information. It provides new opportunities to delineate how cells from different origins form tissues with distinctive structures and functions. One immediate question in spatial molecular profiling data analysis is to identify genes whose expressions exhibit spatially correlated patterns, called spatially variable genes. Most current methods to identify spatially variable genes are built upon the geostatistical model with Gaussian process to capture the spatial patterns, which rely on ad hoc kernels that could limit the models' ability to identify complex spatial patterns. In order to overcome this challenge and capture more types of spatial patterns, we introduce a Bayesian approach to identify spatially variable genes via a modified Ising model. The key idea is to use the energy interaction parameter of the Ising model to characterize spatial expression patterns. We use auxiliary variable Markov chain Monte Carlo algorithms to sample from the posterior distribution with an intractable normalizing constant in the model. Simulation studies using both simulated and synthetic data showed that the energy-based modeling approach led to higher accuracy in detecting spatially variable genes than those kernel-based methods. When applied to two real spatial transcriptomics datasets, the proposed method discovered novel spatial patterns that shed light on the biological mechanisms. In summary, the proposed method presents a new perspective for analyzing spatial transcriptomics data.

</details>

<details>

<summary>2021-10-06 04:22:13 - Genomic Data Analysis using a Two Stage Expectation Propagation Algorithm for Analysis of Sparse Bayesian High-Dimensional Instrumental Variables Regression</summary>

- *Morteza Amini*

- `2110.02496v1` - [abs](http://arxiv.org/abs/2110.02496v1) - [pdf](http://arxiv.org/pdf/2110.02496v1)

> Simultaneous analysis of gene expression data and genetic variants is highly of interest, especially when the number of gene expressions and genetic variants are both greater than the sample size. Association of both causal genes and effective SNPs makes the use of sparse modeling of such genetic data sets, highly important. The high-dimensional sparse instrumental variables models are one of such useful association models, which models the simultaneous relation of the gene expressions and genetic variants with complex traits. From a Bayesian viewpoint, the sparsity can be favored using sparsity-enforcing priors such as spike-and-slab priors. A two-stage modification of the expectation propagation (EP) algorithm is proposed and examined for approximate inference in high-dimensional sparse instrumental variables models with spike-and-slab priors. This method is an adoption of the classical two-stage least squares method, to be used with the Bayes context. A simulation study is performed to examine the performance of the methods. The proposed method is applied to analysis of the mouse obesity data.

</details>

<details>

<summary>2021-10-06 07:34:12 - Bayesian views of generalized additive modelling</summary>

- *David L. Miller*

- `1902.01330v3` - [abs](http://arxiv.org/abs/1902.01330v3) - [pdf](http://arxiv.org/pdf/1902.01330v3)

> Generalized additive models (GAMs) are a commonly used, flexible framework applied to many problems in statistical ecology. GAMs are often considered to be a purely frequentist framework (`generalized linear models with wiggly bits'), however links between frequentist and Bayesian approaches to these models were highlighted early on in the literature. Bayesian thinking underlies many parts of the implementation in the popular R package \texttt{mgcv} as well as in GAM theory more generally. This article aims to highlight useful links (and differences) between Bayesian and frequentist approaches to smoothing, and their practical applications in ecology (with an \texttt{mgcv}-centric viewpoint). Here I give some background for these results then move onto two important topics for quantitative ecologists: term/model selection and uncertainty estimation.

</details>

<details>

<summary>2021-10-06 10:37:44 - Variable Selection Using a Smooth Information Criterion for Multi-Parameter Regression Models</summary>

- *Meadhbh O'Neill, Kevin Burke*

- `2110.02643v1` - [abs](http://arxiv.org/abs/2110.02643v1) - [pdf](http://arxiv.org/pdf/2110.02643v1)

> Modern variable selection procedures make use of penalization methods to execute simultaneous model selection and estimation. A popular method is the LASSO (least absolute shrinkage and selection operator), which contains a tuning parameter. This parameter is typically tuned by minimizing the cross-validation error or Bayesian information criterion (BIC) but this can be computationally intensive as it involves fitting an array of different models and selecting the best one. However, we have developed a procedure based on the so-called "smooth IC" (SIC) in which the tuning parameter is automatically selected in one step. We also extend this model selection procedure to the so-called "multi-parameter regression" framework, which is more flexible than classical regression modelling. Multi-parameter regression introduces flexibility by taking account of the effect of covariates through multiple distributional parameters simultaneously, e.g., mean and variance. These models are useful in the context of normal linear regression when the process under study exhibits heteroscedastic behaviour. Reformulating the multi-parameter regression estimation problem in terms of penalized likelihood enables us to take advantage of the close relationship between model selection criteria and penalization. Utilizing the SIC is computationally advantageous, as it obviates the issue of having to choose multiple tuning parameters.

</details>

<details>

<summary>2021-10-06 12:01:33 - Latent Gaussian Models for High-Dimensional Spatial Extremes</summary>

- *Arnab Hazra, Raphaël Huser, Árni V. Jóhannesson*

- `2110.02680v1` - [abs](http://arxiv.org/abs/2110.02680v1) - [pdf](http://arxiv.org/pdf/2110.02680v1)

> In this chapter, we show how to efficiently model high-dimensional extreme peaks-over-threshold events over space in complex non-stationary settings, using extended latent Gaussian Models (LGMs), and how to exploit the fitted model in practice for the computation of long-term return levels. The extended LGM framework assumes that the data follow a specific parametric distribution, whose unknown parameters are transformed using a multivariate link function and are then further modeled at the latent level in terms of fixed and random effects that have a joint Gaussian distribution. In the extremal context, we here assume that the data level distribution is described in terms of a Poisson point process likelihood, motivated by asymptotic extreme-value theory, and which conveniently exploits information from all threshold exceedances. This contrasts with the more common data-wasteful approach based on block maxima, which are typically modeled with the generalized extreme-value (GEV) distribution. When conditional independence can be assumed at the data level and latent random effects have a sparse probabilistic structure, fast approximate Bayesian inference becomes possible in very high dimensions, and we here present the recently proposed inference approach called "Max-and-Smooth", which provides exceptional speed-up compared to alternative methods. The proposed methodology is illustrated by application to satellite-derived precipitation data over Saudi Arabia, obtained from the Tropical Rainfall Measuring Mission, with 2738 grid cells and about 20 million spatio-temporal observations in total. Our fitted model captures the spatial variability of extreme precipitation satisfactorily and our results show that the most intense precipitation events are expected near the south-western part of Saudi Arabia, along the Red Sea coastline.

</details>

<details>

<summary>2021-10-06 14:10:38 - Relative Entropy Gradient Sampler for Unnormalized Distributions</summary>

- *Xingdong Feng, Yuan Gao, Jian Huang, Yuling Jiao, Xu Liu*

- `2110.02787v1` - [abs](http://arxiv.org/abs/2110.02787v1) - [pdf](http://arxiv.org/pdf/2110.02787v1)

> We propose a relative entropy gradient sampler (REGS) for sampling from unnormalized distributions. REGS is a particle method that seeks a sequence of simple nonlinear transforms iteratively pushing the initial samples from a reference distribution into the samples from an unnormalized target distribution. To determine the nonlinear transforms at each iteration, we consider the Wasserstein gradient flow of relative entropy. This gradient flow determines a path of probability distributions that interpolates the reference distribution and the target distribution. It is characterized by an ODE system with velocity fields depending on the density ratios of the density of evolving particles and the unnormalized target density. To sample with REGS, we need to estimate the density ratios and simulate the ODE system with particle evolution. We propose a novel nonparametric approach to estimating the logarithmic density ratio using neural networks. Extensive simulation studies on challenging multimodal 1D and 2D mixture distributions and Bayesian logistic regression on real datasets demonstrate that the REGS outperforms the state-of-the-art sampling methods included in the comparison.

</details>

<details>

<summary>2021-10-06 16:12:37 - Bayesian neural network unit priors and generalized Weibull-tail property</summary>

- *Mariia Vladimirova, Julyan Arbel, Stéphane Girard*

- `2110.02885v1` - [abs](http://arxiv.org/abs/2110.02885v1) - [pdf](http://arxiv.org/pdf/2110.02885v1)

> The connection between Bayesian neural networks and Gaussian processes gained a lot of attention in the last few years. Hidden units are proven to follow a Gaussian process limit when the layer width tends to infinity. Recent work has suggested that finite Bayesian neural networks may outperform their infinite counterparts because they adapt their internal representations flexibly. To establish solid ground for future research on finite-width neural networks, our goal is to study the prior induced on hidden units. Our main result is an accurate description of hidden units tails which shows that unit priors become heavier-tailed going deeper, thanks to the introduced notion of generalized Weibull-tail. This finding sheds light on the behavior of hidden units of finite Bayesian neural networks.

</details>

<details>

<summary>2021-10-06 17:05:33 - Residual Overfit Method of Exploration</summary>

- *James McInerney, Nathan Kallus*

- `2110.02919v1` - [abs](http://arxiv.org/abs/2110.02919v1) - [pdf](http://arxiv.org/pdf/2110.02919v1)

> Exploration is a crucial aspect of bandit and reinforcement learning algorithms. The uncertainty quantification necessary for exploration often comes from either closed-form expressions based on simple models or resampling and posterior approximations that are computationally intensive. We propose instead an approximate exploration methodology based on fitting only two point estimates, one tuned and one overfit. The approach, which we term the residual overfit method of exploration (ROME), drives exploration towards actions where the overfit model exhibits the most overfitting compared to the tuned model. The intuition is that overfitting occurs the most at actions and contexts with insufficient data to form accurate predictions of the reward. We justify this intuition formally from both a frequentist and a Bayesian information theoretic perspective. The result is a method that generalizes to a wide variety of models and avoids the computational overhead of resampling or posterior approximations. We compare ROME against a set of established contextual bandit methods on three datasets and find it to be one of the best performing.

</details>

<details>

<summary>2021-10-06 20:21:07 - Fast methods for posterior inference of two-group normal-normal models</summary>

- *Philip Greengard, Jeremy Hoskins, Charles C. Margossian, Andrew Gelman, Aki Vehtari*

- `2110.03055v1` - [abs](http://arxiv.org/abs/2110.03055v1) - [pdf](http://arxiv.org/pdf/2110.03055v1)

> We describe a class of algorithms for evaluating posterior moments of certain Bayesian linear regression models with a normal likelihood and a normal prior on the regression coefficients. The proposed methods can be used for hierarchical mixed effects models with partial pooling over one group of predictors, as well as random effects models with partial pooling over two groups of predictors. We demonstrate the performance of the methods on two applications, one involving U.S. opinion polls and one involving the modeling of COVID-19 outbreaks in Israel using survey data. The algorithms involve analytical marginalization of regression coefficients followed by numerical integration of the remaining low-dimensional density. The dominant cost of the algorithms is an eigendecomposition computed once for each value of the outside parameter of integration. Our approach drastically reduces run times compared to state-of-the-art Markov chain Monte Carlo (MCMC) algorithms. The latter, in addition to being computationally expensive, can also be difficult to tune when applied to hierarchical models.

</details>

<details>

<summary>2021-10-07 12:54:28 - Investigating Growth at Risk Using a Multi-country Non-parametric Quantile Factor Model</summary>

- *Todd E. Clark, Florian Huber, Gary Koop, Massimiliano Marcellino, Michael Pfarrhofer*

- `2110.03411v1` - [abs](http://arxiv.org/abs/2110.03411v1) - [pdf](http://arxiv.org/pdf/2110.03411v1)

> We develop a Bayesian non-parametric quantile panel regression model. Within each quantile, the response function is a convex combination of a linear model and a non-linear function, which we approximate using Bayesian Additive Regression Trees (BART). Cross-sectional information at the pth quantile is captured through a conditionally heteroscedastic latent factor. The non-parametric feature of our model enhances flexibility, while the panel feature, by exploiting cross-country information, increases the number of observations in the tails. We develop Bayesian Markov chain Monte Carlo (MCMC) methods for estimation and forecasting with our quantile factor BART model (QF-BART), and apply them to study growth at risk dynamics in a panel of 11 advanced economies.

</details>

<details>

<summary>2021-10-07 14:07:11 - Langevin Monte Carlo: random coordinate descent and variance reduction</summary>

- *Zhiyan Ding, Qin Li*

- `2007.14209v8` - [abs](http://arxiv.org/abs/2007.14209v8) - [pdf](http://arxiv.org/pdf/2007.14209v8)

> Langevin Monte Carlo (LMC) is a popular Bayesian sampling method. For the log-concave distribution function, the method converges exponentially fast, up to a controllable discretization error. However, the method requires the evaluation of a full gradient in each iteration, and for a problem on $\mathbb{R}^d$, this amounts to $d$ times partial derivative evaluations per iteration. The cost is high when $d\gg1$. In this paper, we investigate how to enhance computational efficiency through the application of RCD (random coordinate descent) on LMC. There are two sides of the theory:   1 By blindly applying RCD to LMC, one surrogates the full gradient by a randomly selected directional derivative per iteration. Although the cost is reduced per iteration, the total number of iteration is increased to achieve a preset error tolerance. Ultimately there is no computational gain;   2 We then incorporate variance reduction techniques, such as SAGA (stochastic average gradient) and SVRG (stochastic variance reduced gradient), into RCD-LMC. It will be proved that the cost is reduced compared with the classical LMC, and in the underdamped case, convergence is achieved with the same number of iterations, while each iteration requires merely one-directional derivative. This means we obtain the best possible computational cost in the underdamped-LMC framework.

</details>

<details>

<summary>2021-10-07 19:06:23 - Global sensitivity analysis in probabilistic graphical models</summary>

- *Rafael Ballester-Ripoll, Manuele Leonelli*

- `2110.03749v1` - [abs](http://arxiv.org/abs/2110.03749v1) - [pdf](http://arxiv.org/pdf/2110.03749v1)

> We show how to apply Sobol's method of global sensitivity analysis to measure the influence exerted by a set of nodes' evidence on a quantity of interest expressed by a Bayesian network. Our method exploits the network structure so as to transform the problem of Sobol index estimation into that of marginalization inference. This way, we can efficiently compute indices for networks where brute-force or Monte Carlo based estimators for variance-based sensitivity analysis would require millions of costly samples. Moreover, our method gives exact results when exact inference is used, and also supports the case of correlated inputs. The proposed algorithm is inspired by the field of tensor networks, and generalizes earlier tensor sensitivity techniques from the acyclic to the cyclic case. We demonstrate the method on three medium to large Bayesian networks that cover the areas of project risk management and reliability engineering.

</details>

<details>

<summary>2021-10-07 19:59:46 - De-randomizing MCMC dynamics with the diffusion Stein operator</summary>

- *Zheyang Shen, Markus Heinonen, Samuel Kaski*

- `2110.03768v1` - [abs](http://arxiv.org/abs/2110.03768v1) - [pdf](http://arxiv.org/pdf/2110.03768v1)

> Approximate Bayesian inference estimates descriptors of an intractable target distribution - in essence, an optimization problem within a family of distributions. For example, Langevin dynamics (LD) extracts asymptotically exact samples from a diffusion process because the time evolution of its marginal distributions constitutes a curve that minimizes the KL-divergence via steepest descent in the Wasserstein space. Parallel to LD, Stein variational gradient descent (SVGD) similarly minimizes the KL, albeit endowed with a novel Stein-Wasserstein distance, by deterministically transporting a set of particle samples, thus de-randomizes the stochastic diffusion process. We propose de-randomized kernel-based particle samplers to all diffusion-based samplers known as MCMC dynamics. Following previous work in interpreting MCMC dynamics, we equip the Stein-Wasserstein space with a fiber-Riemannian Poisson structure, with the capacity of characterizing a fiber-gradient Hamiltonian flow that simulates MCMC dynamics. Such dynamics discretizes into generalized SVGD (GSVGD), a Stein-type deterministic particle sampler, with particle updates coinciding with applying the diffusion Stein operator to a kernel function. We demonstrate empirically that GSVGD can de-randomize complex MCMC dynamics, which combine the advantages of auxiliary momentum variables and Riemannian structure, while maintaining the high sample quality from an interacting particle system.

</details>

<details>

<summary>2021-10-07 20:55:26 - Scaling Bayesian Optimization With Game Theory</summary>

- *L. Mathesen, G. Pedrielli, R. L. Smith*

- `2110.03790v1` - [abs](http://arxiv.org/abs/2110.03790v1) - [pdf](http://arxiv.org/pdf/2110.03790v1)

> We introduce the algorithm Bayesian Optimization (BO) with Fictitious Play (BOFiP) for the optimization of high dimensional black box functions. BOFiP decomposes the original, high dimensional, space into several sub-spaces defined by non-overlapping sets of dimensions. These sets are randomly generated at the start of the algorithm, and they form a partition of the dimensions of the original space. BOFiP searches the original space with alternating BO, within sub-spaces, and information exchange among sub-spaces, to update the sub-space function evaluation. The basic idea is to distribute the high dimensional optimization across low dimensional sub-spaces, where each sub-space is a player in an equal interest game. At each iteration, BO produces approximate best replies that update the players belief distribution. The belief update and BO alternate until a stopping condition is met.   High dimensional problems are common in real applications, and several contributions in the BO literature have highlighted the difficulty in scaling to high dimensions due to the computational complexity associated to the estimation of the model hyperparameters. Such complexity is exponential in the problem dimension, resulting in substantial loss of performance for most techniques with the increase of the input dimensionality.   We compare BOFiP to several state-of-the-art approaches in the field of high dimensional black box optimization. The numerical experiments show the performance over three benchmark objective functions from 20 up to 1000 dimensions. A neural network architecture design problem is tested with 42 up to 911 nodes in 6 up to 92 layers, respectively, resulting into networks with 500 up to 10,000 weights. These sets of experiments empirically show that BOFiP outperforms its competitors, showing consistent performance across different problems and increasing problem dimensionality.

</details>

<details>

<summary>2021-10-07 22:29:19 - Approximate Post-Selective Inference for Regression with the Group LASSO</summary>

- *Snigdha Panigrahi, Peter W. MacDonald, Daniel Kessler*

- `2012.15664v3` - [abs](http://arxiv.org/abs/2012.15664v3) - [pdf](http://arxiv.org/pdf/2012.15664v3)

> We develop a post-selective Bayesian framework to jointly and consistently estimate parameters in group-sparse linear regression models. After selection with the Group LASSO (or generalized variants such as the overlapping, sparse, or standardized Group LASSO), uncertainty estimates for the selected parameters are unreliable in the absence of adjustments for selection bias. Existing post-selective approaches are limited to uncertainty estimation for (i) real-valued projections onto very specific selected subspaces for the group-sparse problem, (ii) selection events categorized broadly as polyhedral events that are expressible as linear inequalities in the data variables. Our Bayesian methods address these gaps by deriving a likelihood adjustment factor, and an approximation thereof, that eliminates bias from selection. Paying a very nominal price for this adjustment, experiments on simulated data, and data from the Human Connectome Project demonstrate the efficacy of our methods for a joint estimation of group-sparse parameters and their uncertainties post selection.

</details>

<details>

<summary>2021-10-07 22:42:40 - Estimation of Constrained Mean-Covariance of Normal Distributions</summary>

- *Anupam Kundu, Mohsen Pourahmadi*

- `2110.03819v1` - [abs](http://arxiv.org/abs/2110.03819v1) - [pdf](http://arxiv.org/pdf/2110.03819v1)

> Estimation of the mean vector and covariance matrix is of central importance in the analysis of multivariate data. In the framework of generalized linear models, usually the variances are certain functions of the means with the normal distribution being an exception. We study some implications of functional relationships between covariance and the mean by focusing on the maximum likelihood and Bayesian estimation of the mean-covariance under the joint constraint $\bm{\Sigma}\bm{\mu} = \bm{\mu}$ for a multivariate normal distribution. A novel structured covariance is proposed through reparameterization of the spectral decomposition of $\bm{\Sigma}$ involving its eigenvalues and $\bm{\mu}$. This is designed to address the challenging issue of positive-definiteness and to reduce the number of covariance parameters from quadratic to linear function of the dimension. We propose a fast (noniterative) method for approximating the maximum likelihood estimator by maximizing a lower bound for the profile likelihood function, which is concave. We use normal and inverse gamma priors on the mean and eigenvalues, and approximate the maximum aposteriori estimators by both MH within Gibbs sampling and a faster iterative method. A simulation study shows good performance of our estimators.

</details>

<details>

<summary>2021-10-08 00:47:30 - Multifile Partitioning for Record Linkage and Duplicate Detection</summary>

- *Serge Aleshin-Guendel, Mauricio Sadinle*

- `2110.03839v1` - [abs](http://arxiv.org/abs/2110.03839v1) - [pdf](http://arxiv.org/pdf/2110.03839v1)

> Merging datafiles containing information on overlapping sets of entities is a challenging task in the absence of unique identifiers, and is further complicated when some entities are duplicated in the datafiles. Most approaches to this problem have focused on linking two files assumed to be free of duplicates, or on detecting which records in a single file are duplicates. However, it is common in practice to encounter scenarios that fit somewhere in between or beyond these two settings. We propose a Bayesian approach for the general setting of multifile record linkage and duplicate detection. We use a novel partition representation to propose a structured prior for partitions that can incorporate prior information about the data collection processes of the datafiles in a flexible manner, and extend previous models for comparison data to accommodate the multifile setting. We also introduce a family of loss functions to derive Bayes estimates of partitions that allow uncertain portions of the partitions to be left unresolved. The performance of our proposed methodology is explored through extensive simulations. Code implementing the methodology is available at https://github.com/aleshing/multilink .

</details>

<details>

<summary>2021-10-08 00:55:42 - Do Ceasefires Work? A Bayesian autoregressive hidden Markov model to explore how ceasefires shape the dynamics of violence in civil war</summary>

- *Jonathan P Williams, Gudmund H Hermansen, Håvard Mokleiv Nygård, Govinda Clayton, Siri Aas Rustad, Håvard Strand*

- `2110.05475v1` - [abs](http://arxiv.org/abs/2110.05475v1) - [pdf](http://arxiv.org/pdf/2110.05475v1)

> Despite a growing body of literature focusing on ceasefires, it is unclear if most ceasefires achieve their primary objective of stopping violence. Motivated by this question and the new availability of the ETH-PRIO Civil Conflict Ceasefires Dataset, we develop a Bayesian hidden Markov modeling (HMM) framework for studying the dynamics of violence in civil wars. This ceasefires data set is the first systematic and globally comprehensive data on ceasefires, and our work is the first to analyze this new data and to explore the effect of ceasefires on conflict dynamics in a comprehensive and cross-country manner. We find that ceasefires do indeed seem to produce a significant decline in subsequent violence. However, the pre-ceasefire period (the period typically after a ceasefire agreement has been negotiated but before it is in effect) is shown to be prone to periods of intensified violence that are most likely a cause and effect of the subsequent ceasefire. This finding has significant implications for the research and practice community. Moreover, manifesting from the ubiquity of modern data repositories combined with a deficiency in meaningful labels, HMM-based semi-supervised data labeling strategies could pave the way for the next decade of conflict research progress.

</details>

<details>

<summary>2021-10-08 04:39:39 - Efficient Selection Between Hierarchical Cognitive Models: Cross-validation With Variational Bayes</summary>

- *Viet-Hung Dao, David Gunawan, Minh-Ngoc Tran, Robert Kohn, Guy E. Hawkins, Scott D. Brown*

- `2102.06814v2` - [abs](http://arxiv.org/abs/2102.06814v2) - [pdf](http://arxiv.org/pdf/2102.06814v2)

> Model comparison is the cornerstone of theoretical progress in psychological research. Common practice overwhelmingly relies on tools that evaluate competing models by balancing in-sample descriptive adequacy against model flexibility, with modern approaches advocating the use of marginal likelihood for hierarchical cognitive models. Cross-validation is another popular approach but its implementation has remained out of reach for cognitive models evaluated in a Bayesian hierarchical framework, with the major hurdle being prohibitive computational cost. To address this issue, we develop novel algorithms that make variational Bayes (VB) inference for hierarchical models feasible and computationally efficient for complex cognitive models of substantive theoretical interest. It is well known that VB produces good estimates of the first moments of the parameters which gives good predictive densities estimates. We thus develop a novel VB algorithm with Bayesian prediction as a tool to perform model comparison by cross-validation, which we refer to as CVVB. In particular, the CVVB can be used as a model screening device that quickly identifies bad models. We demonstrate the utility of CVVB by revisiting a classic question in decision making research: what latent components of processing drive the ubiquitous speed-accuracy tradeoff? We demonstrate that CVVB strongly agrees with model comparison via marginal likelihood yet achieves the outcome in much less time. Our approach brings cross-validation within reach of theoretically important psychological models, and makes it feasible to compare much larger families of hierarchically specified cognitive models than has previously been possible.

</details>

<details>

<summary>2021-10-08 06:49:36 - Semi-supervised learning objectives as log-likelihoods in a generative model of data curation</summary>

- *Stoil Ganev, Laurence Aitchison*

- `2008.05913v2` - [abs](http://arxiv.org/abs/2008.05913v2) - [pdf](http://arxiv.org/pdf/2008.05913v2)

> We currently do not have an understanding of semi-supervised learning (SSL) objectives such as pseudo-labelling and entropy minimization as log-likelihoods, which precludes the development of e.g. Bayesian SSL. Here, we note that benchmark image datasets such as CIFAR-10 are carefully curated, and we formulate SSL objectives as a log-likelihood in a generative model of data curation that was initially developed to explain the cold-posterior effect (Aitchison 2020). SSL methods, from entropy minimization and pseudo-labelling, to state-of-the-art techniques similar to FixMatch can be understood as lower-bounds on our principled log-likelihood. We are thus able to give a proof-of-principle for Bayesian SSL on toy data. Finally, our theory suggests that SSL is effective in part due to the statistical patterns induced by data curation. This provides an explanation of past results which show SSL performs better on clean datasets without any "out of distribution" examples. Confirming these results we find that SSL gave much larger performance improvements on curated than on uncurated data, using matched curated and uncurated datasets based on Galaxy Zoo 2.

</details>

<details>

<summary>2021-10-08 12:58:33 - Rule-based Bayesian regression</summary>

- *Themistoklis Botsas, Lachlan R. Mason, Indranil Pan*

- `2008.00422v2` - [abs](http://arxiv.org/abs/2008.00422v2) - [pdf](http://arxiv.org/pdf/2008.00422v2)

> We introduce a novel rule-based approach for handling regression problems. The new methodology carries elements from two frameworks: (i) it provides information about the uncertainty of the parameters of interest using Bayesian inference, and (ii) it allows the incorporation of expert knowledge through rule-based systems. The blending of those two different frameworks can be particularly beneficial for various domains (e.g. engineering), where, even though the significance of uncertainty quantification motivates a Bayesian approach, there is no simple way to incorporate researcher intuition into the model. We validate our models by applying them to synthetic applications: a simple linear regression problem and two more complex structures based on partial differential equations. Finally, we review the advantages of our methodology, which include the simplicity of the implementation, the uncertainty reduction due to the added information and, in some occasions, the derivation of better point predictions, and we address limitations, mainly from the computational complexity perspective, such as the difficulty in choosing an appropriate algorithm and the added computational burden.

</details>

<details>

<summary>2021-10-08 13:18:55 - mBART: Multidimensional Monotone BART</summary>

- *Hugh A. Chipman, Edward I. George, Robert E. McCulloch, Thomas S. Shively*

- `1612.01619v3` - [abs](http://arxiv.org/abs/1612.01619v3) - [pdf](http://arxiv.org/pdf/1612.01619v3)

> For the discovery of regression relationships between Y and a large set of p potential predictors x 1 , . . . , x p , the flexible nonparametric nature of BART (Bayesian Additive Regression Trees) allows for a much richer set of possibilities than restrictive parametric approaches. However, subject matter considerations sometimes warrant a minimal assumption of monotonicity in at least some of the predictors. For such contexts, we introduce mBART, a constrained version of BART that can flexibly incorporate monotonicity in any predesignated subset of predictors using a multivariate basis of monotone trees, while avoiding the further confines of a full parametric form. For such monotone relationships, mBART provides (i) function estimates that are smoother and more interpretable, (ii) better out-of-sample predictive performance, and (iii) less post-data uncertainty. While many key aspects of the unconstrained BART model carry over directly to mBART, the introduction of monotonicity constraints necessitates a fundamental rethinking of how the model is implemented. In particular, the original BART Markov Chain Monte Carlo algorithm relied on a conditional conjugacy that is no longer available in a monotonically constrained space. Various simulated and real examples demonstrate the wide ranging potential of mBART.

</details>

<details>

<summary>2021-10-08 17:53:46 - Is MC Dropout Bayesian?</summary>

- *Loic Le Folgoc, Vasileios Baltatzis, Sujal Desai, Anand Devaraj, Sam Ellis, Octavio E. Martinez Manzanera, Arjun Nair, Huaqi Qiu, Julia Schnabel, Ben Glocker*

- `2110.04286v1` - [abs](http://arxiv.org/abs/2110.04286v1) - [pdf](http://arxiv.org/pdf/2110.04286v1)

> MC Dropout is a mainstream "free lunch" method in medical imaging for approximate Bayesian computations (ABC). Its appeal is to solve out-of-the-box the daunting task of ABC and uncertainty quantification in Neural Networks (NNs); to fall within the variational inference (VI) framework; and to propose a highly multimodal, faithful predictive posterior. We question the properties of MC Dropout for approximate inference, as in fact MC Dropout changes the Bayesian model; its predictive posterior assigns $0$ probability to the true model on closed-form benchmarks; the multimodality of its predictive posterior is not a property of the true predictive posterior but a design artefact. To address the need for VI on arbitrary models, we share a generic VI engine within the pytorch framework. The code includes a carefully designed implementation of structured (diagonal plus low-rank) multivariate normal variational families, and mixtures thereof. It is intended as a go-to no-free-lunch approach, addressing shortcomings of mean-field VI with an adjustable trade-off between expressivity and computational complexity.

</details>

<details>

<summary>2021-10-09 22:10:08 - Random Feature Stein Discrepancies</summary>

- *Jonathan H. Huggins, Lester Mackey*

- `1806.07788v5` - [abs](http://arxiv.org/abs/1806.07788v5) - [pdf](http://arxiv.org/pdf/1806.07788v5)

> Computable Stein discrepancies have been deployed for a variety of applications, ranging from sampler selection in posterior inference to approximate Bayesian inference to goodness-of-fit testing. Existing convergence-determining Stein discrepancies admit strong theoretical guarantees but suffer from a computational cost that grows quadratically in the sample size. While linear-time Stein discrepancies have been proposed for goodness-of-fit testing, they exhibit avoidable degradations in testing power -- even when power is explicitly optimized. To address these shortcomings, we introduce feature Stein discrepancies ($\Phi$SDs), a new family of quality measures that can be cheaply approximated using importance sampling. We show how to construct $\Phi$SDs that provably determine the convergence of a sample to its target and develop high-accuracy approximations -- random $\Phi$SDs (R$\Phi$SDs) -- which are computable in near-linear time. In our experiments with sampler selection for approximate posterior inference and goodness-of-fit testing, R$\Phi$SDs perform as well or better than quadratic-time KSDs while being orders of magnitude faster to compute.

</details>

<details>

<summary>2021-10-10 07:49:48 - Ordinary Differential Equation Models and their Computation Methods</summary>

- *Jaeyong Lee*

- `2110.04726v1` - [abs](http://arxiv.org/abs/2110.04726v1) - [pdf](http://arxiv.org/pdf/2110.04726v1)

> In this article, I introduce the differential equation model and review their frequentist and Bayesian computation methods. A numerical example of the FitzHugh-Nagumo model is given.

</details>

<details>

<summary>2021-10-10 15:59:01 - Hotel Preference Rank based on Online Customer Review</summary>

- *Muhammad Apriandito Arya Saputra, Andry Alamsyah, Fajar Ibnu Fatihan*

- `2110.06133v1` - [abs](http://arxiv.org/abs/2110.06133v1) - [pdf](http://arxiv.org/pdf/2110.06133v1)

> Topline hotels are now shifting into the digital way in how they understand their customers to maintain and ensuring satisfaction. Rather than the conventional way which uses written reviews or interviews, the hotel is now heavily investing in Artificial Intelligence particularly Machine Learning solutions. Analysis of online customer reviews changes the way companies make decisions in a more effective way than using conventional analysis. The purpose of this research is to measure hotel service quality. The proposed approach emphasizes service quality dimensions reviews of the top-5 luxury hotel in Indonesia that appear on the online travel site TripAdvisor based on section Best of 2018. In this research, we use a model based on a simple Bayesian classifier to classify each customer review into one of the service quality dimensions. Our model was able to separate each classification properly by accuracy, kappa, recall, precision, and F-measure measurements. To uncover latent topics in the customer's opinion we use Topic Modeling. We found that the common issue that occurs is about responsiveness as it got the lowest percentage compared to others. Our research provides a faster outlook of hotel rank based on service quality to end customers based on a summary of the previous online review.

</details>

<details>

<summary>2021-10-10 17:02:41 - Mixture representations for likelihood ratio ordered distributions</summary>

- *Michael Jauch, Andrés F. Barrientos, Víctor Peña, David S. Matteson*

- `2110.04852v1` - [abs](http://arxiv.org/abs/2110.04852v1) - [pdf](http://arxiv.org/pdf/2110.04852v1)

> In this article, we introduce mixture representations for likelihood ratio ordered distributions. Essentially, the ratio of two probability densities, or mass functions, is monotone if and only if one can be expressed as a mixture of one-sided truncations of the other. To illustrate the practical value of the mixture representations, we address the problem of density estimation for likelihood ratio ordered distributions. In particular, we propose a nonparametric Bayesian solution which takes advantage of the mixture representations. The prior distribution is constructed from Dirichlet process mixtures and has large support on the space of pairs of densities satisfying the monotone ratio constraint. With a simple modification to the prior distribution, we can test the equality of two distributions against the alternative of likelihood ratio ordering. We develop a Markov chain Monte Carlo algorithm for posterior inference and demonstrate the method in a biomedical application.

</details>

<details>

<summary>2021-10-10 18:31:55 - FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning</summary>

- *Hong-You Chen, Wei-Lun Chao*

- `2009.01974v4` - [abs](http://arxiv.org/abs/2009.01974v4) - [pdf](http://arxiv.org/pdf/2009.01974v4)

> Federated learning aims to collaboratively train a strong global model by accessing users' locally trained models but not their own data. A crucial step is therefore to aggregate local models into a global model, which has been shown challenging when users have non-i.i.d. data. In this paper, we propose a novel aggregation algorithm named FedBE, which takes a Bayesian inference perspective by sampling higher-quality global models and combining them via Bayesian model Ensemble, leading to much robust aggregation. We show that an effective model distribution can be constructed by simply fitting a Gaussian or Dirichlet distribution to the local models. Our empirical studies validate FedBE's superior performance, especially when users' data are not i.i.d. and when the neural networks go deeper. Moreover, FedBE is compatible with recent efforts in regularizing users' model training, making it an easily applicable module: you only need to replace the aggregation method but leave other parts of your federated learning algorithm intact. Our code is publicly available at https://github.com/hongyouc/FedBE.

</details>

<details>

<summary>2021-10-11 04:11:01 - Graphical Assistant Grouped Network Autoregression Model: a Bayesian Nonparametric Recourse</summary>

- *Yimeng Ren, Xuening Zhu, Guanyu Hu*

- `2110.04991v1` - [abs](http://arxiv.org/abs/2110.04991v1) - [pdf](http://arxiv.org/pdf/2110.04991v1)

> Vector autoregression model is ubiquitous in classical time series data analysis. With the rapid advance of social network sites, time series data over latent graph is becoming increasingly popular. In this paper, we develop a novel Bayesian grouped network autoregression model to simultaneously estimate group information (number of groups and group configurations) and group-wise parameters. Specifically, a graphically assisted Chinese restaurant process is incorporated under framework of the network autoregression model to improve the statistical inference performance. An efficient Markov chain Monte Carlo sampling algorithm is used to sample from the posterior distribution. Extensive studies are conducted to evaluate the finite sample performance of our proposed methodology. Additionally, we analyze two real datasets as illustrations of the effectiveness of our approach.

</details>

<details>

<summary>2021-10-11 08:52:58 - A General Framework for the Disintegration of PAC-Bayesian Bounds</summary>

- *Paul Viallard, Pascal Germain, Amaury Habrard, Emilie Morvant*

- `2102.08649v2` - [abs](http://arxiv.org/abs/2102.08649v2) - [pdf](http://arxiv.org/pdf/2102.08649v2)

> PAC-Bayesian bounds are known to be tight and informative when studying the generalization ability of randomized classifiers. However, when applied to some family of deterministic models such as neural networks, they require a loose and costly derandomization step. As an alternative to this step, we introduce new PAC-Bayesian generalization bounds that have the originality to provide disintegrated bounds, i.e., they give guarantees over one single hypothesis instead of the usual averaged analysis. Our bounds are easily optimizable and can be used to design learning algorithms. We illustrate the interest of our result on neural networks and show a significant practical improvement over the state-of-the-art framework.

</details>

<details>

<summary>2021-10-11 13:39:20 - Optional Pólya trees: posterior rates and uncertainty quantification</summary>

- *Ismaël Castillo, Thibault Randrianarisoa*

- `2110.05265v1` - [abs](http://arxiv.org/abs/2110.05265v1) - [pdf](http://arxiv.org/pdf/2110.05265v1)

> We consider statistical inference in the density estimation model using a tree-based Bayesian approach, with Optional P\'olya trees as prior distribution. We derive near-optimal convergence rates for corresponding posterior distributions with respect to the supremum norm. For broad classes of H\"older-smooth densities, we show that the method automatically adapts to the unknown H\"older regularity parameter. We consider the question of uncertainty quantification by providing mathematical guarantees for credible sets from the obtained posterior distributions, leading to near-optimal uncertainty quantification for the density function, as well as related functionals such as the cumulative distribution function. The results are illustrated through a brief simulation study.

</details>

<details>

<summary>2021-10-11 19:33:07 - Bayesian Regularization for Functional Graphical Models</summary>

- *Jiajing Niu, Boyoung Hur, John Absher, D. Andrew Brown*

- `2110.05575v1` - [abs](http://arxiv.org/abs/2110.05575v1) - [pdf](http://arxiv.org/pdf/2110.05575v1)

> Graphical models, used to express conditional dependence between random variables observed at various nodes, are used extensively in many fields such as genetics, neuroscience, and social network analysis. While most current statistical methods for estimating graphical models focus on scalar data, there is interest in estimating analogous dependence structures when the data observed at each node are functional, such as signals or images. In this paper, we propose a fully Bayesian regularization scheme for estimating functional graphical models. We first consider a direct Bayesian analog of the functional graphical lasso proposed by Qiao et al. (2019). We then propose a regularization strategy via the graphical horseshoe. We compare these approaches via simulation study and apply our proposed functional graphical horseshoe to two motivating applications, electroencephalography data for comparing brain activation between an alcoholic group and controls, as well as changes in structural connectivity in the presence of traumatic brain injury (TBI). Our results yield insight into how the brain attempts to compensate for disconnected networks after injury.

</details>

<details>

<summary>2021-10-11 20:05:22 - Bayesian optimization for modular black-box systems with switching costs</summary>

- *Chi-Heng Lin, Joseph D. Miano, Eva L. Dyer*

- `2006.02624v2` - [abs](http://arxiv.org/abs/2006.02624v2) - [pdf](http://arxiv.org/pdf/2006.02624v2)

> Most existing black-box optimization methods assume that all variables in the system being optimized have equal cost and can change freely at each iteration. However, in many real world systems, inputs are passed through a sequence of different operations or modules, making variables in earlier stages of processing more costly to update. Such structure imposes a cost on switching variables in early parts of a data processing pipeline. In this work, we propose a new algorithm for switch cost-aware optimization called Lazy Modular Bayesian Optimization (LaMBO). This method efficiently identifies the global optimum while minimizing cost through a passive change of variables in early modules. The method is theoretical grounded and achieves vanishing regret when augmented with switching cost. We apply LaMBO to multiple synthetic functions and a three-stage image segmentation pipeline used in a neuroscience application, where we obtain promising improvements over prevailing cost-aware Bayesian optimization algorithms. Our results demonstrate that LaMBO is an effective strategy for black-box optimization that is capable of minimizing switching costs in modular systems.

</details>

<details>

<summary>2021-10-12 11:56:51 - Designing truncated priors for direct and inverse Bayesian problems</summary>

- *Sergios Agapiou, Peter Mathé*

- `2105.10254v2` - [abs](http://arxiv.org/abs/2105.10254v2) - [pdf](http://arxiv.org/pdf/2105.10254v2)

> The Bayesian approach to inverse problems with functional unknowns, has received significant attention in recent years. An important component of the developing theory is the study of the asymptotic performance of the posterior distribution in the frequentist setting. The present paper contributes to the area of Bayesian inverse problems by formulating a posterior contraction theory for linear inverse problems, with truncated Gaussian series priors, and under general smoothness assumptions. Emphasis is on the intrinsic role of the truncation point both for the direct as well as for the inverse problem, which are related through the modulus of continuity as this was recently highlighted by Knapik and Salomond (2018).

</details>

<details>

<summary>2021-10-12 12:58:26 - Generalized Parallel Tempering on Bayesian Inverse Problems</summary>

- *Jonas Latz, Juan P. Madrigal-Cianci, Fabio Nobile, Raul Tempone*

- `2003.03341v3` - [abs](http://arxiv.org/abs/2003.03341v3) - [pdf](http://arxiv.org/pdf/2003.03341v3)

> In the current work we present two generalizations of the Parallel Tempering algorithm, inspired by the so-called continuous-time Infinite Swapping algorithm. Such a method, found its origins in the molecular dynamics community, and can be understood as the limit case of the continuous-time Parallel Tempering algorithm, where the (random) time between swaps of states between two parallel chains goes to zero. Thus, swapping states between chains occurs continuously. In the current work, we extend this idea to the context of time-discrete Markov chains and present two Markov chain Monte Carlo algorithms that follow the same paradigm as the continuous-time infinite swapping procedure. We analyze the convergence properties of such discrete-time algorithms in terms of their spectral gap, and implement them to sample from different target distributions. Numerical results show that the proposed methods significantly improve over more traditional sampling algorithms such as Random Walk Metropolis and (traditional) Parallel Tempering.

</details>

<details>

<summary>2021-10-13 13:25:13 - Integrative Bayesian models using Post-selective Inference: a case study in Radiogenomics</summary>

- *Snigdha Panigrahi, Shariq Mohammed, Arvind Rao, Veerabhadran Baladandayuthapani*

- `2004.12012v3` - [abs](http://arxiv.org/abs/2004.12012v3) - [pdf](http://arxiv.org/pdf/2004.12012v3)

> Integrative analyses based on statistically relevant associations between genomics and a wealth of intermediary phenotypes (such as imaging) provide vital insights into their clinical relevance in terms of the disease mechanisms. Estimates for uncertainty in the resulting integrative models are however unreliable unless inference accounts for the selection of these associations with accuracy. In this article, we develop selection-aware Bayesian methods which: (i) counteract the impact of model selection bias through a "selection-aware posterior" in a flexible class of integrative Bayesian models post a selection of promising variables via $\ell_1$-regularized algorithms; (ii) strike an inevitable tradeoff between the quality of model selection and inferential power when the same dataset is used for both selection and uncertainty estimation. Central to our methodological development, a carefully constructed conditional likelihood function deployed with a reparameterization mapping provides notably tractable updates when gradient-based MCMC sampling is used for estimating uncertainties from the selection-aware posterior. Applying our methods to a radiogenomic analysis, we successfully recover several important gene pathways and estimate uncertainties for their associations with patient survival times.

</details>

<details>

<summary>2021-10-13 15:11:25 - Incremental Ensemble Gaussian Processes</summary>

- *Qin Lu, Georgios V. Karanikolas, Georgios B. Giannakis*

- `2110.06777v1` - [abs](http://arxiv.org/abs/2110.06777v1) - [pdf](http://arxiv.org/pdf/2110.06777v1)

> Belonging to the family of Bayesian nonparametrics, Gaussian process (GP) based approaches have well-documented merits not only in learning over a rich class of nonlinear functions, but also in quantifying the associated uncertainty. However, most GP methods rely on a single preselected kernel function, which may fall short in characterizing data samples that arrive sequentially in time-critical applications. To enable {\it online} kernel adaptation, the present work advocates an incremental ensemble (IE-) GP framework, where an EGP meta-learner employs an {\it ensemble} of GP learners, each having a unique kernel belonging to a prescribed kernel dictionary. With each GP expert leveraging the random feature-based approximation to perform online prediction and model update with {\it scalability}, the EGP meta-learner capitalizes on data-adaptive weights to synthesize the per-expert predictions. Further, the novel IE-GP is generalized to accommodate time-varying functions by modeling structured dynamics at the EGP meta-learner and within each GP learner. To benchmark the performance of IE-GP and its dynamic variant in the adversarial setting where the modeling assumptions are violated, rigorous performance analysis has been conducted via the notion of regret, as the norm in online convex optimization. Last but not the least, online unsupervised learning for dimensionality reduction is explored under the novel IE-GP framework. Synthetic and real data tests demonstrate the effectiveness of the proposed schemes.

</details>

<details>

<summary>2021-10-13 17:03:21 - Bayesian logistic regression for online recalibration and revision of risk prediction models with performance guarantees</summary>

- *Jean Feng, Alexej Gossmann, Berkman Sahiner, Romain Pirracchio*

- `2110.06866v1` - [abs](http://arxiv.org/abs/2110.06866v1) - [pdf](http://arxiv.org/pdf/2110.06866v1)

> After deploying a clinical prediction model, subsequently collected data can be used to fine-tune its predictions and adapt to temporal shifts. Because model updating carries risks of over-updating/fitting, we study online methods with performance guarantees. We introduce two procedures for continual recalibration or revision of an underlying prediction model: Bayesian logistic regression (BLR) and a Markov variant that explicitly models distribution shifts (MarBLR). We perform empirical evaluation via simulations and a real-world study predicting COPD risk. We derive "Type I and II" regret bounds, which guarantee the procedures are non-inferior to a static model and competitive with an oracle logistic reviser in terms of the average loss. Both procedures consistently outperformed the static model and other online logistic revision methods. In simulations, the average estimated calibration index (aECI) of the original model was 0.828 (95%CI 0.818-0.938). Online recalibration using BLR and MarBLR improved the aECI, attaining 0.265 (95%CI 0.230-0.300) and 0.241 (95%CI 0.216-0.266), respectively. When performing more extensive logistic model revisions, BLR and MarBLR increased the average AUC (aAUC) from 0.767 (95%CI 0.765-0.769) to 0.800 (95%CI 0.798-0.802) and 0.799 (95%CI 0.797-0.801), respectively, in stationary settings and protected against substantial model decay. In the COPD study, BLR and MarBLR dynamically combined the original model with a continually-refitted gradient boosted tree to achieve aAUCs of 0.924 (95%CI 0.913-0.935) and 0.925 (95%CI 0.914-0.935), compared to the static model's aAUC of 0.904 (95%CI 0.892-0.916). Despite its simplicity, BLR is highly competitive with MarBLR. MarBLR outperforms BLR when its prior better reflects the data. BLR and MarBLR can improve the transportability of clinical prediction models and maintain their performance over time.

</details>

<details>

<summary>2021-10-13 21:58:05 - Sparse Linear Mixed Model Selection via Streamlined Variational Bayes</summary>

- *Emanuele Degani, Luca Maestrini, Dorota Toczydłowska, Matt P. Wand*

- `2110.07048v1` - [abs](http://arxiv.org/abs/2110.07048v1) - [pdf](http://arxiv.org/pdf/2110.07048v1)

> Linear mixed models are a versatile statistical tool to study data by accounting for fixed effects and random effects from multiple sources of variability. In many situations, a large number of candidate fixed effects is available and it is of interest to select a parsimonious subset of those being effectively relevant for predicting the response variable. Variational approximations facilitate fast approximate Bayesian inference for the parameters of a variety of statistical models, including linear mixed models. However, for models having a high number of fixed or random effects, simple application of standard variational inference principles does not lead to fast approximate inference algorithms, due to the size of model design matrices and inefficient treatment of sparse matrix problems arising from the required approximating density parameters updates. We illustrate how recently developed streamlined variational inference procedures can be generalized to make fast and accurate inference for the parameters of linear mixed models with nested random effects and global-local priors for Bayesian fixed effects selection. Our variational inference algorithms achieve convergence to the same optima of their standard implementations, although with significantly lower computational effort, memory usage and time, especially for large numbers of random effects. Using simulated and real data examples, we assess the quality of automated procedures for fixed effects selection that are free from hyperparameters tuning and only rely upon variational posterior approximations. Moreover, we show high accuracy of variational approximations against model fitting via Markov Chain Monte Carlo sampling.

</details>

<details>

<summary>2021-10-13 22:02:18 - Fast Approximate Inference for Spatial Extreme Value Models</summary>

- *Meixi Chen, Reza Ramezan, Martin Lysy*

- `2110.07051v1` - [abs](http://arxiv.org/abs/2110.07051v1) - [pdf](http://arxiv.org/pdf/2110.07051v1)

> The generalized extreme value (GEV) distribution is a popular model for analyzing and forecasting extreme weather data. To increase prediction accuracy, spatial information is often pooled via a latent Gaussian process on the GEV parameters. Inference for such hierarchical GEV models is typically carried out using Markov chain Monte Carlo (MCMC) methods. However, MCMC can be prohibitively slow and computationally intensive when the number of latent variables is moderate to large. In this paper, we develop a fast Bayesian inference method for spatial GEV models based on the Laplace approximation. Through simulation studies, we compare the speed and accuracy of our method to both MCMC and a more sophisticated but less flexible Bayesian approximation. A case study in forecasting extreme wind speeds is presented.

</details>

<details>

<summary>2021-10-14 13:45:55 - Distribution-Free Bayesian multivariate predictive inference</summary>

- *Daniel Yekutieli*

- `2110.07361v1` - [abs](http://arxiv.org/abs/2110.07361v1) - [pdf](http://arxiv.org/pdf/2110.07361v1)

> We introduce a comprehensive Bayesian multivariate predictive inference framework. The basis for our framework is a hierarchical Bayesian model, that is a mixture of finite Polya trees corresponding to multiple dyadic partitions of the unit cube. Given a sample of observations from an unknown multivariate distribution, the posterior predictive distribution is used to model and generate future observations from the unknown distribution. We illustrate the implementation of our methodology and study its performance on simulated examples. We introduce an algorithm for constructing conformal prediction sets, that provide finite sample probability assurances for future observations, with our Bayesian model.

</details>

<details>

<summary>2021-10-14 17:29:54 - General dependence structures for some models based on exponential families with quadratic variance functions</summary>

- *Luis Nieto-Barajas, Eduardo Gutiérrez-Peña*

- `2103.01218v2` - [abs](http://arxiv.org/abs/2103.01218v2) - [pdf](http://arxiv.org/pdf/2103.01218v2)

> We describe a procedure to introduce general dependence structures on a set of random variables. These include order-$q$ moving average-type structures, as well as seasonal, periodic, spatial and spatio-temporal dependences. The invariant marginal distribution can be in any family that is conjugate to an exponential family with quadratic variance function. Dependence is induced via a set of suitable latent variables whose conditional distribution mirrors the sampling distribution in a Bayesian conjugate analysis of such exponential families. We obtain strict stationarity as a special case.

</details>

<details>

<summary>2021-10-14 18:24:54 - A class of dependent Dirichlet processes via latent multinomial processes</summary>

- *Luis E. Nieto-Barajas*

- `2108.12396v2` - [abs](http://arxiv.org/abs/2108.12396v2) - [pdf](http://arxiv.org/pdf/2108.12396v2)

> We describe a procedure to introduce general dependence structures on a set of Dirichlet processes. Dependence can be in one direction to define a time series or in two directions to define spatial dependencies. More directions can also be considered. Dependence is induced via a set of latent processes and exploit the conjugacy property between the Dirichlet and the multinomial processes to ensure that the marginal law for each element of the set is a Dirichlet process. Dependence is characterised through the correlation between any two elements. Posterior distributions are obtained when we use the set of Dirichlet processes as prior distributions in a bayesian nonparametric context. Posterior predictive distributions induce partially exchangeable sequences defined by generalised P\'olya urs. A numerical example to illustrate is also included.

</details>

<details>

<summary>2021-10-14 20:03:10 - Averting A Crisis In Simulation-Based Inference</summary>

- *Joeri Hermans, Arnaud Delaunoy, François Rozet, Antoine Wehenkel, Gilles Louppe*

- `2110.06581v2` - [abs](http://arxiv.org/abs/2110.06581v2) - [pdf](http://arxiv.org/pdf/2110.06581v2)

> We present extensive empirical evidence showing that current Bayesian simulation-based inference algorithms are inadequate for the falsificationist methodology of scientific inquiry. Our results collected through months of experimental computations show that all benchmarked algorithms -- (S)NPE, (S)NRE, SNL and variants of ABC -- may produce overconfident posterior approximations, which makes them demonstrably unreliable and dangerous if one's scientific goal is to constrain parameters of interest. We believe that failing to address this issue will lead to a well-founded trust crisis in simulation-based inference. For this reason, we argue that research efforts should now consider theoretical and methodological developments of conservative approximate inference algorithms and present research directions towards this objective. In this regard, we show empirical evidence that ensembles are consistently more reliable.

</details>

<details>

<summary>2021-10-14 22:16:18 - Practical Bayesian System Identification using Hamiltonian Monte Carlo</summary>

- *Johannes Hendriks, Adrian Wills, Brett Ninness, Johan Dahlin*

- `2011.04117v2` - [abs](http://arxiv.org/abs/2011.04117v2) - [pdf](http://arxiv.org/pdf/2011.04117v2)

> This paper considers Bayesian parameter estimation of dynamic systems using a Markov Chain Monte Carlo (MCMC) approach. The Metroplis-Hastings (MH) algorithm is employed, and the main contribution of the paper is to examine and illustrate the efficacy of a particular proposal density based on energy preserving Hamiltonian dynamics, which results in what is known in the statistics literature as ``Hamiltonian Monte--Carlo'' (HMC). The very significant utility of this approach is that, as will be illustrated, it greatly reduces (almost to the point of elimination) the typically very high correlation in the Metropolis--Hastings chain which has been observed by several authors to restrict the application of the MH approach to only very low dimension model structures. The paper illustrates how the HMC approach may be applied to both significant dimension linear and nonlinear model structures, even when the system order is unknown, and using both simulated and real data.

</details>

<details>

<summary>2021-10-15 02:32:31 - A novel framework to quantify uncertainty in peptide-tandem mass spectrum matches with application to nanobody peptide identification</summary>

- *Chris McKennan, Zhe Sang, Yi Shi*

- `2110.07818v1` - [abs](http://arxiv.org/abs/2110.07818v1) - [pdf](http://arxiv.org/pdf/2110.07818v1)

> Nanobodies are small antibody fragments derived from camelids that selectively bind to antigens. These proteins have marked physicochemical properties that support advanced therapeutics, including treatments for SARS-CoV-2. To realize their potential, bottom-up proteomics via liquid chromatography-tandem mass spectrometry (LC-MS/MS) has been proposed to identify antigen-specific nanobodies at the proteome scale, where a critical component of this pipeline is matching nanobody peptides to their begotten tandem mass spectra. While peptide-spectrum matching is a well-studied problem, we show the sequence similarity between nanobody peptides violates key assumptions necessary to infer nanobody peptide-spectrum matches (PSMs) with the standard target-decoy paradigm, and prove these violations beget inflated error rates. To address these issues, we then develop a novel framework and method that treats peptide-spectrum matching as a Bayesian model selection problem with an incomplete model space, which are, to our knowledge, the first to account for all sources of PSM error without relying on the aforementioned assumptions. In addition to illustrating our method's improved performance on simulated and real nanobody data, our work demonstrates how to leverage novel retention time and spectrum prediction tools to develop accurate and discriminating data-generating models, and, to our knowledge, provides the first rigorous description of MS/MS spectrum noise.

</details>

<details>

<summary>2021-10-15 09:44:21 - Pathologies in priors and inference for Bayesian transformers</summary>

- *Tristan Cinquin, Alexander Immer, Max Horn, Vincent Fortuin*

- `2110.04020v2` - [abs](http://arxiv.org/abs/2110.04020v2) - [pdf](http://arxiv.org/pdf/2110.04020v2)

> In recent years, the transformer has established itself as a workhorse in many applications ranging from natural language processing to reinforcement learning. Similarly, Bayesian deep learning has become the gold-standard for uncertainty estimation in safety-critical applications, where robustness and calibration are crucial. Surprisingly, no successful attempts to improve transformer models in terms of predictive uncertainty using Bayesian inference exist. In this work, we study this curiously underpopulated area of Bayesian transformers. We find that weight-space inference in transformers does not work well, regardless of the approximate posterior. We also find that the prior is at least partially at fault, but that it is very hard to find well-specified weight priors for these models. We hypothesize that these problems stem from the complexity of obtaining a meaningful mapping from weight-space to function-space distributions in the transformer. Therefore, moving closer to function-space, we propose a novel method based on the implicit reparameterization of the Dirichlet distribution to apply variational inference directly to the attention weights. We find that this proposed method performs competitively with our baselines.

</details>

<details>

<summary>2021-10-15 09:52:28 - Tailored Bayes: a risk modelling framework under unequal misclassification costs</summary>

- *Solon Karapanagiotis, Umberto Benedetto, Sach Mukherjee, Paul D. W. Kirk, Paul J. Newcombe*

- `2104.01822v3` - [abs](http://arxiv.org/abs/2104.01822v3) - [pdf](http://arxiv.org/pdf/2104.01822v3)

> Risk prediction models are a crucial tool in healthcare. Risk prediction models with a binary outcome (i.e., binary classification models) are often constructed using methodology which assumes the costs of different classification errors are equal. In many healthcare applications this assumption is not valid, and the differences between misclassification costs can be quite large. For instance, in a diagnostic setting, the cost of misdiagnosing a person with a life-threatening disease as healthy may be larger than the cost of misdiagnosing a healthy person as a patient. In this work, we present Tailored Bayes (TB), a novel Bayesian inference framework which "tailors" model fitting to optimise predictive performance with respect to unbalanced misclassification costs. We use simulation studies to showcase when TB is expected to outperform standard Bayesian methods in the context of logistic regression. We then apply TB to three real-world applications, a cardiac surgery, a breast cancer prognostication task and a breast cancer tumour classification task, and demonstrate the improvement in predictive performance over standard methods.

</details>

<details>

<summary>2021-10-15 11:20:40 - Cross-validation based adaptive sampling for Gaussian process models</summary>

- *Hossein Mohammadi, Peter Challenor, Daniel Williamson, Marc Goodfellow*

- `2005.01814v6` - [abs](http://arxiv.org/abs/2005.01814v6) - [pdf](http://arxiv.org/pdf/2005.01814v6)

> In many real-world applications, we are interested in approximating black-box, costly functions as accurately as possible with the smallest number of function evaluations. A complex computer code is an example of such a function. In this work, a Gaussian process (GP) emulator is used to approximate the output of complex computer code. We consider the problem of extending an initial experiment (set of model runs) sequentially to improve the emulator. A sequential sampling approach based on leave-one-out (LOO) cross-validation is proposed that can be easily extended to a batch mode. This is a desirable property since it saves the user time when parallel computing is available. After fitting a GP to training data points, the expected squared LOO (ES-LOO) error is calculated at each design point. ES-LOO is used as a measure to identify important data points. More precisely, when this quantity is large at a point it means that the quality of prediction depends a great deal on that point and adding more samples nearby could improve the accuracy of the GP. As a result, it is reasonable to select the next sample where ES-LOO is maximised. However, ES-LOO is only known at the experimental design and needs to be estimated at unobserved points. To do this, a second GP is fitted to the ES-LOO errors and where the maximum of the modified expected improvement (EI) criterion occurs is chosen as the next sample. EI is a popular acquisition function in Bayesian optimisation and is used to trade-off between local/global search. However, it has a tendency towards exploitation, meaning that its maximum is close to the (current) "best" sample. To avoid clustering, a modified version of EI, called pseudo expected improvement, is employed which is more explorative than EI yet allows us to discover unexplored regions. Our results show that the proposed sampling method is promising.

</details>

<details>

<summary>2021-10-15 13:03:09 - GaussED: A Probabilistic Programming Language for Sequential Experimental Design</summary>

- *Matthew A. Fisher, Onur Teymur, Chris. J. Oates*

- `2110.08072v1` - [abs](http://arxiv.org/abs/2110.08072v1) - [pdf](http://arxiv.org/pdf/2110.08072v1)

> Sequential algorithms are popular for experimental design, enabling emulation, optimisation and inference to be efficiently performed. For most of these applications bespoke software has been developed, but the approach is general and many of the actual computations performed in such software are identical. Motivated by the diverse problems that can in principle be solved with common code, this paper presents GaussED, a simple probabilistic programming language coupled to a powerful experimental design engine, which together automate sequential experimental design for approximating a (possibly nonlinear) quantity of interest in Gaussian processes models. Using a handful of commands, GaussED can be used to: solve linear partial differential equations, perform tomographic reconstruction from integral data and implement Bayesian optimisation with gradient data.

</details>

<details>

<summary>2021-10-15 14:29:30 - Quickest Inference of Network Cascades with Noisy Information</summary>

- *Anirudh Sridhar, H. Vincent Poor*

- `2110.08115v1` - [abs](http://arxiv.org/abs/2110.08115v1) - [pdf](http://arxiv.org/pdf/2110.08115v1)

> We study the problem of estimating the source of a network cascade given a time series of noisy information about the spread. Initially, there is a single vertex affected by the cascade (the source) and the cascade spreads in discrete time steps across the network. The cascade evolution is hidden, but one can observe a time series of noisy signals from each vertex. The time series of a vertex is assumed to be a sequence of i.i.d. samples from a pre-change distribution $Q_0$ before the cascade affects the vertex, and the time series is a sequence of i.i.d. samples from a post-change distribution $Q_1$ once the cascade has affected the vertex. Given the time series of noisy signals, which can be viewed as a noisy measurement of the cascade evolution, we aim to devise a procedure to reliably estimate the cascade source as fast as possible.   We investigate Bayesian and minimax formulations of the source estimation problem, and derive near-optimal estimators for simple cascade dynamics and network topologies. In the Bayesian setting, an estimator which observes samples until the error of the Bayes-optimal estimator falls below a threshold achieves optimal performance. In the minimax setting, optimal performance is achieved by designing a novel multi-hypothesis sequential probability ratio test (MSPRT). We find that these optimal estimators require $\log \log n / \log (k - 1)$ observations of the noisy time series when the network topology is a $k$-regular tree, and $(\log n)^{\frac{1}{\ell + 1}}$ observations are required for $\ell$-dimensional lattices. Finally, we discuss how our methods may be extended to cascades on arbitrary graphs.

</details>

<details>

<summary>2021-10-15 17:24:03 - Choice functions based multi-objective Bayesian optimisation</summary>

- *Alessio Benavoli, Dario Azzimonti, Dario Piga*

- `2110.08217v1` - [abs](http://arxiv.org/abs/2110.08217v1) - [pdf](http://arxiv.org/pdf/2110.08217v1)

> In this work we introduce a new framework for multi-objective Bayesian optimisation where the multi-objective functions can only be accessed via choice judgements, such as ``I pick options A,B,C among this set of five options A,B,C,D,E''. The fact that the option D is rejected means that there is at least one option among the selected ones A,B,C that I strictly prefer over D (but I do not have to specify which one). We assume that there is a latent vector function f for some dimension $n_e$ which embeds the options into the real vector space of dimension n, so that the choice set can be represented through a Pareto set of non-dominated options. By placing a Gaussian process prior on f and deriving a novel likelihood model for choice data, we propose a Bayesian framework for choice functions learning. We then apply this surrogate model to solve a novel multi-objective Bayesian optimisation from choice data problem.

</details>

<details>

<summary>2021-10-15 17:34:15 - Frequentist-Bayes Hybrid Covariance Estimationfor Unfolding Problems</summary>

- *Pim Jordi Verschuuren*

- `2110.09382v1` - [abs](http://arxiv.org/abs/2110.09382v1) - [pdf](http://arxiv.org/pdf/2110.09382v1)

> In this paper we present a frequentist-Bayesian hybrid method for estimating covariances of unfolded distributions using pseudo-experiments. The method is compared with other covariance estimation methods using the unbiased Rao-Cramer bound (RCB) and frequentist pseudo-experiments. We show that the unbiased RCB method diverges from the other two methods when regularization is introduced. The new hybrid method agrees well with the frequentist pseudo-experiment method for various amounts of regularization. However, the hybrid method has the added advantage of not requiring a clear likelihood definition and can be used in combination with any unfolding algorithm that uses a response matrix to model the detector response.

</details>

<details>

<summary>2021-10-16 00:44:06 - A Nested Weighted Tchebycheff Multi-Objective Bayesian Optimization Approach for Flexibility of Unknown Utopia Estimation in Expensive Black-box Design Problems</summary>

- *Arpan Biswas, Claudio Fuentes, Christopher Hoyle*

- `2110.11070v1` - [abs](http://arxiv.org/abs/2110.11070v1) - [pdf](http://arxiv.org/pdf/2110.11070v1)

> We propose a nested weighted Tchebycheff Multi-objective Bayesian optimization framework where we build a regression model selection procedure from an ensemble of models, towards better estimation of the uncertain parameters of the weighted-Tchebycheff expensive black-box multi-objective function. In existing work, a weighted Tchebycheff MOBO approach has been demonstrated which attempts to estimate the unknown utopia in formulating acquisition function, through calibration using a priori selected regression model. However, the existing MOBO model lacks flexibility in selecting the appropriate regression models given the guided sampled data and therefore, can under-fit or over-fit as the iterations of the MOBO progress, reducing the overall MOBO performance. As it is too complex to a priori guarantee a best model in general, this motivates us to consider a portfolio of different families of predictive models fitted with current training data, guided by the WTB MOBO; the best model is selected following a user-defined prediction root mean-square-error-based approach. The proposed approach is implemented in optimizing a multi-modal benchmark problem and a thin tube design under constant loading of temperature-pressure, with minimizing the risk of creep-fatigue failure and design cost. Finally, the nested weighted Tchebycheff MOBO model performance is compared with different MOBO frameworks with respect to accuracy in parameter estimation, Pareto-optimal solutions and function evaluation cost. This method is generalized enough to consider different families of predictive models in the portfolio for best model selection, where the overall design architecture allows for solving any high-dimensional (multiple functions) complex black-box problems and can be extended to any other global criterion multi-objective optimization methods where prior knowledge of utopia is required.

</details>

<details>

<summary>2021-10-17 15:27:10 - A Bayesian Selection Model for Correcting Outcome Reporting Bias With Application to a Meta-analysis on Heart Failure Interventions</summary>

- *Ray Bai, Xiaokang Liu, Lifeng Lin, Yulun Liu, Stephen E. Kimmel, Haitao Chu, Yong Chen*

- `2110.08849v1` - [abs](http://arxiv.org/abs/2110.08849v1) - [pdf](http://arxiv.org/pdf/2110.08849v1)

> Multivariate meta-analysis (MMA) is a powerful tool for jointly estimating multiple outcomes' treatment effects. However, the validity of results from MMA is potentially compromised by outcome reporting bias (ORB), or the tendency for studies to selectively report outcomes. Until recently, ORB has been understudied. Since ORB can lead to biased conclusions, it is crucial to correct the estimates of effect sizes and quantify their uncertainty in the presence of ORB. With this goal, we develop a Bayesian selection model to adjust for ORB in MMA. We further propose a measure for quantifying the impact of ORB on the results from MMA. We evaluate our approaches through a meta-evaluation of 748 bivariate meta-analyses from the Cochrane Database of Systematic Reviews. Our model is motivated by and applied to a meta-analysis of interventions on hospital readmission and quality of life for heart failure patients. In our analysis, the relative risk (RR) of hospital readmission for the intervention group changes from a significant decrease (RR: 0.931, 95% confidence interval [CI]: 0.862-0.993) to a statistically nonsignificant effect (RR: 0.955, 95% CI: 0.876-1.051) after adjusting for ORB. This study demonstrates that failing to account for ORB can lead to different conclusions in a meta-analysis.

</details>

<details>

<summary>2021-10-17 18:17:01 - Persuasion by Dimension Reduction</summary>

- *Semyon Malamud, Andreas Schrimpf*

- `2110.08884v1` - [abs](http://arxiv.org/abs/2110.08884v1) - [pdf](http://arxiv.org/pdf/2110.08884v1)

> How should an agent (the sender) observing multi-dimensional data (the state vector) persuade another agent to take the desired action? We show that it is always optimal for the sender to perform a (non-linear) dimension reduction by projecting the state vector onto a lower-dimensional object that we call the "optimal information manifold." We characterize geometric properties of this manifold and link them to the sender's preferences. Optimal policy splits information into "good" and "bad" components. When the sender's marginal utility is linear, revealing the full magnitude of good information is always optimal. In contrast, with concave marginal utility, optimal information design conceals the extreme realizations of good information and only reveals its direction (sign). We illustrate these effects by explicitly solving several multi-dimensional Bayesian persuasion problems.

</details>

<details>

<summary>2021-10-17 20:28:19 - Fast selection of nonlinear mixed effect models using penalized likelihood</summary>

- *Edouard Ollier*

- `2103.01621v2` - [abs](http://arxiv.org/abs/2103.01621v2) - [pdf](http://arxiv.org/pdf/2103.01621v2)

> Nonlinear Mixed effects models are hidden variables models that are widely used in many fields such as pharmacometrics. In such models, the distribution characteristics of hidden variables can be specified by including several parameters such as covariates or correlations which must be selected. Recent development of pharmacogenomics has brought averaged/high dimensional problems to the field of nonlinear mixed effects modeling for which standard covariates selection techniques like stepwise methods are not well suited. The selection of covariates and correlation parameters using a penalized likelihood approach is proposed. The penalized likelihood problem is solved using a stochastic proximal gradient algorithm to avoid inner-outer iterations. Speed of convergence of the proximal gradient algorithm is improved using component-wise adaptive gradient step sizes. The practical implementation and tuning of the proximal gradient algorithm are explored using simulations. Calibration of regularization parameters is performed by minimizing the Bayesian Information Criterion using particle swarm optimization, a zero-order optimization procedure. The use of warm restart and parallelization allowed computing time to be reduced significantly . The performance of the proposed method compared to the traditional grid search strategy is explored using simulated data. Finally, an application to real data from two pharmacokinetics studies is provided, one studying an antifibrinolytic and the other studying an antibiotic.

</details>

<details>

<summary>2021-10-18 01:30:43 - Assessing Ecosystem State Space Models: Identifiability and Estimation</summary>

- *John W. Smith, Leah R. Johnson, Robert Q. Thomas*

- `2110.08967v1` - [abs](http://arxiv.org/abs/2110.08967v1) - [pdf](http://arxiv.org/pdf/2110.08967v1)

> Bayesian methods are increasingly being applied to parameterize mechanistic process models used in environmental prediction and forecasting. In particular, models describing ecosystem dynamics with multiple states that are linear and autoregressive at each step in time can be treated as statistical state space models. In this paper we examine this subset of ecosystem models, giving closed form Gibbs sampling updates for latent states and process precision parameters when process and observation errors are normally distributed. We use simulated data from an example model (DALECev) to assess the performance of parameter estimation and identifiability under scenarios of gaps in observations. We show that process precision estimates become unreliable as temporal gaps between observed state data increase. To improve estimates, particularly precisions, we introduce a method of tuning the timestep of the latent states to leverage higher-frequency driver information. Further, we show that data cloning is a suitable method for assessing parameter identifiability in this class of models. Overall, our study helps inform the application of state space models to ecological forecasting applications where 1) data are not available for all states and transfers at the operational timestep for the ecosystem model and 2) process uncertainty estimation is desired.

</details>

<details>

<summary>2021-10-18 04:36:07 - A Concavification Approach to Ambiguous Persuasion</summary>

- *Xiaoyu Cheng*

- `2106.11270v2` - [abs](http://arxiv.org/abs/2106.11270v2) - [pdf](http://arxiv.org/pdf/2106.11270v2)

> This note shows that the value of ambiguous persuasion characterized in Beauchene, Li and Li(2019) can be given by a concavification program as in Bayesian persuasion (Kamenica and Gentzkow, 2011). In addition, it implies that an ambiguous persuasion game can be equivalently formalized as a Bayesian persuasion game by distorting the utility functions. This result is obtained under a novel construction of ambiguous persuasion.

</details>

<details>

<summary>2021-10-18 06:25:38 - A Bayesian approach to multi-task learning with network lasso</summary>

- *Kaito Shimamura, Shuichi Kawano*

- `2110.09040v1` - [abs](http://arxiv.org/abs/2110.09040v1) - [pdf](http://arxiv.org/pdf/2110.09040v1)

> Network lasso is a method for solving a multi-task learning problem through the regularized maximum likelihood method. A characteristic of network lasso is setting a different model for each sample. The relationships among the models are represented by relational coefficients. A crucial issue in network lasso is to provide appropriate values for these relational coefficients. In this paper, we propose a Bayesian approach to solve multi-task learning problems by network lasso. This approach allows us to objectively determine the relational coefficients by Bayesian estimation. The effectiveness of the proposed method is shown in a simulation study and a real data analysis.

</details>

<details>

<summary>2021-10-18 07:09:38 - Task Agnostic Continual Learning Using Online Variational Bayes with Fixed-Point Updates</summary>

- *Chen Zeno, Itay Golan, Elad Hoffer, Daniel Soudry*

- `2010.00373v2` - [abs](http://arxiv.org/abs/2010.00373v2) - [pdf](http://arxiv.org/pdf/2010.00373v2)

> Background: Catastrophic forgetting is the notorious vulnerability of neural networks to the changes in the data distribution during learning. This phenomenon has long been considered a major obstacle for using learning agents in realistic continual learning settings. A large body of continual learning research assumes that task boundaries are known during training. However, only a few works consider scenarios in which task boundaries are unknown or not well defined -- task agnostic scenarios. The optimal Bayesian solution for this requires an intractable online Bayes update to the weights posterior. Contributions: We aim to approximate the online Bayes update as accurately as possible. To do so, we derive novel fixed-point equations for the online variational Bayes optimization problem, for multivariate Gaussian parametric distributions. By iterating the posterior through these fixed-point equations, we obtain an algorithm (FOO-VB) for continual learning which can handle non-stationary data distribution using a fixed architecture and without using external memory (i.e. without access to previous data). We demonstrate that our method (FOO-VB) outperforms existing methods in task agnostic scenarios. FOO-VB Pytorch implementation will be available online.

</details>

<details>

<summary>2021-10-18 13:59:44 - Exact marginal prior distributions of finite Bayesian neural networks</summary>

- *Jacob A. Zavatone-Veth, Cengiz Pehlevan*

- `2104.11734v3` - [abs](http://arxiv.org/abs/2104.11734v3) - [pdf](http://arxiv.org/pdf/2104.11734v3)

> Bayesian neural networks are theoretically well-understood only in the infinite-width limit, where Gaussian priors over network weights yield Gaussian priors over network outputs. Recent work has suggested that finite Bayesian networks may outperform their infinite counterparts, but their non-Gaussian function space priors have been characterized only though perturbative approaches. Here, we derive exact solutions for the function space priors for individual input examples of a class of finite fully-connected feedforward Bayesian neural networks. For deep linear networks, the prior has a simple expression in terms of the Meijer $G$-function. The prior of a finite ReLU network is a mixture of the priors of linear networks of smaller widths, corresponding to different numbers of active units in each layer. Our results unify previous descriptions of finite network priors in terms of their tail decay and large-width behavior.

</details>

<details>

<summary>2021-10-18 14:02:21 - A portfolio approach to massively parallel Bayesian optimization</summary>

- *Mickael Binois, Nicholson Collier, Jonathan Ozik*

- `2110.09334v1` - [abs](http://arxiv.org/abs/2110.09334v1) - [pdf](http://arxiv.org/pdf/2110.09334v1)

> One way to reduce the time of conducting optimization studies is to evaluate designs in parallel rather than just one-at-a-time. For expensive-to-evaluate black-boxes, batch versions of Bayesian optimization have been proposed. They work by building a surrogate model of the black-box that can be used to select the designs to evaluate efficiently via an infill criterion. Still, with higher levels of parallelization becoming available, the strategies that work for a few tens of parallel evaluations become limiting, in particular due to the complexity of selecting more evaluations. It is even more crucial when the black-box is noisy, necessitating more evaluations as well as repeating experiments. Here we propose a scalable strategy that can keep up with massive batching natively, focused on the exploration/exploitation trade-off and a portfolio allocation. We compare the approach with related methods on deterministic and noisy functions, for mono and multiobjective optimization tasks. These experiments show similar or better performance than existing methods, while being orders of magnitude faster.

</details>

<details>

<summary>2021-10-18 14:36:02 - Modeling tail risks of inflation using unobserved component quantile regressions</summary>

- *Michael Pfarrhofer*

- `2103.03632v2` - [abs](http://arxiv.org/abs/2103.03632v2) - [pdf](http://arxiv.org/pdf/2103.03632v2)

> This paper proposes methods for Bayesian inference in time-varying parameter (TVP) quantile regression (QR) models featuring conditional heteroskedasticity. I use data augmentation schemes to render the model conditionally Gaussian and develop an efficient Gibbs sampling algorithm. Regularization of the high-dimensional parameter space is achieved via flexible dynamic shrinkage priors. A simple version of TVP-QR based on an unobserved component model is applied to dynamically trace the quantiles of the distribution of inflation in the United States, the United Kingdom and the euro area. In an out-of-sample forecast exercise, I find the proposed model to be competitive and perform particularly well for higher-order and tail forecasts. A detailed analysis of the resulting predictive distributions reveals that they are sometimes skewed and occasionally feature heavy tails.

</details>

<details>

<summary>2021-10-18 14:43:50 - Prediction of liquid fuel properties using machine learning models with Gaussian processes and probabilistic conditional generative learning</summary>

- *Rodolfo S. M. Freitas, Ágatha P. F. Lima, Cheng Chen, Fernando A. Rochinha, Daniel Mira, Xi Jiang*

- `2110.09360v1` - [abs](http://arxiv.org/abs/2110.09360v1) - [pdf](http://arxiv.org/pdf/2110.09360v1)

> Accurate determination of fuel properties of complex mixtures over a wide range of pressure and temperature conditions is essential to utilizing alternative fuels. The present work aims to construct cheap-to-compute machine learning (ML) models to act as closure equations for predicting the physical properties of alternative fuels. Those models can be trained using the database from MD simulations and/or experimental measurements in a data-fusion-fidelity approach. Here, Gaussian Process (GP) and probabilistic generative models are adopted. GP is a popular non-parametric Bayesian approach to build surrogate models mainly due to its capacity to handle the aleatory and epistemic uncertainties. Generative models have shown the ability of deep neural networks employed with the same intent. In this work, ML analysis is focused on a particular property, the fuel density, but it can also be extended to other physicochemical properties. This study explores the versatility of the ML models to handle multi-fidelity data. The results show that ML models can predict accurately the fuel properties of a wide range of pressure and temperature conditions.

</details>

<details>

<summary>2021-10-18 14:44:34 - Efficient Exploration in Binary and Preferential Bayesian Optimization</summary>

- *Tristan Fauvel, Matthew Chalk*

- `2110.09361v1` - [abs](http://arxiv.org/abs/2110.09361v1) - [pdf](http://arxiv.org/pdf/2110.09361v1)

> Bayesian optimization (BO) is an effective approach to optimize expensive black-box functions, that seeks to trade-off between exploitation (selecting parameters where the maximum is likely) and exploration (selecting parameters where we are uncertain about the objective function). In many real-world situations, direct measurements of the objective function are not possible, and only binary measurements such as success/failure or pairwise comparisons are available. To perform efficient exploration in this setting, we show that it is important for BO algorithms to distinguish between different types of uncertainty: epistemic uncertainty, about the unknown objective function, and aleatoric uncertainty, which comes from noisy observations and cannot be reduced. In effect, only the former is important for efficient exploration. Based on this, we propose several new acquisition functions that outperform state-of-the-art heuristics in binary and preferential BO, while being fast to compute and easy to implement. We then generalize these acquisition rules to batch learning, where multiple queries are performed simultaneously.

</details>

<details>

<summary>2021-10-18 18:10:42 - Robustness against conflicting prior information in regression</summary>

- *Philippe Gagnon*

- `2110.09556v1` - [abs](http://arxiv.org/abs/2110.09556v1) - [pdf](http://arxiv.org/pdf/2110.09556v1)

> Including prior information about model parameters is a fundamental step of any Bayesian statistical analysis. It is viewed positively by some as it allows, among others, to quantitatively incorporate expert opinion about model parameters. It is viewed negatively by others because it sets the stage for subjectivity in statistical analysis. Certainly, it creates problems when the inference is skewed due to a conflict with the data collected. According to the theory of conflict resolution (O'Hagan and Pericchi, 2012), a solution to such problems is to diminish the impact of conflicting prior information, yielding inference consistent with the data. This is typically achieved by using heavy-tailed priors. We study both theoretically and numerically the efficacy of such a solution in regression where the prior information about the coefficients takes the form of a product of density functions with known location and scale parameters. We study functions with regularly-varying tails (Student distributions), log-regularly-varying tails (as introduced in Desgagn\'e (2015)), and propose functions with slower tail decays that allow to resolve any conflict that can happen under that regression framework, contrarily to the two previous types of functions. The code to reproduce all numerical experiments is available online.

</details>

<details>

<summary>2021-10-18 23:50:29 - A simple Bayesian state-space model for the collective risk model</summary>

- *Jae Youn Ahn, Himchan Jeong, Yang Lu*

- `2110.09657v1` - [abs](http://arxiv.org/abs/2110.09657v1) - [pdf](http://arxiv.org/pdf/2110.09657v1)

> The collective risk model (CRM) for frequency and severity is an important tool for retail insurance ratemaking, macro-level catastrophic risk forecasting, as well as operational risk in banking regulation. This model, which is initially designed for cross-sectional data, has recently been adapted to a longitudinal context to conduct both a priori and a posteriori ratemaking, through the introduction of random effects. However, so far, the random effect(s) is usually assumed static due to computational concerns, leading to predictive premium that omit the seniority of the claims. In this paper, we propose a new CRM model with bivariate dynamic random effect process. The model is based on Bayesian state-space models. It is associated with the simple predictive mean and closed form expression for the likelihood function, while also allowing for the dependence between the frequency and severity components. Real data application to auto insurance is proposed to show the performance of our method.

</details>

<details>

<summary>2021-10-19 08:00:40 - Efficient and Consistent Data-Driven Model Selection for Time Series</summary>

- *Jean-Marc Bardet, Kamila Kare, William Kengne*

- `2110.09785v1` - [abs](http://arxiv.org/abs/2110.09785v1) - [pdf](http://arxiv.org/pdf/2110.09785v1)

> This paper studies the model selection problem in a large class of causal time series models, which includes both the ARMA or AR($\infty$) processes, as well as the GARCH or ARCH($\infty$), APARCH, ARMA-GARCH and many others processes. We first study the asymptotic behavior of the ideal penalty that minimizes the risk induced by a quasi-likelihood estimation among a finite family of models containing the true model. Then, we provide general conditions on the penalty term for obtaining the consistency and efficiency properties. We notably prove that consistent model selection criteria outperform classical AIC criterion in terms of efficiency. Finally, we derive from a Bayesian approach the usual BIC criterion, and by keeping all the second order terms of the Laplace approximation, a data-driven criterion denoted KC'. Monte-Carlo experiments exhibit the obtained asymptotic results and show that KC' criterion does better than the AIC and BIC ones in terms of consistency and efficiency.

</details>

<details>

<summary>2021-10-19 11:50:20 - Bayes Factors can only Quantify Evidence w.r.t. Sets of Parameters, not w.r.t. (Prior) Distributions on the Parameter</summary>

- *Patrick Schwaferts, Thomas Augustin*

- `2110.09871v1` - [abs](http://arxiv.org/abs/2110.09871v1) - [pdf](http://arxiv.org/pdf/2110.09871v1)

> Bayes factors are characterized by both the powerful mathematical framework of Bayesian statistics and the useful interpretation as evidence quantification. Former requires a parameter distribution that changes by seeing the data, latter requires two fixed hypotheses w.r.t. which the evidence quantification refers to. Naturally, these fixed hypotheses must not change by seeing the data, only their credibility should! Yet, it is exactly such a change of the hypotheses themselves (not only their credibility) that occurs by seeing the data, if their content is represented by parameter distributions (a recent trend in the context of Bayes factors for about one decade), rendering a correct interpretation of the Bayes factor rather useless. Instead, this paper argues that the inferential foundation of Bayes factors can only be maintained, if hypotheses are sets of parameters, not parameter distributions. In addition, particular attention has been paid to providing an explicit terminology of the big picture of statistical inference in the context of Bayes factors as well as to the distinction between knowledge (formalized by the prior distribution and being allowed to change) and theoretical positions (formalized as hypotheses and required to stay fixed) of the phenomenon of interest.

</details>

<details>

<summary>2021-10-19 13:15:23 - Revisiting identification concepts in Bayesian analysis</summary>

- *Jean-Pierre Florens, Anna Simoni*

- `2110.09954v1` - [abs](http://arxiv.org/abs/2110.09954v1) - [pdf](http://arxiv.org/pdf/2110.09954v1)

> This paper studies the role played by identification in the Bayesian analysis of statistical and econometric models. First, for unidentified models we demonstrate that there are situations where the introduction of a non-degenerate prior distribution can make a parameter that is nonidentified in frequentist theory identified in Bayesian theory. In other situations, it is preferable to work with the unidentified model and construct a Markov Chain Monte Carlo (MCMC) algorithms for it instead of introducing identifying assumptions. Second, for partially identified models we demonstrate how to construct the prior and posterior distributions for the identified set parameter and how to conduct Bayesian analysis. Finally, for models that contain some parameters that are identified and others that are not we show that marginalizing out the identified parameter from the likelihood with respect to its conditional prior, given the nonidentified parameter, allows the data to be informative about the nonidentified and partially identified parameter. The paper provides examples and simulations that illustrate how to implement our techniques.

</details>

<details>

<summary>2021-10-19 13:54:07 - How to Guide Decisions with Bayes Factors</summary>

- *Patrick Schwaferts, Thomas Augustin*

- `2110.09981v1` - [abs](http://arxiv.org/abs/2110.09981v1) - [pdf](http://arxiv.org/pdf/2110.09981v1)

> Some scientific research questions ask to guide decisions and others do not. By their nature frequentist hypothesis-tests yield a dichotomous test decision as result, rendering them rather inappropriate for latter types of research questions. Bayes factors, however, are argued to be both able to refrain from making decisions and to be employed in guiding decisions. This paper elaborates on how to use a Bayes factor for guiding a decision. In this regard, its embedding within the framework of Bayesian decision theory is delineated, in which a (hypothesis-based) loss function needs to be specified. Typically, such a specification is difficult for an applied scientist as relevant information might be scarce, vague, partial, and ambiguous. To tackle this issue, a robust, interval-valued specification of this loss function shall be allowed, such that the essential but partial information can be included into the analysis as is. Further, the restriction of the prior distributions to be proper distributions (which is necessary to calculate Bayes factors) can be alleviated if a decision is of interest. Both the resulting framework of hypothesis-based Bayesian decision theory with robust loss function and how to derive optimal decisions from already existing Bayes factors are depicted by user-friendly and straightforward step-by-step guides.

</details>

<details>

<summary>2021-10-19 15:07:09 - Coalitional Bayesian Autoencoders -- Towards explainable unsupervised deep learning</summary>

- *Bang Xiang Yong, Alexandra Brintrup*

- `2110.10038v1` - [abs](http://arxiv.org/abs/2110.10038v1) - [pdf](http://arxiv.org/pdf/2110.10038v1)

> This paper aims to improve the explainability of Autoencoder's (AE) predictions by proposing two explanation methods based on the mean and epistemic uncertainty of log-likelihood estimate, which naturally arise from the probabilistic formulation of the AE called Bayesian Autoencoders (BAE). To quantitatively evaluate the performance of explanation methods, we test them in sensor network applications, and propose three metrics based on covariate shift of sensors : (1) G-mean of Spearman drift coefficients, (2) G-mean of sensitivity-specificity of explanation ranking and (3) sensor explanation quality index (SEQI) which combines the two aforementioned metrics. Surprisingly, we find that explanations of BAE's predictions suffer from high correlation resulting in misleading explanations. To alleviate this, a "Coalitional BAE" is proposed, which is inspired by agent-based system theory. Our comprehensive experiments on publicly available condition monitoring datasets demonstrate the improved quality of explanations using the Coalitional BAE.

</details>

<details>

<summary>2021-10-19 17:54:55 - Decision Making in Drug Development via Inference on Power</summary>

- *Geoffrey S Johnson*

- `2005.04721v15` - [abs](http://arxiv.org/abs/2005.04721v15) - [pdf](http://arxiv.org/pdf/2005.04721v15)

> A typical power calculation is performed by replacing unknown population-level quantities in the power function with what is observed in external studies. Many authors and practitioners view this as an assumed value of power and offer the Bayesian quantity probability of success or assurance as an alternative. The claim is by averaging over a prior or posterior distribution, probability of success transcends power by capturing the uncertainty around the unknown true treatment effect and any other population-level parameters. We use p-value functions to frame both the probability of success calculation and the typical power calculation as merely producing two different point estimates of power. We demonstrate that Go/No-Go decisions based on either point estimate of power do not adequately quantify and control the risk involved, and instead we argue for Go/No-Go decisions that utilize inference on power for better risk management and decision making.

</details>

<details>

<summary>2021-10-19 17:59:38 - Identification of Tissue Optical Properties During Thermal Laser-Tissue Interactions: An Ensemble Kalman Filter-Based Approach</summary>

- *Andrea Arnold, Loris Fichera*

- `2107.10340v2` - [abs](http://arxiv.org/abs/2107.10340v2) - [pdf](http://arxiv.org/pdf/2107.10340v2)

> In this paper, we propose a computational framework to estimate the physical properties that govern the thermal response of laser-irradiated tissue. We focus in particular on two quantities, the absorption and scattering coefficients, which describe the optical absorption of light in the tissue and whose knowledge is vital to correctly plan medical laser treatments. To perform the estimation, we utilize an implementation of the Ensemble Kalman Filter (EnKF), a type of Bayesian filtering algorithm for data assimilation. Unlike prior approaches, in this work we estimate the tissue optical properties based on observations of the tissue thermal response to laser irradiation. This method has the potential for straightforward implementation in a clinical setup, as it would only require a simple thermal sensor, e.g., a miniaturized infrared camera. Because the optical properties of tissue can undergo shifts during laser exposure, we employ a variant of EnKF capable of tracking time-varying parameters. Through simulated experimental studies, we demonstrate the ability of the proposed technique to identify the tissue optical properties and track their dynamic changes during laser exposure, while simultaneously tracking changes in the tissue temperature at locations beneath the surface. We further demonstrate the framework's capability in estimating additional unknown tissue properties (i.e., the volumetric heat capacity and thermal conductivity) along with the optical properties of interest.

</details>

<details>

<summary>2021-10-19 20:37:21 - Bayesian spatial voting model to characterize the legislative behavior of the Colombian Senate 2010-2014</summary>

- *Carolina Luque, Juan Sosa*

- `2110.10250v1` - [abs](http://arxiv.org/abs/2110.10250v1) - [pdf](http://arxiv.org/pdf/2110.10250v1)

> This paper applies Bayesian methodologies to characterize the legislative behavior of the Colombian Senate during the period 2010-2014. The analysis is carried out through the plenary roll call votes of this legislative chamber. In addition, parliamentary electoral behavior is operationalized by implementing the one-dimensional standard Bayesian ideal point estimator via the Markov chain Monte Carlo algorithms. The results contribute mainly to two points: political space dimensionality and the identification of pivot legislators. The pattern revealed by the estimated ideal points suggests a latent non-ideological trait (opposition - non-opposition) underlying the vote of deputies in the Senate. Thus, in addition to providing empirical evidence for a better understanding of legislative policy in Colombia during the period under analysis, this work also offers methodological and theoretical tools to guide the analysis of roll call vote data in contexts of unbalanced parliaments (as opposed to the U.S. parliament), taking the particular case of the Colombian Senate as a reference.

</details>

<details>

<summary>2021-10-19 22:29:33 - A Bayesian Approach for the Variance of Fine Stratification</summary>

- *Sepideh Mosaferi*

- `2110.10296v1` - [abs](http://arxiv.org/abs/2110.10296v1) - [pdf](http://arxiv.org/pdf/2110.10296v1)

> Fine stratification is a popular design as it permits the stratification to be carried out to the fullest possible extent. Some examples include the Current Population Survey and National Crime Victimization Survey both conducted by the U.S. Census Bureau, and the National Survey of Family Growth conducted by the University of Michigan's Institute for Social Research. Clearly, the fine stratification survey has proved useful in many applications as its point estimator is unbiased and efficient. A common practice to estimate the variance in this context is collapsing the adjacent strata to create pseudo-strata and then estimating the variance, but the attained estimator of variance is not design-unbiased, and the bias increases as the population means of the pseudo-strata become more variant. Additionally, the estimator may suffer from a large mean squared error (MSE). In this paper, we propose a hierarchical Bayesian estimator for the variance of collapsed strata and compare the results with a nonparametric Bayes variance estimator. Additionally, we make comparisons with a kernel-based variance estimator recently proposed by Breidt et al. (2016). We show our proposed estimator is superior compared to the alternatives given in the literature such that it has a smaller frequentist MSE and bias. We verify this throughout multiple simulation studies and data analysis from the 2007-8 National Health and Nutrition Examination Survey and the 1998 Survey of Mental Health Organizations.

</details>

<details>

<summary>2021-10-20 05:34:14 - Can We Leverage Predictive Uncertainty to Detect Dataset Shift and Adversarial Examples in Android Malware Detection?</summary>

- *Deqiang Li, Tian Qiu, Shuo Chen, Qianmu Li, Shouhuai Xu*

- `2109.09654v2` - [abs](http://arxiv.org/abs/2109.09654v2) - [pdf](http://arxiv.org/pdf/2109.09654v2)

> The deep learning approach to detecting malicious software (malware) is promising but has yet to tackle the problem of dataset shift, namely that the joint distribution of examples and their labels associated with the test set is different from that of the training set. This problem causes the degradation of deep learning models without users' notice. In order to alleviate the problem, one approach is to let a classifier not only predict the label on a given example but also present its uncertainty (or confidence) on the predicted label, whereby a defender can decide whether to use the predicted label or not. While intuitive and clearly important, the capabilities and limitations of this approach have not been well understood. In this paper, we conduct an empirical study to evaluate the quality of predictive uncertainties of malware detectors. Specifically, we re-design and build 24 Android malware detectors (by transforming four off-the-shelf detectors with six calibration methods) and quantify their uncertainties with nine metrics, including three metrics dealing with data imbalance. Our main findings are: (i) predictive uncertainty indeed helps achieve reliable malware detection in the presence of dataset shift, but cannot cope with adversarial evasion attacks; (ii) approximate Bayesian methods are promising to calibrate and generalize malware detectors to deal with dataset shift, but cannot cope with adversarial evasion attacks; (iii) adversarial evasion attacks can render calibration methods useless, and it is an open problem to quantify the uncertainty associated with the predicted labels of adversarial examples (i.e., it is not effective to use predictive uncertainty to detect adversarial examples).

</details>

<details>

<summary>2021-10-20 07:11:33 - BNPdensity: Bayesian nonparametric mixture modeling in R</summary>

- *Julyan Arbel, Guillaume Kon Kam King, Antonio Lijoi, Luis Enrique Nieto-Barajas, Igor Prünster*

- `2110.10019v2` - [abs](http://arxiv.org/abs/2110.10019v2) - [pdf](http://arxiv.org/pdf/2110.10019v2)

> Robust statistical data modelling under potential model mis-specification often requires leaving the parametric world for the nonparametric. In the latter, parameters are infinite dimensional objects such as functions, probability distributions or infinite vectors. In the Bayesian nonparametric approach, prior distributions are designed for these parameters, which provide a handle to manage the complexity of nonparametric models in practice. However, most modern Bayesian nonparametric models seem often out of reach to practitioners, as inference algorithms need careful design to deal with the infinite number of parameters. The aim of this work is to facilitate the journey by providing computational tools for Bayesian nonparametric inference. The article describes a set of functions available in the \R package BNPdensity in order to carry out density estimation with an infinite mixture model, including all types of censored data. The package provides access to a large class of such models based on normalized random measures, which represent a generalization of the popular Dirichlet process mixture. One striking advantage of this generalization is that it offers much more robust priors on the number of clusters than the Dirichlet. Another crucial advantage is the complete flexibility in specifying the prior for the scale and location parameters of the clusters, because conjugacy is not required. Inference is performed using a theoretically grounded approximate sampling methodology known as the Ferguson & Klass algorithm. The package also offers several goodness of fit diagnostics such as QQ-plots, including a cross-validation criterion, the conditional predictive ordinate. The proposed methodology is illustrated on a classical ecological risk assessment method called the Species Sensitivity Distribution (SSD) problem, showcasing the benefits of the Bayesian nonparametric framework.

</details>

<details>

<summary>2021-10-20 11:39:30 - Targeted Active Learning for Bayesian Decision-Making</summary>

- *Louis Filstroff, Iiris Sundin, Petrus Mikkola, Aleksei Tiulpin, Juuso Kylmäoja, Samuel Kaski*

- `2106.04193v2` - [abs](http://arxiv.org/abs/2106.04193v2) - [pdf](http://arxiv.org/pdf/2106.04193v2)

> Active learning is usually applied to acquire labels of informative data points in supervised learning, to maximize accuracy in a sample-efficient way. However, maximizing the accuracy is not the end goal when the results are used for decision-making, for example in personalized medicine or economics. We argue that when acquiring samples sequentially, separating learning and decision-making is sub-optimal, and we introduce an active learning strategy which takes the down-the-line decision problem into account. Specifically, we introduce a novel active learning criterion which maximizes the expected information gain on the posterior distribution of the optimal decision. We compare our targeted active learning strategy to existing alternatives on both simulated and real data, and show improved performance in decision-making accuracy.

</details>

<details>

<summary>2021-10-20 13:52:38 - A Gentle Introduction to Bayesian Hierarchical Linear Regression Models</summary>

- *Juan Sosa, Jeimy Aristizabal*

- `2110.10565v1` - [abs](http://arxiv.org/abs/2110.10565v1) - [pdf](http://arxiv.org/pdf/2110.10565v1)

> Considering the flexibility and applicability of Bayesian modeling, in this work we revise the main characteristics of two hierarchical models in a regression setting. We study the full probabilistic structure of the models along with the full conditional distribution for each model parameter. Under our hierarchical extensions, we allow the mean of the second stage of the model to have a linear dependency on a set of covariates. The Gibbs sampling algorithms used to obtain samples when fitting the models are fully described and derived. In addition, we consider a case study in which the plant size is characterized as a function of nitrogen soil concentration and a grouping factor (farm).

</details>

<details>

<summary>2021-10-20 20:27:50 - Pick-and-Mix Information Operators for Probabilistic ODE Solvers</summary>

- *Nathanael Bosch, Filip Tronarp, Philipp Hennig*

- `2110.10770v1` - [abs](http://arxiv.org/abs/2110.10770v1) - [pdf](http://arxiv.org/pdf/2110.10770v1)

> Probabilistic numerical solvers for ordinary differential equations compute posterior distributions over the solution of an initial value problem via Bayesian inference. In this paper, we leverage their probabilistic formulation to seamlessly include additional information as general likelihood terms. We show that second-order differential equations should be directly provided to the solver, instead of transforming the problem to first order. Additionally, by including higher-order information or physical conservation laws in the model, solutions become more accurate and more physically meaningful. Lastly, we demonstrate the utility of flexible information operators by solving differential-algebraic equations. In conclusion, the probabilistic formulation of numerical solvers offers a flexible way to incorporate various types of information, thus improving the resulting solutions.

</details>

<details>

<summary>2021-10-20 21:23:45 - Adversarial attacks against Bayesian forecasting dynamic models</summary>

- *Roi Naveiro*

- `2110.10783v1` - [abs](http://arxiv.org/abs/2110.10783v1) - [pdf](http://arxiv.org/pdf/2110.10783v1)

> The last decade has seen the rise of Adversarial Machine Learning (AML). This discipline studies how to manipulate data to fool inference engines, and how to protect those systems against such manipulation attacks. Extensive work on attacks against regression and classification systems is available, while little attention has been paid to attacks against time series forecasting systems. In this paper, we propose a decision analysis based attacking strategy that could be utilized against Bayesian forecasting dynamic models.

</details>

<details>

<summary>2021-10-20 22:02:32 - Fast and Accurate Estimation of Non-Nested Binomial Hierarchical Models Using Variational Inference</summary>

- *Max Goplerud*

- `2007.12300v4` - [abs](http://arxiv.org/abs/2007.12300v4) - [pdf](http://arxiv.org/pdf/2007.12300v4)

> Non-linear hierarchical models are commonly used in many disciplines. However, inference in the presence of non-nested effects and on large datasets is challenging and computationally burdensome. This paper provides two contributions to scalable and accurate inference. First, I derive a new mean-field variational algorithm for estimating binomial logistic hierarchical models with an arbitrary number of non-nested random effects. Second, I propose "marginally augmented variational Bayes" (MAVB) that further improves the initial approximation through a step of Bayesian post-processing. I prove that MAVB provides a guaranteed improvement in the approximation quality at low computational cost and induces dependencies that were assumed away by the initial factorization assumptions.   I apply these techniques to a study of voter behavior using a high-dimensional application of the popular approach of multilevel regression and post-stratification (MRP). Existing estimation took hours whereas the algorithms proposed run in minutes. The posterior means are well-recovered even under strong factorization assumptions. Applying MAVB further improves the approximation by partially correcting the under-estimated variance. The proposed methodology is implemented in an open source software package.

</details>

<details>

<summary>2021-10-21 01:22:24 - Combining Parametric and Nonparametric Models to Estimate Treatment Effects in Observational Studies</summary>

- *Daniel Daly-Grafstein, Paul Gustafson*

- `2110.11349v1` - [abs](http://arxiv.org/abs/2110.11349v1) - [pdf](http://arxiv.org/pdf/2110.11349v1)

> Performing causal inference in observational studies requires we assume confounding variables are correctly adjusted for. G-computation methods are often used in these scenarios, with several recent proposals using Bayesian versions of g-computation. In settings with few confounders, standard models can be employed, however as the number of confounders increase these models become less feasible as there are fewer observations available for each unique combination of confounding variables. In this paper we propose a new model for estimating treatment effects in observational studies that incorporates both parametric and nonparametric outcome models. By conceptually splitting the data, we can combine these models while maintaining a conjugate framework, allowing us to avoid the use of MCMC methods. Approximations using the central limit theorem and random sampling allows our method to be scaled to high dimensional confounders while maintaining computational efficiency. We illustrate the model using carefully constructed simulation studies, as well as compare the computational costs to other benchmark models.

</details>

<details>

<summary>2021-10-21 10:44:23 - Bayesian Meta-Learning Through Variational Gaussian Processes</summary>

- *Vivek Myers, Nikhil Sardana*

- `2110.11044v1` - [abs](http://arxiv.org/abs/2110.11044v1) - [pdf](http://arxiv.org/pdf/2110.11044v1)

> Recent advances in the field of meta-learning have tackled domains consisting of large numbers of small ("few-shot") supervised learning tasks. Meta-learning algorithms must be able to rapidly adapt to any individual few-shot task, fitting to a small support set within a task and using it to predict the labels of the task's query set. This problem setting can be extended to the Bayesian context, wherein rather than predicting a single label for each query data point, a model predicts a distribution of labels capturing its uncertainty. Successful methods in this domain include Bayesian ensembling of MAML-based models, Bayesian neural networks, and Gaussian processes with learned deep kernel and mean functions. While Gaussian processes have a robust Bayesian interpretation in the meta-learning context, they do not naturally model non-Gaussian predictive posteriors for expressing uncertainty. In this paper, we design a theoretically principled method, VMGP, extending Gaussian-process-based meta-learning to allow for high-quality, arbitrary non-Gaussian uncertainty predictions. On benchmark environments with complex non-smooth or discontinuous structure, we find our VMGP method performs significantly better than existing Bayesian meta-learning baselines.

</details>

<details>

<summary>2021-10-21 11:18:45 - Improving the assessment of the probability of success in late stage drug development</summary>

- *Lisa V Hampson, Björn Bornkamp, Björn Holzhauer, Joseph Kahn, Markus R Lange, Wen-Lin Luo, Giovanni Della Cioppa, Kelvin Stott, Steffen Ballerstedt*

- `2102.02752v2` - [abs](http://arxiv.org/abs/2102.02752v2) - [pdf](http://arxiv.org/pdf/2102.02752v2)

> There are several steps to confirming the safety and efficacy of a new medicine. A sequence of trials, each with its own objectives, is usually required. Quantitative risk metrics can be useful for informing decisions about whether a medicine should transition from one stage of development to the next. To obtain an estimate of the probability of regulatory approval, pharmaceutical companies may start with industry-wide success rates and then apply to these subjective adjustments to reflect program-specific information. However, this approach lacks transparency and fails to make full use of data from previous clinical trials. We describe a quantitative Bayesian approach for calculating the probability of success (PoS) at the end of phase II which incorporates internal clinical data from one or more phase IIb studies, industry-wide success rates, and expert opinion or external data if needed. Using an example, we illustrate how PoS can be calculated accounting for differences between the phase IIb data and future phase III trials, and discuss how the methods can be extended to accommodate accelerated drug development pathways.

</details>

<details>

<summary>2021-10-21 12:00:17 - A unified performance analysis of likelihood-informed subspace methods</summary>

- *Tiangang Cui, Xin T. Tong*

- `2101.02417v3` - [abs](http://arxiv.org/abs/2101.02417v3) - [pdf](http://arxiv.org/pdf/2101.02417v3)

> The likelihood-informed subspace (LIS) method offers a viable route to reducing the dimensionality of high-dimensional probability distributions arising in Bayesian inference. LIS identifies an intrinsic low-dimensional linear subspace where the target distribution differs the most from some tractable reference distribution. Such a subspace can be identified using the leading eigenvectors of a Gram matrix of the gradient of the log-likelihood function. Then, the original high-dimensional target distribution is approximated through various forms of marginalization of the likelihood function, in which the approximated likelihood only has support on the intrinsic low-dimensional subspace. This approximation enables the design of inference algorithms that can scale sub-linearly with the apparent dimensionality of the problem. Intuitively, the accuracy of the approximation, and hence the performance of the inference algorithms, are influenced by three factors -- the dimension truncation error in identifying the subspace, Monte Carlo error in estimating the Gram matrices, and Monte Carlo error in constructing marginalizations. %This work establishes a unified framework to analyze each of these three factors and their interplay. Under mild technical assumptions, we establish error bounds for a range of existing dimension reduction techniques based on the principle of LIS. Our error bounds also provide useful insights into the accuracy of these methods. In addition, we analyze the integration of LIS with sampling methods such as Markov Chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC). We also demonstrate the applicability of our analysis on a linear inverse problem with Gaussian prior, which shows that all the estimates can be dimension-independent if the prior covariance is a trace-class operator.

</details>

<details>

<summary>2021-10-21 21:42:50 - Imprecise Subset Simulation</summary>

- *Dimitris G. Giovanis, Michael Shields*

- `2110.11955v1` - [abs](http://arxiv.org/abs/2110.11955v1) - [pdf](http://arxiv.org/pdf/2110.11955v1)

> The objective of this work is to quantify the uncertainty in probability of failure estimates resulting from incomplete knowledge of the probability distributions for the input random variables. We propose a framework that couples the widely used Subset simulation (SuS) with Bayesian/information theoretic multi-model inference. The process starts with data used to infer probability distributions for the model inputs. Often such data sets are small. Multi-model inference is used to assess uncertainty associated with the model-form and parameters of these random variables in the form of model probabilities and the associated joint parameter probability densities. A sampling procedure is used to construct a set of equally probable candidate probability distributions and an optimal importance sampling distribution is determined analytically from this set. Subset simulation is then performed using this optimal sampling density and the resulting conditional probabilities are re-weighted using importance sampling. The result of this process are empirical probability distributions of failure probabilities that provide direct estimates of the uncertainty in failure probability estimates that result from inference on small data sets. The method is demonstrated to be both computationally efficient -- requiring only a single subset simulation and nominal cost of sample re-weighting -- and to provide reasonable estimates of the uncertainty in failure probabilities.

</details>

<details>

<summary>2021-10-22 10:06:35 - Detection of Two-Way Outliers in Multivariate Data and Application to Cheating Detection in Educational Tests</summary>

- *Yunxiao Chen, Yan Lu, Irini Moustaki*

- `1911.09408v4` - [abs](http://arxiv.org/abs/1911.09408v4) - [pdf](http://arxiv.org/pdf/1911.09408v4)

> The paper proposes a new latent variable model for the simultaneous (two-way) detection of outlying individuals and items for item-response-type data. The proposed model is a synergy between a factor model for binary responses and continuous response times that captures normal item response behaviour and a latent class model that captures the outlying individuals and items. A statistical decision framework is developed under the proposed model that provides compound decision rules for controlling local false discovery/nondiscovery rates of outlier detection. Statistical inference is carried out under a Bayesian framework, for which a Markov chain Monte Carlo algorithm is developed. The proposed method is applied to the detection of cheating in educational tests due to item leakage using a case study of a computer-based nonadaptive licensure assessment. The performance of the proposed method is evaluated by simulation studies.

</details>

<details>

<summary>2021-10-22 16:26:02 - Multimodel Bayesian Analysis of Load Duration Effects in Lumber Reliability</summary>

- *Yunfeng Yang, Martin Lysy, Samuel W. K. Wong*

- `2110.11896v1` - [abs](http://arxiv.org/abs/2110.11896v1) - [pdf](http://arxiv.org/pdf/2110.11896v1)

> This paper evaluates the reliability of lumber, accounting for the duration-of-load (DOL) effect under different load profiles based on a multimodel Bayesian approach. Three individual DOL models previously used for reliability assessment are considered: the US model, the Canadian model, and the Gamma process model. Procedures for stochastic generation of residential, snow, and wind loads are also described. We propose Bayesian model-averaging (BMA) as a method for combining the reliability estimates of individual models under a given load profile that coherently accounts for statistical uncertainty in the choice of model and parameter values. The method is applied to the analysis of a Hemlock experimental dataset, where the BMA results are illustrated via estimated reliability indices together with 95% interval bands.

</details>

<details>

<summary>2021-10-22 22:09:16 - Fast expectation-maximization algorithms for spatial generalized linear mixed models</summary>

- *Yawen Guan, Murali Haran*

- `1909.05440v4` - [abs](http://arxiv.org/abs/1909.05440v4) - [pdf](http://arxiv.org/pdf/1909.05440v4)

> Spatial generalized linear mixed models (SGLMMs) are popular and flexible models for non-Gaussian spatial data. They are useful for spatial interpolations as well as for fitting regression models that account for spatial dependence, and are commonly used in many disciplines such as epidemiology, atmospheric science, and sociology. Inference for SGLMMs is typically carried out under the Bayesian framework at least in part because computational issues make maximum likelihood estimation challenging, especially when high-dimensional spatial data are involved. Here we provide a computationally efficient projection-based maximum likelihood approach and two computationally efficient algorithms for routinely fitting SGLMMs. The two algorithms proposed are both variants of expectation maximization algorithm, using either Markov chain Monte Carlo or a Laplace approximation for the conditional expectation. Our methodology is general and applies to both discrete-domain (Gaussian Markov random field) as well as continuous-domain (Gaussian process) spatial models. We show, via simulation and real data applications, that our methods perform well both in terms of parameter estimation as well as prediction. Crucially, our methodology is computationally efficient and scales well with the size of the data and is applicable to problems where maximum likelihood estimation was previously infeasible.

</details>

<details>

<summary>2021-10-22 23:19:35 - Bayesian Shrinkage for Functional Network Models, with Applications to Longitudinal Item Response Data</summary>

- *Jaewoo Park, Yeseul Jeon, Minsuk Shin, Minjeong Jeon, Ick Hoon Jin*

- `2006.13698v2` - [abs](http://arxiv.org/abs/2006.13698v2) - [pdf](http://arxiv.org/pdf/2006.13698v2)

> Longitudinal item response data are common in social science, educational science, and psychology, among other disciplines. Studying the time-varying relationships between items is crucial for educational assessment or designing marketing strategies from survey questions. Although dynamic network models have been widely developed, we cannot apply them directly to item response data because there are multiple systems of nodes with various types of local interactions among items, resulting in multiplex network structures. We propose a new model to study these temporal interactions among items by embedding the functional parameters within the exponential random graph model framework. Inference on such models is difficult because the likelihood functions contain intractable normalizing constants. Furthermore, the number of functional parameters grows exponentially as the number of items increases. Variable selection for such models is not trivial because standard shrinkage approaches do not consider temporal trends in functional parameters. To overcome these challenges, we develop a novel Bayes approach by combining an auxiliary variable MCMC algorithm and a recently-developed functional shrinkage method. We apply our algorithm to survey and review data sets, illustrating that the proposed approach can avoid the evaluation of intractable normalizing constants as well as the detection of significant temporal interactions among items. Through a simulation study under different scenarios, we examine the performance of our algorithm. Our method is, to our knowledge, the first attempt to select functional variables for models with intractable normalizing constants.

</details>

<details>

<summary>2021-10-22 23:56:55 - Bayesian Meta-reinforcement Learning for Traffic Signal Control</summary>

- *Yayi Zou, Zhiwei Qin*

- `2010.00163v2` - [abs](http://arxiv.org/abs/2010.00163v2) - [pdf](http://arxiv.org/pdf/2010.00163v2)

> In recent years, there has been increasing amount of interest around meta reinforcement learning methods for traffic signal control, which have achieved better performance compared with traditional control methods. However, previous methods lack robustness in adaptation and stability in training process in complex situations, which largely limits its application in real-world traffic signal control. In this paper, we propose a novel value-based Bayesian meta-reinforcement learning framework BM-DQN to robustly speed up the learning process in new scenarios by utilizing well-trained prior knowledge learned from existing scenarios. This framework is based on our proposed fast-adaptation variation to Gradient-EM Bayesian Meta-learning and the fast-update advantage of DQN, which allows for fast adaptation to new scenarios with continual learning ability and robustness to uncertainty. The experiments on restricted 2D navigation and traffic signal control show that our proposed framework adapts more quickly and robustly in new scenarios than previous methods, and specifically, much better continual learning ability in heterogeneous scenarios.

</details>

<details>

<summary>2021-10-23 05:37:29 - Prior Intensified Information Criterion</summary>

- *Yoshiyuki Ninomiya*

- `2110.12145v1` - [abs](http://arxiv.org/abs/2110.12145v1) - [pdf](http://arxiv.org/pdf/2110.12145v1)

> The widely applicable information criterion (WAIC) has been used as a model selection criterion for Bayesian statistics in recent years. It is an asymptotically unbiased estimator of the Kullback-Leibler divergence between a Bayesian predictive distribution and the true distribution. Not only is the WAIC theoretically more sound than other information criteria, its usefulness in practice has also been reported. On the other hand, the WAIC is intended for settings in which the prior distribution does not have an asymptotic influence, and as we set the class of the prior distribution to be more complex, it never fails to select the most complex one. To alleviate these concerns, this paper proposed the prior intensified information criterion (PIIC). In addition, it customizes this criterion to incorporate sparse estimation and causal inference. Numerical experiments show that the PIIC clearly outperforms the WAIC in terms of prediction performance when the above concerns are manifested. A real data analysis confirms that the results of variable selection and Bayesian estimators of the WAIC and PIIC differ significantly.

</details>

<details>

<summary>2021-10-23 12:55:30 - Item Quality Control in Educational Testing: Change Point Model, Compound Risk, and Sequential Detection</summary>

- *Yunxiao Chen, Yi-Hsuan Lee, Xiaoou Li*

- `2008.10104v2` - [abs](http://arxiv.org/abs/2008.10104v2) - [pdf](http://arxiv.org/pdf/2008.10104v2)

> In standardized educational testing, test items are reused in multiple test administrations. To ensure the validity of test scores, the psychometric properties of items should remain unchanged over time. In this paper, we consider the sequential monitoring of test items, in particular, the detection of abrupt changes to their psychometric properties, where a change can be caused by, for example, leakage of the item or change of the corresponding curriculum. We propose a statistical framework for the detection of abrupt changes in individual items. This framework consists of (1) a multi-stream Bayesian change point model describing sequential changes in items, (2) a compound risk function quantifying the risk in sequential decisions, and (3) sequential decision rules that control the compound risk. Throughout the sequential decision process, the proposed decision rule balances the trade-off between two sources of errors, the false detection of pre-change items and the non-detection of post-change items. An item-specific monitoring statistic is proposed based on an item response theory model that eliminates the confounding from the examinee population which changes over time. Sequential decision rules and their theoretical properties are developed under two settings: the oracle setting where the Bayesian change point model is completely known and a more realistic setting where some parameters of the model are unknown. Simulation studies are conducted under settings that mimic real operational tests.

</details>

<details>

<summary>2021-10-23 13:21:38 - Conditional Deep Gaussian Processes: empirical Bayes hyperdata learning</summary>

- *Chi-Ken Lu, Patrick Shafto*

- `2110.00568v2` - [abs](http://arxiv.org/abs/2110.00568v2) - [pdf](http://arxiv.org/pdf/2110.00568v2)

> It is desirable to combine the expressive power of deep learning with Gaussian Process (GP) in one expressive Bayesian learning model. Deep kernel learning showed success in adopting a deep network for feature extraction followed by a GP used as function model. Recently,it was suggested that, albeit training with marginal likelihood, the deterministic nature of feature extractor might lead to overfitting while the replacement with a Bayesian network seemed to cure it. Here, we propose the conditional Deep Gaussian Process (DGP) in which the intermediate GPs in hierarchical composition are supported by the hyperdata and the exposed GP remains zero mean. Motivated by the inducing points in sparse GP, the hyperdata also play the role of function supports, but are hyperparameters rather than random variables. We follow our previous moment matching approach to approximate the marginal prior for conditional DGP with a GP carrying an effective kernel. Thus, as in empirical Bayes, the hyperdata are learned by optimizing the approximate marginal likelihood which implicitly depends on the hyperdata via the kernel. We shall show the equivalence with the deep kernel learning in the limit of dense hyperdata in latent space. However, the conditional DGP and the corresponding approximate inference enjoy the benefit of being more Bayesian than deep kernel learning. Preliminary extrapolation results demonstrate expressive power from the depth of hierarchy by exploiting the exact covariance and hyperdata learning, in comparison with GP kernel composition, DGP variational inference and deep kernel learning. We also address the non-Gaussian aspect of our model as well as way of upgrading to a full Bayes inference.

</details>

<details>

<summary>2021-10-23 13:32:05 - On the Behavioral Consequences of Reverse Causality</summary>

- *Ran Spiegler*

- `2110.12218v1` - [abs](http://arxiv.org/abs/2110.12218v1) - [pdf](http://arxiv.org/pdf/2110.12218v1)

> Reverse causality is a common causal misperception that distorts the evaluation of private actions and public policies. This paper explores the implications of this error when a decision maker acts on it and therefore affects the very statistical regularities from which he draws faulty inferences. Using a quadratic-normal parameterization and applying the Bayesian-network approach of Spiegler (2016), I demonstrate the subtle equilibrium effects of a certain class of reverse-causality errors, with illustrations in diverse areas: development psychology, social policy, monetary economics and IO. In particular, the decision context may protect the decision maker from his own reverse-causality causal error. That is, the cost of reverse-causality errors can be lower for everyday decision makers than for an outside observer who evaluates their choices.

</details>

<details>

<summary>2021-10-23 19:42:11 - Generalized Resubstitution for Classification Error Estimation</summary>

- *Parisa Ghane, Ulisses Braga-Neto*

- `2110.12285v1` - [abs](http://arxiv.org/abs/2110.12285v1) - [pdf](http://arxiv.org/pdf/2110.12285v1)

> We propose the family of generalized resubstitution classifier error estimators based on empirical measures. These error estimators are computationally efficient and do not require re-training of classifiers. The plain resubstitution error estimator corresponds to choosing the standard empirical measure. Other choices of empirical measure lead to bolstered, posterior-probability, Gaussian-process, and Bayesian error estimators; in addition, we propose bolstered posterior-probability error estimators as a new family of generalized resubstitution estimators. In the two-class case, we show that a generalized resubstitution estimator is consistent and asymptotically unbiased, regardless of the distribution of the features and label, if the corresponding generalized empirical measure converges uniformly to the standard empirical measure and the classification rule has a finite VC dimension. A generalized resubstitution estimator typically has hyperparameters that can be tuned to control its bias and variance, which adds flexibility. Numerical experiments with various classification rules trained on synthetic data assess the thefinite-sample performance of several representative generalized resubstitution error estimators. In addition, results of an image classification experiment using the LeNet-5 convolutional neural network and the MNIST data set demonstrate the potential of this class of error estimators in deep learning for computer vision.

</details>

<details>

<summary>2021-10-23 20:22:53 - Relative Maximum Likelihood Updating of Ambiguous Beliefs</summary>

- *Xiaoyu Cheng*

- `1911.02678v6` - [abs](http://arxiv.org/abs/1911.02678v6) - [pdf](http://arxiv.org/pdf/1911.02678v6)

> This paper proposes and axiomatizes a new updating rule: Relative Maximum Likelihood (RML) for ambiguous beliefs represented by a set of priors (C). This rule takes the form of applying Bayes' rule to a subset of C. This subset is a linear contraction of C towards its subset ascribing a maximal probability to the observed event. The degree of contraction captures the extent of willingness to discard priors based on likelihood when updating. Two well-known updating rules of multiple priors, Full Bayesian (FB) and Maximum Likelihood (ML), are included as special cases of RML. An axiomatic characterization of conditional preferences generated by RML updating is provided when the preferences admit Maxmin Expected Utility representations. The axiomatization relies on weakening the axioms characterizing FB and ML. The axiom characterizing ML is identified for the first time in this paper, addressing a long-standing open question in the literature.

</details>

<details>

<summary>2021-10-23 22:50:44 - Learner-Private Convex Optimization</summary>

- *Jiaming Xu, Kuang Xu, Dana Yang*

- `2102.11976v2` - [abs](http://arxiv.org/abs/2102.11976v2) - [pdf](http://arxiv.org/pdf/2102.11976v2)

> Convex optimization with feedback is a framework where a learner relies on iterative queries and feedback to arrive at the minimizer of a convex function. It has gained considerable popularity thanks to its scalability in large-scale optimization and machine learning. The repeated interactions, however, expose the learner to privacy risks from eavesdropping adversaries that observe the submitted queries. In this paper, we study how to optimally obfuscate the learner's queries in convex optimization with first-order feedback, so that their learned optimal value is provably difficult to estimate for an eavesdropping adversary. We consider two formulations of learner privacy: a Bayesian formulation in which the convex function is drawn randomly, and a minimax formulation in which the function is fixed and the adversary's probability of error is measured with respect to a minimax criterion.   Suppose that the learner wishes to ensure the adversary cannot estimate accurately with probability greater than $1/L$ for some $L>0$. Our main results show that the query complexity overhead is additive in $L$ in the minimax formulation, but multiplicative in $L$ in the Bayesian formulation. Compared to existing learner-private sequential learning models with binary feedback, our results apply to the significantly richer family of general convex functions with full-gradient feedback. Our proofs learn on tools from the theory of Dirichlet processes, as well as a novel strategy designed for measuring information leakage under a full-gradient oracle.

</details>

<details>

<summary>2021-10-24 05:55:02 - Bayesian data assimilation for estimating epidemic evolution: a COVID-19 study</summary>

- *Xian Yang, Shuo Wang, Yuting Xing, Ling Li, Richard Yi Da Xu, Karl J. Friston, Yike Guo*

- `2101.01532v2` - [abs](http://arxiv.org/abs/2101.01532v2) - [pdf](http://arxiv.org/pdf/2101.01532v2)

> The evolution of epidemiological parameters, such as instantaneous reproduction number Rt, is important for understanding the transmission dynamics of infectious diseases. Current estimates of time-varying epidemiological parameters often face problems such as lagging observations, averaging inference, and improper quantification of uncertainties. To address these problems, we propose a Bayesian data assimilation framework for time-varying parameter estimation. Specifically, this framework is applied to Rt estimation, resulting in the state-of-the-art DARt system. With DARt, time misalignment caused by lagging observations is tackled by incorporating observation delays into the joint inference of infections and Rt; the drawback of averaging is overcome by instantaneously updating upon new observations and developing a model selection mechanism that captures abrupt changes; the uncertainty is quantified and reduced by employing Bayesian smoothing. We validate the performance of DARt and demonstrate its power in revealing the transmission dynamics of COVID-19. The proposed approach provides a promising solution for accurate and timely estimating transmission dynamics from reported data.

</details>

<details>

<summary>2021-10-24 15:01:59 - Epidemia: An R Package for Semi-Mechanistic Bayesian Modelling of Infectious Diseases using Point Processes</summary>

- *James A. Scott, Axel Gandy, Swapnil Mishra, Samir Bhatt, Seth Flaxman, H. Juliette T. Unwin, Jonathan Ish-Horowicz*

- `2110.12461v1` - [abs](http://arxiv.org/abs/2110.12461v1) - [pdf](http://arxiv.org/pdf/2110.12461v1)

> This article introduces epidemia, an R package for Bayesian, regression-oriented modeling of infectious diseases. The implemented models define a likelihood for all observed data while also explicitly modeling transmission dynamics: an approach often termed as semi-mechanistic. Infections are propagated over time using renewal equations. This approach is inspired by self-exciting, continuous-time point processes such as the Hawkes process. A variety of inferential tasks can be performed using the package. Key epidemiological quantities, including reproduction numbers and latent infections, may be estimated within the framework. The models may be used to evaluate the determinants of changes in transmission rates, including the effects of control measures. Epidemic dynamics may be simulated either from a fitted model or a prior model; allowing for prior/posterior predictive checks, experimentation, and forecasting.

</details>

<details>

<summary>2021-10-24 19:30:55 - Erlang mixture modeling for Poisson process intensities</summary>

- *Hyotae Kim, Athanasios Kottas*

- `2110.12513v1` - [abs](http://arxiv.org/abs/2110.12513v1) - [pdf](http://arxiv.org/pdf/2110.12513v1)

> We develop a prior probability model for temporal Poisson process intensities through structured mixtures of Erlang densities with common scale parameter, mixing on the integer shape parameters. The mixture weights are constructed through increments of a cumulative intensity function which is modeled nonparametrically with a gamma process prior. Such model specification provides a novel extension of Erlang mixtures for density estimation to the intensity estimation setting. The prior model structure supports general shapes for the point process intensity function, and it also enables effective handling of the Poisson process likelihood normalizing term resulting in efficient posterior simulation. The Erlang mixture modeling approach is further elaborated to develop an inference method for spatial Poisson processes. The methodology is examined relative to existing Bayesian nonparametric modeling approaches, including empirical comparison with Gaussian process prior based models, and is illustrated with synthetic and real data examples.

</details>

<details>

<summary>2021-10-24 19:33:07 - Imputation of Missing Data Using Linear Gaussian Cluster-Weighted Modeling</summary>

- *Luis Alejandro Masmela-Caita, Thais Paiva Galletti, Marcos Oliveira Prates*

- `2110.12514v1` - [abs](http://arxiv.org/abs/2110.12514v1) - [pdf](http://arxiv.org/pdf/2110.12514v1)

> Missing data theory deals with the statistical methods in the occurrence of missing data. Missing data occurs when some values are not stored or observed for variables of interest. However, most of the statistical theory assumes that data is fully observed. An alternative to deal with incomplete databases is to fill in the spaces corresponding to the missing information based on some criteria, this technique is called imputation. We introduce a new imputation methodology for databases with univariate missing patterns based on additional information from fully-observed auxiliary variables. We assume that the non-observed variable is continuous, and that auxiliary variables assist to improve the imputation capacity of the model. In a fully Bayesian framework, our method uses a flexible mixture of multivariate normal distributions to model the response and the auxiliary variables jointly. Under this framework, we use the properties of Gaussian Cluster-Weighted modeling to construct a predictive model to impute the missing values using the information from the covariates. Simulations studies and a real data illustration are presented to show the method imputation capacity under a variety of scenarios and in comparison to other literature methods.

</details>

<details>

<summary>2021-10-25 11:43:40 - Bayesian Analysis of Stochastic Volatility Model using Finite Gaussian Mixtures with Unknown Number of Components</summary>

- *Soham Mukherjee*

- `2110.12824v1` - [abs](http://arxiv.org/abs/2110.12824v1) - [pdf](http://arxiv.org/pdf/2110.12824v1)

> Financial studies require volatility based models which provides useful insights on risks related to investments. Stochastic volatility models are one of the most popular approaches to model volatility in such studies. The asset returns under study may come in multiple clusters which are not captured well assuming standard distributions. Mixture distributions are more appropriate in such situations. In this work, an algorithm is demonstrated which is capable of studying finite mixtures but with unknown number of components. This algorithm uses a Birth-Death process to adjust the number of components in the mixture distribution and the weights are assigned accordingly. This mixture distribution specification is then used for asset returns and a semi-parametric stochastic volatility model is fitted in a Bayesian framework. A specific case of Gaussian mixtures is studied. Using appropriate prior specification, Gibbs sampling method is used to generate posterior chains and assess model convergence. A case study of stock return data for State Bank of India is used to illustrate the methodology.

</details>

<details>

<summary>2021-10-25 15:30:25 - Variational Bayesian Reinforcement Learning with Regret Bounds</summary>

- *Brendan O'Donoghue*

- `1807.09647v3` - [abs](http://arxiv.org/abs/1807.09647v3) - [pdf](http://arxiv.org/pdf/1807.09647v3)

> In reinforcement learning the Q-values summarize the expected future rewards that the agent will attain. However, they cannot capture the epistemic uncertainty about those rewards. In this work we derive a new Bellman operator with associated fixed point we call the `knowledge values'. These K-values compress both the expected future rewards and the epistemic uncertainty into a single value, so that high uncertainty, high reward, or both, can yield high K-values. The key principle is to endow the agent with a risk-seeking utility function that is carefully tuned to balance exploration and exploitation. When the agent follows a Boltzmann policy over the K-values it yields a Bayes regret bound of $\tilde O(L^{3/2} \sqrt{S A T})$, where $L$ is the time horizon, $S$ is the number of states, $A$ is the number of actions, and $T$ is the total number of elapsed timesteps. We show deep connections of this approach to the soft-max and maximum-entropy strands of research in reinforcement learning.

</details>

<details>

<summary>2021-10-25 15:56:02 - Compositional Modeling of Nonlinear Dynamical Systems with ODE-based Random Features</summary>

- *Thomas M. McDonald, Mauricio A. Álvarez*

- `2106.05960v2` - [abs](http://arxiv.org/abs/2106.05960v2) - [pdf](http://arxiv.org/pdf/2106.05960v2)

> Effectively modeling phenomena present in highly nonlinear dynamical systems whilst also accurately quantifying uncertainty is a challenging task, which often requires problem-specific techniques. We present a novel, domain-agnostic approach to tackling this problem, using compositions of physics-informed random features, derived from ordinary differential equations. The architecture of our model leverages recent advances in approximate inference for deep Gaussian processes, such as layer-wise weight-space approximations which allow us to incorporate random Fourier features, and stochastic variational inference for approximate Bayesian inference. We provide evidence that our model is capable of capturing highly nonlinear behaviour in real-world multivariate time series data. In addition, we find that our approach achieves comparable performance to a number of other probabilistic models on benchmark regression tasks.

</details>

<details>

<summary>2021-10-25 18:06:12 - Deconditional Downscaling with Gaussian Processes</summary>

- *Siu Lun Chau, Shahine Bouabid, Dino Sejdinovic*

- `2105.12909v3` - [abs](http://arxiv.org/abs/2105.12909v3) - [pdf](http://arxiv.org/pdf/2105.12909v3)

> Refining low-resolution (LR) spatial fields with high-resolution (HR) information, often known as statistical downscaling, is challenging as the diversity of spatial datasets often prevents direct matching of observations. Yet, when LR samples are modeled as aggregate conditional means of HR samples with respect to a mediating variable that is globally observed, the recovery of the underlying fine-grained field can be framed as taking an "inverse" of the conditional expectation, namely a deconditioning problem. In this work, we propose a Bayesian formulation of deconditioning which naturally recovers the initial reproducing kernel Hilbert space formulation from Hsu and Ramos (2019). We extend deconditioning to a downscaling setup and devise efficient conditional mean embedding estimator for multiresolution data. By treating conditional expectations as inter-domain features of the underlying field, a posterior for the latent field can be established as a solution to the deconditioning problem. Furthermore, we show that this solution can be viewed as a two-staged vector-valued kernel ridge regressor and show that it has a minimax optimal convergence rate under mild assumptions. Lastly, we demonstrate its proficiency in a synthetic and a real-world atmospheric field downscaling problem, showing substantial improvements over existing methods.

</details>

<details>

<summary>2021-10-25 19:53:17 - Bridging the Gap Between Practice and PAC-Bayes Theory in Few-Shot Meta-Learning</summary>

- *Nan Ding, Xi Chen, Tomer Levinboim, Sebastian Goodman, Radu Soricut*

- `2105.14099v2` - [abs](http://arxiv.org/abs/2105.14099v2) - [pdf](http://arxiv.org/pdf/2105.14099v2)

> Despite recent advances in its theoretical understanding, there still remains a significant gap in the ability of existing PAC-Bayesian theories on meta-learning to explain performance improvements in the few-shot learning setting, where the number of training examples in the target tasks is severely limited. This gap originates from an assumption in the existing theories which supposes that the number of training examples in the observed tasks and the number of training examples in the target tasks follow the same distribution, an assumption that rarely holds in practice. By relaxing this assumption, we develop two PAC-Bayesian bounds tailored for the few-shot learning setting and show that two existing meta-learning algorithms (MAML and Reptile) can be derived from our bounds, thereby bridging the gap between practice and PAC-Bayesian theories. Furthermore, we derive a new computationally-efficient PACMAML algorithm, and show it outperforms existing meta-learning algorithms on several few-shot benchmark datasets.

</details>

<details>

<summary>2021-10-25 22:04:38 - Batch Multi-Fidelity Bayesian Optimization with Deep Auto-Regressive Networks</summary>

- *Shibo Li, Robert M. Kirby, Shandian Zhe*

- `2106.09884v2` - [abs](http://arxiv.org/abs/2106.09884v2) - [pdf](http://arxiv.org/pdf/2106.09884v2)

> Bayesian optimization (BO) is a powerful approach for optimizing black-box, expensive-to-evaluate functions. To enable a flexible trade-off between the cost and accuracy, many applications allow the function to be evaluated at different fidelities. In order to reduce the optimization cost while maximizing the benefit-cost ratio, in this paper, we propose Batch Multi-fidelity Bayesian Optimization with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of Bayesian neural networks to construct a fully auto-regressive model, which is expressive enough to capture strong yet complex relationships across all the fidelities, so as to improve the surrogate learning and optimization performance. Furthermore, to enhance the quality and diversity of queries, we develop a simple yet efficient batch querying method, without any combinatorial search over the fidelities. We propose a batch acquisition function based on Max-value Entropy Search (MES) principle, which penalizes highly correlated queries and encourages diversity. We use posterior samples and moment matching to fulfill efficient computation of the acquisition function and conduct alternating optimization over every fidelity-input pair, which guarantees an improvement at each step. We demonstrate the advantage of our approach on four real-world hyperparameter optimization applications.

</details>

<details>

<summary>2021-10-25 22:17:38 - Scalable Bayesian divergence time estimation with ratio transformations</summary>

- *Xiang Ji, Alexander A. Fisher, Shuo Su, Jeffrey L. Thorne, Barney Potter, Philippe Lemey, Guy Baele, Marc A. Suchard*

- `2110.13298v1` - [abs](http://arxiv.org/abs/2110.13298v1) - [pdf](http://arxiv.org/pdf/2110.13298v1)

> Divergence time estimation is crucial to provide temporal signals for dating biologically important events, from species divergence to viral transmissions in space and time. With the advent of high-throughput sequencing, recent Bayesian phylogenetic studies have analyzed hundreds to thousands of sequences. Such large-scale analyses challenge divergence time reconstruction by requiring inference on highly-correlated internal node heights that often become computationally infeasible. To overcome this limitation, we explore a ratio transformation that maps the original N - 1 internal node heights into a space of one height parameter and N - 2 ratio parameters. To make analyses scalable, we develop a collection of linear-time algorithms to compute the gradient and Jacobian-associated terms of the log-likelihood with respect to these ratios. We then apply Hamiltonian Monte Carlo sampling with the ratio transform in a Bayesian framework to learn the divergence times in four pathogenic virus phylogenies: West Nile virus, rabies virus, Lassa virus and Ebola virus. Our method both resolves a mixing issue in the West Nile virus example and improves inference efficiency by at least 5-fold for the Lassa and rabies virus examples. Our method also makes it now computationally feasible to incorporate mixed-effects molecular clock models for the Ebola virus example, confirms the findings from the original study and reveals clearer multimodal distributions of the divergence times of some clades of interest.

</details>

<details>

<summary>2021-10-25 23:20:42 - Evaluating Sensitivity to the Stick-Breaking Prior in Bayesian Nonparametrics</summary>

- *Ryan Giordano, Runjing Liu, Michael I. Jordan, Tamara Broderick*

- `2107.03584v3` - [abs](http://arxiv.org/abs/2107.03584v3) - [pdf](http://arxiv.org/pdf/2107.03584v3)

> Bayesian models based on the Dirichlet process and other stick-breaking priors have been proposed as core ingredients for clustering, topic modeling, and other unsupervised learning tasks. Prior specification is, however, relatively difficult for such models, given that their flexibility implies that the consequences of prior choices are often relatively opaque. Moreover, these choices can have a substantial effect on posterior inferences. Thus, considerations of robustness need to go hand in hand with nonparametric modeling. In the current paper, we tackle this challenge by exploiting the fact that variational Bayesian methods, in addition to having computational advantages in fitting complex nonparametric models, also yield sensitivities with respect to parametric and nonparametric aspects of Bayesian models. In particular, we demonstrate how to assess the sensitivity of conclusions to the choice of concentration parameter and stick-breaking distribution for inferences under Dirichlet process mixtures and related mixture models. We provide both theoretical and empirical support for our variational approach to Bayesian sensitivity analysis.

</details>

<details>

<summary>2021-10-26 06:00:20 - Optimal Bayesian Estimation of a Regression Curve, a Conditional Density and a Conditional Distribution</summary>

- *A. G. Nogales*

- `2110.13427v1` - [abs](http://arxiv.org/abs/2110.13427v1) - [pdf](http://arxiv.org/pdf/2110.13427v1)

> In this paper several related estimation problems are addressed from a Bayesian point of view and optimal estimators are obtained for each of them when some natural loss functions are considered. Namely, we are interested in estimating a regression curve. Simultaneously, the estimation problems of a conditional distribution function, or a conditional density, or even the conditional distribution itself, are considered. All these problems are posed in a sufficiently general framework to cover continuous and discrete, univariate and multivariate, parametric and non-parametric cases, without the need to use a specific prior distribution. The loss functions considered come naturally from the quadratic error loss function comonly used in estimating a real function of the unknown parameter. The cornerstone of the mentioned Bayes estimators is the posterior predictive distribution. Some examples are provided to illustrate these results.

</details>

<details>

<summary>2021-10-26 08:03:57 - Personalized Federated Learning with Gaussian Processes</summary>

- *Idan Achituve, Aviv Shamsian, Aviv Navon, Gal Chechik, Ethan Fetaya*

- `2106.15482v2` - [abs](http://arxiv.org/abs/2106.15482v2) - [pdf](http://arxiv.org/pdf/2106.15482v2)

> Federated learning aims to learn a global model that performs well on client devices with limited cross-client communication. Personalized federated learning (PFL) further extends this setup to handle data heterogeneity between clients by learning personalized models. A key challenge in this setting is to learn effectively across clients even though each client has unique data that is often limited in size. Here we present pFedGP, a solution to PFL that is based on Gaussian processes (GPs) with deep kernel learning. GPs are highly expressive models that work well in the low data regime due to their Bayesian nature. However, applying GPs to PFL raises multiple challenges. Mainly, GPs performance depends heavily on access to a good kernel function, and learning a kernel requires a large training set. Therefore, we propose learning a shared kernel function across all clients, parameterized by a neural network, with a personal GP classifier for each client. We further extend pFedGP to include inducing points using two novel methods, the first helps to improve generalization in the low data regime and the second reduces the computational cost. We derive a PAC-Bayes generalization bound on novel clients and empirically show that it gives non-vacuous guarantees. Extensive experiments on standard PFL benchmarks with CIFAR-10, CIFAR-100, and CINIC-10, and on a new setup of learning under input noise show that pFedGP achieves well-calibrated predictions while significantly outperforming baseline methods, reaching up to 21% in accuracy gain.

</details>

<details>

<summary>2021-10-26 08:19:44 - Truncated Marginal Neural Ratio Estimation</summary>

- *Benjamin Kurt Miller, Alex Cole, Patrick Forré, Gilles Louppe, Christoph Weniger*

- `2107.01214v2` - [abs](http://arxiv.org/abs/2107.01214v2) - [pdf](http://arxiv.org/pdf/2107.01214v2)

> Parametric stochastic simulators are ubiquitous in science, often featuring high-dimensional input parameters and/or an intractable likelihood. Performing Bayesian parameter inference in this context can be challenging. We present a neural simulation-based inference algorithm which simultaneously offers simulation efficiency and fast empirical posterior testability, which is unique among modern algorithms. Our approach is simulation efficient by simultaneously estimating low-dimensional marginal posteriors instead of the joint posterior and by proposing simulations targeted to an observation of interest via a prior suitably truncated by an indicator function. Furthermore, by estimating a locally amortized posterior our algorithm enables efficient empirical tests of the robustness of the inference results. Since scientists cannot access the ground truth, these tests are necessary for trusting inference in real-world applications. We perform experiments on a marginalized version of the simulation-based inference benchmark and two complex and narrow posteriors, highlighting the simulator efficiency of our algorithm as well as the quality of the estimated marginal posteriors.

</details>

<details>

<summary>2021-10-26 09:28:25 - Deep Bandits Show-Off: Simple and Efficient Exploration with Deep Networks</summary>

- *Rong Zhu, Mattia Rigotti*

- `2105.04683v2` - [abs](http://arxiv.org/abs/2105.04683v2) - [pdf](http://arxiv.org/pdf/2105.04683v2)

> Designing efficient exploration is central to Reinforcement Learning due to the fundamental problem posed by the exploration-exploitation dilemma. Bayesian exploration strategies like Thompson Sampling resolve this trade-off in a principled way by modeling and updating the distribution of the parameters of the action-value function, the outcome model of the environment. However, this technique becomes infeasible for complex environments due to the computational intractability of maintaining probability distributions over parameters of outcome models of corresponding complexity. Moreover, the approximation techniques introduced to mitigate this issue typically result in poor exploration-exploitation trade-offs, as observed in the case of deep neural network models with approximate posterior methods that have been shown to underperform in the deep bandit scenario. In this paper we introduce Sample Average Uncertainty (SAU), a simple and efficient uncertainty measure for contextual bandits. While Bayesian approaches like Thompson Sampling estimate outcomes uncertainty indirectly by first quantifying the variability over the parameters of the outcome model, SAU is a frequentist approach that directly estimates the uncertainty of the outcomes based on the value predictions. Importantly, we show theoretically that the uncertainty measure estimated by SAU asymptotically matches the uncertainty provided by Thompson Sampling, as well as its regret bounds. Because of its simplicity SAU can be seamlessly applied to deep contextual bandits as a very scalable drop-in replacement for epsilon-greedy exploration. We confirm empirically our theory by showing that SAU-based exploration outperforms current state-of-the-art deep Bayesian bandit methods on several real-world datasets at modest computation cost. Code is available at \url{https://github.com/ibm/sau-explore}.

</details>

<details>

<summary>2021-10-26 09:36:04 - Highly Scalable Maximum Likelihood and Conjugate Bayesian Inference for ERGMs on Graph Sets with Equivalent Vertices</summary>

- *Fan Yin, Carter T. Butts*

- `2110.13527v1` - [abs](http://arxiv.org/abs/2110.13527v1) - [pdf](http://arxiv.org/pdf/2110.13527v1)

> The exponential family random graph modeling (ERGM) framework provides a flexible approach for the statistical analysis of networks. As ERGMs typically involve normalizing factors that are costly to compute, practical inference relies on a variety of approximations or other workarounds. Markov Chain Monte Carlo maximum likelihood (MCMC MLE) provides a powerful tool to approximate the MLE of ERGM parameters, and is feasible for typical models on single networks with as many as a few thousand nodes. MCMC-based algorithms for Bayesian analysis are more expensive, and high-quality answers are challenging to obtain on large graphs. For both strategies, extension to the pooled case - in which we observe multiple networks from a common generative process - adds further computational cost, with both time and memory scaling linearly in the number of graphs. This becomes prohibitive for large networks, or where large numbers of graph observations are available. Here, we exploit some basic properties of the discrete exponential families to develop an approach for ERGM inference in the pooled case that (where applicable) allows an arbitrarily large number of graph observations to be fit at no additional computational cost beyond preprocessing the data itself. Moreover, a variant of our approach can also be used to perform Bayesian inference under conjugate priors, again with no additional computational cost in the estimation phase. As we show, the conjugate prior is easily specified, and is well-suited to applications such as regularization. Simulation studies show that the pooled method leads to estimates with good frequentist properties, and posterior estimates under the conjugate prior are well-behaved. We demonstrate our approach with applications to pooled analysis of brain functional connectivity networks and to replicated x-ray crystal structures of hen egg-white lysozyme.

</details>

<details>

<summary>2021-10-26 09:47:50 - Bayesian Estimation and Comparison of Conditional Moment Models</summary>

- *Siddhartha Chib, Minchul Shin, Anna Simoni*

- `2110.13531v1` - [abs](http://arxiv.org/abs/2110.13531v1) - [pdf](http://arxiv.org/pdf/2110.13531v1)

> We consider the Bayesian analysis of models in which the unknown distribution of the outcomes is specified up to a set of conditional moment restrictions. The nonparametric exponentially tilted empirical likelihood function is constructed to satisfy a sequence of unconditional moments based on an increasing (in sample size) vector of approximating functions (such as tensor splines based on the splines of each conditioning variable). For any given sample size, results are robust to the number of expanded moments. We derive Bernstein-von Mises theorems for the behavior of the posterior distribution under both correct and incorrect specification of the conditional moments, subject to growth rate conditions (slower under misspecification) on the number of approximating functions. A large-sample theory for comparing different conditional moment models is also developed. The central result is that the marginal likelihood criterion selects the model that is less misspecified. We also introduce sparsity-based model search for high-dimensional conditioning variables, and provide efficient MCMC computations for high-dimensional parameters. Along with clarifying examples, the framework is illustrated with real-data applications to risk-factor determination in finance, and causal inference under conditional ignorability.

</details>

<details>

<summary>2021-10-26 12:23:43 - AgEBO-Tabular: Joint Neural Architecture and Hyperparameter Search with Autotuned Data-Parallel Training for Tabular Data</summary>

- *Romain Egele, Prasanna Balaprakash, Venkatram Vishwanath, Isabelle Guyon, Zhengying Liu*

- `2010.16358v2` - [abs](http://arxiv.org/abs/2010.16358v2) - [pdf](http://arxiv.org/pdf/2010.16358v2)

> Developing high-performing predictive models for large tabular data sets is a challenging task. The state-of-the-art methods are based on expert-developed model ensembles from different supervised learning methods. Recently, automated machine learning (AutoML) is emerging as a promising approach to automate predictive model development. Neural architecture search (NAS) is an AutoML approach that generates and evaluates multiple neural network architectures concurrently and improves the accuracy of the generated models iteratively. A key issue in NAS, particularly for large data sets, is the large computation time required to evaluate each generated architecture. While data-parallel training is a promising approach that can address this issue, its use within NAS is difficult. For different data sets, the data-parallel training settings such as the number of parallel processes, learning rate, and batch size need to be adapted to achieve high accuracy and reduction in training time. To that end, we have developed AgEBO-Tabular, an approach to combine aging evolution (AgE), a parallel NAS method that searches over neural architecture space, and an asynchronous Bayesian optimization method for tuning the hyperparameters of the data-parallel training simultaneously. We demonstrate the efficacy of the proposed method to generate high-performing neural network models for large tabular benchmark data sets. Furthermore, we demonstrate that the automatically discovered neural network models using our method outperform the state-of-the-art AutoML ensemble models in inference speed by two orders of magnitude while reaching similar accuracy values.

</details>

<details>

<summary>2021-10-26 15:10:53 - Optimal Regime-Switching Density Forecasts</summary>

- *Graziano Moramarco*

- `2110.13761v1` - [abs](http://arxiv.org/abs/2110.13761v1) - [pdf](http://arxiv.org/pdf/2110.13761v1)

> This paper proposes an approach for enhancing density forecasts of non-normal macroeconomic variables using Bayesian Markov-switching models. Alternative views about economic regimes are combined to produce flexible forecasts, which are optimized with respect to standard objective functions of density forecasting. The optimization procedure explores both forecast combinations and Bayesian model averaging. In an application to U.S. GDP growth, the approach is shown to achieve good accuracy in terms of average predictive densities and to produce well-calibrated forecast distributions. The proposed framework can be used to evaluate the contribution of economists' views to density forecast performance. In the empirical application, we consider views derived from the Fed macroeconomic scenarios used for bank stress tests.

</details>

<details>

<summary>2021-10-26 16:18:45 - GIBBON: General-purpose Information-Based Bayesian OptimisatioN</summary>

- *Henry B. Moss, David S. Leslie, Javier Gonzalez, Paul Rayson*

- `2102.03324v2` - [abs](http://arxiv.org/abs/2102.03324v2) - [pdf](http://arxiv.org/pdf/2102.03324v2)

> This paper describes a general-purpose extension of max-value entropy search, a popular approach for Bayesian Optimisation (BO). A novel approximation is proposed for the information gain -- an information-theoretic quantity central to solving a range of BO problems, including noisy, multi-fidelity and batch optimisations across both continuous and highly-structured discrete spaces. Previously, these problems have been tackled separately within information-theoretic BO, each requiring a different sophisticated approximation scheme, except for batch BO, for which no computationally-lightweight information-theoretic approach has previously been proposed. GIBBON (General-purpose Information-Based Bayesian OptimisatioN) provides a single principled framework suitable for all the above, out-performing existing approaches whilst incurring substantially lower computational overheads. In addition, GIBBON does not require the problem's search space to be Euclidean and so is the first high-performance yet computationally light-weight acquisition function that supports batch BO over general highly structured input spaces like molecular search and gene design. Moreover, our principled derivation of GIBBON yields a natural interpretation of a popular batch BO heuristic based on determinantal point processes. Finally, we analyse GIBBON across a suite of synthetic benchmark tasks, a molecular search loop, and as part of a challenging batch multi-fidelity framework for problems with controllable experimental noise.

</details>

<details>

<summary>2021-10-26 17:46:44 - Dynamic Causal Bayesian Optimization</summary>

- *Virginia Aglietti, Neil Dhir, Javier González, Theodoros Damoulas*

- `2110.13891v1` - [abs](http://arxiv.org/abs/2110.13891v1) - [pdf](http://arxiv.org/pdf/2110.13891v1)

> This paper studies the problem of performing a sequence of optimal interventions in a causal dynamical system where both the target variable of interest and the inputs evolve over time. This problem arises in a variety of domains e.g. system biology and operational research. Dynamic Causal Bayesian Optimization (DCBO) brings together ideas from sequential decision making, causal inference and Gaussian process (GP) emulation. DCBO is useful in scenarios where all causal effects in a graph are changing over time. At every time step DCBO identifies a local optimal intervention by integrating both observational and past interventional data collected from the system. We give theoretical results detailing how one can transfer interventional information across time steps and define a dynamic causal GP model which can be used to quantify uncertainty and find optimal interventions in practice. We demonstrate how DCBO identifies optimal interventions faster than competing approaches in multiple settings and applications.

</details>

<details>

<summary>2021-10-26 18:43:21 - Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement</summary>

- *Samuel Daulton, Maximilian Balandat, Eytan Bakshy*

- `2105.08195v2` - [abs](http://arxiv.org/abs/2105.08195v2) - [pdf](http://arxiv.org/pdf/2105.08195v2)

> Optimizing multiple competing black-box objectives is a challenging problem in many fields, including science, engineering, and machine learning. Multi-objective Bayesian optimization (MOBO) is a sample-efficient approach for identifying the optimal trade-offs between the objectives. However, many existing methods perform poorly when the observations are corrupted by noise. We propose a novel acquisition function, NEHVI, that overcomes this important practical limitation by applying a Bayesian treatment to the popular expected hypervolume improvement (EHVI) criterion and integrating over this uncertainty in the Pareto frontier. We argue that, even in the noiseless setting, generating multiple candidates in parallel is an incarnation of EHVI with uncertainty in the Pareto frontier and therefore can be addressed using the same underlying technique. Through this lens, we derive a natural parallel variant, $q$NEHVI, that reduces computational complexity of parallel EHVI from exponential to polynomial with respect to the batch size. $q$NEHVI is one-step Bayes-optimal for hypervolume maximization in both noisy and noiseless environments, and we show that it can be optimized effectively with gradient-based methods via sample average approximation. Empirically, we demonstrate not only that $q$NEHVI is substantially more robust to observation noise than existing MOBO approaches, but also that it achieves state-of-the-art optimization performance and competitive wall-times in large-batch environments.

</details>

<details>

<summary>2021-10-26 20:34:04 - Detecting and Adapting to Irregular Distribution Shifts in Bayesian Online Learning</summary>

- *Aodong Li, Alex Boyd, Padhraic Smyth, Stephan Mandt*

- `2012.08101v3` - [abs](http://arxiv.org/abs/2012.08101v3) - [pdf](http://arxiv.org/pdf/2012.08101v3)

> We consider the problem of online learning in the presence of distribution shifts that occur at an unknown rate and of unknown intensity. We derive a new Bayesian online inference approach to simultaneously infer these distribution shifts and adapt the model to the detected changes by integrating ideas from change point detection, switching dynamical systems, and Bayesian online learning. Using a binary 'change variable,' we construct an informative prior such that--if a change is detected--the model partially erases the information of past model updates by tempering to facilitate adaptation to the new data distribution. Furthermore, the approach uses beam search to track multiple change-point hypotheses and selects the most probable one in hindsight. Our proposed method is model-agnostic, applicable in both supervised and unsupervised learning settings, suitable for an environment of concept drifts or covariate drifts, and yields improvements over state-of-the-art Bayesian online learning approaches.

</details>

<details>

<summary>2021-10-26 20:41:20 - Graph Posterior Network: Bayesian Predictive Uncertainty for Node Classification</summary>

- *Maximilian Stadler, Bertrand Charpentier, Simon Geisler, Daniel Zügner, Stephan Günnemann*

- `2110.14012v1` - [abs](http://arxiv.org/abs/2110.14012v1) - [pdf](http://arxiv.org/pdf/2110.14012v1)

> The interdependence between nodes in graphs is key to improve class predictions on nodes and utilized in approaches like Label Propagation (LP) or in Graph Neural Networks (GNN). Nonetheless, uncertainty estimation for non-independent node-level predictions is under-explored. In this work, we explore uncertainty quantification for node classification in three ways: (1) We derive three axioms explicitly characterizing the expected predictive uncertainty behavior in homophilic attributed graphs. (2) We propose a new model Graph Posterior Network (GPN) which explicitly performs Bayesian posterior updates for predictions on interdependent nodes. GPN provably obeys the proposed axioms. (3) We extensively evaluate GPN and a strong set of baselines on semi-supervised node classification including detection of anomalous features, and detection of left-out classes. GPN outperforms existing approaches for uncertainty estimation in the experiments.

</details>

<details>

<summary>2021-10-26 21:50:50 - Adaptive random neighbourhood informed Markov chain Monte Carlo for high-dimensional Bayesian variable Selection</summary>

- *Xitong Liang, Samuel Livingstone, Jim Griffin*

- `2110.11747v3` - [abs](http://arxiv.org/abs/2110.11747v3) - [pdf](http://arxiv.org/pdf/2110.11747v3)

> We introduce a framework for efficient Markov Chain Monte Carlo (MCMC) algorithms targeting discrete-valued high-dimensional distributions, such as posterior distributions in Bayesian variable selection (BVS) problems. We show that many recently introduced algorithms, such as the locally informed sampler and the Adaptively Scaled Individual adaptation sampler (ASI), can be viewed as particular cases within the framework. We then describe a novel algorithm, the Adaptive Random Neighbourhood Informed sampler (ARNI), by combining ideas from both of these existing approaches. We show using several examples of both real and simulated datasets that a computationally efficient point-wise implementation (PARNI) leads to relatively more reliable inferences on a range of variable selection problems, particularly in the very large $p$ setting.

</details>

<details>

<summary>2021-10-26 22:59:30 - Differentiable Annealed Importance Sampling and the Perils of Gradient Noise</summary>

- *Guodong Zhang, Kyle Hsu, Jianing Li, Chelsea Finn, Roger Grosse*

- `2107.10211v2` - [abs](http://arxiv.org/abs/2107.10211v2) - [pdf](http://arxiv.org/pdf/2107.10211v2)

> Annealed importance sampling (AIS) and related algorithms are highly effective tools for marginal likelihood estimation, but are not fully differentiable due to the use of Metropolis-Hastings correction steps. Differentiability is a desirable property as it would admit the possibility of optimizing marginal likelihood as an objective using gradient-based methods. To this end, we propose Differentiable AIS (DAIS), a variant of AIS which ensures differentiability by abandoning the Metropolis-Hastings corrections. As a further advantage, DAIS allows for mini-batch gradients. We provide a detailed convergence analysis for Bayesian linear regression which goes beyond previous analyses by explicitly accounting for the sampler not having reached equilibrium. Using this analysis, we prove that DAIS is consistent in the full-batch setting and provide a sublinear convergence rate. Furthermore, motivated by the problem of learning from large-scale datasets, we study a stochastic variant of DAIS that uses mini-batch gradients. Surprisingly, stochastic DAIS can be arbitrarily bad due to a fundamental incompatibility between the goals of last-iterate convergence to the posterior and elimination of the accumulated stochastic error. This is in stark contrast with other settings such as gradient-based optimization and Langevin dynamics, where the effect of gradient noise can be washed out by taking smaller steps. This indicates that annealing-based marginal likelihood estimation with stochastic gradients may require new ideas.

</details>

<details>

<summary>2021-10-27 00:28:15 - Stochastic volatility model with range-based correction and leverage</summary>

- *Yuta Kurose*

- `2110.00039v2` - [abs](http://arxiv.org/abs/2110.00039v2) - [pdf](http://arxiv.org/pdf/2110.00039v2)

> This study presents contemporaneous modeling of asset return and price range within the framework of stochastic volatility with leverage. A new representation of the probability density function for the price range is provided, and its accurate sampling algorithm is developed. A Bayesian estimation using Markov chain Monte Carlo (MCMC) method is provided for the model parameters and unobserved variables. MCMC samples can be generated rigorously, despite the estimation procedure requiring sampling from a density function with the sum of an infinite series. The empirical results obtained using data from the U.S. market indices are consistent with the stylized facts in the financial market, such as the existence of the leverage effect. In addition, to explore the model's predictive ability, a model comparison based on the volatility forecast performance is conducted.

</details>

<details>

<summary>2021-10-27 02:19:07 - Locally Valid and Discriminative Prediction Intervals for Deep Learning Models</summary>

- *Zhen Lin, Shubhendu Trivedi, Jimeng Sun*

- `2106.00225v4` - [abs](http://arxiv.org/abs/2106.00225v4) - [pdf](http://arxiv.org/pdf/2106.00225v4)

> Crucial for building trust in deep learning models for critical real-world applications is efficient and theoretically sound uncertainty quantification, a task that continues to be challenging. Useful uncertainty information is expected to have two key properties: It should be valid (guaranteeing coverage) and discriminative (more uncertain when the expected risk is high). Moreover, when combined with deep learning (DL) methods, it should be scalable and affect the DL model performance minimally. Most existing Bayesian methods lack frequentist coverage guarantees and usually affect model performance. The few available frequentist methods are rarely discriminative and/or violate coverage guarantees due to unrealistic assumptions. Moreover, many methods are expensive or require substantial modifications to the base neural network. Building upon recent advances in conformal prediction [13, 33] and leveraging the classical idea of kernel regression, we propose Locally Valid and Discriminative prediction intervals (LVD), a simple, efficient, and lightweight method to construct discriminative prediction intervals (PIs) for almost any DL model. With no assumptions on the data distribution, such PIs also offer finite-sample local coverage guarantees (contrasted to the simpler marginal coverage). We empirically verify, using diverse datasets, that besides being the only locally valid method for DL, LVD also exceeds or matches the performance (including coverage rate and prediction accuracy) of existing uncertainty quantification methods, while offering additional benefits in scalability and flexibility.

</details>

<details>

<summary>2021-10-27 09:15:05 - A PAC-Bayes Analysis of Adversarial Robustness</summary>

- *Paul Viallard, Guillaume Vidot, Amaury Habrard, Emilie Morvant*

- `2102.11069v2` - [abs](http://arxiv.org/abs/2102.11069v2) - [pdf](http://arxiv.org/pdf/2102.11069v2)

> We propose the first general PAC-Bayesian generalization bounds for adversarial robustness, that estimate, at test time, how much a model will be invariant to imperceptible perturbations in the input. Instead of deriving a worst-case analysis of the risk of a hypothesis over all the possible perturbations, we leverage the PAC-Bayesian framework to bound the averaged risk on the perturbations for majority votes (over the whole class of hypotheses). Our theoretically founded analysis has the advantage to provide general bounds (i) that are valid for any kind of attacks (i.e., the adversarial attacks), (ii) that are tight thanks to the PAC-Bayesian framework, (iii) that can be directly minimized during the learning phase to obtain a robust model on different attacks at test time.

</details>

<details>

<summary>2021-10-27 10:52:17 - A Scalable Inference Method For Large Dynamic Economic Systems</summary>

- *Pratha Khandelwal, Philip Nadler, Rossella Arcucci, William Knottenbelt, Yi-Ke Guo*

- `2110.14346v1` - [abs](http://arxiv.org/abs/2110.14346v1) - [pdf](http://arxiv.org/pdf/2110.14346v1)

> The nature of available economic data has changed fundamentally in the last decade due to the economy's digitisation. With the prevalence of often black box data-driven machine learning methods, there is a necessity to develop interpretable machine learning methods that can conduct econometric inference, helping policymakers leverage the new nature of economic data. We therefore present a novel Variational Bayesian Inference approach to incorporate a time-varying parameter auto-regressive model which is scalable for big data. Our model is applied to a large blockchain dataset containing prices, transactions of individual actors, analyzing transactional flows and price movements on a very granular level. The model is extendable to any dataset which can be modelled as a dynamical system. We further improve the simple state-space modelling by introducing non-linearities in the forward model with the help of machine learning architectures.

</details>

<details>

<summary>2021-10-27 12:02:43 - A sequential estimation problem with control and discretionary stopping</summary>

- *Erik Ekström, Ioannis Karatzas*

- `2110.14366v1` - [abs](http://arxiv.org/abs/2110.14366v1) - [pdf](http://arxiv.org/pdf/2110.14366v1)

> We show that "full-bang" control is optimal in a problem that combines features of (i) sequential least-squares {\it estimation} with Bayesian updating, for a random quantity observed in a bath of white noise; (ii) bounded {\it control} of the rate at which observations are received, with a superquadratic cost per unit time; and (iii) "fast" discretionary {\it stopping}. We develop also the optimal filtering and stopping rules in this context.

</details>

<details>

<summary>2021-10-27 12:38:43 - Non-asymptotic error estimates for the Laplace approximation in Bayesian inverse problems</summary>

- *Tapio Helin, Remo Kretschmann*

- `2012.06603v3` - [abs](http://arxiv.org/abs/2012.06603v3) - [pdf](http://arxiv.org/pdf/2012.06603v3)

> In this paper we study properties of the Laplace approximation of the posterior distribution arising in nonlinear Bayesian inverse problems. Our work is motivated by Schillings et al. (2020), where it is shown that in such a setting the Laplace approximation error in Hellinger distance converges to zero in the order of the noise level. Here, we prove novel error estimates for a given noise level that also quantify the effect due to the nonlinearity of the forward mapping and the dimension of the problem. In particular, we are interested in settings in which a linear forward mapping is perturbed by a small nonlinear mapping. Our results indicate that in this case, the Laplace approximation error is of the size of the perturbation. The paper provides insight into Bayesian inference in nonlinear inverse problems, where linearization of the forward mapping has suitable approximation properties.

</details>

<details>

<summary>2021-10-27 13:36:43 - Locally Differentially Private Bayesian Inference</summary>

- *Tejas Kulkarni, Joonas Jälkö, Samuel Kaski, Antti Honkela*

- `2110.14426v1` - [abs](http://arxiv.org/abs/2110.14426v1) - [pdf](http://arxiv.org/pdf/2110.14426v1)

> In recent years, local differential privacy (LDP) has emerged as a technique of choice for privacy-preserving data collection in several scenarios when the aggregator is not trustworthy. LDP provides client-side privacy by adding noise at the user's end. Thus, clients need not rely on the trustworthiness of the aggregator.   In this work, we provide a noise-aware probabilistic modeling framework, which allows Bayesian inference to take into account the noise added for privacy under LDP, conditioned on locally perturbed observations. Stronger privacy protection (compared to the central model) provided by LDP protocols comes at a much harsher privacy-utility trade-off. Our framework tackles several computational and statistical challenges posed by LDP for accurate uncertainty quantification under Bayesian settings. We demonstrate the efficacy of our framework in parameter estimation for univariate and multi-variate distributions as well as logistic and linear regression.

</details>

<details>

<summary>2021-10-27 15:05:12 - Spatial Bayesian GLM on the cortical surface produces reliable task activations in individuals and groups</summary>

- *Daniel Spencer, Yu, Yue, David Bolin, Sarah Ryan, Amanda F. Mejia*

- `2106.06669v3` - [abs](http://arxiv.org/abs/2106.06669v3) - [pdf](http://arxiv.org/pdf/2106.06669v3)

> The general linear model (GLM) is a widely popular and convenient tool for estimating the functional brain response and identifying areas of significant activation during a task or stimulus. However, the classical GLM is based on a massive univariate approach that does not explicitly leverage the similarity of activation patterns among neighboring brain locations. As a result, it tends to produce noisy estimates and be underpowered to detect significant activations, particularly in individual subjects and small groups. A recent alternative, a cortical surface-based spatial Bayesian GLM, leverages spatial dependencies among neighboring cortical vertices to produce more accurate estimates and areas of functional activation. The spatial Bayesian GLM can be applied to individual and group-level analysis. In this study, we assess the reliability and power of individual and group-average measures of task activation produced via the surface-based spatial Bayesian GLM. We analyze motor task data from 45 subjects in the Human Connectome Project (HCP) and HCP Retest datasets. We also extend the model to multi-run analysis and employ subject-specific cortical surfaces rather than surfaces inflated to a sphere for more accurate distance-based modeling. Results show that the surface-based spatial Bayesian GLM produces highly reliable activations in individual subjects and is powerful enough to detect trait-like functional topologies. Additionally, spatial Bayesian modeling enhances reliability of group-level analysis even in moderately sized samples (n=45). The power of the spatial Bayesian GLM to detect activations above a scientifically meaningful effect size is nearly invariant to sample size, exhibiting high power even in small samples (n=10). The spatial Bayesian GLM is computationally efficient in individuals and groups and is convenient to implement with the open-source BayesfMRI R package.

</details>

<details>

<summary>2021-10-27 16:32:13 - T-LoHo: A Bayesian Regularization Model for Structured Sparsity and Smoothness on Graphs</summary>

- *Changwoo J. Lee, Zhao Tang Luo, Huiyan Sang*

- `2107.02510v2` - [abs](http://arxiv.org/abs/2107.02510v2) - [pdf](http://arxiv.org/pdf/2107.02510v2)

> Graphs have been commonly used to represent complex data structures. In models dealing with graph-structured data, multivariate parameters may not only exhibit sparse patterns but have structured sparsity and smoothness in the sense that both zero and non-zero parameters tend to cluster together. We propose a new prior for high-dimensional parameters with graphical relations, referred to as the Tree-based Low-rank Horseshoe (T-LoHo) model, that generalizes the popular univariate Bayesian horseshoe shrinkage prior to the multivariate setting to detect structured sparsity and smoothness simultaneously. The T-LoHo prior can be embedded in many high-dimensional hierarchical models. To illustrate its utility, we apply it to regularize a Bayesian high-dimensional regression problem where the regression coefficients are linked by a graph, so that the resulting clusters have flexible shapes and satisfy the cluster contiguity constraint with respect to the graph. We design an efficient Markov chain Monte Carlo algorithm that delivers full Bayesian inference with uncertainty measures for model parameters such as the number of clusters. We offer theoretical investigations of the clustering effects and posterior concentration results. Finally, we illustrate the performance of the model with simulation studies and a real data application for anomaly detection on a road network. The results indicate substantial improvements over other competing methods such as the sparse fused lasso.

</details>

<details>

<summary>2021-10-27 17:28:43 - Minimum Probability of Error of List M-ary Hypothesis Testing</summary>

- *Ehsan Asadi Kangarshahi, Albert Guillen i Fabregas*

- `2110.14608v1` - [abs](http://arxiv.org/abs/2110.14608v1) - [pdf](http://arxiv.org/pdf/2110.14608v1)

> We study a variation of Bayesian M-ary hypothesis testing in which the test outputs a list of L candidates out of the M possible upon processing the observation. We study the minimum error probability of list hypothesis testing, where an error is defined as the event where the true hypothesis is not in the list output by the test. We derive two exact expressions of the minimum probability or error. The first is expressed as the error probability of a certain non-Bayesian binary hypothesis test, and is reminiscent of the meta-converse bound. The second, is expressed as the tail probability of the likelihood ratio between the two distributions involved in the aforementioned non-Bayesian binary hypothesis test.

</details>

<details>

<summary>2021-10-28 08:30:48 - The Bayesian Spatial Bradley--Terry Model: Urban Deprivation Modeling in Tanzania</summary>

- *R. G. Seymour, D. Sirl, S. Preston, I. L. Dryden, M. J. A. Ellis, B. Perrat, J. Goulding*

- `2010.14128v4` - [abs](http://arxiv.org/abs/2010.14128v4) - [pdf](http://arxiv.org/pdf/2010.14128v4)

> Identifying the most deprived regions of any country or city is key if policy makers are to design successful interventions. However, locating areas with the greatest need is often surprisingly challenging in developing countries. Due to the logistical challenges of traditional household surveying, official statistics can be slow to be updated; estimates that exist can be coarse, a consequence of prohibitive costs and poor infrastructures; and mass urbanisation can render manually surveyed figures rapidly out-of-date. Comparative judgement models, such as the Bradley--Terry model, offer a promising solution. Leveraging local knowledge, elicited via comparisons of different areas' affluence, such models can both simplify logistics and circumvent biases inherent to house-hold surveys. Yet widespread adoption remains limited, due to the large amount of data existing approaches still require. We address this via development of a novel Bayesian Spatial Bradley--Terry model, which substantially decreases the amount of data comparisons required for effective inference. This model integrates a network representation of the city or country, along with assumptions of spatial smoothness that allow deprivation in one area to be informed by neighbouring areas. We demonstrate the practical effectiveness of this method, through a novel comparative judgement data set collected in Dar es Salaam, Tanzania.

</details>

<details>

<summary>2021-10-28 10:50:43 - On rereading Savage</summary>

- *Yudi Pawitan, Youngjo Lee*

- `2110.15012v1` - [abs](http://arxiv.org/abs/2110.15012v1) - [pdf](http://arxiv.org/pdf/2110.15012v1)

> If we accept Savage's set of axioms, then all uncertainties must be treated like ordinary probability. Savage espoused subjective probability, allowing, for example, the probability of Donald Trump's re-election. But Savage's probability also covers the objective version, such as the probability of heads in a fair toss of a coin. In other words, there is no distinction between objective and subjective probability. Savage's system has great theoretical implications; for example, prior probabilities can be elicited from subjective preferences, and then get updated by objective evidence, a learning step that forms the basis of Bayesian computations. Non-Bayesians have generally refused to accept the subjective aspect of probability or to allow priors in formal statistical modelling. As demanded, for example, by the late Dennis Lindley, since Bayesian probability is axiomatic, it is the non-Bayesians' duty to point out which axioms are not acceptable to them. This is not a simple request, since the Bayesian axioms are not commonly covered in our professional training, even in the Bayesian statistics courses. So our aim is to provide a readable exposition the Bayesian axioms from a close rereading Savage's classic book.

</details>

<details>

<summary>2021-10-28 15:02:09 - Bayesian Optimization with High-Dimensional Outputs</summary>

- *Wesley J. Maddox, Maximilian Balandat, Andrew Gordon Wilson, Eytan Bakshy*

- `2106.12997v2` - [abs](http://arxiv.org/abs/2106.12997v2) - [pdf](http://arxiv.org/pdf/2106.12997v2)

> Bayesian Optimization is a sample-efficient black-box optimization procedure that is typically applied to problems with a small number of independent objectives. However, in practice we often wish to optimize objectives defined over many correlated outcomes (or "tasks"). For example, scientists may want to optimize the coverage of a cell tower network across a dense grid of locations. Similarly, engineers may seek to balance the performance of a robot across dozens of different environments via constrained or robust optimization. However, the Gaussian Process (GP) models typically used as probabilistic surrogates for multi-task Bayesian Optimization scale poorly with the number of outcomes, greatly limiting applicability. We devise an efficient technique for exact multi-task GP sampling that combines exploiting Kronecker structure in the covariance matrices with Matheron's identity, allowing us to perform Bayesian Optimization using exact multi-task GP models with tens of thousands of correlated outputs. In doing so, we achieve substantial improvements in sample efficiency compared to existing approaches that only model aggregate functions of the outcomes. We demonstrate how this unlocks a new class of applications for Bayesian Optimization across a range of tasks in science and engineering, including optimizing interference patterns of an optical interferometer with more than 65,000 outputs.

</details>

<details>

<summary>2021-10-28 15:26:47 - Asymptotics of cut distributions and robust modular inference using Posterior Bootstrap</summary>

- *Emilia Pompe, Pierre E. Jacob*

- `2110.11149v2` - [abs](http://arxiv.org/abs/2110.11149v2) - [pdf](http://arxiv.org/pdf/2110.11149v2)

> Bayesian inference provides a framework to combine an arbitrary number of model components with shared parameters, allowing joint uncertainty estimation and the use of all available data sources. However, misspecification of any part of the model might propagate to all other parts and lead to unsatisfactory results. Cut distributions have been proposed as a remedy, where the information is prevented from flowing along certain directions. We consider cut distributions from an asymptotic perspective, find the equivalent of the Laplace approximation, and notice a lack of frequentist coverage for the associate credible regions. We propose algorithms based on the Posterior Bootstrap that deliver credible regions with the nominal frequentist asymptotic coverage. The algorithms involve numerical optimization programs that can be performed fully in parallel. The results and methods are illustrated in various settings, such as causal inference with propensity scores and epidemiological studies.

</details>

<details>

<summary>2021-10-28 16:49:32 - Bayesian OOD detection with aleatoric uncertainty and outlier exposure</summary>

- *Xi Wang, Laurence Aitchison*

- `2102.12959v3` - [abs](http://arxiv.org/abs/2102.12959v3) - [pdf](http://arxiv.org/pdf/2102.12959v3)

> Typical Bayesian approaches to OOD detection use epistemic uncertainty. Surprisingly from the Bayesian perspective, there are a number of methods that successfully use aleatoric uncertainty to detect OOD points (e.g. Hendryks et al. 2018). In addition, it is difficult to use outlier exposure to improve a Bayesian OOD detection model, as it is not clear whether it is possible or desirable to increase posterior (epistemic) uncertainty at outlier points. We show that a generative model of data curation provides a principled account of aleatoric uncertainty for OOD detection. In particular, aleatoric uncertainty signals a specific type of OOD point: one without a well-defined class-label, and our model of data curation gives a likelihood for these points, giving us a mechanism for conditioning on outlier points and thus performing principled Bayesian outlier exposure. Our principled Bayesian approach, combining aleatoric and epistemic uncertainty with outlier exposure performs better than methods using aleatoric or epistemic alone.

</details>

<details>

<summary>2021-10-28 18:00:12 - Clearing the hurdle: The mass of globular cluster systems as a function of host galaxy mass</summary>

- *Gwendolyn M. Eadie, William E. Harris, Aaron Springford*

- `2110.15376v1` - [abs](http://arxiv.org/abs/2110.15376v1) - [pdf](http://arxiv.org/pdf/2110.15376v1)

> Current observational evidence suggests that all large galaxies contain globular clusters (GCs), while the smallest galaxies do not. Over what galaxy mass range does the transition from GCs to no GCs occur? We investigate this question using galaxies in the Local Group, nearby dwarf galaxies, and galaxies in the Virgo Cluster Survey. We consider four types of statistical models: (1) logistic regression to model the probability that a galaxy of stellar mass $M_{\star}$ has any number of GCs; (2) Poisson regression to model the number of GCs versus $M_{\star}$, (3) linear regression to model the relation between GC system mass ($\log{M_{gcs}}$) and host galaxy mass ($\log{M_{\star}}$), and (4) a Bayesian lognormal hurdle model of the GC system mass as a function of galaxy stellar mass for the entire data sample. From the logistic regression, we find that the 50% probability point for a galaxy to contain GCs is $M_{\star}=10^{6.8}M_{\odot}$. From post-fit diagnostics, we find that Poisson regression is an inappropriate description of the data. Ultimately, we find that the Bayesian lognormal hurdle model, which is able to describe how the mass of the GC system varies with $M_{\star}$ even in the presence of many galaxies with no GCs, is the most appropriate model over the range of our data. In an Appendix, we also present photometry for the little-known GC in the Local Group dwarf Ursa Major II.

</details>

<details>

<summary>2021-10-29 02:57:24 - Structured Dropout Variational Inference for Bayesian Neural Networks</summary>

- *Son Nguyen, Duong Nguyen, Khai Nguyen, Khoat Than, Hung Bui, Nhat Ho*

- `2102.07927v4` - [abs](http://arxiv.org/abs/2102.07927v4) - [pdf](http://arxiv.org/pdf/2102.07927v4)

> Approximate inference in Bayesian deep networks exhibits a dilemma of how to yield high fidelity posterior approximations while maintaining computational efficiency and scalability. We tackle this challenge by introducing a novel variational structured approximation inspired by the Bayesian interpretation of Dropout regularization. Concretely, we focus on the inflexibility of the factorized structure in Dropout posterior and then propose an improved method called Variational Structured Dropout (VSD). VSD employs an orthogonal transformation to learn a structured representation on the variational Gaussian noise with plausible complexity, and consequently induces statistical dependencies in the approximate posterior. Theoretically, VSD successfully addresses the pathologies of previous Variational Dropout methods and thus offers a standard Bayesian justification. We further show that VSD induces an adaptive regularization term with several desirable properties which contribute to better generalization. Finally, we conduct extensive experiments on standard benchmarks to demonstrate the effectiveness of VSD over state-of-the-art variational methods on predictive accuracy, uncertainty estimation, and out-of-distribution detection.

</details>

<details>

<summary>2021-10-29 08:48:02 - Neyman-Pearson lemma for Bayes factors</summary>

- *Andrew Fowlie*

- `2110.15625v1` - [abs](http://arxiv.org/abs/2110.15625v1) - [pdf](http://arxiv.org/pdf/2110.15625v1)

> We point out that the Neyman-Pearson lemma applies to Bayes factors if we consider expected type-1 and type-2 error rates. That is, the Bayes factor is the test statistic that maximises the expected power for a fixed expected type-1 error rate. For Bayes factors involving a simple null hypothesis, the expected type-1 error rate is just the completely frequentist type-1 error rate. Lastly we remark on connections between the Karlin-Rubin theorem and uniformly most powerful tests, and Bayes factors. This provides frequentist motivations for computing the Bayes factor and could help reconcile Bayesians and frequentists.

</details>

<details>

<summary>2021-10-29 10:00:56 - A Hadamard fractioal total variation-Gaussian (HFTG) prior for Bayesian inverse problems</summary>

- *Li-Li Wang, Ming-Hui Ding, Guang-Hui Zheng*

- `2110.15656v1` - [abs](http://arxiv.org/abs/2110.15656v1) - [pdf](http://arxiv.org/pdf/2110.15656v1)

> This paper studies the infinite-dimensional Bayesian inference method with Hadamard fractional total variation-Gaussian (HFTG) prior for solving inverse problems. First, Hadamard fractional Sobolev space is established and proved to be a separable Banach space under some mild conditions. Afterwards, the HFTG prior is constructed in this separable fractional space, and the proposed novel hybrid prior not only captures the texture details of the region and avoids step effects, but also provides a complete theoretical analysis in the infinite dimensional Bayesian inversion. Based on the HFTG prior, the well-posedness and finite-dimensional approximation of the posterior measure of the Bayesian inverse problem are given, and samples are extracted from the posterior distribution using the standard pCN algorithm. Finally, numerical results under different models indicate that the Bayesian inference method with HFTG prior is effective and accurate.

</details>

<details>

<summary>2021-10-29 11:28:29 - Variational Bayesian Optimistic Sampling</summary>

- *Brendan O'Donoghue, Tor Lattimore*

- `2110.15688v1` - [abs](http://arxiv.org/abs/2110.15688v1) - [pdf](http://arxiv.org/pdf/2110.15688v1)

> We consider online sequential decision problems where an agent must balance exploration and exploitation. We derive a set of Bayesian `optimistic' policies which, in the stochastic multi-armed bandit case, includes the Thompson sampling policy. We provide a new analysis showing that any algorithm producing policies in the optimistic set enjoys $\tilde O(\sqrt{AT})$ Bayesian regret for a problem with $A$ actions after $T$ rounds. We extend the regret analysis for optimistic policies to bilinear saddle-point problems which include zero-sum matrix games and constrained bandits as special cases. In this case we show that Thompson sampling can produce policies outside of the optimistic set and suffer linear regret in some instances. Finding a policy inside the optimistic set amounts to solving a convex optimization problem and we call the resulting algorithm `variational Bayesian optimistic sampling' (VBOS). The procedure works for any posteriors, \ie, it does not require the posterior to have any special properties, such as log-concavity, unimodality, or smoothness. The variational view of the problem has many useful properties, including the ability to tune the exploration-exploitation tradeoff, add regularization, incorporate constraints, and linearly parameterize the policy.

</details>

<details>

<summary>2021-10-29 13:18:13 - Aligned Multi-Task Gaussian Process</summary>

- *Olga Mikheeva, Ieva Kazlauskaite, Adam Hartshorne, Hedvig Kjellström, Carl Henrik Ek, Neill D. F. Campbell*

- `2110.15761v1` - [abs](http://arxiv.org/abs/2110.15761v1) - [pdf](http://arxiv.org/pdf/2110.15761v1)

> Multi-task learning requires accurate identification of the correlations between tasks. In real-world time-series, tasks are rarely perfectly temporally aligned; traditional multi-task models do not account for this and subsequent errors in correlation estimation will result in poor predictive performance and uncertainty quantification. We introduce a method that automatically accounts for temporal misalignment in a unified generative model that improves predictive performance. Our method uses Gaussian processes (GPs) to model the correlations both within and between the tasks. Building on the previous work by Kazlauskaiteet al. [2019], we include a separate monotonic warp of the input data to model temporal misalignment. In contrast to previous work, we formulate a lower bound that accounts for uncertainty in both the estimates of the warping process and the underlying functions. Also, our new take on a monotonic stochastic process, with efficient path-wise sampling for the warp functions, allows us to perform full Bayesian inference in the model rather than MAP estimates. Missing data experiments, on synthetic and real time-series, demonstrate the advantages of accounting for misalignments (vs standard unaligned method) as well as modelling the uncertainty in the warping process(vs baseline MAP alignment approach).

</details>

<details>

<summary>2021-10-29 15:42:08 - A Domain-Shrinking based Bayesian Optimization Algorithm with Order-Optimal Regret Performance</summary>

- *Sudeep Salgia, Sattar Vakili, Qing Zhao*

- `2010.13997v3` - [abs](http://arxiv.org/abs/2010.13997v3) - [pdf](http://arxiv.org/pdf/2010.13997v3)

> We consider sequential optimization of an unknown function in a reproducing kernel Hilbert space. We propose a Gaussian process-based algorithm and establish its order-optimal regret performance (up to a poly-logarithmic factor). This is the first GP-based algorithm with an order-optimal regret guarantee. The proposed algorithm is rooted in the methodology of domain shrinking realized through a sequence of tree-based region pruning and refining to concentrate queries in increasingly smaller high-performing regions of the function domain. The search for high-performing regions is localized and guided by an iterative estimation of the optimal function value to ensure both learning efficiency and computational efficiency. Compared with the prevailing GP-UCB family of algorithms, the proposed algorithm reduces computational complexity by a factor of $O(T^{2d-1})$ (where $T$ is the time horizon and $d$ the dimension of the function domain).

</details>

<details>

<summary>2021-10-29 20:52:56 - Bayesian Markov Renewal Mixed Models for Vocalization Syntax</summary>

- *Yutong Wu, Erich D. Jarvis, Abhra Sarkar*

- `2107.07648v3` - [abs](http://arxiv.org/abs/2107.07648v3) - [pdf](http://arxiv.org/pdf/2107.07648v3)

> Studying the neurological, genetic and evolutionary basis of human vocal communication mechanisms is an important field of neuroscience. In the absence of high quality data on humans, mouse vocalization experiments in laboratory settings have been proven to be useful in providing valuable insights into mammalian vocal development and evolution, including especially the impact of certain genetic mutations. Data sets from mouse vocalization experiments usually consist of categorical syllable sequences along with continuous inter-syllable interval times for mice of different genotypes vocalizing under various contexts. Few statistical models have considered the inference for both transition probabilities and inter-state intervals. The latter is of particular importance as increased inter-state intervals can be an indication of possible vocal impairment. In this paper, we propose a class of novel Markov renewal mixed models that capture the stochastic dynamics of both state transitions and inter-state interval times. Specifically, we model the transition dynamics and the inter-state intervals using Dirichlet and gamma mixtures, respectively, allowing the mixture probabilities in both cases to vary flexibly with fixed covariate effects as well as random individual-specific effects. We apply our model to analyze the impact of a mutation in the Foxp2 gene on mouse vocal behavior. We find that genotypes and social contexts significantly affect the inter-state interval times but, compared to previous analyses, the influences of genotype and social context on the syllable transition dynamics are weaker.

</details>

<details>

<summary>2021-10-30 06:03:08 - Semi-parametric modeling of SARS-CoV-2 transmission in Orange County, California using tests, cases, deaths, and seroprevalence data</summary>

- *Damon Bayer, Jonathan Fintzi, Isaac Goldstein, Keith Lumbard, Emily Ricotta, Sarah Warner, Lindsay M. Busch, Jeffrey R. Strich, Daniel S. Chertow, Daniel M. Parker, Bernadette Boden-Albala, Alissa Dratch, Richard Chhuon, Nichole Quick, Matthew Zahn, Vladimir N. Minin*

- `2009.02654v2` - [abs](http://arxiv.org/abs/2009.02654v2) - [pdf](http://arxiv.org/pdf/2009.02654v2)

> Mechanistic modeling of SARS-CoV-2 transmission dynamics and frequently estimating model parameters using streaming surveillance data are important components of the pandemic response toolbox. However, transmission model parameter estimation can be imprecise, and sometimes even impossible, because surveillance data are noisy and not informative about all aspects of the mechanistic model. To partially overcome this obstacle, we propose a Bayesian modeling framework that integrates multiple surveillance data streams. Our model uses both SARS-CoV-2 diagnostics test and mortality time series to estimate our model parameters, while also explicitly integrating seroprevalence data from cross-sectional studies. Importantly, our data generating model for incidence data takes into account changes in the total number of tests performed. We model transmission rate, infection-to-fatality ratio, and a parameter controlling a functional relationship between the true case incidence and the fraction of positive tests as time-varying quantities and estimate changes of these parameters nonparameterically. We apply our Bayesian data integration method to COVID-19 surveillance data collected in Orange County, California between March, 2020 and March, 2021 and find that 33-62% of the Orange County residents experienced SARS-CoV-2 infection by the end of February, 2021. Despite this high number of infections, our results show that the abrupt end of the winter surge in January, 2021, was due to both behavioral changes and a high level of accumulated natural immunity.

</details>

<details>

<summary>2021-10-30 18:20:02 - Bayesian surface regression versus spatial spectral nonparametric curve regression</summary>

- *M. D. Ruiz-Medina, D. Miranda*

- `2111.00302v1` - [abs](http://arxiv.org/abs/2111.00302v1) - [pdf](http://arxiv.org/pdf/2111.00302v1)

> COVID-19 incidence is analyzed at the provinces of some Spanish Communities during the period February-October, 2020. Two infinite-dimensional regression approaches are tested. The first one is implemented in the regression framework introduced in Ruiz-Medina, Miranda and Espejo (2019). Specifically, a bayesian framework is adopted in the estimation of the pure point spectrum of the temporal autocorrelation operator, characterizing the second-order structure of a surface sequence. The second approach is formulated in the context of spatial curve regression. A nonparametric estimator of the spectral density operator, based on the spatial periodogram operator, is computed to approximate the spatial correlation between curves. Dimension reduction is achieved by projection onto the empirical eigenvectors of the long-run spatial covariance operator. Cross-validation procedures are implemented to test the performance of the two functional regression approaches.

</details>

<details>

<summary>2021-10-31 05:15:52 - Geometric Rates of Convergence for Kernel-based Sampling Algorithms</summary>

- *Rajiv Khanna, Liam Hodgkinson, Michael W. Mahoney*

- `1907.08410v4` - [abs](http://arxiv.org/abs/1907.08410v4) - [pdf](http://arxiv.org/pdf/1907.08410v4)

> The rate of convergence of weighted kernel herding (WKH) and sequential Bayesian quadrature (SBQ), two kernel-based sampling algorithms for estimating integrals with respect to some target probability measure, is investigated. Under verifiable conditions on the chosen kernel and target measure, we establish a near-geometric rate of convergence for target measures that are nearly atomic. Furthermore, we show these algorithms perform comparably to the theoretical best possible sampling algorithm under the maximum mean discrepancy. An analysis is also conducted in a distributed setting. Our theoretical developments are supported by empirical observations on simulated data as well as a real world application.

</details>

<details>

<summary>2021-10-31 11:28:09 - Trace-class Gaussian priors for Bayesian learning of neural networks with MCMC</summary>

- *Torben Sell, Sumeetpal S. Singh*

- `2012.10943v2` - [abs](http://arxiv.org/abs/2012.10943v2) - [pdf](http://arxiv.org/pdf/2012.10943v2)

> This paper introduces a new neural network based prior for real valued functions on $\mathbb R^d$ which, by construction, is more easily and cheaply scaled up in the domain dimension $d$ compared to the usual Karhunen-Lo\`eve function space prior. The new prior is a Gaussian neural network prior, where each weight and bias has an independent Gaussian prior, but with the key difference that the variances decrease in the width of the network in such a way that the resulting function is almost surely well defined in the limit of an infinite width network. We show that in a Bayesian treatment of inferring unknown functions, the induced posterior over functions is amenable to Monte Carlo sampling using Hilbert space Markov chain Monte Carlo (MCMC) methods. This type of MCMC is popular, e.g. in the Bayesian Inverse Problems literature, because it is stable under mesh refinement, i.e. the acceptance probability does not shrink to $0$ as more parameters of the function's prior are introduced, even ad infinitum. In numerical examples we demonstrate these stated competitive advantages over other function space priors. We also implement examples in Bayesian Reinforcement Learning to automate tasks from data and demonstrate, for the first time, stability of MCMC to mesh refinement for these type of problems.

</details>

<details>

<summary>2021-10-31 19:52:36 - Bivariate Analysis of Birth Weight and Gestational Age Depending on Environmental Exposures: Bayesian Distributional Regression with Copulas</summary>

- *Jonathan Rathjens, Arthur Kolbe, Jürgen Hölzer, Katja Ickstadt, Nadja Klein*

- `2104.14243v2` - [abs](http://arxiv.org/abs/2104.14243v2) - [pdf](http://arxiv.org/pdf/2104.14243v2)

> In this article, we analyze perinatal data with birth weight (BW) as primarily interesting response variable. Gestational age (GA) is usually an important covariate and included in polynomial form. However, in opposition to this univariate regression, bivariate modeling of BW and GA is recommended to distinguish effects on each, on both, and between them. Rather than a parametric bivariate distribution, we apply conditional copula regression, where marginal distributions of BW and GA (not necessarily of the same form) can be estimated independently, and where the dependence structure is modeled conditional on the covariates separately from these marginals. In the resulting distributional regression models, all parameters of the two marginals and the copula parameter are observation-specific. Besides biometric and obstetric information, data on drinking water contamination and maternal smoking are included as environmental covariates. While the Gaussian distribution is suitable for BW, the skewed GA data are better modeled by the three-parametric Dagum distribution. The Clayton copula performs better than the Gumbel and the symmetric Gaussian copula, indicating lower tail dependence (stronger dependence when both variables are low), although this non-linear dependence between BW and GA is surprisingly weak and only influenced by Cesarean section. A non-linear trend of BW on GA is detected by a classical univariate model that is polynomial with respect to the effect of GA. Linear effects on BW mean are similar in both models, while our distributional copula regression also reveals covariates' effects on all other parameters.

</details>


## 2021-11

<details>

<summary>2021-11-01 00:42:31 - End-to-End Learning of Deep Kernel Acquisition Functions for Bayesian Optimization</summary>

- *Tomoharu Iwata*

- `2111.00639v1` - [abs](http://arxiv.org/abs/2111.00639v1) - [pdf](http://arxiv.org/pdf/2111.00639v1)

> For Bayesian optimization (BO) on high-dimensional data with complex structure, neural network-based kernels for Gaussian processes (GPs) have been used to learn flexible surrogate functions by the high representation power of deep learning. However, existing methods train neural networks by maximizing the marginal likelihood, which do not directly improve the BO performance. In this paper, we propose a meta-learning method for BO with neural network-based kernels that minimizes the expected gap between the true optimum value and the best value found by BO. We model a policy, which takes the current evaluated data points as input and outputs the next data point to be evaluated, by a neural network, where neural network-based kernels, GPs, and mutual information-based acquisition functions are used as its layers. With our model, the neural network-based kernel is trained to be appropriate for the acquisition function by backpropagating the gap through the acquisition function and GP. Our model is trained by a reinforcement learning framework from multiple tasks. Since the neural network is shared across different tasks, we can gather knowledge on BO from multiple training tasks, and use the knowledge for unseen test tasks. In experiments using three text document datasets, we demonstrate that the proposed method achieves better BO performance than the existing methods.

</details>

<details>

<summary>2021-11-01 01:08:59 - Posterior Inference for Quantile Regression: Adaptation to Sparsity</summary>

- *Yuanzhi Li, Xuming He*

- `2111.00642v1` - [abs](http://arxiv.org/abs/2111.00642v1) - [pdf](http://arxiv.org/pdf/2111.00642v1)

> Quantile regression is a powerful data analysis tool that accommodates heterogeneous covariate-response relationships. We find that by coupling the asymmetric Laplace working likelihood with appropriate shrinkage priors, we can deliver posterior inference that automatically adapts to possible sparsity in quantile regression analysis. After a suitable adjustment on the posterior variance, the posterior inference provides asymptotically valid inference under heterogeneity. Furthermore, the proposed approach leads to oracle asymptotic efficiency for the active (nonzero) quantile regression coefficients and super-efficiency for the non-active ones. By avoiding the need to pursue dichotomous variable selection, the Bayesian computational framework demonstrates desirable inference stability with respect to tuning parameter selection. Our work helps to uncloak the value of Bayesian computational methods in frequentist inference for quantile regression.

</details>

<details>

<summary>2021-11-01 02:12:04 - Early Detection of COVID-19 Hotspots Using Spatio-Temporal Data</summary>

- *Shixiang Zhu, Alexander Bukharin, Liyan Xie, Khurram Yamin, Shihao Yang, Pinar Keskinocak, Yao Xie*

- `2106.00072v2` - [abs](http://arxiv.org/abs/2106.00072v2) - [pdf](http://arxiv.org/pdf/2106.00072v2)

> Recently, the Centers for Disease Control and Prevention (CDC) has worked with other federal agencies to identify counties with increasing coronavirus disease 2019 (COVID-19) incidence (hotspots) and offers support to local health departments to limit the spread of the disease. Understanding the spatio-temporal dynamics of hotspot events is of great importance to support policy decisions and prevent large-scale outbreaks. This paper presents a spatio-temporal Bayesian framework for early detection of COVID-19 hotspots (at the county level) in the United States. We assume both the observed number of cases and hotspots depend on a class of latent random variables, which encode the underlying spatio-temporal dynamics of the transmission of COVID-19. Such latent variables follow a zero-mean Gaussian process, whose covariance is specified by a non-stationary kernel function. The most salient feature of our kernel function is that deep neural networks are introduced to enhance the model's representative power while still enjoying the interpretability of the kernel. We derive a sparse model and fit the model using a variational learning strategy to circumvent the computational intractability for large data sets. Our model demonstrates better interpretability and superior hotspot-detection performance compared to other baseline methods.

</details>

<details>

<summary>2021-11-01 05:50:15 - Detecting Out-of-distribution Samples via Variational Auto-encoder with Reliable Uncertainty Estimation</summary>

- *Xuming Ran, Mingkun Xu, Lingrui Mei, Qi Xu, Quanying Liu*

- `2007.08128v3` - [abs](http://arxiv.org/abs/2007.08128v3) - [pdf](http://arxiv.org/pdf/2007.08128v3)

> Variational autoencoders (VAEs) are influential generative models with rich representation capabilities from the deep neural network architecture and Bayesian method. However, VAE models have a weakness that assign a higher likelihood to out-of-distribution (OOD) inputs than in-distribution (ID) inputs. To address this problem, a reliable uncertainty estimation is considered to be critical for in-depth understanding of OOD inputs. In this study, we propose an improved noise contrastive prior (INCP) to be able to integrate into the encoder of VAEs, called INCPVAE. INCP is scalable, trainable and compatible with VAEs, and it also adopts the merits from the INCP for uncertainty estimation. Experiments on various datasets demonstrate that compared to the standard VAEs, our model is superior in uncertainty estimation for the OOD data and is robust in anomaly detection tasks. The INCPVAE model obtains reliable uncertainty estimation for OOD inputs and solves the OOD problem in VAE models.

</details>

<details>

<summary>2021-11-01 11:20:09 - Expanding Multi-Market Monopoly and Nonconcavity in the Value of Information</summary>

- *Stefan Behringer*

- `2111.00839v1` - [abs](http://arxiv.org/abs/2111.00839v1) - [pdf](http://arxiv.org/pdf/2111.00839v1)

> In this paper I investigate a Bayesian inverse problem in the specific setting of a price setting monopolist facing a randomly growing demand in multiple possibly interconnected markets. Investigating the Value of Information of a signal to the monopolist in a fully dynamic discrete model employing the Kalman-Bucy-Stratonovich filter, we find that it may be non-monotonic in the variance of the signal. In the classical static settings of the Value of Information literature this relationship may be convex or concave, but is always monotonic. The existence of the non-monotonicity depends critically on the exogenous growth rate of the system.

</details>

<details>

<summary>2021-11-01 17:17:34 - NOTMAD: Estimating Bayesian Networks with Sample-Specific Structures and Parameters</summary>

- *Ben Lengerich, Caleb Ellington, Bryon Aragam, Eric P. Xing, Manolis Kellis*

- `2111.01104v1` - [abs](http://arxiv.org/abs/2111.01104v1) - [pdf](http://arxiv.org/pdf/2111.01104v1)

> Context-specific Bayesian networks (i.e. directed acyclic graphs, DAGs) identify context-dependent relationships between variables, but the non-convexity induced by the acyclicity requirement makes it difficult to share information between context-specific estimators (e.g. with graph generator functions). For this reason, existing methods for inferring context-specific Bayesian networks have favored breaking datasets into subsamples, limiting statistical power and resolution, and preventing the use of multidimensional and latent contexts. To overcome this challenge, we propose NOTEARS-optimized Mixtures of Archetypal DAGs (NOTMAD). NOTMAD models context-specific Bayesian networks as the output of a function which learns to mix archetypal networks according to sample context. The archetypal networks are estimated jointly with the context-specific networks and do not require any prior knowledge. We encode the acyclicity constraint as a smooth regularization loss which is back-propagated to the mixing function; in this way, NOTMAD shares information between context-specific acyclic graphs, enabling the estimation of Bayesian network structures and parameters at even single-sample resolution. We demonstrate the utility of NOTMAD and sample-specific network inference through analysis and experiments, including patient-specific gene expression networks which correspond to morphological variation in cancer.

</details>

<details>

<summary>2021-11-01 20:50:28 - Bayesian Functional Registration of fMRI Activation Maps</summary>

- *Guoqing Wang, Abhirup Datta, Martin A. Lindquist*

- `2102.10179v2` - [abs](http://arxiv.org/abs/2102.10179v2) - [pdf](http://arxiv.org/pdf/2102.10179v2)

> Functional magnetic resonance imaging (fMRI) has provided invaluable insight into our understanding of human behavior. However, large inter-individual differences in both brain anatomy and functional localization after anatomical alignment remain a major limitation in conducting group analyses and performing population-level inference. This paper addresses this problem by developing and validating a new computational technique for reducing misalignment across individuals in functional brain systems by spatially transforming each subject's functional data to a common reference map. Our proposed Bayesian functional registration approach allows us to assess differences in brain function across subjects and individual differences in activation topology. It combines intensity-based and feature-based information into an integrated framework and allows inference to be performed on the transformation via the posterior samples. We evaluate the method in a simulation study and apply it to data from a study of thermal pain. We find that the proposed approach provides increased sensitivity for group-level inference.

</details>

<details>

<summary>2021-11-01 22:32:42 - Computing with R-INLA: Accuracy and reproducibility with implications for the analysis of COVID-19 data</summary>

- *Kori Khan, Hengrui Luo, Wenna Xi*

- `2111.01285v1` - [abs](http://arxiv.org/abs/2111.01285v1) - [pdf](http://arxiv.org/pdf/2111.01285v1)

> The statistical methods used to analyze medical data are becoming increasingly complex. Novel statistical methods increasingly rely on simulation studies to assess their validity. Such assessments typically appear in statistical or computational journals, and the methodology is later introduced to the medical community through tutorials. This can be problematic if applied researchers use the methodologies in settings that have not been evaluated. In this paper, we explore a case study of one such method that has become popular in the analysis of coronavirus disease 2019 (COVID-19) data. The integrated nested Laplace approximations (INLA), as implemented in the R-INLA package, approximates the marginal posterior distributions of target parameters that would have been obtained from a fully Bayesian analysis. We seek to answer an important question: Does existing research on the accuracy of INLA's approximations support how researchers are currently using it to analyze COVID-19 data? We identify three limitations to work assessing INLA's accuracy: 1) inconsistent definitions of accuracy, 2) a lack of studies validating how researchers are actually using INLA, and 3) a lack of research into the reproducibility of INLA's output. We explore the practical impact of each limitation with simulation studies based on models and data used in COVID-19 research. Our results suggest existing methods of assessing the accuracy of the INLA technique may not support how COVID-19 researchers are using it. Guided in part by our results, we offer a proposed set of minimum guidelines for researchers using statistical methodologies primarily validated through simulation studies.

</details>

<details>

<summary>2021-11-02 01:34:50 - BayesDLMfMRI: Bayesian Matrix-Variate Dynamic Linear Models for Task-based fRMI Modeling in R</summary>

- *Johnatan Cardona Jiménez*

- `2111.01318v1` - [abs](http://arxiv.org/abs/2111.01318v1) - [pdf](http://arxiv.org/pdf/2111.01318v1)

> This article introduces an R package to perform statistical analysis for task-based fMRI data at both individual and group levels. The analysis to detect brain activation at the individual level is based on modeling the fMRI signal using Matrix-Variate Dynamic Linear Models (MDLM). Therefore, the analysis for the group stage is based on posterior distributions of the state parameter obtained from the modeling at the individual level. In this way, this package offers several R functions with different algorithms to perform inference on the state parameter to assess brain activation for both individual and group stages. Those functions allow for parallel computation when the analysis is performed for the entire brain as well as analysis at specific voxels when it is required.

</details>

<details>

<summary>2021-11-02 11:09:05 - OutbreakFlow: Model-based Bayesian inference of disease outbreak dynamics with invertible neural networks and its application to the COVID-19 pandemics in Germany</summary>

- *Stefan T. Radev, Frederik Graw, Simiao Chen, Nico T. Mutters, Vanessa M. Eichel, Till Bärnighausen, Ullrich Köthe*

- `2010.00300v4` - [abs](http://arxiv.org/abs/2010.00300v4) - [pdf](http://arxiv.org/pdf/2010.00300v4)

> Mathematical models in epidemiology are an indispensable tool to determine the dynamics and important characteristics of infectious diseases. Apart from their scientific merit, these models are often used to inform political decisions and intervention measures during an ongoing outbreak. However, reliably inferring the dynamics of ongoing outbreaks by connecting complex models to real data is still hard and requires either laborious manual parameter fitting or expensive optimization methods which have to be repeated from scratch for every application of a given model. In this work, we address this problem with a novel combination of epidemiological modeling with specialized neural networks. Our approach entails two computational phases: In an initial training phase, a mathematical model describing the epidemic is used as a coach for a neural network, which acquires global knowledge about the full range of possible disease dynamics. In the subsequent inference phase, the trained neural network processes the observed data of an actual outbreak and infers the parameters of the model in order to realistically reproduce the observed dynamics and reliably predict future progression. With its flexible framework, our simulation-based approach is applicable to a variety of epidemiological models. Moreover, since our method is fully Bayesian, it is designed to incorporate all available prior knowledge about plausible parameter values and returns complete joint posterior distributions over these parameters. Application of our method to the early Covid-19 outbreak phase in Germany demonstrates that we are able to obtain reliable probabilistic estimates for important disease characteristics, such as generation time, fraction of undetected infections, likelihood of transmission before symptom onset, and reporting delays using a very moderate amount of real-world observations.

</details>

<details>

<summary>2021-11-02 12:33:42 - Likelihood-Free Inference in State-Space Models with Unknown Dynamics</summary>

- *Alexander Aushev, Thong Tran, Henri Pesonen, Andrew Howes, Samuel Kaski*

- `2111.01555v1` - [abs](http://arxiv.org/abs/2111.01555v1) - [pdf](http://arxiv.org/pdf/2111.01555v1)

> We introduce a method for inferring and predicting latent states in the important and difficult case of state-space models where observations can only be simulated, and transition dynamics are unknown. In this setting, the likelihood of observations is not available and only synthetic observations can be generated from a black-box simulator. We propose a way of doing likelihood-free inference (LFI) of states and state prediction with a limited number of simulations. Our approach uses a multi-output Gaussian process for state inference, and a Bayesian Neural Network as a model of the transition dynamics for state prediction. We improve upon existing LFI methods for the inference task, while also accurately learning transition dynamics. The proposed method is necessary for modelling inverse problems in dynamical systems with computationally expensive simulations, as demonstrated in experiments with non-stationary user models.

</details>

<details>

<summary>2021-11-02 15:05:11 - Information Spillover in Multiple Zero-sum Games</summary>

- *Lucas Pahl*

- `2111.01647v1` - [abs](http://arxiv.org/abs/2111.01647v1) - [pdf](http://arxiv.org/pdf/2111.01647v1)

> This paper considers an infinitely repeated three-player Bayesian game with lack of information on two sides, in which an informed player plays two zero-sum games simultaneously at each stage against two uninformed players. This is a generalization of the Aumann et al. [1] two-player zero-sum one-sided incomplete information model. Under a correlated prior, the informed player faces the problem of how to optimally disclose information among two uninformed players in order to maximize his long-term average payoffs. Our objective is to understand the adverse effects of \information spillover" from one game to the other in the equilibrium payoff set of the informed player. We provide conditions under which the informed player can fully overcome such adverse effects and characterize equilibrium payoffs. In a second result, we show how the effects of information spillover on the equilibrium payoff set of the informed player might be severe.

</details>

<details>

<summary>2021-11-02 15:09:56 - Adjusting for misclassification of an exposure in an individual participant data meta-analysis</summary>

- *Valentijn M. T. de Jong, Harlan Campbell, Lauren Maxwell, Thomas Jaenisch, Paul Gustafson, Thomas P. A. Debray*

- `2111.01650v1` - [abs](http://arxiv.org/abs/2111.01650v1) - [pdf](http://arxiv.org/pdf/2111.01650v1)

> A common problem in the analysis of multiple data sources, including individual participant data meta-analysis (IPD-MA), is the misclassification of binary variables. Misclassification may lead to biased estimates of model parameters, even when the misclassification is entirely random. We aimed to develop statistical methods that facilitate unbiased estimation of adjusted and unadjusted exposure-outcome associations and between-study heterogeneity in IPD-MA, where the extent and nature of exposure misclassification may vary across studies.   We present Bayesian methods that allow misclassification of binary exposure variables to depend on study- and participant-level characteristics. In an example of the differential diagnosis of dengue using two variables, where the gold standard measurement for the exposure variable was unavailable for some studies which only measured a surrogate prone to misclassification, our methods yielded more accurate estimates than analyses naive with regard to misclassification or based on gold standard measurements alone. In a simulation study, the evaluated misclassification model yielded valid estimates of the exposure-outcome association, and was more accurate than analyses restricted to gold standard measurements.   Our proposed framework can appropriately account for the presence of binary exposure misclassification in IPD-MA. It requires that some studies supply IPD for the surrogate and gold standard exposure and misclassification is exchangeable across studies conditional on observed covariates (and outcome). The proposed methods are most beneficial when few large studies that measured the gold standard are available, and when misclassification is frequent.

</details>

<details>

<summary>2021-11-02 18:01:19 - A Bayesian Approach to Invariant Deep Neural Networks</summary>

- *Nikolaos Mourdoukoutas, Marco Federici, Georges Pantalos, Mark van der Wilk, Vincent Fortuin*

- `2107.09301v2` - [abs](http://arxiv.org/abs/2107.09301v2) - [pdf](http://arxiv.org/pdf/2107.09301v2)

> We propose a novel Bayesian neural network architecture that can learn invariances from data alone by inferring a posterior distribution over different weight-sharing schemes. We show that our model outperforms other non-invariant architectures, when trained on datasets that contain specific invariances. The same holds true when no data augmentation is performed.

</details>

<details>

<summary>2021-11-02 23:24:57 - Discrete Bilal distribution with right-censored data</summary>

- *Bruno Caparroz Lopes de Freitas, Jorge Alberto Achcar, Marcos Vinicius de Oliveira Peres, Edson Zangiacomi Martinez*

- `2111.01943v1` - [abs](http://arxiv.org/abs/2111.01943v1) - [pdf](http://arxiv.org/pdf/2111.01943v1)

> This paper presents inferences for the discrete Bilal (DB) distribution introduced by Altun et al. (2020). We consider parameter estimation for DB distribution in the presence of randomly right-censored data.We use maximum likelihood and Bayesian methods for the estimation of the model parameters. We also consider the inclusion of a cure fraction in the model. The usefulness of the proposed model was illustrated with three examples considering real datasets. These applications suggested that the model based on DB distribution performs at least as good as some other traditional discrete models as the DsFx-I, discrete Lindley, discrete Rayleigh, and discrete Burr- Hatke distributions. R codes are provided in an appendix at the end of the paper so that reader can carry out their own analysis.

</details>

<details>

<summary>2021-11-03 04:05:06 - Uncertainty and Value of Information in Risk Prediction Modeling</summary>

- *Mohsen Sadatsafavi, Tae Yoon Lee, Paul Gustafson*

- `2106.10721v3` - [abs](http://arxiv.org/abs/2106.10721v3) - [pdf](http://arxiv.org/pdf/2106.10721v3)

> Background: Due to the finite size of the development sample, predicted probabilities from a risk prediction model are inevitably uncertain. We apply Value of Information methodology to evaluate the decision-theoretic implications of prediction uncertainty.   Methods: Adopting a Bayesian perspective, we extend the definition of the Expected Value of Perfect Information (EVPI) from decision analysis to net benefit calculations in risk prediction. In the context of model development, EVPI is the expected gain in net benefit by using the correct predictions as opposed to predictions from a proposed model. We suggest bootstrap methods for sampling from the posterior distribution of predictions for EVPI calculation using Monte Carlo simulations. In a case study, we used subsets of data of various sizes from a clinical trial for predicting mortality after myocardial infarction to show how EVPI changes with sample size.   Results: With a sample size of 1,000 and at the pre-specified threshold of 2% on predicted risks, the gain in net benefit by using the proposed and the correct models were 0.0006 and 0.0011, respectively, resulting in an EVPI of 0.0005 and a relative EVPI of 87%. EVPI was zero only at unrealistically high thresholds (>85%). As expected, EVPI declined with larger samples. We summarize an algorithm for incorporating EVPI calculations into the commonly used bootstrap method for optimism correction.   Conclusion: Value of Information methods can be applied to explore decision-theoretic consequences of uncertainty in risk prediction and can complement inferential methods when developing risk prediction models. R code for implementing this method is provided.

</details>

<details>

<summary>2021-11-03 04:47:37 - Scalable mixed-domain Gaussian processes</summary>

- *Juho Timonen, Harri Lähdesmäki*

- `2111.02019v1` - [abs](http://arxiv.org/abs/2111.02019v1) - [pdf](http://arxiv.org/pdf/2111.02019v1)

> Gaussian process (GP) models that combine both categorical and continuous input variables have found use e.g. in longitudinal data analysis and computer experiments. However, standard inference for these models has the typical cubic scaling, and common scalable approximation schemes for GPs cannot be applied since the covariance function is non-continuous. In this work, we derive a basis function approximation scheme for mixed-domain covariance functions, which scales linearly with respect to the number of observations and total number of basis functions. The proposed approach is naturally applicable to Bayesian GP regression with arbitrary observation models. We demonstrate the approach in a longitudinal data modelling context and show that it approximates the exact GP model accurately, requiring only a fraction of the runtime compared to fitting the corresponding exact model.

</details>

<details>

<summary>2021-11-03 09:34:02 - Bayes-Newton Methods for Approximate Bayesian Inference with PSD Guarantees</summary>

- *William J. Wilkinson, Simo Särkkä, Arno Solin*

- `2111.01721v2` - [abs](http://arxiv.org/abs/2111.01721v2) - [pdf](http://arxiv.org/pdf/2111.01721v2)

> We formulate natural gradient variational inference (VI), expectation propagation (EP), and posterior linearisation (PL) as extensions of Newton's method for optimising the parameters of a Bayesian posterior distribution. This viewpoint explicitly casts inference algorithms under the framework of numerical optimisation. We show that common approximations to Newton's method from the optimisation literature, namely Gauss-Newton and quasi-Newton methods (e.g., the BFGS algorithm), are still valid under this 'Bayes-Newton' framework. This leads to a suite of novel algorithms which are guaranteed to result in positive semi-definite covariance matrices, unlike standard VI and EP. Our unifying viewpoint provides new insights into the connections between various inference schemes. All the presented methods apply to any model with a Gaussian prior and non-conjugate likelihood, which we demonstrate with (sparse) Gaussian processes and state space models.

</details>

<details>

<summary>2021-11-03 09:56:48 - Quasi-Bayesian Dual Instrumental Variable Regression</summary>

- *Ziyu Wang, Yuhao Zhou, Tongzheng Ren, Jun Zhu*

- `2106.08750v2` - [abs](http://arxiv.org/abs/2106.08750v2) - [pdf](http://arxiv.org/pdf/2106.08750v2)

> Recent years have witnessed an upsurge of interest in employing flexible machine learning models for instrumental variable (IV) regression, but the development of uncertainty quantification methodology is still lacking. In this work we present a novel quasi-Bayesian procedure for IV regression, building upon the recently developed kernelized IV models and the dual/minimax formulation of IV regression. We analyze the frequentist behavior of the proposed method, by establishing minimax optimal contraction rates in $L_2$ and Sobolev norms, and discussing the frequentist validity of credible balls. We further derive a scalable inference algorithm which can be extended to work with wide neural network models. Empirical evaluation shows that our method produces informative uncertainty estimates on complex high-dimensional problems.

</details>

<details>

<summary>2021-11-03 11:31:17 - A Bayesian perspective on sampling of alternatives</summary>

- *Thijs Dekker, Prateek Bansal*

- `2101.06211v2` - [abs](http://arxiv.org/abs/2101.06211v2) - [pdf](http://arxiv.org/pdf/2101.06211v2)

> In this paper, we apply a Bayesian perspective to the sampling of alternatives for multinomial logit (MNL) and mixed multinomial logit (MMNL) models. A sampling of alternatives reduces the computational challenge of evaluating the denominator of the logit choice probability for large choice sets by only using a smaller subset of sampled alternatives including the chosen alternative. To correct for the resulting overestimation of the choice probability, a correction factor has to be applied. McFadden (1978) proposes a correction factor to the utility of each alternative which is based on the probability of sampling the smaller subset of alternatives and that alternative being chosen. McFadden's correction factor ensures consistency of parameter estimates under a wide range of sampling protocols. A special sampling protocol discussed by McFadden is uniform conditioning, which assigns the same sampling probability and therefore the same correction factor to each alternative in the sampled choice set. Since a constant is added to each alternative the correction factor cancels out, but consistent estimates are still obtained. Bayesian estimation is focused on describing the full posterior distributions of the parameters of interest instead of the consistency of their point estimates. We theoretically show that uniform conditioning is sufficient to minimise the loss of information from a sampling of alternatives on the parameters of interest over the full posterior distribution in Bayesian MNL models. Minimum loss of information is, however, not guaranteed for other sampling protocols. This result extends to Bayesian MMNL models estimated using the principle of data augmentation. The application of uniform conditioning, a more restrictive sampling protocol, is thus sufficient in a Bayesian estimation context to achieve finite sample properties of MNL and MMNL parameter estimates.

</details>

<details>

<summary>2021-11-03 16:02:27 - Online Learning of Energy Consumption for Navigation of Electric Vehicles</summary>

- *Niklas Åkerblom, Yuxin Chen, Morteza Haghir Chehreghani*

- `2111.02314v1` - [abs](http://arxiv.org/abs/2111.02314v1) - [pdf](http://arxiv.org/pdf/2111.02314v1)

> Energy-efficient navigation constitutes an important challenge in electric vehicles, due to their limited battery capacity. We employ a Bayesian approach to model the energy consumption at road segments for efficient navigation. In order to learn the model parameters, we develop an online learning framework and investigate several exploration strategies such as Thompson Sampling and Upper Confidence Bound. We then extend our online learning framework to multi-agent setting, where multiple vehicles adaptively navigate and learn the parameters of the energy model. We analyze Thompson Sampling and establish rigorous regret bounds on its performance in the single-agent and multi-agent settings, through an analysis of the algorithm under batched feedback. Finally, we demonstrate the performance of our methods via experiments on several real-world city road networks.

</details>

<details>

<summary>2021-11-03 16:24:05 - Implicit Deep Adaptive Design: Policy-Based Experimental Design without Likelihoods</summary>

- *Desi R. Ivanova, Adam Foster, Steven Kleinegesse, Michael U. Gutmann, Tom Rainforth*

- `2111.02329v1` - [abs](http://arxiv.org/abs/2111.02329v1) - [pdf](http://arxiv.org/pdf/2111.02329v1)

> We introduce implicit Deep Adaptive Design (iDAD), a new method for performing adaptive experiments in real-time with implicit models. iDAD amortizes the cost of Bayesian optimal experimental design (BOED) by learning a design policy network upfront, which can then be deployed quickly at the time of the experiment. The iDAD network can be trained on any model which simulates differentiable samples, unlike previous design policy work that requires a closed form likelihood and conditionally independent experiments. At deployment, iDAD allows design decisions to be made in milliseconds, in contrast to traditional BOED approaches that require heavy computation during the experiment itself. We illustrate the applicability of iDAD on a number of experiments, and show that it provides a fast and effective mechanism for performing adaptive design with implicit models.

</details>

<details>

<summary>2021-11-03 18:21:36 - Faster MCMC for Gaussian Latent Position Network Models</summary>

- *Neil A. Spencer, Brian Junker, Tracy M. Sweet*

- `2006.07687v2` - [abs](http://arxiv.org/abs/2006.07687v2) - [pdf](http://arxiv.org/pdf/2006.07687v2)

> Latent position network models are a versatile tool in network science; applications include clustering entities, controlling for causal confounders, and defining priors over unobserved graphs. Estimating each node's latent position is typically framed as a Bayesian inference problem, with Metropolis within Gibbs being the most popular tool for approximating the posterior distribution. However, it is well-known that Metropolis within Gibbs is inefficient for large networks; the acceptance ratios are expensive to compute, and the resultant posterior draws are highly correlated. In this article, we propose an alternative Markov chain Monte Carlo strategy -- defined using a combination of split Hamiltonian Monte Carlo and Firefly Monte Carlo -- that leverages the posterior distribution's functional form for more efficient posterior computation. We demonstrate that these strategies outperform Metropolis within Gibbs and other algorithms on synthetic networks, as well as on real information-sharing networks of teachers and staff in a school district.

</details>

<details>

<summary>2021-11-03 18:22:25 - On Johnson's "sufficientness" postulates for features-sampling models</summary>

- *Federico Camerlenghi, Stefano Favaro*

- `2111.02456v1` - [abs](http://arxiv.org/abs/2111.02456v1) - [pdf](http://arxiv.org/pdf/2111.02456v1)

> In the 1920's, the English philosopher W.E. Johnson introduced a characterization of the symmetric Dirichlet prior distribution in terms of its predictive distribution. This is typically referred to as Johnson's "sufficientness" postulate, and it has been the subject of many contributions in Bayesian statistics, leading to predictive characterization for infinite-dimensional generalizations of the Dirichlet distribution, i.e. species-sampling models. In this paper, we review "sufficientness" postulates for species-sampling models, and then investigate analogous predictive characterizations for the more general features-sampling models. In particular, we present a "sufficientness" postulate for a class of features-sampling models referred to as Scaled Processes (SPs), and then discuss analogous characterizations in the general setup of features-sampling models.

</details>

<details>

<summary>2021-11-03 18:28:50 - Joint Species Distribution Modeling with species competition and non-stationary spatial random effects</summary>

- *Juho Kettunen, Lauri Mehtätalo, Eeva-Stiina Tuittila, Aino Korrensalo, Jarno Vanhatalo*

- `2111.02460v1` - [abs](http://arxiv.org/abs/2111.02460v1) - [pdf](http://arxiv.org/pdf/2111.02460v1)

> Joint species distribution models (JSDM) are among the most important statistical tools in community ecology. However, existing JSDMs cannot model mutual exclusion between species. We tackle this deficiency by developing a novel hierarchical JSDM with Dirichlet-Multinomial observation process for mutually exclusive species groups. We apply non-stationary multivariate Gaussian processes to describe species niche preferences and conduct Bayesian inference using Markov chain Monte Carlo. We propose decision theoretic model comparison and validation methods to assess the goodness of the proposed model and its alternatives in a case study on modeling vegetation cover in a boreal peatland in Finland. Our results show that ignoring the interspecific interactions and competition significantly reduces models predictive performance and through that leads to biased estimates for total cover of individual species and over all species combined. Models relative predictive performance also depends on the predictive task highlighting that model comparison and assessment method should resemble the true predictive task. Our results also demonstrate that the proposed joint species distribution model can be used to simultaneously infer interspecific correlations in niche preference as well as mutual competition for space and through that provide novel insight into ecological research.

</details>

<details>

<summary>2021-11-03 20:40:36 - Output Space Entropy Search Framework for Multi-Objective Bayesian Optimization</summary>

- *Syrine Belakaria, Aryan Deshwal, Janardhan Rao Doppa*

- `2110.06980v2` - [abs](http://arxiv.org/abs/2110.06980v2) - [pdf](http://arxiv.org/pdf/2110.06980v2)

> We consider the problem of black-box multi-objective optimization (MOO) using expensive function evaluations (also referred to as experiments), where the goal is to approximate the true Pareto set of solutions by minimizing the total resource cost of experiments. For example, in hardware design optimization, we need to find the designs that trade-off performance, energy, and area overhead using expensive computational simulations. The key challenge is to select the sequence of experiments to uncover high-quality solutions using minimal resources. In this paper, we propose a general framework for solving MOO problems based on the principle of output space entropy (OSE) search: select the experiment that maximizes the information gained per unit resource cost about the true Pareto front. We appropriately instantiate the principle of OSE search to derive efficient algorithms for the following four MOO problem settings: 1) The most basic em single-fidelity setting, where experiments are expensive and accurate; 2) Handling em black-box constraints} which cannot be evaluated without performing experiments; 3) The discrete multi-fidelity setting, where experiments can vary in the amount of resources consumed and their evaluation accuracy; and 4) The em continuous-fidelity setting, where continuous function approximations result in a huge space of experiments. Experiments on diverse synthetic and real-world benchmarks show that our OSE search based algorithms improve over state-of-the-art methods in terms of both computational-efficiency and accuracy of MOO solutions.

</details>

<details>

<summary>2021-11-04 04:48:17 - Probability Paths and the Structure of Predictions over Time</summary>

- *Zhiyuan Jerry Lin, Hao Sheng, Sharad Goel*

- `2106.06515v2` - [abs](http://arxiv.org/abs/2106.06515v2) - [pdf](http://arxiv.org/pdf/2106.06515v2)

> In settings ranging from weather forecasts to political prognostications to financial projections, probability estimates of future binary outcomes often evolve over time. For example, the estimated likelihood of rain on a specific day changes by the hour as new information becomes available. Given a collection of such probability paths, we introduce a Bayesian framework -- which we call the Gaussian latent information martingale, or GLIM -- for modeling the structure of dynamic predictions over time. Suppose, for example, that the likelihood of rain in a week is 50 %, and consider two hypothetical scenarios. In the first, one expects the forecast to be equally likely to become either 25 % or 75 % tomorrow; in the second, one expects the forecast to stay constant for the next several days. A time-sensitive decision-maker might select a course of action immediately in the latter scenario, but may postpone their decision in the former, knowing that new information is imminent. We model these trajectories by assuming predictions update according to a latent process of information flow, which is inferred from historical data. In contrast to general methods for time series analysis, this approach preserves important properties of probability paths such as the martingale structure and appropriate amount of volatility and better quantifies future uncertainties around probability paths. We show that GLIM outperforms three popular baseline methods, producing better estimated posterior probability path distributions measured by three different metrics. By elucidating the dynamic structure of predictions over time, we hope to help individuals make more informed choices.

</details>

<details>

<summary>2021-11-04 12:50:22 - Mixed Models and Shrinkage Estimation for Balanced and Unbalanced Designs</summary>

- *Yihan Bao, James G. Booth*

- `2111.02829v1` - [abs](http://arxiv.org/abs/2111.02829v1) - [pdf](http://arxiv.org/pdf/2111.02829v1)

> The known connection between shrinkage estimation, empirical Bayes, and mixed effects models is explored and applied to balanced and unbalanced designs in which the responses are correlated. As an illustration, a mixed model is proposed for predicting the outcome of English Premier League games that takes into account both home and away team effects. Results based on empirical best linear unbiased predictors obtained from fitting mixed linear models are compared with fully Bayesian predictors that utilize prior information from the previous season.

</details>

<details>

<summary>2021-11-04 13:01:20 - Adversarial Attacks on Graph Classification via Bayesian Optimisation</summary>

- *Xingchen Wan, Henry Kenlay, Binxin Ru, Arno Blaas, Michael A. Osborne, Xiaowen Dong*

- `2111.02842v1` - [abs](http://arxiv.org/abs/2111.02842v1) - [pdf](http://arxiv.org/pdf/2111.02842v1)

> Graph neural networks, a popular class of models effective in a wide range of graph-based learning tasks, have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to analysing adversarial attacks on graph-level classification, an important problem with numerous real-life applications such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method for graph classification models. Our method is black-box, query-efficient and parsimonious with respect to the perturbation applied. We empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. Finally, we analyse common interpretable patterns behind the adversarial samples produced, which may shed further light on the adversarial robustness of graph classification models.

</details>

<details>

<summary>2021-11-04 19:42:54 - Revisiting the Effects of Stochasticity for Hamiltonian Samplers</summary>

- *Giulio Franzese, Dimitrios Milios, Maurizio Filippone, Pietro Michiardi*

- `2106.16200v2` - [abs](http://arxiv.org/abs/2106.16200v2) - [pdf](http://arxiv.org/pdf/2106.16200v2)

> We revisit the theoretical properties of Hamiltonian stochastic differential equations (SDES) for Bayesian posterior sampling, and we study the two types of errors that arise from numerical SDE simulation: the discretization error and the error due to noisy gradient estimates in the context of data subsampling. Our main result is a novel analysis for the effect of mini-batches through the lens of differential operator splitting, revising previous literature results. The stochastic component of a Hamiltonian SDE is decoupled from the gradient noise, for which we make no normality assumptions. This leads to the identification of a convergence bottleneck: when considering mini-batches, the best achievable error rate is $\mathcal{O}(\eta^2)$, with $\eta$ being the integrator step size. Our theoretical results are supported by an empirical study on a variety of regression and classification tasks for Bayesian neural networks.

</details>

<details>

<summary>2021-11-05 01:02:57 - A Probabilistic Domain-knowledge Framework for Nosocomial Infection Risk Estimation of Communicable Viral Diseases in Healthcare Personnel: A Case Study for COVID-19</summary>

- *Phat K. Huynh, Arveity R. Setty, Om P. Yadav, Trung Q. Le*

- `2111.05761v1` - [abs](http://arxiv.org/abs/2111.05761v1) - [pdf](http://arxiv.org/pdf/2111.05761v1)

> Hospital-acquired infections of communicable viral diseases (CVDs) are posing a tremendous challenge to healthcare workers globally. Healthcare personnel (HCP) is facing a consistent risk of hospital-acquired infections, and subsequently higher rates of morbidity and mortality. We proposed a domain knowledge-driven infection risk model to quantify the individual HCP and the population-level healthcare facility risks. For individual-level risk estimation, a time-variant infection risk model is proposed to capture the transmission dynamics of CVDs. At the population-level, the infection risk is estimated using a Bayesian network model constructed from three feature sets including individual-level factors, engineering control factors, and administrative control factors. The sensitivity analyses indicated that the uncertainty in the individual infection risk can be attributed to two variables: the number of close contacts and the viral transmission probability. The model validation was implemented in the transmission probability model, individual level risk model, and population-level risk model using a Coronavirus disease case study. Regarding the first, multivariate logistic regression was applied for a cross-sectional data in the UK with an AIC value of 7317.70 and a 10-fold cross validation accuracy of 78.23%. For the second model, we collected laboratory-confirmed COVID-19 cases of HCP in different occupations. The occupation-specific risk evaluation suggested the highest-risk occupations were registered nurses, medical assistants, and respiratory therapists, with estimated risks of 0.0189, 0.0188, and 0.0176, respectively. To validate the population-level risk model, the infection risk in Texas and California was estimated. The proposed model will significantly influence the PPE allocation and safety plans for HCP

</details>

<details>

<summary>2021-11-05 11:05:55 - On the relevance of prognostic information for clinical trials: A theoretical quantification</summary>

- *Sandra Siegfried, Stephen Senn, Torsten Hothorn*

- `2111.03391v1` - [abs](http://arxiv.org/abs/2111.03391v1) - [pdf](http://arxiv.org/pdf/2111.03391v1)

> The question of how individual patient data from cohort studies or historical clinical trials can be leveraged for designing more powerful, or smaller yet equally powerful, clinical trials becomes increasingly important in the era of digitalisation. Today, the traditional statistical analyses approaches may seem questionable to practitioners in light of ubiquitous historical covariate information.   Several methodological developments aim at incorporating historical information in the design and analysis of future clinical trials, most importantly Bayesian information borrowing, propensity score methods, stratification, and covariate adjustment. Recently, adjusting the analysis with respect to a prognostic score, which was obtained from some machine learning procedure applied to historical data, has been suggested and we study the potential of this approach for randomised clinical trials.   In an idealised situation of a normal outcome in a two-arm trial with 1:1 allocation, we derive a simple sample size reduction formula as a function of two criteria characterising the prognostic score: (1) The coefficient of determination $R^2$ on historical data and (2) the correlation $\rho$ between the estimated and the true unknown prognostic scores. While maintaining the same power, the original total sample size $n$ planned for the unadjusted analysis reduces to $(1 - R^2 \rho^2) \times n$ in an adjusted analysis. Robustness in less ideal situations was assessed empirically. We conclude that there is potential for substantially more powerful or smaller trials, but only when prognostic scores can be accurately estimated.

</details>

<details>

<summary>2021-11-05 12:09:46 - Contextual Bayesian optimization with binary outputs</summary>

- *Tristan Fauvel, Matthew Chalk*

- `2111.03447v1` - [abs](http://arxiv.org/abs/2111.03447v1) - [pdf](http://arxiv.org/pdf/2111.03447v1)

> Bayesian optimization (BO) is an efficient method to optimize expensive black-box functions. It has been generalized to scenarios where objective function evaluations return stochastic binary feedback, such as success/failure in a given test, or preference between different parameter settings. In many real-world situations, the objective function can be evaluated in controlled 'contexts' or 'environments' that directly influence the observations. For example, one could directly alter the 'difficulty' of the test that is used to evaluate a system's performance. With binary feedback, the context determines the information obtained from each observation. For example, if the test is too easy/hard, the system will always succeed/fail, yielding uninformative binary outputs. Here we combine ideas from Bayesian active learning and optimization to efficiently choose the best context and optimization parameter on each iteration. We demonstrate the performance of our algorithm and illustrate how it can be used to tackle a concrete application in visual psychophysics: efficiently improving patients' vision via corrective lenses, using psychophysics measurements.

</details>

<details>

<summary>2021-11-05 12:55:44 - Bayesian Model Calibration and Sensitivity Analysis for Oscillating Biological Experiments</summary>

- *Youngdeok Hwang, Hang J. Kim, Won Chang, Christian Hong, Steve N. MacEachern*

- `2110.10604v2` - [abs](http://arxiv.org/abs/2110.10604v2) - [pdf](http://arxiv.org/pdf/2110.10604v2)

> Most organisms exhibit various endogenous oscillating behaviors which provide crucial information as to how the internal biochemical processes are connected and regulated. Understanding the molecular mechanisms behind these oscillators requires interdisciplinary efforts combining both biological and computer experiments, as the latter can complement the former by simulating perturbed conditions with higher resolution. Harmonizing the two types of experiment, however, poses significant statistical challenges due to identifiability issues, numerical instability, and ill behavior in high dimension. This article devises a new Bayesian calibration framework for oscillating biochemical models. The proposed Bayesian model is estimated using an advanced MCMC which can efficiently infer the parameter values that match the simulated and observed oscillatory processes. Also proposed is an approach to sensitivity analysis approach based on the intervention posterior. This approach measures the influence of individual parameters on the target process by utilizing the obtained MCMC samples as a computational tool. The proposed framework is illustrated with circadian oscillations observed in a filamentous fungus, Neurospora crassa.

</details>

<details>

<summary>2021-11-05 15:06:05 - An Empirical Study of Neural Kernel Bandits</summary>

- *Michal Lisicki, Arash Afkanpour, Graham W. Taylor*

- `2111.03543v1` - [abs](http://arxiv.org/abs/2111.03543v1) - [pdf](http://arxiv.org/pdf/2111.03543v1)

> Neural bandits have enabled practitioners to operate efficiently on problems with non-linear reward functions. While in general contextual bandits commonly utilize Gaussian process (GP) predictive distributions for decision making, the most successful neural variants use only the last layer parameters in the derivation. Research on neural kernels (NK) has recently established a correspondence between deep networks and GPs that take into account all the parameters of a NN and can be trained more efficiently than most Bayesian NNs. We propose to directly apply NK-induced distributions to guide an upper confidence bound or Thompson sampling-based policy. We show that NK bandits achieve state-of-the-art performance on highly non-linear structured data. Furthermore, we analyze practical considerations such as training frequency and model partitioning. We believe our work will help better understand the impact of utilizing NKs in applied settings.

</details>

<details>

<summary>2021-11-05 15:52:48 - Mixtures of Laplace Approximations for Improved Post-Hoc Uncertainty in Deep Learning</summary>

- *Runa Eschenhagen, Erik Daxberger, Philipp Hennig, Agustinus Kristiadi*

- `2111.03577v1` - [abs](http://arxiv.org/abs/2111.03577v1) - [pdf](http://arxiv.org/pdf/2111.03577v1)

> Deep neural networks are prone to overconfident predictions on outliers. Bayesian neural networks and deep ensembles have both been shown to mitigate this problem to some extent. In this work, we aim to combine the benefits of the two approaches by proposing to predict with a Gaussian mixture model posterior that consists of a weighted sum of Laplace approximations of independently trained deep neural networks. The method can be used post hoc with any set of pre-trained networks and only requires a small computational and memory overhead compared to regular ensembles. We theoretically validate that our approach mitigates overconfidence "far away" from the training data and empirically compare against state-of-the-art baselines on standard uncertainty quantification benchmarks.

</details>

<details>

<summary>2021-11-05 18:31:14 - The Ball Pit Algorithm: A Markov Chain Monte Carlo Method Based on Path Integrals</summary>

- *Miguel Fudolig, Reka Howard*

- `2111.03691v1` - [abs](http://arxiv.org/abs/2111.03691v1) - [pdf](http://arxiv.org/pdf/2111.03691v1)

> The Ball Pit Algorithm (BPA) is a novel Markov chain Monte Carlo (MCMC) algorithm for sampling marginal posterior distributions developed from the path integral formulation of the Bayesian analysis for Markov chains. The BPA yielded comparable results to the Hamiltonian Monte Carlo as implemented by the adaptive No U-Turn Sampler (NUTS) in sampling posterior distributions for simulated data from Bernoulli and Poisson likelihoods. One major advantage of the BPA is its significantly lower computational time, which was measured to be at least 95% faster than NUTS in analyzing single parameter models. The BPA was also applied to a multi-parameter Cauchy model using real data of the height differences of cross- and self-fertilized plants. The posterior medians for the location parameter were consistent with other Bayesian sampling methods. Additionally, the posterior median for the logarithm of the scale parameter obtained from the BPA was close to the estimated posterior median calculated using the Laplace normal approximation. The computational time of the BPA implementation of the Cauchy analysis is 55% faster compared to that for NUTS. Overall, we have found that the BPA is a highly efficient alternative to the Hamiltonian Monte Carlo and other standard MCMC methods.

</details>

<details>

<summary>2021-11-06 15:18:48 - In Nonparametric and High-Dimensional Models, Bayesian Ignorability is an Informative Prior</summary>

- *Antonio R. Linero*

- `2111.05137v1` - [abs](http://arxiv.org/abs/2111.05137v1) - [pdf](http://arxiv.org/pdf/2111.05137v1)

> In problems with large amounts of missing data one must model two distinct data generating processes: the outcome process which generates the response and the missing data mechanism which determines the data we observe. Under the ignorability condition of Rubin (1976), however, likelihood-based inference for the outcome process does not depend on the missing data mechanism so that only the former needs to be estimated; partially because of this simplification, ignorability is often used as a baseline assumption. We study the implications of Bayesian ignorability in the presence of high-dimensional nuisance parameters and argue that ignorability is typically incompatible with sensible prior beliefs about the amount of selection bias. We show that, for many problems, ignorability directly implies that the prior on the selection bias is tightly concentrated around zero. This is demonstrated on several models of practical interest, and the effect of ignorability on the posterior distribution is characterized for high-dimensional linear models with a ridge regression prior. We then show both how to build high-dimensional models which encode sensible beliefs about the selection bias and also show that under certain narrow circumstances ignorability is less problematic.

</details>

<details>

<summary>2021-11-06 18:35:28 - Reliable Post hoc Explanations: Modeling Uncertainty in Explainability</summary>

- *Dylan Slack, Sophie Hilgard, Sameer Singh, Himabindu Lakkaraju*

- `2008.05030v4` - [abs](http://arxiv.org/abs/2008.05030v4) - [pdf](http://arxiv.org/pdf/2008.05030v4)

> As black box explanations are increasingly being employed to establish model credibility in high-stakes settings, it is important to ensure that these explanations are accurate and reliable. However, prior work demonstrates that explanations generated by state-of-the-art techniques are inconsistent, unstable, and provide very little insight into their correctness and reliability. In addition, these methods are also computationally inefficient, and require significant hyper-parameter tuning. In this paper, we address the aforementioned challenges by developing a novel Bayesian framework for generating local explanations along with their associated uncertainty. We instantiate this framework to obtain Bayesian versions of LIME and KernelSHAP which output credible intervals for the feature importances, capturing the associated uncertainty. The resulting explanations not only enable us to make concrete inferences about their quality (e.g., there is a 95% chance that the feature importance lies within the given range), but are also highly consistent and stable. We carry out a detailed theoretical analysis that leverages the aforementioned uncertainty to estimate how many perturbations to sample, and how to sample for faster convergence. This work makes the first attempt at addressing several critical issues with popular explanation methods in one shot, thereby generating consistent, stable, and reliable explanations with guarantees in a computationally efficient manner. Experimental evaluation with multiple real world datasets and user studies demonstrate that the efficacy of the proposed framework.

</details>

<details>

<summary>2021-11-08 10:04:56 - A Bayesian Analysis of Migration Pathways using Chain Event Graphs of Agent Based Models</summary>

- *Peter Strong, Alys McAlpine, Jim Q Smith*

- `2111.04368v1` - [abs](http://arxiv.org/abs/2111.04368v1) - [pdf](http://arxiv.org/pdf/2111.04368v1)

> Agent-Based Models (ABMs) are often used to model migration and are increasingly used to simulate individual migrant decision-making and unfolding events through a sequence of heuristic if-then rules. However, ABMs lack the methods to embed more principled strategies of performing inference to estimate and validate the models, both of which are of significant importance for real-world case studies. Chain Event Graphs (CEGs) can fill this need: they can be used to provide a Bayesian framework which represents an ABM accurately. Through the use of the CEG, we illustrate how to transform an elicited ABM into a Bayesian framework and outline the benefits of this approach.

</details>

<details>

<summary>2021-11-08 11:27:22 - Topological Approximate Bayesian Computation for Parameter Inference of an Angiogenesis Model</summary>

- *Thomas Thorne, Paul D. W. Kirk, Heather A. Harrington*

- `2108.11640v2` - [abs](http://arxiv.org/abs/2108.11640v2) - [pdf](http://arxiv.org/pdf/2108.11640v2)

> Inferring the parameters of models describing biological systems is an important problem in the reverse engineering of the mechanisms underlying these systems. Much work has focused on parameter inference of stochastic and ordinary differential equation models using Approximate Bayesian Computation (ABC). While there is some recent work on inference in spatial models, this remains an open problem. Simultaneously, advances in topological data analysis (TDA), a field of computational mathematics, have enabled spatial patterns in data to be characterised. Here we focus on recent work using topological data analysis to study different regimes of parameter space for a well-studied model of angiogenesis. We propose a method for combining TDA with ABC to infer parameters in the Anderson-Chaplain model of angiogenesis. We demonstrate that this topological approach outperforms ABC approaches that use simpler statistics based on spatial features of the data. This is a first step towards a general framework of spatial parameter inference for biological systems, for which there may be a variety of filtrations, vectorisations, and summary statistics to be considered. All code used to produce our results is available as a Snakemake workflow.

</details>

<details>

<summary>2021-11-08 14:04:19 - Bayesian profile regression for clustering analysis involving a longitudinal response and explanatory variables</summary>

- *Anaïs Rouanet, Rob Johnson, Magdalena E Strauss, Sylvia Richardson, Brian D Tom, Simon R White, Paul D W Kirk*

- `2111.04518v1` - [abs](http://arxiv.org/abs/2111.04518v1) - [pdf](http://arxiv.org/pdf/2111.04518v1)

> The identification of sets of co-regulated genes that share a common function is a key question of modern genomics. Bayesian profile regression is a semi-supervised mixture modelling approach that makes use of a response to guide inference toward relevant clusterings. Previous applications of profile regression have considered univariate continuous, categorical, and count outcomes. In this work, we extend Bayesian profile regression to cases where the outcome is longitudinal (or multivariate continuous) and provide PReMiuMlongi, an updated version of PReMiuM, the R package for profile regression. We consider multivariate normal and Gaussian process regression response models and provide proof of principle applications to four simulation studies. The model is applied on budding yeast data to identify groups of genes co-regulated during the Saccharomyces cerevisiae cell cycle. We identify 4 distinct groups of genes associated with specific patterns of gene expression trajectories, along with the bound transcriptional factors, likely involved in their co-regulation process.

</details>

<details>

<summary>2021-11-08 16:37:49 - Streaming Variational Monte Carlo</summary>

- *Yuan Zhao, Josue Nassar, Ian Jordan, Mónica Bugallo, Il Memming Park*

- `1906.01549v4` - [abs](http://arxiv.org/abs/1906.01549v4) - [pdf](http://arxiv.org/pdf/1906.01549v4)

> Nonlinear state-space models are powerful tools to describe dynamical structures in complex time series. In a streaming setting where data are processed one sample at a time, simultaneous inference of the state and its nonlinear dynamics has posed significant challenges in practice. We develop a novel online learning framework, leveraging variational inference and sequential Monte Carlo, which enables flexible and accurate Bayesian joint filtering. Our method provides an approximation of the filtering posterior which can be made arbitrarily close to the true filtering distribution for a wide class of dynamics models and observation models. Specifically, the proposed framework can efficiently approximate a posterior over the dynamics using sparse Gaussian processes, allowing for an interpretable model of the latent dynamics. Constant time complexity per sample makes our approach amenable to online learning scenarios and suitable for real-time applications.

</details>

<details>

<summary>2021-11-08 16:57:29 - Procurements with Bidder Asymmetry in Cost and Risk-Aversion</summary>

- *Gaurab Aryal, Hanna Charankevich, Seungwon Jeong, Dong-Hyuk Kim*

- `2111.04626v1` - [abs](http://arxiv.org/abs/2111.04626v1) - [pdf](http://arxiv.org/pdf/2111.04626v1)

> We propose an empirical method to analyze data from first-price procurements where bidders are asymmetric in their risk-aversion (CRRA) coefficients and distributions of private costs. Our Bayesian approach evaluates the likelihood by solving type-symmetric equilibria using the boundary-value method and integrates out unobserved heterogeneity through data augmentation. We study a new dataset from Russian government procurements focusing on the category of printing papers. We find that there is no unobserved heterogeneity (presumably because the job is routine), but bidders are highly asymmetric in their cost and risk-aversion. Our counterfactual study shows that choosing a type-specific cost-minimizing reserve price marginally reduces the procurement cost; however, inviting one more bidder substantially reduces the cost, by at least 5.5%. Furthermore, incorrectly imposing risk-neutrality would severely mislead inference and policy recommendations, but the bias from imposing homogeneity in risk-aversion is small.

</details>

<details>

<summary>2021-11-08 17:15:38 - Bayesian modelling of statistical region- and family-level clustered ordinal outcome data from Turkey</summary>

- *Ozgur Asar*

- `2111.04645v1` - [abs](http://arxiv.org/abs/2111.04645v1) - [pdf](http://arxiv.org/pdf/2111.04645v1)

> This study is concerned with the analysis of three-level ordinal outcome data with polytomous logistic regression in the presence of random-effects. It is assumed that the random-effects follow a Bridge distribution for the logit link, which allows one to obtain marginal interpretations of the regression coefficients. The data are obtained from the Turkish Income and Living Conditions Study, where the outcome variable is self-rated health (SRH), which is ordinal in nature. The analysis of these data is to compare covariate sub-groups and draw region- and family-level inferences in terms of SRH. Parameters and random-effects are sampled from the joint posterior densities following a Bayesian paradigm. Three criteria are used for model selection: Watenable information criterion, log pseudo marginal likelihood, and deviance information criterion. All three suggest that we need to account for both region- and family-level variabilities in order to model SRH. The extent to which the models replicate the observed data is examined by posterior predictive checks. Differences in SRH are found between levels of economic and demographic variables, regions of Turkey, and families who participated in the survey. Some of the interesting findings are that unemployed people are 19% more likely to report poorer health than employed people, and rural Aegean is the region that has the least probability of reporting poorer health.

</details>

<details>

<summary>2021-11-08 17:38:29 - Approximate Neural Architecture Search via Operation Distribution Learning</summary>

- *Xingchen Wan, Binxin Ru, Pedro M. Esperança, Fabio M. Carlucci*

- `2111.04670v1` - [abs](http://arxiv.org/abs/2111.04670v1) - [pdf](http://arxiv.org/pdf/2111.04670v1)

> The standard paradigm in Neural Architecture Search (NAS) is to search for a fully deterministic architecture with specific operations and connections. In this work, we instead propose to search for the optimal operation distribution, thus providing a stochastic and approximate solution, which can be used to sample architectures of arbitrary length. We propose and show, that given an architectural cell, its performance largely depends on the ratio of used operations, rather than any specific connection pattern in typical search spaces; that is, small changes in the ordering of the operations are often irrelevant. This intuition is orthogonal to any specific search strategy and can be applied to a diverse set of NAS algorithms. Through extensive validation on 4 data-sets and 4 NAS techniques (Bayesian optimisation, differentiable search, local search and random search), we show that the operation distribution (1) holds enough discriminating power to reliably identify a solution and (2) is significantly easier to optimise than traditional encodings, leading to large speed-ups at little to no cost in performance. Indeed, this simple intuition significantly reduces the cost of current approaches and potentially enable NAS to be used in a broader range of applications.

</details>

<details>

<summary>2021-11-09 02:50:51 - User-friendly introduction to PAC-Bayes bounds</summary>

- *Pierre Alquier*

- `2110.11216v4` - [abs](http://arxiv.org/abs/2110.11216v4) - [pdf](http://arxiv.org/pdf/2110.11216v4)

> Aggregated predictors are obtained by making a set of basic predictors vote according to some weights, that is, to some probability distribution.   Randomized predictors are obtained by sampling in a set of basic predictors, according to some prescribed probability distribution.   Thus, aggregated and randomized predictors have in common that they are not defined by a minimization problem, but by a probability distribution on the set of predictors. In statistical learning theory, there is a set of tools designed to understand the generalization ability of such procedures: PAC-Bayesian or PAC-Bayes bounds.   Since the original PAC-Bayes bounds of D. McAllester, these tools have been considerably improved in many directions (we will for example describe a simplified version of the localization technique of O. Catoni that was missed by the community, and later rediscovered as "mutual information bounds"). Very recently, PAC-Bayes bounds received a considerable attention: for example there was workshop on PAC-Bayes at NIPS 2017, "(Almost) 50 Shades of Bayesian Learning: PAC-Bayesian trends and insights", organized by B. Guedj, F. Bach and P. Germain. One of the reason of this recent success is the successful application of these bounds to neural networks by G. Dziugaite and D. Roy.   An elementary introduction to PAC-Bayes theory is still missing. This is an attempt to provide such an introduction.

</details>

<details>

<summary>2021-11-09 03:25:15 - Optimizing Bayesian acquisition functions in Gaussian Processes</summary>

- *Ashish Anil Pawar, Ujwal Warbhe*

- `2111.04930v1` - [abs](http://arxiv.org/abs/2111.04930v1) - [pdf](http://arxiv.org/pdf/2111.04930v1)

> Bayesian Optimization is an effective method for searching the global maxima of an objective function especially if the function is unknown. The process comprises of using a surrogate function and choosing an acquisition function followed by optimizing the acquisition function to find the next sampling point. This paper analyzes different acquistion functions like Maximum Probability of Improvement and Expected Improvement and various optimizers like L-BFGS and TNC to optimize the acquisitions functions for finding the next sampling point. Along with the analysis of time taken, the paper also shows the importance of position of initial samples chosen.

</details>

<details>

<summary>2021-11-09 08:09:48 - Ensemble Kalman Variational Objectives: Nonlinear Latent Trajectory Inference with A Hybrid of Variational Inference and Ensemble Kalman Filter</summary>

- *Tsuyoshi Ishizone, Tomoyuki Higuchi, Kazuyuki Nakamura*

- `2010.08729v2` - [abs](http://arxiv.org/abs/2010.08729v2) - [pdf](http://arxiv.org/pdf/2010.08729v2)

> Variational inference (VI) combined with Bayesian nonlinear filtering produces state-of-the-art results for latent time-series modeling. A body of recent work has focused on sequential Monte Carlo (SMC) and its variants, e.g., forward filtering backward simulation (FFBSi). Although these studies have succeeded, serious problems remain in particle degeneracy and biased gradient estimators. In this paper, we propose Ensemble Kalman Variational Objective (EnKO), a hybrid method of VI and the ensemble Kalman filter (EnKF), to infer state space models (SSMs). Our proposed method can efficiently identify latent dynamics because of its particle diversity and unbiased gradient estimators. We demonstrate that our EnKO outperforms SMC-based methods in terms of predictive ability and particle efficiency for three benchmark nonlinear system identification tasks.

</details>

<details>

<summary>2021-11-09 08:09:50 - Efficient Sampling and Structure Learning of Bayesian Networks</summary>

- *Jack Kuipers, Polina Suter, Giusi Moffa*

- `1803.07859v4` - [abs](http://arxiv.org/abs/1803.07859v4) - [pdf](http://arxiv.org/pdf/1803.07859v4)

> Bayesian networks are probabilistic graphical models widely employed to understand dependencies in high dimensional data, and even to facilitate causal discovery. Learning the underlying network structure, which is encoded as a directed acyclic graph (DAG) is highly challenging mainly due to the vast number of possible networks in combination with the acyclicity constraint. Efforts have focussed on two fronts: constraint-based methods that perform conditional independence tests to exclude edges and score and search approaches which explore the DAG space with greedy or MCMC schemes. Here we synthesise these two fields in a novel hybrid method which reduces the complexity of MCMC approaches to that of a constraint-based method. Individual steps in the MCMC scheme only require simple table lookups so that very long chains can be efficiently obtained. Furthermore, the scheme includes an iterative procedure to correct for errors from the conditional independence tests. The algorithm offers markedly superior performance to alternatives, particularly because DAGs can also be sampled from the posterior distribution, enabling full Bayesian model averaging for much larger Bayesian networks.

</details>

<details>

<summary>2021-11-09 11:11:26 - Changepoint detection in non-exchangeable data</summary>

- *Karl L. Hallgren, Nicholas A. Heard, Niall M. Adams*

- `2111.05054v1` - [abs](http://arxiv.org/abs/2111.05054v1) - [pdf](http://arxiv.org/pdf/2111.05054v1)

> Changepoint models typically assume the data within each segment are independent and identically distributed conditional on some parameters which change across segments. This construction may be inadequate when data are subject to local correlation patterns, often resulting in many more changepoints fitted than preferable. This article proposes a Bayesian changepoint model which relaxes the assumption of exchangeability within segments. The proposed model supposes data within a segment are $m$-dependent for some unkown $m \geqslant0$ which may vary between segments, resulting in a model suitable for detecting clear discontinuities in data which are subject to different local temporal correlations. The approach is suited to both continuous and discrete data. A novel reversible jump MCMC algorithm is proposed to sample from the model; in particular, a detailed analysis of the parameter space is exploited to build proposals for the orders of dependence. Two applications demonstrate the benefits of the proposed model: computer network monitoring via change detection in count data, and segmentation of financial time series.

</details>

<details>

<summary>2021-11-09 15:05:14 - Arbitrary Marginal Neural Ratio Estimation for Simulation-based Inference</summary>

- *François Rozet, Gilles Louppe*

- `2110.00449v3` - [abs](http://arxiv.org/abs/2110.00449v3) - [pdf](http://arxiv.org/pdf/2110.00449v3)

> In many areas of science, complex phenomena are modeled by stochastic parametric simulators, often featuring high-dimensional parameter spaces and intractable likelihoods. In this context, performing Bayesian inference can be challenging. In this work, we present a novel method that enables amortized inference over arbitrary subsets of the parameters, without resorting to numerical integration, which makes interpretation of the posterior more convenient. Our method is efficient and can be implemented with arbitrary neural network architectures. We demonstrate the applicability of the method on parameter inference of binary black hole systems from gravitational waves observations.

</details>

<details>

<summary>2021-11-09 18:25:38 - HNPE: Leveraging Global Parameters for Neural Posterior Estimation</summary>

- *Pedro L. C. Rodrigues, Thomas Moreau, Gilles Louppe, Alexandre Gramfort*

- `2102.06477v3` - [abs](http://arxiv.org/abs/2102.06477v3) - [pdf](http://arxiv.org/pdf/2102.06477v3)

> Inferring the parameters of a stochastic model based on experimental observations is central to the scientific method. A particularly challenging setting is when the model is strongly indeterminate, i.e. when distinct sets of parameters yield identical observations. This arises in many practical situations, such as when inferring the distance and power of a radio source (is the source close and weak or far and strong?) or when estimating the amplifier gain and underlying brain activity of an electrophysiological experiment. In this work, we present hierarchical neural posterior estimation (HNPE), a novel method for cracking such indeterminacy by exploiting additional information conveyed by an auxiliary set of observations sharing global parameters. Our method extends recent developments in simulation-based inference (SBI) based on normalizing flows to Bayesian hierarchical models. We validate quantitatively our proposal on a motivating example amenable to analytical solutions and then apply it to invert a well known non-linear model from computational neuroscience, using both simulated and real EEG data.

</details>

<details>

<summary>2021-11-09 19:20:58 - Probabilistic predictions of SIS epidemics on networks based on population-level observations</summary>

- *Tanja Zerenner, Francesco Di Lauro, Masoumeh Dashti, Luc Berthouze, Istvan Z. Kiss*

- `2111.05369v1` - [abs](http://arxiv.org/abs/2111.05369v1) - [pdf](http://arxiv.org/pdf/2111.05369v1)

> We predict the future course of ongoing susceptible-infected-susceptible (SIS) epidemics on regular, Erd\H{o}s-R\'{e}nyi and Barab\'asi-Albert networks. It is known that the contact network influences the spread of an epidemic within a population. Therefore, observations of an epidemic, in this case at the population-level, contain information about the underlying network. This information, in turn, is useful for predicting the future course of an ongoing epidemic. To exploit this in a prediction framework, the exact high-dimensional stochastic model of an SIS epidemic on a network is approximated by a lower-dimensional surrogate model. The surrogate model is based on a birth-and-death process; the effect of the underlying network is described by a parametric model for the birth rates. We demonstrate empirically that the surrogate model captures the intrinsic stochasticity of the epidemic once it reaches a point from which it will not die out. Bayesian parameter inference allows for uncertainty about the model parameters and the class of the underlying network to be incorporated directly into probabilistic predictions. An evaluation of a number of scenarios shows that in most cases the resulting prediction intervals adequately quantify the prediction uncertainty. As long as the population-level data is available over a long-enough period, even if not sampled frequently, the model leads to excellent predictions where the underlying network is correctly identified and prediction uncertainty mainly reflects the intrinsic stochasticity of the spreading epidemic. For predictions inferred from shorter observational periods, uncertainty about parameters and network class dominate prediction uncertainty. The proposed method relies on minimal data and is numerically efficient, which makes it attractive either as a standalone inference and prediction scheme or in conjunction with other methods.

</details>

<details>

<summary>2021-11-09 19:31:04 - Function-on-function linear quantile regression</summary>

- *Ufuk Beyaztas, Han Lin Shang*

- `2111.05374v1` - [abs](http://arxiv.org/abs/2111.05374v1) - [pdf](http://arxiv.org/pdf/2111.05374v1)

> In this study, we propose a function-on-function linear quantile regression model that allows for more than one functional predictor to establish a more flexible and robust approach. The proposed model is first transformed into a finite-dimensional space via the functional principal component analysis paradigm in the estimation phase. It is then approximated using the estimated functional principal component functions, and the estimated parameter of the quantile regression model is constructed based on the principal component scores. In addition, we propose a Bayesian information criterion to determine the optimum number of truncation constants used in the functional principal component decomposition. Moreover, a stepwise forward procedure and the Bayesian information criterion are used to determine the significant predictors for including in the model. We employ a nonparametric bootstrap procedure to construct prediction intervals for the response functions. The finite sample performance of the proposed method is evaluated via several Monte Carlo experiments and an empirical data example, and the results produced by the proposed method are compared with the ones from existing models.

</details>

<details>

<summary>2021-11-09 22:17:44 - Stochastic Approximation Cut Algorithm for Inference in Modularized Bayesian Models</summary>

- *Yang Liu, Robert J. B. Goudie*

- `2006.01584v4` - [abs](http://arxiv.org/abs/2006.01584v4) - [pdf](http://arxiv.org/pdf/2006.01584v4)

> Bayesian modelling enables us to accommodate complex forms of data and make a comprehensive inference, but the effect of partial misspecification of the model is a concern. One approach in this setting is to modularize the model, and prevent feedback from suspect modules, using a cut model. After observing data, this leads to the cut distribution which normally does not have a closed-form. Previous studies have proposed algorithms to sample from this distribution, but these algorithms have unclear theoretical convergence properties. To address this, we propose a new algorithm called the Stochastic Approximation Cut algorithm (SACut) as an alternative. The algorithm is divided into two parallel chains. The main chain targets an approximation to the cut distribution; the auxiliary chain is used to form an adaptive proposal distribution for the main chain. We prove convergence of the samples drawn by the proposed algorithm and present the exact limit. Although SACut is biased, since the main chain does not target the exact cut distribution, we prove this bias can be reduced geometrically by increasing a user-chosen tuning parameter. In addition, parallel computing can be easily adopted for SACut, which greatly reduces computation time.

</details>

<details>

<summary>2021-11-10 10:47:14 - Comparing dominance of tennis' big three via multiple-output Bayesian quantile regression models</summary>

- *Bruno Santos*

- `2111.05631v1` - [abs](http://arxiv.org/abs/2111.05631v1) - [pdf](http://arxiv.org/pdf/2111.05631v1)

> Tennis has seen a myriad of great male tennis players throughout its history and we are often interested in the discussion of who is/was the greatest player of all time. While we do not try to answer this question here, we delve into comparing some key statistics related to dominance over their opponents for the male players with the most Grand Slam titles, currently: Djokovic, Federer and Nadal, in alphabetical order. Here we consider the minutes played and the relative points in each of their completed matches, as a measure of dominance against other players. We consider important covariates such as surface, win or loss, type of tournament and whether their opponent was a top 20 ranked player in the world or not, to create a more complete comparison of their performance. We consider a Bayesian quantile regression model for multiple-output response variables to take into account the dependence between minutes and relative points won. This approach is compelling since we do not need to choose a probability distribution for the joint probability distribution of our response variable. Our results agree with the common intuition of Nadal's superiority in clay courts, Federer's superiority in grass courts and Djokovic's superiority in hard courts given their success in each of these surfaces; though Nadal's dominance in clay court games is unique. Federer shows his dominance regarding minutes spent in the court in wins, while Djokovic takes the edge when considering the dimension of relative points won, for most of the comparisons. While minutes can be directly connected to style of play, the relative points dimension could express more directly different levels of advantage over their opponent, in which Djokovic seems to be the overall leader in this analysis.

</details>

<details>

<summary>2021-11-10 16:35:03 - Patch-Based Image Restoration using Expectation Propagation</summary>

- *Dan Yao, Stephen McLaughlin, Yoann Altmann*

- `2106.15327v2` - [abs](http://arxiv.org/abs/2106.15327v2) - [pdf](http://arxiv.org/pdf/2106.15327v2)

> This paper presents a new Expectation Propagation (EP) framework for image restoration using patch-based prior distributions. While Monte Carlo techniques are classically used to sample from intractable posterior distributions, they can suffer from scalability issues in high-dimensional inference problems such as image restoration. To address this issue, EP is used here to approximate the posterior distributions using products of multivariate Gaussian densities. Moreover, imposing structural constraints on the covariance matrices of these densities allows for greater scalability and distributed computation. While the method is naturally suited to handle additive Gaussian observation noise, it can also be extended to non-Gaussian noise. Experiments conducted for denoising, inpainting and deconvolution problems with Gaussian and Poisson noise illustrate the potential benefits of such flexible approximate Bayesian method for uncertainty quantification in imaging problems, at a reduced computational cost compared to sampling techniques.

</details>

<details>

<summary>2021-11-10 18:25:10 - Searching in the Forest for Local Bayesian Optimization</summary>

- *Difan Deng, Marius Lindauer*

- `2111.05834v1` - [abs](http://arxiv.org/abs/2111.05834v1) - [pdf](http://arxiv.org/pdf/2111.05834v1)

> Because of its sample efficiency, Bayesian optimization (BO) has become a popular approach dealing with expensive black-box optimization problems, such as hyperparameter optimization (HPO). Recent empirical experiments showed that the loss landscapes of HPO problems tend to be more benign than previously assumed, i.e. in the best case uni-modal and convex, such that a BO framework could be more efficient if it can focus on those promising local regions. In this paper, we propose BOinG, a two-stage approach that is tailored toward mid-sized configuration spaces, as one encounters in many HPO problems. In the first stage, we build a scalable global surrogate model with a random forest to describe the overall landscape structure. Further, we choose a promising subregion via a bottom-up approach on the upper-level tree structure. In the second stage, a local model in this subregion is utilized to suggest the point to be evaluated next. Empirical experiments show that BOinG is able to exploit the structure of typical HPO problems and performs particularly well on mid-sized problems from synthetic functions and HPO.

</details>

<details>

<summary>2021-11-10 20:24:38 - Performance of Bayesian linear regression in a model with mismatch</summary>

- *Jean Barbier, Wei-Kuo Chen, Dmitry Panchenko, Manuel Sáenz*

- `2107.06936v2` - [abs](http://arxiv.org/abs/2107.06936v2) - [pdf](http://arxiv.org/pdf/2107.06936v2)

> In this paper we analyze, for a model of linear regression with gaussian covariates, the performance of a Bayesian estimator given by the mean of a log-concave posterior distribution with gaussian prior, in the high-dimensional limit where the number of samples and the covariates' dimension are large and proportional. Although the high-dimensional analysis of Bayesian estimators has been previously studied for Bayesian-optimal linear regression where the correct posterior is used for inference, much less is known when there is a mismatch. Here we consider a model in which the responses are corrupted by gaussian noise and are known to be generated as linear combinations of the covariates, but the distributions of the ground-truth regression coefficients and of the noise are unknown. This regression task can be rephrased as a statistical mechanics model known as the Gardner spin glass, an analogy which we exploit. Using a leave-one-out approach we characterize the mean-square error for the regression coefficients. We also derive the log-normalizing constant of the posterior. Similar models have been studied by Shcherbina and Tirozzi and by Talagrand, but our arguments are much more straightforward. An interesting consequence of our analysis is that in the quadratic loss case, the performance of the Bayesian estimator is independent of a global "temperature" hyperparameter and matches the ridge estimator: sampling and optimizing are equally good.

</details>

<details>

<summary>2021-11-10 23:15:20 - Posterior consistency for the spectral density of non-Gaussian stationary time series</summary>

- *Yifu Tang, Claudia Kirch, Jeong Eun Lee, Renate Meyer*

- `2103.01357v3` - [abs](http://arxiv.org/abs/2103.01357v3) - [pdf](http://arxiv.org/pdf/2103.01357v3)

> Various nonparametric approaches for Bayesian spectral density estimation of stationary time series have been suggested in the literature, mostly based on the Whittle likelihood approximation. A generalization of this approximation has been proposed in Kirch et al. who prove posterior consistency for spectral density estimation in combination with the Bernstein-Dirichlet process prior for Gaussian time series. In this paper, we will extend the posterior consistency result to non-Gaussian time series by employing a general consistency theorem of Shalizi for dependent data and misspecified models. As a special case, posterior consistency for the spectral density under the Whittle likelihood as proposed by Choudhuri, Ghosal and Roy is also extended to non-Gaussian time series. Small sample properties of this approach are illustrated with several examples of non-Gaussian time series.

</details>

<details>

<summary>2021-11-11 12:03:26 - Gaussian Processes with Differential Privacy</summary>

- *Antti Honkela, Laila Melkas*

- `2106.00474v2` - [abs](http://arxiv.org/abs/2106.00474v2) - [pdf](http://arxiv.org/pdf/2106.00474v2)

> Gaussian processes (GPs) are non-parametric Bayesian models that are widely used for diverse prediction tasks. Previous work in adding strong privacy protection to GPs via differential privacy (DP) has been limited to protecting only the privacy of the prediction targets (model outputs) but not inputs. We break this limitation by introducing GPs with DP protection for both model inputs and outputs. We achieve this by using sparse GP methodology and publishing a private variational approximation on known inducing points. The approximation covariance is adjusted to approximately account for the added uncertainty from DP noise. The approximation can be used to compute arbitrary predictions using standard sparse GP techniques. We propose a method for hyperparameter learning using a private selection protocol applied to validation set log-likelihood. Our experiments demonstrate that given sufficient amount of data, the method can produce accurate models under strong privacy protection.

</details>

<details>

<summary>2021-11-11 12:44:38 - BOiLS: Bayesian Optimisation for Logic Synthesis</summary>

- *Antoine Grosnit, Cedric Malherbe, Rasul Tutunov, Xingchen Wan, Jun Wang, Haitham Bou Ammar*

- `2111.06178v1` - [abs](http://arxiv.org/abs/2111.06178v1) - [pdf](http://arxiv.org/pdf/2111.06178v1)

> Optimising the quality-of-results (QoR) of circuits during logic synthesis is a formidable challenge necessitating the exploration of exponentially sized search spaces. While expert-designed operations aid in uncovering effective sequences, the increase in complexity of logic circuits favours automated procedures. Inspired by the successes of machine learning, researchers adapted deep learning and reinforcement learning to logic synthesis applications. However successful, those techniques suffer from high sample complexities preventing widespread adoption. To enable efficient and scalable solutions, we propose BOiLS, the first algorithm adapting modern Bayesian optimisation to navigate the space of synthesis operations. BOiLS requires no human intervention and effectively trades-off exploration versus exploitation through novel Gaussian process kernels and trust-region constrained acquisitions. In a set of experiments on EPFL benchmarks, we demonstrate BOiLS's superior performance compared to state-of-the-art in terms of both sample efficiency and QoR values.

</details>

<details>

<summary>2021-11-11 15:00:08 - Pool samples to efficiently estimate pathogen prevalence dynamics</summary>

- *Braden Scherting, Alison Peel, Raina Plowright, Andrew Hoegh*

- `2111.06249v1` - [abs](http://arxiv.org/abs/2111.06249v1) - [pdf](http://arxiv.org/pdf/2111.06249v1)

> Estimating the prevalence of a disease is necessary for evaluating and mitigating risks of its transmission within or between populations. Estimates that consider how prevalence changes with time provide more information about these risks but are difficult to obtain due to the necessary sampling intensity and commensurate testing costs. We propose pooling and jointly testing multiple samples to reduce testing costs and use a novel nonparametric, hierarchical Bayesian model to infer population prevalence from the pooled test results. This approach is shown to reduce uncertainty compared to individual testing at the same budget and to produce similar estimates compared to individual testing at a much higher budget through two synthetic studies and two case studies of natural infection data.

</details>

<details>

<summary>2021-11-11 17:19:15 - Variance bounding of delayed-acceptance kernels</summary>

- *Chris Sherlock, Anthony Lee*

- `1706.02142v4` - [abs](http://arxiv.org/abs/1706.02142v4) - [pdf](http://arxiv.org/pdf/1706.02142v4)

> A delayed-acceptance version of a Metropolis--Hastings algorithm can be useful for Bayesian inference when it is computationally expensive to calculate the true posterior, but a computationally cheap approximation is available; the delayed-acceptance kernel targets the same posterior as its associated "parent" Metropolis-Hastings kernel. Although the asymptotic variance of the ergodic average of any functional of the chain cannot be less than that obtained using its parent, the average computational time per iteration can be much smaller and so for a given computational budget the delayed-acceptance kernel can be more efficient.   When the asymptotic variance of the ergodic averages of all $L^2$ functionals of the chain is finite, the kernel is said to be variance bounding. It has recently been noted that a delayed-acceptance kernel need not be variance bounding even when its parent is. We provide sufficient conditions for inheritance: for non-local algorithms, such as the independence sampler, the discrepancy between the log density of the approximation and that of the truth should be bounded; for local algorithms, two alternative sets of conditions are provided.   As a by-product of our initial, general result we also supply sufficient conditions on any pair of proposals such that, for any shared target distribution, if a Metropolis-Hastings kernel using one of the proposals is variance bounding then so is the Metropolis-Hastings kernel using the other proposal.

</details>

<details>

<summary>2021-11-11 18:59:21 - Kalman Filtering with Adversarial Corruptions</summary>

- *Sitan Chen, Frederic Koehler, Ankur Moitra, Morris Yau*

- `2111.06395v1` - [abs](http://arxiv.org/abs/2111.06395v1) - [pdf](http://arxiv.org/pdf/2111.06395v1)

> Here we revisit the classic problem of linear quadratic estimation, i.e. estimating the trajectory of a linear dynamical system from noisy measurements. The celebrated Kalman filter gives an optimal estimator when the measurement noise is Gaussian, but is widely known to break down when one deviates from this assumption, e.g. when the noise is heavy-tailed. Many ad hoc heuristics have been employed in practice for dealing with outliers. In a pioneering work, Schick and Mitter gave provable guarantees when the measurement noise is a known infinitesimal perturbation of a Gaussian and raised the important question of whether one can get similar guarantees for large and unknown perturbations.   In this work we give a truly robust filter: we give the first strong provable guarantees for linear quadratic estimation when even a constant fraction of measurements have been adversarially corrupted. This framework can model heavy-tailed and even non-stationary noise processes. Our algorithm robustifies the Kalman filter in the sense that it competes with the optimal algorithm that knows the locations of the corruptions. Our work is in a challenging Bayesian setting where the number of measurements scales with the complexity of what we need to estimate. Moreover, in linear dynamical systems past information decays over time. We develop a suite of new techniques to robustly extract information across different time steps and over varying time scales.

</details>

<details>

<summary>2021-11-12 02:18:26 - Multi-Step Budgeted Bayesian Optimization with Unknown Evaluation Costs</summary>

- *Raul Astudillo, Daniel R. Jiang, Maximilian Balandat, Eytan Bakshy, Peter I. Frazier*

- `2111.06537v1` - [abs](http://arxiv.org/abs/2111.06537v1) - [pdf](http://arxiv.org/pdf/2111.06537v1)

> Bayesian optimization (BO) is a sample-efficient approach to optimizing costly-to-evaluate black-box functions. Most BO methods ignore how evaluation costs may vary over the optimization domain. However, these costs can be highly heterogeneous and are often unknown in advance. This occurs in many practical settings, such as hyperparameter tuning of machine learning algorithms or physics-based simulation optimization. Moreover, those few existing methods that acknowledge cost heterogeneity do not naturally accommodate a budget constraint on the total evaluation cost. This combination of unknown costs and a budget constraint introduces a new dimension to the exploration-exploitation trade-off, where learning about the cost incurs the cost itself. Existing methods do not reason about the various trade-offs of this problem in a principled way, leading often to poor performance. We formalize this claim by proving that the expected improvement and the expected improvement per unit of cost, arguably the two most widely used acquisition functions in practice, can be arbitrarily inferior with respect to the optimal non-myopic policy. To overcome the shortcomings of existing approaches, we propose the budgeted multi-step expected improvement, a non-myopic acquisition function that generalizes classical expected improvement to the setting of heterogeneous and unknown evaluation costs. Finally, we show that our acquisition function outperforms existing methods in a variety of synthetic and real problems.

</details>

<details>

<summary>2021-11-12 10:14:32 - Using Bayesian Network Analysis to Reveal Complex Natures of Relationships</summary>

- *Panchika Lortaraprasert, Pongpak Manoret, Chanati Jantrachotechatchawan, Kobchai Duangrattanalert*

- `2111.06640v1` - [abs](http://arxiv.org/abs/2111.06640v1) - [pdf](http://arxiv.org/pdf/2111.06640v1)

> Relationships are vital for mankind in many aspects. According to Maslow hierarchy of needs, it is suggested that while a healthy relationship is an essential part of a human life that fundamentally determines our goals and purposes, an unsuccessful relationship can lead to suicide and other major psychological problems. However, a complete understanding of this topic still remains a challenge and the divorce rate is rising more than ever before to almost 50 percents. The objective of this research is to explore the association between each group of behaviors by performing Bayesian network analysis on a large publically available Experiences in Close Relationships Scale, a test of attachment style survey (ECR) data from openpsychometrics database. The resulting directed acyclic graph has 2 root nodes (Q02 from avoidant and Q05 from anxious attachment) and 5 end nodes (Q16, Q34, and Q36 from anxious attachment). The network can be divided into 5 clusters, 2 avoidance and 3 anxiety clusters. Furthermore, our list of items in the clusters are consistent with the findings of previous factor analysis studies and our estimated coefficients are significantly correlated with those of one partial correlation network study.

</details>

<details>

<summary>2021-11-12 10:35:57 - Additive Bayesian variable selection under censoring and misspecification</summary>

- *David Rossell, Francisco Javier Rubio*

- `1907.13563v5` - [abs](http://arxiv.org/abs/1907.13563v5) - [pdf](http://arxiv.org/pdf/1907.13563v5)

> We discuss the role of misspecification and censoring on Bayesian model selection in the contexts of right-censored survival and concave log-likelihood regression. Misspecification includes wrongly assuming the censoring mechanism to be non-informative. Emphasis is placed on additive accelerated failure time, Cox proportional hazards and probit models. We offer a theoretical treatment that includes local and non-local priors, and a general non-linear effect decomposition to improve power-sparsity trade-offs. We discuss a fundamental question: what solution can one hope to obtain when (inevitably) models are misspecified, and how to interpret it? Asymptotically, covariates that do not have predictive power for neither the outcome nor (for survival data) censoring times, in the sense of reducing a likelihood-associated loss, are discarded. Misspecification and censoring have an asymptotically negligible effect on false positives, but their impact on power is exponential. We show that it can be advantageous to consider simple models that are computationally practical yet attain good power to detect potentially complex effects, including the use of finite-dimensional basis to detect truly non-parametric effects. We also discuss algorithms to capitalize on sufficient statistics and fast likelihood approximations for Gaussian-based survival and binary models.

</details>

<details>

<summary>2021-11-12 14:45:53 - Learning Bayesian Networks from Ordinal Data</summary>

- *Xiang Ge Luo, Giusi Moffa, Jack Kuipers*

- `2010.15808v3` - [abs](http://arxiv.org/abs/2010.15808v3) - [pdf](http://arxiv.org/pdf/2010.15808v3)

> Bayesian networks are a powerful framework for studying the dependency structure of variables in a complex system. The problem of learning Bayesian networks is tightly associated with the given data type. Ordinal data, such as stages of cancer, rating scale survey questions, and letter grades for exams, are ubiquitous in applied research. However, existing solutions are mainly for continuous and nominal data. In this work, we propose an iterative score-and-search method - called the Ordinal Structural EM (OSEM) algorithm - for learning Bayesian networks from ordinal data. Unlike traditional approaches designed for nominal data, we explicitly respect the ordering amongst the categories. More precisely, we assume that the ordinal variables originate from marginally discretizing a set of Gaussian variables, whose structural dependence in the latent space follows a directed acyclic graph. Then, we adopt the Structural EM algorithm and derive closed-form scoring functions for efficient graph searching. Through simulation studies, we illustrate the superior performance of the OSEM algorithm compared to the alternatives and analyze various factors that may influence the learning accuracy. Finally, we demonstrate the practicality of our method with a real-world application on psychological survey data from 408 patients with co-morbid symptoms of obsessive-compulsive disorder and depression.

</details>

<details>

<summary>2021-11-12 16:32:50 - Controlled Accuracy Gibbs Sampling of Order Constrained Non-IID Ordered Random Variates</summary>

- *Jem N. Corcoran, Caleb Miller*

- `2012.15452v2` - [abs](http://arxiv.org/abs/2012.15452v2) - [pdf](http://arxiv.org/pdf/2012.15452v2)

> Order statistics arising from $m$ independent but not identically distributed random variables are typically constructed by arranging some $X_{1}, X_{2}, \ldots, X_{m}$, with $X_{i}$ having distribution function $F_{i}(x)$, in increasing order denoted as $X_{(1)} \leq X_{(2)} \leq \ldots \leq X_{(m)}$. In this case, $X_{(i)}$ is not necessarily associated with $F_{i}(x)$. Assuming one can simulate values from each distribution, one can generate such "non-iid" order statistics by simulating $X_{i}$ from $F_{i}$, for $i=1,2,\ldots, m$, and arranging them in order. In this paper, we consider the problem of simulating ordered values $X_{(1)}, X_{(2)}, \ldots, X_{(m)}$ such that the marginal distribution of $X_{(i)}$ is $F_{i}(x)$. This problem arises in Bayesian principal components analysis (BPCA) where the $X_{i}$ are ordered eigenvalues that are a posteriori independent but not identically distributed. We propose a novel coupling-from-the-past algorithm to "perfectly" (up to computable order of accuracy) simulate such {\emph{order-constrained non-iid}} order statistics. We demonstrate the effectiveness of our approach for several examples, including the BPCA problem.

</details>

<details>

<summary>2021-11-12 18:08:01 - Wasserstein convergence in Bayesian deconvolution models</summary>

- *Judith Rousseau, Catia Scricciolo*

- `2111.06846v1` - [abs](http://arxiv.org/abs/2111.06846v1) - [pdf](http://arxiv.org/pdf/2111.06846v1)

> We study the reknown deconvolution problem of recovering a distribution function from independent replicates (signal) additively contaminated with random errors (noise), whose distribution is known. We investigate whether a Bayesian nonparametric approach for modelling the latent distribution of the signal can yield inferences with asymptotic frequentist validity under the $L^1$-Wasserstein metric. When the error density is ordinary smooth, we develop two inversion inequalities relating either the $L^1$ or the $L^1$-Wasserstein distance between two mixture densities (of the observations) to the $L^1$-Wasserstein distance between the corresponding distributions of the signal. This smoothing inequality improves on those in the literature. We apply this general result to a Bayesian approach bayes on a Dirichlet process mixture of normal distributions as a prior on the mixing distribution (or distribution of the signal), with a Laplace or Linnik noise. In particular we construct an \textit{adaptive} approximation of the density of the observations by the convolution of a Laplace (or Linnik) with a well chosen mixture of normal densities and show that the posterior concentrates at the minimax rate up to a logarithmic factor. The same prior law is shown to also adapt to the Sobolev regularity level of the mixing density, thus leading to a new Bayesian estimation method, relative to the Wasserstein distance, for distributions with smooth densities.

</details>

<details>

<summary>2021-11-12 23:42:12 - Bayesian Knockoff Generators for Robust Inference Under Complex Data Structure</summary>

- *Michael J. Martens, Anjishnu Banerjee, Xinran Qi, Yushu Shi*

- `2111.06985v1` - [abs](http://arxiv.org/abs/2111.06985v1) - [pdf](http://arxiv.org/pdf/2111.06985v1)

> The recent proliferation of medical data, such as genetics and electronic health records (EHR), offers new opportunities to find novel predictors of health outcomes. Presented with a large set of candidate features, interest often lies in selecting the ones most likely to be predictive of an outcome for further study such that the goal is to control the false discovery rate (FDR) at a specified level. Knockoff filtering is an innovative strategy for FDR-controlled feature selection. But, existing knockoff methods make strong distributional assumptions that hinder their applicability to real world data. We propose Bayesian models for generating high quality knockoff copies that utilize available knowledge about the data structure, thus improving the resolution of prognostic features. Applications to two feature sets are considered: those with categorical and/or continuous variables possibly having a population substructure, such as in EHR; and those with microbiome features having a compositional constraint and phylogenetic relatedness. Through simulations and real data applications, these methods are shown to identify important features with good FDR control and power.

</details>

<details>

<summary>2021-11-13 18:21:35 - Comparing regional and provincial-wide COVID-19 models with physical distancing in British Columbia</summary>

- *Geoffrey McGregor, Jennifer Tippett, Andy T. S. Wan, Mengxiao Wang, Samuel W. K. Wong*

- `2104.10878v2` - [abs](http://arxiv.org/abs/2104.10878v2) - [pdf](http://arxiv.org/pdf/2104.10878v2)

> We study the effects of physical distancing measures for the spread of COVID-19 in regional areas within British Columbia, using the reported cases of the five provincial Health Authorities. Building on the Bayesian epidemiological model of Anderson et al. (2020), we propose a hierarchical regional Bayesian model with time-varying regional parameters between March to December of 2020. In the absence of COVID-19 variants and vaccinations during this period, we examine the regionalized basic reproduction number, modelled prevalence, relative reduction in contact due to physical distancing, and proportion of anticipated cases that have been tested and reported. We observe significant differences between the regional and provincial-wide models and demonstrate the hierarchical regional model can better estimate regional prevalence, especially in rural regions. These results indicate that it can be useful to apply similar regional models to other parts of Canada or other countries.

</details>

<details>

<summary>2021-11-13 18:48:11 - Asymmetric Conjugate Priors for Large Bayesian VARs</summary>

- *Joshua C. C. Chan*

- `2111.07170v1` - [abs](http://arxiv.org/abs/2111.07170v1) - [pdf](http://arxiv.org/pdf/2111.07170v1)

> Large Bayesian VARs are now widely used in empirical macroeconomics. One popular shrinkage prior in this setting is the natural conjugate prior as it facilitates posterior simulation and leads to a range of useful analytical results. This is, however, at the expense of modeling flexibility, as it rules out cross-variable shrinkage -- i.e., shrinking coefficients on lags of other variables more aggressively than those on own lags. We develop a prior that has the best of both worlds: it can accommodate cross-variable shrinkage, while maintaining many useful analytical results, such as a closed-form expression of the marginal likelihood. This new prior also leads to fast posterior simulation -- for a BVAR with 100 variables and 4 lags, obtaining 10,000 posterior draws takes less than half a minute on a standard desktop. We demonstrate the usefulness of the new prior via a structural analysis using a 15-variable VAR with sign restrictions to identify 5 structural shocks.

</details>

<details>

<summary>2021-11-14 02:52:28 - Large Order-Invariant Bayesian VARs with Stochastic Volatility</summary>

- *Joshua C. C. Chan, Gary Koop, Xuewen Yu*

- `2111.07225v1` - [abs](http://arxiv.org/abs/2111.07225v1) - [pdf](http://arxiv.org/pdf/2111.07225v1)

> Many popular specifications for Vector Autoregressions (VARs) with multivariate stochastic volatility are not invariant to the way the variables are ordered due to the use of a Cholesky decomposition for the error covariance matrix. We show that the order invariance problem in existing approaches is likely to become more serious in large VARs. We propose the use of a specification which avoids the use of this Cholesky decomposition. We show that the presence of multivariate stochastic volatility allows for identification of the proposed model and prove that it is invariant to ordering. We develop a Markov Chain Monte Carlo algorithm which allows for Bayesian estimation and prediction. In exercises involving artificial and real macroeconomic data, we demonstrate that the choice of variable ordering can have non-negligible effects on empirical results. In a macroeconomic forecasting exercise involving VARs with 20 variables we find that our order-invariant approach leads to the best forecasts and that some choices of variable ordering can lead to poor forecasts using a conventional, non-order invariant, approach.

</details>

<details>

<summary>2021-11-15 15:30:59 - Bayesian modelling and computation utilising cycles in multiple network data</summary>

- *Anastasia Mantziou, Robin Mitra, Simon Lunagomez*

- `2111.07840v1` - [abs](http://arxiv.org/abs/2111.07840v1) - [pdf](http://arxiv.org/pdf/2111.07840v1)

> Modelling multiple network data is crucial for addressing a wide range of applied research questions. However, there are many challenges, both theoretical and computational, to address. Network cycles are often of particular interest in many applications, such as ecological studies, and an unexplored area has been how to incorporate networks' cycles within the inferential framework in an explicit way. The recently developed Spherical Network Family of models (SNF) offers a flexible formulation for modelling multiple network data that permits any type of metric. This has opened up the possibility to formulate network models that focus on network properties hitherto not possible or practical to consider. In this article we propose a novel network distance metric that measures similarities between networks with respect to their cycles, and incorporate this within the SNF model to allow inferences that explicitly capture information on cycles. These network motifs are of particular interest in ecological studies. We further propose a novel computational framework to allow posterior inferences from the intractable SNF model for moderate sized networks. Lastly, we apply the resulting methodology to a set of ecological network data studying aggressive interactions between species of fish. We show our model is able to make cogent inferences concerning the cycle behaviour amongst the species, and beyond those possible from a model that does not consider this network motif.

</details>

<details>

<summary>2021-11-15 15:40:58 - A Two-Dimensional Intrinsic Gaussian Markov Random Field for Blood Pressure Data</summary>

- *Maria-Zafeiria Spyropoulou, James Bentham*

- `2111.07848v1` - [abs](http://arxiv.org/abs/2111.07848v1) - [pdf](http://arxiv.org/pdf/2111.07848v1)

> Many real-world phenomena are naturally bivariate. This includes blood pressure, which comprises systolic and diastolic levels. Here, we develop a Bayesian hierarchical model that estimates these values and their interactions simultaneously, using sparse data that vary substantially between countries and over time. A key element of the model is a two-dimensional second-order Intrinsic Gaussian Markov Random Field, which captures non-linear trends in the variables and their interactions. The model is fitted using Markov chain Monte Carlo methods, with a block Metropolis-Hastings algorithm providing efficient updates. Performance is demonstrated using simulated and real data.

</details>

<details>

<summary>2021-11-15 16:34:08 - An Approach of Bayesian Variable Selection for Ultrahigh Dimensional Multivariate Regression</summary>

- *Xiaotian Dai, Guifang Fu, Randall Reese, Shaofei Zhao, Zuofeng Shang*

- `2111.07878v1` - [abs](http://arxiv.org/abs/2111.07878v1) - [pdf](http://arxiv.org/pdf/2111.07878v1)

> In many practices, scientists are particularly interested in detecting which of the predictors are truly associated with a multivariate response. It is more accurate to model multiple responses as one vector rather than separating each component one by one. This is particularly true for complex traits having multiple correlated components. A Bayesian multivariate variable selection (BMVS) approach is proposed to select important predictors influencing the multivariate response from a candidate pool with an ultrahigh dimension. By applying the sample-size-dependent spike and slab priors, the BMVS approach satisfies the strong selection consistency property under certain conditions, which represents the advantages of BMVS over other existing Bayesian multivariate regression-based approaches. The proposed approach considers the covariance structure of multiple responses without assuming independence and integrates the estimation of covariance-related parameters together with all regression parameters into one framework through a fast updating MCMC procedure. It is demonstrated through simulations that the BMVS approach outperforms some other relevant frequentist and Bayesian approaches. The proposed BMVS approach possesses the flexibility of wide applications, including genome-wide association studies with multiple correlated phenotypes and a large scale of genetic variants and/or environmental variables, as demonstrated in the real data analyses section. The computer code and test data of the proposed method are available as an R package.

</details>

<details>

<summary>2021-11-15 18:00:14 - Stochastic Gradient Line Bayesian Optimization: Reducing Measurement Shots in Optimizing Parameterized Quantum Circuits</summary>

- *Shiro Tamiya, Hayata Yamasaki*

- `2111.07952v1` - [abs](http://arxiv.org/abs/2111.07952v1) - [pdf](http://arxiv.org/pdf/2111.07952v1)

> Optimization of parameterized quantum circuits is indispensable for applications of near-term quantum devices to computational tasks with variational quantum algorithms (VQAs). However, the existing optimization algorithms for VQAs require an excessive number of quantum-measurement shots in estimating expectation values of observables or iterating updates of circuit parameters, whose cost has been a crucial obstacle for practical use. To address this problem, we develop an efficient framework, \textit{stochastic gradient line Bayesian optimization} (SGLBO), for the circuit optimization with fewer measurement shots. The SGLBO reduces the cost of measurement shots by estimating an appropriate direction of updating the parameters based on stochastic gradient descent (SGD) and further by utilizing Bayesian optimization (BO) to estimate the optimal step size in each iteration of the SGD. We formulate an adaptive measurement-shot strategy to achieve the optimization feasibly without relying on precise expectation-value estimation and many iterations; moreover, we show that a technique of suffix averaging can significantly reduce the effect of statistical and hardware noise in the optimization for the VQAs. Our numerical simulation demonstrates that the SGLBO augmented with these techniques can drastically reduce the required number of measurement shots, improve the accuracy in the optimization, and enhance the robustness against the noise compared to other state-of-art optimizers in representative tasks for the VQAs. These results establish a framework of quantum-circuit optimizers integrating two different optimization approaches, SGD and BO, to reduce the cost of measurement shots significantly.

</details>

<details>

<summary>2021-11-15 19:29:46 - A Bayesian Approach for Characterizing and Mitigating Gate and Measurement Errors</summary>

- *Muqing Zheng, Ang Li, Tamás Terlaky, Xiu Yang*

- `2010.09188v3` - [abs](http://arxiv.org/abs/2010.09188v3) - [pdf](http://arxiv.org/pdf/2010.09188v3)

> Various noise models have been developed in quantum computing study to describe the propagation and effect of the noise which is caused by imperfect implementation of hardware. Identifying parameters such as gate and readout error rates are critical to these models. We use a Bayesian inference approach to identity posterior distributions of these parameters, such that they can be characterized more elaborately. By characterizing the device errors in this way, we can further improve the accuracy of quantum error mitigation. Experiments conducted on IBM's quantum computing devices suggest that our approach provides better error mitigation performance than existing techniques used by the vendor. Also, our approach outperforms the standard Bayesian inference method in such experiments.

</details>

<details>

<summary>2021-11-15 23:15:03 - Bayesian inference of the climbing grade scale</summary>

- *Alexei Drummond, Alex Popinga*

- `2111.08140v1` - [abs](http://arxiv.org/abs/2111.08140v1) - [pdf](http://arxiv.org/pdf/2111.08140v1)

> Climbing grades are used to classify a climbing route based on its perceived difficulty, and have come to play a central role in the sport of rock climbing. Recently, the first statistically rigorous method for estimating climbing grades from whole-history ascent data was described, based on the dynamic Bradley-Terry model for games between players of time-varying ability. In this paper, we implement inference under the whole-history rating model using Markov chain Monte Carlo and apply the method to a curated data set made up of climbers who climb regularly. We use these data to get an estimate of the model's fundamental scale parameter m, which defines the proportional increase in difficulty associated with an increment of grade. We show that the data conform to assumptions that the climbing grade scale is a logarithmic scale of difficulty, like decibels or stellar magnitude. We estimate that an increment in Ewbank, French and UIAA climbing grade systems corresponds to 2.1, 2.09 and 2.13 times increase in difficulty respectively, assuming a logistic model of probability of success as a function of grade. Whereas we find that the Vermin scale for bouldering (V-grade scale) corresponds to a 3.17 increase in difficulty per grade increment. In addition, we highlight potential connections between the logarithmic properties of climbing grade scales and the psychophysical laws of Weber and Fechner.

</details>

<details>

<summary>2021-11-16 01:54:44 - Absolute and Relative Bias in Eight Common Observational Study Designs: Evidence from a Meta-analysis</summary>

- *Jelena Zurovac, Thomas D. Cook, John Deke, Mariel M. Finucane, Duncan Chaplin, Jared S. Coopersmith, Michael Barna, Lauren Vollmer Forrow*

- `2111.06941v2` - [abs](http://arxiv.org/abs/2111.06941v2) - [pdf](http://arxiv.org/pdf/2111.06941v2)

> Observational studies are needed when experiments are not possible. Within study comparisons (WSC) compare observational and experimental estimates that test the same hypothesis using the same treatment group, outcome, and estimand. Meta-analyzing 39 of them, we compare mean bias and its variance for the eight observational designs that result from combining whether there is a pretest measure of the outcome or not, whether the comparison group is local to the treatment group or not, and whether there is a relatively rich set of other covariates or not. Of these eight designs, one combines all three design elements, another has none, and the remainder include any one or two. We found that both the mean and variance of bias decline as design elements are added, with the lowest mean and smallest variance in a design with all three elements. The probability of bias falling within 0.10 standard deviations of the experimental estimate varied from 59 to 83 percent in Bayesian analyses and from 86 to 100 percent in non-Bayesian ones -- the ranges depending on the level of data aggregation. But confounding remains possible due to each of the eight observational study design cells including a different set of WSC studies.

</details>

<details>

<summary>2021-11-16 03:03:26 - A Bayesian Approach for De-duplication in the Presence of Relational Data</summary>

- *Juan Sosa, Abel Rodriguez*

- `1909.06519v4` - [abs](http://arxiv.org/abs/1909.06519v4) - [pdf](http://arxiv.org/pdf/1909.06519v4)

> In this paper, we study the impact of combining profile and network data in a de-duplication setting. We also assess the influence of a range of prior distributions on the linkage structure. Furthermore, we explore stochastic gradient Hamiltonian Monte Carlo methods as a faster alternative to obtain samples from the posterior distribution for network parameters. Our methodology is evaluated using the RLdata500 data, which is a popular dataset in the record linkage literature.

</details>

<details>

<summary>2021-11-16 04:01:21 - Functional Group Bridge for Simultaneous Regression and Support Estimation</summary>

- *Zhengjia Wang, John Magnotti, Michael S. Beauchamp, Meng Li*

- `2006.10163v3` - [abs](http://arxiv.org/abs/2006.10163v3) - [pdf](http://arxiv.org/pdf/2006.10163v3)

> This article is motivated by studying multisensory effects on brain activities in intracranial electroencephalography (iEEG) experiments. Differential brain activities to multisensory stimulus presentations are zero in most regions and non-zero in some local regions, yielding locally sparse functions. Such studies are essentially a function-on-scalar regression problem, with interest being focused not only on estimating nonparametric functions but also on recovering the function supports. We propose a weighted group bridge approach for simultaneous function estimation and support recovery in function-on-scalar mixed effect models, while accounting for heterogeneity present in functional data. We use B-splines to transform sparsity of functions to its sparse vector counterpart of increasing dimension, and propose a fast non-convex optimization algorithm using nested alternative direction method of multipliers (ADMM) for estimation. Large sample properties are established. In particular, we show that the estimated coefficient functions are rate optimal in the minimax sense under the $L_2$ norm and resemble a phase transition phenomenon. For support estimation, we derive a convergence rate under the $L_{\infty}$ norm that leads to a sparsistency property under $\delta$-sparsity, and provide a simple sufficient regularity condition under which a strict sparsistency property is established. An adjusted extended Bayesian information criterion is proposed for parameter tuning. The developed method is illustrated through simulation and an application to a novel iEEG dataset to study multisensory integration. We integrate the proposed method into RAVE, an R package that gains increasing popularity in the iEEG community.

</details>

<details>

<summary>2021-11-16 08:45:39 - Accounting for Gaussian Process Imprecision in Bayesian Optimization</summary>

- *Julian Rodemann, Thomas Augustin*

- `2111.08299v1` - [abs](http://arxiv.org/abs/2111.08299v1) - [pdf](http://arxiv.org/pdf/2111.08299v1)

> Bayesian optimization (BO) with Gaussian processes (GP) as surrogate models is widely used to optimize analytically unknown and expensive-to-evaluate functions. In this paper, we propose Prior-mean-RObust Bayesian Optimization (PROBO) that outperforms classical BO on specific problems. First, we study the effect of the Gaussian processes' prior specifications on classical BO's convergence. We find the prior's mean parameters to have the highest influence on convergence among all prior components. In response to this result, we introduce PROBO as a generalization of BO that aims at rendering the method more robust towards prior mean parameter misspecification. This is achieved by explicitly accounting for GP imprecision via a prior near-ignorance model. At the heart of this is a novel acquisition function, the generalized lower confidence bound (GLCB). We test our approach against classical BO on a real-world problem from material science and observe PROBO to converge faster. Further experiments on multimodal and wiggly target functions confirm the superiority of our method.

</details>

<details>

<summary>2021-11-16 09:20:12 - Bayesian model averaging for mortality forecasting using leave-future-out validation</summary>

- *Karim Barigou, Pierre-Olivier Goffard, Stéphane Loisel, Yahia Salhi*

- `2103.15434v2` - [abs](http://arxiv.org/abs/2103.15434v2) - [pdf](http://arxiv.org/pdf/2103.15434v2)

> Predicting the evolution of mortality rates plays a central role for life insurance and pension funds.Various stochastic frameworks have been developed to model mortality patterns taking into account the main stylized facts driving these patterns. However, relying on the prediction of one specific model can be too restrictive and lead to some well documented drawbacks including model misspecification, parameter uncertainty and overfitting. To address these issues we first consider mortality modelling in a Bayesian Negative-Binomial framework to account for overdispersion and the uncertainty about the parameter estimates in a natural and coherent way. Model averaging techniques are then considered as a response to model misspecifications. In this paper, we propose two methods based on leave-future-out validation which are compared to the standard Bayesian model averaging (BMA) based on marginal likelihood. An intensive numerical study is carried out over a large range of simulation setups to compare the performances of the proposed methodologies. An illustration is then proposed on real-life mortality datasets which includes a sensitivity analysis to a Covid-type scenario. Overall, we found that both methods based on out-of-sample criterion outperform the standard BMA approach in terms of prediction performance and robustness.

</details>

<details>

<summary>2021-11-16 12:18:48 - Shrinkage Bayesian Causal Forests for Heterogeneous Treatment Effects Estimation</summary>

- *Alberto Caron, Gianluca Baio, Ioanna Manolopoulou*

- `2102.06573v4` - [abs](http://arxiv.org/abs/2102.06573v4) - [pdf](http://arxiv.org/pdf/2102.06573v4)

> This paper develops a sparsity-inducing version of Bayesian Causal Forests, a recently proposed nonparametric causal regression model that employs Bayesian Additive Regression Trees and is specifically designed to estimate heterogeneous treatment effects using observational data. The sparsity-inducing component we introduce is motivated by empirical studies where not all the available covariates are relevant, leading to different degrees of sparsity underlying the surfaces of interest in the estimation of individual treatment effects. The extended version presented in this work, which we name Shrinkage Bayesian Causal Forest, is equipped with an additional pair of priors allowing the model to adjust the weight of each covariate through the corresponding number of splits in the tree ensemble. These priors improve the model's adaptability to sparse data generating processes and allow to perform fully Bayesian feature shrinkage in a framework for treatment effects estimation, and thus to uncover the moderating factors driving heterogeneity. In addition, the method allows prior knowledge about the relevant confounding covariates and the relative magnitude of their impact on the outcome to be incorporated in the model. We illustrate the performance of our method in simulated studies, in comparison to Bayesian Causal Forest and other state-of-the-art models, to demonstrate how it scales up with an increasing number of covariates and how it handles strongly confounded scenarios. Finally, we also provide an example of application using real-world data.

</details>

<details>

<summary>2021-11-16 13:46:35 - A Two-Sample Robust Bayesian Mendelian Randomization Method Accounting for Linkage Disequilibrium and Idiosyncratic Pleiotropy with Applications to the COVID-19 Outcome</summary>

- *Anqi Wang, Zhonghua Liu*

- `2103.02877v3` - [abs](http://arxiv.org/abs/2103.02877v3) - [pdf](http://arxiv.org/pdf/2103.02877v3)

> Mendelian randomization (MR) is a statistical method exploiting genetic variants as instrumental variables to estimate the causal effect of modifiable risk factors on an outcome of interest. Despite wide uses of various popular two-sample MR methods based on genome-wide association study summary level data, however, those methods could suffer from potential power loss or/and biased inference when the chosen genetic variants are in linkage disequilibrium (LD), and also have relatively large direct effects on the outcome whose distribution might be heavy-tailed which is commonly referred to as the idiosyncratic pleiotropy phenomenon. To resolve those two issues, we propose a novel Robust Bayesian Mendelian Randomization (RBMR) model that uses the more robust multivariate generalized t-distribution to model such direct effects in a probabilistic model framework which can also incorporate the LD structure explicitly. The generalized t-distribution can be represented as a Gaussian scaled mixture so that our model parameters can be estimated by the EM-type algorithms. We compute the standard errors by calibrating the evidence lower bound using the likelihood ratio test. Through extensive simulation studies, we show that our RBMR has robust performance compared to other competing methods. We also apply our RBMR method to two benchmark data sets and find that RBMR has smaller bias and standard errors. Using our proposed RBMR method, we find that coronary artery disease is associated with increased risk of critically ill coronavirus disease 2019 (COVID-19). We also develop a user-friendly R package RBMR for public use.

</details>

<details>

<summary>2021-11-16 16:43:33 - A dual-stress Bayesian Weibull accelerated life testing model</summary>

- *Neill Smit, Lizanne Raubenheimer*

- `2111.14608v1` - [abs](http://arxiv.org/abs/2111.14608v1) - [pdf](http://arxiv.org/pdf/2111.14608v1)

> In this paper, a Bayesian accelerated life testing model is presented. The Weibull distribution is used as the life distribution and the generalised Eyring model as the time transformation function. This is a model that allows for the use of more than one stressor, whereas other commonly used acceleration models, such as the Arrhenius and power law models, allow for only one stressor. The generalised Eyring-Weibull model is used in an application, where MCMC methods are utilised to generate samples for posterior inference.

</details>

<details>

<summary>2021-11-16 16:44:49 - Bayesian, frequentist and fiducial intervals for the difference between two binomial proportions</summary>

- *Lizanne Raubenheimer*

- `2111.08610v1` - [abs](http://arxiv.org/abs/2111.08610v1) - [pdf](http://arxiv.org/pdf/2111.08610v1)

> Estimating the difference between two binomial proportions will be investigated, where Bayesian, frequentist and fiducial (BFF) methods will be considered. Three vague priors will be used, the Jeffreys prior, a divergence prior and the probability matching prior. A probability matching prior is a prior distribution under which the posterior probabilities of certain regions coincide with their coverage probabilities. Fiducial inference can be viewed as a procedure that obtains a measure on a parameter space while assuming less than what Bayesian inference does, i.e. no prior. Fisher introduced the idea of fiducial probability and fiducial inference. In some cases the fiducial distribution is equivalent to the Jeffreys posterior. The performance of the Jeffreys prior, divergence prior and the probability matching prior will be compared to a fiducial method and other classical methods of constructing confidence intervals for the difference between two independent binomial parameters. These intervals will be compared and evaluated by looking at their coverage rates and average interval lengths. The probability matching and divergence priors perform better than the Jeffreys prior.

</details>

<details>

<summary>2021-11-16 17:16:21 - A general Bayesian bootstrap for censored data based on the beta-Stacy process</summary>

- *Andrea Arfè, Pietro Muliere*

- `2002.04081v2` - [abs](http://arxiv.org/abs/2002.04081v2) - [pdf](http://arxiv.org/pdf/2002.04081v2)

> We introduce a novel procedure to perform Bayesian non-parametric inference with right-censored data, the \emph{beta-Stacy bootstrap}. This approximates the posterior law of summaries of the survival distribution (e.g. the mean survival time). More precisely, our procedure approximates the joint posterior law of functionals of the beta-Stacy process, a non-parametric process prior that generalizes the Dirichlet process and that is widely used in survival analysis. The beta-Stacy bootstrap generalizes and unifies other common Bayesian bootstraps for complete or censored data based on non-parametric priors. It is defined by an exact sampling algorithm that does not require tuning of Markov Chain Monte Carlo steps. We illustrate the beta-Stacy bootstrap by analyzing survival data from a real clinical trial.

</details>

<details>

<summary>2021-11-16 22:14:33 - Online Learning of Network Bottlenecks via Minimax Paths</summary>

- *Niklas Åkerblom, Fazeleh Sadat Hoseini, Morteza Haghir Chehreghani*

- `2109.08467v2` - [abs](http://arxiv.org/abs/2109.08467v2) - [pdf](http://arxiv.org/pdf/2109.08467v2)

> In this paper, we study bottleneck identification in networks via extracting minimax paths. Many real-world networks have stochastic weights for which full knowledge is not available in advance. Therefore, we model this task as a combinatorial semi-bandit problem to which we apply a combinatorial version of Thompson Sampling and establish an upper bound on the corresponding Bayesian regret. Due to the computational intractability of the problem, we then devise an alternative problem formulation which approximates the original objective. Finally, we experimentally evaluate the performance of Thompson Sampling with the approximate formulation on real-world directed and undirected networks.

</details>

<details>

<summary>2021-11-17 00:03:44 - The Feature-First Block Model</summary>

- *Lawrence Tray, Ioannis Kontoyiannis*

- `2105.13762v2` - [abs](http://arxiv.org/abs/2105.13762v2) - [pdf](http://arxiv.org/pdf/2105.13762v2)

> Labelled networks are an important class of data, naturally appearing in numerous applications in science and engineering. A typical inference goal is to determine how the vertex labels (or features) affect the network's structure. In this work, we introduce a new generative model, the feature-first block model (FFBM), that facilitates the use of rich queries on labelled networks. We develop a Bayesian framework and devise a two-level Markov chain Monte Carlo approach to efficiently sample from the relevant posterior distribution of the FFBM parameters. This allows us to infer if and how the observed vertex-features affect macro-structure. We apply the proposed methods to a variety of network data to extract the most important features along which the vertices are partitioned. The main advantages of the proposed approach are that the whole feature-space is used automatically and that features can be rank-ordered implicitly according to impact.

</details>

<details>

<summary>2021-11-17 02:46:31 - Some Case Studies Using Bayesian Statistical Models</summary>

- *Juan Sosa, Lina Buitrago*

- `2111.08870v1` - [abs](http://arxiv.org/abs/2111.08870v1) - [pdf](http://arxiv.org/pdf/2111.08870v1)

> We provide four case studies that use Bayesian machinery to making inductive reasoning. Our main motivation relies in offering several instances where the Bayesian approach to data analysis is exploited at its best to perform complex tasks, such as description, testing, estimation, and prediction. This work is not meant to be either a reference text or a survey in Bayesian statistical inference. Our goal is simply to provide several examples that use Bayesian methodology to solve data-driven problems. The topics we cover here, include problems in Bayesian nonparametrics, Bayesian analysis of times series, and Bayesian analysis of spatial data.

</details>

<details>

<summary>2021-11-17 07:40:41 - Complete Deterministic Dynamics and Spectral Decomposition of the Ensemble Kalman Inversion</summary>

- *Leon Bungert, Philipp Wacker*

- `2104.13281v4` - [abs](http://arxiv.org/abs/2104.13281v4) - [pdf](http://arxiv.org/pdf/2104.13281v4)

> The Ensemble Kalman inversion (EKI), proposed by Iglesias et al. for the solution of Bayesian inverse problems of type $y=A u^\dagger +\varepsilon$, with $u^\dagger$ being an unknown parameter and $y$ a given datum, is a powerful tool usually derived from a sequential Monte Carlo point of view. It describes the dynamics of an ensemble of particles $\{u^j(t)\}_{j=1}^J$, whose initial empirical measure is sampled from the prior, evolving over an artificial time $t$ towards an approximate solution of the inverse problem. Using spectral techniques, we provide a complete description of the deterministic dynamics of EKI and their asymptotic behavior in parameter space. In particular, we analyze the dynamics of deterministic EKI and mean-field EKI. We demonstrate that the Bayesian posterior can only be recovered with the mean-field limit and not with finite sample sizes or deterministic EKI. Furthermore, we show that -- even in the deterministic case -- residuals in parameter space do not decrease monotonously in the Euclidean norm and suggest a problem-adapted norm, where monotonicity can be proved. Finally, we derive a system of ordinary differential equations governing the spectrum and eigenvectors of the covariance matrix.

</details>

<details>

<summary>2021-11-17 08:43:27 - Bayesian experimental design without posterior calculations: an adversarial approach</summary>

- *Dennis Prangle, Sophie Harbisher, Colin S Gillespie*

- `1904.05703v5` - [abs](http://arxiv.org/abs/1904.05703v5) - [pdf](http://arxiv.org/pdf/1904.05703v5)

> Most computational approaches to Bayesian experimental design require making posterior calculations repeatedly for a large number of potential designs and/or simulated datasets. This can be expensive and prohibit scaling up these methods to models with many parameters, or designs with many unknowns to select. We introduce an efficient alternative approach without posterior calculations, based on optimising the expected trace of the Fisher information, as discussed by Walker (2016). We illustrate drawbacks of this approach, including lack of invariance to reparameterisation and encouraging designs in which one parameter combination is inferred accurately but not any others. We show these can be avoided by using an adversarial approach: the experimenter must select their design while a critic attempts to select the least favourable parameterisation. We present theoretical properties of this approach and show it can be used with gradient based optimisation methods to find designs efficiently in practice.

</details>

<details>

<summary>2021-11-17 09:49:24 - Scaling priors in two dimensions for Intrinsic Gaussian MarkovRandom Fields</summary>

- *Maria-Zafeiria Spyropoulou, James Bentham*

- `2111.09003v1` - [abs](http://arxiv.org/abs/2111.09003v1) - [pdf](http://arxiv.org/pdf/2111.09003v1)

> Intrinsic Gaussian Markov Random Fields (IGMRFs) can be used to induce conditional dependence in Bayesian hierarchical models. IGMRFs have both a precision matrix, which defines the neighbourhood structure of the model, and a precision, or scaling, parameter. Previous studies have shown the importance of selecting this scaling parameter appropriately for different types of IGMRF, as it can have a substantial impact on posterior results. Here, we focus on the two-dimensional case, where tuning of the parameter is achieved by mapping it to the marginal standard deviation of a two-dimensional IGMRF. We compare the effects of scaling various classes of IGMRF, including an application to blood pressure data using MCMC methods.

</details>

<details>

<summary>2021-11-17 16:28:06 - A Bayesian model of microbiome data for simultaneous identification of covariate associations and prediction of phenotypic outcomes</summary>

- *Matthew D. Koslovsky, Kristi L. Hoffman, Carrie R. Daniel, Marina Vannucci*

- `2004.14817v2` - [abs](http://arxiv.org/abs/2004.14817v2) - [pdf](http://arxiv.org/pdf/2004.14817v2)

> One of the major research questions regarding human microbiome studies is the feasibility of designing interventions that modulate the composition of the microbiome to promote health and cure disease. This requires extensive understanding of the modulating factors of the microbiome, such as dietary intake, as well as the relation between microbial composition and phenotypic outcomes, such as body mass index (BMI). Previous efforts have modeled these data separately, employing two-step approaches that can produce biased interpretations of the results. Here, we propose a Bayesian joint model that simultaneously identifies clinical covariates associated with microbial composition data and predicts a phenotypic response using information contained in the compositional data. Using spike-and-slab priors, our approach can handle high-dimensional compositional as well as clinical data. Additionally, we accommodate the compositional structure of the data via balances and overdispersion typically found in microbial samples. We apply our model to understand the relations between dietary intake, microbial samples, and BMI. In this analysis, we find numerous associations between microbial taxa and dietary factors that may lead to a microbiome that is generally more hospitable to the development of chronic diseases, such as obesity. Additionally, we demonstrate on simulated data how our method outperforms two-step approaches and also present a sensitivity analysis.

</details>

<details>

<summary>2021-11-17 18:29:26 - A revision to the theory of organic fiducial inference</summary>

- *Russell J. Bowater*

- `2111.09279v1` - [abs](http://arxiv.org/abs/2111.09279v1) - [pdf](http://arxiv.org/pdf/2111.09279v1)

> A principle is modified that underlies the theory of organic fiducial inference as this theory was presented in an earlier paper. This modification, which is arguably a natural one to make, allows Bayesian inference to sometimes have a minor role within the theory in question and, as a consequence, allows more information from the data to be incorporated into the way a full conditional fiducial density is defined in certain cases. The new version of the principle concerned is applied to examples that were analysed previously using the older version of this principle.

</details>

<details>

<summary>2021-11-17 18:55:07 - The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament between 1901 and 2018</summary>

- *Rohan Alexander, Monica Alexander*

- `2111.09299v1` - [abs](http://arxiv.org/abs/2111.09299v1) - [pdf](http://arxiv.org/pdf/2111.09299v1)

> Politics and discussion in parliament is likely to be influenced by the party in power and associated election cycles. However, little is known about the extent to which these events affect discussion and how this has changed over time. We systematically analyse how discussion in the Australian Federal Parliament changes in response to two types of political events: elections and changed prime ministers. We use a newly constructed dataset of what was said in the Australian Federal Parliament from 1901 through to 2018 based on extracting and cleaning available public records. We reduce the dimensionality of discussion in this dataset by using a correlated topic model to obtain a set of comparable topics over time. We then relate those topics to the Comparative Agendas Project, and then analyse the effect of these two types of events using a Bayesian hierarchical Dirichlet model. We find that: changes in prime minister tend to be associated with topic changes even when the party in power does not change; and the effect of elections has been increasing since the 1980s, regardless of whether the election results in a change of prime minister.

</details>

<details>

<summary>2021-11-18 09:25:04 - Bayes factors for accelerated life testing models</summary>

- *Neill Smit, Lizanne Raubenheimer*

- `2111.09593v1` - [abs](http://arxiv.org/abs/2111.09593v1) - [pdf](http://arxiv.org/pdf/2111.09593v1)

> In Bayesian accelerated life testing, the most used tool for model comparison is the deviance information criterion. An alternative and more formal approach is to use Bayes factors to compare models. However, Bayesian accelerated life testing models with more than one stressor often have mathematically intractable posterior distributions and Markov chain Monte Carlo methods are employed to obtain posterior samples to base inference on. The computation of the marginal likelihood is challenging when working with such complex models. In this paper, methods for approximating the marginal likelihood and the application thereof in the accelerated life testing paradigm are explored for dual-stress models.

</details>

<details>

<summary>2021-11-18 10:25:05 - Efficient and Generalizable Tuning Strategies for Stochastic Gradient MCMC</summary>

- *Jeremie Coullon, Leah South, Christopher Nemeth*

- `2105.13059v3` - [abs](http://arxiv.org/abs/2105.13059v3) - [pdf](http://arxiv.org/pdf/2105.13059v3)

> Stochastic gradient Markov chain Monte Carlo (SGMCMC) is a popular class of algorithms for scalable Bayesian inference. However, these algorithms include hyperparameters such as step size or batch size that influence the accuracy of estimators based on the obtained posterior samples. As a result, these hyperparameters must be tuned by the practitioner and currently no principled and automated way to tune them exists. Standard MCMC tuning methods based on acceptance rates cannot be used for SGMCMC, thus requiring alternative tools and diagnostics. We propose a novel bandit-based algorithm that tunes the SGMCMC hyperparameters by minimizing the Stein discrepancy between the true posterior and its Monte Carlo approximation. We provide theoretical results supporting this approach and assess various Stein-based discrepancies. We support our results with experiments on both simulated and real datasets, and find that this method is practical for a wide range of applications.

</details>

<details>

<summary>2021-11-18 16:45:35 - Stacking for Non-mixing Bayesian Computations: The Curse and Blessing of Multimodal Posteriors</summary>

- *Yuling Yao, Aki Vehtari, Andrew Gelman*

- `2006.12335v3` - [abs](http://arxiv.org/abs/2006.12335v3) - [pdf](http://arxiv.org/pdf/2006.12335v3)

> When working with multimodal Bayesian posterior distributions, Markov chain Monte Carlo (MCMC) algorithms have difficulty moving between modes, and default variational or mode-based approximate inferences will understate posterior uncertainty. And, even if the most important modes can be found, it is difficult to evaluate their relative weights in the posterior. Here we propose an approach using parallel runs of MCMC, variational, or mode-based inference to hit as many modes or separated regions as possible and then combine these using Bayesian stacking, a scalable method for constructing a weighted average of distributions. The result from stacking efficiently samples from multimodal posterior distribution, minimizes cross validation prediction error, and represents the posterior uncertainty better than variational inference, but it is not necessarily equivalent, even asymptotically, to fully Bayesian inference. We present theoretical consistency with an example where the stacked inference approximates the true data generating process from the misspecified model and a non-mixing sampler, from which the predictive performance is better than full Bayesian inference, hence the multimodality can be considered a blessing rather than a curse under model misspecification. We demonstrate practical implementation in several model families: latent Dirichlet allocation, Gaussian process regression, hierarchical regression, horseshoe variable selection, and neural networks.

</details>

<details>

<summary>2021-11-18 17:10:00 - Deep Ensembles from a Bayesian Perspective</summary>

- *Lara Hoffmann, Clemens Elster*

- `2105.13283v2` - [abs](http://arxiv.org/abs/2105.13283v2) - [pdf](http://arxiv.org/pdf/2105.13283v2)

> Deep ensembles can be considered as the current state-of-the-art for uncertainty quantification in deep learning. While the approach was originally proposed as a non-Bayesian technique, arguments supporting its Bayesian footing have been put forward as well. We show that deep ensembles can be viewed as an approximate Bayesian method by specifying the corresponding assumptions. Our findings lead to an improved approximation which results in an enlarged epistemic part of the uncertainty. Numerical examples suggest that the improved approximation can lead to more reliable uncertainties. Analytical derivations ensure easy calculation of results.

</details>

<details>

<summary>2021-11-18 18:59:35 - Optimal Simple Regret in Bayesian Best Arm Identification</summary>

- *Junpei Komiyama, Kaito Ariu, Masahiro Kato, Chao Qin*

- `2111.09885v1` - [abs](http://arxiv.org/abs/2111.09885v1) - [pdf](http://arxiv.org/pdf/2111.09885v1)

> We consider Bayesian best arm identification in the multi-armed bandit problem. Assuming certain continuity conditions of the prior, we characterize the rate of the Bayesian simple regret. Differing from Bayesian regret minimization (Lai, 1987), the leading factor in Bayesian simple regret derives from the region where the gap between optimal and sub-optimal arms is smaller than $\sqrt{\frac{\log T}{T}}$. We propose a simple and easy-to-compute algorithm with its leading factor matches with the lower bound up to a constant factor; simulation results support our theoretical findings.

</details>

<details>

<summary>2021-11-19 01:15:10 - On Numerical Considerations for Riemannian Manifold Hamiltonian Monte Carlo</summary>

- *James A. Brofos, Roy R. Lederman*

- `2111.09995v1` - [abs](http://arxiv.org/abs/2111.09995v1) - [pdf](http://arxiv.org/pdf/2111.09995v1)

> Riemannian manifold Hamiltonian Monte Carlo (RMHMC) is a sampling algorithm that seeks to adapt proposals to the local geometry of the posterior distribution. The specific form of the Hamiltonian used in RMHMC necessitates {\it implicitly-defined} numerical integrators in order to sustain reversibility and volume-preservation, two properties that are necessary to establish detailed balance of RMHMC. In practice, these implicit equations are solved to a non-zero convergence tolerance via fixed-point iteration. However, the effect of these convergence thresholds on the ergodicity and computational efficiency properties of RMHMC are not well understood. The purpose of this research is to elucidate these relationships through numerous case studies. Our analysis reveals circumstances wherein the RMHMC algorithm is sensitive, and insensitive, to these convergence tolerances. Our empirical analysis examines several aspects of the computation: (i) we examine the ergodicity of the RMHMC Markov chain by employing statistical methods for comparing probability measures based on collections of samples; (ii) we investigate the degree to which detailed balance is violated by measuring errors in reversibility and volume-preservation; (iii) we assess the efficiency of the RMHMC Markov chain in terms of time-normalized ESS. In each of these cases, we investigate the sensitivity of these metrics to the convergence threshold and further contextualize our results in terms of comparison against Euclidean HMC. We propose a method by which one may select the convergence tolerance within a Bayesian inference application using techniques of stochastic approximation and we examine Newton's method, an alternative to fixed point iterations, which can eliminate much of the sensitivity of RMHMC to the convergence threshold.

</details>

<details>

<summary>2021-11-19 06:14:40 - Bayesian Shrinkage Approaches to Unbalanced Problems of Estimation and Prediction on the Basis of Negative Multinomial Samples</summary>

- *Yasuyuki Hamura*

- `2010.03141v2` - [abs](http://arxiv.org/abs/2010.03141v2) - [pdf](http://arxiv.org/pdf/2010.03141v2)

> In this paper, we treat estimation and prediction problems where negative multinomial variables are observed and in particular consider unbalanced settings. First, the problem of estimating multiple negative multinomial parameter vectors under the standardized squared error loss is treated and a new empirical Bayes estimator which dominates the UMVU estimator under suitable conditions is derived. Second, we consider estimation of the joint predictive density of several multinomial tables under the Kullback-Leibler divergence and obtain a sufficient condition under which the Bayesian predictive density with respect to a hierarchical shrinkage prior dominates the Bayesian predictive density with respect to the Jeffreys prior. Third, our proposed Bayesian estimator and predictive density give risk improvements in simulations. Finally, the problem of estimating the joint predictive density of negative multinomial variables is discussed.

</details>

<details>

<summary>2021-11-19 09:29:13 - Analysis of an interventional protein experiment using a vine copula based structural equation model</summary>

- *Claudia Czado, Sebastian Scharl*

- `2111.10113v1` - [abs](http://arxiv.org/abs/2111.10113v1) - [pdf](http://arxiv.org/pdf/2111.10113v1)

> While there is considerable effort to identify signaling pathways using linear Gaussian Bayesian networks from data, there is less emphasis of understanding and quantifying conditional densities and probabilities of nodes given its parents from the identifed Bayesian network. Most graphical models for continuous data assume a multivariate Gaussian distribution, which might be too restrictive. We re-analyse data from an experimental setting considered in Sachs et al. (2005) to illustrate the effects of such restrictions. For this we propose a novel non Gaussian nonlinear structural equation model based on vine copulas. In particular the D-vine regression approach of Kraus and Czado (2017) is adapted. We show that this model class is more suited to fit the data than the standard linear structural equation model based on the biological consent graph given in Sachs et al. (2005). The modelling approach also allows to study which pathway edges are supported by the data and which can be removed. For data experiment cd3cd28+aktinhib this approach identified three edges, which are no longer supported by the data. For each of these edges a plausible explanation based on underlying the experimental conditions could be found.

</details>

<details>

<summary>2021-11-19 10:10:28 - CRPS Learning</summary>

- *Jonathan Berrisch, Florian Ziel*

- `2102.00968v3` - [abs](http://arxiv.org/abs/2102.00968v3) - [pdf](http://arxiv.org/pdf/2102.00968v3)

> Combination and aggregation techniques can significantly improve forecast accuracy. This also holds for probabilistic forecasting methods where predictive distributions are combined. There are several time-varying and adaptive weighting schemes such as Bayesian model averaging (BMA). However, the quality of different forecasts may vary not only over time but also within the distribution. For example, some distribution forecasts may be more accurate in the center of the distributions, while others are better at predicting the tails. Therefore, we introduce a new weighting method that considers the differences in performance over time and within the distribution. We discuss pointwise combination algorithms based on aggregation across quantiles that optimize with respect to the continuous ranked probability score (CRPS). After analyzing the theoretical properties of pointwise CRPS learning, we discuss B- and P-Spline-based estimation techniques for batch and online learning, based on quantile regression and prediction with expert advice. We prove that the proposed fully adaptive Bernstein online aggregation (BOA) method for pointwise CRPS online learning has optimal convergence properties. They are confirmed in simulations and a probabilistic forecasting study for European emission allowance (EUA) prices.

</details>

<details>

<summary>2021-11-19 12:58:00 - History and Nature of the Jeffreys-Lindley Paradox</summary>

- *Eric-Jan Wagenmakers, Alexander Ly*

- `2111.10191v1` - [abs](http://arxiv.org/abs/2111.10191v1) - [pdf](http://arxiv.org/pdf/2111.10191v1)

> The Jeffreys-Lindley paradox exposes a rift between Bayesian and frequentist hypothesis testing that strikes at the heart of statistical inference. Contrary to what most current literature suggests, the paradox was central to the Bayesian testing methodology developed by Sir Harold Jeffreys in the late 1930s. Jeffreys showed that the evidence against a point-null hypothesis $\mathcal{H}_0$ scales with $\sqrt{n}$ and repeatedly argued that it would therefore be mistaken to set a threshold for rejecting $\mathcal{H}_0$ at a constant multiple of the standard error. Here we summarize Jeffreys's early work on the paradox and clarify his reasons for including the $\sqrt{n}$ term. The prior distribution is seen to play a crucial role; by implicitly correcting for selection, small parameter values are identified as relatively surprising under $\mathcal{H}_1$. We highlight the general nature of the paradox by presenting both a fully frequentist and a fully Bayesian version. We also demonstrate that the paradox does not depend on assigning prior mass to a point hypothesis, as is commonly believed.

</details>

<details>

<summary>2021-11-19 14:25:21 - Posterior concentration and fast convergence rates for generalized Bayesian learning</summary>

- *Lam Si Tung Ho, Binh T. Nguyen, Vu Dinh, Duy Nguyen*

- `2111.10243v1` - [abs](http://arxiv.org/abs/2111.10243v1) - [pdf](http://arxiv.org/pdf/2111.10243v1)

> In this paper, we study the learning rate of generalized Bayes estimators in a general setting where the hypothesis class can be uncountable and have an irregular shape, the loss function can have heavy tails, and the optimal hypothesis may not be unique. We prove that under the multi-scale Bernstein's condition, the generalized posterior distribution concentrates around the set of optimal hypotheses and the generalized Bayes estimator can achieve fast learning rate. Our results are applied to show that the standard Bayesian linear regression is robust to heavy-tailed distributions.

</details>

<details>

<summary>2021-11-20 07:23:02 - Kalman filters as the steady-state solution of gradient descent on variational free energy</summary>

- *Manuel Baltieri, Takuya Isomura*

- `2111.10530v1` - [abs](http://arxiv.org/abs/2111.10530v1) - [pdf](http://arxiv.org/pdf/2111.10530v1)

> The Kalman filter is an algorithm for the estimation of hidden variables in dynamical systems under linear Gauss-Markov assumptions with widespread applications across different fields. Recently, its Bayesian interpretation has received a growing amount of attention especially in neuroscience, robotics and machine learning. In neuroscience, in particular, models of perception and control under the banners of predictive coding, optimal feedback control, active inference and more generally the so-called Bayesian brain hypothesis, have all heavily relied on ideas behind the Kalman filter. Active inference, an algorithmic theory based on the free energy principle, specifically builds on approximate Bayesian inference methods proposing a variational account of neural computation and behaviour in terms of gradients of variational free energy. Using this ambitious framework, several works have discussed different possible relations between free energy minimisation and standard Kalman filters. With a few exceptions, however, such relations point at a mere qualitative resemblance or are built on a set of very diverse comparisons based on purported differences between free energy minimisation and Kalman filtering. In this work, we present a straightforward derivation of Kalman filters consistent with active inference via a variational treatment of free energy minimisation in terms of gradient descent. The approach considered here offers a more direct link between models of neural dynamics as gradient descent and standard accounts of perception and decision making based on probabilistic inference, further bridging the gap between hypotheses about neural implementation and computational principles in brain and behavioural sciences.

</details>

<details>

<summary>2021-11-20 10:27:57 - Observing Actions in Global Games</summary>

- *Dominik Grafenhofer, Wolfgang Kuhle*

- `2111.10554v1` - [abs](http://arxiv.org/abs/2111.10554v1) - [pdf](http://arxiv.org/pdf/2111.10554v1)

> We study Bayesian coordination games where agents receive noisy private information over the game's payoffs, and over each others' actions. If private information over actions is of low quality, equilibrium uniqueness obtains in a manner similar to a global games setting. On the contrary, if private information over actions (and thus over the game's payoff coefficient) is precise, agents can coordinate on multiple equilibria. We argue that our results apply to phenomena such as bank-runs, currency crises, recessions, or riots and revolutions, where agents monitor each other closely.

</details>

<details>

<summary>2021-11-20 21:34:55 - Empirical Bayes Model Averaging with Influential Observations: Tuning Zellner's g Prior for Predictive Robustness</summary>

- *Christopher M. Hans, Mario Peruggia, Junyan Wang*

- `2103.01252v2` - [abs](http://arxiv.org/abs/2103.01252v2) - [pdf](http://arxiv.org/pdf/2103.01252v2)

> The behavior of Bayesian model averaging (BMA) for the normal linear regression model in the presence of influential observations that contribute to model misfit is investigated. Remedies to attenuate the potential negative impacts of such observations on inference and prediction are proposed. The methodology is motivated by the view that well-behaved residuals and good predictive performance often go hand-in-hand. Focus is placed on regression models that use variants on Zellner's g prior. Studying the impact of various forms of model misfit on BMA predictions in simple situations points to prescriptive guidelines for "tuning" Zellner's g prior to obtain optimal predictions. The tuning of the prior distribution is obtained by considering theoretical properties that should be enjoyed by the optimal fits of the various models in the BMA ensemble. The methodology can be thought of as an "empirical Bayes" approach to modeling, as the data help to inform the specification of the prior in an attempt to attenuate the negative impact of influential cases.

</details>

<details>

<summary>2021-11-21 02:46:26 - The R2D2 Prior for Generalized Linear Mixed Models</summary>

- *Eric Yanchenko, Howard D. Bondell, Brian J. Reich*

- `2111.10718v1` - [abs](http://arxiv.org/abs/2111.10718v1) - [pdf](http://arxiv.org/pdf/2111.10718v1)

> In Bayesian analysis, the selection of a prior distribution is typically done by considering each parameter in the model. While this can be convenient, in many scenarios it may be desirable to place a prior on a summary measure of the model instead. In this work, we propose a prior on the model fit, as measured by a Bayesian coefficient of determination (R2), which then induces a prior on the individual parameters. We achieve this by placing a beta prior on R2 and then deriving the induced prior on the global variance parameter for generalized linear mixed models. We derive closed-form expressions in many scenarios and present several approximation strategies when an analytic form is not possible and/or to allow for easier computation. In these situations, we suggest to approximate the prior by using a generalized beta prime distribution that matches it closely. This approach is quite flexible and can be easily implemented in standard Bayesian software. Lastly, we demonstrate the performance of the method on simulated data where it particularly shines in high-dimensional examples as well as real-world data which shows its ability to model spatial correlation in the random effects.

</details>

<details>

<summary>2021-11-21 19:24:55 - Efficient Bayesian network structure learning via local Markov boundary search</summary>

- *Ming Gao, Bryon Aragam*

- `2110.06082v2` - [abs](http://arxiv.org/abs/2110.06082v2) - [pdf](http://arxiv.org/pdf/2110.06082v2)

> We analyze the complexity of learning directed acyclic graphical models from observational data in general settings without specific distributional assumptions. Our approach is information-theoretic and uses a local Markov boundary search procedure in order to recursively construct ancestral sets in the underlying graphical model. Perhaps surprisingly, we show that for certain graph ensembles, a simple forward greedy search algorithm (i.e. without a backward pruning phase) suffices to learn the Markov boundary of each node. This substantially improves the sample complexity, which we show is at most polynomial in the number of nodes. This is then applied to learn the entire graph under a novel identifiability condition that generalizes existing conditions from the literature. As a matter of independent interest, we establish finite-sample guarantees for the problem of recovering Markov boundaries from data. Moreover, we apply our results to the special case of polytrees, for which the assumptions simplify, and provide explicit conditions under which polytrees are identifiable and learnable in polynomial time. We further illustrate the performance of the algorithm, which is easy to implement, in a simulation study. Our approach is general, works for discrete or continuous distributions without distributional assumptions, and as such sheds light on the minimal assumptions required to efficiently learn the structure of directed graphical models from data.

</details>

<details>

<summary>2021-11-21 20:32:26 - Test Distribution-Aware Active Learning: A Principled Approach Against Distribution Shift and Outliers</summary>

- *Andreas Kirsch, Tom Rainforth, Yarin Gal*

- `2106.11719v2` - [abs](http://arxiv.org/abs/2106.11719v2) - [pdf](http://arxiv.org/pdf/2106.11719v2)

> Expanding on MacKay (1992), we argue that conventional model-based methods for active learning - like BALD - have a fundamental shortfall: they fail to directly account for the test-time distribution of the input variables. This can lead to pathologies in the acquisition strategy, as what is maximally informative for model parameters may not be maximally informative for prediction: for example, when the data in the pool set is more dispersed than that of the final prediction task, or when the distribution of pool and test samples differs. To correct this, we revisit an acquisition strategy that is based on maximizing the expected information gained about possible future predictions, referring to this as the Expected Predictive Information Gain (EPIG). As EPIG does not scale well for batch acquisition, we further examine an alternative strategy, a hybrid between BALD and EPIG, which we call the Joint Expected Predictive Information Gain (JEPIG). We consider using both for active learning with Bayesian neural networks on a variety of datasets, examining the behavior under distribution shift in the pool set.

</details>

<details>

<summary>2021-11-21 22:27:22 - Tests of Bayesian Rationality</summary>

- *Pooya Molavi*

- `2109.07007v3` - [abs](http://arxiv.org/abs/2109.07007v3) - [pdf](http://arxiv.org/pdf/2109.07007v3)

> What are the testable implications of the Bayesian rationality hypothesis? This paper argues that the absolute continuity of posteriors with respect to priors constitutes the entirety of the empirical content of this hypothesis. I consider a decision-maker who chooses a sequence of actions and an econometrician who observes the decision-maker's actions, but not her signals. The econometrician is interested in testing the hypothesis that the decision-maker follows Bayes' rule to update her belief. I show that without a priori knowledge of the set of models considered by the decision-maker, there are almost no observations that would lead the econometrician to conclude that the decision-maker is not Bayesian. The absolute continuity of posteriors with respect to priors remains the only implication of Bayesian rationality, even if the set of actions is sufficiently rich that the decision-maker's actions fully reveal her beliefs, and even if the econometrician observes a large number of ex ante identical agents who observe i.i.d. signals and face the same sequence of decision problems.

</details>

<details>

<summary>2021-11-21 23:00:11 - Operator-induced structural variable selection for identifying materials genes</summary>

- *Shengbin Ye, Thomas P. Senftle, Meng Li*

- `2110.10195v2` - [abs](http://arxiv.org/abs/2110.10195v2) - [pdf](http://arxiv.org/pdf/2110.10195v2)

> In the emerging field of materials informatics, a fundamental task is to identify physicochemically meaningful descriptors, or materials genes, which are engineered from primary variables and a set of elementary algebraic operators through compositions. Standard practice directly analyzes the high-dimensional candidate predictor space in a linear model; statistical analyses are then substantially hampered by the daunting challenge posed by the astronomically large number of correlated predictors with limited sample size. We formulate this problem as variable selection with operator-induced structure (OIS), and propose a new method to achieve unconventional dimension reduction by utilizing the geometry embedded in OIS. Although the model remains linear, we iterate nonparametric variable selection for effective dimension reduction. This enables variable selection based on ab initio primary variables, leading to a method that is orders of magnitude faster than existing methods, with improved accuracy. An OIS screening property for variable selection with OIS is introduced; interestingly, finite sample assessment indicates that the employed Bayesian Additive Regression Trees (BART)-based variable selection method enjoys this property under the simulation settings. Numerical studies show the superiority of the proposed method, which continues to exhibit robust performance when the dimension of engineered features is out of reach of existing methods. Our analysis to single-atom catalysis identifies physical descriptors that explain the binding energy of metal-support pairs with high explanatory power, leading to interpretable insights to guide the prevention of a notorious problem called sintering and aid catalysis design.

</details>

<details>

<summary>2021-11-22 09:28:08 - Martingale posterior distributions</summary>

- *Edwin Fong, Chris Holmes, Stephen G. Walker*

- `2103.15671v2` - [abs](http://arxiv.org/abs/2103.15671v2) - [pdf](http://arxiv.org/pdf/2103.15671v2)

> The prior distribution on parameters of a sampling distribution is the usual starting point for Bayesian uncertainty quantification. In this paper, we present a different perspective which focuses on missing observations as the source of statistical uncertainty, with the parameter of interest being known precisely given the entire population. We argue that the foundation of Bayesian inference is to assign a distribution on missing observations conditional on what has been observed. In the conditionally i.i.d. setting with an observed sample of size $n$, the Bayesian would thus assign a predictive distribution on the missing $Y_{n+1:\infty}$ conditional on $Y_{1:n}$, which then induces a distribution on the parameter. Demonstrating an application of martingales, Doob shows that choosing the Bayesian predictive distribution returns the conventional posterior as the distribution of the parameter. Taking this as our cue, we relax the predictive machine, avoiding the need for the predictive to be derived solely from the usual prior to posterior to predictive density formula. We introduce the \textit{martingale posterior distribution}, which returns Bayesian uncertainty directly on any statistic of interest without the need for the likelihood and prior, and this distribution can be sampled through a computational scheme we name \textit{predictive resampling}. To that end, we introduce new predictive methodologies for multivariate density estimation, regression and classification that build upon recent work on bivariate copulas.

</details>

<details>

<summary>2021-11-22 12:56:11 - Foundations of Structural Causal Models with Cycles and Latent Variables</summary>

- *Stephan Bongers, Patrick Forré, Jonas Peters, Joris M. Mooij*

- `1611.06221v6` - [abs](http://arxiv.org/abs/1611.06221v6) - [pdf](http://arxiv.org/pdf/1611.06221v6)

> Structural causal models (SCMs), also known as (nonparametric) structural equation models (SEMs), are widely used for causal modeling purposes. In particular, acyclic SCMs, also known as recursive SEMs, form a well-studied subclass of SCMs that generalize causal Bayesian networks to allow for latent confounders. In this paper, we investigate SCMs in a more general setting, allowing for the presence of both latent confounders and cycles. We show that in the presence of cycles, many of the convenient properties of acyclic SCMs do not hold in general: they do not always have a solution; they do not always induce unique observational, interventional and counterfactual distributions; a marginalization does not always exist, and if it exists the marginal model does not always respect the latent projection; they do not always satisfy a Markov property; and their graphs are not always consistent with their causal semantics. We prove that for SCMs in general each of these properties does hold under certain solvability conditions. Our work generalizes results for SCMs with cycles that were only known for certain special cases so far. We introduce the class of simple SCMs that extends the class of acyclic SCMs to the cyclic setting, while preserving many of the convenient properties of acyclic SCMs. With this paper we aim to provide the foundations for a general theory of statistical causal modeling with SCMs.

</details>

<details>

<summary>2021-11-22 15:29:50 - Functional Model of Residential Consumption Elasticity under Dynamic Tariffs</summary>

- *Kamalanathan Ganesan, João Tomé Saraiva, Ricardo J. Bessa*

- `2111.11875v1` - [abs](http://arxiv.org/abs/2111.11875v1) - [pdf](http://arxiv.org/pdf/2111.11875v1)

> One of the major barriers for the retailers is to understand the consumption elasticity they can expect from their contracted demand response (DR) clients. The current trend of DR products provided by retailers are not consumer-specific, which poses additional barriers for the active engagement of consumers in these programs. The elasticity of consumers demand behavior varies from individual to individual. The utility will benefit from knowing more accurately how changes in its prices will modify the consumption pattern of its clients. This work proposes a functional model for the consumption elasticity of the DR contracted consumers. The model aims to determine the load adjustment the DR consumers can provide to the retailers or utilities for different price levels. The proposed model uses a Bayesian probabilistic approach to identify the actual load adjustment an individual contracted client can provide for different price levels it can experience. The developed framework provides the retailers or utilities with a tool to obtain crucial information on how an individual consumer will respond to different price levels. This approach is able to quantify the likelihood with which the consumer reacts to a DR signal and identify the actual load adjustment an individual contracted DR client provides for different price levels they can experience. This information can be used to maximize the control and reliability of the services the retailer or utility can offer to the System Operators.

</details>

<details>

<summary>2021-11-22 16:57:57 - Local policy search with Bayesian optimization</summary>

- *Sarah Müller, Alexander von Rohr, Sebastian Trimpe*

- `2106.11899v2` - [abs](http://arxiv.org/abs/2106.11899v2) - [pdf](http://arxiv.org/pdf/2106.11899v2)

> Reinforcement learning (RL) aims to find an optimal policy by interaction with an environment. Consequently, learning complex behavior requires a vast number of samples, which can be prohibitive in practice. Nevertheless, instead of systematically reasoning and actively choosing informative samples, policy gradients for local search are often obtained from random perturbations. These random samples yield high variance estimates and hence are sub-optimal in terms of sample complexity. Actively selecting informative samples is at the core of Bayesian optimization, which constructs a probabilistic surrogate of the objective from past samples to reason about informative subsequent ones. In this paper, we propose to join both worlds. We develop an algorithm utilizing a probabilistic model of the objective function and its gradient. Based on the model, the algorithm decides where to query a noisy zeroth-order oracle to improve the gradient estimates. The resulting algorithm is a novel type of policy search method, which we compare to existing black-box algorithms. The comparison reveals improved sample complexity and reduced variance in extensive empirical evaluations on synthetic objectives. Further, we highlight the benefits of active sampling on popular RL benchmarks.

</details>

<details>

<summary>2021-11-22 19:10:10 - Bayesian Classifier Fusion with an Explicit Model of Correlation</summary>

- *Susanne Trick, Constantin A. Rothkopf*

- `2106.01770v2` - [abs](http://arxiv.org/abs/2106.01770v2) - [pdf](http://arxiv.org/pdf/2106.01770v2)

> Combining the outputs of multiple classifiers or experts into a single probabilistic classification is a fundamental task in machine learning with broad applications from classifier fusion to expert opinion pooling. Here we present a hierarchical Bayesian model of probabilistic classifier fusion based on a new correlated Dirichlet distribution. This distribution explicitly models positive correlations between marginally Dirichlet-distributed random vectors thereby allowing explicit modeling of correlations between base classifiers or experts. The proposed model naturally accommodates the classic Independent Opinion Pool and other independent fusion algorithms as special cases. It is evaluated by uncertainty reduction and correctness of fusion on synthetic and real-world data sets. We show that a change in performance of the fused classifier due to uncertainty reduction can be Bayes optimal even for highly correlated base classifiers.

</details>

<details>

<summary>2021-11-22 19:29:08 - Sparse Uncertainty Representation in Deep Learning with Inducing Weights</summary>

- *Hippolyt Ritter, Martin Kukla, Cheng Zhang, Yingzhen Li*

- `2105.14594v2` - [abs](http://arxiv.org/abs/2105.14594v2) - [pdf](http://arxiv.org/pdf/2105.14594v2)

> Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, which enables our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\leq 24.3\%$ of that of a $single$ neural network.

</details>

<details>

<summary>2021-11-22 21:03:35 - Bayesian Robust Learning in Chain Graph Models for Integrative Pharmacogenomics</summary>

- *Moumita Chakraborty, Veerabhadran Baladandayuthapani, Anindya Bhadra, Min Jin Ha*

- `2111.11529v1` - [abs](http://arxiv.org/abs/2111.11529v1) - [pdf](http://arxiv.org/pdf/2111.11529v1)

> Integrative analysis of multi-level pharmacogenomic data for modeling dependencies across various biological domains is crucial for developing genomic-testing based treatments. Chain graphs characterize conditional dependence structures of such multi-level data where variables are naturally partitioned into multiple ordered layers, consisting of both directed and undirected edges. Existing literature mostly focus on Gaussian chain graphs, which are ill-suited for non-normal distributions with heavy-tailed marginals, potentially leading to inaccurate inferences. We propose a Bayesian robust chain graph model (RCGM) based on random transformations of marginals using Gaussian scale mixtures to account for node-level non-normality in continuous multivariate data. This flexible modeling strategy facilitates identification of conditional sign dependencies among non-normal nodes while still being able to infer conditional dependencies among normal nodes. In simulations, we demonstrate that RCGM outperforms existing Gaussian chain graph inference methods in data generated from various non-normal mechanisms. We apply our method to genomic, transcriptomic and proteomic data to understand underlying biological processes holistically for drug response and resistance in lung cancer cell lines. Our analysis reveals inter- and intra- platform dependencies of key signaling pathways to monotherapies of icotinib, erlotinib and osimertinib among other drugs, along with shared patterns of molecular mechanisms behind drug actions.

</details>

<details>

<summary>2021-11-22 21:23:52 - Bayesian Persuasion in Sequential Trials</summary>

- *Shih-Tang Su, Vijay G. Subramanian, Grant Schoenebeck*

- `2110.09594v3` - [abs](http://arxiv.org/abs/2110.09594v3) - [pdf](http://arxiv.org/pdf/2110.09594v3)

> We consider a Bayesian persuasion or information design problem where the sender tries to persuade the receiver to take a particular action via a sequence of signals. This we model by considering multi-phase trials with different experiments conducted based on the outcomes of prior experiments. In contrast to most of the literature, we consider the problem with constraints on signals imposed on the sender. This we achieve by fixing some of the experiments in an exogenous manner; these are called determined experiments. This modeling helps us understand real-world situations where this occurs: e.g., multi-phase drug trials where the FDA determines some of the experiments, funding of a startup by a venture capital firm, start-up acquisition by big firms where late-stage assessments are determined by the potential acquirer, multi-round job interviews where the candidates signal initially by presenting their qualifications but the rest of the screening procedures are determined by the interviewer. The non-determined experiments (signals) in the multi-phase trial are to be chosen by the sender in order to persuade the receiver best. With a binary state of the world, we start by deriving the optimal signaling policy in the only non-trivial configuration of a two-phase trial with binary-outcome experiments. We then generalize to multi-phase trials with binary-outcome experiments where the determined experiments can be placed at any chosen node in the trial tree. Here we present a dynamic programming algorithm to derive the optimal signaling policy that uses the two-phase trial solution's structural insights. We also contrast the optimal signaling policy structure with classical Bayesian persuasion strategies to highlight the impact of the signaling constraints on the sender.

</details>

<details>

<summary>2021-11-23 09:30:58 - Sparse Linear Spectral Unmixing of Hyperspectral images using Expectation-Propagation</summary>

- *Zeng Li, Yoann Altmann, Jie Chen, Stephen Mclaughlin, Susanto Rahardja*

- `2106.09985v2` - [abs](http://arxiv.org/abs/2106.09985v2) - [pdf](http://arxiv.org/pdf/2106.09985v2)

> This paper presents a novel Bayesian approach for hyperspectral image unmixing. The observed pixels are modeled by a linear combination of material signatures weighted by their corresponding abundances. A spike-and-slab abundance prior is adopted to promote sparse mixtures and an Ising prior model is used to capture spatial correlation of the mixture support across pixels. We approximate the posterior distribution of the abundances using the expectation-propagation (EP) method. We show that it can significantly reduce the computational complexity of the unmixing stage and meanwhile provide uncertainty measures, compared to expensive Monte Carlo strategies traditionally considered for uncertainty quantification. Moreover, many variational parameters within each EP factor can be updated in a parallel manner, which enables mapping of efficient algorithmic architectures based on graphics processing units (GPU). Under the same approximate Bayesian framework, we then extend the proposed algorithm to semi-supervised unmixing, whereby the abundances are viewed as latent variables and the expectation-maximization (EM) algorithm is used to refine the endmember matrix. Experimental results on synthetic data and real hyperspectral data illustrate the benefits of the proposed framework over state-of-art linear unmixing methods.

</details>

<details>

<summary>2021-11-23 10:10:28 - Efficient Hierarchical Bayesian Inference for Spatio-temporal Regression Models in Neuroimaging</summary>

- *Ali Hashemi, Yijing Gao, Chang Cai, Sanjay Ghosh, Klaus-Robert Müller, Srikantan S. Nagarajan, Stefan Haufe*

- `2111.01692v2` - [abs](http://arxiv.org/abs/2111.01692v2) - [pdf](http://arxiv.org/pdf/2111.01692v2)

> Several problems in neuroimaging and beyond require inference on the parameters of multi-task sparse hierarchical regression models. Examples include M/EEG inverse problems, neural encoding models for task-based fMRI analyses, and climate science. In these domains, both the model parameters to be inferred and the measurement noise may exhibit a complex spatio-temporal structure. Existing work either neglects the temporal structure or leads to computationally demanding inference schemes. Overcoming these limitations, we devise a novel flexible hierarchical Bayesian framework within which the spatio-temporal dynamics of model parameters and noise are modeled to have Kronecker product covariance structure. Inference in our framework is based on majorization-minimization optimization and has guaranteed convergence properties. Our highly efficient algorithms exploit the intrinsic Riemannian geometry of temporal autocovariance matrices. For stationary dynamics described by Toeplitz matrices, the theory of circulant embeddings is employed. We prove convex bounding properties and derive update rules of the resulting algorithms. On both synthetic and real neural data from M/EEG, we demonstrate that our methods lead to improved performance.

</details>

<details>

<summary>2021-11-23 10:18:41 - Uncertainty estimation under model misspecification in neural network regression</summary>

- *Maria R. Cervera, Rafael Dätwyler, Francesco D'Angelo, Hamza Keurti, Benjamin F. Grewe, Christian Henning*

- `2111.11763v1` - [abs](http://arxiv.org/abs/2111.11763v1) - [pdf](http://arxiv.org/pdf/2111.11763v1)

> Although neural networks are powerful function approximators, the underlying modelling assumptions ultimately define the likelihood and thus the hypothesis class they are parameterizing. In classification, these assumptions are minimal as the commonly employed softmax is capable of representing any categorical distribution. In regression, however, restrictive assumptions on the type of continuous distribution to be realized are typically placed, like the dominant choice of training via mean-squared error and its underlying Gaussianity assumption. Recently, modelling advances allow to be agnostic to the type of continuous distribution to be modelled, granting regression the flexibility of classification models. While past studies stress the benefit of such flexible regression models in terms of performance, here we study the effect of the model choice on uncertainty estimation. We highlight that under model misspecification, aleatoric uncertainty is not properly captured, and that a Bayesian treatment of a misspecified model leads to unreliable epistemic uncertainty estimates. Overall, our study provides an overview on how modelling choices in regression may influence uncertainty estimation and thus any downstream decision making process.

</details>

<details>

<summary>2021-11-23 11:15:56 - Flexible Bayesian Nonlinear Model Configuration</summary>

- *Aliaksandr Hubin, Geir Storvik, Florian Frommlet*

- `2003.02929v2` - [abs](http://arxiv.org/abs/2003.02929v2) - [pdf](http://arxiv.org/pdf/2003.02929v2)

> Regression models are used in a wide range of applications providing a powerful scientific tool for researchers from different fields. Linear, or simple parametric, models are often not sufficient to describe complex relationships between input variables and a response. Such relationships can be better described through flexible approaches such as neural networks, but this results in less interpretable models and potential overfitting. Alternatively, specific parametric nonlinear functions can be used, but the specification of such functions is in general complicated. In this paper, we introduce a flexible approach for the construction and selection of highly flexible nonlinear parametric regression models. Nonlinear features are generated hierarchically, similarly to deep learning, but have additional flexibility on the possible types of features to be considered. This flexibility, combined with variable selection, allows us to find a small set of important features and thereby more interpretable models. Within the space of possible functions, a Bayesian approach, introducing priors for functions based on their complexity, is considered. A genetically modified mode jumping Markov chain Monte Carlo algorithm is adopted to perform Bayesian inference and estimate posterior probabilities for model averaging. In various applications, we illustrate how our approach is used to obtain meaningful nonlinear models. Additionally, we compare its predictive performance with several machine learning algorithms.

</details>

<details>

<summary>2021-11-23 15:48:47 - Depth induces scale-averaging in overparameterized linear Bayesian neural networks</summary>

- *Jacob A. Zavatone-Veth, Cengiz Pehlevan*

- `2111.11954v1` - [abs](http://arxiv.org/abs/2111.11954v1) - [pdf](http://arxiv.org/pdf/2111.11954v1)

> Inference in deep Bayesian neural networks is only fully understood in the infinite-width limit, where the posterior flexibility afforded by increased depth washes out and the posterior predictive collapses to a shallow Gaussian process. Here, we interpret finite deep linear Bayesian neural networks as data-dependent scale mixtures of Gaussian process predictors across output channels. We leverage this observation to study representation learning in these networks, allowing us to connect limiting results obtained in previous studies within a unified framework. In total, these results advance our analytical understanding of how depth affects inference in a simple class of Bayesian neural networks.

</details>

<details>

<summary>2021-11-23 16:11:45 - Removing the mini-batching error in Bayesian inference using Adaptive Langevin dynamics</summary>

- *Inass Sekkat, Gabriel Stoltz*

- `2105.10347v3` - [abs](http://arxiv.org/abs/2105.10347v3) - [pdf](http://arxiv.org/pdf/2105.10347v3)

> Bayesian inference allows to obtain useful information on the parameters of models, either in computational statistics or more recently in the context of Bayesian Neural Networks. The computational cost of usual Monte Carlo methods for sampling a posteriori laws in Bayesian inference scales linearly with the number of data points. One option to reduce it to a fraction of this cost is to resort to mini-batching in conjunction with unadjusted discretizations of Langevin dynamics, in which case only a random fraction of the data is used to estimate the gradient. However, this leads to an additional noise in the dynamics and hence a bias on the invariant measure which is sampled by the Markov chain. We advocate using the so-called Adaptive Langevin dynamics, which is a modification of standard inertial Langevin dynamics with a dynamical friction which automatically corrects for the increased noise arising from mini-batching. We investigate the practical relevance of the assumptions underpinning Adaptive Langevin (constant covariance for the estimation of the gradient), which are not satisfied in typical models of Bayesian inference, and quantify the bias induced by minibatching in this case. We also show how to extend AdL in order to systematically reduce the bias on the posterior distribution by considering a dynamical friction depending on the current value of the parameter to sample.

</details>

<details>

<summary>2021-11-23 16:13:37 - Measurement That Matches Theory: Theory-Driven Identification in IRT Models</summary>

- *Marco Morucci, Margaret Foster, Kaitlyn Webster, So Jin Lee, David Siegel*

- `2111.11979v1` - [abs](http://arxiv.org/abs/2111.11979v1) - [pdf](http://arxiv.org/pdf/2111.11979v1)

> Measurement bridges theory and empirics. Without measures that appropriately capture theoretical concepts, description will fail to represent reality and true causal inference will be impossible. Yet, the social sciences traffic in complex concepts and their measurement is difficult. Item Response Theory (IRT) models reduce variation in multiple variables to continuous variation along one or more latent dimensions intended to capture key theoretical concepts. Unfortunately, those latent dimensions have no intrinsic conceptual meaning. Partial solutions to that problem include limiting the number of dimensions to one or assigning meaning post-analysis, but either can lead to potential bias and a lack of reliability across data sources. We propose, detail, and validate a semi-supervised approach employing Bayesian Item Response Theory on multiple latent dimensions and binary data. Our approach, which we validate on simulated and real data, yields conceptually meaningful latent dimensions that are reliable across different data sources without additional exogenous assumptions.

</details>

<details>

<summary>2021-11-24 16:25:43 - State-space deep Gaussian processes with applications</summary>

- *Zheng Zhao*

- `2111.12604v1` - [abs](http://arxiv.org/abs/2111.12604v1) - [pdf](http://arxiv.org/pdf/2111.12604v1)

> This thesis is mainly concerned with state-space approaches for solving deep (temporal) Gaussian process (DGP) regression problems. More specifically, we represent DGPs as hierarchically composed systems of stochastic differential equations (SDEs), and we consequently solve the DGP regression problem by using state-space filtering and smoothing methods. The resulting state-space DGP (SS-DGP) models generate a rich class of priors compatible with modelling a number of irregular signals/functions. Moreover, due to their Markovian structure, SS-DGPs regression problems can be solved efficiently by using Bayesian filtering and smoothing methods. The second contribution of this thesis is that we solve continuous-discrete Gaussian filtering and smoothing problems by using the Taylor moment expansion (TME) method. This induces a class of filters and smoothers that can be asymptotically exact in predicting the mean and covariance of stochastic differential equations (SDEs) solutions. Moreover, the TME method and TME filters and smoothers are compatible with simulating SS-DGPs and solving their regression problems. Lastly, this thesis features a number of applications of state-space (deep) GPs. These applications mainly include, (i) estimation of unknown drift functions of SDEs from partially observed trajectories and (ii) estimation of spectro-temporal features of signals.

</details>

<details>

<summary>2021-11-24 22:40:27 - Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges</summary>

- *Bernd Bischl, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, Theresa Ullmann, Marc Becker, Anne-Laure Boulesteix, Difan Deng, Marius Lindauer*

- `2107.05847v3` - [abs](http://arxiv.org/abs/2107.05847v3) - [pdf](http://arxiv.org/pdf/2107.05847v3)

> Most machine learning algorithms are configured by one or several hyperparameters that must be carefully chosen and often considerably impact performance. To avoid a time consuming and unreproducible manual trial-and-error process to find well-performing hyperparameter configurations, various automatic hyperparameter optimization (HPO) methods, e.g., based on resampling error estimation for supervised machine learning, can be employed. After introducing HPO from a general perspective, this paper reviews important HPO methods such as grid or random search, evolutionary algorithms, Bayesian optimization, Hyperband and racing. It gives practical recommendations regarding important choices to be made when conducting HPO, including the HPO algorithms themselves, performance evaluation, how to combine HPO with ML pipelines, runtime improvements, and parallelization. This work is accompanied by an appendix that contains information on specific software packages in R and Python, as well as information and recommended hyperparameter search spaces for specific learning algorithms. We also provide notebooks that demonstrate concepts from this work as supplementary files.

</details>

<details>

<summary>2021-11-25 05:23:37 - Contamination mapping in Bangladesh using a multivariate spatial Bayesian model for left-censored data</summary>

- *Indranil Sahoo, Arnab Hazra*

- `2106.15730v2` - [abs](http://arxiv.org/abs/2106.15730v2) - [pdf](http://arxiv.org/pdf/2106.15730v2)

> Arsenic (As) and other toxic elements contamination of groundwater in Bangladesh poses a major threat to millions of people on a daily basis. Understanding complex relationships between arsenic and other elements can provide useful insights for mitigating arsenic poisoning in drinking water and requires multivariate modeling of the elements. However, environmental monitoring of such contaminants often involves a substantial proportion of left-censored observations falling below a minimum detection limit (MDL). This problem motivates us to propose a multivariate spatial Bayesian model for left-censored data for investigating the abundance of arsenic in Bangladesh groundwater and for creating spatial maps of the contaminants. Inference about the model parameters is drawn using an adaptive Markov Chain Monte Carlo (MCMC) sampling. The computation time for the proposed model is of the same order as a multivariate Gaussian process model that does not impute the censored values. The proposed method is applied to the arsenic contamination dataset made available by the Bangladesh Water Development Board (BWDB). Spatial maps of arsenic, barium (Ba), and calcium (Ca) concentrations in groundwater are prepared using the posterior predictive means calculated on a fine lattice over Bangladesh. Our results indicate that Chittagong and Dhaka divisions suffer from excessive concentrations of arsenic and only the divisions of Rajshahi and Rangpur have safe drinking water based on recommendations by the World Health Organization (WHO).

</details>

<details>

<summary>2021-11-25 11:30:11 - Using sequential drift detection to test the API economy</summary>

- *Samuel Ackerman, Parijat Dube, Eitan Farchi*

- `2111.05136v2` - [abs](http://arxiv.org/abs/2111.05136v2) - [pdf](http://arxiv.org/pdf/2111.05136v2)

> The API economy refers to the widespread integration of API (advanced programming interface) microservices, where software applications can communicate with each other, as a crucial element in business models and functions. The number of possible ways in which such a system could be used is huge. It is thus desirable to monitor the usage patterns and identify when the system is used in a way that was never used before. This provides a warning to the system analysts and they can ensure uninterrupted operation of the system.   In this work we analyze both histograms and call graph of API usage to determine if the usage patterns of the system has shifted. We compare the application of nonparametric statistical and Bayesian sequential analysis to the problem. This is done in a way that overcomes the issue of repeated statistical tests and insures statistical significance of the alerts. The technique was simulated and tested and proven effective in detecting the drift in various scenarios. We also mention modifications to the technique to decrease its memory so that it can respond more quickly when the distribution drift occurs at a delay from when monitoring begins.

</details>

<details>

<summary>2021-11-26 00:45:08 - Generative Adversarial Networks and Adversarial Autoencoders: Tutorial and Survey</summary>

- *Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley*

- `2111.13282v1` - [abs](http://arxiv.org/abs/2111.13282v1) - [pdf](http://arxiv.org/pdf/2111.13282v1)

> This is a tutorial and survey paper on Generative Adversarial Network (GAN), adversarial autoencoders, and their variants. We start with explaining adversarial learning and the vanilla GAN. Then, we explain the conditional GAN and DCGAN. The mode collapse problem is introduced and various methods, including minibatch GAN, unrolled GAN, BourGAN, mixture GAN, D2GAN, and Wasserstein GAN, are introduced for resolving this problem. Then, maximum likelihood estimation in GAN are explained along with f-GAN, adversarial variational Bayes, and Bayesian GAN. Then, we cover feature matching in GAN, InfoGAN, GRAN, LSGAN, energy-based GAN, CatGAN, MMD GAN, LapGAN, progressive GAN, triple GAN, LAG, GMAN, AdaGAN, CoGAN, inverse GAN, BiGAN, ALI, SAGAN, Few-shot GAN, SinGAN, and interpolation and evaluation of GAN. Then, we introduce some applications of GAN such as image-to-image translation (including PatchGAN, CycleGAN, DeepFaceDrawing, simulated GAN, interactive GAN), text-to-image translation (including StackGAN), and mixing image characteristics (including FineGAN and MixNMatch). Finally, we explain the autoencoders based on adversarial learning including adversarial autoencoder, PixelGAN, and implicit autoencoder.

</details>

<details>

<summary>2021-11-26 02:23:05 - Approximate Bayesian Computation for Physical Inverse Modeling</summary>

- *Neel Chatterjee, Somya Sharma, Sarah Swisher, Snigdhansu Chatterjee*

- `2111.13296v1` - [abs](http://arxiv.org/abs/2111.13296v1) - [pdf](http://arxiv.org/pdf/2111.13296v1)

> Semiconductor device models are essential to understand the charge transport in thin film transistors (TFTs). Using these TFT models to draw inference involves estimating parameters used to fit to the experimental data. These experimental data can involve extracted charge carrier mobility or measured current. Estimating these parameters help us draw inferences about device performance. Fitting a TFT model for a given experimental data using the model parameters relies on manual fine tuning of multiple parameters by human experts. Several of these parameters may have confounding effects on the experimental data, making their individual effect extraction a non-intuitive process during manual tuning. To avoid this convoluted process, we propose a new method for automating the model parameter extraction process resulting in an accurate model fitting. In this work, model choice based approximate Bayesian computation (aBc) is used for generating the posterior distribution of the estimated parameters using observed mobility at various gate voltage values. Furthermore, it is shown that the extracted parameters can be accurately predicted from the mobility curves using gradient boosted trees. This work also provides a comparative analysis of the proposed framework with fine-tuned neural networks wherein the proposed framework is shown to perform better.

</details>

<details>

<summary>2021-11-26 05:42:26 - Bayesian Optimization for Cascade-type Multi-stage Processes</summary>

- *Shunya Kusakawa, Shion Takeno, Yu Inatsu, Kentaro Kutsukake, Shogo Iwazaki, Takashi Nakano, Toru Ujihara, Masayuki Karasuyama, Ichiro Takeuchi*

- `2111.08330v2` - [abs](http://arxiv.org/abs/2111.08330v2) - [pdf](http://arxiv.org/pdf/2111.08330v2)

> Complex processes in science and engineering are often formulated as multi-stage decision-making problems. In this paper, we consider a type of multi-stage decision-making process called a cascade process. A cascade process is a multi-stage process in which the output of one stage is used as an input for the next stage. When the cost of each stage is expensive, it is difficult to search for the optimal controllable parameters for each stage exhaustively. To address this problem, we formulate the optimization of the cascade process as an extension of Bayesian optimization framework and propose two types of acquisition functions (AFs) based on credible intervals and expected improvement. We investigate the theoretical properties of the proposed AFs and demonstrate their effectiveness through numerical experiments. In addition, we consider an extension called suspension setting in which we are allowed to suspend the cascade process at the middle of the multi-stage decision-making process that often arises in practical problems. We apply the proposed method in the optimization problem of the solar cell simulator, which was the motivation for this study.

</details>

<details>

<summary>2021-11-26 09:12:15 - Fitting Structural Equation Models via Variational Approximations</summary>

- *Khue-Dung Dang, Luca Maestrini*

- `2105.15036v2` - [abs](http://arxiv.org/abs/2105.15036v2) - [pdf](http://arxiv.org/pdf/2105.15036v2)

> Structural equation models are commonly used to capture the relationship between sets of observed and unobservable variables. Traditionally these models are fitted using frequentist approaches but recently researchers and practitioners have developed increasing interest in Bayesian inference. In Bayesian settings, inference for these models is typically performed via Markov chain Monte Carlo methods, which may be computationally intensive for models with a large number of manifest variables or complex structures. Variational approximations can be a fast alternative; however, they have not been adequately explored for this class of models. We develop a mean field variational Bayes approach for fitting elemental structural equation models and demonstrate how bootstrap can considerably improve the variational approximation quality. We show that this variational approximation method can provide reliable inference while being significantly faster than Markov chain Monte Carlo.

</details>

<details>

<summary>2021-11-26 10:01:15 - Distributed Computation for Marginal Likelihood based Model Choice</summary>

- *Alexander Buchholz, Daniel Ahfock, Sylvia Richardson*

- `1910.04672v3` - [abs](http://arxiv.org/abs/1910.04672v3) - [pdf](http://arxiv.org/pdf/1910.04672v3)

> We propose a general method for distributed Bayesian model choice, using the marginal likelihood, where a data set is split in non-overlapping subsets. These subsets are only accessed locally by individual workers and no data is shared between the workers. We approximate the model evidence for the full data set through Monte Carlo sampling from the posterior on every subset generating a model evidence per subset. The results are combined using a novel approach which corrects for the splitting using summary statistics of the generated samples. Our divide-and-conquer approach enables Bayesian model choice in the large data setting, exploiting all available information but limiting communication between workers. We derive theoretical error bounds that quantify the resulting trade-off between computational gain and loss in precision. The embarrassingly parallel nature yields important speed-ups when used on massive data sets as illustrated by our real world experiments. In addition, we show how the suggested approach can be extended to model choice within a reversible jump setting that explores multiple feature combinations within one run.

</details>

<details>

<summary>2021-11-28 10:48:07 - Bayesian multistate modelling of incomplete chronic disease burden data</summary>

- *Christopher Jackson, Belen Zapata-Diomedi, James Woodcock*

- `2111.14100v1` - [abs](http://arxiv.org/abs/2111.14100v1) - [pdf](http://arxiv.org/pdf/2111.14100v1)

> A widely-used model for determining the long-term health impacts of public health interventions, often called a "multistate lifetable", requires estimates of incidence, case fatality, and sometimes also remission rates, for multiple diseases by age and gender. Generally, direct data on both incidence and case fatality are not available in every disease and setting. For example, we may know population mortality and prevalence rather than case fatality and incidence. This paper presents Bayesian continuous-time multistate models for estimating transition rates between disease states based on incomplete data. This builds on previous methods by using a formal statistical model with transparent data-generating assumptions, while providing accessible software as an R package. Rates for people of different ages and areas can be related flexibly through splines or hierarchical models. Previous methods are also extended to allow age-specific trends through calendar time. The model is used to estimate case fatality for multiple diseases in the city regions of England, based on incidence, prevalence and mortality data from the Global Burden of Disease study. The estimates can be used to inform health impact models relating to those diseases and areas. Different assumptions about rates are compared, and we check the influence of different data sources.

</details>

<details>

<summary>2021-11-28 14:37:08 - Optimal Inspection and Maintenance Planning for Deteriorating Structural Components through Dynamic Bayesian Networks and Markov Decision Processes</summary>

- *P. G. Morato, K. G. Papakonstantinou, C. P. Andriotis, J. S. Nielsen, P. Rigo*

- `2009.04547v2` - [abs](http://arxiv.org/abs/2009.04547v2) - [pdf](http://arxiv.org/pdf/2009.04547v2)

> Civil and maritime engineering systems, among others, from bridges to offshore platforms and wind turbines, must be efficiently managed as they are exposed to deterioration mechanisms throughout their operational life, such as fatigue or corrosion. Identifying optimal inspection and maintenance policies demands the solution of a complex sequential decision-making problem under uncertainty, with the main objective of efficiently controlling the risk associated with structural failures. Addressing this complexity, risk-based inspection planning methodologies, supported often by dynamic Bayesian networks, evaluate a set of pre-defined heuristic decision rules to reasonably simplify the decision problem. However, the resulting policies may be compromised by the limited space considered in the definition of the decision rules. Avoiding this limitation, Partially Observable Markov Decision Processes (POMDPs) provide a principled mathematical methodology for stochastic optimal control under uncertain action outcomes and observations, in which the optimal actions are prescribed as a function of the entire, dynamically updated, state probability distribution. In this paper, we combine dynamic Bayesian networks with POMDPs in a joint framework for optimal inspection and maintenance planning, and we provide the formulation for developing both infinite and finite horizon POMDPs in a structural reliability context. The proposed methodology is implemented and tested for the case of a structural component subject to fatigue deterioration, demonstrating the capability of state-of-the-art point-based POMDP solvers for solving the underlying planning optimization problem. Within the numerical experiments, POMDP and heuristic-based policies are thoroughly compared, and results showcase that POMDPs achieve substantially lower costs as compared to their counterparts, even for traditional problem settings.

</details>

<details>

<summary>2021-11-28 18:36:58 - Using the Softplus Function to Construct Alternative Link Functions in Generalized Linear Models and Beyond</summary>

- *Paul F. V. Wiemann, Thomas Kneib, Julien Hambuckers*

- `2111.14207v1` - [abs](http://arxiv.org/abs/2111.14207v1) - [pdf](http://arxiv.org/pdf/2111.14207v1)

> Response functions linking regression predictors to properties of the response distribution are fundamental components in many statistical models. However, the choice of these functions is typically based on the domain of the modeled quantities and is not further scrutinized. For example, the exponential response function is usually assumed for parameters restricted to be positive although it implies a multiplicative model which may not necessarily be desired. Consequently, applied researchers might easily face misleading results when relying on defaults without further investigation. As an alternative to the exponential response function, we propose the use of the softplus function to construct alternative link functions for parameters restricted to be positive. As a major advantage, we can construct differentiable link functions corresponding closely to the identity function for positive values of the regression predictor, which implies an quasi-additive model and thus allows for an additive interpretation of the estimated effects by practitioners. We demonstrate the applicability of the softplus response function using both simulations and real data. In four applications featuring count data regression and Bayesian distributional regression, we contrast our approach to the commonly used exponential response function.

</details>

<details>

<summary>2021-11-29 02:44:07 - A Variational Inference Approach to Inverse Problems with Gamma Hyperpriors</summary>

- *Shiv Agrawal, Hwanwoo Kim, Daniel Sanz-Alonso, Alexander Strang*

- `2111.13329v2` - [abs](http://arxiv.org/abs/2111.13329v2) - [pdf](http://arxiv.org/pdf/2111.13329v2)

> Hierarchical models with gamma hyperpriors provide a flexible, sparse-promoting framework to bridge $L^1$ and $L^2$ regularizations in Bayesian formulations to inverse problems. Despite the Bayesian motivation for these models, existing methodologies are limited to \textit{maximum a posteriori} estimation. The potential to perform uncertainty quantification has not yet been realized. This paper introduces a variational iterative alternating scheme for hierarchical inverse problems with gamma hyperpriors. The proposed variational inference approach yields accurate reconstruction, provides meaningful uncertainty quantification, and is easy to implement. In addition, it lends itself naturally to conduct model selection for the choice of hyperparameters. We illustrate the performance of our methodology in several computed examples, including a deconvolution problem and sparse identification of dynamical systems from time series data.

</details>

<details>

<summary>2021-11-29 07:46:19 - Markovian Persuasion</summary>

- *Ehud Lehrer, Dimitry Shaiderman*

- `2111.14365v1` - [abs](http://arxiv.org/abs/2111.14365v1) - [pdf](http://arxiv.org/pdf/2111.14365v1)

> In the classical Bayesian persuasion model an informed player and an uninformed one engage in a static interaction. The informed player, the sender, knows the state of nature, while the uninformed one, the receiver, does not. The informed player partially shares his private information with the receiver and the latter then, based on her belief about the state, takes an action. This action determines, together with the state of nature, the utility of both players. We consider a dynamic Bayesian persuasion situation where the state of nature evolves according to a Markovian law. In this repeated persuasion model an optimal disclosure strategy of the sender should, at any period, balance between getting high stage payoff and future implications on the receivers' beliefs. We discuss optimal strategies under different discount factors and characterize when the asymptotic value achieves the maximal value possible.

</details>

<details>

<summary>2021-11-29 09:32:09 - Dependence between Bayesian neural network units</summary>

- *Mariia Vladimirova, Julyan Arbel, Stéphane Girard*

- `2111.14397v1` - [abs](http://arxiv.org/abs/2111.14397v1) - [pdf](http://arxiv.org/pdf/2111.14397v1)

> The connection between Bayesian neural networks and Gaussian processes gained a lot of attention in the last few years, with the flagship result that hidden units converge to a Gaussian process limit when the layers width tends to infinity. Underpinning this result is the fact that hidden units become independent in the infinite-width limit. Our aim is to shed some light on hidden units dependence properties in practical finite-width Bayesian neural networks. In addition to theoretical results, we assess empirically the depth and width impacts on hidden units dependence properties.

</details>

<details>

<summary>2021-11-29 16:08:52 - Γ-convergence of Onsager-Machlup functionals. Part I: With applications to maximum a posteriori estimation in Bayesian inverse problems</summary>

- *Birzhan Ayanbayev, Ilja Klebanov, Han Cheng Lie, T. J. Sullivan*

- `2108.04597v3` - [abs](http://arxiv.org/abs/2108.04597v3) - [pdf](http://arxiv.org/pdf/2108.04597v3)

> The Bayesian solution to a statistical inverse problem can be summarised by a mode of the posterior distribution, i.e. a MAP estimator. The MAP estimator essentially coincides with the (regularised) variational solution to the inverse problem, seen as minimisation of the Onsager-Machlup functional of the posterior measure. An open problem in the stability analysis of inverse problems is to establish a relationship between the convergence properties of solutions obtained by the variational approach and by the Bayesian approach. To address this problem, we propose a general convergence theory for modes that is based on the $\Gamma$-convergence of Onsager-Machlup functionals, and apply this theory to Bayesian inverse problems with Gaussian and edge-preserving Besov priors. Part II of this paper considers more general prior distributions.

</details>

<details>

<summary>2021-11-29 16:08:57 - Γ-convergence of Onsager-Machlup functionals. Part II: Infinite product measures on Banach spaces</summary>

- *Birzhan Ayanbayev, Ilja Klebanov, Han Cheng Lie, T. J. Sullivan*

- `2108.04598v3` - [abs](http://arxiv.org/abs/2108.04598v3) - [pdf](http://arxiv.org/pdf/2108.04598v3)

> We derive Onsager-Machlup functionals for countable product measures on weighted $\ell^p$ subspaces of the sequence space $\mathbb{R}^{\mathbb{N}}$. Each measure in the product is a shifted and scaled copy of a reference probability measure on $\mathbb{R}$ that admits a sufficiently regular Lebesgue density. We study the equicoercivity and $\Gamma$-convergence of sequences of Onsager-Machlup functionals associated to convergent sequences of measures within this class. We use these results to establish analogous results for probability measures on separable Banach or Hilbert spaces, including Gaussian, Cauchy, and Besov measures with summability parameter $1 \leq p \leq 2$. Together with Part I of this paper, this provides a basis for analysis of the convergence of maximum a posteriori estimators in Bayesian inverse problems and most likely paths in transition path theory.

</details>

<details>

<summary>2021-11-29 18:02:56 - Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers</summary>

- *Julia Moosbauer, Martin Binder, Lennart Schneider, Florian Pfisterer, Marc Becker, Michel Lang, Lars Kotthoff, Bernd Bischl*

- `2111.14756v1` - [abs](http://arxiv.org/abs/2111.14756v1) - [pdf](http://arxiv.org/pdf/2111.14756v1)

> Automated hyperparameter optimization (HPO) has gained great popularity and is an important ingredient of most automated machine learning frameworks. The process of designing HPO algorithms, however, is still an unsystematic and manual process: Limitations of prior work are identified and the improvements proposed are -- even though guided by expert knowledge -- still somewhat arbitrary. This rarely allows for gaining a holistic understanding of which algorithmic components are driving performance, and carries the risk of overlooking good algorithmic design choices. We present a principled approach to automated benchmark-driven algorithm design applied to multifidelity HPO (MF-HPO): First, we formalize a rich space of MF-HPO candidates that includes, but is not limited to common HPO algorithms, and then present a configurable framework covering this space. To find the best candidate automatically and systematically, we follow a programming-by-optimization approach and search over the space of algorithm candidates via Bayesian optimization. We challenge whether the found design choices are necessary or could be replaced by more naive and simpler ones by performing an ablation analysis. We observe that using a relatively simple configuration, in some ways simpler than established methods, performs very well as long as some critical configuration parameters have the right value.

</details>

<details>

<summary>2021-11-29 18:47:26 - Locally Learned Synaptic Dropout for Complete Bayesian Inference</summary>

- *Kevin L. McKee, Ian C. Crandell, Rishidev Chaudhuri, Randall C. O'Reilly*

- `2111.09780v3` - [abs](http://arxiv.org/abs/2111.09780v3) - [pdf](http://arxiv.org/pdf/2111.09780v3)

> The Bayesian brain hypothesis postulates that the brain accurately operates on statistical distributions according to Bayes' theorem. The random failure of presynaptic vesicles to release neurotransmitters may allow the brain to sample from posterior distributions of network parameters, interpreted as epistemic uncertainty. It has not been shown previously how random failures might allow networks to sample from observed distributions, also known as aleatoric or residual uncertainty. Sampling from both distributions enables probabilistic inference, efficient search, and creative or generative problem solving. We demonstrate that under a population-code based interpretation of neural activity, both types of distribution can be represented and sampled with synaptic failure alone. We first define a biologically constrained neural network and sampling scheme based on synaptic failure and lateral inhibition. Within this framework, we derive drop-out based epistemic uncertainty, then prove an analytic mapping from synaptic efficacy to release probability that allows networks to sample from arbitrary, learned distributions represented by a receiving layer. Second, our result leads to a local learning rule by which synapses adapt their release probabilities. Our result demonstrates complete Bayesian inference, related to the variational learning method of dropout, in a biologically constrained network using only locally-learned synaptic failure rates.

</details>

<details>

<summary>2021-11-29 20:58:57 - Model-based clustering via skewed matrix-variate cluster-weighted models</summary>

- *Michael P. B. Gallaugher, Salvatore D. Tomarchio, Paul D. McNicholas, Antonio Punzo*

- `2111.14952v1` - [abs](http://arxiv.org/abs/2111.14952v1) - [pdf](http://arxiv.org/pdf/2111.14952v1)

> Cluster-weighted models (CWMs) extend finite mixtures of regressions (FMRs) in order to allow the distribution of covariates to contribute to the clustering process. In a matrix-variate framework, the matrix-variate normal CWM has been recently introduced. However, problems may be encountered when data exhibit skewness or other deviations from normality in the responses, covariates or both. Thus, we introduce a family of 24 matrix-variate CWMs which are obtained by allowing both the responses and covariates to be modelled by using one of four existing skewed matrix-variate distributions or the matrix-variate normal distribution. Endowed with a greater flexibility, our matrix-variate CWMs are able to handle this kind of data in a more suitable manner. As a by-product, the four skewed matrix-variate FMRs are also introduced. Maximum likelihood parameter estimates are derived using an expectation-conditional maximization algorithm. Parameter recovery, classification assessment, and the capability of the Bayesian information criterion to detect the underlying groups are investigated using simulated data. Lastly, our matrix-variate CWMs, along with the matrix-variate normal CWM and matrix-variate FMRs, are applied to two real datasets for illustrative purposes.

</details>

<details>

<summary>2021-11-29 22:08:11 - Metropolis-Hastings via Classification</summary>

- *Tetsuya Kaji, Veronika Rockova*

- `2103.04177v3` - [abs](http://arxiv.org/abs/2103.04177v3) - [pdf](http://arxiv.org/pdf/2103.04177v3)

> This paper develops a Bayesian computational platform at the interface between posterior sampling and optimization in models whose marginal likelihoods are difficult to evaluate. Inspired by adversarial optimization, namely Generative Adversarial Networks (GAN), we reframe the likelihood function estimation problem as a classification problem. Pitting a Generator, who simulates fake data, against a Classifier, who tries to distinguish them from the real data, one obtains likelihood (ratio) estimators which can be plugged into the Metropolis-Hastings algorithm. The resulting Markov chains generate, at a steady state, samples from an approximate posterior whose asymptotic properties we characterize. Drawing upon connections with empirical Bayes and Bayesian mis-specification, we quantify the convergence rate in terms of the contraction speed of the actual posterior and the convergence rate of the Classifier. Asymptotic normality results are also provided which justify inferential potential of our approach. We illustrate the usefulness of our approach on examples which have posed a challenge for existing Bayesian likelihood-free approaches.

</details>

<details>

<summary>2021-11-30 07:16:50 - Asymptotically Optimal Sampling Policy for Selecting Top-m Alternatives</summary>

- *Gongbo Zhang, Yijie Peng, Jianghua Zhang, Enlu Zhou*

- `2111.15172v1` - [abs](http://arxiv.org/abs/2111.15172v1) - [pdf](http://arxiv.org/pdf/2111.15172v1)

> We consider selecting the top-$m$ alternatives from a finite number of alternatives via Monte Carlo simulation. Under a Bayesian framework, we formulate the sampling decision as a stochastic dynamic programming problem, and develop a sequential sampling policy that maximizes a value function approximation one-step look ahead. To show the asymptotic optimality of the proposed procedure, the asymptotically optimal sampling ratios which optimize large deviations rate of the probability of false selection for selecting top-$m$ alternatives has been rigorously defined. The proposed sampling policy is not only proved to be consistent but also achieves the asymptotically optimal sampling ratios. Numerical experiments demonstrate superiority of the proposed allocation procedure over existing ones.

</details>

<details>

<summary>2021-11-30 09:56:15 - Optimal friction matrix for underdamped Langevin sampling</summary>

- *Martin Chak, Nikolas Kantas, Tony Lelièvre, Grigorios A. Pavliotis*

- `2112.06844v1` - [abs](http://arxiv.org/abs/2112.06844v1) - [pdf](http://arxiv.org/pdf/2112.06844v1)

> A systematic procedure for optimising the friction coefficient in underdamped Langevin dynamics as a sampling tool is given by taking the gradient of the associated asymptotic variance with respect to friction. We give an expression for this gradient in terms of the solution to an appropriate Poisson equation and show that it can be approximated by short simulations of the associated first variation/tangent process under concavity assumptions on the log density. Our algorithm is applied to the estimation of posterior means in Bayesian inference problems and reduced variance is demonstrated when compared to the original underdamped and overdamped Langevin dynamics in both full and stochastic gradient cases.

</details>

<details>

<summary>2021-11-30 12:41:04 - Finding, Scoring and Explaining Arguments in Bayesian Networks</summary>

- *Jaime Sevilla*

- `2112.00799v1` - [abs](http://arxiv.org/abs/2112.00799v1) - [pdf](http://arxiv.org/pdf/2112.00799v1)

> We propose a new approach to explain Bayesian Networks. The approach revolves around a new definition of a probabilistic argument and the evidence it provides. We define a notion of independent arguments, and propose an algorithm to extract a list of relevant, independent arguments given a Bayesian Network, a target node and a set of observations. To demonstrate the relevance of the arguments, we show how we can use the extracted arguments to approximate message passing. Finally, we show a simple scheme to explain the arguments in natural language.

</details>

<details>

<summary>2021-11-30 13:22:07 - Perspectives on Constrained Forecasting</summary>

- *Mike West*

- `2007.11037v3` - [abs](http://arxiv.org/abs/2007.11037v3) - [pdf](http://arxiv.org/pdf/2007.11037v3)

> This expository paper discusses Bayesian decision analysis perspectives on problems of constrained forecasting. Foundational and pedagogic discussion contrasts decision analytic approaches with the traditional, but typically inappropriate, inferential approach. Illustrative examples include development of novel constrained point forecasting and entropic tilting methodology to explore consistency of a predictive distribution with an imposed or hypothesized constraint. Linear, aggregate constraints define illuminating examples that relate to broadly important problems involving aggregate and hierarchical constraints in commercial and economic forecasting. Discussion explores the impact of different loss functions, questions of how constrained forecasting is impacted by dependencies among outcomes being predicted, and promotes the broader use of decision analysis including routine evaluation of predictive distributions of loss under chosen forecasts/decisions. Extensions to more general constrained forecasting problems, connections with broader interests in forecast reconciliation and other considerations are noted.

</details>

<details>

<summary>2021-11-30 14:35:43 - A Puzzle of Proportions: Two Popular Bayesian Tests Can Yield Dramatically Different Conclusions</summary>

- *Fabian Dablander, Karoline Huth, Quentin F. Gronau, Alexander Etz, Eric-Jan Wagenmakers*

- `2108.04909v2` - [abs](http://arxiv.org/abs/2108.04909v2) - [pdf](http://arxiv.org/pdf/2108.04909v2)

> Testing the equality of two proportions is a common procedure in science, especially in medicine and public health. In these domains it is crucial to be able to quantify evidence for the absence of a treatment effect. Bayesian hypothesis testing by means of the Bayes factor provides one avenue to do so, requiring the specification of prior distributions for parameters. The most popular analysis approach views the comparison of proportions from a contingency table perspective, assigning prior distributions directly to the two proportions. Another, less popular approach views the problem from a logistic regression perspective, assigning prior distributions to logit-transformed parameters. Reanalyzing 39 null results from the New England Journal of Medicine with both approaches, we find that they can lead to markedly different conclusions, especially when the observed proportions are at the extremes (i.e., very low or very high). We explain these stark differences and provide recommendations for researchers interested in testing the equality of two proportions and users of Bayes factors more generally. The test that assigns prior distributions to logit-transformed parameters creates prior dependence between the two proportions and yields weaker evidence when the observations are at the extremes. When comparing two proportions, we argue that this test should become the new default.

</details>

<details>

<summary>2021-11-30 15:31:03 - Bayesian Modelling of Multivalued Power Curves from an Operational Wind Farm</summary>

- *L. A. Bull, P. A. Gardner, T. J. Rogers, N. Dervilis, E. J. Cross, E. Papatheou, A. E. Maguire, C. Campos, K. Worden*

- `2111.15496v1` - [abs](http://arxiv.org/abs/2111.15496v1) - [pdf](http://arxiv.org/pdf/2111.15496v1)

> Power curves capture the relationship between wind speed and output power for a specific wind turbine. Accurate regression models of this function prove useful in monitoring, maintenance, design, and planning. In practice, however, the measurements do not always correspond to the ideal curve: power curtailments will appear as (additional) functional components. Such multivalued relationships cannot be modelled by conventional regression, and the associated data are usually removed during pre-processing. The current work suggests an alternative method to infer multivalued relationships in curtailed power data. Using a population-based approach, an overlapping mixture of probabilistic regression models is applied to signals recorded from turbines within an operational wind farm. The model is shown to provide an accurate representation of practical power data across the population.

</details>

<details>

<summary>2021-11-30 18:33:53 - Dynamic Inference</summary>

- *Aolin Xu*

- `2111.14746v2` - [abs](http://arxiv.org/abs/2111.14746v2) - [pdf](http://arxiv.org/pdf/2111.14746v2)

> Traditional statistical estimation, or statistical inference in general, is static, in the sense that the estimate of the quantity of interest does not change the future evolution of the quantity. In some sequential estimation problems however, we encounter the situation where the future values of the quantity to be estimated depend on the estimate of its current value. Examples include stock price prediction by big investors, interactive product recommendation, and behavior prediction in multi-agent systems. We may call such problems as dynamic inference. In this work, a formulation of this problem under a Bayesian probabilistic framework is given, and the optimal estimation strategy is derived as the solution to minimize the overall inference loss. How the optimal estimation strategy works is illustrated through two examples, stock trend prediction and vehicle behavior prediction. When the underlying models for dynamic inference are unknown, we can consider the problem of learning for dynamic inference. This learning problem can potentially unify several familiar machine learning problems, including supervised learning, imitation learning, and reinforcement learning.

</details>

<details>

<summary>2021-11-30 21:50:57 - Heterogeneous causal effects with imperfect compliance: a Bayesian machine learning approach</summary>

- *Falco J. Bargagli-Stoffi, Kristof De-Witte, Giorgio Gnecco*

- `1905.12707v4` - [abs](http://arxiv.org/abs/1905.12707v4) - [pdf](http://arxiv.org/pdf/1905.12707v4)

> This paper introduces an innovative Bayesian machine learning algorithm to draw interpretable inference on heterogeneous causal effects in the presence of imperfect compliance (e.g., under an irregular assignment mechanism). We show, through Monte Carlo simulations, that the proposed Bayesian Causal Forest with Instrumental Variable (BCF-IV) methodology outperforms other machine learning techniques tailored for causal inference in discovering and estimating the heterogeneous causal effects while controlling for the familywise error rate (or - less stringently - for the false discovery rate) at leaves' level. BCF-IV sheds a light on the heterogeneity of causal effects in instrumental variable scenarios and, in turn, provides the policy-makers with a relevant tool for targeted policies. Its empirical application evaluates the effects of additional funding on students' performances. The results indicate that BCF-IV could be used to enhance the effectiveness of school funding on students' performance.

</details>


## 2021-12

<details>

<summary>2021-12-01 07:08:18 - Manifold lifting: scaling MCMC to the vanishing noise regime</summary>

- *Khai Xiang Au, Matthew M. Graham, Alexandre H. Thiery*

- `2003.03950v2` - [abs](http://arxiv.org/abs/2003.03950v2) - [pdf](http://arxiv.org/pdf/2003.03950v2)

> Standard Markov chain Monte Carlo methods struggle to explore distributions that are concentrated in the neighbourhood of low-dimensional structures. These pathologies naturally occur in a number of situations. For example, they are common to Bayesian inverse problem modelling and Bayesian neural networks, when observational data are highly informative, or when a subset of the statistical parameters of interest are non-identifiable. In this paper, we propose a strategy that transforms the original sampling problem into the task of exploring a distribution supported on a manifold embedded in a higher dimensional space; in contrast to the original posterior this lifted distribution remains diffuse in the vanishing noise limit. We employ a constrained Hamiltonian Monte Carlo method which exploits the manifold geometry of this lifted distribution, to perform efficient approximate inference. We demonstrate in several numerical experiments that, contrarily to competing approaches, the sampling efficiency of our proposed methodology does not degenerate as the target distribution to be explored concentrates near low dimensional structures.

</details>

<details>

<summary>2021-12-01 08:19:51 - Prior knowledge elicitation: The past, present, and future</summary>

- *Petrus Mikkola, Osvaldo A. Martin, Suyog Chandramouli, Marcelo Hartmann, Oriol Abril Pla, Owen Thomas, Henri Pesonen, Jukka Corander, Aki Vehtari, Samuel Kaski, Paul-Christian Bürkner, Arto Klami*

- `2112.01380v1` - [abs](http://arxiv.org/abs/2112.01380v1) - [pdf](http://arxiv.org/pdf/2112.01380v1)

> Specification of the prior distribution for a Bayesian model is a central part of the Bayesian workflow for data analysis, but it is often difficult even for statistical experts. Prior elicitation transforms domain knowledge of various kinds into well-defined prior distributions, and offers a solution to the prior specification problem, in principle. In practice, however, we are still fairly far from having usable prior elicitation tools that could significantly influence the way we build probabilistic models in academia and industry. We lack elicitation methods that integrate well into the Bayesian workflow and perform elicitation efficiently in terms of costs of time and effort. We even lack a comprehensive theoretical framework for understanding different facets of the prior elicitation problem.   Why are we not widely using prior elicitation? We analyze the state of the art by identifying a range of key aspects of prior knowledge elicitation, from properties of the modelling task and the nature of the priors to the form of interaction with the expert. The existing prior elicitation literature is reviewed and categorized in these terms. This allows recognizing under-studied directions in prior elicitation research, finally leading to a proposal of several new avenues to improve prior elicitation methodology.

</details>

<details>

<summary>2021-12-01 12:23:42 - Subgroup Analysis for Longitudinal data via Semiparametric Additive Mixed Effect Model</summary>

- *Xiaolin Bo, Weiping Zhang*

- `2112.00453v1` - [abs](http://arxiv.org/abs/2112.00453v1) - [pdf](http://arxiv.org/pdf/2112.00453v1)

> In this paper, we propose a general subgroup analysis framework based on semiparametric additive mixed effect models in longitudinal analysis, which can identify subgroups on each covariate and estimate the corresponding regression functions simultaneously. In addition, the proposed procedure is applicable for both balanced and unbalanced longitudinal data. A backfitting combined with k-means algorithm is developed to estimate each semiparametric additive component across subgroups and detect subgroup structure on each covariate respectively. The actual number of groups is estimated by minimizing a Bayesian information criteria. The numerical studies demonstrate the efficacy and accuracy of the proposed procedure in identifying the subgroups and estimating the regression functions. In addition, we illustrate the usefulness of our method with an application to PBC data and provide a meaningful partition of the population.

</details>

<details>

<summary>2021-12-01 16:05:54 - An Online Learning Framework for Energy-Efficient Navigation of Electric Vehicles</summary>

- *Niklas Åkerblom, Yuxin Chen, Morteza Haghir Chehreghani*

- `2003.01416v3` - [abs](http://arxiv.org/abs/2003.01416v3) - [pdf](http://arxiv.org/pdf/2003.01416v3)

> Energy-efficient navigation constitutes an important challenge in electric vehicles, due to their limited battery capacity. We employ a Bayesian approach to model the energy consumption at road segments for efficient navigation. In order to learn the model parameters, we develop an online learning framework and investigate several exploration strategies such as Thompson Sampling and Upper Confidence Bound. We then extend our online learning framework to multi-agent setting, where multiple vehicles adaptively navigate and learn the parameters of the energy model. We analyze Thompson Sampling and establish rigorous regret bounds on its performance. Finally, we demonstrate the performance of our methods via several real-world experiments on Luxembourg SUMO Traffic dataset.

</details>

<details>

<summary>2021-12-01 18:05:45 - Algebra, Geometry and Topology of ERK Kinetics</summary>

- *Lewis Marsh, Emilie Dufresne, Helen M. Byrne, Heather A. Harrington*

- `2112.00688v1` - [abs](http://arxiv.org/abs/2112.00688v1) - [pdf](http://arxiv.org/pdf/2112.00688v1)

> The MEK/ERK signalling pathway is involved in cell division, cell specialisation, survival and cell death. Here we study a polynomial dynamical system describing the dynamics of MEK/ERK proposed by Yeung et al. with their experimental setup, data and known biological information. The experimental dataset is a time-course of ERK measurements in different phosphorylation states following activation of either wild-type MEK or MEK mutations associated with cancer or developmental defects. We demonstrate how methods from computational algebraic geometry, differential algebra, Bayesian statistics and computational algebraic topology can inform the model reduction, identification and parameter inference of MEK variants, respectively. Throughout, we show how this algebraic viewpoint offers a rigorous and systematic analysis of such models.

</details>

<details>

<summary>2021-12-01 18:44:20 - hIPPYlib-MUQ: A Bayesian Inference Software Framework for Integration of Data with Complex Predictive Models under Uncertainty</summary>

- *Ki-Tae Kim, Umberto Villa, Matthew Parno, Youssef Marzouk, Omar Ghattas, Noemi Petra*

- `2112.00713v1` - [abs](http://arxiv.org/abs/2112.00713v1) - [pdf](http://arxiv.org/pdf/2112.00713v1)

> Bayesian inference provides a systematic means of quantifying uncertainty in the solution of the inverse problem. However, solution of Bayesian inverse problems governed by complex forward models described by partial differential equations (PDEs) remains prohibitive with black-box Markov chain Monte Carlo (MCMC) methods. We present hIPPYlib-MUQ, an extensible and scalable software framework that contains implementations of state-of-the art algorithms aimed to overcome the challenges of high-dimensional, PDE-constrained Bayesian inverse problems. hIPPYlib-MUQ integrates two complementary open-source software packages. hIPPYlib solves PDE-constrained inverse problems using automatically-generated adjoint-based derivatives, but it lacks full Bayesian capabilities. MUQ provides numerous powerful Bayesian inversion algorithms, but expects forward models to come equipped with derivatives to permit large-scale solution. By combining these two libraries, we created a robust, scalable, and efficient software framework that can be used to tackle complex large-scale Bayesian inverse problems across a broad spectrum of scientific and engineering disciplines. To illustrate the capabilities of hIPPYlib-MUQ, we compare a number of MCMC methods on several high-dimensional Bayesian inverse problems. The results demonstrate that large ($\sim 50\times$) speedups over conventional black box and gradient-based MCMC algorithms can be obtained by exploiting Hessian information (from the log-posterior), underscoring the power of the integrated hIPPYlib-MUQ framework.

</details>

<details>

<summary>2021-12-02 03:20:25 - Regularized Bayesian calibration and scoring of the WD-FAB IRT model improves predictive performance over marginal maximum likelihood</summary>

- *Joshua C. Chang, Julia Porcino, Elizabeth K. Rasch, Larry Tang*

- `2010.01396v2` - [abs](http://arxiv.org/abs/2010.01396v2) - [pdf](http://arxiv.org/pdf/2010.01396v2)

> Item response theory (IRT) is the statistical paradigm underlying a dominant family of generative probabilistic models for test responses, used to quantify traits in individuals relative to target populations. The graded response model (GRM) is a particular IRT model that is used for ordered polytomous test responses. Both the development and the application of the GRM and other IRT models require statistical decisions. For formulating these models (calibration), one needs to decide on methodologies for item selection, inference, and regularization. For applying these models (test scoring), one needs to make similar decisions, often prioritizing computational tractability and/or interpretability. In many applications, such as in the Work Disability Functional Assessment Battery (WD-FAB), tractability implies approximating an individual's score distribution using estimates of mean and variance, and obtaining that score conditional on only point estimates of the calibrated model. In this manuscript, we evaluate the calibration and scoring of models under this common use-case using Bayesian cross-validation. Applied to the WD-FAB responses collected for the National Institutes of Health, we assess the predictive power of implementations of the GRM based on their ability to yield, on validation sets of respondents, ability estimates that are most predictive of patterns of item responses. Our main finding indicates that regularized Bayesian calibration of the GRM outperforms the regularization-free empirical Bayesian procedure of marginal maximum likelihood. We also motivate the use of compactly supported priors in test scoring.

</details>

<details>

<summary>2021-12-02 07:00:28 - Bayesian design for minimising prediction uncertainty in bivariate spatial responses with applications to air quality monitoring</summary>

- *S. G. Jagath Senarathne, Werner G. Müller, James M. McGree*

- `2001.08308v3` - [abs](http://arxiv.org/abs/2001.08308v3) - [pdf](http://arxiv.org/pdf/2001.08308v3)

> Model-based geostatistical design involves the selection of locations to collect data to minimise an expected loss function over a set of all possible locations. The loss function is specified to reflect the aim of data collection, which, for geostatistical studies, could be to minimise the prediction uncertainty at unobserved locations. In this paper, we propose a new approach to design such studies via a loss function derived through considering the entropy about the model predictions and the parameters of the model. The approach also includes a multivariate extension to generalised linear spatial models, and thus can be used to design experiments with more than one response. Unfortunately, evaluating our proposed loss function is computationally expensive so we provide an approximation such that our approach can be adopted to design realistically sized geostatistical studies. This is demonstrated through a simulated study and through designing an air quality monitoring program in Queensland, Australia. The results show that our designs remain highly efficient in achieving each experimental objective individually, providing an ideal compromise between the two objectives. Accordingly, we advocate that our approach could be adopted more generally in model-based geostatistical design.

</details>

<details>

<summary>2021-12-02 13:35:54 - Bayesian Probabilistic Numerical Integration with Tree-Based Models</summary>

- *Harrison Zhu, Xing Liu, Ruya Kang, Zhichao Shen, Seth Flaxman, François-Xavier Briol*

- `2006.05371v3` - [abs](http://arxiv.org/abs/2006.05371v3) - [pdf](http://arxiv.org/pdf/2006.05371v3)

> Bayesian quadrature (BQ) is a method for solving numerical integration problems in a Bayesian manner, which allows users to quantify their uncertainty about the solution. The standard approach to BQ is based on a Gaussian process (GP) approximation of the integrand. As a result, BQ is inherently limited to cases where GP approximations can be done in an efficient manner, thus often prohibiting very high-dimensional or non-smooth target functions. This paper proposes to tackle this issue with a new Bayesian numerical integration algorithm based on Bayesian Additive Regression Trees (BART) priors, which we call BART-Int. BART priors are easy to tune and well-suited for discontinuous functions. We demonstrate that they also lend themselves naturally to a sequential design setting and that explicit convergence rates can be obtained in a variety of settings. The advantages and disadvantages of this new methodology are highlighted on a set of benchmark tests including the Genz functions, and on a Bayesian survey design problem.

</details>

<details>

<summary>2021-12-02 14:14:43 - Bradley-Terry Modeling with Multiple Game Outcomes with Applications to College Hockey</summary>

- *John T. Whelan, Jacob E. Klein*

- `2112.01267v1` - [abs](http://arxiv.org/abs/2112.01267v1) - [pdf](http://arxiv.org/pdf/2112.01267v1)

> The Bradley-Terry model has previously been used in both Bayesian and frequentist interpretations to evaluate the strengths of sports teams based on win-loss game results. It has also been extended to handle additional possible results such as ties. We implement a generalization which includes multiple possible outcomes such as wins or losses in regulation, overtime, or shootouts. A natural application is to ice hockey competitions such as international matches, European professional leagues, and NCAA hockey, all of which use a zero-sum point system which values overtime and shootout wins as 1/3 of a win, and overtime and shootout losses as 1/3 of a win. We incorporate this into the probability model, and evaluate the posterior distributions for the associated strength parameters using techniques such as Gaussian expansion about maximum a posteriori estimates, and Hamiltonian Monte Carlo.

</details>

<details>

<summary>2021-12-02 14:21:39 - Diagnostics for Monte Carlo Algorithms for Models with Intractable Normalizing Functions</summary>

- *Bokgyeong Kang, John Hughes, Murali Haran*

- `2109.05121v2` - [abs](http://arxiv.org/abs/2109.05121v2) - [pdf](http://arxiv.org/pdf/2109.05121v2)

> Models with intractable normalising functions have numerous applications ranging from network models to image analysis to spatial point processes. Because the normalising constants are functions of the parameters of interest, standard Markov chain Monte Carlo cannot be used for Bayesian inference for these models. A number of algorithms have been developed for such models. Some have the posterior distribution as the asymptotic distribution. Other "asymptotically inexact" algorithms do not possess this property. There is limited guidance for evaluating approximations based on these algorithms, and hence it is very hard to tune them. We propose two new diagnostics that address these problems for intractable normalising function models. Our first diagnostic, inspired by the second Bartlett identity, is, in principle, applicable in most any likelihood-based context where misspecification is of concern. We develop an approximate version that is applicable to intractable normalising function problems. Our second diagnostic is a Monte Carlo approximation to a kernel Stein discrepancy-based diagnostic introduced by Gorham and Mackey (2017). We provide theoretical justification for our methods and apply them to several algorithms in the context of challenging simulated and real data examples including an Ising model, an exponential random graph model, and a Markov point process.

</details>

<details>

<summary>2021-12-02 15:04:08 - Nonparametric posterior learning for emission tomography with multimodal data</summary>

- *Fedor Goncharov, Éric Barat, Thomas Dautremer*

- `2108.00866v4` - [abs](http://arxiv.org/abs/2108.00866v4) - [pdf](http://arxiv.org/pdf/2108.00866v4)

> We continue studies of the uncertainty quantification problem in emission tomographies such as PET or SPECT when additional multimodal data (e.g., anatomical MRI images) are available. To solve the aforementioned problem we adapt the recently proposed nonparametric posterior learning technique to the context of Poisson-type data in emission tomography. Using this approach we derive sampling algorithms which are trivially parallelizable, scalable and very easy to implement. In addition, we prove conditional consistency and tightness for the distribution of produced samples in the small noise limit (i.e., when the acquisition time tends to infinity) and derive new geometrical and necessary condition on how MRI images must be used. This condition arises naturally in the context of identifiability problem for misspecified generalized Poisson models. We also contrast our approach with Bayesian Markov Chain Monte Carlo sampling based on one data augmentation scheme which is very popular in the context of Expectation-Maximization algorithms for PET or SPECT. We show theoretically and also numerically that such data augmentation significantly increases mixing times for the Markov chain. In view of this, our algorithms seem to give a reasonable trade-off between design complexity, scalability, numerical load and assessment for the uncertainty.

</details>

<details>

<summary>2021-12-02 17:13:31 - A Practical & Unified Notation for Information-Theoretic Quantities in ML</summary>

- *Andreas Kirsch, Yarin Gal*

- `2106.12062v3` - [abs](http://arxiv.org/abs/2106.12062v3) - [pdf](http://arxiv.org/pdf/2106.12062v3)

> A practical notation can convey valuable intuitions and concisely express new ideas. Information theory is of importance to machine learning, but the notation for information-theoretic quantities is sometimes opaque. We propose a practical and unified notation and extend it to include information-theoretic quantities between observed outcomes (events) and random variables. This includes the point-wise mutual information known in NLP and mixed quantities such as specific surprise and specific information in the cognitive sciences and information gain in Bayesian optimal experimental design. We apply our notation to prove a version of Stirling's approximation for binomial coefficients mentioned by MacKa (2003) using new intuitions. We also concisely rederive the evidence lower bound for variational auto-encoders and variational inference in approximate Bayesian neural networks. Furthermore, we apply the notation to a popular information-theoretic acquisition function in Bayesian active learning which selects the most informative (unlabelled) samples to be labelled by an expert and extend this acquisition function to the core-set problem with the goal of selecting the most informative samples given the labels.

</details>

<details>

<summary>2021-12-02 18:26:30 - Why Calibration Error is Wrong Given Model Uncertainty: Using Posterior Predictive Checks with Deep Learning</summary>

- *Achintya Gopal*

- `2112.01477v1` - [abs](http://arxiv.org/abs/2112.01477v1) - [pdf](http://arxiv.org/pdf/2112.01477v1)

> Within the last few years, there has been a move towards using statistical models in conjunction with neural networks with the end goal of being able to better answer the question, "what do our models know?". From this trend, classical metrics such as Prediction Interval Coverage Probability (PICP) and new metrics such as calibration error have entered the general repertoire of model evaluation in order to gain better insight into how the uncertainty of our model compares to reality. One important component of uncertainty modeling is model uncertainty (epistemic uncertainty), a measurement of what the model does and does not know. However, current evaluation techniques tends to conflate model uncertainty with aleatoric uncertainty (irreducible error), leading to incorrect conclusions. In this paper, using posterior predictive checks, we show how calibration error and its variants are almost always incorrect to use given model uncertainty, and further show how this mistake can lead to trust in bad models and mistrust in good models. Though posterior predictive checks has often been used for in-sample evaluation of Bayesian models, we show it still has an important place in the modern deep learning world.

</details>

<details>

<summary>2021-12-02 19:42:04 - Invariant Priors for Bayesian Quadrature</summary>

- *Masha Naslidnyk, Javier Gonzalez, Maren Mahsereci*

- `2112.01578v1` - [abs](http://arxiv.org/abs/2112.01578v1) - [pdf](http://arxiv.org/pdf/2112.01578v1)

> Bayesian quadrature (BQ) is a model-based numerical integration method that is able to increase sample efficiency by encoding and leveraging known structure of the integration task at hand. In this paper, we explore priors that encode invariance of the integrand under a set of bijective transformations in the input domain, in particular some unitary transformations, such as rotations, axis-flips, or point symmetries. We show initial results on superior performance in comparison to standard Bayesian quadrature on several synthetic and one real world application.

</details>

<details>

<summary>2021-12-02 21:50:42 - Bayesian supervised predictive classification and hypothesis testing toolkit for partition exchangeability</summary>

- *Ville Kinnula, Jing Tang, Ali Amiryousefi*

- `2112.01618v1` - [abs](http://arxiv.org/abs/2112.01618v1) - [pdf](http://arxiv.org/pdf/2112.01618v1)

> Bayesian supervised predictive classifiers, hypothesis testing, and parametric estimation under Partition Exchangeability are implemented. The two classifiers presented are the marginal classifier (that assumes test data is i.i.d.) next to a more computationally costly but accurate simultaneous classifier (that finds a labelling for the entire test dataset at once based on simultanous use of all the test data to predict each label). We also provide the Maximum Likelihood Estimation (MLE) of the only underlying parameter of the partition exchangeability generative model as well as hypothesis testing statistics for equality of this parameter with a single value, alternative, or multiple samples. We present functions to simulate the sequences from Ewens Sampling Formula as the realisation of the Poisson-Dirichlet distribution and their respective probabilities.

</details>

<details>

<summary>2021-12-03 12:34:52 - Bayes in Wonderland! Predictive supervised classification inference hits unpredictability</summary>

- *Ali Amiryousefi, Ville Kinnula, Jing Tang*

- `2112.01880v1` - [abs](http://arxiv.org/abs/2112.01880v1) - [pdf](http://arxiv.org/pdf/2112.01880v1)

> The marginal Bayesian predictive classifiers (mBpc) as opposed to the simultaneous Bayesian predictive classifiers (sBpc), handle each data separately and hence tacitly assumes the independence of the observations. However, due to saturation in learning of generative model parameters, the adverse effect of this false assumption on the accuracy of mBpc tends to wear out in face of increasing amount of training data; guaranteeing the convergence of these two classifiers under de Finetti type of exchangeability. This result however, is far from trivial for the sequences generated under Partition exchangeability (PE), where even umpteen amount of training data is not ruling out the possibility of an unobserved outcome (Wonderland!). We provide a computational scheme that allows the generation of the sequences under PE. Based on that, with controlled increase of the training data, we show the convergence of the sBpc and mBpc. This underlies the use of simpler yet computationally more efficient marginal classifiers instead of simultaneous. We also provide a parameter estimation of the generative model giving rise to the partition exchangeable sequence as well as a testing paradigm for the equality of this parameter across different samples. The package for Bayesian predictive supervised classifications, parameter estimation and hypothesis testing of the Ewens Sampling formula generative model is deposited on CRAN as PEkit package and free available from https://github.com/AmiryousefiLab/PEkit.

</details>

<details>

<summary>2021-12-03 12:41:57 - Randomized multilevel Monte Carlo for embarrassingly parallel inference</summary>

- *Ajay Jasra, Kody J. H. Law, Alexander Tarakanov, Fangyuan Yu*

- `2107.01913v2` - [abs](http://arxiv.org/abs/2107.01913v2) - [pdf](http://arxiv.org/pdf/2107.01913v2)

> This position paper summarizes a recently developed research program focused on inference in the context of data centric science and engineering applications, and forecasts its trajectory forward over the next decade. Often one endeavours in this context to learn complex systems in order to make more informed predictions and high stakes decisions under uncertainty. Some key challenges which must be met in this context are robustness, generalizability, and interpretability. The Bayesian framework addresses these three challenges elegantly, while bringing with it a fourth, undesirable feature: it is typically far more expensive than its deterministic counterparts. In the 21st century, and increasingly over the past decade, a growing number of methods have emerged which allow one to leverage cheap low-fidelity models in order to precondition algorithms for performing inference with more expensive models and make Bayesian inference tractable in the context of high-dimensional and expensive models. Notable examples are multilevel Monte Carlo (MLMC), multi-index Monte Carlo (MIMC), and their randomized counterparts (rMLMC), which are able to provably achieve a dimension-independent (including $\infty-$dimension) canonical complexity rate with respect to mean squared error (MSE) of $1/$MSE. Some parallelizability is typically lost in an inference context, but recently this has been largely recovered via novel double randomization approaches. Such an approach delivers i.i.d. samples of quantities of interest which are unbiased with respect to the infinite resolution target distribution. Over the coming decade, this family of algorithms has the potential to transform data centric science and engineering, as well as classical machine learning applications such as deep learning, by scaling up and scaling out fully Bayesian inference.

</details>

<details>

<summary>2021-12-03 17:24:44 - Bayesian nonparametric strategies for power maximization in rare variants association studies</summary>

- *Lorenzo Masoero, Joshua Schraiber, Tamara Broderick*

- `2112.02032v1` - [abs](http://arxiv.org/abs/2112.02032v1) - [pdf](http://arxiv.org/pdf/2112.02032v1)

> Rare variants are hypothesized to be largely responsible for heritability and susceptibility to disease in humans. So rare variants association studies hold promise for understanding disease. Conversely though, the rareness of the variants poses practical challenges; since these variants are present in few individuals, it can be difficult to develop data-collection and statistical methods that effectively leverage their sparse information. In this work, we develop a novel Bayesian nonparametric model to capture how design choices in rare variants association studies can impact their usefulness. We then show how to use our model to guide design choices under a fixed experimental budget in practice. In particular, we provide a practical workflow and illustrative experiments on simulated data.

</details>

<details>

<summary>2021-12-03 18:19:29 - Clustering Areal Units at Multiple Levels of Resolution to Model Crime Incidence in Philadelphia</summary>

- *Cecilia Balocchi, Edward I. George, Shane T. Jensen*

- `2112.02059v1` - [abs](http://arxiv.org/abs/2112.02059v1) - [pdf](http://arxiv.org/pdf/2112.02059v1)

> Estimation of the spatial heterogeneity in crime incidence across an entire city is an important step towards reducing crime and increasing our understanding of the physical and social functioning of urban environments. This is a difficult modeling endeavor since crime incidence can vary smoothly across space and time but there also exist physical and social barriers that result in discontinuities in crime rates between different regions within a city. A further difficulty is that there are different levels of resolution that can be used for defining regions of a city in order to analyze crime. To address these challenges, we develop a Bayesian non-parametric approach for the clustering of urban areal units at different levels of resolution simultaneously. Our approach is evaluated with an extensive synthetic data study and then applied to the estimation of crime incidence at various levels of resolution in the city of Philadelphia.

</details>

<details>

<summary>2021-12-03 20:49:56 - Zero-state Coupled Markov Switching Count Models for Spatio-temporal Infectious Disease Spread</summary>

- *Dirk Douwes-Schultz, Alexandra M. Schmidt*

- `2102.02334v2` - [abs](http://arxiv.org/abs/2102.02334v2) - [pdf](http://arxiv.org/pdf/2102.02334v2)

> Spatio-temporal counts of infectious disease cases often contain an excess of zeros. With existing zero inflated count models applied to such data it is difficult to quantify space-time heterogeneity in the effects of disease spread between areas. Also, existing methods do not allow for separate dynamics to affect the reemergence and persistence of the disease. As an alternative, we develop a new zero-state coupled Markov switching negative binomial model, under which the disease switches between periods of presence and absence in each area through a series of partially hidden nonhomogeneous Markov chains coupled between neighboring locations. When the disease is present, an autoregressive negative binomial model generates the cases with a possible 0 representing the disease being undetected. Bayesian inference and prediction is illustrated using spatio-temporal counts of dengue fever cases in Rio de Janeiro, Brazil.

</details>

<details>

<summary>2021-12-03 21:59:16 - A Bayesian accelerated failure time model for interval censored three-state screening outcomes</summary>

- *Thomas Klausch, Eddymurphy U. Akwiwu, Mark A. van de Wiel, Veerle M. H. Coupe, Johannes Berkhof*

- `2110.02649v2` - [abs](http://arxiv.org/abs/2110.02649v2) - [pdf](http://arxiv.org/pdf/2110.02649v2)

> Women infected by the Human papilloma virus are at an increased risk to develop cervical intraepithalial neoplasia lesions (CIN). CIN are classified into three grades of increasing severity (CIN-1, CIN-2, and CIN-3) and can eventually develop into cervical cancer. The main purpose of screening is detecting CIN-2 and CIN-3 cases which are usually surgically removed. Screening data from the POBASCAM trial involving 1,454 HPV-positive women is analyzed with two objectives: estimate (a) the transition time from HPV diagnosis to CIN-3; and (b) the transition time from CIN-2 to CIN-3. The screening data have two key characteristics. First, the CIN state is monitored in an interval-censored sequence of screening times. Second, a woman's progression to CIN-3 is only observed, if the woman progresses to, both, CIN-2 and from CIN-2 to CIN-3 in the same screening interval. We propose a Bayesian accelerated failure time model for the two transition times in this three-state model. To deal with the unusual censoring structure of the screening data, we develop a Metropolis-within-Gibbs algorithm with data augmentation from the truncated transition time distributions.

</details>

<details>

<summary>2021-12-03 22:24:34 - Generalized Transitional Markov Chain Monte Carlo Sampling Technique for Bayesian Inversion</summary>

- *Han Lu, Mohammad Khalil, Thomas Catanach, Jiefu Chen, Xuqing Wu, Xin Fu, Cosmin Safta, Yueqin Huang*

- `2112.02180v1` - [abs](http://arxiv.org/abs/2112.02180v1) - [pdf](http://arxiv.org/pdf/2112.02180v1)

> In the context of Bayesian inversion for scientific and engineering modeling, Markov chain Monte Carlo sampling strategies are the benchmark due to their flexibility and robustness in dealing with arbitrary posterior probability density functions (PDFs). However, these algorithms been shown to be inefficient when sampling from posterior distributions that are high-dimensional or exhibit multi-modality and/or strong parameter correlations. In such contexts, the sequential Monte Carlo technique of transitional Markov chain Monte Carlo (TMCMC) provides a more efficient alternative. Despite the recent applicability for Bayesian updating and model selection across a variety of disciplines, TMCMC may require a prohibitive number of tempering stages when the prior PDF is significantly different from the target posterior. Furthermore, the need to start with an initial set of samples from the prior distribution may present a challenge when dealing with implicit priors, e.g. based on feasible regions. Finally, TMCMC can not be used for inverse problems with improper prior PDFs that represent lack of prior knowledge on all or a subset of parameters. In this investigation, a generalization of TMCMC that alleviates such challenges and limitations is proposed, resulting in a tempering sampling strategy of enhanced robustness and computational efficiency. Convergence analysis of the proposed sequential Monte Carlo algorithm is presented, proving that the distance between the intermediate distributions and the target posterior distribution monotonically decreases as the algorithm proceeds. The enhanced efficiency associated with the proposed generalization is highlighted through a series of test inverse problems and an engineering application in the oil and gas industry.

</details>

<details>

<summary>2021-12-03 22:57:46 - Efficient Calibration of Multi-Agent Market Simulators from Time Series with Bayesian Optimization</summary>

- *Yuanlu Bai, Henry Lam, Svitlana Vyetrenko, Tucker Balch*

- `2112.03874v1` - [abs](http://arxiv.org/abs/2112.03874v1) - [pdf](http://arxiv.org/pdf/2112.03874v1)

> Multi-agent market simulation is commonly used to create an environment for downstream machine learning or reinforcement learning tasks, such as training or testing trading strategies before deploying them to real-time trading. In electronic trading markets only the price or volume time series, that result from interaction of multiple market participants, are typically directly observable. Therefore, multi-agent market environments need to be calibrated so that the time series that result from interaction of simulated agents resemble historical -- which amounts to solving a highly complex large-scale optimization problem. In this paper, we propose a simple and efficient framework for calibrating multi-agent market simulator parameters from historical time series observations. First, we consider a novel concept of eligibility set to bypass the potential non-identifiability issue. Second, we generalize the two-sample Kolmogorov-Smirnov (K-S) test with Bonferroni correction to test the similarity between two high-dimensional time series distributions, which gives a simple yet effective distance metric between the time series sample sets. Third, we suggest using Bayesian optimization (BO) and trust-region BO (TuRBO) to minimize the aforementioned distance metric. Finally, we demonstrate the efficiency of our framework using numerical experiments.

</details>

<details>

<summary>2021-12-03 23:42:58 - Bayesian Nonparametric View to Spawning</summary>

- *Bahman Moraffah*

- `2112.06640v1` - [abs](http://arxiv.org/abs/2112.06640v1) - [pdf](http://arxiv.org/pdf/2112.06640v1)

> In tracking multiple objects, it is often assumed that each observation (measurement) is originated from one and only one object. However, we may encounter a situation that each measurement may or may not be associated with multiple objects at each time step --spawning. Therefore, the association of each measurement to multiple objects is a crucial task to perform in order to track multiple objects with birth and death. In this paper, we introduce a novel Bayesian nonparametric approach that models a scenario where each observation may be drawn from an unknown number of objects for which it provides a tractable Markov chain Monte Carlo (MCMC) approach to sample from the posterior distribution. The number of objects at each time step, itself, is also assumed to be unknown. We, then, show through experiments the advantage of nonparametric modeling to scenarios with spawning events. Our experiment results also demonstrate the advantages of our framework over the existing methods.

</details>

<details>

<summary>2021-12-04 09:13:05 - Utilizing Expert Opinion to inform Extrapolation of Survival Models</summary>

- *Philip Cooney, Arthur White*

- `2112.02288v1` - [abs](http://arxiv.org/abs/2112.02288v1) - [pdf](http://arxiv.org/pdf/2112.02288v1)

> In decision modelling with time to event data, there are a variety of parametric models which could be used to extrapolate the survivor function. Each of these implies a different hazard function and in situations where there is moderate censoring, they can result in quite different extrapolations. Expert opinion on the long-term survival or other quantities could reduce model uncertainty. We present a general and easily implementable approach for including a variety of types of expert opinions. Expert opinion is incorporated by penalizing the likelihood function. Inference is performed in a Bayesian framework, however, this approach can also be implemented using frequentist methods. The issue of aggregating pooling expert opinions is also considered and included in the analysis. We validate the method against a previously published approach and include a worked example of this method. This work highlights that expert opinions can be implemented in a straightforward manner using this approach, however, more work is required on the correct elicitation of these quantities.

</details>

<details>

<summary>2021-12-05 14:22:02 - Bayesian Optimal Two-sample Tests in High-dimension</summary>

- *Kyoungjae Lee, Kisung You, Lizhen Lin*

- `2112.02580v1` - [abs](http://arxiv.org/abs/2112.02580v1) - [pdf](http://arxiv.org/pdf/2112.02580v1)

> We propose optimal Bayesian two-sample tests for testing equality of high-dimensional mean vectors and covariance matrices between two populations. In many applications including genomics and medical imaging, it is natural to assume that only a few entries of two mean vectors or covariance matrices are different. Many existing tests that rely on aggregating the difference between empirical means or covariance matrices are not optimal or yield low power under such setups. Motivated by this, we develop Bayesian two-sample tests employing a divide-and-conquer idea, which is powerful especially when the difference between two populations is sparse but large. The proposed two-sample tests manifest closed forms of Bayes factors and allow scalable computations even in high-dimensions. We prove that the proposed tests are consistent under relatively mild conditions compared to existing tests in the literature. Furthermore, the testable regions from the proposed tests turn out to be optimal in terms of rates. Simulation studies show clear advantages of the proposed tests over other state-of-the-art methods in various scenarios. Our tests are also applied to the analysis of the gene expression data of two cancer data sets.

</details>

<details>

<summary>2021-12-05 20:00:10 - Stochastic Local Winner-Takes-All Networks Enable Profound Adversarial Robustness</summary>

- *Konstantinos P. Panousis, Sotirios Chatzis, Sergios Theodoridis*

- `2112.02671v1` - [abs](http://arxiv.org/abs/2112.02671v1) - [pdf](http://arxiv.org/pdf/2112.02671v1)

> This work explores the potency of stochastic competition-based activations, namely Stochastic Local Winner-Takes-All (LWTA), against powerful (gradient-based) white-box and black-box adversarial attacks; we especially focus on Adversarial Training settings. In our work, we replace the conventional ReLU-based nonlinearities with blocks comprising locally and stochastically competing linear units. The output of each network layer now yields a sparse output, depending on the outcome of winner sampling in each block. We rely on the Variational Bayesian framework for training and inference; we incorporate conventional PGD-based adversarial training arguments to increase the overall adversarial robustness. As we experimentally show, the arising networks yield state-of-the-art robustness against powerful adversarial attacks while retaining very high classification rate in the benign case.

</details>

<details>

<summary>2021-12-06 01:20:56 - Covariance Structure Estimation with Laplace Approximation</summary>

- *Bongjung Sung, Jaeyong Lee*

- `2111.02637v3` - [abs](http://arxiv.org/abs/2111.02637v3) - [pdf](http://arxiv.org/pdf/2111.02637v3)

> Gaussian covariance graph model is a popular model in revealing underlying dependency structures among random variables. A Bayesian approach to the estimation of covariance structures uses priors that force zeros on some off-diagonal entries of covariance matrices and put a positive definite constraint on matrices. In this paper, we consider a spike and slab prior on off-diagonal entries, which uses a mixture of point-mass and normal distribution. The point-mass naturally introduces sparsity to covariance structures so that the resulting posterior from this prior renders covariance structure learning. Under this prior, we calculate posterior model probabilities of covariance structures using Laplace approximation. We show that the error due to Laplace approximation becomes asymptotically marginal at some rate depending on the posterior convergence rate of covariance matrix under the Frobenius norm. With the approximated posterior model probabilities, we propose a new framework for estimating a covariance structure. Since the Laplace approximation is done around the mode of conditional posterior of covariance matrix, which cannot be obtained in the closed form, we propose a block coordinate descent algorithm to find the mode and show that the covariance matrix can be estimated using this algorithm once the structure is chosen. Through a simulation study based on five numerical models, we show that the proposed method outperforms graphical lasso and sample covariance matrix in terms of root mean squared error, max norm, spectral norm, specificity, and sensitivity. Also, the advantage of the proposed method is demonstrated in terms of accuracy compared to our competitors when it is applied to linear discriminant analysis (LDA) classification to breast cancer diagnostic dataset.

</details>

<details>

<summary>2021-12-06 03:35:21 - BCD Nets: Scalable Variational Approaches for Bayesian Causal Discovery</summary>

- *Chris Cundy, Aditya Grover, Stefano Ermon*

- `2112.02761v1` - [abs](http://arxiv.org/abs/2112.02761v1) - [pdf](http://arxiv.org/pdf/2112.02761v1)

> A structural equation model (SEM) is an effective framework to reason over causal relationships represented via a directed acyclic graph (DAG). Recent advances have enabled effective maximum-likelihood point estimation of DAGs from observational data. However, a point estimate may not accurately capture the uncertainty in inferring the underlying graph in practical scenarios, wherein the true DAG is non-identifiable and/or the observed dataset is limited. We propose Bayesian Causal Discovery Nets (BCD Nets), a variational inference framework for estimating a distribution over DAGs characterizing a linear-Gaussian SEM. Developing a full Bayesian posterior over DAGs is challenging due to the the discrete and combinatorial nature of graphs. We analyse key design choices for scalable VI over DAGs, such as 1) the parametrization of DAGs via an expressive variational family, 2) a continuous relaxation that enables low-variance stochastic optimization, and 3) suitable priors over the latent variables. We provide a series of experiments on real and synthetic data showing that BCD Nets outperform maximum-likelihood methods on standard causal discovery metrics such as structural Hamming distance in low data regimes.

</details>

<details>

<summary>2021-12-06 07:40:54 - Two-step Lookahead Bayesian Optimization with Inequality Constraints</summary>

- *Yunxiang Zhang, Xiangyu Zhang, Peter I. Frazier*

- `2112.02833v1` - [abs](http://arxiv.org/abs/2112.02833v1) - [pdf](http://arxiv.org/pdf/2112.02833v1)

> Recent advances in computationally efficient non-myopic Bayesian optimization (BO) improve query efficiency over traditional myopic methods like expected improvement while only modestly increasing computational cost. These advances have been largely limited, however, to unconstrained optimization. For constrained optimization, the few existing non-myopic BO methods require heavy computation. For instance, one existing non-myopic constrained BO method [Lam and Willcox, 2017] relies on computationally expensive unreliable brute-force derivative-free optimization of a Monte Carlo rollout acquisition function. Methods that use the reparameterization trick for more efficient derivative-based optimization of non-myopic acquisition functions in the unconstrained setting, like sample average approximation and infinitesimal perturbation analysis, do not extend: constraints introduce discontinuities in the sampled acquisition function surface that hinder its optimization. Moreover, we argue here that being non-myopic is even more important in constrained problems because fear of violating constraints pushes myopic methods away from sampling the boundary between feasible and infeasible regions, slowing the discovery of optimal solutions with tight constraints. In this paper, we propose a computationally efficient two-step lookahead constrained Bayesian optimization acquisition function (2-OPT-C) supporting both sequential and batch settings. To enable fast acquisition function optimization, we develop a novel likelihood-ratio-based unbiased estimator of the gradient of the two-step optimal acquisition function that does not use the reparameterization trick. In numerical experiments, 2-OPT-C typically improves query efficiency by 2x or more over previous methods, and in some cases by 10x or more.

</details>

<details>

<summary>2021-12-06 08:38:20 - Joint Posterior Inference for Latent Gaussian Models with R-INLA</summary>

- *Cristian Chiuchiolo, Janet van Niekerk, Haavard Rue*

- `2112.02861v1` - [abs](http://arxiv.org/abs/2112.02861v1) - [pdf](http://arxiv.org/pdf/2112.02861v1)

> Efficient Bayesian inference remains a computational challenge in hierarchical models. Simulation-based approaches such as Markov Chain Monte Carlo methods are still popular but have a large computational cost. When dealing with the large class of Latent Gaussian Models, the INLA methodology embedded in the R-INLA software provides accurate Bayesian inference by computing deterministic mixture representation to approximate the joint posterior, from which marginals are computed. The INLA approach has from the beginning been targeting to approximate univariate posteriors. In this paper we lay out the development foundation of the tools for also providing joint approximations for subsets of the latent field. These approximations inherit Gaussian copula structure and additionally provide corrections for skewness. The same idea is carried forward also to sampling from the mixture representation, which we now can adjust for skewness.

</details>

<details>

<summary>2021-12-06 09:06:25 - A new measure between sets of probability distributions with applications to erratic financial behavior</summary>

- *Nick James, Max Menzies*

- `2106.07377v2` - [abs](http://arxiv.org/abs/2106.07377v2) - [pdf](http://arxiv.org/pdf/2106.07377v2)

> This paper introduces a new framework to quantify distance between finite sets with uncertainty present, where probability distributions determine the locations of individual elements. Combining this with a Bayesian change point detection algorithm, we produce a new measure of similarity between time series with respect to their structural breaks. First, we demonstrate the algorithm's effectiveness on a collection of piecewise autoregressive processes. Next, we apply this to financial data to study the erratic behavior profiles of 19 countries and 11 sectors over the past 20 years. Our measure provides quantitative evidence that there is greater collective similarity among sectors' erratic behavior profiles than those of countries, which we observe upon individual inspection of these time series. Our measure could be used as a new framework or complementary tool for investors seeking to make asset allocation decisions for financial portfolios.

</details>

<details>

<summary>2021-12-06 09:18:18 - Invitation in Crowdsourcing Contests</summary>

- *Qi Shi, Dong Hao*

- `2112.02884v1` - [abs](http://arxiv.org/abs/2112.02884v1) - [pdf](http://arxiv.org/pdf/2112.02884v1)

> In a crowdsourcing contest, a requester holding a task posts it to a crowd. People in the crowd then compete with each other to win the rewards. Although in real life, a crowd is usually networked and people influence each other via social ties, existing crowdsourcing contest theories do not aim to answer how interpersonal relationships influence peoples' incentives and behaviors, and thereby affect the crowdsourcing performance. In this work, we novelly take peoples' social ties as a key factor in the modeling and designing of agents' incentives for crowdsourcing contests. We then establish a new contest mechanism by which the requester can impel agents to invite their neighbours to contribute to the task. The mechanism has a simple rule and is very easy for agents to play. According to our equilibrium analysis, in the Bayesian Nash equilibrium agents' behaviors show a vast diversity, capturing that besides the intrinsic ability, the social ties among agents also play a central role for decision-making. After that, we design an effective algorithm to automatically compute the Bayesian Nash equilibrium of the invitation crowdsourcing contest and further adapt it to large graphs. Both theoretical and empirical results show that, the invitation crowdsourcing contest can substantially enlarge the number of contributors, whereby the requester can obtain significantly better solutions without a large advertisement expenditure.

</details>

<details>

<summary>2021-12-06 11:47:22 - Bayesian Estimation Approach for Linear Regression Models with Linear Inequality Restrictions</summary>

- *Solmaz Seifollahi, Kaniav Kamary, Hossein Bevrani*

- `2112.02950v1` - [abs](http://arxiv.org/abs/2112.02950v1) - [pdf](http://arxiv.org/pdf/2112.02950v1)

> Univariate and multivariate general linear regression models, subject to linear inequality constraints, arise in many scientific applications. The linear inequality restrictions on model parameters are often available from phenomenological knowledge and motivated by machine learning applications of high-consequence engineering systems (Agrell, 2019; Veiga and Marrel, 2012). Some studies on the multiple linear models consider known linear combinations of the regression coefficient parameters restricted between upper and lower bounds. In the present paper, we consider both univariate and multivariate general linear models subjected to this kind of linear restrictions. So far, research on univariate cases based on Bayesian methods is all under the condition that the coefficient matrix of the linear restrictions is a square matrix of full rank. This condition is not, however, always feasible. Another difficulty arises at the estimation step by implementing the Gibbs algorithm, which exhibits, in most cases, slow convergence. This paper presents a Bayesian method to estimate the regression parameters when the matrix of the constraints providing the set of linear inequality restrictions undergoes no condition. For the multivariate case, our Bayesian method estimates the regression parameters when the number of the constrains is less than the number of the regression coefficients in each multiple linear models. We examine the efficiency of our Bayesian method through simulation studies for both univariate and multivariate regressions. After that, we illustrate that the convergence of our algorithm is relatively faster than the previous methods. Finally, we use our approach to analyze two real datasets.

</details>

<details>

<summary>2021-12-06 14:45:30 - Dangers of Bayesian Model Averaging under Covariate Shift</summary>

- *Pavel Izmailov, Patrick Nicholson, Sanae Lotfi, Andrew Gordon Wilson*

- `2106.11905v2` - [abs](http://arxiv.org/abs/2106.11905v2) - [pdf](http://arxiv.org/pdf/2106.11905v2)

> Approximate Bayesian inference for neural networks is considered a robust alternative to standard training, often providing good performance on out-of-distribution data. However, Bayesian neural networks (BNNs) with high-fidelity approximate inference via full-batch Hamiltonian Monte Carlo achieve poor generalization under covariate shift, even underperforming classical estimation. We explain this surprising result, showing how a Bayesian model average can in fact be problematic under covariate shift, particularly in cases where linear dependencies in the input features cause a lack of posterior contraction. We additionally show why the same issue does not affect many approximate inference procedures, or classical maximum a-posteriori (MAP) training. Finally, we propose novel priors that improve the robustness of BNNs to many sources of covariate shift.

</details>

<details>

<summary>2021-12-06 14:58:39 - Living on the Edge: An Unified Approach to Antithetic Sampling</summary>

- *Roberto Casarin, Radu V. Craiu, Lorenzo Frattarolo, Christian P. Robert*

- `2110.15124v2` - [abs](http://arxiv.org/abs/2110.15124v2) - [pdf](http://arxiv.org/pdf/2110.15124v2)

> We identify recurrent ingredients in the antithetic sampling literature leading to a unified sampling framework. We introduce a new class of antithetic schemes that includes the most used antithetic proposals. This perspective enables the derivation of new properties of the sampling schemes: i) optimality in the Kullback-Leibler sense; ii) closed-form multivariate Kendall's $\tau$ and Spearman's $\rho$; iii)ranking in concordance order and iv) a central limit theorem that characterizes stochastic behavior of Monte Carlo estimators when the sample size tends to infinity. Finally, we provide applications to Monte Carlo integration and Markov Chain Monte Carlo Bayesian estimation.

</details>

<details>

<summary>2021-12-06 16:23:26 - Regularised B-splines projected Gaussian Process priors to estimate time-trends of age-specific COVID-19 deaths related to vaccine roll-out</summary>

- *Mélodie Monod, Alexandra Blenkinsop, Andrea Brizzi, Yu Chen, Carlos Cardoso Correia Perello, Vidoushee Jogarah, Yuanrong Wang, Seth Flaxman, Samir Bhatt, Oliver Ratmann*

- `2106.12360v2` - [abs](http://arxiv.org/abs/2106.12360v2) - [pdf](http://arxiv.org/pdf/2106.12360v2)

> The COVID-19 pandemic has caused severe public health consequences in the United States. In this study, we use a hierarchical Bayesian model to estimate the age-specific COVID-19 attributable deaths over time in the United States. The model is specified by a novel non-parametric spatial approach, a low-rank Gaussian Process (GP) projected by regularised B-splines. We show that this projection defines a new GP with attractive smoothness and computational efficiency properties, derive its kernel function, and discuss the penalty terms induced by the projected GP. Simulation analyses and benchmark results show that the spatial approach performs better than standard B-splines and Bayesian P-splines and equivalently well as a standard GP, for considerably lower runtimes. The B-splines projected GP priors that we develop are likely an appealing addition to the arsenal of Bayesian regularising priors. We apply the model to weekly, age-stratified COVID-19 attributable deaths reported by the US Centers for Disease Control, which are subject to censoring and reporting biases. Using the B-splines projected GP, we can estimate longitudinal trends in COVID-19 associated deaths across the US by 1-year age bands. These estimates are instrumental to calculate age-specific mortality rates, describe variation in age-specific deaths across the US, and for fitting epidemic models. Here, we couple the model with age-specific vaccination rates to show that lower vaccination rates in younger adults aged 18-64 are associated with significantly stronger resurgences in COVID-19 deaths, especially in Florida and Texas. These results underscore the critical importance of medically able individuals of all ages to be vaccinated against COVID-19 in order to limit fatal outcomes.

</details>

<details>

<summary>2021-12-06 19:49:58 - Bayesian Structural Equation Modeling in Multiple Omics Data Integration with Application to Circadian Genes</summary>

- *Arnab Kumar Maity, Sang Chan Lee, Bani K. Mallick, Tapasree Roy Sarkar*

- `2112.03330v1` - [abs](http://arxiv.org/abs/2112.03330v1) - [pdf](http://arxiv.org/pdf/2112.03330v1)

> It is well known that the integration among different data-sources is reliable because of its potential of unveiling new functionalities of the genomic expressions which might be dormant in a single source analysis. Moreover, different studies have justified the more powerful analyses of multi-platform data. Toward this, in this study, we consider the circadian genes' omics profile such as copy number changes and RNA sequence data along with their survival response. We develop a Bayesian structural equation modeling coupled with linear regressions and log normal accelerated failure time regression to integrate the information between these two platforms to predict the survival of the subjects. We place conjugate priors on the regression parameters and derive the Gibbs sampler using the conditional distributions of them. Our extensive simulation study shows that the integrative model provides a better fit to the data than its closest competitor. The analyses of glioblastoma cancer data and the breast cancer data from TCGA, the largest genomics and transcriptomics database, support our findings. The developed method is wrapped in R package semmcmc available at R CRAN.

</details>

<details>

<summary>2021-12-06 20:41:47 - Safe Testing</summary>

- *Peter Grünwald, Rianne de Heide, Wouter Koolen*

- `1906.07801v3` - [abs](http://arxiv.org/abs/1906.07801v3) - [pdf](http://arxiv.org/pdf/1906.07801v3)

> We develop the theory of hypothesis testing based on the E-value, a notion of evidence that, unlike the p-value, allows for effortlessly combining results from several studies in the common scenario where the decision to perform a new study may depend on previous outcomes. Tests based on E-values are safe, i.e. they preserve Type-I error guarantees, under such optional continuation. We define growth-rate optimality (GRO) as an analogue of power in an optional continuation context, and we show how to construct GRO E-variables for general testing problems with composite null and alternative, emphasizing models with nuisance parameters. GRO E-values take the form of Bayes factors with special priors. We illustrate the theory using several classic examples including a one-sample safe t-test (in which the right Haar prior turns out to be GRO) and the 2x2 contingency table (in which the GRO prior is different from standard priors). Sharing Fisherian, Neymanian and Jeffreys-Bayesian interpretations, E-values and the corresponding tests may provide a methodology acceptable to adherents of all three schools.

</details>

<details>

<summary>2021-12-06 23:28:38 - Using Image Transformations to Learn Network Structure</summary>

- *Brayan Ortiz, Amitabh Sinha*

- `2112.03419v1` - [abs](http://arxiv.org/abs/2112.03419v1) - [pdf](http://arxiv.org/pdf/2112.03419v1)

> Many learning tasks require observing a sequence of images and making a decision. In a transportation problem of designing and planning for shipping boxes between nodes, we show how to treat the network of nodes and the flows between them as images. These images have useful structural information that can be statistically summarized. Using image compression techniques, we reduce an image down to a set of numbers that contain interpretable geographic information that we call geographic signatures. Using geographic signatures, we learn network structure that can be utilized to recommend future network connectivity. We develop a Bayesian reinforcement algorithm that takes advantage of statistically summarized network information as priors and user-decisions to reinforce an agent's probabilistic decision.

</details>

<details>

<summary>2021-12-07 03:05:28 - Optimal Echo Chambers</summary>

- *Gabriel Martinez, Nicholas H. Tenev*

- `2010.01249v7` - [abs](http://arxiv.org/abs/2010.01249v7) - [pdf](http://arxiv.org/pdf/2010.01249v7)

> We show that echo chambers are a rational response to uncertain information quality, and that broadening the range of views someone is exposed to can slow their learning by making their beliefs less responsive to the views of others. A Bayesian decision maker draws a signal about the (real-valued) state of the world from a collection of unbiased sources of heterogeneous quality. Exclusively sampling signals close to the prior expectation can improve accuracy, as they are more likely high quality. Signals close to the prior expectation can move beliefs further than more contrary views; ignoring those with opposing beliefs makes one more responsive to others' views, since it increases their perceived accuracy. The optimal echo chamber balances the credibility of views similar to one's own against the usefulness of those further away.

</details>

<details>

<summary>2021-12-07 05:40:34 - A Unifying Bayesian Approach for Sample Size Determination Using Design and Analysis Priors</summary>

- *Jane Pan, Sudipto Banerjee*

- `2112.03509v1` - [abs](http://arxiv.org/abs/2112.03509v1) - [pdf](http://arxiv.org/pdf/2112.03509v1)

> Power and sample size analysis comprises a critical component of clinical trial study design. There is an extensive collection of methods addressing this problem from diverse perspectives. The Bayesian paradigm, in particular, has attracted noticeable attention and includes different perspectives for sample size determination. Building upon a cost-effectiveness analysis undertaken by O'Hagan and Stevens (2001) with different priors in the design and analysis stage, we develop a general Bayesian framework for simulation-based sample size determination that can be easily implemented on modest computing architectures. We further qualify the need for different priors for the design and analysis stage. We work primarily in the context of conjugate Bayesian linear regression models, where we consider the situation with known and unknown variances. Throughout, we draw parallels with frequentist solutions, which arise as special cases, and alternate Bayesian approaches with an emphasis on how the numerical results from existing methods arise as special cases in our framework.

</details>

<details>

<summary>2021-12-07 09:13:10 - Multi-Task Learning on Networks</summary>

- *Andrea Ponti*

- `2112.04891v1` - [abs](http://arxiv.org/abs/2112.04891v1) - [pdf](http://arxiv.org/pdf/2112.04891v1)

> The multi-task learning (MTL) paradigm can be traced back to an early paper of Caruana (1997) in which it was argued that data from multiple tasks can be used with the aim to obtain a better performance over learning each task independently. A solution of MTL with conflicting objectives requires modelling the trade-off among them which is generally beyond what a straight linear combination can achieve. A theoretically principled and computationally effective strategy is finding solutions which are not dominated by others as it is addressed in the Pareto analysis. Multi-objective optimization problems arising in the multi-task learning context have specific features and require adhoc methods. The analysis of these features and the proposal of a new computational approach represent the focus of this work. Multi-objective evolutionary algorithms (MOEAs) can easily include the concept of dominance and therefore the Pareto analysis. The major drawback of MOEAs is a low sample efficiency with respect to function evaluations. The key reason for this drawback is that most of the evolutionary approaches do not use models for approximating the objective function. Bayesian Optimization takes a radically different approach based on a surrogate model, such as a Gaussian Process. In this thesis the solutions in the Input Space are represented as probability distributions encapsulating the knowledge contained in the function evaluations. In this space of probability distributions, endowed with the metric given by the Wasserstein distance, a new algorithm MOEA/WST can be designed in which the model is not directly on the objective function but in an intermediate Information Space where the objects from the input space are mapped into histograms. Computational results show that the sample efficiency and the quality of the Pareto set provided by MOEA/WST are significantly better than in the standard MOEA.

</details>

<details>

<summary>2021-12-07 13:37:54 - Tractable Approximate Gaussian Inference for Bayesian Neural Networks</summary>

- *James-A. Goulet, Luong Ha Nguyen, Saeid Amiri*

- `2004.09281v3` - [abs](http://arxiv.org/abs/2004.09281v3) - [pdf](http://arxiv.org/pdf/2004.09281v3)

> In this paper, we propose an analytical method for performing tractable approximate Gaussian inference (TAGI) in Bayesian neural networks. The method enables the analytical Gaussian inference of the posterior mean vector and diagonal covariance matrix for weights and biases. The method proposed has a computational complexity of $\mathcal{O}(n)$ with respect to the number of parameters $n$, and the tests performed on regression and classification benchmarks confirm that, for a same network architecture, it matches the performance of existing methods relying on gradient backpropagation.

</details>

<details>

<summary>2021-12-07 14:14:30 - A Bayesian take on option pricing with Gaussian processes</summary>

- *Martin Tegner, Stephen Roberts*

- `2112.03718v1` - [abs](http://arxiv.org/abs/2112.03718v1) - [pdf](http://arxiv.org/pdf/2112.03718v1)

> Local volatility is a versatile option pricing model due to its state dependent diffusion coefficient. Calibration is, however, non-trivial as it involves both proposing a hypothesis model of the latent function and a method for fitting it to data. In this paper we present novel Bayesian inference with Gaussian process priors. We obtain a rich representation of the local volatility function with a probabilistic notion of uncertainty attached to the calibrate. We propose an inference algorithm and apply our approach to S&P 500 market data.

</details>

<details>

<summary>2021-12-07 15:39:37 - On the Effectiveness of Mode Exploration in Bayesian Model Averaging for Neural Networks</summary>

- *John T. Holodnak, Allan B. Wollaber*

- `2112.03773v1` - [abs](http://arxiv.org/abs/2112.03773v1) - [pdf](http://arxiv.org/pdf/2112.03773v1)

> Multiple techniques for producing calibrated predictive probabilities using deep neural networks in supervised learning settings have emerged that leverage approaches to ensemble diverse solutions discovered during cyclic training or training from multiple random starting points (deep ensembles). However, only a limited amount of work has investigated the utility of exploring the local region around each diverse solution (posterior mode). Using three well-known deep architectures on the CIFAR-10 dataset, we evaluate several simple methods for exploring local regions of the weight space with respect to Brier score, accuracy, and expected calibration error. We consider both Bayesian inference techniques (variational inference and Hamiltonian Monte Carlo applied to the softmax output layer) as well as utilizing the stochastic gradient descent trajectory near optima. While adding separate modes to the ensemble uniformly improves performance, we show that the simple mode exploration methods considered here produce little to no improvement over ensembles without mode exploration.

</details>

<details>

<summary>2021-12-07 19:43:52 - Change-point Detection for Piecewise Exponential Models</summary>

- *Philip Cooney, Arthur White*

- `2112.03962v1` - [abs](http://arxiv.org/abs/2112.03962v1) - [pdf](http://arxiv.org/pdf/2112.03962v1)

> In decision modelling with time to event data, parametric models are often used to extrapolate the survivor function. One such model is the piecewise exponential model whereby the hazard function is partitioned into segments, with the hazard constant within the segment and independent between segments and the boundaries of these segments are known as change-points. We present an approach for determining the location and number of change-points in piecewise exponential models. Inference is performed in a Bayesian framework using Markov Chain Monte Carlo (MCMC) where the model parameters can be integrated out of the model and the number of change-points can be sampled as part of the MCMC scheme. We can estimate both the uncertainty in the change-point locations and hazards for a given change-point model and obtain a probabilistic interpretation for the number of change-points. We evaluate model performance to determine changepoint numbers and locations in a simulation study and show the utility of the method using two data sets for time to event data. In a dataset of Glioblastoma patients we use the piecewise exponential model to describe the general trends in the hazard function. In a data set of heart transplant patients, we show the piecewise exponential model produces the best statistical fit and extrapolation amongst other standard parametric models. Piecewise exponential models may be useful for survival extrapolation if a long-term constant hazard trend is clinically plausible. A key advantage of this method is that the number and change-point locations are automatically estimated rather than specified by the analyst.

</details>

<details>

<summary>2021-12-07 20:08:14 - Posterior linearisation smoothing with robust iterations</summary>

- *Jakob Lindqvist, Simo Särkkä, Ángel F. García-Fernández, Matti Raitoharju, Lennart Svensson*

- `2112.03969v1` - [abs](http://arxiv.org/abs/2112.03969v1) - [pdf](http://arxiv.org/pdf/2112.03969v1)

> This paper considers the problem of robust iterative Bayesian smoothing in nonlinear state-space models with additive noise using Gaussian approximations. Iterative methods are known to improve smoothed estimates but are not guaranteed to converge, motivating the development of more robust versions of the algorithms. The aim of this article is to present Levenberg-Marquardt (LM) and line-search extensions of the classical iterated extended Kalman smoother (IEKS) as well as the iterated posterior linearisation smoother (IPLS). The IEKS has previously been shown to be equivalent to the Gauss-Newton (GN) method. We derive a similar GN interpretation for the IPLS. Furthermore, we show that an LM extension for both iterative methods can be achieved with a simple modification of the smoothing iterations, enabling algorithms with efficient implementations. Our numerical experiments show the importance of robust methods, in particular for the IEKS-based smoothers. The computationally expensive IPLS-based smoothers are naturally robust but can still benefit from further regularisation.

</details>

<details>

<summary>2021-12-07 20:53:14 - Quantile-based hydrological modelling</summary>

- *Hristos Tyralis, Georgia Papacharalampous*

- `2110.05586v2` - [abs](http://arxiv.org/abs/2110.05586v2) - [pdf](http://arxiv.org/pdf/2110.05586v2)

> Predictive uncertainty in hydrological modelling is quantified by using post-processing or Bayesian-based methods. The former methods are not straightforward and the latter ones are not distribution-free (i.e. assumptions on the probability distribution of the hydrological model's output are necessary). To alleviate possible limitations related to these specific attributes, in this work we propose the calibration of the hydrological model by using the quantile loss function. By following this methodological approach, one can directly simulate pre-specified quantiles of the predictive distribution of streamflow. As a proof of concept, we apply our method in the frameworks of three hydrological models to 511 river basins in contiguous US. We illustrate the predictive quantiles and show how an honest assessment of the predictive performance of the hydrological models can be made by using proper scoring rules. We believe that our method can help towards advancing the field of hydrological uncertainty.

</details>

<details>

<summary>2021-12-07 22:10:17 - Hidden Markov Pólya trees for high-dimensional distributions</summary>

- *Naoki Awaya, Li Ma*

- `2011.03121v3` - [abs](http://arxiv.org/abs/2011.03121v3) - [pdf](http://arxiv.org/pdf/2011.03121v3)

> The P\'olya tree (PT) process is a general-purpose Bayesian nonparametric model that has found wide application in a range of inference problems. It has a simple analytic form and the posterior computation boils down to beta-binomial conjugate updates along a partition tree over the sample space. Recent development in PT models shows that performance of these models can be substantially improved by (i) allowing the partition tree to adapt to the structure of the underlying distributions and (ii) incorporating latent state variables that characterize local features of the underlying distributions. However, important limitations of the PT remain, including (i) the sensitivity in the posterior inference with respect to the choice of the partition tree, and (ii) the lack of scalability with respect to dimensionality of the sample space. We consider a modeling strategy for PT models that incorporates a flexible prior on the partition tree along with latent states with Markov dependency. We introduce a hybrid algorithm combining sequential Monte Carlo (SMC) and recursive message passing for posterior sampling that can scale up to 100 dimensions. While our description of the algorithm assumes a single computer environment, it has the potential to be implemented on distributed systems to further enhance the scalability. Moreover, we investigate the large sample properties of the tree structures and latent states under the posterior model. We carry out extensive numerical experiments in density estimation and two-group comparison, which show that flexible partitioning can substantially improve the performance of PT models in both inference tasks. We demonstrate an application to a mass cytometry data set with 19 dimensions and over 200,000 observations.

</details>

<details>

<summary>2021-12-07 22:14:12 - Estimating causes of maternal death in data-sparse contexts</summary>

- *Monica Alexander, Michael Y. C. Chong, Marija Pejcinovska*

- `2101.05240v4` - [abs](http://arxiv.org/abs/2101.05240v4) - [pdf](http://arxiv.org/pdf/2101.05240v4)

> Understanding the underlying causes of maternal death across all regions of the world is essential to inform policies and resource allocation to reduce the mortality burden. However, in many countries there exists very little data on the causes of maternal death, and data that do exist do not capture the entire population at risk. In this paper, we present a Bayesian hierarchical multinomial model to estimate maternal cause of death distributions globally, regionally, and for all countries worldwide. The framework combines data from various sources to inform estimates, including data from civil registration and vital systems, smaller-scale surveys and studies, and high-quality data from confidential enquiries and surveillance systems. The framework accounts for varying data quality and coverage, and allows for situations where one or more causes of death are missing. We illustrate the results of the model on three case-study countries that have different data availability situations.

</details>

<details>

<summary>2021-12-08 08:21:15 - Aggregation of Pareto optimal models</summary>

- *Hamed Hamze Bajgiran, Houman Owhadi*

- `2112.04161v1` - [abs](http://arxiv.org/abs/2112.04161v1) - [pdf](http://arxiv.org/pdf/2112.04161v1)

> In statistical decision theory, a model is said to be Pareto optimal (or admissible) if no other model carries less risk for at least one state of nature while presenting no more risk for others. How can you rationally aggregate/combine a finite set of Pareto optimal models while preserving Pareto efficiency? This question is nontrivial because weighted model averaging does not, in general, preserve Pareto efficiency. This paper presents an answer in four logical steps: (1) A rational aggregation rule should preserve Pareto efficiency (2) Due to the complete class theorem, Pareto optimal models must be Bayesian, i.e., they minimize a risk where the true state of nature is averaged with respect to some prior. Therefore each Pareto optimal model can be associated with a prior, and Pareto efficiency can be maintained by aggregating Pareto optimal models through their priors. (3) A prior can be interpreted as a preference ranking over models: prior $\pi$ prefers model A over model B if the average risk of A is lower than the average risk of B. (4) A rational/consistent aggregation rule should preserve this preference ranking: If both priors $\pi$ and $\pi'$ prefer model A over model B, then the prior obtained by aggregating $\pi$ and $\pi'$ must also prefer A over B. Under these four steps, we show that all rational/consistent aggregation rules are as follows: Give each individual Pareto optimal model a weight, introduce a weak order/ranking over the set of Pareto optimal models, aggregate a finite set of models S as the model associated with the prior obtained as the weighted average of the priors of the highest-ranked models in S. This result shows that all rational/consistent aggregation rules must follow a generalization of hierarchical Bayesian modeling. Following our main result, we present applications to Kernel smoothing, time-depreciating models, and voting mechanisms.

</details>

<details>

<summary>2021-12-08 09:30:00 - Efficient MCMC Sampling with Dimension-Free Convergence Rate using ADMM-type Splitting</summary>

- *Maxime Vono, Daniel Paulin, Arnaud Doucet*

- `1905.11937v6` - [abs](http://arxiv.org/abs/1905.11937v6) - [pdf](http://arxiv.org/pdf/1905.11937v6)

> Performing exact Bayesian inference for complex models is computationally intractable. Markov chain Monte Carlo (MCMC) algorithms can provide reliable approximations of the posterior distribution but are expensive for large datasets and high-dimensional models. A standard approach to mitigate this complexity consists in using subsampling techniques or distributing the data across a cluster. However, these approaches are typically unreliable in high-dimensional scenarios. We focus here on a recent alternative class of MCMC schemes exploiting a splitting strategy akin to the one used by the celebrated alternating direction of multipliers (ADMM) optimization algorithm. These methods appear to provide empirically state-of-the-art performance but their theoretical behavior in high dimension is currently unknown. In this paper, we propose a detailed theoretical study of one of these algorithms known as the split Gibbs sampler. Under regularity conditions, we establish explicit convergence rates for this scheme using Ricci curvature and coupling ideas. We support our theory with numerical illustrations.

</details>

<details>

<summary>2021-12-08 11:51:52 - Bayesian Modeling of Effective and Functional Brain Connectivity using Hierarchical Vector Autoregressions</summary>

- *Bertil Wegmann, Anders Lundquist, Anders Eklund, Mattias Villani*

- `2112.04249v1` - [abs](http://arxiv.org/abs/2112.04249v1) - [pdf](http://arxiv.org/pdf/2112.04249v1)

> Analysis of brain connectivity is important for understanding how information is processed by the brain. We propose a novel Bayesian vector autoregression (VAR) hierarchical model for analyzing brain connectivity in a resting-state fMRI data set with autism spectrum disorder (ASD) patients and healthy controls. Our approach models functional and effective connectivity simultaneously, which is new in the VAR literature for brain connectivity, and allows for both group- and single-subject inference as well as group comparisons. We combine analytical marginalization with Hamiltonian Monte Carlo (HMC) to obtain highly efficient posterior sampling. The results from more simplified covariance settings are, in general, overly optimistic about functional connectivity between regions compared to our results. In addition, our modeling of heterogeneous subject-specific covariance matrices is shown to give smaller differences in effective connectivity compared to models with a common covariance matrix to all subjects.

</details>

<details>

<summary>2021-12-08 16:23:27 - Semantic TrueLearn: Using Semantic Knowledge Graphs in Recommendation Systems</summary>

- *Sahan Bulathwela, María Pérez-Ortiz, Emine Yilmaz, John Shawe-Taylor*

- `2112.04368v1` - [abs](http://arxiv.org/abs/2112.04368v1) - [pdf](http://arxiv.org/pdf/2112.04368v1)

> In informational recommenders, many challenges arise from the need to handle the semantic and hierarchical structure between knowledge areas. This work aims to advance towards building a state-aware educational recommendation system that incorporates semantic relatedness between knowledge topics, propagating latent information across semantically related topics. We introduce a novel learner model that exploits this semantic relatedness between knowledge components in learning resources using the Wikipedia link graph, with the aim to better predict learner engagement and latent knowledge in a lifelong learning scenario. In this sense, Semantic TrueLearn builds a humanly intuitive knowledge representation while leveraging Bayesian machine learning to improve the predictive performance of the educational engagement. Our experiments with a large dataset demonstrate that this new semantic version of TrueLearn algorithm achieves statistically significant improvements in terms of predictive performance with a simple extension that adds semantic awareness to the model.

</details>

<details>

<summary>2021-12-08 23:26:23 - Calibration Improves Bayesian Optimization</summary>

- *Shachi Deshpande, Volodymyr Kuleshov*

- `2112.04620v1` - [abs](http://arxiv.org/abs/2112.04620v1) - [pdf](http://arxiv.org/pdf/2112.04620v1)

> Bayesian optimization is a procedure that allows obtaining the global optimum of black-box functions and that is useful in applications such as hyper-parameter optimization. Uncertainty estimates over the shape of the objective function are instrumental in guiding the optimization process. However, these estimates can be inaccurate if the objective function violates assumptions made within the underlying model (e.g., Gaussianity). We propose a simple algorithm to calibrate the uncertainty of posterior distributions over the objective function as part of the Bayesian optimization process. We show that by improving the uncertainty estimates of the posterior distribution with calibration, Bayesian optimization makes better decisions and arrives at the global optimum in fewer steps. We show that this technique improves the performance of Bayesian optimization on standard benchmark functions and hyperparameter optimization tasks.

</details>

<details>

<summary>2021-12-08 23:40:14 - An improper estimator with optimal excess risk in misspecified density estimation and logistic regression</summary>

- *Jaouad Mourtada, Stéphane Gaïffas*

- `1912.10784v3` - [abs](http://arxiv.org/abs/1912.10784v3) - [pdf](http://arxiv.org/pdf/1912.10784v3)

> We introduce a procedure for conditional density estimation under logarithmic loss, which we call SMP (Sample Minmax Predictor). This estimator minimizes a new general excess risk bound for statistical learning. On standard examples, this bound scales as $d/n$ with $d$ the model dimension and $n$ the sample size, and critically remains valid under model misspecification. Being an improper (out-of-model) procedure, SMP improves over within-model estimators such as the maximum likelihood estimator, whose excess risk degrades under misspecification. Compared to approaches reducing to the sequential problem, our bounds remove suboptimal $\log n$ factors and can handle unbounded classes. For the Gaussian linear model, the predictions and risk bound of SMP are governed by leverage scores of covariates, nearly matching the optimal risk in the well-specified case without conditions on the noise variance or approximation error of the linear model. For logistic regression, SMP provides a non-Bayesian approach to calibration of probabilistic predictions relying on virtual samples, and can be computed by solving two logistic regressions. It achieves a non-asymptotic excess risk of $O((d + B^2R^2)/n)$, where $R$ bounds the norm of features and $B$ that of the comparison parameter; by contrast, no within-model estimator can achieve better rate than $\min({B R}/{\sqrt{n}}, {d e^{BR}}/{n} )$ in general. This provides a more practical alternative to Bayesian approaches, which require approximate posterior sampling, thereby partly addressing a question raised by Foster et al. (2018).

</details>

<details>

<summary>2021-12-09 15:47:21 - The Peril of Popular Deep Learning Uncertainty Estimation Methods</summary>

- *Yehao Liu, Matteo Pagliardini, Tatjana Chavdarova, Sebastian U. Stich*

- `2112.05000v1` - [abs](http://arxiv.org/abs/2112.05000v1) - [pdf](http://arxiv.org/pdf/2112.05000v1)

> Uncertainty estimation (UE) techniques -- such as the Gaussian process (GP), Bayesian neural networks (BNN), Monte Carlo dropout (MCDropout) -- aim to improve the interpretability of machine learning models by assigning an estimated uncertainty value to each of their prediction outputs. However, since too high uncertainty estimates can have fatal consequences in practice, this paper analyzes the above techniques.   Firstly, we show that GP methods always yield high uncertainty estimates on out of distribution (OOD) data. Secondly, we show on a 2D toy example that both BNNs and MCDropout do not give high uncertainty estimates on OOD samples. Finally, we show empirically that this pitfall of BNNs and MCDropout holds on real world datasets as well. Our insights (i) raise awareness for the more cautious use of currently popular UE methods in Deep Learning, (ii) encourage the development of UE methods that approximate GP-based methods -- instead of BNNs and MCDropout, and (iii) our empirical setups can be used for verifying the OOD performances of any other UE method. The source code is available at https://github.com/epfml/uncertainity-estimation.

</details>

<details>

<summary>2021-12-09 17:07:22 - Reproducible Model Selection Using Bagged Posteriors</summary>

- *Jonathan H. Huggins, Jeffrey W. Miller*

- `2007.14845v4` - [abs](http://arxiv.org/abs/2007.14845v4) - [pdf](http://arxiv.org/pdf/2007.14845v4)

> Bayesian model selection is premised on the assumption that the data are generated from one of the postulated models. However, in many applications, all of these models are incorrect (that is, there is misspecification). When the models are misspecified, two or more models can provide a nearly equally good fit to the data, in which case Bayesian model selection can be highly unstable, potentially leading to self-contradictory findings. To remedy this instability, we propose to use bagging on the posterior distribution ("BayesBag") -- that is, to average the posterior model probabilities over many bootstrapped datasets. We provide theoretical results characterizing the asymptotic behavior of the posterior and the bagged posterior in the (misspecified) model selection setting. We empirically assess the BayesBag approach on synthetic and real-world data in (i) feature selection for linear regression and (ii) phylogenetic tree reconstruction. Our theory and experiments show that, when all models are misspecified, BayesBag (a) provides greater reproducibility and (b) places posterior mass on optimal models more reliably, compared to the usual Bayesian posterior; on the other hand, under correct specification, BayesBag is slightly more conservative than the usual posterior, in the sense that BayesBag posterior probabilities tend to be slightly farther from the extremes of zero and one. Overall, our results demonstrate that BayesBag provides an easy-to-use and widely applicable approach that improves upon Bayesian model selection by making it more stable and reproducible.

</details>

<details>

<summary>2021-12-09 17:08:40 - Bayesian Functional Data Analysis over Dependent Regions and Its Application for Identification of Differentially Methylated Regions</summary>

- *Suvo Chatterjee, Shrabanti Chowdhury, Duchwan Ryu, Sanjib Basu*

- `2112.05041v1` - [abs](http://arxiv.org/abs/2112.05041v1) - [pdf](http://arxiv.org/pdf/2112.05041v1)

> We consider a Bayesian functional data analysis for observations measured as extremely long sequences. Splitting the sequence into a number of small windows with manageable length, the windows may not be independent especially when they are neighboring to each other. We propose to utilize Bayesian smoothing splines to estimate individual functional patterns within each window and to establish transition models for parameters involved in each window to address the dependent structure between windows. The functional difference of groups of individuals at each window can be evaluated by Bayes Factor based on Markov Chain Monte Carlo samples in the analysis. In this paper, we examine the proposed method through simulation studies and apply it to identify differentially methylated genetic regions in TCGA lung adenocarcinoma data.

</details>

<details>

<summary>2021-12-09 17:47:28 - Bayesian Image Reconstruction using Deep Generative Models</summary>

- *Razvan V Marinescu, Daniel Moyer, Polina Golland*

- `2012.04567v5` - [abs](http://arxiv.org/abs/2012.04567v5) - [pdf](http://arxiv.org/pdf/2012.04567v5)

> Machine learning models are commonly trained end-to-end and in a supervised setting, using paired (input, output) data. Examples include recent super-resolution methods that train on pairs of (low-resolution, high-resolution) images. However, these end-to-end approaches require re-training every time there is a distribution shift in the inputs (e.g., night images vs daylight) or relevant latent variables (e.g., camera blur or hand motion). In this work, we leverage state-of-the-art (SOTA) generative models (here StyleGAN2) for building powerful image priors, which enable application of Bayes' theorem for many downstream reconstruction tasks. Our method, Bayesian Reconstruction through Generative Models (BRGM), uses a single pre-trained generator model to solve different image restoration tasks, i.e., super-resolution and in-painting, by combining it with different forward corruption models. We keep the weights of the generator model fixed, and reconstruct the image by estimating the Bayesian maximum a-posteriori (MAP) estimate over the input latent vector that generated the reconstructed image. We further use variational inference to approximate the posterior distribution over the latent vectors, from which we sample multiple solutions. We demonstrate BRGM on three large and diverse datasets: (i) 60,000 images from the Flick Faces High Quality dataset (ii) 240,000 chest X-rays from MIMIC III and (iii) a combined collection of 5 brain MRI datasets with 7,329 scans. Across all three datasets and without any dataset-specific hyperparameter tuning, our simple approach yields performance competitive with current task-specific state-of-the-art methods on super-resolution and in-painting, while being more generalisable and without requiring any training. Our source code and pre-trained models are available online: https://razvanmarinescu.github.io/brgm/.

</details>

<details>

<summary>2021-12-09 18:09:30 - The Bernstein-von Mises theorem for the Pitman-Yor process of nonnegative type</summary>

- *S. E. M. P. Franssen, A. W. van der Vaart*

- `2102.06059v2` - [abs](http://arxiv.org/abs/2102.06059v2) - [pdf](http://arxiv.org/pdf/2102.06059v2)

> The Pitman-Yor process is a random probability distribution, that can be used as a prior distribution in a nonparametric Bayesian analysis. The process is of species sampling type and generates discrete distributions, which yield of the order $n^\sigma$ different values ("species") in a random sample of size $n$, if the type $\sigma$ is positive. Thus this type parameter can be set to target true distributions of various levels of discreteness, making the Pitman-Yor process an interesting prior in this case. It was previously shown that the resulting posterior distribution is consistent if and only if the true distribution of the data is discrete. In this paper we derive the distributional limit of the posterior distribution, in the form of a (corrected) Bernstein-von Mises theorem, which previously was known only in the continuous, inconsistent case. It turns out that the Pitman-Yor posterior distribution has good behaviour if the true distribution of the data is discrete with atoms that decrease not too slowly. Credible sets derived from the posterior distribution provide valid frequentist confidence sets in this case. For a general discrete distribution, the posterior distribution, although consistent, may contain a bias which does not converge to zero at the $\sqrt{n}$ rate and invalidates posterior inference. We propose a bias correction that solves this problem. We also consider the effect of estimating the type parameter from the data, both by empirical Bayes and full Bayes methods. In a small simulation study we illustrate that without bias correction the coverage of credible sets can be arbitrarily low, also for some discrete distributions.

</details>

<details>

<summary>2021-12-09 19:33:34 - Data augmentation in Bayesian neural networks and the cold posterior effect</summary>

- *Seth Nabarro, Stoil Ganev, Adrià Garriga-Alonso, Vincent Fortuin, Mark van der Wilk, Laurence Aitchison*

- `2106.05586v2` - [abs](http://arxiv.org/abs/2106.05586v2) - [pdf](http://arxiv.org/pdf/2106.05586v2)

> Bayesian neural networks that incorporate data augmentation implicitly use a ``randomly perturbed log-likelihood [which] does not have a clean interpretation as a valid likelihood function'' (Izmailov et al. 2021). Here, we provide several approaches to developing principled Bayesian neural networks incorporating data augmentation. We introduce a ``finite orbit'' setting which allows likelihoods to be computed exactly, and give tight multi-sample bounds in the more usual ``full orbit'' setting. These models cast light on the origin of the cold posterior effect. In particular, we find that the cold posterior effect persists even in these principled models incorporating data augmentation. This suggests that the cold posterior effect cannot be dismissed as an artifact of data augmentation using incorrect likelihoods.

</details>

<details>

<summary>2021-12-09 20:17:34 - A Bayesian Surveillance Model to Track Variable Rainfall-Runoff Responses for Small Watersheds</summary>

- *Xiao Peng, John D. Albertson*

- `2108.11470v2` - [abs](http://arxiv.org/abs/2108.11470v2) - [pdf](http://arxiv.org/pdf/2108.11470v2)

> Understanding dynamics of hydrological responses is essential in producing skillful runoff forecast. This can be quantitatively done by tracking changes in hydrology model parameters that represent physical characteristics. In this study, we implement a Bayesian estimation method in continuously estimating hydrology model parameters given observations of rainfall and runoff for small watersheds. The method is coupled with a conceptual hydrology model using a Gamma distribution-based Instantaneous Unit Hydrograph. The whole analytical framework is tested using synthetic data as well as observational data from the Fall Creek watershed. The results show that the Bayesian method can well track the hidden parameters that change inter-annually. Then the model is applied to examine temporal and spatial variability of the rainfall-runoff responses and we find 1) a systematic shift in the rainfall-runoff response for the Fall Creek watershed around 1943 and 2) a statistically significant relationship between rainfall-runoff responses and watershed sizes for selected NY watersheds. Our results demonstrate potential of the Bayesian estimation method as a rapid surveillance tool in monitoring and tracking changes of hydrological responses for small watersheds.

</details>

<details>

<summary>2021-12-10 01:43:59 - Solving linear Bayesian inverse problems using a fractional total variation-Gaussian (FTG) prior and transport map</summary>

- *Zejun Sun, Guang-Hui Zheng*

- `2112.05288v1` - [abs](http://arxiv.org/abs/2112.05288v1) - [pdf](http://arxiv.org/pdf/2112.05288v1)

> The Bayesian inference is widely used in many scientific and engineering problems, especially in the linear inverse problems in infinite-dimensional setting where the unknowns are functions. In such problems, choosing an appropriate prior distribution is an important task. In particular, when the function to infer has much detail information, such as many sharp jumps, corners, and the discontinuous and nonsmooth oscillation, the so-called total variation-Gaussian (TG) prior is proposed in function space to address it. However, the TG prior is easy to lead the blocky (staircase) effect in numerical results. In this work, we present a fractional order-TG (FTG) hybrid prior to deal with such problems, where the fractional order total variation (FTV) term is used to capture the detail information of the unknowns and simultaneously uses the Gaussian measure to ensure that it results in a well-defined posterior measure. For the numerical implementations of linear inverse problems in function spaces, we also propose an efficient independence sampler based on a transport map, which uses a proposal distribution derived from a diagonal map, and the acceptance probability associated to the proposal is independent of discretization dimensionality. And in order to take full advantage of the transport map, the hierarchical Bayesian framework is applied to flexibly determine the regularization parameter. Finally we provide some numerical examples to demonstrate the performance of the FTG prior and the efficiency and robustness of the proposed independence sampler method.

</details>

<details>

<summary>2021-12-10 04:41:41 - Posterior Regularization on Bayesian Hierarchical Mixture Clustering</summary>

- *Weipeng Huang, Tin Lok James Ng, Nishma Laitonjam, Neil J. Hurley*

- `2105.06903v4` - [abs](http://arxiv.org/abs/2105.06903v4) - [pdf](http://arxiv.org/pdf/2105.06903v4)

> Bayesian hierarchical mixture clustering (BHMC) is an interesting model that improves on the traditional Bayesian hierarchical clustering approaches. Regarding the parent-to-node diffusion in the generative process, BHMC replaces the conventional Gaussian-to-Gaussian (G2G) kernels with a Hierarchical Dirichlet Process Mixture Model (HDPMM). However, the drawback of the BHMC lies in that it might obtain comparatively high nodal variance in the higher levels (i.e., those closer to the root node). This can be interpreted as that the separation between the nodes, in particular those in the higher levels, might be weak. Attempting to overcome this drawback, we consider a recent inferential framework named posterior regularization, which facilitates a simple manner to impose extra constraints on a Bayesian model to address some weakness of the original model. Hence, to enhance the separation of clusters, we apply posterior regularization to impose max-margin constraints on the nodes at every level of the hierarchy. In this paper, we illustrate how the framework integrates with the BHMC and achieves the desired improvements over the original model.

</details>

<details>

<summary>2021-12-10 11:52:06 - Taking advantage of sampling deisgns in Bayesian spatial small ares survey studies</summary>

- *Carlos Vergara-Hernández, Marc Marí-DellOlmo, Laura Oliveras, Miguel A. Martinez-Beneito*

- `2112.05468v1` - [abs](http://arxiv.org/abs/2112.05468v1) - [pdf](http://arxiv.org/pdf/2112.05468v1)

> Spatial small area estimation models have become very popular in some contexts, such as disease mapping. Data in disease mapping studies are exhaustive, that is, the available data are supposed to be a complete register of all the observable events. In contrast, some other small area studies do not use exhaustive data, such as survey based studies, where a particular sampling design is typically followed and inferences are later extrapolated to the entire population. In this paper we propose a spatial model for small area survey studies, taking advantage of spatial dependence between units, which is the key assumption used for yielding reliable estimates in exhaustive data based studies. In addition, and in contrast to most spatial survey studies, we take the approach of also considering information on the sampling design and additional supplementary variables in order to yield small area estimates. This makes it possible to merge spatial and sampling based approaches into a common proposal

</details>

<details>

<summary>2021-12-10 15:36:30 - Comparison of Markov chains via weak Poincaré inequalities with application to pseudo-marginal MCMC</summary>

- *Christophe Andrieu, Anthony Lee, Sam Power, Andi Q. Wang*

- `2112.05605v1` - [abs](http://arxiv.org/abs/2112.05605v1) - [pdf](http://arxiv.org/pdf/2112.05605v1)

> We investigate the use of a certain class of functional inequalities known as weak Poincar\'e inequalities to bound convergence of Markov chains to equilibrium. We show that this enables the straightforward and transparent derivation of subgeometric convergence bounds for methods such as the Independent Metropolis--Hastings sampler and pseudo-marginal methods for intractable likelihoods, the latter being subgeometric in many practical settings. These results rely on novel quantitative comparison theorems between Markov chains. Associated proofs are simpler than those relying on drift/minorization conditions and the tools developed allow us to recover and further extend known results as particular cases. We are then able to provide new insights into the practical use of pseudo-marginal algorithms, analyse the effect of averaging in Approximate Bayesian Computation (ABC) and the use of products of independent averages, and also to study the case of lognormal weights relevant to particle marginal Metropolis--Hastings (PMMH).

</details>

<details>

<summary>2021-12-10 17:18:48 - Multivariate Dynamic Modeling for Bayesian Forecasting of Business Revenue</summary>

- *Anna K. Yanchenko, Graham Tierney, Joseph Lawson, Christoph Hellmayr, Andrew Cron, Mike West*

- `2112.05678v1` - [abs](http://arxiv.org/abs/2112.05678v1) - [pdf](http://arxiv.org/pdf/2112.05678v1)

> Forecasting enterprise-wide revenue is critical to many companies and presents several challenges and opportunities for significant business impact. This case study is based on model developments to address these challenges for forecasting in a large-scale retail company. Focused on multivariate revenue forecasting across collections of supermarkets and product Categories, hierarchical dynamic models are natural: these are able to couple revenue streams in an integrated forecasting model, while allowing conditional decoupling to enable relevant and sensitive analysis together with scalable computation. Structured models exploit multi-scale modeling to cascade information on price and promotion activities as predictors relevant across Categories and groups of stores. With a context-relevant focus on forecasting revenue 12 weeks ahead, the study highlights product Categories that benefit from multi-scale information, defines insights into when, how and why multivariate models improve forecast accuracy, and shows how cross-Category dependencies can relate to promotion decisions in one Category impacting others. Bayesian modeling developments underlying the case study are accessible in custom code for interested readers.

</details>

<details>

<summary>2021-12-10 17:20:05 - Laplace priors and spatial inhomogeneity in Bayesian inverse problems</summary>

- *Sergios Agapiou, Sven Wang*

- `2112.05679v1` - [abs](http://arxiv.org/abs/2112.05679v1) - [pdf](http://arxiv.org/pdf/2112.05679v1)

> Spatially inhomogeneous functions, which may be smooth in some regions and rough in other regions, are modelled naturally in a Bayesian manner using so-called Besov priors which are given by random wavelet expansions with Laplace-distributed coefficients. This paper studies theoretical guarantees for such prior measures - specifically, we examine their frequentist posterior contraction rates in the setting of non-linear inverse problems with Gaussian white noise. Our results are first derived under a general local Lipschitz assumption on the forward map. We then verify the assumption for two non-linear inverse problems arising from elliptic partial differential equations, the Darcy flow model from geophysics as well as a model for the Schr\"odinger equation appearing in tomography. In the course of the proofs, we also obtain novel concentration inequalities for penalized least squares estimators with $\ell^1$ wavelet penalty, which have a natural interpretation as maximum a posteriori (MAP) estimators. The true parameter is assumed to belong to some spatially inhomogeneous Besov class $B^{\alpha}_{11}$, $\alpha>0$.   In a setting with direct observations, we complement these upper bounds with a lower bound on the rate of contraction for arbitrary Gaussian priors. An immediate consequence of our results is that while Laplace priors can achieve minimax-optimal rates over $B^{\alpha}_{11}$-classes, Gaussian priors are limited to a (by a polynomial factor) slower contraction rate. This gives information-theoretical justification for the intuition that Laplace priors are more compatible with $\ell^1$ regularity structure in the underlying parameter.

</details>

<details>

<summary>2021-12-10 18:44:19 - Replacing the do-calculus with Bayes rule</summary>

- *Finnian Lattimore, David Rohde*

- `1906.07125v3` - [abs](http://arxiv.org/abs/1906.07125v3) - [pdf](http://arxiv.org/pdf/1906.07125v3)

> The concept of causality has a controversial history. The question of whether it is possible to represent and address causal problems with probability theory, or if fundamentally new mathematics such as the do calculus is required has been hotly debated, e.g. Pearl (2001) states "the building blocks of our scientific and everyday knowledge are elementary facts such as "mud does not cause rain" and "symptoms do not cause disease" and those facts, strangely enough, cannot be expressed in the vocabulary of probability calculus". This has lead to a dichotomy between advocates of causal graphical modeling and the do calculus, and researchers applying Bayesian methods. In this paper we demonstrate that, while it is critical to explicitly model our assumptions on the impact of intervening in a system, provided we do so, estimating causal effects can be done entirely within the standard Bayesian paradigm. The invariance assumptions underlying causal graphical models can be encoded in ordinary Probabilistic graphical models, allowing causal estimation with Bayesian statistics, equivalent to the do calculus. Elucidating the connections between these approaches is a key step toward enabling the insights provided by each to be combined to solve real problems.

</details>

<details>

<summary>2021-12-11 09:59:13 - A User-Guided Bayesian Framework for Ensemble Feature Selection in Life Science Applications (UBayFS)</summary>

- *Anna Jenul, Stefan Schrunner, Jürgen Pilz, Oliver Tomic*

- `2104.14787v3` - [abs](http://arxiv.org/abs/2104.14787v3) - [pdf](http://arxiv.org/pdf/2104.14787v3)

> Feature selection represents a measure to reduce the complexity of high-dimensional datasets and gain insights into the systematic variation in the data. This aspect is of specific importance in domains that rely on model interpretability, such as life sciences. We propose UBayFS, an ensemble feature selection technique embedded in a Bayesian statistical framework. Our approach considers two sources of information: data and domain knowledge. We build a meta-model from an ensemble of elementary feature selectors and aggregate this information in a multinomial likelihood. The user guides UBayFS by weighting features and penalizing specific feature blocks or combinations, implemented via a Dirichlet-type prior distribution and a regularization term. In a quantitative evaluation, we demonstrate that our framework (a) allows for a balanced trade-off between user knowledge and data observations, and (b) achieves competitive performance with state-of-the-art methods.

</details>

<details>

<summary>2021-12-12 17:13:14 - Spatial-Temporal-Fusion BNN: Variational Bayesian Feature Layer</summary>

- *Shiye Lei, Zhuozhuo Tu, Leszek Rutkowski, Feng Zhou, Li Shen, Fengxiang He, Dacheng Tao*

- `2112.06281v1` - [abs](http://arxiv.org/abs/2112.06281v1) - [pdf](http://arxiv.org/pdf/2112.06281v1)

> Bayesian neural networks (BNNs) have become a principal approach to alleviate overconfident predictions in deep learning, but they often suffer from scaling issues due to a large number of distribution parameters. In this paper, we discover that the first layer of a deep network possesses multiple disparate optima when solely retrained. This indicates a large posterior variance when the first layer is altered by a Bayesian layer, which motivates us to design a spatial-temporal-fusion BNN (STF-BNN) for efficiently scaling BNNs to large models: (1) first normally train a neural network from scratch to realize fast training; and (2) the first layer is converted to Bayesian and inferred by employing stochastic variational inference, while other layers are fixed. Compared to vanilla BNNs, our approach can greatly reduce the training time and the number of parameters, which contributes to scale BNNs efficiently. We further provide theoretical guarantees on the generalizability and the capability of mitigating overconfidence of STF-BNN. Comprehensive experiments demonstrate that STF-BNN (1) achieves the state-of-the-art performance on prediction and uncertainty quantification; (2) significantly improves adversarial robustness and privacy preservation; and (3) considerably reduces training time and memory costs.

</details>

<details>

<summary>2021-12-13 10:06:28 - A Complete Characterisation of ReLU-Invariant Distributions</summary>

- *Jan Macdonald, Stephan Wäldchen*

- `2112.06532v1` - [abs](http://arxiv.org/abs/2112.06532v1) - [pdf](http://arxiv.org/pdf/2112.06532v1)

> We give a complete characterisation of families of probability distributions that are invariant under the action of ReLU neural network layers. The need for such families arises during the training of Bayesian networks or the analysis of trained neural networks, e.g., in the context of uncertainty quantification (UQ) or explainable artificial intelligence (XAI). We prove that no invariant parametrised family of distributions can exist unless at least one of the following three restrictions holds: First, the network layers have a width of one, which is unreasonable for practical neural networks. Second, the probability measures in the family have finite support, which basically amounts to sampling distributions. Third, the parametrisation of the family is not locally Lipschitz continuous, which excludes all computationally feasible families. Finally, we show that these restrictions are individually necessary. For each of the three cases we can construct an invariant family exploiting exactly one of the restrictions but not the other two.

</details>

<details>

<summary>2021-12-13 12:54:36 - A Bayesian approach for partial Gaussian graphical models with sparsity</summary>

- *Eunice Okome Obiang, Pascal Jézéquel, Frédéric Proïa*

- `2105.10888v2` - [abs](http://arxiv.org/abs/2105.10888v2) - [pdf](http://arxiv.org/pdf/2105.10888v2)

> We explore various Bayesian approaches to estimate partial Gaussian graphical models. Our hierarchical structures enable to deal with single-output as well as multiple-output linear regressions, in small or high dimension, enforcing either no sparsity, sparsity, group sparsity or even sparse-group sparsity for a bi-level selection through partial correlations (direct links) between predictors and responses, thanks to spike-and-slab priors corresponding to each setting. Adaptative and global shrinkages are also incorporated in the Bayesian modeling of the direct links. An existing result for model selection consistency is reformulated to stick to our sparse and group-sparse settings, providing a theoretical guarantee under some technical assumptions. Gibbs samplers are developed and a simulation study shows the efficiency of our models which give very competitive results, especially in terms of support recovery. To conclude, a real dataset is investigated.

</details>

<details>

<summary>2021-12-13 23:09:57 - Fiducial Inference and Decision Theory</summary>

- *G. Taraldsen, B. H. Lindquist*

- `2112.07060v1` - [abs](http://arxiv.org/abs/2112.07060v1) - [pdf](http://arxiv.org/pdf/2112.07060v1)

> The majority of the statisticians concluded many decades ago that fiducial inference was nonsensical to them. Hannig et al. (2016) and others have, however, contributed to a renewed interest and focus. Fiducial inference is similar to Bayesian analysis, but without requiring a prior. The prior information is replaced by assuming a particular data generating equation. Berger (1985) explains that Bayesian analysis and statistical decision theory are in harmony. Taraldsen and Lindqvist (2013) show that fiducial theory and statistical decision theory also play well together. The purpose of this text is to explain and exemplify this together with recent mathematical results.

</details>

<details>

<summary>2021-12-14 04:57:37 - Mitigating the Effects of Non-Identifiability on Inference for Bayesian Neural Networks with Latent Variables</summary>

- *Yaniv Yacoby, Weiwei Pan, Finale Doshi-Velez*

- `1911.00569v3` - [abs](http://arxiv.org/abs/1911.00569v3) - [pdf](http://arxiv.org/pdf/1911.00569v3)

> Bayesian Neural Networks with Latent Variables (BNN+LVs) capture predictive uncertainty by explicitly modeling model uncertainty (via priors on network weights) and environmental stochasticity (via a latent input noise variable). In this work, we first show that BNN+LV suffers from a serious form of non-identifiability: explanatory power can be transferred between the model parameters and latent variables while fitting the data equally well. We demonstrate that as a result, in the limit of infinite data, the posterior mode over the network weights and latent variables is asymptotically biased away from the ground-truth. Due to this asymptotic bias, traditional inference methods may in practice yield parameters that generalize poorly and misestimate uncertainty. Next, we develop a novel inference procedure that explicitly mitigates the effects of likelihood non-identifiability during training and yields high-quality predictions as well as uncertainty estimates. We demonstrate that our inference method improves upon benchmark methods across a range of synthetic and real data-sets.

</details>

<details>

<summary>2021-12-14 13:41:07 - Bayesian nonparametric discontinuity design</summary>

- *Max Hinne, David Leeftink, Marcel A. J. van Gerven, Luca Ambrogioni*

- `1911.06722v3` - [abs](http://arxiv.org/abs/1911.06722v3) - [pdf](http://arxiv.org/pdf/1911.06722v3)

> Quasi-experimental research designs, such as regression discontinuity and interrupted time series, allow for causal inference in the absence of a randomized controlled trial, at the cost of additional assumptions. In this paper, we provide a framework for discontinuity-based designs using Bayesian model comparison and Gaussian process regression, which we refer to as 'Bayesian nonparametric discontinuity design', or BNDD for short. BNDD addresses the two major shortcomings in most implementations of such designs: overconfidence due to implicit conditioning on the alleged effect, and model misspecification due to reliance on overly simplistic regression models. With the appropriate Gaussian process covariance function, our approach can detect discontinuities of any order, and in spectral features. We demonstrate the usage of BNDD in simulations, and apply the framework to determine the effect of running for political positions on longevity, of the effect of an alleged historical phantom border in the Netherlands on Dutch voting behaviour, and of Kundalini Yoga meditation on heart rate.

</details>

<details>

<summary>2021-12-14 13:54:04 - Accounting for survey design in Bayesian disaggregation of survey-based areal estimates of proportions: an application to the American Community Survey</summary>

- *Marco H. Benedetti, Veronica J. Berrocal, Roderick J. Little*

- `2112.06802v2` - [abs](http://arxiv.org/abs/2112.06802v2) - [pdf](http://arxiv.org/pdf/2112.06802v2)

> Understanding the effects of social determinants of health on health outcomes requires data on characteristics of the neighborhoods in which subjects live. However, estimates of these characteristics are often aggregated over space and time in a fashion that diminishes their utility. Take, for example, estimates from the American Community Survey (ACS), a multi-year nationwide survey administered by the U.S. Census Bureau: estimates for small municipal areas are aggregated over 5-year periods, whereas 1-year estimates are only available for municipal areas with populations $>$65,000. Researchers may wish to use ACS estimates in studies of population health to characterize neighborhood-level exposures. However, 5-year estimates may not properly characterize temporal changes or align temporally with other data in the study, while the coarse spatial resolution of the 1-year estimates diminishes their utility in characterizing neighborhood exposure. To circumvent this issue, in this paper we propose a modeling framework to disaggregate estimates of proportions derived from sampling surveys which explicitly accounts for the survey design effect. We illustrate the utility of our model by applying it to the ACS data, generating estimates of poverty for the state of Michigan at fine spatio-temporal resolution.

</details>

<details>

<summary>2021-12-14 14:48:24 - Bayesian Learning of Play Styles in Multiplayer Video Games</summary>

- *Aline Normoyle, Shane T. Jensen*

- `2112.07437v1` - [abs](http://arxiv.org/abs/2112.07437v1) - [pdf](http://arxiv.org/pdf/2112.07437v1)

> The complexity of game play in online multiplayer games has generated strong interest in modeling the different play styles or strategies used by players for success. We develop a hierarchical Bayesian regression approach for the online multiplayer game Battlefield 3 where performance is modeled as a function of the roles, game type, and map taken on by that player in each of their matches. We use a Dirichlet process prior that enables the clustering of players that have similar player-specific coefficients in our regression model, which allows us to discover common play styles amongst our sample of Battlefield 3 players. This Bayesian semi-parametric clustering approach has several advantages: the number of common play styles do not need to be specified, players can move between multiple clusters, and the resulting groupings often have a straight-forward interpretations. We examine the most common play styles among Battlefield 3 players in detail and find groups of players that exhibit overall high performance, as well as groupings of players that perform particularly well in specific game types, maps and roles. We are also able to differentiate between players that are stable members of a particular play style from hybrid players that exhibit multiple play styles across their matches. Modeling this landscape of different play styles will aid game developers in developing specialized tutorials for new participants as well as improving the construction of complementary teams in their online matching queues.

</details>

<details>

<summary>2021-12-14 16:27:31 - An Empirical Comparison of GANs and Normalizing Flows for Density Estimation</summary>

- *Tianci Liu, Jeffrey Regier*

- `2006.10175v2` - [abs](http://arxiv.org/abs/2006.10175v2) - [pdf](http://arxiv.org/pdf/2006.10175v2)

> Generative adversarial networks (GANs) and normalizing flows are both approaches to density estimation that use deep neural networks to transform samples from an uninformative prior distribution to an approximation of the data distribution. There is great interest in both for general-purpose statistical modeling, but the two approaches have seldom been compared to each other for modeling non-image data. The difficulty of computing likelihoods with GANs, which are implicit models, makes conducting such a comparison challenging. We work around this difficulty by considering several low-dimensional synthetic datasets. An extensive grid search over GAN architectures, hyperparameters, and training procedures suggests that no GAN is capable of modeling our simple low-dimensional data well, a task we view as a prerequisite for an approach to be considered suitable for general-purpose statistical modeling. Several normalizing flows, on the other hand, excelled at these tasks, even substantially outperforming WGAN in terms of Wasserstein distance -- the metric that WGAN alone targets. Scientists and other practitioners should be wary of relying on WGAN for applications that require accurate density estimation.

</details>

<details>

<summary>2021-12-14 20:58:27 - Neighborhood Random Walk Graph Sampling for Regularized Bayesian Graph Convolutional Neural Networks</summary>

- *Aneesh Komanduri, Justin Zhan*

- `2112.07743v1` - [abs](http://arxiv.org/abs/2112.07743v1) - [pdf](http://arxiv.org/pdf/2112.07743v1)

> In the modern age of social media and networks, graph representations of real-world phenomena have become an incredibly useful source to mine insights. Often, we are interested in understanding how entities in a graph are interconnected. The Graph Neural Network (GNN) has proven to be a very useful tool in a variety of graph learning tasks including node classification, link prediction, and edge classification. However, in most of these tasks, the graph data we are working with may be noisy and may contain spurious edges. That is, there is a lot of uncertainty associated with the underlying graph structure. Recent approaches to modeling uncertainty have been to use a Bayesian framework and view the graph as a random variable with probabilities associated with model parameters. Introducing the Bayesian paradigm to graph-based models, specifically for semi-supervised node classification, has been shown to yield higher classification accuracies. However, the method of graph inference proposed in recent work does not take into account the structure of the graph. In this paper, we propose a novel algorithm called Bayesian Graph Convolutional Network using Neighborhood Random Walk Sampling (BGCN-NRWS), which uses a Markov Chain Monte Carlo (MCMC) based graph sampling algorithm utilizing graph structure, reduces overfitting by using a variational inference layer, and yields consistently competitive classification results compared to the state-of-the-art in semi-supervised node classification.

</details>

<details>

<summary>2021-12-14 21:48:14 - Separate Exchangeability as Modeling Principle in Bayesian Nonparametrics</summary>

- *Qiaohui Lin, Giovanni Rebaudo, Peter Mueller*

- `2112.07755v1` - [abs](http://arxiv.org/abs/2112.07755v1) - [pdf](http://arxiv.org/pdf/2112.07755v1)

> We argue for the use of separate exchangeability as a modeling principle in Bayesian inference, especially for nonparametric Bayesian models. While in some areas, such as random graphs, separate and (closely related) joint exchangeability are widely used, and it naturally arises for example in simple mixed models, it is curiously underused for other applications. We briefly review the definition of separate exchangeability. We then discuss two specific models that implement separate exchangeability. One example is about nested random partitions for a data matrix, defining a partition of columns and nested partitions of rows, nested within column clusters. Many recently proposed models for nested partitions implement partially exchangeable models. We argue that inference under such models in some cases ignores important features of the experimental setup. The second example is about setting up separately exchangeable priors for a nonparametric regression model when multiple sets of experimental units are involved.

</details>

<details>

<summary>2021-12-14 22:28:23 - A flexible Bayesian framework for individualized inference via adaptive borrowing</summary>

- *Ziyu Ji, Julian Wolfson*

- `2106.13431v2` - [abs](http://arxiv.org/abs/2106.13431v2) - [pdf](http://arxiv.org/pdf/2106.13431v2)

> The explosion in high-resolution data capture technologies in health has increased interest in making inferences about individual-level parameters. While technology may provide substantial data on a single individual, how best to use multisource population data to improve individualized inference remains an open research question. One possible approach, the multisource exchangeability model (MEM), is a Bayesian method for integrating data from supplementary sources into the analysis of a primary source. MEM was originally developed to improve inference for a single study by asymmetrically borrowing information from a set of similar previous studies and was further developed to apply a more computationally intensive symmetric borrowing in the context of basket trial; however, even for asymmetric borrowing, its computational burden grows exponentially with the number of supplementary sources, making it unsuitable for applications where hundreds or thousands of supplementary sources (i.e., individuals) could contribute to inference on a given individual. In this paper, we propose the data-driven MEM (dMEM), a two-stage approach that includes both source selection and clustering to enable the inclusion of an arbitrary number of sources to contribute to individualized inference in a computationally tractable and data-efficient way. We illustrate the application of dMEM to individual-level human behavior and mental well-being data collected via smartphones, where our approach increases individual-level estimation precision by 84% compared with a standard no-borrowing method and outperforms recently-proposed competing methods in 80% of individuals.

</details>

<details>

<summary>2021-12-15 07:55:39 - IID Sampling from Doubly Intractable Distributions</summary>

- *Sourabh Bhattacharya*

- `2112.07939v1` - [abs](http://arxiv.org/abs/2112.07939v1) - [pdf](http://arxiv.org/pdf/2112.07939v1)

> Intractable posterior distributions of parameters with intractable normalizing constants depending upon the parameters are known as doubly intractable posterior distributions. The terminology itself indicates that obtaining Bayesian inference from such posteriors is doubly difficult compared to traditional intractable posteriors where the normalizing constants are tractable and admit traditional Markov Chain Monte Carlo (MCMC) solutions.   As can be anticipated, a plethora of MCMC-based methods have originated in the literature to deal with doubly intractable distributions. Yet, it remains very much unclear if any of the methods can satisfactorily sample from such posteriors, particularly in high-dimensional setups.   In this article, we consider efficient Monte Carlo and importance sampling approximations of the intractable normalizing constant for a few values of the parameters, and Gaussian process interpolations for the remaining values of the parameters, using the approximations. We then incorporate this strategy within the exact iid sampling framework developed in Bhattacharya (2021a) and Bhattacharya (2021b), and illustrate the methodology with simulation experiments comprising a two-dimensional normal-gamma posterior, a two-dimensional Ising model posterior, a two-dimensional Strauss process posterior and a 100-dimensional autologistic model posterior. In each case we demonstrate great accuracy of our methodology, which is also computationally extremely efficient, often taking only a few minutes for generating 10, 000 iid realizations on 80 processors.

</details>

<details>

<summary>2021-12-15 11:52:38 - Bayesian Search for Robust Optima</summary>

- *Nicholas D. Sanders, Richard M. Everson, Jonathan E. Fieldsend, Alma A. M. Rahat*

- `1904.11416v3` - [abs](http://arxiv.org/abs/1904.11416v3) - [pdf](http://arxiv.org/pdf/1904.11416v3)

> Many expensive black-box optimisation problems are sensitive to their inputs. In these problems it makes more sense to locate a region of good designs, than a single-possibly fragile-optimal design. Expensive black-box functions can be optimised effectively with Bayesian optimisation, where a Gaussian process is a popular choice as a prior over the expensive function. We propose a method for robust optimisation using Bayesian optimisation to find a region of design space in which the expensive function's performance is relatively insensitive to the inputs whilst retaining a good quality. This is achieved by sampling realisations from a Gaussian process that is modelling the expensive function, and evaluating the improvement for each realisation. The expectation of these improvements can be optimised cheaply with an evolutionary algorithm to determine the next location at which to evaluate the expensive function. We describe an efficient process to locate the optimum expected improvement. We show empirically that evaluating the expensive function at the location in the candidate uncertainty region about which the model is most uncertain, or at random, yield the best convergence in contrast to exploitative schemes. We illustrate our method on six test functions in two, five, and ten dimensions, and demonstrate that it is able to outperform two state-of-the-art approaches from the literature. We also demonstrate our method one two real-world problems in 4 and 8 dimensions, which involve training robot arms to push objects onto targets.

</details>

<details>

<summary>2021-12-15 13:13:53 - Measuring the accuracy of likelihood-free inference</summary>

- *Aden Forrow, Ruth E. Baker*

- `2112.08096v1` - [abs](http://arxiv.org/abs/2112.08096v1) - [pdf](http://arxiv.org/pdf/2112.08096v1)

> Complex scientific models where the likelihood cannot be evaluated present a challenge for statistical inference. Over the past two decades, a wide range of algorithms have been proposed for learning parameters in computationally feasible ways, often under the heading of approximate Bayesian computation or likelihood-free inference. There is, however, no consensus on how to rigorously evaluate the performance of these algorithms. Here, we argue for scoring algorithms by the mean squared error in estimating expectations of functions with respect to the posterior. We show that score implies common alternatives, including the acceptance rate and effective sample size, as limiting special cases. We then derive asymptotically optimal distributions for choosing or sampling discrete or continuous simulation parameters, respectively. Our recommendations differ significantly from guidelines based on alternative scores outside of their region of validity. As an application, we show sequential Monte Carlo in this context can be made more accurate with no new samples by accepting particles from all rounds.

</details>

<details>

<summary>2021-12-15 14:17:25 - Bayesian Mendelian randomization with study heterogeneity and data partitioning for large studies</summary>

- *Linyi Zou, Hui Guo, Carlo Berzuini*

- `2112.08147v1` - [abs](http://arxiv.org/abs/2112.08147v1) - [pdf](http://arxiv.org/pdf/2112.08147v1)

> Background: Mendelian randomization (MR) is a useful approach to causal inference from observational studies when randomised controlled trials are not feasible. However, study heterogeneity of two association studies required in MR is often overlooked. When dealing with large studies, recently developed Bayesian MR is limited by its computational expensiveness. Methods: We addressed study heterogeneity by proposing a random effect Bayesian MR model with multiple exposures and outcomes. For large studies, we adopted a subset posterior aggregation method to tackle the problem of computation. In particular, we divided data into subsets and combine estimated subset causal effects obtained from the subsets". The performance of our method was evaluated by a number of simulations, in which part of exposure data was missing. Results: Random effect Bayesian MR outperformed conventional inverse-variance weighted estimation, whether the true causal effects are zero or non-zero. Data partitioning of large studies had little impact on variations of the estimated causal effects, whereas it notably affected unbiasedness of the estimates with weak instruments and high missing rate of data. Our simulation results indicate that data partitioning is a good way of improving computational efficiency, for little cost of decrease in unbiasedness of the estimates, as long as the sample size of subsets is reasonably large. Conclusions: We have further advanced Bayesian MR by including random effects to explicitly account for study heterogeneity. We also adopted a subset posterior aggregation method to address the issue of computational expensiveness of MCMC, which is important especially when dealing with large studies. Our proposed work is likely to pave the way for more general model settings, as Bayesian approach itself renders great flexibility in model constructions.

</details>

<details>

<summary>2021-12-15 14:41:55 - Learning from Neighbors about a Changing State</summary>

- *Krishna Dasaratha, Benjamin Golub, Nir Hak*

- `1801.02042v7` - [abs](http://arxiv.org/abs/1801.02042v7) - [pdf](http://arxiv.org/pdf/1801.02042v7)

> Agents learn about a changing state using private signals and past actions of neighbors in a network. We characterize equilibrium learning and examine when agents can aggregate information well, responding quickly to recent changes. A key sufficient condition for good aggregation is that each individual's neighbors have sufficiently different types of private information. In contrast, when signals are homogeneous, aggregation is suboptimal on any network. Behavioral variations of the model demonstrate that achieving good aggregation requires a sophisticated response to correlations in neighbors' actions. Finally, we study social influence, finding that an agent's influence grows much faster than linearly in private signal precision, in contrast to benchmark models. The model provides a Bayesian foundation for a tractable learning dynamic in networks, closely related to the DeGroot model, and offers new tools for counterfactual and welfare analyses.

</details>

<details>

<summary>2021-12-15 18:01:11 - Informed Bayesian survival analysis</summary>

- *František Bartoš, Frederik Aust, Julia M. Haaf*

- `2112.08311v1` - [abs](http://arxiv.org/abs/2112.08311v1) - [pdf](http://arxiv.org/pdf/2112.08311v1)

> We overview Bayesian estimation, hypothesis testing, and model-averaging and illustrate how they benefit parametric survival analysis. We contrast the Bayesian framework to the currently dominant frequentist approach and highlight advantages, such as seamless incorporation of historical data, continuous monitoring of evidence, and incorporating uncertainty about the true data generating process.   We illustrate the application of the Bayesian approaches on an example data set from a colon cancer trial. We compare the Bayesian parametric survival analysis and frequentist models with AIC/BIC model selection in fixed-n and sequential designs with a simulation study.   In the example data set, the Bayesian framework provided evidence for the absence of a positive treatment effect on disease-free survival in patients with resected colon cancer. Furthermore, the Bayesian sequential analysis would have terminated the trial 13.3 months earlier than the standard frequentist analysis. In a simulation study with sequential designs, the Bayesian framework on average reached a decision in almost half the time required by the frequentist counterparts, while maintaining the same power, and an appropriate false-positive rate. Under model misspecification, the Bayesian framework resulted in higher false-negative rate compared to the frequentist counterparts, which resulted in a higher proportion of undecided trials. In fixed-n designs, the Bayesian framework showed slightly higher power, slightly elevated error rates, and lower bias and RMSE when estimating treatment effects in small samples. We have made the analytic approach readily available in RoBSA R package.   The outlined Bayesian framework provides several benefits when applied to parametric survival analyses. It uses data more efficiently, is capable of greatly shortening the length of clinical trials, and provides a richer set of inferences.

</details>

<details>

<summary>2021-12-15 18:58:55 - Estimating Uncertainty For Vehicle Motion Prediction on Yandex Shifts Dataset</summary>

- *Alexey Pustynnikov, Dmitry Eremeev*

- `2112.08355v1` - [abs](http://arxiv.org/abs/2112.08355v1) - [pdf](http://arxiv.org/pdf/2112.08355v1)

> Motion prediction of surrounding agents is an important task in context of autonomous driving since it is closely related to driver's safety. Vehicle Motion Prediction (VMP) track of Shifts Challenge focuses on developing models which are robust to distributional shift and able to measure uncertainty of their predictions. In this work we present the approach that significantly improved provided benchmark and took 2nd place on the leaderboard.

</details>

<details>

<summary>2021-12-15 19:02:54 - Real-time Detection of Anomalies in Multivariate Time Series of Astronomical Data</summary>

- *Daniel Muthukrishna, Kaisey S. Mandel, Michelle Lochner, Sara Webb, Gautham Narayan*

- `2112.08415v1` - [abs](http://arxiv.org/abs/2112.08415v1) - [pdf](http://arxiv.org/pdf/2112.08415v1)

> Astronomical transients are stellar objects that become temporarily brighter on various timescales and have led to some of the most significant discoveries in cosmology and astronomy. Some of these transients are the explosive deaths of stars known as supernovae while others are rare, exotic, or entirely new kinds of exciting stellar explosions. New astronomical sky surveys are observing unprecedented numbers of multi-wavelength transients, making standard approaches of visually identifying new and interesting transients infeasible. To meet this demand, we present two novel methods that aim to quickly and automatically detect anomalous transient light curves in real-time. Both methods are based on the simple idea that if the light curves from a known population of transients can be accurately modelled, any deviations from model predictions are likely anomalies. The first approach is a probabilistic neural network built using Temporal Convolutional Networks (TCNs) and the second is an interpretable Bayesian parametric model of a transient. We show that the flexibility of neural networks, the attribute that makes them such a powerful tool for many regression tasks, is what makes them less suitable for anomaly detection when compared with our parametric model.

</details>

<details>

<summary>2021-12-15 19:18:58 - Uncertainty-Aware (UNA) Bases for Deep Bayesian Regression Using Multi-Headed Auxiliary Networks</summary>

- *Sujay Thakur, Cooper Lorsung, Yaniv Yacoby, Finale Doshi-Velez, Weiwei Pan*

- `2006.11695v4` - [abs](http://arxiv.org/abs/2006.11695v4) - [pdf](http://arxiv.org/pdf/2006.11695v4)

> Neural Linear Models (NLM) are deep Bayesian models that produce predictive uncertainties by learning features from the data and then performing Bayesian linear regression over these features. Despite their popularity, few works have focused on methodically evaluating the predictive uncertainties of these models. In this work, we demonstrate that traditional training procedures for NLMs drastically underestimate uncertainty on out-of-distribution inputs, and that they therefore cannot be naively deployed in risk-sensitive applications. We identify the underlying reasons for this behavior and propose a novel training framework that captures useful predictive uncertainties for downstream tasks.

</details>

<details>

<summary>2021-12-16 05:48:02 - On Gibbs Sampling for Structured Bayesian Models Discussion of paper by Zanella and Roberts</summary>

- *Xiaodong Yang, Jun S. Liu*

- `2112.08641v1` - [abs](http://arxiv.org/abs/2112.08641v1) - [pdf](http://arxiv.org/pdf/2112.08641v1)

> This article is a discussion of Zanella and Roberts' paper: Multilevel linear models, gibbs samplers and multigrid decompositions. We consider several extensions in which the multigrid decomposition would bring us interesting insights, including vector hierarchical models, linear mixed effects models and partial centering parametrizations.

</details>

<details>

<summary>2021-12-16 11:26:03 - Constructing a Chain Event Graph from a Staged Tree</summary>

- *Aditi Shenvi, Jim Q. Smith*

- `2006.15857v2` - [abs](http://arxiv.org/abs/2006.15857v2) - [pdf](http://arxiv.org/pdf/2006.15857v2)

> Chain Event Graphs (CEGs) are a recent family of probabilistic graphical models - a generalisation of Bayesian Networks - providing an explicit representation of structural zeros, structural missing values and context-specific conditional independences within their graph topology. A CEG is constructed from an event tree through a sequence of transformations beginning with the colouring of the vertices of the event tree to identify one-step transition symmetries. This coloured event tree, also known as a staged tree, is the output of the learning algorithms used for this family. Surprisingly, no general algorithm has yet been devised that automatically transforms any staged tree into a CEG representation. In this paper we provide a simple iterative backward algorithm for this transformation. Additionally, we show that no information is lost from transforming a staged tree into a CEG. Finally, we demonstrate that with an optimal stopping criterion, our algorithm is more efficient than the generalisation of a special case presented in Silander and Leong (2013). We also provide Python code using this algorithm to obtain a CEG from any staged tree along with the functionality to add edges with sampling zeros.

</details>

<details>

<summary>2021-12-16 11:41:22 - A Bayesian decision support system for counteracting activities of terrorist groups</summary>

- *Aditi Shenvi, F. Oliver Bunnin, Jim Q. Smith*

- `2007.04410v2` - [abs](http://arxiv.org/abs/2007.04410v2) - [pdf](http://arxiv.org/pdf/2007.04410v2)

> Activities of terrorist groups present a serious threat to the security and well-being of the general public. Counterterrorism authorities aim to identify and frustrate the plans of terrorist groups before they are put into action. Whilst the activities of terrorist groups are likely to be hidden and disguised, the members of such groups need to communicate and coordinate to organise their activities. Such observable behaviour and communications data can be utilised by the authorities to estimate the threat posed by a terrorist group. However, to be credible, any such statistical model needs to fold in the level of threat posed by each member of the group. Unlike in other benign forms of social networks, considering the members of terrorist groups as exchangeable gives an incomplete picture of the combined capacity of the group to do harm. Here we develop a Bayesian integrating decision support system that can bring together information relating to each of the members of a terrorist group as well as the combined activities of the group.

</details>

<details>

<summary>2021-12-16 15:11:18 - Using Bayesian Evidence Synthesis Methods to Incorporate Real World Evidence in Surrogate Endpoint Evaluation</summary>

- *Lorna Wheaton, Anastasios Papanikos, Anne Thomas, Sylwia Bujkiewicz*

- `2112.08948v1` - [abs](http://arxiv.org/abs/2112.08948v1) - [pdf](http://arxiv.org/pdf/2112.08948v1)

> Objective: Traditionally validation of surrogate endpoints has been carried out using RCT data. However, RCT data may be too limited to validate surrogate endpoints. In this paper, we sought to improve validation of surrogate endpoints with the inclusion of real world evidence (RWE).   Study Design and Setting: We use data from comparative RWE (cRWE) and single arm RWE (sRWE), to supplement RCT evidence for evaluation of progression free survival (PFS) as a surrogate endpoint to overall survival (OS) in metastatic colorectal cancer (mCRC). Treatment effect estimates from RCTs, cRWE and matched sRWE, comparing anti-angiogenic treatments with chemotherapy, were used to inform surrogacy patterns and predictions of the treatment effect on OS from the treatment effect on PFS.   Results: Seven RCTs, four cRWE studies and three matched sRWE studies were identified. The addition of RWE to RCTs reduced the uncertainty around the estimates of the parameters for the surrogate relationship. Addition of RWE to RCTs also improved the accuracy and precision of predictions of the treatment effect on OS obtained using data on the observed effect on PFS.   Conclusion: The addition of RWE to RCT data improved the precision of the parameters describing the surrogate relationship between treatment effects on PFS and OS and the predicted clinical benefit.

</details>

<details>

<summary>2021-12-16 18:28:16 - DiBS: Differentiable Bayesian Structure Learning</summary>

- *Lars Lorch, Jonas Rothfuss, Bernhard Schölkopf, Andreas Krause*

- `2105.11839v3` - [abs](http://arxiv.org/abs/2105.11839v3) - [pdf](http://arxiv.org/pdf/2105.11839v3)

> Bayesian structure learning allows inferring Bayesian network structure from data while reasoning about the epistemic uncertainty -- a key element towards enabling active causal discovery and designing interventions in real world systems. In this work, we propose a general, fully differentiable framework for Bayesian structure learning (DiBS) that operates in the continuous space of a latent probabilistic graph representation. Contrary to existing work, DiBS is agnostic to the form of the local conditional distributions and allows for joint posterior inference of both the graph structure and the conditional distribution parameters. This makes our formulation directly applicable to posterior inference of complex Bayesian network models, e.g., with nonlinear dependencies encoded by neural networks. Using DiBS, we devise an efficient, general purpose variational inference method for approximating distributions over structural models. In evaluations on simulated and real-world data, our method significantly outperforms related approaches to joint posterior inference.

</details>

<details>

<summary>2021-12-16 21:21:18 - Bayesian Inference for Stationary Points in Gaussian Process Regression Models for Event-Related Potentials Analysis</summary>

- *Cheng-Han Yu, Meng Li, Colin Noe, Simon Fischer-Baum, Marina Vannucci*

- `2009.07745v3` - [abs](http://arxiv.org/abs/2009.07745v3) - [pdf](http://arxiv.org/pdf/2009.07745v3)

> Stationary points embedded in the derivatives are often critical for a model to be interpretable and may be considered as key features of interest in many applications. We propose a semiparametric Bayesian model to efficiently infer the locations of stationary points of a nonparametric function, while treating the function itself as a nuisance parameter. We use Gaussian processes as a flexible prior for the underlying function and impose derivative constraints to control the function's shape via conditioning. We develop an inferential strategy that intentionally restricts estimation to the case of at least one stationary point, bypassing possible mis-specifications in the number of stationary points and avoiding the varying dimension problem that often brings in computational complexity. We illustrate the proposed methods using simulations and then apply the method to the estimation of event-related potentials (ERP) derived from electroencephalography (EEG) signals. We show how the proposed method automatically identifies characteristic components and their latencies at the individual level, which avoids the excessive averaging across subjects which is routinely done in the field to obtain smooth curves. By applying this approach to EEG data collected from younger and older adults during a speech perception task, we are able to demonstrate how the time course of speech perception processes changes with age.

</details>

<details>

<summary>2021-12-16 21:27:12 - Parallel and Flexible Sampling from Autoregressive Models via Langevin Dynamics</summary>

- *Vivek Jayaram, John Thickstun*

- `2105.08164v2` - [abs](http://arxiv.org/abs/2105.08164v2) - [pdf](http://arxiv.org/pdf/2105.08164v2)

> This paper introduces an alternative approach to sampling from autoregressive models. Autoregressive models are typically sampled sequentially, according to the transition dynamics defined by the model. Instead, we propose a sampling procedure that initializes a sequence with white noise and follows a Markov chain defined by Langevin dynamics on the global log-likelihood of the sequence. This approach parallelizes the sampling process and generalizes to conditional sampling. Using an autoregressive model as a Bayesian prior, we can steer the output of a generative model using a conditional likelihood or constraints. We apply these techniques to autoregressive models in the visual and audio domains, with competitive results for audio source separation, super-resolution, and inpainting.

</details>

<details>

<summary>2021-12-16 21:49:52 - Marginalization in Bayesian Networks: Integrating Exact and Approximate Inference</summary>

- *Fritz M. Bayer, Giusi Moffa, Niko Beerenwinkel, Jack Kuipers*

- `2112.09217v1` - [abs](http://arxiv.org/abs/2112.09217v1) - [pdf](http://arxiv.org/pdf/2112.09217v1)

> Bayesian Networks are probabilistic graphical models that can compactly represent dependencies among random variables. Missing data and hidden variables require calculating the marginal probability distribution of a subset of the variables. While knowledge of the marginal probability distribution is crucial for various problems in statistics and machine learning, its exact computation is generally not feasible for categorical variables due to the NP-hardness of this task. We develop a divide-and-conquer approach using the graphical properties of Bayesian networks to split the computation of the marginal probability distribution into sub-calculations of lower dimensionality, reducing the overall computational complexity. Exploiting this property, we present an efficient and scalable algorithm for estimating the marginal probability distribution for categorical variables. The novel method is compared against state-of-the-art approximate inference methods in a benchmarking study, where it displays superior performance. As an immediate application, we demonstrate how the marginal probability distribution can be used to classify incomplete data against Bayesian networks and use this approach for identifying the cancer subtype of kidney cancer patient samples.

</details>

<details>

<summary>2021-12-16 22:41:26 - Block Gibbs samplers for logistic mixed models: convergence properties and a comparison with full Gibbs samplers</summary>

- *Yalin Rao, Vivekananda Roy*

- `2101.03849v3` - [abs](http://arxiv.org/abs/2101.03849v3) - [pdf](http://arxiv.org/pdf/2101.03849v3)

> The logistic linear mixed model (LLMM) is one of the most widely used statistical models. Generally, Markov chain Monte Carlo algorithms are used to explore the posterior densities associated with the Bayesian LLMMs. Polson, Scott and Windle's (2013) Polya-Gamma data augmentation (DA) technique can be used to construct full Gibbs (FG) samplers for the LLMMs. Here, we develop efficient block Gibbs (BG) samplers for Bayesian LLMMs using the Polya-Gamma DA method. We compare the FG and BG samplers in the context of a real data example, as the correlation between the fixed effects and the random effects changes as well as when the dimensions of the design matrices vary. These numerical examples demonstrate superior performance of the BG samplers over the FG samplers. We also derive conditions guaranteeing geometric ergodicity of the BG Markov chain when the popular improper uniform prior is assigned on the regression coefficients, and proper or improper priors are placed on the variance parameters of the random effects. This theoretical result has important practical implications as it justifies the use of asymptotically valid Monte Carlo standard errors for Markov chain based estimates of the posterior quantities.

</details>

<details>

<summary>2021-12-17 07:56:20 - Improving evidential deep learning via multi-task learning</summary>

- *Dongpin Oh, Bonggun Shin*

- `2112.09368v1` - [abs](http://arxiv.org/abs/2112.09368v1) - [pdf](http://arxiv.org/pdf/2112.09368v1)

> The Evidential regression network (ENet) estimates a continuous target and its predictive uncertainty without costly Bayesian model averaging. However, it is possible that the target is inaccurately predicted due to the gradient shrinkage problem of the original loss function of the ENet, the negative log marginal likelihood (NLL) loss. In this paper, the objective is to improve the prediction accuracy of the ENet while maintaining its efficient uncertainty estimation by resolving the gradient shrinkage problem. A multi-task learning (MTL) framework, referred to as MT-ENet, is proposed to accomplish this aim. In the MTL, we define the Lipschitz modified mean squared error (MSE) loss function as another loss and add it to the existing NLL loss. The Lipschitz modified MSE loss is designed to mitigate the gradient conflict with the NLL loss by dynamically adjusting its Lipschitz constant. By doing so, the Lipschitz MSE loss does not disturb the uncertainty estimation of the NLL loss. The MT-ENet enhances the predictive accuracy of the ENet without losing uncertainty estimation capability on the synthetic dataset and real-world benchmarks, including drug-target affinity (DTA) regression. Furthermore, the MT-ENet shows remarkable calibration and out-of-distribution detection capability on the DTA benchmarks.

</details>

<details>

<summary>2021-12-17 09:51:24 - ABCpy: A High-Performance Computing Perspective to Approximate Bayesian Computation</summary>

- *Ritabrata Dutta, Marcel Schoengens, Lorenzo Pacchiardi, Avinash Ummadisingu, Nicole Widmer, Pierre Künzli, Jukka-Pekka Onnela, Antonietta Mira*

- `1711.04694v5` - [abs](http://arxiv.org/abs/1711.04694v5) - [pdf](http://arxiv.org/pdf/1711.04694v5)

> ABCpy is a highly modular scientific library for Approximate Bayesian Computation (ABC) written in Python. The main contribution of this paper is to document a software engineering effort that enables domain scientists to easily apply ABC to their research without being ABC experts; using ABCpy they can easily run large parallel simulations without much knowledge about parallelization. Further, ABCpy enables ABC experts to easily develop new inference schemes and evaluate them in a standardized environment and to extend the library with new algorithms. These benefits come mainly from the modularity of ABCpy. We give an overview of the design of ABCpy and provide a performance evaluation concentrating on parallelization. This points us towards the inherent imbalance in some of the ABC algorithms. We develop a dynamic scheduling MPI implementation to mitigate this issue and evaluate the various ABC algorithms according to their adaptability towards high-performance computing.

</details>

<details>

<summary>2021-12-17 14:19:02 - Dynamic modeling of mortality via mixtures of skewed distribution functions</summary>

- *Emanuele Aliverti, Stefano Mazzuco, Bruno Scarpa*

- `2102.01599v2` - [abs](http://arxiv.org/abs/2102.01599v2) - [pdf](http://arxiv.org/pdf/2102.01599v2)

> There has been growing interest on forecasting mortality. In this article, we propose a novel dynamic Bayesian approach for modeling and forecasting the age-at-death distribution, focusing on a three-components mixture of a Dirac mass, a Gaussian distribution and a Skew-Normal distribution. According to the specified model, the age-at-death distribution is characterized via seven parameters corresponding to the main aspects of infant, adult and old-age mortality. The proposed approach focuses on coherent modeling of multiple countries, and following a Bayesian approach to inference we allow to borrow information across populations and to shrink parameters towards a common mean level, implicitly penalizing diverging scenarios. Dynamic modeling across years is induced trough an hierarchical dynamic prior distribution that allows to characterize the temporal evolution of each mortality component and to forecast the age-at-death distribution. Empirical results on multiple countries indicate that the proposed approach outperforms popular methods for forecasting mortality, providing interpretable insights on the evolution of mortality.

</details>

<details>

<summary>2021-12-17 14:30:36 - A flexible Bayesian hierarchical modeling framework for spatially dependent peaks-over-threshold data</summary>

- *Rishikesh Yadav, Raphaël Huser, Thomas Opitz*

- `2112.09530v1` - [abs](http://arxiv.org/abs/2112.09530v1) - [pdf](http://arxiv.org/pdf/2112.09530v1)

> In this work, we develop a constructive modeling framework for extreme threshold exceedances in repeated observations of spatial fields, based on general product mixtures of random fields possessing light or heavy-tailed margins and various spatial dependence characteristics, which are suitably designed to provide high flexibility in the tail and at sub-asymptotic levels. Our proposed model is akin to a recently proposed Gamma-Gamma model using a ratio of processes with Gamma marginal distributions, but it possesses a higher degree of flexibility in its joint tail structure, capturing strong dependence more easily. We focus on constructions with the following three product factors, whose different roles ensure their statistical identifiability: a heavy-tailed spatially-dependent field, a lighter-tailed spatially-constant field, and another lighter-tailed spatially-independent field. Thanks to the model's hierarchical formulation, inference may be conveniently performed based on Markov chain Monte Carlo methods. We leverage the Metropolis adjusted Langevin algorithm (MALA) with random block proposals for latent variables, as well as the stochastic gradient Langevin dynamics (SGLD) algorithm for hyperparameters, in order to fit our proposed model very efficiently in relatively high spatio-temporal dimensions, while simultaneously censoring non-threshold exceedances and performing spatial prediction at multiple sites. The censoring mechanism is applied to the spatially independent component, such that only univariate cumulative distribution functions have to be evaluated. We explore the theoretical properties of the novel model, and illustrate the proposed methodology by simulation and application to daily precipitation data from North-Eastern Spain measured at about 100 stations over the period 2011-2020.

</details>

<details>

<summary>2021-12-17 17:34:40 - On Frequentist and Bayesian Sequential Clinical Trial Designs</summary>

- *Tianjian Zhou, Yuan Ji*

- `2112.09644v1` - [abs](http://arxiv.org/abs/2112.09644v1) - [pdf](http://arxiv.org/pdf/2112.09644v1)

> Clinical trials usually involve sequential patient entry. When designing a clinical trial, it is often desirable to include a provision for interim analyses of accumulating data with the potential for stopping the trial early. We review frequentist and Bayesian sequential clinical trial designs with a focus on their fundamental and philosophical differences. Frequentist designs utilize repeated significance testing or conditional power to make early stopping decisions. The majority of frequentist designs are concerned with controlling the overall type I error rate of falsely rejecting the null hypothesis at any analysis. On the other hand, Bayesian designs utilize posterior or posterior predictive probabilities for decision-making. The prior and threshold values in a Bayesian design can be chosen to either achieve desirable frequentist operating characteristics or reflect the investigator's subjective belief. We also comment on the likelihood principle, which is commonly tied with statistical inference and decision-making in sequential clinical trials. A single-arm trial example with normally distributed outcomes is used throughout to illustrate some frequentist and Bayesian designs. Numerical studies are conducted to assess these designs.

</details>

<details>

<summary>2021-12-17 19:09:30 - Bayesian nonparametric dynamic hazard rates in evolutionary life tables</summary>

- *Luis E. Nieto-Barajas*

- `2103.07005v2` - [abs](http://arxiv.org/abs/2103.07005v2) - [pdf](http://arxiv.org/pdf/2103.07005v2)

> In the study of life tables the random variable of interest is usually assumed discrete since mortality rates are studied for integer ages. In dynamic life tables a time domain is included to account for the evolution effect of the hazard rates in time. In this article we follow a survival analysis approach and use a nonparametric description of the hazard rates. We construct a discrete time stochastic processes that reflects dependence across age as well as in time. This process is used as a bayesian nonparametric prior distribution for the hazard rates for the study of evolutionary life tables. Prior properties of the process are studied and posterior distributions are derived. We present a simulation study, with the inclusion of right censored observations, as well as a real data analysis to show the performance of our model.

</details>

<details>

<summary>2021-12-17 19:44:40 - Spatiotemporal Modeling of Nursery Habitat Using Bayesian Inference: Environmental Drivers of Juvenile Blue Crab Abundance</summary>

- *A. Challen Hyman, Grace S. Chiu, Mary C. Fabrizio, Romuald N. Lipcius*

- `2201.06926v1` - [abs](http://arxiv.org/abs/2201.06926v1) - [pdf](http://arxiv.org/pdf/2201.06926v1)

> Nursery grounds are favorable for growth and survival of juvenile fish and crustaceans through abundant food resources and refugia, and enhance secondary production of populations. While small-scale studies remain important tools to assess nursery value of habitats, targeted applications that unify survey data over large spatiotemporal scales are vital to generalize inference of nursery function, identify highly productive regions, and inform management strategies. Using 21 years of GIS and spatiotemporally indexed field survey data on potential nursery habitats, we constructed five Bayesian models with varying spatiotemporal dependence structures to infer nursery habitat value for juveniles of the blue crab C. sapidus within three tributaries in lower Chesapeake Bay. Out-of-sample predictions of juvenile counts from a fully nonseparable spatiotemporal model outperformed predictions from simpler models. Salt marsh surface area, turbidity, and their interaction showed the strongest associations (and positively) with abundance. Relative seagrass area, previously emphasized as the most valuable nursery in small spatial-scale studies, was not associated with abundance. Hence, we argue that salt marshes should be considered a key nursery habitat for blue crabs, even amidst extensive seagrass beds. Moreover, identification of nurseries should be based on investigations at broad spatiotemporal scales incorporating multiple potential nursery habitats, and on rigorously addressing spatiotemporal dependence.

</details>

<details>

<summary>2021-12-17 22:35:47 - Nested Bayesian Optimization for Computer Experiments</summary>

- *Yan Wang, Meng Wang, Areej AlBahar, Xiaowei Yue*

- `2112.09797v1` - [abs](http://arxiv.org/abs/2112.09797v1) - [pdf](http://arxiv.org/pdf/2112.09797v1)

> Computer experiments can emulate the physical systems, help computational investigations, and yield analytic solutions. They have been widely employed with many engineering applications (e.g., aerospace, automotive, energy systems. Conventional Bayesian optimization did not incorporate the nested structures in computer experiments. This paper proposes a novel nested Bayesian optimization for complex computer experiments with multi-step or hierarchical characteristics. We prove the theoretical properties of nested outputs given two cases: Gaussian or non-Gaussian. The closed forms of nested expected improvement are derived. We also propose the computational algorithms for nested Bayesian optimization. Three numerical studies show that the proposed nested Bayesian optimization outperforms the five benchmark Bayesian optimization methods ignoring the intermediate outputs of the inner computer code. The case study shows that the nested Bayesian optimization can efficiently minimize the residual stress during composite structures assembly and avoid convergence to the local optimum.

</details>

<details>

<summary>2021-12-18 00:58:01 - A Bayesian hierarchical small-area population model accounting for data source specific methodologies from American Community Survey, Population Estimates Program, and Decennial Census data</summary>

- *Emily N Peterson, Rachel C Nethery, Tullia Padellini, Jarvis T Chen, Brent A Coull, Frederic B Piel, Jon Wakefield, Marta Blangiardo, Lance A Waller*

- `2112.09813v1` - [abs](http://arxiv.org/abs/2112.09813v1) - [pdf](http://arxiv.org/pdf/2112.09813v1)

> Small area estimates of population are necessary for many epidemiological studies, yet their quality and accuracy are often not assessed. In the United States, small area estimates of population counts are published by the United States Census Bureau (USCB) in the form of the Decennial census counts, Intercensal population projections (PEP), and American Community Survey (ACS) estimates. Although there are significant relationships between these data sources, there are important contrasts in data collection and processing methodologies, such that each set of estimates may be subject to different sources and magnitudes of error. Additionally, these data sources do not report identical small area population counts due to post-survey adjustments specific to each data source. Resulting small area disease/mortality rates may differ depending on which data source is used for population counts (denominator data). To accurately capture annual small area population counts, and associated uncertainties, we present a Bayesian population model (B-Pop), which fuses information from all three USCB sources, accounting for data source specific methodologies and associated errors. The main features of our framework are: 1) a single model integrating multiple data sources, 2) accounting for data source specific data generating mechanisms, and specifically accounting for data source specific errors, and 3) prediction of estimates for years without USCB reported data. We focus our study on the 159 counties of Georgia, and produce estimates for years 2005-2021.

</details>

<details>

<summary>2021-12-18 10:08:21 - Building A Bayesian Decision Support System for Evaluating COVID-19 Countermeasure Strategies</summary>

- *Peter Strong, Aditi Shenvi, Xuewen Yu, K. Nadia Papamichail, Henry P Wynn, Jim Q Smith*

- `2101.04774v3` - [abs](http://arxiv.org/abs/2101.04774v3) - [pdf](http://arxiv.org/pdf/2101.04774v3)

> Decision making in the face of a disaster requires the consideration of several complex factors. In such cases, Bayesian multi-criteria decision analysis provides a framework for decision making. In this paper, we present how to construct a multi-attribute decision support system for choosing between countermeasure strategies, such as lockdowns, designed to mitigate the effects of COVID-19. Such an analysis can evaluate both the short term and long term efficacy of various candidate countermeasures. The expected utility scores of a countermeasure strategy capture the expected impact of the policies on health outcomes and other measures of population well-being. The broad methodologies we use here have been established for some time. However, this application has many novel elements to it: the pervasive uncertainty of the science; the necessary dynamic shifts between regimes within each candidate suite of countermeasures; and the fast moving stochastic development of the underlying threat all present new challenges to this domain. Our methodology is illustrated by demonstrating in a simplified example how the efficacy of various strategies can be formally compared through balancing impacts of countermeasures, not only on the short term (e.g. COVID-19 deaths) but the medium to long term effects on the population (e.g increased poverty).

</details>

<details>

<summary>2021-12-18 14:22:44 - Adversarial Attack for Uncertainty Estimation: Identifying Critical Regions in Neural Networks</summary>

- *Ismail Alarab, Simant Prakoonwit*

- `2107.07618v2` - [abs](http://arxiv.org/abs/2107.07618v2) - [pdf](http://arxiv.org/pdf/2107.07618v2)

> We propose a novel method to capture data points near decision boundary in neural network that are often referred to a specific type of uncertainty. In our approach, we sought to perform uncertainty estimation based on the idea of adversarial attack method. In this paper, uncertainty estimates are derived from the input perturbations, unlike previous studies that provide perturbations on the model's parameters as in Bayesian approach. We are able to produce uncertainty with couple of perturbations on the inputs. Interestingly, we apply the proposed method to datasets derived from blockchain. We compare the performance of model uncertainty with the most recent uncertainty methods. We show that the proposed method has revealed a significant outperformance over other methods and provided less risk to capture model uncertainty in machine learning.

</details>

<details>

<summary>2021-12-18 20:08:50 - Bayesian Assessments of Aeroengine Performance with Transfer Learning</summary>

- *Pranay Seshadri, Andrew Duncan, George Thorne, Geoffrey Parks, Raul Vazquez Diaz, Mark Girolami*

- `2011.14698v2` - [abs](http://arxiv.org/abs/2011.14698v2) - [pdf](http://arxiv.org/pdf/2011.14698v2)

> Aeroengine performance is determined by temperature and pressure profiles along various axial stations within an engine. Given limited sensor measurements both along and between axial stations, we require a statistically principled approach to inferring these profiles. In this paper we detail a Bayesian methodology for interpolating the spatial temperature or pressure profile at axial stations within an aeroengine. The profile at any given axial station is represented as a spatial Gaussian random field on an annulus, with circumferential variations modelled using a Fourier basis and radial variations modelled with a squared exponential kernel. This Gaussian random field is extended to ingest data from multiple axial measurement planes, with the aim of transferring information across the planes. To facilitate this type of transfer learning, a novel planar covariance kernel is proposed, with hyperparameters that characterise the correlation between any two measurement planes. In the scenario where precise frequencies comprising the temperature field are unknown, we utilise a sparsity-promoting prior on the frequencies to encourage sparse representations. This easily extends to cases with multiple engine planes whilst accommodating frequency variations between the planes. The main quantity of interest, the spatial area average is readily obtained in closed form. We term this the Bayesian area average and demonstrate how this metric offers far more precise averages than a sector area average -- a widely used area averaging approach. Furthermore, the Bayesian area average naturally decomposes the posterior uncertainty into terms characterising insufficient sampling and sensor measurement error respectively. This too provides a significant improvement over prior standard deviation based uncertainty breakdowns.

</details>

<details>

<summary>2021-12-18 22:35:15 - Bayesian Mass Averaging in Rigs and Engines</summary>

- *Pranay Seshadri, Andrew Duncan, George Thorne*

- `2011.09240v2` - [abs](http://arxiv.org/abs/2011.09240v2) - [pdf](http://arxiv.org/pdf/2011.09240v2)

> This paper introduces the Bayesian mass average and details its computation. Owing to the complexity of flow in an engine and the limited instrumentation and the precision of the sensor apparatus used, it is difficult to rigorously calculate mass averages. Building upon related work, this paper views any thermodynamic quantity's spatial variation at an axial plane in an engine (or a rig) as a Gaussian random field. In cases where the mass flow rate is constant in the circumferential direction but can be expressed via a polynomial or spline radially, this paper presents an analytical calculation of the Bayesian mass average. In cases where the mass flow rate itself can be expressed as a Gaussian random field, a sampling procedure is presented to calculate the Bayesian mass average. Examples of the calculation of the Bayesian mass average for temperature are presented, including with a real engine case study where velocity profiles are inferred from stagnation pressure measurements.

</details>

<details>

<summary>2021-12-18 23:41:29 - Dynamic Pricing and Demand Learning on a Large Network of Products: A PAC-Bayesian Approach</summary>

- *N. Bora Keskin, David Simchi-Levi, Prem Talwai*

- `2111.00790v3` - [abs](http://arxiv.org/abs/2111.00790v3) - [pdf](http://arxiv.org/pdf/2111.00790v3)

> We consider a seller offering a large network of $N$ products over a time horizon of $T$ periods. The seller does not know the parameters of the products' linear demand model, and can dynamically adjust product prices to learn the demand model based on sales observations. The seller aims to minimize its pseudo-regret, i.e., the expected revenue loss relative to a clairvoyant who knows the underlying demand model. We consider a sparse set of demand relationships between products to characterize various connectivity properties of the product network. In particular, we study three different sparsity frameworks: (1) $L_0$ sparsity, which constrains the number of connections in the network, and (2) off-diagonal sparsity, which constrains the magnitude of cross-product price sensitivities, and (3) a new notion of spectral sparsity, which constrains the asymptotic decay of a similarity metric on network nodes. We propose a dynamic pricing-and-learning policy that combines the optimism-in-the-face-of-uncertainty and PAC-Bayesian approaches, and show that this policy achieves asymptotically optimal performance in terms of $N$ and $T$. We also show that in the case of spectral and off-diagonal sparsity, the seller can have a pseudo-regret linear in $N$, even when the network is dense.

</details>

<details>

<summary>2021-12-19 06:45:22 - Statistically-informed deep learning for gravitational wave parameter estimation</summary>

- *Hongyu Shen, E. A. Huerta, Eamonn O'Shea, Prayush Kumar, Zhizhen Zhao*

- `1903.01998v4` - [abs](http://arxiv.org/abs/1903.01998v4) - [pdf](http://arxiv.org/pdf/1903.01998v4)

> We introduce deep learning models to estimate the masses of the binary components of black hole mergers, $(m_1,m_2)$, and three astrophysical properties of the post-merger compact remnant, namely, the final spin, $a_f$, and the frequency and damping time of the ringdown oscillations of the fundamental $\ell=m=2$ bar mode, $(\omega_R, \omega_I)$. Our neural networks combine a modified $\texttt{WaveNet}$ architecture with contrastive learning and normalizing flow. We validate these models against a Gaussian conjugate prior family whose posterior distribution is described by a closed analytical expression. Upon confirming that our models produce statistically consistent results, we used them to estimate the astrophysical parameters $(m_1,m_2, a_f, \omega_R, \omega_I)$ of five binary black holes: $\texttt{GW150914}, \texttt{GW170104}, \texttt{GW170814}, \texttt{GW190521}$ and $\texttt{GW190630}$. We use $\texttt{PyCBC Inference}$ to directly compare traditional Bayesian methodologies for parameter estimation with our deep-learning-based posterior distributions. Our results show that our neural network models predict posterior distributions that encode physical correlations, and that our data-driven median results and 90$\%$ confidence intervals are similar to those produced with gravitational wave Bayesian analyses. This methodology requires a single V100 $\texttt{NVIDIA}$ GPU to produce median values and posterior distributions within two milliseconds for each event. This neural network, and a tutorial for its use, are available at the $\texttt{Data and Learning Hub for Science}$.

</details>

<details>

<summary>2021-12-19 11:19:16 - Hierarchical Bayesian state-space modeling of age- and sex-structured wildlife population dynamics</summary>

- *Sabyasachi Mukhopadhyay, Hans-Peter Piepho, Sourabh Bhattacharya, Holly T. Dublin, Joseph O. Ogutu*

- `2005.07468v2` - [abs](http://arxiv.org/abs/2005.07468v2) - [pdf](http://arxiv.org/pdf/2005.07468v2)

> Biodiversity is declining at alarming rates worldwide, including for large wild mammals. It is therefore imperative to develop effective population conservation and recovery strategies. Population dynamics models can provide insights into processes driving declines of particular populations of a species and their relative importance. We develop an integrated Bayesian state-space population dynamics model for wildlife populations and illustrate it using a topi population inhabiting the Masai Mara Ecosystem in Kenya. The model is general and integrates ground demographic survey with aerial survey monitoring data. It incorporates population age- and sex-structure and life-history traits and relates birth rates, age-specific survival rates and sex ratio with meteorological covariates, prior population density, environmental seasonality and predation risk. The model runs on a monthly time step, enabling accurate characterization of reproductive seasonality, phenology, synchrony and prolificacy of births and juvenile recruitment. Model performance is evaluated using balanced bootstrap sampling and comparing predictions with aerial population size estimates. The model is implemented using MCMC methods and reproduces several well-known features of the Mara topi population, including striking and persistent population decline, seasonality of births and juvenile recruitment. It can be readily adapted for other wildlife species and extended to incorporate several additional useful features.

</details>

<details>

<summary>2021-12-19 20:24:22 - Realistic and Fast Modeling of Spatial Extremes over Large Geographical Domains</summary>

- *Arnab Hazra, Raphaël Huser, David Bolin*

- `2112.10248v1` - [abs](http://arxiv.org/abs/2112.10248v1) - [pdf](http://arxiv.org/pdf/2112.10248v1)

> Various natural phenomena exhibit spatial extremal dependence at short distances only, while it usually vanishes as the distance between sites increases arbitrarily. However, models proposed in the literature for spatial extremes, which are based on max-stable or Pareto processes or comparatively less computationally demanding ``sub-asymptotic'' models based on Gaussian location and/or scale mixtures, generally assume that spatial extremal dependence persists across the entire spatial domain. This is a clear limitation when modeling extremes over large geographical domains, but surprisingly, it has been mostly overlooked in the literature. In this paper, we develop a more realistic Bayesian framework based on a novel Gaussian scale mixture model, where the Gaussian process component is defined by a stochastic partial differential equation that yields a sparse precision matrix, and the random scale component is modeled as a low-rank Pareto-tailed or Weibull-tailed spatial process determined by compactly supported basis functions. We show that our proposed model is approximately tail-stationary despite its non-stationary construction in terms of basis functions, and we demonstrate that it can capture a wide range of extremal dependence structures as a function of distance. Furthermore, the inherently sparse structure of our spatial model allows fast Bayesian computations, even in high spatial dimensions, based on a customized Markov chain Monte Carlo algorithm, which prioritize calibration in the tail. In our application, we fit our model to analyze heavy monsoon rainfall data in Bangladesh. Our study indicates that the proposed model outperforms some natural alternatives, and that the model fits precipitation extremes satisfactorily well. Finally, we use the fitted model to draw inferences on long-term return levels for marginal precipitation at each site, and for spatial aggregates.

</details>

<details>

<summary>2021-12-19 21:32:52 - Bayesian Error-in-Variables Models for the Identification of Power Networks</summary>

- *Jean-Sébastien Brouillon, Emanuele Fabbiani, Pulkit Nahata, Keith Moffat, Florian Dörfler, Giancarlo Ferrari-Trecate*

- `2107.04480v2` - [abs](http://arxiv.org/abs/2107.04480v2) - [pdf](http://arxiv.org/pdf/2107.04480v2)

> The increasing integration of intermittent renewable generation, especially at the distribution level,necessitates advanced planning and optimisation methodologies contingent on the knowledge of thegrid, specifically the admittance matrix capturing the topology and line parameters of an electricnetwork. However, a reliable estimate of the admittance matrix may either be missing or quicklybecome obsolete for temporally varying grids. In this work, we propose a data-driven identificationmethod utilising voltage and current measurements collected from micro-PMUs. More precisely,we first present a maximum likelihood approach and then move towards a Bayesian framework,leveraging the principles of maximum a posteriori estimation. In contrast with most existing con-tributions, our approach not only factors in measurement noise on both voltage and current data,but is also capable of exploiting available a priori information such as sparsity patterns and knownline parameters. Simulations conducted on benchmark cases demonstrate that, compared to otheralgorithms, our method can achieve significantly greater accuracy.

</details>

<details>

<summary>2021-12-20 05:37:07 - Approximating Bayes in the 21st Century</summary>

- *Gael M. Martin, David T. Frazier, Christian P. Robert*

- `2112.10342v1` - [abs](http://arxiv.org/abs/2112.10342v1) - [pdf](http://arxiv.org/pdf/2112.10342v1)

> The 21st century has seen an enormous growth in the development and use of approximate Bayesian methods. Such methods produce computational solutions to certain intractable statistical problems that challenge exact methods like Markov chain Monte Carlo: for instance, models with unavailable likelihoods, high-dimensional models, and models featuring large data sets. These approximate methods are the subject of this review. The aim is to help new researchers in particular -- and more generally those interested in adopting a Bayesian approach to empirical work -- distinguish between different approximate techniques; understand the sense in which they are approximate; appreciate when and why particular methods are useful; and see the ways in which they can can be combined.

</details>

<details>

<summary>2021-12-20 06:03:22 - Convergence properties of data augmentation algorithms for high-dimensional robit regression</summary>

- *Sourav Mukherjee, Kshitij Khare, Saptarshi Chakraborty*

- `2112.10349v1` - [abs](http://arxiv.org/abs/2112.10349v1) - [pdf](http://arxiv.org/pdf/2112.10349v1)

> The logistic and probit link functions are the most common choices for regression models with a binary response. However, these choices are not robust to the presence of outliers/unexpected observations. The robit link function, which is equal to the inverse CDF of the Student's $t$-distribution, provides a robust alternative to the probit and logistic link functions. A multivariate normal prior for the regression coefficients is the standard choice for Bayesian inference in robit regression models. The resulting posterior density is intractable and a Data Augmentation (DA) Markov chain is used to generate approximate samples from the desired posterior distribution. Establishing geometric ergodicity for this DA Markov chain is important as it provides theoretical guarantees for asymptotic validity of MCMC standard errors for desired posterior expectations/quantiles. Previous work [Roy(2012)] established geometric ergodicity of this robit DA Markov chain assuming (i) the sample size $n$ dominates the number of predictors $p$, and (ii) an additional constraint which requires the sample size to be bounded above by a fixed constant which depends on the design matrix $X$. In particular, modern high-dimensional settings where $n < p$ are not considered. In this work, we show that the robit DA Markov chain is trace-class (i.e., the eigenvalues of the corresponding Markov operator are summable) for arbitrary choices of the sample size $n$, the number of predictors $p$, the design matrix $X$, and the prior mean and variance parameters. The trace-class property implies geometric ergodicity. Moreover, this property allows us to conclude that the sandwich robit chain (obtained by inserting an inexpensive extra step in between the two steps of the DA chain) is strictly better than the robit DA chain in an appropriate sense.

</details>

<details>

<summary>2021-12-20 08:27:12 - Bayesian nonparametric model based clustering with intractable distributions: an ABC approach</summary>

- *Mario Beraha, Riccardo Corradin*

- `2112.10393v1` - [abs](http://arxiv.org/abs/2112.10393v1) - [pdf](http://arxiv.org/pdf/2112.10393v1)

> Bayesian nonparametric mixture models offer a rich framework for model based clustering. We consider the situation where the kernel of the mixture is available only up to an intractable normalizing constant. In this case, most of the commonly used Markov chain Monte Carlo (MCMC) methods are not suitable. We propose an approximate Bayesian computational (ABC) strategy, whereby we approximate the posterior to avoid the intractability of the kernel. We derive an ABC-MCMC algorithm which combines (i) the use of the predictive distribution induced by the nonparametric prior as proposal and (ii) the use of the Wasserstein distance and its connection to optimal matching problems. To overcome the sensibility with respect to the parameters of our algorithm, we further propose an adaptive strategy. We illustrate the use of the proposed algorithm with several simulation studies and an application on real data, where we cluster a population of networks, comparing its performance with standard MCMC algorithms and validating the adaptive strategy.

</details>

<details>

<summary>2021-12-20 09:36:21 - A Bayesian Nonparametric Conditional Two-sample Test with an Application to Local Causal Discovery</summary>

- *Philip A. Boeken, Joris M. Mooij*

- `2008.07382v3` - [abs](http://arxiv.org/abs/2008.07382v3) - [pdf](http://arxiv.org/pdf/2008.07382v3)

> For a continuous random variable $Z$, testing conditional independence $X \perp\!\!\!\perp Y |Z$ is known to be a particularly hard problem. It constitutes a key ingredient of many constraint-based causal discovery algorithms. These algorithms are often applied to datasets containing binary variables, which indicate the 'context' of the observations, e.g. a control or treatment group within an experiment. In these settings, conditional independence testing with $X$ or $Y$ binary (and the other continuous) is paramount to the performance of the causal discovery algorithm. To our knowledge no nonparametric 'mixed' conditional independence test currently exists, and in practice tests that assume all variables to be continuous are used instead. In this paper we aim to fill this gap, as we combine elements of Holmes et al. (2015) and Teymur and Filippi (2020) to propose a novel Bayesian nonparametric conditional two-sample test. Applied to the Local Causal Discovery algorithm, we investigate its performance on both synthetic and real-world data, and compare with state-of-the-art conditional independence tests.

</details>

<details>

<summary>2021-12-20 11:57:05 - Periodic Activation Functions Induce Stationarity</summary>

- *Lassi Meronen, Martin Trapp, Arno Solin*

- `2110.13572v2` - [abs](http://arxiv.org/abs/2110.13572v2) - [pdf](http://arxiv.org/pdf/2110.13572v2)

> Neural network models are known to reinforce hidden data biases, making them unreliable and difficult to interpret. We seek to build models that `know what they do not know' by introducing inductive biases in the function space. We show that periodic activation functions in Bayesian neural networks establish a connection between the prior on the network weights and translation-invariant, stationary Gaussian process priors. Furthermore, we show that this link goes beyond sinusoidal (Fourier) activations by also covering triangular wave and periodic ReLU activation functions. In a series of experiments, we show that periodic activation functions obtain comparable performance for in-domain data and capture sensitivity to perturbed inputs in deep neural networks for out-of-domain detection.

</details>

<details>

<summary>2021-12-20 17:03:49 - Overcoming Free-Riding in Bandit Games</summary>

- *Johannes Hörner, Nicolas Klein, Sven Rady*

- `1910.08953v5` - [abs](http://arxiv.org/abs/1910.08953v5) - [pdf](http://arxiv.org/pdf/1910.08953v5)

> This paper considers a class of experimentation games with L\'{e}vy bandits encompassing those of Bolton and Harris (1999) and Keller, Rady and Cripps (2005). Its main result is that efficient (perfect Bayesian) equilibria exist whenever players' payoffs have a diffusion component. Hence, the trade-offs emphasized in the literature do not rely on the intrinsic nature of bandit models but on the commonly adopted solution concept (MPE). This is not an artifact of continuous time: we prove that efficient equilibria arise as limits of equilibria in the discrete-time game. Furthermore, it suffices to relax the solution concept to strongly symmetric equilibrium.

</details>

<details>

<summary>2021-12-20 17:14:30 - Mathematical modelling, selection and hierarchical inference to determine the minimal dose in IFN$α$ therapy against Myeloproliferative Neoplasms</summary>

- *Gurvan Hermange, William Vainchenker, Isabelle Plo, Paul-Henry Cournède*

- `2112.10688v1` - [abs](http://arxiv.org/abs/2112.10688v1) - [pdf](http://arxiv.org/pdf/2112.10688v1)

> Myeloproliferative Neoplasms (MPN) are blood cancers that appear after acquiring a driver mutation in a hematopoietic stem cell. These hematological malignancies result in the overproduction of mature blood cells and, if not treated, induce a risk of cardiovascular events and thrombosis. Pegylated IFN$\alpha$ is commonly used to treat MPN, but no clear guidelines exist concerning the dose prescribed to patients. We applied a model selection procedure and ran a hierarchical Bayesian inference method to decipher how dose variations impact the response to the therapy. We inferred that IFN$\alpha$ acts on mutated stem cells by inducing their differentiation into progenitor cells, the higher the dose, the higher the effect. We found that when a sufficient (patient-dependent) dose is reached, the treatment can induce a long-term remission. We determined this minimal dose for individuals in a cohort of patients and estimated the most suitable starting dose to give to a new patient to increase the chances of being cured.

</details>

<details>

<summary>2021-12-20 21:34:09 - Blindness of score-based methods to isolated components and mixing proportions</summary>

- *Li K. Wenliang, Heishiro Kanagawa*

- `2008.10087v3` - [abs](http://arxiv.org/abs/2008.10087v3) - [pdf](http://arxiv.org/pdf/2008.10087v3)

> Statistical tasks such as density estimation and approximate Bayesian inference often involve densities with unknown normalising constants. Score-based methods, including score matching, are popular techniques as they are free of normalising constants. Although these methods enjoy theoretical guarantees, a little-known fact is that they exhibit practical failure modes when the unnormalised distribution of interest has isolated components -- they cannot discover isolated components or identify the correct mixing proportions between components. We demonstrate these findings using simple distributions and present heuristic attempts to address these issues. We hope to bring the attention of theoreticians and practitioners to these issues when developing new algorithms and applications.

</details>

<details>

<summary>2021-12-20 21:46:56 - BOP2-DC: Bayesian optimal phase II designs with dual-criterion decision making</summary>

- *Yujie Zhao, Daniel Li, Rong Liu, Ying Yuan*

- `2112.10880v1` - [abs](http://arxiv.org/abs/2112.10880v1) - [pdf](http://arxiv.org/pdf/2112.10880v1)

> The conventional phase II trial design paradigm is to make the go/no-go decision based on the hypothesis testing framework. Statistical significance itself alone, however, may not be sufficient to establish that the drug is clinically effective enough to warrant confirmatory phase III trials. We propose the Bayesian optimal phase II trial design with dual-criterion decision making (BOP2-DC), which incorporates both statistical significance and clinical relevance into decision making. Based on the posterior probability that the treatment effect reaches the lower reference value (statistical significance) and the clinically meaningful value (clinical significance), BOP2-DC allows for go/consider/no-go decisions, rather than a binary go/no-go decision, and it is optimized to maximize the probability of a go decision when the treatment is effective or minimize the sample size when the treatment is futile. BOP2-DC is highly flexible and accommodates various types of endpoints, including binary, continuous, time-to-event, multiple, and co-primary endpoints, in single-arm and randomized trials. Simulation studies show that the BOP2-DC design yields desirable operating characteristics. The software to implement BOP2-DC is freely available at \url{www.trialdesign.org}.

</details>

<details>

<summary>2021-12-20 23:08:30 - A Review of Bayesian Modelling in Glaciology</summary>

- *Giri Gopalan, Andrew Zammit-Mangion, Felicity McCormack*

- `2112.13663v1` - [abs](http://arxiv.org/abs/2112.13663v1) - [pdf](http://arxiv.org/pdf/2112.13663v1)

> Bayesian methods for modelling and inference are being increasingly used in the cryospheric sciences, and glaciology in particular. Here, we present a review of recent works in glaciology that adopt a Bayesian approach when conducting an analysis. We organise the chapter into three categories: i) Gaussian-Gaussian models, ii) Bayesian hierarchical models, and iii) Bayesian calibration approaches. In addition, we present two detailed case studies that involve the application of Bayesian hierarchical models in glaciology. The first case study is on the spatial prediction of surface mass balance across the Icelandic mountain glacier Langj\"okull, and the second is on the prediction of sea-level rise contributions from the Antactcic ice sheet. This chapter is presented in such a way that it is accessible to both statisticians as well as earth scientists.

</details>

<details>

<summary>2021-12-20 23:14:21 - An imprecise-probabilistic characterization of frequentist statistical inference</summary>

- *Ryan Martin*

- `2112.10904v1` - [abs](http://arxiv.org/abs/2112.10904v1) - [pdf](http://arxiv.org/pdf/2112.10904v1)

> Between the two dominant schools of thought in statistics, namely, Bayesian and classical/frequentist, a main difference is that the former is grounded in the mathematically rigorous theory of probability while the latter is not. In this paper, I show that the latter is grounded in a different but equally mathematically rigorous theory of imprecise probability. Specifically, I show that for every suitable testing or confidence procedure with error rate control guarantees, there exists a consonant plausibility function whose derived testing or confidence procedure is no less efficient. Beyond its foundational implications, this characterization has at least two important practical consequences: first, it simplifies the interpretation of p-values and confidence regions, thus creating opportunities for improved education and scientific communication; second, the constructive proof of the main results leads to a strategy for new and improved methods in challenging inference problems.

</details>

<details>

<summary>2021-12-21 03:59:06 - Differentiated uniformization: A new method for inferring Markov chains on combinatorial state spaces including stochastic epidemic models</summary>

- *Kevin Rupp, Rudolf Schill, Jonas Süskind, Peter Georg, Maren Klever, Andreas Lösch, Lars Grasedyck, Tilo Wettig, Rainer Spang*

- `2112.10971v1` - [abs](http://arxiv.org/abs/2112.10971v1) - [pdf](http://arxiv.org/pdf/2112.10971v1)

> Motivation: We consider continuous-time Markov chains that describe the stochastic evolution of a dynamical system by a transition-rate matrix $Q$ which depends on a parameter $\theta$. Computing the probability distribution over states at time $t$ requires the matrix exponential $\exp(tQ)$, and inferring $\theta$ from data requires its derivative $\partial\exp\!(tQ)/\partial\theta$. Both are challenging to compute when the state space and hence the size of $Q$ is huge. This can happen when the state space consists of all combinations of the values of several interacting discrete variables. Often it is even impossible to store $Q$. However, when $Q$ can be written as a sum of tensor products, computing $\exp(tQ)$ becomes feasible by the uniformization method, which does not require explicit storage of $Q$.   Results: Here we provide an analogous algorithm for computing $\partial\exp\!(tQ)/\partial\theta$, the differentiated uniformization method. We demonstrate our algorithm for the stochastic SIR model of epidemic spread, for which we show that $Q$ can be written as a sum of tensor products. We estimate monthly infection and recovery rates during the first wave of the COVID-19 pandemic in Austria and quantify their uncertainty in a full Bayesian analysis.   Availability: Implementation and data are available at https://github.com/spang-lab/TenSIR.

</details>

<details>

<summary>2021-12-21 04:20:37 - Tree-informed Bayesian multi-source domain adaptation: cross-population probabilistic cause-of-death assignment using verbal autopsy</summary>

- *Zhenke Wu, Zehang Richard Li, Irena Chen, Mengbing Li*

- `2112.10978v1` - [abs](http://arxiv.org/abs/2112.10978v1) - [pdf](http://arxiv.org/pdf/2112.10978v1)

> Determining causes of deaths (COD) occurred outside of civil registration and vital statistics systems is challenging. A technique called verbal autopsy (VA) is widely adopted to gather information on deaths in practice. A VA consists of interviewing relatives of a deceased person about symptoms of the deceased in the period leading to the death, often resulting in multivariate binary responses. While statistical methods have been devised for estimating the cause-specific mortality fractions (CSMFs) for a study population, continued expansion of VA to new populations (or "domains") necessitates approaches that recognize between-domain differences while capitalizing on potential similarities. In this paper, we propose such a domain-adaptive method that integrates external between-domain similarity information encoded by a pre-specified rooted weighted tree. Given a cause, we use latent class models to characterize the conditional distributions of the responses that may vary by domain. We specify a logistic stick-breaking Gaussian diffusion process prior along the tree for class mixing weights with node-specific spike-and-slab priors to pool information between the domains in a data-driven way. Posterior inference is conducted via a scalable variational Bayes algorithm. Simulation studies show that the domain adaptation enabled by the proposed method improves CSMF estimation and individual COD assignment. We also illustrate and evaluate the method using a validation data set. The paper concludes with a discussion on limitations and future directions.

</details>

<details>

<summary>2021-12-21 05:08:09 - Shared Frailty Models Based on Cancer Data</summary>

- *Shikhar Tyagi, Arvind Pandey, David D Hanagal*

- `2112.10986v1` - [abs](http://arxiv.org/abs/2112.10986v1) - [pdf](http://arxiv.org/pdf/2112.10986v1)

> Traditional survival analysis techniques focus on the occurrence of failures over the time. During analysis of such events, ignoring the related unobserved covariates or heterogeneity involved in data sample may leads us to adverse consequences. In this context, frailty models are the viable choice to investigate the effect of the unobserved covariates. In this article, we assume that frailty acts multiplicatively to hazard rate. We propose inverse Gaussian (IG) and generalized Lindley (GL) shared frailty models with generalized Weibull (GW) as baseline distribution in order to analyze the unobserved heterogeneity. To estimate the parameters in models, Bayesian paradigm of Markov Chain Monte Carlo technique has been proposed. Model selection criteria have been used for the comparison of models. Three different cancer data sets have been analyzed using the shared frailty models. Better models have been suggested for the data sets.

</details>

<details>

<summary>2021-12-21 08:34:12 - Automatic Termination for Hyperparameter Optimization</summary>

- *Anastasia Makarova, Huibin Shen, Valerio Perrone, Aaron Klein, Jean Baptiste Faddoul, Andreas Krause, Matthias Seeger, Cedric Archambeau*

- `2104.08166v3` - [abs](http://arxiv.org/abs/2104.08166v3) - [pdf](http://arxiv.org/pdf/2104.08166v3)

> Bayesian optimization (BO) is a widely popular approach for the hyperparameter optimization (HPO) of machine learning algorithms. At its core, BO iteratively evaluates promising configurations until a user-defined budget, such as wall-clock time or number of iterations, is exhausted. While the final performance after tuning heavily depends on the provided budget, it is hard to pre-specify an optimal value in advance. In this work, we propose an effective and intuitive termination criterion for BO that automatically stops the procedure if it is sufficiently close to the global optima. Across an extensive range of real-world HPO problems, we show that our termination criterion achieves better test performance compared to existing baselines from the literature, such as stopping when the probability of improvement drops below a fixed threshold. We also provide evidence that these baselines are, compared to our method, highly sensitive to the choices of their own hyperparameters. Additionally, we find that overfitting might occur in the context of HPO, which is arguably an overlooked problem in the literature, and show that our termination criterion mitigates this phenomenon on both small and large datasets.

</details>

<details>

<summary>2021-12-21 19:17:21 - Probabilistic Feature Selection in Joint Quantile Time Series Analysis</summary>

- *Ning Ning*

- `2010.01654v2` - [abs](http://arxiv.org/abs/2010.01654v2) - [pdf](http://arxiv.org/pdf/2010.01654v2)

> Quantile feature selection over correlated multivariate time series data has always been a methodological challenge and is an open problem. In this paper, we propose a general probabilistic methodology for feature selection in joint quantile time series analysis, under the name of quantile feature selection time series (QFSTS) model. The QFSTS model is a general structural time series model, where each component yields an additive contribution to the time series modeling with direct interpretations. Its flexibility is compound in the sense that users can add/deduct components for each times series and each time series can have its own specific valued components of different sizes. Feature selection is conducted in the quantile regression component, where each time series has its own pool of contemporaneous external predictors allowing "nowcasting". Creative probabilistic methodology in extending feature selection to the quantile time series research area is developed by means of multivariate asymmetric Laplace distribution, ``spike-and-slab" prior setup, the Metropolis-Hastings algorithm, and the Bayesian model averaging technique, all implemented consistently in the Bayesian paradigm. Different from most machine learning algorithms, the QFSTS model requires small datasets to train, converges fast, and is executable on ordinary personal computers. Extensive examinations on simulated data and empirical data confirmed that the QFSTS model has superior performance in feature selection, parameter estimation, and forecast.

</details>

<details>

<summary>2021-12-21 20:42:59 - Algorithms for Adaptive Experiments that Trade-off Statistical Analysis with Reward: Combining Uniform Random Assignment and Reward Maximization</summary>

- *Jacob Nogas, Tong Li, Fernando J. Yanez, Arghavan Modiri, Nina Deliu, Ben Prystawski, Sofia S. Villar, Anna Rafferty, Joseph J. Williams*

- `2112.08507v2` - [abs](http://arxiv.org/abs/2112.08507v2) - [pdf](http://arxiv.org/pdf/2112.08507v2)

> Multi-armed bandit algorithms like Thompson Sampling can be used to conduct adaptive experiments, in which maximizing reward means that data is used to progressively assign more participants to more effective arms. Such assignment strategies increase the risk of statistical hypothesis tests identifying a difference between arms when there is not one, and failing to conclude there is a difference in arms when there truly is one. We present simulations for 2-arm experiments that explore two algorithms that combine the benefits of uniform randomization for statistical analysis, with the benefits of reward maximization achieved by Thompson Sampling (TS). First, Top-Two Thompson Sampling adds a fixed amount of uniform random allocation (UR) spread evenly over time. Second, a novel heuristic algorithm, called TS PostDiff (Posterior Probability of Difference). TS PostDiff takes a Bayesian approach to mixing TS and UR: the probability a participant is assigned using UR allocation is the posterior probability that the difference between two arms is `small' (below a certain threshold), allowing for more UR exploration when there is little or no reward to be gained. We find that TS PostDiff method performs well across multiple effect sizes, and thus does not require tuning based on a guess for the true effect size.

</details>

<details>

<summary>2021-12-21 20:57:33 - Bayesian kernel machine regression-causal mediation analysis</summary>

- *Katrina L. Devick, Jennifer F. Bobb, Maitreyi Mazumdar, Birgit Claus Henn, David C. Bellinger, David C. Christiani, Robert O. Wright, Paige L. Williams, Brent A. Coull, Linda Valeri*

- `1811.10453v3` - [abs](http://arxiv.org/abs/1811.10453v3) - [pdf](http://arxiv.org/pdf/1811.10453v3)

> Greater understanding of the pathways through which an environmental mixture operates is important to design effective interventions. We present new methodology to estimate natural direct and indirect effects and controlled direct effects of a complex mixture exposure on an outcome through a mediator variable. We implement Bayesian Kernel Machine Regression (BKMR) to allow for all possible interactions and nonlinear effects of (1) the co-exposures on the mediator, (2) the co-exposures and mediator on the outcome, and (3) selected covariates on the mediator and/or outcome. From the posterior predictive distributions of the mediator and outcome, we simulate counterfactuals to obtain posterior samples, estimates, and credible intervals of the mediation effects. Our simulation study demonstrates that when the exposure-mediator and exposure-mediator-outcome relationships are complex, BKMR--Causal Mediation Analysis performs better than current mediation methods. We applied our methodology to quantify the contribution of birth length as a mediator between in utero co-exposure to arsenic, manganese and lead, and children's neurodevelopmental scores, in a prospective birth cohort in Bangladesh. Among younger children, we found a negative (adverse) association between the metal mixture and neurodevelopment. We also found evidence that birth length mediates the effect of exposure to the metal mixture on neurodevelopment for younger children. If birth length were fixed to its $75^{th}$ percentile value, the harmful effect of the metal mixture on neurodevelopment is attenuated, suggesting nutritional interventions to help increase fetal growth, and thus birth length, could potentially block the harmful effect of the metal mixture on neurodevelopment.

</details>

<details>

<summary>2021-12-22 01:04:50 - Identifying Mixtures of Bayesian Network Distributions</summary>

- *Spencer L. Gordon, Bijan Mazaheri, Yuval Rabani, Leonard J. Schulman*

- `2112.11602v1` - [abs](http://arxiv.org/abs/2112.11602v1) - [pdf](http://arxiv.org/pdf/2112.11602v1)

> A Bayesian Network is a directed acyclic graph (DAG) on a set of $n$ random variables (identified with the vertices); a Bayesian Network Distribution (BND) is a probability distribution on the rv's that is Markovian on the graph. A finite mixture of such models is the projection on these variables of a BND on the larger graph which has an additional "hidden" (or "latent") random variable $U$, ranging in $\{1,\ldots,k\}$, and a directed edge from $U$ to every other vertex.   Models of this type are fundamental to research in Causal Inference, where $U$ models a confounding effect. One extremely special case has been of longstanding interest in the theory literature: the empty graph. Such a distribution is simply a mixture of $k$ product distributions. A longstanding problem has been, given the joint distribution of a mixture of $k$ product distributions, to identify each of the product distributions, and their mixture weights. Our results are:   (1) We improve the sample complexity (and runtime) for identifying mixtures of $k$ product distributions from $\exp(O(k^2))$ to $\exp(O(k \log k))$. This is almost best possible in view of a known $\exp(\Omega(k))$ lower bound.   (2) We give the first algorithm for the case of non-empty graphs. The complexity for a graph of maximum degree $\Delta$ is $\exp(O(k(\Delta^2 + \log k)))$.   (The above complexities are approximate and suppress dependence on secondary parameters.)

</details>

<details>

<summary>2021-12-22 08:06:11 - A Comparison of Bayesian Inference Techniques for Sparse Factor Analysis</summary>

- *Yong See Foo, Heejung Shim*

- `2112.11719v1` - [abs](http://arxiv.org/abs/2112.11719v1) - [pdf](http://arxiv.org/pdf/2112.11719v1)

> Dimension reduction algorithms aim to discover latent variables which describe underlying structures in high-dimensional data. Methods such as factor analysis and principal component analysis have the downside of not offering much interpretability of its inferred latent variables. Sparse factor analysis addresses this issue by imposing sparsity on its factor loadings, allowing each latent variable to be related to only a subset of features, thus increasing interpretability. Sparse factor analysis has been used in a wide range of areas including genomics, signal processing, and economics. We compare two Bayesian inference techniques for sparse factor analysis, namely Markov chain Monte Carlo (MCMC), and variational inference (VI). VI is computationally faster than MCMC, at the cost of a loss in accuracy. We derive MCMC and VI algorithms and perform a comparison using both simulated and biological data, demonstrating that the higher computational efficiency of VI is desirable over the small gain in accuracy when using MCMC. Our implementation of MCMC and VI algorithms for sparse factor analysis is available at https://github.com/ysfoo/sparsefactor.

</details>

<details>

<summary>2021-12-22 09:35:27 - Bayesian Approaches to Shrinkage and Sparse Estimation</summary>

- *Dimitris Korobilis, Kenichi Shimizu*

- `2112.11751v1` - [abs](http://arxiv.org/abs/2112.11751v1) - [pdf](http://arxiv.org/pdf/2112.11751v1)

> In all areas of human knowledge, datasets are increasing in both size and complexity, creating the need for richer statistical models. This trend is also true for economic data, where high-dimensional and nonlinear/nonparametric inference is the norm in several fields of applied econometric work. The purpose of this paper is to introduce the reader to the world of Bayesian model determination, by surveying modern shrinkage and variable selection algorithms and methodologies. Bayesian inference is a natural probabilistic framework for quantifying uncertainty and learning about model parameters, and this feature is particularly important for inference in modern models of high dimensions and increased complexity.   We begin with a linear regression setting in order to introduce various classes of priors that lead to shrinkage/sparse estimators of comparable value to popular penalized likelihood estimators (e.g.\ ridge, lasso). We explore various methods of exact and approximate inference, and discuss their pros and cons. Finally, we explore how priors developed for the simple regression setting can be extended in a straightforward way to various classes of interesting econometric models. In particular, the following case-studies are considered, that demonstrate application of Bayesian shrinkage and variable selection strategies to popular econometric contexts: i) vector autoregressive models; ii) factor models; iii) time-varying parameter regressions; iv) confounder selection in treatment effects models; and v) quantile regression models. A MATLAB package and an accompanying technical manual allow the reader to replicate many of the algorithms described in this review.

</details>

<details>

<summary>2021-12-22 13:07:55 - Investigating the 'old boy network' using latent space models</summary>

- *Ian Hamilton*

- `2112.11857v1` - [abs](http://arxiv.org/abs/2112.11857v1) - [pdf](http://arxiv.org/pdf/2112.11857v1)

> This paper investigates the nature of institutional ties between a group of English schools, including a large proportion of private schools that might be thought of as contributing to the 'old boy network'. The analysis is based on a network of bilaterally-determined school rugby union fixtures. The primary importance of geographical proximity in the determination of these fixtures supplies a spatial 'ground truth' against which the performance of models is assessed. A Bayesian fitting of the latent position cluster model is found to provide the best fit of the models examined. This is used to demonstrate a variety of methods that together provide a consistent and nuanced interpretation of the factors influencing community and edge formation in the network. The influence of homophily in fees and the proportion of boarders is identified as notable, with evidence that this is driven by a community of schools having the highest proportion of boarders and charging the highest fees, suggestive of the existence and nature of an 'old boy network' at an institutional level.

</details>

<details>

<summary>2021-12-22 13:22:30 - Constraining cosmological parameters from N-body simulations with Bayesian Neural Networks</summary>

- *Hector J. Hortua*

- `2112.11865v1` - [abs](http://arxiv.org/abs/2112.11865v1) - [pdf](http://arxiv.org/pdf/2112.11865v1)

> In this paper, we use The Quijote simulations in order to extract the cosmological parameters through Bayesian Neural Networks. This kind of model has a remarkable ability to estimate the associated uncertainty, which is one of the ultimate goals in the precision cosmology era. We demonstrate the advantages of BNNs for extracting more complex output distributions and non-Gaussianities information from the simulations.

</details>

<details>

<summary>2021-12-22 15:36:47 - Sparse online variational Bayesian regression</summary>

- *Kody J. H. Law, Vitaly Zankin*

- `2102.12261v2` - [abs](http://arxiv.org/abs/2102.12261v2) - [pdf](http://arxiv.org/pdf/2102.12261v2)

> This work considers variational Bayesian inference as an inexpensive and scalable alternative to a fully Bayesian approach in the context of sparsity-promoting priors. In particular, the priors considered arise from scale mixtures of Normal distributions with a generalized inverse Gaussian mixing distribution. This includes the variational Bayesian LASSO as an inexpensive and scalable alternative to the Bayesian LASSO introduced in [65]. It also includes a family of priors which more strongly promote sparsity. For linear models the method requires only the iterative solution of deterministic least squares problems. Furthermore, for p unknown covariates the method can be implemented exactly online with a cost of $O(p^3)$ in computation and $O(p^2)$ in memory per iteration -- in other words, the cost per iteration is independent of n, and in principle infinite data can be considered. For large $p$ an approximation is able to achieve promising results for a cost of $O(p)$ per iteration, in both computation and memory. Strategies for hyper-parameter tuning are also considered. The method is implemented for real and simulated data. It is shown that the performance in terms of variable selection and uncertainty quantification of the variational Bayesian LASSO can be comparable to the Bayesian LASSO for problems which are tractable with that method, and for a fraction of the cost. The present method comfortably handles $n = 65536$, $p = 131073$ on a laptop in less than 30 minutes, and $n = 10^5$, $p = 2.1 \times 10^6$ overnight.

</details>

<details>

<summary>2021-12-22 15:42:57 - Efficient Multifidelity Likelihood-Free Bayesian Inference with Adaptive Computational Resource Allocation</summary>

- *Thomas P Prescott, David J Warne, Ruth E Baker*

- `2112.11971v1` - [abs](http://arxiv.org/abs/2112.11971v1) - [pdf](http://arxiv.org/pdf/2112.11971v1)

> Likelihood-free Bayesian inference algorithms are popular methods for calibrating the parameters of complex, stochastic models, required when the likelihood of the observed data is intractable. These algorithms characteristically rely heavily on repeated model simulations. However, whenever the computational cost of simulation is even moderately expensive, the significant burden incurred by likelihood-free algorithms leaves them unviable in many practical applications. The multifidelity approach has been introduced (originally in the context of approximate Bayesian computation) to reduce the simulation burden of likelihood-free inference without loss of accuracy, by using the information provided by simulating computationally cheap, approximate models in place of the model of interest. The first contribution of this work is to demonstrate that multifidelity techniques can be applied in the general likelihood-free Bayesian inference setting. Analytical results on the optimal allocation of computational resources to simulations at different levels of fidelity are derived, and subsequently implemented practically. We provide an adaptive multifidelity likelihood-free inference algorithm that learns the relationships between models at different fidelities and adapts resource allocation accordingly, and demonstrate that this algorithm produces posterior estimates with near-optimal efficiency.

</details>

<details>

<summary>2021-12-22 19:49:45 - Surrogate Likelihoods for Variational Annealed Importance Sampling</summary>

- *Martin Jankowiak, Du Phan*

- `2112.12194v1` - [abs](http://arxiv.org/abs/2112.12194v1) - [pdf](http://arxiv.org/pdf/2112.12194v1)

> Variational inference is a powerful paradigm for approximate Bayesian inference with a number of appealing properties, including support for model learning and data subsampling. By contrast MCMC methods like Hamiltonian Monte Carlo do not share these properties but remain attractive since, contrary to parametric methods, MCMC is asymptotically unbiased. For these reasons researchers have sought to combine the strengths of both classes of algorithms, with recent approaches coming closer to realizing this vision in practice. However, supporting data subsampling in these hybrid methods can be a challenge, a shortcoming that we address by introducing a surrogate likelihood that can be learned jointly with other variational parameters. We argue theoretically that the resulting algorithm permits the user to make an intuitive trade-off between inference fidelity and computational cost. In an extensive empirical comparison we show that our method performs well in practice and that it is well-suited for black-box inference in probabilistic programming frameworks.

</details>

<details>

<summary>2021-12-22 22:33:34 - Model-based clustering for multidimensional social networks</summary>

- *Silvia D'Angelo, Marco Alfò, Michael Fop*

- `2001.05260v2` - [abs](http://arxiv.org/abs/2001.05260v2) - [pdf](http://arxiv.org/pdf/2001.05260v2)

> Social network data are relational data recorded among a group of actors, interacting in different contexts. Often, the same set of actors can be characterized by multiple social relations, captured by a multidimensional network. A common situation is that of colleagues working in the same institution, whose social interactions can be defined on professional and personal levels. In addition, individuals in a network tend to interact more frequently with similar others, naturally creating communities. Latent space models for network data are useful to recover clustering of the actors, as they allow to represent similarities between them by their positions and relative distances in an interpretable low dimensional social space. We propose the infinite latent position cluster model for multidimensional network data, which enables model-based clustering of actors interacting across multiple social dimensions. The model is based on a Bayesian nonparametric framework, that allows to perform automatic inference on the clustering allocations, the number of clusters, and the latent social space. The method is tested on simulated data experiments, and it is employed to investigate the presence of communities in two multidimensional networks recording relationships of different types among colleagues.

</details>

<details>

<summary>2021-12-22 22:53:46 - Density Regression with Bayesian Additive Regression Trees</summary>

- *Vittorio Orlandi, Jared Murray, Antonio Linero, Alexander Volfovsky*

- `2112.12259v1` - [abs](http://arxiv.org/abs/2112.12259v1) - [pdf](http://arxiv.org/pdf/2112.12259v1)

> Flexibly modeling how an entire density changes with covariates is an important but challenging generalization of mean and quantile regression. While existing methods for density regression primarily consist of covariate-dependent discrete mixture models, we consider a continuous latent variable model in general covariate spaces, which we call DR-BART. The prior mapping the latent variable to the observed data is constructed via a novel application of Bayesian Additive Regression Trees (BART). We prove that the posterior induced by our model concentrates quickly around true generative functions that are sufficiently smooth. We also analyze the performance of DR-BART on a set of challenging simulated examples, where it outperforms various other methods for Bayesian density regression. Lastly, we apply DR-BART to two real datasets from educational testing and economics, to study student growth and predict returns to education. Our proposed sampler is efficient and allows one to take advantage of BART's flexibility in many applied settings where the entire distribution of the response is of primary interest. Furthermore, our scheme for splitting on latent variables within BART facilitates its future application to other classes of models that can be described via latent variables, such as those involving hierarchical or time series data.

</details>

<details>

<summary>2021-12-23 07:15:09 - Reinforcement Learning based Sequential Batch-sampling for Bayesian Optimal Experimental Design</summary>

- *Yonatan Ashenafi, Piyush Pandita, Sayan Ghosh*

- `2112.10944v2` - [abs](http://arxiv.org/abs/2112.10944v2) - [pdf](http://arxiv.org/pdf/2112.10944v2)

> Engineering problems that are modeled using sophisticated mathematical methods or are characterized by expensive-to-conduct tests or experiments, are encumbered with limited budget or finite computational resources. Moreover, practical scenarios in the industry, impose restrictions, based on logistics and preference, on the manner in which the experiments can be conducted. For example, material supply may enable only a handful of experiments in a single-shot or in the case of computational models one may face significant wait-time based on shared computational resources. In such scenarios, one usually resorts to performing experiments in a manner that allows for maximizing one's state-of-knowledge while satisfying the above mentioned practical constraints. Sequential design of experiments (SDOE) is a popular suite of methods, that has yielded promising results in recent years across different engineering and practical problems. A common strategy, that leverages Bayesian formalism is the Bayesian SDOE, which usually works best in the one-step-ahead or myopic scenario of selecting a single experiment at each step of a sequence of experiments. In this work, we aim to extend the SDOE strategy, to query the experiment or computer code at a batch of inputs. To this end, we leverage deep reinforcement learning (RL) based policy gradient methods, to propose batches of queries that are selected taking into account entire budget in hand. The algorithm retains the sequential nature, inherent in the SDOE, while incorporating elements of reward based on task from the domain of deep RL. A unique capability of the proposed methodology is its ability to be applied to multiple tasks, for example optimization of a function, once its trained. We demonstrate the performance of the proposed algorithm on a synthetic problem, and a challenging high-dimensional engineering problem.

</details>

<details>

<summary>2021-12-23 17:19:33 - Bayesian Learning: A Selective Overview</summary>

- *Yu Lin Hsu, Chu Chuan Jeng, Pavithra Sripathanallur Murali, Mohammadreza Torkjazi, Jonathan West, Michaela Zuber, Vadim Sokolov*

- `2112.12722v1` - [abs](http://arxiv.org/abs/2112.12722v1) - [pdf](http://arxiv.org/pdf/2112.12722v1)

> This paper presents an overview of some of the concepts of Bayesian Learning. The number of scientific and industrial applications of Bayesian learning has been growing in size rapidly over the last few decades. This process has started with the wide use of Markov Chain Monte Carlo methods that emerged as a dominant computational technique for Bayesian in the early 1990's. Since then Bayesian learning has spread well across several fields from robotics and machine learning to medical applications. This paper provides an overview of some of the widely used concepts and shows several applications. This is a paper based on the series of seminars given by students of a PhD course on Bayesian Learning at George Mason University. The course was taught in the Fall of 2021. Thus, the topics covered in the paper reflect the topics students selected to study.

</details>

<details>

<summary>2021-12-23 17:31:47 - Latent Time Neural Ordinary Differential Equations</summary>

- *Srinivas Anumasa, P. K. Srijith*

- `2112.12728v1` - [abs](http://arxiv.org/abs/2112.12728v1) - [pdf](http://arxiv.org/pdf/2112.12728v1)

> Neural ordinary differential equations (NODE) have been proposed as a continuous depth generalization to popular deep learning models such as Residual networks (ResNets). They provide parameter efficiency and automate the model selection process in deep learning models to some extent. However, they lack the much-required uncertainty modelling and robustness capabilities which are crucial for their use in several real-world applications such as autonomous driving and healthcare. We propose a novel and unique approach to model uncertainty in NODE by considering a distribution over the end-time $T$ of the ODE solver. The proposed approach, latent time NODE (LT-NODE), treats $T$ as a latent variable and apply Bayesian learning to obtain a posterior distribution over $T$ from the data. In particular, we use variational inference to learn an approximate posterior and the model parameters. Prediction is done by considering the NODE representations from different samples of the posterior and can be done efficiently using a single forward pass. As $T$ implicitly defines the depth of a NODE, posterior distribution over $T$ would also help in model selection in NODE. We also propose, adaptive latent time NODE (ALT-NODE), which allow each data point to have a distinct posterior distribution over end-times. ALT-NODE uses amortized variational inference to learn an approximate posterior using inference networks. We demonstrate the effectiveness of the proposed approaches in modelling uncertainty and robustness through experiments on synthetic and several real-world image classification data.

</details>

<details>

<summary>2021-12-23 20:52:33 - ABC of the Future</summary>

- *Henri Pesonen, Umberto Simola, Alvaro Köhn-Luque, Henri Vuollekoski, Xiaoran Lai, Arnoldo Frigessi, Samuel Kaski, David T. Frazier, Worapree Maneesoonthorn, Gael M. Martin, Jukka Corander*

- `2112.12841v1` - [abs](http://arxiv.org/abs/2112.12841v1) - [pdf](http://arxiv.org/pdf/2112.12841v1)

> Approximate Bayesian computation (ABC) has advanced in two decades from a seminal idea to a practically applicable inference tool for simulator-based statistical models, which are becoming increasingly popular in many research domains. The computational feasibility of ABC for practical applications has been recently boosted by adopting techniques from machine learning to build surrogate models for the approximate likelihood or posterior and by the introduction of a general-purpose software platform with several advanced features, including automated parallelization. Here we demonstrate the strengths of the advances in ABC by going beyond the typical benchmark examples and considering real applications in astronomy, infectious disease epidemiology, personalised cancer therapy and financial prediction. We anticipate that the emerging success of ABC in producing actual added value and quantitative insights in the real world will continue to inspire a plethora of further applications across different fields of science, social science and technology.

</details>

<details>

<summary>2021-12-24 01:03:54 - Concave-Convex PDMP-based sampling</summary>

- *Matthew Sutton, Paul Fearnhead*

- `2112.12897v1` - [abs](http://arxiv.org/abs/2112.12897v1) - [pdf](http://arxiv.org/pdf/2112.12897v1)

> Recently non-reversible samplers based on simulating piecewise deterministic Markov processes (PDMPs) have shown potential for efficient sampling in Bayesian inference problems. However, there remains a lack of guidance on how to best implement these algorithms. If implemented poorly, the computational costs of simulating event times can out-weigh the statistical efficiency of the non-reversible dynamics. Drawing on the adaptive rejection literature, we propose the concave-convex adaptive thinning approach for simulating a piecewise deterministic Markov process (CC-PDMP). This approach provides a general guide for constructing bounds that may be used to facilitate PDMP-based sampling. A key advantage of this method is its additive structure - adding concave-convex decompositions yields a concave-convex decomposition. This facilitates swapping priors, simple implementation and computationally efficient thinning. In particular, our approach is well suited to local PDMP simulation where known conditional independence of the target can be exploited for potentially huge computational gains. We provide an R package for implementing the CC-PDMP approach and illustrate how our method outperforms existing approaches to simulating events in the PDMP literature.

</details>

<details>

<summary>2021-12-24 02:10:38 - Annealed Leap-Point Sampler for Multimodal Target Distributions</summary>

- *Nicholas G. Tawn, Matthew T. Moores, Gareth O. Roberts*

- `2112.12908v1` - [abs](http://arxiv.org/abs/2112.12908v1) - [pdf](http://arxiv.org/pdf/2112.12908v1)

> In Bayesian statistics, exploring multimodal posterior distribution poses major challenges for existing techniques such as Markov Chain Monte Carlo (MCMC). These problems are exacerbated in high-dimensional settings where MCMC methods typically rely upon localised proposal mechanisms. This paper introduces the Annealed Leap-Point Sampler (ALPS), which augments the target distribution state space with modified annealed (cooled) target distributions, in contrast to traditional approaches which have employed tempering. The temperature of the coldest state is chosen such that its corresponding annealed target density can be sufficiently well-approximated by a Laplace approximation. As a result, a Gaussian mixture independence Metropolis-Hastings sampler can perform mode-jumping proposals even in high-dimensional problems. The ability of this procedure to "mode hop" at this super-cold state is then filtered through to the target state using a sequence of tempered targets in a similar way to that in parallel tempering methods. ALPS also incorporates the best aspects of current gold-standard approaches to multimodal sampling in high-dimensional contexts. A theoretical analysis of the ALPS approach in high dimensions is given, providing practitioners with a gauge on the optimal setup as well as the scalability of the algorithm. For a $d$-dimensional problem the it is shown that the coldest inverse temperature level required for the ALPS only needs to be linear in the dimension, $\mathcal{O}(d)$, and this means that for a collection of multimodal problems the algorithmic cost is polynomial, $\mathcal{O}\left(d^{3}\right)$. ALPS is illustrated on a complex multimodal posterior distribution that arises from a seemingly-unrelated regression (SUR) model of longitudinal data from U.S. manufacturing firms.

</details>

<details>

<summary>2021-12-24 17:53:48 - Bayesian Nested Latent Class Models for Cause-of-Death Assignment using Verbal Autopsies Across Multiple Domains</summary>

- *Zehang Richard Li, Zhenke Wu, Irena Chen, Samuel J. Clark*

- `2112.12186v2` - [abs](http://arxiv.org/abs/2112.12186v2) - [pdf](http://arxiv.org/pdf/2112.12186v2)

> Understanding cause-specific mortality rates is crucial for monitoring population health and designing public health interventions. Worldwide, two-thirds of deaths do not have a cause assigned. Verbal autopsy (VA) is a well-established tool to collect information describing deaths outside of hospitals by conducting surveys to caregivers of a deceased person. It is routinely implemented in many low- and middle-income countries. Statistical algorithms to assign cause of death using VAs are typically vulnerable to the distribution shift between the data used to train the model and the target population. This presents a major challenge for analyzing VAs as labeled data are usually unavailable in the target population. This article proposes a Latent Class model framework for VA data (LCVA) that jointly models VAs collected over multiple heterogeneous domains, assign cause of death for out-of-domain observations, and estimate cause-specific mortality fractions for a new domain. We introduce a parsimonious representation of the joint distribution of the collected symptoms using nested latent class models and develop an efficient algorithm for posterior inference. We demonstrate that LCVA outperforms existing methods in predictive performance and scalability. Supplementary materials for this article and the R package to implement the model are available online.

</details>

<details>

<summary>2021-12-24 20:13:33 - Sensitivity Analysis of Wind Energy Resources with Bayesian non-Gaussian and nonstationary Functional ANOVA</summary>

- *Jiachen Zhang, Paola Crippa, Marc G. Genton, Stefano Castruccio*

- `2112.13136v1` - [abs](http://arxiv.org/abs/2112.13136v1) - [pdf](http://arxiv.org/pdf/2112.13136v1)

> The transition from non-renewable to renewable energies represents a global societal challenge, and developing a sustainable energy portfolio is an especially daunting task for developing countries where little to no information is available regarding the abundance of renewable resources such as wind. Weather model simulations are key to obtain such information when observational data are scarce and sparse over a country as large and geographically diverse as Saudi Arabia. However, output from such models is uncertain, as it depends on inputs such as the parametrization of the physical processes and the spatial resolution of the simulated domain. In such situations, a sensitivity analysis must be performed and the input may have a spatially heterogeneous influence of wind. In this work, we propose a latent Gaussian functional analysis of variance (ANOVA) model that relies on a nonstationary Gaussian Markov random field approximation of a continuous latent process. The proposed approach is able to capture the local sensitivity of Gaussian and non-Gaussian wind characteristics such as speed and threshold exceedances over a large simulation domain, and a continuous underlying process also allows us to assess the effect of different spatial resolutions. Our results indicate that (1) the non-local planetary boundary layer schemes and high spatial resolution are both instrumental in capturing wind speed and energy (especially over complex mountainous terrain), and (2) their impact on the energy output of Saudi Arabia's planned wind farms is small (at most 1.4%). Thus, our results lend support for the construction of these wind farms in the next decade.

</details>

<details>

<summary>2021-12-25 11:06:44 - Differentially private methods for managing model uncertainty in linear regression models</summary>

- *Víctor Peña, Andrés F. Barrientos*

- `2109.03949v3` - [abs](http://arxiv.org/abs/2109.03949v3) - [pdf](http://arxiv.org/pdf/2109.03949v3)

> In this article, we develop differentially private tools for handling model uncertainty in linear regression models. We introduce hypothesis tests for nested linear models and methods for model averaging and selection. We consider Bayesian approaches based on mixtures of $g$-priors as well as non-Bayesian approaches based on information criteria. The procedures are straightforward to implement with existing software for non-private data and are asymptotically consistent under certain regularity conditions. We address practical issues such as calibrating the tests so that they have adequate type I error rates or quantifying the uncertainty introduced by the privacy mechanisms. Additionally, we provide specific guidelines to maximize the statistical utility of the methods in finite samples.

</details>

<details>

<summary>2021-12-25 14:58:47 - Bayesian Shrinkage Estimation for Stratified Count Data</summary>

- *Yasuyuki Hamura*

- `2112.13245v1` - [abs](http://arxiv.org/abs/2112.13245v1) - [pdf](http://arxiv.org/pdf/2112.13245v1)

> In this paper, we consider the problem of simultaneously estimating Poisson parameters under the standardized squared error loss in situations where we can use side information in aggregated data. Bayesian shrinkage estimators are constructed using conjugate gamma and Dirichlet priors. We compare the risk functions of estimators, obtain conditions for domination, and prove minimaxity and admissibility of a proposed estimator. Finally, two extensions are discussed.

</details>

<details>

<summary>2021-12-26 06:58:29 - Dynamic transformation of prior knowledge into Bayesian models for data streams</summary>

- *Tran Xuan Bach, Nguyen Duc Anh, Ngo Van Linh, Khoat Than*

- `2003.06123v4` - [abs](http://arxiv.org/abs/2003.06123v4) - [pdf](http://arxiv.org/pdf/2003.06123v4)

> We consider how to effectively use prior knowledge when learning a Bayesian model from streaming environments where the data come infinitely and sequentially. This problem is highly important in the era of data explosion and rich sources of precious external knowledge such as pre-trained models, ontologies, Wikipedia, etc. We show that some existing approaches can forget any knowledge very fast. We then propose a novel framework that enables to incorporate the prior knowledge of different forms into a base Bayesian model for data streams. Our framework subsumes some existing popular models for time-series/dynamic data. Extensive experiments show that our framework outperforms existing methods with a large margin. In particular, our framework can help Bayesian models generalize well on extremely short text while other methods overfit. The implementation of our framework is available at https://github.com/bachtranxuan/TPS.git.

</details>

<details>

<summary>2021-12-27 15:07:38 - Decision Theory and Large Deviations for Dynamical Hypotheses Tests: Neyman-Pearson, Min-Max and Bayesian Tests</summary>

- *Hermes H. Ferreira, Artur O. Lopes, Silvia R. C. Lopes*

- `2101.08227v3` - [abs](http://arxiv.org/abs/2101.08227v3) - [pdf](http://arxiv.org/pdf/2101.08227v3)

> We analyze hypotheses tests using classical results on large deviations to compare two models, each one described by a different H\"older Gibbs probability measure. One main difference to the classical hypothesis tests in Decision Theory is that here the two measures are singular with respect to each other. Among other objectives, we are interested in the decay rate of the wrong decisions probability, when the sample size $n$ goes to infinity. We show a dynamical version of the Neyman-Pearson Lemma displaying the ideal test within a certain class of similar tests. This test becomes exponentially better, compared to other alternative tests, when the sample size goes to infinity. We are able to present the explicit exponential decay rate. We also consider both, the Min-Max and a certain type of Bayesian hypotheses tests. We shall consider these tests in the log likelihood framework by using several tools of Thermodynamic Formalism. Versions of the Stein's Lemma and Chernoff's information are also presented.

</details>

<details>

<summary>2021-12-27 18:30:23 - Greedy Algorithm almost Dominates in Smoothed Contextual Bandits</summary>

- *Manish Raghavan, Aleksandrs Slivkins, Jennifer Wortman Vaughan, Zhiwei Steven Wu*

- `2005.10624v2` - [abs](http://arxiv.org/abs/2005.10624v2) - [pdf](http://arxiv.org/pdf/2005.10624v2)

> Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users in order to gain information that will lead to better decisions in the future. While necessary in the worst case, explicit exploration has a number of disadvantages compared to the greedy algorithm that always "exploits" by choosing an action that currently looks optimal. We ask under what conditions inherent diversity in the data makes explicit exploration unnecessary. We build on a recent line of work on the smoothed analysis of the greedy algorithm in the linear contextual bandits model. We improve on prior results to show that a greedy approach almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold, and that this regret is at most $\tilde O(T^{1/3})$.

</details>

<details>

<summary>2021-12-28 02:31:02 - Monitoring Deforestation Using Multivariate Bayesian Online Changepoint Detection with Outliers</summary>

- *Laura J. Wendelberger, Josh M. Gray, Brian J. Reich, Alyson G. Wilson*

- `2112.12899v2` - [abs](http://arxiv.org/abs/2112.12899v2) - [pdf](http://arxiv.org/pdf/2112.12899v2)

> Near real time change detection is important for a variety of Earth monitoring applications and remains a high priority for remote sensing science. Data sparsity, subtle changes, seasonal trends, and the presence of outliers make detecting actual landscape changes challenging. Adams and MacKay (2007) introduced Bayesian Online Changepoint Detection (BOCPD), a computationally efficient, exact Bayesian method for change detection. Incorporation of prior information allows for relaxed dependence on dense data and an extensive stable period, making this method applicable to relatively short time series and multiple changepoint detection. In this paper we conduct BOCPD with a multivariate linear regression framework that supports seasonal trends. We introduce a mechanism to make BOCPD robust against occasional outliers without compromising the computational efficiency of an exact posterior change distribution nor the detection latency. We show via simulations that the method effectively detects change in the presence of outliers. The method is then applied to monitor deforestation in Myanmar where we show superior performance compared to current online changepoint detection methods.

</details>

<details>

<summary>2021-12-28 04:28:21 - Analysis of N-of-1 trials using Bayesian distributed lag model with autocorrelated errors</summary>

- *Ziwei Liao, Min Qian, Ian M. Kronish, Ying Kuen Cheung*

- `2112.13991v1` - [abs](http://arxiv.org/abs/2112.13991v1) - [pdf](http://arxiv.org/pdf/2112.13991v1)

> An N-of-1 trial is a multi-period crossover trial performed in a single individual, with a primary goal to estimate treatment effect on the individual instead of population-level mean responses. As in a conventional crossover trial, it is critical to understand carryover effects of the treatment in an N-of-1 trial, especially when no washout periods between treatment periods are instituted to reduce trial duration. To deal with this issue in situations where high volume of measurements is made during the study, we introduce a novel Bayesian distributed lag model that facilitates the estimation of carryover effects, while accounting for temporal correlations using an autoregressive model. Specifically, we propose a prior variance-covariance structure on the lag coefficients to address collinearity caused by the fact that treatment exposures are typically identical on successive days. A connection between the proposed Bayesian model and penalized regression is noted. Simulation results demonstrate that the proposed model substantially reduces the root mean squared error in the estimation of carryover effects and immediate effects when compared to other existing methods, while being comparable in the estimation of the total effects. We also apply the proposed method to assess the extent of carryover effects of light therapies in relieving depressive symptoms in cancer survivors.

</details>

<details>

<summary>2021-12-28 05:11:41 - Variable Selection Using Bayesian Additive Regression Trees</summary>

- *Chuji Luo, Michael J. Daniels*

- `2112.13998v1` - [abs](http://arxiv.org/abs/2112.13998v1) - [pdf](http://arxiv.org/pdf/2112.13998v1)

> Variable selection is an important statistical problem. This problem becomes more challenging when the candidate predictors are of mixed type (e.g. continuous and binary) and impact the response variable in nonlinear and/or non-additive ways. In this paper, we review existing variable selection approaches for the Bayesian additive regression trees (BART) model, a nonparametric regression model, which is flexible enough to capture the interactions between predictors and nonlinear relationships with the response. An emphasis of this review is on the capability of identifying relevant predictors. We also propose two variable importance measures which can be used in a permutation-based variable selection approach, and a backward variable selection procedure for BART. We present simulations demonstrating that our approaches exhibit improved performance in terms of the ability to recover all the relevant predictors in a variety of data settings, compared to existing BART-based variable selection methods.

</details>

<details>

<summary>2021-12-28 05:14:14 - Solving Bayesian Inverse Problems via Variational Autoencoders</summary>

- *Hwan Goh, Sheroze Sheriffdeen, Jonathan Wittmer, Tan Bui-Thanh*

- `1912.04212v9` - [abs](http://arxiv.org/abs/1912.04212v9) - [pdf](http://arxiv.org/pdf/1912.04212v9)

> In recent years, the field of machine learning has made phenomenal progress in the pursuit of simulating real-world data generation processes. One notable example of such success is the variational autoencoder (VAE). In this work, with a small shift in perspective, we leverage and adapt VAEs for a different purpose: uncertainty quantification in scientific inverse problems. We introduce UQ-VAE: a flexible, adaptive, hybrid data/model-informed framework for training neural networks capable of rapid modelling of the posterior distribution representing the unknown parameter of interest. Specifically, from divergence-based variational inference, our framework is derived such that most of the information usually present in scientific inverse problems is fully utilized in the training procedure. Additionally, this framework includes an adjustable hyperparameter that allows selection of the notion of distance between the posterior model and the target distribution. This introduces more flexibility in controlling how optimization directs the learning of the posterior model. Further, this framework possesses an inherent adaptive optimization property that emerges through the learning of the posterior uncertainty.

</details>

<details>

<summary>2021-12-28 22:47:55 - MUSE: Marginal Unbiased Score Expansion and Application to CMB Lensing</summary>

- *Marius Millea, Uros Seljak*

- `2112.09354v2` - [abs](http://arxiv.org/abs/2112.09354v2) - [pdf](http://arxiv.org/pdf/2112.09354v2)

> We present the marginal unbiased score expansion (MUSE) method, an algorithm for generic high-dimensional hierarchical Bayesian inference. MUSE performs approximate marginalization over arbitrary non-Gaussian latent parameter spaces, yielding Gaussianized asymptotically unbiased and near-optimal constraints on global parameters of interest. It is computationally much cheaper than exact alternatives like Hamiltonian Monte Carlo (HMC), excelling on funnel problems which challenge HMC, and does not require any problem-specific user supervision like other approximate methods such as Variational Inference or many Simulation-Based Inference methods. MUSE makes possible the first joint Bayesian estimation of the delensed Cosmic Microwave Background (CMB) power spectrum and gravitational lensing potential power spectrum, demonstrated here on a simulated data set as large as the upcoming South Pole Telescope 3G 1500 deg$^2$ survey, corresponding to a latent dimensionality of ${\sim}\,6$ million and of order 100 global bandpower parameters. On a subset of the problem where an exact but more expensive HMC solution is feasible, we verify that MUSE yields nearly optimal results. We also demonstrate that existing spectrum-based forecasting tools which ignore pixel-masking underestimate predicted error bars by only ${\sim}\,10\%$. This method is a promising path forward for fast lensing and delensing analyses which will be necessary for future CMB experiments such as SPT-3G, Simons Observatory, or CMB-S4, and can complement or supersede existing HMC approaches. The success of MUSE on this challenging problem strengthens its case as a generic procedure for a broad class of high-dimensional inference problems.

</details>

<details>

<summary>2021-12-29 03:21:50 - Bounding Wasserstein distance with couplings</summary>

- *Niloy Biswas, Lester Mackey*

- `2112.03152v2` - [abs](http://arxiv.org/abs/2112.03152v2) - [pdf](http://arxiv.org/pdf/2112.03152v2)

> Markov chain Monte Carlo (MCMC) provides asymptotically consistent estimates of intractable posterior expectations as the number of iterations tends to infinity. However, in large data applications, MCMC can be computationally expensive per iteration. This has catalyzed interest in sampling methods such as approximate MCMC, which trade off asymptotic consistency for improved computational speed. In this article, we propose estimators based on couplings of Markov chains to assess the quality of such asymptotically biased sampling methods. The estimators give empirical upper bounds of the Wassertein distance between the limiting distribution of the asymptotically biased sampling method and the original target distribution of interest. We establish theoretical guarantees for our upper bounds and show that our estimators can remain effective in high dimensions. We apply our quality measures to stochastic gradient MCMC, variational Bayes, and Laplace approximations for tall data and to approximate MCMC for Bayesian logistic regression in 4500 dimensions and Bayesian linear regression in 50000 dimensions.

</details>

<details>

<summary>2021-12-29 16:07:02 - BayesPPD: An R Package for Bayesian Sample Size Determination Using the Power and Normalized Power Prior for Generalized Linear Models</summary>

- *Yueqi Shen, Matthew A. Psioda, Joseph G. Ibrahim*

- `2112.14616v1` - [abs](http://arxiv.org/abs/2112.14616v1) - [pdf](http://arxiv.org/pdf/2112.14616v1)

> The R package BayesPPD (Bayesian Power Prior Design) supports Bayesian power and type I error calculation and model fitting after incorporating historical data with the power prior and the normalized power prior for generalized linear models (GLM). The package accommodates summary level data or subject level data with covariate information. It supports use of multiple historical datasets as well as design without historical data. Supported distributions for responses include normal, binary (Bernoulli/binomial), Poisson and exponential. The power parameter $a_0$ can be fixed or modeled as random using a normalized power prior for each of these distributions. In addition, the package supports the use of arbitrary sampling priors for computing Bayesian power and type I error rates, and has specific features for GLMs that semi-automatically generate sampling priors from historical data. Since sample size determination (SSD) for GLMs is computationally intensive, an approximation method based on asymptotic theory has been implemented to support applications using the power prior. In addition to describing the statistical methodology and functions implemented in the package to enable SSD, we also demonstrate the use of BayesPPD in two comprehensive case studies.

</details>

<details>

<summary>2021-12-29 23:17:42 - Simplified quasi-likelihood analysis for a locally asymptotically quadratic random field</summary>

- *Nakahiro Yoshida*

- `2102.12460v2` - [abs](http://arxiv.org/abs/2102.12460v2) - [pdf](http://arxiv.org/pdf/2102.12460v2)

> The asymptotic decision theory by Le Cam and Hajek has been given a lucid perspective by the Ibragimov-Hasminskii theory on convergence of the likelihood random field. Their scheme has been applied to stochastic processes by Kutoyants, and today this plot is called the IHK program. This scheme ensures that asymptotic properties of an estimator follow directly from the convergence of the random field if a large deviation estimate exists. The quasi-likelihood analysis (QLA) proved a polynomial type large deviation (PLD) inequality to go through a bottleneck of the program. A conclusion of the QLA is that if the quasi-likelihood random field is asymptotically quadratic and if a key index reflecting identifiability the random field has is non-degenerate, then the PLD inequality is always valid, and as a result, the IHK program can run. Many studies already took advantage of the QLA theory. However, not a few of them are using it in an inefficient way yet. The aim of this paper is to provide a reformed and simplified version of the QLA and to improve accessibility to the theory. As an example of the effects of the theory based on the PLD, the user can obtain asymptotic properties of the quasi-Bayesian estimator by only verifying non-degeneracy of the key index.

</details>

<details>

<summary>2021-12-30 04:21:45 - A combined statistical and machine learning approach for spatial prediction of extreme wildfire frequencies and sizes</summary>

- *Daniela Cisneros, Yan Gong, Rishikesh Yadav, Arnab Hazra, Raphael Huser*

- `2112.14920v1` - [abs](http://arxiv.org/abs/2112.14920v1) - [pdf](http://arxiv.org/pdf/2112.14920v1)

> Motivated by the Extreme Value Analysis 2021 (EVA 2021) data challenge we propose a method based on statistics and machine learning for the spatial prediction of extreme wildfire frequencies and sizes. This method is tailored to handle large datasets, including missing observations. Our approach relies on a four-stage high-dimensional bivariate sparse spatial model for zero-inflated data, which is developed using stochastic partial differential equations(SPDE). In Stage 1, the observations are categorized in zero/nonzero categories and are modeled using a two-layered hierarchical Bayesian sparse spatial model to estimate the probabilities of these two categories. In Stage 2, before modeling the positive observations using spatially-varying coefficients, smoothed parameter surfaces are obtained from empirical estimates using fixed rank kriging. This approximate Bayesian method inference was employed to avoid the high computational burden of large spatial data modeling using spatially-varying coefficients. In Stage 3, the standardized log-transformed positive observations from the second stage are further modeled using a sparse bivariate spatial Gaussian process. The Gaussian distribution assumption for wildfire counts developed in the third stage is computationally effective but erroneous. Thus in Stage 4, the predicted values are rectified using Random Forests. The posterior inference is drawn for Stages 1 and 3 using Markov chain Monte Carlo (MCMC) sampling. A cross-validation scheme is then created for the artificially generated gaps, and the EVA 2021 prediction scores of the proposed model are compared to those obtained using certain natural competitors.

</details>

<details>

<summary>2021-12-30 05:16:10 - Bayesian Quantile Regression with Multiple Proxy Variables</summary>

- *Dongyoung Go, Jongho Im, Ick Hoon Jin*

- `2112.12904v2` - [abs](http://arxiv.org/abs/2112.12904v2) - [pdf](http://arxiv.org/pdf/2112.12904v2)

> Data integration has become more challenging with the emerging availability of multiple data sources. This paper considers Bayesian quantile regression estimation when the key covariate is not directly observed, but the unobserved covariate has multiple proxies. In a unified estimation procedure, the proposed method incorporates these multiple proxies, which have various relationships with the unobserved covariate. The proposed approach allows the inference of both the quantile function and unobserved covariate. Moreover, it requires no linearity of the quantile function or parametric assumptions on the regression error distribution and simultaneously accommodates both linear and nonlinear proxies. The simulation studies show that this methodology successfully integrates multiple proxies and reveals the quantile relationship for a wide range of nonlinear data. The proposed method is applied to the administrative data obtained from the Survey of Household Finances and Living Conditions provided by Statistics Korea. The proposed Bayesian quantile regression is implemented to specify the relationship between assets and salary income in the presence of multiple income records.

</details>

<details>

<summary>2021-12-30 15:31:35 - Bayesian Algorithms Learn to Stabilize Unknown Continuous-Time Systems</summary>

- *Mohamad Kazem Shirani Faradonbeh, Mohamad Sadegh Shirani Faradonbeh*

- `2112.15094v1` - [abs](http://arxiv.org/abs/2112.15094v1) - [pdf](http://arxiv.org/pdf/2112.15094v1)

> Linear dynamical systems are canonical models for learning-based control of plants with uncertain dynamics. The setting consists of a stochastic differential equation that captures the state evolution of the plant understudy, while the true dynamics matrices are unknown and need to be learned from the observed data of state trajectory. An important issue is to ensure that the system is stabilized and destabilizing control actions due to model uncertainties are precluded as soon as possible. A reliable stabilization procedure for this purpose that can effectively learn from unstable data to stabilize the system in a finite time is not currently available. In this work, we propose a novel Bayesian learning algorithm that stabilizes unknown continuous-time stochastic linear systems. The presented algorithm is flexible and exposes effective stabilization performance after a remarkably short time period of interacting with the system.

</details>

<details>

<summary>2021-12-31 05:35:21 - Bayesian Optimization of Function Networks</summary>

- *Raul Astudillo, Peter I. Frazier*

- `2112.15311v1` - [abs](http://arxiv.org/abs/2112.15311v1) - [pdf](http://arxiv.org/pdf/2112.15311v1)

> We consider Bayesian optimization of the output of a network of functions, where each function takes as input the output of its parent nodes, and where the network takes significant time to evaluate. Such problems arise, for example, in reinforcement learning, engineering design, and manufacturing. While the standard Bayesian optimization approach observes only the final output, our approach delivers greater query efficiency by leveraging information that the former ignores: intermediate output within the network. This is achieved by modeling the nodes of the network using Gaussian processes and choosing the points to evaluate using, as our acquisition function, the expected improvement computed with respect to the implied posterior on the objective. Although the non-Gaussian nature of this posterior prevents computing our acquisition function in closed form, we show that it can be efficiently maximized via sample average approximation. In addition, we prove that our method is asymptotically consistent, meaning that it finds a globally optimal solution as the number of evaluations grows to infinity, thus generalizing previously known convergence results for the expected improvement. Notably, this holds even though our method might not evaluate the domain densely, instead leveraging problem structure to leave regions unexplored. Finally, we show that our approach dramatically outperforms standard Bayesian optimization methods in several synthetic and real-world problems.

</details>

<details>

<summary>2021-12-31 06:05:06 - Bayesian Testing Of Granger Causality In Functional Time Series</summary>

- *Rituparna Sen, Anandamayee Majumdar, Shubhangi Sikaria*

- `2112.15315v1` - [abs](http://arxiv.org/abs/2112.15315v1) - [pdf](http://arxiv.org/pdf/2112.15315v1)

> We develop a multivariate functional autoregressive model (MFAR), which captures the cross-correlation among multiple functional time series and thus improves forecast accuracy. We estimate the parameters under the Bayesian dynamic linear models (DLM) framework. In order to capture Granger causality from one FAR series to another we employ Bayes Factor. Motivated by the broad application of functional data in finance, we investigate the causality between the yield curves of two countries. Furthermore, we illustrate a climatology example, examining whether the weather conditions Granger cause pollutant daily levels in a city.

</details>

<details>

<summary>2021-12-31 07:24:48 - An empirical Bayes approach to estimating dynamic models of co-regulated gene expression</summary>

- *Sara Venkatraman, Sumanta Basu, Andrew G. Clark, Sofie Delbare, Myung Hee Lee, Martin T. Wells*

- `2112.15326v1` - [abs](http://arxiv.org/abs/2112.15326v1) - [pdf](http://arxiv.org/pdf/2112.15326v1)

> Time-course gene expression datasets provide insight into the dynamics of complex biological processes, such as immune response and organ development. It is of interest to identify genes with similar temporal expression patterns because such genes are often biologically related. However, this task is challenging due to the high dimensionality of these datasets and the nonlinearity of gene expression time dynamics. We propose an empirical Bayes approach to estimating ordinary differential equation (ODE) models of gene expression, from which we derive a similarity metric between genes called the Bayesian lead-lag $R^2$ (LLR2). Importantly, the calculation of the LLR2 leverages biological databases that document known interactions amongst genes; this information is automatically used to define informative prior distributions on the ODE model's parameters. As a result, the LLR2 is a biologically-informed metric that can be used to identify clusters or networks of functionally-related genes with co-moving or time-delayed expression patterns. We then derive data-driven shrinkage parameters from Stein's unbiased risk estimate that optimally balance the ODE model's fit to both data and external biological information. Using real gene expression data, we demonstrate that our methodology allows us to recover interpretable gene clusters and sparse networks. These results reveal new insights about the dynamics of biological systems.

</details>

<details>

<summary>2021-12-31 10:42:33 - The Integrated Nested Laplace Approximation for fitting Dirichlet regression models</summary>

- *Joaquín Martínez-Minaya, Finn Lindgren, Antonio López-Quílez, Daniel Simpson, David Conesa*

- `1907.04059v3` - [abs](http://arxiv.org/abs/1907.04059v3) - [pdf](http://arxiv.org/pdf/1907.04059v3)

> This paper introduces a Laplace approximation to Bayesian inference in Dirichlet regression models, which can be used to analyze a set of variables on a simplex exhibiting skewness and heteroscedasticity, without having to transform the data. These data, which mainly consist of proportions or percentages of disjoint categories, are widely known as compositional data and are common in areas such as ecology, geology, and psychology. We provide both the theoretical foundations and a description of how Laplace approximation can be implemented in the case of Dirichlet regression. The paper also introduces the package dirinla in the R-language that extends the R-INLA package, which can not deal directly with Dirichlet likelihoods. Simulation studies are presented to validate the good behaviour of the proposed method, while a real data case-study is used to show how this approach can be applied.

</details>

<details>

<summary>2021-12-31 22:28:13 - Bayesian Nonparametric Common Atoms Regression for Generating Synthetic Controls in Clinical Trials</summary>

- *Noirrit Kiran Chandra, Abhra Sarkar, John F. de Groot, Ying Yuan, Peter Müller*

- `2201.00068v1` - [abs](http://arxiv.org/abs/2201.00068v1) - [pdf](http://arxiv.org/pdf/2201.00068v1)

> The availability of electronic health records (EHR) has opened opportunities to supplement increasingly expensive and difficult to carry out randomized controlled trials (RCT) with evidence from readily available real world data. In this paper, we use EHR data to construct synthetic control arms for treatment-only single arm trials. We propose a novel nonparametric Bayesian common atoms mixture model that allows us to find equivalent population strata in the EHR and the treatment arm and then resample the EHR data to create equivalent patient populations under both the single arm trial and the resampled EHR. Resampling is implemented via a density-free importance sampling scheme. Using the synthetic control arm, inference for the treatment effect can then be carried out using any method available for RCTs. Alternatively the proposed nonparametric Bayesian model allows straightforward model-based inference. In simulation experiments, the proposed method vastly outperforms alternative methods. We apply the method to supplement single arm treatment-only glioblastoma studies with a synthetic control arm based on historical trials.

</details>

<details>

<summary>2021-12-31 23:43:13 - Continuity of Generalized Entropy and Statistical Learning</summary>

- *Aolin Xu*

- `2012.15829v2` - [abs](http://arxiv.org/abs/2012.15829v2) - [pdf](http://arxiv.org/pdf/2012.15829v2)

> We study the continuity property of the generalized entropy as a function of the underlying probability distribution, defined with an action space and a loss function, and use this property to answer the basic questions in statistical learning theory: the excess risk analyses for various learning methods. We first derive upper and lower bounds for the entropy difference of two distributions in terms of several commonly used f-divergences, the Wasserstein distance, a distance that depends on the action space and the loss function, and the Bregman divergence generated by the entropy, which also induces bounds in terms of the Euclidean distance between the two distributions. Examples are given along with the discussion of each general result, comparisons are made with the existing entropy difference bounds, and new mutual information upper bounds are derived based on the new results. We then apply the entropy difference bounds to the theory of statistical learning. It is shown that the excess risks in the two popular learning paradigms, the frequentist learning and the Bayesian learning, both can be studied with the continuity property of different forms of the generalized entropy. The analysis is then extended to the continuity of generalized conditional entropy. The extension provides performance bounds for Bayes decision making with mismatched distributions. It also leads to excess risk bounds for a third paradigm of learning, where the decision rule is optimally designed under the projection of the empirical distribution to a predefined family of distributions. We thus establish a unified method of excess risk analysis for the three major paradigms of statistical learning, through the continuity of generalized entropy.

</details>

<details>

<summary>2021-12-31 23:57:15 - Minimum Excess Risk in Bayesian Learning</summary>

- *Aolin Xu, Maxim Raginsky*

- `2012.14868v2` - [abs](http://arxiv.org/abs/2012.14868v2) - [pdf](http://arxiv.org/pdf/2012.14868v2)

> We analyze the best achievable performance of Bayesian learning under generative models by defining and upper-bounding the minimum excess risk (MER): the gap between the minimum expected loss attainable by learning from data and the minimum expected loss that could be achieved if the model realization were known. The definition of MER provides a principled way to define different notions of uncertainties in Bayesian learning, including the aleatoric uncertainty and the minimum epistemic uncertainty. Two methods for deriving upper bounds for the MER are presented. The first method, generally suitable for Bayesian learning with a parametric generative model, upper-bounds the MER by the conditional mutual information between the model parameters and the quantity being predicted given the observed data. It allows us to quantify the rate at which the MER decays to zero as more data becomes available. Under realizable models, this method also relates the MER to the richness of the generative function class, notably the VC dimension in binary classification. The second method, particularly suitable for Bayesian learning with a parametric predictive model, relates the MER to the minimum estimation error of the model parameters from data. It explicitly shows how the uncertainty in model parameter estimation translates to the MER and to the final prediction uncertainty. We also extend the definition and analysis of MER to the setting with multiple model families and the setting with nonparametric models. Along the discussions we draw some comparisons between the MER in Bayesian learning and the excess risk in frequentist learning.

</details>

