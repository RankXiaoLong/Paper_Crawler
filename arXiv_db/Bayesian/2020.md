# 2020

## TOC

- [2020-01](#2020-01)
- [2020-02](#2020-02)
- [2020-03](#2020-03)
- [2020-04](#2020-04)
- [2020-05](#2020-05)
- [2020-06](#2020-06)
- [2020-07](#2020-07)
- [2020-08](#2020-08)
- [2020-09](#2020-09)
- [2020-10](#2020-10)
- [2020-11](#2020-11)
- [2020-12](#2020-12)

## 2020-01

<details>

<summary>2020-01-01 04:27:54 - Particle Flow Bayes' Rule</summary>

- *Xinshi Chen, Hanjun Dai, Le Song*

- `1902.00640v3` - [abs](http://arxiv.org/abs/1902.00640v3) - [pdf](http://arxiv.org/pdf/1902.00640v3)

> We present a particle flow realization of Bayes' rule, where an ODE-based neural operator is used to transport particles from a prior to its posterior after a new observation. We prove that such an ODE operator exists. Its neural parameterization can be trained in a meta-learning framework, allowing this operator to reason about the effect of an individual observation on the posterior, and thus generalize across different priors, observations and to sequential Bayesian inference. We demonstrated the generalization ability of our particle flow Bayes operator in several canonical and high dimensional examples.

</details>

<details>

<summary>2020-01-01 11:15:28 - Bayesian data analysis in empirical software engineering---The case of missing data</summary>

- *Richard Torkar, Robert Feldt, Carlo A. Furia*

- `1904.00661v3` - [abs](http://arxiv.org/abs/1904.00661v3) - [pdf](http://arxiv.org/pdf/1904.00661v3)

> Bayesian data analysis (BDA) is today used by a multitude of research disciplines. These disciplines use BDA as a way to embrace uncertainty by using multilevel models and making use of all available information at hand. In this chapter, we first introduce the reader to BDA and then provide an example from empirical software engineering, where we also deal with a common issue in our field, i.e., missing data.   The example we make use of presents the steps done when conducting state of the art statistical analysis. First, we need to understand the problem we want to solve. Second, we conduct causal analysis. Third, we analyze non-identifiability. Fourth, we conduct missing data analysis. Finally, we do a sensitivity analysis of priors. All this before we design our statistical model. Once we have a model, we present several diagnostics one can use to conduct sanity checks.   We hope that through these examples, the reader will see the advantages of using BDA. This way, we hope Bayesian statistics will become more prevalent in our field, thus partly avoiding the reproducibility crisis we have seen in other disciplines.

</details>

<details>

<summary>2020-01-01 20:33:40 - A Bayesian Nonparametric Estimation to Entropy</summary>

- *Luai Al-Labadi, Viskakh Patel, Kasra Vakiloroayaei, Clement Wan*

- `1903.00655v4` - [abs](http://arxiv.org/abs/1903.00655v4) - [pdf](http://arxiv.org/pdf/1903.00655v4)

> A Bayesian nonparametric estimator to entropy is proposed. The derivation of the new estimator relies on using the Dirichlet process and adapting the well-known frequentist estimators of Vasicek (1976) and Ebrahimi, Pflughoeft and Soofi (1994). Several theoretical properties, such as consistency, of the proposed estimator are obtained. The quality of the proposed estimator has been investigated through several examples, in which it exhibits excellent performance.

</details>

<details>

<summary>2020-01-02 11:53:01 - CircSpaceTime: an R package for spatial and spatio-temporal modeling of Circular data</summary>

- *Giovanna Jona Lasinio, Mario Santoro, Gianluca Mastrantonio*

- `2001.00405v1` - [abs](http://arxiv.org/abs/2001.00405v1) - [pdf](http://arxiv.org/pdf/2001.00405v1)

> CircSpaceTime is the only R package currently available that implements Bayesian models for spatial and spatio-temporal interpolation of circular data. Such data are often found in applications where, among the many, wind directions, animal movement directions, and wave directions are involved. To analyze such data we need models for observations at locations s and times t, as the so-called geostatistical models, providing structured dependence assumed to decay in distance and time. The approach we take begins with Gaussian processes defined for linear variables over space and time. Then, we use either wrapping or projection to obtain processes for circular data. The models are cast as hierarchical, with fitting and inference within a Bayesian framework. Altogether, this package implements work developed by a series of papers; the most relevant being Jona Lasinio, Gelfand, and Jona Lasinio (2012); Wang and Gelfand (2014); Mastrantonio, Jona Lasinio, and Gelfand (2016). All procedures are written using Rcpp. Estimates are obtained by MCMC allowing parallelized multiple chains run. The implementation of the proposed models is considerably improved on the simple routines adopted in the research papers. As original running examples, for the spatial and spatio-temporal settings, we use wind directions datasets over central Italy.

</details>

<details>

<summary>2020-01-02 21:46:48 - Bayesian task embedding for few-shot Bayesian optimization</summary>

- *Steven Atkinson, Sayan Ghosh, Natarajan Chennimalai-Kumar, Genghis Khan, Liping Wang*

- `2001.00637v1` - [abs](http://arxiv.org/abs/2001.00637v1) - [pdf](http://arxiv.org/pdf/2001.00637v1)

> We describe a method for Bayesian optimization by which one may incorporate data from multiple systems whose quantitative interrelationships are unknown a priori. All general (nonreal-valued) features of the systems are associated with continuous latent variables that enter as inputs into a single metamodel that simultaneously learns the response surfaces of all of the systems. Bayesian inference is used to determine appropriate beliefs regarding the latent variables. We explain how the resulting probabilistic metamodel may be used for Bayesian optimization tasks and demonstrate its implementation on a variety of synthetic and real-world examples, comparing its performance under zero-, one-, and few-shot settings against traditional Bayesian optimization, which usually requires substantially more data from the system of interest.

</details>

<details>

<summary>2020-01-03 20:47:45 - Leave-One-Out Cross-Validation for Bayesian Model Comparison in Large Data</summary>

- *Måns Magnusson, Michael Riis Andersen, Johan Jonasson, Aki Vehtari*

- `2001.00980v1` - [abs](http://arxiv.org/abs/2001.00980v1) - [pdf](http://arxiv.org/pdf/2001.00980v1)

> Recently, new methods for model assessment, based on subsampling and posterior approximations, have been proposed for scaling leave-one-out cross-validation (LOO) to large datasets. Although these methods work well for estimating predictive performance for individual models, they are less powerful in model comparison. We propose an efficient method for estimating differences in predictive performance by combining fast approximate LOO surrogates with exact LOO subsampling using the difference estimator and supply proofs with regards to scaling characteristics. The resulting approach can be orders of magnitude more efficient than previous approaches, as well as being better suited to model comparison.

</details>

<details>

<summary>2020-01-05 16:54:10 - A Bayesian Model for Bivariate Causal Inference</summary>

- *Maximilian Kurthen, Torsten A. Enßlin*

- `1812.09895v2` - [abs](http://arxiv.org/abs/1812.09895v2) - [pdf](http://arxiv.org/pdf/1812.09895v2)

> We address the problem of two-variable causal inference without intervention. This task is to infer an existing causal relation between two random variables, i.e. $X \rightarrow Y$ or $Y \rightarrow X$ , from purely observational data. As the option to modify a potential cause is not given in many situations only structural properties of the data can be used to solve this ill-posed problem. We briefly review a number of state-of-the-art methods for this, including very recent ones. A novel inference method is introduced, Bayesian Causal Inference (BCI), which assumes a generative Bayesian hierarchical model to pursue the strategy of Bayesian model selection. In the adopted model the distribution of the cause variable is given by a Poisson lognormal distribution, which allows to explicitly regard the discrete nature of datasets, correlations in the parameter spaces, as well as the variance of probability densities on logarithmic scales. We assume Fourier diagonal Field covariance operators. The model itself is restricted to use cases where a direct causal relation $X \rightarrow Y$ has to be decided against a relation $Y \rightarrow X$ , therefore we compare it other methods for this exact problem setting. The generative model assumed provides synthetic causal data for benchmarking our model in comparison to existing State-of-the-art models, namely LiNGAM , ANM-HSIC , ANM-MML , IGCI and CGNN . We explore how well the above methods perform in case of high noise settings, strongly discretized data and very sparse data. BCI performs generally reliable with synthetic data as well as with the real world TCEP benchmark set, with an accuracy comparable to state-of-the-art algorithms. We discuss directions for the future development of BCI .

</details>

<details>

<summary>2020-01-05 23:37:20 - Emergent limits of an indirect measurement from phase transitions of inference</summary>

- *Satoru Tokuda, Kenji Nagata, Masato Okada*

- `2001.01335v1` - [abs](http://arxiv.org/abs/2001.01335v1) - [pdf](http://arxiv.org/pdf/2001.01335v1)

> Measurements are inseparable from inference, where the estimation of signals of interest from other observations is called an indirect measurement. While a variety of measurement limits have been defined by the physical constraint on each setup, the fundamental limit of an indirect measurement is essentially the limit of inference. Here, we propose the concept of statistical limits on indirect measurement: the bounds of distinction between signals and noise and between a signal and another signal. By developing the asymptotic theory of Bayesian regression, we investigate the phenomenology of a typical indirect measurement and demonstrate the existence of these limits. Based on the connection between inference and statistical physics, we also provide a unified interpretation in which these limits emerge from phase transitions of inference. Our results could pave the way for novel experimental design, enabling assess to the required quality of observations according to the assumed ground truth before the concerned indirect measurement is actually performed.

</details>

<details>

<summary>2020-01-05 23:50:51 - Strong consistency of the AIC, BIC, $C_p$ and KOO methods in high-dimensional multivariate linear regression</summary>

- *Zhidong Bai, Yasunori Fujikoshi, Jiang Hu*

- `1810.12609v3` - [abs](http://arxiv.org/abs/1810.12609v3) - [pdf](http://arxiv.org/pdf/1810.12609v3)

> Variable selection is essential for improving inference and interpretation in multivariate linear regression. Although a number of alternative regressor selection criteria have been suggested, the most prominent and widely used are the Akaike information criterion (AIC), Bayesian information criterion (BIC), Mallow's $C_p$, and their modifications. However, for high-dimensional data, experience has shown that the performance of these classical criteria is not always satisfactory. In the present article, we begin by presenting the necessary and sufficient conditions (NSC) for the strong consistency of the high-dimensional AIC, BIC, and $C_p$, based on which we can identify some reasons for their poor performance. Specifically, we show that under certain mild high-dimensional conditions, if the BIC is strongly consistent, then the AIC is strongly consistent, but not vice versa. This result contradicts the classical understanding. In addition, we consider some NSC for the strong consistency of the high-dimensional kick-one-out (KOO) methods introduced by Zhao et al. (1986) and Nishii et al. (1988). Furthermore, we propose two general methods based on the KOO methods and prove their strong consistency. The proposed general methods remove the penalties while simultaneously reducing the conditions for the dimensions and sizes of the regressors. A simulation study supports our consistency conclusions and shows that the convergence rates of the two proposed general KOO methods are much faster than those of the original methods.

</details>

<details>

<summary>2020-01-06 02:57:58 - Bayesian inference of Stochastic reaction networks using Multifidelity Sequential Tempered Markov Chain Monte Carlo</summary>

- *Thomas A. Catanach, Huy D. Vo, Brian Munsky*

- `2001.01373v1` - [abs](http://arxiv.org/abs/2001.01373v1) - [pdf](http://arxiv.org/pdf/2001.01373v1)

> Stochastic reaction network models are often used to explain and predict the dynamics of gene regulation in single cells. These models usually involve several parameters, such as the kinetic rates of chemical reactions, that are not directly measurable and must be inferred from experimental data. Bayesian inference provides a rigorous probabilistic framework for identifying these parameters by finding a posterior parameter distribution that captures their uncertainty. Traditional computational methods for solving inference problems such as Markov Chain Monte Carlo methods based on classical Metropolis-Hastings algorithm involve numerous serial evaluations of the likelihood function, which in turn requires expensive forward solutions of the chemical master equation (CME). We propose an alternative approach based on a multifidelity extension of the Sequential Tempered Markov Chain Monte Carlo (ST-MCMC) sampler. This algorithm is built upon Sequential Monte Carlo and solves the Bayesian inference problem by decomposing it into a sequence of efficiently solved subproblems that gradually increase model fidelity and the influence of the observed data. We reformulate the finite state projection (FSP) algorithm, a well-known method for solving the CME, to produce a hierarchy of surrogate master equations to be used in this multifidelity scheme. To determine the appropriate fidelity, we introduce a novel information-theoretic criteria that seeks to extract the most information about the ultimate Bayesian posterior from each model in the hierarchy without inducing significant bias. This novel sampling scheme is tested with high performance computing resources using biologically relevant problems.

</details>

<details>

<summary>2020-01-06 05:21:39 - Variational Bayesian Methods for Stochastically Constrained System Design Problems</summary>

- *Prateek Jaiswal, Harsha Honnappa, Vinayak A. Rao*

- `2001.01404v1` - [abs](http://arxiv.org/abs/2001.01404v1) - [pdf](http://arxiv.org/pdf/2001.01404v1)

> We study system design problems stated as parameterized stochastic programs with a chance-constraint set. We adopt a Bayesian approach that requires the computation of a posterior predictive integral which is usually intractable. In addition, for the problem to be a well-defined convex program, we must retain the convexity of the feasible set. Consequently, we propose a variational Bayes-based method to approximately compute the posterior predictive integral that ensures tractability and retains the convexity of the feasible set. Under certain regularity conditions, we also show that the solution set obtained using variational Bayes converges to the true solution set as the number of observations tends to infinity. We also provide bounds on the probability of qualifying a true infeasible point (with respect to the true constraints) as feasible under the VB approximation for a given number of samples.

</details>

<details>

<summary>2020-01-06 20:12:58 - An Automatic Relevance Determination Prior Bayesian Neural Network for Controlled Variable Selection</summary>

- *Rendani Mbuvha, Illyes Boulkaibet, Tshilidzi Marwala*

- `2001.01765v1` - [abs](http://arxiv.org/abs/2001.01765v1) - [pdf](http://arxiv.org/pdf/2001.01765v1)

> We present an Automatic Relevance Determination prior Bayesian Neural Network(BNN-ARD) weight l2-norm measure as a feature importance statistic for the model-x knockoff filter. We show on both simulated data and the Norwegian wind farm dataset that the proposed feature importance statistic yields statistically significant improvements relative to similar feature importance measures in both variable selection power and predictive performance on a real world dataset.

</details>

<details>

<summary>2020-01-06 22:12:18 - Offline Contextual Bayesian Optimization for Nuclear Fusion</summary>

- *Youngseog Chung, Ian Char, Willie Neiswanger, Kirthevasan Kandasamy, Andrew Oakleigh Nelson, Mark D Boyer, Egemen Kolemen, Jeff Schneider*

- `2001.01793v1` - [abs](http://arxiv.org/abs/2001.01793v1) - [pdf](http://arxiv.org/pdf/2001.01793v1)

> Nuclear fusion is regarded as the energy of the future since it presents the possibility of unlimited clean energy. One obstacle in utilizing fusion as a feasible energy source is the stability of the reaction. Ideally, one would have a controller for the reactor that makes actions in response to the current state of the plasma in order to prolong the reaction as long as possible. In this work, we make preliminary steps to learning such a controller. Since learning on a real world reactor is infeasible, we tackle this problem by attempting to learn optimal controls offline via a simulator, where the state of the plasma can be explicitly set. In particular, we introduce a theoretically grounded Bayesian optimization algorithm that recommends a state and action pair to evaluate at every iteration and show that this results in more efficient use of the simulator.

</details>

<details>

<summary>2020-01-07 00:12:36 - Benchmarking Deep Learning Architectures for Predicting Readmission to the ICU and Describing Patients-at-Risk</summary>

- *Sebastiano Barbieri, James Kemp, Oscar Perez-Concha, Sradha Kotwal, Martin Gallagher, Angus Ritchie, Louisa Jorm*

- `1905.08547v3` - [abs](http://arxiv.org/abs/1905.08547v3) - [pdf](http://arxiv.org/pdf/1905.08547v3)

> Objective: To compare different deep learning architectures for predicting the risk of readmission within 30 days of discharge from the intensive care unit (ICU). The interpretability of attention-based models is leveraged to describe patients-at-risk. Methods: Several deep learning architectures making use of attention mechanisms, recurrent layers, neural ordinary differential equations (ODEs), and medical concept embeddings with time-aware attention were trained using publicly available electronic medical record data (MIMIC-III) associated with 45,298 ICU stays for 33,150 patients. Bayesian inference was used to compute the posterior over weights of an attention-based model. Odds ratios associated with an increased risk of readmission were computed for static variables. Diagnoses, procedures, medications, and vital signs were ranked according to the associated risk of readmission. Results: A recurrent neural network, with time dynamics of code embeddings computed by neural ODEs, achieved the highest average precision of 0.331 (AUROC: 0.739, F1-Score: 0.372). Predictive accuracy was comparable across neural network architectures. Groups of patients at risk included those suffering from infectious complications, with chronic or progressive conditions, and for whom standard medical care was not suitable. Conclusions: Attention-based networks may be preferable to recurrent networks if an interpretable model is required, at only marginal cost in predictive accuracy.

</details>

<details>

<summary>2020-01-07 00:39:15 - Scaling Bayesian Probabilistic Record Linkage with Post-Hoc Blocking: An Application to the California Great Registers</summary>

- *Brendan S. McVeigh, Bradley T. Spahn, Jared S. Murray*

- `1905.05337v2` - [abs](http://arxiv.org/abs/1905.05337v2) - [pdf](http://arxiv.org/pdf/1905.05337v2)

> Probabilistic record linkage (PRL) is the process of determining which records in two databases correspond to the same underlying entity in the absence of a unique identifier. Bayesian solutions to this problem provide a powerful mechanism for propagating uncertainty due to uncertain links between records (via the posterior distribution). However, computational considerations severely limit the practical applicability of existing Bayesian approaches. We propose a new computational approach, providing both a fast algorithm for deriving point estimates of the linkage structure that properly account for one-to-one matching and a restricted MCMC algorithm that samples from an approximate posterior distribution. Our advances make it possible to perform Bayesian PRL for larger problems, and to assess the sensitivity of results to varying prior specifications. We demonstrate the methods on a subset of an OCR'd dataset, the California Great Registers, a collection of 57 million voter registrations from 1900 to 1968 that comprise the only panel data set of party registration collected before the advent of scientific surveys.

</details>

<details>

<summary>2020-01-07 05:21:21 - Machine-learning classifiers for logographic name matching in public health applications: approaches for incorporating phonetic, visual, and keystroke similarity in large-scale probabilistic record linkage</summary>

- *Philip A. Collender, Zhiyue Tom Hu, Charles Li, Qu Cheng, Xintong Li, Yue You, Song Liang, Changhong Yang, Justin V. Remais*

- `2001.01895v1` - [abs](http://arxiv.org/abs/2001.01895v1) - [pdf](http://arxiv.org/pdf/2001.01895v1)

> Approximate string-matching methods to account for complex variation in highly discriminatory text fields, such as personal names, can enhance probabilistic record linkage. However, discriminating between matching and non-matching strings is challenging for logographic scripts, where similarities in pronunciation, appearance, or keystroke sequence are not directly encoded in the string data. We leverage a large Chinese administrative dataset with known match status to develop logistic regression and Xgboost classifiers integrating measures of visual, phonetic, and keystroke similarity to enhance identification of potentially-matching name pairs. We evaluate three methods of leveraging name similarity scores in large-scale probabilistic record linkage, which can adapt to varying match prevalence and information in supporting fields: (1) setting a threshold score based on predicted quality of name-matching across all record pairs; (2) setting a threshold score based on predicted discriminatory power of the linkage model; and (3) using empirical score distributions among matches and nonmatches to perform Bayesian adjustment of matching probabilities estimated from exact-agreement linkage. In experiments on holdout data, as well as data simulated with varying name error rates and supporting fields, a logistic regression classifier incorporated via the Bayesian method demonstrated marked improvements over exact-agreement linkage with respect to discriminatory power, match probability estimation, and accuracy, reducing the total number of misclassified record pairs by 21% in test data and up to an average of 93% in simulated datasets. Our results demonstrate the value of incorporating visual, phonetic, and keystroke similarity for logographic name matching, as well as the promise of our Bayesian approach to leverage name-matching within large-scale record linkage.

</details>

<details>

<summary>2020-01-07 08:22:07 - Attention-Aware Answers of the Crowd</summary>

- *Jingzheng Tu, Guoxian Yu, Jun Wang, Carlotta Domeniconi, Xiangliang Zhang*

- `1912.11238v2` - [abs](http://arxiv.org/abs/1912.11238v2) - [pdf](http://arxiv.org/pdf/1912.11238v2)

> Crowdsourcing is a relatively economic and efficient solution to collect annotations from the crowd through online platforms. Answers collected from workers with different expertise may be noisy and unreliable, and the quality of annotated data needs to be further maintained. Various solutions have been attempted to obtain high-quality annotations. However, they all assume that workers' label quality is stable over time (always at the same level whenever they conduct the tasks). In practice, workers' attention level changes over time, and the ignorance of which can affect the reliability of the annotations. In this paper, we focus on a novel and realistic crowdsourcing scenario involving attention-aware annotations. We propose a new probabilistic model that takes into account workers' attention to estimate the label quality. Expectation propagation is adopted for efficient Bayesian inference of our model, and a generalized Expectation Maximization algorithm is derived to estimate both the ground truth of all tasks and the label-quality of each individual crowd worker with attention. In addition, the number of tasks best suited for a worker is estimated according to changes in attention. Experiments against related methods on three real-world and one semi-simulated datasets demonstrate that our method quantifies the relationship between workers' attention and label-quality on the given tasks, and improves the aggregated labels.

</details>

<details>

<summary>2020-01-07 10:07:43 - Bayesian calibration and sensitivity analysis of heat transfer models for fire insulation panels</summary>

- *P. -R. Wagner, R. Fahrni, M. Klippel, A. Frangi, B. Sudret*

- `1909.07060v3` - [abs](http://arxiv.org/abs/1909.07060v3) - [pdf](http://arxiv.org/pdf/1909.07060v3)

> A common approach to assess the performance of fire insulation panels is the component additive method (CAM). The parameters of the CAM are based on the temperature-dependent thermal material properties of the panels. These material properties can be derived by calibrating finite element heat transfer models using experimentally measured temperature records. In the past, the calibration of the material properties was done manually by trial and error approaches, which was inefficient and prone to error. In this contribution, the calibration problem is reformulated in a probabilistic setting and solved using the Bayesian model calibration framework. This not only gives a set of best-fit parameters but also confidence bounds on the latter. To make this framework feasible, the procedure is accelerated through the use of advanced surrogate modelling techniques: polynomial chaos expansions combined with principal component analysis. This surrogate modelling technique additionally allows one to conduct a variance-based sensitivity analysis at no additional cost by giving access to the Sobol' indices. The calibration is finally validated by using the calibrated material properties to predict the temperature development in different experimental setups.

</details>

<details>

<summary>2020-01-07 12:02:11 - Classification Algorithm for High Dimensional Protein Markers in Time-course Data</summary>

- *Souvik Banerjee, Gajendra K. Vishwakarma, Atanu Bhattacharjee*

- `1907.12853v2` - [abs](http://arxiv.org/abs/1907.12853v2) - [pdf](http://arxiv.org/pdf/1907.12853v2)

> Identification of biomarkers is an emerging area in Oncology. In this article, we develop an efficient statistical procedure for classification of protein markers according to their effect on cancer progression. A high-dimensional time-course dataset of protein markers for 80 patients motivates us for developing the model. We obtain the optimal threshold values for markers using Cox proportional hazard model. The optimal threshold value is defined as a level of a marker having maximum impact on cancer progression. The classification was validated by comparing random components using both proportional hazard and accelerated failure time frailty models. The study elucidates the application of two separate joint modeling techniques using auto regressive-type model and mixed effect model for time-course data and proportional hazard model for survival data with proper utilization of Bayesian methodology. Also, a prognostic score has been developed on the basis of few selected genes with application on patients. The complete analysis is performed by R programming code. This study facilitates to identify relevant biomarkers from a set of markers.

</details>

<details>

<summary>2020-01-07 12:36:54 - Bayesian Tensor Network with Polynomial Complexity for Probabilistic Machine Learning</summary>

- *Shi-Ju Ran*

- `1912.12923v2` - [abs](http://arxiv.org/abs/1912.12923v2) - [pdf](http://arxiv.org/pdf/1912.12923v2)

> It is known that describing or calculating the conditional probabilities of multiple events is exponentially expensive. In this work, Bayesian tensor network (BTN) is proposed to efficiently capture the conditional probabilities of multiple sets of events with polynomial complexity. BTN is a directed acyclic graphical model that forms a subset of TN. To testify its validity for exponentially many events, BTN is implemented to the image recognition, where the classification is mapped to capturing the conditional probabilities in an exponentially large sample space. Competitive performance is achieved by the BTN with simple tree network structures. Analogous to the tensor network simulations of quantum systems, the validity of the simple-tree BTN implies an ``area law'' of fluctuations in image recognition problems.

</details>

<details>

<summary>2020-01-08 09:02:13 - Bayesian Factor Analysis for Inference on Interactions</summary>

- *Federico Ferrari, David B Dunson*

- `1904.11603v2` - [abs](http://arxiv.org/abs/1904.11603v2) - [pdf](http://arxiv.org/pdf/1904.11603v2)

> This article is motivated by the problem of inference on interactions among chemical exposures impacting human health outcomes. Chemicals often co-occur in the environment or in synthetic mixtures and as a result exposure levels can be highly correlated. We propose a latent factor joint model, which includes shared factors in both the predictor and response components while assuming conditional independence. By including a quadratic regression in the latent variables in the response component, we induce flexible dimension reduction in characterizing main effects and interactions. We propose a Bayesian approach to inference under this Factor analysis for INteractions (FIN) framework. Through appropriate modifications of the factor modeling structure, FIN can accommodate higher order interactions and multivariate outcomes. We provide theory on posterior consistency and the impact of misspecifying the number of factors. We evaluate the performance using a simulation study and data from the National Health and Nutrition Examination Survey (NHANES). Code is available on GitHub.

</details>

<details>

<summary>2020-01-08 14:49:13 - Quantile Graphical Models: Bayesian Approaches</summary>

- *Nilabja Guha, Veera Baladandayuthapani, Bani K. Mallick*

- `1611.02480v3` - [abs](http://arxiv.org/abs/1611.02480v3) - [pdf](http://arxiv.org/pdf/1611.02480v3)

> Graphical models are ubiquitous tools to describe the interdependence between variables measured simultaneously such as large-scale gene or protein expression data. Gaussian graphical models (GGMs) are well-established tools for probabilistic exploration of dependence structures using precision matrices and they are generated under a multivariate normal joint distribution. However, they suffer from several shortcomings since they are based on Gaussian distribution assumptions. In this article, we propose a Bayesian quantile based approach for sparse estimation of graphs. We demonstrate that the resulting graph estimation is robust to outliers and applicable under general distributional assumptions. Furthermore, we develop efficient variational Bayes approximations to scale the methods for large data sets. Our methods are applied to a novel cancer proteomics data dataset wherein multiple proteomic antibodies are simultaneously assessed on tumor samples using reverse-phase protein arrays (RPPA) technology.

</details>

<details>

<summary>2020-01-08 15:09:27 - Bayesian Inversion Of Generative Models For Geologic Storage Of Carbon Dioxide</summary>

- *Gavin H. Graham, Yan Chen*

- `2001.04829v1` - [abs](http://arxiv.org/abs/2001.04829v1) - [pdf](http://arxiv.org/pdf/2001.04829v1)

> Carbon capture and storage (CCS) can aid decarbonization of the atmosphere to limit further global temperature increases. A framework utilizing unsupervised learning is used to generate a range of subsurface geologic volumes to investigate potential sites for long-term storage of carbon dioxide. Generative adversarial networks are used to create geologic volumes, with a further neural network used to sample the posterior distribution of a trained Generator conditional to sparsely sampled physical measurements. These generative models are further conditioned to historic dynamic fluid flow data through Bayesian inversion to improve the resolution of the forecast of the storage capacity of injected carbon dioxide.

</details>

<details>

<summary>2020-01-08 16:26:34 - A causal exposure response function with local adjustment for confounding: Estimating health effects of exposure to low levels of ambient fine particulate matter</summary>

- *Georgia Papadogeorgou, Francesca Dominici*

- `1806.00928v3` - [abs](http://arxiv.org/abs/1806.00928v3) - [pdf](http://arxiv.org/pdf/1806.00928v3)

> The Clean Air Act mandates that the National Ambient Air Quality Standards (NAAQS) must be routinely assessed to protect populations based on the latest science. Therefore, researchers should continue to address whether exposure to levels of air pollution below the NAAQS is harmful to human health. The contentious nature surrounding environmental regulations urges us to cast this question within a causal inference framework. Parametric and semi-parametric regression approaches have been used to estimate the exposure-response (ER) curve between ambient air pollution and health outcomes. Most of these approaches are not formulated within a causal framework, adjust for the same covariates across all levels of exposure, and do not account for model uncertainty. We introduce a Bayesian framework for the estimation of a causal ER curve called LERCA (Local Exposure Response Confounding Adjustment), which allows for different confounders and different strength of confounding at the different exposure levels; and propagates uncertainty regarding confounders' selection and the shape of the ER. LERCA provides a principled way of assessing the covariates' confounding importance at different exposure levels, providing researchers with information regarding the variables to adjust for in regression models. Using simulations, we show that state of the art approaches perform poorly in estimating the ER curve in the presence of local confounding. LERCA is used to evaluate the relationship between exposure to ambient PM2.5 and cardiovascular hospitalizations for 5,362 zip codes in the US, while adjusting for a potentially varying set of confounders across the exposure range. Ambient PM2.5 leads to an increase in cardiovascular hospitalization rates when focusing at the low exposure range. Our results indicate that there is no threshold for the effect of PM2.5 on cardiovascular hospitalizations.

</details>

<details>

<summary>2020-01-08 18:35:26 - Bayesian waveform-based calibration of high-pressure acoustic emission systems with ball drop measurements</summary>

- *Chen Gu, Ulrich Mok, Youssef M. Marzouk, Germán A Prieto Gomez, Farrokh Sheibani, J. Brian Evans, Bradford H. Hager*

- `1906.10098v2` - [abs](http://arxiv.org/abs/1906.10098v2) - [pdf](http://arxiv.org/pdf/1906.10098v2)

> Acoustic emission (AE) is a widely used technology to study source mechanisms and material properties during high-pressure rock failure experiments. It is important to understand the physical quantities that acoustic emission sensors measure, as well as the response of these sensors as a function of frequency. This study calibrates the newly built AE system in the MIT Rock Physics Laboratory using a ball-bouncing system. Full waveforms of multi-bounce events due to ball drops are used to infer the transfer function of lead zirconate titanate (PZT) sensors in high pressure environments. Uncertainty in the sensor transfer functions is quantified using a waveform-based Bayesian approach. The quantification of \textit{in situ} sensor transfer functions makes it possible to apply full waveform analysis for acoustic emissions at high pressures.

</details>

<details>

<summary>2020-01-09 07:32:43 - Global optimization via inverse distance weighting and radial basis functions</summary>

- *Alberto Bemporad*

- `1906.06498v2` - [abs](http://arxiv.org/abs/1906.06498v2) - [pdf](http://arxiv.org/pdf/1906.06498v2)

> Global optimization problems whose objective function is expensive to evaluate can be solved effectively by recursively fitting a surrogate function to function samples and minimizing an acquisition function to generate new samples. The acquisition step trades off between seeking for a new optimization vector where the surrogate is minimum (exploitation of the surrogate) and looking for regions of the feasible space that have not yet been visited and that may potentially contain better values of the objective function (exploration of the feasible space). This paper proposes a new global optimization algorithm that uses a combination of inverse distance weighting (IDW) and radial basis functions (RBF) to construct the acquisition function. Rather arbitrary constraints that are simple to evaluate can be easily taken into account. Compared to Bayesian optimization, the proposed algorithm, that we call GLIS (GLobal minimum using Inverse distance weighting and Surrogate radial basis functions), is competitive and computationally lighter, as we show in a set of benchmark global optimization and hyperparameter tuning problems. MATLAB and Python implementations of GLIS are available at \url{http://cse.lab.imtlucca.it/~bemporad/glis}.

</details>

<details>

<summary>2020-01-09 16:00:16 - disaggregation: An R Package for Bayesian Spatial Disaggregation Modelling</summary>

- *Anita K. Nandi, Tim C. D. Lucas, Rohan Arambepola, Peter Gething, Daniel J. Weiss*

- `2001.04847v1` - [abs](http://arxiv.org/abs/2001.04847v1) - [pdf](http://arxiv.org/pdf/2001.04847v1)

> Disaggregation modelling, or downscaling, has become an important discipline in epidemiology. Surveillance data, aggregated over large regions, is becoming more common, leading to an increasing demand for modelling frameworks that can deal with this data to understand spatial patterns. Disaggregation regression models use response data aggregated over large heterogenous regions to make predictions at fine-scale over the region by using fine-scale covariates to inform the heterogeneity. This paper presents the R package disaggregation, which provides functionality to streamline the process of running a disaggregation model for fine-scale predictions.

</details>

<details>

<summary>2020-01-09 20:19:24 - Noncooperative dynamics in election interference</summary>

- *David Rushing Dewhurst, Christopher M. Danforth, Peter Sheridan Dodds*

- `1908.02793v4` - [abs](http://arxiv.org/abs/1908.02793v4) - [pdf](http://arxiv.org/pdf/1908.02793v4)

> Foreign power interference in domestic elections is an existential threat to societies. Manifested through myriad methods from war to words, such interference is a timely example of strategic interaction between economic and political agents. We model this interaction between rational game players as a continuous-time differential game, constructing an analytical model of this competition with a variety of payoff structures. All-or-nothing attitudes by only one player regarding the outcome of the game lead to an arms race in which both countries spend increasing amounts on interference and counter-interference operations. We then confront our model with data pertaining to the Russian interference in the 2016 United States presidential election contest. We introduce and estimate a Bayesian structural time series model of election polls and social media posts by Russian Twitter troll accounts. Our analytical model, while purposefully abstract and simple, adequately captures many temporal characteristics of the election and social media activity. We close with a discussion of our model's shortcomings and suggestions for future research.

</details>

<details>

<summary>2020-01-10 08:17:24 - Bayesian Recurrent Framework for Missing Data Imputation and Prediction with Clinical Time Series</summary>

- *Yang Guo, Zhengyuan Liu, Pavitra Krishnswamy, Savitha Ramasamy*

- `1911.07572v2` - [abs](http://arxiv.org/abs/1911.07572v2) - [pdf](http://arxiv.org/pdf/1911.07572v2)

> Real-world clinical time series data sets exhibit a high prevalence of missing values. Hence, there is an increasing interest in missing data imputation. Traditional statistical approaches impose constraints on the data-generating process and decouple imputation from prediction. Recent works propose recurrent neural network based approaches for missing data imputation and prediction with time series data. However, they generate deterministic outputs and neglect the inherent uncertainty. In this work, we introduce a unified Bayesian recurrent framework for simultaneous imputation and prediction on time series data sets. We evaluate our approach on two real-world mortality prediction tasks using the MIMIC-III and PhysioNet benchmark datasets. We demonstrate strong performance gains over state-of-the-art (SOTA) methods, and provide strategies to use the resulting probability distributions to better assess reliability of the imputations and predictions.

</details>

<details>

<summary>2020-01-10 22:46:59 - A Bayesian Monte-Carlo Uncertainty Model for Assessment of Shear Stress Entropy</summary>

- *Amin Kazemian-Kale-Kale, Azadeh Gholami, Mohammad Rezaie-Balf, Amir Mosavi, Ahmed A Sattar, Bahram Gharabaghi, Hossein Bonakdari*

- `2001.04802v1` - [abs](http://arxiv.org/abs/2001.04802v1) - [pdf](http://arxiv.org/pdf/2001.04802v1)

> The entropy models have been recently adopted in many studies to evaluate the distribution of the shear stress in circular channels. However, the uncertainty in their predictions and their reliability remains an open question. We present a novel method to evaluate the uncertainty of four popular entropy models, including Shannon, Shannon-Power Low (PL), Tsallis, and Renyi, in shear stress estimation in circular channels. The Bayesian Monte-Carlo (BMC) uncertainty method is simplified considering a 95% Confidence Bound (CB). We developed a new statistic index called as FREEopt-based OCB (FOCB) using the statistical indices Forecasting Range of Error Estimation (FREE) and the percentage of observed data in the CB (Nin), which integrates their combined effect. The Shannon and Shannon PL entropies had close values of the FOCB equal to 8.781 and 9.808, respectively, had the highest certainty in the calculation of shear stress values in circular channels followed by traditional uniform flow shear stress and Tsallis models with close values of 14.491 and 14.895, respectively. However, Renyi entropy with much higher values of FOCB equal to 57.726 has less certainty in the estimation of shear stress than other models. Using the presented results in this study, the amount of confidence in entropy methods in the calculation of shear stress to design and implement different types of open channels and their stability is determined.

</details>

<details>

<summary>2020-01-10 23:19:24 - Calibrate, Emulate, Sample</summary>

- *Emmet Cleary, Alfredo Garbuno-Inigo, Shiwei Lan, Tapio Schneider, Andrew M Stuart*

- `2001.03689v1` - [abs](http://arxiv.org/abs/2001.03689v1) - [pdf](http://arxiv.org/pdf/2001.03689v1)

> Many parameter estimation problems arising in applications are best cast in the framework of Bayesian inversion. This allows not only for an estimate of the parameters, but also for the quantification of uncertainties in the estimates. Often in such problems the parameter-to-data map is very expensive to evaluate, and computing derivatives of the map, or derivative-adjoints, may not be feasible. Additionally, in many applications only noisy evaluations of the map may be available. We propose an approach to Bayesian inversion in such settings that builds on the derivative-free optimization capabilities of ensemble Kalman inversion methods. The overarching approach is to first use ensemble Kalman sampling (EKS) to calibrate the unknown parameters to fit the data; second, to use the output of the EKS to emulate the parameter-to-data map; third, to sample from an approximate Bayesian posterior distribution in which the parameter-to-data map is replaced by its emulator. This results in a principled approach to approximate Bayesian inference that requires only a small number of evaluations of the (possibly noisy approximation of the) parameter-to-data map. It does not require derivatives of this map, but instead leverages the documented power of ensemble Kalman methods. Furthermore, the EKS has the desirable property that it evolves the parameter ensembles towards the regions in which the bulk of the parameter posterior mass is located, thereby locating them well for the emulation phase of the methodology. In essence, the EKS methodology provides a cheap solution to the design problem of where to place points in parameter space to efficiently train an emulator of the parameter-to-data map for the purposes of Bayesian inversion.

</details>

<details>

<summary>2020-01-11 21:31:25 - Bayesian Semi-supervised learning under nonparanormality</summary>

- *Rui Zhu, Subhashis Ghosal*

- `2001.03798v1` - [abs](http://arxiv.org/abs/2001.03798v1) - [pdf](http://arxiv.org/pdf/2001.03798v1)

> Semi-supervised learning is a classification method which makes use of both labeled data and unlabeled data for training. In this paper, we propose a semi-supervised learning algorithm using a Bayesian semi-supervised model. We make a general assumption that the observations will follow two multivariate normal distributions depending on their true labels after the same unknown transformation. We use B-splines to put a prior on the transformation function for each component. To use unlabeled data in a semi-supervised setting, we assume the labels are missing at random. The posterior distributions can then be described using our assumptions, which we compute by the Gibbs sampling technique. The proposed method is then compared with several other available methods through an extensive simulation study. Finally we apply the proposed method in real data contexts for diagnosing breast cancer and classify radar returns. We conclude that the proposed method has better prediction accuracy in a wide variety of cases.

</details>

<details>

<summary>2020-01-12 16:26:30 - On the Consistency of Graph-based Bayesian Learning and the Scalability of Sampling Algorithms</summary>

- *Nicolas Garcia Trillos, Zachary Kaplan, Thabo Samakhoana, Daniel Sanz-Alonso*

- `1710.07702v2` - [abs](http://arxiv.org/abs/1710.07702v2) - [pdf](http://arxiv.org/pdf/1710.07702v2)

> A popular approach to semi-supervised learning proceeds by endowing the input data with a graph structure in order to extract geometric information and incorporate it into a Bayesian framework. We introduce new theory that gives appropriate scalings of graph parameters that provably lead to a well-defined limiting posterior as the size of the unlabeled data set grows. Furthermore, we show that these consistency results have profound algorithmic implications. When consistency holds, carefully designed graph-based Markov chain Monte Carlo algorithms are proved to have a uniform spectral gap, independent of the number of unlabeled inputs. Several numerical experiments corroborate both the statistical consistency and the algorithmic scalability established by the theory.

</details>

<details>

<summary>2020-01-12 20:51:21 - Bayesian Quantile and Expectile Optimisation</summary>

- *Léonard Torossian, Victor Picheny, Nicolas Durrande*

- `2001.04833v1` - [abs](http://arxiv.org/abs/2001.04833v1) - [pdf](http://arxiv.org/pdf/2001.04833v1)

> Bayesian optimisation is widely used to optimise stochastic black box functions. While most strategies are focused on optimising conditional expectations, a large variety of applications require risk-averse decisions and alternative criteria accounting for the distribution tails need to be considered. In this paper, we propose new variational models for Bayesian quantile and expectile regression that are well-suited for heteroscedastic settings. Our models consist of two latent Gaussian processes accounting respectively for the conditional quantile (or expectile) and variance that are chained through asymmetric likelihood functions. Furthermore, we propose two Bayesian optimisation strategies, either derived from a GP-UCB or Thompson sampling, that are tailored to such models and that can accommodate large batches of points. As illustrated in the experimental section, the proposed approach clearly outperforms the state of the art.

</details>

<details>

<summary>2020-01-12 23:30:31 - Generalizing Information to the Evolution of Rational Belief</summary>

- *Jed A. Duersch, Thomas A. Catanach*

- `1911.09559v2` - [abs](http://arxiv.org/abs/1911.09559v2) - [pdf](http://arxiv.org/pdf/1911.09559v2)

> Information theory provides a mathematical foundation to measure uncertainty in belief. Belief is represented by a probability distribution that captures our understanding of an outcome's plausibility. Information measures based on Shannon's concept of entropy include realization information, Kullback-Leibler divergence, Lindley's information in experiment, cross entropy, and mutual information.   We derive a general theory of information from first principles that accounts for evolving belief and recovers all of these measures. Rather than simply gauging uncertainty, information is understood in this theory to measure change in belief. We may then regard entropy as the information we expect to gain upon realization of a discrete latent random variable.   This theory of information is compatible with the Bayesian paradigm in which rational belief is updated as evidence becomes available. Furthermore, this theory admits novel measures of information with well-defined properties, which we explore in both analysis and experiment. This view of information illuminates the study of machine learning by allowing us to quantify information captured by a predictive model and distinguish it from residual information contained in training data. We gain related insights regarding feature selection, anomaly detection, and novel Bayesian approaches.

</details>

<details>

<summary>2020-01-13 06:02:44 - Numerical Sequence Prediction using Bayesian Concept Learning</summary>

- *Mohith Damarapati, Inavamsi B. Enaganti, Alfred Ajay Aureate Rajakumar*

- `2001.04072v1` - [abs](http://arxiv.org/abs/2001.04072v1) - [pdf](http://arxiv.org/pdf/2001.04072v1)

> When people learn mathematical patterns or sequences, they are able to identify the concepts (or rules) underlying those patterns. Having learned the underlying concepts, humans are also able to generalize those concepts to other numbers, so far as to even identify previously unseen combinations of those rules. Current state-of-the art RNN architectures like LSTMs perform well in predicting successive elements of sequential data, but require vast amounts of training examples. Even with extensive data, these models struggle to generalize concepts. From our behavioral study, we also found that humans are able to disregard noise and identify the underlying rules generating the corrupted sequences. We therefore propose a Bayesian model that captures these human-like learning capabilities to predict next number in a given sequence, better than traditional LSTMs.

</details>

<details>

<summary>2020-01-13 13:21:46 - Prediction and Evaluation in College Hockey using the Bradley-Terry-Zermelo Model</summary>

- *John T. Whelan, Adam Wodon*

- `2001.04226v1` - [abs](http://arxiv.org/abs/2001.04226v1) - [pdf](http://arxiv.org/pdf/2001.04226v1)

> We describe the application of the Bradley-Terry model to NCAA Division I Men's Ice Hockey. A Bayesian construction gives a joint posterior probability distribution for the log-strength parameters, given a set of game results and a choice of prior distribution. For several suitable choices of prior, it is straightforward to find the maximum a posteriori point (MAP) and a Hessian matrix, allowing a Gaussian approximation to be constructed. Posterior predictive probabilities can be estimated by 1) setting the log-strengths to their MAP values, 2) using the Gaussian approximation for analytical or Monte Carlo integration, or 3) applying importance sampling to re-weight the results of a Monte Carlo simulation. We define a method to evaluate any models which generate predicted probabilities for future outcomes, using the Bayes factor given the actual outcomes, and apply it to NCAA tournament results. Finally, we describe an on-line tool which currently estimates probabilities of future results using MAP evaluation and describe how it can be refined using the Gaussian approximation or importance sampling.

</details>

<details>

<summary>2020-01-13 17:58:25 - Unit Level Modeling of Survey Data for Small Area Estimation Under Informative Sampling: A Comprehensive Overview with Extensions</summary>

- *Paul A. Parker, Ryan Janicki, Scott H. Holan*

- `1908.10488v2` - [abs](http://arxiv.org/abs/1908.10488v2) - [pdf](http://arxiv.org/pdf/1908.10488v2)

> Model-based small area estimation is frequently used in conjunction with survey data in order to establish estimates for under-sampled or unsampled geographies. These models can be specified at either the area-level, or the unit-level, but unit-level models often offer potential advantages such as more precise estimates and easy spatial aggregation. Nevertheless, relative to area-level models, literature on unit-level models is less prevalent. In modeling small areas at the unit level, challenges often arise as a consequence of the informative sampling mechanism used to collect the survey data. This paper provides a comprehensive methodological review for unit-level models under informative sampling, with an emphasis on Bayesian approaches. To provide insight into the differences between methods, we conduct a simulation study that compares several of the described approaches. In addition, the methods used for simulation are further illustrated through an application to the American Community Survey. Finally, we present several extensions and areas for future research.

</details>

<details>

<summary>2020-01-13 18:11:27 - Quantification of the weight of fingerprint evidence using a ROC-based Approximate Bayesian Computation algorithm for model selection</summary>

- *J. H. Hendricks, C. Neumann, C. P. Saunders*

- `1803.10121v3` - [abs](http://arxiv.org/abs/1803.10121v3) - [pdf](http://arxiv.org/pdf/1803.10121v3)

> For more than a century, fingerprints have been used with considerable success to identify criminals or verify the identity of individuals. The categorical conclusion scheme used by fingerprint examiners, and more generally the inference process followed by forensic scientists, have been heavily criticised in the scientific and legal literature. Instead, scholars have proposed to characterise the weight of forensic evidence using the Bayes factor as the key element of the inference process. In forensic science, quantifying the magnitude of support is equally as important as determining which model is supported. Unfortunately, the complexity of fingerprint patterns render likelihood-based inference impossible. In this paper, we use an Approximate Bayesian Computation model selection algorithm to quantify the weight of fingerprint evidence. We supplement the ABC algorithm using a Receiver Operating Characteristic curve to mitigate the effect of the curse of dimensionality. Our modified algorithm is computationally efficient and makes it easier to monitor convergence as the number of simulations increase. We use our method to quantify the weight of fingerprint evidence in forensic science, but we note that it can be applied to any other forensic pattern evidence.

</details>

<details>

<summary>2020-01-14 06:21:51 - Robust Gaussian Process Regression with a Bias Model</summary>

- *Chiwoo Park, David J. Borth, Nicholas S. Wilson, Chad N. Hunter, Fritz J. Friedersdorf*

- `2001.04639v1` - [abs](http://arxiv.org/abs/2001.04639v1) - [pdf](http://arxiv.org/pdf/2001.04639v1)

> This paper presents a new approach to a robust Gaussian process (GP) regression. Most existing approaches replace an outlier-prone Gaussian likelihood with a non-Gaussian likelihood induced from a heavy tail distribution, such as the Laplace distribution and Student-t distribution. However, the use of a non-Gaussian likelihood would incur the need for a computationally expensive Bayesian approximate computation in the posterior inferences. The proposed approach models an outlier as a noisy and biased observation of an unknown regression function, and accordingly, the likelihood contains bias terms to explain the degree of deviations from the regression function. We entail how the biases can be estimated accurately with other hyperparameters by a regularized maximum likelihood estimation. Conditioned on the bias estimates, the robust GP regression can be reduced to a standard GP regression problem with analytical forms of the predictive mean and variance estimates. Therefore, the proposed approach is simple and very computationally attractive. It also gives a very robust and accurate GP estimate for many tested scenarios. For the numerical evaluation, we perform a comprehensive simulation study to evaluate the proposed approach with the comparison to the existing robust GP approaches under various simulated scenarios of different outlier proportions and different noise levels. The approach is applied to data from two measurement systems, where the predictors are based on robust environmental parameter measurements and the response variables utilize more complex chemical sensing methods that contain a certain percentage of outliers. The utility of the measurement systems and value of the environmental data are improved through the computationally efficient GP regression and bias model.

</details>

<details>

<summary>2020-01-14 12:54:34 - Asymptotic Performance Analysis of Non-Bayesian Quickest Change Detection with an Energy Harvesting Sensor</summary>

- *Subhrakanti Dey*

- `2001.04752v1` - [abs](http://arxiv.org/abs/2001.04752v1) - [pdf](http://arxiv.org/pdf/2001.04752v1)

> In this paper, we consider a non-Bayesian sequential change detection based on the Cumulative Sum (CUSUM) algorithm employed by an energy harvesting sensor where the distributions before and after the change are assumed to be known. In a slotted discrete-time model, the sensor, exclusively powered by randomly available harvested energy, obtains a sample and computes the log-likelihood ratio of the two distributions if it has enough energy to sense and process a sample. If it does not have enough energy in a given slot, it waits until it harvests enough energy to perform the task in a future time slot. We derive asymptotic expressions for the expected detection delay (when a change actually occurs), and the asymptotic tail distribution of the run-length to a false alarm (when a change never happens). We show that when the average harvested energy ($\bar H$) is greater than or equal to the energy required to sense and process a sample ($E_s$), standard existing asymptotic results for the CUSUM test apply since the energy storage level at the sensor is greater than $E_s$ after a sufficiently long time. However, when the $\bar H < E_s$, the energy storage level can be modelled by a positive Harris recurrent Markov chain with a unique stationary distribution. Using asymptotic results from Markov random walk theory and associated nonlinear Markov renewal theory, we establish asymptotic expressions for the expected detection delay and asymptotic exponentiality of the tail distribution of the run-length to a false alarm in this non-trivial case. Numerical results are provided to support the theoretical results.

</details>

<details>

<summary>2020-01-14 14:49:02 - Variational Bayesian Optimal Experimental Design</summary>

- *Adam Foster, Martin Jankowiak, Eli Bingham, Paul Horsfall, Yee Whye Teh, Tom Rainforth, Noah Goodman*

- `1903.05480v3` - [abs](http://arxiv.org/abs/1903.05480v3) - [pdf](http://arxiv.org/pdf/1903.05480v3)

> Bayesian optimal experimental design (BOED) is a principled framework for making efficient use of limited experimental resources. Unfortunately, its applicability is hampered by the difficulty of obtaining accurate estimates of the expected information gain (EIG) of an experiment. To address this, we introduce several classes of fast EIG estimators by building on ideas from amortized variational inference. We show theoretically and empirically that these estimators can provide significant gains in speed and accuracy over previous approaches. We further demonstrate the practicality of our approach on a number of end-to-end experiments.

</details>

<details>

<summary>2020-01-14 17:22:02 - Analysis of Bayesian Inference Algorithms by the Dynamical Functional Approach</summary>

- *Burak Çakmak, Manfred Opper*

- `2001.04918v1` - [abs](http://arxiv.org/abs/2001.04918v1) - [pdf](http://arxiv.org/pdf/2001.04918v1)

> We analyze the dynamics of an algorithm for approximate inference with large Gaussian latent variable models in a student-teacher scenario. To model nontrivial dependencies between the latent variables, we assume random covariance matrices drawn from rotation invariant ensembles. For the case of perfect data-model matching, the knowledge of static order parameters derived from the replica method allows us to obtain efficient algorithmic updates in terms of matrix-vector multiplications with a fixed matrix. Using the dynamical functional approach, we obtain an exact effective stochastic process in the thermodynamic limit for a single node. From this, we obtain closed-form expressions for the rate of the convergence. Analytical results are excellent agreement with simulations of single instances of large models.

</details>

<details>

<summary>2020-01-14 19:40:00 - Ultra High-dimensional Multivariate Posterior Contraction Rate Under Shrinkage Priors</summary>

- *Ruoyang Zhang, Malay Ghosh*

- `1904.04417v2` - [abs](http://arxiv.org/abs/1904.04417v2) - [pdf](http://arxiv.org/pdf/1904.04417v2)

> In recent years, shrinkage priors have received much attention in high-dimensional data analysis from a Bayesian perspective. Compared with widely used spike-and-slab priors, shrinkage priors have better computational efficiency. But the theoretical properties, especially posterior contraction rate, which is important in uncertainty quantification, are not established in many cases. In this paper, we apply global-local shrinkage priors to high-dimensional multivariate linear regression with unknown covariance matrix. We show that when the prior is highly concentrated near zero and has heavy tail, the posterior contraction rates for both coefficients matrix and covariance matrix are nearly optimal. Our results hold when number of features p grows much faster than the sample size n, which is of great interest in modern data analysis. We show that a class of readily implementable scale mixture of normal priors satisfies the conditions of the main theorem.

</details>

<details>

<summary>2020-01-14 23:32:01 - A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning</summary>

- *Soochan Lee, Junsoo Ha, Dongsu Zhang, Gunhee Kim*

- `2001.00689v2` - [abs](http://arxiv.org/abs/2001.00689v2) - [pdf](http://arxiv.org/pdf/2001.00689v2)

> Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and image generation.

</details>

<details>

<summary>2020-01-15 04:10:53 - A deep-learning based Bayesian approach to seismic imaging and uncertainty quantification</summary>

- *Ali Siahkoohi, Gabrio Rizzuti, Felix J. Herrmann*

- `2001.04567v2` - [abs](http://arxiv.org/abs/2001.04567v2) - [pdf](http://arxiv.org/pdf/2001.04567v2)

> Uncertainty quantification is essential when dealing with ill-conditioned inverse problems due to the inherent nonuniqueness of the solution. Bayesian approaches allow us to determine how likely an estimation of the unknown parameters is via formulating the posterior distribution. Unfortunately, it is often not possible to formulate a prior distribution that precisely encodes our prior knowledge about the unknown. Furthermore, adherence to handcrafted priors may greatly bias the outcome of the Bayesian analysis. To address this issue, we propose to use the functional form of a randomly initialized convolutional neural network as an implicit structured prior, which is shown to promote natural images and excludes images with unnatural noise. In order to incorporate the model uncertainty into the final estimate, we sample the posterior distribution using stochastic gradient Langevin dynamics and perform Bayesian model averaging on the obtained samples. Our synthetic numerical experiment verifies that deep priors combined with Bayesian model averaging are able to partially circumvent imaging artifacts and reduce the risk of overfitting in the presence of extreme noise. Finally, we present pointwise variance of the estimates as a measure of uncertainty, which coincides with regions that are more difficult to image.

</details>

<details>

<summary>2020-01-15 11:19:47 - Dis-entangling Mixture of Interventions on a Causal Bayesian Network Using Aggregate Observations</summary>

- *Gaurav Sinha, Ayush Chauhan, Aurghya Maiti, Naman Poddar, Pulkit Goel*

- `1912.00163v2` - [abs](http://arxiv.org/abs/1912.00163v2) - [pdf](http://arxiv.org/pdf/1912.00163v2)

> We study the problem of separating a mixture of distributions, all of which come from interventions on a known causal bayesian network. Given oracle access to marginals of all distributions resulting from interventions on the network, and estimates of marginals from the mixture distribution, we want to recover the mixing proportions of different mixture components.   We show that in the worst case, mixing proportions cannot be identified using marginals only. If exact marginals of the mixture distribution were known, under a simple assumption of excluding a few distributions from the mixture, we show that the mixing proportions become identifiable. Our identifiability proof is constructive and gives an efficient algorithm recovering the mixing proportions exactly. When exact marginals are not available, we design an optimization framework to estimate the mixing proportions.   Our problem is motivated from a real-world scenario of an e-commerce business, where multiple interventions occur at a given time, leading to deviations in expected metrics. We conduct experiments on the well known publicly available ALARM network and on a proprietary dataset from a large e-commerce company validating the performance of our method.

</details>

<details>

<summary>2020-01-15 15:45:28 - Bayesian Modeling of Multiple Structural Connectivity Networks During the Progression of Alzheimer's Disease</summary>

- *Christine B. Peterson, Nathan Osborne, Francesco C. Stingo, Pierrick Bourgeat, James D. Doecke, Marina Vannucci*

- `1908.10965v2` - [abs](http://arxiv.org/abs/1908.10965v2) - [pdf](http://arxiv.org/pdf/1908.10965v2)

> Alzheimer's disease is the most common neurodegenerative disease. The aim of this study is to infer structural changes in brain connectivity resulting from disease progression using cortical thickness measurements from a cohort of participants who were either healthy control, or with mild cognitive impairment, or Alzheimer's disease patients. For this purpose, we develop a novel approach for inference of multiple networks with related edge values across groups. Specifically, we infer a Gaussian graphical model for each group within a joint framework, where we rely on Bayesian hierarchical priors to link the precision matrix entries across groups. Our proposal differs from existing approaches in that it flexibly learns which groups have the most similar edge values, and accounts for the strength of connection (rather than only edge presence or absence) when sharing information across groups. Our results identify key alterations in structural connectivity which may reflect disruptions to the healthy brain, such as decreased connectivity within the occipital lobe with increasing disease severity. We also illustrate the proposed method through simulations, where we demonstrate its performance in structure learning and precision matrix estimation with respect to alternative approaches.

</details>

<details>

<summary>2020-01-15 16:15:05 - Can we disregard the whole model? Omnibus non-inferiority testing for $R^{2}$ in multivariable linear regression and $\hatη^{2}$ in ANOVA</summary>

- *Harlan Campbell, Daniël Lakens*

- `1905.11875v2` - [abs](http://arxiv.org/abs/1905.11875v2) - [pdf](http://arxiv.org/pdf/1905.11875v2)

> Determining a lack of association between an outcome variable and a number of different explanatory variables is frequently necessary in order to disregard a proposed model (i.e., to confirm the lack of an association between an outcome and predictors). Despite this, the literature rarely offers information about, or technical recommendations concerning, the appropriate statistical methodology to be used to accomplish this task. This paper introduces non-inferiority tests for ANOVA and linear regression analyses, that correspond to the standard widely used $F$-test for $\hat{\eta}^2$ and $R^{2}$, respectively. A simulation study is conducted to examine the type I error rates and statistical power of the tests, and a comparison is made with an alternative Bayesian testing approach. The results indicate that the proposed non-inferiority test is a potentially useful tool for 'testing the null.'

</details>

<details>

<summary>2020-01-15 16:21:48 - Automated extraction of mutual independence patterns using Bayesian comparison of partition models</summary>

- *Guillaume Marrelec, Alain Giron*

- `2001.05407v1` - [abs](http://arxiv.org/abs/2001.05407v1) - [pdf](http://arxiv.org/pdf/2001.05407v1)

> Mutual independence is a key concept in statistics that characterizes the structural relationships between variables. Existing methods to investigate mutual independence rely on the definition of two competing models, one being nested into the other and used to generate a null distribution for a statistic of interest, usually under the asymptotic assumption of large sample size. As such, these methods have a very restricted scope of application. In the present manuscript, we propose to change the investigation of mutual independence from a hypothesis-driven task that can only be applied in very specific cases to a blind and automated search within patterns of mutual independence. To this end, we treat the issue as one of model comparison that we solve in a Bayesian framework. We show the relationship between such an approach and existing methods in the case of multivariate normal distributions as well as cross-classified multinomial distributions. We propose a general Markov chain Monte Carlo (MCMC) algorithm to numerically approximate the posterior distribution on the space of all patterns of mutual independence. The relevance of the method is demonstrated on synthetic data as well as two real datasets, showing the unique insight provided by this approach.

</details>

<details>

<summary>2020-01-15 18:23:00 - A Fast and Greedy Subset-of-Data (SoD) Scheme for Sparsification in Gaussian processes</summary>

- *Vidhi Lalchand, A. C. Faul*

- `1811.07199v2` - [abs](http://arxiv.org/abs/1811.07199v2) - [pdf](http://arxiv.org/pdf/1811.07199v2)

> In their standard form Gaussian processes (GPs) provide a powerful non-parametric framework for regression and classificaton tasks. Their one limiting property is their $\mathcal{O}(N^{3})$ scaling where $N$ is the number of training data points. In this paper we present a framework for GP training with sequential selection of training data points using an intuitive selection metric. The greedy forward selection strategy is devised to target two factors - regions of high predictive uncertainty and underfit. Under this technique the complexity of GP training is reduced to $\mathcal{O}(M^{3})$ where $(M \ll N)$ if $M$ data points (out of $N$) are eventually selected. The sequential nature of the algorithm circumvents the need to invert the covariance matrix of dimension $N \times N$ and enables the use of favourable matrix inverse update identities. We outline the algorithm and sequential updates to the posterior mean and variance. We demonstrate our method on selected one dimensional functions and show that the loss in accuracy due to using a subset of data points is marginal compared to the computational gains.

</details>

<details>

<summary>2020-01-16 01:05:46 - Sequential Selection for Accelerated Life Testing via Approximate Bayesian Inference</summary>

- *Ye Chen, Qiong Zhang, Mingyang Li, Wenjun Cai*

- `2001.05602v1` - [abs](http://arxiv.org/abs/2001.05602v1) - [pdf](http://arxiv.org/pdf/2001.05602v1)

> Accelerated life testing (ALT) is typically used to assess the reliability of material's lifetime under desired stress levels. Recent advances in material engineering have made a variety of material alternatives readily available. To identify the most reliable material setting with efficient experimental design, a sequential test planning strategy is preferred. To guarantee a tractable statistical mechanism for information collection and update, we develop explicit model parameter update formulas via approximate Bayesian inference. Theories show that our explicit update formulas give consistent parameter estimates. Simulation study and a case study show that the proposed sequential selection approach can significantly improve the probability of identifying the material alternative with best reliability performance over other design approaches.

</details>

<details>

<summary>2020-01-16 02:11:21 - Bayesian Symbolic Regression</summary>

- *Ying Jin, Weilin Fu, Jian Kang, Jiadong Guo, Jian Guo*

- `1910.08892v3` - [abs](http://arxiv.org/abs/1910.08892v3) - [pdf](http://arxiv.org/pdf/1910.08892v3)

> Interpretability is crucial for machine learning in many scenarios such as quantitative finance, banking, healthcare, etc. Symbolic regression (SR) is a classic interpretable machine learning method by bridging X and Y using mathematical expressions composed of some basic functions. However, the search space of all possible expressions grows exponentially with the length of the expression, making it infeasible for enumeration. Genetic programming (GP) has been traditionally and commonly used in SR to search for the optimal solution, but it suffers from several limitations, e.g. the difficulty in incorporating prior knowledge; overly-complicated output expression and reduced interpretability etc. To address these issues, we propose a new method to fit SR under a Bayesian framework. Firstly, Bayesian model can naturally incorporate prior knowledge (e.g., preference of basis functions, operators and raw features) to improve the efficiency of fitting SR. Secondly, to improve interpretability of expressions in SR, we aim to capture concise but informative signals. To this end, we assume the expected signal has an additive structure, i.e., a linear combination of several concise expressions, whose complexity is controlled by a well-designed prior distribution. In our setup, each expression is characterized by a symbolic tree, and the proposed SR model could be solved by sampling symbolic trees from the posterior distribution using an efficient Markov chain Monte Carlo (MCMC) algorithm. Finally, compared with GP, the proposed BSR(Bayesian Symbolic Regression) method saves computer memory with no need to keep an updated 'genome pool'. Numerical experiments show that, compared with GP, the solutions of BSR are closer to the ground truth and the expressions are more concise. Meanwhile we find the solution of BSR is robust to hyper-parameter specifications such as the number of trees.

</details>

<details>

<summary>2020-01-16 06:07:25 - Variational Bayesian Inference for Mixed Logit Models with Unobserved Inter- and Intra-Individual Heterogeneity</summary>

- *Rico Krueger, Prateek Bansal, Michel Bierlaire, Ricardo A. Daziano, Taha H. Rashidi*

- `1905.00419v3` - [abs](http://arxiv.org/abs/1905.00419v3) - [pdf](http://arxiv.org/pdf/1905.00419v3)

> Variational Bayes (VB), a method originating from machine learning, enables fast and scalable estimation of complex probabilistic models. Thus far, applications of VB in discrete choice analysis have been limited to mixed logit models with unobserved inter-individual taste heterogeneity. However, such a model formulation may be too restrictive in panel data settings, since tastes may vary both between individuals as well as across choice tasks encountered by the same individual. In this paper, we derive a VB method for posterior inference in mixed logit models with unobserved inter- and intra-individual heterogeneity. In a simulation study, we benchmark the performance of the proposed VB method against maximum simulated likelihood (MSL) and Markov chain Monte Carlo (MCMC) methods in terms of parameter recovery, predictive accuracy and computational efficiency. The simulation study shows that VB can be a fast, scalable and accurate alternative to MSL and MCMC estimation, especially in applications in which fast predictions are paramount. VB is observed to be between 2.8 and 17.7 times faster than the two competing methods, while affording comparable or superior accuracy. Besides, the simulation study demonstrates that a parallelised implementation of the MSL estimator with analytical gradients is a viable alternative to MCMC in terms of both estimation accuracy and computational efficiency, as the MSL estimator is observed to be between 0.9 and 2.1 times faster than MCMC.

</details>

<details>

<summary>2020-01-16 10:15:55 - Scalable Hyperparameter Optimization with Lazy Gaussian Processes</summary>

- *Raju Ram, Sabine Müller, Franz-Josef Pfreundt, Nicolas R. Gauger, Janis Keuper*

- `2001.05726v1` - [abs](http://arxiv.org/abs/2001.05726v1) - [pdf](http://arxiv.org/pdf/2001.05726v1)

> Most machine learning methods require careful selection of hyper-parameters in order to train a high performing model with good generalization abilities. Hence, several automatic selection algorithms have been introduced to overcome tedious manual (try and error) tuning of these parameters. Due to its very high sample efficiency, Bayesian Optimization over a Gaussian Processes modeling of the parameter space has become the method of choice. Unfortunately, this approach suffers from a cubic compute complexity due to underlying Cholesky factorization, which makes it very hard to be scaled beyond a small number of sampling steps. In this paper, we present a novel, highly accurate approximation of the underlying Gaussian Process. Reducing its computational complexity from cubic to quadratic allows an efficient strong scaling of Bayesian Optimization while outperforming the previous approach regarding optimization accuracy. The first experiments show speedups of a factor of 162 in single node and further speed up by a factor of 5 in a parallel environment.

</details>

<details>

<summary>2020-01-16 10:21:45 - Multiscale stick-breaking mixture models</summary>

- *Marco Stefanucci, Antonio Canale*

- `2001.05729v1` - [abs](http://arxiv.org/abs/2001.05729v1) - [pdf](http://arxiv.org/pdf/2001.05729v1)

> We introduce a family of multiscale stick-breaking mixture models for Bayesian nonparametric density estimation. The Bayesian nonparametric literature is dominated by single scale methods, exception made for P\`olya trees and allied approaches. Our proposal is based on a mixture specification exploiting an infinitely-deep binary tree of random weights that grows according to a multiscale generalization of a large class of stick-breaking processes; this multiscale stick-breaking is paired with specific stochastic processes generating sequences of parameters that induce stochastically ordered kernel functions. Properties of this family of multiscale stick-breaking mixtures are described. Focusing on a Gaussian specification, a Markov Chain Montecarlo algorithm for posterior computation is introduced. The performance of the method is illustrated analyzing both synthetic and real data sets. The method is well-suited for data living in $\mathbb{R}$ and is able to detect densities with varying degree of smoothness and local features.

</details>

<details>

<summary>2020-01-16 15:35:06 - Masking schemes for universal marginalisers</summary>

- *Divya Gautam, Maria Lomeli, Kostis Gourgoulias, Daniel H. Thompson, Saurabh Johri*

- `2001.05895v1` - [abs](http://arxiv.org/abs/2001.05895v1) - [pdf](http://arxiv.org/pdf/2001.05895v1)

> We consider the effect of structure-agnostic and structure-dependent masking schemes when training a universal marginaliser (arXiv:1711.00695) in order to learn conditional distributions of the form $P(x_i |\mathbf x_{\mathbf b})$, where $x_i$ is a given random variable and $\mathbf x_{\mathbf b}$ is some arbitrary subset of all random variables of the generative model of interest. In other words, we mimic the self-supervised training of a denoising autoencoder, where a dataset of unlabelled data is used as partially observed input and the neural approximator is optimised to minimise reconstruction loss. We focus on studying the underlying process of the partially observed data---how good is the neural approximator at learning all conditional distributions when the observation process at prediction time differs from the masking process during training? We compare networks trained with different masking schemes in terms of their predictive performance and generalisation properties.

</details>

<details>

<summary>2020-01-16 16:36:53 - Neutron drip line in the Ca region from Bayesian model averaging</summary>

- *Léo Neufcourt, Yuchen Cao, Witold Nazarewicz, Erik Olsen, Frederi Viens*

- `1901.07632v2` - [abs](http://arxiv.org/abs/1901.07632v2) - [pdf](http://arxiv.org/pdf/1901.07632v2)

> The region of heavy calcium isotopes forms the frontier of experimental and theoretical nuclear structure research where the basic concepts of nuclear physics are put to stringent test. The recent discovery of the extremely neutron-rich nuclei around $^{60}$Ca [Tarasov, 2018] and the experimental determination of masses for $^{55-57}$Ca (Michimasa, 2018] provide unique information about the binding energy surface in this region. To assess the impact of these experimental discoveries on the nuclear landscape's extent, we use global mass models and statistical machine learning to make predictions, with quantified levels of certainty, for bound nuclides between Si and Ti. Using a Bayesian model averaging analysis based on Gaussian-process-based extrapolations we introduce the posterior probability $p_{ex}$ for each nucleus to be bound to neutron emission. We find that extrapolations for drip-line locations, at which the nuclear binding ends, are consistent across the global mass models used, in spite of significant variations between their raw predictions. In particular, considering the current experimental information and current global mass models, we predict that $^{68}$Ca has an average posterior probability ${p_{ex}\approx76}$% to be bound to two-neutron emission while the nucleus $^{61}$Ca is likely to decay by emitting a neutron (${p_{ex}\approx 46}$ %).

</details>

<details>

<summary>2020-01-16 16:47:57 - Beyond the proton drip line: Bayesian analysis of proton-emitting nuclei</summary>

- *Léo Neufcourt, Yuchen Cao, Samuel Giuliani, Witold Nazarewicz, Erik Olsen, Oleg B. Tarasov*

- `1910.12624v2` - [abs](http://arxiv.org/abs/1910.12624v2) - [pdf](http://arxiv.org/pdf/1910.12624v2)

> The limits of the nuclear landscape are determined by nuclear binding energies. Beyond the proton drip lines, where the separation energy becomes negative, there is not enough binding energy to prevent protons from escaping the nucleus. Predicting properties of unstable nuclear states in the vast territory of proton emitters poses an appreciable challenge for nuclear theory as it often involves far extrapolations. In addition, significant discrepancies between nuclear models in the proton-rich territory call for quantified predictions. With the help of Bayesian methodology, we mix a family of nuclear mass models corrected with statistical emulators trained on the experimental mass measurements, in the proton-rich region of the nuclear chart. Separation energies were computed within nuclear density functional theory using several Skyrme and Gogny energy density functionals. We also considered mass predictions based on two models used in astrophysical studies. Quantified predictions were obtained for each model using Bayesian Gaussian processes trained on separation-energy residuals and combined via Bayesian model averaging. We obtained a good agreement between averaged predictions of statistically corrected models and experiment. In particular, we quantified model results for one- and two-proton separation energies and derived probabilities of proton emission. This information enabled us to produce a quantified landscape of proton-rich nuclei. The most promising candidates for two-proton decay studies have been identified. The methodology used in this work has broad applications to model-based extrapolations of various nuclear observables. It also provides a reliable uncertainty quantification of theoretical predictions.

</details>

<details>

<summary>2020-01-17 01:00:18 - Approximate Bayesian Bootstrap Procedures to Estimate Multilevel Treatment Effects in Observational Studies with Application to Type 2 Diabetes Treatment Regimens</summary>

- *Anthony D. Scotina, Andrew R. Zullo, Robert J. Smith, Roee Gutman*

- `2001.06125v1` - [abs](http://arxiv.org/abs/2001.06125v1) - [pdf](http://arxiv.org/pdf/2001.06125v1)

> Randomized clinical trials are considered the gold standard for estimating causal effects. Nevertheless, in studies that are aimed at examining adverse effects of interventions, such trials are often impractical because of ethical and financial considerations. In observational studies, matching on the generalized propensity scores was proposed as a possible solution to estimate the treatment effects of multiple interventions. However, the derivation of point and interval estimates for these matching procedures can become complex with non-continuous or censored outcomes. We propose a novel Approximate Bayesian Bootstrap algorithm that result in statistically valid point and interval estimates of the treatment effects with categorical outcomes. The procedure relies on the estimated generalized propensity scores and multiply imputes the unobserved potential outcomes for each unit. In addition, we describe a corresponding interpretable sensitivity analysis to examine the unconfoundedness assumption. We apply this approach to examines the cardiovascular safety of common, real-world anti-diabetic treatment regimens for Type 2 diabetes mellitus in a large observational database.

</details>

<details>

<summary>2020-01-17 03:49:31 - Estimation of Causal Effects of Multiple Treatments in Observational Studies with a Binary Outcome</summary>

- *Liangyuan Hu, Chenyang Gu, Michael Lopez, Jiayi Ji, Juan Wisnivesky*

- `2001.06483v1` - [abs](http://arxiv.org/abs/2001.06483v1) - [pdf](http://arxiv.org/pdf/2001.06483v1)

> There is a dearth of robust methods to estimate the causal effects of multiple treatments when the outcome is binary. This paper uses two unique sets of simulations to propose and evaluate the use of Bayesian Additive Regression Trees (BART) in such settings. First, we compare BART to several approaches that have been proposed for continuous outcomes, including inverse probability of treatment weighting (IPTW), targeted maximum likelihood estimator (TMLE), vector matching and regression adjustment. Results suggest that under conditions of non-linearity and non-additivity of both the treatment assignment and outcome generating mechanisms, BART, TMLE and IPTW using generalized boosted models (GBM) provide better bias reduction and smaller root mean squared error. BART and TMLE provide more consistent 95 per cent CI coverage and better large-sample convergence property. Second, we supply BART with a strategy to identify a common support region for retaining inferential units and for avoiding extrapolating over areas of the covariate space where common support does not exist. BART retains more inferential units than the generalized propensity score based strategy, and shows lower bias, compared to TMLE or GBM, in a variety of scenarios differing by the degree of covariate overlap. A case study examining the effects of three surgical approaches for non-small cell lung cancer demonstrates the methods.

</details>

<details>

<summary>2020-01-17 10:38:05 - Input complexity and out-of-distribution detection with likelihood-based generative models</summary>

- *Joan Serrà, David Álvarez, Vicenç Gómez, Olga Slizovskaia, José F. Núñez, Jordi Luque*

- `1909.11480v3` - [abs](http://arxiv.org/abs/1909.11480v3) - [pdf](http://arxiv.org/pdf/1909.11480v3)

> Likelihood-based generative models are a promising resource to detect out-of-distribution (OOD) inputs which could compromise the robustness or reliability of a machine learning system. However, likelihoods derived from such models have been shown to be problematic for detecting certain types of inputs that significantly differ from training data. In this paper, we pose that this problem is due to the excessive influence that input complexity has in generative models' likelihoods. We report a set of experiments supporting this hypothesis, and use an estimate of input complexity to derive an efficient and parameter-free OOD score, which can be seen as a likelihood-ratio, akin to Bayesian model comparison. We find such score to perform comparably to, or even better than, existing OOD detection approaches under a wide range of data sets, models, model sizes, and complexity estimates.

</details>

<details>

<summary>2020-01-17 11:48:52 - Markov Chain Monte Carlo Methods, a survey with some frequent misunderstandings</summary>

- *Christian P. Robert, Wu Changye*

- `2001.06249v1` - [abs](http://arxiv.org/abs/2001.06249v1) - [pdf](http://arxiv.org/pdf/2001.06249v1)

> In this chapter, we review some of the most standard MCMC tools used in Bayesian computation, along with vignettes on standard misunderstandings of these approaches taken from Q \&~A's on the forum Cross-validated answered by the first author.

</details>

<details>

<summary>2020-01-17 16:07:50 - Bernstein-von Mises theorems and uncertainty quantification for linear inverse problems</summary>

- *Matteo Giordano, Hanne Kekkonen*

- `1811.04058v4` - [abs](http://arxiv.org/abs/1811.04058v4) - [pdf](http://arxiv.org/pdf/1811.04058v4)

> We consider the statistical inverse problem of recovering an unknown function $f$ from a linear measurement corrupted by additive Gaussian white noise. We employ a nonparametric Bayesian approach with standard Gaussian priors, for which the posterior-based reconstruction of $f$ corresponds to a Tikhonov regulariser $\bar f$ with a reproducing kernel Hilbert space norm penalty. We prove a semiparametric Bernstein-von Mises theorem for a large collection of linear functionals of $f$, implying that semiparametric posterior estimation and uncertainty quantification are valid and optimal from a frequentist point of view. The result is applied to study three concrete examples that cover both the mildly and severely ill-posed cases: specifically, an elliptic inverse problem, an elliptic boundary value problem and the heat equation. For the elliptic boundary value problem, we also obtain a nonparametric version of the theorem that entails the convergence of the posterior distribution to a prior-independent infinite-dimensional Gaussian probability measure with minimal covariance. As a consequence, it follows that the Tikhonov regulariser $\bar f$ is an efficient estimator of $f$, and we derive frequentist guarantees for certain credible balls centred at $\bar{f}$.

</details>

<details>

<summary>2020-01-17 18:55:06 - Bayesian Inference for Big Spatial Data Using Non-stationary Spectral Simulation</summary>

- *Hou-Cheng Yang, Jonathan R. Bradley*

- `2001.06477v1` - [abs](http://arxiv.org/abs/2001.06477v1) - [pdf](http://arxiv.org/pdf/2001.06477v1)

> It is increasingly understood that the assumption of stationarity is unrealistic for many spatial processes. In this article, we combine dimension expansion with a spectral method to model big non-stationary spatial fields in a computationally efficient manner. Specifically, we use Mejia and Rodriguez-Iturbe (1974)'s spectral simulation approach to simulate a spatial process with a covariogram at locations that have an expanded dimension. We introduce Bayesian hierarchical modelling to dimension expansion, which originally has only been modeled using a method of moments approach. In particular, we simulate from the posterior distribution using a collapsed Gibbs sampler. Our method is both full rank and non-stationary, and can be applied to big spatial data because it does not involve storing and inverting large covariance matrices. Additionally, we have fewer parameters than many other non-stationary spatial models. We demonstrate the wide applicability of our approach using a simulation study, and an application using ozone data obtained from the National Aeronautics and Space Administration (NASA).

</details>

<details>

<summary>2020-01-17 22:02:23 - Fragmentation Coagulation Based Mixed Membership Stochastic Blockmodel</summary>

- *Zheng Yu, Xuhui Fan, Marcin Pietrasik, Marek Reformat*

- `2002.00901v1` - [abs](http://arxiv.org/abs/2002.00901v1) - [pdf](http://arxiv.org/pdf/2002.00901v1)

> The Mixed-Membership Stochastic Blockmodel~(MMSB) is proposed as one of the state-of-the-art Bayesian relational methods suitable for learning the complex hidden structure underlying the network data. However, the current formulation of MMSB suffers from the following two issues: (1), the prior information~(e.g. entities' community structural information) can not be well embedded in the modelling; (2), community evolution can not be well described in the literature. Therefore, we propose a non-parametric fragmentation coagulation based Mixed Membership Stochastic Blockmodel (fcMMSB). Our model performs entity-based clustering to capture the community information for entities and linkage-based clustering to derive the group information for links simultaneously. Besides, the proposed model infers the network structure and models community evolution, manifested by appearances and disappearances of communities, using the discrete fragmentation coagulation process (DFCP). By integrating the community structure with the group compatibility matrix we derive a generalized version of MMSB. An efficient Gibbs sampling scheme with Polya Gamma (PG) approach is implemented for posterior inference. We validate our model on synthetic and real world data.

</details>

<details>

<summary>2020-01-18 03:26:03 - FlexiBO: Cost-Aware Multi-Objective Optimization of Deep Neural Networks</summary>

- *Md Shahriar Iqbal, Jianhai Su, Lars Kotthoff, Pooyan Jamshidi*

- `2001.06588v1` - [abs](http://arxiv.org/abs/2001.06588v1) - [pdf](http://arxiv.org/pdf/2001.06588v1)

> One of the key challenges in designing machine learning systems is to determine the right balance amongst several objectives, which also oftentimes are incommensurable and conflicting. For example, when designing deep neural networks (DNNs), one often has to trade-off between multiple objectives, such as accuracy, energy consumption, and inference time. Typically, there is no single configuration that performs equally well for all objectives. Consequently, one is interested in identifying Pareto-optimal designs. Although different multi-objective optimization algorithms have been developed to identify Pareto-optimal configurations, state-of-the-art multi-objective optimization methods do not consider the different evaluation costs attending the objectives under consideration. This is particularly important for optimizing DNNs: the cost arising on account of assessing the accuracy of DNNs is orders of magnitude higher than that of measuring the energy consumption of pre-trained DNNs. We propose FlexiBO, a flexible Bayesian optimization method, to address this issue. We formulate a new acquisition function based on the improvement of the Pareto hyper-volume weighted by the measurement cost of each objective. Our acquisition function selects the next sample and objective that provides maximum information gain per unit of cost. We evaluated FlexiBO on 7 state-of-the-art DNNs for object detection, natural language processing, and speech recognition. Our results indicate that, when compared to other state-of-the-art methods across the 7 architectures we tested, the Pareto front obtained using FlexiBO has, on average, a 28.44% higher contribution to the true Pareto front and achieves 25.64% better diversity.

</details>

<details>

<summary>2020-01-19 09:29:58 - Learning Options from Demonstration using Skill Segmentation</summary>

- *Matthew Cockcroft, Shahil Mawjee, Steven James, Pravesh Ranchod*

- `2001.06793v1` - [abs](http://arxiv.org/abs/2001.06793v1) - [pdf](http://arxiv.org/pdf/2001.06793v1)

> We present a method for learning options from segmented demonstration trajectories. The trajectories are first segmented into skills using nonparametric Bayesian clustering and a reward function for each segment is then learned using inverse reinforcement learning. From this, a set of inferred trajectories for the demonstration are generated. Option initiation sets and termination conditions are learned from these trajectories using the one-class support vector machine clustering algorithm. We demonstrate our method in the four rooms domain, where an agent is able to autonomously discover usable options from human demonstration. Our results show that these inferred options can then be used to improve learning and planning.

</details>

<details>

<summary>2020-01-19 12:00:33 - Distributionally Robust Bayesian Quadrature Optimization</summary>

- *Thanh Tang Nguyen, Sunil Gupta, Huong Ha, Santu Rana, Svetha Venkatesh*

- `2001.06814v1` - [abs](http://arxiv.org/abs/2001.06814v1) - [pdf](http://arxiv.org/pdf/2001.06814v1)

> Bayesian quadrature optimization (BQO) maximizes the expectation of an expensive black-box integrand taken over a known probability distribution. In this work, we study BQO under distributional uncertainty in which the underlying probability distribution is unknown except for a limited set of its i.i.d. samples. A standard BQO approach maximizes the Monte Carlo estimate of the true expected objective given the fixed sample set. Though Monte Carlo estimate is unbiased, it has high variance given a small set of samples; thus can result in a spurious objective function. We adopt the distributionally robust optimization perspective to this problem by maximizing the expected objective under the most adversarial distribution. In particular, we propose a novel posterior sampling based algorithm, namely distributionally robust BQO (DRBQO) for this purpose. We demonstrate the empirical effectiveness of our proposed framework in synthetic and real-world problems, and characterize its theoretical convergence via Bayesian regret.

</details>

<details>

<summary>2020-01-19 16:15:55 - Finding Optimal Points for Expensive Functions Using Adaptive RBF-Based Surrogate Model Via Uncertainty Quantification</summary>

- *Ray-Bing Chen, Yuan Wang, C. F. Jeff Wu*

- `2001.06858v1` - [abs](http://arxiv.org/abs/2001.06858v1) - [pdf](http://arxiv.org/pdf/2001.06858v1)

> Global optimization of expensive functions has important applications in physical and computer experiments. It is a challenging problem to develop efficient optimization scheme, because each function evaluation can be costly and the derivative information of the function is often not available. We propose a novel global optimization framework using adaptive Radial Basis Functions (RBF) based surrogate model via uncertainty quantification. The framework consists of two iteration steps. It first employs an RBF-based Bayesian surrogate model to approximate the true function, where the parameters of the RBFs can be adaptively estimated and updated each time a new point is explored. Then it utilizes a model-guided selection criterion to identify a new point from a candidate set for function evaluation. The selection criterion adopted here is a sample version of the expected improvement (EI) criterion. We conduct simulation studies with standard test functions, which show that the proposed method has some advantages, especially when the true surface is not very smooth. In addition, we also propose modified approaches to improve the search performance for identifying global optimal points and to deal with the higher dimension scenarios.

</details>

<details>

<summary>2020-01-19 23:24:47 - A Review on Quantile Regression for Stochastic Computer Experiments</summary>

- *Léonard Torossian, Victor Picheny, Robert Faivre, Aurélien Garivier*

- `1901.07874v4` - [abs](http://arxiv.org/abs/1901.07874v4) - [pdf](http://arxiv.org/pdf/1901.07874v4)

> We report on an empirical study of the main strategies for quantile regression in the context of stochastic computer experiments. To ensure adequate diversity, six metamodels are presented, divided into three categories based on order statistics, functional approaches, and those of Bayesian inspiration. The metamodels are tested on several problems characterized by the size of the training set, the input dimension, the signal-to-noise ratio and the value of the probability density function at the targeted quantile. The metamodels studied reveal good contrasts in our set of experiments, enabling several patterns to be extracted. Based on our results, guidelines are proposed to allow users to select the best method for a given problem.

</details>

<details>

<summary>2020-01-20 05:09:52 - Investigation of Patient-sharing Networks Using a Bayesian Network Model Selection Approach for Congruence Class Models</summary>

- *Ravi Goyal, Victor De Gruttola*

- `2001.06974v1` - [abs](http://arxiv.org/abs/2001.06974v1) - [pdf](http://arxiv.org/pdf/2001.06974v1)

> A Bayesian approach to conduct network model selection is presented for a general class of network models referred to as the congruence class models (CCMs). CCMs form a broad class that includes as special cases several common network models, such as the Erd\H{o}s-R\'{e}nyi-Gilbert model, stochastic block model and many exponential random graph models. Due to the range of models able to be specified as a CCM, investigators are better able to select a model consistent with generative mechanisms associated with the observed network compared to current approaches. In addition, the approach allows for incorporation of prior information. We utilize the proposed Bayesian network model selection approach for CCMs to investigate several mechanisms that may be responsible for the structure of patient-sharing networks, which are associated with the cost and quality of medical care. We found evidence in support of heterogeneity in sociality but not selective mixing by provider type nor degree.

</details>

<details>

<summary>2020-01-20 16:14:51 - Assessing Dynamic Effects on a Bayesian Matrix-Variate Dynamic Linear Model: an Application to fMRI Data Analysis</summary>

- *Johnatan Cardona Jiménez, Carlos A. de B. Pereira, Victor Fossaluza*

- `1910.12058v2` - [abs](http://arxiv.org/abs/1910.12058v2) - [pdf](http://arxiv.org/pdf/1910.12058v2)

> In this work, we propose a modeling procedure for fMRI data analysis using a Bayesian Matrix-Variate Dynamic Linear Model (MVDLM). With this type of model, less complex than the more traditional temporal-spatial models, we are able to take into account the temporal and -- at least locally -- the spatial structures that are usually present in this type of data. Despite employing a voxel-wise approach, every voxel in the brain is jointly modeled with its nearest neighbors, which are defined through a euclidian metric. MVDLM's have been widely used in applications where the interest lies in to perform predictions and/or analysis of covariance structures among time series. In this context, our interest is rather to assess the dynamic effects which are related to voxel activation. In order to do so, we develop three algorithms to simulate online-trajectories related to the state parameter and with those curves or simulated trajectories we compute a Monte Carlo evidence for voxel activation. Through two practical examples and two different types of assessments, we show that our method can be viewed for the practitioners as a reliable tool for fMRI data analysis. Despite all the examples and analysis are illustrated just for a single subject analysis, we also describe how more general group analysis can be implemented.

</details>

<details>

<summary>2020-01-20 18:56:27 - Learning Fair Representations for Kernel Models</summary>

- *Zilong Tan, Samuel Yeom, Matt Fredrikson, Ameet Talwalkar*

- `1906.11813v2` - [abs](http://arxiv.org/abs/1906.11813v2) - [pdf](http://arxiv.org/pdf/1906.11813v2)

> Fair representations are a powerful tool for establishing criteria like statistical parity, proxy non-discrimination, and equality of opportunity in learned models. Existing techniques for learning these representations are typically model-agnostic, as they preprocess the original data such that the output satisfies some fairness criterion, and can be used with arbitrary learning methods. In contrast, we demonstrate the promise of learning a model-aware fair representation, focusing on kernel-based models. We leverage the classical Sufficient Dimension Reduction (SDR) framework to construct representations as subspaces of the reproducing kernel Hilbert space (RKHS), whose member functions are guaranteed to satisfy fairness. Our method supports several fairness criteria, continuous and discrete data, and multiple protected attributes. We further show how to calibrate the accuracy tradeoff by characterizing it in terms of the principal angles between subspaces of the RKHS. Finally, we apply our approach to obtain the first Fair Gaussian Process (FGP) prior for fair Bayesian learning, and show that it is competitive with, and in some cases outperforms, state-of-the-art methods on real data.

</details>

<details>

<summary>2020-01-20 22:27:00 - Characterizing functional relationships between anthropogenic and biological sounds: A western New York state soundscape case study</summary>

- *Jeffrey W. Doser, Kristina M. Hannam, Andrew O. Finley*

- `1905.01218v2` - [abs](http://arxiv.org/abs/1905.01218v2) - [pdf](http://arxiv.org/pdf/1905.01218v2)

> Roads are a widespread feature of landscapes worldwide, and road traffic sound potentially makes nearby habitat unsuitable for acoustically communicating organisms. It is important to understand the influence of roads at the soundscape level to mitigate negative impacts of road sound on individual species as well as subsequent effects on the surrounding landscape. We seek to characterize the relationship between anthropogenic and biological sounds in western New York and assess the extent to which available traffic data explains variability in anthropogenic noise. Recordings were obtained in the spring of 2016 at 18 sites throughout western New York. We used the Welch Power Spectral Density (PSD) at low frequencies (0.5-2 kHz) to represent anthropogenic noise and PSD values at higher frequencies (2-11 kHz) to represent biological sound. Relationships were modeled using a novel two-stage hierarchical Bayesian model utilizing beta regression and basis splines. Model results and map predictions illustrate that anthropogenic noise and biological sound have an inverse relationship, and anthropogenic noise is greatest in close proximity to high traffic volume roads. The predictions have large uncertainty, resulting from the temporal coarseness of public road data used as a proxy for traffic sound. Results suggest that finer temporal resolution traffic sound data, such as crowd-sourced time-indexed traffic data from geographic positioning systems, might better account for observed temporal changes in the soundscape. The use of such data, in combination with the proposed modeling framework, could have important implications for the development of sound management policies.

</details>

<details>

<summary>2020-01-21 02:59:17 - Bayesian Spatial Models for Voxel-wise Prostate Cancer Classification Using Multi-parametric MRI Data</summary>

- *Jin Jin, Lin Zhang, Ethan Leng, Gregory J. Metzger, Joseph S. Koopmeiners*

- `2001.07316v1` - [abs](http://arxiv.org/abs/2001.07316v1) - [pdf](http://arxiv.org/pdf/2001.07316v1)

> Multi-parametric magnetic resonance imaging (mpMRI) plays an increasingly important role in the diagnosis of prostate cancer. Various computer-aided detection algorithms have been proposed for automated prostate cancer detection by combining information from various mpMRI data components. However, there exist other features of mpMRI, including the spatial correlation between voxels and between-patient heterogeneity in the mpMRI parameters, that have not been fully explored in the literature but could potentially improve cancer detection if leveraged appropriately. This paper proposes novel voxel-wise Bayesian classifiers for prostate cancer that account for the spatial correlation and between-patient heterogeneity in mpMRI. Modeling the spatial correlation is challenging due to the extreme high dimensionality of the data, and we consider three computationally efficient approaches using Nearest Neighbor Gaussian Process (NNGP), knot-based reduced-rank approximation, and a conditional autoregressive (CAR) model, respectively. The between-patient heterogeneity is accounted for by adding a subject-specific random intercept on the mpMRI parameter model. Simulation results show that properly modeling the spatial correlation and between-patient heterogeneity improves classification accuracy. Application to in vivo data illustrates that classification is improved by spatial modeling using NNGP and reduced-rank approximation but not the CAR model, while modeling the between-patient heterogeneity does not further improve our classifier. Among our proposed models, the NNGP-based model is recommended considering its robust classification accuracy and high computational efficiency.

</details>

<details>

<summary>2020-01-21 03:02:43 - Why Non-myopic Bayesian Optimization is Promising and How Far Should We Look-ahead? A Study via Rollout</summary>

- *Xubo Yue, Raed Al Kontar*

- `1911.01004v2` - [abs](http://arxiv.org/abs/1911.01004v2) - [pdf](http://arxiv.org/pdf/1911.01004v2)

> Lookahead, also known as non-myopic, Bayesian optimization (BO) aims to find optimal sampling policies through solving a dynamic programming (DP) formulation that maximizes a long-term reward over a rolling horizon. Though promising, lookahead BO faces the risk of error propagation through its increased dependence on a possibly mis-specified model. In this work we focus on the rollout approximation for solving the intractable DP. We first prove the improving nature of rollout in tackling lookahead BO and provide a sufficient condition for the used heuristic to be rollout improving. We then provide both a theoretical and practical guideline to decide on the rolling horizon stagewise. This guideline is built on quantifying the negative effect of a mis-specified model. To illustrate our idea, we provide case studies on both single and multi-information source BO. Empirical results show the advantageous properties of our method over several myopic and non-myopic BO algorithms.

</details>

<details>

<summary>2020-01-21 06:41:37 - Bayesian Optimization using Pseudo-Points</summary>

- *Chao Qian, Hang Xiong, Ke Xue*

- `1910.05484v2` - [abs](http://arxiv.org/abs/1910.05484v2) - [pdf](http://arxiv.org/pdf/1910.05484v2)

> Bayesian optimization (BO) is a popular approach for expensive black-box optimization, with applications including parameter tuning, experimental design, robotics. BO usually models the objective function by a Gaussian process (GP), and iteratively samples the next data point by maximizing an acquisition function. In this paper, we propose a new general framework for BO by generating pseudo-points (i.e., data points whose objective values are not evaluated) to improve the GP model. With the classic acquisition function, i.e., upper confidence bound (UCB), we prove that the cumulative regret can be generally upper bounded. Experiments using UCB and other acquisition functions, i.e., probability of improvement (PI) and expectation of improvement (EI), on synthetic as well as real-world problems clearly show the advantage of generating pseudo-points.

</details>

<details>

<summary>2020-01-21 08:27:58 - Large-scale Heteroscedastic Regression via Gaussian Process</summary>

- *Haitao Liu, Yew-Soon Ong, Jianfei Cai*

- `1811.01179v3` - [abs](http://arxiv.org/abs/1811.01179v3) - [pdf](http://arxiv.org/pdf/1811.01179v3)

> Heteroscedastic regression considering the varying noises among observations has many applications in the fields like machine learning and statistics. Here we focus on the heteroscedastic Gaussian process (HGP) regression which integrates the latent function and the noise function together in a unified non-parametric Bayesian framework. Though showing remarkable performance, HGP suffers from the cubic time complexity, which strictly limits its application to big data. To improve the scalability, we first develop a variational sparse inference algorithm, named VSHGP, to handle large-scale datasets. Furthermore, two variants are developed to improve the scalability and capability of VSHGP. The first is stochastic VSHGP (SVSHGP) which derives a factorized evidence lower bound, thus enhancing efficient stochastic variational inference. The second is distributed VSHGP (DVSHGP) which (i) follows the Bayesian committee machine formalism to distribute computations over multiple local VSHGP experts with many inducing points; and (ii) adopts hybrid parameters for experts to guard against over-fitting and capture local variety. The superiority of DVSHGP and SVSHGP as compared to existing scalable heteroscedastic/homoscedastic GPs is then extensively verified on various datasets.

</details>

<details>

<summary>2020-01-21 08:32:15 - Gibbs flow for approximate transport with applications to Bayesian computation</summary>

- *Jeremy Heng, Arnaud Doucet, Yvo Pokern*

- `1509.08787v3` - [abs](http://arxiv.org/abs/1509.08787v3) - [pdf](http://arxiv.org/pdf/1509.08787v3)

> Let $\pi_{0}$ and $\pi_{1}$ be two distributions on the Borel space $(\mathbb{R}^{d},\mathcal{B}(\mathbb{R}^{d}))$. Any measurable function $T:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}$ such that $Y=T(X)\sim\pi_{1}$ if $X\sim\pi_{0}$ is called a transport map from $\pi_{0}$ to $\pi_{1}$. For any $\pi_{0}$ and $\pi_{1}$, if one could obtain an analytical expression for a transport map from $\pi_{0}$ to $\pi_{1}$, then this could be straightforwardly applied to sample from any distribution. One would map draws from an easy-to-sample distribution $\pi_{0}$ to the target distribution $\pi_{1}$ using this transport map. Although it is usually impossible to obtain an explicit transport map for complex target distributions, we show here how to build a tractable approximation of a novel transport map. This is achieved by moving samples from $\pi_{0}$ using an ordinary differential equation with a velocity field that depends on the full conditional distributions of the target. Even when this ordinary differential equation is time-discretized and the full conditional distributions are numerically approximated, the resulting distribution of mapped samples can be efficiently evaluated and used as a proposal within sequential Monte Carlo samplers. We demonstrate significant gains over state-of-the-art sequential Monte Carlo samplers at a fixed computational complexity on a variety of applications.

</details>

<details>

<summary>2020-01-21 15:57:21 - Clinical Prediction Models to Predict the Risk of Multiple Binary Outcomes: a comparison of approaches</summary>

- *Glen P. Martin, Matthew Sperrin, Kym I. E. Snell, Iain Buchan, Richard D. Riley*

- `2001.07624v1` - [abs](http://arxiv.org/abs/2001.07624v1) - [pdf](http://arxiv.org/pdf/2001.07624v1)

> Clinical prediction models (CPMs) are used to predict clinically relevant outcomes or events. Typically, prognostic CPMs are derived to predict the risk of a single future outcome. However, with rising emphasis on the prediction of multi-morbidity, there is growing need for CPMs to simultaneously predict risks for each of multiple future outcomes. A common approach to multi-outcome risk prediction is to derive a CPM for each outcome separately, then multiply the predicted risks. This approach is only valid if the outcomes are conditionally independent given the covariates, and it fails to exploit the potential relationships between the outcomes. This paper outlines several approaches that could be used to develop prognostic CPMs for multiple outcomes. We consider four methods, ranging in complexity and assumed conditional independence assumptions: namely, probabilistic classifier chain, multinomial logistic regression, multivariate logistic regression, and a Bayesian probit model. These are compared with methods that rely on conditional independence: separate univariate CPMs and stacked regression. Employing a simulation study and real-world example via the MIMIC-III database, we illustrate that CPMs for joint risk prediction of multiple outcomes should only be derived using methods that model the residual correlation between outcomes. In such a situation, our results suggest that probabilistic classification chains, multinomial logistic regression or the Bayesian probit model are all appropriate choices. We call into question the development of CPMs for each outcome in isolation when multiple correlated or structurally related outcomes are of interest and recommend more holistic risk prediction.

</details>

<details>

<summary>2020-01-22 05:01:32 - Quantization-Based Regularization for Autoencoders</summary>

- *Hanwei Wu, Markus Flierl*

- `1905.11062v2` - [abs](http://arxiv.org/abs/1905.11062v2) - [pdf](http://arxiv.org/pdf/1905.11062v2)

> Autoencoders and their variations provide unsupervised models for learning low-dimensional representations for downstream tasks. Without proper regularization, autoencoder models are susceptible to the overfitting problem and the so-called posterior collapse phenomenon. In this paper, we introduce a quantization-based regularizer in the bottleneck stage of autoencoder models to learn meaningful latent representations. We combine both perspectives of Vector Quantized-Variational AutoEncoders (VQ-VAE) and classical denoising regularization methods of neural networks. We interpret quantizers as regularizers that constrain latent representations while fostering a similarity-preserving mapping at the encoder. Before quantization, we impose noise on the latent codes and use a Bayesian estimator to optimize the quantizer-based representation. The introduced bottleneck Bayesian estimator outputs the posterior mean of the centroids to the decoder, and thus, is performing soft quantization of the noisy latent codes. We show that our proposed regularization method results in improved latent representations for both supervised learning and clustering downstream tasks when compared to autoencoders using other bottleneck structures.

</details>

<details>

<summary>2020-01-22 07:51:46 - On the Local Lipschitz Stability of Bayesian Inverse Problems</summary>

- *Björn Sprungk*

- `1906.07120v3` - [abs](http://arxiv.org/abs/1906.07120v3) - [pdf](http://arxiv.org/pdf/1906.07120v3)

> In this note we consider the stability of posterior measures occuring in Bayesian inference w.r.t. perturbations of the prior measure and the log-likelihood function. This extends the well-posedness analysis of Bayesian inverse problems. In particular, we prove a general local Lipschitz continuous dependence of the posterior on the prior and the log-likelihood w.r.t. various common distances of probability measures. These include the total variation, Hellinger, and Wasserstein distance and the Kullback-Leibler divergence. We only assume the boundedness of the likelihoods and measure their perturbations in an $L^p$-norm w.r.t. the prior. The obtained stability yields under mild assumptions the well-posedness of Bayesian inverse problems, in particular, a well-posedness w.r.t. the Wasserstein distance. Moreover, our results indicate an increasing sensitivity of Bayesian inference as the posterior becomes more concentrated, e.g., due to more or more accurate data. This confirms and extends previous observations made in the sensitivity analysis of Bayesian inference.

</details>

<details>

<summary>2020-01-22 08:30:39 - From abstract items to latent spaces to observed data and back: Compositional Variational Auto-Encoder</summary>

- *Victor Berger, Michèle Sebag*

- `2001.07910v1` - [abs](http://arxiv.org/abs/2001.07910v1) - [pdf](http://arxiv.org/pdf/2001.07910v1)

> Conditional Generative Models are now acknowledged an essential tool in Machine Learning. This paper focuses on their control. While many approaches aim at disentangling the data through the coordinate-wise control of their latent representations, another direction is explored in this paper. The proposed CompVAE handles data with a natural multi-ensemblist structure (i.e. that can naturally be decomposed into elements). Derived from Bayesian variational principles, CompVAE learns a latent representation leveraging both observational and symbolic information. A first contribution of the approach is that this latent representation supports a compositional generative model, amenable to multi-ensemblist operations (addition or subtraction of elements in the composition). This compositional ability is enabled by the invariance and generality of the whole framework w.r.t. respectively, the order and number of the elements. The second contribution of the paper is a proof of concept on synthetic 1D and 2D problems, demonstrating the efficiency of the proposed approach.

</details>

<details>

<summary>2020-01-22 15:08:30 - On Last-Layer Algorithms for Classification: Decoupling Representation from Uncertainty Estimation</summary>

- *Nicolas Brosse, Carlos Riquelme, Alice Martin, Sylvain Gelly, Éric Moulines*

- `2001.08049v1` - [abs](http://arxiv.org/abs/2001.08049v1) - [pdf](http://arxiv.org/pdf/2001.08049v1)

> Uncertainty quantification for deep learning is a challenging open problem. Bayesian statistics offer a mathematically grounded framework to reason about uncertainties; however, approximate posteriors for modern neural networks still require prohibitive computational costs. We propose a family of algorithms which split the classification task into two stages: representation learning and uncertainty estimation. We compare four specific instances, where uncertainty estimation is performed via either an ensemble of Stochastic Gradient Descent or Stochastic Gradient Langevin Dynamics snapshots, an ensemble of bootstrapped logistic regressions, or via a number of Monte Carlo Dropout passes. We evaluate their performance in terms of \emph{selective} classification (risk-coverage), and their ability to detect out-of-distribution samples. Our experiments suggest there is limited value in adding multiple uncertainty layers to deep classifiers, and we observe that these simple methods strongly outperform a vanilla point-estimate SGD in some complex benchmarks like ImageNet.

</details>

<details>

<summary>2020-01-23 17:23:38 - Bayesian estimates of transmission line outage rates that consider line dependencies</summary>

- *Kai Zhou, James R. Cruise, Chris J. Dent, Ian Dobson, Louis Wehenkel, Zhaoyu Wang, Amy L. Wilson*

- `2001.08681v1` - [abs](http://arxiv.org/abs/2001.08681v1) - [pdf](http://arxiv.org/pdf/2001.08681v1)

> Transmission line outage rates are fundamental to power system reliability analysis. Line outages are infrequent, occurring only about once a year, so outage data are limited. We propose a Bayesian hierarchical model that leverages line dependencies to better estimate outage rates of individual transmission lines from limited outage data. The Bayesian estimates have a lower standard deviation than estimating the outage rates simply by dividing the number of outages by the number of years of data, especially when the number of outages is small. The Bayesian model produces more accurate individual line outage rates, as well as estimates of the uncertainty of these rates. Better estimates of line outage rates can improve system risk assessment, outage prediction, and maintenance scheduling.

</details>

<details>

<summary>2020-01-24 13:00:56 - Sparse Semi-supervised Heterogeneous Interbattery Bayesian Analysis</summary>

- *Carlos Sevilla-Salcedo, Vanessa Gómez-Verdejo, Pablo M. Olmos*

- `2001.08975v1` - [abs](http://arxiv.org/abs/2001.08975v1) - [pdf](http://arxiv.org/pdf/2001.08975v1)

> The Bayesian approach to feature extraction, known as factor analysis (FA), has been widely studied in machine learning to obtain a latent representation of the data. An adequate selection of the probabilities and priors of these bayesian models allows the model to better adapt to the data nature (i.e. heterogeneity, sparsity), obtaining a more representative latent space.   The objective of this article is to propose a general FA framework capable of modelling any problem. To do so, we start from the Bayesian Inter-Battery Factor Analysis (BIBFA) model, enhancing it with new functionalities to be able to work with heterogeneous data, include feature selection, and handle missing values as well as semi-supervised problems.   The performance of the proposed model, Sparse Semi-supervised Heterogeneous Interbattery Bayesian Analysis (SSHIBA) has been tested on 4 different scenarios to evaluate each one of its novelties, showing not only a great versatility and an interpretability gain, but also outperforming most of the state-of-the-art algorithms.

</details>

<details>

<summary>2020-01-24 15:10:13 - Joint modeling with time-dependent treatment and heteroskedasticity: Bayesian analysis with application to the Framingham Heart Study</summary>

- *Zhuozhao Zhan, Vasan S. Ramachandran, Edwin R. van den Heuvel*

- `1912.06398v2` - [abs](http://arxiv.org/abs/1912.06398v2) - [pdf](http://arxiv.org/pdf/1912.06398v2)

> Medical studies for chronic disease are often interested in the relation between longitudinal risk factor profiles and individuals' later life disease outcomes. These profiles may typically be subject to intermediate structural changes due to treatment or environmental influences. Analysis of such studies may be handled by the joint model framework. However, current joint modeling does not consider structural changes in the residual variability of the risk profile nor consider the influence of subject-specific residual variability on the time-to-event outcome. In the present paper, we extend the joint model framework to address these two heterogeneous intra-individual variabilities. A Bayesian approach is used to estimate the unknown parameters and simulation studies are conducted to investigate the performance of the method. The proposed joint model is applied to the Framingham Heart Study to investigate the influence of anti-hypertensive medication on the systolic blood pressure variability together with its effect on the risk of developing cardiovascular disease. We show that anti-hypertensive medication is associated with elevated systolic blood pressure variability and increased variability elevates risk of developing cardiovascular disease.

</details>

<details>

<summary>2020-01-24 21:19:29 - Bayesian Inference for Regression Copulas</summary>

- *Michael Stanley Smith, Nadja Klein*

- `1907.04529v2` - [abs](http://arxiv.org/abs/1907.04529v2) - [pdf](http://arxiv.org/pdf/1907.04529v2)

> We propose a new semi-parametric distributional regression smoother that is based on a copula decomposition of the joint distribution of the vector of response values. The copula is high-dimensional and constructed by inversion of a pseudo regression, where the conditional mean and variance are semi-parametric functions of covariates modeled using regularized basis functions. By integrating out the basis coefficients, an implicit copula process on the covariate space is obtained, which we call a `regression copula'. We combine this with a non-parametric margin to define a copula model, where the entire distribution - including the mean and variance - of the response is a smooth semi-parametric function of the covariates. The copula is estimated using both Hamiltonian Monte Carlo and variational Bayes; the latter of which is scalable to high dimensions. Using real data examples and a simulation study we illustrate the efficacy of these estimators and the copula model. In a substantive example, we estimate the distribution of half-hourly electricity spot prices as a function of demand and two time covariates using radial bases and horseshoe regularization. The copula model produces distributional estimates that are locally adaptive with respect to the covariates, and predictions that are more accurate than those from benchmark models.

</details>

<details>

<summary>2020-01-25 02:49:37 - Testing Bayesian Networks</summary>

- *Clement Canonne, Ilias Diakonikolas, Daniel Kane, Alistair Stewart*

- `1612.03156v2` - [abs](http://arxiv.org/abs/1612.03156v2) - [pdf](http://arxiv.org/pdf/1612.03156v2)

> This work initiates a systematic investigation of testing high-dimensional structured distributions by focusing on testing Bayesian networks -- the prototypical family of directed graphical models. A Bayesian network is defined by a directed acyclic graph, where we associate a random variable with each node. The value at any particular node is conditionally independent of all the other non-descendant nodes once its parents are fixed. Specifically, we study the properties of identity testing and closeness testing of Bayesian networks. Our main contribution is the first non-trivial efficient testing algorithms for these problems and corresponding information-theoretic lower bounds. For a wide range of parameter settings, our testing algorithms have sample complexity sublinear in the dimension and are sample-optimal, up to constant factors.

</details>

<details>

<summary>2020-01-25 04:03:55 - An automatic robust Bayesian approach to principal component regression</summary>

- *Philippe Gagnon, Mylène Bédard, Alain Desgagné*

- `1711.06341v3` - [abs](http://arxiv.org/abs/1711.06341v3) - [pdf](http://arxiv.org/pdf/1711.06341v3)

> Principal component regression uses principal components as regressors. It is particularly useful in prediction settings with high-dimensional covariates. The existing literature treating of Bayesian approaches is relatively sparse. We introduce a Bayesian approach that is robust to outliers in both the dependent variable and the covariates. Outliers can be thought of as observations that are not in line with the general trend. The proposed approach automatically penalises these observations so that their impact on the posterior gradually vanishes as they move further and further away from the general trend, corresponding to a concept in Bayesian statistics called whole robustness. The predictions produced are thus consistent with the bulk of the data. The approach also exploits the geometry of principal components to efficiently identify those that are significant. Individual predictions obtained from the resulting models are consolidated according to model-averaging mechanisms to account for model uncertainty. The approach is evaluated on real data and compared to its nonrobust Bayesian counterpart, the traditional frequentist approach, and a commonly employed robust frequentist method. Detailed guidelines to automate the entire statistical procedure are provided. All required code is made available, see ArXiv:1711.06341.

</details>

<details>

<summary>2020-01-25 11:16:30 - Bayesian Panel Quantile Regression for Binary Outcomes with Correlated Random Effects: An Application on Crime Recidivism in Canada</summary>

- *Georges Bresson, Guy Lacroix, Mohammad Arshad Rahman*

- `2001.09295v1` - [abs](http://arxiv.org/abs/2001.09295v1) - [pdf](http://arxiv.org/pdf/2001.09295v1)

> This article develops a Bayesian approach for estimating panel quantile regression with binary outcomes in the presence of correlated random effects. We construct a working likelihood using an asymmetric Laplace (AL) error distribution and combine it with suitable prior distributions to obtain the complete joint posterior distribution. For posterior inference, we propose two Markov chain Monte Carlo (MCMC) algorithms but prefer the algorithm that exploits the blocking procedure to produce lower autocorrelation in the MCMC draws. We also explain how to use the MCMC draws to calculate the marginal effects, relative risk and odds ratio. The performance of our preferred algorithm is demonstrated in multiple simulation studies and shown to perform extremely well. Furthermore, we implement the proposed framework to study crime recidivism in Quebec, a Canadian Province, using a novel data from the administrative correctional files. Our results suggest that the recently implemented "tough-on-crime" policy of the Canadian government has been largely successful in reducing the probability of repeat offenses in the post-policy period. Besides, our results support existing findings on crime recidivism and offer new insights at various quantiles.

</details>

<details>

<summary>2020-01-25 14:33:38 - Bayesian optimization for backpropagation in Monte-Carlo tree search</summary>

- *Yueqin Li, Nengli Lim*

- `2001.09325v1` - [abs](http://arxiv.org/abs/2001.09325v1) - [pdf](http://arxiv.org/pdf/2001.09325v1)

> In large domains, Monte-Carlo tree search (MCTS) is required to estimate the values of the states as efficiently and accurately as possible. However, the standard update rule in backpropagation assumes a stationary distribution for the returns, and particularly in min-max trees, convergence to the true value can be slow because of averaging. We present two methods, Softmax MCTS and Monotone MCTS, which generalize previous attempts to improve upon the backpropagation strategy. We demonstrate that both methods reduce to finding optimal monotone functions, which we do so by performing Bayesian optimization with a Gaussian process (GP) prior. We conduct experiments on computer Go, where the returns are given by a deep value neural network, and show that our proposed framework outperforms previous methods.

</details>

<details>

<summary>2020-01-25 22:11:51 - Particle-Gibbs Sampling For Bayesian Feature Allocation Models</summary>

- *Alexandre Bouchard-Côté, Andrew Roth*

- `2001.09367v1` - [abs](http://arxiv.org/abs/2001.09367v1) - [pdf](http://arxiv.org/pdf/2001.09367v1)

> Bayesian feature allocation models are a popular tool for modelling data with a combinatorial latent structure. Exact inference in these models is generally intractable and so practitioners typically apply Markov Chain Monte Carlo (MCMC) methods for posterior inference. The most widely used MCMC strategies rely on an element wise Gibbs update of the feature allocation matrix. These element wise updates can be inefficient as features are typically strongly correlated. To overcome this problem we have developed a Gibbs sampler that can update an entire row of the feature allocation matrix in a single move. However, this sampler is impractical for models with a large number of features as the computational complexity scales exponentially in the number of features. We develop a Particle Gibbs sampler that targets the same distribution as the row wise Gibbs updates, but has computational complexity that only grows linearly in the number of features. We compare the performance of our proposed methods to the standard Gibbs sampler using synthetic data from a range of feature allocation models. Our results suggest that row wise updates using the PG methodology can significantly improve the performance of samplers for feature allocation models.

</details>

<details>

<summary>2020-01-26 09:21:39 - Stagewise Safe Bayesian Optimization with Gaussian Processes</summary>

- *Yanan Sui, Vincent Zhuang, Joel W. Burdick, Yisong Yue*

- `1806.07555v2` - [abs](http://arxiv.org/abs/1806.07555v2) - [pdf](http://arxiv.org/pdf/1806.07555v2)

> Enforcing safety is a key aspect of many problems pertaining to sequential decision making under uncertainty, which require the decisions made at every step to be both informative of the optimal decision and also safe. For example, we value both efficacy and comfort in medical therapy, and efficiency and safety in robotic control. We consider this problem of optimizing an unknown utility function with absolute feedback or preference feedback subject to unknown safety constraints. We develop an efficient safe Bayesian optimization algorithm, StageOpt, that separates safe region expansion and utility function maximization into two distinct stages. Compared to existing approaches which interleave between expansion and optimization, we show that StageOpt is more efficient and naturally applicable to a broader class of problems. We provide theoretical guarantees for both the satisfaction of safety constraints as well as convergence to the optimal utility value. We evaluate StageOpt on both a variety of synthetic experiments, as well as in clinical practice. We demonstrate that StageOpt is more effective than existing safe optimization approaches, and is able to safely and effectively optimize spinal cord stimulation therapy in our clinical experiments.

</details>

<details>

<summary>2020-01-26 21:46:48 - Bayesian Elicitation</summary>

- *Mark Whitmeyer*

- `1902.00976v2` - [abs](http://arxiv.org/abs/1902.00976v2) - [pdf](http://arxiv.org/pdf/1902.00976v2)

> How can a receiver design an information structure in order to elicit information from a sender? We study how a decision-maker can acquire more information from an agent by reducing her own ability to observe what the agent transmits. Intuitively, when the two parties' preferences are not perfectly aligned, this garbling relaxes the sender's concern that the receiver will use her information to the sender's disadvantage. We characterize the optimal information structure for the receiver. The main result is that under broad conditions, the receiver can do just as well as if she could commit to a rule mapping the sender's message to actions: information design is just as good as full commitment. Similarly, we show that these conditions guarantee that ex ante information acquisition always benefits the receiver, even though this learning might actually lower the receiver's expected payoff in the absence of garbling. We illustrate these effects in a range of economically relevant examples.

</details>

<details>

<summary>2020-01-27 15:05:14 - Data-Driven Model Set Design for Model Averaged Particle Filter</summary>

- *Bin Liu*

- `1910.00011v5` - [abs](http://arxiv.org/abs/1910.00011v5) - [pdf](http://arxiv.org/pdf/1910.00011v5)

> This paper is concerned with sequential state filtering in the presence of nonlinearity, non-Gaussianity and model uncertainty. For this problem, the Bayesian model averaged particle filter (BMAPF) is perhaps one of the most efficient solutions. Major advances of BMAPF have been made, while it still lacks a generic and practical approach to design the model set. This paper fills in this gap by proposing a generic data-driven method for BMAPF model set design. Unlike existent methods, the proposed solution does not require any prior knowledge on the parameter value of the true model; it only assumes that a small number of noisy observations are pre-obtained. The Bayesian optimization (BO) method is adapted to search the model components, each of which is associated with a specific segment of the pre-obtained dataset.The average performance of these model components is guaranteed since each one's parameter value is elaborately tuned via BO to maximize the marginal likelihood. The diversity in the model components is also ensured, as different components match the different segments of the pre-obtained dataset, respectively. Computer simulations are used to demonstrate the effectiveness of the proposed method.

</details>

<details>

<summary>2020-01-27 15:45:46 - Predictive inference with Fleming--Viot-driven dependent Dirichlet processes</summary>

- *Filippo Ascolani, Antonio Lijoi, Matteo Ruggiero*

- `2001.09868v1` - [abs](http://arxiv.org/abs/2001.09868v1) - [pdf](http://arxiv.org/pdf/2001.09868v1)

> We consider predictive inference using a class of temporally dependent Dirichlet processes driven by Fleming--Viot diffusions, which have a natural bearing in Bayesian nonparametrics and lend the resulting family of random probability measures to analytical posterior analysis. Formulating the implied statistical model as a hidden Markov model, we fully describe the predictive distribution induced by these Fleming--Viot-driven dependent Dirichlet processes, for a sequence of observations collected at a certain time given another set of draws collected at several previous times. This is identified as a mixture of P\'olya urns, whereby the observations can be values from the baseline distribution or copies of previous draws collected at the same time as in the usual P\`olya urn, or can be sampled from a random subset of the data collected at previous times. We characterise the time-dependent weights of the mixture which select such subsets and discuss the asymptotic regimes. We describe the induced partition by means of a Chinese restaurant process metaphor with a conveyor belt, whereby new customers who do not sit at an occupied table open a new table by picking a dish either from the baseline distribution or from a time-varying offer available on the conveyor belt. We lay out explicit algorithms for exact and approximate posterior sampling of both observations and partitions, and illustrate our results on predictive problems with synthetic and real data.

</details>

<details>

<summary>2020-01-27 16:19:20 - Bayesian nonparametric shared multi-sequence time series segmentation</summary>

- *Olga Mikheeva, Ieva Kazlauskaite, Hedvig Kjellström, Carl Henrik Ek*

- `2001.09886v1` - [abs](http://arxiv.org/abs/2001.09886v1) - [pdf](http://arxiv.org/pdf/2001.09886v1)

> In this paper, we introduce a method for segmenting time series data using tools from Bayesian nonparametrics. We consider the task of temporal segmentation of a set of time series data into representative stationary segments. We use Gaussian process (GP) priors to impose our knowledge about the characteristics of the underlying stationary segments, and use a nonparametric distribution to partition the sequences into such segments, formulated in terms of a prior distribution on segment length. Given the segmentation, the model can be viewed as a variant of a Gaussian mixture model where the mixture components are described using the covariance function of a GP. We demonstrate the effectiveness of our model on synthetic data as well as on real time-series data of heartbeats where the task is to segment the indicative types of beats and to classify the heartbeat recordings into classes that correspond to healthy and abnormal heart sounds.

</details>

<details>

<summary>2020-01-27 22:53:02 - Bayesian inference for treatment effects under nested subsets of controls</summary>

- *Spencer Woody, Carlos M. Carvalho, Jared S. Murray*

- `2001.07256v2` - [abs](http://arxiv.org/abs/2001.07256v2) - [pdf](http://arxiv.org/pdf/2001.07256v2)

> When constructing a model to estimate the causal effect of a treatment, it is necessary to control for other factors which may have confounding effects. Because the ignorability assumption is not testable, however, it is usually unclear which set of controls is appropriate, and effect estimation is generally sensitive to this choice. A common approach in this case is to fit several models, each with a different set of controls, but it is difficult to reconcile inference under the multiple resulting posterior distributions for the treatment effect. Therefore we propose a two-stage approach to measure the sensitivity of effect estimation with respect to control specification. In the first stage, a model is fit with all available controls using a prior carefully selected to adjust for confounding. In the second stage, posterior distributions are calculated for the treatment effect under nested sets of controls by propagating posterior uncertainty in the original model. We demonstrate how our approach can be used to detect the most significant confounders in a dataset, and apply it in a sensitivity analysis of an observational study measuring the effect of legalized abortion on crime rates.

</details>

<details>

<summary>2020-01-28 10:05:13 - MultiVerse: Causal Reasoning using Importance Sampling in Probabilistic Programming</summary>

- *Yura Perov, Logan Graham, Kostis Gourgoulias, Jonathan G. Richens, Ciarán M. Lee, Adam Baker, Saurabh Johri*

- `1910.08091v2` - [abs](http://arxiv.org/abs/1910.08091v2) - [pdf](http://arxiv.org/pdf/1910.08091v2)

> We elaborate on using importance sampling for causal reasoning, in particular for counterfactual inference. We show how this can be implemented natively in probabilistic programming. By considering the structure of the counterfactual query, one can significantly optimise the inference process. We also consider design choices to enable further optimisations. We introduce MultiVerse, a probabilistic programming prototype engine for approximate causal reasoning. We provide experimental results and compare with Pyro, an existing probabilistic programming framework with some of causal reasoning tools.

</details>

<details>

<summary>2020-01-28 14:08:53 - Parameter Calibration in Crowd Simulation Models using Approximate Bayesian Computation</summary>

- *Nikolai Bode*

- `2001.10330v1` - [abs](http://arxiv.org/abs/2001.10330v1) - [pdf](http://arxiv.org/pdf/2001.10330v1)

> Simulation models for pedestrian crowds are a ubiquitous tool in research and industry. It is crucial that the parameters of these models are calibrated carefully and ultimately it will be of interest to compare competing models to decide which model is best suited for a particular purpose. In this contribution, I demonstrate how Approximate Bayesian Computation (ABC), which is already a popular tool in other areas of science, can be used for model fitting and model selection in a pedestrian dynamics context. I fit two different models for pedestrian dynamics to data on a crowd passing in one direction through a bottleneck. One model describes movement in continuous-space, the other model is a cellular automaton and thus describes movement in discrete-space. In addition, I compare models to data using two metrics. The first is based on egress times and the second on the velocity of pedestrians in front of the bottleneck. My results show that while model fitting is successful, a substantial degree of uncertainty about the value of some model parameters remains after model fitting. Importantly, the choice of metric in model fitting can influence parameter estimates. Model selection is inconclusive for the egress time metric but supports the continuous-space model for the velocity-based metric. These findings show that ABC is a flexible approach and highlight the difficulties associated with model fitting and model selection for pedestrian dynamics. ABC requires many simulation runs and choosing appropriate metrics for comparing data to simulations requires careful attention. Despite this, I suggest ABC is a promising tool, because it is versatile and easily implemented for the growing number of openly available crowd simulators and data sets.

</details>

<details>

<summary>2020-01-28 18:45:26 - Skills to not fall behind in school</summary>

- *Felipe Maia Polo*

- `2001.10519v1` - [abs](http://arxiv.org/abs/2001.10519v1) - [pdf](http://arxiv.org/pdf/2001.10519v1)

> Many recent studies emphasize how important the role of cognitive and social-emotional skills can be in determining people's quality of life. Although skills are of great importance in many aspects, in this paper we will focus our efforts to better understand the relationship between several types of skills with academic progress delay. Our dataset contains the same students in 2012 and 2017, and we consider that there was a academic progress delay for a specific student if he or she progressed less than expected in school grades. Our methodology primarily includes the use of a Bayesian logistic regression model and our results suggest that both cognitive and social-emotional skills may impact the conditional probability of falling behind in school, and the magnitude of the impact between the two types of skills can be comparable.

</details>

<details>

<summary>2020-01-28 22:20:25 - Position Dilution of Precision: a Bayesian point of view</summary>

- *Alexandra Koulouri, Ville Rimpiläinen, Nathan D. Smith*

- `2001.02198v3` - [abs](http://arxiv.org/abs/2001.02198v3) - [pdf](http://arxiv.org/pdf/2001.02198v3)

> The expected position error in many cases is far from feasible to be estimated experimentally using real satellite measurements which makes the model-based position dilution of precision (PDOP) crucial in positioning and navigation applications. In the following text we derive the relationship between PDOP and position error and we explain that this relationship holds as long as the model for the observation errors represents the true sources of errors.

</details>

<details>

<summary>2020-01-29 01:28:15 - The Indian Chefs Process</summary>

- *Patrick Dallaire, Luca Ambrogioni, Ludovic Trottier, Umut Güçlü, Max Hinne, Philippe Giguère, Brahim Chaib-Draa, Marcel van Gerven, Francois Laviolette*

- `2001.10657v1` - [abs](http://arxiv.org/abs/2001.10657v1) - [pdf](http://arxiv.org/pdf/2001.10657v1)

> This paper introduces the Indian Chefs Process (ICP), a Bayesian nonparametric prior on the joint space of infinite directed acyclic graphs (DAGs) and orders that generalizes Indian Buffet Processes. As our construction shows, the proposed distribution relies on a latent Beta Process controlling both the orders and outgoing connection probabilities of the nodes, and yields a probability distribution on sparse infinite graphs. The main advantage of the ICP over previously proposed Bayesian nonparametric priors for DAG structures is its greater flexibility. To the best of our knowledge, the ICP is the first Bayesian nonparametric model supporting every possible DAG. We demonstrate the usefulness of the ICP on learning the structure of deep generative sigmoid networks as well as convolutional neural networks.

</details>

<details>

<summary>2020-01-29 01:57:04 - BUDD: Multi-modal Bayesian Updating Deforestation Detections</summary>

- *Alice M. S Durieux, Christopher X. Ren, Matthew T. Calef, Rick Chartrand, Michael S. Warren*

- `2001.10661v1` - [abs](http://arxiv.org/abs/2001.10661v1) - [pdf](http://arxiv.org/pdf/2001.10661v1)

> The global phenomenon of forest degradation is a pressing issue with severe implications for climate stability and biodiversity protection. In this work we generate Bayesian updating deforestation detection (BUDD) algorithms by incorporating Sentinel-1 backscatter and interferometric coherence with Sentinel-2 normalized vegetation index data. We show that the algorithm provides good performance in validation AOIs. We compare the effectiveness of different combinations of the three data modalities as inputs into the BUDD algorithm and compare against existing benchmarks based on optical imagery.

</details>

<details>

<summary>2020-01-29 02:15:09 - Quantifying Observed Prior Impact</summary>

- *David E Jones, Robert N Trangucci, Yang Chen*

- `2001.10664v1` - [abs](http://arxiv.org/abs/2001.10664v1) - [pdf](http://arxiv.org/pdf/2001.10664v1)

> We distinguish two questions (i) how much information does the prior contain? and (ii) what is the effect of the prior? Several measures have been proposed for quantifying effective prior sample size, for example Clarke [1996] and Morita et al. [2008]. However, these measures typically ignore the likelihood for the inference currently at hand, and therefore address (i) rather than (ii). Since in practice (ii) is of great concern, Reimherr et al. [2014] introduced a new class of effective prior sample size measures based on prior-likelihood discordance. We take this idea further towards its natural Bayesian conclusion by proposing measures of effective prior sample size that not only incorporate the general mathematical form of the likelihood but also the specific data at hand. Thus, our measures do not average across datasets from the working model, but condition on the current observed data. Consequently, our measures can be highly variable, but we demonstrate that this is because the impact of a prior can be highly variable. Our measures are Bayes estimates of meaningful quantities and well communicate the extent to which inference is determined by the prior, or framed differently, the amount of effort saved due to having prior information. We illustrate our ideas through a number of examples including a Gaussian conjugate model (continuous observations), a Beta-Binomial model (discrete observations), and a linear regression model (two unknown parameters). Future work on further developments of the methodology and an application to astronomy are discussed at the end.

</details>

<details>

<summary>2020-01-29 16:57:07 - Reducing complexity and unidentifiability when modelling human atrial cells</summary>

- *C. Houston, B. Marchand, L. Engelbert, C. D. Cantwell*

- `2001.10954v1` - [abs](http://arxiv.org/abs/2001.10954v1) - [pdf](http://arxiv.org/pdf/2001.10954v1)

> Mathematical models of a cellular action potential in cardiac modelling have become increasingly complex, particularly in gating kinetics which control the opening and closing of individual ion channel currents. As cardiac models advance towards use in personalised medicine to inform clinical decision-making, it is critical to understand the uncertainty hidden in parameter estimates from their calibration to experimental data. This study applies approximate Bayesian computation to re-calibrate the gating kinetics of four ion channels in two existing human atrial cell models to their original datasets, providing a measure of uncertainty and indication of potential issues with selecting a single unique value given the available experimental data. Two approaches are investigated to reduce the uncertainty present: re-calibrating the models to a more complete dataset and using a less complex formulation with fewer parameters to constrain. The re-calibrated models are inserted back into the full cell model to study the overall effect on the action potential. The use of more complete datasets does not eliminate uncertainty present in parameter estimates. The less complex model, particularly for the fast sodium current, gave a better fit to experimental data alongside lower parameter uncertainty and improved computational speed.

</details>

<details>

<summary>2020-01-29 18:08:52 - The Case for Bayesian Deep Learning</summary>

- *Andrew Gordon Wilson*

- `2001.10995v1` - [abs](http://arxiv.org/abs/2001.10995v1) - [pdf](http://arxiv.org/pdf/2001.10995v1)

> The key distinguishing property of a Bayesian approach is marginalization instead of optimization, not the prior, or Bayes rule. Bayesian inference is especially compelling for deep neural networks. (1) Neural networks are typically underspecified by the data, and can represent many different but high performing models corresponding to different settings of parameters, which is exactly when marginalization will make the biggest difference for both calibration and accuracy. (2) Deep ensembles have been mistaken as competing approaches to Bayesian methods, but can be seen as approximate Bayesian marginalization. (3) The structure of neural networks gives rise to a structured prior in function space, which reflects the inductive biases of neural networks that help them generalize. (4) The observed correlation between parameters in flat regions of the loss and a diversity of solutions that provide good generalization is further conducive to Bayesian marginalization, as flat regions occupy a large volume in a high dimensional space, and each different solution will make a good contribution to a Bayesian model average. (5) Recent practical advances for Bayesian deep learning provide improvements in accuracy and calibration compared to standard training, while retaining scalability.

</details>

<details>

<summary>2020-01-29 21:30:31 - Learning Bayesian posteriors with neural networks for gravitational-wave inference</summary>

- *Alvin J. K. Chua, Michele Vallisneri*

- `1909.05966v3` - [abs](http://arxiv.org/abs/1909.05966v3) - [pdf](http://arxiv.org/pdf/1909.05966v3)

> We seek to achieve the Holy Grail of Bayesian inference for gravitational-wave astronomy: using deep-learning techniques to instantly produce the posterior $p(\theta|D)$ for the source parameters $\theta$, given the detector data $D$. To do so, we train a deep neural network to take as input a signal + noise data set (drawn from the astrophysical source-parameter prior and the sampling distribution of detector noise), and to output a parametrized approximation of the corresponding posterior. We rely on a compact representation of the data based on reduced-order modeling, which we generate efficiently using a separate neural-network waveform interpolant [A. J. K. Chua, C. R. Galley & M. Vallisneri, Phys. Rev. Lett. 122, 211101 (2019)]. Our scheme has broad relevance to gravitational-wave applications such as low-latency parameter estimation and characterizing the science returns of future experiments. Source code and trained networks are available online at https://github.com/vallis/truebayes.

</details>

<details>

<summary>2020-01-30 12:55:08 - Log-Scale Shrinkage Priors and Adaptive Bayesian Global-Local Shrinkage Estimation</summary>

- *Daniel F. Schmidt, Enes Makalic*

- `1801.02321v2` - [abs](http://arxiv.org/abs/1801.02321v2) - [pdf](http://arxiv.org/pdf/1801.02321v2)

> Global-local shrinkage hierarchies are an important innovation in Bayesian estimation. We propose the use of log-scale distributions as a novel basis for generating familes of prior distributions for local shrinkage hyperparameters. By varying the scale parameter one may vary the degree to which the prior distribution promotes sparsity in the coefficient estimates. By examining the class of distributions over the logarithm of the local shrinkage parameter that have log-linear, or sub-log-linear tails, we show that many standard prior distributions for local shrinkage parameters can be unified in terms of the tail behaviour and concentration properties of their corresponding marginal distributions over the coefficients $\beta_j$. We derive upper bounds on the rate of concentration around $|\beta_j|=0$, and the tail decay as $|\beta_j| \to \infty$, achievable by this wide class of prior distributions.   We then propose a new type of ultra-heavy tailed prior, called the log-$t$ prior with the property that, irrespective of the choice of associated scale parameter, the marginal distribution always diverges at $\beta_j = 0$, and always possesses super-Cauchy tails. We develop results demonstrating when prior distributions with (sub)-log-linear tails attain Kullback--Leibler super-efficiency and prove that the log-$t$ prior distribution is always super-efficient. We show that the log-$t$ prior is less sensitive to misspecification of the global shrinkage parameter than the horseshoe or lasso priors. By incorporating the scale parameter of the log-scale prior distributions into the Bayesian hierarchy we derive novel adaptive shrinkage procedures. Simulations show that the adaptive log-$t$ procedure appears to always perform well, irrespective of the level of sparsity or signal-to-noise ratio of the underlying model.

</details>

<details>

<summary>2020-01-30 14:39:12 - Black-Box Saliency Map Generation Using Bayesian Optimisation</summary>

- *Mamuku Mokuwe, Michael Burke, Anna Sergeevna Bosman*

- `2001.11366v1` - [abs](http://arxiv.org/abs/2001.11366v1) - [pdf](http://arxiv.org/pdf/2001.11366v1)

> Saliency maps are often used in computer vision to provide intuitive interpretations of what input regions a model has used to produce a specific prediction. A number of approaches to saliency map generation are available, but most require access to model parameters. This work proposes an approach for saliency map generation for black-box models, where no access to model parameters is available, using a Bayesian optimisation sampling method. The approach aims to find the global salient image region responsible for a particular (black-box) model's prediction. This is achieved by a sampling-based approach to model perturbations that seeks to localise salient regions of an image to the black-box model. Results show that the proposed approach to saliency map generation outperforms grid-based perturbation approaches, and performs similarly to gradient-based approaches which require access to model parameters.

</details>

<details>

<summary>2020-01-30 15:00:23 - Assessing Bayes factor surfaces using interactive visualization and computer surrogate modeling</summary>

- *Christopher T. Franck, Robert B. Gramacy*

- `1809.05580v3` - [abs](http://arxiv.org/abs/1809.05580v3) - [pdf](http://arxiv.org/pdf/1809.05580v3)

> Bayesian model selection provides a natural alternative to classical hypothesis testing based on p-values. While many papers mention that Bayesian model selection is frequently sensitive to prior specification on the parameters, there are few practical strategies to assess and report this sensitivity. This article has two goals. First, we aim educate the broader statistical community about the extent of potential sensitivity through visualization of the Bayes factor surface. The Bayes factor surface shows the value a Bayes factor takes (usually on the log scale) as a function of user-specified hyperparameters. We provide interactive visualization through an R shiny application that allows the user to explore sensitivity in Bayes factor over a range of hyperparameter settings in a familiar regression setting. We compare the surface with three automatic procedures. Second, we suggest surrogate modeling via Gaussian processes (GPs) to visualize the Bayes factor surface in situations where computation of Bayes factors is expensive. That is, we treat Bayes factor calculation as a computer simulation experiment. In this context, we provide a fully reproducible example using accessible GP libraries to augment an important study of the influence of outliers in empirical finance. We suggest Bayes factor surfaces are valuable for scientific reporting since they (i) increase transparency, making potential instability in Bayes factors easy to visualize, (ii) generalize to simple and more complicated examples, and (iii) provide a path for researchers to assess the impact of prior choice on modeling decisions in a wide variety research areas.

</details>

<details>

<summary>2020-01-30 15:02:03 - Metric Gaussian Variational Inference</summary>

- *Jakob Knollmüller, Torsten A. Enßlin*

- `1901.11033v3` - [abs](http://arxiv.org/abs/1901.11033v3) - [pdf](http://arxiv.org/pdf/1901.11033v3)

> Solving Bayesian inference problems approximately with variational approaches can provide fast and accurate results. Capturing correlation within the approximation requires an explicit parametrization. This intrinsically limits this approach to either moderately dimensional problems, or requiring the strongly simplifying mean-field approach. We propose Metric Gaussian Variational Inference (MGVI) as a method that goes beyond mean-field. Here correlations between all model parameters are taken into account, while still scaling linearly in computational time and memory. With this method we achieve higher accuracy and in many cases a significant speedup compared to traditional methods. MGVI is an iterative method that performs a series of Gaussian approximations to the posterior. We alternate between approximating the covariance with the inverse Fisher information metric evaluated at an intermediate mean estimate and optimizing the KL-divergence for the given covariance with respect to the mean. This procedure is iterated until the uncertainty estimate is self-consistent with the mean parameter. We achieve linear scaling by avoiding to store the covariance explicitly at any time. Instead we draw samples from the approximating distribution relying on an implicit representation and numerical schemes to approximately solve linear equations. Those samples are used to approximate the KL-divergence and its gradient. The usage of natural gradient descent allows for rapid convergence. Formulating the Bayesian model in standardized coordinates makes MGVI applicable to any inference problem with continuous parameters. We demonstrate the high accuracy of MGVI by comparing it to HMC and its fast convergence relative to other established methods in several examples. We investigate real-data applications, as well as synthetic examples of varying size and complexity and up to a million model parameters.

</details>

<details>

<summary>2020-01-30 15:13:08 - On the Convergence of Extended Variational Inference for Non-Gaussian Statistical Models</summary>

- *Zhanyu Ma, Jalil Taghia, Jun Guo*

- `1902.05068v2` - [abs](http://arxiv.org/abs/1902.05068v2) - [pdf](http://arxiv.org/pdf/1902.05068v2)

> Variational inference (VI) is a widely used framework in Bayesian estimation. For most of the non-Gaussian statistical models, it is infeasible to find an analytically tractable solution to estimate the posterior distributions of the parameters. Recently, an improved framework, namely the extended variational inference (EVI), has been introduced and applied to derive analytically tractable solution by employing lower-bound approximation to the variational objective function. Two conditions required for EVI implementation, namely the weak condition and the strong condition, are discussed and compared in this paper. In practical implementation, the convergence of the EVI depends on the selection of the lower-bound approximation, no matter with the weak condition or the strong condition. In general, two approximation strategies, the single lower-bound (SLB) approximation and the multiple lower-bounds (MLB) approximation, can be applied to carry out the lower-bound approximation. To clarify the differences between the SLB and the MLB, we will also discuss the convergence properties of the aforementioned two approximations. Extensive comparisons are made based on some existing EVI-based non-Gaussian statistical models. Theoretical analysis are conducted to demonstrate the differences between the weak and the strong conditions. Qualitative and quantitative experimental results are presented to show the advantages of the SLB approximation.

</details>

<details>

<summary>2020-01-30 15:39:39 - Asymptotic Bayesian Generalization Error in Latent Dirichlet Allocation and Stochastic Matrix Factorization</summary>

- *Naoki Hayashi, Sumio Watanabe*

- `1709.04212v8` - [abs](http://arxiv.org/abs/1709.04212v8) - [pdf](http://arxiv.org/pdf/1709.04212v8)

> Latent Dirichlet allocation (LDA) is useful in document analysis, image processing, and many information systems; however, its generalization performance has been left unknown because it is a singular learning machine to which regular statistical theory can not be applied.   Stochastic matrix factorization (SMF) is a restricted matrix factorization in which matrix factors are stochastic; the column of the matrix is in a simplex. SMF is being applied to image recognition and text mining. We can understand SMF as a statistical model by which a stochastic matrix of given data is represented by a product of two stochastic matrices, whose generalization performance has also been left unknown because of non-regularity.   In this paper, by using an algebraic and geometric method, we show the analytic equivalence of LDA and SMF, both of which have the same real log canonical threshold (RLCT), resulting in that they asymptotically have the same Bayesian generalization error and the same log marginal likelihood. Moreover, we derive the upper bound of the RLCT and prove that it is smaller than the dimension of the parameter divided by two, hence the Bayesian generalization errors of them are smaller than those of regular statistical models.

</details>

<details>

<summary>2020-01-30 16:35:02 - Modelling Competing Legal Arguments using Bayesian Model Comparison and Averaging</summary>

- *Martin Neil, Norman Fenton, David Lagnado, Richard D. Gill*

- `1903.04891v2` - [abs](http://arxiv.org/abs/1903.04891v2) - [pdf](http://arxiv.org/pdf/1903.04891v2)

> Bayesian models of legal arguments generally aim to produce a single integrated model, combining each of the legal arguments under consideration. This combined approach implicitly assumes that variables and their relationships can be represented without any contradiction or misalignment, and in a way that makes sense with respect to the competing argument narratives. This paper describes a novel approach to compare and 'average' Bayesian models of legal arguments that have been built independently and with no attempt to make them consistent in terms of variables, causal assumptions or parametrisation. The approach involves assessing whether competing models of legal arguments are explained or predict facts uncovered before or during the trial process. Those models that are more heavily disconfirmed by the facts are given lower weight, as model plausibility measures, in the Bayesian model comparison and averaging framework adopted. In this way a plurality of arguments is allowed yet a single judgement based on all arguments is possible and rational.

</details>

<details>

<summary>2020-01-30 16:45:25 - Adversarial $α$-divergence Minimization for Bayesian Approximate Inference</summary>

- *Simón Rodríguez Santana, Daniel Hernández-Lobato*

- `1909.06945v3` - [abs](http://arxiv.org/abs/1909.06945v3) - [pdf](http://arxiv.org/pdf/1909.06945v3)

> Neural networks are popular state-of-the-art models for many different tasks.They are often trained via back-propagation to find a value of the weights that correctly predicts the observed data. Although back-propagation has shown good performance in many applications, it cannot easily output an estimate of the uncertainty in the predictions made. Estimating the uncertainty in the predictions is a critical aspect with important applications, and one method to obtain this information is following a Bayesian approach to estimate a posterior distribution on the model parameters. This posterior distribution summarizes which parameter values are compatible with the data, but is usually intractable and has to be approximated. Several mechanisms have been considered for solving this problem. We propose here a general method for approximate Bayesian inference that is based on minimizing{\alpha}-divergences and that allows for flexible approximate distributions. The method is evaluated in the context of Bayesian neural networks on extensive experiments. The results show that, in regression problems, it often gives better performance in terms of the test log-likelihoodand sometimes in terms of the squared error. In classification problems, however, it gives competitive results.

</details>

<details>

<summary>2020-01-30 17:44:21 - Transport Gaussian Processes for Regression</summary>

- *Gonzalo Rios*

- `2001.11473v1` - [abs](http://arxiv.org/abs/2001.11473v1) - [pdf](http://arxiv.org/pdf/2001.11473v1)

> Gaussian process (GP) priors are non-parametric generative models with appealing modelling properties for Bayesian inference: they can model non-linear relationships through noisy observations, have closed-form expressions for training and inference, and are governed by interpretable hyperparameters. However, GP models rely on Gaussianity, an assumption that does not hold in several real-world scenarios, e.g., when observations are bounded or have extreme-value dependencies, a natural phenomenon in physics, finance and social sciences. Although beyond-Gaussian stochastic processes have caught the attention of the GP community, a principled definition and rigorous treatment is still lacking. In this regard, we propose a methodology to construct stochastic processes, which include GPs, warped GPs, Student-t processes and several others under a single unified approach. We also provide formulas and algorithms for training and inference of the proposed models in the regression problem. Our approach is inspired by layers-based models, where each proposed layer changes a specific property over the generated stochastic process. That, in turn, allows us to push-forward a standard Gaussian white noise prior towards other more expressive stochastic processes, for which marginals and copulas need not be Gaussian, while retaining the appealing properties of GPs. We validate the proposed model through experiments with real-world data.

</details>

<details>

<summary>2020-01-31 15:04:30 - Mean shift cluster recognition method implementation in the nested sampling algorithm</summary>

- *M. Trassinelli, Pierre Ciccodicola*

- `2002.01431v1` - [abs](http://arxiv.org/abs/2002.01431v1) - [pdf](http://arxiv.org/pdf/2002.01431v1)

> Nested sampling is an efficient algorithm for the calculation of the Bayesian evidence and posterior parameter probability distributions. It is based on the step-by-step exploration of the parameter space by Monte Carlo sampling with a series of values sets called live points that evolve towards the region of interest, i.e. where the likelihood function is maximal. In presence of several local likelihood maxima, the algorithm converges with difficulty. Some systematic errors can also be introduced by unexplored parameter volume regions. In order to avoid this, different methods are proposed in the literature for an efficient search of new live points, even in presence of local maxima. Here we present a new solution based on the mean shift cluster recognition method implemented in a random walk search algorithm. The clustering recognition is integrated within the Bayesian analysis program NestedFit. It is tested with the analysis of some difficult cases. Compared to the analysis results without cluster recognition, the computation time is considerably reduced. At the same time, the entire parameter space is efficiently explored, which translates into a smaller uncertainty of the extracted value of the Bayesian evidence.

</details>

<details>

<summary>2020-01-31 15:37:37 - Low-Complexity Nonparametric Bayesian Online Prediction with Universal Guarantees</summary>

- *Alix Lhéritier, Frédéric Cazals*

- `1901.07662v4` - [abs](http://arxiv.org/abs/1901.07662v4) - [pdf](http://arxiv.org/pdf/1901.07662v4)

> We propose a novel nonparametric online predictor for discrete labels conditioned on multivariate continuous features. The predictor is based on a feature space discretization induced by a full-fledged k-d tree with randomly picked directions and a recursive Bayesian distribution, which allows to automatically learn the most relevant feature scales characterizing the conditional distribution. We prove its pointwise universality, i.e., it achieves a normalized log loss performance asymptotically as good as the true conditional entropy of the labels given the features. The time complexity to process the $n$-th sample point is $O(\log n)$ in probability with respect to the distribution generating the data points, whereas other exact nonparametric methods require to process all past observations. Experiments on challenging datasets show the computational and statistical efficiency of our algorithm in comparison to standard and state-of-the-art methods.

</details>

<details>

<summary>2020-01-31 16:57:53 - Non-reversibly updating a uniform [0,1] value for Metropolis accept/reject decisions</summary>

- *Radford M. Neal*

- `2001.11950v1` - [abs](http://arxiv.org/abs/2001.11950v1) - [pdf](http://arxiv.org/pdf/2001.11950v1)

> I show how it can be beneficial to express Metropolis accept/reject decisions in terms of comparison with a uniform [0,1] value, u, and to then update u non-reversibly, as part of the Markov chain state, rather than sampling it independently each iteration. This provides a small improvement for random walk Metropolis and Langevin updates in high dimensions. It produces a larger improvement when using Langevin updates with persistent momentum, giving performance comparable to that of Hamiltonian Monte Carlo (HMC) with long trajectories. This is of significance when some variables are updated by other methods, since if HMC is used, these updates can be done only between trajectories, whereas they can be done more often with Langevin updates. I demonstrate that for a problem with some continuous variables, updated by HMC or Langevin updates, and also discrete variables, updated by Gibbs sampling between updates of the continuous variables, Langevin with persistent momentum and non-reversible updates to u samples nearly a factor of two more efficiently than HMC. Benefits are also seen for a Bayesian neural network model in which hyperparameters are updated by Gibbs sampling.

</details>

<details>

<summary>2020-01-31 21:33:56 - Simultaneous Skull Conductivity and Focal Source Imaging from EEG Recordings with the help of Bayesian Uncertainty Modelling</summary>

- *Alexandra Koulouri, Ville Rimpilainen*

- `2002.00066v1` - [abs](http://arxiv.org/abs/2002.00066v1) - [pdf](http://arxiv.org/pdf/2002.00066v1)

> The electroencephalography (EEG) source imaging problem is very sensitive to the electrical modelling of the skull of the patient under examination. Unfortunately, the currently available EEG devices and their embedded software do not take this into account; instead, it is common to use a literature-based skull conductivity parameter. In this paper, we propose a statistical method based on the Bayesian approximation error approach to compensate for source imaging errors due to the unknown skull conductivity and, simultaneously, to compute a low-order estimate for the actual skull conductivity value. By using simulated EEG data that corresponds to focal source activity, we demonstrate the potential of the method to reconstruct the underlying focal sources and low-order errors induced by the unknown skull conductivity. Subsequently, the estimated errors are used to approximate the skull conductivity. The results indicate clear improvements in the source localization accuracy and feasible skull conductivity estimates.

</details>


## 2020-02

<details>

<summary>2020-02-01 01:41:08 - On the Consistency of Optimal Bayesian Feature Selection in the Presence of Correlations</summary>

- *Ali Foroughi pour, Lori A. Dalton*

- `2002.00120v1` - [abs](http://arxiv.org/abs/2002.00120v1) - [pdf](http://arxiv.org/pdf/2002.00120v1)

> Optimal Bayesian feature selection (OBFS) is a multivariate supervised screening method designed from the ground up for biomarker discovery. In this work, we prove that Gaussian OBFS is strongly consistent under mild conditions, and provide rates of convergence for key posteriors in the framework. These results are of enormous importance, since they identify precisely what features are selected by OBFS asymptotically, characterize the relative rates of convergence for posteriors on different types of features, provide conditions that guarantee convergence, justify the use of OBFS when its internal assumptions are invalid, and set the stage for understanding the asymptotic behavior of other algorithms based on the OBFS framework.

</details>

<details>

<summary>2020-02-01 03:37:37 - Efficient Bayesian synthetic likelihood with whitening transformations</summary>

- *Jacob W. Priddle, Scott A. Sisson, David T. Frazier, Christopher Drovandi*

- `1909.04857v2` - [abs](http://arxiv.org/abs/1909.04857v2) - [pdf](http://arxiv.org/pdf/1909.04857v2)

> Likelihood-free methods are an established approach for performing approximate Bayesian inference for models with intractable likelihood functions. However, they can be computationally demanding. Bayesian synthetic likelihood (BSL) is a popular such method that approximates the likelihood function of the summary statistic with a known, tractable distribution -- typically Gaussian -- and then performs statistical inference using standard likelihood-based techniques. However, as the number of summary statistics grows, the number of model simulations required to accurately estimate the covariance matrix for this likelihood rapidly increases. This poses significant challenge for the application of BSL, especially in cases where model simulation is expensive. In this article we propose whitening BSL (wBSL) -- an efficient BSL method that uses approximate whitening transformations to decorrelate the summary statistics at each algorithm iteration. We show empirically that this can reduce the number of model simulations required to implement BSL by more than an order of magnitude, without much loss of accuracy. We explore a range of whitening procedures and demonstrate the performance of wBSL on a range of simulated and real modelling scenarios from ecology and biology.

</details>

<details>

<summary>2020-02-01 11:19:43 - Bayesian Item Response Modeling in R with brms and Stan</summary>

- *Paul-Christian Bürkner*

- `1905.09501v3` - [abs](http://arxiv.org/abs/1905.09501v3) - [pdf](http://arxiv.org/pdf/1905.09501v3)

> Item Response Theory (IRT) is widely applied in the human sciences to model persons' responses on a set of items measuring one or more latent constructs. While several R packages have been developed that implement IRT models, they tend to be restricted to respective prespecified classes of models. Further, most implementations are frequentist while the availability of Bayesian methods remains comparably limited. We demonstrate how to use the R package brms together with the probabilistic programming language Stan to specify and fit a wide range of Bayesian IRT models using flexible and intuitive multilevel formula syntax. Further, item and person parameters can be related in both a linear or non-linear manner. Various distributions for categorical, ordinal, and continuous responses are supported. Users may even define their own custom response distribution for use in the presented framework. Common IRT model classes that can be specified natively in the presented framework include 1PL and 2PL logistic models optionally also containing guessing parameters, graded response and partial credit ordinal models, as well as drift diffusion models of response times coupled with binary decisions. Posterior distributions of item and person parameters can be conveniently extracted and post-processed. Model fit can be evaluated and compared using Bayes factors and efficient cross-validation procedures.

</details>

<details>

<summary>2020-02-01 17:30:59 - Online Bayesian phylodynamic inference in BEAST with application to epidemic reconstruction</summary>

- *Mandev S. Gill, Philippe Lemey, Marc A. Suchard, Andrew Rambaut, Guy Baele*

- `2002.00245v1` - [abs](http://arxiv.org/abs/2002.00245v1) - [pdf](http://arxiv.org/pdf/2002.00245v1)

> Reconstructing pathogen dynamics from genetic data as they become available during an outbreak or epidemic represents an important statistical scenario in which observations arrive sequentially in time and one is interested in performing inference in an 'online' fashion. Widely-used Bayesian phylogenetic inference packages are not set up for this purpose, generally requiring one to recompute trees and evolutionary model parameters de novo when new data arrive. To accommodate increasing data flow in a Bayesian phylogenetic framework, we introduce a methodology to efficiently update the posterior distribution with newly available genetic data. Our procedure is implemented in the BEAST 1.10 software package, and relies on a distance-based measure to insert new taxa into the current estimate of the phylogeny and imputes plausible values for new model parameters to accommodate growing dimensionality. This augmentation creates informed starting values and re-uses optimally tuned transition kernels for posterior exploration of growing data sets, reducing the time necessary to converge to target posterior distributions. We apply our framework to data from the recent West African Ebola virus epidemic and demonstrate a considerable reduction in time required to obtain posterior estimates at different time points of the outbreak. Beyond epidemic monitoring, this framework easily finds other applications within the phylogenetics community, where changes in the data -- in terms of alignment changes, sequence addition or removal -- present common scenarios that can benefit from online inference.

</details>

<details>

<summary>2020-02-02 08:34:32 - Bayesian Reliability Analysis of the Power Law Process with Respect to the Higgins-Tsokos Loss Function for Modeling Software Failure Times</summary>

- *Freeh Alenezi, Chris. Tsokos*

- `2002.00351v1` - [abs](http://arxiv.org/abs/2002.00351v1) - [pdf](http://arxiv.org/pdf/2002.00351v1)

> The Power Law Process, also known as Non-Homogeneous Poisson Process, has been used in various aspects, one of which is the software reliability assessment. Specifically, by using its intensity function to compute the rate of change of a software reliability as time-varying function. Justification of Bayesian analysis applicability to the Power Law Process was shown using real data. The probability distribution that best characterizes the behavior of the key parameter of the intensity function was first identified, then the likelihood-based Bayesian reliability estimate of the Power Law Process under the Higgins-Tsokos loss function was obtained. As a result of a simulation study and using real data, the Bayesian estimate shows an outstanding performance compared to the maximum likelihood estimate using different sample sizes. In addition, a sensitivity analysis was performed, resulting in the Bayesian estimate being sensitive to the prior selection; whether parametric or non-parametric.

</details>

<details>

<summary>2020-02-02 09:30:14 - Bayesian analysis of Turkish Income and Living Conditions data, using clustered longitudinal ordinal modelling with Bridge distributed random-effects</summary>

- *Özgür Asar*

- `1905.01106v2` - [abs](http://arxiv.org/abs/1905.01106v2) - [pdf](http://arxiv.org/pdf/1905.01106v2)

> This paper is motivated by the panel surveys, called Statistics on Income and Living Conditions (SILC), conducted annually on (randomly selected) country-representative households to monitor EU 2020 aims on poverty reduction. We particularly consider the surveys conducted in Turkey, within the scope of integration to the EU, between 2010 and 2013. Our main interests are on health aspects of economic and living conditions. The outcome is {\it self-reported health} that is clustered longitudinal ordinal, since repeated measures of it are nested within individuals and individuals are nested within families. Economic and living conditions were measured through a number of individual- and family-level explanatory variables. The questions of interest are on the marginal relationships between the outcome and covariates that are addressed using a polytomous logistic regression with Bridge distributed random-effects. This choice of distribution allows one to {\it directly} obtain marginal inferences in the presence of random-effects. Widely used Normal distribution is also considered as the random-effects distribution. Samples from the joint posterior density of parameters and random-effects are drawn using Markov Chain Monte Carlo. Interesting findings from public health point of view are that differences were found between sub-groups of employment status, income level and panel year in terms of odds of reporting better health.

</details>

<details>

<summary>2020-02-02 09:49:04 - Infinite Mixture of Inverted Dirichlet Distributions</summary>

- *Zhanyu Ma, Yuping Lai*

- `1807.10693v2` - [abs](http://arxiv.org/abs/1807.10693v2) - [pdf](http://arxiv.org/pdf/1807.10693v2)

> In this work, we develop a novel Bayesian estimation method for the Dirichlet process (DP) mixture of the inverted Dirichlet distributions, which has been shown to be very flexible for modeling vectors with positive elements. The recently proposed extended variational inference (EVI) framework is adopted to derive an analytically tractable solution. The convergency of the proposed algorithm is theoretically guaranteed by introducing single lower bound approximation to the original objective function in the VI framework. In principle, the proposed model can be viewed as an infinite inverted Dirichelt mixture model (InIDMM) that allows the automatic determination of the number of mixture components from data. Therefore, the problem of pre-determining the optimal number of mixing components has been overcome. Moreover, the problems of over-fitting and under-fitting are avoided by the Bayesian estimation approach. Comparing with several recently proposed DP-related methods, the good performance and effectiveness of the proposed method have been demonstrated with both synthesized data and real data evaluations.

</details>

<details>

<summary>2020-02-02 11:30:31 - Bayesian inference for nonlinear inverse problems</summary>

- *Vladimir Spokoiny*

- `1912.12694v2` - [abs](http://arxiv.org/abs/1912.12694v2) - [pdf](http://arxiv.org/pdf/1912.12694v2)

> Bayesian methods are actively used for parameter identification and uncertainty quantification when solving nonlinear inverse problems with random noise. However, there are only few theoretical results justifying the Bayesian approach. Recent papers, see e.g. \cite{Nickl2017,lu2017bernsteinvon} and references therein, illustrate the main difficulties and challenges in studying the properties of the posterior distribution in the nonparametric setup. This paper offers a new approach for study the frequentist properties of the nonparametric Bayes procedures. The idea of the approach is to relax the nonlinear structural equation by introducing an auxiliary functional parameter and replacing the structural equation with a penalty and by imposing a prior on the auxiliary parameter. For the such extended model, we state sharp bounds on posterior concentration and on the accuracy of the penalized MLE and on Gaussian approximation of the posterior, and a number of further results. All the bounds are given in terms of effective dimension, and we show that the proposed calming device does not significantly affect this value.

</details>

<details>

<summary>2020-02-03 01:00:59 - AVATAR -- Machine Learning Pipeline Evaluation Using Surrogate Model</summary>

- *Tien-Dung Nguyen, Tomasz Maszczyk, Katarzyna Musial, Marc-Andre Zöller, Bogdan Gabrys*

- `2001.11158v2` - [abs](http://arxiv.org/abs/2001.11158v2) - [pdf](http://arxiv.org/pdf/2001.11158v2)

> The evaluation of machine learning (ML) pipelines is essential during automatic ML pipeline composition and optimisation. The previous methods such as Bayesian-based and genetic-based optimisation, which are implemented in Auto-Weka, Auto-sklearn and TPOT, evaluate pipelines by executing them. Therefore, the pipeline composition and optimisation of these methods requires a tremendous amount of time that prevents them from exploring complex pipelines to find better predictive models. To further explore this research challenge, we have conducted experiments showing that many of the generated pipelines are invalid, and it is unnecessary to execute them to find out whether they are good pipelines. To address this issue, we propose a novel method to evaluate the validity of ML pipelines using a surrogate model (AVATAR). The AVATAR enables to accelerate automatic ML pipeline composition and optimisation by quickly ignoring invalid pipelines. Our experiments show that the AVATAR is more efficient in evaluating complex pipelines in comparison with the traditional evaluation approaches requiring their execution.

</details>

<details>

<summary>2020-02-03 14:48:45 - Efron-Stein PAC-Bayesian Inequalities</summary>

- *Ilja Kuzborskij, Csaba Szepesvári*

- `1909.01931v2` - [abs](http://arxiv.org/abs/1909.01931v2) - [pdf](http://arxiv.org/pdf/1909.01931v2)

> We prove semi-empirical concentration inequalities for random variables which are given as possibly nonlinear functions of independent random variables. These inequalities describe concentration of random variable in terms of the data/distribution-dependent Efron-Stein (ES) estimate of its variance and they do not require any additional assumptions on the moments. In particular, this allows us to state semi-empirical Bernstein type inequalities for general functions of unbounded random variables, which gives user-friendly concentration bounds for cases where related methods (e.g. bounded differences) might be more challenging to apply. We extend these results to Efron-Stein PAC-Bayesian inequalities which hold for arbitrary probability kernels that define a random, data-dependent choice of the function of interest. Finally, we demonstrate a number of applications, including PAC-Bayesian generalization bounds for unbounded loss functions, empirical Bernstein type generalization bounds, new truncation-free bounds for off-policy evaluation with Weighted Importance Sampling (WIS), and off-policy PAC-Bayesian learning with WIS.

</details>

<details>

<summary>2020-02-03 15:07:08 - Adaptive posterior contraction rates for empirical Bayesian drift estimation of a diffusion</summary>

- *Jan van Waaij*

- `1909.12710v2` - [abs](http://arxiv.org/abs/1909.12710v2) - [pdf](http://arxiv.org/pdf/1909.12710v2)

> Due to their conjugate posteriors, Gaussian process priors are attractive for estimating the drift of stochastic differential equations with continuous time observations. However, their performance strongly depends on the choice of the hyper-parameters. We employ the marginal maximum likelihood estimator to estimate the scaling and/or smoothness parameter(s) of the prior and show that the corresponding posterior has optimal rates of convergence. General theorems do not apply directly to this model as the usual test functions are with respect to a random Hellinger-type metric. We allow for continuous and discrete, one- and two-dimensional sets of hyper-parameters, where optimising over the two-dimensional set of smoothness and scaling hyper-parameters is shown to be beneficial in terms of the adaptive range.

</details>

<details>

<summary>2020-02-03 18:00:18 - Non-linear regression models for behavioral and neural data analysis</summary>

- *Vincent Adam, Alexandre Hyafil*

- `2002.00920v1` - [abs](http://arxiv.org/abs/2002.00920v1) - [pdf](http://arxiv.org/pdf/2002.00920v1)

> Regression models are popular tools in empirical sciences to infer the influence of a set of variables onto a dependent variable given an experimental dataset. In neuroscience and cognitive psychology, Generalized Linear Models (GLMs) -including linear regression, logistic regression, and Poisson GLM- is the regression model of choice to study the factors that drive participant's choices, reaction times and neural activations. These methods are however limited as they only capture linear contributions of each regressors. Here, we introduce an extension of GLMs called Generalized Unrestricted Models (GUMs), which allows to infer a much richer set of contributions of the regressors to the dependent variable, including possible interactions between the regressors. In a GUM, each regressor is passed through a linear or nonlinear function, and the contribution of the different resulting transformed regressors can be summed or multiplied to generate a predictor for the dependent variable. We propose a Bayesian treatment of these models in which we endow functions with Gaussian Process priors, and we present two methods to compute a posterior over the functions given a dataset: the Laplace method and a sparse variational approach, which scales better for large dataset. For each method, we assess the quality of the model estimation and we detail how the hyperparameters (defining for example the expected smoothness of the function) can be fitted. Finally, we illustrate the power of the method on a behavioral dataset where subjects reported the average perceived orientation of a series of gratings. The method allows to recover the mapping of the grating angle onto perceptual evidence for each subject, as well as the impact of the grating based on its position. Overall, GUMs provides a very rich and flexible framework to run nonlinear regression analysis in neuroscience, psychology, and beyond.

</details>

<details>

<summary>2020-02-03 18:47:08 - Constrained Bayesian Nonparametric Regression for Grain Boundary Energy Predictions</summary>

- *Haoyu Wang, Srikanth Patala, Brian J. Reich*

- `2002.00938v1` - [abs](http://arxiv.org/abs/2002.00938v1) - [pdf](http://arxiv.org/pdf/2002.00938v1)

> Grain boundary (GB) energy is a fundamental property that affects the form of grain boundary and plays an important role to unveil the behavior of polycrystalline materials. With a better understanding of grain boundary energy distribution (GBED), we can produce more durable and efficient materials that will further improve productivity and reduce loss. The lack of robust GB structure-property relationships still remains one of the biggest obstacles towards developing true bottom-up models for the behavior of polycrystalline materials. Progress has been slow because of the inherent complexity associated with the structure of interfaces and the vast five-dimensional configurational space in which they reside. Estimating the GBED is challenging from a statistical perspective because there are not direct measurements on the grain boundary energy. We only have indirect information in the form of an unidentifiable homogeneous set of linear equations. In this paper, we propose a new statistical model to determine the GBED from the microstructures of polycrystalline materials. We apply spline-based regression with constraints to successfully recover the GB energy surface. Hamiltonian Monte Carlo and Gibbs sampling are used for computation and model fitting. Compared with conventional methods, our method not only gives more accurate predictions but also provides prediction uncertainties.

</details>

<details>

<summary>2020-02-04 00:06:48 - Transfer Learning for HVAC System Fault Detection</summary>

- *Chase P. Dowling, Baosen Zhang*

- `2002.01060v1` - [abs](http://arxiv.org/abs/2002.01060v1) - [pdf](http://arxiv.org/pdf/2002.01060v1)

> Faults in HVAC systems degrade thermal comfort and energy efficiency in buildings and have received significant attention from the research community, with data driven methods gaining in popularity. Yet the lack of labeled data, such as normal versus faulty operational status, has slowed the application of machine learning to HVAC systems. In addition, for any particular building, there may be an insufficient number of observed faults over a reasonable amount of time for training. To overcome these challenges, we present a transfer methodology for a novel Bayesian classifier designed to distinguish between normal operations and faulty operations. The key is to train this classifier on a building with a large amount of sensor and fault data (for example, via simulation or standard test data) then transfer the classifier to a new building using a small amount of normal operations data from the new building. We demonstrate a proof-of-concept for transferring a classifier between architecturally similar buildings in different climates and show few samples are required to maintain classification precision and recall.

</details>

<details>

<summary>2020-02-04 13:48:30 - Dichotomize and Generalize: PAC-Bayesian Binary Activated Deep Neural Networks</summary>

- *Gaël Letarte, Pascal Germain, Benjamin Guedj, François Laviolette*

- `1905.10259v5` - [abs](http://arxiv.org/abs/1905.10259v5) - [pdf](http://arxiv.org/pdf/1905.10259v5)

> We present a comprehensive study of multilayer neural networks with binary activation, relying on the PAC-Bayesian theory. Our contributions are twofold: (i) we develop an end-to-end framework to train a binary activated deep neural network, (ii) we provide nonvacuous PAC-Bayesian generalization bounds for binary activated deep neural networks. Our results are obtained by minimizing the expected loss of an architecture-dependent aggregation of binary activated deep neural networks. Our analysis inherently overcomes the fact that binary activation function is non-differentiable. The performance of our approach is assessed on a thorough numerical experiment protocol on real-life datasets.

</details>

<details>

<summary>2020-02-04 16:37:52 - BOFFIN TTS: Few-Shot Speaker Adaptation by Bayesian Optimization</summary>

- *Henry B. Moss, Vatsal Aggarwal, Nishant Prateek, Javier González, Roberto Barra-Chicote*

- `2002.01953v1` - [abs](http://arxiv.org/abs/2002.01953v1) - [pdf](http://arxiv.org/pdf/2002.01953v1)

> We present BOFFIN TTS (Bayesian Optimization For FIne-tuning Neural Text To Speech), a novel approach for few-shot speaker adaptation. Here, the task is to fine-tune a pre-trained TTS model to mimic a new speaker using a small corpus of target utterances. We demonstrate that there does not exist a one-size-fits-all adaptation strategy, with convincing synthesis requiring a corpus-specific configuration of the hyper-parameters that control fine-tuning. By using Bayesian optimization to efficiently optimize these hyper-parameter values for a target speaker, we are able to perform adaptation with an average 30% improvement in speaker similarity over standard techniques. Results indicate, across multiple corpora, that BOFFIN TTS can learn to synthesize new speakers using less than ten minutes of audio, achieving the same naturalness as produced for the speakers used to train the base model.

</details>

<details>

<summary>2020-02-04 21:32:52 - Estimation of Z-Thickness and XY-Anisotropy of Electron Microscopy Images using Gaussian Processes</summary>

- *Thanuja D. Ambegoda, Julien N. P. Martel, Jozef Adamcik, Matthew Cook, Richard H. R. Hahnloser*

- `2002.00228v2` - [abs](http://arxiv.org/abs/2002.00228v2) - [pdf](http://arxiv.org/pdf/2002.00228v2)

> Serial section electron microscopy (ssEM) is a widely used technique for obtaining volumetric information of biological tissues at nanometer scale. However, accurate 3D reconstructions of identified cellular structures and volumetric quantifications require precise estimates of section thickness and anisotropy (or stretching) along the XY imaging plane. In fact, many image processing algorithms simply assume isotropy within the imaging plane. To ameliorate this problem, we present a method for estimating thickness and stretching of electron microscopy sections using non-parametric Bayesian regression of image statistics. We verify our thickness and stretching estimates using direct measurements obtained by atomic force microscopy (AFM) and show that our method has a lower estimation error compared to a recent indirect thickness estimation method as well as a relative Z coordinate estimation method. Furthermore, we have made the first dataset of ssSEM images with directly measured section thickness values publicly available for the evaluation of indirect thickness estimation methods.

</details>

<details>

<summary>2020-02-04 21:35:03 - Accelerating Psychometric Screening Tests With Bayesian Active Differential Selection</summary>

- *Trevor J. Larsen, Gustavo Malkomes, Dennis L. Barbour*

- `2002.01547v1` - [abs](http://arxiv.org/abs/2002.01547v1) - [pdf](http://arxiv.org/pdf/2002.01547v1)

> Classical methods for psychometric function estimation either require excessive measurements or produce only a low-resolution approximation of the target psychometric function. In this paper, we propose a novel solution for rapid screening for a change in the psychometric function estimation of a given patient. We use Bayesian active model selection to perform an automated pure-tone audiogram test with the goal of quickly finding if the current audiogram will be different from a previous audiogram. We validate our approach using audiometric data from the National Institute for Occupational Safety and Health NIOSH. Initial results show that with a few tones we can detect if the patient's audiometric function has changed between the two test sessions with high confidence.

</details>

<details>

<summary>2020-02-04 22:48:07 - Uncertainty Quantification for Bayesian Optimization</summary>

- *Rui Tuo, Wenjia Wang*

- `2002.01569v1` - [abs](http://arxiv.org/abs/2002.01569v1) - [pdf](http://arxiv.org/pdf/2002.01569v1)

> Bayesian optimization is a class of global optimization techniques. It regards the underlying objective function as a realization of a Gaussian process. Although the outputs of Bayesian optimization are random according to the Gaussian process assumption, quantification of this uncertainty is rarely studied in the literature. In this work, we propose a novel approach to assess the output uncertainty of Bayesian optimization algorithms, in terms of constructing confidence regions of the maximum point or value of the objective function. These regions can be computed efficiently, and their confidence levels are guaranteed by newly developed uniform error bounds for sequential Gaussian process regression. Our theory provides a unified uncertainty quantification framework for all existing sequential sampling policies and stopping criteria.

</details>

<details>

<summary>2020-02-05 10:11:26 - Semiparametric Bayesian Forecasting of Spatial Earthquake Occurrences</summary>

- *Aleksandar A. Kolev, Gordon J. Ross*

- `2002.01706v1` - [abs](http://arxiv.org/abs/2002.01706v1) - [pdf](http://arxiv.org/pdf/2002.01706v1)

> Self-exciting Hawkes processes are used to model events which cluster in time and space, and have been widely studied in seismology under the name of the Epidemic Type Aftershock Sequence (ETAS) model. In the ETAS framework, the occurrence of the mainshock earthquakes in a geographical region is assumed to follow an inhomogeneous spatial point process, and aftershock events are then modelled via a separate triggering kernel. Most previous studies of the ETAS model have relied on point estimates of the model parameters due to the complexity of the likelihood function, and the difficulty in estimating an appropriate mainshock distribution. In order to take estimation uncertainty into account, we instead propose a fully Bayesian formulation of the ETAS model which uses a nonparametric Dirichlet process mixture prior to capture the spatial mainshock process. Direct inference for the resulting model is problematic due to the strong correlation of the parameters for the mainshock and triggering processes, so we instead use an auxiliary latent variable routine to perform efficient inference.

</details>

<details>

<summary>2020-02-05 13:46:35 - On constraining projections of future climate using observations and simulations from multiple climate models</summary>

- *Philip G. Sansom, David B. Stephenson, Thomas J. Bracegirdle*

- `1711.04139v4` - [abs](http://arxiv.org/abs/1711.04139v4) - [pdf](http://arxiv.org/pdf/1711.04139v4)

> Numerical climate models are used to project future climate change due to both anthropogenic and natural causes. Differences between projections from different climate models are a major source of uncertainty about future climate. Emergent relationships shared by multiple climate models have the potential to constrain our uncertainty when combined with historical observations. We combine projections from 13 climate models with observational data to quantify the impact of emergent relationships on projections of future warming in the Arctic at the end of the 21st century. We propose a hierarchical Bayesian framework based on a coexchangeable representation of the relationship between climate models and the Earth system. We show how emergent constraints fit into the coexchangeable representation, and extend it to account for internal variability simulated by the models and natural variability in the Earth system. Our analysis shows that projected warming in some regions of the Arctic may be more than 2C lower and our uncertainty reduced by up to 30% when constrained by historical observations. A detailed theoretical comparison with existing multi-model projection frameworks is also provided. In particular, we show that projections may be biased if we do not account for internal variability in climate model predictions.

</details>

<details>

<summary>2020-02-06 13:26:54 - Bayesian stochastic blockmodeling</summary>

- *Tiago P. Peixoto*

- `1705.10225v8` - [abs](http://arxiv.org/abs/1705.10225v8) - [pdf](http://arxiv.org/pdf/1705.10225v8)

> This chapter provides a self-contained introduction to the use of Bayesian inference to extract large-scale modular structures from network data, based on the stochastic blockmodel (SBM), as well as its degree-corrected and overlapping generalizations. We focus on nonparametric formulations that allow their inference in a manner that prevents overfitting, and enables model selection. We discuss aspects of the choice of priors, in particular how to avoid underfitting via increased Bayesian hierarchies, and we contrast the task of sampling network partitions from the posterior distribution with finding the single point estimate that maximizes it, while describing efficient algorithms to perform either one. We also show how inferring the SBM can be used to predict missing and spurious links, and shed light on the fundamental limitations of the detectability of modular structures in networks.

</details>

<details>

<summary>2020-02-06 15:29:11 - A Bayesian decision-theoretic approach to incorporate preclinical information into phase I oncology trials</summary>

- *Haiyan Zheng, Lisa V. Hampson*

- `1907.13620v2` - [abs](http://arxiv.org/abs/1907.13620v2) - [pdf](http://arxiv.org/pdf/1907.13620v2)

> Leveraging preclinical animal data for a phase I first-in-man trial is appealing yet challenging. A prior based on animal data may place large probability mass on values of the dose-toxicity model parameter(s), which appear infeasible in light of data accrued from the ongoing phase I clinical trial. In this paper, we seek to use animal data to improve decision making in a model-based dose-escalation procedure for phase I oncology trials. Specifically, animal data are incorporated via a robust mixture prior for the parameters of the dose-toxicity relationship. This prior changes dynamically as the trial progresses. After completion of treatment for each cohort, the weight allocated to the informative component, obtained based on animal data alone, is updated using a decision-theoretic approach to assess the commensurability of the animal data with the human toxicity data observed thus far. In particular, we measure commensurability as a function of the utility of optimal prior predictions for the human responses (toxicity or no toxicity) on each administered dose. The proposed methodology is illustrated through several examples and an extensive simulation study. Results show that our proposal can address difficulties in coping with prior-data conflict commencing in sequential trials with a small sample size.

</details>

<details>

<summary>2020-02-06 17:22:20 - Macroscopic Traffic Flow Modeling with Physics Regularized Gaussian Process: A New Insight into Machine Learning Applications</summary>

- *Yun Yuan, Xianfeng Terry Yang, Zhao Zhang, Shandian Zhe*

- `2002.02374v1` - [abs](http://arxiv.org/abs/2002.02374v1) - [pdf](http://arxiv.org/pdf/2002.02374v1)

> Despite the wide implementation of machine learning (ML) techniques in traffic flow modeling recently, those data-driven approaches often fall short of accuracy in the cases with a small or noisy dataset. To address this issue, this study presents a new modeling framework, named physics regularized machine learning (PRML), to encode classical traffic flow models (referred as physical models) into the ML architecture and to regularize the ML training process. More specifically, a stochastic physics regularized Gaussian process (PRGP) model is developed and a Bayesian inference algorithm is used to estimate the mean and kernel of the PRGP. A physical regularizer based on macroscopic traffic flow models is also developed to augment the estimation via a shadow GP and an enhanced latent force model is used to encode physical knowledge into stochastic processes. Based on the posterior regularization inference framework, an efficient stochastic optimization algorithm is also developed to maximize the evidence lowerbound of the system likelihood. To prove the effectiveness of the proposed model, this paper conducts empirical studies on a real-world dataset which is collected from a stretch of I-15 freeway, Utah. Results show the new PRGP model can outperform the previous compatible methods, such as calibrated pure physical models and pure machine learning methods, in estimation precision and input robustness.

</details>

<details>

<summary>2020-02-06 17:29:04 - Product Kanerva Machines: Factorized Bayesian Memory</summary>

- *Adam Marblestone, Yan Wu, Greg Wayne*

- `2002.02385v1` - [abs](http://arxiv.org/abs/2002.02385v1) - [pdf](http://arxiv.org/pdf/2002.02385v1)

> An ideal cognitively-inspired memory system would compress and organize incoming items. The Kanerva Machine (Wu et al, 2018) is a Bayesian model that naturally implements online memory compression. However, the organization of the Kanerva Machine is limited by its use of a single Gaussian random matrix for storage. Here we introduce the Product Kanerva Machine, which dynamically combines many smaller Kanerva Machines. Its hierarchical structure provides a principled way to abstract invariant features and gives scaling and capacity advantages over single Kanerva Machines. We show that it can exhibit unsupervised clustering, find sparse and combinatorial allocation patterns, and discover spatial tunings that approximately factorize simple images by object.

</details>

<details>

<summary>2020-02-06 22:37:23 - Accelerated Bayesian Optimization throughWeight-Prior Tuning</summary>

- *Alistair Shilton, Sunil Gupta, Santu Rana, Pratibha Vellanki, Laurence Park, Cheng Li, Svetha Venkatesh, Alessandra Sutti, David Rubin, Thomas Dorin, Alireza Vahid, Murray Height, Teo Slezak*

- `1805.07852v2` - [abs](http://arxiv.org/abs/1805.07852v2) - [pdf](http://arxiv.org/pdf/1805.07852v2)

> Bayesian optimization (BO) is a widely-used method for optimizing expensive (to evaluate) problems. At the core of most BO methods is the modeling of the objective function using a Gaussian Process (GP) whose covariance is selected from a set of standard covariance functions. From a weight-space view, this models the objective as a linear function in a feature space implied by the given covariance K, with an arbitrary Gaussian weight prior ${\bf w} \sim \mathcal{N} ({\bf 0}, {\bf I})$. In many practical applications there is data available that has a similar (covariance) structure to the objective, but which, having different form, cannot be used directly in standard transfer learning. In this paper we show how such auxiliary data may be used to construct a GP covariance corresponding to a more appropriate weight prior for the objective function. Building on this, we show that we may accelerate BO by modeling the objective function using this (learned) weight prior, which we demonstrate on both test functions and a practical application to short-polymer fibre manufacture.

</details>

<details>

<summary>2020-02-07 01:54:24 - Bayesian leveraging of historical control data for a clinical trial with time-to-event endpoint</summary>

- *Satrajit Roychoudhury, Beat Neuenschwander*

- `1908.07265v2` - [abs](http://arxiv.org/abs/1908.07265v2) - [pdf](http://arxiv.org/pdf/1908.07265v2)

> The recent 21st Century Cures Act propagates innovations to accelerate the discovery, development, and delivery of 21st century cures. It includes the broader application of Bayesian statistics and the use of evidence from clinical expertise. An example of the latter is the use of trial-external (or historical) data, which promises more efficient or ethical trial designs. We propose a Bayesian meta-analytic approach to leveraging historical data for time-to-event endpoints, which are common in oncology and cardiovascular diseases. The approach is based on a robust hierarchical model for piecewise exponential data. It allows for various degrees of between trial-heterogeneity and for leveraging individual as well as aggregate data. An ovarian carcinoma trial and a non-small-cell cancer trial illustrate methodological and practical aspects of leveraging historical data for the analysis and design of time-to-event trials.

</details>

<details>

<summary>2020-02-07 05:49:25 - On Learning Causal Structures from Non-Experimental Data without Any Faithfulness Assumption</summary>

- *Hanti Lin, Jiji Zhang*

- `1802.07051v2` - [abs](http://arxiv.org/abs/1802.07051v2) - [pdf](http://arxiv.org/pdf/1802.07051v2)

> Consider the problem of learning, from non-experimental data, the causal (Markov equivalence) structure of the true, unknown causal Bayesian network (CBN) on a given, fixed set of (categorical) variables. This learning problem is known to be so hard that there is no learning algorithm that converges to the truth for all possible CBNs (on the given set of variables). So the convergence property has to be sacrificed for some CBNs---but for which? In response, the standard practice has been to design and employ learning algorithms that secure the convergence property for at least all the CBNs that satisfy the famous faithfulness condition, which implies sacrificing the convergence property for some CBNs that violate the faithfulness condition (Spirtes et al. 2000). This standard design practice can be justified by assuming---that is, accepting on faith---that the true, unknown CBN satisfies the faithfulness condition. But the real question is this: Is it possible to explain, without assuming the faithfulness condition or any of its weaker variants, why it is mandatory rather than optional to follow the standard design practice? This paper aims to answer the above question in the affirmative. We first define an array of modes of convergence to the truth as desiderata that might or might not be achieved by a causal learning algorithm. Those modes of convergence concern (i) how pervasive the domain of convergence is on the space of all possible CBNs and (ii) how uniformly the convergence happens. Then we prove a result to the following effect: for any learning algorithm that tackles the causal learning problem in question, if it achieves the best achievable mode of convergence (considered in this paper), then it must follow the standard design practice of converging to the truth for at least all CBNs that satisfy the faithfulness condition---it is a requirement, not an option.

</details>

<details>

<summary>2020-02-07 06:35:53 - Confirmatory Bayesian Online Change Point Detection in the Covariance Structure of Gaussian Processes</summary>

- *Jiyeon Han, Kyowoon Lee, Anh Tong, Jaesik Choi*

- `1905.13168v2` - [abs](http://arxiv.org/abs/1905.13168v2) - [pdf](http://arxiv.org/pdf/1905.13168v2)

> In the analysis of sequential data, the detection of abrupt changes is important in predicting future changes. In this paper, we propose statistical hypothesis tests for detecting covariance structure changes in locally smooth time series modeled by Gaussian Processes (GPs). We provide theoretically justified thresholds for the tests, and use them to improve Bayesian Online Change Point Detection (BOCPD) by confirming statistically significant changes and non-changes. Our Confirmatory BOCPD (CBOCPD) algorithm finds multiple structural breaks in GPs even when hyperparameters are not tuned precisely. We also provide conditions under which CBOCPD provides the lower prediction error compared to BOCPD. Experimental results on synthetic and real-world datasets show that our new tests correctly detect changes in the covariance structure in GPs. The proposed algorithm also outperforms existing methods for the prediction of nonstationarity in terms of both regression error and log likelihood.

</details>

<details>

<summary>2020-02-07 10:42:24 - Multi-Agent Thompson Sampling for Bandit Applications with Sparse Neighbourhood Structures</summary>

- *Timothy Verstraeten, Eugenio Bargiacchi, Pieter JK Libin, Jan Helsen, Diederik M Roijers, Ann Nowé*

- `1911.10120v2` - [abs](http://arxiv.org/abs/1911.10120v2) - [pdf](http://arxiv.org/pdf/1911.10120v2)

> Multi-agent coordination is prevalent in many real-world applications. However, such coordination is challenging due to its combinatorial nature. An important observation in this regard is that agents in the real world often only directly affect a limited set of neighbouring agents. Leveraging such loose couplings among agents is key to making coordination in multi-agent systems feasible. In this work, we focus on learning to coordinate. Specifically, we consider the multi-agent multi-armed bandit framework, in which fully cooperative loosely-coupled agents must learn to coordinate their decisions to optimize a common objective. We propose multi-agent Thompson sampling (MATS), a new Bayesian exploration-exploitation algorithm that leverages loose couplings. We provide a regret bound that is sublinear in time and low-order polynomial in the highest number of actions of a single agent for sparse coordination graphs. Additionally, we empirically show that MATS outperforms the state-of-the-art algorithm, MAUCE, on two synthetic benchmarks, and a novel benchmark with Poisson distributions. An example of a loosely-coupled multi-agent system is a wind farm. Coordination within the wind farm is necessary to maximize power production. As upstream wind turbines only affect nearby downstream turbines, we can use MATS to efficiently learn the optimal control mechanism for the farm. To demonstrate the benefits of our method toward applications we apply MATS to a realistic wind farm control task. In this task, wind turbines must coordinate their alignments with respect to the incoming wind vector in order to optimize power production. Our results show that MATS improves significantly upon state-of-the-art coordination methods in terms of performance, demonstrating the value of using MATS in practical applications with sparse neighbourhood structures.

</details>

<details>

<summary>2020-02-07 12:06:22 - Bayesian epidemiological modeling over high-resolution network data</summary>

- *Stefan Engblom, Robin Eriksson, Stefan Widgren*

- `1910.11720v2` - [abs](http://arxiv.org/abs/1910.11720v2) - [pdf](http://arxiv.org/pdf/1910.11720v2)

> Mathematical epidemiological models have a broad use, including both qualitative and quantitative applications. With the increasing availability of data, large-scale quantitative disease spread models can nowadays be formulated. Such models have a great potential, e.g., in risk assessments in public health. Their main challenge is model parameterization given surveillance data, a problem which often limits their practical usage.   We offer a solution to this problem by developing a Bayesian methodology suitable to epidemiological models driven by network data. The greatest difficulty in obtaining a concentrated parameter posterior is the quality of surveillance data; disease measurements are often scarce and carry little information about the parameters. The often overlooked problem of the model's identifiability therefore needs to be addressed, and we do so using a hierarchy of increasingly realistic known truth experiments.   Our proposed Bayesian approach performs convincingly across all our synthetic tests. From pathogen measurements of shiga toxin-producing Escherichia coli O157 in Swedish cattle, we are able to produce an accurate statistical model of first-principles confronted with data. Within this model we explore the potential of a Bayesian public health framework by assessing the efficiency of disease detection and -intervention scenarios.

</details>

<details>

<summary>2020-02-07 12:10:56 - A General Framework for Uncertainty Estimation in Deep Learning</summary>

- *Antonio Loquercio, Mattia Segù, Davide Scaramuzza*

- `1907.06890v4` - [abs](http://arxiv.org/abs/1907.06890v4) - [pdf](http://arxiv.org/pdf/1907.06890v4)

> Neural networks predictions are unreliable when the input sample is out of the training distribution or corrupted by noise. Being able to detect such failures automatically is fundamental to integrate deep learning algorithms into robotics. Current approaches for uncertainty estimation of neural networks require changes to the network and optimization process, typically ignore prior knowledge about the data, and tend to make over-simplifying assumptions which underestimate uncertainty. To address these limitations, we propose a novel framework for uncertainty estimation. Based on Bayesian belief networks and Monte-Carlo sampling, our framework not only fully models the different sources of prediction uncertainty, but also incorporates prior data information, e.g. sensor noise. We show theoretically that this gives us the ability to capture uncertainty better than existing methods. In addition, our framework has several desirable properties: (i) it is agnostic to the network architecture and task; (ii) it does not require changes in the optimization process; (iii) it can be applied to already trained architectures. We thoroughly validate the proposed framework through extensive experiments on both computer vision and control tasks, where we outperform previous methods by up to 23% in accuracy.

</details>

<details>

<summary>2020-02-07 14:48:16 - Noisy-Input Entropy Search for Efficient Robust Bayesian Optimization</summary>

- *Lukas P. Fröhlich, Edgar D. Klenske, Julia Vinogradska, Christian Daniel, Melanie N. Zeilinger*

- `2002.02820v1` - [abs](http://arxiv.org/abs/2002.02820v1) - [pdf](http://arxiv.org/pdf/2002.02820v1)

> We consider the problem of robust optimization within the well-established Bayesian optimization (BO) framework. While BO is intrinsically robust to noisy evaluations of the objective function, standard approaches do not consider the case of uncertainty about the input parameters. In this paper, we propose Noisy-Input Entropy Search (NES), a novel information-theoretic acquisition function that is designed to find robust optima for problems with both input and measurement noise. NES is based on the key insight that the robust objective in many cases can be modeled as a Gaussian process, however, it cannot be observed directly. We evaluate NES on several benchmark problems from the optimization literature and from engineering. The results show that NES reliably finds robust optima, outperforming existing methods from the literature on all benchmarks.

</details>

<details>

<summary>2020-02-07 15:07:53 - Bayesian inference of quasi-linear radial diffusion parameters using Van Allen Probes</summary>

- *Rakesh Sarma, Mandar Chandorkar, Irina Zhelavskaya, Yuri Shprits, Alexander Drozdov, Enrico Camporeale*

- `2002.02832v1` - [abs](http://arxiv.org/abs/2002.02832v1) - [pdf](http://arxiv.org/pdf/2002.02832v1)

> The Van Allen radiation belts in the magnetosphere have been extensively studied using models based on radial diffusion theory, which is based on a quasi-linear approach with prescribed inner and outer boundary conditions. The 1-d diffusion model requires the knowledge of a diffusion coefficient and an electron loss timescale, which are typically parameterized in terms of various quantities such as the spatial ($L$) coordinate or a geomagnetic index (for example, $Kp$). These terms are empirically derived, not directly measurable, and hence are not known precisely, due to the inherent non-linearity of the process and the variable boundary conditions. In this work, we demonstrate a probabilistic approach by inferring the values of the diffusion and loss term parameters, along with their uncertainty, in a Bayesian framework, where identification is obtained using the Van Allen Probe measurements. Our results show that the probabilistic approach statistically improves the performance of the model, compared to the parameterization employed in the literature.

</details>

<details>

<summary>2020-02-07 15:29:22 - Assessing the Adversarial Robustness of Monte Carlo and Distillation Methods for Deep Bayesian Neural Network Classification</summary>

- *Meet P. Vadera, Satya Narayan Shukla, Brian Jalaian, Benjamin M. Marlin*

- `2002.02842v1` - [abs](http://arxiv.org/abs/2002.02842v1) - [pdf](http://arxiv.org/pdf/2002.02842v1)

> In this paper, we consider the problem of assessing the adversarial robustness of deep neural network models under both Markov chain Monte Carlo (MCMC) and Bayesian Dark Knowledge (BDK) inference approximations. We characterize the robustness of each method to two types of adversarial attacks: the fast gradient sign method (FGSM) and projected gradient descent (PGD). We show that full MCMC-based inference has excellent robustness, significantly outperforming standard point estimation-based learning. On the other hand, BDK provides marginal improvements. As an additional contribution, we present a storage-efficient approach to computing adversarial examples for large Monte Carlo ensembles using both the FGSM and PGD attacks.

</details>

<details>

<summary>2020-02-07 17:47:07 - Extended Stochastic Gradient MCMC for Large-Scale Bayesian Variable Selection</summary>

- *Qifan Song, Yan Sun, Mao Ye, Faming Liang*

- `2002.02919v1` - [abs](http://arxiv.org/abs/2002.02919v1) - [pdf](http://arxiv.org/pdf/2002.02919v1)

> Stochastic gradient Markov chain Monte Carlo (MCMC) algorithms have received much attention in Bayesian computing for big data problems, but they are only applicable to a small class of problems for which the parameter space has a fixed dimension and the log-posterior density is differentiable with respect to the parameters. This paper proposes an extended stochastic gradient MCMC lgoriathm which, by introducing appropriate latent variables, can be applied to more general large-scale Bayesian computing problems, such as those involving dimension jumping and missing data. Numerical studies show that the proposed algorithm is highly scalable and much more efficient than traditional MCMC algorithms. The proposed algorithms have much alleviated the pain of Bayesian methods in big data computing.

</details>

<details>

<summary>2020-02-07 20:28:28 - Bayesian Methods for the Analysis of Early-Phase Oncology Basket Trials with Information Borrowing across Cancer Types</summary>

- *Jin Jin, Marie-Karelle Riviere, Xiaodong Luo, Yingwen Dong*

- `2002.03007v1` - [abs](http://arxiv.org/abs/2002.03007v1) - [pdf](http://arxiv.org/pdf/2002.03007v1)

> Research in oncology has changed the focus from histological properties of tumors in a specific organ to a specific genomic aberration potentially shared by multiple cancer types. This motivates the basket trial, which assesses the efficacy of treatment simultaneously on multiple cancer types that have a common aberration. Although the assumption of homogeneous treatment effects seems reasonable given the shared aberration, in reality, the treatment effect may vary by cancer type, and potentially only a subgroup of the cancer types respond to the treatment. Various approaches have been proposed to increase the trial power by borrowing information across cancer types, which, however, tend to inflate the type I error rate. In this paper, we review some representative Bayesian information borrowing methods for the analysis of early-phase basket trials. We then propose a novel method called the Bayesian hierarchical model with a correlated prior (CBHM), which conducts more flexible borrowing across cancer types according to sample similarity. We did simulation studies to compare CBHM with independent analysis and three information borrowing approaches: the conventional Bayesian hierarchical model, the EXNEX approach and Liu's two-stage approach. Simulation results show that all information borrowing approaches substantially improve the power of independent analysis if a large proportion of the cancer types truly respond to the treatment. Our proposed CBHM approach shows an advantage over the existing information borrowing approaches, with a power similar to that of EXNEX or Liu's approach, but the potential to provide substantially better control of type I error rate.

</details>

<details>

<summary>2020-02-08 14:47:59 - On a scalable entropic breaching of the overfitting barrier in machine learning</summary>

- *Illia Horenko*

- `2002.03176v1` - [abs](http://arxiv.org/abs/2002.03176v1) - [pdf](http://arxiv.org/pdf/2002.03176v1)

> Overfitting and treatment of "small data" are among the most challenging problems in the machine learning (ML), when a relatively small data statistics size $T$ is not enough to provide a robust ML fit for a relatively large data feature dimension $D$. Deploying a massively-parallel ML analysis of generic classification problems for different $D$ and $T$, existence of statistically-significant linear overfitting barriers for common ML methods is demonstrated. For example, these results reveal that for a robust classification of bioinformatics-motivated generic problems with the Long Short-Term Memory deep learning classifier (LSTM) one needs in a best case a statistics $T$ that is at least 13.8 times larger then the feature dimension $D$. It is shown that this overfitting barrier can be breached at a $10^{-12}$ fraction of the computational cost by means of the entropy-optimal Scalable Probabilistic Approximations algorithm (eSPA), performing a joint solution of the entropy-optimal Bayesian network inference and feature space segmentation problems. Application of eSPA to experimental single cell RNA sequencing data exhibits a 30-fold classification performance boost when compared to standard bioinformatics tools - and a 7-fold boost when compared to the deep learning LSTM classifier.

</details>

<details>

<summary>2020-02-08 23:40:09 - A comparison of Bayesian accelerated failure time models with spatially varying coefficients</summary>

- *Guanyu Hu, Yishu Xue, Fred Huffer*

- `2002.03251v1` - [abs](http://arxiv.org/abs/2002.03251v1) - [pdf](http://arxiv.org/pdf/2002.03251v1)

> The accelerated failure time (AFT) model is a commonly used tool in analyzing survival data. In public health studies, data is often collected from medical service providers in different locations. Survival rates from different locations often present geographically varying patterns. In this paper, we focus on the accelerated failure time model with spatially varying coefficients. We compare three types of the priors for spatially varying coefficients. A model selection criterion, logarithm of the pseudo-marginal likelihood (LPML), is developed to assess the fit of AFT model with different priors. Extensive simulation studies are carried out to examine the empirical performance of the proposed methods. Finally, we apply our model to SEER data on prostate cancer in Louisiana and demonstrate the existence of spatially varying effects on survival rates from prostate cancer data.

</details>

<details>

<summary>2020-02-09 06:23:53 - BINOCULARS for Efficient, Nonmyopic Sequential Experimental Design</summary>

- *Shali Jiang, Henry Chai, Javier Gonzalez, Roman Garnett*

- `1909.04568v3` - [abs](http://arxiv.org/abs/1909.04568v3) - [pdf](http://arxiv.org/pdf/1909.04568v3)

> Finite-horizon sequential experimental design (SED) arises naturally in many contexts, including hyperparameter tuning in machine learning among more traditional settings. Computing the optimal policy for such problems requires solving Bellman equations, which are generally intractable. Most existing work resorts to severely myopic approximations by limiting the decision horizon to only a single time-step, which can underweight exploration in favor of exploitation. We present BINOCULARS: Batch-Informed NOnmyopic Choices, Using Long-horizons for Adaptive, Rapid SED, a general framework for deriving efficient, nonmyopic approximations to the optimal experimental policy. Our key idea is simple and surprisingly effective: we first compute a one-step optimal batch of experiments, then select a single point from this batch to evaluate. We realize BINOCULARS for Bayesian optimization and Bayesian quadrature -- two notable SED problems with radically different objectives -- and demonstrate that BINOCULARS significantly outperforms myopic alternatives in real-world scenarios.

</details>

<details>

<summary>2020-02-09 09:34:06 - Conciliation of Bayes and Pointwise Quantum State Estimation: Asymptotic information bounds in quantum statistics</summary>

- *Richard D. Gill*

- `0512443v5` - [abs](http://arxiv.org/abs/0512443v5) - [pdf](http://arxiv.org/pdf/math/0512443v5)

> We derive an asymptotic lower bound on the Bayes risk when N identical quantum systems whose state depends on a vector of unknown parameters are jointly measured in an arbitrary way and the parameters of interest estimated on the basis of the resulting data. The bound is an integrated version of a quantum Cram\'er-Rao bound due to Holevo (1982), and it thereby links the fixed N exact Bayesian optimality usually pursued in the physics literature with the pointwise asymptotic optimality favoured in classical mathematical statistics. By heuristic arguments the bound can be expected to be sharp. This does turn out to be the case in various important examples, where it can be used to prove asymptotic optimality of interesting and useful measurement-and-estimation schemes. On the way we obtain a new family of "dual Holevo bounds" of independent interest.

</details>

<details>

<summary>2020-02-10 07:19:52 - Hierarchical Gaussian Process Priors for Bayesian Neural Network Weights</summary>

- *Theofanis Karaletsos, Thang D. Bui*

- `2002.04033v1` - [abs](http://arxiv.org/abs/2002.04033v1) - [pdf](http://arxiv.org/pdf/2002.04033v1)

> Probabilistic neural networks are typically modeled with independent weight priors, which do not capture weight correlations in the prior and do not provide a parsimonious interface to express properties in function space. A desirable class of priors would represent weights compactly, capture correlations between weights, facilitate calibrated reasoning about uncertainty, and allow inclusion of prior knowledge about the function space such as periodicity or dependence on contexts such as inputs. To this end, this paper introduces two innovations: (i) a Gaussian process-based hierarchical model for network weights based on unit embeddings that can flexibly encode correlated weight structures, and (ii) input-dependent versions of these weight priors that can provide convenient ways to regularize the function space through the use of kernels defined on contextual inputs. We show these models provide desirable test-time uncertainty estimates on out-of-distribution data, demonstrate cases of modeling inductive biases for neural networks with kernels which help both interpolation and extrapolation from training data, and demonstrate competitive predictive performance on an active learning benchmark.

</details>

<details>

<summary>2020-02-10 08:41:03 - The Tensor Brain: Semantic Decoding for Perception and Memory</summary>

- *Volker Tresp, Sahand Sharifzadeh, Dario Konopatzki, Yunpu Ma*

- `2001.11027v3` - [abs](http://arxiv.org/abs/2001.11027v3) - [pdf](http://arxiv.org/pdf/2001.11027v3)

> We analyse perception and memory, using mathematical models for knowledge graphs and tensors, to gain insights into the corresponding functionalities of the human mind. Our discussion is based on the concept of propositional sentences consisting of \textit{subject-predicate-object} (SPO) triples for expressing elementary facts. SPO sentences are the basis for most natural languages but might also be important for explicit perception and declarative memories, as well as intra-brain communication and the ability to argue and reason. A set of SPO sentences can be described as a knowledge graph, which can be transformed into an adjacency tensor. We introduce tensor models, where concepts have dual representations as indices and associated embeddings, two constructs we believe are essential for the understanding of implicit and explicit perception and memory in the brain. We argue that a biological realization of perception and memory imposes constraints on information processing. In particular, we propose that explicit perception and declarative memories require a semantic decoder, which, in a simple realization, is based on four layers: First, a sensory memory layer, as a buffer for sensory input, second, an index layer representing concepts, third, a memoryless representation layer for the broadcasting of information ---the "blackboard", or the "canvas" of the brain--- and fourth, a working memory layer as a processing center and data buffer. We discuss the operations of the four layers and relate them to the global workspace theory. In a Bayesian brain interpretation, semantic memory defines the prior for observable triple statements. We propose that ---in evolution and during development--- semantic memory, episodic memory, and natural language evolved as emergent properties in agents' process to gain a deeper understanding of sensory information.

</details>

<details>

<summary>2020-02-10 14:16:55 - Why Simple Quadrature is just as good as Monte Carlo</summary>

- *Kevin Vanslette, Abdullatif Al Alsheikh, Kamal Youcef-Toumi*

- `1908.00947v3` - [abs](http://arxiv.org/abs/1908.00947v3) - [pdf](http://arxiv.org/pdf/1908.00947v3)

> We motive and calculate Newton--Cotes quadrature integration variance and compare it directly with Monte Carlo (MC) integration variance. We find an equivalence between deterministic quadrature sampling and random MC sampling by noting that MC random sampling is statistically indistinguishable from a method that uses deterministic sampling on a randomly shuffled (permuted) function. We use this statistical equivalence to regularize the form of permissible Bayesian quadrature integration priors such that they are guaranteed to be objectively comparable with MC. This leads to the proof that simple quadrature methods have expected variances that are less than or equal to their corresponding theoretical MC integration variances. Separately, using Bayesian probability theory, we find that the theoretical standard deviations of the unbiased errors of simple Newton--Cotes composite quadrature integrations improve over their worst case errors by an extra dimension independent factor $\propto N^{-1/2}$. This dimension independent factor is validated in our simulations.

</details>

<details>

<summary>2020-02-10 14:45:31 - Corrected score methods for estimating Bayesian networks with error-prone nodes</summary>

- *Xianzheng Huang, Hongmei Zhang*

- `2002.03817v1` - [abs](http://arxiv.org/abs/2002.03817v1) - [pdf](http://arxiv.org/pdf/2002.03817v1)

> Motivated by inferring cellular signaling networks using noisy flow cytometry data, we develop procedures to draw inference for Bayesian networks based on error-prone data. Two methods for inferring causal relationships between nodes in a network are proposed based on penalized estimation methods that account for measurement error and encourage sparsity. We discuss consistency of the proposed network estimators and develop an approach for selecting the tuning parameter in the penalized estimation methods. Empirical studies are carried out to compare the proposed methods and a naive method that ignores measurement error with applications to synthetic data and to single cell flow cytometry data.

</details>

<details>

<summary>2020-02-10 16:33:42 - Playing to Learn Better: Repeated Games for Adversarial Learning with Multiple Classifiers</summary>

- *Prithviraj Dasgupta, Joseph B. Collins, Michael McCarrick*

- `2002.03924v1` - [abs](http://arxiv.org/abs/2002.03924v1) - [pdf](http://arxiv.org/pdf/2002.03924v1)

> We consider the problem of prediction by a machine learning algorithm, called learner, within an adversarial learning setting. The learner's task is to correctly predict the class of data passed to it as a query. However, along with queries containing clean data, the learner could also receive malicious or adversarial queries from an adversary. The objective of the adversary is to evade the learner's prediction mechanism by sending adversarial queries that result in erroneous class prediction by the learner, while the learner's objective is to reduce the incorrect prediction of these adversarial queries without degrading the prediction quality of clean queries. We propose a game theory-based technique called a Repeated Bayesian Sequential Game where the learner interacts repeatedly with a model of the adversary using self play to determine the distribution of adversarial versus clean queries. It then strategically selects a classifier from a set of pre-trained classifiers that balances the likelihood of correct prediction for the query along with reducing the costs to use the classifier. We have evaluated our proposed technique using clean and adversarial text data with deep neural network-based classifiers and shown that the learner can select an appropriate classifier that is commensurate with the query type (clean or adversarial) while remaining aware of the cost to use the classifier.

</details>

<details>

<summary>2020-02-11 00:34:32 - The role of intrinsic dimension in high-resolution player tracking data -- Insights in basketball</summary>

- *Edgar Santos-Fernandez, Francesco Denti, Kerrie Mengersen, Antonietta Mira*

- `2002.04148v1` - [abs](http://arxiv.org/abs/2002.04148v1) - [pdf](http://arxiv.org/pdf/2002.04148v1)

> A new range of statistical analysis has emerged in sports after the introduction of the high-resolution player tracking technology, specifically in basketball. However, this high dimensional data is often challenging for statistical inference and decision making. In this article, we employ Hidalgo, a state-of-the-art Bayesian mixture model that allows the estimation of heterogeneous intrinsic dimensions (ID) within a dataset and propose some theoretical enhancements. ID results can be interpreted as indicators of variability and complexity of basketball plays and games. This technique allows classification and clustering of NBA basketball player's movement and shot charts data. Analyzing movement data, Hidalgo identifies key stages of offensive actions such as creating space for passing, preparation/shooting and following through. We found that the ID value spikes reaching a peak between 4 and 8 seconds in the offensive part of the court after which it declines. In shot charts, we obtained groups of shots that produce substantially higher and lower successes. Overall, game-winners tend to have a larger intrinsic dimension which is an indication of more unpredictability and unique shot placements. Similarly, we found higher ID values in plays when the score margin is small compared to large margin ones. These outcomes could be exploited by coaches to obtain better offensive/defensive results.

</details>

<details>

<summary>2020-02-11 02:14:58 - Multi-objective Bayesian Optimization using Pareto-frontier Entropy</summary>

- *Shinya Suzuki, Shion Takeno, Tomoyuki Tamura, Kazuki Shitara, Masayuki Karasuyama*

- `1906.00127v2` - [abs](http://arxiv.org/abs/1906.00127v2) - [pdf](http://arxiv.org/pdf/1906.00127v2)

> This paper studies an entropy-based multi-objective Bayesian optimization (MBO). The entropy search is successful approach to Bayesian optimization. However, for MBO, existing entropy-based methods ignore trade-off among objectives or introduce unreliable approximations. We propose a novel entropy-based MBO called Pareto-frontier entropy search (PFES) by considering the entropy of Pareto-frontier, which is an essential notion of the optimality of the multi-objective problem. Our entropy can incorporate the trade-off relation of the optimal values, and further, we derive an analytical formula without introducing additional approximations or simplifications to the standard entropy search setting. We also show that our entropy computation is practically feasible by using a recursive decomposition technique which has been known in studies of the Pareto hyper-volume computation. Besides the usual MBO setting, in which all the objectives are simultaneously observed, we also consider the "decoupled" setting, in which the objective functions can be observed separately. PFES can easily adapt to the decoupled setting by considering the entropy of the marginal density for each output dimension. This approach incorporates dependency among objectives conditioned on Pareto-frontier, which is ignored by the existing method. Our numerical experiments show effectiveness of PFES through several benchmark datasets.

</details>

<details>

<summary>2020-02-11 15:21:24 - Generalized Poisson Difference Autoregressive Processes</summary>

- *Giulia Carallo, Roberto Casarin, Christian P. Robert*

- `2002.04470v1` - [abs](http://arxiv.org/abs/2002.04470v1) - [pdf](http://arxiv.org/pdf/2002.04470v1)

> This paper introduces a new stochastic process with values in the set Z of integers with sign. The increments of process are Poisson differences and the dynamics has an autoregressive structure. We study the properties of the process and exploit the thinning representation to derive stationarity conditions and the stationary distribution of the process. We provide a Bayesian inference method and an efficient posterior approximation procedure based on Monte Carlo. Numerical illustrations on both simulated and real data show the effectiveness of the proposed inference.

</details>

<details>

<summary>2020-02-11 16:05:28 - Functional Regularisation for Continual Learning with Gaussian Processes</summary>

- *Michalis K. Titsias, Jonathan Schwarz, Alexander G. de G. Matthews, Razvan Pascanu, Yee Whye Teh*

- `1901.11356v4` - [abs](http://arxiv.org/abs/1901.11356v4) - [pdf](http://arxiv.org/pdf/1901.11356v4)

> We introduce a framework for Continual Learning (CL) based on Bayesian inference over the function space rather than the parameters of a deep neural network. This method, referred to as functional regularisation for Continual Learning, avoids forgetting a previous task by constructing and memorising an approximate posterior belief over the underlying task-specific function. To achieve this we rely on a Gaussian process obtained by treating the weights of the last layer of a neural network as random and Gaussian distributed. Then, the training algorithm sequentially encounters tasks and constructs posterior beliefs over the task-specific functions by using inducing point sparse Gaussian process methods. At each step a new task is first learnt and then a summary is constructed consisting of (i) inducing inputs -- a fixed-size subset of the task inputs selected such that it optimally represents the task -- and (ii) a posterior distribution over the function values at these inputs. This summary then regularises learning of future tasks, through Kullback-Leibler regularisation terms. Our method thus unites approaches focused on (pseudo-)rehearsal with those derived from a sequential Bayesian inference perspective in a principled way, leading to strong results on accepted benchmarks.

</details>

<details>

<summary>2020-02-11 23:38:01 - Generalized Bayesian Cramér-Rao Inequality via Information Geometry of Relative $α$-Entropy</summary>

- *Kumar Vijay Mishra, M. Ashok Kumar*

- `2002.04732v1` - [abs](http://arxiv.org/abs/2002.04732v1) - [pdf](http://arxiv.org/pdf/2002.04732v1)

> The relative $\alpha$-entropy is the R\'enyi analog of relative entropy and arises prominently in information-theoretic problems. Recent information geometric investigations on this quantity have enabled the generalization of the Cram\'{e}r-Rao inequality, which provides a lower bound for the variance of an estimator of an escort of the underlying parametric probability distribution. However, this framework remains unexamined in the Bayesian framework. In this paper, we propose a general Riemannian metric based on relative $\alpha$-entropy to obtain a generalized Bayesian Cram\'{e}r-Rao inequality. This establishes a lower bound for the variance of an unbiased estimator for the $\alpha$-escort distribution starting from an unbiased estimator for the underlying distribution. We show that in the limiting case when the entropy order approaches unity, this framework reduces to the conventional Bayesian Cram\'{e}r-Rao inequality. Further, in the absence of priors, the same framework yields the deterministic Cram\'{e}r-Rao inequality.

</details>

<details>

<summary>2020-02-12 15:01:15 - Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms</summary>

- *Christian A. Naesseth, Francisco J. R. Ruiz, Scott W. Linderman, David M. Blei*

- `1610.05683v3` - [abs](http://arxiv.org/abs/1610.05683v3) - [pdf](http://arxiv.org/pdf/1610.05683v3)

> Variational inference using the reparameterization trick has enabled large-scale approximate Bayesian inference in complex probabilistic models, leveraging stochastic optimization to sidestep intractable expectations. The reparameterization trick is applicable when we can simulate a random variable by applying a differentiable deterministic function on an auxiliary random variable whose distribution is fixed. For many distributions of interest (such as the gamma or Dirichlet), simulation of random variables relies on acceptance-rejection sampling. The discontinuity introduced by the accept-reject step means that standard reparameterization tricks are not applicable. We propose a new method that lets us leverage reparameterization gradients even when variables are outputs of a acceptance-rejection sampling algorithm. Our approach enables reparameterization on a larger class of variational distributions. In several studies of real and synthetic data, we show that the variance of the estimator of the gradient is significantly lower than other state-of-the-art methods. This leads to faster convergence of stochastic gradient variational inference.

</details>

<details>

<summary>2020-02-12 17:34:16 - Adaptive Tuning Of Hamiltonian Monte Carlo Within Sequential Monte Carlo</summary>

- *Alexander Buchholz, Nicolas Chopin, Pierre E. Jacob*

- `1808.07730v2` - [abs](http://arxiv.org/abs/1808.07730v2) - [pdf](http://arxiv.org/pdf/1808.07730v2)

> Sequential Monte Carlo (SMC) samplers form an attractive alternative to MCMC for Bayesian computation. However, their performance depends strongly on the Markov kernels used to rejuvenate particles. We discuss how to calibrate automatically (using the current particles) Hamiltonian Monte Carlo kernels within SMC. To do so, we build upon the adaptive SMC approach of Fearnhead and Taylor (2013), and we also suggest alternative methods. We illustrate the advantages of using HMC kernels within an SMC sampler via an extensive numerical study.

</details>

<details>

<summary>2020-02-12 18:57:14 - Learnable Bernoulli Dropout for Bayesian Deep Learning</summary>

- *Shahin Boluki, Randy Ardywibowo, Siamak Zamani Dadaneh, Mingyuan Zhou, Xiaoning Qian*

- `2002.05155v1` - [abs](http://arxiv.org/abs/2002.05155v1) - [pdf](http://arxiv.org/pdf/2002.05155v1)

> In this work, we propose learnable Bernoulli dropout (LBD), a new model-agnostic dropout scheme that considers the dropout rates as parameters jointly optimized with other model parameters. By probabilistic modeling of Bernoulli dropout, our method enables more robust prediction and uncertainty quantification in deep models. Especially, when combined with variational auto-encoders (VAEs), LBD enables flexible semi-implicit posterior representations, leading to new semi-implicit VAE~(SIVAE) models. We solve the optimization for training with respect to the dropout parameters using Augment-REINFORCE-Merge (ARM), an unbiased and low-variance gradient estimator. Our experiments on a range of tasks show the superior performance of our approach compared with other commonly used dropout schemes. Overall, LBD leads to improved accuracy and uncertainty estimates in image classification and semantic segmentation. Moreover, using SIVAE, we can achieve state-of-the-art performance on collaborative filtering for implicit feedback on several public datasets.

</details>

<details>

<summary>2020-02-12 20:03:14 - Maximizing Overall Diversity for Improved Uncertainty Estimates in Deep Ensembles</summary>

- *Siddhartha Jain, Ge Liu, Jonas Mueller, David Gifford*

- `1906.07380v2` - [abs](http://arxiv.org/abs/1906.07380v2) - [pdf](http://arxiv.org/pdf/1906.07380v2)

> The inaccuracy of neural network models on inputs that do not stem from the training data distribution is both problematic and at times unrecognized. Model uncertainty estimation can address this issue, where uncertainty estimates are often based on the variation in predictions produced by a diverse ensemble of models applied to the same input. Here we describe Maximize Overall Diversity (MOD), a straightforward approach to improve ensemble-based uncertainty estimates by encouraging larger overall diversity in ensemble predictions across all possible inputs that might be encountered in the future. When applied to various neural network ensembles, MOD significantly improves predictive performance for out-of-distribution test examples without sacrificing in-distribution performance on 38 Protein-DNA binding regression datasets, 9 UCI datasets, and the IMDB-Wiki image dataset. Across many Bayesian optimization tasks, the performance of UCB acquisition is also greatly improved by leveraging MOD uncertainty estimates.

</details>

<details>

<summary>2020-02-12 21:03:12 - Bayesian high-dimensional linear regression with generic spike-and-slab priors</summary>

- *Bai Jiang, Qiang Sun*

- `1912.08993v2` - [abs](http://arxiv.org/abs/1912.08993v2) - [pdf](http://arxiv.org/pdf/1912.08993v2)

> Spike-and-slab priors are popular Bayesian solutions for high-dimensional linear regression problems. Previous theoretical studies on spike-and-slab methods focus on specific prior formulations and use prior-dependent conditions and analyses, and thus can not be generalized directly. In this paper, we propose a class of generic spike-and-slab priors and develop a unified framework to rigorously assess their theoretical properties. Technically, we provide general conditions under which generic spike-and-slab priors can achieve the nearly-optimal posterior contraction rate and the model selection consistency. Our results include those of Narisetty and He (2014) and Castillo et al. (2015) as special cases.

</details>

<details>

<summary>2020-02-12 21:23:53 - Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling</summary>

- *Xiaocheng Shang, Zhanxing Zhu, Benedict Leimkuhler, Amos J. Storkey*

- `1510.08692v2` - [abs](http://arxiv.org/abs/1510.08692v2) - [pdf](http://arxiv.org/pdf/1510.08692v2)

> Monte Carlo sampling for Bayesian posterior inference is a common approach used in machine learning. The Markov Chain Monte Carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations (SDEs). These SDEs are guaranteed to leave invariant the required posterior distribution. An area of current research addresses the computational benefits of stochastic gradient methods in this setting. Existing techniques rely on estimating the variance or covariance of the subsampling error, and typically assume constant variance. In this article, we propose a covariance-controlled adaptive Langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution. The proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications.

</details>

<details>

<summary>2020-02-13 03:16:57 - Multi-fidelity Bayesian Optimization with Max-value Entropy Search and its parallelization</summary>

- *Shion Takeno, Hitoshi Fukuoka, Yuhki Tsukada, Toshiyuki Koyama, Motoki Shiga, Ichiro Takeuchi, Masayuki Karasuyama*

- `1901.08275v2` - [abs](http://arxiv.org/abs/1901.08275v2) - [pdf](http://arxiv.org/pdf/1901.08275v2)

> In a standard setting of Bayesian optimization (BO), the objective function evaluation is assumed to be highly expensive. Multi-fidelity Bayesian optimization (MFBO) accelerates BO by incorporating lower fidelity observations available with a lower sampling cost. In this paper, we focus on the information-based approach, which is a popular and empirically successful approach in BO. For MFBO, however, existing information-based methods are plagued by difficulty in estimating the information gain. We propose an approach based on max-value entropy search (MES), which greatly facilitates computations by considering the entropy of the optimal function value instead of the optimal input point. We show that, in our multi-fidelity MES (MF-MES), most of additional computations, compared with usual MES, is reduced to analytical computations. Although an additional numerical integration is necessary for the information across different fidelities, this is only in one dimensional space, which can be performed efficiently and accurately. Further, we also propose parallelization of MF-MES. Since there exist a variety of different sampling costs, queries typically occur asynchronously in MFBO. We show that similar simple computations can be derived for asynchronous parallel MFBO. We demonstrate effectiveness of our approach by using benchmark datasets and a real-world application to materials science data.

</details>

<details>

<summary>2020-02-13 08:11:18 - Long-term prediction intervals of economic time series</summary>

- *Marek Chudy, Sayar Karmakar, Wei Biao Wu*

- `2002.05384v1` - [abs](http://arxiv.org/abs/2002.05384v1) - [pdf](http://arxiv.org/pdf/2002.05384v1)

> We construct long-term prediction intervals for time-aggregated future values of univariate economic time series. We propose computational adjustments of the existing methods to improve coverage probability under a small sample constraint. A pseudo-out-of-sample evaluation shows that our methods perform at least as well as selected alternative methods based on model-implied Bayesian approaches and bootstrapping. Our most successful method yields prediction intervals for eight macroeconomic indicators over a horizon spanning several decades.

</details>

<details>

<summary>2020-02-13 21:41:38 - Stochastic Neural Network with Kronecker Flow</summary>

- *Chin-Wei Huang, Ahmed Touati, Pascal Vincent, Gintare Karolina Dziugaite, Alexandre Lacoste, Aaron Courville*

- `1906.04282v2` - [abs](http://arxiv.org/abs/1906.04282v2) - [pdf](http://arxiv.org/pdf/1906.04282v2)

> Recent advances in variational inference enable the modelling of highly structured joint distributions, but are limited in their capacity to scale to the high-dimensional setting of stochastic neural networks. This limitation motivates a need for scalable parameterizations of the noise generation process, in a manner that adequately captures the dependencies among the various parameters. In this work, we address this need and present the Kronecker Flow, a generalization of the Kronecker product to invertible mappings designed for stochastic neural networks. We apply our method to variational Bayesian neural networks on predictive tasks, PAC-Bayes generalization bound estimation, and approximate Thompson sampling in contextual bandits. In all setups, our methods prove to be competitive with existing methods and better than the baselines.

</details>

<details>

<summary>2020-02-13 21:54:19 - Minimaxity and Limits of Risks Ratios of Shrinkage Estimators of a Multivariate Normal Mean in the Bayesian Case</summary>

- *Abdenour Hamdaoui, Abdelkader Benkhaled, Nadia Mezouar*

- `2002.05792v1` - [abs](http://arxiv.org/abs/2002.05792v1) - [pdf](http://arxiv.org/pdf/2002.05792v1)

> In this article, we consider two forms of shrinkage estimators of the mean $\theta$ of a multivariate normal distribution $X\sim N_{p}\left(\theta, \sigma^{2}I_{p}\right)$ where $\sigma^{2}$ is unknown. We take the prior law $\theta \sim N_{p}\left(\upsilon, \tau^{2}I_{p}\right)$ and we constuct a Modified Bayes estimator $\delta_{B}^{\ast}$ and an Empirical Modified Bayes estimator $\delta_{EB}^{\ast}$. We are interested in studying the minimaxity and the limits of risks ratios of these estimators, to the maximum likelihood estimator $X$, when $n$ and $p$ tend to infinity.

</details>

<details>

<summary>2020-02-14 03:02:51 - A study on the leverage effect on financial series using a TAR model: a Bayesian approach</summary>

- *Oscar Espinosa, Fabio Nieto*

- `2002.05319v2` - [abs](http://arxiv.org/abs/2002.05319v2) - [pdf](http://arxiv.org/pdf/2002.05319v2)

> This research shows that under certain mathematical conditions, a threshold autoregressive model (TAR) can represent the leverage effect based on its conditional variance function. Furthermore, the analytical expressions for the third and fourth moment of the TAR model are obtained when it is weakly stationary.

</details>

<details>

<summary>2020-02-14 08:30:45 - Max-and-Smooth: a two-step approach for approximate Bayesian inference in latent Gaussian models</summary>

- *Birgir Hrafnkelsson, Stefan Siegert, Raphaël Huser, Haakon Bakka, Árni V. Jóhannesson*

- `1907.11969v2` - [abs](http://arxiv.org/abs/1907.11969v2) - [pdf](http://arxiv.org/pdf/1907.11969v2)

> With modern high-dimensional data, complex statistical models are necessary, requiring computationally feasible inference schemes. We introduce Max-and-Smooth, an approximate Bayesian inference scheme for a flexible class of latent Gaussian models (LGMs) where one or more of the likelihood parameters are modeled by latent additive Gaussian processes. Max-and-Smooth consists of two-steps. In the first step (Max), the likelihood function is approximated by a Gaussian density with mean and covariance equal to either (a) the maximum likelihood estimate and the inverse observed information, respectively, or (b) the mean and covariance of the normalized likelihood function. In the second step (Smooth), the latent parameters and hyperparameters are inferred and smoothed with the approximated likelihood function. The proposed method ensures that the uncertainty from the first step is correctly propagated to the second step. Since the approximated likelihood function is Gaussian, the approximate posterior density of the latent parameters of the LGM (conditional on the hyperparameters) is also Gaussian, thus facilitating efficient posterior inference in high dimensions. Furthermore, the approximate marginal posterior distribution of the hyperparameters is tractable, and as a result, the hyperparameters can be sampled independently of the latent parameters. In the case of a large number of independent data replicates, sparse precision matrices, and high-dimensional latent vectors, the speedup is substantial in comparison to an MCMC scheme that infers the posterior density from the exact likelihood function. The proposed inference scheme is demonstrated on one spatially referenced real dataset and on simulated data mimicking spatial, temporal, and spatio-temporal inference problems. Our results show that Max-and-Smooth is accurate and fast.

</details>

<details>

<summary>2020-02-14 10:19:20 - On Bayesian inference for the Extended Plackett-Luce model</summary>

- *Stephen R. Johnson, Daniel A. Henderson, Richard J. Boys*

- `2002.05953v1` - [abs](http://arxiv.org/abs/2002.05953v1) - [pdf](http://arxiv.org/pdf/2002.05953v1)

> The analysis of rank ordered data has a long history in the statistical literature across a diverse range of applications. In this paper we consider the Extended Plackett-Luce model that induces a flexible (discrete) distribution over permutations. The parameter space of this distribution is a combination of potentially high-dimensional discrete and continuous components and this presents challenges for parameter interpretability and also posterior computation. Particular emphasis is placed on the interpretation of the parameters in terms of observable quantities and we propose a general framework for preserving the mode of the prior predictive distribution. Posterior sampling is achieved using an effective simulation based approach that does not require imposing restrictions on the parameter space. Working in the Bayesian framework permits a natural representation of the posterior predictive distribution and we draw on this distribution to address the rank aggregation problem and also to identify potential lack of model fit. The flexibility of the Extended Plackett-Luce model along with the effectiveness of the proposed sampling scheme are demonstrated using several simulation studies and real data examples.

</details>

<details>

<summary>2020-02-14 10:46:53 - A Bayesian seamless phase I-II trial design with two stages for cancer clinical trials with drug combinations</summary>

- *José L. Jiménez, Sungjin Kim, Mourad Tighiouart*

- `1809.04348v3` - [abs](http://arxiv.org/abs/1809.04348v3) - [pdf](http://arxiv.org/pdf/1809.04348v3)

> The use of drug combinations in clinical trials is increasingly common during the last years since a more favorable therapeutic response may be obtained by combining drugs. In phase I clinical trials, most of the existing methodology recommends a one unique dose combination as "optimal", which may result in a subsequent failed phase II clinical trial since other dose combinations may present higher treatment efficacy for the same level of toxicity. We are particularly interested in the setting where it is necessary to wait a few cycles of therapy to observe an efficacy outcome and the phase I and II population of patients are different with respect to treatment efficacy. Under these circumstances, it is common practice to implement two-stage designs where a set of maximum tolerated dose combinations is selected in a first stage, and then studied in a second stage for treatment efficacy. In this article we present a new two-stage design for early phase clinical trials with drug combinations. In the first stage, binary toxicity data is used to guide the dose escalation and set the maximum tolerated dose combinations. In the second stage, we take the set of maximum tolerated dose combinations recommended from the first stage, which remains fixed along the entire second stage, and through adaptive randomization, we allocate subsequent cohorts of patients in dose combinations that are likely to have high posterior median time to progression. The methodology is assessed with extensive simulations and exemplified with a real trial.

</details>

<details>

<summary>2020-02-14 11:24:07 - Mendelian Randomization with Incomplete Exposure Data: a Bayesian Approach</summary>

- *Teresa Fazia, Leonardo Egidi, Burcu Ayoglu, Ashley Beecham, Pier Paolo Bitti, Anna Ticca, Hui Guo, Jacob L. McCauley, Peter Nilsson, Rosanna Asselta, Carlo Berzuini, Luisa Bernardinelli*

- `2002.04872v2` - [abs](http://arxiv.org/abs/2002.04872v2) - [pdf](http://arxiv.org/pdf/2002.04872v2)

> We expand Mendelian Randomization (MR) methodology to deal with randomly missing data on either the exposure or the outcome variable, and furthermore with data from nonindependent individuals (eg components of a family). Our method rests on the Bayesian MR framework proposed by Berzuini et al (2018), which we apply in a study of multiplex Multiple Sclerosis (MS) Sardinian families to characterise the role of certain plasma proteins in MS causation. The method is robust to presence of pleiotropic effects in an unknown number of instruments, and is able to incorporate inter-individual kinship information. Introduction of missing data allows us to overcome the bias introduced by the (reverse) effect of treatment (in MS cases) on level of protein. From a substantive point of view, our study results confirm recent suspicion that an increase in circulating IL12A and STAT4 protein levels does not cause an increase in MS risk, as originally believed, suggesting that these two proteins may not be suitable drug targets for MS.

</details>

<details>

<summary>2020-02-14 11:46:09 - Stepwise Model Selection for Sequence Prediction via Deep Kernel Learning</summary>

- *Yao Zhang, Daniel Jarrett, Mihaela van der Schaar*

- `2001.03898v3` - [abs](http://arxiv.org/abs/2001.03898v3) - [pdf](http://arxiv.org/pdf/2001.03898v3)

> An essential problem in automated machine learning (AutoML) is that of model selection. A unique challenge in the sequential setting is the fact that the optimal model itself may vary over time, depending on the distribution of features and labels available up to each point in time. In this paper, we propose a novel Bayesian optimization (BO) algorithm to tackle the challenge of model selection in this setting. This is accomplished by treating the performance at each time step as its own black-box function. In order to solve the resulting multiple black-box function optimization problem jointly and efficiently, we exploit potential correlations among black-box functions using deep kernel learning (DKL). To the best of our knowledge, we are the first to formulate the problem of stepwise model selection (SMS) for sequence prediction, and to design and demonstrate an efficient joint-learning algorithm for this purpose. Using multiple real-world datasets, we verify that our proposed method outperforms both standard BO and multi-objective BO algorithms on a variety of sequence prediction tasks.

</details>

<details>

<summary>2020-02-14 13:24:57 - Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization</summary>

- *Michael Volpp, Lukas P. Fröhlich, Kirsten Fischer, Andreas Doerr, Stefan Falkner, Frank Hutter, Christian Daniel*

- `1904.02642v5` - [abs](http://arxiv.org/abs/1904.02642v5) - [pdf](http://arxiv.org/pdf/1904.02642v5)

> Transferring knowledge across tasks to improve data-efficiency is one of the open key challenges in the field of global black-box optimization. Readily available algorithms are typically designed to be universal optimizers and, therefore, often suboptimal for specific tasks. We propose a novel transfer learning method to obtain customized optimizers within the well-established framework of Bayesian optimization, allowing our algorithm to utilize the proven generalization capabilities of Gaussian processes. Using reinforcement learning to meta-train an acquisition function (AF) on a set of related tasks, the proposed method learns to extract implicit structural information and to exploit it for improved data-efficiency. We present experiments on a simulation-to-real transfer task as well as on several synthetic functions and on two hyperparameter search problems. The results show that our algorithm (1) automatically identifies structural properties of objective functions from available source tasks or simulations, (2) performs favourably in settings with both scarse and abundant source data, and (3) falls back to the performance level of general AFs if no particular structure is present.

</details>

<details>

<summary>2020-02-14 15:40:10 - Bayesian Learning of Causal Relationships for System Reliability</summary>

- *Xuewen Yu, Jim Q. Smith, Linda Nichols*

- `2002.06084v1` - [abs](http://arxiv.org/abs/2002.06084v1) - [pdf](http://arxiv.org/pdf/2002.06084v1)

> Causal theory is now widely developed with many applications to medicine and public health. However within the discipline of reliability, although causation is a key concept in this field, there has been much less theoretical attention. In this paper, we will demonstrate how some aspects of established causal methodology can be translated via trees, and more specifically chain event graphs, into domain of reliability theory to help the probability modeling of failures. We further show how various domain specific concepts of causality particular to reliability can be imported into more generic causal algebras and so demonstrate how these disciplines can inform each other. This paper is informed by a detailed analysis of maintenance records associated with a large electrical distribution company. Causal hypotheses embedded within these natural language texts are extracted and analyzed using the new graphical framework we introduced here.

</details>

<details>

<summary>2020-02-14 16:18:02 - A Bayesian time-to-event pharmacokinetic model for phase I dose-escalation trials with multiple schedules</summary>

- *Burak Kürsad Günhan, Sebastian Weber, Tim Friede*

- `2002.06204v1` - [abs](http://arxiv.org/abs/2002.06204v1) - [pdf](http://arxiv.org/pdf/2002.06204v1)

> Phase I dose-escalation trials must be guided by a safety model in order to avoid exposing patients to unacceptably high risk of toxicities. Traditionally, these trials are based on one type of schedule. In more recent practice, however, there is often a need to consider more than one schedule, which means that in addition to the dose itself, the schedule needs to be varied in the trial. Hence, the aim is finding an acceptable dose-schedule combination. However, most established methods for dose-escalation trials are designed to escalate the dose only and ad-hoc choices must be made to adapt these to the more complicated setting of finding an acceptable dose-schedule combination. In this paper, we introduce a Bayesian time-to-event model which takes explicitly the dose amount and schedule into account through the use of pharmacokinetic principles. The model uses a time-varying exposure measure to account for the risk of a dose-limiting toxicity over time. The dose-schedule decisions are informed by an escalation with overdose control criterion. The model is formulated using interpretable parameters which facilitates the specification of priors. In a simulation study, we compared the proposed method with an existing method. The simulation study demonstrates that the proposed method yields similar or better results compared to an existing method in terms of recommending acceptable dose-schedule combinations, yet reduces the number of patients enrolled in most of scenarios. The \texttt{R} and \texttt{Stan} code to implement the proposed method is publicly available from Github (\url{https://github.com/gunhanb/TITEPK_code}).

</details>

<details>

<summary>2020-02-14 16:55:49 - Meta-Learning Mean Functions for Gaussian Processes</summary>

- *Vincent Fortuin, Heiko Strathmann, Gunnar Rätsch*

- `1901.08098v4` - [abs](http://arxiv.org/abs/1901.08098v4) - [pdf](http://arxiv.org/pdf/1901.08098v4)

> When fitting Bayesian machine learning models on scarce data, the main challenge is to obtain suitable prior knowledge and encode it into the model. Recent advances in meta-learning offer powerful methods for extracting such prior knowledge from data acquired in related tasks. When it comes to meta-learning in Gaussian process models, approaches in this setting have mostly focused on learning the kernel function of the prior, but not on learning its mean function. In this work, we explore meta-learning the mean function of a Gaussian process prior. We present analytical and empirical evidence that mean function learning can be useful in the meta-learning setting, discuss the risk of overfitting, and draw connections to other meta-learning approaches, such as model agnostic meta-learning and functional PCA.

</details>

<details>

<summary>2020-02-14 18:42:06 - Accounting for phenology in the analysis of animal movement</summary>

- *Henry R. Scharf, Mevin B. Hooten, Ryan R. Wilson, George M. Durner, Todd C. Atwood*

- `1806.09473v3` - [abs](http://arxiv.org/abs/1806.09473v3) - [pdf](http://arxiv.org/pdf/1806.09473v3)

> The analysis of animal tracking data provides an important source of scientific understanding and discovery in ecology. Observations of animal trajectories using telemetry devices provide researchers with information about the way animals interact with their environment and each other. For many species, specific geographical features in the landscape can have a strong effect on behavior. Such features may correspond to a single point (e.g., dens or kill sites), or to higher-dimensional subspaces (e.g., rivers or lakes). Features may be relatively static in time (e.g., coastlines or home-range centers), or may be dynamic (e.g., sea ice extent or areas of high-quality forage for herbivores). We introduce a novel model for animal movement that incorporates active selection for dynamic features in a landscape.   Our approach is motivated by the study of polar bear (Ursus maritimus) movement. During the sea ice melt season, polar bears spend much of their time on sea ice above shallow, biologically productive water where they hunt seals. The changing distribution and characteristics of sea ice throughout the late spring through early fall means that the location of valuable habitat is constantly shifting. We develop a model for the movement of polar bears that accounts for the effect of this important landscape feature. We introduce a two-stage procedure for approximate Bayesian inference that allows us to analyze over 300,000 observed locations of 186 polar bears from 2012--2016. We use our proposed model to answer a particular question posed by wildlife managers who seek to cluster polar bears from the Beaufort and Chukchi seas into sub-populations.

</details>

<details>

<summary>2020-02-15 08:37:54 - Variance reduction for Markov chains with application to MCMC</summary>

- *D. Belomestny, L. Iosipoi, E. Moulines, A. Naumov, S. Samsonov*

- `1910.03643v2` - [abs](http://arxiv.org/abs/1910.03643v2) - [pdf](http://arxiv.org/pdf/1910.03643v2)

> In this paper we propose a novel variance reduction approach for additive functionals of Markov chains based on minimization of an estimate for the asymptotic variance of these functionals over suitable classes of control variates. A distinctive feature of the proposed approach is its ability to significantly reduce the overall finite sample variance. This feature is theoretically demonstrated by means of a deep non asymptotic analysis of a variance reduced functional as well as by a thorough simulation study. In particular we apply our method to various MCMC Bayesian estimation problems where it favourably compares to the existing variance reduction approaches.

</details>

<details>

<summary>2020-02-15 10:19:42 - Optimization-Based MCMC Methods for Nonlinear Hierarchical Statistical Inverse Problems</summary>

- *Johnathan Bardsley, Tiangang Cui*

- `2002.06358v1` - [abs](http://arxiv.org/abs/2002.06358v1) - [pdf](http://arxiv.org/pdf/2002.06358v1)

> In many hierarchical inverse problems, not only do we want to estimate high- or infinite-dimensional model parameters in the parameter-to-observable maps, but we also have to estimate hyperparameters that represent critical assumptions in the statistical and mathematical modeling processes. As a joint effect of high-dimensionality, nonlinear dependence, and non-concave structures in the joint posterior posterior distribution over model parameters and hyperparameters, solving inverse problems in the hierarchical Bayesian setting poses a significant computational challenge. In this work, we aim to develop scalable optimization-based Markov chain Monte Carlo (MCMC) methods for solving hierarchical Bayesian inverse problems with nonlinear parameter-to-observable maps and a broader class of hyperparameters. Our algorithmic development is based on the recently developed scalable randomize-then-optimize (RTO) method [4] for exploring the high- or infinite-dimensional model parameter space. By using RTO either as a proposal distribution in a Metropolis-within-Gibbs update or as a biasing distribution in the pseudo-marginal MCMC [2], we are able to design efficient sampling tools for hierarchical Bayesian inversion. In particular, the integration of RTO and the pseudo-marginal MCMC has sampling performance robust to model parameter dimensions. We also extend our methods to nonlinear inverse problems with Poisson-distributed measurements. Numerical examples in PDE-constrained inverse problems and positron emission tomography (PET) are used to demonstrate the performance of our methods.

</details>

<details>

<summary>2020-02-16 00:27:50 - Spectral Subsampling MCMC for Stationary Time Series</summary>

- *Robert Salomone, Matias Quiroz, Robert Kohn, Mattias Villani, Minh-Ngoc Tran*

- `1910.13627v2` - [abs](http://arxiv.org/abs/1910.13627v2) - [pdf](http://arxiv.org/pdf/1910.13627v2)

> Bayesian inference using Markov Chain Monte Carlo (MCMC) on large datasets has developed rapidly in recent years. However, the underlying methods are generally limited to relatively simple settings where the data have specific forms of independence. We propose a novel technique for speeding up MCMC for time series data by efficient data subsampling in the frequency domain. For several challenging time series models, we demonstrate a speedup of up to two orders of magnitude while incurring negligible bias compared to MCMC on the full dataset. We also propose alternative control variates for variance reduction based on data grouping and coreset constructions.

</details>

<details>

<summary>2020-02-16 21:05:45 - Bayesian Spatial Homogeneity Pursuit Regression for Count Value Data</summary>

- *Peng Zhao, Hou-Cheng Yang, Dipak K. Dey, Guanyu Hu*

- `2002.06678v1` - [abs](http://arxiv.org/abs/2002.06678v1) - [pdf](http://arxiv.org/pdf/2002.06678v1)

> Spatial regression models are ubiquitous in many different areas such as environmental science, geoscience, and public health. Exploring relationships between response variables and covariates with complex spatial patterns is a very important work. In this paper, we propose a novel spatially clustered coefficients regression model for count value data based on nonparametric Bayesian methods. Our proposed method detects the spatial homogeneity of the Poisson regression coefficients. A Markov random field constraint mixture of finite mixtures prior provides a consistent estimator of the number of the clusters of regression coefficients with the geographically neighborhood information. The theoretical properties of our proposed method are established. An efficient Markov chain Monte Carlo algorithm is developed by using multivariate log gamma distribution as a base distribution. Extensive simulation studies are carried out to examine empirical performance of the proposed method. Additionally, we analyze Georgia premature deaths data as an illustration of the effectiveness of our approach.

</details>

<details>

<summary>2020-02-17 11:37:46 - Structure learning of Bayesian networks involving cyclic structures</summary>

- *Witold Wiecek, Frederic Y. Bois, Ghislaine Gayraud*

- `1906.04992v3` - [abs](http://arxiv.org/abs/1906.04992v3) - [pdf](http://arxiv.org/pdf/1906.04992v3)

> Many biological networks include cyclic structures. In such cases, Bayesian networks (BNs), which must be acyclic, are not sound models for structure learning. Dynamic BNs can be used but require relatively large time series data. We discuss an alternative model that embeds cyclic structures within acyclic BNs, allowing us to still use the factorization property and informative priors on network structure. We present an implementation in the linear Gaussian case, where cyclic structures are treated as multivariate nodes. We use a Markov Chain Monte Carlo algorithm for inference, allowing us to work with posterior distribution on the space of graphs.

</details>

<details>

<summary>2020-02-17 17:31:47 - A Divide and Conquer Algorithm of Bayesian Density Estimation</summary>

- *Ya Su*

- `2002.07094v1` - [abs](http://arxiv.org/abs/2002.07094v1) - [pdf](http://arxiv.org/pdf/2002.07094v1)

> Data sets for statistical analysis become extremely large even with some difficulty of being stored on one single machine. Even when the data can be stored in one machine, the computational cost would still be intimidating. We propose a divide and conquer solution to density estimation using Bayesian mixture modeling including the infinite mixture case. The methodology can be generalized to other application problems where a Bayesian mixture model is adopted. The proposed prior on each machine or subsample modifies the original prior on both mixing probabilities as well as on the rest of parameters in the distributions being mixed. The ultimate estimator is obtained by taking the average of the posterior samples corresponding to the proposed prior on each subset. Despite the tremendous reduction in time thanks to data splitting, the posterior contraction rate of the proposed estimator stays the same (up to a log factor) as that of the original prior when the data is analyzed as a whole. Simulation studies also justify the competency of the proposed method compared to the established WASP estimator in the finite dimension case. In addition, one of our simulations is performed in a shape constrained deconvolution context and reveals promising results. The application to a GWAS data set reveals the advantage over a naive method that uses the original prior.

</details>

<details>

<summary>2020-02-17 18:40:21 - Bayesian Structure Learning in Multi-layered Genomic Networks</summary>

- *Min Jin Ha, Francesco Stingo, Veerabhadran Baladandayuthapani*

- `2002.07122v1` - [abs](http://arxiv.org/abs/2002.07122v1) - [pdf](http://arxiv.org/pdf/2002.07122v1)

> Integrative network modeling of data arising from multiple genomic platforms provides insight into the holistic picture of the interactive system, as well as the flow of information across many disease domains including cancer. The basic data structure consists of a sequence of hierarchically ordered datasets for each individual subject, which facilitates integration of diverse inputs, such as genomic, transcriptomic, and proteomic data. A primary analytical task in such contexts is to model the layered architecture of networks where the vertices can be naturally partitioned into ordered layers, dictated by multiple platforms, and exhibit both undirected and directed relationships. We propose a multi-layered Gaussian graphical model (mlGGM) to investigate conditional independence structures in such multi-level genomic networks in human cancers. We implement a Bayesian node-wise selection (BANS) approach based on variable selection techniques that coherently accounts for the multiple types of dependencies in mlGGM; this flexible strategy exploits edge-specific prior knowledge and selects sparse and interpretable models. Through simulated data generated under various scenarios, we demonstrate that BANS outperforms other existing multivariate regression-based methodologies. Our integrative genomic network analysis for key signaling pathways across multiple cancer types highlights commonalities and differences of p53 integrative networks and epigenetic effects of BRCA2 on p53 and its interaction with T68 phosphorylated CHK2, that may have translational utilities of finding biomarkers and therapeutic targets.

</details>

<details>

<summary>2020-02-17 20:48:01 - Bayesian Quantile Factor Models</summary>

- *Kelly C. M. Gonçalves, Afonso C. B. Silva*

- `2002.07242v1` - [abs](http://arxiv.org/abs/2002.07242v1) - [pdf](http://arxiv.org/pdf/2002.07242v1)

> Factor analysis is a flexible technique for assessment of multivariate dependence and codependence. Besides being an exploratory tool used to reduce the dimensionality of multivariate data, it allows estimation of common factors that often have an interesting theoretical interpretation in real problems. However, in some specific cases the interest involves the effects of latent factors not only in the mean, but in the entire response distribution, represented by a quantile. This paper introduces a new class of models, named quantile factor models, which combines factor model theory with distribution-free quantile regression producing a robust statistical method. Bayesian estimation for the proposed model is performed using an efficient Markov chain Monte Carlo algorithm. The proposed model is evaluated using synthetic datasets in different settings, in order to evaluate its robustness and performance under different quantiles compared to more usual methods. The model is also applied to a financial sector dataset and a heart disease experiment.

</details>

<details>

<summary>2020-02-17 21:18:51 - Nonparametric Bayesian Deconvolution of a Symmetric Unimodal Density</summary>

- *Ya Su, Anirban Bhattacharya, Yan Zhang, Nilanjan Chatterjee, Raymond J. Carroll*

- `2002.07255v1` - [abs](http://arxiv.org/abs/2002.07255v1) - [pdf](http://arxiv.org/pdf/2002.07255v1)

> We consider nonparametric measurement error density deconvolution subject to heteroscedastic measurement errors as well as symmetry about zero and shape constraints, in particular unimodality. The problem is motivated by applications where the observed data are estimated effect sizes from regressions on multiple factors, where the target is the distribution of the true effect sizes. We exploit the fact that any symmetric and unimodal density can be expressed as a mixture of symmetric uniform densities, and model the mixing density in a new way using a Dirichlet process location-mixture of Gamma distributions. We do the computations within a Bayesian context, describe a simple scalable implementation that is linear in the sample size, and show that the estimate of the unknown target density is consistent. Within our application context of regression effect sizes, the target density is likely to have a large probability near zero (the near null effects) coupled with a heavy-tailed distribution (the actual effects). Simulations show that unlike standard deconvolution methods, our Constrained Bayesian Deconvolution method does a much better job of reconstruction of the target density. Applications to a genome-wise association study (GWAS) and microarray data reveal similar results.

</details>

<details>

<summary>2020-02-18 02:19:16 - Stabilities of Shape Identification Inverse Problems in a Bayesian Framework</summary>

- *Hajime Kawakami*

- `2002.07337v1` - [abs](http://arxiv.org/abs/2002.07337v1) - [pdf](http://arxiv.org/pdf/2002.07337v1)

> A general shape identification inverse problem is studied in a Bayesian framework. This problem requires the determination of the unknown shape of a domain in the Euclidean space from finite-dimensional observation data with some Gaussian random noise. Then, the stability of posterior is studied for observation data. For each point of the space, the conditional probability that the point is included in the unknown domain given the observation data is considered. The stability is also studied for this probability distribution. As a model problem for our inverse problem, a heat inverse problem is considered. This problem requires the determination of the unknown shape of cavities in a heat conductor from temperature data of some portion of the surface of the heat conductor. To apply the above stability results to this model problem, one needs the measurability and some boundedness of the forward operator. These properties are shown.

</details>

<details>

<summary>2020-02-18 06:45:42 - Accelerating Metropolis-within-Gibbs sampler with localized computations of differential equations</summary>

- *Qiang Liu, Xin T. Tong*

- `1906.10541v2` - [abs](http://arxiv.org/abs/1906.10541v2) - [pdf](http://arxiv.org/pdf/1906.10541v2)

> Inverse problem is ubiquitous in science and engineering, and Bayesian methodologies are often used to infer the underlying parameters. For high dimensional temporal-spatial models, classical Markov chain Monte Carlo (MCMC) methods are often slow to converge, and it is necessary to apply Metropolis-within-Gibbs (MwG) sampling on parameter blocks. However, the computation cost of each MwG iteration is typically $O(n^2)$, where $n$ is the model dimension. This can be too expensive in practice. This paper introduces a new reduced computation method to bring down the computation cost to $O(n)$, for the inverse initial value problem of a stochastic differential equation (SDE) with local interactions. The key observation is that each MwG proposal is only different from the original iterate at one parameter block, and this difference will only propagate within a local domain in the SDE computations. Therefore we can approximate the global SDE computation with a surrogate updated only within the local domain for reduced computation cost. Both theoretically and numerically, we show that the approximation errors can be controlled by the local domain size. We discuss how to implement the local computation scheme using Euler--Maruyama and 4th order Runge--Kutta methods. We numerically demonstrate the performance of the proposed method with the Lorenz 96 model and a linear stochastic flow model.

</details>

<details>

<summary>2020-02-18 10:14:31 - Differential Bayesian Neural Nets</summary>

- *Andreas Look, Melih Kandemir*

- `1912.00796v2` - [abs](http://arxiv.org/abs/1912.00796v2) - [pdf](http://arxiv.org/pdf/1912.00796v2)

> Neural Ordinary Differential Equations (N-ODEs) are a powerful building block for learning systems, which extend residual networks to a continuous-time dynamical system. We propose a Bayesian version of N-ODEs that enables well-calibrated quantification of prediction uncertainty, while maintaining the expressive power of their deterministic counterpart. We assign Bayesian Neural Nets (BNNs) to both the drift and the diffusion terms of a Stochastic Differential Equation (SDE) that models the flow of the activation map in time. We infer the posterior on the BNN weights using a straightforward adaptation of Stochastic Gradient Langevin Dynamics (SGLD). We illustrate significantly improved stability on two synthetic time series prediction tasks and report better model fit on UCI regression benchmarks with our method when compared to its non-Bayesian counterpart.

</details>

<details>

<summary>2020-02-18 10:16:59 - A Neural Network Based on First Principles</summary>

- *Paul M Baggenstoss*

- `2002.07469v1` - [abs](http://arxiv.org/abs/2002.07469v1) - [pdf](http://arxiv.org/pdf/2002.07469v1)

> In this paper, a Neural network is derived from first principles, assuming only that each layer begins with a linear dimension-reducing transformation. The approach appeals to the principle of Maximum Entropy (MaxEnt) to find the posterior distribution of the input data of each layer, conditioned on the layer output variables. This posterior has a well-defined mean, the conditional mean estimator, that is calculated using a type of neural network with theoretically-derived activation functions similar to sigmoid, softplus, and relu. This implicitly provides a theoretical justification for their use. A theorem that finds the conditional distribution and conditional mean estimator under the MaxEnt prior is proposed, unifying results for special cases. Combining layers results in an auto-encoder with conventional feed-forward analysis network and a type of linear Bayesian belief network in the reconstruction path.

</details>

<details>

<summary>2020-02-18 19:00:01 - Constraining the recent star formation history of galaxies : an Approximate Bayesian Computation approach</summary>

- *G. Aufort, L. Ciesla, P. Pudlo, V. Buat*

- `2002.07815v1` - [abs](http://arxiv.org/abs/2002.07815v1) - [pdf](http://arxiv.org/pdf/2002.07815v1)

> [Abridged] Although galaxies are found to follow a tight relation between their star formation rate and stellar mass, they are expected to exhibit complex star formation histories (SFH), with short-term fluctuations. The goal of this pilot study is to present a method that will identify galaxies that are undergoing a strong variation of star formation activity in the last tens to hundreds Myr. In other words, the proposed method will determine whether a variation in the last few hundreds of Myr of the SFH is needed to properly model the SED rather than a smooth normal SFH. To do so, we analyze a sample of COSMOS galaxies using high signal-to-noise ratio broad band photometry. We apply Approximate Bayesian Computation, a state-of-the-art statistical method to perform model choice, associated to machine learning algorithms to provide the probability that a flexible SFH is preferred based on the observed flux density ratios of galaxies. We present the method and test it on a sample of simulated SEDs. The input information fed to the algorithm is a set of broadband UV to NIR (rest-frame) flux ratios for each galaxy. The method has an error rate of 21% in recovering the right SFH and is sensitive to SFR variations larger than 1 dex. A more traditional SED fitting method using CIGALE is tested to achieve the same goal, based on fits comparisons through Bayesian Information Criterion but the best error rate obtained is higher, 28%. We apply our new method to the COSMOS galaxies sample. The stellar mass distribution of galaxies with a strong to decisive evidence against the smooth delayed-$\tau$ SFH peaks at lower M* compared to galaxies where the smooth delayed-$\tau$ SFH is preferred. We discuss the fact that this result does not come from any bias due to our training. Finally, we argue that flexible SFHs are needed to be able to cover that largest SFR-M* parameter space possible.

</details>

<details>

<summary>2020-02-19 08:02:44 - Targeting Solutions in Bayesian Multi-Objective Optimization: Sequential and Batch Versions</summary>

- *David Gaudrie, Rodolphe Le Riche, Victor Picheny, Benoit Enaux, Vincent Herbert*

- `1811.03862v5` - [abs](http://arxiv.org/abs/1811.03862v5) - [pdf](http://arxiv.org/pdf/1811.03862v5)

> Multi-objective optimization aims at finding trade-off solutions to conflicting objectives. These constitute the Pareto optimal set. In the context of expensive-to-evaluate functions, it is impossible and often non-informative to look for the entire set. As an end-user would typically prefer a certain part of the objective space, we modify the Bayesian multi-objective optimization algorithm which uses Gaussian Processes to maximize the Expected Hypervolume Improvement, to focus the search in the preferred region. The cumulated effects of the Gaussian Processes and the targeting strategy lead to a particularly efficient convergence to the desired part of the Pareto set. To take advantage of parallel computing, a multi-point extension of the targeting criterion is proposed and analyzed.

</details>

<details>

<summary>2020-02-19 10:41:40 - Point-process based Bayesian modeling of space-time structures of forest fire occurrences in Mediterranean France</summary>

- *Thomas Opitz, Florent Bonneu, Edith Gabriel*

- `2002.12318v1` - [abs](http://arxiv.org/abs/2002.12318v1) - [pdf](http://arxiv.org/pdf/2002.12318v1)

> Due to climate change and human activity, wildfires are expected to become more frequent and extreme worldwide, causing economic and ecological disasters. The deployment of preventive measures and operational forecasts can be aided by stochastic modeling that helps to understand and quantify the mechanisms governing the occurrence intensity. We here develop a point process framework for wildfire ignition points observed in the French Mediterranean basin since 1995, and we fit a spatio-temporal log-Gaussian Cox process with monthly temporal resolution in a Bayesian framework using the integrated nested Laplace approximation (INLA). Human activity is the main direct cause of wildfires and is indirectly measured through a number of appropriately defined proxies related to land-use covariates (urbanization, road network) in our approach, and we further integrate covariates of climatic and environmental conditions to explain wildfire occurrences. We include spatial random effects with Mat\'ern covariance and temporal autoregression at yearly resolution. Two major methodological challenges are tackled: first, handling and unifying multi-scale structures in data is achieved through computer-intensive preprocessing steps with GIS software and kriging techniques; second, INLA-based estimation with high-dimensional response vectors and latent models is facilitated through intra-year subsampling, taking into account the occurrence structure of wildfires.

</details>

<details>

<summary>2020-02-19 15:27:53 - Logistic Regression Regret: What's the Catch?</summary>

- *Gil I. Shamir*

- `2002.02950v2` - [abs](http://arxiv.org/abs/2002.02950v2) - [pdf](http://arxiv.org/pdf/2002.02950v2)

> We address the problem of the achievable regret rates with online logistic regression. We derive lower bounds with logarithmic regret under $L_1$, $L_2$, and $L_\infty$ constraints on the parameter values. The bounds are dominated by $d/2 \log T$, where $T$ is the horizon and $d$ is the dimensionality of the parameter space. We show their achievability for $d=o(T^{1/3})$ in all these cases with Bayesian methods, that achieve them up to a $d/2 \log d$ term. Interesting different behaviors are shown for larger dimensionality. Specifically, on the negative side, if $d = \Omega(\sqrt{T})$, any algorithm is guaranteed regret of $\Omega(d \log T)$ (greater than $\Omega(\sqrt{T})$) under $L_\infty$ constraints on the parameters (and the example features). On the positive side, under $L_1$ constraints on the parameters, there exist algorithms that can achieve regret that is sub-linear in $d$ for the asymptotically larger values of $d$. For $L_2$ constraints, it is shown that for large enough $d$, the regret remains linear in $d$ but no longer logarithmic in $T$. Adapting the redundancy-capacity theorem from information theory, we demonstrate a principled methodology based on grids of parameters to derive lower bounds. Grids are also utilized to derive some upper bounds. Our results strengthen results by Kakade and Ng (2005) and Foster et al. (2018) for upper bounds for this problem, introduce novel lower bounds, and adapt a methodology that can be used to obtain such bounds for other related problems. They also give a novel characterization of the asymptotic behavior when the dimension of the parameter space is allowed to grow with $T$. They additionally establish connections to the information theory literature, demonstrating that the actual regret for logistic regression depends on the richness of the parameter class, where even within this problem, richer classes lead to greater regret.

</details>

<details>

<summary>2020-02-20 00:34:46 - A Bayes Factor Approach with Informative Prior for Rare Genetic Variant Analysis from Next Generation Sequencing Data</summary>

- *Jingxiong Xu, Wei Xu, Laurent Briollais*

- `2002.08505v1` - [abs](http://arxiv.org/abs/2002.08505v1) - [pdf](http://arxiv.org/pdf/2002.08505v1)

> The discovery of rare genetic variants through Next Generation Sequencing is a very challenging issue in the field of human genetics. We propose a novel region-based statistical approach based on a Bayes Factor (BF) to assess evidence of association between a set of rare variants (RVs) located on the same genomic region and a disease outcome in the context of case-control design. Marginal likelihoods are computed under the null and alternative hypotheses assuming a binomial distribution for the RV count in the region and a beta or mixture of Dirac and beta prior distribution for the probability of RV. We derive the theoretical null distribution of the BF under our prior setting and show that a Bayesian control of the False Discovery Rate (BFDR) can be obtained for genome-wide inference. Informative priors are introduced using prior evidence of association from a Kolmogorov-Smirnov test statistic. We use our simulation program, sim1000G, to generate RV data similar to the 1,000 genomes sequencing project. Our simulation studies showed that the new BF statistic outperforms standard methods (SKAT, SKAT-O, Burden test) in case-control studies with moderate sample sizes and is equivalent to them under large sample size scenarios. Our real data application to a lung cancer case-control study found enrichment for RVs in known and novel cancer genes. It also suggests that using the BF with informative prior improves the overall gene discovery compared to the BF with non-informative prior.

</details>

<details>

<summary>2020-02-20 01:08:22 - Uncertainty-guided Continual Learning with Bayesian Neural Networks</summary>

- *Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, Marcus Rohrbach*

- `1906.02425v2` - [abs](http://arxiv.org/abs/1906.02425v2) - [pdf](http://arxiv.org/pdf/1906.02425v2)

> Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms need an external representation and extra computation to measure the parameters' \textit{importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in networks. Uncertainty is a natural way to identify \textit{what to remember} and \textit{what to change} as we continually learn, and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning and retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.

</details>

<details>

<summary>2020-02-20 02:52:05 - Information Condensing Active Learning</summary>

- *Siddhartha Jain, Ge Liu, David Gifford*

- `2002.07916v2` - [abs](http://arxiv.org/abs/2002.07916v2) - [pdf](http://arxiv.org/pdf/2002.07916v2)

> We introduce Information Condensing Active Learning (ICAL), a batch mode model agnostic Active Learning (AL) method targeted at Deep Bayesian Active Learning that focuses on acquiring labels for points which have as much information as possible about the still unacquired points. ICAL uses the Hilbert Schmidt Independence Criterion (HSIC) to measure the strength of the dependency between a candidate batch of points and the unlabeled set. We develop key optimizations that allow us to scale our method to large unlabeled sets. We show significant improvements in terms of model accuracy and negative log likelihood (NLL) on several image datasets compared to state of the art batch mode AL methods for deep learning.

</details>

<details>

<summary>2020-02-20 07:24:59 - Kalman Filtering With Censored Measurements</summary>

- *Kostas Loumponias, George Tsaklidis*

- `2002.08597v1` - [abs](http://arxiv.org/abs/2002.08597v1) - [pdf](http://arxiv.org/pdf/2002.08597v1)

> This paper concerns Kalman filtering when the measurements of the process are censored. The censored measurements are addressed by the Tobit model of Type I and are one-dimensional with two censoring limits, while the (hidden) state vectors are multidimensional. For this model, Bayesian estimates for the state vectors are provided through a recursive algorithm of Kalman filtering type. Experiments are presented to illustrate the effectiveness and applicability of the algorithm. The experiments show that the proposed method outperforms other filtering methodologies in minimizing the computational cost as well as the overall Root Mean Square Error (RMSE) for synthetic and real data sets.

</details>

<details>

<summary>2020-02-20 07:40:55 - Temporal Parallelization of Bayesian Smoothers</summary>

- *Simo Särkkä, Ángel F. García-Fernández*

- `1905.13002v2` - [abs](http://arxiv.org/abs/1905.13002v2) - [pdf](http://arxiv.org/pdf/1905.13002v2)

> This paper presents algorithms for temporal parallelization of Bayesian smoothers. We define the elements and the operators to pose these problems as the solutions to all-prefix-sums operations for which efficient parallel scan-algorithms are available. We present the temporal parallelization of the general Bayesian filtering and smoothing equations and specialize them to linear/Gaussian models. The advantage of the proposed algorithms is that they reduce the linear complexity of standard smoothing algorithms with respect to time to logarithmic.

</details>

<details>

<summary>2020-02-20 08:07:03 - A Bayesian Feature Allocation Model for Identification of Cell Subpopulations Using Cytometry Data</summary>

- *Arthur Lui, Juhee Lee, Peter F. Thall, May Daher, Katy Rezvani, Rafet Barar*

- `2002.08609v1` - [abs](http://arxiv.org/abs/2002.08609v1) - [pdf](http://arxiv.org/pdf/2002.08609v1)

> A Bayesian feature allocation model (FAM) is presented for identifying cell subpopulations based on multiple samples of cell surface or intracellular marker expression level data obtained by cytometry by time of flight (CyTOF). Cell subpopulations are characterized by differences in expression patterns of makers, and individual cells are clustered into the subpopulations based on the patterns of their observed expression levels. A finite Indian buffet process is used to model subpopulations as latent features, and a model-based method based on these latent feature subpopulations is used to construct cell clusters within each sample. Non-ignorable missing data due to technical artifacts in mass cytometry instruments are accounted for by defining a static missing data mechanism. In contrast to conventional cell clustering methods based on observed marker expression levels that are applied separately to different samples, the FAM based method can be applied simultaneously to multiple samples, and can identify important cell subpopulations likely to be missed by conventional clustering. The proposed FAM based method is applied to jointly analyze three datasets, generated by CyTOF, to study natural killer (NK) cells. Because the subpopulations identified by the FAM may define novel NK cell subsets, this statistical analysis may provide useful information about the biology of NK cells and their potential role in cancer immunotherapy which may lead, in turn, to development of improved cellular therapies. Simulation studies of the proposed method's behavior under two cases of known subpopulations also are presented, followed by analysis of the CyTOF NK cell surface marker data.

</details>

<details>

<summary>2020-02-20 11:00:13 - Variational Approximation Error in Bayesian Non-negative Matrix Factorization</summary>

- *Naoki Hayashi*

- `1809.02963v4` - [abs](http://arxiv.org/abs/1809.02963v4) - [pdf](http://arxiv.org/pdf/1809.02963v4)

> Non-negative matrix factorization (NMF) is a knowledge discovery method that is used in many fields. Variational inference and Gibbs sampling methods for it are also wellknown. However, the variational approximation error has not been clarified yet, because NMF is not statistically regular and the prior distribution used in variational Bayesian NMF (VBNMF) has zero or divergence points. In this paper, using algebraic geometrical methods, we theoretically analyze the difference in negative log evidence (a.k.a. free energy) between VBNMF and Bayesian NMF, i.e., the Kullback-Leibler divergence between the variational posterior and the true posterior. We derive an upper bound for the learning coefficient (a.k.a. the real log canonical threshold) in Bayesian NMF. By using the upper bound, we find a lower bound for the approximation error, asymptotically. The result quantitatively shows how well the VBNMF algorithm can approximate Bayesian NMF; the lower bound depends on the hyperparameters and the true nonnegative rank. A numerical experiment demonstrates the theoretical result.

</details>

<details>

<summary>2020-02-20 22:02:52 - Knockoff Boosted Tree for Model-Free Variable Selection</summary>

- *Tao Jiang, Yuanyuan Li, Alison A. Motsinger-Reif*

- `2002.09032v1` - [abs](http://arxiv.org/abs/2002.09032v1) - [pdf](http://arxiv.org/pdf/2002.09032v1)

> In this article, we propose a novel strategy for conducting variable selection without prior model topology knowledge using the knockoff method with boosted tree models. Our method is inspired by the original knockoff method, where the differences between original and knockoff variables are used for variable selection with false discovery rate control. The original method uses Lasso for regression models and assumes there are more samples than variables. We extend this method to both model-free and high-dimensional variable selection. We propose two new sampling methods for generating knockoffs, namely the sparse covariance and principal component knockoff methods. We test these methods and compare them with the original knockoff method in terms of their ability to control type I errors and power. The boosted tree model is a complex system and has more hyperparameters than models with simpler assumptions. In our framework, these hyperparameters are either tuned through Bayesian optimization or fixed at multiple levels for trend detection. In simulation tests, we also compare the properties and performance of importance test statistics of tree models. The results include combinations of different knockoffs and importance test statistics. We also consider scenarios that include main-effect, interaction, exponential, and second-order models while assuming the true model structures are unknown. We apply our algorithm for tumor purity estimation and tumor classification using the Cancer Genome Atlas (TCGA) gene expression data. The proposed algorithm is included in the KOBT package, available at \url{https://cran.r-project.org/web/packages/KOBT/index.html}.

</details>

<details>

<summary>2020-02-21 02:31:12 - Surrogate-Based Bayesian Inverse Modeling of the Hydrological System: An Adaptive Approach Considering Surrogate Approximation Error</summary>

- *Jiangjiang Zhang, Qiang Zheng, Dingjiang Chen, Laosheng Wu, Lingzao Zeng*

- `1807.05187v4` - [abs](http://arxiv.org/abs/1807.05187v4) - [pdf](http://arxiv.org/pdf/1807.05187v4)

> Bayesian inverse modeling is important for a better understanding of hydrological processes. However, this approach can be computationally demanding, as it usually requires a large number of model evaluations. To address this issue, one can take advantage of surrogate modeling techniques. Nevertheless, when approximation error of the surrogate model is neglected, the inversion result will be biased. In this paper, we develop a surrogate-based Bayesian inversion framework that explicitly quantifies and gradually reduces the approximation error of the surrogate. Specifically, two strategies are proposed to quantify the surrogate error. The first strategy works by quantifying the surrogate prediction uncertainty with a Bayesian method, while the second strategy uses another surrogate to simulate and correct the approximation error of the primary surrogate. By adaptively refining the surrogate over the posterior distribution, we can gradually reduce the surrogate approximation error to a small level. Demonstrated with three case studies involving high dimensionality, multimodality, and a real-world application, it is found that both strategies can reduce the bias introduced by surrogate approximation error, while the second strategy that integrates two methods (i.e., polynomial chaos expansion and Gaussian process in this work) that complement each other shows the best performance.

</details>

<details>

<summary>2020-02-21 05:16:00 - Degrees of Freedom and Model Selection for k-means Clustering</summary>

- *David P. Hofmeyr*

- `1806.02034v4` - [abs](http://arxiv.org/abs/1806.02034v4) - [pdf](http://arxiv.org/pdf/1806.02034v4)

> This paper investigates the model degrees of freedom in k-means clustering. An extension of Stein's lemma provides an expression for the effective degrees of freedom in the k-means model. Approximating the degrees of freedom in practice requires simplifications of this expression, however empirical studies evince the appropriateness of our proposed approach. The practical relevance of this new degrees of freedom formulation for k-means is demonstrated through model selection using the Bayesian Information Criterion. The reliability of this method is validated through experiments on simulated data as well as on a large collection of publicly available benchmark data sets from diverse application areas. Comparisons with popular existing techniques indicate that this approach is extremely competitive for selecting high quality clustering solutions. Code to implement the proposed approach is available in the form of an R package from https://github.com/DavidHofmeyr/edfkmeans.

</details>

<details>

<summary>2020-02-21 16:06:11 - Split-BOLFI for for misspecification-robust likelihood free inference in high dimensions</summary>

- *Owen Thomas, Henri Pesonen, Raquel Sá-Leão, Hermínia de Lencastre, Samuel Kaski, Jukka Corander*

- `2002.09377v1` - [abs](http://arxiv.org/abs/2002.09377v1) - [pdf](http://arxiv.org/pdf/2002.09377v1)

> Likelihood-free inference for simulator-based statistical models has recently grown rapidly from its infancy to a useful tool for practitioners. However, models with more than a very small number of parameters as the target of inference have remained an enigma, in particular for the approximate Bayesian computation (ABC) community. To advance the possibilities for performing likelihood-free inference in high-dimensional parameter spaces, here we introduce an extension of the popular Bayesian optimisation based approach to approximate discrepancy functions in a probabilistic manner which lends itself to an efficient exploration of the parameter space. Our method achieves computational scalability by using separate acquisition procedures for the discrepancies defined for different parameters. These efficient high-dimensional simulation acquisitions are combined with exponentiated loss-likelihoods to provide a misspecification-robust characterisation of the marginal posterior distribution for all model parameters. The method successfully performs computationally efficient inference in a 100-dimensional space on canonical examples and compares favourably to existing Copula-ABC methods. We further illustrate the potential of this approach by fitting a bacterial transmission dynamics model to daycare centre data, which provides biologically coherent results on the strain competition in a 30-dimensional parameter space.

</details>

<details>

<summary>2020-02-21 16:34:34 - Bayesian Structure Adaptation for Continual Learning</summary>

- *Abhishek Kumar, Sunabha Chatterjee, Piyush Rai*

- `1912.03624v2` - [abs](http://arxiv.org/abs/1912.03624v2) - [pdf](http://arxiv.org/pdf/1912.03624v2)

> Continual Learning is a learning paradigm where learning systems are trained with sequential or streaming tasks. Two notable directions among the recent advances in continual learning with neural networks are ($i$) variational Bayes based regularization by learning priors from previous tasks, and, ($ii$) learning the structure of deep networks to adapt to new tasks. So far, these two approaches have been orthogonal. We present a novel Bayesian approach to continual learning based on learning the structure of deep neural networks, addressing the shortcomings of both these approaches. The proposed model learns the deep structure for each task by learning which weights to be used, and supports inter-task transfer through the overlapping of different sparse subsets of weights learned by different tasks. Experimental results on supervised and unsupervised benchmarks shows that our model performs comparably or better than recent advances in continual learning setting.

</details>

<details>

<summary>2020-02-21 20:32:05 - Knot Selection in Sparse Gaussian Processes</summary>

- *Nathaniel Garton, Jarad Niemi, Alicia Carriquiry*

- `2002.09538v1` - [abs](http://arxiv.org/abs/2002.09538v1) - [pdf](http://arxiv.org/pdf/2002.09538v1)

> Knot-based, sparse Gaussian processes have enjoyed considerable success as scalable approximations to full Gaussian processes. Problems can occur, however, when knot selection is done by optimizing the marginal likelihood. For example, the marginal likelihood surface is highly multimodal, which can cause suboptimal knot placement where some knots serve practically no function. This is especially a problem when many more knots are used than are necessary, resulting in extra computational cost for little to no gains in accuracy.   We propose a one-at-a-time knot selection algorithm to select both the number and placement of knots. Our algorithm uses Bayesian optimization to efficiently propose knots that are likely to be good and largely avoids the pathologies encountered when using the marginal likelihood as the objective function. We provide empirical results showing improved accuracy and speed over the current standard approaches.

</details>

<details>

<summary>2020-02-22 05:39:32 - Bayesian Survival Analysis Using the rstanarm R Package</summary>

- *Samuel L. Brilleman, Eren M. Elci, Jacqueline Buros Novik, Rory Wolfe*

- `2002.09633v1` - [abs](http://arxiv.org/abs/2002.09633v1) - [pdf](http://arxiv.org/pdf/2002.09633v1)

> Survival data is encountered in a range of disciplines, most notably health and medical research. Although Bayesian approaches to the analysis of survival data can provide a number of benefits, they are less widely used than classical (e.g. likelihood-based) approaches. This may be in part due to a relative absence of user-friendly implementations of Bayesian survival models. In this article we describe how the rstanarm R package can be used to fit a wide range of Bayesian survival models. The rstanarm package facilitates Bayesian regression modelling by providing a user-friendly interface (users specify their model using customary R formula syntax and data frames) and using the Stan software (a C++ library for Bayesian inference) for the back-end estimation. The suite of models that can be estimated using rstanarm is broad and includes generalised linear models (GLMs), generalised linear mixed models (GLMMs), generalised additive models (GAMs) and more. In this article we focus only on the survival modelling functionality. This includes standard parametric (exponential, Weibull, Gompertz) and flexible parametric (spline-based) hazard models, as well as standard parametric accelerated failure time (AFT) models. All types of censoring (left, right, interval) are allowed, as is delayed entry (left truncation), time-varying covariates, time-varying effects, and frailty effects. We demonstrate the functionality through worked examples. We anticipate these implementations will increase the uptake of Bayesian survival analysis in applied research.

</details>

<details>

<summary>2020-02-22 09:56:20 - Nonmyopic Gaussian Process Optimization with Macro-Actions</summary>

- *Dmitrii Kharkovskii, Chun Kai Ling, Kian Hsiang Low*

- `2002.09670v1` - [abs](http://arxiv.org/abs/2002.09670v1) - [pdf](http://arxiv.org/pdf/2002.09670v1)

> This paper presents a multi-staged approach to nonmyopic adaptive Gaussian process optimization (GPO) for Bayesian optimization (BO) of unknown, highly complex objective functions that, in contrast to existing nonmyopic adaptive BO algorithms, exploits the notion of macro-actions for scaling up to a further lookahead to match up to a larger available budget. To achieve this, we generalize GP upper confidence bound to a new acquisition function defined w.r.t. a nonmyopic adaptive macro-action policy, which is intractable to be optimized exactly due to an uncountable set of candidate outputs. The contribution of our work here is thus to derive a nonmyopic adaptive epsilon-Bayes-optimal macro-action GPO (epsilon-Macro-GPO) policy. To perform nonmyopic adaptive BO in real time, we then propose an asymptotically optimal anytime variant of our epsilon-Macro-GPO policy with a performance guarantee. We empirically evaluate the performance of our epsilon-Macro-GPO policy and its anytime variant in BO with synthetic and real-world datasets.

</details>

<details>

<summary>2020-02-22 12:35:45 - Stochastic Gradient MCMC with Repulsive Forces</summary>

- *Victor Gallego, David Rios Insua*

- `1812.00071v2` - [abs](http://arxiv.org/abs/1812.00071v2) - [pdf](http://arxiv.org/pdf/1812.00071v2)

> We propose a unifying view of two different Bayesian inference algorithms, Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) and Stein Variational Gradient Descent (SVGD), leading to improved and efficient novel sampling schemes. We show that SVGD combined with a noise term can be framed as a multiple chain SG-MCMC method. Instead of treating each parallel chain independently from others, our proposed algorithm implements a repulsive force between particles, avoiding collapse and facilitating a better exploration of the parameter space. We also show how the addition of this noise term is necessary to obtain a valid SG-MCMC sampler, a significant difference with SVGD. Experiments with both synthetic distributions and real datasets illustrate the benefits of the proposed scheme.

</details>

<details>

<summary>2020-02-22 13:26:21 - Variationally Inferred Sampling Through a Refined Bound for Probabilistic Programs</summary>

- *Victor Gallego, David Rios Insua*

- `1908.09744v4` - [abs](http://arxiv.org/abs/1908.09744v4) - [pdf](http://arxiv.org/pdf/1908.09744v4)

> A framework to boost the efficiency of Bayesian inference in probabilistic programs is introduced by embedding a sampler inside a variational posterior approximation. We call it the refined variational approximation. Its strength lies both in ease of implementation and automatically tuning of the sampler parameters to speed up mixing time using automatic differentiation. Several strategies to approximate \emph{evidence lower bound} (ELBO) computation are introduced.   Experimental evidence of its efficient performance is shown solving an influence diagram in a high-dimensional space using a conditional variational autoencoder (cVAE) as a deep Bayes classifier; an unconditional VAE on density estimation tasks; and state-space models for time-series data.

</details>

<details>

<summary>2020-02-22 13:44:19 - Online Statistics Teaching and Learning</summary>

- *Jim Albert, Mine Cetinkaya-Rundel, Jingchen Hu*

- `2002.09700v1` - [abs](http://arxiv.org/abs/2002.09700v1) - [pdf](http://arxiv.org/pdf/2002.09700v1)

> For statistics courses at all levels, teaching and learning online poses challenges in different aspects. Particular online challenges include how to effectively and interactively conduct exploratory data analyses, how to incorporate statistical programming, how to include individual or team projects, and how to present mathematical derivations efficiently and effectively.   This article draws from the authors' experience with seven different online statistics courses to address some of the aforementioned challenges. One course is an online exploratory data analysis course taught at Bowling Green State University. A second course is an upper level Bayesian statistics course taught at Vassar College and shared among 10 liberal arts colleges through a hybrid model. We alo describes a five-course MOOC specialization on Coursera, offered by Duke University.

</details>

<details>

<summary>2020-02-23 06:40:48 - SetRank: A Setwise Bayesian Approach for Collaborative Ranking from Implicit Feedback</summary>

- *Chao Wang, Hengshu Zhu, Chen Zhu, Chuan Qin, Hui Xiong*

- `2002.09841v1` - [abs](http://arxiv.org/abs/2002.09841v1) - [pdf](http://arxiv.org/pdf/2002.09841v1)

> The recent development of online recommender systems has a focus on collaborative ranking from implicit feedback, such as user clicks and purchases. Different from explicit ratings, which reflect graded user preferences, the implicit feedback only generates positive and unobserved labels. While considerable efforts have been made in this direction, the well-known pairwise and listwise approaches have still been limited by various challenges. Specifically, for the pairwise approaches, the assumption of independent pairwise preference is not always held in practice. Also, the listwise approaches cannot efficiently accommodate "ties" due to the precondition of the entire list permutation. To this end, in this paper, we propose a novel setwise Bayesian approach for collaborative ranking, namely SetRank, to inherently accommodate the characteristics of implicit feedback in recommender system. Specifically, SetRank aims at maximizing the posterior probability of novel setwise preference comparisons and can be implemented with matrix factorization and neural networks. Meanwhile, we also present the theoretical analysis of SetRank to show that the bound of excess risk can be proportional to $\sqrt{M/N}$, where $M$ and $N$ are the numbers of items and users, respectively. Finally, extensive experiments on four real-world datasets clearly validate the superiority of SetRank compared with various state-of-the-art baselines.

</details>

<details>

<summary>2020-02-23 09:05:03 - On the generalization of bayesian deep nets for multi-class classification</summary>

- *Yossi Adi, Yaniv Nemcovsky, Alex Schwing, Tamir Hazan*

- `2002.09866v1` - [abs](http://arxiv.org/abs/2002.09866v1) - [pdf](http://arxiv.org/pdf/2002.09866v1)

> Generalization bounds which assess the difference between the true risk and the empirical risk have been studied extensively. However, to obtain bounds, current techniques use strict assumptions such as a uniformly bounded or a Lipschitz loss function. To avoid these assumptions, in this paper, we propose a new generalization bound for Bayesian deep nets by exploiting the contractivity of the Log-Sobolev inequalities. Using these inequalities adds an additional loss-gradient norm term to the generalization bound, which is intuitively a surrogate of the model complexity. Empirically, we analyze the affect of this loss-gradient norm term using different deep nets.

</details>

<details>

<summary>2020-02-23 15:52:08 - Weighting Is Worth the Wait: Bayesian Optimization with Importance Sampling</summary>

- *Setareh Ariafar, Zelda Mariet, Ehsan Elhamifar, Dana Brooks, Jennifer Dy, Jasper Snoek*

- `2002.09927v1` - [abs](http://arxiv.org/abs/2002.09927v1) - [pdf](http://arxiv.org/pdf/2002.09927v1)

> Many contemporary machine learning models require extensive tuning of hyperparameters to perform well. A variety of methods, such as Bayesian optimization, have been developed to automate and expedite this process. However, tuning remains extremely costly as it typically requires repeatedly fully training models. We propose to accelerate the Bayesian optimization approach to hyperparameter tuning for neural networks by taking into account the relative amount of information contributed by each training example. To do so, we leverage importance sampling (IS); this significantly increases the quality of the black-box function evaluations, but also their runtime, and so must be done carefully. Casting hyperparameter search as a multi-task Bayesian optimization problem over both hyperparameters and importance sampling design achieves the best of both worlds: by learning a parameterization of IS that trades-off evaluation complexity and quality, we improve upon Bayesian optimization state-of-the-art runtime and final validation error across a variety of datasets and complex neural architectures.

</details>

<details>

<summary>2020-02-23 21:59:46 - Targeted Smooth Bayesian Causal Forests: An analysis of heterogeneous treatment effects for simultaneous versus interval medical abortion regimens over gestation</summary>

- *Jennifer E. Starling, Jared S. Murray, Patricia A. Lohr, Abigail R. A. Aiken, Carlos M. Carvalho, James G. Scott*

- `1905.09405v2` - [abs](http://arxiv.org/abs/1905.09405v2) - [pdf](http://arxiv.org/pdf/1905.09405v2)

> We introduce Targeted Smooth Bayesian Causal Forests (tsBCF), a nonparametric Bayesian approach for estimating heterogeneous treatment effects which vary smoothly over a single covariate in the observational data setting. The tsBCF method induces smoothness by parameterizing terminal tree nodes with smooth functions, and allows for separate regularization of treatment effects versus prognostic effect of control covariates. Smoothing parameters for prognostic and treatment effects can be chosen to reflect prior knowledge or tuned in a data-dependent way.   We use tsBCF to analyze a new clinical protocol for early medical abortion. Our aim is to assess relative effectiveness of simultaneous versus interval administration of mifepristone and misoprostol over the first nine weeks of gestation. The model reflects our expectation that the relative effectiveness varies smoothly over gestation, but not necessarily over other covariates. We demonstrate the performance of the tsBCF method on benchmarking experiments. Software for tsBCF is available at https://github.com/jestarling/tsbcf/.

</details>

<details>

<summary>2020-02-24 02:15:34 - Bayesian bandwidth estimation and semi-metric selection for a functional partial linear model with unknown error density</summary>

- *Han Lin Shang*

- `2002.10038v1` - [abs](http://arxiv.org/abs/2002.10038v1) - [pdf](http://arxiv.org/pdf/2002.10038v1)

> This study examines the optimal selections of bandwidth and semi-metric for a functional partial linear model. Our proposed method begins by estimating the unknown error density using a kernel density estimator of residuals, where the regression function, consisting of parametric and nonparametric components, can be estimated by functional principal component and functional Nadayara-Watson estimators. The estimation accuracy of the regression function and error density crucially depends on the optimal estimations of bandwidth and semi-metric. A Bayesian method is utilized to simultaneously estimate the bandwidths in the regression function and kernel error density by minimizing the Kullback-Leibler divergence. For estimating the regression function and error density, a series of simulation studies demonstrate that the functional partial linear model gives improved estimation and forecast accuracies compared with the functional principal component regression and functional nonparametric regression. Using a spectroscopy dataset, the functional partial linear model yields better forecast accuracy than some commonly used functional regression models. As a by-product of the Bayesian method, a pointwise prediction interval can be obtained, and marginal likelihood can be used to select the optimal semi-metric.

</details>

<details>

<summary>2020-02-24 05:18:03 - Demystify Lindley's Paradox by Interpreting P-value as Posterior Probability</summary>

- *Guosheng Yin, Haolun Shi*

- `2002.10883v1` - [abs](http://arxiv.org/abs/2002.10883v1) - [pdf](http://arxiv.org/pdf/2002.10883v1)

> In the hypothesis testing framework, p-value is often computed to determine rejection of the null hypothesis or not. On the other hand, Bayesian approaches typically compute the posterior probability of the null hypothesis to evaluate its plausibility. We revisit Lindley's paradox (Lindley, 1957) and demystify the conflicting results between Bayesian and frequentist hypothesis testing procedures by casting a two-sided hypothesis as a combination of two one-sided hypotheses along the opposite directions. This can naturally circumvent the ambiguities of assigning a point mass to the null and choices of using local or non-local prior distributions. As p-value solely depends on the observed data without incorporating any prior information, we consider non-informative prior distributions for fair comparisons with p-value. The equivalence of p-value and the Bayesian posterior probability of the null hypothesis can be established to reconcile Lindley's paradox. Extensive simulation studies are conducted with multivariate normal data and random effects models to examine the relationship between the p-value and posterior probability.

</details>

<details>

<summary>2020-02-24 14:07:50 - Bayesian Inference in High-Dimensional Time-varying Parameter Models using Integrated Rotated Gaussian Approximations</summary>

- *Florian Huber, Gary Koop, Michael Pfarrhofer*

- `2002.10274v1` - [abs](http://arxiv.org/abs/2002.10274v1) - [pdf](http://arxiv.org/pdf/2002.10274v1)

> Researchers increasingly wish to estimate time-varying parameter (TVP) regressions which involve a large number of explanatory variables. Including prior information to mitigate over-parameterization concerns has led to many using Bayesian methods. However, Bayesian Markov Chain Monte Carlo (MCMC) methods can be very computationally demanding. In this paper, we develop computationally efficient Bayesian methods for estimating TVP models using an integrated rotated Gaussian approximation (IRGA). This exploits the fact that whereas constant coefficients on regressors are often important, most of the TVPs are often unimportant. Since Gaussian distributions are invariant to rotations we can split the the posterior into two parts: one involving the constant coefficients, the other involving the TVPs. Approximate methods are used on the latter and, conditional on these, the former are estimated with precision using MCMC methods. In empirical exercises involving artificial data and a large macroeconomic data set, we show the accuracy and computational benefits of IRGA methods.

</details>

<details>

<summary>2020-02-24 20:59:28 - Scalable Global Optimization via Local Bayesian Optimization</summary>

- *David Eriksson, Michael Pearce, Jacob R Gardner, Ryan Turner, Matthias Poloczek*

- `1910.01739v4` - [abs](http://arxiv.org/abs/1910.01739v4) - [pdf](http://arxiv.org/pdf/1910.01739v4)

> Bayesian optimization has recently emerged as a popular method for the sample-efficient optimization of expensive black-box functions. However, the application to high-dimensional problems with several thousand observations remains challenging, and on difficult problems Bayesian optimization is often not competitive with other paradigms. In this paper we take the view that this is due to the implicit homogeneity of the global probabilistic models and an overemphasized exploration that results from global acquisition. This motivates the design of a local probabilistic approach for global optimization of large-scale high-dimensional problems. We propose the $\texttt{TuRBO}$ algorithm that fits a collection of local models and performs a principled global allocation of samples across these models via an implicit bandit approach. A comprehensive evaluation demonstrates that $\texttt{TuRBO}$ outperforms state-of-the-art methods from machine learning and operations research on problems spanning reinforcement learning, robotics, and the natural sciences.

</details>

<details>

<summary>2020-02-24 22:50:19 - A Comparative Study of Machine Learning Models for Predicting the State of Reactive Mixing</summary>

- *B. Ahmmed, M. K. Mudunuru, S. Karra, S. C. James, V. V. Vesselinov*

- `2002.11511v1` - [abs](http://arxiv.org/abs/2002.11511v1) - [pdf](http://arxiv.org/pdf/2002.11511v1)

> Accurate predictions of reactive mixing are critical for many Earth and environmental science problems. To investigate mixing dynamics over time under different scenarios, a high-fidelity, finite-element-based numerical model is built to solve the fast, irreversible bimolecular reaction-diffusion equations to simulate a range of reactive-mixing scenarios. A total of 2,315 simulations are performed using different sets of model input parameters comprising various spatial scales of vortex structures in the velocity field, time-scales associated with velocity oscillations, the perturbation parameter for the vortex-based velocity, anisotropic dispersion contrast, and molecular diffusion. Outputs comprise concentration profiles of the reactants and products. The inputs and outputs of these simulations are concatenated into feature and label matrices, respectively, to train 20 different machine learning (ML) emulators to approximate system behavior. The 20 ML emulators based on linear methods, Bayesian methods, ensemble learning methods, and multilayer perceptron (MLP), are compared to assess these models. The ML emulators are specifically trained to classify the state of mixing and predict three quantities of interest (QoIs) characterizing species production, decay, and degree of mixing. Linear classifiers and regressors fail to reproduce the QoIs; however, ensemble methods (classifiers and regressors) and the MLP accurately classify the state of reactive mixing and the QoIs. Among ensemble methods, random forest and decision-tree-based AdaBoost faithfully predict the QoIs. At run time, trained ML emulators are $\approx10^5$ times faster than the high-fidelity numerical simulations. Speed and accuracy of the ensemble and MLP models facilitate uncertainty quantification, which usually requires 1,000s of model run, to estimate the uncertainty bounds on the QoIs.

</details>

<details>

<summary>2020-02-25 00:24:26 - Bayesian functional optimisation with shape prior</summary>

- *Pratibha Vellanki, Santu Rana, Sunil Gupta, David Rubin de Celis Leal, Alessandra Sutti, Murray Height, Svetha Venkatesh*

- `1809.07260v2` - [abs](http://arxiv.org/abs/1809.07260v2) - [pdf](http://arxiv.org/pdf/1809.07260v2)

> Real world experiments are expensive, and thus it is important to reach a target in minimum number of experiments. Experimental processes often involve control variables that changes over time. Such problems can be formulated as a functional optimisation problem. We develop a novel Bayesian optimisation framework for such functional optimisation of expensive black-box processes. We represent the control function using Bernstein polynomial basis and optimise in the coefficient space. We derive the theory and practice required to dynamically adjust the order of the polynomial degree, and show how prior information about shape can be integrated. We demonstrate the effectiveness of our approach for short polymer fibre design and optimising learning rate schedules for deep networks.

</details>

<details>

<summary>2020-02-25 09:22:05 - Monotonic Gaussian Process Flow</summary>

- *Ivan Ustyuzhaninov, Ieva Kazlauskaite, Carl Henrik Ek, Neill D. F. Campbell*

- `1905.12930v2` - [abs](http://arxiv.org/abs/1905.12930v2) - [pdf](http://arxiv.org/pdf/1905.12930v2)

> We propose a new framework for imposing monotonicity constraints in a Bayesian nonparametric setting based on numerical solutions of stochastic differential equations. We derive a nonparametric model of monotonic functions that allows for interpretable priors and principled quantification of hierarchical uncertainty. We demonstrate the efficacy of the proposed model by providing competitive results to other probabilistic monotonic models on a number of benchmark functions. In addition, we consider the utility of a monotonic random process as a part of a hierarchical probabilistic model; we examine the task of temporal alignment of time-series data where it is beneficial to use a monotonic random process in order to preserve the uncertainty in the temporal warpings.

</details>

<details>

<summary>2020-02-25 12:30:21 - Variational Inference and Bayesian CNNs for Uncertainty Estimation in Multi-Factorial Bone Age Prediction</summary>

- *Stefan Eggenreich, Christian Payer, Martin Urschler, Darko Štern*

- `2002.10819v1` - [abs](http://arxiv.org/abs/2002.10819v1) - [pdf](http://arxiv.org/pdf/2002.10819v1)

> Additionally to the extensive use in clinical medicine, biological age (BA) in legal medicine is used to assess unknown chronological age (CA) in applications where identification documents are not available. Automatic methods for age estimation proposed in the literature are predicting point estimates, which can be misleading without the quantification of predictive uncertainty. In our multi-factorial age estimation method from MRI data, we used the Variational Inference approach to estimate the uncertainty of a Bayesian CNN model. Distinguishing model uncertainty from data uncertainty, we interpreted data uncertainty as biological variation, i.e. the range of possible CA of subjects having the same BA.

</details>

<details>

<summary>2020-02-25 17:31:31 - Consensus Monte Carlo for Random Subsets using Shared Anchors</summary>

- *Yang Ni, Yuan Ji, Peter Mueller*

- `1906.12309v2` - [abs](http://arxiv.org/abs/1906.12309v2) - [pdf](http://arxiv.org/pdf/1906.12309v2)

> We present a consensus Monte Carlo algorithm that scales existing Bayesian nonparametric models for clustering and feature allocation to big data. The algorithm is valid for any prior on random subsets such as partitions and latent feature allocation, under essentially any sampling model. Motivated by three case studies, we focus on clustering induced by a Dirichlet process mixture sampling model, inference under an Indian buffet process prior with a binomial sampling model, and with a categorical sampling model. We assess the proposed algorithm with simulation studies and show results for inference with three datasets: an MNIST image dataset, a dataset of pancreatic cancer mutations, and a large set of electronic health records (EHR). Supplementary materials for this article are available online.

</details>

<details>

<summary>2020-02-25 18:53:15 - Bayesian Multi-scale Modeling of Factor Matrix without using Partition Tree</summary>

- *Maoran Xu, Leo L. Duan*

- `2002.09606v2` - [abs](http://arxiv.org/abs/2002.09606v2) - [pdf](http://arxiv.org/pdf/2002.09606v2)

> The multi-scale factor models are particularly appealing for analyzing matrix- or tensor-valued data, due to their adaptiveness to local geometry and intuitive interpretation. However, the reliance on the binary tree for recursive partitioning creates high complexity in the parameter space, making it extremely challenging to quantify its uncertainty. In this article, we discover an alternative way to generate multi-scale matrix using simple matrix operation: starting from a random matrix with each column having two unique values, its Cholesky whitening transform obeys a recursive partitioning structure. This allows us to consider a generative distribution with large prior support on common multi-scale factor models, and efficient posterior computation via Hamiltonian Monte Carlo. We demonstrate its potential in a multi-scale factor model to find broader regions of interest for human brain connectivity.

</details>

<details>

<summary>2020-02-25 20:02:06 - Smoothing Graphons for Modelling Exchangeable Relational Data</summary>

- *Xuhui Fan, Yaqiong Li, Ling Chen, Bin Li, Scott A. Sisson*

- `2002.11159v1` - [abs](http://arxiv.org/abs/2002.11159v1) - [pdf](http://arxiv.org/pdf/2002.11159v1)

> Modelling exchangeable relational data can be described by \textit{graphon theory}. Most Bayesian methods for modelling exchangeable relational data can be attributed to this framework by exploiting different forms of graphons. However, the graphons adopted by existing Bayesian methods are either piecewise-constant functions, which are insufficiently flexible for accurate modelling of the relational data, or are complicated continuous functions, which incur heavy computational costs for inference. In this work, we introduce a smoothing procedure to piecewise-constant graphons to form {\em smoothing graphons}, which permit continuous intensity values for describing relations, but without impractically increasing computational costs. In particular, we focus on the Bayesian Stochastic Block Model (SBM) and demonstrate how to adapt the piecewise-constant SBM graphon to the smoothed version. We initially propose the Integrated Smoothing Graphon (ISG) which introduces one smoothing parameter to the SBM graphon to generate continuous relational intensity values. We then develop the Latent Feature Smoothing Graphon (LFSG), which improves on the ISG by introducing auxiliary hidden labels to decompose the calculation of the ISG intensity and enable efficient inference. Experimental results on real-world data sets validate the advantages of applying smoothing strategies to the Stochastic Block Model, demonstrating that smoothing graphons can greatly improve AUC and precision for link prediction without increasing computational complexity.

</details>

<details>

<summary>2020-02-25 21:19:12 - Compositional uncertainty in deep Gaussian processes</summary>

- *Ivan Ustyuzhaninov, Ieva Kazlauskaite, Markus Kaiser, Erik Bodin, Neill D. F. Campbell, Carl Henrik Ek*

- `1909.07698v3` - [abs](http://arxiv.org/abs/1909.07698v3) - [pdf](http://arxiv.org/pdf/1909.07698v3)

> Gaussian processes (GPs) are nonparametric priors over functions. Fitting a GP implies computing a posterior distribution of functions consistent with the observed data. Similarly, deep Gaussian processes (DGPs) should allow us to compute a posterior distribution of compositions of multiple functions giving rise to the observations. However, exact Bayesian inference is intractable for DGPs, motivating the use of various approximations. We show that the application of simplifying mean-field assumptions across the hierarchy leads to the layers of a DGP collapsing to near-deterministic transformations. We argue that such an inference scheme is suboptimal, not taking advantage of the potential of the model to discover the compositional structure in the data. To address this issue, we examine alternative variational inference schemes allowing for dependencies across different layers and discuss their advantages and limitations.

</details>

<details>

<summary>2020-02-25 22:25:02 - Classical and Bayesian Analyses of a Mixture of Exponential and Lomax Distributions</summary>

- *Maqsood Ali, Abdul Haq, Muhammad Aslam*

- `2002.11204v1` - [abs](http://arxiv.org/abs/2002.11204v1) - [pdf](http://arxiv.org/pdf/2002.11204v1)

> The exponential and the Lomax distributions are widely used in life testing experiments in mixture models. A mixture model of exponential distribution and Lomax distribution is proposed. Parameters of the proposed model are estimated using classical and Bayesian procedures under type-I right censoring. Expressions for Bayes estimators are derived assuming noninformative (uniform and Jeffreys) priors under symmetric and asymmetric loss functions. Posterior predictive distributions of a future observation are derived and predictive estimates are obtained. Extensive Monte Carlo simulations are carried out to investigate performance of the estimators in terms of sample sizes, censoring times and mixing proportions. The analysis of mixture model is carried out using a data set of lifetime of transmitter receivers. Interesting properties of estimators are observed and discussed.

</details>

<details>

<summary>2020-02-26 00:45:12 - Paired Comparisons Modeling using t-Distribution with Bayesian Analysis</summary>

- *Maqsood Ali, Muhammad Aslam*

- `2002.11236v1` - [abs](http://arxiv.org/abs/2002.11236v1) - [pdf](http://arxiv.org/pdf/2002.11236v1)

> A paired comparison analysis is the simplest way to make comparative judgments between objects where objects may be goods, services or skills. For a set of problems, this technique helps to choose the most important problem to solve first and/or provides the solution that will be the most effective. This paper presents the theory of paired comparisons method and contributes to the paired comparisons models by developing a new model based on t-distribution. The developed model is illustrated using a data set of citations among four famous journals of Statistics. Using Bayesian analysis, the journals are ranked as JRSS-B --> Biometrika --> JASA --> Comm. in Stats.

</details>

<details>

<summary>2020-02-26 01:57:36 - Incorporating Expert Prior Knowledge into Experimental Design via Posterior Sampling</summary>

- *Cheng Li, Sunil Gupta, Santu Rana, Vu Nguyen, Antonio Robles-Kelly, Svetha Venkatesh*

- `2002.11256v1` - [abs](http://arxiv.org/abs/2002.11256v1) - [pdf](http://arxiv.org/pdf/2002.11256v1)

> Scientific experiments are usually expensive due to complex experimental preparation and processing. Experimental design is therefore involved with the task of finding the optimal experimental input that results in the desirable output by using as few experiments as possible. Experimenters can often acquire the knowledge about the location of the global optimum. However, they do not know how to exploit this knowledge to accelerate experimental design. In this paper, we adopt the technique of Bayesian optimization for experimental design since Bayesian optimization has established itself as an efficient tool for optimizing expensive black-box functions. Again, it is unknown how to incorporate the expert prior knowledge about the global optimum into Bayesian optimization process. To address it, we represent the expert knowledge about the global optimum via placing a prior distribution on it and we then derive its posterior distribution. An efficient Bayesian optimization approach has been proposed via posterior sampling on the posterior distribution of the global optimum. We theoretically analyze the convergence of the proposed algorithm and discuss the robustness of incorporating expert prior. We evaluate the efficiency of our algorithm by optimizing synthetic functions and tuning hyperparameters of classifiers along with a real-world experiment on the synthesis of short polymer fiber. The results clearly demonstrate the advantages of our proposed method.

</details>

<details>

<summary>2020-02-26 08:10:03 - Combining interdependent climate model outputs in CMIP5: A spatial Bayesian approach</summary>

- *Huang Huang, Dorit Hammerling, Bo Li, Richard Smith*

- `2001.00074v2` - [abs](http://arxiv.org/abs/2001.00074v2) - [pdf](http://arxiv.org/pdf/2001.00074v2)

> Projections of future climate change rely heavily on climate models, and combining climate models through a multi-model ensemble is both more accurate than a single climate model and valuable for uncertainty quantification. However, Bayesian approaches to multi-model ensembles have been criticized for making oversimplified assumptions about bias and variability, as well as treating different models as statistically independent. This paper extends the Bayesian hierarchical approach of Sansom et al. (2017) by explicitly accounting for spatial variability and inter-model dependence. We propose a Bayesian hierarchical model that accounts for bias between climate models and observations, spatial and inter-model dependence, the emergent relationship between historical and future periods, and natural variability. Extensive simulations show that our model provides better estimates and uncertainty quantification than the commonly used simple model mean. These results are illustrated using data from the CMIP5 model archive. As examples, for Central North America our projected mean temperature for 2070--2100 is about 0.8 K lower than the simple model mean, while for East Asia it is about 0.5 K higher; however, in both cases, the widths of the 90% credible intervals are of the order 3--6 K, so the uncertainties overwhelm the relatively small differences in projected mean temperatures.

</details>

<details>

<summary>2020-02-26 12:06:42 - On Thompson Sampling for Smoother-than-Lipschitz Bandits</summary>

- *James A. Grant, David S. Leslie*

- `2001.02323v2` - [abs](http://arxiv.org/abs/2001.02323v2) - [pdf](http://arxiv.org/pdf/2001.02323v2)

> Thompson Sampling is a well established approach to bandit and reinforcement learning problems. However its use in continuum armed bandit problems has received relatively little attention. We provide the first bounds on the regret of Thompson Sampling for continuum armed bandits under weak conditions on the function class containing the true function and sub-exponential observation noise. Our bounds are realised by analysis of the eluder dimension, a recently proposed measure of the complexity of a function class, which has been demonstrated to be useful in bounding the Bayesian regret of Thompson Sampling for simpler bandit problems under sub-Gaussian observation noise. We derive a new bound on the eluder dimension for classes of functions with Lipschitz derivatives, and generalise previous analyses in multiple regards.

</details>

<details>

<summary>2020-02-26 12:21:59 - Uncertainty in Neural Networks: Approximately Bayesian Ensembling</summary>

- *Tim Pearce, Felix Leibfried, Alexandra Brintrup, Mohamed Zaki, Andy Neely*

- `1810.05546v5` - [abs](http://arxiv.org/abs/1810.05546v5) - [pdf](http://arxiv.org/pdf/1810.05546v5)

> Understanding the uncertainty of a neural network's (NN) predictions is essential for many purposes. The Bayesian framework provides a principled approach to this, however applying it to NNs is challenging due to large numbers of parameters and data. Ensembling NNs provides an easily implementable, scalable method for uncertainty quantification, however, it has been criticised for not being Bayesian. This work proposes one modification to the usual process that we argue does result in approximate Bayesian inference; regularising parameters about values drawn from a distribution which can be set equal to the prior. A theoretical analysis of the procedure in a simplified setting suggests the recovered posterior is centred correctly but tends to have an underestimated marginal variance, and overestimated correlation. However, two conditions can lead to exact recovery. We argue that these conditions are partially present in NNs. Empirical evaluations demonstrate it has an advantage over standard ensembling, and is competitive with variational methods.

</details>

<details>

<summary>2020-02-26 13:16:09 - Adaptive Bayesian Reticulum</summary>

- *Giuseppe Nuti, Lluís Antoni Jiménez Rugama, Kaspar Thommen*

- `1912.05901v3` - [abs](http://arxiv.org/abs/1912.05901v3) - [pdf](http://arxiv.org/pdf/1912.05901v3)

> Neural Networks and Decision Trees: two popular techniques for supervised learning that are seemingly disconnected in their formulation and optimization method, have recently been combined in a single construct. The connection pivots on assembling an artificial Neural Network with nodes that allow for a gate-like function to mimic a tree split, optimized using the standard approach of recursively applying the chain rule to update its parameters. Yet two main challenges have impeded wide use of this hybrid approach: (a) the inability of global gradient ascent techniques to optimize hierarchical parameters (as introduced by the gate function); and (b) the construction of the tree structure, which has relied on standard decision tree algorithms to learn the network topology or incrementally (and heuristically) searching the space at random. Here we propose a probabilistic construct that exploits the idea of a node's unexplained potential (the total error channeled through the node) in order to decide where to expand further, mimicking the standard tree construction in a Neural Network setting, alongside a modified gradient ascent that first locally optimizes an expanded node before a global optimization. The probabilistic approach allows us to evaluate each new split as a ratio of likelihoods that balances the statistical improvement in explaining the evidence against the additional model complexity --- thus providing a natural stopping condition. The result is a novel classification and regression technique that leverages the strength of both: a tree-structure that grows naturally and is simple to interpret with the plasticity of Neural Networks that allow for soft margins and slanted boundaries.

</details>

<details>

<summary>2020-02-26 19:27:08 - A Bayesian Hierarchical Model for Evaluating Forensic Footwear Evidence</summary>

- *Neil A. Spencer, Jared S. Murray*

- `1906.05244v2` - [abs](http://arxiv.org/abs/1906.05244v2) - [pdf](http://arxiv.org/pdf/1906.05244v2)

> When a latent shoeprint is discovered at a crime scene, forensic analysts inspect it for distinctive patterns of wear such as scratches and holes (known as accidentals) on the source shoe's sole. If its accidentals correspond to those of a suspect's shoe, the print can be used as forensic evidence to place the suspect at the crime scene. The strength of this evidence depends on the random match probability---the chance that a shoe chosen at random would match the crime scene print's accidentals. Evaluating random match probabilities requires an accurate model for the spatial distribution of accidentals on shoe soles. A recent report by the President's Council of Advisors in Science and Technology criticized existing models in the literature, calling for new empirically validated techniques. We respond to this request with a new spatial point process model for accidental locations, developed within a hierarchical Bayesian framework. We treat the tread pattern of each shoe as a covariate, allowing us to pool information across large heterogeneous databases of shoes. Existing models ignore this information; our results show that including it leads to significantly better model fit. We demonstrate this by fitting our model to one such database.

</details>

<details>

<summary>2020-02-27 09:25:29 - Assessing causal effects in the presence of treatment switching through principal stratification</summary>

- *Alessandra Mattei, Fabrizia Mealli, Peng Ding*

- `2002.11989v1` - [abs](http://arxiv.org/abs/2002.11989v1) - [pdf](http://arxiv.org/pdf/2002.11989v1)

> Clinical trials focusing on survival outcomes often allow patients in the control arm to switch to the treatment arm if their physical conditions are worse than certain tolerance levels. The Intention-To-Treat analysis provides valid causal estimates of the effect of assignment, but it does not measure the effect of the actual receipt of the treatment and ignores the information of treatment switching. Other existing methods propose to reconstruct the outcome a unit would have had if s/he had not switched under strong assumptions. We propose to re-define the problem of treatment switching using principal stratification focusing on principal causal effects for patients belonging to subpopulations defined by the switching behavior under control. We use a Bayesian approach to inference taking into account that (i) switching happens in continuous time generating infinitely many principal strata; (ii) switching time is not defined for units who never switch in a particular experiment; and (iii) survival time and switching time are subject to censoring. We illustrate our framework using a synthetic dataset based on the Concorde study, a randomized controlled trial aimed to assess causal effects on time-to-disease progression or death of immediate versus deferred treatment with zidovudine among patients with asymptomatic HIV infection.

</details>

<details>

<summary>2020-02-27 09:54:44 - A Kernel to Exploit Informative Missingness in Multivariate Time Series from EHRs</summary>

- *Karl Øyvind Mikalsen, Cristina Soguero-Ruiz, Robert Jenssen*

- `2002.12359v1` - [abs](http://arxiv.org/abs/2002.12359v1) - [pdf](http://arxiv.org/pdf/2002.12359v1)

> A large fraction of the electronic health records (EHRs) consists of clinical measurements collected over time, such as lab tests and vital signs, which provide important information about a patient's health status. These sequences of clinical measurements are naturally represented as time series, characterized by multiple variables and large amounts of missing data, which complicate the analysis. In this work, we propose a novel kernel which is capable of exploiting both the information from the observed values as well the information hidden in the missing patterns in multivariate time series (MTS) originating e.g. from EHRs. The kernel, called TCK$_{IM}$, is designed using an ensemble learning strategy in which the base models are novel mixed mode Bayesian mixture models which can effectively exploit informative missingness without having to resort to imputation methods. Moreover, the ensemble approach ensures robustness to hyperparameters and therefore TCK$_{IM}$ is particularly well suited if there is a lack of labels - a known challenge in medical applications. Experiments on three real-world clinical datasets demonstrate the effectiveness of the proposed kernel.

</details>

<details>

<summary>2020-02-27 09:55:35 - The variation of the posterior variance and Bayesian sample size determination</summary>

- *Jörg Martin, Clemens Elster*

- `1907.12795v2` - [abs](http://arxiv.org/abs/1907.12795v2) - [pdf](http://arxiv.org/pdf/1907.12795v2)

> We consider Bayesian sample size determination using a criterion that utilizes the first two moments of the expected posterior variance. We study the resulting sample size in dependence on the chosen prior and explore the success rate for bounding the posterior variance below a prescribed limit under the true sampling distribution. Compared with sample size determination based on the expected average of the posterior variance the proposed criterion leads to an increase in sample size and significantly improved success rates. Generic asymptotic properties are proven, such as an asymptotic expression for the sample size and a sort of phase transition. Our study is illustrated using two real world datasets with Poisson and normally distributed data. Based on our results some recommendations are given.

</details>

<details>

<summary>2020-02-27 12:30:00 - PHS: A Toolbox for Parallel Hyperparameter Search</summary>

- *Peter Michael Habelitz, Janis Keuper*

- `2002.11429v2` - [abs](http://arxiv.org/abs/2002.11429v2) - [pdf](http://arxiv.org/pdf/2002.11429v2)

> We introduce an open source python framework named PHS - Parallel Hyperparameter Search to enable hyperparameter optimization on numerous compute instances of any arbitrary python function. This is achieved with minimal modifications inside the target function. Possible applications appear in expensive to evaluate numerical computations which strongly depend on hyperparameters such as machine learning. Bayesian optimization is chosen as a sample efficient method to propose the next query set of parameters.

</details>

<details>

<summary>2020-02-27 15:16:45 - A Unified Stochastic Gradient Approach to Designing Bayesian-Optimal Experiments</summary>

- *Adam Foster, Martin Jankowiak, Matthew O'Meara, Yee Whye Teh, Tom Rainforth*

- `1911.00294v2` - [abs](http://arxiv.org/abs/1911.00294v2) - [pdf](http://arxiv.org/pdf/1911.00294v2)

> We introduce a fully stochastic gradient based approach to Bayesian optimal experimental design (BOED). Our approach utilizes variational lower bounds on the expected information gain (EIG) of an experiment that can be simultaneously optimized with respect to both the variational and design parameters. This allows the design process to be carried out through a single unified stochastic gradient ascent procedure, in contrast to existing approaches that typically construct a pointwise EIG estimator, before passing this estimator to a separate optimizer. We provide a number of different variational objectives including the novel adaptive contrastive estimation (ACE) bound. Finally, we show that our gradient-based approaches are able to provide effective design optimization in substantially higher dimensional settings than existing approaches.

</details>

<details>

<summary>2020-02-27 21:19:29 - The Estimation of Causal Effects of Multiple Treatments in Observational Studies Using Bayesian Additive Regression Trees</summary>

- *Chenyang Gu, Michael J. Lopez, Liangyuan Hu*

- `1901.04312v2` - [abs](http://arxiv.org/abs/1901.04312v2) - [pdf](http://arxiv.org/pdf/1901.04312v2)

> There is currently a dearth of appropriate methods to estimate the causal effects of multiple treatments when the outcome is binary. For such settings, we propose the use of nonparametric Bayesian modeling, Bayesian Additive Regression Trees (BART). We conduct an extensive simulation study to compare BART to several existing, propensity score-based methods and to identify its operating characteristics when estimating average treatment effects on the treated. BART consistently demonstrates low bias and mean-squared errors. We illustrate the use of BART through a comparative effectiveness analysis of a large dataset, drawn from the latest SEER-Medicare linkage, on patients who were operated via robotic-assisted surgery, video-assisted thoratic surgery or open thoracotomy.

</details>

<details>

<summary>2020-02-28 08:02:34 - Mixed Reinforcement Learning with Additive Stochastic Uncertainty</summary>

- *Yao Mu, Shengbo Eben Li, Chang Liu, Qi Sun, Bingbing Nie, Bo Cheng, Baiyu Peng*

- `2003.00848v1` - [abs](http://arxiv.org/abs/2003.00848v1) - [pdf](http://arxiv.org/pdf/2003.00848v1)

> Reinforcement learning (RL) methods often rely on massive exploration data to search optimal policies, and suffer from poor sampling efficiency. This paper presents a mixed reinforcement learning (mixed RL) algorithm by simultaneously using dual representations of environmental dynamics to search the optimal policy with the purpose of improving both learning accuracy and training speed. The dual representations indicate the environmental model and the state-action data: the former can accelerate the learning process of RL, while its inherent model uncertainty generally leads to worse policy accuracy than the latter, which comes from direct measurements of states and actions. In the framework design of the mixed RL, the compensation of the additive stochastic model uncertainty is embedded inside the policy iteration RL framework by using explored state-action data via iterative Bayesian estimator (IBE). The optimal policy is then computed in an iterative way by alternating between policy evaluation (PEV) and policy improvement (PIM). The convergence of the mixed RL is proved using the Bellman's principle of optimality, and the recursive stability of the generated policy is proved via the Lyapunov's direct method. The effectiveness of the mixed RL is demonstrated by a typical optimal control problem of stochastic non-affine nonlinear systems (i.e., double lane change task with an automated vehicle).

</details>

<details>

<summary>2020-02-28 10:02:59 - Causality and Robust Optimization</summary>

- *Akihiro Yabe*

- `2002.12626v1` - [abs](http://arxiv.org/abs/2002.12626v1) - [pdf](http://arxiv.org/pdf/2002.12626v1)

> A decision-maker must consider cofounding bias when attempting to apply machine learning prediction, and, while feature selection is widely recognized as important process in data-analysis, it could cause cofounding bias. A causal Bayesian network is a standard tool for describing causal relationships, and if relationships are known, then adjustment criteria can determine with which features cofounding bias disappears. A standard modification would thus utilize causal discovery algorithms for preventing cofounding bias in feature selection. Causal discovery algorithms, however, essentially rely on the faithfulness assumption, which turn out to be easily violated in practical feature selection settings. In this paper, we propose a meta-algorithm that can remedy existing feature selection algorithms in terms of cofounding bias. Our algorithm is induced from a novel adjustment criterion that requires rather than faithfulness, an assumption which can be induced from another well-known assumption of the causal sufficiency. We further prove that the features added through our modification convert cofounding bias into prediction variance. With the aid of existing robust optimization technologies that regularize risky strategies with high variance, then, we are able to successfully improve the throughput performance of decision-making optimization, as is shown in our experimental results.

</details>

<details>

<summary>2020-02-28 10:09:05 - Deep Neural Network Hyperparameter Optimization with Orthogonal Array Tuning</summary>

- *Xiang Zhang, Xiaocong Chen, Lina Yao, Chang Ge, Manqing Dong*

- `1907.13359v2` - [abs](http://arxiv.org/abs/1907.13359v2) - [pdf](http://arxiv.org/pdf/1907.13359v2)

> Deep learning algorithms have achieved excellent performance lately in a wide range of fields (e.g., computer version). However, a severe challenge faced by deep learning is the high dependency on hyper-parameters. The algorithm results may fluctuate dramatically under the different configuration of hyper-parameters. Addressing the above issue, this paper presents an efficient Orthogonal Array Tuning Method (OATM) for deep learning hyper-parameter tuning. We describe the OATM approach in five detailed steps and elaborate on it using two widely used deep neural network structures (Recurrent Neural Networks and Convolutional Neural Networks). The proposed method is compared to the state-of-the-art hyper-parameter tuning methods including manually (e.g., grid search and random search) and automatically (e.g., Bayesian Optimization) ones. The experiment results state that OATM can significantly save the tuning time compared to the state-of-the-art methods while preserving the satisfying performance. The codes are open in GitHub (https://github.com/xiangzhang1015/OATM)

</details>

<details>

<summary>2020-02-28 11:27:48 - Synchronization in 5G: a Bayesian Approach</summary>

- *M. Goodarzi, D. Cvetkovski, N. Maletic, J. Gutierrez, E. Grass*

- `2002.12660v1` - [abs](http://arxiv.org/abs/2002.12660v1) - [pdf](http://arxiv.org/pdf/2002.12660v1)

> In this work, we propose a hybrid approach to synchronize large scale networks. In particular, we draw on Kalman Filtering (KF) along with time-stamps generated by the Precision Time Protocol (PTP) for pairwise node synchronization. Furthermore, we investigate the merit of Factor Graphs (FGs) along with Belief Propagation (BP) algorithm in achieving high precision end-to-end network synchronization. Finally, we present the idea of dividing the large-scale network into local synchronization domains, for each of which a suitable sync algorithm is utilized. The simulation results indicate that, despite the simplifications in the hybrid approach, the error in the offset estimation remains below 5 ns.

</details>

<details>

<summary>2020-02-28 13:45:56 - Robust Optimisation Monte Carlo</summary>

- *Borislav Ikonomov, Michael U. Gutmann*

- `1904.00670v3` - [abs](http://arxiv.org/abs/1904.00670v3) - [pdf](http://arxiv.org/pdf/1904.00670v3)

> This paper is on Bayesian inference for parametric statistical models that are defined by a stochastic simulator which specifies how data is generated. Exact sampling is then possible but evaluating the likelihood function is typically prohibitively expensive. Approximate Bayesian Computation (ABC) is a framework to perform approximate inference in such situations. While basic ABC algorithms are widely applicable, they are notoriously slow and much research has focused on increasing their efficiency. Optimisation Monte Carlo (OMC) has recently been proposed as an efficient and embarrassingly parallel method that leverages optimisation to accelerate the inference. In this paper, we demonstrate an important previously unrecognised failure mode of OMC: It generates strongly overconfident approximations by collapsing regions of similar or near-constant likelihood into a single point. We propose an efficient, robust generalisation of OMC that corrects this. It makes fewer assumptions, retains the main benefits of OMC, and can be performed either as post-processing to OMC or as a stand-alone computation. We demonstrate the effectiveness of the proposed Robust OMC on toy examples and tasks in inverse-graphics where we perform Bayesian inference with a complex image renderer.

</details>

<details>

<summary>2020-02-28 15:18:08 - Calibration of Deep Probabilistic Models with Decoupled Bayesian Neural Networks</summary>

- *Juan Maroñas, Roberto Paredes, Daniel Ramos*

- `1908.08972v3` - [abs](http://arxiv.org/abs/1908.08972v3) - [pdf](http://arxiv.org/pdf/1908.08972v3)

> Deep Neural Networks (DNNs) have achieved state-of-the-art accuracy performance in many tasks. However, recent works have pointed out that the outputs provided by these models are not well-calibrated, seriously limiting their use in critical decision scenarios. In this work, we propose to use a decoupled Bayesian stage, implemented with a Bayesian Neural Network (BNN), to map the uncalibrated probabilities provided by a DNN to calibrated ones, consistently improving calibration. Our results evidence that incorporating uncertainty provides more reliable probabilistic models, a critical condition for achieving good calibration. We report a generous collection of experimental results using high-accuracy DNNs in standardized image classification benchmarks, showing the good performance, flexibility and robust behavior of our approach with respect to several state-of-the-art calibration methods. Code for reproducibility is provided.

</details>

<details>

<summary>2020-02-28 15:33:35 - Stochastic triangular mesh mapping: A terrain mapping technique for autonomous mobile robots</summary>

- *Clint D. Lombard, Corné E. van Daalen*

- `1910.03644v2` - [abs](http://arxiv.org/abs/1910.03644v2) - [pdf](http://arxiv.org/pdf/1910.03644v2)

> For mobile robots to operate autonomously in general environments, perception is required in the form of a dense metric map. For this purpose, we present the stochastic triangular mesh (STM) mapping technique: a 2.5-D representation of the surface of the environment using a continuous mesh of triangular surface elements, where each surface element models the mean plane and roughness of the underlying surface. In contrast to existing mapping techniques, a STM map models the structure of the environment by ensuring a continuous model, while also being able to be incrementally updated with linear computational cost in the number of measurements. We reduce the effect of uncertainty in the robot pose (position and orientation) by using landmark-relative submaps. The uncertainty in the measurements and robot pose are accounted for by the use of Bayesian inference techniques during the map update. We demonstrate that a STM map can be used with sensors that generate point measurements, such as light detection and ranging (LiDAR) sensors and stereo cameras. We show that a STM map is a more accurate model than the only comparable online surface mapping technique$\unicode{x2014}$a standard elevation map$\unicode{x2014}$and we also provide qualitative results on practical datasets.

</details>

<details>

<summary>2020-02-28 17:41:26 - Toward a principled Bayesian workflow in cognitive science</summary>

- *Daniel J. Schad, Michael Betancourt, Shravan Vasishth*

- `1904.12765v3` - [abs](http://arxiv.org/abs/1904.12765v3) - [pdf](http://arxiv.org/pdf/1904.12765v3)

> Experiments in research on memory, language, and in other areas of cognitive science are increasingly being analyzed using Bayesian methods. This has been facilitated by the development of probabilistic programming languages such as Stan, and easily accessible front-end packages such as brms. The utility of Bayesian methods, however, ultimately depends on the relevance of the Bayesian model, in particular whether or not it accurately captures the structure of the data and the data analyst's domain expertise. Even with powerful software, the analyst is responsible for verifying the utility of their model. To demonstrate this point, we introduce a principled Bayesian workflow (Betancourt, 2018) to cognitive science. Using a concrete working example, we describe basic questions one should ask about the model: prior predictive checks, computational faithfulness, model sensitivity, and posterior predictive checks. The running example for demonstrating the workflow is data on reading times with a linguistic manipulation of object versus subject relative clause sentences. This principled Bayesian workflow also demonstrates how to use domain knowledge to inform prior distributions. It provides guidelines and checks for valid data analysis, avoiding overfitting complex models to noise, and capturing relevant data structure in a probabilistic model. Given the increasing use of Bayesian methods, we aim to discuss how these methods can be properly employed to obtain robust answers to scientific questions. All data and code accompanying this paper are available from https://osf.io/b2vx9/.

</details>

<details>

<summary>2020-02-28 18:34:38 - Model interpretation through lower-dimensional posterior summarization</summary>

- *Spencer Woody, Carlos M. Carvalho, Jared S. Murray*

- `1905.07103v4` - [abs](http://arxiv.org/abs/1905.07103v4) - [pdf](http://arxiv.org/pdf/1905.07103v4)

> Nonparametric regression models have recently surged in their power and popularity, accompanying the trend of increasing dataset size and complexity. While these models have proven their predictive ability in empirical settings, they are often difficult to interpret and do not address the underlying inferential goals of the analyst or decision maker. In this paper, we propose a modular two-stage approach for creating parsimonious, interpretable summaries of complex models which allow freedom in the choice of modeling technique and the inferential target. In the first stage a flexible model is fit which is believed to be as accurate as possible. In the second stage, lower-dimensional summaries are constructed by projecting draws from the distribution onto simpler structures. These summaries naturally come with valid Bayesian uncertainty estimates. Further, since we use the data only once to move from prior to posterior, these uncertainty estimates remain valid across multiple summaries and after iteratively refining a summary. We apply our method and demonstrate its strengths across a range of simulated and real datasets. Code to reproduce the examples shown is avaiable at github.com/spencerwoody/ghost

</details>

<details>

<summary>2020-02-29 03:08:37 - Variational Bayesian Weighted Complex Network Reconstruction</summary>

- *Shuang Xu, Chun-Xia Zhang, Pei Wang, Jiangshe Zhang*

- `1812.04369v3` - [abs](http://arxiv.org/abs/1812.04369v3) - [pdf](http://arxiv.org/pdf/1812.04369v3)

> Complex network reconstruction is a hot topic in many fields. Currently, the most popular data-driven reconstruction framework is based on lasso. However, it is found that, in the presence of noise, lasso loses efficiency for weighted networks. This paper builds a new framework to cope with this problem. The key idea is to employ a series of linear regression problems to model the relationship between network nodes, and then to use an efficient variational Bayesian algorithm to infer the unknown coefficients. The numerical experiments conducted on both synthetic and real data demonstrate that the new method outperforms lasso with regard to both reconstruction accuracy and running speed.

</details>

<details>

<summary>2020-02-29 03:36:22 - On Generalization Error Bounds of Noisy Gradient Methods for Non-Convex Learning</summary>

- *Jian Li, Xuanyuan Luo, Mingda Qiao*

- `1902.00621v4` - [abs](http://arxiv.org/abs/1902.00621v4) - [pdf](http://arxiv.org/pdf/1902.00621v4)

> Generalization error (also known as the out-of-sample error) measures how well the hypothesis learned from training data generalizes to previously unseen data. Proving tight generalization error bounds is a central question in statistical learning theory. In this paper, we obtain generalization error bounds for learning general non-convex objectives, which has attracted significant attention in recent years. We develop a new framework, termed Bayes-Stability, for proving algorithm-dependent generalization error bounds. The new framework combines ideas from both the PAC-Bayesian theory and the notion of algorithmic stability. Applying the Bayes-Stability method, we obtain new data-dependent generalization bounds for stochastic gradient Langevin dynamics (SGLD) and several other noisy gradient methods (e.g., with momentum, mini-batch and acceleration, Entropy-SGD). Our result recovers (and is typically tighter than) a recent result in Mou et al. (2018) and improves upon the results in Pensia et al. (2018). Our experiments demonstrate that our data-dependent bounds can distinguish randomly labelled data from normal data, which provides an explanation to the intriguing phenomena observed in Zhang et al. (2017a). We also study the setting where the total loss is the sum of a bounded loss and an additional \ell_2 regularization term. We obtain new generalization bounds for the continuous Langevin dynamic in this setting by developing a new Log-Sobolev inequality for the parameter distribution at any time. Our new bounds are more desirable when the noisy level of the process is not small, and do not become vacuous even when T tends to infinity.

</details>

<details>

<summary>2020-02-29 04:06:03 - Validated Variational Inference via Practical Posterior Error Bounds</summary>

- *Jonathan H. Huggins, Mikołaj Kasprzak, Trevor Campbell, Tamara Broderick*

- `1910.04102v4` - [abs](http://arxiv.org/abs/1910.04102v4) - [pdf](http://arxiv.org/pdf/1910.04102v4)

> Variational inference has become an increasingly attractive fast alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, a major obstacle to the widespread use of variational methods is the lack of post-hoc accuracy measures that are both theoretically justified and computationally efficient. In this paper, we provide rigorous bounds on the error of posterior mean and uncertainty estimates that arise from full-distribution approximations, as in variational inference. Our bounds are widely applicable, as they require only that the approximating and exact posteriors have polynomial moments. Our bounds are also computationally efficient for variational inference because they require only standard values from variational objectives, straightforward analytic calculations, and simple Monte Carlo estimates. We show that our analysis naturally leads to a new and improved workflow for validated variational inference. Finally, we demonstrate the utility of our proposed workflow and error bounds on a robust regression problem and on a real-data example with a widely used multilevel hierarchical model.

</details>

<details>

<summary>2020-02-29 06:57:43 - AMAGOLD: Amortized Metropolis Adjustment for Efficient Stochastic Gradient MCMC</summary>

- *Ruqi Zhang, A. Feder Cooper, Christopher De Sa*

- `2003.00193v1` - [abs](http://arxiv.org/abs/2003.00193v1) - [pdf](http://arxiv.org/pdf/2003.00193v1)

> Stochastic gradient Hamiltonian Monte Carlo (SGHMC) is an efficient method for sampling from continuous distributions. It is a faster alternative to HMC: instead of using the whole dataset at each iteration, SGHMC uses only a subsample. This improves performance, but introduces bias that can cause SGHMC to converge to the wrong distribution. One can prevent this using a step size that decays to zero, but such a step size schedule can drastically slow down convergence. To address this tension, we propose a novel second-order SG-MCMC algorithm---AMAGOLD---that infrequently uses Metropolis-Hastings (M-H) corrections to remove bias. The infrequency of corrections amortizes their cost. We prove AMAGOLD converges to the target distribution with a fixed, rather than a diminishing, step size, and that its convergence rate is at most a constant factor slower than a full-batch baseline. We empirically demonstrate AMAGOLD's effectiveness on synthetic distributions, Bayesian logistic regression, and Bayesian neural networks.

</details>

<details>

<summary>2020-02-29 22:41:21 - Survival Cluster Analysis</summary>

- *Paidamoyo Chapfuwa, Chunyuan Li, Nikhil Mehta, Lawrence Carin, Ricardo Henao*

- `2003.00355v1` - [abs](http://arxiv.org/abs/2003.00355v1) - [pdf](http://arxiv.org/pdf/2003.00355v1)

> Conventional survival analysis approaches estimate risk scores or individualized time-to-event distributions conditioned on covariates. In practice, there is often great population-level phenotypic heterogeneity, resulting from (unknown) subpopulations with diverse risk profiles or survival distributions. As a result, there is an unmet need in survival analysis for identifying subpopulations with distinct risk profiles, while jointly accounting for accurate individualized time-to-event predictions. An approach that addresses this need is likely to improve characterization of individual outcomes by leveraging regularities in subpopulations, thus accounting for population-level heterogeneity. In this paper, we propose a Bayesian nonparametrics approach that represents observations (subjects) in a clustered latent space, and encourages accurate time-to-event predictions and clusters (subpopulations) with distinct risk profiles. Experiments on real-world datasets show consistent improvements in predictive performance and interpretability relative to existing state-of-the-art survival analysis models.

</details>

<details>

<summary>2020-02-29 23:46:37 - Asynchronous Gibbs Sampling</summary>

- *Alexander Terenin, Daniel Simpson, David Draper*

- `1509.08999v7` - [abs](http://arxiv.org/abs/1509.08999v7) - [pdf](http://arxiv.org/pdf/1509.08999v7)

> Gibbs sampling is a Markov Chain Monte Carlo (MCMC) method often used in Bayesian learning. MCMC methods can be difficult to deploy on parallel and distributed systems due to their inherently sequential nature. We study asynchronous Gibbs sampling, which achieves parallelism by simply ignoring sequential requirements. This method has been shown to produce good empirical results for some hierarchical models, and is popular in the topic modeling community, but was also shown to diverge for other targets. We introduce a theoretical framework for analyzing asynchronous Gibbs sampling and other extensions of MCMC that do not possess the Markov property. We prove that asynchronous Gibbs can be modified so that it converges under appropriate regularity conditions -- we call this the exact asynchronous Gibbs algorithm. We study asynchronous Gibbs on a set of examples by comparing the exact and approximate algorithms, including two where it works well, and one where it fails dramatically. We conclude with a set of heuristics to describe settings where the algorithm can be effectively used.

</details>


## 2020-03

<details>

<summary>2020-03-01 00:46:36 - PlaNet of the Bayesians: Reconsidering and Improving Deep Planning Network by Incorporating Bayesian Inference</summary>

- *Masashi Okada, Norio Kosaka, Tadahiro Taniguchi*

- `2003.00370v1` - [abs](http://arxiv.org/abs/2003.00370v1) - [pdf](http://arxiv.org/pdf/2003.00370v1)

> In the present paper, we propose an extension of the Deep Planning Network (PlaNet), also referred to as PlaNet of the Bayesians (PlaNet-Bayes). There has been a growing demand in model predictive control (MPC) in partially observable environments in which complete information is unavailable because of, for example, lack of expensive sensors. PlaNet is a promising solution to realize such latent MPC, as it is used to train state-space models via model-based reinforcement learning (MBRL) and to conduct planning in the latent space. However, recent state-of-the-art strategies mentioned in MBRR literature, such as involving uncertainty into training and planning, have not been considered, significantly suppressing the training performance. The proposed extension is to make PlaNet uncertainty-aware on the basis of Bayesian inference, in which both model and action uncertainty are incorporated. Uncertainty in latent models is represented using a neural network ensemble to approximately infer model posteriors. The ensemble of optimal action candidates is also employed to capture multimodal uncertainty in the optimality. The concept of the action ensemble relies on a general variational inference MPC (VI-MPC) framework and its instance, probabilistic action ensemble with trajectory sampling (PaETS). In this paper, we extend VI-MPC and PaETS, which have been originally introduced in previous literature, to address partially observable cases. We experimentally compare the performances on continuous control tasks, and conclude that our method can consistently improve the asymptotic performance compared with PlaNet.

</details>

<details>

<summary>2020-03-01 03:01:51 - A Theoretical Case Study of Structured Variational Inference for Community Detection</summary>

- *Mingzhang Yin, Y. X. Rachel Wang, Purnamrita Sarkar*

- `1907.12203v5` - [abs](http://arxiv.org/abs/1907.12203v5) - [pdf](http://arxiv.org/pdf/1907.12203v5)

> Mean-field variational inference (MFVI) has been widely applied in large scale Bayesian inference. However MFVI, which assumes a product distribution on the latent variables, often leads to objective functions with many local optima, making optimization algorithms sensitive to initialization. In this paper, we study the advantage of structured variational inference for the two class Stochastic Blockmodel. The variational distribution is constructed to have pairwise dependency structure on the nodes of the network. We prove that, in a broad density regime and for general random initializations, unlike MFVI, the class labels estimated from our method converge to the ground truth with high probability, when the model parameters are known, estimated within a reasonable range or jointly optimized with the variational parameters. In addition, empirically we demonstrate structured VI is more robust compared with MFVI when the graph is sparse and the signal to noise ratio is low. The paper takes a first step towards understanding the importance of dependency structure in variational inference for community detection.

</details>

<details>

<summary>2020-03-01 16:42:38 - Probabilistic Trajectory Segmentation by Means of Hierarchical Dirichlet Process Switching Linear Dynamical Systems</summary>

- *Maximilian Sieb, Matthias Schultheis, Sebastian Szelag, Rudolf Lioutikov, Jan Peters*

- `1806.06063v3` - [abs](http://arxiv.org/abs/1806.06063v3) - [pdf](http://arxiv.org/pdf/1806.06063v3)

> Using movement primitive libraries is an effective means to enable robots to solve more complex tasks. In order to build these movement libraries, current algorithms require a prior segmentation of the demonstration trajectories. A promising approach is to model the trajectory as being generated by a set of Switching Linear Dynamical Systems and inferring a meaningful segmentation by inspecting the transition points characterized by the switching dynamics. With respect to the learning, a nonparametric Bayesian approach is employed utilizing a Gibbs sampler.

</details>

<details>

<summary>2020-03-01 20:14:58 - Fast and Exact Simulation of Multivariate Normal and Wishart Random Variables with Box Constraints</summary>

- *Hillary Koch, Gregory P. Bopp*

- `1907.00057v2` - [abs](http://arxiv.org/abs/1907.00057v2) - [pdf](http://arxiv.org/pdf/1907.00057v2)

> Models which include domain constraints occur in myriad contexts such as econometrics, genomics, and environmetrics, though simulating from constrained distributions can be computationally expensive. In particular, repeated sampling from constrained distributions is a common task in Bayesian inferential methods, where coping with these constraints can cause troublesome computational burden. Here, we introduce computationally efficient methods to make exact and independent draws from both the multivariate normal and Wishart distributions with box constraints. In both cases, these variables are sampled using a direct algorithm. By substantially reducing computing time, these new algorithms improve the feasibility of Monte Carlo-based inference for box-constrained, multivariate normal and Wishart distributions.

</details>

<details>

<summary>2020-03-01 21:31:01 - True and false discoveries with independent e-values</summary>

- *Vladimir Vovk, Ruodu Wang*

- `2003.00593v1` - [abs](http://arxiv.org/abs/2003.00593v1) - [pdf](http://arxiv.org/pdf/2003.00593v1)

> In this note we use e-values (a non-Bayesian version of Bayes factors) in the context of multiple hypothesis testing assuming that the base tests produce independent e-values. Our simulation studies and theoretical considerations suggest that, under this assumption, our new algorithms are superior to the known algorithms using independent p-values and to our recent algorithms using e-values that are not necessarily independent.

</details>

<details>

<summary>2020-03-02 01:01:57 - Multivariate, Multistep Forecasting, Reconstruction and Feature Selection of Ocean Waves via Recurrent and Sequence-to-Sequence Networks</summary>

- *Mohammad Pirhooshyaran, Lawrence V. Snyder*

- `1906.00195v2` - [abs](http://arxiv.org/abs/1906.00195v2) - [pdf](http://arxiv.org/pdf/1906.00195v2)

> This article explores the concepts of ocean wave multivariate multistep forecasting, reconstruction and feature selection. We introduce recurrent neural network frameworks, integrated with Bayesian hyperparameter optimization and Elastic Net methods. We consider both short- and long-term forecasts and reconstruction, for significant wave height and output power of the ocean waves. Sequence-to-sequence neural networks are being developed for the first time to reconstruct the missing characteristics of ocean waves based on information from nearby wave sensors. Our results indicate that the Adam and AMSGrad optimization algorithms are the most robust ones to optimize the sequence-to-sequence network. For the case of significant wave height reconstruction, we compare the proposed methods with alternatives on a well-studied dataset. We show the superiority of the proposed methods considering several error metrics. We design a new case study based on measurement stations along the east coast of the United States and investigate the feature selection concept. Comparisons substantiate the benefit of utilizing Elastic Net. Moreover, case study results indicate that when the number of features is considerable, having deeper structures improves the performance.

</details>

<details>

<summary>2020-03-02 06:16:03 - New Estimation Approaches for the Hierarchical Linear Ballistic Accumulator Model</summary>

- *David Gunawan, Guy E. Hawkins, Minh-Ngoc Tran, Robert Kohn, Scott Brown*

- `1806.10089v5` - [abs](http://arxiv.org/abs/1806.10089v5) - [pdf](http://arxiv.org/pdf/1806.10089v5)

> The Linear Ballistic Accumulator (Brown & Heathcote, 2008) model is used as a measurement tool to answer questions about applied psychology. The analyses based on this model depend upon the model selected and its estimated parameters. Modern approaches use hierarchical Bayesian models and Markov chain Monte-Carlo (MCMC) methods to estimate the posterior distribution of the parameters. Although there are several approaches available for model selection, they are all based on the posterior samples produced via MCMC, which means that the model selection inference inherits the properties of the MCMC sampler. To improve on current approaches to LBA inference we propose two methods that are based on recent advances in particle MCMC methodology; they are qualitatively different from existing approaches as well as from each other. The first approach is particle Metropolis-within-Gibbs; the second approach is density tempered sequential Monte Carlo. Both new approaches provide very efficient sampling and can be applied to estimate the marginal likelihood, which provides Bayes factors for model selection. The first approach is usually faster. The second approach provides a direct estimate of the marginal likelihood, uses the first approach in its Markov move step and is very efficient to parallelize on high performance computers. The new methods are illustrated by applying them to simulated and real data, and through pseudo code. The code implementing the methods is freely available.

</details>

<details>

<summary>2020-03-02 13:41:14 - Integrals over Gaussians under Linear Domain Constraints</summary>

- *Alexandra Gessner, Oindrila Kanjilal, Philipp Hennig*

- `1910.09328v2` - [abs](http://arxiv.org/abs/1910.09328v2) - [pdf](http://arxiv.org/pdf/1910.09328v2)

> Integrals of linearly constrained multivariate Gaussian densities are a frequent problem in machine learning and statistics, arising in tasks like generalized linear models and Bayesian optimization. Yet they are notoriously hard to compute, and to further complicate matters, the numerical values of such integrals may be very small. We present an efficient black-box algorithm that exploits geometry for the estimation of integrals over a small, truncated Gaussian volume, and to simulate therefrom. Our algorithm uses the Holmes-Diaconis-Ross (HDR) method combined with an analytic version of elliptical slice sampling (ESS). Adapted to the linear setting, ESS allows for rejection-free sampling, because intersections of ellipses and domain boundaries have closed-form solutions. The key idea of HDR is to decompose the integral into easier-to-compute conditional probabilities by using a sequence of nested domains. Remarkably, it allows for direct computation of the logarithm of the integral value and thus enables the computation of extremely small probability masses. We demonstrate the effectiveness of our tailored combination of HDR and ESS on high-dimensional integrals and on entropy search for Bayesian optimization.

</details>

<details>

<summary>2020-03-02 16:23:41 - Generating Higher-Fidelity Synthetic Datasets with Privacy Guarantees</summary>

- *Aleksei Triastcyn, Boi Faltings*

- `2003.00997v1` - [abs](http://arxiv.org/abs/2003.00997v1) - [pdf](http://arxiv.org/pdf/2003.00997v1)

> This paper considers the problem of enhancing user privacy in common machine learning development tasks, such as data annotation and inspection, by substituting the real data with samples form a generative adversarial network. We propose employing Bayesian differential privacy as the means to achieve a rigorous theoretical guarantee while providing a better privacy-utility trade-off. We demonstrate experimentally that our approach produces higher-fidelity samples, compared to prior work, allowing to (1) detect more subtle data errors and biases, and (2) reduce the need for real data labelling by achieving high accuracy when training directly on artificial samples.

</details>

<details>

<summary>2020-03-02 16:30:59 - Robust Policy Search for Robot Navigation with Stochastic Meta-Policies</summary>

- *Javier Garcia-Barcos, Ruben Martinez-Cantin*

- `2003.01000v1` - [abs](http://arxiv.org/abs/2003.01000v1) - [pdf](http://arxiv.org/pdf/2003.01000v1)

> Bayesian optimization is an efficient nonlinear optimization method where the queries are carefully selected to gather information about the optimum location. Thus, in the context of policy search, it has been called active policy search. The main ingredients of Bayesian optimization for sample efficiency are the probabilistic surrogate model and the optimal decision heuristics. In this work, we exploit those to provide robustness to different issues for policy search algorithms. We combine several methods and show how their interaction works better than the sum of the parts. First, to deal with input noise and provide a safe and repeatable policy we use an improved version of unscented Bayesian optimization. Then, to deal with mismodeling errors and improve exploration we use stochastic meta-policies for query selection and an adaptive kernel. We compare the proposed algorithm with previous results in several optimization benchmarks and robot tasks, such as pushing objects with a robot arm, or path finding with a rover.

</details>

<details>

<summary>2020-03-02 18:06:27 - Gaussian Process Policy Optimization</summary>

- *Ashish Rao, Bidipta Sarkar, Tejas Narayanan*

- `2003.01074v1` - [abs](http://arxiv.org/abs/2003.01074v1) - [pdf](http://arxiv.org/pdf/2003.01074v1)

> We propose a novel actor-critic, model-free reinforcement learning algorithm which employs a Bayesian method of parameter space exploration to solve environments. A Gaussian process is used to learn the expected return of a policy given the policy's parameters. The system is trained by updating the parameters using gradient descent on a new surrogate loss function consisting of the Proximal Policy Optimization 'Clipped' loss function and a bonus term representing the expected improvement acquisition function given by the Gaussian process. This new method is shown to be comparable to and at times empirically outperform current algorithms on environments that simulate robotic locomotion using the MuJoCo physics engine.

</details>

<details>

<summary>2020-03-02 22:06:00 - Wasserstein Measure Coresets</summary>

- *Sebastian Claici, Aude Genevay, Justin Solomon*

- `1805.07412v2` - [abs](http://arxiv.org/abs/1805.07412v2) - [pdf](http://arxiv.org/pdf/1805.07412v2)

> The proliferation of large data sets and Bayesian inference techniques motivates demand for better data sparsification. Coresets provide a principled way of summarizing a large dataset via a smaller one that is guaranteed to match the performance of the full data set on specific problems. Classical coresets, however, neglect the underlying data distribution, which is often continuous. We address this oversight by introducing Wasserstein measure coresets, an extension of coresets which by definition takes into account generalization. Our formulation of the problem, which essentially consists in minimizing the Wasserstein distance, is solvable via stochastic gradient descent. This yields an algorithm which simply requires sample access to the data distribution and is able to handle large data streams in an online manner. We validate our construction for inference and clustering.

</details>

<details>

<summary>2020-03-03 01:25:45 - MPC-guided Imitation Learning of Neural Network Policies for the Artificial Pancreas</summary>

- *Hongkai Chen, Nicola Paoletti, Scott A. Smolka, Shan Lin*

- `2003.01283v1` - [abs](http://arxiv.org/abs/2003.01283v1) - [pdf](http://arxiv.org/pdf/2003.01283v1)

> Even though model predictive control (MPC) is currently the main algorithm for insulin control in the artificial pancreas (AP), it usually requires complex online optimizations, which are infeasible for resource-constrained medical devices. MPC also typically relies on state estimation, an error-prone process. In this paper, we introduce a novel approach to AP control that uses Imitation Learning to synthesize neural-network insulin policies from MPC-computed demonstrations. Such policies are computationally efficient and, by instrumenting MPC at training time with full state information, they can directly map measurements into optimal therapy decisions, thus bypassing state estimation. We apply Bayesian inference via Monte Carlo Dropout to learn policies, which allows us to quantify prediction uncertainty and thereby derive safer therapy decisions. We show that our control policies trained under a specific patient model readily generalize (in terms of model parameters and disturbance distributions) to patient cohorts, consistently outperforming traditional MPC with state estimation.

</details>

<details>

<summary>2020-03-03 02:47:49 - Instance-Dependent PU Learning by Bayesian Optimal Relabeling</summary>

- *Fengxiang He, Tongliang Liu, Geoffrey I Webb, Dacheng Tao*

- `1808.02180v2` - [abs](http://arxiv.org/abs/1808.02180v2) - [pdf](http://arxiv.org/pdf/1808.02180v2)

> When learning from positive and unlabelled data, it is a strong assumption that the positive observations are randomly sampled from the distribution of $X$ conditional on $Y = 1$, where X stands for the feature and Y the label. Most existing algorithms are optimally designed under the assumption. However, for many real-world applications, the observed positive examples are dependent on the conditional probability $P(Y = 1|X)$ and should be sampled biasedly. In this paper, we assume that a positive example with a higher $P(Y = 1|X)$ is more likely to be labelled and propose a probabilistic-gap based PU learning algorithms. Specifically, by treating the unlabelled data as noisy negative examples, we could automatically label a group positive and negative examples whose labels are identical to the ones assigned by a Bayesian optimal classifier with a consistency guarantee. The relabelled examples have a biased domain, which is remedied by the kernel mean matching technique. The proposed algorithm is model-free and thus do not have any parameters to tune. Experimental results demonstrate that our method works well on both generated and real-world datasets.

</details>

<details>

<summary>2020-03-03 06:30:43 - Linear-Time Algorithm in Bayesian Image Denoising based on Gaussian Markov Random Field</summary>

- *Muneki Yasuda, Junpei Watanabe, Shun Kataoka, kazuyuki Tanaka*

- `1710.07393v2` - [abs](http://arxiv.org/abs/1710.07393v2) - [pdf](http://arxiv.org/pdf/1710.07393v2)

> In this paper, we consider Bayesian image denoising based on a Gaussian Markov random field (GMRF) model, for which we propose an new algorithm. Our method can solve Bayesian image denoising problems, including hyperparameter estimation, in $O(n)$-time, where $n$ is the number of pixels in a given image. From the perspective of the order of the computational time, this is a state-of-the-art algorithm for the present problem setting. Moreover, the results of our numerical experiments we show our method is in fact effective in practice.

</details>

<details>

<summary>2020-03-03 09:18:51 - The joint distribution of pin-point plant cover data: a reparametrized Dirichlet -- multinomial distribution</summary>

- *Christian Damgaard*

- `1808.04582v7` - [abs](http://arxiv.org/abs/1808.04582v7) - [pdf](http://arxiv.org/pdf/1808.04582v7)

> A reparametrized Dirichlet-multinomial distribution is introduced, and the covariance matrix, as well as, the algorithm for calculating the PDF for n species are provided. The distribution is suited for modelling the joint distribution of pin-point cover data of spatially aggregated plant species, and the parametrization ensures that the degree of spatially aggregation is modelled by a parameter and the mean cover of the species is modelled by the other parameters. This last property is convenient for using the distribution in Bayesian hierarchical models where the mean cover of the different species typically is modelled as latent variables.

</details>

<details>

<summary>2020-03-03 13:52:08 - Spike-and-Slab Group Lassos for Grouped Regression and Sparse Generalized Additive Models</summary>

- *Ray Bai, Gemma E. Moran, Joseph Antonelli, Yong Chen, Mary R. Boland*

- `1903.01979v6` - [abs](http://arxiv.org/abs/1903.01979v6) - [pdf](http://arxiv.org/pdf/1903.01979v6)

> We introduce the spike-and-slab group lasso (SSGL) for Bayesian estimation and variable selection in linear regression with grouped variables. We further extend the SSGL to sparse generalized additive models (GAMs), thereby introducing the first nonparametric variant of the spike-and-slab lasso methodology. Our model simultaneously performs group selection and estimation, while our fully Bayes treatment of the mixture proportion allows for model complexity control and automatic self-adaptivity to different levels of sparsity. We develop theory to uniquely characterize the global posterior mode under the SSGL and introduce a highly efficient block coordinate ascent algorithm for maximum a posteriori (MAP) estimation. We further employ de-biasing methods to provide uncertainty quantification of our estimates. Thus, implementation of our model avoids the computational intensiveness of Markov chain Monte Carlo (MCMC) in high dimensions. We derive posterior concentration rates for both grouped linear regression and sparse GAMs when the number of covariates grows at nearly exponential rate with sample size. Finally, we illustrate our methodology through extensive simulations and data analysis.

</details>

<details>

<summary>2020-03-03 19:16:19 - Probabilistic Detection and Estimation of Conic Sections from Noisy Data</summary>

- *Subharup Guha, Sujit K. Ghosh*

- `1910.14078v2` - [abs](http://arxiv.org/abs/1910.14078v2) - [pdf](http://arxiv.org/pdf/1910.14078v2)

> Inferring unknown conic sections on the basis of noisy data is a challenging problem with applications in computer vision. A major limitation of the currently available methods for conic sections is that estimation methods rely on the underlying shape of the conics (being known to be ellipse, parabola or hyperbola). A general purpose Bayesian hierarchical model is proposed for conic sections and corresponding estimation method based on noisy data is shown to work even when the specific nature of the conic section is unknown. The model, thus, provides probabilistic detection of the underlying conic section and inference about the associated parameters of the conic section. Through extensive simulation studies where the true conics may not be known, the methodology is demonstrated to have practical and methodological advantages relative to many existing techniques. In addition, the proposed method provides probabilistic measures of uncertainty of the estimated parameters. Furthermore, we observe high fidelity to the true conics even in challenging situations, such as data arising from partial conics in arbitrarily rotated and non-standard form, and where a visual inspection is unable to correctly identify the type of conic section underlying the data.

</details>

<details>

<summary>2020-03-03 20:26:23 - TATi-Thermodynamic Analytics ToolkIt: TensorFlow-based software for posterior sampling in machine learning applications</summary>

- *Frederik Heber, Zofia Trstanova, Benedict Leimkuhler*

- `1903.08640v2` - [abs](http://arxiv.org/abs/1903.08640v2) - [pdf](http://arxiv.org/pdf/1903.08640v2)

> With the advent of GPU-assisted hardware and maturing high-efficiency software platforms such as TensorFlow and PyTorch, Bayesian posterior sampling for neural networks becomes plausible. In this article we discuss Bayesian parametrization in machine learning based on Markov Chain Monte Carlo methods, specifically discretized stochastic differential equations such as Langevin dynamics and extended system methods in which an ensemble of walkers is employed to enhance sampling. We provide a glimpse of the potential of the sampling-intensive approach by studying (and visualizing) the loss landscape of a neural network applied to the MNIST data set. Moreover, we investigate how the sampling efficiency itself can be significantly enhanced through an ensemble quasi-Newton preconditioning method. This article accompanies the release of a new TensorFlow software package, the Thermodynamic Analytics ToolkIt, which is used in the computational experiments.

</details>

<details>

<summary>2020-03-03 22:37:57 - Multi-Attribute Bayesian Optimization With Interactive Preference Learning</summary>

- *Raul Astudillo, Peter I. Frazier*

- `1911.05934v2` - [abs](http://arxiv.org/abs/1911.05934v2) - [pdf](http://arxiv.org/pdf/1911.05934v2)

> We consider black-box global optimization of time-consuming-to-evaluate functions on behalf of a decision-maker (DM) whose preferences must be learned. Each feasible design is associated with a time-consuming-to-evaluate vector of attributes and each vector of attributes is assigned a utility by the DM's utility function, which may be learned approximately using preferences expressed over pairs of attribute vectors. Past work has used a point estimate of this utility function as if it were error-free within single-objective optimization. However, utility estimation errors may yield a poor suggested design. Furthermore, this approach produces a single suggested "best" design, whereas DMs often prefer to choose from a menu. We propose a novel multi-attribute Bayesian optimization with preference learning approach. Our approach acknowledges the uncertainty in preference estimation and implicitly chooses designs to evaluate that are good not just for a single estimated utility function but a range of likely ones. The outcome of our approach is a menu of designs and evaluated attributes from which the DM makes a final selection. We demonstrate the value and flexibility of our approach in a variety of experiments.

</details>

<details>

<summary>2020-03-04 04:43:16 - Meta Cyclical Annealing Schedule: A Simple Approach to Avoiding Meta-Amortization Error</summary>

- *Yusuke Hayashi, Taiji Suzuki*

- `2003.01889v1` - [abs](http://arxiv.org/abs/2003.01889v1) - [pdf](http://arxiv.org/pdf/2003.01889v1)

> The ability to learn new concepts with small amounts of data is a crucial aspect of intelligence that has proven challenging for deep learning methods. Meta-learning for few-shot learning offers a potential solution to this problem: by learning to learn across data from many previous tasks, few-shot learning algorithms can discover the structure among tasks to enable fast learning of new tasks. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be very ambiguous to acquire a single model for that task. The Bayesian meta-learning models can naturally resolve this problem by putting a sophisticated prior distribution and let the posterior well regularized through Bayesian decision theory. However, currently known Bayesian meta-learning procedures such as VERSA suffer from the so-called {\it information preference problem}, that is, the posterior distribution is degenerated to one point and is far from the exact one. To address this challenge, we design a novel meta-regularization objective using {\it cyclical annealing schedule} and {\it maximum mean discrepancy} (MMD) criterion. The cyclical annealing schedule is quite effective at avoiding such degenerate solutions. This procedure includes a difficult KL-divergence estimation, but we resolve the issue by employing MMD instead of KL-divergence. The experimental results show that our approach substantially outperforms standard meta-learning algorithms.

</details>

<details>

<summary>2020-03-04 08:35:58 - Iterative Bayesian-based Localization Mechanism for Industry Verticals</summary>

- *Henrique Hilleshein, Carlos H. M. de Lima, Hirley Alves, Matti Latva-aho*

- `2001.04791v2` - [abs](http://arxiv.org/abs/2001.04791v2) - [pdf](http://arxiv.org/pdf/2001.04791v2)

> We propose and evaluate an iterative localization mechanism employing Bayesian inference to estimate the position of a target using received signal strength measurements. The probability density functions of the target's coordinates are estimated through a Bayesian network. Herein, we consider an iterative procedure whereby our predictor (posterior distribution) is updated in a sequential order whenever new measurements are made available. The performance of the mechanism is assessed in terms of the respective root mean square error and kernel density estimation of the target coordinates. Our numerical results showed the proposed iterative mechanism achieves increasingly better estimation of the target node position each updating round of the Bayesian network with new input measurements.

</details>

<details>

<summary>2020-03-04 13:57:15 - Distributed, partially collapsed MCMC for Bayesian Nonparametrics</summary>

- *Avinava Dubey, Michael Minyi Zhang, Eric P. Xing, Sinead A. Williamson*

- `2001.05591v3` - [abs](http://arxiv.org/abs/2001.05591v3) - [pdf](http://arxiv.org/pdf/2001.05591v3)

> Bayesian nonparametric (BNP) models provide elegant methods for discovering underlying latent features within a data set, but inference in such models can be slow. We exploit the fact that completely random measures, which commonly used models like the Dirichlet process and the beta-Bernoulli process can be expressed as, are decomposable into independent sub-measures. We use this decomposition to partition the latent measure into a finite measure containing only instantiated components, and an infinite measure containing all other components. We then select different inference algorithms for the two components: uncollapsed samplers mix well on the finite measure, while collapsed samplers mix well on the infinite, sparsely occupied tail. The resulting hybrid algorithm can be applied to a wide class of models, and can be easily distributed to allow scalable inference without sacrificing asymptotic convergence guarantees.

</details>

<details>

<summary>2020-03-04 14:48:13 - Model error covariance estimation in particle and ensemble Kalman filters using an online expectation-maximization algorithm</summary>

- *Tadeo Javier Cocucci, Manuel Pulido, Magdalena Lucini, Pierre Tandeo*

- `2003.02109v1` - [abs](http://arxiv.org/abs/2003.02109v1) - [pdf](http://arxiv.org/pdf/2003.02109v1)

> The performance of ensemble-based data assimilation techniques that estimate the state of a dynamical system from partial observations depends crucially on the prescribed uncertainty of the model dynamics and of the observations. These are not usually known and have to be inferred. Many approaches have been proposed to tackle this problem, including fully Bayesian, likelihood maximization and innovation-based techniques. This work focuses on maximization of the likelihood function via the expectation-maximization (EM) algorithm to infer the model error covariance combined with ensemble Kalman filters and particle filters to estimate the state. The classical application of the EM algorithm in a data assimilation context involves filtering and smoothing a fixed batch of observations in order to complete a single iteration. This is an inconvenience when using sequential filtering in high-dimensional applications. Motivated by this, an adaptation of the algorithm that can process observations and update the parameters on the fly, with some underlying simplifications, is presented. The proposed technique was evaluated and achieved good performance in experiments with the Lorenz-63 and the 40-variable Lorenz-96 dynamical systems designed to represent some common scenarios in data assimilation such as non-linearity, chaoticity and model misspecification.

</details>

<details>

<summary>2020-03-04 15:04:12 - Bayesian Image Classification with Deep Convolutional Gaussian Processes</summary>

- *Vincent Dutordoir, Mark van der Wilk, Artem Artemev, James Hensman*

- `1902.05888v2` - [abs](http://arxiv.org/abs/1902.05888v2) - [pdf](http://arxiv.org/pdf/1902.05888v2)

> In decision-making systems, it is important to have classifiers that have calibrated uncertainties, with an optimisation objective that can be used for automated model selection and training. Gaussian processes (GPs) provide uncertainty estimates and a marginal likelihood objective, but their weak inductive biases lead to inferior accuracy. This has limited their applicability in certain tasks (e.g. image classification). We propose a translation-insensitive convolutional kernel, which relaxes the translation invariance constraint imposed by previous convolutional GPs. We show how we can use the marginal likelihood to learn the degree of insensitivity. We also reformulate GP image-to-image convolutional mappings as multi-output GPs, leading to deep convolutional GPs. We show experimentally that our new kernel improves performance in both single-layer and deep models. We also demonstrate that our fully Bayesian approach improves on dropout-based Bayesian deep learning methods in terms of uncertainty and marginal likelihood estimates.

</details>

<details>

<summary>2020-03-04 17:35:29 - Generate Descriptive Social Networks for Large Populations from Available Observations: A Novel Methodology and a Generator</summary>

- *Samuel Thiriot*

- `2003.02213v1` - [abs](http://arxiv.org/abs/2003.02213v1) - [pdf](http://arxiv.org/pdf/2003.02213v1)

> When modeling a social dynamics with an agent-oriented approach, researchers have to describe the structure of interactions within the population. Given the intractability of extensive network collecting, they rely on random network generators that are supposed to explore the space of plausible networks. We first identify the needs of modelers, including placing heterogeneous agents on the network given their attributes and differentiating the various types of social links that lead to different interactions. We point out the existence of data in the form of scattered statistics and qualitative observations, that should be used to parameter the generator. We propose a new approach peculiar to agent-based modeling, in which we will generate social links from individuals' observed attributes, and return them as a multiplex network. Interdependencies between socioeconomic attributes, and generative rules, are encoded as Bayesian networks. A methodology guides modelers through the formalization of these parameters. This approach is illustrated by describing the structure of interactions that supports diffusion of contraceptive solutions in rural Kenya.

</details>

<details>

<summary>2020-03-04 22:48:30 - Bayesian System ID: Optimal management of parameter, model, and measurement uncertainty</summary>

- *Nicholas Galioto, Alex Gorodetsky*

- `2003.02359v1` - [abs](http://arxiv.org/abs/2003.02359v1) - [pdf](http://arxiv.org/pdf/2003.02359v1)

> We evaluate the robustness of a probabilistic formulation of system identification (ID) to sparse, noisy, and indirect data. Specifically, we compare estimators of future system behavior derived from the Bayesian posterior of a learning problem to several commonly used least squares-based optimization objectives used in system ID. Our comparisons indicate that the log posterior has improved geometric properties compared with the objective function surfaces of traditional methods that include differentially constrained least squares and least squares reconstructions of discrete time steppers like dynamic mode decomposition (DMD). These properties allow it to be both more sensitive to new data and less affected by multiple minima --- overall yielding a more robust approach. Our theoretical results indicate that least squares and regularized least squares methods like dynamic mode decomposition and sparse identification of nonlinear dynamics (SINDy) can be derived from the probabilistic formulation by assuming noiseless measurements. We also analyze the computational complexity of a Gaussian filter-based approximate marginal Markov Chain Monte Carlo scheme that we use to obtain the Bayesian posterior for both linear and nonlinear problems. We then empirically demonstrate that obtaining the marginal posterior of the parameter dynamics and making predictions by extracting optimal estimators (e.g., mean, median, mode) yields orders of magnitude improvement over the aforementioned approaches. We attribute this performance to the fact that the Bayesian approach captures parameter, model, and measurement uncertainties, whereas the other methods typically neglect at least one type of uncertainty.

</details>

<details>

<summary>2020-03-05 05:55:07 - Multi-Output Gaussian Processes for Multi-Population Longevity Modeling</summary>

- *Nhan Huynh, Mike Ludkovski*

- `2003.02443v1` - [abs](http://arxiv.org/abs/2003.02443v1) - [pdf](http://arxiv.org/pdf/2003.02443v1)

> We investigate joint modeling of longevity trends using the spatial statistical framework of Gaussian Process regression. Our analysis is motivated by the Human Mortality Database (HMD) that provides unified raw mortality tables for nearly 40 countries. Yet few stochastic models exist for handling more than two populations at a time. To bridge this gap, we leverage a spatial covariance framework from machine learning that treats populations as distinct levels of a factor covariate, explicitly capturing the cross-population dependence. The proposed multi-output Gaussian Process models straightforwardly scale up to a dozen populations and moreover intrinsically generate coherent joint longevity scenarios. In our numerous case studies we investigate predictive gains from aggregating mortality experience across nations and genders, including by borrowing the most recently available "foreign" data. We show that in our approach, information fusion leads to more precise (and statistically more credible) forecasts. We implement our models in \texttt{R}, as well as a Bayesian version in \texttt{Stan} that provides further uncertainty quantification regarding the estimated mortality covariance structure. All examples utilize public HMD datasets.

</details>

<details>

<summary>2020-03-05 06:34:53 - Semi-supervised Learning Meets Factorization: Learning to Recommend with Chain Graph Model</summary>

- *Chaochao Chen, Kevin C. Chang, Qibing Li, Xiaolin Zheng*

- `2003.02452v1` - [abs](http://arxiv.org/abs/2003.02452v1) - [pdf](http://arxiv.org/pdf/2003.02452v1)

> Recently latent factor model (LFM) has been drawing much attention in recommender systems due to its good performance and scalability. However, existing LFMs predict missing values in a user-item rating matrix only based on the known ones, and thus the sparsity of the rating matrix always limits their performance. Meanwhile, semi-supervised learning (SSL) provides an effective way to alleviate the label (i.e., rating) sparsity problem by performing label propagation, which is mainly based on the smoothness insight on affinity graphs. However, graph-based SSL suffers serious scalability and graph unreliable problems when directly being applied to do recommendation. In this paper, we propose a novel probabilistic chain graph model (CGM) to marry SSL with LFM. The proposed CGM is a combination of Bayesian network and Markov random field. The Bayesian network is used to model the rating generation and regression procedures, and the Markov random field is used to model the confidence-aware smoothness constraint between the generated ratings. Experimental results show that our proposed CGM significantly outperforms the state-of-the-art approaches in terms of four evaluation metrics, and with a larger performance margin when data sparsity increases.

</details>

<details>

<summary>2020-03-05 06:35:51 - Individual Claims Forecasting with Bayesian Mixture Density Networks</summary>

- *Kevin Kuo*

- `2003.02453v1` - [abs](http://arxiv.org/abs/2003.02453v1) - [pdf](http://arxiv.org/pdf/2003.02453v1)

> We introduce an individual claims forecasting framework utilizing Bayesian mixture density networks that can be used for claims analytics tasks such as case reserving and triaging. The proposed approach enables incorporating claims information from both structured and unstructured data sources, producing multi-period cash flow forecasts, and generating different scenarios of future payment patterns. We implement and evaluate the modeling framework using publicly available data.

</details>

<details>

<summary>2020-03-05 10:05:06 - Efficient Bayesian estimation for GARCH-type models via Sequential Monte Carlo</summary>

- *Dan Li, Adam Clements, Christopher Drovandi*

- `1906.03828v2` - [abs](http://arxiv.org/abs/1906.03828v2) - [pdf](http://arxiv.org/pdf/1906.03828v2)

> The advantages of sequential Monte Carlo (SMC) are exploited to develop parameter estimation and model selection methods for GARCH (Generalized AutoRegressive Conditional Heteroskedasticity) style models. It provides an alternative method for quantifying estimation uncertainty relative to classical inference. Even with long time series, it is demonstrated that the posterior distribution of model parameters are non-normal, highlighting the need for a Bayesian approach and an efficient posterior sampling method. Efficient approaches for both constructing the sequence of distributions in SMC, and leave-one-out cross-validation, for long time series data are also proposed. Finally, an unbiased estimator of the likelihood is developed for the Bad Environment-Good Environment model, a complex GARCH-type model, which permits exact Bayesian inference not previously available in the literature.

</details>

<details>

<summary>2020-03-05 12:02:44 - Adaptive Prediction Timing for Electronic Health Records</summary>

- *Jacob Deasy, Ari Ercole, Pietro Liò*

- `2003.02554v1` - [abs](http://arxiv.org/abs/2003.02554v1) - [pdf](http://arxiv.org/pdf/2003.02554v1)

> In realistic scenarios, multivariate timeseries evolve over case-by-case time-scales. This is particularly clear in medicine, where the rate of clinical events varies by ward, patient, and application. Increasingly complex models have been shown to effectively predict patient outcomes, but have failed to adapt granularity to these inherent temporal resolutions. As such, we introduce a novel, more realistic, approach to generating patient outcome predictions at an adaptive rate based on uncertainty accumulation in Bayesian recurrent models. We use a Recurrent Neural Network (RNN) and a Bayesian embedding layer with a new aggregation method to demonstrate adaptive prediction timing. Our model predicts more frequently when events are dense or the model is certain of event latent representations, and less frequently when readings are sparse or the model is uncertain. At 48 hours after patient admission, our model achieves equal performance compared to its static-windowed counterparts, while generating patient- and event-specific prediction timings that lead to improved predictive performance over the crucial first 12 hours of the patient stay.

</details>

<details>

<summary>2020-03-05 16:58:47 - Bayesian A/B Testing for Business Decisions</summary>

- *Shafi Kamalbasha, Manuel J. A. Eugster*

- `2003.02769v1` - [abs](http://arxiv.org/abs/2003.02769v1) - [pdf](http://arxiv.org/pdf/2003.02769v1)

> Controlled experiments (A/B tests or randomized field experiments) are the de facto standard to make data-driven decisions when implementing changes and observing customer responses. The methodology to analyze such experiments should be easily understandable to stakeholders like product and marketing managers. Bayesian inference recently gained a lot of popularity and, in terms of A/B testing, one key argument is the easy interpretability. For stakeholders, "probability to be best" (with corresponding credible intervals) provides a natural metric to make business decisions. In this paper, we motivate the quintessential questions a business owner typically has and how to answer them with a Bayesian approach. We present three experiment scenarios that are common in our company, how they are modeled in a Bayesian fashion, and how to use the models to draw business decisions. For each of the scenarios, we present a real-world experiment, the results and the final business decisions drawn.

</details>

<details>

<summary>2020-03-06 08:52:49 - Modeling Network Populations via Graph Distances</summary>

- *Simón Lunagómez, Sofia C. Olhede, Patrick J. Wolfe*

- `1904.07367v2` - [abs](http://arxiv.org/abs/1904.07367v2) - [pdf](http://arxiv.org/pdf/1904.07367v2)

> This article introduces a new class of models for multiple networks. The core idea is to parametrize a distribution on labelled graphs in terms of a Fr\'{e}chet mean graph (which depends on a user-specified choice of metric or graph distance) and a parameter that controls the concentration of this distribution about its mean. Entropy is the natural parameter for such control, varying from a point mass concentrated on the Fr\'{e}chet mean itself to a uniform distribution over all graphs on a given vertex set. We provide a hierarchical Bayesian approach for exploiting this construction, along with straightforward strategies for sampling from the resultant posterior distribution. We conclude by demonstrating the efficacy of our approach via simulation studies and two multiple-network data analysis examples: one drawn from systems biology and the other from neuroscience.

</details>

<details>

<summary>2020-03-06 09:25:14 - Bernoulli Trials With Skewed Propensities for Certification and Validation</summary>

- *Nozer D. Singpurwalla, Boya Lai*

- `2003.03098v1` - [abs](http://arxiv.org/abs/2003.03098v1) - [pdf](http://arxiv.org/pdf/2003.03098v1)

> The impetus for writing this paper are the well publicized media reports that software failure was the cause of the two recent mishaps of the Boeing 737 Max aircraft. The problem considered here though, is a specific one, in the sense that it endeavors to address the general matter of conditions under which an item such as a drug, a material specimen, or a complex, system can be certified for use based on a large number of Bernoulli trials, all successful. More broadly, the paper is an attempt to answer the old and honorable philosophical question, namely," when can empirical testing on its own validate a law of nature?" Our message is that the answer depends on what one starts with, namely, what is one's prior distribution, what unknown does this prior distribution endow, and what has been observed as data.   The paper is expository in that it begins with a historical overview, and ends with some new ideas and proposals for addressing the question posed. In the sequel, it also articulates on Popper's notion of "propensity" and its role in providing a proper framework for Bayesian inference under Bernoulli trials, as well as the need to engage with posterior distributions that are subjectively specified; that is, without a recourse to the usual Bayesian prior to posterior iteration.

</details>

<details>

<summary>2020-03-06 09:28:22 - Bayesreef: A Bayesian inference framework for modelling reef growth in response to environmental change and biological dynamics</summary>

- *Jodie Pall, Rohitash Chandra, Danial Azam, Tristan Salles, Jody M. Webster, Richard Scalzo, Sally Cripps*

- `1808.02763v2` - [abs](http://arxiv.org/abs/1808.02763v2) - [pdf](http://arxiv.org/pdf/1808.02763v2)

> Estimating the impact of environmental processes on vertical reef development in geological time is a very challenging task. pyReef-Core is a deterministic carbonate stratigraphic forward model designed to simulate the key biological and environmental processes that determine vertical reef accretion and assemblage changes in fossil reef drill cores. We present a Bayesian framework called Bayesreef for the estimation and uncertainty quantification of parameters in pyReef-Core that represent environmental conditions affecting the growth of coral assemblages on geological timescales. We demonstrate the existence of multimodal posterior distributions and investigate the challenges of sampling using Markov chain Monte-Carlo (MCMC) methods, which includes parallel tempering MCMC. We use synthetic reef-core to investigate fundamental issues and then apply the methodology to a selected reef-core from the Great Barrier Reef in Australia. The results show that Bayesreef accurately estimates and provides uncertainty quantification of the selected parameters that represent the environment and ecological conditions in pyReef-Core. Bayesreef provides insights into the complex posterior distributions of parameters in pyReef-Core, which provides the groundwork for future research in this area.

</details>

<details>

<summary>2020-03-06 10:09:02 - Approximate Bayesian Computation with the Sliced-Wasserstein Distance</summary>

- *Kimia Nadjahi, Valentin De Bortoli, Alain Durmus, Roland Badeau, Umut Şimşekli*

- `1910.12815v2` - [abs](http://arxiv.org/abs/1910.12815v2) - [pdf](http://arxiv.org/pdf/1910.12815v2)

> Approximate Bayesian Computation (ABC) is a popular method for approximate inference in generative models with intractable but easy-to-sample likelihood. It constructs an approximate posterior distribution by finding parameters for which the simulated data are close to the observations in terms of summary statistics. These statistics are defined beforehand and might induce a loss of information, which has been shown to deteriorate the quality of the approximation. To overcome this problem, Wasserstein-ABC has been recently proposed, and compares the datasets via the Wasserstein distance between their empirical distributions, but does not scale well to the dimension or the number of samples. We propose a new ABC technique, called Sliced-Wasserstein ABC and based on the Sliced-Wasserstein distance, which has better computational and statistical properties. We derive two theoretical results showing the asymptotical consistency of our approach, and we illustrate its advantages on synthetic data and an image denoising task.

</details>

<details>

<summary>2020-03-06 13:22:46 - Excess deaths and Hurricane María</summary>

- *Michael Spagat, Stijn van Weezel*

- `2003.03183v1` - [abs](http://arxiv.org/abs/2003.03183v1) - [pdf](http://arxiv.org/pdf/2003.03183v1)

> We clarify the distinction between direct and indirect effects of disasters such as Hurricane Mar\'ia and use data from the Puerto Rico Vital Statistics System to estimate monthly excess deaths in the immediate aftermath of the hurricane which struck the island in September of 2017. We use a Bayesian linear regression model fitted to monthly data for 2010--16 to predict monthly death tallies for all months in 2017, finding large deviations of actual numbers above predicted ones in September and October of 2017 but much weaker evidence of excess mortality in November and December of 2017. These deviations translate into 910 excess deaths with a 95 percent uncertainty interval of 440 to 1,390. We also find little evidence of big pre-hurricane mortality spikes in 2017, suggesting that such large spikes do not just happen randomly and, therefore, the post-hurricane mortality spike can reasonably be attributed to the hurricane.

</details>

<details>

<summary>2020-03-06 13:30:30 - A Bayesian algorithm for retrosynthesis</summary>

- *Zhongliang Guo, Stephen Wu, Mitsuru Ohno, Ryo Yoshida*

- `2003.03190v1` - [abs](http://arxiv.org/abs/2003.03190v1) - [pdf](http://arxiv.org/pdf/2003.03190v1)

> The identification of synthetic routes that end with a desired product has been an inherently time-consuming process that is largely dependent on expert knowledge regarding a limited fraction of the entire reaction space. At present, emerging machine-learning technologies are overturning the process of retrosynthetic planning. The objective of this study is to discover synthetic routes backwardly from a given desired molecule to commercially available compounds. The problem is reduced to a combinatorial optimization task with the solution space subject to the combinatorial complexity of all possible pairs of purchasable reactants. We address this issue within the framework of Bayesian inference and computation. The workflow consists of two steps: a deep neural network is trained that forwardly predicts a product of the given reactants with a high level of accuracy, following which this forward model is inverted into the backward one via Bayes' law of conditional probability. Using the backward model, a diverse set of highly probable reaction sequences ending with a given synthetic target is exhaustively explored using a Monte Carlo search algorithm. The Bayesian retrosynthesis algorithm could successfully rediscover 80.3% and 50.0% of known synthetic routes of single-step and two-step reactions within top-10 accuracy, respectively, thereby outperforming state-of-the-art algorithms in terms of the overall accuracy. Remarkably, the Monte Carlo method, which was specifically designed for the presence of diverse multiple routes, often revealed a ranked list of hundreds of reaction routes to the same synthetic target. We investigated the potential applicability of such diverse candidates based on expert knowledge from synthetic organic chemistry.

</details>

<details>

<summary>2020-03-06 13:31:09 - Consistency of Bayesian inference with Gaussian process priors in an elliptic inverse problem</summary>

- *Matteo Giordano, Richard Nickl*

- `1910.07343v3` - [abs](http://arxiv.org/abs/1910.07343v3) - [pdf](http://arxiv.org/pdf/1910.07343v3)

> For $\mathcal{O}$ a bounded domain in $\mathbb{R}^d$ and a given smooth function $g:\mathcal{O}\to\mathbb{R}$, we consider the statistical nonlinear inverse problem of recovering the conductivity $f>0$ in the divergence form equation $$   \nabla\cdot(f\nabla u)=g\ \textrm{on}\ \mathcal{O}, \quad   u=0\ \textrm{on}\ \partial\mathcal{O}, $$ from $N$ discrete noisy point evaluations of the solution $u=u_f$ on $\mathcal O$. We study the statistical performance of Bayesian nonparametric procedures based on a flexible class of Gaussian (or hierarchical Gaussian) process priors, whose implementation is feasible by MCMC methods. We show that, as the number $N$ of measurements increases, the resulting posterior distributions concentrate around the true parameter generating the data, and derive a convergence rate $N^{-\lambda}, \lambda>0,$ for the reconstruction error of the associated posterior means, in $L^2(\mathcal{O})$-distance.

</details>

<details>

<summary>2020-03-06 13:35:20 - Parallel Gaussian process surrogate Bayesian inference with noisy likelihood evaluations</summary>

- *Marko Järvenpää, Michael Gutmann, Aki Vehtari, Pekka Marttinen*

- `1905.01252v4` - [abs](http://arxiv.org/abs/1905.01252v4) - [pdf](http://arxiv.org/pdf/1905.01252v4)

> We consider Bayesian inference when only a limited number of noisy log-likelihood evaluations can be obtained. This occurs for example when complex simulator-based statistical models are fitted to data, and synthetic likelihood (SL) method is used to form the noisy log-likelihood estimates using computationally costly forward simulations. We frame the inference task as a sequential Bayesian experimental design problem, where the log-likelihood function is modelled with a hierarchical Gaussian process (GP) surrogate model, which is used to efficiently select additional log-likelihood evaluation locations. Motivated by recent progress in the related problem of batch Bayesian optimisation, we develop various batch-sequential design strategies which allow to run some of the potentially costly simulations in parallel. We analyse the properties of the resulting method theoretically and empirically. Experiments with several toy problems and simulation models suggest that our method is robust, highly parallelisable, and sample-efficient.

</details>

<details>

<summary>2020-03-06 16:30:44 - Bayesian optimization of variable-size design space problems</summary>

- *Julien Pelamatti, Loic Brevault, Mathieu Balesdent, El-Ghazali Talbi, Yannick Guerin*

- `2003.03300v1` - [abs](http://arxiv.org/abs/2003.03300v1) - [pdf](http://arxiv.org/pdf/2003.03300v1)

> Within the framework of complex system design, it is often necessary to solve mixed variable optimization problems, in which the objective and constraint functions can depend simultaneously on continuous and discrete variables. Additionally, complex system design problems occasionally present a variable-size design space. This results in an optimization problem for which the search space varies dynamically (with respect to both number and type of variables) along the optimization process as a function of the values of specific discrete decision variables. Similarly, the number and type of constraints can vary as well. In this paper, two alternative Bayesian Optimization-based approaches are proposed in order to solve this type of optimization problems. The first one consists in a budget allocation strategy allowing to focus the computational budget on the most promising design sub-spaces. The second approach, instead, is based on the definition of a kernel function allowing to compute the covariance between samples characterized by partially different sets of variables. The results obtained on analytical and engineering related test-cases show a faster and more consistent convergence of both proposed methods with respect to the standard approaches.

</details>

<details>

<summary>2020-03-07 03:44:21 - A 1000-fold Acceleration of Hidden Markov Model Fitting using Graphical Processing Units, with application to Nonvolcanic Tremor Classification</summary>

- *Marnus Stoltz, Gene Stoltz, Kazushige Obara, Ting Wang, David Bryant*

- `2003.03508v1` - [abs](http://arxiv.org/abs/2003.03508v1) - [pdf](http://arxiv.org/pdf/2003.03508v1)

> Hidden Markov models (HMMs) are general purpose models for time-series data widely used across the sciences because of their flexibility and elegance. However fitting HMMs can often be computationally demanding and time consuming, particularly when the the number of hidden states is large or the Markov chain itself is long. Here we introduce a new Graphical Processing Unit (GPU) based algorithm designed to fit long chain HMMs, applying our approach to an HMM for nonvolcanic tremor events developed by Wang et al.(2018). Even on a modest GPU, our implementation resulted in a 1000-fold increase in speed over the standard single processor algorithm, allowing a full Bayesian inference of uncertainty related to model parameters. Similar improvements would be expected for HMM models given large number of observations and moderate state spaces (<80 states with current hardware). We discuss the model, general GPU architecture and algorithms and report performance of the method on a tremor dataset from the Shikoku region, Japan.

</details>

<details>

<summary>2020-03-07 04:33:27 - Scalable Approximate Inference and Some Applications</summary>

- *Jun Han*

- `2003.03515v1` - [abs](http://arxiv.org/abs/2003.03515v1) - [pdf](http://arxiv.org/pdf/2003.03515v1)

> Approximate inference in probability models is a fundamental task in machine learning. Approximate inference provides powerful tools to Bayesian reasoning, decision making, and Bayesian deep learning. The main goal is to estimate the expectation of interested functions w.r.t. a target distribution. When it comes to high dimensional probability models and large datasets, efficient approximate inference becomes critically important. In this thesis, we propose a new framework for approximate inference, which combines the advantages of these three frameworks and overcomes their limitations. Our proposed four algorithms are motivated by the recent computational progress of Stein's method. Our proposed algorithms are applied to continuous and discrete distributions under the setting when the gradient information of the target distribution is available or unavailable. Theoretical analysis is provided to prove the convergence of our proposed algorithms. Our adaptive IS algorithm iteratively improves the importance proposal by functionally decreasing the KL divergence between the updated proposal and the target. When the gradient of the target is unavailable, our proposed sampling algorithm leverages the gradient of a surrogate model and corrects induced bias with importance weights, which significantly outperforms other gradient-free sampling algorithms. In addition, our theoretical results enable us to perform the goodness-of-fit test on discrete distributions. At the end of the thesis, we propose an importance-weighted method to efficiently aggregate local models in distributed learning with one-shot communication. Results on simulated and real datasets indicate the statistical efficiency and wide applicability of our algorithm.

</details>

<details>

<summary>2020-03-07 07:14:35 - The Variational InfoMax Learning Objective</summary>

- *Vincenzo Crescimanna, Bruce Graham*

- `2003.03524v1` - [abs](http://arxiv.org/abs/2003.03524v1) - [pdf](http://arxiv.org/pdf/2003.03524v1)

> Bayesian Inference and Information Bottleneck are the two most popular objectives for neural networks, but they can be optimised only via a variational lower bound: the Variational Information Bottleneck (VIB). In this manuscript we show that the two objectives are actually equivalent to the InfoMax: maximise the information between the data and the labels. The InfoMax representation of the two objectives is not relevant only per se, since it helps to understand the role of the network capacity, but also because it allows us to derive a variational objective, the Variational InfoMax (VIM), that maximises them directly without resorting to any lower bound. The theoretical improvement of VIM over VIB is highlighted by the computational experiments, where the model trained by VIM improves the VIB model in three different tasks: accuracy, robustness to noise and representation quality.

</details>

<details>

<summary>2020-03-07 07:29:07 - Convergence of Q-value in case of Gaussian rewards</summary>

- *Konatsu Miyamoto, Masaya Suzuki, Yuma Kigami, Kodai Satake*

- `2003.03526v1` - [abs](http://arxiv.org/abs/2003.03526v1) - [pdf](http://arxiv.org/pdf/2003.03526v1)

> In this paper, as a study of reinforcement learning, we converge the Q function to unbounded rewards such as Gaussian distribution. From the central limit theorem, in some real-world applications it is natural to assume that rewards follow a Gaussian distribution , but existing proofs cannot guarantee convergence of the Q-function. Furthermore, in the distribution-type reinforcement learning and Bayesian reinforcement learning that have become popular in recent years, it is better to allow the reward to have a Gaussian distribution. Therefore, in this paper, we prove the convergence of the Q-function under the condition of $E[r(s,a)^2]<\infty$, which is much more relaxed than the existing research. Finally, as a bonus, a proof of the policy gradient theorem for distributed reinforcement learning is also posted.

</details>

<details>

<summary>2020-03-07 10:30:43 - Adversarial Machine Learning: Perspectives from Adversarial Risk Analysis</summary>

- *David Rios Insua, Roi Naveiro, Victor Gallego, Jason Poulos*

- `2003.03546v1` - [abs](http://arxiv.org/abs/2003.03546v1) - [pdf](http://arxiv.org/pdf/2003.03546v1)

> Adversarial Machine Learning (AML) is emerging as a major field aimed at the protection of automated ML systems against security threats. The majority of work in this area has built upon a game-theoretic framework by modelling a conflict between an attacker and a defender. After reviewing game-theoretic approaches to AML, we discuss the benefits that a Bayesian Adversarial Risk Analysis perspective brings when defending ML based systems. A research agenda is included.

</details>

<details>

<summary>2020-03-07 21:41:35 - A Conceptual Introduction to Markov Chain Monte Carlo Methods</summary>

- *Joshua S. Speagle*

- `1909.12313v2` - [abs](http://arxiv.org/abs/1909.12313v2) - [pdf](http://arxiv.org/pdf/1909.12313v2)

> Markov Chain Monte Carlo (MCMC) methods have become a cornerstone of many modern scientific analyses by providing a straightforward approach to numerically estimate uncertainties in the parameters of a model using a sequence of random samples. This article provides a basic introduction to MCMC methods by establishing a strong conceptual understanding of what problems MCMC methods are trying to solve, why we want to use them, and how they work in theory and in practice. To develop these concepts, I outline the foundations of Bayesian inference, discuss how posterior distributions are used in practice, explore basic approaches to estimate posterior-based quantities, and derive their link to Monte Carlo sampling and MCMC. Using a simple toy problem, I then demonstrate how these concepts can be used to understand the benefits and drawbacks of various MCMC approaches. Exercises designed to highlight various concepts are also included throughout the article.

</details>

<details>

<summary>2020-03-08 13:08:34 - Adversarial Attacks on Probabilistic Autoregressive Forecasting Models</summary>

- *Raphaël Dang-Nhu, Gagandeep Singh, Pavol Bielik, Martin Vechev*

- `2003.03778v1` - [abs](http://arxiv.org/abs/2003.03778v1) - [pdf](http://arxiv.org/pdf/2003.03778v1)

> We develop an effective generation of adversarial attacks on neural models that output a sequence of probability distributions rather than a sequence of single values. This setting includes the recently proposed deep probabilistic autoregressive forecasting models that estimate the probability distribution of a time series given its past and achieve state-of-the-art results in a diverse set of application domains. The key technical challenge we address is effectively differentiating through the Monte-Carlo estimation of statistics of the joint distribution of the output sequence. Additionally, we extend prior work on probabilistic forecasting to the Bayesian setting which allows conditioning on future observations, instead of only on past observations. We demonstrate that our approach can successfully generate attacks with small input perturbations in two challenging tasks where robust decision making is crucial: stock market trading and prediction of electricity consumption.

</details>

<details>

<summary>2020-03-08 17:22:58 - Enhancing Industrial X-ray Tomography by Data-Centric Statistical Methods</summary>

- *Jarkko Suuronen, Muhammad Emzir, Sari Lasanen, Simo Särkkä, Lassi Roininen*

- `2003.03814v1` - [abs](http://arxiv.org/abs/2003.03814v1) - [pdf](http://arxiv.org/pdf/2003.03814v1)

> X-ray tomography has applications in various industrial fields such as sawmill industry, oil and gas industry, chemical engineering, and geotechnical engineering. In this article, we study Bayesian methods for the X-ray tomography reconstruction. In Bayesian methods, the inverse problem of tomographic reconstruction is solved with help of a statistical prior distribution which encodes the possible internal structures by assigning probabilities for smoothness and edge distribution of the object. We compare Gaussian random field priors, that favour smoothness, to non-Gaussian total variation, Besov, and Cauchy priors which promote sharp edges and high-contrast and low-contrast areas in the object. We also present computational schemes for solving the resulting high-dimensional Bayesian inverse problem with 100,000-1,000,000 unknowns. In particular, we study the applicability of a no-U-turn variant of Hamiltonian Monte Carlo methods and of a more classical adaptive Metropolis-within-Gibbs algorithm for this purpose. These methods also enable full uncertainty quantification of the reconstructions. For faster computations, we use maximum a posteriori estimates with limited-memory BFGS optimisation algorithm. As the first industrial application, we consider sawmill industry X-ray log tomography. The logs have knots, rotten parts, and even possibly metallic pieces, making them good examples for non-Gaussian priors. Secondly, we study drill-core rock sample tomography, an example from oil and gas industry. We show that Cauchy priors produce smaller number of artefacts than other choices, especially with sparse high-noise measurements, and choosing Hamiltonian Monte Carlo enables systematic uncertainty quantification.

</details>

<details>

<summary>2020-03-08 22:40:14 - Inference of a mesoscopic population model from population spike trains</summary>

- *Alexandre René, André Longtin, Jakob H. Macke*

- `1910.01618v2` - [abs](http://arxiv.org/abs/1910.01618v2) - [pdf](http://arxiv.org/pdf/1910.01618v2)

> To understand how rich dynamics emerge in neural populations, we require models exhibiting a wide range of activity patterns while remaining interpretable in terms of connectivity and single-neuron dynamics. However, it has been challenging to fit such mechanistic spiking networks at the single neuron scale to empirical population data. To close this gap, we propose to fit such data at a meso scale, using a mechanistic but low-dimensional and hence statistically tractable model. The mesoscopic representation is obtained by approximating a population of neurons as multiple homogeneous `pools' of neurons, and modelling the dynamics of the aggregate population activity within each pool. We derive the likelihood of both single-neuron and connectivity parameters given this activity, which can then be used to either optimize parameters by gradient ascent on the log-likelihood, or to perform Bayesian inference using Markov Chain Monte Carlo (MCMC) sampling. We illustrate this approach using a model of generalized integrate-and-fire neurons for which mesoscopic dynamics have been previously derived, and show that both single-neuron and connectivity parameters can be recovered from simulated data. In particular, our inference method extracts posterior correlations between model parameters, which define parameter subsets able to reproduce the data. We compute the Bayesian posterior for combinations of parameters using MCMC sampling and investigate how the approximations inherent to a mesoscopic population model impact the accuracy of the inferred single-neuron parameters.

</details>

<details>

<summary>2020-03-09 01:45:04 - Information criteria for inhomogeneous spatial point processes</summary>

- *Achmad Choiruddin, Jean-François Coeurjolly, Rasmus Waagepetersen*

- `2003.03880v1` - [abs](http://arxiv.org/abs/2003.03880v1) - [pdf](http://arxiv.org/pdf/2003.03880v1)

> The theoretical foundation for a number of model selection criteria is established in the context of inhomogeneous point processes and under various asymptotic settings: infill, increasing domain, and combinations of these. For inhomogeneous Poisson processes we consider Akaike information criterion and the Bayesian information criterion, and in particular we identify the point process analogue of sample size needed for the Bayesian information criterion. Considering general inhomogeneous point processes we derive new composite likelihood and composite Bayesian information criteria for selecting a regression model for the intensity function. The proposed model selection criteria are evaluated using simulations of Poisson processes and cluster point processes.

</details>

<details>

<summary>2020-03-09 08:51:16 - Bayesian inference and non-linear extensions of the CIRCE method for quantifying the uncertainty of closure relationships integrated into thermal-hydraulic system codes</summary>

- *Guillaume Damblin, Pierre Gaillard*

- `1902.04931v2` - [abs](http://arxiv.org/abs/1902.04931v2) - [pdf](http://arxiv.org/pdf/1902.04931v2)

> Uncertainty Quantification of closure relationships integrated into thermal-hydraulic system codes is a critical prerequisite in applying the Best-Estimate Plus Uncertainty (BEPU) methodology for nuclear safety and licensing processes.The purpose of the CIRCE method is to estimate the (log)-Gaussian probability distribution of a multiplicative factor applied to a reference closure relationship in order to assess its uncertainty. Even though this method has been implemented with success in numerous physical scenarios, it can still suffer from substantial limitations such as the linearity assumption and the difficulty of properly taking into account the inherent statistical uncertainty. In the paper, we will extend the CIRCE method in two aspects. On the one hand, we adopt the Bayesian setting putting prior probability distributions on the parameters of the (log)-Gaussian distribution. The posterior distribution of the parameters is then computed with respect to an experimental database by means of Markov Chain Monte Carlo (MCMC) algorithms. On the other hand, we tackle the more general setting where the simulations do not move linearly against the multiplicative factor(s). MCMC algorithms then become time-prohibitive when the thermal-hydraulic simulations exceed a few minutes. This handicap is overcome by using Gaussian process (GP) emulators which can yield both reliable and fast predictions of the simulations. The GP-based MCMC algorithms will be applied to quantify the uncertainty of two condensation closure relationships at a safety injection with respect to a database of experimental tests. The thermal-hydraulic simulations will be run with the CATHARE 2 computer code.

</details>

<details>

<summary>2020-03-09 10:30:28 - When are Bayesian model probabilities overconfident?</summary>

- *Oscar Oelrich, Shutong Ding, Måns Magnusson, Aki Vehtari, Mattias Villani*

- `2003.04026v1` - [abs](http://arxiv.org/abs/2003.04026v1) - [pdf](http://arxiv.org/pdf/2003.04026v1)

> Bayesian model comparison is often based on the posterior distribution over the set of compared models. This distribution is often observed to concentrate on a single model even when other measures of model fit or forecasting ability indicate no strong preference. Furthermore, a moderate change in the data sample can easily shift the posterior model probabilities to concentrate on another model. We document overconfidence in two high-profile applications in economics and neuroscience. To shed more light on the sources of overconfidence we derive the sampling variance of the Bayes factor in univariate and multivariate linear regression. The results show that overconfidence is likely to happen when i) the compared models give very different approximations of the data-generating process, ii) the models are very flexible with large degrees of freedom that are not shared between the models, and iii) the models underestimate the true variability in the data.

</details>

<details>

<summary>2020-03-09 14:09:38 - Importance sampling type estimators based on approximate marginal MCMC</summary>

- *Matti Vihola, Jouni Helske, Jordan Franks*

- `1609.02541v6` - [abs](http://arxiv.org/abs/1609.02541v6) - [pdf](http://arxiv.org/pdf/1609.02541v6)

> We consider importance sampling (IS) type weighted estimators based on Markov chain Monte Carlo (MCMC) targeting an approximate marginal of the target distribution. In the context of Bayesian latent variable models, the MCMC typically operates on the hyperparameters, and the subsequent weighting may be based on IS or sequential Monte Carlo (SMC), but allows for multilevel techniques as well. The IS approach provides a natural alternative to delayed acceptance (DA) pseudo-marginal/particle MCMC, and has many advantages over DA, including a straightforward parallelisation and additional flexibility in MCMC implementation. We detail minimal conditions which ensure strong consistency of the suggested estimators, and provide central limit theorems with expressions for asymptotic variances. We demonstrate how our method can make use of SMC in the state space models context, using Laplace approximations and time-discretised diffusions. Our experimental results are promising and show that the IS type approach can provide substantial gains relative to an analogous DA scheme, and is often competitive even without parallelisation.

</details>

<details>

<summary>2020-03-09 15:45:57 - Composition of kernel and acquisition functions for High Dimensional Bayesian Optimization</summary>

- *Antonio Candelieri, Ilaria Giordani, Riccardo Perego, Francesco Archetti*

- `2003.04207v1` - [abs](http://arxiv.org/abs/2003.04207v1) - [pdf](http://arxiv.org/pdf/2003.04207v1)

> Bayesian Optimization has become the reference method for the global optimization of black box, expensive and possibly noisy functions. Bayesian Op-timization learns a probabilistic model about the objective function, usually a Gaussian Process, and builds, depending on its mean and variance, an acquisition function whose optimizer yields the new evaluation point, leading to update the probabilistic surrogate model. Despite its sample efficiency, Bayesian Optimiza-tion does not scale well with the dimensions of the problem. The optimization of the acquisition function has received less attention because its computational cost is usually considered negligible compared to that of the evaluation of the objec-tive function. Its efficient optimization is often inhibited, particularly in high di-mensional problems, by multiple extrema. In this paper we leverage the addition-ality of the objective function into mapping both the kernel and the acquisition function of the Bayesian Optimization in lower dimensional subspaces. This ap-proach makes more efficient the learning/updating of the probabilistic surrogate model and allows an efficient optimization of the acquisition function. Experi-mental results are presented for real-life application, that is the control of pumps in urban water distribution systems.

</details>

<details>

<summary>2020-03-09 15:55:16 - Population-aware Hierarchical Bayesian Domain Adaptation via Multiple-component Invariant Learning</summary>

- *Vishwali Mhasawade, Nabeel Abdur Rehman, Rumi Chunara*

- `1908.09222v5` - [abs](http://arxiv.org/abs/1908.09222v5) - [pdf](http://arxiv.org/pdf/1908.09222v5)

> While machine learning is rapidly being developed and deployed in health settings such as influenza prediction, there are critical challenges in using data from one environment in another due to variability in features; even within disease labels there can be differences (e.g. "fever" may mean something different reported in a doctor's office versus in an online app). Moreover, models are often built on passive, observational data which contain different distributions of population subgroups (e.g. men or women). Thus, there are two forms of instability between environments in this observational transport problem. We first harness knowledge from health to conceptualize the underlying causal structure of this problem in a health outcome prediction task. Based on sources of stability in the model, we posit that for human-sourced data and health prediction tasks we can combine environment and population information in a novel population-aware hierarchical Bayesian domain adaptation framework that harnesses multiple invariant components through population attributes when needed. We study the conditions under which invariant learning fails, leading to reliance on the environment-specific attributes. Experimental results for an influenza prediction task on four datasets gathered from different contexts show the model can improve prediction in the case of largely unlabelled target data from a new environment and different constituent population, by harnessing both environment and population invariant information. This work represents a novel, principled way to address a critical challenge by blending domain (health) knowledge and algorithmic innovation. The proposed approach will have a significant impact in many social settings wherein who and where the data comes from matters.

</details>

<details>

<summary>2020-03-09 17:14:11 - Do Informational Cascades Happen with Non-myopic Agents?</summary>

- *Ilai Bistritz, Nasimeh Heydaribeni, Achilleas Anastasopoulos*

- `1905.01327v3` - [abs](http://arxiv.org/abs/1905.01327v3) - [pdf](http://arxiv.org/pdf/1905.01327v3)

> We consider an environment where players need to decide whether to buy a certain product (or adopt a technology) or not. The product is either good or bad but its true value is not known to the players. Instead, each player has her own private information on its quality. Each player can observe the previous actions of other players and estimate the quality of the product. A classic result in the literature shows that in similar settings information cascades occur where learning stops for the whole network and players repeat the actions of their predecessors. In contrast to the existing literature on informational cascades, in this work, players get more than one opportunity to act. In each turn, a player is chosen uniformly at random and can decide to buy the product and leave the market or to wait. We provide a characterization of structured perfect Bayesian equilibria (sPBE) with forward-looking strategies through a fixed-point equation of dimensionality that grows only quadratically with the number of players. In particular, a sufficient state for players' strategies at each time instance is a pair of two integers, the first corresponding to the estimated quality of the good and the second indicating the number of players that cannot offer additional information about the good to the rest of the players. Based on this characterization we study informational cascades in two regimes. First, we show that for a discount factor strictly smaller than one, informational cascades happen with high probability as the number of players increases. Furthermore, only a small portion of the total information in the system is revealed before a cascade occurs. Secondly, and more surprisingly, we show that for a fixed number of players, as the discount factor approaches one, bad informational cascades are benign when the product is bad, and are completely eliminated when the discount factor equals one.

</details>

<details>

<summary>2020-03-09 20:45:18 - On frequentist coverage of Bayesian credible sets for estimation of the mean under constraints</summary>

- *Kevin Duisters, Johannes Schmidt-Hieber*

- `2003.04406v1` - [abs](http://arxiv.org/abs/2003.04406v1) - [pdf](http://arxiv.org/pdf/2003.04406v1)

> Frequentist coverage of $(1-\alpha)$-highest posterior density (HPD) credible sets is studied in a signal plus noise model under a large class of noise distributions. We consider a specific class of spike-and-slab prior distributions. Different regimes are identified and we derive closed form expressions for the $(1-\alpha)$-HPD on each of these regimes. Similar to the earlier work by Marchand and Strawderman, it is shown that under suitable conditions, the frequentist coverage can drop to $1-3\alpha/2.$

</details>

<details>

<summary>2020-03-09 21:04:49 - Spiked Laplacian Graphs: Bayesian Community Detection in Heterogeneous Networks</summary>

- *Leo L Duan, George Michailidis, Mingzhou Ding*

- `1910.02471v2` - [abs](http://arxiv.org/abs/1910.02471v2) - [pdf](http://arxiv.org/pdf/1910.02471v2)

> In network data analysis, it is becoming common to work with a collection of graphs that exhibit \emph{heterogeneity}. For example, neuroimaging data from patient cohorts are increasingly available. A critical analytical task is to identify communities, and graph Laplacian-based methods are routinely used. However, these methods are currently limited to a single network and do not provide measures of uncertainty on the community assignment. In this work, we propose a probabilistic network model called the ``Spiked Laplacian Graph'' that considers each network as an invertible transform of the Laplacian, with its eigenvalues modeled by a modified spiked structure. This effectively reduces the number of parameters in the eigenvectors, and their sign patterns allow efficient estimation of the community structure. Further, the posterior distribution of the eigenvectors provides uncertainty quantification for the community estimates. Subsequently, we introduce a Bayesian non-parametric approach to address the issue of heterogeneity in a collection of graphs. Theoretical results are established on the posterior consistency of the procedure and provide insights on the trade-off between model resolution and accuracy. We illustrate the performance of the methodology on synthetic data sets, as well as a neuroscience study related to brain activity in working memory.   Keywords: Hierarchical Community Detection, Isoperimetric Constant, Mixed-Effect Eigendecomposition, Normalized Graph Cut, Stiefel Manifold

</details>

<details>

<summary>2020-03-10 09:50:07 - Asian Handicap football betting with Rating-based Hybrid Bayesian Networks</summary>

- *Anthony Constantinou*

- `2003.09384v1` - [abs](http://arxiv.org/abs/2003.09384v1) - [pdf](http://arxiv.org/pdf/2003.09384v1)

> Despite the massive popularity of the Asian Handicap (AH) football betting market, it has not been adequately studied by the relevant literature. This paper combines rating systems with hybrid Bayesian networks and presents the first published model specifically developed for prediction and assessment of the AH betting market. The results are based on 13 English Premier League seasons and are compared to the traditional 1X2 market. Different betting situations have been examined including a) both average and maximum (best available) market odds, b) all possible betting decision thresholds between predicted and published odds, c) optimisations for both return-on-investment and profit, and d) simple stake adjustments to investigate how the variance of returns changes when targeting equivalent profit in both 1X2 and AH markets. While the AH market is found to share the inefficiencies of the traditional 1X2 market, the findings reveal both interesting differences as well as similarities between the two.

</details>

<details>

<summary>2020-03-10 14:55:58 - Kernels over Sets of Finite Sets using RKHS Embeddings, with Application to Bayesian (Combinatorial) Optimization</summary>

- *Poompol Buathong, David Ginsbourger, Tipaluck Krityakierne*

- `1910.04086v2` - [abs](http://arxiv.org/abs/1910.04086v2) - [pdf](http://arxiv.org/pdf/1910.04086v2)

> We focus on kernel methods for set-valued inputs and their application to Bayesian set optimization, notably combinatorial optimization. We investigate two classes of set kernels that both rely on Reproducing Kernel Hilbert Space embeddings, namely the ``Double Sum'' (DS) kernels recently considered in Bayesian set optimization, and a class introduced here called ``Deep Embedding'' (DE) kernels that essentially consists in applying a radial kernel on Hilbert space on top of the canonical distance induced by another kernel such as a DS kernel. We establish in particular that while DS kernels typically suffer from a lack of strict positive definiteness, vast subclasses of DE kernels built upon DS kernels do possess this property, enabling in turn combinatorial optimization without requiring to introduce a jitter parameter. Proofs of theoretical results about considered kernels are complemented by a few practicalities regarding hyperparameter fitting. We furthermore demonstrate the applicability of our approach in prediction and optimization tasks, relying both on toy examples and on two test cases from mechanical engineering and hydrogeology, respectively. Experimental results highlight the applicability and compared merits of the considered approaches while opening new perspectives in prediction and sequential design with set inputs.

</details>

<details>

<summary>2020-03-10 16:32:23 - On Hyper-parameter Tuning for Stochastic Optimization Algorithms</summary>

- *Haotian Zhang, Jianyong Sun, Zongben Xu*

- `2003.02038v2` - [abs](http://arxiv.org/abs/2003.02038v2) - [pdf](http://arxiv.org/pdf/2003.02038v2)

> This paper proposes the first-ever algorithmic framework for tuning hyper-parameters of stochastic optimization algorithm based on reinforcement learning. Hyper-parameters impose significant influences on the performance of stochastic optimization algorithms, such as evolutionary algorithms (EAs) and meta-heuristics. Yet, it is very time-consuming to determine optimal hyper-parameters due to the stochastic nature of these algorithms. We propose to model the tuning procedure as a Markov decision process, and resort the policy gradient algorithm to tune the hyper-parameters. Experiments on tuning stochastic algorithms with different kinds of hyper-parameters (continuous and discrete) for different optimization problems (continuous and discrete) show that the proposed hyper-parameter tuning algorithms do not require much less running times of the stochastic algorithms than bayesian optimization method. The proposed framework can be used as a standard tool for hyper-parameter tuning in stochastic algorithms.

</details>

<details>

<summary>2020-03-10 17:04:53 - Modeling Multiscale Variable Renewable Energy and Inflow Scenarios in Very Large Regions with Nonparametric Bayesian Networks</summary>

- *Julio Alberto Dias, Guilherme Machado, Alessandro Soares, Joaquim Dias Garcia*

- `2003.04855v1` - [abs](http://arxiv.org/abs/2003.04855v1) - [pdf](http://arxiv.org/pdf/2003.04855v1)

> In this paper, we propose a non-parametric Bayesian network method to generate synthetic scenarios of hourly generation for variable renewable energy(VRE) plants. The methodology consists of a non-parametric estimation of the probability distribution of VRE generation, followed by an inverse probability integral transform, in order to obtain normally distributed variables of VRE generation. Then, we build a Bayesian network based on the evaluation of the spatial correlation between variables (VRE generation and hydro inflows, but load forecast, temperature, and other types of random variables could also be used with the proposed framework), to generate future synthetic scenarios while keeping the historical spatial correlation structure. Finally, we present a real-life case study, that uses real data from the Brazilian power system, to show the improvements that the present methodology allows for real-life studies.

</details>

<details>

<summary>2020-03-10 20:44:30 - Individual-Level Modelling of Infectious Disease Data: EpiILM</summary>

- *Vineetha Warriyar K. V., Waleed Almutiry, Rob Deardon*

- `2003.04963v1` - [abs](http://arxiv.org/abs/2003.04963v1) - [pdf](http://arxiv.org/pdf/2003.04963v1)

> In this article, we introduce the R package EpiILM, which provides tools for simulation from, and inference for, discrete-time individual-level models of infectious disease transmission proposed by Deardon et al. (2010). The inference is set in a Bayesian framework and is carried out via Metropolis-Hastings Markov chain Monte Carlo (MCMC). For its fast implementation, key functions are coded in Fortran. Both spatial and contact network models are implemented in the package and can be set in either susceptible-infected (SI) or susceptible-infected-removed (SIR) compartmental frameworks. The use of the package is demonstrated through examples involving both simulated and real data.

</details>

<details>

<summary>2020-03-10 22:31:58 - Prediction of Bayesian Intervals for Tropical Storms</summary>

- *Max Chiswick, Sam Ganzfried*

- `2003.05024v1` - [abs](http://arxiv.org/abs/2003.05024v1) - [pdf](http://arxiv.org/pdf/2003.05024v1)

> Building on recent research for prediction of hurricane trajectories using recurrent neural networks (RNNs), we have developed improved methods and generalized the approach to predict Bayesian intervals in addition to simple point estimates. Tropical storms are capable of causing severe damage, so accurately predicting their trajectories can bring significant benefits to cities and lives, especially as they grow more intense due to climate change effects. By implementing the Bayesian interval using dropout in an RNN, we improve the actionability of the predictions, for example by estimating the areas to evacuate in the landfall region. We used an RNN to predict the trajectory of the storms at 6-hour intervals. We used latitude, longitude, windspeed, and pressure features from a Statistical Hurricane Intensity Prediction Scheme (SHIPS) dataset of about 500 tropical storms in the Atlantic Ocean. Our results show how neural network dropout values affect predictions and intervals.

</details>

<details>

<summary>2020-03-11 00:38:08 - Time-varying Gaussian Process Bandit Optimization with Non-constant Evaluation Time</summary>

- *Hideaki Imamura, Nontawat Charoenphakdee, Futoshi Futami, Issei Sato, Junya Honda, Masashi Sugiyama*

- `2003.04691v2` - [abs](http://arxiv.org/abs/2003.04691v2) - [pdf](http://arxiv.org/pdf/2003.04691v2)

> The Gaussian process bandit is a problem in which we want to find a maximizer of a black-box function with the minimum number of function evaluations. If the black-box function varies with time, then time-varying Bayesian optimization is a promising framework. However, a drawback with current methods is in the assumption that the evaluation time for every observation is constant, which can be unrealistic for many practical applications, e.g., recommender systems and environmental monitoring. As a result, the performance of current methods can be degraded when this assumption is violated. To cope with this problem, we propose a novel time-varying Bayesian optimization algorithm that can effectively handle the non-constant evaluation time. Furthermore, we theoretically establish a regret bound of our algorithm. Our bound elucidates that a pattern of the evaluation time sequence can hugely affect the difficulty of the problem. We also provide experimental results to validate the practical effectiveness of the proposed method.

</details>

<details>

<summary>2020-03-11 02:36:45 - A spatiotemporal recommendation engine for malaria control</summary>

- *Qian Guan, Brian J. Reich, Eric B. Laber*

- `2003.05084v1` - [abs](http://arxiv.org/abs/2003.05084v1) - [pdf](http://arxiv.org/pdf/2003.05084v1)

> Malaria is an infectious disease affecting a large population across the world, and interventions need to be efficiently applied to reduce the burden of malaria. We develop a framework to help policy-makers decide how to allocate limited resources in realtime for malaria control. We formalize a policy for the resource allocation as a sequence of decisions, one per intervention decision, that map up-to-date disease related information to a resource allocation. An optimal policy must control the spread of the disease while being interpretable and viewed as equitable to stakeholders. We construct an interpretable class of resource allocation policies that can accommodate allocation of resources residing in a continuous domain, and combine a hierarchical Bayesian spatiotemporal model for disease transmission with a policy-search algorithm to estimate an optimal policy for resource allocation within the pre-specified class. The estimated optimal policy under the proposed framework improves the cumulative long-term outcome compared with naive approaches in both simulation experiments and application to malaria interventions in the Democratic Republic of the Congo.

</details>

<details>

<summary>2020-03-11 08:30:26 - Bessel regression model: Robustness to analyze bounded data</summary>

- *Wagner Barreto-Souza, Vinícius D. Mayrink, Alexandre B. Simas*

- `2003.05157v1` - [abs](http://arxiv.org/abs/2003.05157v1) - [pdf](http://arxiv.org/pdf/2003.05157v1)

> Beta regression has been extensively used by statisticians and practitioners to model bounded continuous data and there is no strong and similar competitor having its main features. A class of normalized inverse-Gaussian (N-IG) process was introduced in the literature, being explored in the Bayesian context as a powerful alternative to the Dirichlet process. Until this moment, no attention has been paid for the univariate N-IG distribution in the classical inference. In this paper, we propose the bessel regression based on the univariate N-IG distribution, which is a robust alternative to the beta model. This robustness is illustrated through simulated and real data applications. The estimation of the parameters is done through an Expectation-Maximization algorithm and the paper discusses how to perform inference. A useful and practical discrimination procedure is proposed for model selection between bessel and beta regressions. Monte Carlo simulation results are presented to verify the finite-sample behavior of the EM-based estimators and the discrimination procedure. Further, the performances of the regressions are evaluated under misspecification, which is a critical point showing the robustness of the proposed model. Finally, three empirical illustrations are explored to confront results from bessel and beta regressions.

</details>

<details>

<summary>2020-03-12 02:15:36 - On Bayesian posterior mean estimators in imaging sciences and Hamilton-Jacobi Partial Differential Equations</summary>

- *Jerome Darbon, Gabriel P. Langlois*

- `2003.05572v1` - [abs](http://arxiv.org/abs/2003.05572v1) - [pdf](http://arxiv.org/pdf/2003.05572v1)

> Variational and Bayesian methods are two approaches that have been widely used to solve image reconstruction problems. In this paper, we propose original connections between Hamilton--Jacobi (HJ) partial differential equations and a broad class of Bayesian methods and posterior mean estimators with Gaussian data fidelity term and log-concave prior. Whereas solutions to certain first-order HJ PDEs with initial data describe maximum a posteriori estimators in a Bayesian setting, here we show that solutions to some viscous HJ PDEs with initial data describe a broad class of posterior mean estimators. These connections allow us to establish several representation formulas and optimal bounds involving the posterior mean estimate. In particular, we use these connections to HJ PDEs to show that some Bayesian posterior mean estimators can be expressed as proximal mappings of twice continuously differentiable functions, and furthermore we derive a representation formula for these functions.

</details>

<details>

<summary>2020-03-12 04:01:14 - Solutions to Multilevel Sparse Matrix Problems</summary>

- *Tui H. Nolan, Matt P. Wand*

- `1903.03089v3` - [abs](http://arxiv.org/abs/1903.03089v3) - [pdf](http://arxiv.org/pdf/1903.03089v3)

> We define and solve classes of sparse matrix problems that arise in multilevel modeling and data analysis. The classes are indexed by the number of nested units, with two-level problems corresponding to the common situation in which data on level 1 units are grouped within a two-level structure. We provide full solutions for two-level and three-level problems and their derivations provide blueprints for the challenging, albeit rarer in applications, higher level versions of the problem. Whilst our linear system solutions are a concise recasting of existing results, our matrix inverse sub-block results are novel and facilitate streamlined computation of standard errors in frequentist inference as well as allowing streamlined mean field variational Bayesian inference for models containing higher level random effects.

</details>

<details>

<summary>2020-03-12 04:12:26 - Efficient testing and effect size estimation for set-based genetic association inference via semiparametric multilevel mixture modeling: Application to a genome-wide association study of coronary artery disease</summary>

- *Shonosuke Sugasawa, Hisashi Noma*

- `2003.05611v1` - [abs](http://arxiv.org/abs/2003.05611v1) - [pdf](http://arxiv.org/pdf/2003.05611v1)

> In genetic association studies, rare variants with extremely small allele frequency play a crucial role in complex traits, and the set-based testing methods that jointly assess the effects of groups of single nucleotide polymorphisms (SNPs) were developed to improve powers for the association tests. However, the powers of these tests are still severely limited due to the extremely small allele frequency, and precise estimations for the effect sizes of individual SNPs are substantially impossible. In this article, we provide an efficient set-based inference framework that addresses the two important issues simultaneously based on a Bayesian semiparametric multilevel mixture model. We propose to use the multilevel hierarchical model that incorporate the variations in set-specific effects and variant-specific effects, and to apply the optimal discovery procedure (ODP) that achieves the largest overall power in multiple significance testing. In addition, we provide Bayesian optimal "set-based" estimator of the empirical distribution of effect sizes. Efficiency of the proposed methods is demonstrated through application to a genome-wide association study of coronary artery disease (CAD), and through simulation studies. These results suggested there could be a lot of rare variants with large effect sizes for CAD, and the number of significant sets detected by the ODP was much greater than those by existing methods.

</details>

<details>

<summary>2020-03-12 05:54:12 - Using prior expansions for prior-data conflict checking</summary>

- *David J. Nott, Max Seah, Luai Al-Labadi, Michael Evans, Hui Khoon Ng, Berthold-Georg Englert*

- `1902.10393v2` - [abs](http://arxiv.org/abs/1902.10393v2) - [pdf](http://arxiv.org/pdf/1902.10393v2)

> Any Bayesian analysis involves combining information represented through different model components, and when different sources of information are in conflict it is important to detect this. Here we consider checking for prior-data conflict in Bayesian models by expanding the prior used for the analysis into a larger family of priors, and considering a marginal likelihood score statistic for the expansion parameter. Consideration of different expansions can be informative about the nature of any conflict, and extensions to hierarchically specified priors and connections with other approaches to prior-data conflict checking are discussed. Implementation in complex situations is illustrated with two applications. The first concerns testing for the appropriateness of a LASSO penalty in shrinkage estimation of coefficients in linear regression. Our method is compared with a recent suggestion in the literature designed to be powerful against alternatives in the exponential power family, and we use this family as the prior expansion for constructing our check. A second application concerns a problem in quantum state estimation, where a multinomial model is considered with physical constraints on the model parameters. In this example, the usefulness of different prior expansions is demonstrated for obtaining checks which are sensitive to different aspects of the prior.

</details>

<details>

<summary>2020-03-12 09:36:22 - Bayesian Indirect Inference and the ABC of GMM</summary>

- *Michael Creel, Jiti Gao, Han Hong, Dennis Kristensen*

- `1512.07385v2` - [abs](http://arxiv.org/abs/1512.07385v2) - [pdf](http://arxiv.org/pdf/1512.07385v2)

> In this paper we propose and study local linear and polynomial based estimators for implementing Approximate Bayesian Computation (ABC) style indirect inference and GMM estimators. This method makes use of nonparametric regression in the computation of GMM and Indirect Inference models. We provide formal conditions under which frequentist inference is asymptotically valid and demonstrate the validity of the estimated posterior quantiles for confidence interval construction. We also show that in this setting, local linear kernel regression methods have theoretical advantages over local constant kernel methods that are also reflected in finite sample simulation results. Our results also apply to both exactly and over identified models. These estimators do not need to rely on numerical optimization or Markov Chain Monte Carlo (MCMC) simulations. They provide an effective complement to the classical M-estimators and to MCMC methods, and can be applied to both likelihood based models and method of moment based models.

</details>

<details>

<summary>2020-03-12 22:08:33 - A spatial causal analysis of wildland fire-contributed PM2.5 using numerical model output</summary>

- *Alexandra Larsen, Shu Yang, Brian J. Reich, Ana G. Rappold*

- `2003.06037v1` - [abs](http://arxiv.org/abs/2003.06037v1) - [pdf](http://arxiv.org/pdf/2003.06037v1)

> Wildland fire smoke contains hazardous levels of fine particulate matter PM2.5, a pollutant shown to adversely effect health. Estimating fire attributable PM2.5 concentrations is key to quantifying the impact on air quality and subsequent health burden. This is a challenging problem since only total PM2.5 is measured at monitoring stations and both fire-attributable PM2.5 and PM2.5 from all other sources are correlated in space and time. We propose a framework for estimating fire-contributed PM2.5 and PM2.5 from all other sources using a novel causal inference framework and bias-adjusted chemical model representations of PM2.5 under counterfactual scenarios. The chemical model representation of PM2.5 for this analysis is simulated using Community Multi-Scale Air Quality Modeling System (CMAQ), run with and without fire emissions across the contiguous U.S. for the 2008-2012 wildfire seasons. The CMAQ output is calibrated with observations from monitoring sites for the same spatial domain and time period. We use a Bayesian model that accounts for spatial variation to estimate the effect of wildland fires on PM2.5 and state assumptions under which the estimate has a valid causal interpretation. Our results include estimates of absolute, relative and cumulative contributions of wildfire smoke to PM2.5 for the contiguous U.S. Additionally, we compute the health burden associated with the PM2.5 attributable to wildfire smoke.

</details>

<details>

<summary>2020-03-12 23:31:11 - Causal datasheet: An approximate guide to practically assess Bayesian networks in the real world</summary>

- *Bradley Butcher, Vincent S. Huang, Jeremy Reffin, Sema K. Sgaier, Grace Charles, Novi Quadrianto*

- `2003.07182v1` - [abs](http://arxiv.org/abs/2003.07182v1) - [pdf](http://arxiv.org/pdf/2003.07182v1)

> In solving real-world problems like changing healthcare-seeking behaviors, designing interventions to improve downstream outcomes requires an understanding of the causal links within the system. Causal Bayesian Networks (BN) have been proposed as one such powerful method. In real-world applications, however, confidence in the results of BNs are often moderate at best. This is due in part to the inability to validate against some ground truth, as the DAG is not available. This is especially problematic if the learned DAG conflicts with pre-existing domain doctrine. At the policy level, one must justify insights generated by such analysis, preferably accompanying them with uncertainty estimation. Here we propose a causal extension to the datasheet concept proposed by Gebru et al (2018) to include approximate BN performance expectations for any given dataset. To generate the results for a prototype Causal Datasheet, we constructed over 30,000 synthetic datasets with properties mirroring characteristics of real data. We then recorded the results given by state-of-the-art structure learning algorithms. These results were used to populate the Causal Datasheet, and recommendations were automatically generated dependent on expected performance. As a proof of concept, we used our Causal Datasheet Generation Tool (CDG-T) to assign expected performance expectations to a maternal health survey we conducted in Uttar Pradesh, India.

</details>

<details>

<summary>2020-03-13 02:30:25 - Value of Information Analysis via Active Learning and Knowledge Sharing in Error-Controlled Adaptive Kriging</summary>

- *Chi Zhang, Zeyu Wang, Abdollah Shafieezadeh*

- `2002.02354v2` - [abs](http://arxiv.org/abs/2002.02354v2) - [pdf](http://arxiv.org/pdf/2002.02354v2)

> Large uncertainties in many phenomena have challenged decision making. Collecting additional information to better characterize reducible uncertainties is among decision alternatives. Value of information (VoI) analysis is a mathematical decision framework that quantifies expected potential benefits of new data and assists with optimal allocation of resources for information collection. However, analysis of VoI is computational very costly because of the underlying Bayesian inference especially for equality-type information. This paper proposes the first surrogate-based framework for VoI analysis. Instead of modeling the limit state functions describing events of interest for decision making, which is commonly pursued in surrogate model-based reliability methods, the proposed framework models system responses. This approach affords sharing equality-type information from observations among surrogate models to update likelihoods of multiple events of interest. Moreover, two knowledge sharing schemes called model and training points sharing are proposed to most effectively take advantage of the knowledge offered by costly model evaluations. Both schemes are integrated with an error rate-based adaptive training approach to efficiently generate accurate Kriging surrogate models. The proposed VoI analysis framework is applied for an optimal decision-making problem involving load testing of a truss bridge. While state-of-the-art methods based on importance sampling and adaptive Kriging Monte Carlo simulation are unable to solve this problem, the proposed method is shown to offer accurate and robust estimates of VoI with a limited number of model evaluations. Therefore, the proposed method facilitates the application of VoI for complex decision problems.

</details>

<details>

<summary>2020-03-13 04:00:42 - B-PINNs: Bayesian Physics-Informed Neural Networks for Forward and Inverse PDE Problems with Noisy Data</summary>

- *Liu Yang, Xuhui Meng, George Em Karniadakis*

- `2003.06097v1` - [abs](http://arxiv.org/abs/2003.06097v1) - [pdf](http://arxiv.org/pdf/2003.06097v1)

> We propose a Bayesian physics-informed neural network (B-PINN) to solve both forward and inverse nonlinear problems described by partial differential equations (PDEs) and noisy data. In this Bayesian framework, the Bayesian neural network (BNN) combined with a PINN for PDEs serves as the prior while the Hamiltonian Monte Carlo (HMC) or the variational inference (VI) could serve as an estimator of the posterior. B-PINNs make use of both physical laws and scattered noisy measurements to provide predictions and quantify the aleatoric uncertainty arising from the noisy data in the Bayesian framework. Compared with PINNs, in addition to uncertainty quantification, B-PINNs obtain more accurate predictions in scenarios with large noise due to their capability of avoiding overfitting. We conduct a systematic comparison between the two different approaches for the B-PINN posterior estimation (i.e., HMC or VI), along with dropout used for quantifying uncertainty in deep neural networks. Our experiments show that HMC is more suitable than VI for the B-PINNs posterior estimation, while dropout employed in PINNs can hardly provide accurate predictions with reasonable uncertainty. Finally, we replace the BNN in the prior with a truncated Karhunen-Lo\`eve (KL) expansion combined with HMC or a deep normalizing flow (DNF) model as posterior estimators. The KL is as accurate as BNN and much faster but this framework cannot be easily extended to high-dimensional problems unlike the BNN based framework.

</details>

<details>

<summary>2020-03-13 16:26:00 - Use of Cross-validation Bayes Factors to Test Equality of Two Densities</summary>

- *Jeffery Hart, Taeryon Choi, Naveed Merchant*

- `2003.06368v1` - [abs](http://arxiv.org/abs/2003.06368v1) - [pdf](http://arxiv.org/pdf/2003.06368v1)

> We propose a non-parametric, two-sample Bayesian test for checking whether or not two data sets share a common distribution. The test makes use of data splitting ideas and does not require priors for high-dimensional parameter vectors as do other nonparametric Bayesian procedures. We provide evidence that the new procedure provides more stable Bayes factors than do methods based on P\'olya trees. Somewhat surprisingly, the behavior of the proposed Bayes factors when the two distributions are the same is usually superior to that of P\'olya tree Bayes factors. We showcase the effectiveness of the test by proving its consistency, conducting a simulation study and applying the test to Higgs boson data.

</details>

<details>

<summary>2020-03-13 18:14:43 - Counterfactual diagnosis</summary>

- *Jonathan G. Richens, Ciaran M. Lee, Saurabh Johri*

- `1910.06772v3` - [abs](http://arxiv.org/abs/1910.06772v3) - [pdf](http://arxiv.org/pdf/1910.06772v3)

> Machine learning promises to revolutionize clinical decision making and diagnosis. In medical diagnosis a doctor aims to explain a patient's symptoms by determining the diseases \emph{causing} them. However, existing diagnostic algorithms are purely associative, identifying diseases that are strongly correlated with a patients symptoms and medical history. We show that this inability to disentangle correlation from causation can result in sub-optimal or dangerous diagnoses. To overcome this, we reformulate diagnosis as a counterfactual inference task and derive new counterfactual diagnostic algorithms. We show that this approach is closer to the diagnostic reasoning of clinicians and significantly improves the accuracy and safety of the resulting diagnoses. We compare our counterfactual algorithm to the standard Bayesian diagnostic algorithm and a cohort of 44 doctors using a test set of clinical vignettes. While the Bayesian algorithm achieves an accuracy comparable to the average doctor, placing in the top 48% of doctors in our cohort, our counterfactual algorithm places in the top 25% of doctors, achieving expert clinical accuracy. This improvement is achieved simply by changing how we query our model, without requiring any additional model improvements. Our results show that counterfactual reasoning is a vital missing ingredient for applying machine learning to medical diagnosis.

</details>

<details>

<summary>2020-03-13 22:54:35 - Bayesian analysis of ranking data with the constrained Extended Plackett-Luce model</summary>

- *Cristina Mollica, Luca Tardella*

- `1810.04671v2` - [abs](http://arxiv.org/abs/1810.04671v2) - [pdf](http://arxiv.org/pdf/1810.04671v2)

> Multistage ranking models, including the popular Plackett-Luce distribution (PL), rely on the assumption that the ranking process is performed sequentially, by assigning the positions from the top to the bottom one (forward order). A recent contribution to the ranking literature relaxed this assumption with the addition of the discrete-valued reference order parameter, yielding the novel Extended Plackett-Luce model (EPL). Inference on the EPL and its generalization into a finite mixture framework was originally addressed from the frequentist perspective. In this work, we propose the Bayesian estimation of the EPL with order constraints on the reference order parameter. The proposed restrictions reflect a meaningful rank assignment process. By combining the restrictions with the data augmentation strategy and the conjugacy of the Gamma prior distribution with the EPL, we facilitate the construction of a tuned joint Metropolis-Hastings algorithm within Gibbs sampling to simulate from the posterior distribution. The Bayesian approach allows to address more efficiently the inference on the additional discrete-valued parameter and the assessment of its estimation uncertainty. The usefulness of the proposal is illustrated with applications to simulated and real datasets.

</details>

<details>

<summary>2020-03-14 20:18:17 - Bayesian Variable Selection For Survival Data Using Inverse Moment Priors</summary>

- *Amir Nikooienejad, Wenyi Wang, Valen E. Johnson*

- `1712.02964v7` - [abs](http://arxiv.org/abs/1712.02964v7) - [pdf](http://arxiv.org/pdf/1712.02964v7)

> Efficient variable selection in high-dimensional cancer genomic studies is critical for discovering genes associated with specific cancer types and for predicting response to treatment. Censored survival data is prevalent in such studies. In this article we introduce a Bayesian variable selection procedure that uses a mixture prior composed of a point mass at zero and an inverse moment prior in conjunction with the partial likelihood defined by the Cox proportional hazard model. The procedure is implemented in the R package BVSNLP, which supports parallel computing and uses a stochastic search method to explore the model space. Bayesian model averaging is used for prediction. The proposed algorithm provides better performance than other variable selection procedures in simulation studies, and appears to provide more consistent variable selection when applied to actual genomic datasets.

</details>

<details>

<summary>2020-03-14 22:53:26 - Optimally Stopping a Brownian Bridge with an Unknown Pinning Time: A Bayesian Approach</summary>

- *Kristoffer Glover*

- `1902.10261v4` - [abs](http://arxiv.org/abs/1902.10261v4) - [pdf](http://arxiv.org/pdf/1902.10261v4)

> We consider the problem of optimally stopping a Brownian bridge with an unknown pinning time so as to maximise the value of the process upon stopping. Adopting a Bayesian approach, we assume the stopper has a general continuous prior and is allowed to update their belief about the value of the pinning time through sequential observations of the process. Uncertainty in the pinning time influences both the conditional dynamics of the process and the expected (random) horizon of the optimal stopping problem. We analyse certain gamma and beta distributed priors in detail. Remarkably, the optimal stopping problem in the gamma case becomes time homogeneous and is completely solvable in closed form. Moreover, in the beta case we find that the optimal stopping boundary takes on a square-root form, similar to the classical solution with a known pinning time.

</details>

<details>

<summary>2020-03-15 11:55:55 - Semi-Modular Inference: enhanced learning in multi-modular models by tempering the influence of components</summary>

- *Chris U. Carmona, Geoff K. Nicholls*

- `2003.06804v1` - [abs](http://arxiv.org/abs/2003.06804v1) - [pdf](http://arxiv.org/pdf/2003.06804v1)

> Bayesian statistical inference loses predictive optimality when generative models are misspecified.   Working within an existing coherent loss-based generalisation of Bayesian inference, we show existing Modular/Cut-model inference is coherent, and write down a new family of Semi-Modular Inference (SMI) schemes, indexed by an influence parameter, with Bayesian inference and Cut-models as special cases. We give a meta-learning criterion and estimation procedure to choose the inference scheme. This returns Bayesian inference when there is no misspecification.   The framework applies naturally to Multi-modular models. Cut-model inference allows directed information flow from well-specified modules to misspecified modules, but not vice versa. An existing alternative power posterior method gives tunable but undirected control of information flow, improving prediction in some settings. In contrast, SMI allows tunable and directed information flow between modules.   We illustrate our methods on two standard test cases from the literature and a motivating archaeological data set.

</details>

<details>

<summary>2020-03-15 14:48:50 - Bayesian Inference of Spatio-Temporal Changes of Arctic Sea Ice</summary>

- *Bohai Zhang, Noel Cressie*

- `2003.06843v1` - [abs](http://arxiv.org/abs/2003.06843v1) - [pdf](http://arxiv.org/pdf/2003.06843v1)

> Arctic sea ice extent has drawn increasing interest and alarm from geoscientists, owing to its rapid decline. In this article, we propose a Bayesian spatio-temporal hierarchical statistical model for binary Arctic sea ice data over two decades, where a latent dynamic spatio-temporal Gaussian process is used to model the data-dependence through a logit link function. Our ultimate goal is to perform inference on the dynamic spatial behavior of Arctic sea ice over a period of two decades. Physically motivated covariates are assessed using autologistic diagnostics. Our Bayesian spatio-temporal model shows how parameter uncertainty in such a complex hierarchical model can influence spatio-temporal prediction. The posterior distributions of new summary statistics are proposed to detect the changing patterns of Arctic sea ice over two decades since 1997.

</details>

<details>

<summary>2020-03-15 21:14:13 - On the spatial and temporal shift in the archetypal seasonal temperature cycle as driven by annual and semi-annual harmonics</summary>

- *Joshua S. North, Erin M. Schliep, Christopher K. Wikle*

- `2003.06924v1` - [abs](http://arxiv.org/abs/2003.06924v1) - [pdf](http://arxiv.org/pdf/2003.06924v1)

> Statistical methods are required to evaluate and quantify the uncertainty in environmental processes, such as land and sea surface temperature, in a changing climate. Typically, annual harmonics are used to characterize the variation in the seasonal temperature cycle. However, an often overlooked feature of the climate seasonal cycle is the semi-annual harmonic, which can account for a significant portion of the variance of the seasonal cycle and varies in amplitude and phase across space. Together, the spatial variation in the annual and semi-annual harmonics can play an important role in driving processes that are tied to seasonality (e.g., ecological and agricultural processes). We propose a multivariate spatio-temporal model to quantify the spatial and temporal change in minimum and maximum temperature seasonal cycles as a function of the annual and semi-annual harmonics. Our approach captures spatial dependence, temporal dynamics, and multivariate dependence of these harmonics through spatially and temporally-varying coefficients. We apply the model to minimum and maximum temperature over North American for the years 1979 to 2018. Formal model inference within the Bayesian paradigm enables the identification of regions experiencing significant changes in minimum and maximum temperature seasonal cycles due to the relative effects of changes in the two harmonics.

</details>

<details>

<summary>2020-03-16 00:27:02 - Model-based Inference for Rare and Clustered Populations from Adaptive Cluster Sampling using Auxiliary Variables</summary>

- *Izabel Nolau de Souza, Kelly Cristina Mota Gonçalves, João Batista de Morais Pereira*

- `2003.06955v1` - [abs](http://arxiv.org/abs/2003.06955v1) - [pdf](http://arxiv.org/pdf/2003.06955v1)

> Rare populations, such as endangered animals and plants, drug users and individuals with rare diseases, tend to cluster in regions. Adaptive cluster sampling is generally applied to obtain information from clustered and sparse populations since it increases survey effort in areas where the individuals of interest are observed. This work aims to propose a unit-level model which assumes that counts are related to auxiliary variables, improving the sampling process, assigning different weights to the cells, besides referring them spatially. The proposed model fits rare and grouped populations, disposed over a regular grid, in a Bayesian framework. The approach is compared to alternative methods using simulated data and a real experiment in which adaptive samples were drawn from an African Buffaloes population in a 24,108 square kilometers area of East Africa. Simulation studies show that the model is efficient under several settings, validating the methodology proposed in this paper for practical situations.

</details>

<details>

<summary>2020-03-16 04:08:44 - Estimating a novel stochastic model for within-field disease dynamics of banana bunchy top virus via approximate Bayesian computation</summary>

- *Abhishek Varghese, Christopher Drovandi, Kerrie Mengersen, Antonietta Mira*

- `1909.02169v2` - [abs](http://arxiv.org/abs/1909.02169v2) - [pdf](http://arxiv.org/pdf/1909.02169v2)

> The Banana Bunchy Top Virus (BBTV) is one of the most economically important vector-borne banana diseases throughout the Asia-Pacific Basin and presents a significant challenge to the agricultural sector. Current models of BBTV are largely deterministic, limited by an incomplete understanding of interactions in complex natural systems, and the appropriate identification of parameters. A stochastic network-based Susceptible-Infected model has been created which simulates the spread of BBTV across the subsections of a banana plantation, parameterising nodal recovery, neighbouring and distant infectivity across summer and winter. Findings from posterior results achieved through Markov Chain Monte Carlo approach to approximate Bayesian computation suggest seasonality in all parameters, which are influenced by correlated changes in inspection accuracy, temperatures and aphid activity. This paper demonstrates how the model may be used for monitoring and forecasting of various disease management strategies to support policy-level decision making.

</details>

<details>

<summary>2020-03-16 12:33:03 - Unbiased Estimation of the Gradient of the Log-Likelihood in Inverse Problems</summary>

- *Ajay Jasra, Kody J. H. Law, Deng Lu*

- `2003.04896v2` - [abs](http://arxiv.org/abs/2003.04896v2) - [pdf](http://arxiv.org/pdf/2003.04896v2)

> We consider the problem of estimating a parameter associated to a Bayesian inverse problem. Treating the unknown initial condition as a nuisance parameter, typically one must resort to a numerical approximation of gradient of the log-likelihood and also adopt a discretization of the problem in space and/or time. We develop a new methodology to unbiasedly estimate the gradient of the log-likelihood with respect to the unknown parameter, i.e. the expectation of the estimate has no discretization bias. Such a property is not only useful for estimation in terms of the original stochastic model of interest, but can be used in stochastic gradient algorithms which benefit from unbiased estimates. Under appropriate assumptions, we prove that our estimator is not only unbiased but of finite variance. In addition, when implemented on a single processor, we show that the cost to achieve a given level of error is comparable to multilevel Monte Carlo methods, both practically and theoretically. However, the new algorithm provides the possibility for parallel computation on arbitrarily many processors without any loss of efficiency, asymptotically. In practice, this means any precision can be achieved in a fixed, finite constant time, provided that enough processors are available.

</details>

<details>

<summary>2020-03-16 13:43:50 - Laplace approximation for fast Bayesian inference in generalized additive models based on penalized regression splines</summary>

- *Oswaldo Gressani, Philippe Lambert*

- `2003.07214v1` - [abs](http://arxiv.org/abs/2003.07214v1) - [pdf](http://arxiv.org/pdf/2003.07214v1)

> Generalized additive models (GAMs) are a well-established statistical tool for modeling complex nonlinear relationships between covariates and a response assumed to have a conditional distribution in the exponential family. In this article, P-splines and the Laplace approximation are coupled for flexible and fast approximate Bayesian inference in GAMs. The proposed Laplace-P-spline model contributes to the development of a new methodology to explore the posterior penalty space by considering a deterministic grid-based strategy or a Markov chain sampler, depending on the number of smooth additive terms in the predictor. Our approach has the merit of relying on closed form analytical expressions for the gradient and Hessian of the approximate posterior penalty vector, which enables to construct accurate posterior pointwise and credible set estimators for latent field variables at a relatively low computational budget even for a large number of smooth additive components. Based upon simple Gaussian approximations of the conditional latent field posterior, the suggested methodology enjoys excellent statistical properties. The performance of the Laplace-P-spline model is confirmed through different simulation scenarios and the method is illustrated on two real datasets.

</details>

<details>

<summary>2020-03-16 17:18:24 - Uncertainty estimation in equality-constrained MAP and maximum likelihood estimation with applications to system identification and state estimation</summary>

- *Dimas Abreu Archanjo Dutra*

- `2002.10975v2` - [abs](http://arxiv.org/abs/2002.10975v2) - [pdf](http://arxiv.org/pdf/2002.10975v2)

> In unconstrained maximum a posteriori (MAP) and maximum likelihood estimation, the inverse of minus the merit-function Hessian matrix is an approximation of the estimate covariance matrix. In the Bayesian context of MAP estimation, it is the covariance of a normal approximation of the posterior around the mode; while in maximum likelihood estimation, it an approximation of the inverse Fisher information matrix, to which the covariance of efficient estimators converge. These measures are routinely used in system identification to evaluate the estimate uncertainties and diagnose problems such as overparametrization, improper excitation and unidentifiability. A wide variety of estimation problems in systems and control, however, can be formulated as equality-constrained optimizations with additional decision variables to exploit parallelism in computer hardware, simplify implementation and increase the convergence basin and efficiency of the nonlinear program solver. The introduction of the extra variables, however, dissociates the inverse Hessian from the covariance matrix. Instead, submatrices of the inverse Hessian of the constrained-problem's Lagrangian must be used. In this paper, we derive these relationships, showing how the estimates' covariance can be estimated directly from the augmented problem. Application examples are shown in system identification with the output-error method and joint state-path and parameter estimation.

</details>

<details>

<summary>2020-03-16 17:19:23 - Variational Item Response Theory: Fast, Accurate, and Expressive</summary>

- *Mike Wu, Richard L. Davis, Benjamin W. Domingue, Chris Piech, Noah Goodman*

- `2002.00276v2` - [abs](http://arxiv.org/abs/2002.00276v2) - [pdf](http://arxiv.org/pdf/2002.00276v2)

> Item Response Theory (IRT) is a ubiquitous model for understanding humans based on their responses to questions, used in fields as diverse as education, medicine and psychology. Large modern datasets offer opportunities to capture more nuances in human behavior, potentially improving test scoring and better informing public policy. Yet larger datasets pose a difficult speed / accuracy challenge to contemporary algorithms for fitting IRT models. We introduce a variational Bayesian inference algorithm for IRT, and show that it is fast and scaleable without sacrificing accuracy. Using this inference approach we then extend classic IRT with expressive Bayesian models of responses. Applying this method to five large-scale item response datasets from cognitive science and education yields higher log likelihoods and improvements in imputing missing data. The algorithm implementation is open-source, and easily usable.

</details>

<details>

<summary>2020-03-16 18:38:40 - Flexible Prior Elicitation via the Prior Predictive Distribution</summary>

- *Marcelo Hartmann, Georgi Agiashvili, Paul Bürkner, Arto Klami*

- `2002.09868v3` - [abs](http://arxiv.org/abs/2002.09868v3) - [pdf](http://arxiv.org/pdf/2002.09868v3)

> The prior distribution for the unknown model parameters plays a crucial role in the process of statistical inference based on Bayesian methods. However, specifying suitable priors is often difficult even when detailed prior knowledge is available in principle. The challenge is to express quantitative information in the form of a probability distribution. Prior elicitation addresses this question by extracting subjective information from an expert and transforming it into a valid prior. Most existing methods, however, require information to be provided on the unobservable parameters, whose effect on the data generating process is often complicated and hard to understand. We propose an alternative approach that only requires knowledge about the observable outcomes - knowledge which is often much easier for experts to provide. Building upon a principled statistical framework, our approach utilizes the prior predictive distribution implied by the model to automatically transform experts judgements about plausible outcome values to suitable priors on the parameters. We also provide computational strategies to perform inference and guidelines to facilitate practical use.

</details>

<details>

<summary>2020-03-16 18:51:33 - A Bayesian Nonparametric Latent Space Approach to Modeling Evolving Communities in Dynamic Networks</summary>

- *Joshua Daniel Loyal, Yuguo Chen*

- `2003.07404v1` - [abs](http://arxiv.org/abs/2003.07404v1) - [pdf](http://arxiv.org/pdf/2003.07404v1)

> The evolution of communities in dynamic (time-varying) network data is a prominent topic of interest. A popular approach to understanding these dynamic networks is to embed the dyadic relations into a latent metric space. While methods for clustering with this approach exist for dynamic networks, they all assume a static community structure. This paper presents a Bayesian nonparametric model for dynamic networks that can model networks with evolving community structures. Our model extends existing latent space approaches by explicitly modeling the additions, deletions, splits, and mergers of groups with a hierarchical Dirichlet process hidden Markov model. Our proposed approach, the hierarchical Dirichlet process latent position clustering model (HDP-LPCM), incorporates transitivity, models both individual and group level aspects of the data, and avoids the computationally expensive selection of the number of groups required by most popular methods. We provide a Markov chain Monte Carlo estimation algorithm and apply our method to synthetic and real-world networks to demonstrate its performance.

</details>

<details>

<summary>2020-03-16 21:42:17 - Poincaré Wasserstein Autoencoder</summary>

- *Ivan Ovinnikov*

- `1901.01427v2` - [abs](http://arxiv.org/abs/1901.01427v2) - [pdf](http://arxiv.org/pdf/1901.01427v2)

> This work presents a reformulation of the recently proposed Wasserstein autoencoder framework on a non-Euclidean manifold, the Poincar\'e ball model of the hyperbolic space. By assuming the latent space to be hyperbolic, we can use its intrinsic hierarchy to impose structure on the learned latent space representations. We demonstrate the model in the visual domain to analyze some of its properties and show competitive results on a graph link prediction task.

</details>

<details>

<summary>2020-03-17 12:26:30 - Degree irregularity and rank probability bias in network meta-analysis</summary>

- *Annabel L Davies, Tobias Galla*

- `2003.07662v1` - [abs](http://arxiv.org/abs/2003.07662v1) - [pdf](http://arxiv.org/pdf/2003.07662v1)

> Network meta-analysis (NMA) is a statistical technique for the comparison of treatment options. The nodes of the network are the competing treatments and edges represent comparisons of treatments in trials. Outcomes of Bayesian NMA include estimates of treatment effects, and the probabilities that each treatment is ranked best, second best and so on. How exactly network geometry affects the accuracy and precision of these outcomes is not fully understood. Here we carry out a simulation study and find that disparity in the number of trials involving different treatments leads to a systematic bias in estimated rank probabilities. This bias is associated with an increased variation in the precision of treatment effect estimates. Using ideas from the theory of complex networks, we define a measure of `degree irregularity' to quantify asymmetry in the number of studies involving each treatment. Our simulations indicate that more regular networks have more precise treatment effect estimates and smaller bias of rank probabilities. We also find that degree regularity is a better indicator of NMA quality than both the total number of studies in a network and the disparity in the number of trials per comparison. These results have implications for planning future trials. We demonstrate that choosing trials which reduce the network's irregularity can improve the precision and accuracy of NMA outcomes.

</details>

<details>

<summary>2020-03-17 13:38:43 - Nonparametric Deconvolution Models</summary>

- *Allison J. B. Chaney, Archit Verma, Young-suk Lee, Barbara E. Engelhardt*

- `2003.07718v1` - [abs](http://arxiv.org/abs/2003.07718v1) - [pdf](http://arxiv.org/pdf/2003.07718v1)

> We describe nonparametric deconvolution models (NDMs), a family of Bayesian nonparametric models for collections of data in which each observation is the average over the features from heterogeneous particles. For example, these types of data are found in elections, where we observe precinct-level vote tallies (observations) of individual citizens' votes (particles) across each of the candidates or ballot measures (features), where each voter is part of a specific voter cohort or demographic (factor). Like the hierarchical Dirichlet process, NDMs rely on two tiers of Dirichlet processes to explain the data with an unknown number of latent factors; each observation is modeled as a weighted average of these latent factors. Unlike existing models, NDMs recover how factor distributions vary locally for each observation. This uniquely allows NDMs both to deconvolve each observation into its constituent factors, and also to describe how the factor distributions specific to each observation vary across observations and deviate from the corresponding global factors. We present variational inference techniques for this family of models and study its performance on simulated data and voting data from California. We show that including local factors improves estimates of global factors and provides a novel scaffold for exploring data.

</details>

<details>

<summary>2020-03-17 15:14:25 - Characterizing and Avoiding Problematic Global Optima of Variational Autoencoders</summary>

- *Yaniv Yacoby, Weiwei Pan, Finale Doshi-Velez*

- `2003.07756v1` - [abs](http://arxiv.org/abs/2003.07756v1) - [pdf](http://arxiv.org/pdf/2003.07756v1)

> Variational Auto-encoders (VAEs) are deep generative latent variable models consisting of two components: a generative model that captures a data distribution p(x) by transforming a distribution p(z) over latent space, and an inference model that infers likely latent codes for each data point (Kingma and Welling, 2013). Recent work shows that traditional training methods tend to yield solutions that violate modeling desiderata: (1) the learned generative model captures the observed data distribution but does so while ignoring the latent codes, resulting in codes that do not represent the data (e.g. van den Oord et al. (2017); Kim et al. (2018)); (2) the aggregate of the learned latent codes does not match the prior p(z). This mismatch means that the learned generative model will be unable to generate realistic data with samples from p(z)(e.g. Makhzani et al. (2015); Tomczak and Welling (2017)). In this paper, we demonstrate that both issues stem from the fact that the global optima of the VAE training objective often correspond to undesirable solutions. Our analysis builds on two observations: (1) the generative model is unidentifiable - there exist many generative models that explain the data equally well, each with different (and potentially unwanted) properties and (2) bias in the VAE objective - the VAE objective may prefer generative models that explain the data poorly but have posteriors that are easy to approximate. We present a novel inference method, LiBI, mitigating the problems identified in our analysis. On synthetic datasets, we show that LiBI can learn generative models that capture the data distribution and inference models that better satisfy modeling assumptions when traditional methods struggle to do so.

</details>

<details>

<summary>2020-03-17 23:09:47 - Mixing Rates for Hamiltonian Monte Carlo Algorithms in Finite and Infinite Dimensions</summary>

- *Nathan E. Glatt-Holtz, Cecilia F. Mondaini*

- `2003.07980v1` - [abs](http://arxiv.org/abs/2003.07980v1) - [pdf](http://arxiv.org/pdf/2003.07980v1)

> We establish the geometric ergodicity of the preconditioned Hamiltonian Monte Carlo (HMC) algorithm defined on an infinite-dimensional Hilbert space, as developed in [Beskos et al., Stochastic Process. Appl., 2011]. This algorithm can be used as a basis to sample from certain classes of target measures which are absolutely continuous with respect to a Gaussian measure. Our work addresses an open question posed in [Beskos et al., Stochastic Process. Appl., 2011], and provides an alternative to a recent proof based on exact coupling techniques given in arXiv:1909.07962. The approach here establishes convergence in a suitable Wasserstein distance by using the weak Harris theorem together with a generalized coupling argument. We also show that a law of large numbers and central limit theorem can be derived as a consequence of our main convergence result. Moreover, our approach yields a novel proof of mixing rates for the classical finite-dimensional HMC algorithm. As such, the methodology we develop provides a flexible framework to tackle the rigorous convergence of other Markov Chain Monte Carlo algorithms. Additionally, we show that the scope of our result includes certain measures that arise in the Bayesian approach to inverse PDE problems, cf. [Stuart, Acta Numer., 2010]. Particularly, we verify all of the required assumptions for a certain class of inverse problems involving the recovery of a divergence free vector field from a passive scalar, arXiv:1808.01084v3.

</details>

<details>

<summary>2020-03-18 08:36:22 - MALA-within-Gibbs samplers for high-dimensional distributions with sparse conditional structure</summary>

- *X. T. Tong, M. Morzfeld, Y. M. Marzouk*

- `1908.09429v2` - [abs](http://arxiv.org/abs/1908.09429v2) - [pdf](http://arxiv.org/pdf/1908.09429v2)

> Markov chain Monte Carlo (MCMC) samplers are numerical methods for drawing samples from a given target probability distribution. We discuss one particular MCMC sampler, the MALA-within-Gibbs sampler, from the theoretical and practical perspectives. We first show that the acceptance ratio and step size of this sampler are independent of the overall problem dimension when (i) the target distribution has sparse conditional structure, and (ii) this structure is reflected in the partial updating strategy of MALA-within-Gibbs. If, in addition, the target density is block-wise log-concave, then the sampler's convergence rate is independent of dimension. From a practical perspective, we expect that MALA-within-Gibbs is useful for solving high-dimensional Bayesian inference problems where the posterior exhibits sparse conditional structure at least approximately. In this context, a partitioning of the state that correctly reflects the sparse conditional structure must be found, and we illustrate this process in two numerical examples. We also discuss trade-offs between the block size used for partial updating and computational requirements that may increase with the number of blocks.

</details>

<details>

<summary>2020-03-19 03:18:24 - Simulator Calibration under Covariate Shift with Kernels</summary>

- *Keiichi Kisamori, Motonobu Kanagawa, Keisuke Yamazaki*

- `1809.08159v4` - [abs](http://arxiv.org/abs/1809.08159v4) - [pdf](http://arxiv.org/pdf/1809.08159v4)

> We propose a novel calibration method for computer simulators, dealing with the problem of covariate shift. Covariate shift is the situation where input distributions for training and test are different, and ubiquitous in applications of simulations. Our approach is based on Bayesian inference with kernel mean embedding of distributions, and on the use of an importance-weighted reproducing kernel for covariate shift adaptation. We provide a theoretical analysis for the proposed method, including a novel theoretical result for conditional mean embedding, as well as empirical investigations suggesting its effectiveness in practice. The experiments include calibration of a widely used simulator for industrial manufacturing processes, where we also demonstrate how the proposed method may be useful for sensitivity analysis of model parameters.

</details>

<details>

<summary>2020-03-19 09:56:48 - Decision-making with multiple correlated binary outcomes in clinical trials</summary>

- *X. M. Kavelaars, J. Mulder, M. C. Kaptein*

- `1908.10158v2` - [abs](http://arxiv.org/abs/1908.10158v2) - [pdf](http://arxiv.org/pdf/1908.10158v2)

> Clinical trials often evaluate multiple outcome variables to form a comprehensive picture of the effects of a new treatment. The resulting multidimensional insight contributes to clinically relevant and efficient decision-making about treatment superiority. Common statistical procedures to make these superiority decisions with multiple outcomes have two important shortcomings however: 1) Outcome variables are often modeled individually, and consequently fail to consider the relation between outcomes; and 2) superiority is often defined as a relevant difference on a single, on any, or on all outcomes(s); and lacks a compensatory mechanism that allows large positive effects on one or multiple outcome(s) to outweigh small negative effects on other outcomes. To address these shortcomings, this paper proposes 1) a Bayesian model for the analysis of correlated binary outcomes based on the multivariate Bernoulli distribution; and 2) a flexible decision criterion with a compensatory mechanism that captures the relative importance of the outcomes. A simulation study demonstrates that efficient and unbiased decisions can be made while Type I error rates are properly controlled. The performance of the framework is illustrated for 1) fixed, group sequential, and adaptive designs; and 2) non-informative and informative prior distributions.

</details>

<details>

<summary>2020-03-19 13:55:48 - Accelerating proximal Markov chain Monte Carlo by using an explicit stabilised method</summary>

- *Luis Vargas, Marcelo Pereyra, Konstantinos C. Zygalakis*

- `1908.08845v3` - [abs](http://arxiv.org/abs/1908.08845v3) - [pdf](http://arxiv.org/pdf/1908.08845v3)

> We present a highly efficient proximal Markov chain Monte Carlo methodology to perform Bayesian computation in imaging problems. Similarly to previous proximal Monte Carlo approaches, the proposed method is derived from an approximation of the Langevin diffusion. However, instead of the conventional Euler-Maruyama approximation that underpins existing proximal Monte Carlo methods, here we use a state-of-the-art orthogonal Runge-Kutta-Chebyshev stochastic approximation that combines several gradient evaluations to significantly accelerate its convergence speed, similarly to accelerated gradient optimisation methods. The proposed methodology is demonstrated via a range of numerical experiments, including non-blind image deconvolution, hyperspectral unmixing, and tomographic reconstruction, with total-variation and $\ell_1$-type priors. Comparisons with Euler-type proximal Monte Carlo methods confirm that the Markov chains generated with our method exhibit significantly faster convergence speeds, achieve larger effective sample sizes, and produce lower mean square estimation errors at equal computational budget.

</details>

<details>

<summary>2020-03-19 16:12:10 - Unified model selection approach based on minimum description length principle in Granger causality analysis</summary>

- *Fei Li, Xuewei Wang, Qiang Lin, Zhenghui Hu*

- `1910.11537v2` - [abs](http://arxiv.org/abs/1910.11537v2) - [pdf](http://arxiv.org/pdf/1910.11537v2)

> Granger causality analysis (GCA) provides a powerful tool for uncovering the patterns of brain connectivity mechanism using neuroimaging techniques. Conventional GCA applies two different mathematical theories in a two-stage scheme: (1) the Bayesian information criterion (BIC) or Akaike information criterion (AIC) for the regression model orders associated with endogenous and exogenous information; (2) F-statistics for determining the causal effects of exogenous variables. While specifying endogenous and exogenous effects are essentially the same model selection problem, this could produce different benchmarks in the two stages and therefore degrade the performance of GCA. In this course, we present a unified model selection approach based on the minimum description length (MDL) principle for GCA in the context of the general regression model paradigm. Compared with conventional methods, our approach emphasize that a single mathematical theory should be held throughout the GCA process. Under this framework, all candidate models within the model space might be compared freely in the context of the code length, without the need for an intermediate model. We illustrate its advantages over conventional two-stage GCA approach in a 3-node network and a 5-node network synthetic experiments. The unified model selection approach is capable of identifying the actual connectivity while avoiding the false influences of noise. More importantly, the proposed approach obtained more consistent results in a challenge fMRI dataset for causality investigation, mental calculation network under visual and auditory stimulus, respectively. The proposed approach has potential to accommodate other Granger causality representations in other function space. The comparison between different GC representations in different function spaces can also be naturally deal with in the framework.

</details>

<details>

<summary>2020-03-19 22:49:08 - Systematic statistical analysis of microbial data from dilution series</summary>

- *J Andrés Christen, Al Parker*

- `2003.09039v1` - [abs](http://arxiv.org/abs/2003.09039v1) - [pdf](http://arxiv.org/pdf/2003.09039v1)

> In microbial studies, samples are often treated under different experimental conditions and then tested for microbial survival. A technique, dating back to the 1880's, consists of diluting the samples several times and incubating each dilution to verify the existence of microbial Colony Forming Units or CFU's, seen by the naked eye. The main problem in the dilution series data analysis is the uncertainty quantification of the simple point estimate of the original number of CFU's in the sample (i.e., at dilution zero). Common approaches such as log-normal or Poisson models do not seem to handle well extreme cases with low or high counts, among other issues. We build a novel binomial model, based on the actual design of the experimental procedure including the dilution series. For repetitions we construct a hierarchical model for experimental results from a single lab and in turn a higher hierarchy for inter-lab analyses. Results seem promising, with a systematic treatment of all data cases, including zeros, censored data, repetitions, intra and inter-laboratory studies. Using a Bayesian approach, a robust and efficient MCMC method is used to analyze several real data sets.

</details>

<details>

<summary>2020-03-20 16:52:10 - Sequential Bayesian Experimental Design for Implicit Models via Mutual Information</summary>

- *Steven Kleinegesse, Christopher Drovandi, Michael U. Gutmann*

- `2003.09379v1` - [abs](http://arxiv.org/abs/2003.09379v1) - [pdf](http://arxiv.org/pdf/2003.09379v1)

> Bayesian experimental design (BED) is a framework that uses statistical models and decision making under uncertainty to optimise the cost and performance of a scientific experiment. Sequential BED, as opposed to static BED, considers the scenario where we can sequentially update our beliefs about the model parameters through data gathered in the experiment. A class of models of particular interest for the natural and medical sciences are implicit models, where the data generating distribution is intractable, but sampling from it is possible. Even though there has been a lot of work on static BED for implicit models in the past few years, the notoriously difficult problem of sequential BED for implicit models has barely been touched upon. We address this gap in the literature by devising a novel sequential design framework for parameter estimation that uses the Mutual Information (MI) between model parameters and simulated data as a utility function to find optimal experimental designs, which has not been done before for implicit models. Our approach uses likelihood-free inference by ratio estimation to simultaneously estimate posterior distributions and the MI. During the sequential BED procedure we utilise Bayesian optimisation to help us optimise the MI utility. We find that our framework is efficient for the various implicit models tested, yielding accurate parameter estimates after only a few iterations.

</details>

<details>

<summary>2020-03-20 20:28:27 - Simultaneous Inference for Multiple Proportions: A Multivariate Beta-Binomial Model</summary>

- *Max Westphal*

- `1911.00098v4` - [abs](http://arxiv.org/abs/1911.00098v4) - [pdf](http://arxiv.org/pdf/1911.00098v4)

> Statistical inference in high-dimensional settings is challenging when standard unregularized methods are employed. In this work, we focus on the case of multiple correlated proportions for which we develop a Bayesian inference framework. For this purpose, we construct an $m$-dimensional Beta distribution from a $2^m$-dimensional Dirichlet distribution, building on work by Olkin and Trikalinos (2015). This readily leads to a multivariate Beta-binomial model for which simple update rules from the common Dirichlet-multinomial model can be adopted. From the frequentist perspective, this approach amounts to adding pseudo-observations to the data and allows a joint shrinkage estimation of mean vector and covariance matrix. For higher dimensions ($m > 10$), the extensive model based on $2^m$ parameters starts to become numerically infeasible. To counter this problem, we utilize a reduced parametrisation which has only $1 + m(m + 1)/2$ parameters describing first and second order moments. A copula model can then be used to approximate the (posterior) multivariate Beta distribution. A natural inference goal is the construction of multivariate credible regions. The properties of different credible regions are assessed in a simulation study in the context of investigating the accuracy of multiple binary classifiers. It is shown that the extensive and copula approach lead to a (Bayes) coverage probability very close to the target level. In this regard, they outperform credible regions based on a normal approximation of the posterior distribution, in particular for small sample sizes. Additionally, they always lead to credible regions which lie entirely in the parameter space which is not the case when the normal approximation is used.

</details>

<details>

<summary>2020-03-21 15:20:03 - Borrowing from Supplemental Sources to Estimate Causal Effects from a Primary Data Source</summary>

- *Jeffrey A. Boatman, David M. Vock, Joseph S. Koopmeiners*

- `2003.09680v1` - [abs](http://arxiv.org/abs/2003.09680v1) - [pdf](http://arxiv.org/pdf/2003.09680v1)

> The increasing multiplicity of data sources offers exciting possibilities in estimating the effects of a treatment, intervention, or exposure, particularly if observational and experimental sources could be used simultaneously. Borrowing between sources can potentially result in more efficient estimators, but it must be done in a principled manner to mitigate increased bias and Type I error. Furthermore, when the effect of treatment is confounded, as in observational sources or in clinical trials with noncompliance, causal effect estimators are needed to simultaneously adjust for confounding and to estimate effects across data sources. We consider the problem of estimating causal effects from a primary source and borrowing from any number of supplemental sources. We propose using regression-based estimators that borrow based on assuming exchangeability of the regression coefficients and parameters between data sources. Borrowing is accomplished with multisource exchangeability models and Bayesian model averaging. We show via simulation that a Bayesian linear model and Bayesian additive regression trees both have desirable properties and borrow under appropriate circumstances. We apply the estimators to recently completed trials of very low nicotine content cigarettes investigating their impact on smoking behavior.

</details>

<details>

<summary>2020-03-22 10:40:30 - Distributionally Robust Bayesian Optimization</summary>

- *Johannes Kirschner, Ilija Bogunovic, Stefanie Jegelka, Andreas Krause*

- `2002.09038v3` - [abs](http://arxiv.org/abs/2002.09038v3) - [pdf](http://arxiv.org/pdf/2002.09038v3)

> Robustness to distributional shift is one of the key challenges of contemporary machine learning. Attaining such robustness is the goal of distributionally robust optimization, which seeks a solution to an optimization problem that is worst-case robust under a specified distributional shift of an uncontrolled covariate. In this paper, we study such a problem when the distributional shift is measured via the maximum mean discrepancy (MMD). For the setting of zeroth-order, noisy optimization, we present a novel distributionally robust Bayesian optimization algorithm (DRBO). Our algorithm provably obtains sub-linear robust regret in various settings that differ in how the uncertain covariate is observed. We demonstrate the robust performance of our method on both synthetic and real-world benchmarks.

</details>

<details>

<summary>2020-03-22 14:51:04 - Cost-aware Bayesian Optimization</summary>

- *Eric Hans Lee, Valerio Perrone, Cedric Archambeau, Matthias Seeger*

- `2003.10870v1` - [abs](http://arxiv.org/abs/2003.10870v1) - [pdf](http://arxiv.org/pdf/2003.10870v1)

> Bayesian optimization (BO) is a class of global optimization algorithms, suitable for minimizing an expensive objective function in as few function evaluations as possible. While BO budgets are typically given in iterations, this implicitly measures convergence in terms of iteration count and assumes each evaluation has identical cost. In practice, evaluation costs may vary in different regions of the search space. For example, the cost of neural network training increases quadratically with layer size, which is a typical hyperparameter. Cost-aware BO measures convergence with alternative cost metrics such as time, energy, or money, for which vanilla BO methods are unsuited. We introduce Cost Apportioned BO (CArBO), which attempts to minimize an objective function in as little cost as possible. CArBO combines a cost-effective initial design with a cost-cooled optimization phase which depreciates a learned cost model as iterations proceed. On a set of 20 black-box function optimization problems we show that, given the same cost budget, CArBO finds significantly better hyperparameter configurations than competing methods.

</details>

<details>

<summary>2020-03-22 22:38:32 - A multiple testing framework for diagnostic accuracy studies with co-primary endpoints</summary>

- *Max Westphal, Antonia Zapf, Werner Brannath*

- `1911.02982v2` - [abs](http://arxiv.org/abs/1911.02982v2) - [pdf](http://arxiv.org/pdf/1911.02982v2)

> Major advances have been made regarding the utilization of artificial intelligence in health care. In particular, deep learning approaches have been successfully applied for automated and assisted disease diagnosis and prognosis based on complex and high-dimensional data. However, despite all justified enthusiasm, overoptimistic assessments of predictive performance are still common. Automated medical testing devices based on machine-learned prediction models should thus undergo a throughout evaluation before being implemented into clinical practice. In this work, we propose a multiple testing framework for (comparative) phase III diagnostic accuracy studies with sensitivity and specificity as co-primary endpoints. Our approach challenges the frequent recommendation to strictly separate model selection and evaluation, i.e. to only assess a single diagnostic model in the evaluation study. We show that our parametric simultaneous test procedure asymptotically allows strong control of the family-wise error rate. Moreover, we demonstrate in extensive simulation studies that our multiple testing strategy on average leads to a better final diagnostic model and increased statistical power. To plan such studies, we propose a Bayesian approach to determine the optimal number of models to evaluate. For this purpose, our algorithm optimizes the expected final model performance given previous (hold-out) data from the model development phase. We conclude that an assessment of multiple promising diagnostic models in the same evaluation study has several advantages when suitable adjustments for multiple comparisons are implemented.

</details>

<details>

<summary>2020-03-23 10:36:52 - Deep Bayesian Gaussian Processes for Uncertainty Estimation in Electronic Health Records</summary>

- *Yikuan Li, Shishir Rao, Abdelaali Hassaine, Rema Ramakrishnan, Yajie Zhu, Dexter Canoy, Gholamreza Salimi-Khorshidi, Thomas Lukasiewicz, Kazem Rahimi*

- `2003.10170v1` - [abs](http://arxiv.org/abs/2003.10170v1) - [pdf](http://arxiv.org/pdf/2003.10170v1)

> One major impediment to the wider use of deep learning for clinical decision making is the difficulty of assigning a level of confidence to model predictions. Currently, deep Bayesian neural networks and sparse Gaussian processes are the main two scalable uncertainty estimation methods. However, deep Bayesian neural network suffers from lack of expressiveness, and more expressive models such as deep kernel learning, which is an extension of sparse Gaussian process, captures only the uncertainty from the higher level latent space. Therefore, the deep learning model under it lacks interpretability and ignores uncertainty from the raw data. In this paper, we merge features of the deep Bayesian learning framework with deep kernel learning to leverage the strengths of both methods for more comprehensive uncertainty estimation. Through a series of experiments on predicting the first incidence of heart failure, diabetes and depression applied to large-scale electronic medical records, we demonstrate that our method is better at capturing uncertainty than both Gaussian processes and deep Bayesian neural networks in terms of indicating data insufficiency and distinguishing true positive and false positive predictions, with a comparable generalisation performance. Furthermore, by assessing the accuracy and area under the receiver operating characteristic curve over the predictive probability, we show that our method is less susceptible to making overconfident predictions, especially for the minority class in imbalanced datasets. Finally, we demonstrate how uncertainty information derived by the model can inform risk factor analysis towards model interpretability.

</details>

<details>

<summary>2020-03-23 21:57:25 - A Hierarchical Max-Infinitely Divisible Spatial Model for Extreme Precipitation</summary>

- *Gregory P. Bopp, Benjamin A. Shaby, Raphaël Huser*

- `1805.06084v3` - [abs](http://arxiv.org/abs/1805.06084v3) - [pdf](http://arxiv.org/pdf/1805.06084v3)

> Understanding the spatial extent of extreme precipitation is necessary for determining flood risk and adequately designing infrastructure (e.g., stormwater pipes) to withstand such hazards. While environmental phenomena typically exhibit weakening spatial dependence at increasingly extreme levels, limiting max-stable process models for block maxima have a rigid dependence structure that does not capture this type of behavior. We propose a flexible Bayesian model from a broader family of (conditionally) max-infinitely divisible processes that allows for weakening spatial dependence at increasingly extreme levels, and due to a hierarchical representation of the likelihood in terms of random effects, our inference approach scales to large datasets. Therefore, our model not only has a flexible dependence structure, but it also allows for fast, fully Bayesian inference, prediction and conditional simulation in high dimensions. The proposed model is constructed using flexible random basis functions that are estimated from the data, allowing for straightforward inspection of the predominant spatial patterns of extremes. In addition, the described process possesses (conditional) max-stability as a special case, making inference on the tail dependence class possible. We apply our model to extreme precipitation in North-Eastern America, and show that the proposed model adequately captures the extremal behavior of the data. Interestingly, we find that the principal modes of spatial variation estimated from our model resemble observed patterns in extreme precipitation events occurring along the coast (e.g., with localized tropical cyclones and convective storms) and mountain range borders. Our model, which can easily be adapted to other types of environmental datasets, is therefore useful to identify extreme weather patterns and regions at risk.

</details>

<details>

<summary>2020-03-24 00:40:29 - A Bayesian semi-parametric hybrid model for spatial extremes with unknown dependence structure</summary>

- *Yuan Tian, Brian J. Reich*

- `2003.10592v1` - [abs](http://arxiv.org/abs/2003.10592v1) - [pdf](http://arxiv.org/pdf/2003.10592v1)

> The max-stable process is an asymptotically justified model for spatial extremes. In particular, we focus on the hierarchical extreme-value process (HEVP), which is a particular max-stable process that is conducive to Bayesian computing. The HEVP and all max-stable process models are parametric and impose strong assumptions including that all marginal distributions belong to the generalized extreme value family and that nearby sites are asymptotically dependent. We generalize the HEVP by relaxing these assumptions to provide a wider class of marginal distributions via a Dirichlet process prior for the spatial random effects distribution. In addition, we present a hybrid max-mixture model that combines the strengths of the parametric and semi-parametric models. We show that this versatile max-mixture model accommodates both asymptotic independence and dependence and can be fit using standard Markov chain Monte Carlo algorithms. The utility of our model is evaluated in Monte Carlo simulation studies and application to Netherlands wind gust data.

</details>

<details>

<summary>2020-03-24 01:08:54 - Heterogeneity Pursuit for Spatial Point Pattern with Application to Tree Locations: A Bayesian Semiparametric Recourse</summary>

- *Jieying Jiao, Guanyu Hu, Jun Yan*

- `2003.10043v2` - [abs](http://arxiv.org/abs/2003.10043v2) - [pdf](http://arxiv.org/pdf/2003.10043v2)

> Spatial point pattern data are routinely encountered. A flexible regression model for the underlying intensity is essential to characterizing the spatial point pattern and understanding the impacts of potential risk factors on such pattern. We propose a Bayesian semiparametric regression model where the observed spatial points follow a spatial Poisson process with an intensity function which adjusts a nonparametric baseline intensity with multiplicative covariate effects. The baseline intensity is piecewise constant, approached with a powered Chinese restaurant process prior which prevents an unnecessarily large number of pieces. The parametric regression part allows for variable selection through the spike-slab prior on the regression coefficients. An efficient Markov chain Monte Carlo (MCMC) algorithm is developed for the proposed methods. The performance of the methods is validated in an extensive simulation study. In application to the locations of Beilschmiedia pendula trees in the Barro Colorado Island forest dynamics research plot in central Panama, the spatial heterogeneity is attributed to a subset of soil measurements in addition to geographic measurements with a spatially varying baseline intensity.

</details>

<details>

<summary>2020-03-24 01:22:03 - Defense Through Diverse Directions</summary>

- *Christopher M. Bender, Yang Li, Yifeng Shi, Michael K. Reiter, Junier B. Oliva*

- `2003.10602v1` - [abs](http://arxiv.org/abs/2003.10602v1) - [pdf](http://arxiv.org/pdf/2003.10602v1)

> In this work we develop a novel Bayesian neural network methodology to achieve strong adversarial robustness without the need for online adversarial training. Unlike previous efforts in this direction, we do not rely solely on the stochasticity of network weights by minimizing the divergence between the learned parameter distribution and a prior. Instead, we additionally require that the model maintain some expected uncertainty with respect to all input covariates. We demonstrate that by encouraging the network to distribute evenly across inputs, the network becomes less susceptible to localized, brittle features which imparts a natural robustness to targeted perturbations. We show empirical robustness on several benchmark datasets.

</details>

<details>

<summary>2020-03-24 09:19:27 - Model selection criteria of the standard censored regression model based on the bootstrap sample augmentation mechanism</summary>

- *Yue Su, Patrick Kandege Mwanakatwe*

- `2003.10726v1` - [abs](http://arxiv.org/abs/2003.10726v1) - [pdf](http://arxiv.org/pdf/2003.10726v1)

> The statistical regression technique is an extraordinarily essential data fitting tool to explore the potential possible generation mechanism of the random phenomenon. Therefore, the model selection or the variable selection is becoming extremely important so as to identify the most appropriate model with the most optimal explanation effect on the interesting response. In this paper, we discuss and compare the bootstrap-based model selection criteria on the standard censored regression model (Tobit regression model) under the circumstance of limited observation information. The Monte Carlo numerical evidence demonstrates that the performances of the model selection criteria based on the bootstrap sample augmentation strategy will become more competitive than their alternative ones, such as the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) etc. under the circumstance of the inadequate observation information. Meanwhile, the numerical simulation experiments further demonstrate that the model identification risk due to the deficiency of the data information, such as the high censoring rate and rather limited number of observations, can be adequately compensated by increasing the scientific computation cost in terms of the bootstrap sample augmentation strategies. We also apply the recommended bootstrap-based model selection criterion on the Tobit regression model to fit the real fidelity dataset.

</details>

<details>

<summary>2020-03-24 10:36:25 - Subsampling Sequential Monte Carlo for Static Bayesian Models</summary>

- *David Gunawan, Khue-Dung Dang, Matias Quiroz, Robert Kohn, Minh-Ngoc Tran*

- `1805.03317v3` - [abs](http://arxiv.org/abs/1805.03317v3) - [pdf](http://arxiv.org/pdf/1805.03317v3)

> We show how to speed up Sequential Monte Carlo (SMC) for Bayesian inference in large data problems by data subsampling. SMC sequentially updates a cloud of particles through a sequence of distributions, beginning with a distribution that is easy to sample from such as the prior and ending with the posterior distribution. Each update of the particle cloud consists of three steps: reweighting, resampling, and moving. In the move step, each particle is moved using a Markov kernel; this is typically the most computationally expensive part, particularly when the dataset is large. It is crucial to have an efficient move step to ensure particle diversity. Our article makes two important contributions. First, in order to speed up the SMC computation, we use an approximately unbiased and efficient annealed likelihood estimator based on data subsampling. The subsampling approach is more memory efficient than the corresponding full data SMC, which is an advantage for parallel computation. Second, we use a Metropolis within Gibbs kernel with two conditional updates. A Hamiltonian Monte Carlo update makes distant moves for the model parameters, and a block pseudo-marginal proposal is used for the particles corresponding to the auxiliary variables for the data subsampling. We demonstrate both the usefulness and limitations of the methodology for estimating four generalized linear models and a generalized additive model with large datasets.

</details>

<details>

<summary>2020-03-24 13:47:56 - Learning relevant features for statistical inference</summary>

- *Cédric Bény*

- `1904.10387v4` - [abs](http://arxiv.org/abs/1904.10387v4) - [pdf](http://arxiv.org/pdf/1904.10387v4)

> Given two views of data, we consider the problem of finding the features of one view which can be most faithfully inferred from the other. We find that these are also the most correlated variables in the sense of deep canonical correlation analysis (DCCA). Moreover, we show that these variables can be used to construct a non-parametric representation of the implied joint probability distribution, which can be thought of as a classical version of the Schmidt decomposition of quantum states. This representation can be used to compute the expectations of functions over one view of data conditioned on the other, such as Bayesian estimators and their standard deviations. We test the approach using inference on occluded MNIST images, and show that our representation contains multiple modes. Surprisingly, when applied to supervised learning (one dataset consists of labels), this approach automatically provides regularization and faster convergence compared to the cross-entropy objective. We also explore using this approach to discover salient independent variables of a single dataset.

</details>

<details>

<summary>2020-03-25 03:45:22 - Posterior Contraction and Credible Sets for Filaments of Regression Functions</summary>

- *Wei Li, Subhashis Ghosal*

- `1803.03898v3` - [abs](http://arxiv.org/abs/1803.03898v3) - [pdf](http://arxiv.org/pdf/1803.03898v3)

> A filament consists of local maximizers of a smooth function $f$ when moving in a certain direction. A filamentary structure is an important feature of the shape of an object and is also considered as an important lower dimensional characterization of multivariate data. There have been some recent theoretical studies of filaments in the nonparametric kernel density estimation context. This paper supplements the current literature in two ways. First, we provide a Bayesian approach to the filament estimation in regression context and study the posterior contraction rates using a finite random series of B-splines basis. Compared with the kernel-estimation method, this has a theoretical advantage as the bias can be better controlled when the function is smoother, which allows obtaining better rates. Assuming that $f: \mathbb{R}^2 \mapsto \mathbb{R}$ belongs to an isotropic H\"{o}lder class of order $\alpha \geq 4$, with the optimal choice of smoothing parameters, the posterior contraction rates for the filament points on some appropriately defined integral curves and for the Hausdorff distance of the filament are both $(n/\log n)^{(2-\alpha)/(2(1+\alpha))}$. Secondly, we provide a way to construct a credible set with sufficient frequentist coverage for the filaments. We demonstrate the success of our proposed method in simulations and one application to earthquake data.

</details>

<details>

<summary>2020-03-25 13:16:18 - Modeling and simulating depositional sequences using latent Gaussian random fields</summary>

- *Denis Allard, Paolo Fabbri, Carlo Gaetan*

- `2003.11383v1` - [abs](http://arxiv.org/abs/2003.11383v1) - [pdf](http://arxiv.org/pdf/2003.11383v1)

> Simulating a depositional (or stratigraphic) sequence conditionally on borehole data is a long-standing problem in hydrogeology and in petroleum geostatistics. This paper presents a new rule-based approach for simulating depositional sequences of surfaces conditionally on lithofacies thickness data. The thickness of each layer is modeled by a transformed latent Gaussian random field allowing for null thickness thanks to a truncation process. Layers are sequentially stacked above each other following the regional stratigraphic sequence. By choosing adequately the variograms of these random fields, the simulated surfaces separating two layers can be continuous and smooth. Borehole information is often incomplete in the sense that it does not provide direct information as to the exact layer some observed thickness belongs to. The latent Gaussian model proposed in this paper offers a natural solution to this problem by means of a Bayesian setting with a Markov Chain Monte Carlo (MCMC) algorithm that can explore all possible configurations compatible with the data. The model and the associated MCMC algorithm are validated on synthetic data and then applied to a subsoil in the Venetian Plain with a moderately dense network of cored boreholes.

</details>

<details>

<summary>2020-03-25 16:40:03 - Uncertainty Estimation in Cancer Survival Prediction</summary>

- *Hrushikesh Loya, Pranav Poduval, Deepak Anand, Neeraj Kumar, Amit Sethi*

- `2003.08573v2` - [abs](http://arxiv.org/abs/2003.08573v2) - [pdf](http://arxiv.org/pdf/2003.08573v2)

> Survival models are used in various fields, such as the development of cancer treatment protocols. Although many statistical and machine learning models have been proposed to achieve accurate survival predictions, little attention has been paid to obtain well-calibrated uncertainty estimates associated with each prediction. The currently popular models are opaque and untrustworthy in that they often express high confidence even on those test cases that are not similar to the training samples, and even when their predictions are wrong. We propose a Bayesian framework for survival models that not only gives more accurate survival predictions but also quantifies the survival uncertainty better. Our approach is a novel combination of variational inference for uncertainty estimation, neural multi-task logistic regression for estimating nonlinear and time-varying risk models, and an additional sparsity-inducing prior to work with high dimensional data.

</details>

<details>

<summary>2020-03-25 18:03:51 - Interval Neural Networks: Uncertainty Scores</summary>

- *Luis Oala, Cosmas Heiß, Jan Macdonald, Maximilian März, Wojciech Samek, Gitta Kutyniok*

- `2003.11566v1` - [abs](http://arxiv.org/abs/2003.11566v1) - [pdf](http://arxiv.org/pdf/2003.11566v1)

> We propose a fast, non-Bayesian method for producing uncertainty scores in the output of pre-trained deep neural networks (DNNs) using a data-driven interval propagating network. This interval neural network (INN) has interval valued parameters and propagates its input using interval arithmetic. The INN produces sensible lower and upper bounds encompassing the ground truth. We provide theoretical justification for the validity of these bounds. Furthermore, its asymmetric uncertainty scores offer additional, directional information beyond what Gaussian-based, symmetric variance estimation can provide. We find that noise in the data is adequately captured by the intervals produced with our method. In numerical experiments on an image reconstruction task, we demonstrate the practical utility of INNs as a proxy for the prediction error in comparison to two state-of-the-art uncertainty quantification methods. In summary, INNs produce fast, theoretically justified uncertainty scores for DNNs that are easy to interpret, come with added information and pose as improved error proxies - features that may prove useful in advancing the usability of DNNs especially in sensitive applications such as health care.

</details>

<details>

<summary>2020-03-25 19:13:38 - A generalized double robust Bayesian model averaging approach to causal effect estimation with application to the Study of Osteoporotic Fractures</summary>

- *Denis Talbot, Claudia Beaudoin*

- `2003.11588v1` - [abs](http://arxiv.org/abs/2003.11588v1) - [pdf](http://arxiv.org/pdf/2003.11588v1)

> Analysts often use data-driven approaches to supplement their substantive knowledge when selecting covariates for causal effect estimation. Multiple variable selection procedures tailored for causal effect estimation have been devised in recent years, but additional developments are still required to adequately address the needs of data analysts. In this paper, we propose a Generalized Bayesian Causal Effect Estimation (GBCEE) algorithm to perform variable selection and produce double robust estimates of causal effects for binary or continuous exposures and outcomes. GBCEE employs a prior distribution that targets the selection of true confounders and predictors of the outcome for the unbiased estimation of causal effects with reduced standard errors. Double robust estimators provide some robustness against model misspecification, whereas the Bayesian machinery allows GBCEE to directly produce inferences for its estimate. GBCEE was compared to multiple alternatives in various simulation scenarios and was observed to perform similarly or to outperform double robust alternatives. Its ability to directly produce inferences is also an important advantage from a computational perspective. The method is finally illustrated for the estimation of the effect of meeting physical activity recommendations on the risk of hip or upper-leg fractures among elderly women in the Study of Osteoporotic Fractures. The 95% confidence interval produced by GBCEE is 61% shorter than that of a double robust estimator adjusting for all potential confounders in this illustration.

</details>

<details>

<summary>2020-03-25 20:07:44 - Bayesian Hierarchical Bernoulli-Weibull Mixture Model for Extremely Rare Events</summary>

- *Yuki Ohnishi, Shinsuke Sugaya*

- `2003.11606v1` - [abs](http://arxiv.org/abs/2003.11606v1) - [pdf](http://arxiv.org/pdf/2003.11606v1)

> Estimating the duration of user behavior is a central concern for most internet companies. Survival analysis is a promising method for analyzing the expected duration of events and usually assumes the same survival function for all subjects and the event will occur in the long run. However, such assumptions are inappropriate when the users behave differently or some events never occur for some users, i.e., the conversion period on web services of the light users with no intention of behaving actively on the service. Especially, if the proportion of inactive users is high, this assumption can lead to undesirable results. To address these challenges, this paper proposes a mixture model that separately addresses active and inactive individuals with a latent variable. First, we define this specific problem setting and show the limitations of conventional survival analysis in addressing this problem. We demonstrate how naturally our Bernoulli-Weibull model can accommodate the challenge. The proposed model was extended further to a Bayesian hierarchical model to incorporate each subject's parameter, offering substantial improvements over conventional, non-hierarchical models in terms of WAIC and WBIC. Second, an experiment and extensive analysis were conducted using real-world data from the Japanese job search website, CareerTrek, offered by BizReach, Inc. In the analysis, some research questions are raised, such as the difference in activation rate and conversion rate between user categories, and how instantaneously the rate of event occurrence changes as time passes. Quantitative answers and interpretations are assigned to them. Furthermore, the model is inferred in a Bayesian manner, which enables us to represent the uncertainty with a credible interval of the parameters and predictive quantities.

</details>

<details>

<summary>2020-03-25 22:38:20 - Analyzing the Role of Model Uncertainty for Electronic Health Records</summary>

- *Michael W. Dusenberry, Dustin Tran, Edward Choi, Jonas Kemp, Jeremy Nixon, Ghassen Jerfel, Katherine Heller, Andrew M. Dai*

- `1906.03842v3` - [abs](http://arxiv.org/abs/1906.03842v3) - [pdf](http://arxiv.org/pdf/1906.03842v3)

> In medicine, both ethical and monetary costs of incorrect predictions can be significant, and the complexity of the problems often necessitates increasingly complex models. Recent work has shown that changing just the random seed is enough for otherwise well-tuned deep neural networks to vary in their individual predicted probabilities. In light of this, we investigate the role of model uncertainty methods in the medical domain. Using RNN ensembles and various Bayesian RNNs, we show that population-level metrics, such as AUC-PR, AUC-ROC, log-likelihood, and calibration error, do not capture model uncertainty. Meanwhile, the presence of significant variability in patient-specific predictions and optimal decisions motivates the need for capturing model uncertainty. Understanding the uncertainty for individual patients is an area with clear clinical impact, such as determining when a model decision is likely to be brittle. We further show that RNNs with only Bayesian embeddings can be a more efficient way to capture model uncertainty compared to ensembles, and we analyze how model uncertainty is impacted across individual input features and patient subgroups.

</details>

<details>

<summary>2020-03-26 14:00:33 - TAP free energy, spin glasses, and variational inference</summary>

- *Zhou Fan, Song Mei, Andrea Montanari*

- `1808.07890v2` - [abs](http://arxiv.org/abs/1808.07890v2) - [pdf](http://arxiv.org/pdf/1808.07890v2)

> We consider the Sherrington-Kirkpatrick model of spin glasses with ferromagnetically biased couplings. For a specific choice of the couplings mean, the resulting Gibbs measure is equivalent to the Bayesian posterior for a high-dimensional estimation problem known as `$Z_2$ synchronization'. Statistical physics suggests to compute the expectation with respect to this Gibbs measure (the posterior mean in the synchronization problem), by minimizing the so-called Thouless-Anderson-Palmer (TAP) free energy, instead of the mean field (MF) free energy. We prove that this identification is correct, provided the ferromagnetic bias is larger than a constant (i.e. the noise level is small enough in synchronization). Namely, we prove that the scaled $\ell_2$ distance between any low energy local minimizers of the TAP free energy and the mean of the Gibbs measure vanishes in the large size limit. Our proof technique is based on upper bounding the expected number of critical points of the TAP free energy using the Kac-Rice formula.

</details>

<details>

<summary>2020-03-26 14:30:24 - Advances in Bayesian Probabilistic Modeling for Industrial Applications</summary>

- *Sayan Ghosh, Piyush Pandita, Steven Atkinson, Waad Subber, Yiming Zhang, Natarajan Chennimalai Kumar, Suryarghya Chakrabarti, Liping Wang*

- `2003.11939v1` - [abs](http://arxiv.org/abs/2003.11939v1) - [pdf](http://arxiv.org/pdf/2003.11939v1)

> Industrial applications frequently pose a notorious challenge for state-of-the-art methods in the contexts of optimization, designing experiments and modeling unknown physical response. This problem is aggravated by limited availability of clean data, uncertainty in available physics-based models and additional logistic and computational expense associated with experiments. In such a scenario, Bayesian methods have played an impactful role in alleviating the aforementioned obstacles by quantifying uncertainty of different types under limited resources. These methods, usually deployed as a framework, allows decision makers to make informed choices under uncertainty while being able to incorporate information on the the fly, usually in the form of data, from multiple sources while being consistent with the physical intuition about the problem. This is a major advantage that Bayesian methods bring to fruition especially in the industrial context. This paper is a compendium of the Bayesian modeling methodology that is being consistently developed at GE Research. The methodology, called GE's Bayesian Hybrid Modeling (GEBHM), is a probabilistic modeling method, based on the Kennedy and O'Hagan framework, that has been continuously scaled-up and industrialized over several years. In this work, we explain the various advancements in GEBHM's methods and demonstrate their impact on several challenging industrial problems.

</details>

<details>

<summary>2020-03-27 04:37:48 - Efficient Batch Black-box Optimization with Deterministic Regret Bounds</summary>

- *Yueming Lyu, Yuan Yuan, Ivor W. Tsang*

- `1905.10041v3` - [abs](http://arxiv.org/abs/1905.10041v3) - [pdf](http://arxiv.org/pdf/1905.10041v3)

> In this work, we investigate black-box optimization from the perspective of frequentist kernel methods. We propose a novel batch optimization algorithm, which jointly maximizes the acquisition function and select points from a whole batch in a holistic way. Theoretically, we derive regret bounds for both the noise-free and perturbation settings irrespective of the choice of kernel. Moreover, we analyze the property of the adversarial regret that is required by a robust initialization for Bayesian Optimization (BO). We prove that the adversarial regret bounds decrease with the decrease of covering radius, which provides a criterion for generating a point set to minimize the bound. We then propose fast searching algorithms to generate a point set with a small covering radius for the robust initialization. Experimental results on both synthetic benchmark problems and real-world problems show the effectiveness of the proposed algorithms.

</details>

<details>

<summary>2020-03-27 06:18:49 - Incorporating Expert Prior in Bayesian Optimisation via Space Warping</summary>

- *Anil Ramachandran, Sunil Gupta, Santu Rana, Cheng Li, Svetha Venkatesh*

- `2003.12250v1` - [abs](http://arxiv.org/abs/2003.12250v1) - [pdf](http://arxiv.org/pdf/2003.12250v1)

> Bayesian optimisation is a well-known sample-efficient method for the optimisation of expensive black-box functions. However when dealing with big search spaces the algorithm goes through several low function value regions before reaching the optimum of the function. Since the function evaluations are expensive in terms of both money and time, it may be desirable to alleviate this problem. One approach to subside this cold start phase is to use prior knowledge that can accelerate the optimisation. In its standard form, Bayesian optimisation assumes the likelihood of any point in the search space being the optimum is equal. Therefore any prior knowledge that can provide information about the optimum of the function would elevate the optimisation performance. In this paper, we represent the prior knowledge about the function optimum through a prior distribution. The prior distribution is then used to warp the search space in such a way that space gets expanded around the high probability region of function optimum and shrinks around low probability region of optimum. We incorporate this prior directly in function model (Gaussian process), by redefining the kernel matrix, which allows this method to work with any acquisition function, i.e. acquisition agnostic approach. We show the superiority of our method over standard Bayesian optimisation method through optimisation of several benchmark functions and hyperparameter tuning of two algorithms: Support Vector Machine (SVM) and Random forest.

</details>

<details>

<summary>2020-03-27 13:47:16 - Learning representations in Bayesian Confidence Propagation neural networks</summary>

- *Naresh Balaji Ravichandran, Anders Lansner, Pawel Herman*

- `2003.12415v1` - [abs](http://arxiv.org/abs/2003.12415v1) - [pdf](http://arxiv.org/pdf/2003.12415v1)

> Unsupervised learning of hierarchical representations has been one of the most vibrant research directions in deep learning during recent years. In this work we study biologically inspired unsupervised strategies in neural networks based on local Hebbian learning. We propose new mechanisms to extend the Bayesian Confidence Propagating Neural Network (BCPNN) architecture, and demonstrate their capability for unsupervised learning of salient hidden representations when tested on the MNIST dataset.

</details>

<details>

<summary>2020-03-27 16:27:51 - Gaussian graphical models reveal inter-modal and inter-regional conditional dependencies of brain alterations in Alzheimer's disease</summary>

- *Martin Dyrba, Reza Mohammadi, Michel J. Grothe, Thomas Kirste, Stefan J. Teipel*

- `1804.00049v3` - [abs](http://arxiv.org/abs/1804.00049v3) - [pdf](http://arxiv.org/pdf/1804.00049v3)

> Alzheimer's disease (AD) is characterized by a sequence of pathological changes, which are commonly assessed in vivo using MRI and PET. Currently, the most approaches to analyze statistical associations between brain regions rely on Pearson correlation. However, these are prone to spurious correlations arising from uninformative shared variance. Notably, there are no appropriate multivariate statistical models available that can easily integrate dozens of variables derived from such data, being able to use the additional information provided from the combination of data sources. Gaussian graphical models (GGMs) can estimate the conditional dependency from given data, which is expected to reflect the underlying causal relationships. We applied GGMs to assess multimodal regional brain alterations in AD. We obtained data from N=972 subjects from the Alzheimer's Disease Neuroimaging Initiative. The mean amyloid load (AV45-PET), glucose metabolism (FDG-PET), and gray matter volume (MRI) were calculated. GGMs were estimated using a Bayesian framework for the combined multimodal data to obtain conditional dependency networks. Conditional dependency matrices were much sparser (10% density) than Pearson correlation matrices (50% density). Within modalities, conditional dependency networks yielded clusters connecting anatomically adjacent regions. For associations between different modalities, only few region-specific connections remained. Graph-theoretical network statistics were significantly altered between groups, with a biphasic u-shape trajectory. GGMs removed shared variance among multimodal measures of regional brain alterations in MCI and AD, and yielded sparser matrices compared to Pearson correlation networks. Therefore, GGMs may be used as alternative to thresholding-approaches typically applied to correlation networks to obtain the most informative relations between variables.

</details>

<details>

<summary>2020-03-27 16:48:13 - Estimating Uncertainty and Interpretability in Deep Learning for Coronavirus (COVID-19) Detection</summary>

- *Biraja Ghoshal, Allan Tucker*

- `2003.10769v2` - [abs](http://arxiv.org/abs/2003.10769v2) - [pdf](http://arxiv.org/pdf/2003.10769v2)

> Deep Learning has achieved state of the art performance in medical imaging. However, these methods for disease detection focus exclusively on improving the accuracy of classification or predictions without quantifying uncertainty in a decision. Knowing how much confidence there is in a computer-based medical diagnosis is essential for gaining clinicians trust in the technology and therefore improve treatment. Today, the 2019 Coronavirus (SARS-CoV-2) infections are a major healthcare challenge around the world. Detecting COVID-19 in X-ray images is crucial for diagnosis, assessment and treatment. However, diagnostic uncertainty in the report is a challenging and yet inevitable task for radiologist. In this paper, we investigate how drop-weights based Bayesian Convolutional Neural Networks (BCNN) can estimate uncertainty in Deep Learning solution to improve the diagnostic performance of the human-machine team using publicly available COVID-19 chest X-ray dataset and show that the uncertainty in prediction is highly correlates with accuracy of prediction. We believe that the availability of uncertainty-aware deep learning solution will enable a wider adoption of Artificial Intelligence (AI) in a clinical setting.

</details>

<details>

<summary>2020-03-27 18:52:54 - GAN-based Priors for Quantifying Uncertainty</summary>

- *Dhruv V. Patel, Assad A. Oberai*

- `2003.12597v1` - [abs](http://arxiv.org/abs/2003.12597v1) - [pdf](http://arxiv.org/pdf/2003.12597v1)

> Bayesian inference is used extensively to quantify the uncertainty in an inferred field given the measurement of a related field when the two are linked by a mathematical model. Despite its many applications, Bayesian inference faces challenges when inferring fields that have discrete representations of large dimension, and/or have prior distributions that are difficult to characterize mathematically. In this work we demonstrate how the approximate distribution learned by a deep generative adversarial network (GAN) may be used as a prior in a Bayesian update to address both these challenges. We demonstrate the efficacy of this approach on two distinct, and remarkably broad, classes of problems. The first class leads to supervised learning algorithms for image classification with superior out of distribution detection and accuracy, and for image inpainting with built-in variance estimation. The second class leads to unsupervised learning algorithms for image denoising and for solving physics-driven inverse problems.

</details>

<details>

<summary>2020-03-27 19:52:25 - Bayesian inference of chaotic dynamics by merging data assimilation, machine learning and expectation-maximization</summary>

- *Marc Bocquet, Julien Brajard, Alberto Carrassi, Laurent Bertino*

- `2001.06270v2` - [abs](http://arxiv.org/abs/2001.06270v2) - [pdf](http://arxiv.org/pdf/2001.06270v2)

> The reconstruction from observations of high-dimensional chaotic dynamics such as geophysical flows is hampered by (i) the partial and noisy observations that can realistically be obtained, (ii) the need to learn from long time series of data, and (iii) the unstable nature of the dynamics. To achieve such inference from the observations over long time series, it has been suggested to combine data assimilation and machine learning in several ways. We show how to unify these approaches from a Bayesian perspective using expectation-maximization and coordinate descents. In doing so, the model, the state trajectory and model error statistics are estimated all together. Implementations and approximations of these methods are discussed. Finally, we numerically and successfully test the approach on two relevant low-order chaotic models with distinct identifiability.

</details>

<details>

<summary>2020-03-29 01:24:56 - Special Function Methods for Bursty Models of Transcription</summary>

- *Gennady Gorin, Lior Pachter*

- `2003.12919v1` - [abs](http://arxiv.org/abs/2003.12919v1) - [pdf](http://arxiv.org/pdf/2003.12919v1)

> We explore a Markov model used in the analysis of gene expression, involving the bursty production of pre-mRNA, its conversion to mature mRNA, and its consequent degradation. We demonstrate that the integration used to compute the solution of the stochastic system can be approximated by the evaluation of special functions. Furthermore, the form of the special function solution generalizes to a broader class of burst distributions. In light of the broader goal of biophysical parameter inference from transcriptomics data, we apply the method to simulated data, demonstrating effective control of precision and runtime. Finally, we suggest a non-Bayesian approach to reducing the computational complexity of parameter inference to linear order in state space size and number of candidate parameters.

</details>

<details>

<summary>2020-03-29 10:10:37 - Fast and scalable non-parametric Bayesian inference for Poisson point processes</summary>

- *Shota Gugushvili, Frank van der Meulen, Moritz Schauer, Peter Spreij*

- `1804.03616v3` - [abs](http://arxiv.org/abs/1804.03616v3) - [pdf](http://arxiv.org/pdf/1804.03616v3)

> We study the problem of non-parametric Bayesian estimation of the intensity function of a Poisson point process. The observations are $n$ independent realisations of a Poisson point process on the interval $[0,T]$. We propose two related approaches. In both approaches we model the intensity function as piecewise constant on $N$ bins forming a partition of the interval $[0,T]$. In the first approach the coefficients of the intensity function are assigned independent gamma priors, leading to a closed form posterior distribution. On the theoretical side, we prove that as $n\rightarrow\infty,$ the posterior asymptotically concentrates around the "true", data-generating intensity function at an optimal rate for $h$-H\"older regular intensity functions ($0 < h\leq 1$). In the second approach we employ a gamma Markov chain prior on the coefficients of the intensity function. The posterior distribution is no longer available in closed form, but inference can be performed using a straightforward version of the Gibbs sampler. Both approaches scale well with sample size, but the second is much less sensitive to the choice of $N$. Practical performance of our methods is first demonstrated via synthetic data examples. We compare our second method with other existing approaches on the UK coal mining disasters data. Furthermore, we apply it to the US mass shootings data and Donald Trump's Twitter data.

</details>

<details>

<summary>2020-03-29 15:25:31 - $ε$-shotgun: $ε$-greedy Batch Bayesian Optimisation</summary>

- *George De Ath, Richard M. Everson, Jonathan E. Fieldsend, Alma A. M. Rahat*

- `2002.01873v2` - [abs](http://arxiv.org/abs/2002.01873v2) - [pdf](http://arxiv.org/pdf/2002.01873v2)

> Bayesian optimisation is a popular, surrogate model-based approach for optimising expensive black-box functions. Given a surrogate model, the next location to expensively evaluate is chosen via maximisation of a cheap-to-query acquisition function. We present an $\epsilon$-greedy procedure for Bayesian optimisation in batch settings in which the black-box function can be evaluated multiple times in parallel. Our $\epsilon$-shotgun algorithm leverages the model's prediction, uncertainty, and the approximated rate of change of the landscape to determine the spread of batch solutions to be distributed around a putative location. The initial target location is selected either in an exploitative fashion on the mean prediction, or -- with probability $\epsilon$ -- from elsewhere in the design space. This results in locations that are more densely sampled in regions where the function is changing rapidly and in locations predicted to be good (i.e close to predicted optima), with more scattered samples in regions where the function is flatter and/or of poorer quality. We empirically evaluate the $\epsilon$-shotgun methods on a range of synthetic functions and two real-world problems, finding that they perform at least as well as state-of-the-art batch methods and in many cases exceed their performance.

</details>

<details>

<summary>2020-03-30 02:15:54 - Optimal experimental design under irreducible uncertainty for linear inverse problems governed by PDEs</summary>

- *Karina Koval, Alen Alexanderian, Georg Stadler*

- `1912.08915v2` - [abs](http://arxiv.org/abs/1912.08915v2) - [pdf](http://arxiv.org/pdf/1912.08915v2)

> We present a method for computing A-optimal sensor placements for infinite-dimensional Bayesian linear inverse problems governed by PDEs with irreducible model uncertainties. Here, irreducible uncertainties refers to uncertainties in the model that exist in addition to the parameters in the inverse problem, and that cannot be reduced through observations. Specifically, given a statistical distribution for the model uncertainties, we compute the optimal design that minimizes the expected value of the posterior covariance trace. The expected value is discretized using Monte Carlo leading to an objective function consisting of a sum of trace operators and a binary-inducing penalty. Minimization of this objective requires a large number of PDE solves in each step. To make this problem computationally tractable, we construct a composite low-rank basis using a randomized range finder algorithm to eliminate forward and adjoint PDE solves. We also present a novel formulation of the A-optimal design objective that requires the trace of an operator in the observation rather than the parameter space. The binary structure is enforced using a weighted regularized $\ell_0$-sparsification approach. We present numerical results for inference of the initial condition in a subsurface flow problem with inherent uncertainty in the flow fields and in the initial times.

</details>

<details>

<summary>2020-03-30 10:22:57 - Adaptation of Engineering Wake Models using Gaussian Process Regression and High-Fidelity Simulation Data</summary>

- *Leif Erik Andersson, Bart Doekemeijer, Daan van der Hoek, Jan-Willem van Wingerden, Lars Imsland*

- `2003.13323v1` - [abs](http://arxiv.org/abs/2003.13323v1) - [pdf](http://arxiv.org/pdf/2003.13323v1)

> This article investigates the optimization of yaw control inputs of a nine-turbine wind farm. The wind farm is simulated using the high-fidelity simulator SOWFA. The optimization is performed with a modifier adaptation scheme based on Gaussian processes. Modifier adaptation corrects for the mismatch between plant and model and helps to converge to the actual plan optimum. In the case study the modifier adaptation approach is compared with the Bayesian optimization approach. Moreover, the use of two different covariance functions in the Gaussian process regression is discussed. Practical recommendations concerning the data preparation and application of the approach are given. It is shown that both the modifier adaptation and the Bayesian optimization approach can improve the power production with overall smaller yaw misalignments in comparison to the Gaussian wake model.

</details>

<details>

<summary>2020-03-30 11:37:44 - Asymptotic Analysis of Model Selection Criteria for General Hidden Markov Models</summary>

- *Shouto Yonekura, Alexandros Beskos, Sumeetpal S. Singh*

- `1811.11834v3` - [abs](http://arxiv.org/abs/1811.11834v3) - [pdf](http://arxiv.org/pdf/1811.11834v3)

> The paper obtains analytical results for the asymptotic properties of Model Selection Criteria -- widely used in practice -- for a general family of hidden Markov models (HMMs), thereby substantially extending the related theory beyond typical i.i.d.-like model structures and filling in an important gap in the relevant literature. In particular, we look at the Bayesian and Akaike Information Criteria (BIC and AIC) and the model evidence. In the setting of nested classes of models, we prove that BIC and the evidence are strongly consistent for HMMs (under regularity conditions), whereas AIC is not weakly consistent. Numerical experiments support our theoretical results.

</details>

<details>

<summary>2020-03-30 20:05:53 - On one-sample Bayesian tests for the mean</summary>

- *Ibrahim Abdelrazeq, Luai Al-Labadi*

- `1903.00851v4` - [abs](http://arxiv.org/abs/1903.00851v4) - [pdf](http://arxiv.org/pdf/1903.00851v4)

> This paper deals with a new Bayesian approach to the standard one-sample $z$- and $t$- tests. More specifically, let $x_1,\ldots,x_n$ be an independent random sample from a normal distribution with mean $\mu$ and variance $\sigma^2$. The goal is to test the null hypothesis $\mathcal{H}_0: \mu=\mu_1$ against all possible alternatives. The approach is based on using the well-known formula of the Kullbak-Leibler divergence between two normal distributions (sampling and hypothesized distributions selected in an appropriate way). The change of the distance from a priori to a posteriori is compared through the relative belief ratio (a measure of evidence). Eliciting the prior, checking for prior-data conflict and bias are also considered. Many theoretical properties of the procedure have been developed. Besides it's simplicity, and unlike the classical approach, the new approach possesses attractive and distinctive features such as giving evidence in favor of the null hypothesis. It also avoids several undesirable paradoxes, such as Lindley's paradox that may be encountered by some existing Bayesian methods. The use of the approach has been illustrated through several examples.

</details>

<details>

<summary>2020-03-30 20:20:43 - Exponential Family Estimation via Adversarial Dynamics Embedding</summary>

- *Bo Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le Song, Dale Schuurmans*

- `1904.12083v3` - [abs](http://arxiv.org/abs/1904.12083v3) - [pdf](http://arxiv.org/pdf/1904.12083v3)

> We present an efficient algorithm for maximum likelihood estimation (MLE) of exponential family models, with a general parametrization of the energy function that includes neural networks. We exploit the primal-dual view of the MLE with a kinetics augmented model to obtain an estimate associated with an adversarial dual sampler. To represent this sampler, we introduce a novel neural architecture, dynamics embedding, that generalizes Hamiltonian Monte-Carlo (HMC). The proposed approach inherits the flexibility of HMC while enabling tractable entropy estimation for the augmented model. By learning both a dual sampler and the primal model simultaneously, and sharing parameters between them, we obviate the requirement to design a separate sampling procedure once the model has been trained, leading to more effective learning. We show that many existing estimators, such as contrastive divergence, pseudo/composite-likelihood, score matching, minimum Stein discrepancy estimator, non-local contrastive objectives, noise-contrastive estimation, and minimum probability flow, are special cases of the proposed approach, each expressed by a different (fixed) dual sampler. An empirical investigation shows that adapting the sampler during MLE can significantly improve on state-of-the-art estimators.

</details>

<details>

<summary>2020-03-31 07:12:42 - An Approximate Bayesian Approach to Model-assisted Survey Estimation with Many Auxiliary Variables</summary>

- *Shonosuke Sugasawa, Jae Kwang Kim*

- `1906.04398v2` - [abs](http://arxiv.org/abs/1906.04398v2) - [pdf](http://arxiv.org/pdf/1906.04398v2)

> Model-assisted estimation with complex survey data is an important practical problem in survey sampling. When there are many auxiliary variables, selecting significant variables associated with the study variable would be necessary to achieve efficient estimation of population parameters of interest. In this paper, we formulate a regularized regression estimator in the framework of Bayesian inference using the penalty function as the shrinkage prior for model selection. The proposed Bayesian approach enables us to get not only efficient point estimates but also reasonable credible intervals. Results from two limited simulation studies are presented to facilitate comparison with existing frequentist methods.

</details>

<details>

<summary>2020-03-31 12:07:01 - Quantified limits of the nuclear landscape</summary>

- *Léo Neufcourt, Yuchen Cao, Samuel A. Giuliani, Witold Nazarewicz, Erik Olsen, Oleg B. Tarasov*

- `2001.05924v2` - [abs](http://arxiv.org/abs/2001.05924v2) - [pdf](http://arxiv.org/pdf/2001.05924v2)

> The chart of the nuclides is limited by particle drip lines beyond which nuclear stability to proton or neutron emission is lost. Predicting the range of particle-bound isotopes poses an appreciable challenge for nuclear theory as it involves extreme extrapolations of nuclear masses beyond the regions where experimental information is available. Still, quantified extrapolations are crucial for a variety of applications, including the modeling of stellar nucleosynthesis. We use microscopic nuclear mass models and Bayesian methodology to provide quantified predictions of proton and neutron separation energies as well as Bayesian probabilities of existence throughout the nuclear landscape all the way to the particle drip lines. We apply nuclear density functional theory with several energy density functionals. To account for uncertainties, Bayesian Gaussian processes are trained on the separation-energy residuals for each individual model, and the resulting predictions are combined via Bayesian model averaging. This framework allows to account for systematic and statistical uncertainties and propagate them to extrapolative predictions. We characterize the drip-line regions where the probability that the nucleus is particle-bound decreases from $1$ to $0$. In these regions, we provide quantified predictions for one- and two-nucleon separation energies. According to our Bayesian model averaging analysis, 7759 nuclei with $Z\leq 119$ have a probability of existence $\geq 0.5$. The extrapolations obtained in this study will be put through stringent tests when new experimental information on exotic nuclei becomes available. In this respect, the quantified landscape of nuclear existence obtained in this study should be viewed as a dynamical prediction that will be fine-tuned when new experimental information and improved global mass models become available.

</details>

<details>

<summary>2020-03-31 13:44:33 - Efficient Particle Smoothing for Bayesian Inference in Dynamic Survival Models</summary>

- *Parfait Munezero*

- `1806.07048v3` - [abs](http://arxiv.org/abs/1806.07048v3) - [pdf](http://arxiv.org/pdf/1806.07048v3)

> This article proposes an efficient Bayesian inference for piecewise exponential hazard (PEH) models, which allow the effect of a covariate on the survival time to vary over time. The proposed inference methodology is based on a particle smoothing (PS) algorithm that depends on three particle filters. Efficient proposal (importance) distributions for the particle filters tailored to the nature of survival data and PEH models are developed using the Laplace approximation of the posterior distribution and linear Bayes theory. The algorithm is applied to both simulated and real data, and the results show that it generates an effective sample size that is more than two orders of magnitude larger than a state-of-the-art MCMC sampler for the same computing time, and scales well in high-dimensional and relatively large data.

</details>

<details>

<summary>2020-03-31 21:14:59 - Exact marginal inference in Latent Dirichlet Allocation</summary>

- *Hartmut Maennel*

- `2004.00115v1` - [abs](http://arxiv.org/abs/2004.00115v1) - [pdf](http://arxiv.org/pdf/2004.00115v1)

> Assume we have potential "causes" $z\in Z$, which produce "events" $w$ with known probabilities $\beta(w|z)$. We observe $w_1,w_2,...,w_n$, what can we say about the distribution of the causes? A Bayesian estimate will assume a prior on distributions on $Z$ (we assume a Dirichlet prior) and calculate a posterior. An average over that posterior then gives a distribution on $Z$, which estimates how much each cause $z$ contributed to our observations. This is the setting of Latent Dirichlet Allocation, which can be applied e.g. to topics "producing" words in a document. In this setting usually the number of observed words is large, but the number of potential topics is small. We are here interested in applications with many potential "causes" (e.g. locations on the globe), but only a few observations. We show that the exact Bayesian estimate can be computed in linear time (and constant space) in $|Z|$ for a given upper bound on $n$ with a surprisingly simple formula. We generalize this algorithm to the case of sparse probabilities $\beta(w|z)$, in which we only need to assume that the tree width of an "interaction graph" on the observations is limited. On the other hand we also show that without such limitation the problem is NP-hard.

</details>


## 2020-04

<details>

<summary>2020-04-01 04:53:21 - Data-driven prediction and origin identification of epidemics in population networks</summary>

- *Karen Larson, Clark Bowman, Zhizhong Chen, Panagiotis Hadjidoukas, Costas Papadimitriou, Petros Koumoutsakos, Anastasios Matzavinos*

- `1710.07880v2` - [abs](http://arxiv.org/abs/1710.07880v2) - [pdf](http://arxiv.org/pdf/1710.07880v2)

> Effective intervention strategies for epidemics rely on the identification of their origin and on the robustness of the predictions made by network disease models. We introduce a Bayesian uncertainty quantification framework to infer model parameters for a disease spreading on a network of communities from limited, noisy observations; the state-of-the-art computational framework compensates for the model complexity by exploiting massively parallel computing architectures. Using noisy, synthetic data, we show the potential of the approach to perform robust model fitting and additionally demonstrate that we can effectively identify the disease origin via Bayesian model selection. As disease-related data are increasingly available, the proposed framework has broad practical relevance for the prediction and management of epidemics.

</details>

<details>

<summary>2020-04-01 12:40:38 - Bayesian space-time gap filling for inference on extreme hot-spots: an application to Red Sea surface temperatures</summary>

- *Daniela Castro-Camilo, Linda Mhalla, Thomas Opitz*

- `2004.00386v1` - [abs](http://arxiv.org/abs/2004.00386v1) - [pdf](http://arxiv.org/pdf/2004.00386v1)

> We develop a method for probabilistic prediction of extreme value hot-spots in a spatio-temporal framework, tailored to big datasets containing important gaps. In this setting, direct calculation of summaries from data, such as the minimum over a space-time domain, is not possible. To obtain predictive distributions for such cluster summaries, we propose a two-step approach. We first model marginal distributions with a focus on accurate modeling of the right tail and then, after transforming the data to a standard Gaussian scale, we estimate a Gaussian space-time dependence model defined locally in the time domain for the space-time subregions where we want to predict. In the first step, we detrend the mean and standard deviation of the data and fit a spatially resolved generalized Pareto distribution to apply a correction of the upper tail. To ensure spatial smoothness of the estimated trends, we either pool data using nearest-neighbor techniques, or apply generalized additive regression modeling. To cope with high space-time resolution of data, the local Gaussian models use a Markov representation of the Mat\'ern correlation function based on the stochastic partial differential equations (SPDE) approach. In the second step, they are fitted in a Bayesian framework through the integrated nested Laplace approximation implemented in R-INLA. Finally, posterior samples are generated to provide statistical inferences through Monte-Carlo estimation. Motivated by the 2019 Extreme Value Analysis data challenge, we illustrate our approach to predict the distribution of local space-time minima in anomalies of Red Sea surface temperatures, using a gridded dataset (11315 days, 16703 pixels) with artificially generated gaps. In particular, we show the improved performance of our two-step approach over a purely Gaussian model without tail transformations.

</details>

<details>

<summary>2020-04-01 14:38:40 - Generate Country-Scale Networks of Interaction from Scattered Statistics</summary>

- *Samuel Thiriot, Jean-Daniel Kant*

- `2004.01031v1` - [abs](http://arxiv.org/abs/2004.01031v1) - [pdf](http://arxiv.org/pdf/2004.01031v1)

> It is common to define the structure of interactions among a population of agents by a network. Most of agent-based models were shown highly sensitive to that network, so the relevance of simulation results directely depends on the descriptive power of that network. When studying social dynamics in large populations, that network cannot be collected, and is rather generated by algorithms which aim to fit general properties of social networks. However, more precise data is available at a country scale in the form of socio-demographic studies, census or sociological studies. These "scattered statistics" provide rich information, especially on agents' attributes, similar properties of tied agents and affiliations. In this paper, we propose a generic methodology to bring up together these scattered statistics with bayesian networks. We explain how to generate a population of heterogeneous agents, and how to create links by using both scattered statistics and knowledge on social selection processes. The methodology is illustrated by generating an interaction network for rural Kenya which includes familial structure, colleagues and friendship constrained given field studies and statistics.

</details>

<details>

<summary>2020-04-01 15:45:09 - Randomization and reweighted $\ell_1$-minimization for A-optimal design of linear inverse problems</summary>

- *Elizabeth Herman, Alen Alexanderian, Arvind K. Saibaba*

- `1906.03791v2` - [abs](http://arxiv.org/abs/1906.03791v2) - [pdf](http://arxiv.org/pdf/1906.03791v2)

> We consider optimal design of PDE-based Bayesian linear inverse problems with infinite-dimensional parameters. We focus on the A-optimal design criterion, defined as the average posterior variance and quantified by the trace of the posterior covariance operator. We propose using structure exploiting randomized methods to compute the A-optimal objective function and its gradient, and provide a detailed analysis of the error for the proposed estimators. To ensure sparse and binary design vectors, we develop a novel reweighted $\ell_1$-minimization algorithm. We also introduce a modified A-optimal criterion and present randomized estimators for its efficient computation. We present numerical results illustrating the proposed methods on a model contaminant source identification problem, where the inverse problem seeks to recover the initial state of a contaminant plume, using discrete measurements of the contaminant in space and time.

</details>

<details>

<summary>2020-04-01 16:06:18 - From scenario-based seismic hazard to scenario-based landslide hazard: fast-forwarding to the future via statistical simulations</summary>

- *Luigi Lombardo, Hakan Tanyas*

- `2004.00537v1` - [abs](http://arxiv.org/abs/2004.00537v1) - [pdf](http://arxiv.org/pdf/2004.00537v1)

> Ground motion scenarios exists for most of the seismically active areas around the globe. They essentially correspond to shaking level maps at given earthquake return times which are used as reference for the likely areas under threat from future ground displacements. Being landslides in seismically actively regions closely controlled by the ground motion, one would expect that landslide susceptibility maps should change as the ground motion patterns change in space and time. However, so far, statistically-based landslide susceptibility assessments have primarily been used as time-invariant.In other words, the vast majority of the statistical models does not include the temporal effect of the main trigger in future landslide scenarios. In this work, we present an approach aimed at filling this gap, bridging current practices in the seismological community to those in the geomorphological and statistical ones. More specifically, we select an earthquake-induced landslide inventory corresponding to the 1994 Northridge earthquake and build a Bayesian Generalized Additive Model of the binomial family, featuring common morphometric and thematic covariates as well as the Peak Ground Acceleration generated by the Northridge earthquake. Once each model component has been estimated, we have run 1000 simulations for each of the 217 possible ground motion scenarios for the study area.   From each batch of 1000 simulations, we have estimated the mean and 95\% Credible Interval to represent the mean susceptibility pattern under a specific earthquake scenario, together with its uncertainty level. Because each earthquake scenario has a specific return time, our simulations allow to incorporate the temporal dimension into any susceptibility model, therefore driving the results toward the definition of landslide hazard.

</details>

<details>

<summary>2020-04-01 16:08:44 - From scenario-based seismic hazard to scenario-based landslide hazard: rewinding to the past via statistical simulations</summary>

- *Luguang Luo, Luigi Lombardo, Cees van Westen, Xiangjun Pei, Runqiu Huang*

- `2004.00539v1` - [abs](http://arxiv.org/abs/2004.00539v1) - [pdf](http://arxiv.org/pdf/2004.00539v1)

> The vast majority of landslide susceptibility studies assumes the slope instability process to be time-invariant under the definition that "the past and present are keys to the future". This assumption may generally be valid. However, the trigger, be it a rainfall or an earthquake event, clearly varies over time. And yet, the temporal component of the trigger is rarely included in landslide susceptibility studies and only confined to hazard assessment. In this work, we investigate a population of landslides triggered in response to the 2017 Jiuzhaigou earthquake ($M_w = 6.5$) including the associated ground motion in the analyses, these being carried out at the Slope Unit (SU) level. We do this by implementing a Bayesian version of a Generalized Additive Model and assuming that the slope instability across the SUs in the study area behaves according to a Bernoulli probability distribution. This procedure would generally produce a susceptibility map reflecting the spatial pattern of the specific trigger and therefore of limited use for land use planning. However, we implement this first analytical step to reliably estimate the ground motion effect, and its distribution, on unstable SUs. We then assume the effect of the ground motion to be time-invariant, enabling statistical simulations for any ground motion scenario that occurred in the area from 1933 to 2017. As a result, we obtain the full spectrum of potential susceptibility patterns over the last century and compress this information into a susceptibility model/map representative of all the possible ground motion patterns since 1933. This backward statistical simulations can also be further exploited in the opposite direction where, by accounting for scenario-based ground motion, one can also use it in a forward direction to estimate future unstable slopes.

</details>

<details>

<summary>2020-04-01 17:04:02 - From Fourier to Koopman: Spectral Methods for Long-term Time Series Prediction</summary>

- *Henning Lange, Steven L. Brunton, Nathan Kutz*

- `2004.00574v1` - [abs](http://arxiv.org/abs/2004.00574v1) - [pdf](http://arxiv.org/pdf/2004.00574v1)

> We propose spectral methods for long-term forecasting of temporal signals stemming from linear and nonlinear quasi-periodic dynamical systems. For linear signals, we introduce an algorithm with similarities to the Fourier transform but which does not rely on periodicity assumptions, allowing for forecasting given potentially arbitrary sampling intervals. We then extend this algorithm to handle nonlinearities by leveraging Koopman theory. The resulting algorithm performs a spectral decomposition in a nonlinear, data-dependent basis. The optimization objective for both algorithms is highly non-convex. However, expressing the objective in the frequency domain allows us to compute global optima of the error surface in a scalable and efficient manner, partially by exploiting the computational properties of the Fast Fourier Transform. Because of their close relation to Bayesian Spectral Analysis, uncertainty quantification metrics are a natural byproduct of the spectral forecasting methods. We extensively benchmark these algorithms against other leading forecasting methods on a range of synthetic experiments as well as in the context of real-world power systems and fluid flows.

</details>

<details>

<summary>2020-04-01 20:04:22 - Bayesian Inverse Problems with Heterogeneous Variance</summary>

- *Natalia Bochkina, Jenovah Rodrigues*

- `1910.06914v3` - [abs](http://arxiv.org/abs/1910.06914v3) - [pdf](http://arxiv.org/pdf/1910.06914v3)

> We consider inverse problems in Hilbert spaces contaminated by Gaussian noise, and use a Bayesian approach to find its regularised smooth solution. We consider the so called conjugate diagonal setting where the covariance operators of the noise and of the prior are diagnolisable in the orthogonal bases associated with the forward operator of the inverse problem. Firstly, we derive the minimax rate of convergence in such problems with known covariance operator of the noise, showing that in the case of heterogeneous variance the ill posed inverse problem can become self regularised in some cases when the eigenvalues of the variance operator decay to zero, achieving parametric rate of convergence; as far as we are aware, this is a striking novel result that have not been observed before in nonparametric problems. Secondly, we give a general expression of the rate of contraction of the posterior distribution in case of known noise covariance operator in case the noise level is small, for a given prior distribution. We also investigate when this contraction rate coincides with the optimal rate in the minimax sense which is typically used as a benchmark for studying the posterior contraction rates. We apply our results to known variance operators with polynomially decreasing or increasing eigenvalues as an example. We also discuss when the plug in estimator of the eigenvalues of the covariance operator of the noise does not affect the rate of the contraction of the posterior distribution of the signal. We show that plugging in the maximum marginal likelihood estimator of the prior scaling parameter leads to the optimal posterior contraction rate, adaptively. Effect of the choice of the prior parameters on the contraction in such models is illustrated on simulated data with Volterra operator.

</details>

<details>

<summary>2020-04-02 01:15:54 - Bayesian model selection approach for colored graphical Gaussian models</summary>

- *Qiong Li, Xin Gao, Helene Massam*

- `2004.00764v1` - [abs](http://arxiv.org/abs/2004.00764v1) - [pdf](http://arxiv.org/pdf/2004.00764v1)

> We consider a class of colored graphical Gaussian models obtained by placing symmetry constraints on the precision matrix in a Bayesian framework. The prior distribution on the precision matrix is the colored $G$-Wishart prior which is the Diaconis-Ylvisaker conjugate prior. In this paper, we develop a computationally efficient model search algorithm which combines linear regression with a double reversible jump Markov chain Monte Carlo (MCMC) method. The latter is to estimate the Bayes factors expressed as the ratio of posterior probabilities of two competing models. We also establish the asymptotic consistency property of the model selection procedure based on the Bayes factors. Our procedure avoids an exhaustive search which is computationally impossible. Our method is illustrated with simulations and a real-world application with a protein signalling data set.

</details>

<details>

<summary>2020-04-02 03:32:51 - Duality between Approximate Bayesian Methods and Prior Robustness</summary>

- *Chaitanya Joshi, Fabrizio Ruggeri*

- `2004.00796v1` - [abs](http://arxiv.org/abs/2004.00796v1) - [pdf](http://arxiv.org/pdf/2004.00796v1)

> In this paper we show that there is a link between approximate Bayesian methods and prior robustness. We show that what is typically recognized as an approximation to the likelihood, either due to the simulated data as in the Approximate Bayesian Computation (ABC) methods or due to the functional approximation to the likelihood, can instead also be viewed upon as an implicit exercise in prior robustness. We first define two new classes of priors for the cases where the sufficient statistics is available, establish their mathematical properties and show, for a simple illustrative example, that these classes of priors can also be used to obtain the posterior distribution that would be obtained by implementing ABC. We then generalize and define two further classes of priors that are applicable in very general scenarios; one where the sufficient statistics is not available and another where the likelihood is approximated using a functional approximation. We then discuss the interpretation and elicitation aspects of the classes proposed here as well as their potential applications and possible computational benefits. These classes establish the duality between approximate Bayesian inference and prior robustness for a wide category of Bayesian inference methods.

</details>

<details>

<summary>2020-04-02 17:52:27 - Bayesian Counterfactual Risk Minimization</summary>

- *Ben London, Ted Sandler*

- `1806.11500v6` - [abs](http://arxiv.org/abs/1806.11500v6) - [pdf](http://arxiv.org/pdf/1806.11500v6)

> We present a Bayesian view of counterfactual risk minimization (CRM) for offline learning from logged bandit feedback. Using PAC-Bayesian analysis, we derive a new generalization bound for the truncated inverse propensity score estimator. We apply the bound to a class of Bayesian policies, which motivates a novel, potentially data-dependent, regularization technique for CRM. Experimental results indicate that this technique outperforms standard $L_2$ regularization, and that it is competitive with variance regularization while being both simpler to implement and more computationally efficient.

</details>

<details>

<summary>2020-04-02 20:55:15 - Human-like Time Series Summaries via Trend Utility Estimation</summary>

- *Pegah Jandaghi, Jay Pujara*

- `2001.05665v2` - [abs](http://arxiv.org/abs/2001.05665v2) - [pdf](http://arxiv.org/pdf/2001.05665v2)

> In many scenarios, humans prefer a text-based representation of quantitative data over numerical, tabular, or graphical representations. The attractiveness of textual summaries for complex data has inspired research on data-to-text systems. While there are several data-to-text tools for time series, few of them try to mimic how humans summarize for time series. In this paper, we propose a model to create human-like text descriptions for time series. Our system finds patterns in time series data and ranks these patterns based on empirical observations of human behavior using utility estimation. Our proposed utility estimation model is a Bayesian network capturing interdependencies between different patterns. We describe the learning steps for this network and introduce baselines along with their performance for each step. The output of our system is a natural language description of time series that attempts to match a human's summary of the same data.

</details>

<details>

<summary>2020-04-03 10:26:04 - Composite mixture of log-linear models for categorical data</summary>

- *Emanuele Aliverti, David B. Dunson*

- `2004.01462v1` - [abs](http://arxiv.org/abs/2004.01462v1) - [pdf](http://arxiv.org/pdf/2004.01462v1)

> Multivariate categorical data are routinely collected in many application areas. As the number of cells in the table grows exponentially with the number of variables, many or even most cells will contain zero observations. This severe sparsity motivates appropriate statistical methodologies that effectively reduce the number of free parameters, with penalized log-linear models and latent structure analysis being popular options. This article proposes a fundamentally new class of methods, which we refer to as Mixture of Log Linear models (mills). Combining latent class analysis and log-linear models, mills defines a novel Bayesian methodology to model complex multivariate categorical with flexibility and interpretability. Mills is shown to have key advantages over alternative methods for contingency tables in simulations and an application investigating the relation among suicide attempts and empathy.

</details>

<details>

<summary>2020-04-03 14:05:09 - Hawkes Process Multi-armed Bandits for Disaster Search and Rescue</summary>

- *Wen-Hao Chiang, George Mohler*

- `2004.01580v1` - [abs](http://arxiv.org/abs/2004.01580v1) - [pdf](http://arxiv.org/pdf/2004.01580v1)

> We propose a novel framework for integrating Hawkes processes with multi-armed bandit algorithms to solve spatio-temporal event forecasting and detection problems when data may be undersampled or spatially biased. In particular, we introduce an upper confidence bound algorithm using Bayesian spatial Hawkes process estimation for balancing the tradeoff between exploiting geographic regions where data has been collected and exploring geographic regions where data is unobserved. We first validate our model using simulated data and then apply it to the problem of disaster search and rescue using calls for service data from hurricane Harvey in 2017. Our model outperforms the state of the art baseline spatial MAB algorithms in terms of cumulative reward and several other ranking evaluation metrics.

</details>

<details>

<summary>2020-04-03 16:05:52 - Deep Compositional Spatial Models</summary>

- *Andrew Zammit-Mangion, Tin Lok James Ng, Quan Vu, Maurizio Filippone*

- `1906.02840v2` - [abs](http://arxiv.org/abs/1906.02840v2) - [pdf](http://arxiv.org/pdf/1906.02840v2)

> Spatial processes with nonstationary and anisotropic covariance structure are often used when modelling, analysing and predicting complex environmental phenomena. Such processes may often be expressed as ones that have stationary and isotropic covariance structure on a warped spatial domain. However, the warping function is generally difficult to fit and not constrained to be injective, often resulting in `space-folding.' Here, we propose modelling an injective warping function through a composition of multiple elemental injective functions in a deep-learning framework. We consider two cases; first, when these functions are known up to some weights that need to be estimated, and, second, when the weights in each layer are random. Inspired by recent methodological and technological advances in deep learning and deep Gaussian processes, we employ approximate Bayesian methods to make inference with these models using graphics processing units. Through simulation studies in one and two dimensions we show that the deep compositional spatial models are quick to fit, and are able to provide better predictions and uncertainty quantification than other deep stochastic models of similar complexity. We also show their remarkable capacity to model nonstationary, anisotropic spatial data using radiances from the MODIS instrument aboard the Aqua satellite.

</details>

<details>

<summary>2020-04-04 00:39:12 - The equivalence between Stein variational gradient descent and black-box variational inference</summary>

- *Casey Chu, Kentaro Minami, Kenji Fukumizu*

- `2004.01822v1` - [abs](http://arxiv.org/abs/2004.01822v1) - [pdf](http://arxiv.org/pdf/2004.01822v1)

> We formalize an equivalence between two popular methods for Bayesian inference: Stein variational gradient descent (SVGD) and black-box variational inference (BBVI). In particular, we show that BBVI corresponds precisely to SVGD when the kernel is the neural tangent kernel. Furthermore, we interpret SVGD and BBVI as kernel gradient flows; we do this by leveraging the recent perspective that views SVGD as a gradient flow in the space of probability distributions and showing that BBVI naturally motivates a Riemannian structure on that space. We observe that kernel gradient flow also describes dynamics found in the training of generative adversarial networks (GANs). This work thereby unifies several existing techniques in variational inference and generative modeling and identifies the kernel as a fundamental object governing the behavior of these algorithms, motivating deeper analysis of its properties.

</details>

<details>

<summary>2020-04-04 06:42:07 - A Bayesian approach for initialization of weights in backpropagation neural net with application to character recognition</summary>

- *Nadir Murru, Rosaria Rossini*

- `2004.01875v1` - [abs](http://arxiv.org/abs/2004.01875v1) - [pdf](http://arxiv.org/pdf/2004.01875v1)

> Convergence rate of training algorithms for neural networks is heavily affected by initialization of weights. In this paper, an original algorithm for initialization of weights in backpropagation neural net is presented with application to character recognition. The initialization method is mainly based on a customization of the Kalman filter, translating it into Bayesian statistics terms. A metrological approach is used in this context considering weights as measurements modeled by mutually dependent normal random variables. The algorithm performance is demonstrated by reporting and discussing results of simulation trials. Results are compared with random weights initialization and other methods. The proposed method shows an improved convergence rate for the backpropagation training algorithm.

</details>

<details>

<summary>2020-04-04 11:15:20 - Incentive compatibility in sender-receiver stopping games</summary>

- *Aditya Aradhye, János Flesch, Mathias Staudigl, Dries Vermeulen*

- `2004.01910v1` - [abs](http://arxiv.org/abs/2004.01910v1) - [pdf](http://arxiv.org/pdf/2004.01910v1)

> We introduce a model of sender-receiver stopping games, where the state of the world follows an iid--process throughout the game. At each period, the sender observes the current state, and sends a message to the receiver, suggesting either to stop or to continue. The receiver, only seeing the message but not the state, decides either to stop the game, or to continue which takes the game to the next period. The payoff to each player is a function of the state when the receiver quits, with higher states leading to better payoffs. The horizon of the game can be finite or infinite.   We prove existence and uniqueness of responsive (i.e. non-babbling) Perfect Bayesian Equilibrium (PBE) under mild conditions on the game primitives in the case where the players are sufficiently patient. The responsive PBE has a remarkably simple structure, which builds on the identification of an easy-to-implement and compute class of threshold strategies for the sender. With the help of these threshold strategies, we derive simple expressions describing this PBE. It turns out that in this PBE the receiver obediently follows the recommendations of the sender. Hence, surprisingly, the sender alone plays the decisive role, and regardless of the payoff function of the receiver the sender always obtains the best possible payoff for himself.

</details>

<details>

<summary>2020-04-04 17:42:33 - Rationally Inattentive Inverse Reinforcement Learning Explains YouTube Commenting Behavior</summary>

- *William Hoiles, Vikram Krishnamurthy, Kunal Pattanayak*

- `1910.11703v2` - [abs](http://arxiv.org/abs/1910.11703v2) - [pdf](http://arxiv.org/pdf/1910.11703v2)

> We consider a novel application of inverse reinforcement learning with behavioral economics constraints to model, learn and predict the commenting behavior of YouTube viewers. Each group of users is modeled as a rationally inattentive Bayesian agent which solves a contextual bandit problem. Our methodology integrates three key components. First, to identify distinct commenting patterns, we use deep embedded clustering to estimate framing information (essential extrinsic features) that clusters users into distinct groups.Second, we present an inverse reinforcement learning algorithm that uses Bayesian revealed preferences to test for rationality: does there exist a utility function that rationalizes the given data, and if yes, can it be used to predict commenting behavior? Finally, we impose behavioral economics constraints stemming from rational inattention to characterize the attention span of groups of users. The test imposes a R{\'e}nyi mutual information cost constraint which impacts how the agent can select attention strategies to maximize their expected utility. After a careful analysis of a massive YouTube dataset, our surprising result is that in most YouTube user groups, the commenting behavior is consistent with optimizing a Bayesian utility with rationally inattentive constraints. The paper also highlights how the rational inattention model can accurately predict commenting behavior. The massive YouTube dataset and analysis used in this paper are available on GitHub and completely reproducible.

</details>

<details>

<summary>2020-04-04 20:06:51 - Random Partition Models for Microclustering Tasks</summary>

- *Brenda Betancourt, Giacomo Zanella, Rebecca C. Steorts*

- `2004.02008v1` - [abs](http://arxiv.org/abs/2004.02008v1) - [pdf](http://arxiv.org/pdf/2004.02008v1)

> Traditional Bayesian random partition models assume that the size of each cluster grows linearly with the number of data points. While this is appealing for some applications, this assumption is not appropriate for other tasks such as entity resolution, modeling of sparse networks, and DNA sequencing tasks. Such applications require models that yield clusters whose sizes grow sublinearly with the total number of data points -- the microclustering property. Motivated by these issues, we propose a general class of random partition models that satisfy the microclustering property with well-characterized theoretical properties. Our proposed models overcome major limitations in the existing literature on microclustering models, namely a lack of interpretability, identifiability, and full characterization of model asymptotic properties. Crucially, we drop the classical assumption of having an exchangeable sequence of data points, and instead assume an exchangeable sequence of clusters. In addition, our framework provides flexibility in terms of the prior distribution of cluster sizes, computational tractability, and applicability to a large number of microclustering tasks. We establish theoretical properties of the resulting class of priors, where we characterize the asymptotic behavior of the number of clusters and of the proportion of clusters of a given size. Our framework allows a simple and efficient Markov chain Monte Carlo algorithm to perform statistical inference. We illustrate our proposed methodology on the microclustering task of entity resolution, where we provide a simulation study and real experiments on survey panel data.

</details>

<details>

<summary>2020-04-05 01:29:57 - ABCMETAapp: R Shiny Application for Simulation-based Estimation of Mean and Standard Deviation for Meta-analysis via Approximate Bayesian Computation (ABC)</summary>

- *Roopesh Reddy Sadashiva Reddy, Isildinha M. Reis, Deukwoo Kwon*

- `2004.02065v1` - [abs](http://arxiv.org/abs/2004.02065v1) - [pdf](http://arxiv.org/pdf/2004.02065v1)

> Background and Objective: In meta-analysis based on continuous outcome, estimated means and corresponding standard deviations from the selected studies are key inputs to obtain a pooled estimate of the mean and its confidence interval. We often encounter the situation that these quantities are not directly reported in the literatures. Instead, other summary statistics are reported such as median, minimum, maximum, quartiles, and study sample size. Based on available summary statistics, we need to estimate estimates of mean and standard deviation for meta-analysis. Methods: We developed a R Shiny code based on approximate Bayesian computation (ABC), ABCMETA, to deal with this situation. Results: In this article, we present an interactive and user-friendly R Shiny application for implementing the proposed method (named ABCMETAapp). In ABCMETAapp, users can choose an underlying outcome distribution other than the normal distribution when the distribution of the outcome variable is skewed or heavy tailed. We show how to run ABCMETAapp with examples. Conclusions: ABCMETAapp provides a R Shiny implementation. This method is more flexible than the existing analytical methods since estimation can be based on five different distribution (Normal, Lognormal, Exponential, Weibull, and Beta) for the outcome variable.

</details>

<details>

<summary>2020-04-05 15:11:15 - Probabilistic Projection of the Sex Ratio at Birth and Missing Female Births by State and Union Territory in India</summary>

- *Fengqing Chao, Christophe Z. Guilmoto, Samir K. C., Hernando Ombao*

- `2004.02228v1` - [abs](http://arxiv.org/abs/2004.02228v1) - [pdf](http://arxiv.org/pdf/2004.02228v1)

> The sex ratio at birth (SRB) in India has been reported imbalanced since the 1970s. Previous studies have shown a great variation in the SRB across geographic locations in India till 2016. As one of the most populous countries and in view of its great regional heterogeneity, it is crucial to produce probabilistic projections for the SRB in India at state level for the purpose of population projection and policy planning. In this paper, we implement a Bayesian hierarchical time series model to project SRB in India by state. We generate SRB probabilistic projections from 2017 to 2030 for 29 States and Union Territories (UTs) in India, and present results in 21 States/UTs with data from the Sample Registration System. Our analysis takes into account two state-specific factors that contribute to sex-selective abortion and resulting sex imbalances at birth: intensity of son preference and fertility squeeze. We project that the largest contribution to female births deficits is in Uttar Pradesh, with cumulative number of missing female births projected to be 2.0 (95% credible interval [1.9; 2.2]) million from 2017 to 2030. The total female birth deficits during 2017-2030 for the whole India is projected to be 6.8 [6.6; 7.0] million.

</details>

<details>

<summary>2020-04-05 21:50:23 - Graphical outputs and Spatial Cross-validation for the R-INLA package using INLAutils</summary>

- *Tim Lucas, Andre Python, David Redding*

- `2004.02324v1` - [abs](http://arxiv.org/abs/2004.02324v1) - [pdf](http://arxiv.org/pdf/2004.02324v1)

> Statistical analyses proceed by an iterative process of model fitting and checking. The R-INLA package facilitates this iteration by fitting many Bayesian models much faster than alternative MCMC approaches. As the interpretation of results and model objects from Bayesian analyses can be complex, the R package INLAutils provides users with easily accessible, clear and customisable graphical summaries of model outputs from R- INLA. Furthermore, it offers a function for performing and visualizing the results of a spatial leave-one-out cross-validation (SLOOCV) approach that can be applied to compare the predictive performance of multiple spatial models. In this paper, we describe and illustrate the use of (1) graphical summary plotting functions and (2) the SLOOCV approach. We conclude the paper by identifying the limits of our approach and discuss future potential improvements.

</details>

<details>

<summary>2020-04-06 14:22:14 - Approximate Inference for Fully Bayesian Gaussian Process Regression</summary>

- *Vidhi Lalchand, Carl Edward Rasmussen*

- `1912.13440v2` - [abs](http://arxiv.org/abs/1912.13440v2) - [pdf](http://arxiv.org/pdf/1912.13440v2)

> Learning in Gaussian Process models occurs through the adaptation of hyperparameters of the mean and the covariance function. The classical approach entails maximizing the marginal likelihood yielding fixed point estimates (an approach called \textit{Type II maximum likelihood} or ML-II). An alternative learning procedure is to infer the posterior over hyperparameters in a hierarchical specification of GPs we call \textit{Fully Bayesian Gaussian Process Regression} (GPR). This work considers two approximation schemes for the intractable hyperparameter posterior: 1) Hamiltonian Monte Carlo (HMC) yielding a sampling-based approximation and 2) Variational Inference (VI) where the posterior over hyperparameters is approximated by a factorized Gaussian (mean-field) or a full-rank Gaussian accounting for correlations between hyperparameters. We analyze the predictive performance for fully Bayesian GPR on a range of benchmark data sets.

</details>

<details>

<summary>2020-04-07 04:07:29 - Repulsive Mixture Models of Exponential Family PCA for Clustering</summary>

- *Maoying Qiao, Tongliang Liu, Jun Yu, Wei Bian, Dacheng Tao*

- `2004.03112v1` - [abs](http://arxiv.org/abs/2004.03112v1) - [pdf](http://arxiv.org/pdf/2004.03112v1)

> The mixture extension of exponential family principal component analysis (EPCA) was designed to encode much more structural information about data distribution than the traditional EPCA does. For example, due to the linearity of EPCA's essential form, nonlinear cluster structures cannot be easily handled, but they are explicitly modeled by the mixing extensions. However, the traditional mixture of local EPCAs has the problem of model redundancy, i.e., overlaps among mixing components, which may cause ambiguity for data clustering. To alleviate this problem, in this paper, a repulsiveness-encouraging prior is introduced among mixing components and a diversified EPCA mixture (DEPCAM) model is developed in the Bayesian framework. Specifically, a determinantal point process (DPP) is exploited as a diversity-encouraging prior distribution over the joint local EPCAs. As required, a matrix-valued measure for L-ensemble kernel is designed, within which, $\ell_1$ constraints are imposed to facilitate selecting effective PCs of local EPCAs, and angular based similarity measure are proposed. An efficient variational EM algorithm is derived to perform parameter learning and hidden variable inference. Experimental results on both synthetic and real-world datasets confirm the effectiveness of the proposed method in terms of model parsimony and generalization ability on unseen test data.

</details>

<details>

<summary>2020-04-07 04:52:37 - Modeling and presentation of vaccination coverage estimates using data from household surveys</summary>

- *Tracy Qi Dong, Jon Wakefield*

- `2004.03127v1` - [abs](http://arxiv.org/abs/2004.03127v1) - [pdf](http://arxiv.org/pdf/2004.03127v1)

> It is becoming increasingly popular to produce high-resolution maps of vaccination coverage by fitting Bayesian geostatistical models to data from household surveys. Often, the surveys adopt a stratified cluster sampling design. We discuss a number of crucial choices with respect to two key aspects of the map production process: the acknowledgment of the survey design in modeling, and the appropriate presentation of estimates and their uncertainties. Specifically, we consider the importance of accounting for survey stratification and cluster-level non-spatial excess variation in survey outcomes when fitting geostatistical models. We also discuss the trade-off between the geographical scale and precision of model-based estimates, and demonstrate visualization methods for mapping and ranking that emphasize the probabilistic interpretation of results. A novel approach to coverage map presentation is proposed to allow comparison and control of the overall map uncertainty level. We use measles vaccination coverage in Nigeria as a motivating example and illustrate the different issues using data from the 2018 Nigeria Demographic and Health Survey.

</details>

<details>

<summary>2020-04-07 12:49:00 - Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision</summary>

- *Fredrik K. Gustafsson, Martin Danelljan, Thomas B. Schön*

- `1906.01620v3` - [abs](http://arxiv.org/abs/1906.01620v3) - [pdf](http://arxiv.org/pdf/1906.01620v3)

> While deep neural networks have become the go-to approach in computer vision, the vast majority of these models fail to properly capture the uncertainty inherent in their predictions. Estimating this predictive uncertainty can be crucial, for example in automotive applications. In Bayesian deep learning, predictive uncertainty is commonly decomposed into the distinct types of aleatoric and epistemic uncertainty. The former can be estimated by letting a neural network output the parameters of a certain probability distribution. Epistemic uncertainty estimation is a more challenging problem, and while different scalable methods recently have emerged, no extensive comparison has been performed in a real-world setting. We therefore accept this task and propose a comprehensive evaluation framework for scalable epistemic uncertainty estimation methods in deep learning. Our proposed framework is specifically designed to test the robustness required in real-world computer vision applications. We also apply this framework to provide the first properly extensive and conclusive comparison of the two current state-of-the-art scalable methods: ensembling and MC-dropout. Our comparison demonstrates that ensembling consistently provides more reliable and practically useful uncertainty estimates. Code is available at https://github.com/fregu856/evaluating_bdl.

</details>

<details>

<summary>2020-04-07 14:01:57 - The Machine Learning Bazaar: Harnessing the ML Ecosystem for Effective System Development</summary>

- *Micah J. Smith, Carles Sala, James Max Kanter, Kalyan Veeramachaneni*

- `1905.08942v4` - [abs](http://arxiv.org/abs/1905.08942v4) - [pdf](http://arxiv.org/pdf/1905.08942v4)

> As machine learning is applied more widely, data scientists often struggle to find or create end-to-end machine learning systems for specific tasks. The proliferation of libraries and frameworks and the complexity of the tasks have led to the emergence of "pipeline jungles" - brittle, ad hoc ML systems. To address these problems, we introduce the Machine Learning Bazaar, a new framework for developing machine learning and automated machine learning software systems. First, we introduce ML primitives, a unified API and specification for data processing and ML components from different software libraries. Next, we compose primitives into usable ML pipelines, abstracting away glue code, data flow, and data storage. We further pair these pipelines with a hierarchy of AutoML strategies - Bayesian optimization and bandit learning. We use these components to create a general-purpose, multi-task, end-to-end AutoML system that provides solutions to a variety of data modalities (image, text, graph, tabular, relational, etc.) and problem types (classification, regression, anomaly detection, graph matching, etc.). We demonstrate 5 real-world use cases and 2 case studies of our approach. Finally, we present an evaluation suite of 456 real-world ML tasks and describe the characteristics of 2.5 million pipelines searched over this task suite.

</details>

<details>

<summary>2020-04-07 16:37:30 - Modelling death rates due to COVID-19: A Bayesian approach</summary>

- *Cristian Bayes, Victor Sal y Rosas, Luis Valdivieso*

- `2004.02386v2` - [abs](http://arxiv.org/abs/2004.02386v2) - [pdf](http://arxiv.org/pdf/2004.02386v2)

> Objective: To estimate the number of deaths in Peru due to COVID-19. Design: With a priori information obtained from the daily number of deaths due to CODIV-19 in China and data from the Peruvian authorities, we constructed a predictive Bayesian non-linear model for the number of deaths in Peru. Exposure: COVID-19. Outcome: Number of deaths. Results: Assuming an intervention level similar to the one implemented in China, the total number of deaths in Peru is expected to be 612 (95%CI: 604.3 - 833.7) persons. Sixty four days after the first reported death, the 99% of expected deaths will be observed. The inflexion point in the number of deaths is estimated to be around day 26 (95%CI: 25.1 - 26.8) after the first reported death. Conclusion: These estimates can help authorities to monitor the epidemic and implement strategies in order to manage the COVID-19 pandemic.

</details>

<details>

<summary>2020-04-07 18:07:20 - Global consensus Monte Carlo</summary>

- *Lewis J. Rendell, Adam M. Johansen, Anthony Lee, Nick Whiteley*

- `1807.09288v3` - [abs](http://arxiv.org/abs/1807.09288v3) - [pdf](http://arxiv.org/pdf/1807.09288v3)

> To conduct Bayesian inference with large data sets, it is often convenient or necessary to distribute the data across multiple machines. We consider a likelihood function expressed as a product of terms, each associated with a subset of the data. Inspired by global variable consensus optimisation, we introduce an instrumental hierarchical model associating auxiliary statistical parameters with each term, which are conditionally independent given the top-level parameters. One of these top-level parameters controls the unconditional strength of association between the auxiliary parameters. This model leads to a distributed MCMC algorithm on an extended state space yielding approximations of posterior expectations. A trade-off between computational tractability and fidelity to the original model can be controlled by changing the association strength in the instrumental model. We further propose the use of a SMC sampler with a sequence of association strengths, allowing both the automatic determination of appropriate strengths and for a bias correction technique to be applied. In contrast to similar distributed Monte Carlo algorithms, this approach requires few distributional assumptions. The performance of the algorithms is illustrated with a number of simulated examples.

</details>

<details>

<summary>2020-04-08 02:16:40 - Bayesian Computation with Intractable Likelihoods</summary>

- *Matthew T. Moores, Anthony N. Pettitt, Kerrie Mengersen*

- `2004.04620v1` - [abs](http://arxiv.org/abs/2004.04620v1) - [pdf](http://arxiv.org/pdf/2004.04620v1)

> This article surveys computational methods for posterior inference with intractable likelihoods, that is where the likelihood function is unavailable in closed form, or where evaluation of the likelihood is infeasible. We review recent developments in pseudo-marginal methods, approximate Bayesian computation (ABC), the exchange algorithm, thermodynamic integration, and composite likelihood, paying particular attention to advancements in scalability for large datasets. We also mention R and MATLAB source code for implementations of these algorithms, where they are available.

</details>

<details>

<summary>2020-04-08 18:45:06 - Bayesian Interpolants as Explanations for Neural Inferences</summary>

- *Kenneth L. McMillan*

- `2004.04198v1` - [abs](http://arxiv.org/abs/2004.04198v1) - [pdf](http://arxiv.org/pdf/2004.04198v1)

> The notion of Craig interpolant, used as a form of explanation in automated reasoning, is adapted from logical inference to statistical inference and used to explain inferences made by neural networks. The method produces explanations that are at the same time concise, understandable and precise.

</details>

<details>

<summary>2020-04-08 23:25:27 - Heterogeneous Regression Models for Clusters of Spatial Dependent Data</summary>

- *Zhihua Ma, Yishu Xue, Guanyu Hu*

- `1907.02212v4` - [abs](http://arxiv.org/abs/1907.02212v4) - [pdf](http://arxiv.org/pdf/1907.02212v4)

> In economic development, there are often regions that share similar economic characteristics, and economic models on such regions tend to have similar covariate effects. In this paper, we propose a Bayesian clustered regression for spatially dependent data in order to detect clusters in the covariate effects. Our proposed method is based on the Dirichlet process which provides a probabilistic framework for simultaneous inference of the number of clusters and the clustering configurations. The usage of our method is illustrated both in simulation studies and an application to a housing cost dataset of Georgia.

</details>

<details>

<summary>2020-04-08 23:51:53 - Estimating Scale Discrepancy in Bayesian Model Calibration for ChemCam on the Mars Curiosity Rover</summary>

- *K. Sham Bhat, Kary Myers, Earl Lawrence, James Colgan, Elizabeth Judge*

- `2004.04301v1` - [abs](http://arxiv.org/abs/2004.04301v1) - [pdf](http://arxiv.org/pdf/2004.04301v1)

> The Mars rover Curiosity carries an instrument called ChemCam to determine the composition of the soil and rocks. ChemCam uses laser-induced breakdown spectroscopy (LIBS) for this purpose. Los Alamos National Laboratory has developed a simulation capability that can predict spectra from ChemCam, but there are major scale differences between the prediction and observation. This presents a challenge when using Bayesian model calibration to determine the unknown physical parameters that describe the LIBS observations. We present an analysis of LIBS data to support ChemCam based on including a structured discrepancy model in a Bayesian model calibration scheme. This is both a novel application of Bayesian model calibration and a general purpose approach to accounting for such systematic differences between theory and observation in this setting.

</details>

<details>

<summary>2020-04-09 02:57:01 - Objective Bayesian analysis for spatial Student-t regression models</summary>

- *Jose A. Ordoñez, Marcos O. Prates, Larissa A. Matos, Victor H. Lachos*

- `2004.04341v1` - [abs](http://arxiv.org/abs/2004.04341v1) - [pdf](http://arxiv.org/pdf/2004.04341v1)

> The choice of the prior distribution is a key aspect of Bayesian analysis. For the spatial regression setting a subjective prior choice for the parameters may not be trivial, from this perspective, using the objective Bayesian analysis framework a reference is introduced for the spatial Student-t regression model with unknown degrees of freedom. The spatial Student-t regression model poses two main challenges when eliciting priors: one for the spatial dependence parameter and the other one for the degrees of freedom. It is well-known that the propriety of the posterior distribution over objective priors is not always guaranteed, whereas the use of proper prior distributions may dominate and bias the posterior analysis. In this paper, we show the conditions under which our proposed reference prior yield to a proper posterior distribution. Simulation studies are used in order to evaluate the performance of the reference prior to a commonly used vague proper prior.

</details>

<details>

<summary>2020-04-09 16:27:29 - Nonparametric Bayesian inference of discretely observed diffusions</summary>

- *Jean-Charles Croix, Masoumeh Dashti, Istvàn Zoltàn Kiss*

- `2004.04636v1` - [abs](http://arxiv.org/abs/2004.04636v1) - [pdf](http://arxiv.org/pdf/2004.04636v1)

> We consider the problem of the Bayesian inference of drift and diffusion coefficient functions in a stochastic differential equation given discrete observations of a realisation of its solution. We give conditions for the well-posedness and stable approximations of the posterior measure. These conditions in particular allow for priors with unbounded support. Our proof relies on the explicit construction of transition probability densities using the parametrix method for general parabolic equations. We then study an application of these results in inferring the rates of Birth-and-Death processes.

</details>

<details>

<summary>2020-04-10 01:44:14 - Stochastic Model Pruning via Weight Dropping Away and Back</summary>

- *Haipeng Jia, Xueshuang Xiang, Da Fan, Meiyu Huang, Changhao Sun, Yang He*

- `1812.02035v2` - [abs](http://arxiv.org/abs/1812.02035v2) - [pdf](http://arxiv.org/pdf/1812.02035v2)

> Deep neural networks have dramatically achieved great success on a variety of challenging tasks. However, most successful DNNs have an extremely complex structure, leading to extensive research on model compression.As a significant area of progress in model compression, traditional gradual pruning approaches involve an iterative prune-retrain procedure and may suffer from two critical issues: local importance judgment, where the pruned weights are merely unimportant in the current model; and an irretrievable pruning process, where the pruned weights have no chance to come back. Addressing these two issues, this paper proposes the Drop Pruning approach, which leverages stochastic optimization in the pruning process by introducing a drop strategy at each pruning step, namely, drop away, which stochastically deletes some unimportant weights, and drop back, which stochastically recovers some pruned weights. The suitable choice of drop probabilities decreases the model size during the pruning process and helps it flow to the target sparsity. Compared to the Bayesian approaches that stochastically train a compact model for pruning, we directly aim at stochastic gradual pruning. We provide a detailed analysis showing that the drop away and drop back approaches have individual contributions. Moreover, Drop Pruning can achieve competitive compression performance and accuracy on many benchmark tasks compared with state-of-the-art weights pruning and Bayesian training approaches.

</details>

<details>

<summary>2020-04-10 10:43:48 - Forecasts with Bayesian vector autoregressions under real time conditions</summary>

- *Michael Pfarrhofer*

- `2004.04984v1` - [abs](http://arxiv.org/abs/2004.04984v1) - [pdf](http://arxiv.org/pdf/2004.04984v1)

> This paper investigates the sensitivity of forecast performance measures to taking a real time versus pseudo out-of-sample perspective. We use monthly vintages for the United States (US) and the Euro Area (EA) and estimate a set of vector autoregressive (VAR) models of different sizes with constant and time-varying parameters (TVPs) and stochastic volatility (SV). Our results suggest differences in the relative ordering of model performance for point and density forecasts depending on whether real time data or truncated final vintages in pseudo out-of-sample simulations are used for evaluating forecasts. No clearly superior specification for the US or the EA across variable types and forecast horizons can be identified, although larger models featuring TVPs appear to be affected the least by missing values and data revisions. We identify substantial differences in performance metrics with respect to whether forecasts are produced for the US or the EA.

</details>

<details>

<summary>2020-04-10 11:36:51 - Reconciling Bayesian and perimeter regularization for binary inversion</summary>

- *Oliver R. A. Dunbar, Matthew M. Dunlop, Charles M. Elliott, Viet Ha Hoang, Andrew M. Stuart*

- `1706.01960v4` - [abs](http://arxiv.org/abs/1706.01960v4) - [pdf](http://arxiv.org/pdf/1706.01960v4)

> A central theme in classical algorithms for the reconstruction of discontinuous functions from observational data is perimeter regularization via the use of the total variation. On the other hand, sparse or noisy data often demands a probabilistic approach to the reconstruction of images, to enable uncertainty quantification; the Bayesian approach to inversion, which itself introduces a form of regularization, is a natural framework in which to carry this out. In this paper the link between Bayesian inversion methods and perimeter regularization is explored. In this paper two links are studied: (i) the maximum a posteriori (MAP) objective function of a suitably chosen Bayesian phase-field approach is shown to be closely related to a least squares plus perimeter regularization objective; (ii) sample paths of a suitably chosen Bayesian level set formulation are shown to possess finite perimeter and to have the ability to learn about the true perimeter.

</details>

<details>

<summary>2020-04-10 14:09:54 - A Modified Bayesian Optimization based Hyper-Parameter Tuning Approach for Extreme Gradient Boosting</summary>

- *Sayan Putatunda, Kiran Rama*

- `2004.05041v1` - [abs](http://arxiv.org/abs/2004.05041v1) - [pdf](http://arxiv.org/pdf/2004.05041v1)

> It is already reported in the literature that the performance of a machine learning algorithm is greatly impacted by performing proper Hyper-Parameter optimization. One of the ways to perform Hyper-Parameter optimization is by manual search but that is time consuming. Some of the common approaches for performing Hyper-Parameter optimization are Grid search Random search and Bayesian optimization using Hyperopt. In this paper, we propose a brand new approach for hyperparameter improvement i.e. Randomized-Hyperopt and then tune the hyperparameters of the XGBoost i.e. the Extreme Gradient Boosting algorithm on ten datasets by applying Random search, Randomized-Hyperopt, Hyperopt and Grid Search. The performances of each of these four techniques were compared by taking both the prediction accuracy and the execution time into consideration. We find that the Randomized-Hyperopt performs better than the other three conventional methods for hyper-paramter optimization of XGBoost.

</details>

<details>

<summary>2020-04-10 19:42:55 - On the quantification and efficient propagation of imprecise probabilities with copula dependence</summary>

- *Jiaxin Zhang, Michael D. Shields*

- `1805.12525v4` - [abs](http://arxiv.org/abs/1805.12525v4) - [pdf](http://arxiv.org/pdf/1805.12525v4)

> This paper addresses the problem of quantification and propagation of uncertainties associated with dependence modeling when data for characterizing probability models are limited. Practically, the system inputs are often assumed to be mutually independent or correlated by a multivariate Gaussian distribution. However, this subjective assumption may introduce bias in the response estimate if the real dependence structure deviates from this assumption. In this work, we overcome this limitation by introducing a flexible copula dependence model to capture complex dependencies. A hierarchical Bayesian multimodel approach is proposed to quantify uncertainty in dependence model-form and model parameters that result from small data sets. This approach begins by identifying, through Bayesian multimodel inference, a set of candidate marginal models and their corresponding model probabilities, and then estimating the uncertainty in the copula-based dependence structure, which is conditional on the marginals and their parameters. The overall uncertainties integrating marginals and copulas are probabilistically represented by an ensemble of multivariate candidate densities. A novel importance sampling reweighting approach is proposed to efficiently propagate the overall uncertainties through a computational model. Through an example studying the influence of constituent properties on the out-of-plane properties of transversely isotropic E- glass fiber composites, we show that the composite property with copula-based dependence model converges to the true estimate as data set size increases, while an independence or arbitrary Gaussian correlation assumption leads to a biased estimate.

</details>

<details>

<summary>2020-04-10 20:16:25 - Multiple repairable systems under dependent competing risks with nonparametric Frailty</summary>

- *Marco Pollo Almeida, Rafael Paixao, Pedro Ramos, Vera Tomazella, Francisco Louzada, Ricardo Ehlers*

- `2004.05217v1` - [abs](http://arxiv.org/abs/2004.05217v1) - [pdf](http://arxiv.org/pdf/2004.05217v1)

> The aim of this article is to analyze data from multiple repairable systems under the presence of dependent competing risks. In order to model this dependence structure, we adopted the well-known shared frailty model. This model provides a suitable theoretical basis for generating dependence between the components failure times in the dependent competing risks model. It is known that the dependence effect in this scenario influences the estimates of the model parameters. Hence, under the assumption that the cause-specific intensities follow a PLP, we propose a frailty-induced dependence approach to incorporate the dependence among the cause-specific recurrent processes. Moreover, the misspecification of the frailty distribution may lead to errors when estimating the parameters of interest. Because of this, we considered a Bayesian nonparametric approach to model the frailty density in order to offer more flexibility and to provide consistent estimates for the PLP model, as well as insights about heterogeneity among the systems. Both simulation studies and real case studies are provided to illustrate the proposed approaches and demonstrate their validity.

</details>

<details>

<summary>2020-04-11 05:18:31 - Bayesian Shrinkage towards Sharp Minimaxity</summary>

- *Qifan Song*

- `2004.05307v1` - [abs](http://arxiv.org/abs/2004.05307v1) - [pdf](http://arxiv.org/pdf/2004.05307v1)

> Shrinkage prior are becoming more and more popular in Bayesian modeling for high dimensional sparse problems due to its computational efficiency. Recent works show that a polynomially decaying prior leads to satisfactory posterior asymptotics under regression models. In the literature, statisticians have investigated how the global shrinkage parameter, i.e., the scale parameter, in a heavy tail prior affects the posterior contraction. In this work, we explore how the shape of the prior, or more specifically, the polynomial order of the prior tail affects the posterior. We discover that, under the sparse normal means models, the polynomial order does affect the multiplicative constant of the posterior contraction rate. More importantly, if the polynomial order is sufficiently close to 1, it will induce the optimal Bayesian posterior convergence, in the sense that the Bayesian contraction rate is sharply minimax, i.e., not only the order, but also the multiplicative constant of the posterior contraction rate are optimal. The above Bayesian sharp minimaxity holds when the global shrinkage parameter follows a deterministic choice which depends on the unknown sparsity $s$. Therefore, a Beta-prior modeling is further proposed, such that our sharply minimax Bayesian procedure is adaptive to unknown $s$. Our theoretical discoveries are justified by simulation studies.

</details>

<details>

<summary>2020-04-11 05:26:47 - Statistical inference and Bayesian optimal life-testing plans under Type-II unified hybrid censoring scheme</summary>

- *Tanmay Sen, Ritwik Bhattacharya, Biswabrata Pradhan, Yogesh Mani Tripathi*

- `2004.05308v1` - [abs](http://arxiv.org/abs/2004.05308v1) - [pdf](http://arxiv.org/pdf/2004.05308v1)

> This article describes the inferential procedures and Bayesian optimal life-testing issues under Type-II unified hybrid censoring scheme. First, the explicit expressions of expected number of failures, expected duration of testing and Fisher information matrix for the unknown parameters of the underlying lifetime model are derived. Then, using these quantities, the Bayesian optimal life-testing plans are computed in subsequent section. A cost constraint D-optimal optimization problem has been formulated and the corresponding solution algorithm is provided to obtain optimal plans. Computational procedures are illustrated through numerical examples.

</details>

<details>

<summary>2020-04-11 12:09:51 - Bayesian Surprise in Indoor Environments</summary>

- *Sebastian Feld, Andreas Sedlmeier, Markus Friedrich, Jan Franz, Lenz Belzner*

- `2004.05381v1` - [abs](http://arxiv.org/abs/2004.05381v1) - [pdf](http://arxiv.org/pdf/2004.05381v1)

> This paper proposes a novel method to identify unexpected structures in 2D floor plans using the concept of Bayesian Surprise. Taking into account that a person's expectation is an important aspect of the perception of space, we exploit the theory of Bayesian Surprise to robustly model expectation and thus surprise in the context of building structures. We use Isovist Analysis, which is a popular space syntax technique, to turn qualitative object attributes into quantitative environmental information. Since isovists are location-specific patterns of visibility, a sequence of isovists describes the spatial perception during a movement along multiple points in space. We then use Bayesian Surprise in a feature space consisting of these isovist readings. To demonstrate the suitability of our approach, we take "snapshots" of an agent's local environment to provide a short list of images that characterize a traversed trajectory through a 2D indoor environment. Those fingerprints represent surprising regions of a tour, characterize the traversed map and enable indoor LBS to focus more on important regions. Given this idea, we propose to use "surprise" as a new dimension of context in indoor location-based services (LBS). Agents of LBS, such as mobile robots or non-player characters in computer games, may use the context surprise to focus more on important regions of a map for a better use or understanding of the floor plan.

</details>

<details>

<summary>2020-04-11 15:30:47 - Scaling Bayesian inference of mixed multinomial logit models to very large datasets</summary>

- *Filipe Rodrigues*

- `2004.05426v1` - [abs](http://arxiv.org/abs/2004.05426v1) - [pdf](http://arxiv.org/pdf/2004.05426v1)

> Variational inference methods have been shown to lead to significant improvements in the computational efficiency of approximate Bayesian inference in mixed multinomial logit models when compared to standard Markov-chain Monte Carlo (MCMC) methods without compromising accuracy. However, despite their demonstrated efficiency gains, existing methods still suffer from important limitations that prevent them to scale to very large datasets, while providing the flexibility to allow for rich prior distributions and to capture complex posterior distributions. In this paper, we propose an Amortized Variational Inference approach that leverages stochastic backpropagation, automatic differentiation and GPU-accelerated computation, for effectively scaling Bayesian inference in Mixed Multinomial Logit models to very large datasets. Moreover, we show how normalizing flows can be used to increase the flexibility of the variational posterior approximations. Through an extensive simulation study, we empirically show that the proposed approach is able to achieve computational speedups of multiple orders of magnitude over traditional MSLE and MCMC approaches for large datasets without compromising estimation accuracy.

</details>

<details>

<summary>2020-04-11 21:02:44 - A Bayesian Nonparametric Approach for Inferring Drug Combination Effects on Mental Health in People with HIV</summary>

- *Wei Jin, Yang Ni, Leah H. Rubin, Amanda B. Spence, Yanxun Xu*

- `2004.05487v1` - [abs](http://arxiv.org/abs/2004.05487v1) - [pdf](http://arxiv.org/pdf/2004.05487v1)

> Although combination antiretroviral therapy (ART) is highly effective in suppressing viral load for people with HIV (PWH), many ART agents may exacerbate central nervous system (CNS)-related adverse effects including depression. Therefore, understanding the effects of ART drugs on the CNS function, especially mental health, can help clinicians personalize medicine with less adverse effects for PWH and prevent them from discontinuing their ART to avoid undesirable health outcomes and increased likelihood of HIV transmission. The emergence of electronic health records offers researchers unprecedented access to HIV data including individuals' mental health records, drug prescriptions, and clinical information over time. However, modeling such data is very challenging due to high-dimensionality of the drug combination space, the individual heterogeneity, and sparseness of the observed drug combinations. We develop a Bayesian nonparametric approach to learn drug combination effect on mental health in PWH adjusting for socio-demographic, behavioral, and clinical factors. The proposed method is built upon the subset-tree kernel method that represents drug combinations in a way that synthesizes known regimen structure into a single mathematical representation. It also utilizes a distance-dependent Chinese restaurant process to cluster heterogeneous population while taking into account individuals' treatment histories. We evaluate the proposed approach through simulation studies, and apply the method to a dataset from the Women's Interagency HIV Study, yielding interpretable and promising results. Our method has clinical utility in guiding clinicians to prescribe more informed and effective personalized treatment based on individuals' treatment histories and clinical characteristics.

</details>

<details>

<summary>2020-04-12 04:52:28 - Efficient parameter sampling for Markov jump processes</summary>

- *Boqian Zhang, Vinayak Rao*

- `1704.02369v3` - [abs](http://arxiv.org/abs/1704.02369v3) - [pdf](http://arxiv.org/pdf/1704.02369v3)

> Markov jump processes (MJPs) are continuous-time stochastic processes widely used in a variety of applied disciplines. Inference for MJPs typically proceeds via Markov chain Monte Carlo, the state-of-the-art being a uniformization-based auxiliary variable Gibbs sampler. This was designed for situations where the MJP parameters are known, and Bayesian inference over unknown parameters is typically carried out by incorporating it into a larger Gibbs sampler. This strategy of sampling parameters given path, and path given parameters can result in poor Markov chain mixing. In this work, we propose a simple and elegant algorithm to address this problem. Our scheme brings Metropolis-Hastings approaches for discrete-time hidden Markov models to the continuous-time setting, resulting in a complete and clean recipe for parameter and path inference in MJPs. In our experiments, we demonstrate superior performance over Gibbs sampling, as well as another popular approach, particle MCMC. We also show our sampler inherits geometric mixing from an `ideal' sampler that operates without computational constraints.

</details>

<details>

<summary>2020-04-12 13:39:52 - Bayesian Hierarchical Words Representation Learning</summary>

- *Oren Barkan, Idan Rejwan, Avi Caciularu, Noam Koenigstein*

- `2004.07126v1` - [abs](http://arxiv.org/abs/2004.07126v1) - [pdf](http://arxiv.org/pdf/2004.07126v1)

> This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm. BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors. By propagating relevant information between related words, BHWR utilizes the taxonomy to improve the quality of such representations. Evaluation of several linguistic datasets demonstrates the advantages of BHWR over suitable alternatives that facilitate Bayesian modeling with or without semantic priors. Finally, we further show that BHWR produces better representations for rare words.

</details>

<details>

<summary>2020-04-13 18:44:04 - Assessing the Performance of the Discrete Generalised Pareto Distribution in Modelling Non-Life Insurance Claims</summary>

- *S. K-B. Dzidzornu, R. Minkah*

- `2004.06150v1` - [abs](http://arxiv.org/abs/2004.06150v1) - [pdf](http://arxiv.org/pdf/2004.06150v1)

> In this paper, non-life insurance claims were modelled under the three parameter discrete generalised Pareto distribution. Data from the National Insurance Commission of Ghana on reported and settled claims were considered for the period 2012-2016. The maximum likelihood estimation principle was adopted in fitting the discrete Pareto distribution to the yearly and aggregated data. The estimation involved two steps. Firstly, the $\mu$ and $(\mu+1)$ frequency method of \citet{Prieto2014} was modified to suit the characteristics of the data under study. Secondly, a bootstrap algorithm was implemented to obtain the standard errors of the estimators of the parameters of the discrete generalised Pareto distribution. The performance of the discrete generalised Pareto distribution is compared to the negative binomial distribution in modelling the non-life insurance claims data using the information criteria of Akaike and Bayesian. The results show that the discrete generalised Pareto distribution provides a better fit to the non-life claims data.   Keywords: Non-life insurance claims, discrete generalised Pareto distribution, negative binomial distribution, maximum likelihood estimation, information criteria.

</details>

<details>

<summary>2020-04-13 18:57:23 - An Adaptive Empirical Bayesian Method for Sparse Deep Learning</summary>

- *Wei Deng, Xiao Zhang, Faming Liang, Guang Lin*

- `1910.10791v2` - [abs](http://arxiv.org/abs/1910.10791v2) - [pdf](http://arxiv.org/pdf/1910.10791v2)

> We propose a novel adaptive empirical Bayesian method for sparse deep learning, where the sparsity is ensured via a class of self-adaptive spike-and-slab priors. The proposed method works by alternatively sampling from an adaptive hierarchical posterior distribution using stochastic gradient Markov Chain Monte Carlo (MCMC) and smoothly optimizing the hyperparameters using stochastic approximation (SA). We further prove the convergence of the proposed method to the asymptotically correct distribution under mild conditions. Empirical applications of the proposed method lead to the state-of-the-art performance on MNIST and Fashion MNIST with shallow convolutional neural networks and the state-of-the-art compression performance on CIFAR10 with Residual Networks. The proposed method also improves resistance to adversarial attacks.

</details>

<details>

<summary>2020-04-13 23:26:43 - Knot Selection in Sparse Gaussian Processes with a Variational Objective Function</summary>

- *Nathaniel Garton, Jarad Niemi, Alicia Carriquiry*

- `2003.02729v2` - [abs](http://arxiv.org/abs/2003.02729v2) - [pdf](http://arxiv.org/pdf/2003.02729v2)

> Sparse, knot-based Gaussian processes have enjoyed considerable success as scalable approximations to full Gaussian processes. Certain sparse models can be derived through specific variational approximations to the true posterior, and knots can be selected to minimize the Kullback-Leibler divergence between the approximate and true posterior. While this has been a successful approach, simultaneous optimization of knots can be slow due to the number of parameters being optimized. Furthermore, there have been few proposed methods for selecting the number of knots, and no experimental results exist in the literature. We propose using a one-at-a-time knot selection algorithm based on Bayesian optimization to select the number and locations of knots. We showcase the competitive performance of this method relative to simultaneous optimization of knots on three benchmark data sets, but at a fraction of the computational cost.

</details>

<details>

<summary>2020-04-14 07:41:19 - A High-Performance Implementation of Bayesian Matrix Factorization with Limited Communication</summary>

- *Tom Vander Aa, Xiangju Qin, Paul Blomstedt, Roel Wuyts, Wilfried Verachtert, Samuel Kaski*

- `2004.02561v2` - [abs](http://arxiv.org/abs/2004.02561v2) - [pdf](http://arxiv.org/pdf/2004.02561v2)

> Matrix factorization is a very common machine learning technique in recommender systems. Bayesian Matrix Factorization (BMF) algorithms would be attractive because of their ability to quantify uncertainty in their predictions and avoid over-fitting, combined with high prediction accuracy. However, they have not been widely used on large-scale data because of their prohibitive computational cost. In recent work, efforts have been made to reduce the cost, both by improving the scalability of the BMF algorithm as well as its implementation, but so far mainly separately. In this paper we show that the state-of-the-art of both approaches to scalability can be combined. We combine the recent highly-scalable Posterior Propagation algorithm for BMF, which parallelizes computation of blocks of the matrix, with a distributed BMF implementation that users asynchronous communication within each block. We show that the combination of the two methods gives substantial improvements in the scalability of BMF on web-scale datasets, when the goal is to reduce the wall-clock time.

</details>

<details>

<summary>2020-04-14 08:37:57 - A geostatistical two field model that combines point observations and nested areal observations, and quantifies long-term spatial variability -- A case study of annual runoff predictions in the Voss area</summary>

- *Thea Roksvåg, Ingelin Steinsland, Kolbjørn Engeland*

- `1904.02519v4` - [abs](http://arxiv.org/abs/1904.02519v4) - [pdf](http://arxiv.org/pdf/1904.02519v4)

> In this study, annual runoff is estimated by using a Bayesian geostatistical model for interpolation of hydrological data of different spatial support. That is, streamflow observations from catchments (areal data), and precipitation and evaporation data (point data). The model contains one climatic spatial effect that is common for all years under study, and one year specific spatial effect. Hence, the framework enables a quantification of the spatial variability that is due to long-term weather patterns and processes. This can contribute to a better understanding of biases and uncertainties in environmental modeling. By using integrated nested Laplace approximations (INLA) and the stochastic partial differential equation approach (SPDE) to spatial modeling, the two field model is computationally feasible and fast. The suggested model is tested by predicting 10 years of annual runoff around Voss in Norway and through a simulation study. We find that on average we benefit from combining point and areal data compared to using only one of the data types, and that the interaction between nested areal data and point data gives a spatial model that takes us beyond smoothing. Another finding is that when climatic effects dominate over annual effects, systematic under- and overestimation of runoff over time can be expected. On the other hand, a dominating climatic spatial effect implies that short records of runoff from an otherwise ungauged catchment can lead to large improvements in the predictability of runoff.

</details>

<details>

<summary>2020-04-14 13:26:01 - The covariance matrix of Green's functions and its application to machine learning</summary>

- *Tomoko Nagai*

- `2004.06481v1` - [abs](http://arxiv.org/abs/2004.06481v1) - [pdf](http://arxiv.org/pdf/2004.06481v1)

> In this paper, a regression algorithm based on Green's function theory is proposed and implemented. We first survey Green's function for the Dirichlet boundary value problem of 2nd order linear ordinary differential equation, which is a reproducing kernel of a suitable Hilbert space. We next consider a covariance matrix composed of the normalized Green's function, which is regarded as aprobability density function. By supporting Bayesian approach, the covariance matrix gives predictive distribution, which has the predictive mean $\mu$ and the confidence interval [$\mu$-2s, $\mu$+2s], where s stands for a standard deviation.

</details>

<details>

<summary>2020-04-14 19:29:24 - Heterogeneous Learning from Demonstration</summary>

- *Rohan Paleja, Matthew Gombolay*

- `2001.09569v2` - [abs](http://arxiv.org/abs/2001.09569v2) - [pdf](http://arxiv.org/pdf/2001.09569v2)

> The development of human-robot systems able to leverage the strengths of both humans and their robotic counterparts has been greatly sought after because of the foreseen, broad-ranging impact across industry and research. We believe the true potential of these systems cannot be reached unless the robot is able to act with a high level of autonomy, reducing the burden of manual tasking or teleoperation. To achieve this level of autonomy, robots must be able to work fluidly with its human partners, inferring their needs without explicit commands. This inference requires the robot to be able to detect and classify the heterogeneity of its partners. We propose a framework for learning from heterogeneous demonstration based upon Bayesian inference and evaluate a suite of approaches on a real-world dataset of gameplay from StarCraft II. This evaluation provides evidence that our Bayesian approach can outperform conventional methods by up to 12.8$%$.

</details>

<details>

<summary>2020-04-15 09:20:20 - A Bayesian Quest for Finding a Unified Model for Predicting Volleyball Games</summary>

- *Leonardo Egidi, Ioannis Ntzoufras*

- `1911.01815v2` - [abs](http://arxiv.org/abs/1911.01815v2) - [pdf](http://arxiv.org/pdf/1911.01815v2)

> Volleyball is a team sport with unique and specific characteristics. We introduce a new two level-hierarchical Bayesian model which accounts for theses volleyball specific characteristics. In the first level, we model the set outcome with a simple logistic regression model. Conditionally on the winner of the set, in the second level, we use a truncated negative binomial distribution for the points earned by the loosing team. An additional Poisson distributed inflation component is introduced to model the extra points played in the case that the two teams have point difference less than two points. The number of points of the winner within each set is deterministically specified by the winner of the set and the points of the inflation component. The team specific abilities and the home effect are used as covariates on all layers of the model (set, point, and extra inflated points). The implementation of the proposed model on the Italian Superlega 2017/2018 data shows an exceptional reproducibility of the final league table and a satisfactory predictive ability.

</details>

<details>

<summary>2020-04-15 10:49:34 - Estimating Bayes factors from minimal ANOVA summaries for repeated-measures designs</summary>

- *Thomas J. Faulkenberry*

- `1905.05569v3` - [abs](http://arxiv.org/abs/1905.05569v3) - [pdf](http://arxiv.org/pdf/1905.05569v3)

> In this paper, I develop a formula for estimating Bayes factors directly from minimal summary statistics produced in repeated measures analysis of variance designs. The formula, which requires knowing only the $F$-statistic, the number of subjects, and the number of repeated measurements per subject, is based on the BIC approximation of the Bayes factor, a common default method for Bayesian computation with linear models. In addition to providing computational examples, I report a simulation study in which I demonstrate that the formula compares favorably to a recently developed, more complex method that accounts for correlation between repeated measurements. The minimal BIC method provides a simple way for researchers to estimate Bayes factors from a minimal set of summary statistics, giving users a powerful index for estimating the evidential value of not only their own data, but also the data reported in published studies.

</details>

<details>

<summary>2020-04-15 13:09:09 - The exact form of the 'Ockham factor' in model selection</summary>

- *Jonathan Rougier, Carey Priebe*

- `1906.11592v2` - [abs](http://arxiv.org/abs/1906.11592v2) - [pdf](http://arxiv.org/pdf/1906.11592v2)

> We explore the arguments for maximizing the `evidence' as an algorithm for model selection. We show, using a new definition of model complexity which we term `flexibility', that maximizing the evidence should appeal to both Bayesian and Frequentist statisticians. This is due to flexibility's unique position in the exact decomposition of log-evidence into log-fit minus flexibility. In the Gaussian linear model, flexibility is asymptotically equal to the Bayesian Information Criterion (BIC) penalty, but we caution against using BIC in place of flexibility for model selection.

</details>

<details>

<summary>2020-04-15 14:24:30 - Fast Exact Bayesian Inference for Sparse Signals in the Normal Sequence Model</summary>

- *Tim van Erven, Botond Szabo*

- `1810.10883v2` - [abs](http://arxiv.org/abs/1810.10883v2) - [pdf](http://arxiv.org/pdf/1810.10883v2)

> We consider exact algorithms for Bayesian inference with model selection priors (including spike-and-slab priors) in the sparse normal sequence model. Because the best existing exact algorithm becomes numerically unstable for sample sizes over n=500, there has been much attention for alternative approaches like approximate algorithms (Gibbs sampling, variational Bayes, etc.), shrinkage priors (e.g. the Horseshoe prior and the Spike-and-Slab LASSO) or empirical Bayesian methods. However, by introducing algorithmic ideas from online sequential prediction, we show that exact calculations are feasible for much larger sample sizes: for general model selection priors we reach n=25000, and for certain spike-and-slab priors we can easily reach n=100000. We further prove a de Finetti-like result for finite sample sizes that characterizes exactly which model selection priors can be expressed as spike-and-slab priors. The computational speed and numerical accuracy of the proposed methods are demonstrated in experiments on simulated data, on a differential gene expression data set, and to compare the effect of multiple hyper-parameter settings in the beta-binomial prior. In our experimental evaluation we compute guaranteed bounds on the numerical accuracy of all new algorithms, which shows that the proposed methods are numerically reliable whereas an alternative based on long division is not.

</details>

<details>

<summary>2020-04-16 09:13:08 - Combining heterogeneous subgroups with graph-structured variable selection priors for Cox regression</summary>

- *Katrin Madjar, Manuela Zucknick, Katja Ickstadt, Jörg Rahnenführer*

- `2004.07542v1` - [abs](http://arxiv.org/abs/2004.07542v1) - [pdf](http://arxiv.org/pdf/2004.07542v1)

> Important objectives in cancer research are the prediction of a patient's risk based on molecular measurements such as gene expression data and the identification of new prognostic biomarkers (e.g. genes). In clinical practice, this is often challenging because patient cohorts are typically small and can be heterogeneous. In classical subgroup analysis, a separate prediction model is fitted using only the data of one specific cohort. However, this can lead to a loss of power when the sample size is small. Simple pooling of all cohorts, on the other hand, can lead to biased results, especially when the cohorts are heterogeneous. For this situation, we propose a new Bayesian approach suitable for continuous molecular measurements and survival outcome that identifies the important predictors and provides a separate risk prediction model for each cohort. It allows sharing information between cohorts to increase power by assuming a graph linking predictors within and across different cohorts. The graph helps to identify pathways of functionally related genes and genes that are simultaneously prognostic in different cohorts. Results demonstrate that our proposed approach is superior to the standard approaches in terms of prediction performance and increased power in variable selection when the sample size is small.

</details>

<details>

<summary>2020-04-16 10:07:02 - Bayesian filtering unifies adaptive and non-adaptive neural network optimization methods</summary>

- *Laurence Aitchison*

- `1807.07540v5` - [abs](http://arxiv.org/abs/1807.07540v5) - [pdf](http://arxiv.org/pdf/1807.07540v5)

> We formulate the problem of neural network optimization as Bayesian filtering, where the observations are the backpropagated gradients. While neural network optimization has previously been studied using natural gradient methods which are closely related to Bayesian inference, they were unable to recover standard optimizers such as Adam and RMSprop with a root-mean-square gradient normalizer, instead getting a mean-square normalizer. To recover the root-mean-square normalizer, we find it necessary to account for the temporal dynamics of all the other parameters as they are geing optimized. The resulting optimizer, AdaBayes, adaptively transitions between SGD-like and Adam-like behaviour, automatically recovers AdamW, a state of the art variant of Adam with decoupled weight decay, and has generalisation performance competitive with SGD.

</details>

<details>

<summary>2020-04-16 11:27:50 - Diversity-Aware Weighted Majority Vote Classifier for Imbalanced Data</summary>

- *Anil Goyal, Jihed Khiari*

- `2004.07605v1` - [abs](http://arxiv.org/abs/2004.07605v1) - [pdf](http://arxiv.org/pdf/2004.07605v1)

> In this paper, we propose a diversity-aware ensemble learning based algorithm, referred to as DAMVI, to deal with imbalanced binary classification tasks. Specifically, after learning base classifiers, the algorithm i) increases the weights of positive examples (minority class) which are "hard" to classify with uniformly weighted base classifiers; and ii) then learns weights over base classifiers by optimizing the PAC-Bayesian C-Bound that takes into account the accuracy and diversity between the classifiers. We show efficiency of the proposed approach with respect to state-of-art models on predictive maintenance task, credit card fraud detection, webpage classification and medical applications.

</details>

<details>

<summary>2020-04-16 15:09:48 - Rapidly evaluating lockdown strategies using spectral analysis: the cycles behind new daily COVID-19 cases and what happens after lockdown</summary>

- *Guy P. Nason*

- `2004.07696v1` - [abs](http://arxiv.org/abs/2004.07696v1) - [pdf](http://arxiv.org/pdf/2004.07696v1)

> Spectral analysis characterises oscillatory time series behaviours such as cycles, but accurate estimation requires reasonable numbers of observations. Current COVID-19 time series for many countries are short: pre- and post-lockdown series are shorter still. Accurate estimation of potentially interesting cycles within such series seems beyond reach. We solve the problem of obtaining accurate estimates from short time series by using recent Bayesian spectral fusion methods. Here we show that transformed new daily COVID-19 cases for many countries generally contain three cycles operating at wavelengths of around 2.7, 4.1 and 6.7 days (weekly). We show that the shorter cycles are suppressed after lockdown. The pre- and post lockdown differences suggest that the weekly effect is at least partly due to non-epidemic factors, whereas the two shorter cycles seem intrinsic to the epidemic. Unconstrained, new cases grow exponentially, but the internal cyclic structure causes periodic falls in cases. This suggests that lockdown success might only be indicated by four or more daily falls in cases. Spectral learning for epidemic time series contributes to the understanding of the epidemic process, helping evaluate interventions and assists with forecasting. Spectral fusion is a general technique that is able to fuse spectra recorded at different sampling rates, which can be applied to a wide range of time series from many disciplines.

</details>

<details>

<summary>2020-04-16 16:49:31 - Gaussian Process Learning-based Probabilistic Optimal Power Flow</summary>

- *Parikshit Pareek, Hung D. Nguyen*

- `2004.07757v1` - [abs](http://arxiv.org/abs/2004.07757v1) - [pdf](http://arxiv.org/pdf/2004.07757v1)

> In this letter, we present a novel Gaussian Process Learning-based Probabilistic Optimal Power Flow (GP-POPF) for solving POPF under renewable and load uncertainties of arbitrary distribution. The proposed method relies on a non-parametric Bayesian inference-based uncertainty propagation approach, called Gaussian Process (GP). We also suggest a new type of sensitivity called Subspace-wise Sensitivity, using observations on the interpretability of GP-POPF hyperparameters. The simulation results on 14-bus and 30-bus systems show that the proposed method provides reasonably accurate solutions when compared with Monte-Carlo Simulations (MCS) solutions at different levels of uncertain renewable penetration as well as load uncertainties, while requiring much less number of samples and elapsed time.

</details>

<details>

<summary>2020-04-16 17:31:10 - Identifying main effects and interactions among exposures using Gaussian processes</summary>

- *Federico Ferrari, David B. Dunson*

- `1911.01910v2` - [abs](http://arxiv.org/abs/1911.01910v2) - [pdf](http://arxiv.org/pdf/1911.01910v2)

> This article is motivated by the problem of studying the joint effect of different chemical exposures on human health outcomes. This is essentially a nonparametric regression problem, with interest being focused not on a black box for prediction but instead on selection of main effects and interactions. For interpretability, we decompose the expected health outcome into a linear main effect, pairwise interactions, and a non-linear deviation. Our interest is in model selection for these different components, accounting for uncertainty and addressing non-identifability between the linear and nonparametric components of the semiparametric model. We propose a Bayesian approach to inference, placing variable selection priors on the different components, and developing a Markov chain Monte Carlo (MCMC) algorithm. A key component of our approach is the incorporation of a heredity constraint to only include interactions in the presence of main effects, effectively reducing dimensionality of the model search. We adapt a projection approach developed in the spatial statistics literature to enforce identifiability in modeling the nonparametric component using a Gaussian process. We also employ a dimension reduction strategy to sample the non-linear random effects that aids the mixing of the MCMC algorithm. The proposed MixSelect framework is evaluated using a simulation study, and is illustrated using data from the National Health and Nutrition Examination Survey (NHANES). Code is available on GitHub.

</details>

<details>

<summary>2020-04-16 18:37:56 - Parameterizing uncertainty by deep invertible networks, an application to reservoir characterization</summary>

- *Gabrio Rizzuti, Ali Siahkoohi, Philipp A. Witte, Felix J. Herrmann*

- `2004.07871v1` - [abs](http://arxiv.org/abs/2004.07871v1) - [pdf](http://arxiv.org/pdf/2004.07871v1)

> Uncertainty quantification for full-waveform inversion provides a probabilistic characterization of the ill-conditioning of the problem, comprising the sensitivity of the solution with respect to the starting model and data noise. This analysis allows to assess the confidence in the candidate solution and how it is reflected in the tasks that are typically performed after imaging (e.g., stratigraphic segmentation following reservoir characterization). Classically, uncertainty comes in the form of a probability distribution formulated from Bayesian principles, from which we seek to obtain samples. A popular solution involves Monte Carlo sampling. Here, we propose instead an approach characterized by training a deep network that "pushes forward" Gaussian random inputs into the model space (representing, for example, density or velocity) as if they were sampled from the actual posterior distribution. Such network is designed to solve a variational optimization problem based on the Kullback-Leibler divergence between the posterior and the network output distributions. This work is fundamentally rooted in recent developments for invertible networks. Special invertible architectures, besides being computational advantageous with respect to traditional networks, do also enable analytic computation of the output density function. Therefore, after training, these networks can be readily used as a new prior for a related inversion problem. This stands in stark contrast with Monte-Carlo methods, which only produce samples. We validate these ideas with an application to angle-versus-ray parameter analysis for reservoir characterization.

</details>

<details>

<summary>2020-04-17 16:42:18 - MAP segmentation in Bayesian hidden Markov models: a case study</summary>

- *Alexey Koloydenko, Kristi Kuljus, Jüri Lember*

- `2004.08336v1` - [abs](http://arxiv.org/abs/2004.08336v1) - [pdf](http://arxiv.org/pdf/2004.08336v1)

> We consider the problem of estimating the maximum posterior probability (MAP) state sequence for a finite state and finite emission alphabet hidden Markov model (HMM) in the Bayesian setup, where both emission and transition matrices have Dirichlet priors. We study a training set consisting of thousands of protein alignment pairs. The training data is used to set the prior hyperparameters for Bayesian MAP segmentation. Since the Viterbi algorithm is not applicable any more, there is no simple procedure to find the MAP path, and several iterative algorithms are considered and compared. The main goal of the paper is to test the Bayesian setup against the frequentist one, where the parameters of HMM are estimated using the training data.

</details>

<details>

<summary>2020-04-18 05:54:28 - Determination of Bayesian optimal warranty length under Type-II unified hybrid censoring scheme</summary>

- *Tanmay Sen, Ritwik Bhattacharya, Biswabrata Pradhan, Yogesh Mani Tripathi*

- `2004.08533v1` - [abs](http://arxiv.org/abs/2004.08533v1) - [pdf](http://arxiv.org/pdf/2004.08533v1)

> Determination of an appropriate warranty length for the lifetime of the product is an important issue to the manufacturer. In this article, optimal warranty length of the product for the combined free replacement and the pro-rata warranty policy is computed based on the Type-II unified hybrid censored data. A non-linear pro-rata warranty policy is proposed in this context. The optimal warranty length is obtained by maximizing an expected utility function. The expectation is taken with respect to the posterior predictive model for the time-to-failure data. It is observed that the non-linear pro-rata warranty policy gives a larger warranty length with maximum profit as compared to linear warranty policy. Finally, a real-data set is analyzed in order to illustrate the advantage of using non-linear pro-rata warranty policy.

</details>

<details>

<summary>2020-04-18 18:57:32 - A Loss-Based Prior for Gaussian Graphical Models</summary>

- *Laurentiu Catalin Hinoveanu, Fabrizio Leisen, Cristiano Villa*

- `1812.05531v2` - [abs](http://arxiv.org/abs/1812.05531v2) - [pdf](http://arxiv.org/pdf/1812.05531v2)

> Gaussian graphical models play an important role in various areas such as genetics, finance, statistical physics and others. They are a powerful modelling tool which allows one to describe the relationships among the variables of interest. From the Bayesian perspective, there are two sources of randomness: one is related to the multivariate distribution and the quantities that may parametrise the model, the other has to do with the underlying graph, $G$, equivalent to describing the conditional independence structure of the model under consideration. In this paper, we propose a prior on G based on two loss components. One considers the loss in information one would incur in selecting the wrong graph, while the second penalises for large number of edges, favouring sparsity. We illustrate the prior on simulated data and on real datasets, and compare the results with other priors on $G$ used in the literature. Moreover, we present a default choice of the prior as well as discuss how it can be calibrated so as to reflect available prior information.

</details>

<details>

<summary>2020-04-18 21:21:56 - Protecting Classifiers From Attacks. A Bayesian Approach</summary>

- *Victor Gallego, Roi Naveiro, Alberto Redondo, David Rios Insua, Fabrizio Ruggeri*

- `2004.08705v1` - [abs](http://arxiv.org/abs/2004.08705v1) - [pdf](http://arxiv.org/pdf/2004.08705v1)

> Classification problems in security settings are usually modeled as confrontations in which an adversary tries to fool a classifier manipulating the covariates of instances to obtain a benefit. Most approaches to such problems have focused on game-theoretic ideas with strong underlying common knowledge assumptions, which are not realistic in the security realm. We provide an alternative Bayesian framework that accounts for the lack of precise knowledge about the attacker's behavior using adversarial risk analysis. A key ingredient required by our framework is the ability to sample from the distribution of originating instances given the possibly attacked observed one. We propose a sampling procedure based on approximate Bayesian computation, in which we simulate the attacker's problem taking into account our uncertainty about his elements. For large scale problems, we propose an alternative, scalable approach that could be used when dealing with differentiable classifiers. Within it, we move the computational load to the training phase, simulating attacks from an adversary, adapting the framework to obtain a classifier robustified against attacks.

</details>

<details>

<summary>2020-04-18 23:04:56 - Bayesian differential programming for robust systems identification under uncertainty</summary>

- *Yibo Yang, Mohamed Aziz Bhouri, Paris Perdikaris*

- `2004.06843v2` - [abs](http://arxiv.org/abs/2004.06843v2) - [pdf](http://arxiv.org/pdf/2004.06843v2)

> This paper presents a machine learning framework for Bayesian systems identification from noisy, sparse and irregular observations of nonlinear dynamical systems. The proposed method takes advantage of recent developments in differentiable programming to propagate gradient information through ordinary differential equation solvers and perform Bayesian inference with respect to unknown model parameters using Hamiltonian Monte Carlo. This allows us to efficiently infer posterior distributions over plausible models with quantified uncertainty, while the use of sparsity-promoting priors enables the discovery of interpretable and parsimonious representations for the underlying latent dynamics. A series of numerical studies is presented to demonstrate the effectiveness of the proposed methods including nonlinear oscillators, predator-prey systems, chaotic dynamics and systems biology. Taken all together, our findings put forth a novel, flexible and robust workflow for data-driven model discovery under uncertainty.

</details>

<details>

<summary>2020-04-19 00:16:10 - Estimating the number of SARS-CoV-2 infections and the impact of social distancing in the United States</summary>

- *James Johndrow, Kristian Lum, Maria Gargiulo, Patrick Ball*

- `2004.02605v2` - [abs](http://arxiv.org/abs/2004.02605v2) - [pdf](http://arxiv.org/pdf/2004.02605v2)

> Understanding the number of individuals who have been infected with the novel coronavirus SARS-CoV-2, and the extent to which social distancing policies have been effective at limiting its spread, are critical for effective policy going forward. Here we present estimates of the extent to which confirmed cases in the United States undercount the true number of infections, and analyze how effective social distancing measures have been at mitigating or suppressing the virus. Our analysis uses a Bayesian model of COVID-19 fatalities with a likelihood based on an underlying differential equation model of the epidemic. We provide analysis for four states with significant epidemics: California, Florida, New York, and Washington. Our short-term forecasts suggest that these states may be following somewhat different trajectories for growth of the number of cases and fatalities.

</details>

<details>

<summary>2020-04-19 00:53:57 - Joint spatio-temporal analysis of multiple response types using the hierarchical generalized transformation model with application to coronavirus disease 2019 and social distancing</summary>

- *Jonathan R. Bradley*

- `2002.09983v3` - [abs](http://arxiv.org/abs/2002.09983v3) - [pdf](http://arxiv.org/pdf/2002.09983v3)

> Social distancing can be described as an effort to maintain a physical distance between individuals and has become a necessary public health measure to combat cornoavirus disease 2019 (COVID-19). Social distancing is known to weaken incidences and deaths due to COVID-19, however, there are detrimental economic and psychological effects. This motivates us to analyze incidences (and deaths) of COVID-19 along with a measure of the health of the US economy (i.e., the adjusted closing price of the Dow Jones Industrial), and a measure of the public interest in COVID-19 through Google Trends data. The model we implement is developed to be easily adapted to a data scientist's preferred method for continuous data, which is done to aid future analyses of this important dataset. This dataset consists of multiple response types (e.g., continuous-valued, count-valued, binomial counts). Thus, we introduce a reasonable easy-to-implement all-purpose method that "converts" a statistical model for continuous responses (the preferred model) into a Bayesian model for multi-response data sets. To do this, we transform the data such that the continuous-valued transformed data can be reasonably modeled using the preferred model and the transformation itself is treated as unknown. The implementation of our approach involves two steps. The first step produces posterior replicates of the transformed data using a latent conjugate multivariate (LCM) model. The second step involves generating values from the posterior distribution implied by the preferred model. We refer to our model as the hierarchical generalized transformation (HGT) model. In a simulation, we demonstrate the flexibility of the HGT model by incorporating two different preferred models: Bayesian additive regression trees (BART) and the spatial mixed effects (spatio-temporal mixed effects) models.

</details>

<details>

<summary>2020-04-19 10:31:46 - A Bayesian Deep Learning Framework for End-To-End Prediction of Emotion from Heartbeat</summary>

- *Ross Harper, Joshua Southern*

- `1902.03043v2` - [abs](http://arxiv.org/abs/1902.03043v2) - [pdf](http://arxiv.org/pdf/1902.03043v2)

> Automatic prediction of emotion promises to revolutionise human-computer interaction. Recent trends involve fusion of multiple data modalities - audio, visual, and physiological - to classify emotional state. However, in practice, collection of physiological data `in the wild' is currently limited to heartbeat time series of the kind generated by affordable wearable heart monitors. Furthermore, real-world applications of emotion prediction often require some measure of uncertainty over model output, in order to inform downstream decision-making. We present here an end-to-end deep learning model for classifying emotional valence from unimodal heartbeat time series. We further propose a Bayesian framework for modelling uncertainty over these valence predictions, and describe a probabilistic procedure for choosing to accept or reject model output according to the intended application. We benchmarked our framework against two established datasets and achieved peak classification accuracy of 90%. These results lay the foundation for applications of affective computing in real-world domains such as healthcare, where a high premium is placed on non-invasive collection of data, and predictive certainty.

</details>

<details>

<summary>2020-04-19 18:09:41 - Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly</summary>

- *Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R. Collins, Jeff Schneider, Barnabas Poczos, Eric P. Xing*

- `1903.06694v2` - [abs](http://arxiv.org/abs/1903.06694v2) - [pdf](http://arxiv.org/pdf/1903.06694v2)

> Bayesian Optimisation (BO) refers to a suite of techniques for global optimisation of expensive black box functions, which use introspective Bayesian models of the function to efficiently search for the optimum. While BO has been applied successfully in many applications, modern optimisation tasks usher in new challenges where conventional methods fail spectacularly. In this work, we present Dragonfly, an open source Python library for scalable and robust BO. Dragonfly incorporates multiple recently developed methods that allow BO to be applied in challenging real world settings; these include better methods for handling higher dimensional domains, methods for handling multi-fidelity evaluations when cheap approximations of an expensive function are available, methods for optimising over structured combinatorial spaces, such as the space of neural network architectures, and methods for handling parallel evaluations. Additionally, we develop new methodological improvements in BO for selecting the Bayesian model, selecting the acquisition function, and optimising over complex domains with different variable types and additional constraints. We compare Dragonfly to a suite of other packages and algorithms for global optimisation and demonstrate that when the above methods are integrated, they enable significant improvements in the performance of BO. The Dragonfly library is available at dragonfly.github.io.

</details>

<details>

<summary>2020-04-19 19:32:19 - Parameter estimation with a class of outer probability measures</summary>

- *Jeremie Houssineau*

- `1801.00569v4` - [abs](http://arxiv.org/abs/1801.00569v4) - [pdf](http://arxiv.org/pdf/1801.00569v4)

> We explore the interplay between random and deterministic phenomena using a representation of uncertainty based on the measure-theoretic concept of outer measure. The meaning of the analogues of different probabilistic concepts is investigated and examples of application are given. The novelty of this article lies mainly in the suitability of the tools introduced for jointly representing random and deterministic uncertainty. These tools are shown to yield intuitive results in simple situations and to generalise easily to more complex cases. Connections with Dempster-Shafer theory, the empirical Bayes methods and generalised Bayesian inference are also highlighted.

</details>

<details>

<summary>2020-04-20 03:44:47 - Consistent Calibration of Economic Scenario Generators: The Case for Conditional Simulation</summary>

- *Misha van Beek*

- `2004.09042v1` - [abs](http://arxiv.org/abs/2004.09042v1) - [pdf](http://arxiv.org/pdf/2004.09042v1)

> Economic Scenario Generators (ESGs) simulate economic and financial variables forward in time for risk management and asset allocation purposes. It is often not feasible to calibrate the dynamics of all variables within the ESG to historical data alone. Calibration to forward-information such as future scenarios and return expectations is needed for stress testing and portfolio optimization, but no generally accepted methodology is available. This paper introduces the Conditional Scenario Simulator, which is a framework for consistently calibrating simulations and projections of economic and financial variables both to historical data and forward-looking information. The framework can be viewed as a multi-period, multi-factor generalization of the Black-Litterman model, and can embed a wide array of financial and macroeconomic models. Two practical examples demonstrate this in a frequentist and Bayesian setting.

</details>

<details>

<summary>2020-04-20 14:00:17 - Joint Bayesian Variable and DAG Selection Consistency for High-dimensional Regression Models with Network-structured Covariates</summary>

- *Xuan Cao, Kyoungjae Lee*

- `2004.09306v1` - [abs](http://arxiv.org/abs/2004.09306v1) - [pdf](http://arxiv.org/pdf/2004.09306v1)

> We consider the joint sparse estimation of regression coefficients and the covariance matrix for covariates in a high-dimensional regression model, where the predictors are both relevant to a response variable of interest and functionally related to one another via a Gaussian directed acyclic graph (DAG) model. Gaussian DAG models introduce sparsity in the Cholesky factor of the inverse covariance matrix, and the sparsity pattern in turn corresponds to specific conditional independence assumptions on the underlying predictors. A variety of methods have been developed in recent years for Bayesian inference in identifying such network-structured predictors in regression setting, yet crucial sparsity selection properties for these models have not been thoroughly investigated. In this paper, we consider a hierarchical model with spike and slab priors on the regression coefficients and a flexible and general class of DAG-Wishart distributions with multiple shape parameters on the Cholesky factors of the inverse covariance matrix. Under mild regularity assumptions, we establish the joint selection consistency for both the variable and the underlying DAG of the covariates when the dimension of predictors is allowed to grow much larger than the sample size. We demonstrate that our method outperforms existing methods in selecting network-structured predictors in several simulation settings.

</details>

<details>

<summary>2020-04-20 16:49:04 - A Bayesian Approach to Recurrence in Neural Networks</summary>

- *Philip N. Garner, Sibo Tong*

- `1910.11247v3` - [abs](http://arxiv.org/abs/1910.11247v3) - [pdf](http://arxiv.org/pdf/1910.11247v3)

> We begin by reiterating that common neural network activation functions have simple Bayesian origins. In this spirit, we go on to show that Bayes's theorem also implies a simple recurrence relation; this leads to a Bayesian recurrent unit with a prescribed feedback formulation. We show that introduction of a context indicator leads to a variable feedback that is similar to the forget mechanism in conventional recurrent units. A similar approach leads to a probabilistic input gate. The Bayesian formulation leads naturally to the two pass algorithm of the Kalman smoother or forward-backward algorithm, meaning that inference naturally depends upon future inputs as well as past ones. Experiments on speech recognition confirm that the resulting architecture can perform as well as a bidirectional recurrent network with the same number of parameters as a unidirectional one. Further, when configured explicitly bidirectionally, the architecture can exceed the performance of a conventional bidirectional recurrence.

</details>

<details>

<summary>2020-04-20 17:32:30 - On statistical Calderón problems</summary>

- *Kweku Abraham, Richard Nickl*

- `1906.03486v4` - [abs](http://arxiv.org/abs/1906.03486v4) - [pdf](http://arxiv.org/pdf/1906.03486v4)

> For $D$ a bounded domain in $\mathbb R^d, d \ge 2,$ with smooth boundary $\partial D$, the non-linear inverse problem of recovering the unknown conductivity $\gamma$ determining solutions $u=u_{\gamma, f}$ of the partial differential equation \begin{equation*} \begin{split} \nabla \cdot(\gamma \nabla u)&=0 \quad \text{ in }D, \\ u&=f \quad \text { on } \partial D, \end{split} \end{equation*} from noisy observations $Y$ of the Dirichlet-to-Neumann map \[f \mapsto \Lambda_\gamma(f) = {\gamma \frac{\partial u_{\gamma,f}}{\partial \nu}}\Big|_{\partial D},\] with $\partial/\partial \nu$ denoting the outward normal derivative, is considered. The data $Y$ consists of $\Lambda_\gamma$ corrupted by additive Gaussian noise at noise level $\varepsilon>0$, and a statistical algorithm $\hat \gamma(Y)$ is constructed which is shown to recover $\gamma$ in supremum-norm loss at a statistical convergence rate of the order $\log(1/\varepsilon)^{-\delta}$ as $\varepsilon \to 0$. It is further shown that this convergence rate is optimal, up to the precise value of the exponent $\delta>0$, in an information theoretic sense. The estimator $\hat \gamma(Y)$ has a Bayesian interpretation in terms of the posterior mean of a suitable Gaussian process prior and can be computed by MCMC methods.

</details>

<details>

<summary>2020-04-21 00:52:09 - Markov Decision Processes with Dynamic Transition Probabilities: An Analysis of Shooting Strategies in Basketball</summary>

- *Nathan Sandholtz, Luke Bornn*

- `1812.05170v2` - [abs](http://arxiv.org/abs/1812.05170v2) - [pdf](http://arxiv.org/pdf/1812.05170v2)

> In this paper we model basketball plays as episodes from team-specific non-stationary Markov decision processes (MDPs) with shot clock dependent transition probabilities. Bayesian hierarchical models are employed in the modeling and parametrization of the transition probabilities to borrow strength across players and through time. To enable computational feasibility, we combine lineup-specific MDPs into team-average MDPs using a novel transition weighting scheme. Specifically, we derive the dynamics of the team-average process such that the expected transition count for an arbitrary state-pair is equal to the weighted sum of the expected counts of the separate lineup-specific MDPs.   We then utilize these non-stationary MDPs in the creation of a basketball play simulator with uncertainty propagated via posterior samples of the model components. After calibration, we simulate seasons both on-policy and under altered policies and explore the net changes in efficiency and production under the alternate policies. Additionally, we discuss the game-theoretic ramifications of testing alternative decision policies.

</details>

<details>

<summary>2020-04-21 06:57:55 - Student-at-risk detection by current learning performance indicators using Bayesian networks</summary>

- *T. A. Kustitskaya, A. A. Kytmanov, M. V. Noskov*

- `2004.09774v1` - [abs](http://arxiv.org/abs/2004.09774v1) - [pdf](http://arxiv.org/pdf/2004.09774v1)

> The present article is focused on the problem of prediction of student failures with the purpose of their possible prevention by timely introducing supportive measures. We propose a concept for building a predictive model based on Bayesian networks for an academic course or module taught in a blended learning format. Our empirical studies confirm that the proposed approach is perspective for the development of an early warning system for various stakeholders of the educational process.

</details>

<details>

<summary>2020-04-21 15:11:19 - Bayesian Optimization of Hyperparameters when the Marginal Likelihood is Estimated by MCMC</summary>

- *Oskar Gustafsson, Mattias Villani, Pär Stockhammar*

- `2004.10092v1` - [abs](http://arxiv.org/abs/2004.10092v1) - [pdf](http://arxiv.org/pdf/2004.10092v1)

> Bayesian models often involve a small set of hyperparameters determined by maximizing the marginal likelihood. Bayesian optimization is a popular iterative method where a Gaussian process posterior of the underlying function is sequentially updated by new function evaluations. An acquisition strategy uses this posterior distribution to decide where to place the next function evaluation. We propose a novel Bayesian optimization framework for situations where the user controls the computational effort, and therefore the precision of the function evaluations. This is a common situation in econometrics where the marginal likelihood is often computed by Markov Chain Monte Carlo (MCMC) methods, with the precision of the marginal likelihood estimate determined by the number of MCMC draws. The proposed acquisition strategy gives the optimizer the option to explore the function with cheap noisy evaluations and therefore finds the optimum faster. Prior hyperparameter estimation in the steady-state Bayesian vector autoregressive (BVAR) model on US macroeconomic time series data is used for illustration. The proposed method is shown to find the optimum much quicker than traditional Bayesian optimization or grid search.

</details>

<details>

<summary>2020-04-21 19:58:05 - Stochastic Epidemic Models inference and diagnosis with Poisson Random Measure Data Augmentation</summary>

- *Benjamin Nguyen-Van-Yen, Pierre Del Moral, Bernard Cazelles*

- `2004.10264v1` - [abs](http://arxiv.org/abs/2004.10264v1) - [pdf](http://arxiv.org/pdf/2004.10264v1)

> We present a new Bayesian inference method for compartmental models that takes into account the intrinsic stochasticity of the process. We show how to formulate a SIR-type Markov jump process as the solution of a stochastic differential equation with respect to a Poisson Random Measure (PRM), and how to simulate the process trajectory deterministically from a parameter value and a PRM realisation.   This forms the basis of our Data Augmented MCMC, which consists in augmenting parameter space with the unobserved PRM value. The resulting simple Metropolis-Hastings sampler acts as an efficient simulation-based inference method, that can easily be transferred from model to model. Compared with a recent Data Augmentation method based on Gibbs sampling of individual infection histories, PRM-augmented MCMC scales much better with epidemic size and is far more flexible.   PRM-augmented MCMC also yields a posteriori estimates of the PRM, that represent process stochasticity, and which can be used to validate the model. If the model is good, the posterior distribution should exhibit no pattern and be close to the PRM prior distribution. We illustrate this by fitting a non-seasonal model to some simulated seasonal case count data. Applied to the Zika epidemic of 2013 in French Polynesia, our approach shows that a simple SEIR model cannot correctly reproduce both the initial sharp increase in the number of cases as well as the final proportion of seropositive.   PRM-augmentation thus provides a coherent story for Stochastic Epidemic Model inference, where explicitly inferring process stochasticity helps with model validation.

</details>

<details>

<summary>2020-04-22 04:14:20 - Finite Mixtures of ERGMs for Modeling Ensembles of Networks</summary>

- *Fan Yin, Weining Shen, Carter T. Butts*

- `1910.11445v3` - [abs](http://arxiv.org/abs/1910.11445v3) - [pdf](http://arxiv.org/pdf/1910.11445v3)

> Ensembles of networks arise in many scientific fields, but there are few statistical tools for inferring their generative processes, particularly in the presence of both dyadic dependence and cross-graph heterogeneity. To fill in this gap, we propose characterizing network ensembles via finite mixtures of exponential family random graph models, a framework for parametric statistical modeling of graphs that has been successful in explicitly modeling the complex stochastic processes that govern the structure of edges in a network. Our proposed modeling framework can also be used for applications such as model-based clustering of ensembles of networks and density estimation for complex graph distributions. We develop a Metropolis-within-Gibbs algorithm to conduct fully Bayesian inference and adapt a version of deviance information criterion for missing data models to choose the number of latent heterogeneous generative mechanisms. Simulation studies show that the proposed procedure can recover the true number of latent heterogeneous generative processes and corresponding parameters. We demonstrate the utility of the proposed approach using an ensemble of political co-voting networks among U.S. Senators.

</details>

<details>

<summary>2020-04-22 05:17:36 - Sequential Anomaly Detection using Inverse Reinforcement Learning</summary>

- *Min-hwan Oh, Garud Iyengar*

- `2004.10398v1` - [abs](http://arxiv.org/abs/2004.10398v1) - [pdf](http://arxiv.org/pdf/2004.10398v1)

> One of the most interesting application scenarios in anomaly detection is when sequential data are targeted. For example, in a safety-critical environment, it is crucial to have an automatic detection system to screen the streaming data gathered by monitoring sensors and to report abnormal observations if detected in real-time. Oftentimes, stakes are much higher when these potential anomalies are intentional or goal-oriented. We propose an end-to-end framework for sequential anomaly detection using inverse reinforcement learning (IRL), whose objective is to determine the decision-making agent's underlying function which triggers his/her behavior. The proposed method takes the sequence of actions of a target agent (and possibly other meta information) as input. The agent's normal behavior is then understood by the reward function which is inferred via IRL. We use a neural network to represent a reward function. Using a learned reward function, we evaluate whether a new observation from the target agent follows a normal pattern. In order to construct a reliable anomaly detection method and take into consideration the confidence of the predicted anomaly score, we adopt a Bayesian approach for IRL. The empirical study on publicly available real-world data shows that our proposed method is effective in identifying anomalies.

</details>

<details>

<summary>2020-04-22 11:36:15 - Stochastic Optimal Control as Approximate Input Inference</summary>

- *Joe Watson, Hany Abdulsamad, Jan Peters*

- `1910.03003v2` - [abs](http://arxiv.org/abs/1910.03003v2) - [pdf](http://arxiv.org/pdf/1910.03003v2)

> Optimal control of stochastic nonlinear dynamical systems is a major challenge in the domain of robot learning. Given the intractability of the global control problem, state-of-the-art algorithms focus on approximate sequential optimization techniques, that heavily rely on heuristics for regularization in order to achieve stable convergence. By building upon the duality between inference and control, we develop the view of Optimal Control as Input Estimation, devising a probabilistic stochastic optimal control formulation that iteratively infers the optimal input distributions by minimizing an upper bound of the control cost. Inference is performed through Expectation Maximization and message passing on a probabilistic graphical model of the dynamical system, and time-varying linear Gaussian feedback controllers are extracted from the joint state-action distribution. This perspective incorporates uncertainty quantification, effective initialization through priors, and the principled regularization inherent to the Bayesian treatment. Moreover, it can be shown that for deterministic linearized systems, our framework derives the maximum entropy linear quadratic optimal control law. We provide a complete and detailed derivation of our probabilistic approach and highlight its advantages in comparison to other deterministic and probabilistic solvers.

</details>

<details>

<summary>2020-04-22 12:23:45 - Practical calibration of the temperature parameter in Gibbs posteriors</summary>

- *Lucie Perrotta*

- `2004.10522v1` - [abs](http://arxiv.org/abs/2004.10522v1) - [pdf](http://arxiv.org/pdf/2004.10522v1)

> PAC-Bayesian algorithms and Gibbs posteriors are gaining popularity due to their robustness against model misspecification even when Bayesian inference is inconsistent. The PAC-Bayesian alpha-posterior is a generalization of the standard Bayes posterior which can be tempered with a parameter alpha to handle inconsistency. Data driven methods for tuning alpha have been proposed but are still few, and are often computationally heavy. Additionally, the adequacy of these methods in cases where we use variational approximations instead of exact alpha-posteriors is not clear. This narrows their usage to simple models and prevents their application to large-scale problems. We hence need fast methods to tune alpha that work with both exact and variational alpha-posteriors. First, we propose two data driven methods for tuning alpha, based on sample-splitting and bootstrapping respectively. Second, we formulate the (exact or variational) posteriors of three popular statistical models, and modify them into alpha-posteriors. For each model, we test our strategies and compare them with standard Bayes and Grunwald's SafeBayes. While bootstrapping achieves mixed results, sample-splitting and SafeBayes perform well on the exact and variational alpha-posteriors we describe, and achieve better results than standard Bayes in misspecified or complex models. Additionally, sample-splitting outperforms SafeBayes in terms of speed. Sample-splitting offers a fast and easy solution to inconsistency and typically performs similarly or better than Bayesian inference. Our results provide hints on the calibration of alpha in PAC-Bayesian and Gibbs posteriors, and may facilitate using these methods in large and complex models.

</details>

<details>

<summary>2020-04-22 13:05:21 - An information-theoretic approach to the analysis of location and co-location patterns</summary>

- *Alje van Dam, Andres Gomez-Lievano, Frank Neffke, Koen Frenken*

- `2004.10548v1` - [abs](http://arxiv.org/abs/2004.10548v1) - [pdf](http://arxiv.org/pdf/2004.10548v1)

> We propose a statistical framework to quantify location and co-location associations of economic activities using information-theoretic measures. We relate the resulting measures to existing measures of revealed comparative advantage, localization and specialization and show that they can all be seen as part of the same framework. Using a Bayesian approach, we provide measures of uncertainty of the estimated quantities. Furthermore, the information-theoretic approach can be readily extended to move beyond pairwise co-locations and instead capture multivariate associations. To illustrate the framework, we apply our measures to the co-location of occupations in US cities, showing the associations between different groups of occupations.

</details>

<details>

<summary>2020-04-22 14:14:17 - Error bounds for some approximate posterior measures in Bayesian inference</summary>

- *Han Cheng Lie, T. J. Sullivan, Aretha Teckentrup*

- `1911.05669v2` - [abs](http://arxiv.org/abs/1911.05669v2) - [pdf](http://arxiv.org/pdf/1911.05669v2)

> In certain applications involving the solution of a Bayesian inverse problem, it may not be possible or desirable to evaluate the full posterior, e.g. due to the high computational cost of doing so. This problem motivates the use of approximate posteriors that arise from approximating the data misfit or forward model. We review some error bounds for random and deterministic approximate posteriors that arise when the approximate data misfits and approximate forward models are random.

</details>

<details>

<summary>2020-04-22 19:07:35 - Bayesian nonparametric modeling for predicting dynamic dependencies in multiple object tracking</summary>

- *Bahman Moraffah, Antonia Papndreou-Suppopola*

- `2004.10798v1` - [abs](http://arxiv.org/abs/2004.10798v1) - [pdf](http://arxiv.org/pdf/2004.10798v1)

> Some challenging problems in tracking multiple objects include the time-dependent cardinality, unordered measurements and object parameter labeling. In this paper, we employ Bayesian Bayesian nonparametric methods to address these challenges. In particular, we propose modeling the multiple object parameter state prior using the dependent Dirichlet and Pitman-Yor processes. These nonparametric models have been shown to be more flexible and robust, when compared to existing methods, for estimating the time-varying number of objects, providing object labeling and identifying measurement to object associations. Monte Carlo sampling methods are then proposed to efficiently learn the trajectory of objects from noisy measurements. Using simulations, we demonstrate the estimation performance advantage of the new methods when compared to existing algorithms such as the generalized labeled multi-Bernoulli filter.

</details>

<details>

<summary>2020-04-22 19:30:23 - Assessing and Visualizing Simultaneous Simulation Error</summary>

- *Nathan Robertson, James M. Flegal, Dootika Vats, Galin L. Jones*

- `1904.11912v2` - [abs](http://arxiv.org/abs/1904.11912v2) - [pdf](http://arxiv.org/pdf/1904.11912v2)

> Monte Carlo experiments produce samples in order to estimate features of a given distribution. However, simultaneous estimation of means and quantiles has received little attention, despite being common practice. In this setting we establish a multivariate central limit theorem for any finite combination of sample means and quantiles under the assumption of a strongly mixing process, which includes the standard Monte Carlo and Markov chain Monte Carlo settings. We build on this to provide a fast algorithm for constructing hyperrectangular confidence regions having the desired simultaneous coverage probability and a convenient marginal interpretation. The methods are incorporated into standard ways of visualizing the results of Monte Carlo experiments enabling the practitioner to more easily assess the reliability of the results. We demonstrate the utility of this approach in various Monte Carlo settings including simulation studies based on independent and identically distributed samples and Bayesian analyses using Markov chain Monte Carlo sampling.

</details>

<details>

<summary>2020-04-22 19:51:36 - Utterance-level Sequential Modeling For Deep Gaussian Process Based Speech Synthesis Using Simple Recurrent Unit</summary>

- *Tomoki Koriyama, Hiroshi Saruwatari*

- `2004.10823v1` - [abs](http://arxiv.org/abs/2004.10823v1) - [pdf](http://arxiv.org/pdf/2004.10823v1)

> This paper presents a deep Gaussian process (DGP) model with a recurrent architecture for speech sequence modeling. DGP is a Bayesian deep model that can be trained effectively with the consideration of model complexity and is a kernel regression model that can have high expressibility. In the previous studies, it was shown that the DGP-based speech synthesis outperformed neural network-based one, in which both models used a feed-forward architecture. To improve the naturalness of synthetic speech, in this paper, we show that DGP can be applied to utterance-level modeling using recurrent architecture models. We adopt a simple recurrent unit (SRU) for the proposed model to achieve a recurrent architecture, in which we can execute fast speech parameter generation by using the high parallelization nature of SRU. The objective and subjective evaluation results show that the proposed SRU-DGP-based speech synthesis outperforms not only feed-forward DGP but also automatically tuned SRU- and long short-term memory (LSTM)-based neural networks.

</details>

<details>

<summary>2020-04-23 13:39:17 - Scalable GWR: A linear-time algorithm for large-scale geographically weighted regression with polynomial kernels</summary>

- *Daisuke Murakami, Narumasa Tsutsumida, Takahiro Yoshida, Tomoki Nakaya, Binbin Lu*

- `1905.00266v3` - [abs](http://arxiv.org/abs/1905.00266v3) - [pdf](http://arxiv.org/pdf/1905.00266v3)

> Although a number of studies have developed fast geographically weighted regression (GWR) algorithms for large samples, none of them has achieved linear-time estimation, which is considered a requisite for big data analysis in machine learning, geostatistics, and related domains. Against this backdrop, this study proposes a scalable GWR (ScaGWR) for large datasets. The key improvement is the calibration of the model through a pre-compression of the matrices and vectors whose size depends on the sample size, prior to the leave-one-out cross-validation, which is the heaviest computational step in conventional GWR. This pre-compression allows us to run the proposed GWR extension so that its computation time increases linearly with the sample size. With this improvement, the ScaGWR can be calibrated with one million observations without parallelization. Moreover, the ScaGWR estimator can be regarded as an empirical Bayesian estimator that is more stable than the conventional GWR estimator. We compare the ScaGWR with the conventional GWR in terms of estimation accuracy and computational efficiency using a Monte Carlo simulation. Then, we apply these methods to a US income analysis. The code for ScaGWR is available in the R package scgwr. The code is embedded into C++ code and implemented in another R package, GWmodel.

</details>

<details>

<summary>2020-04-23 17:38:23 - Estimating the number of infections and the impact of non-pharmaceutical interventions on COVID-19 in European countries: technical description update</summary>

- *Seth Flaxman, Swapnil Mishra, Axel Gandy, H Juliette T Unwin, Helen Coupland, Thomas A Mellan, Harrison Zhu, Tresnia Berah, Jeffrey W Eaton, Pablo N P Guzman, Nora Schmit, Lucia Callizo, Imperial College COVID-19 Response Team, Charles Whittaker, Peter Winskill, Xiaoyue Xi, Azra Ghani, Christl A. Donnelly, Steven Riley, Lucy C Okell, Michaela A C Vollmer, Neil M. Ferguson, Samir Bhatt*

- `2004.11342v1` - [abs](http://arxiv.org/abs/2004.11342v1) - [pdf](http://arxiv.org/pdf/2004.11342v1)

> Following the emergence of a novel coronavirus (SARS-CoV-2) and its spread outside of China, Europe has experienced large epidemics. In response, many European countries have implemented unprecedented non-pharmaceutical interventions including case isolation, the closure of schools and universities, banning of mass gatherings and/or public events, and most recently, wide-scale social distancing including local and national lockdowns.   In this technical update, we extend a semi-mechanistic Bayesian hierarchical model that infers the impact of these interventions and estimates the number of infections over time. Our methods assume that changes in the reproductive number - a measure of transmission - are an immediate response to these interventions being implemented rather than broader gradual changes in behaviour. Our model estimates these changes by calculating backwards from temporal data on observed to estimate the number of infections and rate of transmission that occurred several weeks prior, allowing for a probabilistic time lag between infection and death.   In this update we extend our original model [Flaxman, Mishra, Gandy et al 2020, Report #13, Imperial College London] to include (a) population saturation effects, (b) prior uncertainty on the infection fatality ratio, (c) a more balanced prior on intervention effects and (d) partial pooling of the lockdown intervention covariate. We also (e) included another 3 countries (Greece, the Netherlands and Portugal).   The model code is available at https://github.com/ImperialCollegeLondon/covid19model/   We are now reporting the results of our updated model online at https://mrc-ide.github.io/covid19estimates/   We estimated parameters jointly for all M=14 countries in a single hierarchical model. Inference is performed in the probabilistic programming language Stan using an adaptive Hamiltonian Monte Carlo (HMC) sampler.

</details>

<details>

<summary>2020-04-23 23:10:04 - High-dimensional macroeconomic forecasting using message passing algorithms</summary>

- *Dimitris Korobilis*

- `2004.11485v1` - [abs](http://arxiv.org/abs/2004.11485v1) - [pdf](http://arxiv.org/pdf/2004.11485v1)

> This paper proposes two distinct contributions to econometric analysis of large information sets and structural instabilities. First, it treats a regression model with time-varying coefficients, stochastic volatility and exogenous predictors, as an equivalent high-dimensional static regression problem with thousands of covariates. Inference in this specification proceeds using Bayesian hierarchical priors that shrink the high-dimensional vector of coefficients either towards zero or time-invariance. Second, it introduces the frameworks of factor graphs and message passing as a means of designing efficient Bayesian estimation algorithms. In particular, a Generalized Approximate Message Passing (GAMP) algorithm is derived that has low algorithmic complexity and is trivially parallelizable. The result is a comprehensive methodology that can be used to estimate time-varying parameter regressions with arbitrarily large number of exogenous predictors. In a forecasting exercise for U.S. price inflation this methodology is shown to work very well.

</details>

<details>

<summary>2020-04-23 23:15:33 - Machine Learning Econometrics: Bayesian algorithms and methods</summary>

- *Dimitris Korobilis, Davide Pettenuzzo*

- `2004.11486v1` - [abs](http://arxiv.org/abs/2004.11486v1) - [pdf](http://arxiv.org/pdf/2004.11486v1)

> As the amount of economic and other data generated worldwide increases vastly, a challenge for future generations of econometricians will be to master efficient algorithms for inference in empirical models with large information sets. This Chapter provides a review of popular estimation algorithms for Bayesian inference in econometrics and surveys alternative algorithms developed in machine learning and computing science that allow for efficient computation in high-dimensional settings. The focus is on scalability and parallelizability of each algorithm, as well as their ability to be adopted in various empirical settings in economics and finance.

</details>

<details>

<summary>2020-04-24 16:25:00 - Increasing the efficiency of Sequential Monte Carlo samplers through the use of approximately optimal L-kernels</summary>

- *Peter L Green, Robert E Moore, Ryan J Jackson, Jinglai Li, Simon Maskell*

- `2004.12838v1` - [abs](http://arxiv.org/abs/2004.12838v1) - [pdf](http://arxiv.org/pdf/2004.12838v1)

> By facilitating the generation of samples from arbitrary probability distributions, Markov Chain Monte Carlo (MCMC) is, arguably, \emph{the} tool for the evaluation of Bayesian inference problems that yield non-standard posterior distributions. In recent years, however, it has become apparent that Sequential Monte Carlo (SMC) samplers have the potential to outperform MCMC in a number of ways. SMC samplers are better suited to highly parallel computing architectures and also feature various tuning parameters that are not available to MCMC. One such parameter - the `L-kernel' - is a user-defined probability distribution that can be used to influence the efficiency of the sampler. In the current paper, the authors explain how to derive an expression for the L-kernel that minimises the variance of the estimates realised by an SMC sampler. Various approximation methods are then proposed to aid implementation of the proposed L-kernel. The improved performance of the resulting algorithm is demonstrated in multiple scenarios. For the examples shown in the current paper, the use of an approximately optimum L-kernel has reduced the variance of the SMC estimates by up to 99 % while also reducing the number of times that resampling was required by between 65 % and 70 %. Python code and code tests accompanying this manuscript are available through the Github repository \url{https://github.com/plgreenLIRU/SMC_approx_optL}.

</details>

<details>

<summary>2020-04-24 16:47:22 - A Bayesian Multilevel Random-Effects Model for Estimating Noise in Image Sensors</summary>

- *Gabriel Riutort-Mayol, Virgilio Gómez-Rubio, Ángel Marqués-Mateu, José Luis Lerma, Antonio López-Quílez*

- `2004.11849v1` - [abs](http://arxiv.org/abs/2004.11849v1) - [pdf](http://arxiv.org/pdf/2004.11849v1)

> Sensor noise sources cause differences in the signal recorded across pixels in a single image and across multiple images. This paper presents a Bayesian approach to decomposing and characterizing the sensor noise sources involved in imaging with digital cameras. A Bayesian probabilistic model based on the (theoretical) model for noise sources in image sensing is fitted to a set of a time-series of images with different reflectance and wavelengths under controlled lighting conditions. The image sensing model is a complex model, with several interacting components dependent on reflectance and wavelength. The properties of the Bayesian approach of defining conditional dependencies among parameters in a fully probabilistic model, propagating all sources of uncertainty in inference, makes the Bayesian modeling framework more attractive and powerful than classical methods for approaching the image sensing model. A feasible correspondence of noise parameters to their expected theoretical behaviors and well calibrated posterior predictive distributions with a small root mean square error for model predictions have been achieved in this study, thus showing that the proposed model accurately approximates the image sensing model. The Bayesian approach could be extended to formulate further components aimed at identifying even more specific parameters of the imaging process.

</details>

<details>

<summary>2020-04-25 08:14:03 - A Deeper Look at the Unsupervised Learning of Disentangled Representations in $β$-VAE from the Perspective of Core Object Recognition</summary>

- *Harshvardhan Sikka*

- `2005.07114v1` - [abs](http://arxiv.org/abs/2005.07114v1) - [pdf](http://arxiv.org/pdf/2005.07114v1)

> The ability to recognize objects despite there being differences in appearance, known as Core Object Recognition, forms a critical part of human perception. While it is understood that the brain accomplishes Core Object Recognition through feedforward, hierarchical computations through the visual stream, the underlying algorithms that allow for invariant representations to form downstream is still not well understood. (DiCarlo et al., 2012) Various computational perceptual models have been built to attempt and tackle the object identification task in an artificial perceptual setting. Artificial Neural Networks, computational graphs consisting of weighted edges and mathematical operations at vertices, are loosely inspired by neural networks in the brain and have proven effective at various visual perceptual tasks, including object characterization and identification. (Pinto et al., 2008) (DiCarlo et al., 2012) For many data analysis tasks, learning representations where each dimension is statistically independent and thus disentangled from the others is useful. If the underlying generative factors of the data are also statistically independent, Bayesian inference of latent variables can form disentangled representations. This thesis constitutes a research project exploring a generalization of the Variational Autoencoder (VAE), $\beta$-VAE, that aims to learn disentangled representations using variational inference. $\beta$-VAE incorporates the hyperparameter $\beta$, and enforces conditional independence of its bottleneck neurons, which is in general not compatible with the statistical independence of latent variables. This text examines this architecture, and provides analytical and numerical arguments, with the goal of demonstrating that this incompatibility leads to a non-monotonic inference performance in $\beta$-VAE with a finite optimal $\beta$.

</details>

<details>

<summary>2020-04-25 14:42:13 - A Bayesian machine scientist to aid in the solution of challenging scientific problems</summary>

- *Roger Guimera, Ignasi Reichardt, Antoni Aguilar-Mogas, Francesco A Massucci, Manuel Miranda, Jordi Pallares, Marta Sales-Pardo*

- `2004.12157v1` - [abs](http://arxiv.org/abs/2004.12157v1) - [pdf](http://arxiv.org/pdf/2004.12157v1)

> Closed-form, interpretable mathematical models have been instrumental for advancing our understanding of the world; with the data revolution, we may now be in a position to uncover new such models for many systems from physics to the social sciences. However, to deal with increasing amounts of data, we need "machine scientists" that are able to extract these models automatically from data. Here, we introduce a Bayesian machine scientist, which establishes the plausibility of models using explicit approximations to the exact marginal posterior over models and establishes its prior expectations about models by learning from a large empirical corpus of mathematical expressions. It explores the space of models using Markov chain Monte Carlo. We show that this approach uncovers accurate models for synthetic and real data and provides out-of-sample predictions that are more accurate than those of existing approaches and of other nonparametric methods.

</details>

<details>

<summary>2020-04-25 19:21:14 - Learning to Guide Random Search</summary>

- *Ozan Sener, Vladlen Koltun*

- `2004.12214v1` - [abs](http://arxiv.org/abs/2004.12214v1) - [pdf](http://arxiv.org/pdf/2004.12214v1)

> We are interested in derivative-free optimization of high-dimensional functions. The sample complexity of existing methods is high and depends on problem dimensionality, unlike the dimensionality-independent rates of first-order methods. The recent success of deep learning suggests that many datasets lie on low-dimensional manifolds that can be represented by deep nonlinear models. We therefore consider derivative-free optimization of a high-dimensional function that lies on a latent low-dimensional manifold. We develop an online learning approach that learns this manifold while performing the optimization. In other words, we jointly learn the manifold and optimize the function. Our analysis suggests that the presented method significantly reduces sample complexity. We empirically evaluate the method on continuous optimization benchmarks and high-dimensional continuous control problems. Our method achieves significantly lower sample complexity than Augmented Random Search, Bayesian optimization, covariance matrix adaptation (CMA-ES), and other derivative-free optimization algorithms.

</details>

<details>

<summary>2020-04-25 21:49:52 - A sparse negative binomial mixture model for clustering RNA-seq count data</summary>

- *Tanbin Rahman, Yujia Li, Tianzhou Ma, Lu Tang, George Tseng*

- `1912.02399v2` - [abs](http://arxiv.org/abs/1912.02399v2) - [pdf](http://arxiv.org/pdf/1912.02399v2)

> Clustering with variable selection is a challenging yet critical task for modern small-n-large-p data. Existing methods based on sparse Gaussian mixture models or sparse K-means provide solutions to continuous data. With the prevalence of RNA-seq technology and lack of count data modeling for clustering, the current practice is to normalize count expression data into continuous measures and apply existing models with Gaussian assumption. In this paper, we develop a negative binomial mixture model with lasso or fused lasso gene regularization to cluster samples (small n) with high-dimensional gene features (large p). EM algorithm and Bayesian information criterion are used for inference and determining tuning parameters. The method is compared with existing methods using extensive simulations and two real transcriptomic applications in rat brain and breast cancer studies. The result shows superior performance of the proposed count data model in clustering accuracy, feature selection and biological interpretation in pathways.

</details>

<details>

<summary>2020-04-26 09:41:47 - What did we learn from forty years of research on semantic interference? A Bayesian metaanalysis</summary>

- *A. Bürki, S. Elbuy, S. Madec, S. Vasishth*

- `2004.05895v2` - [abs](http://arxiv.org/abs/2004.05895v2) - [pdf](http://arxiv.org/pdf/2004.05895v2)

> When participants in an experiment have to name pictures while ignoring distractor words superimposed on the picture or presented auditorily (i.e., picture-word interference paradigm), they take more time when the word to be named (or target) and distractor words are from the same semantic category (e.g., cat-dog). This experimental effect is known as the semantic interference effect, and is probably one of the most studied in the language production literature. The functional origin of the effect and the exact conditions in which it occurs are however still debated. Since Lupker reported the effect in the first response time experiment about 40 years ago, more than 300 similar experiments have been conducted. The semantic interference effect was replicated in many experiments, but several studies also reported the absence of an effect in a subset of experimental conditions. The aim of the present study is to provide a comprehensive theoretical review of the existing evidence to date and several Bayesian meta-analyses and meta-regressions to determine the size of the effect and explore the experimental conditions in which the effect surfaces. The results are discussed in the light of current debates about the functional origin of the semantic interference effect and its implications for our understanding of the language production system.

</details>

<details>

<summary>2020-04-26 12:05:48 - pexm: a JAGS module for applications involving the piecewise exponential distribution</summary>

- *Vinícius D. Mayrink, João Daniel N. Duarte, Fábio N. Demarqui*

- `2004.12359v1` - [abs](http://arxiv.org/abs/2004.12359v1) - [pdf](http://arxiv.org/pdf/2004.12359v1)

> In this study, we present a new module built for users interested in a programming language similar to BUGS to fit a Bayesian model based on the piecewise exponential (PE) distribution. The module is an extension to the open-source program JAGS by which a Gibbs sampler can be applied without requiring the derivation of complete conditionals and the subsequent implementation of strategies to draw samples from unknown distributions. The PE distribution is widely used in the fields of survival analysis and reliability. Currently, it can only be implemented in JAGS through methods to indirectly specify the likelihood based on the Poisson or Bernoulli probabilities. Our module provides a more straightforward implementation and is thus more attractive to the researchers aiming to spend more time exploring the results from the Bayesian inference rather than implementing the Markov Chain Monte Carlo (MCMC) algorithm. For those interested in extending JAGS, this work can be seen as a tutorial including important information not well investigated or organized in other materials. Here, we describe how to use the module taking advantage of the interface between R and JAGS. A short simulation study is developed to ensure that the module behaves well and a real illustration, involving two PE models, exhibits a context where the module can be used in practice.

</details>

<details>

<summary>2020-04-26 18:08:56 - Deep Structured Mixtures of Gaussian Processes</summary>

- *Martin Trapp, Robert Peharz, Franz Pernkopf, Carl E. Rasmussen*

- `1910.04536v2` - [abs](http://arxiv.org/abs/1910.04536v2) - [pdf](http://arxiv.org/pdf/1910.04536v2)

> Gaussian Processes (GPs) are powerful non-parametric Bayesian regression models that allow exact posterior inference, but exhibit high computational and memory costs. In order to improve scalability of GPs, approximate posterior inference is frequently employed, where a prominent class of approximation techniques is based on local GP experts. However, local-expert techniques proposed so far are either not well-principled, come with limited approximation guarantees, or lead to intractable models. In this paper, we introduce deep structured mixtures of GP experts, a stochastic process model which i) allows exact posterior inference, ii) has attractive computational and memory costs, and iii) when used as GP approximation, captures predictive uncertainties consistently better than previous expert-based approximations. In a variety of experiments, we show that deep structured mixtures have a low approximation error and often perform competitive or outperform prior work.

</details>

<details>

<summary>2020-04-27 02:45:38 - Neural Network and Particle Filtering: A Hybrid Framework for Crack Propagation Prediction</summary>

- *Seyed Fouad Karimian, Ramin Moradi, Sergio Cofre-Martel, Katrina M. Groth, Mohammad Modarres*

- `2004.13556v1` - [abs](http://arxiv.org/abs/2004.13556v1) - [pdf](http://arxiv.org/pdf/2004.13556v1)

> Crack detection, length estimation, and Remaining Useful Life (RUL) prediction are among the most studied topics in reliability engineering. Several research efforts have studied physics of failure (PoF) of different materials, along with data-driven approaches as an alternative to the traditional PoF studies. To bridge the gap between these two techniques, we propose a novel hybrid framework for fatigue crack length estimation and prediction. Physics-based modeling is performed on the fracture mechanics degradation data by estimating parameters of the Paris Law, including the associated uncertainties. Crack length estimations are inferred by feeding manually extracted features from ultrasonic signals to a Neural Network (NN). The crack length prediction is then performed using the Particle Filter (PF) approach, which takes the Paris Law as a move function and uses the NN's output as observation to update the crack growth path. This hybrid framework combines machine learning, physics-based modeling, and Bayesian updating with promising results.

</details>

<details>

<summary>2020-04-27 03:42:54 - Schwartz type model selection for ergodic stochastic differential equation models</summary>

- *Shoichi Eguchi, Yuma Uehara*

- `1904.12398v4` - [abs](http://arxiv.org/abs/1904.12398v4) - [pdf](http://arxiv.org/pdf/1904.12398v4)

> We study the construction of the theoretical foundation of model comparison for ergodic stochastic differential equation (SDE) models and an extension of the applicable scope of the conventional Bayesian information criterion. Different from previous studies, we suppose that the candidate models are possibly misspecified models, and we consider both Wiener and a pure-jump L\'{e}vy noise driven SDE. Based on the asymptotic behavior of the marginal quasi-log likelihood, the Schwarz type statistics and stepwise model selection procedure are proposed. We also prove the model selection consistency of the proposed statistics with respect to an optimal model. We conduct some numerical experiments and they support our theoretical findings.

</details>

<details>

<summary>2020-04-27 12:22:31 - A Bayesian dose-response meta-analysis model: simulation study and application</summary>

- *Tasnim Hamza, Andrea Cipriani, Toshi A. Furukawa, Matthias Egger, Nicola Orsini, Georgia Salanti*

- `2004.12737v1` - [abs](http://arxiv.org/abs/2004.12737v1) - [pdf](http://arxiv.org/pdf/2004.12737v1)

> Dose-response models express the effect of different dose or exposure levels on a specific outcome. In meta-analysis, where aggregated-level data is available, dose-response evidence is synthesized using either one-stage or two-stage models in a frequentist setting. We propose a hierarchical dose-response model implemented in a Bayesian framework. We present the model with cubic dose-response shapes for a dichotomous outcome and take into account heterogeneity due to variability in the dose-response shape. We develop our Bayesian model assuming normal or binomial likelihood and accounting for exposures grouped in clusters. We implement these models in R using JAGS and we compare our approach to the one-stage dose-response meta-analysis model in a simulation study. We found that the Bayesian dose-response model with binomial likelihood has slightly lower bias than the Bayesian model with the normal likelihood and the frequentist one-stage model. However, all three models perform very well and give practically identical results. We also re-analyze the data from 60 randomized controlled trials (15,984 participants) examining the efficacy (response) of various doses of antidepressant drugs. All models suggest that the dose-response curve increases between zero dose and 40 mg of fluoxetine-equivalent dose, and thereafter is constant. We draw the same conclusion when we take into account the fact that five different antidepressants have been studied in the included trials. We show that implementation of the hierarchical model in Bayesian framework has similar performance to, but overcomes some of the limitations of the frequentist approaches and offers maximum flexibility to accommodate features of the data.

</details>

<details>

<summary>2020-04-27 12:58:26 - Retrospective analysis of a fatal dose-finding trial</summary>

- *David C. Norris*

- `2004.12755v1` - [abs](http://arxiv.org/abs/2004.12755v1) - [pdf](http://arxiv.org/pdf/2004.12755v1)

> The commonplace description of phase 1 clinical trials in oncology as "primarily concerned with safety" is belied by their near universal adoption of dose-escalation practices which are inherently unsafe. In contrast with dose titration, cohort-wise dose escalation regards patients as exchangeable, an indefensible assumption in the face of widely appreciated inter-individual heterogeneity in pharmacokinetics and pharmacodynamics (PKPD). I have previously advanced this argument in terms of a precautionary coherence principle that brings the well-known coherence notion of Cheung (2005) into contact with modern imperatives of patient-centeredness and precision dosing. Here, however, I explore these matters in some mechanistic detail by analyzing a trial of the bispecific T cell engager AFM11, in which a fatal toxicity occurred. To this end, I develop a Bayesian dose-response model for a single ordinal toxicity. By constructing this model's priors to align with the AFM11 trial as designed and conducted, I demonstrate the incompatibility of that design with any reasonable expectation of safety. Indeed, the model readily yields prospective estimates of toxic response probabilities that suggest the fatality in this trial could have been foreseen as likely.

</details>

<details>

<summary>2020-04-27 18:06:04 - DYNOTEARS: Structure Learning from Time-Series Data</summary>

- *Roxana Pamfil, Nisara Sriwattanaworachai, Shaan Desai, Philip Pilgerstorfer, Paul Beaumont, Konstantinos Georgatzis, Bryon Aragam*

- `2002.00498v2` - [abs](http://arxiv.org/abs/2002.00498v2) - [pdf](http://arxiv.org/pdf/2002.00498v2)

> We revisit the structure learning problem for dynamic Bayesian networks and propose a method that simultaneously estimates contemporaneous (intra-slice) and time-lagged (inter-slice) relationships between variables in a time-series. Our approach is score-based, and revolves around minimizing a penalized loss subject to an acyclicity constraint. To solve this problem, we leverage a recent algebraic result characterizing the acyclicity constraint as a smooth equality constraint. The resulting algorithm, which we call DYNOTEARS, outperforms other methods on simulated data, especially in high-dimensions as the number of variables increases. We also apply this algorithm on real datasets from two different domains, finance and molecular biology, and analyze the resulting output. Compared to state-of-the-art methods for learning dynamic Bayesian networks, our method is both scalable and accurate on real data. The simple formulation and competitive performance of our method make it suitable for a variety of problems where one seeks to learn connections between variables across time.

</details>

<details>

<summary>2020-04-27 19:10:29 - A Bayesian approach to regional decadal predictability: Sparse parameter estimation in high-dimensional linear inverse models of high-latitude sea surface temperature variability</summary>

- *Dallas Foster, Darin Comeau, Nathan M. Urban*

- `2004.13105v1` - [abs](http://arxiv.org/abs/2004.13105v1) - [pdf](http://arxiv.org/pdf/2004.13105v1)

> Stochastic reduced models are an important tool in climate systems whose many spatial and temporal scales cannot be fully discretized or underlying physics may not be fully accounted for. One form of reduced model, the linear inverse model (LIM), has been widely used for regional climate predictability studies - typically focusing more on tropical or mid-latitude studies. However, most LIM fitting techniques rely on point estimation techniques deriving from fluctuation-dissipation theory. In this methodological study we explore the use of Bayesian inference techniques for LIM parameter estimation of sea surface temperature (SST), to quantify the skillful decadal predictability of Bayesian LIM models at high latitudes. We show that Bayesian methods, when compared to traditional point estimation methods for LIM-type models, provide better calibrated probabilistic skill, while simultaneously providing better point estimates due to the regularization effect of the prior distribution in high-dimensional problems. We compare the effect of several priors, as well as maximum likelihood estimates, on (1) estimating parameter values on a perfect model experiment and (2) producing calibrated 1-year SST anomaly forecast distributions using a pre-industrial control run of the Community Earth System Model (CESM). Finally, we employ a host of probabilistic skill metrics to determine the extent to which a LIM can forecast SST anomalies at high latitudes. We find that the choice of prior distribution has an appreciable impact on estimation outcomes, and priors that emphasize physically relevant properties enhance the model's ability to capture variability of SST anomalies.

</details>

<details>

<summary>2020-04-27 19:31:20 - Using reference models in variable selection</summary>

- *Federico Pavone, Juho Piironen, Paul-Christian Bürkner, Aki Vehtari*

- `2004.13118v1` - [abs](http://arxiv.org/abs/2004.13118v1) - [pdf](http://arxiv.org/pdf/2004.13118v1)

> Variable selection, or more generally, model reduction is an important aspect of the statistical workflow aiming to provide insights from data. In this paper, we discuss and demonstrate the benefits of using a reference model in variable selection. A reference model acts as a noise-filter on the target variable by modeling its data generating mechanism. As a result, using the reference model predictions in the model selection procedure reduces the variability and improves stability leading to improved model selection performance. Assuming that a Bayesian reference model describes the true distribution of future data well, the theoretically preferred usage of the reference model is to project its predictive distribution to a reduced model leading to projection predictive variable selection approach. Alternatively, reference models may also be used in an ad-hoc manner in combination with common variable selection methods. In several numerical experiments, we investigate the performance of the projective prediction approach as well as alternative variable selection methods with and without reference models. Our results indicate that the use of reference models generally translates into better and more stable variable selection. Additionally, we demonstrate that the projection predictive approach shows superior performance as compared to alternative variable selection methods independently of whether or not they use reference models.

</details>

<details>

<summary>2020-04-27 21:34:33 - Nonstationary Bayesian modeling for a large data set of derived surface temperature return values</summary>

- *Mark Risser*

- `2005.03658v1` - [abs](http://arxiv.org/abs/2005.03658v1) - [pdf](http://arxiv.org/pdf/2005.03658v1)

> Heat waves resulting from prolonged extreme temperatures pose a significant risk to human health globally. Given the limitations of observations of extreme temperature, climate models are often used to characterize extreme temperature globally, from which one can derive quantities like return values to summarize the magnitude of a low probability event for an arbitrary geographic location. However, while these derived quantities are useful on their own, it is also often important to apply a spatial statistical model to such data in order to, e.g., understand how the spatial dependence properties of the return values vary over space and emulate the climate model for generating additional spatial fields with corresponding statistical properties. For these objectives, when modeling global data it is critical to use a nonstationary covariance function. Furthermore, given that the output of modern global climate models can be on the order of $\mathcal{O}(10^4)$, it is important to utilize approximate Gaussian process methods to enable inference. In this paper, we demonstrate the application of methodology introduced in Risser and Turek (2020) to conduct a nonstationary and fully Bayesian analysis of a large data set of 20-year return values derived from an ensemble of global climate model runs with over 50,000 spatial locations. This analysis uses the freely available BayesNSGP software package for R.

</details>

<details>

<summary>2020-04-28 09:28:08 - The Immersion of Directed Multi-graphs in Embedding Fields. Generalisations</summary>

- *Bogdan Bocse, Ioan Radu Jinga*

- `2004.13384v1` - [abs](http://arxiv.org/abs/2004.13384v1) - [pdf](http://arxiv.org/pdf/2004.13384v1)

> The purpose of this paper is to outline a generalised model for representing hybrids of relational-categorical, symbolic, perceptual-sensory and perceptual-latent data, so as to embody, in the same architectural data layer, representations for the input, output and latent tensors. This variety of representation is currently used by various machine-learning models in computer vision, NLP/NLU, reinforcement learning which allows for direct application of cross-domain queries and functions. This is achieved by endowing a directed Tensor-Typed Multi-Graph with at least some edge attributes which represent the embeddings from various latent spaces, so as to define, construct and compute new similarity and distance relationships between and across tensorial forms, including visual, linguistic, auditory latent representations, thus stitching the logical-categorical view of the observed universe to the Bayesian/statistical view.

</details>

<details>

<summary>2020-04-28 11:41:13 - The e-value: A Fully Bayesian Significance Measure for Precise Statistical Hypotheses and its Research Program</summary>

- *Julio Michael Stern, Carlos Alberto de Braganca Pereira*

- `2001.10577v3` - [abs](http://arxiv.org/abs/2001.10577v3) - [pdf](http://arxiv.org/pdf/2001.10577v3)

> This article gives a survey of the e-value, a statistical significance measure a.k.a. the evidence rendered by observational data, X, in support of a statistical hypothesis, H, or, the other way around, the epistemic value of H given X. The $e$-value and the accompanying FBST, the Full Bayesian Significance Test, constitute the core of a research program that was started at IME-USP, is being developed by over 20 researchers worldwide, and has, so far, been referenced by over 200 publications.   The e-value and the FBST comply with the best principles of Bayesian inference, including the likelihood principle, complete invariance, asymptotic consistency, etc. Furthermore, they exhibit powerful logic or algebraic properties in situations where one needs to compare or compose distinct hypotheses that can be formulated either in the same or in different statistical models. Moreover, they effortlessly accommodate the case of sharp or precise hypotheses, a situation where alternative methods often require ad hoc and convoluted procedures. Finally, the FBST has outstanding robustness and reliability characteristics, outperforming traditional tests of hypotheses in many practical applications of statistical modeling and operations research.

</details>

<details>

<summary>2020-04-28 13:20:47 - Predicting Infection of COVID-19 in Japan: State Space Modeling Approach</summary>

- *Genya Kobayashi, Shonosuke Sugasawa, Hiromasa Tamae, Takayuki Ozu*

- `2004.13483v1` - [abs](http://arxiv.org/abs/2004.13483v1) - [pdf](http://arxiv.org/pdf/2004.13483v1)

> The number of confirmed cases of the coronavirus disease (COVID-19) in Japan has been increasing day by day and has had a serious impact on the society especially after the declaration of the state of emergency on April 7, 2020. This study analyzes the real time data from March 1 to April 22, 2020 by adopting a sophisticated statistical modeling tool based on the state space model combined with the well-known susceptible-exposed-infected (SIR) model. The model estimation and forecasting are conducted using the Bayesian methodology. The present study provides the parameter estimates of the unknown parameters that critically determine the epidemic process derived from the SIR model and prediction of the future transition of the infectious proportion including the size and timing of the epidemic peak with the prediction intervals that naturally accounts for the uncertainty. The prediction results under various scenarios reveals that the temporary reduction in the infection rate until the planned lifting of the state on May 6 will only delay the epidemic peak slightly. In order to minimize the spread of the epidemic, it is strongly suggested that an intervention is carried out for an extended period of time and that the government and individuals make a long term effort to reduce the infection rate even after the lifting.

</details>

<details>

<summary>2020-04-28 13:30:12 - A novel algorithmic approach to Bayesian Logic Regression</summary>

- *Aliaksandr Hubin, Geir Storvik, Florian Frommlet*

- `1705.07616v3` - [abs](http://arxiv.org/abs/1705.07616v3) - [pdf](http://arxiv.org/pdf/1705.07616v3)

> Logic regression was developed more than a decade ago as a tool to construct predictors from Boolean combinations of binary covariates. It has been mainly used to model epistatic effects in genetic association studies, which is very appealing due to the intuitive interpretation of logic expressions to describe the interaction between genetic variations. Nevertheless logic regression has (partly due to computational challenges) remained less well known than other approaches to epistatic association mapping. Here we will adapt an advanced evolutionary algorithm called GMJMCMC (Genetically modified Mode Jumping Markov Chain Monte Carlo) to perform Bayesian model selection in the space of logic regression models. After describing the algorithmic details of GMJMCMC we perform a comprehensive simulation study that illustrates its performance given logic regression terms of various complexity. Specifically GMJMCMC is shown to be able to identify three-way and even four-way interactions with relatively large power, a level of complexity which has not been achieved by previous implementations of logic regression. We apply GMJMCMC to reanalyze QTL mapping data for Recombinant Inbred Lines in \textit{Arabidopsis thaliana} and from a backcross population in \textit{Drosophila} where we identify several interesting epistatic effects. The method is implemented in an R package which is available on github.

</details>

<details>

<summary>2020-04-28 13:47:21 - Interpreting Deep Neural Networks Through Variable Importance</summary>

- *Jonathan Ish-Horowicz, Dana Udwin, Seth Flaxman, Sarah Filippi, Lorin Crawford*

- `1901.09839v3` - [abs](http://arxiv.org/abs/1901.09839v3) - [pdf](http://arxiv.org/pdf/1901.09839v3)

> While the success of deep neural networks (DNNs) is well-established across a variety of domains, our ability to explain and interpret these methods is limited. Unlike previously proposed local methods which try to explain particular classification decisions, we focus on global interpretability and ask a universally applicable question: given a trained model, which features are the most important? In the context of neural networks, a feature is rarely important on its own, so our strategy is specifically designed to leverage partial covariance structures and incorporate variable dependence into feature ranking. Our methodological contributions in this paper are two-fold. First, we propose an effect size analogue for DNNs that is appropriate for applications with highly collinear predictors (ubiquitous in computer vision). Second, we extend the recently proposed "RelATive cEntrality" (RATE) measure (Crawford et al., 2019) to the Bayesian deep learning setting. RATE applies an information theoretic criterion to the posterior distribution of effect sizes to assess feature significance. We apply our framework to three broad application areas: computer vision, natural language processing, and social science.

</details>

<details>

<summary>2020-04-28 17:44:50 - A Bayesian binomial regression model with latent Gaussian processes for modelling DNA methylation</summary>

- *Aliaksandr Hubin, Geir O Storvik, Paul E Grini, Melinka A Butenko*

- `2004.13689v1` - [abs](http://arxiv.org/abs/2004.13689v1) - [pdf](http://arxiv.org/pdf/2004.13689v1)

> Epigenetic observations are represented by the total number of reads from a given pool of cells and the number of methylated reads, making it reasonable to model this data by a binomial distribution. There are numerous factors that can influence the probability of success in a particular region. Moreover, there is a strong spatial (alongside the genome) dependence of these probabilities. We incorporate dependence on the covariates and the spatial dependence of the methylation probability for observations from a pool of cells by means of a binomial regression model with a latent Gaussian field and a logit link function. We apply a Bayesian approach including prior specifications on model configurations. We run a mode jumping Markov chain Monte Carlo algorithm (MJMCMC) across different choices of covariates in order to obtain the joint posterior distribution of parameters and models. This also allows finding the best set of covariates to model methylation probability within the genomic region of interest and individual marginal inclusion probabilities of the covariates.

</details>

<details>

<summary>2020-04-28 20:11:21 - Privacy for Spatial Point Process Data</summary>

- *Adam Walder, Ephraim M. Hanks, Aleksandra Slavković*

- `2003.12816v2` - [abs](http://arxiv.org/abs/2003.12816v2) - [pdf](http://arxiv.org/pdf/2003.12816v2)

> In this work we develop methods for privatizing spatial location data, such as spatial locations of individual disease cases. We propose two novel Bayesian methods for generating synthetic location data based on log-Gaussian Cox processes (LGCPs). We show that conditional predictive ordinate (CPO) estimates can easily be obtained for point process data. We construct a novel risk metric that utilizes CPO estimates to evaluate individual disclosure risks. We adapt the propensity mean square error (pMSE) data utility metric for LGCPs. We demonstrate that our synthesis methods offer an improved risk vs. utility balance in comparison to radial synthesis with a case study of Dr. John Snow's cholera outbreak data.

</details>

<details>

<summary>2020-04-28 22:39:27 - Bayesian Model Selection on Random Networks</summary>

- *Papamichalis Marios*

- `2004.13880v1` - [abs](http://arxiv.org/abs/2004.13880v1) - [pdf](http://arxiv.org/pdf/2004.13880v1)

> A general Bayesian framework for model selection on random network models regarding their features is considered. The goal is to develop a principle Bayesian model selection approach to compare different fittable, not necessarily nested, models for inference on those network realisations. The criterion for random network models regarding the comparison is formulated via Bayes factors and penalizing using the mostwidely used loss functions. Parametrizations are different in different spaces. To overcome this problem we incorporate and encode different aspects of complexities in terms of observable spaces. Thus, given a range of values for a feature, network realisationsare extracted. The proposed principle approach is based on finding random network models, such that a reasonable trade off between the interested feature and the complexity of the model is preserved, avoiding overfitting problems.

</details>

<details>

<summary>2020-04-29 09:43:48 - Optional Stopping with Bayes Factors: a categorization and extension of folklore results, with an application to invariant situations</summary>

- *Allard Hendriksen, Rianne de Heide, Peter Grünwald*

- `1807.09077v3` - [abs](http://arxiv.org/abs/1807.09077v3) - [pdf](http://arxiv.org/pdf/1807.09077v3)

> It is often claimed that Bayesian methods, in particular Bayes factor methods for hypothesis testing, can deal with optional stopping. We first give an overview, using elementary probability theory, of three different mathematical meanings that various authors give to this claim: (1) stopping rule independence, (2) posterior calibration and (3) (semi-) frequentist robustness to optional stopping. We then prove theorems to the effect that these claims do indeed hold in a general measure-theoretic setting. For claims of type (2) and (3), such results are new. By allowing for non-integrable measures based on improper priors, we obtain particularly strong results for the practically important case of models with nuisance parameters satisfying a group invariance (such as location or scale). We also discuss the practical relevance of (1)--(3), and conclude that whether Bayes factor methods actually perform well under optional stopping crucially depends on details of models, priors and the goal of the analysis.

</details>

<details>

<summary>2020-04-29 13:44:26 - Autoregressive Identification of Kronecker Graphical Models</summary>

- *Mattia Zorzi*

- `2004.14199v1` - [abs](http://arxiv.org/abs/2004.14199v1) - [pdf](http://arxiv.org/pdf/2004.14199v1)

> We address the problem to estimate a Kronecker graphical model corresponding to an autoregressive Gaussian stochastic process. The latter is completely described by the power spectral density function whose inverse has support which admits a Kronecker product decomposition. We propose a Bayesian approach to estimate such a model. We test the effectiveness of the proposed method by some numerical experiments. We also apply the procedure to urban pollution monitoring data.

</details>

<details>

<summary>2020-04-30 07:47:45 - Adaptive Approximate Bayesian Computation Tolerance Selection</summary>

- *Umberto Simola, Jessica Cisewski-Kehe, Michael U. Gutmann, Jukka Corander*

- `1907.01505v2` - [abs](http://arxiv.org/abs/1907.01505v2) - [pdf](http://arxiv.org/pdf/1907.01505v2)

> Approximate Bayesian Computation (ABC) methods are increasingly used for inference in situations in which the likelihood function is either computationally costly or intractable to evaluate. Extensions of the basic ABC rejection algorithm have improved the computational efficiency of the procedure and broadened its applicability. The ABC-Population Monte Carlo (ABC-PMC) approach of Beaumont et al. (2009) has become a popular choice for approximate sampling from the posterior. ABC-PMC is a sequential sampler with an iteratively decreasing value of the tolerance, which specifies how close the simulated data need to be to the real data for acceptance. We propose a method for adaptively selecting a sequence of tolerances that improves the computational efficiency of the algorithm over other common techniques. In addition we define a stopping rule as a by-product of the adaptation procedure, which assists in automating termination of sampling. The proposed automatic ABC-PMC algorithm can be easily implemented and we present several examples demonstrating its benefits in terms of computational efficiency.

</details>

<details>

<summary>2020-04-30 18:05:17 - Bayesian Characterizations of Properties of Stochastic Processes with Applications</summary>

- *Sucharita Roy, Sourabh Bhattacharya*

- `2005.00035v1` - [abs](http://arxiv.org/abs/2005.00035v1) - [pdf](http://arxiv.org/pdf/2005.00035v1)

> In this article, we primarily propose a novel Bayesian characterization of stationary and nonstationary stochastic processes. In practice, this theory aims to distinguish between global stationarity and nonstationarity for both parametric and nonparametric stochastic processes. Interestingly, our theory builds on our previous work on Bayesian characterization of infinite series, which was applied to verification of the (in)famous Riemann Hypothesis. Thus, there seems to be interesting and important connections between pure mathematics and Bayesian statistics, with respect to our proposed ideas. We validate our proposed method with simulation and real data experiments associated with different setups. In particular, applications of our method include stationarity and nonstationarity determination in various time series models, spatial and spatio-temporal setups, and convergence diagnostics of Markov Chain Monte Carlo. Our results demonstrate very encouraging performance, even in very subtle situations. Using similar principles, we also provide a novel Bayesian characterization of mutual independence among any number of random variables, using which we characterize the properties of point processes, including characterizations of Poisson point processes, complete spatial randomness, stationarity and nonstationarity. Applications to simulation experiments with ample Poisson and non-Poisson point process models again indicate quite encouraging performance of our proposed ideas. We further propose a novel recursive Bayesian method for determination of frequencies of oscillatory stochastic processes, based on our general principle. Simulation studies and real data experiments with varieties of time series models consisting of single and multiple frequencies bring out the worth of our method.

</details>

<details>

<summary>2020-04-30 19:28:27 - High-dimensional Asymptotic Theory of Bayesian Multiple Testing Procedures Under General Dependent Setup and Possible Misspecification</summary>

- *Noirrit Kiran Chandra, Sourabh Bhattacharya*

- `2005.00066v1` - [abs](http://arxiv.org/abs/2005.00066v1) - [pdf](http://arxiv.org/pdf/2005.00066v1)

> In this article, we investigate the asymptotic properties of Bayesian multiple testing procedures under general dependent setup, when the sample size and the number of hypotheses both tend to infinity. Specifically, we investigate strong consistency of the procedures and asymptotic properties of different versions of false discovery and false non-discovery rates under the high dimensional setup. We particularly focus on a novel Bayesian non-marginal multiple testing procedure and its associated error rates in this regard. Our results show that the asymptotic convergence rates of the error rates are directly associated with the Kullback-Leibler divergence from the true model, and the results hold even when the postulated class of models is misspecified. For illustration of our high-dimensional asymptotic theory, we consider a Bayesian variable selection problem in a time-varying covariate selection framework, with autoregressive response variables. We particularly focus on the setup where the number of hypotheses increases at a faster rate compared to the sample size, which is the so-called ultra-high dimensional situation.

</details>

<details>

<summary>2020-04-30 23:59:36 - Bayesian Inference for Johnson's SB and Weibull distributions</summary>

- *Mahdi Teimouri*

- `2005.02302v1` - [abs](http://arxiv.org/abs/2005.02302v1) - [pdf](http://arxiv.org/pdf/2005.02302v1)

> The four-parameter Johnson's SB (JSB) and three-parameter Weibull distributions have received much attention in the field of forestry for characterizing diameters at breast height (DBH). In this work, we suggest the Bayesian method for estimating parameters of the JBS distribution. The maximum likelihood approach uses iterative methods such as Newton-Raphson (NR) algorithm for maximizing the logarithm of the likelihood function. But there is no guarantee that the NR method converges. This fact that the NR method for estimating the parameters of the JSB distribution sometimes fails to converge was verified through simulation in this study. Further, it was shown that the Bayesian estimators presented in this work were robust with respect to the initial values and estimate the parameters of the JSB distribution efficiently. The performance of the JSB and three-parameter Weibull distributions was compared in a Bayesian paradigm when these models were fitted to DBH data of three plots that randomly selected from a study established in 107 plots of mixed-age ponderosa pine (Pinus ponderosa Dougl. ex Laws.) with scattered western junipers at the Malheur National Forest in south end of the Blue Mountains near Burns, Oregon, USA. Bayesian paradigm demonstrated that JBS was superior model than the three-parameter Weibull for characterizing the DBH distribution when these models were fitted to the DBH data of the three plots.

</details>


## 2020-05

<details>

<summary>2020-05-01 05:55:19 - Posterior Convergence of Nonparametric Binary and Poisson Regression Under Possible Misspecifications</summary>

- *Debashis Chatterjee, Sourabh Bhattacharya*

- `2005.00234v1` - [abs](http://arxiv.org/abs/2005.00234v1) - [pdf](http://arxiv.org/pdf/2005.00234v1)

> In this article, we investigate posterior convergence of nonparametric binary and Poisson regression under possible model misspecification, assuming general stochastic process prior with appropriate properties. Our model setup and objective for binary regression is similar to that of Ghosal and Roy (2006) where the authors have used the approach of entropy bound and exponentially consistent tests with the sieve method to achieve consistency with respect to their Gaussian process prior. In contrast, for both binary and Poisson regression, using general stochastic process prior, our approach involves verification of asymptotic equipartition property along with the method of sieve, which is a manoeuvre of the general results of Shalizi (2009), useful even for misspecified models. Moreover, we will establish not only posterior consistency but also the rates at which the posterior probabilities converge, which turns out to be the Kullback-Leibler divergence rate. We also investgate the traditional posterior convergence rates. Interestingly, from subjective Bayesian viewpoint we will show that the posterior predictive distribution can accurately approximate the best possible predictive distribution in the sense that the Hellinger distance, as well as the total variation distance between the two distributions can tend to zero, in spite of misspecifications.

</details>

<details>

<summary>2020-05-01 06:04:18 - Posterior Consistency of Bayesian Inverse Regression and Inverse Reference Distributions</summary>

- *Debashis Chatterjee, Sourabh Bhattacharya*

- `2005.00236v1` - [abs](http://arxiv.org/abs/2005.00236v1) - [pdf](http://arxiv.org/pdf/2005.00236v1)

> We consider Bayesian inference in inverse regression problems where the objective is to infer about unobserved covariates from observed responses and covariates. We establish posterior consistency of such unobserved covariates in Bayesian inverse regression problemsunder appropriate priors in a leave-one-out cross-validation setup. We relate this to posterior consistency of inverse reference distributions (Bhattacharya (2013)) for assessing model adequacy. We illustrate our theory and methods with various examples of Bayesian inverse regression, along with adequate simulation experiments.

</details>

<details>

<summary>2020-05-01 06:45:09 - A Fully Bayesian Approach to Assessment of Model Adequacy in Inverse Problems</summary>

- *Sourabh Bhattacharya*

- `1203.2403v2` - [abs](http://arxiv.org/abs/1203.2403v2) - [pdf](http://arxiv.org/pdf/1203.2403v2)

> We consider the problem of assessing goodness of fit of a single Bayesian model to the observed data in the inverse problem context. A novel procedure of goodness of fit test is proposed, based on construction of reference distributions using the `inverse' part of the given model. This is motivated by an example from palaeoclimatology in which it is of interest to reconstruct past climates using information obtained from fossils deposited in lake sediment.   Technically, given a model $f(Y\mid X,\theta)$, where $Y$ is the observed data and $X$ is a set of (non-random) covariates, we obtain reference distributions based on the posterior $\pi(\tilde X\mid Y)$, where $\tilde X$ must be interpreted as the {\it unobserved} random vector corresponding to the {\it observed} covariates $X$. Put simply, if the posterior distribution $\pi(\tilde X\mid Y)$ gives high density to the observed covariates $X$, or equivalently, if the posterior distribution of $T(\tilde X)$ gives high density to $T(X)$, where $T$ is any appropriate statistic, then we say that the model fits the data. Otherwise the model in question is not adequate. We provide decision-theoretic justification of our proposed approach and discuss other theoretical and computational advantages. We demonstrate our methodology with many simulated examples and three complex, high-dimensional, realistic palaeoclimate problems, including the motivating palaeoclimate problem.

</details>

<details>

<summary>2020-05-01 11:00:32 - Nonstationary, Nonparametric, Nonseparable Bayesian Spatio-Temporal Modeling Using Kernel Convolution of Order Based Dependent Dirichlet Process</summary>

- *Moumita Das, Sourabh Bhattacharya*

- `1405.4955v2` - [abs](http://arxiv.org/abs/1405.4955v2) - [pdf](http://arxiv.org/pdf/1405.4955v2)

> In this article, using kernel convolution of order based dependent Dirichlet process (Griffin and Steel (2006)) we construct a nonstationary, nonseparable, nonparametric space-time process, which, as we show, satisfies desirable properties, and includes the stationary, separable, parametric processes as special cases. We also investigate the smoothness properties of our proposed model. Since our model entails an infinite random series, for Bayesian model fitting purpose we must either truncate the series or more appropriately consider a random number of summands, which renders the model dimension a random variable. We attack the variable dimensionality problem using Transdimensional Transformation based Markov Chain Monte Carlo introduced by Das and Bhattacharya (2019b), which can update all the variables and also change dimensions in a single block using essentially a single random variable drawn from some arbitrary density defined on a relevant support. For the sake of completeness we also address the problem of truncating the infinite series by providing a uniform bound on the error incurred by truncating the infinite series.   We illustrate the effectiveness of our model and methodologies on a simulated data set and demonstrate that our approach significantly outperforms that of Fuentes and Reich (2013) which is based on principles somewhat similar to ours. We also fit two real, spatial and spatio-temporal datasets with our approach and obtain quite encouraging results in both the cases.

</details>

<details>

<summary>2020-05-01 11:36:23 - A Non-Gaussian, Nonparametric Structure for Gene-Gene and Gene-Environment Interactions in Case-Control Studies Based on Hierarchies of Dirichlet Processes</summary>

- *Durba Bhattacharya, Sourabh Bhattacharya*

- `1704.07349v2` - [abs](http://arxiv.org/abs/1704.07349v2) - [pdf](http://arxiv.org/pdf/1704.07349v2)

> It is becoming increasingly clear that complex interactions among genes and environmental factors play crucial roles in triggering complex diseases. Thus, understanding such interactions is vital, which is possible only through statistical models that adequately account for such intricate, albeit unknown, dependence structures. Bhattacharya & Bhattacharya (2016b) attempt such modeling, relating finite mixtures composed of Dirichlet processes that represent unknown number of genetic sub-populations through a hierarchical matrix-normal structure that incorporates gene-gene interactions, and possible mutations, induced by environmental variables. However, the product dependence structure implied by their matrix-normal model seems to be too simple to be appropriate for general complex, realistic situations. In this article, we propose and develop a novel nonparametric Bayesian model for case-control genotype data using hierarchies of Dirichlet processes that offers a more realistic and nonparametric dependence structure between the genes, induced by the environmental variables. In this regard, we propose a novel and highly parallelisable MCMC algorithm that is rendered quite efficient by the combination of modern parallel computing technology, effective Gibbs sampling steps, retrospective sampling and Transformation based Markov Chain Monte Carlo (TMCMC). We use appropriate Bayesian hypothesis testing procedures to detect the roles of genes and environment in case-control studies. We apply our ideas to 5 biologically realistic case-control genotype datasets simulated under distinct set-ups, and obtain encouraging results in each case. We finally apply our ideas to a real, myocardial infarction dataset, and obtain interesting results on gene-gene and gene-environment interaction, while broadly agreeing with the results reported in the literature.

</details>

<details>

<summary>2020-05-01 11:54:58 - On Classical and Bayesian Asymptotics in Stochastic Differential Equations with Random Effects having Mixture Normal Distributions</summary>

- *Trisha Maitra, Sourabh Bhattacharya*

- `1605.03333v4` - [abs](http://arxiv.org/abs/1605.03333v4) - [pdf](http://arxiv.org/pdf/1605.03333v4)

> Delattre et al. (2013) considered a system of stochastic differential equations (SDEs) in a random effects setup. Under the independent and identical (iid) situation, and assuming normal distribution of the random effects, they established weak consistency of the maximum likelihood estimators (M LEs) of the population parameters of the random effects.   In this article, respecting the increasing importance and versatility of normal mixtures and their ability to approximate any standard distribution, we consider the random effects having mixture of normal distributions and prove asymptotic results associated with the MLEs in both independent and identical (iid) and independent but not identical (non-iid) situations. Besides, we consider iid and non-iid setups under the Bayesian paradigm and establish posterior consistency and asymptotic normality of the posterior distribution of the population parameters, even when the number of mixture components is unknown and treated as a random variable.   Although ours is an independent work, we later noted that Delattre et al. (2016) also assumed the SDE setup with normal mixture distribution of the random effect parameters but considered only the iid case and proved only weak consistency of the M LE under an extra, strong assumption as opposed to strong consistency that we are able to prove without the extra assumption. Furthermore, they did not deal with asymptotic normality of M LE or the Bayesian asymptotics counterpart which we investigate in details.   Ample simulation experiments and application to a real, stock market data set reveal the importance and usefulness of our methods even for small samples.

</details>

<details>

<summary>2020-05-01 17:16:16 - Multivariate Log-Skewed Distributions with normal kernel and their Applications</summary>

- *Marina M. de Queiroz, Rosangela H. Loschi, Roger W. C. Silva*

- `2005.00501v1` - [abs](http://arxiv.org/abs/2005.00501v1) - [pdf](http://arxiv.org/pdf/2005.00501v1)

> We introduce two classes of multivariate log skewed distributions with normal kernel: the log canonical fundamental skew-normal (log-CFUSN) and the log unified skew-normal (log-SUN). We also discuss some properties of the log-CFUSN family of distributions. These new classes of log-skewed distributions include the log-normal and multivariate log-skew normal families as particular cases. We discuss some issues related to Bayesian inference in the log-CFUSN family of distributions, mainly we focus on how to model the prior uncertainty about the skewing parameter. Based on the stochastic representation of the log-CFUSN family, we propose a data augmentation strategy for sampling from the posterior distributions. This proposed family is used to analyze the US national monthly precipitation data. We conclude that a high dimensional skewing function lead to a better model fit.

</details>

<details>

<summary>2020-05-01 20:59:56 - Rejoinder for the discussion of the paper "A novel algorithmic approach to Bayesian Logic Regression"</summary>

- *Aliaksandr Hubin, Geir Storvik, Florian Frommlet*

- `2005.00605v1` - [abs](http://arxiv.org/abs/2005.00605v1) - [pdf](http://arxiv.org/pdf/2005.00605v1)

> In this rejoinder we summarize the comments, questions and remarks on the paper "A novel algorithmic approach to Bayesian Logic Regression" from the discussants. We then respond to those comments, questions and remarks, provide several extensions of the original model and give a tutorial on our R-package EMJMCMC (http://aliaksah.github.io/EMJMCMC2016/)

</details>

<details>

<summary>2020-05-03 10:58:59 - Elements of asymptotic theory with outer probability measures</summary>

- *Jeremie Houssineau, Neil K. Chada, Emmanuel Delande*

- `1908.04331v3` - [abs](http://arxiv.org/abs/1908.04331v3) - [pdf](http://arxiv.org/pdf/1908.04331v3)

> Outer measures can be used for statistical inference in place of probability measures to bring flexibility in terms of model specification. The corresponding statistical procedures such as Bayesian inference, estimators or hypothesis testing need to be analysed in order to understand their behaviour, and motivate their use. In this article, we consider a class of outer measures based on the supremum of particular functions that we refer to as possibility functions. We then characterise the asymptotic behaviour of the corresponding Bayesian posterior uncertainties, from which the properties of the corresponding maximum a posteriori estimators can be deduced. These results are largely based on versions of both the law of large numbers and the central limit theorem that are adapted to possibility functions. Our motivation with outer measures is through the notion of uncertainty quantification, where verification of these procedures is of crucial importance. These introduced concepts shed a new light on some standard concepts such as the Fisher information and sufficient statistics and naturally strengthen the link between the frequentist and Bayesian approaches.

</details>

<details>

<summary>2020-05-04 10:16:45 - Uncertainty quantification in the stochastic block model with an unknown number of classes</summary>

- *J. van Waaij, B. J. K. Kleijn*

- `2005.01362v1` - [abs](http://arxiv.org/abs/2005.01362v1) - [pdf](http://arxiv.org/pdf/2005.01362v1)

> We study the frequentist properties of Bayesian statistical inference for the stochastic block model, with an unknown number of classes of varying sizes. We equip the space of vertex labellings with a prior on the number of classes and, conditionally, a prior on the labels. The number of classes may grow to infinity as a function of the number of vertices, depending on the sparsity of the graph. We derive non-asymptotic posterior contraction rates of the form $P_{\theta_{0,n}}\Pi_n(B_n\mid X^n)\le \epsilon_n$, where $X^n$ is the observed graph, generated according to $P_{\theta_{0,n}}$, $B_n$ is either $\{\theta_{0, n}\}$ or, in the very sparse case, a ball around $\theta_{0,n}$ of known extent, and $\epsilon_n$ is an explicit rate of convergence.   These results enable conversion of credible sets to confidence sets. In the sparse case, credible tests are shown to be confidence sets. In the very sparse case, credible sets are enlarged to form confidence sets. Confidence levels are explicit, for each $n$, as a function of the credible level and the rate of convergence.   Hypothesis testing between the number of classes is considered with the help of posterior odds, and is shown to be consistent. Explicit upper bounds on errors of the first and second type and an explicit lower bound on the power of the tests are given.

</details>

<details>

<summary>2020-05-04 15:16:30 - Off-the-shelf deep learning is not enough: parsimony, Bayes and causality</summary>

- *Rama K. Vasudevan, Maxim Ziatdinov, Lukas Vlcek, Sergei V. Kalinin*

- `2005.01557v1` - [abs](http://arxiv.org/abs/2005.01557v1) - [pdf](http://arxiv.org/pdf/2005.01557v1)

> Deep neural networks ("deep learning") have emerged as a technology of choice to tackle problems in natural language processing, computer vision, speech recognition and gameplay, and in just a few years has led to superhuman level performance and ushered in a new wave of "AI." Buoyed by these successes, researchers in the physical sciences have made steady progress in incorporating deep learning into their respective domains. However, such adoption brings substantial challenges that need to be recognized and confronted. Here, we discuss both opportunities and roadblocks to implementation of deep learning within materials science, focusing on the relationship between correlative nature of machine learning and causal hypothesis driven nature of physical sciences. We argue that deep learning and AI are now well positioned to revolutionize fields where causal links are known, as is the case for applications in theory. When confounding factors are frozen or change only weakly, this leaves open the pathway for effective deep learning solutions in experimental domains. Similarly, these methods offer a pathway towards understanding the physics of real-world systems, either via deriving reduced representations, deducing algorithmic complexity, or recovering generative physical models. However, extending deep learning and "AI" for models with unclear causal relationship can produce misleading and potentially incorrect results. Here, we argue the broad adoption of Bayesian methods incorporating prior knowledge, development of DL solutions with incorporated physical constraints, and ultimately adoption of causal models, offers a path forward for fundamental and applied research. Most notably, while these advances can change the way science is carried out in ways we cannot imagine, machine learning is not going to substitute science any time soon.

</details>

<details>

<summary>2020-05-04 16:13:01 - Optimal Scaling of Random-Walk Metropolis Algorithms on General Target Distributions</summary>

- *Jun Yang, Gareth O. Roberts, Jeffrey S. Rosenthal*

- `1904.12157v3` - [abs](http://arxiv.org/abs/1904.12157v3) - [pdf](http://arxiv.org/pdf/1904.12157v3)

> One main limitation of the existing optimal scaling results for Metropolis--Hastings algorithms is that the assumptions on the target distribution are unrealistic. In this paper, we consider optimal scaling of random-walk Metropolis algorithms on general target distributions in high dimensions arising from practical MCMC models from Bayesian statistics. For optimal scaling by maximizing expected squared jumping distance (ESJD), we show the asymptotically optimal acceptance rate $0.234$ can be obtained under general realistic sufficient conditions on the target distribution. The new sufficient conditions are easy to be verified and may hold for some general classes of MCMC models arising from Bayesian statistics applications, which substantially generalize the product i.i.d. condition required in most existing literature of optimal scaling. Furthermore, we show one-dimensional diffusion limits can be obtained under slightly stronger conditions, which still allow dependent coordinates of the target distribution. We also connect the new diffusion limit results to complexity bounds of Metropolis algorithms in high dimensions.

</details>

<details>

<summary>2020-05-04 19:20:04 - Tractable Bayesian Density Regression via Logit Stick-Breaking Priors</summary>

- *Tommaso Rigon, Daniele Durante*

- `1701.02969v5` - [abs](http://arxiv.org/abs/1701.02969v5) - [pdf](http://arxiv.org/pdf/1701.02969v5)

> There is a growing interest in learning how the distribution of a response variable changes with a set of predictors. Bayesian nonparametric dependent mixture models provide a flexible approach to address this goal. However, several formulations require computationally demanding algorithms for posterior inference. Motivated by this issue, we study a class of predictor-dependent infinite mixture models, which relies on a simple representation of the stick-breaking prior via sequential logistic regressions. This formulation maintains the same desirable properties of popular predictor-dependent stick-breaking priors, and leverages a recent P\'olya-gamma data augmentation to facilitate the implementation of several computational methods for posterior inference. These routines include Markov chain Monte Carlo via Gibbs sampling, expectation-maximization algorithms, and mean-field variational Bayes for scalable inference, thereby stimulating a wider implementation of Bayesian density regression by practitioners. The algorithms associated with these methods are presented in detail and tested in a toxicology study.

</details>

<details>

<summary>2020-05-05 03:54:43 - Regret Bounds for Safe Gaussian Process Bandit Optimization</summary>

- *Sanae Amani, Mahnoosh Alizadeh, Christos Thrampoulidis*

- `2005.01936v1` - [abs](http://arxiv.org/abs/2005.01936v1) - [pdf](http://arxiv.org/pdf/2005.01936v1)

> Many applications require a learner to make sequential decisions given uncertainty regarding both the system's payoff function and safety constraints. In safety-critical systems, it is paramount that the learner's actions do not violate the safety constraints at any stage of the learning process. In this paper, we study a stochastic bandit optimization problem where the unknown payoff and constraint functions are sampled from Gaussian Processes (GPs) first considered in [Srinivas et al., 2010]. We develop a safe variant of GP-UCB called SGP-UCB, with necessary modifications to respect safety constraints at every round. The algorithm has two distinct phases. The first phase seeks to estimate the set of safe actions in the decision set, while the second phase follows the GP-UCB decision rule. Our main contribution is to derive the first sub-linear regret bounds for this problem. We numerically compare SGP-UCB against existing safe Bayesian GP optimization algorithms.

</details>

<details>

<summary>2020-05-05 09:41:43 - Robust Bayesian Cluster Enumeration Based on the $t$ Distribution</summary>

- *Freweyni K. Teklehaymanot, Michael Muma, Abdelhak M. Zoubir*

- `1811.12337v2` - [abs](http://arxiv.org/abs/1811.12337v2) - [pdf](http://arxiv.org/pdf/1811.12337v2)

> A major challenge in cluster analysis is that the number of data clusters is mostly unknown and it must be estimated prior to clustering the observed data. In real-world applications, the observed data is often subject to heavy tailed noise and outliers which obscure the true underlying structure of the data. Consequently, estimating the number of clusters becomes challenging. To this end, we derive a robust cluster enumeration criterion by formulating the problem of estimating the number of clusters as maximization of the posterior probability of multivariate $t_\nu$ distributed candidate models. We utilize Bayes' theorem and asymptotic approximations to come up with a robust criterion that possesses a closed-form expression. Further, we refine the derivation and provide a robust cluster enumeration criterion for data sets with finite sample size. The robust criteria require an estimate of cluster parameters for each candidate model as an input. Hence, we propose a two-step cluster enumeration algorithm that uses the expectation maximization algorithm to partition the data and estimate cluster parameters prior to the calculation of one of the robust criteria. The performance of the proposed algorithm is tested and compared to existing cluster enumeration methods using numerical and real data experiments.

</details>

<details>

<summary>2020-05-05 15:11:47 - Mixture models applied to heterogeneous populations</summary>

- *Carolina Valani Cavalcante, Kelly Cristina Mota Gonçalves*

- `1510.02871v2` - [abs](http://arxiv.org/abs/1510.02871v2) - [pdf](http://arxiv.org/pdf/1510.02871v2)

> Mixture models provide a flexible representation of heterogeneity in a finite number of latent classes. From the Bayesian point of view, Markov Chain Monte Carlo methods provide a way to draw inferences from these models. In particular, when the number of subpopulations is considered unknown, more sophisticated methods are required to perform Bayesian analysis. The Reversible Jump Markov Chain Monte Carlo is an alternative method for computing the posterior distribution by simulation in this case. Some problems associated with the Bayesian analysis of these class of models are frequent, such as the so-called "label-switching" problem. However, as the level of heterogeneity in the population increases, these problems are expected to become less frequent and the model's performance to improve. Thus, the aim of this work is to evaluate the normal mixture model fit using simulated data under different settings of heterogeneity and prior information about the mixture proportions. A simulation study is also presented to evaluate the model's performance considering the number of components known and estimating it. Finally, the model is applied to a censored real dataset containing antibody levels of Cytomegalovirus in individuals.

</details>

<details>

<summary>2020-05-05 15:35:33 - Bayesian longitudinal models for exploring European sardine fishing in the Mediterranean Sea</summary>

- *Gabriel Calvo, Carmen Armero, Maria Grazia Pennino, Luigi Spezia*

- `2005.02282v1` - [abs](http://arxiv.org/abs/2005.02282v1) - [pdf](http://arxiv.org/pdf/2005.02282v1)

> In the Mediterranean Sea, catches are dominated by small pelagic fish, representing nearly the 49\% of the total harvest. Among them, the European sardine (Sardina pilchardus) is one of the most commercially important species showing high over-exploitation rates in recent last years. In this study we analysed the European sardine landings in the Mediterranean Sea from 1970 to 2014. We made use of Bayesian longitudinal linear mixed models in order to assess differences in the temporal evolution of fishing between and within countries. Furthermore, we modelled the subsequent joint evolution of artisanal and industrial fisheries. Overall results confirmed that Mediterranean fishery time series were highly diverse along their dynamics and this heterogeneity was persistent throughout the time. In addition, results highlighted a positive relationship between the two types of fishing.

</details>

<details>

<summary>2020-05-05 18:45:03 - Towards On-Chip Bayesian Neuromorphic Learning</summary>

- *Nathan Wycoff, Prasanna Balaprakash, Fangfang Xia*

- `2005.04165v1` - [abs](http://arxiv.org/abs/2005.04165v1) - [pdf](http://arxiv.org/pdf/2005.04165v1)

> If edge devices are to be deployed to critical applications where their decisions could have serious financial, political, or public-health consequences, they will need a way to signal when they are not sure how to react to their environment. For instance, a lost delivery drone could make its way back to a distribution center or contact the client if it is confused about how exactly to make its delivery, rather than taking the action which is "most likely" correct. This issue is compounded for health care or military applications. However, the brain-realistic temporal credit assignment problem neuromorphic computing algorithms have to solve is difficult. The double role weights play in backpropagation-based-learning, dictating how the network reacts to both input and feedback, needs to be decoupled. e-prop 1 is a promising learning algorithm that tackles this with Broadcast Alignment (a technique where network weights are replaced with random weights during feedback) and accumulated local information. We investigate under what conditions the Bayesian loss term can be expressed in a similar fashion, proposing an algorithm that can be computed with only local information as well and which is thus no more difficult to implement on hardware. This algorithm is exhibited on a store-recall problem, which suggests that it can learn good uncertainty on decisions to be made over time.

</details>

<details>

<summary>2020-05-06 03:56:14 - A Bayesian approach for clustering skewed data using mixtures of multivariate normal-inverse Gaussian distributions</summary>

- *Yuan Fang, Dimitris Karlis, Sanjeena Subedi*

- `2005.02585v1` - [abs](http://arxiv.org/abs/2005.02585v1) - [pdf](http://arxiv.org/pdf/2005.02585v1)

> Non-Gaussian mixture models are gaining increasing attention for mixture model-based clustering particularly when dealing with data that exhibit features such as skewness and heavy tails. Here, such a mixture distribution is presented, based on the multivariate normal inverse Gaussian (MNIG) distribution. For parameter estimation of the mixture, a Bayesian approach via Gibbs sampler is used; for this, a novel approach to simulate univariate generalized inverse Gaussian random variables and matrix generalized inverse Gaussian random matrices is provided. The proposed algorithm will be applied to both simulated and real data. Through simulation studies and real data analysis, we show parameter recovery and that our approach provides competitive clustering results compared to other clustering approaches.

</details>

<details>

<summary>2020-05-06 08:14:29 - Bayesian Multivariate Spatial Models for Lattice Data with INLA</summary>

- *Francisco Palmi-Perales, Virgilio Gomez-Rubio, Miguel A. Martinez-Beneito*

- `1909.10804v2` - [abs](http://arxiv.org/abs/1909.10804v2) - [pdf](http://arxiv.org/pdf/1909.10804v2)

> The INLAMSM package for the R programming language provides a collection of multivariate spatial models for lattice data that can be used with package INLA for Bayesian inference. The multivariate spatial models include different structures to model the spatial variation of the variables and the between-variables variability. In this way, fitting multivariate spatial models becomes faster and easier. The use of the different models included in the package is illustrated using two different datasets: the well-known North Carolina SIDS data and mortality by three causes of death in Comunidad Valenciana (Spain).

</details>

<details>

<summary>2020-05-06 09:43:16 - Bayesian dynamic variable selection in high dimensions</summary>

- *Gary Koop, Dimitris Korobilis*

- `1809.03031v2` - [abs](http://arxiv.org/abs/1809.03031v2) - [pdf](http://arxiv.org/pdf/1809.03031v2)

> This paper proposes a variational Bayes algorithm for computationally efficient posterior and predictive inference in time-varying parameter (TVP) models. Within this context we specify a new dynamic variable/model selection strategy for TVP dynamic regression models in the presence of a large number of predictors. This strategy allows for assessing in individual time periods which predictors are relevant (or not) for forecasting the dependent variable. The new algorithm is evaluated numerically using synthetic data and its computational advantages are established. Using macroeconomic data for the US we find that regression models that combine time-varying parameters with the information in many predictors have the potential to improve forecasts of price inflation over a number of alternative forecasting models.

</details>

<details>

<summary>2020-05-06 12:16:20 - Implicitly Adaptive Importance Sampling</summary>

- *Topi Paananen, Juho Piironen, Paul-Christian Bürkner, Aki Vehtari*

- `1906.08850v2` - [abs](http://arxiv.org/abs/1906.08850v2) - [pdf](http://arxiv.org/pdf/1906.08850v2)

> Adaptive importance sampling is a class of techniques for finding good proposal distributions for importance sampling. Often the proposal distributions are standard probability distributions whose parameters are adapted based on the mismatch between the current proposal and a target distribution. In this work, we present an implicit adaptive importance sampling method that applies to complicated distributions which are not available in closed form. The method iteratively matches the moments of a set of Monte Carlo draws to weighted moments based on importance weights. We apply the method to Bayesian leave-one-out cross-validation and show that it performs better than many existing parametric adaptive importance sampling methods while being computationally inexpensive.

</details>

<details>

<summary>2020-05-07 00:39:02 - Statistical aspects of nuclear mass models</summary>

- *Vojtech Kejzlar, Léo Neufcourt, Witold Nazarewicz, Paul-Gerhard Reinhard*

- `2002.04151v3` - [abs](http://arxiv.org/abs/2002.04151v3) - [pdf](http://arxiv.org/pdf/2002.04151v3)

> We study the information content of nuclear masses from the perspective of global models of nuclear binding energies. To this end, we employ a number of statistical methods and diagnostic tools, including Bayesian calibration, Bayesian model averaging, chi-square correlation analysis, principal component analysis, and empirical coverage probability. Using a Bayesian framework, we investigate the structure of the 4-parameter Liquid Drop Model by considering discrepant mass domains for calibration. We then use the chi-square correlation framework to analyze the 14-parameter Skyrme energy density functional calibrated using homogeneous and heterogeneous datasets. We show that a quite dramatic parameter reduction can be achieved in both cases. The advantage of Bayesian model averaging for improving uncertainty quantification is demonstrated. The statistical approaches used are pedagogically described; in this context this work can serve as a guide for future applications.

</details>

<details>

<summary>2020-05-07 05:22:46 - On a computationally-scalable sparse formulation of the multidimensional and non-stationary maximum entropy principle</summary>

- *Horenko Illia, Marchenko Ganna, Gagliardini Patrick*

- `2005.03253v1` - [abs](http://arxiv.org/abs/2005.03253v1) - [pdf](http://arxiv.org/pdf/2005.03253v1)

> Data-driven modelling and computational predictions based on maximum entropy principle (MaxEnt-principle) aim at finding as-simple-as-possible - but not simpler then necessary - models that allow to avoid the data overfitting problem. We derive a multivariate non-parametric and non-stationary formulation of the MaxEnt-principle and show that its solution can be approximated through a numerical maximisation of the sparse constrained optimization problem with regularization. Application of the resulting algorithm to popular financial benchmarks reveals memoryless models allowing for simple and qualitative descriptions of the major stock market indexes data. We compare the obtained MaxEnt-models to the heteroschedastic models from the computational econometrics (GARCH, GARCH-GJR, MS-GARCH, GARCH-PML4) in terms of the model fit, complexity and prediction quality. We compare the resulting model log-likelihoods, the values of the Bayesian Information Criterion, posterior model probabilities, the quality of the data autocorrelation function fits as well as the Value-at-Risk prediction quality. We show that all of the considered seven major financial benchmark time series (DJI, SPX, FTSE, STOXX, SMI, HSI and N225) are better described by conditionally memoryless MaxEnt-models with nonstationary regime-switching than by the common econometric models with finite memory. This analysis also reveals a sparse network of statistically-significant temporal relations for the positive and negative latent variance changes among different markets. The code is provided for open access.

</details>

<details>

<summary>2020-05-07 13:47:46 - Bayesian factor models for multivariate categorical data obtained from questionnaires</summary>

- *Vitor G. C. da Silva, Kelly C. M. Gonçalves, João B. M. Pereira*

- `1910.04283v2` - [abs](http://arxiv.org/abs/1910.04283v2) - [pdf](http://arxiv.org/pdf/1910.04283v2)

> Factor analysis is a flexible technique for assessment of multivariate dependence and codependence. Besides being an exploratory tool used to reduce the dimensionality of multivariate data, it allows estimation of common factors that often have an interesting theoretical interpretation in real problems. However, standard factor analysis is only applicable when the variables are scaled, which is often inappropriate, for example, in data obtained from questionnaires in the field of psychology,where the variables are often categorical. In this framework, we propose a factor model for the analysis of multivariate ordered and non-ordered polychotomous data. The inference procedure is done under the Bayesian approach via Markov chain Monte Carlo methods. Two Monte-Carlo simulation studies are presented to investigate the performance of this approach in terms of estimation bias, precision and assessment of the number of factors. We also illustrate the proposed method to analyze participants' responses to the Motivational State Questionnaire dataset, developed to study emotions in laboratory and field settings.

</details>

<details>

<summary>2020-05-07 17:55:27 - Data-Space Inversion Using a Recurrent Autoencoder for Time-Series Parameterization</summary>

- *Su Jiang, Louis J. Durlofsky*

- `2005.00061v2` - [abs](http://arxiv.org/abs/2005.00061v2) - [pdf](http://arxiv.org/pdf/2005.00061v2)

> Data-space inversion (DSI) and related procedures represent a family of methods applicable for data assimilation in subsurface flow settings. These methods differ from model-based techniques in that they provide only posterior predictions for quantities (time series) of interest, not posterior models with calibrated parameters. DSI methods require a large number of flow simulations to first be performed on prior geological realizations. Given observed data, posterior predictions can then be generated directly. DSI operates in a Bayesian setting and provides posterior samples of the data vector. In this work we develop and evaluate a new approach for data parameterization in DSI. Parameterization reduces the number of variables to determine in the inversion, and it maintains the physical character of the data variables. The new parameterization uses a recurrent autoencoder (RAE) for dimension reduction, and a long-short-term memory (LSTM) network to represent flow-rate time series. The RAE-based parameterization is combined with an ensemble smoother with multiple data assimilation (ESMDA) for posterior generation. Results are presented for two- and three-phase flow in a 2D channelized system and a 3D multi-Gaussian model. The RAE procedure, along with existing DSI treatments, are assessed through comparison to reference rejection sampling (RS) results. The new DSI methodology is shown to consistently outperform existing approaches, in terms of statistical agreement with RS results. The method is also shown to accurately capture derived quantities, which are computed from variables considered directly in DSI. This requires correlation and covariance between variables to be properly captured, and accuracy in these relationships is demonstrated. The RAE-based parameterization developed here is clearly useful in DSI, and it may also find application in other subsurface flow problems.

</details>

<details>

<summary>2020-05-07 20:54:19 - Inference, Prediction, and Entropy-Rate Estimation of Continuous-time, Discrete-event Processes</summary>

- *S. E. Marzen, J. P. Crutchfield*

- `2005.03750v1` - [abs](http://arxiv.org/abs/2005.03750v1) - [pdf](http://arxiv.org/pdf/2005.03750v1)

> Inferring models, predicting the future, and estimating the entropy rate of discrete-time, discrete-event processes is well-worn ground. However, a much broader class of discrete-event processes operates in continuous-time. Here, we provide new methods for inferring, predicting, and estimating them. The methods rely on an extension of Bayesian structural inference that takes advantage of neural network's universal approximation power. Based on experiments with complex synthetic data, the methods are competitive with the state-of-the-art for prediction and entropy-rate estimation.

</details>

<details>

<summary>2020-05-07 23:33:07 - Meta-Learning Initializations for Image Segmentation</summary>

- *Sean M. Hendryx, Andrew B. Leach, Paul D. Hein, Clayton T. Morrison*

- `1912.06290v4` - [abs](http://arxiv.org/abs/1912.06290v4) - [pdf](http://arxiv.org/pdf/1912.06290v4)

> We extend first-order model agnostic meta-learning algorithms (including FOMAML and Reptile) to image segmentation, present a novel neural network architecture built for fast learning which we call EfficientLab, and leverage a formal definition of the test error of meta-learning algorithms to decrease error on out of distribution tasks. We show state of the art results on the FSS-1000 dataset by meta-training EfficientLab with FOMAML and using Bayesian optimization to infer the optimal test-time adaptation routine hyperparameters. We also construct a small benchmark dataset, FP-k, for the empirical study of how meta-learning systems perform in both few- and many-shot settings. On the FP-k dataset, we show that meta-learned initializations provide value for canonical few-shot image segmentation but their performance is quickly matched by conventional transfer learning with performance being equal beyond 10 labeled examples. Our code, meta-learned model, and the FP-k dataset are available at https://github.com/ml4ai/mliis .

</details>

<details>

<summary>2020-05-08 05:03:04 - Efficient Computation Reduction in Bayesian Neural Networks Through Feature Decomposition and Memorization</summary>

- *Xiaotao Jia, Jianlei Yang, Runze Liu, Xueyan Wang, Sorin Dan Cotofana, Weisheng Zhao*

- `2005.03857v1` - [abs](http://arxiv.org/abs/2005.03857v1) - [pdf](http://arxiv.org/pdf/2005.03857v1)

> Bayesian method is capable of capturing real world uncertainties/incompleteness and properly addressing the over-fitting issue faced by deep neural networks. In recent years, Bayesian Neural Networks (BNNs) have drawn tremendous attentions of AI researchers and proved to be successful in many applications. However, the required high computation complexity makes BNNs difficult to be deployed in computing systems with limited power budget. In this paper, an efficient BNN inference flow is proposed to reduce the computation cost then is evaluated by means of both software and hardware implementations. A feature decomposition and memorization (\texttt{DM}) strategy is utilized to reform the BNN inference flow in a reduced manner. About half of the computations could be eliminated compared to the traditional approach that has been proved by theoretical analysis and software validations. Subsequently, in order to resolve the hardware resource limitations, a memory-friendly computing framework is further deployed to reduce the memory overhead introduced by \texttt{DM} strategy. Finally, we implement our approach in Verilog and synthesise it with 45 $nm$ FreePDK technology. Hardware simulation results on multi-layer BNNs demonstrate that, when compared with the traditional BNN inference method, it provides an energy consumption reduction of 73\% and a 4$\times$ speedup at the expense of 14\% area overhead.

</details>

<details>

<summary>2020-05-08 08:40:09 - Dynamic Shrinkage Priors for Large Time-varying Parameter Regressions using Scalable Markov Chain Monte Carlo Methods</summary>

- *Niko Hauzenberger, Florian Huber, Gary Koop*

- `2005.03906v1` - [abs](http://arxiv.org/abs/2005.03906v1) - [pdf](http://arxiv.org/pdf/2005.03906v1)

> Time-varying parameter (TVP) regression models can involve a huge number of coefficients. Careful prior elicitation is required to yield sensible posterior and predictive inferences. In addition, the computational demands of Markov Chain Monte Carlo (MCMC) methods mean their use is limited to the case where the number of predictors is not too large. In light of these two concerns, this paper proposes a new dynamic shrinkage prior which reflects the empirical regularity that TVPs are typically sparse (i.e. time variation may occur only episodically and only for some of the coefficients). A scalable MCMC algorithm is developed which is capable of handling very high dimensional TVP regressions or TVP Vector Autoregressions. In an exercise using artificial data we demonstrate the accuracy and computational efficiency of our methods. In an application involving the term structure of interest rates in the eurozone, we find our dynamic shrinkage prior to effectively pick out small amounts of parameter change and our methods to forecast well.

</details>

<details>

<summary>2020-05-08 11:14:11 - Borrowing of information across patient subgroups in a basket trial based on distributional discrepancy</summary>

- *Haiyan Zheng, James M. S. Wason*

- `1908.05091v3` - [abs](http://arxiv.org/abs/1908.05091v3) - [pdf](http://arxiv.org/pdf/1908.05091v3)

> Basket trials have emerged as a new class of efficient approaches in oncology to evaluate a new treatment in several patient subgroups simultaneously. In this paper, we extend the key ideas to disease areas outside of oncology, developing a robust Bayesian methodology for randomised, placebo-controlled basket trials with a continuous endpoint to enable borrowing of information across subtrials with similar treatment effects. After adjusting for covariates, information from a complementary subtrial can be represented into a commensurate prior for the parameter that underpins the subtrial under consideration. We propose using distributional discrepancy to characterise the commensurability between subtrials for appropriate borrowing of information through a spike-and-slab prior, which is placed on the prior precision factor. When the basket trial has at least three subtrials, commensurate priors for point-to-point borrowing are combined into a marginal predictive prior, according to the weights transformed from the pairwise discrepancy measures. In this way, only information from subtrial(s) with the most commensurate treatment effect is leveraged. The marginal predictive prior is updated to a robust posterior by the contemporary subtrial data to inform decision making. Operating characteristics of the proposed methodology are evaluated through simulations motivated by a real basket trial in chronic diseases. The proposed methodology has advantages compared to other selected Bayesian analysis models, for (i) identifying the most commensurate source of information, and (ii) gauging the degree of borrowing from specific subtrials. Numerical results also suggest that our methodology can improve the precision of estimates and, potentially, the statistical power for hypothesis testing.

</details>

<details>

<summary>2020-05-08 11:54:38 - What do you Mean? The Role of the Mean Function in Bayesian Optimisation</summary>

- *George De Ath, Jonathan E. Fieldsend, Richard M. Everson*

- `2004.08349v2` - [abs](http://arxiv.org/abs/2004.08349v2) - [pdf](http://arxiv.org/pdf/2004.08349v2)

> Bayesian optimisation is a popular approach for optimising expensive black-box functions. The next location to be evaluated is selected via maximising an acquisition function that balances exploitation and exploration. Gaussian processes, the surrogate models of choice in Bayesian optimisation, are often used with a constant prior mean function equal to the arithmetic mean of the observed function values. We show that the rate of convergence can depend sensitively on the choice of mean function. We empirically investigate 8 mean functions (constant functions equal to the arithmetic mean, minimum, median and maximum of the observed function evaluations, linear, quadratic polynomials, random forests and RBF networks), using 10 synthetic test problems and two real-world problems, and using the Expected Improvement and Upper Confidence Bound acquisition functions. We find that for design dimensions $\ge5$ using a constant mean function equal to the worst observed quality value is consistently the best choice on the synthetic problems considered. We argue that this worst-observed-quality function promotes exploitation leading to more rapid convergence. However, for the real-world tasks the more complex mean functions capable of modelling the fitness landscape may be effective, although there is no clearly optimum choice.

</details>

<details>

<summary>2020-05-08 13:19:31 - Approximate leave-future-out cross-validation for Bayesian time series models</summary>

- *Paul-Christian Bürkner, Jonah Gabry, Aki Vehtari*

- `1902.06281v5` - [abs](http://arxiv.org/abs/1902.06281v5) - [pdf](http://arxiv.org/pdf/1902.06281v5)

> One of the common goals of time series analysis is to use the observed series to inform predictions for future observations. In the absence of any actual new data to predict, cross-validation can be used to estimate a model's future predictive accuracy, for instance, for the purpose of model comparison or selection. Exact cross-validation for Bayesian models is often computationally expensive, but approximate cross-validation methods have been developed, most notably methods for leave-one-out cross-validation (LOO-CV). If the actual prediction task is to predict the future given the past, LOO-CV provides an overly optimistic estimate because the information from future observations is available to influence predictions of the past. To properly account for the time series structure, we can use leave-future-out cross-validation (LFO-CV). Like exact LOO-CV, exact LFO-CV requires refitting the model many times to different subsets of the data. Using Pareto smoothed importance sampling, we propose a method for approximating exact LFO-CV that drastically reduces the computational costs while also providing informative diagnostics about the quality of the approximation.

</details>

<details>

<summary>2020-05-08 14:30:53 - A bi-partite generative model framework for analyzing and simulating large scale multiple discrete-continuous travel behaviour data</summary>

- *Melvin Wong, Bilal Farooq*

- `1901.06415v3` - [abs](http://arxiv.org/abs/1901.06415v3) - [pdf](http://arxiv.org/pdf/1901.06415v3)

> The emergence of data-driven demand analysis has led to the increased use of generative modelling to learn the probabilistic dependencies between random variables. Although their apparent use has mostly been limited to image recognition and classification in recent years, generative machine learning algorithms can be a powerful tool for travel behaviour research by replicating travel behaviour by the underlying properties of data structures. In this paper, we examine the use of generative machine learning approach for analyzing multiple discrete-continuous (MDC) travel behaviour data. We provide a plausible perspective of how we can exploit the use of machine learning techniques to interpret the underlying heterogeneities in the data. We show that generative models are conceptually similar to the choice selection behaviour process through information entropy and variational Bayesian inference. Without loss of generality, we consider a restricted Boltzmann machine (RBM) based algorithm with multiple discrete-continuous layers, formulated as a variational Bayesian inference optimization problem. We systematically describe the proposed machine learning algorithm and develop a process of analyzing travel behaviour data from a generative learning perspective. We show parameter stability from model analysis and simulation tests on an open dataset with multiple discrete-continuous dimensions from a data size of 293,330 observations. For interpretability, we derive the conditional probabilities, elasticities and perform statistical analysis on the latent variables. We show that our model can generate statistically similar data distributions for travel forecasting and prediction and performs better than purely discriminative methods in validation. Our results indicate that latent constructs in generative models can accurately represent the joint distribution consistently on MDC data.

</details>

<details>

<summary>2020-05-08 19:09:19 - Shrinkage estimation for dose-response modeling in phase II trials with multiple schedules</summary>

- *Burak Kürsad Günhan, Paul Meyvisch, Tim Friede*

- `2005.04261v1` - [abs](http://arxiv.org/abs/2005.04261v1) - [pdf](http://arxiv.org/pdf/2005.04261v1)

> Recently, phase II trials with multiple schedules (frequency of administrations) have become more popular, for instance in the development of treatments for atopic dermatitis. If the relationship of the dose and response is described by a parametric model, a simplistic approach is to pool doses from different schedules. However, this approach ignores the potential heterogeneity in dose-response curves between schedules. A more reasonable approach is the partial pooling, i.e. certain parameters of the dose-response curves are shared, while others are allowed to vary. Rather than using schedule-specific fixed-effects, we propose a Bayesian hierarchical model with random-effects to model the between-schedule heterogeneity with regard to certain parameters. Schedule-specific dose-response relationships can then be estimated using shrinkage estimation. Considering Emax models, the proposed method displayed desirable performance in terms of the mean absolute error and the coverage probabilities for the dose-response curve compared to the complete pooling. Furthermore, it outperformed the partial pooling with schedule-specific fixed-effects by producing lower mean absolute error and shorter credible intervals. The methods are illustrated using simulations and a phase II trial example in atopic dermatitis. A publicly available R package, \texttt{ModStan}, is developed to automate the implementation of the proposed method (\href{https://github.com/gunhanb/ModStan}{https://github.com/gunhanb/ModStan}).

</details>

<details>

<summary>2020-05-08 23:49:13 - BOP-Elites, a Bayesian Optimisation algorithm for Quality-Diversity search</summary>

- *Paul Kent, Juergen Branke*

- `2005.04320v1` - [abs](http://arxiv.org/abs/2005.04320v1) - [pdf](http://arxiv.org/pdf/2005.04320v1)

> Quality Diversity (QD) algorithms such as MAP-Elites are a class of optimisation techniques that attempt to find a set of high-performing points from an objective function while enforcing behavioural diversity of the points over one or more interpretable, user chosen, feature functions.   In this paper we propose the Bayesian Optimisation of Elites (BOP-Elites) algorithm that uses techniques from Bayesian Optimisation to explicitly model both quality and diversity with Gaussian Processes. By considering user defined regions of the feature space as 'niches' our task is to find the optimal solution in each niche. We propose a novel acquisition function to intelligently choose new points that provide the highest expected improvement to the ensemble problem of identifying the best solution in every niche. In this way each function evaluation enriches our modelling and provides insight to the whole problem, naturally balancing exploration and exploitation of the search space. The resulting algorithm is very effective in identifying the parts of the search space that belong to a niche in feature space, and finding the optimal solution in each niche. It is also significantly more sample efficient than simpler benchmark approaches. BOP-Elites goes further than existing QD algorithms by quantifying the uncertainty around our predictions and offering additional illumination of the search space through surrogate models.

</details>

<details>

<summary>2020-05-09 21:32:30 - Semivariogram methods for modeling Whittle-Matérn priors in Bayesian inverse problems</summary>

- *Richard D. Brown, Johnathan M. Bardsley, Tiangang Cui*

- `1811.09446v3` - [abs](http://arxiv.org/abs/1811.09446v3) - [pdf](http://arxiv.org/pdf/1811.09446v3)

> We present a new technique, based on semivariogram methodology, for obtaining point estimates for use in prior modeling for solving Bayesian inverse problems. This method requires a connection between Gaussian processes with covariance operators defined by the Mat\'ern covariance function and Gaussian processes with precision (inverse-covariance) operators defined by the Green's functions of a class of elliptic stochastic partial differential equations (SPDEs). We present a detailed mathematical description of this connection. We will show that there is an equivalence between these two Gaussian processes when the domain is infinite -- for us, $\mathbb{R}^2$ -- which breaks down when the domain is finite due to the effect of boundary conditions on Green's functions of PDEs. We show how this connection can be re-established using extended domains. We then introduce the semivariogram method for estimating the Mat\'ern covariance parameters, which specify the Gaussian prior needed for stabilizing the inverse problem. Results are extended from the isotropic case to the anisotropic case where the correlation length in one direction is larger than another. Finally, we consider the situation where the correlation length is spatially dependent rather than constant. We implement each method in two-dimensional image inpainting test cases to show that it works on practical examples.

</details>

<details>

<summary>2020-05-10 14:33:52 - HNet: Graphical Hypergeometric Networks</summary>

- *Erdogan Taskesen*

- `2005.04679v1` - [abs](http://arxiv.org/abs/2005.04679v1) - [pdf](http://arxiv.org/pdf/2005.04679v1)

> Motivation: Real-world data often contain measurements with both continuous and discrete values. Despite the availability of many libraries, data sets with mixed data types require intensive pre-processing steps, and it remains a challenge to describe the relationships between variables. The data understanding phase is an important step in the data mining process, however, without making any assumptions on the data, the search space is super-exponential in the number of variables. Methods: We propose graphical hypergeometric networks (HNet), a method to test associations across variables for significance using statistical inference. The aim is to determine a network using only the significant associations in order to shed light on the complex relationships across variables. HNet processes raw unstructured data sets and outputs a network that consists of (partially) directed or undirected edges between the nodes (i.e., variables). To evaluate the accuracy of HNet, we used well known data sets and in addition generated data sets with known ground truth. The performance of HNet is compared to Bayesian structure learning. Results: We demonstrate that HNet showed high accuracy and performance in the detection of node links. In the case of the Alarm data set we can demonstrate on average an MCC score of 0.33 + 0.0002 (P<1x10-6), whereas Bayesian structure learning resulted in an average MCC score of 0.52 + 0.006 (P<1x10-11), and randomly assigning edges resulted in a MCC score of 0.004 + 0.0003 (P=0.49). Conclusions: HNet can process raw unstructured data sets, allows analysis of mixed data types, it easily scales up in number of variables, and allows detailed examination of the detected associations. Availability: https://erdogant.github.io/hnet/

</details>

<details>

<summary>2020-05-11 10:32:47 - Prior choice affects ability of Bayesian neural networks to identify unknowns</summary>

- *Daniele Silvestro, Tobias Andermann*

- `2005.04987v1` - [abs](http://arxiv.org/abs/2005.04987v1) - [pdf](http://arxiv.org/pdf/2005.04987v1)

> Deep Bayesian neural networks (BNNs) are a powerful tool, though computationally demanding, to perform parameter estimation while jointly estimating uncertainty around predictions. BNNs are typically implemented using arbitrary normal-distributed prior distributions on the model parameters. Here, we explore the effects of different prior distributions on classification tasks in BNNs and evaluate the evidence supporting the predictions based on posterior probabilities approximated by Markov Chain Monte Carlo sampling and by computing Bayes factors. We show that the choice of priors has a substantial impact on the ability of the model to confidently assign data to the correct class (true positive rates). Prior choice also affects significantly the ability of a BNN to identify out-of-distribution instances as unknown (false positive rates). When comparing our results against neural networks (NN) with Monte Carlo dropout we found that BNNs generally outperform NNs. Finally, in our tests we did not find a single best choice as prior distribution. Instead, each dataset yielded the best results under a different prior, indicating that testing alternative options can improve the performance of BNNs.

</details>

<details>

<summary>2020-05-11 14:46:11 - Fast Bayesian Inference in Nonparametric Double Additive Location-Scale Models With Right- and Interval-Censored Data</summary>

- *Philippe Lambert*

- `2005.05156v1` - [abs](http://arxiv.org/abs/2005.05156v1) - [pdf](http://arxiv.org/pdf/2005.05156v1)

> Penalized B-splines are routinely used in additive models to describe smooth changes in a response with quantitative covariates. It is typically done through the conditional mean in the exponential family using generalized additive models with an indirect impact on other conditional moments. Another common strategy consists in focussing on several low-order conditional moments, leaving the complete conditional distribution unspecified. Alternatively, a multi-parameter distribution could be assumed for the response with several of its parameters jointly regressed on covariates using additive expressions.   Our work can be connected to the latter proposal for a right- or interval-censored continuous response with a highly flexible and smooth nonparametric density. We focus on location-scale models with additive terms in the conditional mean and standard deviation. Starting from recent results in the Bayesian framework, we propose a quickly converging algorithm to select penalty parameters from their marginal posteriors. It relies on Laplace approximations to the conditional posterior of the spline parameters. Simulations suggest that the so-obtained estimators own excellent frequentist properties and increase efficiency as compared to approaches with a working Gaussian hypothesis. We illustrate the methodology with the analysis of imprecisely measured income data.

</details>

<details>

<summary>2020-05-11 17:08:27 - Infinite mixtures of multivariate normal-inverse Gaussian distributions for clustering of skewed data</summary>

- *Yuan Fang, Dimitris Karlis, Sanjeena Subedi*

- `2005.05324v1` - [abs](http://arxiv.org/abs/2005.05324v1) - [pdf](http://arxiv.org/pdf/2005.05324v1)

> Mixtures of multivariate normal inverse Gaussian (MNIG) distributions can be used to cluster data that exhibit features such as skewness and heavy tails. However, for cluster analysis, using a traditional finite mixture model framework, either the number of components needs to be known $a$-$priori$ or needs to be estimated $a$-$posteriori$ using some model selection criterion after deriving results for a range of possible number of components. However, different model selection criteria can sometimes result in different number of components yielding uncertainty. Here, an infinite mixture model framework, also known as Dirichlet process mixture model, is proposed for the mixtures of MNIG distributions. This Dirichlet process mixture model approach allows the number of components to grow or decay freely from 1 to $\infty$ (in practice from 1 to $N$) and the number of components is inferred along with the parameter estimates in a Bayesian framework thus alleviating the need for model selection criteria. We provide real data applications with benchmark datasets as well as a small simulation experiment to compare with other existing models. The proposed method provides competitive clustering results to other clustering approaches for both simulation and real data and parameter recovery are illustrated using simulation studies.

</details>

<details>

<summary>2020-05-11 19:52:13 - A general approach to maximise information density in neutron reflectometry analysis</summary>

- *Andrew R. McCluskey, Thomas Arnold, Joshaniel F. K. Cooper, Tim Snow*

- `1910.10581v4` - [abs](http://arxiv.org/abs/1910.10581v4) - [pdf](http://arxiv.org/pdf/1910.10581v4)

> Neutron and X-ray reflectometry are powerful techniques facilitating the study of the structure of interfacial materials. The analysis of these techniques is ill-posed in nature requiring the application of a model-dependent methods. This can lead to the over- and under- analysis of experimental data, when too many or too few parameters are allowed to vary in the model. In this work, we outline a robust and generic framework for the determination of the set of free parameters that is capable of maximising the in-formation density of the model. This framework involves the determination of the Bayesian evidence for each permutation of free parameters; and is applied to a simple phospholipid monolayer. We believe this framework should become an important component in reflectometry data analysis, and hope others more regularly consider the relative evidence for their analytical models.

</details>

<details>

<summary>2020-05-11 20:49:28 - Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning</summary>

- *Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, Andrew Gordon Wilson*

- `1902.03932v2` - [abs](http://arxiv.org/abs/1902.03932v2) - [pdf](http://arxiv.org/pdf/1902.03932v2)

> The posteriors over neural network weights are high dimensional and multimodal. Each mode typically characterizes a meaningfully different representation of the data. We develop Cyclical Stochastic Gradient MCMC (SG-MCMC) to automatically explore such distributions. In particular, we propose a cyclical stepsize schedule, where larger steps discover new modes, and smaller steps characterize each mode. We also prove non-asymptotic convergence of our proposed algorithm. Moreover, we provide extensive experimental results, including ImageNet, to demonstrate the scalability and effectiveness of cyclical SG-MCMC in learning complex multimodal distributions, especially for fully Bayesian inference with modern deep neural networks.

</details>

<details>

<summary>2020-05-11 23:34:50 - Objective Priors in the Empirical Bayes Framework</summary>

- *Ilja Klebanov, Alexander Sikorski, Christof Schütte, Susanna Röblitz*

- `1612.00064v5` - [abs](http://arxiv.org/abs/1612.00064v5) - [pdf](http://arxiv.org/pdf/1612.00064v5)

> When dealing with Bayesian inference the choice of the prior often remains a debatable question. Empirical Bayes methods offer a data-driven solution to this problem by estimating the prior itself from an ensemble of data. In the nonparametric case, the maximum likelihood estimate is known to overfit the data, an issue that is commonly tackled by regularization. However, the majority of regularizations are ad hoc choices which lack invariance under reparametrization of the model and result in inconsistent estimates for equivalent models. We introduce a non-parametric, transformation invariant estimator for the prior distribution. Being defined in terms of the missing information similar to the reference prior, it can be seen as an extension of the latter to the data-driven setting. This implies a natural interpretation as a trade-off between choosing the least informative prior and incorporating the information provided by the data, a symbiosis between the objective and empirical Bayes methodologies.

</details>

<details>

<summary>2020-05-12 06:56:00 - Preparation of ordered states in ultra-cold gases using Bayesian optimization</summary>

- *Rick Mukherjee, Frederic Sauvage, Harry Xie, Robert Löw, Florian Mintert*

- `2001.03520v3` - [abs](http://arxiv.org/abs/2001.03520v3) - [pdf](http://arxiv.org/pdf/2001.03520v3)

> Ultra-cold atomic gases are unique in terms of the degree of controllability, both for internal and external degrees of freedom. This makes it possible to use them for the study of complex quantum many-body phenomena. However in many scenarios, the prerequisite condition of faithfully preparing a desired quantum state despite decoherence and system imperfections is not always adequately met. To path the way to a specific target state, we explore quantum optimal control framework based on Bayesian optimization. The probabilistic modeling and broad exploration aspects of Bayesian optimization is particularly suitable for quantum experiments where data acquisition can be expensive. Using numerical simulations for the superfluid to Mott-insulator transition for bosons in a lattice as well for the formation of Rydberg crystals as explicit examples, we demonstrate that Bayesian optimization is capable of finding better control solutions with regards to finite and noisy data compared to existing methods of optimal control.

</details>

<details>

<summary>2020-05-12 08:59:51 - Upper Trust Bound Feasibility Criterion for Mixed Constrained Bayesian Optimization with Application to Aircraft Design</summary>

- *Rémy Priem, Nathalie Bartoli, Youssef Diouane, Alessandro Sgueglia*

- `2005.05067v2` - [abs](http://arxiv.org/abs/2005.05067v2) - [pdf](http://arxiv.org/pdf/2005.05067v2)

> Bayesian optimization methods have been successfully applied to black box optimization problems that are expensive to evaluate. In this paper, we adapt the so-called super effcient global optimization algorithm to solve more accurately mixed constrained problems. The proposed approach handles constraints by means of upper trust bound, the latter encourages exploration of the feasible domain by combining the mean prediction and the associated uncertainty function given by the Gaussian processes. On top of that, a refinement procedure, based on a learning rate criterion, is introduced to enhance the exploitation and exploration trade-off. We show the good potential of the approach on a set of numerical experiments. Finally, we present an application to conceptual aircraft configuration upon which we show the superiority of the proposed approach compared to a set of the state-of-the-art black box optimization solvers. Keywords: Global Optimization, Mixed Constrained Optimization, Black box optimization, Bayesian Optimization, Gaussian Process.

</details>

<details>

<summary>2020-05-12 10:20:22 - Informed attribution of flood changes to decadal variation of atmospheric, catchment and river drivers in Upper Austria</summary>

- *Miriam Bertola, Alberto Viglione, Günter Blöschl*

- `2005.05665v1` - [abs](http://arxiv.org/abs/2005.05665v1) - [pdf](http://arxiv.org/pdf/2005.05665v1)

> Flood changes may be attributed to drivers of change that belong to three main classes: atmospheric, catchment and river system drivers. In this work, we propose a data-based attribution approach for selecting which driver best relates to variations in time of the flood frequency curve. The flood peaks are assumed to follow a Gumbel distribution, whose location parameter changes in time as a function of the decadal variations of one of the following alternative covariates: annual and extreme precipitation for different durations, an agricultural land-use intensification index, and reservoir construction in the catchment, quantified by an index. The parameters of this attribution model are estimated by Bayesian inference. Prior information on one of these parameters, the elasticity of flood peaks to the respective driver, is taken from the existing literature to increase the robustness of the method to spurious correlations between flood and covariate time series. Therefore, the attribution model is informed in two ways: by the use of covariates, representing the drivers of change, and by the priors, representing the hydrological understanding of how these covariates influence floods. The Watanabe-Akaike information criterion is used to compare models involving alternative covariates. We apply the approach to 96 catchments in Upper Austria, where positive flood peak trends have been observed in the past 50 years. Results show that, in Upper Austria, one or seven day extreme precipitation is usually a better covariate for variations of the flood frequency curve than precipitation at longer time scales. Agricultural land-use intensification rarely is the best covariate, and the reservoir index never is, suggesting that catchment and river drivers are less important than atmospheric ones.

</details>

<details>

<summary>2020-05-13 12:28:50 - Bayesian aggregation of average data: An application in drug development</summary>

- *Sebastian Weber, Andrew Gelman, Daniel Lee, Michael Betancourt, Aki Vehtari, Amy Racine*

- `1602.02055v2` - [abs](http://arxiv.org/abs/1602.02055v2) - [pdf](http://arxiv.org/pdf/1602.02055v2)

> Throughout the different phases of a drug development program, randomized trials are used to establish the tolerability, safety, and efficacy of a candidate drug. At each stage one aims to optimize the design of future studies by extrapolation from the available evidence at the time. This includes collected trial data and relevant external data. However, relevant external data are typically available as averages only, for example from trials on alternative treatments reported in the literature. Here we report on such an example from a drug development for wet age-related macular degeneration. This disease is the leading cause of severe vision loss in the elderly. While current treatment options are efficacious, they are also a substantial burden for the patient. Hence, new treatments are under development which need to be compared against existing treatments. The general statistical problem this leads to is meta-analysis, which addresses the question of how we can combine datasets collected under different conditions. Bayesian methods have long been used to achieve partial pooling. Here we consider the challenge when the model of interest is complex (hierarchical and nonlinear) and one dataset is given as raw data while the second dataset is given as averages only. In such a situation, common meta-analytic methods can only be applied when the model is sufficiently simple for analytic approaches. When the model is too complex, for example nonlinear, an analytic approach is not possible. We provide a Bayesian solution by using simulation to approximately reconstruct the likelihood of the external summary and allowing the parameters in the model to vary under the different conditions. We first evaluate our approach using fake-data simulations and then report results for the drug development program that motivated this research.

</details>

<details>

<summary>2020-05-13 13:02:50 - Accounting for Location Measurement Error in Imaging Data with Application to Atomic Resolution Images of Crystalline Materials</summary>

- *Matthew J. Miller, Matthew J. Cabral, Elizabeth C. Dickey, James M. LeBeau, Brian J. Reich*

- `1910.14195v2` - [abs](http://arxiv.org/abs/1910.14195v2) - [pdf](http://arxiv.org/pdf/1910.14195v2)

> Scientists use imaging to identify objects of interest and infer properties of these objects. The locations of these objects are often measured with error, which when ignored leads to biased parameter estimates and inflated variance. Current measurement error methods require an estimate or knowledge of the measurement error variance to correct these estimates, which may not be available. Instead, we create a spatial Bayesian hierarchical model that treats the locations as parameters, it using the image itself to incorporate positional uncertainty. We lower the computational burden by approximating the likelihood using a non-contiguous block design around the object locations. We apply this model in a materials science setting to study the relationship between the chemistry and displacement of hundreds of atom columns in crystal structures directly imaged via scanning transmission electron microscopy. Greater knowledge of this relationship can lead to engineering materials with improved properties of interest. We find strong evidence of a negative relationship between atom column displacement and the intensity of neighboring atom columns, which is related to the local chemistry. A simulation study shows our method corrects the bias in the parameter of interest and drastically improves coverage in high noise scenarios compared to non-measurement error models.

</details>

<details>

<summary>2020-05-13 16:40:09 - Crackovid: Optimizing Group Testing</summary>

- *Louis Abraham, Gary Bécigneul, Bernhard Schölkopf*

- `2005.06413v1` - [abs](http://arxiv.org/abs/2005.06413v1) - [pdf](http://arxiv.org/pdf/2005.06413v1)

> We study the problem usually referred to as group testing in the context of COVID-19. Given $n$ samples taken from patients, how should we select mixtures of samples to be tested, so as to maximize information and minimize the number of tests? We consider both adaptive and non-adaptive strategies, and take a Bayesian approach with a prior both for infection of patients and test errors. We start by proposing a mathematically principled objective, grounded in information theory. We then optimize non-adaptive optimization strategies using genetic algorithms, and leverage the mathematical framework of adaptive sub-modularity to obtain theoretical guarantees for the greedy-adaptive method.

</details>

<details>

<summary>2020-05-13 17:26:08 - Dynamic information design</summary>

- *Deepanshu Vasal*

- `2005.07267v1` - [abs](http://arxiv.org/abs/2005.07267v1) - [pdf](http://arxiv.org/pdf/2005.07267v1)

> We consider the problem of dynamic information design with one sender and one receiver where the sender observers a private state of the system and takes an action to send a signal based on its observation to a receiver. Based on this signal, the receiver takes an action that determines rewards for both the sender and the receiver and controls the state of the system. In this technical note, we show that this problem can be considered as a problem of dynamic game of asymmetric information and its perfect Bayesian equilibrium (PBE) and Stackelberg equilibrium (SE) can be analyzed using the algorithms presented in [1], [2] by the same author (among others). We then extend this model when there is one sender and multiple receivers and provide algorithms to compute a class of equilibria of this game.

</details>

<details>

<summary>2020-05-13 20:14:42 - Scalable Bayesian inference for self-excitatory stochastic processes applied to big American gunfire data</summary>

- *Andrew J. Holbrook, Charles E. Loeffler, Seth R. Flaxman, Marc A. Suchard*

- `2005.10123v1` - [abs](http://arxiv.org/abs/2005.10123v1) - [pdf](http://arxiv.org/pdf/2005.10123v1)

> The Hawkes process and its extensions effectively model self-excitatory phenomena including earthquakes, viral pandemics, financial transactions, neural spike trains and the spread of memes through social networks. The usefulness of these stochastic process models within a host of economic sectors and scientific disciplines is undercut by the processes' computational burden: complexity of likelihood evaluations grows quadratically in the number of observations for both the temporal and spatiotemporal Hawkes processes. We show that, with care, one may parallelize these calculations using both central and graphics processing unit implementations to achieve over 100-fold speedups over single-core processing. Using a simple adaptive Metropolis-Hastings scheme, we apply our high-performance computing framework to a Bayesian analysis of big gunshot data generated in Washington D.C. between the years of 2006 and 2019, thereby extending a past analysis of the same data from under 10,000 to over 85,000 observations. To encourage wide-spread use, we provide hpHawkes, an open-source R package, and discuss high-level implementation and program design for leveraging aspects of computational hardware that become necessary in a big data setting.

</details>

<details>

<summary>2020-05-14 04:42:43 - Distributed Bayesian clustering using finite mixture of mixtures</summary>

- *Hanyu Song, Yingjian Wang, David B. Dunson*

- `2003.13936v2` - [abs](http://arxiv.org/abs/2003.13936v2) - [pdf](http://arxiv.org/pdf/2003.13936v2)

> In many modern applications, there is interest in analyzing enormous data sets that cannot be easily moved across computers or loaded into memory on a single computer. In such settings, it is very common to be interested in clustering. Existing distributed clustering algorithms are mostly distance or density based without a likelihood specification, precluding the possibility of formal statistical inference. Model-based clustering allows statistical inference, yet research on distributed inference has emphasized nonparametric Bayesian mixture models over finite mixture models. To fill this gap, we introduce a nearly embarrassingly parallel algorithm for clustering under a Bayesian overfitted finite mixture of Gaussian mixtures, which we term distributed Bayesian clustering (DIB-C). DIB-C can flexibly accommodate data sets with various shapes (e.g. skewed or multi-modal). With data randomly partitioned and distributed, we first run Markov chain Monte Carlo in an embarrassingly parallel manner to obtain local clustering draws and then refine across workers for a final clustering estimate based on any loss function on the space of partitions. DIB-C can also estimate cluster densities, quickly classify new subjects and provide a posterior predictive distribution. Both simulation studies and real data applications show superior performance of DIB-C in terms of robustness and computational efficiency.

</details>

<details>

<summary>2020-05-14 09:20:41 - Bayesian quantification for coherent anti-Stokes Raman scattering spectroscopy</summary>

- *Teemu Härkönen, Lassi Roininen, Matthew T. Moores, Erik M. Vartiainen*

- `2005.06830v1` - [abs](http://arxiv.org/abs/2005.06830v1) - [pdf](http://arxiv.org/pdf/2005.06830v1)

> We propose a Bayesian statistical model for analyzing coherent anti-Stokes Raman scattering (CARS) spectra. Our quantitative analysis includes statistical estimation of constituent line-shape parameters, underlying Raman signal, error-corrected CARS spectrum, and the measured CARS spectrum. As such, this work enables extensive uncertainty quantification in the context of CARS spectroscopy. Furthermore, we present an unsupervised method for improving spectral resolution of Raman-like spectra requiring little to no \textit{a priori} information. Finally, the recently-proposed wavelet prism method for correcting the experimental artefacts in CARS is enhanced by using interpolation techniques for wavelets. The method is validated using CARS spectra of adenosine mono-, di-, and triphosphate in water, as well as, equimolar aqueous solutions of D-fructose, D-glucose, and their disaccharide combination sucrose.

</details>

<details>

<summary>2020-05-14 12:41:47 - Surrogate-assisted parallel tempering for Bayesian neural learning</summary>

- *Rohitash Chandra, Konark Jain, Arpit Kapoor, Ashray Aman*

- `1811.08687v3` - [abs](http://arxiv.org/abs/1811.08687v3) - [pdf](http://arxiv.org/pdf/1811.08687v3)

> Due to the need for robust uncertainty quantification, Bayesian neural learning has gained attention in the era of deep learning and big data. Markov Chain Monte-Carlo (MCMC) methods typically implement Bayesian inference which faces several challenges given a large number of parameters, complex and multimodal posterior distributions, and computational complexity of large neural network models. Parallel tempering MCMC addresses some of these limitations given that they can sample multimodal posterior distributions and utilize high-performance computing. However, certain challenges remain given large neural network models and big data. Surrogate-assisted optimization features the estimation of an objective function for models which are computationally expensive. In this paper, we address the inefficiency of parallel tempering MCMC for large-scale problems by combining parallel computing features with surrogate assisted likelihood estimation that describes the plausibility of a model parameter value, given specific observed data. Hence, we present surrogate-assisted parallel tempering for Bayesian neural learning for simple to computationally expensive models. Our results demonstrate that the methodology significantly lowers the computational cost while maintaining quality in decision making with Bayesian neural networks. The method has applications for a Bayesian inversion and uncertainty quantification for a broad range of numerical models.

</details>

<details>

<summary>2020-05-14 15:29:45 - Simulation-Based Inference for Global Health Decisions</summary>

- *Christian Schroeder de Witt, Bradley Gram-Hansen, Nantas Nardelli, Andrew Gambardella, Rob Zinkov, Puneet Dokania, N. Siddharth, Ana Belen Espinosa-Gonzalez, Ara Darzi, Philip Torr, Atılım Güneş Baydin*

- `2005.07062v1` - [abs](http://arxiv.org/abs/2005.07062v1) - [pdf](http://arxiv.org/pdf/2005.07062v1)

> The COVID-19 pandemic has highlighted the importance of in-silico epidemiological modelling in predicting the dynamics of infectious diseases to inform health policy and decision makers about suitable prevention and containment strategies. Work in this setting involves solving challenging inference and control problems in individual-based models of ever increasing complexity. Here we discuss recent breakthroughs in machine learning, specifically in simulation-based inference, and explore its potential as a novel venue for model calibration to support the design and evaluation of public health interventions. To further stimulate research, we are developing software interfaces that turn two cornerstone COVID-19 and malaria epidemiology models COVID-sim, (https://github.com/mrc-ide/covid-sim/) and OpenMalaria (https://github.com/SwissTPH/openmalaria) into probabilistic programs, enabling efficient interpretable Bayesian inference within those simulators.

</details>

<details>

<summary>2020-05-14 16:32:00 - Asymptotic Theory of Dependent Bayesian Multiple Testing Procedures Under Possible Model Misspecification</summary>

- *Noirrit K. Chandra, Sourabh Bhattacharya*

- `1611.01369v5` - [abs](http://arxiv.org/abs/1611.01369v5) - [pdf](http://arxiv.org/pdf/1611.01369v5)

> We study asymptotic properties of Bayesian multiple testing procedures and provide sufficient conditions for strong consistency under general dependence structure. We also consider a novel Bayesian multiple testing procedure and associated error measures that coherently accounts for the dependence structure present in the model. We advocate posterior versions of FDR and FNR as appropriate error rates and show that their asymptotic convergence rates are directly associated with the Kullback-Leibler divergence from the true model. Our results hold even when the class of postulated models is misspecified. We illustrate our results in a variable selection problem with autoregressive response variables, and compare the new Bayesian procedure with some existing methods through extensive simulation studies in the variable selection problem. Superior performance of the new procedure compared to the others vindicate that proper exploitation of the dependence structure by multiple testing methods is indeed important. Moreover, we obtain encouraging results in a real, maize data context, where we select influential marker variables.

</details>

<details>

<summary>2020-05-14 20:51:58 - Numerical Linear Algebra in Data Assimilation</summary>

- *Melina A. Freitag*

- `1912.13336v2` - [abs](http://arxiv.org/abs/1912.13336v2) - [pdf](http://arxiv.org/pdf/1912.13336v2)

> Data assimilation is a method that combines observations (that is, real world data) of a state of a system with model output for that system in order to improve the estimate of the state of the system and thereby the model output. The model is usually represented by a discretised partial differential equation. The data assimilation problem can be formulated as a large scale Bayesian inverse problem. Based on this interpretation we will derive the most important variational and sequential data assimilation approaches, in particular three-dimensional and four-dimensional variational data assimilation (3D-Var and 4D-Var) and the Kalman filter. We will then consider more advanced methods which are extensions of the Kalman filter and variational data assimilation and pay particular attention to their advantages and disadvantages. The data assimilation problem usually results in a very large optimisation problem and/or a very large linear system to solve (due to inclusion of time and space dimensions). Therefore, the second part of this article aims to review advances and challenges, in particular from the numerical linear algebra perspective, within the various data assimilation approaches.

</details>

<details>

<summary>2020-05-15 06:11:57 - A new Bayesian two-sample t-test for effect size estimation under uncertainty based on a two-component Gaussian mixture with known allocations and the region of practical equivalence</summary>

- *Riko Kelter*

- `1906.07524v2` - [abs](http://arxiv.org/abs/1906.07524v2) - [pdf](http://arxiv.org/pdf/1906.07524v2)

> Testing differences between a treatment and control group is common practice in biomedical research like randomized controlled trials (RCT). The standard two-sample t-test relies on null hypothesis significance testing (NHST) via p-values, which has several drawbacks. Bayesian alternatives were recently introduced using the Bayes factor, which has its own limitations. This paper introduces an alternative to current Bayesian two-sample t-tests by interpreting the underlying model as a two-component Gaussian mixture in which the effect size is the quantity of interest, which is most relevant in clinical research. Unlike p-values or the Bayes factor, the proposed method focusses on estimation under uncertainty instead of explicit hypothesis testing. Therefore, via a Gibbs sampler the posterior of the effect size is produced, which is used subsequently for either estimation under uncertainty or explicit hypothesis testing based on the region of practical equivalence (ROPE). An illustrative example, theoretical results and a simulation study show the usefulness of the proposed method, and the test is made available in the R package bayest.

</details>

<details>

<summary>2020-05-15 08:15:47 - Stopping criterion for active learning based on deterministic generalization bounds</summary>

- *Hideaki Ishibashi, Hideitsu Hino*

- `2005.07402v1` - [abs](http://arxiv.org/abs/2005.07402v1) - [pdf](http://arxiv.org/pdf/2005.07402v1)

> Active learning is a framework in which the learning machine can select the samples to be used for training. This technique is promising, particularly when the cost of data acquisition and labeling is high. In active learning, determining the timing at which learning should be stopped is a critical issue. In this study, we propose a criterion for automatically stopping active learning. The proposed stopping criterion is based on the difference in the expected generalization errors and hypothesis testing. We derive a novel upper bound for the difference in expected generalization errors before and after obtaining a new training datum based on PAC-Bayesian theory. Unlike ordinary PAC-Bayesian bounds, though, the proposed bound is deterministic; hence, there is no uncontrollable trade-off between the confidence and tightness of the inequality. We combine the upper bound with a statistical test to derive a stopping criterion for active learning. We demonstrate the effectiveness of the proposed method via experiments with both artificial and real datasets.

</details>

<details>

<summary>2020-05-15 09:54:09 - Excursion Search for Constrained Bayesian Optimization under a Limited Budget of Failures</summary>

- *Alonso Marco, Alexander von Rohr, Dominik Baumann, José Miguel Hernández-Lobato, Sebastian Trimpe*

- `2005.07443v1` - [abs](http://arxiv.org/abs/2005.07443v1) - [pdf](http://arxiv.org/pdf/2005.07443v1)

> When learning to ride a bike, a child falls down a number of times before achieving the first success. As falling down usually has only mild consequences, it can be seen as a tolerable failure in exchange for a faster learning process, as it provides rich information about an undesired behavior. In the context of Bayesian optimization under unknown constraints (BOC), typical strategies for safe learning explore conservatively and avoid failures by all means. On the other side of the spectrum, non conservative BOC algorithms that allow failing may fail an unbounded number of times before reaching the optimum. In this work, we propose a novel decision maker grounded in control theory that controls the amount of risk we allow in the search as a function of a given budget of failures. Empirical validation shows that our algorithm uses the failures budget more efficiently in a variety of optimization experiments, and generally achieves lower regret, than state-of-the-art methods. In addition, we propose an original algorithm for unconstrained Bayesian optimization inspired by the notion of excursion sets in stochastic processes, upon which the failures-aware algorithm is built.

</details>

<details>

<summary>2020-05-15 17:30:59 - A global-local approach for detecting hotspots in multiple-response regression</summary>

- *Hélène Ruffieux, Anthony C. Davison, Jörg Hager, Jamie Inshaw, Benjamin P. Fairfax, Sylvia Richardson, Leonardo Bottolo*

- `1811.03334v3` - [abs](http://arxiv.org/abs/1811.03334v3) - [pdf](http://arxiv.org/pdf/1811.03334v3)

> We tackle modelling and inference for variable selection in regression problems with many predictors and many responses. We focus on detecting hotspots, i.e., predictors associated with several responses. Such a task is critical in statistical genetics, as hotspot genetic variants shape the architecture of the genome by controlling the expression of many genes and may initiate decisive functional mechanisms underlying disease endpoints. Existing hierarchical regression approaches designed to model hotspots suffer from two limitations: their discrimination of hotspots is sensitive to the choice of top-level scale parameters for the propensity of predictors to be hotspots, and they do not scale to large predictor and response vectors, e.g., of dimensions $10^3-10^5$ in genetic applications. We address these shortcomings by introducing a flexible hierarchical regression framework that is tailored to the detection of hotspots and scalable to the above dimensions. Our proposal implements a fully Bayesian model for hotspots based on the horseshoe shrinkage prior. Its global-local formulation shrinks noise globally and hence accommodates the highly sparse nature of genetic analyses, while being robust to individual signals, thus leaving the effects of hotspots unshrunk. Inference is carried out using a fast variational algorithm coupled with a novel simulated annealing procedure that allows efficient exploration of multimodal distributions.

</details>

<details>

<summary>2020-05-15 17:45:11 - Power laws distributions in objective priors</summary>

- *Pedro L. Ramos, Francisco A. Rodrigues, Eduardo Ramos, Dipak K. Dey, Francisco Louzada*

- `2005.07674v1` - [abs](http://arxiv.org/abs/2005.07674v1) - [pdf](http://arxiv.org/pdf/2005.07674v1)

> The use of objective prior in Bayesian applications has become a common practice to analyze data without subjective information. Formal rules usually obtain these priors distributions, and the data provide the dominant information in the posterior distribution. However, these priors are typically improper and may lead to improper posterior. Here, we show, for a general family of distributions, that the obtained objective priors for the parameters either follow a power-law distribution or has an asymptotic power-law behavior. As a result, we observed that the exponents of the model are between 0.5 and 1. Understand these behaviors allow us to easily verify if such priors lead to proper or improper posteriors directly from the exponent of the power-law. The general family considered in our study includes essential models such as Exponential, Gamma, Weibull, Nakagami-m, Haf-Normal, Rayleigh, Erlang, and Maxwell Boltzmann distributions, to list a few. In summary, we show that comprehending the mechanisms describing the shapes of the priors provides essential information that can be used in situations where additional complexity is presented.

</details>

<details>

<summary>2020-05-15 21:03:42 - Bayesian Pseudo Posterior Synthesis for Data Privacy Protection</summary>

- *Jingchen Hu, Terrance D. Savitsky*

- `1901.06462v3` - [abs](http://arxiv.org/abs/1901.06462v3) - [pdf](http://arxiv.org/pdf/1901.06462v3)

> Statistical agencies utilize models to synthesize respondent-level data for release to the general public as an alternative to the actual data records. A Bayesian model synthesizer encodes privacy protection by employing a hierarchical prior construction that induces smoothing of the real data distribution. Synthetic respondent-level data records are often preferred to summary data tables due to the many possible uses by researchers and data analysts. Agencies balance a trade-off between utility of the synthetic data versus disclosure risks and hold a specific target threshold for disclosure risk before releasing synthetic datasets. We introduce a pseudo posterior likelihood that exponentiates each contribution by an observation record-indexed weight in (0, 1), defined to be inversely proportional to the disclosure risk for that record in the synthetic data. Our use of a vector of weights allows more precise downweighting of high risk records in a fashion that better preserves utility as compared with using a scalar weight. We illustrate our method with a simulation study and an application to the Consumer Expenditure Survey of the U.S. Bureau of Labor Statistics. We demonstrate how the frequentist consistency and uncertainty quantification are affected by the inverse risk-weighting.

</details>

<details>

<summary>2020-05-15 21:55:34 - Nonparametric graphical model for counts</summary>

- *Arkaprava Roy, David B Dunson*

- `1901.00886v2` - [abs](http://arxiv.org/abs/1901.00886v2) - [pdf](http://arxiv.org/pdf/1901.00886v2)

> Although multivariate count data are routinely collected in many application areas, there is surprisingly little work developing flexible models for characterizing their dependence structure. This is particularly true when interest focuses on inferring the conditional independence graph. In this article, we propose a new class of pairwise Markov random field-type models for the joint distribution of a multivariate count vector. By employing a novel type of transformation, we avoid restricting to non-negative dependence structures or inducing other restrictions through truncations. Taking a Bayesian approach to inference, we choose a Dirichlet process prior for the distribution of a random effect to induce great flexibility in the specification. An efficient Markov chain Monte Carlo (MCMC) algorithm is developed for posterior computation. We prove various theoretical properties, including posterior consistency, and show that our COunt Nonparametric Graphical Analysis (CONGA) approach has good performance relative to competitors in simulation studies. The methods are motivated by an application to neuron spike count data in mice.

</details>

<details>

<summary>2020-05-15 22:14:16 - High-dimensional Bayesian Optimization of Personalized Cardiac Model Parameters via an Embedded Generative Model</summary>

- *Jwala Dhamala, Sandesh Ghimire, John L. Sapp, B. Milan Horácek, Linwei Wang*

- `2005.07804v1` - [abs](http://arxiv.org/abs/2005.07804v1) - [pdf](http://arxiv.org/pdf/2005.07804v1)

> The estimation of patient-specific tissue properties in the form of model parameters is important for personalized physiological models. However, these tissue properties are spatially varying across the underlying anatomical model, presenting a significance challenge of high-dimensional (HD) optimization at the presence of limited measurement data. A common solution to reduce the dimension of the parameter space is to explicitly partition the anatomical mesh, either into a fixed small number of segments or a multi-scale hierarchy. This anatomy-based reduction of parameter space presents a fundamental bottleneck to parameter estimation, resulting in solutions that are either too low in resolution to reflect tissue heterogeneity, or too high in dimension to be reliably estimated within feasible computation. In this paper, we present a novel concept that embeds a generative variational auto-encoder (VAE) into the objective function of Bayesian optimization, providing an implicit low-dimensional (LD) search space that represents the generative code of the HD spatially-varying tissue properties. In addition, the VAE-encoded knowledge about the generative code is further used to guide the exploration of the search space. The presented method is applied to estimating tissue excitability in a cardiac electrophysiological model. Synthetic and real-data experiments demonstrate its ability to improve the accuracy of parameter estimation with more than 10x gain in efficiency.

</details>

<details>

<summary>2020-05-16 03:29:46 - Gaussbock: Fast parallel-iterative cosmological parameter estimation with Bayesian nonparametrics</summary>

- *Ben Moews, Joe Zuntz*

- `1905.09800v2` - [abs](http://arxiv.org/abs/1905.09800v2) - [pdf](http://arxiv.org/pdf/1905.09800v2)

> We present and apply Gaussbock, a new embarrassingly parallel iterative algorithm for cosmological parameter estimation designed for an era of cheap parallel computing resources. Gaussbock uses Bayesian nonparametrics and truncated importance sampling to accurately draw samples from posterior distributions with an orders-of-magnitude speed-up in wall time over alternative methods. Contemporary problems in this area often suffer from both increased computational costs due to high-dimensional parameter spaces and consequent excessive time requirements, as well as the need for fine tuning of proposal distributions or sampling parameters. Gaussbock is designed specifically with these issues in mind. We explore and validate the performance and convergence of the algorithm on a fast approximation to the Dark Energy Survey Year 1 (DES Y1) posterior, finding reasonable scaling behavior with the number of parameters. We then test on the full DES Y1 posterior using large-scale supercomputing facilities, and recover reasonable agreement with previous chains, although the algorithm can underestimate the tails of poorly-constrained parameters. Additionally, we discuss and demonstrate how Gaussbock recovers complex posterior shapes very well at lower dimensions, but faces challenges to perform well on such distributions in higher dimensions. In addition, we provide the community with a user-friendly software tool for accelerated cosmological parameter estimation based on the methodology described in this paper.

</details>

<details>

<summary>2020-05-16 10:01:54 - BART-based inference for Poisson processes</summary>

- *Stamatina Lamprinakou, Emma McCoy, Mauricio Barahona, Axel Gandy, Seth Flaxman, Sarah Filippi*

- `2005.07927v1` - [abs](http://arxiv.org/abs/2005.07927v1) - [pdf](http://arxiv.org/pdf/2005.07927v1)

> The effectiveness of Bayesian Additive Regression Trees (BART) has been demonstrated in a variety of contexts including non parametric regression and classification. Here we introduce a BART scheme for estimating the intensity of inhomogeneous Poisson Processes. Poisson intensity estimation is a vital task in various applications including medical imaging, astrophysics and network traffic analysis. Our approach enables full posterior inference of the intensity in a nonparametric regression setting. We demonstrate the performance of our scheme through simulation studies on synthetic and real datasets in one and two dimensions, and compare our approach to alternative approaches.

</details>

<details>

<summary>2020-05-16 21:40:47 - Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks</summary>

- *Meet P. Vadera, Brian Jalaian, Benjamin M. Marlin*

- `2005.08110v1` - [abs](http://arxiv.org/abs/2005.08110v1) - [pdf](http://arxiv.org/pdf/2005.08110v1)

> In this paper, we present a general framework for distilling expectations with respect to the Bayesian posterior distribution of a deep neural network classifier, extending prior work on the Bayesian Dark Knowledge framework. The proposed framework takes as input "teacher" and student model architectures and a general posterior expectation of interest. The distillation method performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples. We focus on the posterior predictive distribution and expected entropy as distillation targets. We investigate several aspects of this framework including the impact of uncertainty and the choice of student model architecture. We study methods for student model architecture search from a speed-storage-accuracy perspective and evaluate down-stream tasks leveraging entropy distillation including uncertainty ranking and out-of-distribution detection.

</details>

<details>

<summary>2020-05-17 08:44:45 - Posterior properties of the Weibull distribution for censored data</summary>

- *Eduardo Ramos, Pedro L. Ramos*

- `2005.08195v1` - [abs](http://arxiv.org/abs/2005.08195v1) - [pdf](http://arxiv.org/pdf/2005.08195v1)

> The Weibull distribution is one of the most used tools in reliability analysis. In this paper, assuming a Bayesian approach, we propose necessary and sufficient conditions to verify when improper priors lead to proper posteriors for the parameters of the Weibull distribution in the presence of complete or right-censored data. Additionally, we proposed sufficient conditions to verify if the obtained posterior moments are finite. These results can be achieved by checking the behavior of the improper priors, which are applied in different objective priors to illustrate the usefulness of the new results. As an application of our theorem, we prove that if the improper prior leads to a proper posterior, the posterior mean, as well as other higher moments of the scale parameter, are not finite and, therefore, should not be used.

</details>

<details>

<summary>2020-05-17 15:14:57 - Latent Space Approaches to Community Detection in Dynamic Networks</summary>

- *Daniel K. Sewell, Yuguo Chen*

- `2005.08276v1` - [abs](http://arxiv.org/abs/2005.08276v1) - [pdf](http://arxiv.org/pdf/2005.08276v1)

> Embedding dyadic data into a latent space has long been a popular approach to modeling networks of all kinds. While clustering has been done using this approach for static networks, this paper gives two methods of community detection within dynamic network data, building upon the distance and projection models previously proposed in the literature. Our proposed approaches capture the time-varying aspect of the data, can model directed or undirected edges, inherently incorporate transitivity and account for each actor's individual propensity to form edges. We provide Bayesian estimation algorithms, and apply these methods to a ranked dynamic friendship network and world export/import data.

</details>

<details>

<summary>2020-05-17 15:26:47 - Network Autocorrelation Models with Egocentric Data</summary>

- *Daniel K. Sewell*

- `2005.09487v1` - [abs](http://arxiv.org/abs/2005.09487v1) - [pdf](http://arxiv.org/pdf/2005.09487v1)

> Network autocorrelation models have been widely used for decades to model the joint distribution of the attributes of a network's actors. This class of models can estimate both the effect of individual characteristics as well as the network effect, or social influence, on some actor attribute of interest. Collecting data on the entire network, however, is very often infeasible or impossible if the network boundary is unknown or difficult to define. Obtaining egocentric network data overcomes these obstacles, but as of yet there has been no clear way to model this type of data and still appropriately capture the network effect on the actor attributes in a way that is compatible with a joint distribution on the full network data. This paper adapts the class of network autocorrelation models to handle egocentric data. The proposed methods thus incorporate the complex dependence structure of the data induced by the network rather than simply using ad hoc measures of the egos' networks to model the mean structure, and can estimate the network effect on the actor attribute of interest. The vast quantities of unknown information about the network can be succinctly represented in such a way that only depends on the number of alters in the egocentric network data and not on the total number of actors in the network. Estimation is done within a Bayesian framework. A simulation study is performed to evaluate the estimation performance, and an egocentric data set is analyzed where the aim is to determine if there is a network effect on environmental mastery, an important aspect of psychological well-being.

</details>

<details>

<summary>2020-05-17 22:56:10 - A Symmetric Prior for Multinomial Probit Models</summary>

- *Lane F. Burgette, David Puelz, P. Richard Hahn*

- `1912.10334v4` - [abs](http://arxiv.org/abs/1912.10334v4) - [pdf](http://arxiv.org/pdf/1912.10334v4)

> Fitted probabilities from widely used Bayesian multinomial probit models can depend strongly on the choice of a base category, which is used to uniquely identify the parameters of the model. This paper proposes a novel identification strategy, and associated prior distribution for the model parameters, that renders the prior symmetric with respect to relabeling the outcome categories. The new prior permits an efficient Gibbs algorithm that samples rank-deficient covariance matrices without resorting to Metropolis-Hastings updates.

</details>

<details>

<summary>2020-05-18 02:43:26 - Deep Learning and Bayesian Deep Learning Based Gender Prediction in Multi-Scale Brain Functional Connectivity</summary>

- *Gengyan Zhao, Gyujoon Hwang, Cole J. Cook, Fang Liu, Mary E. Meyerand, Rasmus M. Birn*

- `2005.08431v1` - [abs](http://arxiv.org/abs/2005.08431v1) - [pdf](http://arxiv.org/pdf/2005.08431v1)

> Brain gender differences have been known for a long time and are the possible reason for many psychological, psychiatric and behavioral differences between males and females. Predicting genders from brain functional connectivity (FC) can build the relationship between brain activities and gender, and extracting important gender related FC features from the prediction model offers a way to investigate the brain gender difference. Current predictive models applied to gender prediction demonstrate good accuracies, but usually extract individual functional connections instead of connectivity patterns in the whole connectivity matrix as features. In addition, current models often omit the effect of the input brain FC scale on prediction and cannot give any model uncertainty information. Hence, in this study we propose to predict gender from multiple scales of brain FC with deep learning, which can extract full FC patterns as features. We further develop the understanding of the feature extraction mechanism in deep neural network (DNN) and propose a DNN feature ranking method to extract the highly important features based on their contributions to the prediction. Moreover, we apply Bayesian deep learning to the brain FC gender prediction, which as a probabilistic model can not only make accurate predictions but also generate model uncertainty for each prediction. Experiments were done on the high-quality Human Connectome Project S1200 release dataset comprising the resting state functional MRI data of 1003 healthy adults. First, DNN reaches 83.0%, 87.6%, 92.0%, 93.5% and 94.1% accuracies respectively with the FC input derived from 25, 50, 100, 200, 300 independent component analysis (ICA) components. DNN outperforms the conventional machine learning methods on the 25-ICA-component scale FC, but the linear machine learning method catches up as the number of ICA components increases...

</details>

<details>

<summary>2020-05-18 09:32:24 - A Bayesian Multi-Layered Record Linkage Procedure to Analyze Functional Status of Medicare Patients with Traumatic Brain Injury</summary>

- *Mingyang Shan, Kali Thomas, Roee Gutman*

- `2005.08549v1` - [abs](http://arxiv.org/abs/2005.08549v1) - [pdf](http://arxiv.org/pdf/2005.08549v1)

> Understanding the association between injury severity and patients' potential for recovery is crucial to providing better care for patients with traumatic brain injury (TBI). Estimation of this relationship requires clinical information on injury severity, patient demographics, and healthcare utilization, which are often obtained from separate data sources. Because of privacy and confidentiality regulations, these data sources do not include unique identifiers to link records across data sources. Record linkage is a process to identify records that represent the same entity across data sources in the absence of unique identifiers. These processes commonly rely on agreement between variables that appear in both data sources to link records. However, when the number of records in each file is large, this task is computationally intensive and may result in false links. Blocking is a data partitioning technique that reduces the number of possible links that should be considered. Healthcare providers can be used as blocks in applications of record linkage with healthcare datasets. However, providers may not be uniquely identified across files. We propose a Bayesian record linkage procedure that simultaneously performs block-level and record-level linkage. This iterative approach incorporates the record-level linkage within block pairs to improve the accuracy of the block-level linkage. Subsequently, the algorithm improves record-level linkage using the accurate partitioning of the linkage space through blocking. We demonstrate that our proposed method provides improved performance compared to existing Bayesian record linkage methods that do not incorporate blocking. The proposed procedure is then used to merge registry data from the National Trauma Data Bank with Medicare claims data to estimate the relationship between injury severity and TBI patients' recovery.

</details>

<details>

<summary>2020-05-18 12:19:24 - Scalable Variational Gaussian Process Regression Networks</summary>

- *Shibo Li, Wei Xing, Mike Kirby, Shandian Zhe*

- `2003.11489v2` - [abs](http://arxiv.org/abs/2003.11489v2) - [pdf](http://arxiv.org/pdf/2003.11489v2)

> Gaussian process regression networks (GPRN) are powerful Bayesian models for multi-output regression, but their inference is intractable. To address this issue, existing methods use a fully factorized structure (or a mixture of such structures) over all the outputs and latent functions for posterior approximation, which, however, can miss the strong posterior dependencies among the latent variables and hurt the inference quality. In addition, the updates of the variational parameters are inefficient and can be prohibitively expensive for a large number of outputs. To overcome these limitations, we propose a scalable variational inference algorithm for GPRN, which not only captures the abundant posterior dependencies but also is much more efficient for massive outputs. We tensorize the output space and introduce tensor/matrix-normal variational posteriors to capture the posterior correlations and to reduce the parameters. We jointly optimize all the parameters and exploit the inherent Kronecker product structure in the variational model evidence lower bound to accelerate the computation. We demonstrate the advantages of our method in several real-world applications.

</details>

<details>

<summary>2020-05-18 14:08:49 - Sparse Methods for Automatic Relevance Determination</summary>

- *Samuel H. Rudy, Themistoklis P. Sapsis*

- `2005.08741v1` - [abs](http://arxiv.org/abs/2005.08741v1) - [pdf](http://arxiv.org/pdf/2005.08741v1)

> This work considers methods for imposing sparsity in Bayesian regression with applications in nonlinear system identification. We first review automatic relevance determination (ARD) and analytically demonstrate the need to additional regularization or thresholding to achieve sparse models. We then discuss two classes of methods, regularization based and thresholding based, which build on ARD to learn parsimonious solutions to linear problems. In the case of orthogonal covariates, we analytically demonstrate favorable performance with regards to learning a small set of active terms in a linear system with a sparse solution. Several example problems are presented to compare the set of proposed methods in terms of advantages and limitations to ARD in bases with hundreds of elements. The aim of this paper is to analyze and understand the assumptions that lead to several algorithms and to provide theoretical and empirical results so that the reader may gain insight and make more informed choices regarding sparse Bayesian regression.

</details>

<details>

<summary>2020-05-18 16:01:35 - Bayesian Optimization on Large Graphs via a Graph Convolutional Generative Model: Application in Cardiac Model Personalization</summary>

- *Jwala Dhamala, Sandesh Ghimire, John L. Sapp, B. Milan Horacek, Linwei Wang*

- `1907.01406v2` - [abs](http://arxiv.org/abs/1907.01406v2) - [pdf](http://arxiv.org/pdf/1907.01406v2)

> Personalization of cardiac models involves the optimization of organ tissue properties that vary spatially over the non-Euclidean geometry model of the heart. To represent the high-dimensional (HD) unknown of tissue properties, most existing works rely on a low-dimensional (LD) partitioning of the geometrical model. While this exploits the geometry of the heart, it is of limited expressiveness to allow partitioning that is small enough for effective optimization. Recently, a variational auto-encoder (VAE) was utilized as a more expressive generative model to embed the HD optimization into the LD latent space. Its Euclidean nature, however, neglects the rich geometrical information in the heart. In this paper, we present a novel graph convolutional VAE to allow generative modeling of non-Euclidean data, and utilize it to embed Bayesian optimization of large graphs into a small latent space. This approach bridges the gap of previous works by introducing an expressive generative model that is able to incorporate the knowledge of spatial proximity and hierarchical compositionality of the underlying geometry. It further allows transferring of the learned features across different geometries, which was not possible with a regular VAE. We demonstrate these benefits of the presented method in synthetic and real data experiments of estimating tissue excitability in a cardiac electrophysiological model.

</details>

<details>

<summary>2020-05-18 18:25:52 - B-CONCORD -- A scalable Bayesian high-dimensional precision matrix estimation procedure</summary>

- *Peyman Jalali, Kshitij Khare, George Michailidis*

- `2005.09017v1` - [abs](http://arxiv.org/abs/2005.09017v1) - [pdf](http://arxiv.org/pdf/2005.09017v1)

> Sparse estimation of the precision matrix under high-dimensional scaling constitutes a canonical problem in statistics and machine learning. Numerous regression and likelihood based approaches, many frequentist and some Bayesian in nature have been developed. Bayesian methods provide direct uncertainty quantification of the model parameters through the posterior distribution and thus do not require a second round of computations for obtaining debiased estimates of the model parameters and their confidence intervals. However, they are computationally expensive for settings involving more than 500 variables. To that end, we develop B-CONCORD for the problem at hand, a Bayesian analogue of the CONvex CORrelation selection methoD (CONCORD) introduced by Khare et al. (2015). B-CONCORD leverages the CONCORD generalized likelihood function together with a spike-and-slab prior distribution to induce sparsity in the precision matrix parameters. We establish model selection and estimation consistency under high-dimensional scaling; further, we develop a procedure that refits only the non-zero parameters of the precision matrix, leading to significant improvements in the estimates in finite samples. Extensive numerical work illustrates the computational scalability of the proposed approach vis-a-vis competing Bayesian methods, as well as its accuracy.

</details>

<details>

<summary>2020-05-18 20:26:50 - Physiological Gaussian Process Priors for the Hemodynamics in fMRI Analysis</summary>

- *Josef Wilzén, Anders Eklund, Mattias Villani*

- `1708.06152v3` - [abs](http://arxiv.org/abs/1708.06152v3) - [pdf](http://arxiv.org/pdf/1708.06152v3)

> Background: Inference from fMRI data faces the challenge that the hemodynamic system that relates neural activity to the observed BOLD fMRI signal is unknown.   New Method: We propose a new Bayesian model for task fMRI data with the following features: (i) joint estimation of brain activity and the underlying hemodynamics, (ii) the hemodynamics is modeled nonparametrically with a Gaussian process (GP) prior guided by physiological information and (iii) the predicted BOLD is not necessarily generated by a linear time-invariant (LTI) system. We place a GP prior directly on the predicted BOLD response, rather than on the hemodynamic response function as in previous literature. This allows us to incorporate physiological information via the GP prior mean in a flexible way, and simultaneously gives us the nonparametric flexibility of the GP.   Results: Results on simulated data show that the proposed model is able to discriminate between active and non-active voxels also when the GP prior deviates from the true hemodynamics. Our model finds time varying dynamics when applied to real fMRI data.   Comparison with Existing Method(s): The proposed model is better at detecting activity in simulated data than standard models, without inflating the false positive rate. When applied to real fMRI data, our GP model in several cases finds brain activity where previously proposed LTI models does not.   Conclusions: We have proposed a new non-linear model for the hemodynamics in task fMRI, that is able to detect active voxels, and gives the opportunity to ask new kinds of questions related to hemodynamics.

</details>

<details>

<summary>2020-05-19 03:29:18 - Mass-shifting phenomenon of truncated multivariate normal priors</summary>

- *Shuang Zhou, Pallavi Ray, Debdeep Pati, Anirban Bhattacharya*

- `2001.09391v2` - [abs](http://arxiv.org/abs/2001.09391v2) - [pdf](http://arxiv.org/pdf/2001.09391v2)

> We show that lower-dimensional marginal densities of dependent zero-mean normal distributions truncated to the positive orthant exhibit a mass-shifting phenomenon. Despite the truncated multivariate normal density having a mode at the origin, the marginal density assigns increasingly small mass near the origin as the dimension increases. The phenomenon accentuates with stronger correlation between the random variables. A precise quantification characterizing the role of the dimension as well as the dependence is provided. This surprising behavior has serious implications towards Bayesian constrained estimation and inference, where the prior, in addition to having a full support, is required to assign a substantial probability near the origin to capture at parts of the true function of interest. Without further modification, we show that truncated normal priors are not suitable for modeling at regions and propose a novel alternative strategy based on shrinking the coordinates using a multiplicative scale parameter. The proposed shrinkage prior is empirically shown to guard against the mass shifting phenomenon while retaining computational efficiency.

</details>

<details>

<summary>2020-05-19 08:05:40 - A note on 'Collider bias undermines our understanding of COVID-19 disease risk and severity' and how causal Bayesian networks both expose and resolve the problem</summary>

- *Norman Fenton*

- `2005.08608v2` - [abs](http://arxiv.org/abs/2005.08608v2) - [pdf](http://arxiv.org/pdf/2005.08608v2)

> An important recent preprint by Griffith et al highlights how 'collider bias' in studies of COVID19 undermines our understanding of the disease risk and severity. This is typically caused by the data being restricted to people who have undergone COVID19 testing, among whom healthcare workers are overrepresented. For example, collider bias caused by smokers being underrepresented in the dataset may (at least partly) explain empirical results that suggest smoking reduces the risk of COVID19. We extend the work of Griffith et al making more explicit use of graphical causal models to interpret observed data. We show that their smoking example can be clarified and improved using Bayesian network models with realistic data and assumptions. We show that there is an even more fundamental problem for risk factors like 'stress' which, unlike smoking, is more rather than less prevalent among healthcare workers; in this case, because of a combination of collider bias from the biased dataset and the fact that 'healthcare worker' is a confounding variable, it is likely that studies will wrongly conclude that stress reduces rather than increases the risk of COVID19. Indeed, "being in close contact with COVID19 people" reduces the risk of COVID19. To avoid such potentially erroneous conclusions, any analysis of observational data must take account of the underlying causal structure including colliders and confounders. If analysts fail to do this explicitly then any conclusions they make about the effect of specific risk factors on COVID19 are likely to be flawed.

</details>

<details>

<summary>2020-05-19 14:34:30 - Implementability of Honest Multi-Agent Sequential Decision-Making with Dynamic Population</summary>

- *Tao Zhang, Quanyan Zhu*

- `2003.03173v2` - [abs](http://arxiv.org/abs/2003.03173v2) - [pdf](http://arxiv.org/pdf/2003.03173v2)

> We study the design of decision-making mechanism for resource allocations over a multi-agent system in a dynamic environment. Agents' privately observed preference over resources evolves over time and the population is dynamic due to the adoption of stopping rules. The proposed model designs the rules of encounter for agents participating in the dynamic mechanism by specifying an allocation rule and three payment rules to elicit agents' coupled decision makings of honest preference reporting and optimal stopping over multiple periods. The mechanism provides a special posted-price payment rule that depends only on each agent's realized stopping time to directly influence the population dynamics. This letter focuses on the theoretical implementability of the rules in perfect Bayesian Nash equilibrium and characterizes necessary and sufficient conditions to guarantee agents' honest equilibrium behaviors over periods. We provide the design principles to construct the payments in terms of the allocation rules and identify the restrictions of the designer's ability to influence the population dynamics. The established conditions make the designer's problem of finding multiple rules to determine an optimal allocation rule.

</details>

<details>

<summary>2020-05-19 16:42:47 - Increasing Domain Infill Asymptotics for Stochastic Differential Equations Driven by Fractional Brownian Motion</summary>

- *Trisha Maitra, Sourabh Bhattacharya*

- `2005.09577v1` - [abs](http://arxiv.org/abs/2005.09577v1) - [pdf](http://arxiv.org/pdf/2005.09577v1)

> Although statistical inference in stochastic differential equations (SDEs) driven by Wiener process has received significant attention in the literature, inference in those driven by fractional Brownian motion seem to have seen much less development in comparison, despite their importance in modeling long range dependence. In this article, we consider both classical and Bayesian inference in such fractional Brownian motion based SDEs. In particular, we consider asymptotic inference for two parameters in this regard; a multiplicative parameter associated with the drift function, and the so-called "Hurst parameter" of the fractional Brownian motion, when the time domain tends to infinity. For unknown Hurst parameter, the likelihood does not lend itself amenable to the popular Girsanov form, rendering usual asymptotic development difficult. As such, we develop increasing domain infill asymptotic theory, by discretizing the SDE. In this setup, we establish consistency and asymptotic normality of the maximum likelihood estimators, as well as consistency and asymptotic normality of the Bayesian posterior distributions. However, classical or Bayesian asymptotic normality with respect to the Hurst parameter could not be established. We supplement our theoretical investigations with simulation studies in a non-asymptotic setup, prescribing suitable methodologies for classical and Bayesian analyses of SDEs driven by fractional Brownian motion.

</details>

<details>

<summary>2020-05-19 19:52:09 - A Bayesian Markov model with Pólya-Gamma sampling for estimating individual behavior transition probabilities from accelerometer classifications</summary>

- *Toryn L. J. Schafer, Christopher K. Wikle, Jay A. VonBank, Bart M. Ballard, Mitch D. Weegman*

- `1908.02806v3` - [abs](http://arxiv.org/abs/1908.02806v3) - [pdf](http://arxiv.org/pdf/1908.02806v3)

> The use of accelerometers in wildlife tracking provides a fine-scale data source for understanding animal behavior and decision-making. Current methods in movement ecology focus on behavior as a driver of movement mechanisms. Our Markov model is a flexible and efficient method for inference related to effects on behavior that considers dependence between current and past behaviors. We applied this model to behavior data from six greater white-fronted geese (Anser albifrons frontalis) during spring migration in mid-continent North America and considered likely drivers of behavior, including habitat, weather and time of day effects. We modeled the transitions between flying, feeding, stationary and walking behavior states using a first-order Bayesian Markov model. We introduced P\'olya-Gamma latent variables for automatic sampling of the covariate coefficients from the posterior distribution and we calculated the odds ratios from the posterior samples. Our model provides a unifying framework for including both acceleration and Global Positioning System data. We found significant differences in behavioral transition rates among habitat types, diurnal behavior and behavioral changes due to weather. Our model provides straightforward inference of behavioral time allocation across used habitats, which is not amenable in activity budget or resource selection frameworks.

</details>

<details>

<summary>2020-05-19 20:13:52 - Weight Priors for Learning Identity Relations</summary>

- *Radha Kopparti, Tillman Weyde*

- `2003.03125v2` - [abs](http://arxiv.org/abs/2003.03125v2) - [pdf](http://arxiv.org/pdf/2003.03125v2)

> Learning abstract and systematic relations has been an open issue in neural network learning for over 30 years. It has been shown recently that neural networks do not learn relations based on identity and are unable to generalize well to unseen data. The Relation Based Pattern (RBP) approach has been proposed as a solution for this problem. In this work, we extend RBP by realizing it as a Bayesian prior on network weights to model the identity relations. This weight prior leads to a modified regularization term in otherwise standard network learning. In our experiments, we show that the Bayesian weight priors lead to perfect generalization when learning identity based relations and do not impede general neural network learning. We believe that the approach of creating an inductive bias with weight priors can be extended easily to other forms of relations and will be beneficial for many other learning tasks.

</details>

<details>

<summary>2020-05-20 08:23:48 - Accounting for Input Noise in Gaussian Process Parameter Retrieval</summary>

- *J. Emmanuel Johnson, Valero Laparra, Gustau Camps-Valls*

- `2005.09907v1` - [abs](http://arxiv.org/abs/2005.09907v1) - [pdf](http://arxiv.org/pdf/2005.09907v1)

> Gaussian processes (GPs) are a class of Kernel methods that have shown to be very useful in geoscience and remote sensing applications for parameter retrieval, model inversion, and emulation. They are widely used because they are simple, flexible, and provide accurate estimates. GPs are based on a Bayesian statistical framework which provides a posterior probability function for each estimation. Therefore, besides the usual prediction (given in this case by the mean function), GPs come equipped with the possibility to obtain a predictive variance (i.e., error bars, confidence intervals) for each prediction. Unfortunately, the GP formulation usually assumes that there is no noise in the inputs, only in the observations. However, this is often not the case in earth observation problems where an accurate assessment of the measuring instrument error is typically available, and where there is huge interest in characterizing the error propagation through the processing pipeline. In this letter, we demonstrate how one can account for input noise estimates using a GP model formulation which propagates the error terms using the derivative of the predictive mean function. We analyze the resulting predictive variance term and show how they more accurately represent the model error in a temperature prediction problem from infrared sounding data.

</details>

<details>

<summary>2020-05-20 09:59:11 - Decompounding discrete distributions: A non-parametric Bayesian approach</summary>

- *Shota Gugushvili, Ester Mariucci, Frank van der Meulen*

- `1903.11142v2` - [abs](http://arxiv.org/abs/1903.11142v2) - [pdf](http://arxiv.org/pdf/1903.11142v2)

> Suppose that a compound Poisson process is observed discretely in time and assume that its jump distribution is supported on the set of natural numbers. In this paper we propose a non-parametric Bayesian approach to estimate the intensity of the underlying Poisson process and the distribution of the jumps. We provide a MCMC scheme for obtaining samples from the posterior. We apply our method on both simulated and real data examples, and compare its performance with the frequentist plug-in estimator proposed by Buchmann and Gr\"ubel. On a theoretical side, we study the posterior from the frequentist point of view and prove that as the sample size $n\rightarrow\infty$, it contracts around the `true', data-generating parameters at rate $1/\sqrt{n}$, up to a $\log n$ factor.

</details>

<details>

<summary>2020-05-20 12:50:58 - Uncertainty representation for early phase clinical test evaluations: a case study</summary>

- *Sara Graziadio, Kevin J. Wilson*

- `2005.10011v1` - [abs](http://arxiv.org/abs/2005.10011v1) - [pdf](http://arxiv.org/pdf/2005.10011v1)

> In early clinical test evaluations the potential benefits of the introduction of a new technology into the healthcare system are assessed in the challenging situation of limited available empirical data. The aim of these evaluations is to provide additional evidence for the decision maker, who is typically a funder or the company developing the test, to evaluate which technologies should progress to the next stage of evaluation. In this paper we consider the evaluation of a diagnostic test for patients suffering from Chronic Obstructive Pulmonary Disease (COPD). We describe the use of graphical models, prior elicitation and uncertainty analysis to provide the required evidence to allow the test to progress to the next stage of evaluation. We specifically discuss inferring an influence diagram from a care pathway and conducting an elicitation exercise to allow specification of prior distributions over all model parameters. We describe the uncertainty analysis, via Monte Carlo simulation, which allowed us to demonstrate that the potential value of the test was robust to uncertainties. This paper provides a case study illustrating how a careful Bayesian analysis can be used to enhance early clinical test evaluations.

</details>

<details>

<summary>2020-05-20 15:52:10 - Bayesian design and analysis of external pilot trials for complex interventions</summary>

- *Duncan T. Wilson, James M. S. Wason, Julia Brown, Amanda J. Farrin, Rebecca E. A. Walwyn*

- `1908.05955v2` - [abs](http://arxiv.org/abs/1908.05955v2) - [pdf](http://arxiv.org/pdf/1908.05955v2)

> External pilot trials of complex interventions are used to help determine if and how a confirmatory trial should be undertaken, providing estimates of parameters such as recruitment, retention and adherence rates. The decision to progress to the confirmatory trial is typically made by comparing these estimates to pre-specified thresholds known as progression criteria, although the statistical properties of such decision rules are rarely assessed. Such assessment is complicated by several methodological challenges, including the simultaneous evaluation of multiple endpoints, complex multi-level models, small sample sizes, and uncertainty in nuisance parameters. In response to these challenges, we describe a Bayesian approach to the design and analysis of external pilot trials. We show how progression decisions can be made by minimising the expected value of a loss function, defined over the whole parameter space to allow for preferences and trade-offs between multiple parameters to be articulated and used in the decision making process. The assessment of preferences is kept feasible by using a piecewise constant parameterisation of the loss function, the parameters of which are chosen at the design stage to lead to desirable operating characteristics. We describe a flexible, yet computationally intensive, nested Monte Carlo algorithm for estimating operating characteristics. The method is used to revisit the design of an external pilot trial of a complex intervention designed to increase the physical activity of care home residents.

</details>

<details>

<summary>2020-05-20 18:38:44 - Adjusting for Partial Compliance in SMARTs: a Bayesian Semiparametric Approach</summary>

- *William J. Artman, Ashkan Ertefaie, Kevin G. Lynch, James R. McKay, Brent A. Johnson*

- `2005.10307v1` - [abs](http://arxiv.org/abs/2005.10307v1) - [pdf](http://arxiv.org/pdf/2005.10307v1)

> The cyclical and heterogeneous nature of many substance use disorders highlights the need to adapt the type or the dose of treatment to accommodate the specific and changing needs of individuals. The Adaptive Treatment for Alcohol and Cocaine Dependence study (ENGAGE) is a multi-stage randomized trial that aimed to provide longitudinal data for constructing treatment strategies to improve patients' engagement in therapy. However, the high rate of noncompliance and lack of analytic tools to account for noncompliance have impeded researchers from using the data to achieve the main goal of the trial. We overcome this issue by defining our target parameter as the mean outcome under different treatment strategies for given potential compliance strata and propose a Bayesian semiparametric model to estimate this quantity. While it adds substantial complexities to the analysis, one important feature of our work is that we consider partial rather than binary compliance classes which is more relevant in longitudinal studies. We assess the performance of our method through comprehensive simulation studies. We illustrate its application on the ENGAGE study and demonstrate that the optimal treatment strategy depends on compliance strata.

</details>

<details>

<summary>2020-05-20 21:18:55 - varstan: An R package for Bayesian analysis of structured time series models with Stan</summary>

- *Izhar Asael Alonzo Matamoros, Cristian Andres Cruz Torres*

- `2005.10361v1` - [abs](http://arxiv.org/abs/2005.10361v1) - [pdf](http://arxiv.org/pdf/2005.10361v1)

> varstan is an \proglang{R} package for Bayesian analysis of time series models using \proglang{Stan}. The package offers a dynamic way to choose a model, define priors in a wide range of distributions, check model's fit, and forecast with the m-steps ahead predictive distribution. The users can widely choose between implemented models such as \textit{multiplicative seasonal ARIMA, dynamic regression, random walks, GARCH, dynamic harmonic regressions,VARMA, stochastic Volatility Models, and generalized t-student with unknown degree freedom GARCH models}. Every model constructor in \pkg{varstan} defines weakly informative priors, but prior specifications can be changed in a dynamic and flexible way, so the prior distributions reflect the parameter's initial beliefs. For model selection, the package offers the classical information criteria: AIC, AICc, BIC, DIC, Bayes factor. And more recent criteria such as Widely-applicable information criteria (\textit{WAIC}), and the Bayesian leave one out cross-validation (\textit{loo}). In addition, a Bayesian version for automatic order selection in seasonal ARIMA and dynamic regression models can be used as an initial step for the time series analysis.

</details>

<details>

<summary>2020-05-21 08:56:50 - HyperSTAR: Task-Aware Hyperparameters for Deep Networks</summary>

- *Gaurav Mittal, Chang Liu, Nikolaos Karianakis, Victor Fragoso, Mei Chen, Yun Fu*

- `2005.10524v1` - [abs](http://arxiv.org/abs/2005.10524v1) - [pdf](http://arxiv.org/pdf/2005.10524v1)

> While deep neural networks excel in solving visual recognition tasks, they require significant effort to find hyperparameters that make them work optimally. Hyperparameter Optimization (HPO) approaches have automated the process of finding good hyperparameters but they do not adapt to a given task (task-agnostic), making them computationally inefficient. To reduce HPO time, we present HyperSTAR (System for Task Aware Hyperparameter Recommendation), a task-aware method to warm-start HPO for deep neural networks. HyperSTAR ranks and recommends hyperparameters by predicting their performance conditioned on a joint dataset-hyperparameter space. It learns a dataset (task) representation along with the performance predictor directly from raw images in an end-to-end fashion. The recommendations, when integrated with an existing HPO method, make it task-aware and significantly reduce the time to achieve optimal performance. We conduct extensive experiments on 10 publicly available large-scale image classification datasets over two different network architectures, validating that HyperSTAR evaluates 50% less configurations to achieve the best performance compared to existing methods. We further demonstrate that HyperSTAR makes Hyperband (HB) task-aware, achieving the optimal accuracy in just 25% of the budget required by both vanilla HB and Bayesian Optimized HB~(BOHB).

</details>

<details>

<summary>2020-05-21 17:17:03 - On the Locality of the Natural Gradient for Deep Learning</summary>

- *Nihat Ay*

- `2005.10791v1` - [abs](http://arxiv.org/abs/2005.10791v1) - [pdf](http://arxiv.org/pdf/2005.10791v1)

> We study the natural gradient method for learning in deep Bayesian networks, including neural networks. There are two natural geometries associated with such learning systems consisting of visible and hidden units. One geometry is related to the full system, the other one to the visible sub-system. These two geometries imply different natural gradients. In a first step, we demonstrate a great simplification of the natural gradient with respect to the first geometry, due to locality properties of the Fisher information matrix. This simplification does not directly translate to a corresponding simplification with respect to the second geometry. We develop the theory for studying the relation between the two versions of the natural gradient and outline a method for the simplification of the natural gradient with respect to the second geometry based on the first one. This method suggests to incorporate a recognition model as an auxiliary model for the efficient application of the natural gradient method in deep networks.

</details>

<details>

<summary>2020-05-21 20:59:11 - Global Optimization of Gaussian processes</summary>

- *Artur M. Schweidtmann, Dominik Bongartz, Daniel Grothe, Tim Kerkenhoff, Xiaopeng Lin, Jaromil Najman, Alexander Mitsos*

- `2005.10902v1` - [abs](http://arxiv.org/abs/2005.10902v1) - [pdf](http://arxiv.org/pdf/2005.10902v1)

> Gaussian processes~(Kriging) are interpolating data-driven models that are frequently applied in various disciplines. Often, Gaussian processes are trained on datasets and are subsequently embedded as surrogate models in optimization problems. These optimization problems are nonconvex and global optimization is desired. However, previous literature observed computational burdens limiting deterministic global optimization to Gaussian processes trained on few data points. We propose a reduced-space formulation for deterministic global optimization with trained Gaussian processes embedded. For optimization, the branch-and-bound solver branches only on the degrees of freedom and McCormick relaxations are propagated through explicit Gaussian process models. The approach also leads to significantly smaller and computationally cheaper subproblems for lower and upper bounding. To further accelerate convergence, we derive envelopes of common covariance functions for GPs and tight relaxations of acquisition functions used in Bayesian optimization including expected improvement, probability of improvement, and lower confidence bound. In total, we reduce computational time by orders of magnitude compared to state-of-the-art methods, thus overcoming previous computational burdens. We demonstrate the performance and scaling of the proposed method and apply it to Bayesian optimization with global optimization of the acquisition function and chance-constrained programming. The Gaussian process models, acquisition functions, and training scripts are available open-source within the "MeLOn - Machine Learning Models for Optimization" toolbox~(https://git.rwth-aachen.de/avt.svt/public/MeLOn).

</details>

<details>

<summary>2020-05-22 04:14:41 - Space-Time VON CRAMM: Evaluating Decision-Making in Tennis with Variational generatiON of Complete Resolution Arcs via Mixture Modeling</summary>

- *Stephanie Kovalchik, Martin Ingram, Kokum Weeratunga, Cagatay Goncu*

- `2005.12853v1` - [abs](http://arxiv.org/abs/2005.12853v1) - [pdf](http://arxiv.org/pdf/2005.12853v1)

> Sports tracking data are the high-resolution spatiotemporal observations of a competitive event. The growing collection of these data in professional sport allows us to address a fundamental problem of modern sport: how to attribute value to individual actions? Taking advantage of the smoothness of ball and player movement in tennis, we present a functional data framework for estimating expected shot value (ESV) in continuous time. Our approach is a three-step recipe: 1) a generative model for a full-resolution functional representation of ball and player trajectories using an infinite Bayesian Gaussian mixture model (GMM), 2) conditioning of the GMM on observed positional data, and 3) the prediction of shot outcomes given the functional encoding of a shot event. From the ESV we derive three metrics of central interest: value added with shot taking (VAST), Shot IQ, and value added with court coverage (VACC), which respectively attribute value to shot execution, shot selection and movement around the court. We rate player performance at the 2019 US Open on these advanced metrics and show how each adds a novel perspective to performance evaluation in tennis that goes beyond simple counts of outcomes by quantitatively assessing the decisions players make throughout a point.

</details>

<details>

<summary>2020-05-22 17:48:06 - Model Evidence with Fast Tree Based Quadrature</summary>

- *Thomas Foster, Chon Lok Lei, Martin Robinson, David Gavaghan, Ben Lambert*

- `2005.11300v1` - [abs](http://arxiv.org/abs/2005.11300v1) - [pdf](http://arxiv.org/pdf/2005.11300v1)

> High dimensional integration is essential to many areas of science, ranging from particle physics to Bayesian inference. Approximating these integrals is hard, due in part to the difficulty of locating and sampling from regions of the integration domain that make significant contributions to the overall integral. Here, we present a new algorithm called Tree Quadrature (TQ) that separates this sampling problem from the problem of using those samples to produce an approximation of the integral. TQ places no qualifications on how the samples provided to it are obtained, allowing it to use state-of-the-art sampling algorithms that are largely ignored by existing integration algorithms. Given a set of samples, TQ constructs a surrogate model of the integrand in the form of a regression tree, with a structure optimised to maximise integral precision. The tree divides the integration domain into smaller containers, which are individually integrated and aggregated to estimate the overall integral. Any method can be used to integrate each individual container, so existing integration methods, like Bayesian Monte Carlo, can be combined with TQ to boost their performance. On a set of benchmark problems, we show that TQ provides accurate approximations to integrals in up to 15 dimensions; and in dimensions 4 and above, it outperforms simple Monte Carlo and the popular Vegas method.

</details>

<details>

<summary>2020-05-22 18:55:02 - Dose-response modeling in high-throughput cancer drug screenings: An end-to-end approach</summary>

- *Wesley Tansey, Kathy Li, Haoran Zhang, Scott W. Linderman, Raul Rabadan, David M. Blei, Chris H. Wiggins*

- `1812.05691v2` - [abs](http://arxiv.org/abs/1812.05691v2) - [pdf](http://arxiv.org/pdf/1812.05691v2)

> Personalized cancer treatments based on the molecular profile of a patient's tumor are an emerging and exciting class of treatments in oncology. As genomic tumor profiling is becoming more common, targeted treatments to specific molecular alterations are gaining traction. To discover new potential therapeutics that may apply to broad classes of tumors matching some molecular pattern, experimentalists and pharmacologists rely on high-throughput, in-vitro screens of many compounds against many different cell lines. We propose a hierarchical Bayesian model of how cancer cell lines respond to drugs in these experiments and develop a method for fitting the model to real-world high-throughput screening data. Through a case study, the model is shown to capture nontrivial associations between molecular features and drug response, such as requiring both wild type TP53 and overexpression of MDM2 to be sensitive to Nutlin-3(a). In quantitative benchmarks, the model outperforms a standard approach in biology, with ~20% lower predictive error on held out data. When combined with a conditional randomization testing procedure, the model discovers biomarkers of therapeutic response that recapitulate known biology and suggest new avenues for investigation. All code for the paper is publicly available at https://github.com/tansey/deep-dose-response.

</details>

<details>

<summary>2020-05-22 20:58:26 - MANGO: A Python Library for Parallel Hyperparameter Tuning</summary>

- *Sandeep Singh Sandha, Mohit Aggarwal, Igor Fedorov, Mani Srivastava*

- `2005.11394v1` - [abs](http://arxiv.org/abs/2005.11394v1) - [pdf](http://arxiv.org/pdf/2005.11394v1)

> Tuning hyperparameters for machine learning algorithms is a tedious task, one that is typically done manually. To enable automated hyperparameter tuning, recent works have started to use techniques based on Bayesian optimization. However, to practically enable automated tuning for large scale machine learning training pipelines, significant gaps remain in existing libraries, including lack of abstractions, fault tolerance, and flexibility to support scheduling on any distributed computing framework. To address these challenges, we present Mango, a Python library for parallel hyperparameter tuning. Mango enables the use of any distributed scheduling framework, implements intelligent parallel search strategies, and provides rich abstractions for defining complex hyperparameter search spaces that are compatible with scikit-learn. Mango is comparable in performance to Hyperopt, another widely used library. Mango is available open-source and is currently used in production at Arm Research to provide state-of-art hyperparameter tuning capabilities.

</details>

<details>

<summary>2020-05-22 21:34:23 - A Bayesian framework for functional calibration of expensive computational models through non-isometric matching</summary>

- *Babak Farmanesh, Arash Pourhabib, Balabhaskar Balasundaram, Austin Buchanan*

- `1508.01240v4` - [abs](http://arxiv.org/abs/1508.01240v4) - [pdf](http://arxiv.org/pdf/1508.01240v4)

> We study statistical calibration, i.e., adjusting features of a computational model that are not observable or controllable in its associated physical system. We focus on functional calibration, which arises in many manufacturing processes where the unobservable features, called calibration variables, are a function of the input variables. A major challenge in many applications is that computational models are expensive and can only be evaluated a limited number of times. Furthermore, without making strong assumptions, the calibration variables are not identifiable. We propose Bayesian non-isometric matching calibration (BNMC) that allows calibration of expensive computational models with only a limited number of samples taken from a computational model and its associated physical system. BNMC replaces the computational model with a dynamic Gaussian process (GP) whose parameters are trained in the calibration procedure. To resolve the identifiability issue, we present the calibration problem from a geometric perspective of non-isometric curve to surface matching, which enables us to take advantage of combinatorial optimization techniques to extract necessary information for constructing prior distributions. Our numerical experiments demonstrate that in terms of prediction accuracy BNMC outperforms, or is comparable to, other existing calibration frameworks.

</details>

<details>

<summary>2020-05-23 18:50:44 - Bayesian Integrative Analysis and Prediction with Application to Atherosclerosis Cardiovascular Disease</summary>

- *Thierry Chekouo, Sandra E. Safo*

- `2005.11586v1` - [abs](http://arxiv.org/abs/2005.11586v1) - [pdf](http://arxiv.org/pdf/2005.11586v1)

> Cardiovascular diseases (CVD), including atherosclerosis CVD (ASCVD), are multifactorial diseases that present a major economic and social burden worldwide. Tremendous efforts have been made to understand traditional risk factors for ASCVD, but these risk factors account for only about half of all cases of ASCVD. It remains a critical need to identify nontraditional risk factors (e.g., genetic variants, genes) contributing to ASCVD. Further, incorporating functional knowledge in prediction models have the potential to reveal pathways associated with disease risk. We propose Bayesian hierarchical factor analysis models that associate multiple omics data, predict a clinical outcome, allow for prior functional information, and can accommodate clinical covariates. The models, motivated by available data and the need for other risk factors of ASCVD, are used for the integrative analysis of clinical, demographic, and multi-omics data to identify genetic variants, genes, and gene pathways potentially contributing to 10-year ASCVD risk in healthy adults. Our findings revealed several genetic variants, genes and gene pathways that were highly associated with ASCVD risk. Interestingly, some of these have been implicated in CVD risk. The others could be explored for their potential roles in CVD. Our findings underscore the merit in joint association and prediction models.

</details>

<details>

<summary>2020-05-23 22:08:59 - Improving multilevel regression and poststratification with structured priors</summary>

- *Yuxiang Gao, Lauren Kennedy, Daniel Simpson, Andrew Gelman*

- `1908.06716v4` - [abs](http://arxiv.org/abs/1908.06716v4) - [pdf](http://arxiv.org/pdf/1908.06716v4)

> A central theme in the field of survey statistics is estimating population-level quantities through data coming from potentially non-representative samples of the population. Multilevel Regression and Poststratification (MRP), a model-based approach, is gaining traction against the traditional weighted approach for survey estimates. MRP estimates are susceptible to bias if there is an underlying structure that the methodology does not capture. This work aims to provide a new framework for specifying structured prior distributions that lead to bias reduction in MRP estimates. We use simulation studies to explore the benefit of these prior distributions and demonstrate their efficacy on non-representative US survey data. We show that structured prior distributions offer absolute bias reduction and variance reduction for posterior MRP estimates in a large variety of data regimes.

</details>

<details>

<summary>2020-05-24 01:38:40 - A Bayesian Spatial Model for Imaging Genetics</summary>

- *Yin Song, Shufei Ge, Jiguo Cao, Liangliang Wang, Farouk S. Nathoo*

- `1901.00068v4` - [abs](http://arxiv.org/abs/1901.00068v4) - [pdf](http://arxiv.org/pdf/1901.00068v4)

> We develop a Bayesian bivariate spatial model for multivariate regression analysis applicable to studies examining the influence of genetic variation on brain structure. Our model is motivated by an imaging genetics study of the Alzheimer's Disease Neuroimaging Initiative (ADNI), where the objective is to examine the association between images of volumetric and cortical thickness values summarizing the structure of the brain as measured by magnetic resonance imaging (MRI) and a set of 486 SNPs from 33 Alzheimer's Disease (AD) candidate genes obtained from 632 subjects. A bivariate spatial process model is developed to accommodate the correlation structures typically seen in structural brain imaging data. First, we allow for spatial correlation on a graph structure in the imaging phenotypes obtained from a neighbourhood matrix for measures on the same hemisphere of the brain. Second, we allow for correlation in the same measures obtained from different hemispheres (left/right) of the brain. We develop a mean-field variational Bayes algorithm and a Gibbs sampling algorithm to fit the model. We also incorporate Bayesian false discovery rate (FDR) procedures to select SNPs. We implement the methodology in a new release of the R package bgsmtr. We show that the new spatial model demonstrates superior performance over a standard model in our application. Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu).

</details>

<details>

<summary>2020-05-24 03:17:07 - Bayesian Model Selection with Graph Structured Sparsity</summary>

- *Youngseok Kim, Chao Gao*

- `1902.03316v2` - [abs](http://arxiv.org/abs/1902.03316v2) - [pdf](http://arxiv.org/pdf/1902.03316v2)

> We propose a general algorithmic framework for Bayesian model selection. A spike-and-slab Laplacian prior is introduced to model the underlying structural assumption. Using the notion of effective resistance, we derive an EM-type algorithm with closed-form iterations to efficiently explore possible candidates for Bayesian model selection. The deterministic nature of the proposed algorithm makes it more scalable to large-scale and high-dimensional data sets compared with existing stochastic search algorithms. When applied to sparse linear regression, our framework recovers the EMVS algorithm [Rockova and George, 2014] as a special case. We also discuss extensions of our framework using tools from graph algebra to incorporate complex Bayesian models such as biclustering and submatrix localization. Extensive simulation studies and real data applications are conducted to demonstrate the superior performance of our methods over its frequentist competitors such as $\ell_0$ or $\ell_1$ penalization.

</details>

<details>

<summary>2020-05-24 08:21:11 - Maximum Entropy-Regularized Multi-Goal Reinforcement Learning</summary>

- *Rui Zhao, Xudong Sun, Volker Tresp*

- `1905.08786v3` - [abs](http://arxiv.org/abs/1905.08786v3) - [pdf](http://arxiv.org/pdf/1905.08786v3)

> In Multi-Goal Reinforcement Learning, an agent learns to achieve multiple goals with a goal-conditioned policy. During learning, the agent first collects the trajectories into a replay buffer, and later these trajectories are selected randomly for replay. However, the achieved goals in the replay buffer are often biased towards the behavior policies. From a Bayesian perspective, when there is no prior knowledge about the target goal distribution, the agent should learn uniformly from diverse achieved goals. Therefore, we first propose a novel multi-goal RL objective based on weighted entropy. This objective encourages the agent to maximize the expected return, as well as to achieve more diverse goals. Secondly, we developed a maximum entropy-based prioritization framework to optimize the proposed objective. For evaluation of this framework, we combine it with Deep Deterministic Policy Gradient, both with or without Hindsight Experience Replay. On a set of multi-goal robotic tasks of OpenAI Gym, we compare our method with other baselines and show promising improvements in both performance and sample-efficiency.

</details>

<details>

<summary>2020-05-24 10:50:15 - Multi-view Alignment and Generation in CCA via Consistent Latent Encoding</summary>

- *Yaxin Shi, Yuangang Pan, Donna Xu, Ivor W. Tsang*

- `2005.11716v1` - [abs](http://arxiv.org/abs/2005.11716v1) - [pdf](http://arxiv.org/pdf/2005.11716v1)

> Multi-view alignment, achieving one-to-one correspondence of multi-view inputs, is critical in many real-world multi-view applications, especially for cross-view data analysis problems. Recently, an increasing number of works study this alignment problem with Canonical Correlation Analysis (CCA). However, existing CCA models are prone to misalign the multiple views due to either the neglect of uncertainty or the inconsistent encoding of the multiple views. To tackle these two issues, this paper studies multi-view alignment from the Bayesian perspective. Delving into the impairments of inconsistent encodings, we propose to recover correspondence of the multi-view inputs by matching the marginalization of the joint distribution of multi-view random variables under different forms of factorization. To realize our design, we present Adversarial CCA (ACCA) which achieves consistent latent encodings by matching the marginalized latent encodings through the adversarial training paradigm. Our analysis based on conditional mutual information reveals that ACCA is flexible for handling implicit distributions. Extensive experiments on correlation analysis and cross-view generation under noisy input settings demonstrate the superiority of our model.

</details>

<details>

<summary>2020-05-24 13:00:59 - A New Spatial Count Data Model with Bayesian Additive Regression Trees for Accident Hot Spot Identification</summary>

- *Rico Krueger, Prateek Bansal, Prasad Buddhavarapu*

- `2005.11738v1` - [abs](http://arxiv.org/abs/2005.11738v1) - [pdf](http://arxiv.org/pdf/2005.11738v1)

> The identification of accident hot spots is a central task of road safety management. Bayesian count data models have emerged as the workhorse method for producing probabilistic rankings of hazardous sites in road networks. Typically, these methods assume simple linear link function specifications, which, however, limit the predictive power of a model. Furthermore, extensive specification searches are precluded by complex model structures arising from the need to account for unobserved heterogeneity and spatial correlations. Modern machine learning (ML) methods offer ways to automate the specification of the link function. However, these methods do not capture estimation uncertainty, and it is also difficult to incorporate spatial correlations. In light of these gaps in the literature, this paper proposes a new spatial negative binomial model, which uses Bayesian additive regression trees to endogenously select the specification of the link function. Posterior inference in the proposed model is made feasible with the help of the Polya-Gamma data augmentation technique. We test the performance of this new model on a crash count data set from a metropolitan highway network. The empirical results show that the proposed model performs at least as well as a baseline spatial count data model with random parameters in terms of goodness of fit and site ranking ability.

</details>

<details>

<summary>2020-05-25 11:55:19 - Hierarchical Bayesian Regression for Multi-Site Normative Modeling of Neuroimaging Data</summary>

- *Seyed Mostafa Kia, Hester Huijsdens, Richard Dinga, Thomas Wolfers, Maarten Mennes, Ole A. Andreassen, Lars T. Westlye, Christian F. Beckmann, Andre F. Marquand*

- `2005.12055v1` - [abs](http://arxiv.org/abs/2005.12055v1) - [pdf](http://arxiv.org/pdf/2005.12055v1)

> Clinical neuroimaging has recently witnessed explosive growth in data availability which brings studying heterogeneity in clinical cohorts to the spotlight. Normative modeling is an emerging statistical tool for achieving this objective. However, its application remains technically challenging due to difficulties in properly dealing with nuisance variation, for example due to variability in image acquisition devices. Here, in a fully probabilistic framework, we propose an application of hierarchical Bayesian regression (HBR) for multi-site normative modeling. Our experimental results confirm the superiority of HBR in deriving more accurate normative ranges on large multi-site neuroimaging data compared to widely used methods. This provides the possibility i) to learn the normative range of structural and functional brain measures on large multi-site data; ii) to recalibrate and reuse the learned model on local small data; therefore, HBR closes the technical loop for applying normative modeling as a medical tool for the diagnosis and prognosis of mental disorders.

</details>

<details>

<summary>2020-05-25 13:35:50 - Bayesian non-asymptotic extreme value models for environmental data</summary>

- *Enrico Zorzetto, Antonio Canale, Marco Marani*

- `2005.12101v1` - [abs](http://arxiv.org/abs/2005.12101v1) - [pdf](http://arxiv.org/pdf/2005.12101v1)

> Motivated by the analysis of extreme rainfall data, we introduce a general Bayesian hierarchical model for estimating the probability distribution of extreme values of intermittent random sequences, a common problem in geophysical and environmental science settings. The approach presented here relaxes the asymptotic assumption typical of the traditional extreme value (EV) theory, and accounts for the possible underlying variability in the distribution of event magnitudes and occurrences, which are described through a latent temporal process. Focusing on daily rainfall extremes, the structure of the proposed model lends itself to incorporating prior geo-physical understanding of the rainfall process. By means of an extensive simulation study, we show that this methodology can significantly reduce estimation uncertainty with respect to Bayesian formulations of traditional asymptotic EV methods, particularly in the case of relatively small samples. The benefits of the approach are further illustrated with an application to a large data set of 479 long daily rainfall historical records from across the continental United States. By comparing measures of in-sample and out-of-sample predictive accuracy, we find that the model structure developed here, combined with the use of all available observations for inference, significantly improves robustness with respect to overfitting to the specific sample.

</details>

<details>

<summary>2020-05-25 17:43:35 - Rethinking Parameter Counting in Deep Models: Effective Dimensionality Revisited</summary>

- *Wesley J. Maddox, Gregory Benton, Andrew Gordon Wilson*

- `2003.02139v2` - [abs](http://arxiv.org/abs/2003.02139v2) - [pdf](http://arxiv.org/pdf/2003.02139v2)

> Neural networks appear to have mysterious generalization properties when using parameter counting as a proxy for complexity. Indeed, neural networks often have many more parameters than there are data points, yet still provide good generalization performance. Moreover, when we measure generalization as a function of parameters, we see double descent behaviour, where the test error decreases, increases, and then again decreases. We show that many of these properties become understandable when viewed through the lens of effective dimensionality, which measures the dimensionality of the parameter space determined by the data. We relate effective dimensionality to posterior contraction in Bayesian deep learning, model selection, width-depth tradeoffs, double descent, and functional diversity in loss surfaces, leading to a richer understanding of the interplay between parameters and functions in deep models. We also show that effective dimensionality compares favourably to alternative norm- and flatness- based generalization measures.

</details>

<details>

<summary>2020-05-25 23:03:52 - Bayesian item response models for citizen science ecological data</summary>

- *Edgar Santos-Fernandez, Kerrie Mengersen*

- `2003.06966v2` - [abs](http://arxiv.org/abs/2003.06966v2) - [pdf](http://arxiv.org/pdf/2003.06966v2)

> So-called 'citizen science' data elicited from crowds has become increasingly popular in many fields including ecology. However, the quality of this information is being frequently debated by many within the scientific community. Therefore, modern citizen science implementations require measures of the users' proficiency that account for the difficulty of the tasks. We introduce a new methodological framework of item response and linear logistic test models with application to citizen science data used in ecology research. This approach accommodates spatial autocorrelation within the item difficulties and produces relevant ecological measures of species and site-related difficulties, discriminatory power and guessing behavior. These, along with estimates of the subject abilities allow better management of these programs and provide deeper insights. This paper also highlights the fit of item response models to big data via divide-and-conquer. We found that the suggested methods outperform the traditional item response models in terms of RMSE, accuracy, and WAIC based on leave-one-out cross-validation on simulated and empirical data. We present a comprehensive implementation using a case study of species identification in the Serengeti, Tanzania. The R and Stan codes are provided for full reproducibility. Multiple statistical illustrations and visualizations are given which allow practitioners the extrapolation to a wide range of citizen science ecological problems.

</details>

<details>

<summary>2020-05-26 01:39:43 - Bayesian Multiresolution Modeling Of Georeferenced Data</summary>

- *John Paige, Geir-Arne Fuglstad, Andrea Riebler, Jon Wakefield*

- `2005.11805v2` - [abs](http://arxiv.org/abs/2005.11805v2) - [pdf](http://arxiv.org/pdf/2005.11805v2)

> Current implementations of multiresolution methods are limited in terms of possible types of responses and approaches to inference. We provide a multiresolution approach for spatial analysis of non-Gaussian responses using latent Gaussian models and Bayesian inference via integrated nested Laplace approximation (INLA). The approach builds on `LatticeKrig', but uses a reparameterization of the model parameters that is intuitive and interpretable so that modeling and prior selection can be guided by expert knowledge about the different spatial scales at which dependence acts. The priors can be used to make inference robust and integration over model parameters allows for more accurate posterior estimates of uncertainty. The extended LatticeKrig (ELK) model is compared to a standard implementation of LatticeKrig (LK), and a standard Mat\'ern model, and we find modest improvement in spatial oversmoothing and prediction for the ELK model for counts of secondary education completion for women in Kenya collected in the 2014 Kenya demographic health survey. Through a simulation study with Gaussian responses and a realistic mix of short and long scale dependencies, we demonstrate that the differences between the three approaches for prediction increases with distance to nearest observation.

</details>

<details>

<summary>2020-05-26 10:39:07 - Robust Bayesian Regression with Synthetic Posterior</summary>

- *Shintaro Hashimoto, Shonosuke Sugasawa*

- `1910.00812v2` - [abs](http://arxiv.org/abs/1910.00812v2) - [pdf](http://arxiv.org/pdf/1910.00812v2)

> Although linear regression models are fundamental tools in statistical science, the estimation results can be sensitive to outliers. While several robust methods have been proposed in frequentist frameworks, statistical inference is not necessarily straightforward. We here propose a Bayesian approach to robust inference on linear regression models using synthetic posterior distributions based on $\gamma$-divergence, which enables us to naturally assess the uncertainty of the estimation through the posterior distribution. We also consider the use of shrinkage priors for the regression coefficients to carry out robust Bayesian variable selection and estimation simultaneously. We develop an efficient posterior computation algorithm by adopting the Bayesian bootstrap within Gibbs sampling. The performance of the proposed method is illustrated through simulation studies and applications to famous datasets.

</details>

<details>

<summary>2020-05-26 10:57:50 - Causal Bayesian Optimization</summary>

- *Virginia Aglietti, Xiaoyu Lu, Andrei Paleyes, Javier González*

- `2005.11741v2` - [abs](http://arxiv.org/abs/2005.11741v2) - [pdf](http://arxiv.org/pdf/2005.11741v2)

> This paper studies the problem of globally optimizing a variable of interest that is part of a causal model in which a sequence of interventions can be performed. This problem arises in biology, operational research, communications and, more generally, in all fields where the goal is to optimize an output metric of a system of interconnected nodes. Our approach combines ideas from causal inference, uncertainty quantification and sequential decision making. In particular, it generalizes Bayesian optimization, which treats the input variables of the objective function as independent, to scenarios where causal information is available. We show how knowing the causal graph significantly improves the ability to reason about optimal decision making strategies decreasing the optimization cost while avoiding suboptimal solutions. We propose a new algorithm called Causal Bayesian Optimization (CBO). CBO automatically balances two trade-offs: the classical exploration-exploitation and the new observation-intervention, which emerges when combining real interventional data with the estimated intervention effects computed via do-calculus. We demonstrate the practical benefits of this method in a synthetic setting and in two real-world applications.

</details>

<details>

<summary>2020-05-26 14:57:14 - SEIRD Model for Qatar Covid-19 Outbreak: A Case Study</summary>

- *Ryad Ghanam, Edward L. Boone, Abdel-Salam G. Abdel-Salam*

- `2005.12777v1` - [abs](http://arxiv.org/abs/2005.12777v1) - [pdf](http://arxiv.org/pdf/2005.12777v1)

> The Covid-19 outbreak of 2020 has required many governments to develop mathematical-statistical models of the outbreak for policy and planning purposes. This work provides a tutorial on building a compartmental model using Susceptibles, Exposed, Infected, Recovered and Deaths status through time. A Bayesian Framework is utilized to perform both parameter estimation and predictions. This model uses interventions to quantify the impact of various government attempts to slow the spread of the virus. Predictions are also made to determine when the peak Active Infections will occur.

</details>

<details>

<summary>2020-05-26 15:56:45 - Bayesian joint models for longitudinal and survival data</summary>

- *Carmen Armero*

- `2005.12822v1` - [abs](http://arxiv.org/abs/2005.12822v1) - [pdf](http://arxiv.org/pdf/2005.12822v1)

> This paper takes a quick look at Bayesian joint models (BJM) for longitudinal and survival data. A general formulation for BJM is examined in terms of the sampling distribution of the longitudinal and survival processes, the conditional distribution of the random effects and the prior distribution. Next a basic BJM defined in terms of a mixed linear model and a Cox survival regression models is discussed and some extensions and other Bayesian topics are briefly outlined.

</details>

<details>

<summary>2020-05-26 16:12:04 - Periodic Strategies II: Generalizations and Extensions</summary>

- *V. K. Oikonomou, J. Jost*

- `2005.12832v1` - [abs](http://arxiv.org/abs/2005.12832v1) - [pdf](http://arxiv.org/pdf/2005.12832v1)

> At a mixed Nash equilibrium, the payoff of a player does not depend on her own action, as long as her opponent sticks to his. In a periodic strategy, a concept developed in a previous paper (arXiv:1307.2035v4), in contrast, the own payoff does not depend on the opponent's action. Here, we generalize this to multi-player simultaneous perfect information strategic form games. We show that also in this class of games, there always exists at least one periodic strategy, and we investigate the mathematical properties of such periodic strategies. In addition, we demonstrate that periodic strategies may exist in games with incomplete information; we shall focus on Bayesian games. Moreover we discuss the differences between the periodic strategies formalism and cooperative game theory. In fact, the periodic strategies are obtained in a purely non-cooperative way, and periodic strategies are as cooperative as the Nash equilibria are. Finally, we incorporate the periodic strategies in an epistemic game theory framework, and discuss several features of this approach.

</details>

<details>

<summary>2020-05-26 16:39:22 - GP-ETAS: Semiparametric Bayesian inference for the spatio-temporal Epidemic Type Aftershock Sequence model</summary>

- *Christian Molkenthin, Christian Donner, Sebastian Reich, Gert Zöller, Sebastian Hainzl, Matthias Holschneider, Manfred Opper*

- `2005.12857v1` - [abs](http://arxiv.org/abs/2005.12857v1) - [pdf](http://arxiv.org/pdf/2005.12857v1)

> The spatio-temporal Epidemic Type Aftershock Sequence (ETAS) model is widely used to describe the self-exciting nature of earthquake occurrences. While traditional inference methods provide only point estimates of the model parameters, we aim at a full Bayesian treatment of model inference, allowing naturally to incorporate prior knowledge and uncertainty quantification of the resulting estimates. Therefore, we introduce a highly flexible, non-parametric representation for the spatially varying ETAS background intensity through a Gaussian process (GP) prior. Combined with classical triggering functions this results in a new model formulation, namely the GP-ETAS model. We enable tractable and efficient Gibbs sampling by deriving an augmented form of the GP-ETAS inference problem. This novel sampling approach allows us to assess the posterior model variables conditioned on observed earthquake catalogues, i.e., the spatial background intensity and the parameters of the triggering function. Empirical results on two synthetic data sets indicate that GP-ETAS outperforms standard models and thus demonstrate the predictive power for observed earthquake catalogues including uncertainty quantification for the estimated parameters. Finally, a case study for the l'Aquila region, Italy, with the devastating event on 6 April 2009, is presented.

</details>

<details>

<summary>2020-05-26 17:25:18 - Bayesian sparse convex clustering via global-local shrinkage priors</summary>

- *Kaito Shimamura, Shuichi Kawano*

- `1911.08703v2` - [abs](http://arxiv.org/abs/1911.08703v2) - [pdf](http://arxiv.org/pdf/1911.08703v2)

> Sparse convex clustering is to cluster observations and conduct variable selection simultaneously in the framework of convex clustering. Although a weighted $L_1$ norm is usually employed for the regularization term in sparse convex clustering, its use increases the dependence on the data and reduces the estimation accuracy if the sample size is not sufficient. To tackle these problems, this paper proposes a Bayesian sparse convex clustering method based on the ideas of Bayesian lasso and global-local shrinkage priors. We introduce Gibbs sampling algorithms for our method using scale mixtures of normal distributions. The effectiveness of the proposed methods is shown in simulation studies and a real data analysis.

</details>

<details>

<summary>2020-05-26 19:13:03 - Skew Gaussian Processes for Classification</summary>

- *Alessio Benavoli, Dario Azzimonti, Dario Piga*

- `2005.12987v1` - [abs](http://arxiv.org/abs/2005.12987v1) - [pdf](http://arxiv.org/pdf/2005.12987v1)

> Gaussian processes (GPs) are distributions over functions, which provide a Bayesian nonparametric approach to regression and classification. In spite of their success, GPs have limited use in some applications, for example, in some cases a symmetric distribution with respect to its mean is an unreasonable model. This implies, for instance, that the mean and the median coincide, while the mean and median in an asymmetric (skewed) distribution can be different numbers. In this paper, we propose Skew-Gaussian processes (SkewGPs) as a non-parametric prior over functions. A SkewGP extends the multivariate Unified Skew-Normal distribution over finite dimensional vectors to a stochastic processes. The SkewGP class of distributions includes GPs and, therefore, SkewGPs inherit all good properties of GPs and increase their flexibility by allowing asymmetry in the probabilistic model. By exploiting the fact that SkewGP and probit likelihood are conjugate model, we derive closed form expressions for the marginal likelihood and predictive distribution of this new nonparametric classifier. We verify empirically that the proposed SkewGP classifier provides a better performance than a GP classifier based on either Laplace's method or Expectation Propagation.

</details>

<details>

<summary>2020-05-26 20:35:02 - Probabilistic solution of chaotic dynamical system inverse problems using Bayesian Artificial Neural Networks</summary>

- *David K. E. Green, Filip Rindler*

- `2005.13028v1` - [abs](http://arxiv.org/abs/2005.13028v1) - [pdf](http://arxiv.org/pdf/2005.13028v1)

> This paper demonstrates the application of Bayesian Artificial Neural Networks to Ordinary Differential Equation (ODE) inverse problems. We consider the case of estimating an unknown chaotic dynamical system transition model from state observation data. Inverse problems for chaotic systems are numerically challenging as small perturbations in model parameters can cause very large changes in estimated forward trajectories. Bayesian Artificial Neural Networks can be used to simultaneously fit a model and estimate model parameter uncertainty. Knowledge of model parameter uncertainty can then be incorporated into the probabilistic estimates of the inferred system's forward time evolution. The method is demonstrated numerically by analysing the chaotic Sprott B system. Observations of the system are used to estimate a posterior predictive distribution over the weights of a parametric polynomial kernel Artificial Neural Network. It is shown that the proposed method is able to perform accurate time predictions. Further, the proposed method is able to correctly account for model uncertainties and provide useful prediction uncertainty bounds.

</details>

<details>

<summary>2020-05-27 06:03:04 - How to choose between different Bayesian posterior indices for hypothesis testing in practice</summary>

- *Riko Kelter*

- `2005.13181v1` - [abs](http://arxiv.org/abs/2005.13181v1) - [pdf](http://arxiv.org/pdf/2005.13181v1)

> Hypothesis testing is an essential statistical method in psychology and the cognitive sciences. The problems of traditional null hypothesis significance testing (NHST) have been discussed widely, and among the proposed solutions to the replication problems caused by the inappropriate use of significance tests and $p$-values is a shift towards Bayesian data analysis. However, Bayesian hypothesis testing is concerned with various posterior indices for significance and the size of an effect. This complicates Bayesian hypothesis testing in practice, as the availability of multiple Bayesian alternatives to the traditional $p$-value causes confusion which one to select and why. In this paper, we compare various Bayesian posterior indices which have been proposed in the literature and discuss their benefits and limitations. Our comparison shows that conceptually not all proposed Bayesian alternatives to NHST and $p$-values are beneficial, and the usefulness of some indices strongly depends on the study design and research goal. However, our comparison also reveals that there exist at least two candidates among the available Bayesian posterior indices which have appealing theoretical properties and are, to our best knowledge, widely underused among psychologists.

</details>

<details>

<summary>2020-05-27 06:57:27 - Bayesian model selection in the $\mathcal{M}$-open setting -- Approximate posterior inference and probability-proportional-to-size subsampling for efficient large-scale leave-one-out cross-validation</summary>

- *Riko Kelter*

- `2005.13199v1` - [abs](http://arxiv.org/abs/2005.13199v1) - [pdf](http://arxiv.org/pdf/2005.13199v1)

> Comparison of competing statistical models is an essential part of psychological research. From a Bayesian perspective, various approaches to model comparison and selection have been proposed in the literature. However, the applicability of these approaches strongly depends on the assumptions about the model space $\mathcal{M}$, the so-called model view. Furthermore, traditional methods like leave-one-out cross-validation (LOO-CV) estimate the expected log predictive density (ELPD) of a model to investigate how the model generalises out-of-sample, which quickly becomes computationally inefficient when sample size becomes large. Here, we provide a tutorial on approximate Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO), a computationally efficient method for Bayesian model comparison. First, we discuss several model views and the available Bayesian model comparison methods in each. We then use Bayesian logistic regression as a running example how to apply the method in practice, and show that it outperforms other methods like LOO-CV or information criteria in terms of computational effort while providing similarly accurate ELPD estimates. In a second step, we show how even large-scale models can be compared efficiently by using posterior approximations in combination with probability-proportional-to-size subsampling. We show how to compare competing models based on the ELPD estimates provided, and how to conduct posterior predictive checks to safeguard against overconfidence in one of the models under consideration. We conclude that the method is attractive for mathematical psychologists who aim at comparing several competing statistical models, which are possibly high-dimensional and in the big-data regime.

</details>

<details>

<summary>2020-05-27 12:52:05 - Optimality of testing procedures for survival data</summary>

- *Andrea Arfé, Brian Alexander, Lorenzo Trippa*

- `1902.00161v3` - [abs](http://arxiv.org/abs/1902.00161v3) - [pdf](http://arxiv.org/pdf/1902.00161v3)

> Most statistical tests for treatment effects used in randomized clinical trials with survival outcomes are based on the proportional hazards assumption, which often fails in practice. Data from early exploratory studies may provide evidence of non-proportional hazards which can guide the choice of alternative tests in the design of practice-changing confirmatory trials. We study a test to detect treatment effects in a late-stage trial which accounts for the deviations from proportional hazards suggested by early-stage data. Conditional on early-stage data, among all tests which control the frequentist Type I error rate at a fixed $\alpha$ level, our testing procedure maximizes the Bayesian prediction of the finite-sample power. Hence, the proposed test provides a useful benchmark for other tests commonly used in presence of non-proportional hazards, for example weighted log-rank tests. We illustrate the approach in a simulations based on data from a published cancer immunotherapy phase III trial.

</details>

<details>

<summary>2020-05-27 19:54:19 - Bayesian Generative Models for Knowledge Transfer in MRI Semantic Segmentation Problems</summary>

- *Anna Kuzina, Evgenii Egorov, Evgeny Burnaev*

- `2005.12639v2` - [abs](http://arxiv.org/abs/2005.12639v2) - [pdf](http://arxiv.org/pdf/2005.12639v2)

> Automatic segmentation methods based on deep learning have recently demonstrated state-of-the-art performance, outperforming the ordinary methods. Nevertheless, these methods are inapplicable for small datasets, which are very common in medical problems. To this end, we propose a knowledge transfer method between diseases via the Generative Bayesian Prior network. Our approach is compared to a pre-train approach and random initialization and obtains the best results in terms of Dice Similarity Coefficient metric for the small subsets of the Brain Tumor Segmentation 2018 database (BRATS2018).

</details>

<details>

<summary>2020-05-28 00:49:59 - Synthetic control method with convex hull restrictions: A Bayesian maximum a posteriori approach</summary>

- *Gyuhyeong Goh, Jisang Yu*

- `2005.13719v1` - [abs](http://arxiv.org/abs/2005.13719v1) - [pdf](http://arxiv.org/pdf/2005.13719v1)

> Synthetic control methods have gained popularity among causal studies with observational data, particularly when estimating the impacts of the interventions that are implemented to a small number of large units. Implementing the synthetic control methods faces two major challenges: a) estimating weights for each control unit to create a synthetic control and b) providing statistical inferences. To overcome these challenges, we propose a Bayesian framework that implements the synthetic control method with the parallelly shiftable convex hull and provides a useful Bayesian inference, which is drawn from the duality between a penalized least squares method and a Bayesian Maximum A Posteriori (MAP) approach. Simulation results indicate that the proposed method leads to smaller biases compared to alternatives. We apply our Bayesian method to the real data example of Abadie and Gardeazabal (2003) and find that the treatment effects are statistically significant during the subset of the post-treatment period.

</details>

<details>

<summary>2020-05-28 08:54:28 - Bayesian estimation of the latent dimension and communities in stochastic blockmodels</summary>

- *Francesco Sanna Passino, Nicholas A. Heard*

- `1904.05333v3` - [abs](http://arxiv.org/abs/1904.05333v3) - [pdf](http://arxiv.org/pdf/1904.05333v3)

> Spectral embedding of adjacency or Laplacian matrices of undirected graphs is a common technique for representing a network in a lower dimensional latent space, with optimal theoretical guarantees. The embedding can be used to estimate the community structure of the network, with strong consistency results in the stochastic blockmodel framework. One of the main practical limitations of standard algorithms for community detection from spectral embeddings is that the number of communities and the latent dimension of the embedding must be specified in advance. In this article, a novel Bayesian model for simultaneous and automatic selection of the appropriate dimension of the latent space and the number of blocks is proposed. Extensions to directed and bipartite graphs are discussed. The model is tested on simulated and real world network data, showing promising performance for recovering latent community structure.

</details>

<details>

<summary>2020-05-28 13:32:08 - Clinical trials impacted by the COVID-19 pandemic: Adaptive designs to the rescue?</summary>

- *Cornelia Ursula Kunz, Silke Jörgens, Frank Bretz, Nigel Stallard, Kelly Van Lancker, Dong Xi, Sarah Zohar, Christoph Gerlinger, Tim Friede*

- `2005.13979v1` - [abs](http://arxiv.org/abs/2005.13979v1) - [pdf](http://arxiv.org/pdf/2005.13979v1)

> Very recently the new pathogen severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) was identified and the coronavirus disease 2019 (COVID-19) declared a pandemic by the World Health Organization. The pandemic has a number of consequences for the ongoing clinical trials in non-COVID-19 conditions. Motivated by four currently ongoing clinical trials in a variety of disease areas we illustrate the challenges faced by the pandemic and sketch out possible solutions including adaptive designs. Guidance is provided on (i) where blinded adaptations can help; (ii) how to achieve type I error rate control, if required; (iii) how to deal with potential treatment effect heterogeneity; (iv) how to utilize early readouts; and (v) how to utilize Bayesian techniques. In more detail approaches to resizing a trial affected by the pandemic are developed including considerations to stop a trial early, the use of group-sequential designs or sample size adjustment. All methods considered are implemented in a freely available R shiny app. Furthermore, regulatory and operational issues including the role of data monitoring committees are discussed.

</details>

<details>

<summary>2020-05-28 16:47:06 - Functional Space Variational Inference for Uncertainty Estimation in Computer Aided Diagnosis</summary>

- *Pranav Poduval, Hrushikesh Loya, Amit Sethi*

- `2005.11797v2` - [abs](http://arxiv.org/abs/2005.11797v2) - [pdf](http://arxiv.org/pdf/2005.11797v2)

> Deep neural networks have revolutionized medical image analysis and disease diagnosis. Despite their impressive performance, it is difficult to generate well-calibrated probabilistic outputs for such networks, which makes them uninterpretable black boxes. Bayesian neural networks provide a principled approach for modelling uncertainty and increasing patient safety, but they have a large computational overhead and provide limited improvement in calibration. In this work, by taking skin lesion classification as an example task, we show that by shifting Bayesian inference to the functional space we can craft meaningful priors that give better calibrated uncertainty estimates at a much lower computational cost.

</details>

<details>

<summary>2020-05-28 18:00:00 - Data Analysis Recipes: Products of multivariate Gaussians in Bayesian inferences</summary>

- *David W. Hogg, Adrian M. Price-Whelan, Boris Leistedt*

- `2005.14199v1` - [abs](http://arxiv.org/abs/2005.14199v1) - [pdf](http://arxiv.org/pdf/2005.14199v1)

> A product of two Gaussians (or normal distributions) is another Gaussian. That's a valuable and useful fact! Here we use it to derive a refactoring of a common product of multivariate Gaussians: The product of a Gaussian likelihood times a Gaussian prior, where some or all of those parameters enter the likelihood only in the mean and only linearly. That is, a linear, Gaussian, Bayesian model. This product of a likelihood times a prior pdf can be refactored into a product of a marginalized likelihood (or a Bayesian evidence) times a posterior pdf, where (in this case) both of these are also Gaussian. The means and variance tensors of the refactored Gaussians are straightforward to obtain as closed-form expressions; here we deliver these expressions, with discussion. The closed-form expressions can be used to speed up and improve the precision of inferences that contain linear parameters with Gaussian priors. We connect these methods to inferences that arise frequently in physics and astronomy.   If all you want is the answer, the question is posed and answered at the beginning of Section 3. We show two toy examples, in the form of worked exercises, in Section 4. The solutions, discussion, and exercises in this Note are aimed at someone who is already familiar with the basic ideas of Bayesian inference and probability.

</details>

<details>

<summary>2020-05-28 21:26:14 - Model selection for ecological community data using tree shrinkage priors</summary>

- *Trevor Hefley*

- `2005.14303v1` - [abs](http://arxiv.org/abs/2005.14303v1) - [pdf](http://arxiv.org/pdf/2005.14303v1)

> Researchers and managers model ecological communities to infer the biotic and abiotic variables that shape species' ranges, habitat use, and co-occurrence which, in turn, are used to support management decisions and test ecological theories. Recently, species distribution models were developed for and applied to data from ecological communities. Model development and selection for ecological community data is difficult because a high level of complexity is desired and achieved by including numerous parameters, which can degrade predictive accuracy and be challenging to interpret and communicate. Like other statistical models, multi-species distribution models can be overparameterized. Regularization is a technique that optimizes predictive accuracy by shrinking or eliminating model parameters. For Bayesian models, the prior distribution automatically regularizes parameters. We propose a tree shrinkage prior for Bayesian multi-species distributions models that performs regularization and reduces the number of regression coefficients associated with predictor variables. Using this prior, the number of regression coefficients in multi-species distributions models is reduced by estimation of unique regression coefficients for a smaller number of guilds rather than a larger number of species. We demonstrated our tree shrinkage prior using examples of presence-absence data for six species of aquatic vegetation and relative abundance data for 15 species of fish. Our results show that the tree shrinkage prior can increase the predictive accuracy of multi-species distribution models and enable researchers to infer the number and species composition of guilds from ecological community data.

</details>

<details>

<summary>2020-05-28 23:18:54 - Bayesian Neural Networks at Scale: A Performance Analysis and Pruning Study</summary>

- *Himanshu Sharma, Elise Jennings*

- `2005.11619v2` - [abs](http://arxiv.org/abs/2005.11619v2) - [pdf](http://arxiv.org/pdf/2005.11619v2)

> Bayesian neural Networks (BNNs) are a promising method of obtaining statistical uncertainties for neural network predictions but with a higher computational overhead which can limit their practical usage. This work explores the use of high performance computing with distributed training to address the challenges of training BNNs at scale. We present a performance and scalability comparison of training the VGG-16 and Resnet-18 models on a Cray-XC40 cluster. We demonstrate that network pruning can speed up inference without accuracy loss and provide an open source software package, {\it{BPrune}} to automate this pruning. For certain models we find that pruning up to 80\% of the network results in only a 7.0\% loss in accuracy. With the development of new hardware accelerators for Deep Learning, BNNs are of considerable interest for benchmarking performance. This analysis of training a BNN at scale outlines the limitations and benefits compared to a conventional neural network.

</details>

<details>

<summary>2020-05-29 10:06:26 - A Noise-Robust Fast Sparse Bayesian Learning Model</summary>

- *Ingvild M. Helgøy, Yushu Li*

- `1908.07220v2` - [abs](http://arxiv.org/abs/1908.07220v2) - [pdf](http://arxiv.org/pdf/1908.07220v2)

> This paper utilizes the hierarchical model structure from the Bayesian Lasso in the Sparse Bayesian Learning process to develop a new type of probabilistic supervised learning approach. The hierarchical model structure in this Bayesian framework is designed such that the priors do not only penalize the unnecessary complexity of the model but will also be conditioned on the variance of the random noise in the data. The hyperparameters in the model are estimated by the Fast Marginal Likelihood Maximization algorithm which can achieve sparsity, low computational cost and faster learning process. We compare our methodology with two other popular learning models; the Relevance Vector Machine and the Bayesian Lasso. We test our model on examples involving both simulated and empirical data, and the results show that this approach has several performance advantages, such as being fast, sparse and also robust to the variance in random noise. In addition, our method can give out a more stable estimation of variance of random error, compared with the other methods in the study.

</details>

<details>

<summary>2020-05-29 14:02:45 - Existence of structured perfect Bayesian equilibrium in dynamic games of asymmetric information</summary>

- *Deepanshu Vasal*

- `2005.05586v2` - [abs](http://arxiv.org/abs/2005.05586v2) - [pdf](http://arxiv.org/pdf/2005.05586v2)

> In~[1],authors considered a general finite horizon model of dynamic game of asymmetric information, where N players have types evolving as independent Markovian process, where each player observes its own type perfectly and actions of all players. The authors present a sequential decomposition algorithm to find all structured perfect Bayesian equilibria of the game. The algorithm consists of solving a class of fixed-point of equations for each time $t,\pi_t$, whose existence was left as an open question. In this paper, we prove existence of these fixed-point equations for compact metric spaces.

</details>

<details>

<summary>2020-05-29 22:34:50 - A Nonparametric Bayesian Item Response Modeling Approach for Clustering Items and Individuals Simultaneously</summary>

- *Guanyu Hu, Zhihua Ma, Insu Paek*

- `2006.00105v1` - [abs](http://arxiv.org/abs/2006.00105v1) - [pdf](http://arxiv.org/pdf/2006.00105v1)

> Item response theory (IRT) is a popular modeling paradigm for measuring subject latent traits and item properties according to discrete responses in tests or questionnaires. There are very limited discussions on heterogeneity pattern detection for both items and individuals. In this paper, we introduce a nonparametric Bayesian approach for clustering items and individuals simultaneously under the Rasch model. Specifically, our proposed method is based on the mixture of finite mixtures (MFM) model. MFM obtains the number of clusters and the clustering configurations for both items and individuals simultaneously. The performance of parameters estimation and parameters clustering under the MFM Rasch model is evaluated by simulation studies, and a real date set is applied to illustrate the MFM Rasch modeling.

</details>

<details>

<summary>2020-05-30 00:06:17 - Continuous Time Individual-Level Models of Infectious Disease: a Package EpiILMCT</summary>

- *Waleed Almutiry, Vineetha Warriyar K V, Rob Deardon*

- `2006.00135v1` - [abs](http://arxiv.org/abs/2006.00135v1) - [pdf](http://arxiv.org/pdf/2006.00135v1)

> This paper describes the R package EpiILMCT, which allows users to study the spread of infectious disease using continuous time individual level models (ILMs). The package provides tools for simulation from continuous time ILMs that are based on either spatial demographic, contact network, or a combination of both of them, and for the graphical summarization of epidemics. Model fitting is carried out within a Bayesian Markov Chain Monte Carlo (MCMC) framework. The continuous time ILMs can be implemented within either susceptible-infected-removed (SIR) or susceptible-infected-notified-removed (SINR) compartmental frameworks. As infectious disease data is often partially observed, data uncertainties in the form of missing infection times - and in some situations missing removal times - are accounted for using data augmentation techniques. The package is illustrated using both simulated and an experimental data set on the spread of the tomato spotted wilt virus (TSWV) disease.

</details>

<details>

<summary>2020-05-30 13:08:08 - Efficient stochastic optimisation by unadjusted Langevin Monte Carlo. Application to maximum marginal likelihood and empirical Bayesian estimation</summary>

- *Valentin De Bortoli, Alain Durmus, Marcelo Pereyra, Ana F. Vidal*

- `1906.12281v2` - [abs](http://arxiv.org/abs/1906.12281v2) - [pdf](http://arxiv.org/pdf/1906.12281v2)

> Stochastic approximation methods play a central role in maximum likelihood estimation problems involving intractable likelihood functions, such as marginal likelihoods arising in problems with missing or incomplete data, and in parametric empirical Bayesian estimation. Combined with Markov chain Monte Carlo algorithms, these stochastic optimisation methods have been successfully applied to a wide range of problems in science and industry. However, this strategy scales poorly to large problems because of methodological and theoretical difficulties related to using high-dimensional Markov chain Monte Carlo algorithms within a stochastic approximation scheme. This paper proposes to address these difficulties by using unadjusted Langevin algorithms to construct the stochastic approximation. This leads to a highly efficient stochastic optimisation methodology with favourable convergence properties that can be quantified explicitly and easily checked. The proposed methodology is demonstrated with three experiments, including a challenging application to high-dimensional statistical audio analysis and a sparse Bayesian logistic regression with random effects problem.

</details>

<details>

<summary>2020-05-30 17:43:10 - Bayesian Nonparametric Monotone Regression</summary>

- *Ander Wilson, Jessica Tryner, Christian L'Orange, John Volckens*

- `2006.00326v1` - [abs](http://arxiv.org/abs/2006.00326v1) - [pdf](http://arxiv.org/pdf/2006.00326v1)

> In many applications there is interest in estimating the relation between a predictor and an outcome when the relation is known to be monotone or otherwise constrained due to the physical processes involved. We consider one such application--inferring time-resolved aerosol concentration from a low-cost differential pressure sensor. The objective is to estimate a monotone function and make inference on the scaled first derivative of the function. We proposed Bayesian nonparametric monotone regression which uses a Bernstein polynomial basis to construct the regression function and puts a Dirichlet process prior on the regression coefficients. The base measure of the Dirichlet process is a finite mixture of a mass point at zero and a truncated normal. This construction imposes monotonicity while clustering the basis functions. Clustering the basis functions reduces the parameter space and allows the estimated regression function to be linear. With the proposed approach we can make closed-formed inference on the derivative of the estimated function including full quantification of uncertainty. In a simulation study the proposed method performs similar to other monotone regression approaches when the true function is wavy but performs better when the true function is linear. We apply the method to estimate time-resolved aerosol concentration with a newly-developed portable aerosol monitor. The R package bnmr is made available to implement the method.

</details>

<details>

<summary>2020-05-30 17:50:40 - Integrative Methods for Post-Selection Inference Under Convex Constraints</summary>

- *Snigdha Panigrahi, Jonathan Taylor, Asaf Weinstein*

- `1605.08824v7` - [abs](http://arxiv.org/abs/1605.08824v7) - [pdf](http://arxiv.org/pdf/1605.08824v7)

> Inference after model selection has been an active research topic in the past few years, with numerous works offering different approaches to addressing the perils of the reuse of data. In particular, major progress has been made recently on large and useful classes of problems by harnessing general theory of hypothesis testing in exponential families, but these methods have their limitations. Perhaps most immediate is the gap between theory and practice: implementing the exact theoretical prescription in realistic situations---for example, when new data arrives and inference needs to be adjusted accordingly---may be a prohibitive task. In this paper we propose a Bayesian framework for carrying out inference after model selection in the linear model. Our framework is very flexible in the sense that it naturally accommodates different models for the data, instead of requiring a case-by-case treatment. At the core of our methods is a new approximation to the exact likelihood conditional on selection, the latter being generally intractable. We prove that, under appropriate conditions, our approximation is asymptotically consistent with the exact truncated likelihood. The advantages of our methods in practical data analysis are demonstrated in simulations and in application to HIV drug-resistance data.

</details>

<details>

<summary>2020-05-31 14:56:49 - Delayed acceptance ABC-SMC</summary>

- *Richard G. Everitt, Paulina A. Rowińska*

- `1708.02230v2` - [abs](http://arxiv.org/abs/1708.02230v2) - [pdf](http://arxiv.org/pdf/1708.02230v2)

> Approximate Bayesian computation (ABC) is now an established technique for statistical inference used in cases where the likelihood function is computationally expensive or not available. It relies on the use of a~model that is specified in the form of a~simulator, and approximates the likelihood at a~parameter value $\theta$ by simulating auxiliary data sets $x$ and evaluating the distance of $x$ from the true data $y$. However, ABC is not computationally feasible in cases where using the simulator for each $\theta$ is very expensive. This paper investigates this situation in cases where a~cheap, but approximate, simulator is available. The approach is to employ delayed acceptance Markov chain Monte Carlo (MCMC) within an ABC sequential Monte Carlo (SMC) sampler in order to, in a~first stage of the kernel, use the cheap simulator to rule out parts of the parameter space that are not worth exploring, so that the ``true'' simulator is only run (in the second stage of the kernel) where there is a~reasonable chance of accepting proposed values of $\theta$. We show that this approach can be used quite automatically, with few tuning parameters. Applications to stochastic differential equation models and latent doubly intractable distributions are presented.

</details>

<details>

<summary>2020-05-31 23:42:22 - Bayesian Optimisation vs. Input Uncertainty Reduction</summary>

- *Juan Ungredda, Michael Pearce, Juergen Branke*

- `2006.00643v1` - [abs](http://arxiv.org/abs/2006.00643v1) - [pdf](http://arxiv.org/pdf/2006.00643v1)

> Simulators often require calibration inputs estimated from real world data and the quality of the estimate can significantly affect simulation output. Particularly when performing simulation optimisation to find an optimal solution, the uncertainty in the inputs significantly affects the quality of the found solution. One remedy is to search for the solution that has the best performance on average over the uncertain range of inputs yielding an optimal compromise solution. We consider the more general setting where a user may choose between either running simulations or instead collecting real world data. A user may choose an input and a solution and observe the simulation output, or instead query an external data source improving the input estimate enabling the search for a more focused, less compromised solution. We explicitly examine the trade-off between simulation and real data collection in order to find the optimal solution of the simulator with the true inputs. Using a value of information procedure, we propose a novel unified simulation optimisation procedure called Bayesian Information Collection and Optimisation (BICO) that, in each iteration, automatically determines which of the two actions (running simulations or data collection) is more beneficial. Numerical experiments demonstrate that the proposed algorithm is able to automatically determine an appropriate balance between optimisation and data collection.

</details>


## 2020-06

<details>

<summary>2020-06-01 06:41:24 - Correcting misclassification errors in crowdsourced ecological data: A Bayesian perspective</summary>

- *Edgar Santos-Fernandez, Erin E. Peterson, Julie Vercelloni, Em Rushworth, Kerrie Mengersen*

- `2006.00741v1` - [abs](http://arxiv.org/abs/2006.00741v1) - [pdf](http://arxiv.org/pdf/2006.00741v1)

> Many research domains use data elicited from "citizen scientists" when a direct measure of a process is expensive or infeasible. However, participants may report incorrect estimates or classifications due to their lack of skill. We demonstrate how Bayesian hierarchical models can be used to learn about latent variables of interest, while accounting for the participants' abilities. The model is described in the context of an ecological application that involves crowdsourced classifications of georeferenced coral-reef images from the Great Barrier Reef, Australia. The latent variable of interest is the proportion of coral cover, which is a common indicator of coral reef health. The participants' abilities are expressed in terms of sensitivity and specificity of a correctly classified set of points on the images. The model also incorporates a spatial component, which allows prediction of the latent variable in locations that have not been surveyed. We show that the model outperforms traditional weighted-regression approaches used to account for uncertainty in citizen science data. Our approach produces more accurate regression coefficients and provides a better characterization of the latent process of interest. This new method is implemented in the probabilistic programming language Stan and can be applied to a wide number of problems that rely on uncertain citizen science data.

</details>

<details>

<summary>2020-06-01 16:38:27 - Reinforcement learning and Bayesian data assimilation for model-informed precision dosing in oncology</summary>

- *Corinna Maier, Niklas Hartung, Charlotte Kloft, Wilhelm Huisinga, Jana de Wiljes*

- `2006.01061v1` - [abs](http://arxiv.org/abs/2006.01061v1) - [pdf](http://arxiv.org/pdf/2006.01061v1)

> Model-informed precision dosing (MIPD) using therapeutic drug/biomarker monitoring offers the opportunity to significantly improve the efficacy and safety of drug therapies. Current strategies comprise model-informed dosing tables or are based on maximum a-posteriori estimates. These approaches, however, lack a quantification of uncertainty and/or consider only part of the available patient-specific information. We propose three novel approaches for MIPD employing Bayesian data assimilation (DA) and/or reinforcement learning (RL) to control neutropenia, the major dose-limiting side effect in anticancer chemotherapy. These approaches have the potential to substantially reduce the incidence of life-threatening grade 4 and subtherapeutic grade 0 neutropenia compared to existing approaches. We further show that RL allows to gain further insights by identifying patient factors that drive dose decisions. Due to its flexibility, the proposed combined DA-RL approach can easily be extended to integrate multiple endpoints or patient-reported outcomes, thereby promising important benefits for future personalized therapies.

</details>

<details>

<summary>2020-06-01 21:27:01 - Mitigating Unobserved Spatial Confounding when Estimating the Effect of Supermarket Access on Cardiovascular Disease Deaths</summary>

- *Patrick Schnell, Georgia Papadogeorgou*

- `1907.12150v3` - [abs](http://arxiv.org/abs/1907.12150v3) - [pdf](http://arxiv.org/pdf/1907.12150v3)

> Confounding by unmeasured spatial variables has received some attention in the spatial statistics and causal inference literatures, but concepts and approaches have remained largely separated. In this paper, we aim to bridge these distinct strands of statistics by considering unmeasured spatial confounding within a causal inference framework, and estimating effects using outcome regression tools popular within the spatial literature. First, we show how using spatially correlated random effects in the outcome model, an approach common among spatial statisticians, does not necessarily mitigate bias due to spatial confounding, a previously published but not universally known result. Motivated by the bias term of commonly-used estimators, we propose an affine estimator which addresses this deficiency. We discuss how unbiased estimation of causal parameters in the presence of unmeasured spatial confounding can only be achieved under an untestable set of assumptions which will often be application-specific. We provide a set of assumptions which describe how the exposure and outcome of interest relate to the unmeasured variables, and we show that this set of assumptions is sufficient for identification of the causal effect based on the observed data when spatial dependencies can be represented by a ring graph. We implement our method using a fully Bayesian approach applicable to any type of outcome variable. This work is motivated by and used to estimate the effect of county-level limited access to supermarkets on the rate of cardiovascular disease deaths in the elderly across the whole continental United States. Even though standard approaches return null or protective effects, our approach uncovers evidence of unobserved spatial confounding, and indicates that limited supermarket access has a harmful effect on cardiovascular mortality.

</details>

<details>

<summary>2020-06-02 02:53:30 - Variational Bayesian Inference for Crowdsourcing Predictions</summary>

- *Desmond Cai, Duc Thien Nguyen, Shiau Hong Lim, Laura Wynter*

- `2006.00778v2` - [abs](http://arxiv.org/abs/2006.00778v2) - [pdf](http://arxiv.org/pdf/2006.00778v2)

> Crowdsourcing has emerged as an effective means for performing a number of machine learning tasks such as annotation and labelling of images and other data sets. In most early settings of crowdsourcing, the task involved classification, that is assigning one of a discrete set of labels to each task. Recently, however, more complex tasks have been attempted including asking crowdsource workers to assign continuous labels, or predictions. In essence, this involves the use of crowdsourcing for function estimation. We are motivated by this problem to drive applications such as collaborative prediction, that is, harnessing the wisdom of the crowd to predict quantities more accurately. To do so, we propose a Bayesian approach aimed specifically at alleviating overfitting, a typical impediment to accurate prediction models in practice. In particular, we develop a variational Bayesian technique for two different worker noise models - one that assumes workers' noises are independent and the other that assumes workers' noises have a latent low-rank structure. Our evaluations on synthetic and real-world datasets demonstrate that these Bayesian approaches perform significantly better than existing non-Bayesian approaches and are thus potentially useful for this class of crowdsourcing problems.

</details>

<details>

<summary>2020-06-02 09:38:00 - Meta Learning as Bayes Risk Minimization</summary>

- *Shin-ichi Maeda, Toshiki Nakanishi, Masanori Koyama*

- `2006.01488v1` - [abs](http://arxiv.org/abs/2006.01488v1) - [pdf](http://arxiv.org/pdf/2006.01488v1)

> Meta-Learning is a family of methods that use a set of interrelated tasks to learn a model that can quickly learn a new query task from a possibly small contextual dataset. In this study, we use a probabilistic framework to formalize what it means for two tasks to be related and reframe the meta-learning problem into the problem of Bayesian risk minimization (BRM). In our formulation, the BRM optimal solution is given by the predictive distribution computed from the posterior distribution of the task-specific latent variable conditioned on the contextual dataset, and this justifies the philosophy of Neural Process. However, the posterior distribution in Neural Process violates the way the posterior distribution changes with the contextual dataset. To address this problem, we present a novel Gaussian approximation for the posterior distribution that generalizes the posterior of the linear Gaussian model. Unlike that of the Neural Process, our approximation of the posterior distributions converges to the maximum likelihood estimate with the same rate as the true posterior distribution. We also demonstrate the competitiveness of our approach on benchmark datasets.

</details>

<details>

<summary>2020-06-02 15:59:42 - Toward Optimal Probabilistic Active Learning Using a Bayesian Approach</summary>

- *Daniel Kottke, Marek Herde, Christoph Sandrock, Denis Huseljic, Georg Krempl, Bernhard Sick*

- `2006.01732v1` - [abs](http://arxiv.org/abs/2006.01732v1) - [pdf](http://arxiv.org/pdf/2006.01732v1)

> Gathering labeled data to train well-performing machine learning models is one of the critical challenges in many applications. Active learning aims at reducing the labeling costs by an efficient and effective allocation of costly labeling resources. In this article, we propose a decision-theoretic selection strategy that (1) directly optimizes the gain in misclassification error, and (2) uses a Bayesian approach by introducing a conjugate prior distribution to determine the class posterior to deal with uncertainties. By reformulating existing selection strategies within our proposed model, we can explain which aspects are not covered in current state-of-the-art and why this leads to the superior performance of our approach. Extensive experiments on a large variety of datasets and different kernels validate our claims.

</details>

<details>

<summary>2020-06-02 17:14:36 - A probabilistic generative model for semi-supervised training of coarse-grained surrogates and enforcing physical constraints through virtual observables</summary>

- *Maximilian Rixner, Phaedon-Stelios Koutsourelakis*

- `2006.01789v1` - [abs](http://arxiv.org/abs/2006.01789v1) - [pdf](http://arxiv.org/pdf/2006.01789v1)

> The data-centric construction of inexpensive surrogates for fine-grained, physical models has been at the forefront of computational physics due to its significant utility in many-query tasks such as uncertainty quantification. Recent efforts have taken advantage of the enabling technologies from the field of machine learning (e.g. deep neural networks) in combination with simulation data. While such strategies have shown promise even in higher-dimensional problems, they generally require large amounts of training data even though the construction of surrogates is by definition a Small Data problem. Rather than employing data-based loss functions, it has been proposed to make use of the governing equations (in the simplest case at collocation points) in order to imbue domain knowledge in the training of the otherwise black-box-like interpolators. The present paper provides a flexible, probabilistic framework that accounts for physical structure and information both in the training objectives as well as in the surrogate model itself. We advocate a probabilistic (Bayesian) model in which equalities that are available from the physics (e.g. residuals, conservation laws) can be introduced as virtual observables and can provide additional information through the likelihood. We further advocate a generative model i.e. one that attempts to learn the joint density of inputs and outputs that is capable of making use of unlabeled data (i.e. only inputs) in a semi-supervised fashion in order to promote the discovery of lower-dimensional embeddings which are nevertheless predictive of the fine-grained model's output.

</details>

<details>

<summary>2020-06-02 20:49:37 - Non-Convex Optimization via Non-Reversible Stochastic Gradient Langevin Dynamics</summary>

- *Yuanhan Hu, Xiaoyu Wang, Xuefeng Gao, Mert Gurbuzbalaban, Lingjiong Zhu*

- `2004.02823v2` - [abs](http://arxiv.org/abs/2004.02823v2) - [pdf](http://arxiv.org/pdf/2004.02823v2)

> Stochastic Gradient Langevin Dynamics (SGLD) is a powerful algorithm for optimizing a non-convex objective, where a controlled and properly scaled Gaussian noise is added to the stochastic gradients to steer the iterates towards a global minimum. SGLD is based on the overdamped Langevin diffusion which is reversible in time. By adding an anti-symmetric matrix to the drift term of the overdamped Langevin diffusion, one gets a non-reversible diffusion that converges to the same stationary distribution with a faster convergence rate. In this paper, we study the non reversible Stochastic Gradient Langevin Dynamics (NSGLD) which is based on discretization of the non-reversible Langevin diffusion. We provide finite-time performance bounds for the global convergence of NSGLD for solving stochastic non-convex optimization problems. Our results lead to non-asymptotic guarantees for both population and empirical risk minimization problems. Numerical experiments for Bayesian independent component analysis and neural network models show that NSGLD can outperform SGLD with proper choices of the anti-symmetric matrix.

</details>

<details>

<summary>2020-06-03 05:14:05 - Reaping the Informational Surplus in Bayesian Persuasion</summary>

- *Ronen Gradwohl, Niklas Hahn, Martin Hoefer, Rann Smorodinsky*

- `2006.02048v1` - [abs](http://arxiv.org/abs/2006.02048v1) - [pdf](http://arxiv.org/pdf/2006.02048v1)

> The Bayesian persuasion model studies communication between an informed sender and a receiver with a payoff-relevant action, emphasizing the ability of a sender to extract maximal surplus from his informational advantage. In this paper we study a setting with multiple senders, but in which the receiver interacts with only one sender of his choice: senders commit to signals and the receiver then chooses, at the interim stage, with which sender to interact. Our main result is that whenever senders are even slightly uncertain about each other's preferences, the receiver receives all the informational surplus in all equilibria of this game.

</details>

<details>

<summary>2020-06-03 08:53:48 - Automatic Setting of DNN Hyper-Parameters by Mixing Bayesian Optimization and Tuning Rules</summary>

- *Michele Fraccaroli, Evelina Lamma, Fabrizio Riguzzi*

- `2006.02105v1` - [abs](http://arxiv.org/abs/2006.02105v1) - [pdf](http://arxiv.org/pdf/2006.02105v1)

> Deep learning techniques play an increasingly important role in industrial and research environments due to their outstanding results. However, the large number of hyper-parameters to be set may lead to errors if they are set manually. The state-of-the-art hyper-parameters tuning methods are grid search, random search, and Bayesian Optimization. The first two methods are expensive because they try, respectively, all possible combinations and random combinations of hyper-parameters. Bayesian Optimization, instead, builds a surrogate model of the objective function, quantifies the uncertainty in the surrogate using Gaussian Process Regression and uses an acquisition function to decide where to sample the new set of hyper-parameters. This work faces the field of Hyper-Parameters Optimization (HPO). The aim is to improve Bayesian Optimization applied to Deep Neural Networks. For this goal, we build a new algorithm for evaluating and analyzing the results of the network on the training and validation sets and use a set of tuning rules to add new hyper-parameters and/or to reduce the hyper-parameter search space to select a better combination.

</details>

<details>

<summary>2020-06-03 16:11:45 - From Probability to Consilience: How Explanatory Values Implement Bayesian Reasoning</summary>

- *Zachary Wojtowicz, Simon DeDeo*

- `2006.02359v1` - [abs](http://arxiv.org/abs/2006.02359v1) - [pdf](http://arxiv.org/pdf/2006.02359v1)

> Recent work in cognitive science has uncovered a diversity of explanatory values, or dimensions along which we judge explanations as better or worse. We propose a Bayesian account of how these values fit together to guide explanation. The resulting taxonomy provides a set of predictors for which explanations people prefer and shows how core values from psychology, statistics, and the philosophy of science emerge from a common mathematical framework. In addition to operationalizing the explanatory virtues associated with, for example, scientific argument-making, this framework also enables us to reinterpret the explanatory vices that drive conspiracy theories, delusions, and extremist ideologies.

</details>

<details>

<summary>2020-06-03 16:55:22 - When and How to Lift the Lockdown? Global COVID-19 Scenario Analysis and Policy Assessment using Compartmental Gaussian Processes</summary>

- *Zhaozhi Qian, Ahmed M. Alaa, Mihaela van der Schaar*

- `2005.08837v2` - [abs](http://arxiv.org/abs/2005.08837v2) - [pdf](http://arxiv.org/pdf/2005.08837v2)

> The coronavirus disease 2019 (COVID-19) global pandemic has led many countries to impose unprecedented lockdown measures in order to slow down the outbreak. Questions on whether governments have acted promptly enough, and whether lockdown measures can be lifted soon have since been central in public discourse. Data-driven models that predict COVID-19 fatalities under different lockdown policy scenarios are essential for addressing these questions and informing governments on future policy directions. To this end, this paper develops a Bayesian model for predicting the effects of COVID-19 lockdown policies in a global context -- we treat each country as a distinct data point, and exploit variations of policies across countries to learn country-specific policy effects. Our model utilizes a two-layer Gaussian process (GP) prior -- the lower layer uses a compartmental SEIR (Susceptible, Exposed, Infected, Recovered) model as a prior mean function with "country-and-policy-specific" parameters that capture fatality curves under "counterfactual" policies within each country, whereas the upper layer is shared across all countries, and learns lower-layer SEIR parameters as a function of a country's features and its policy indicators. Our model combines the solid mechanistic foundations of SEIR models (Bayesian priors) with the flexible data-driven modeling and gradient-based optimization routines of machine learning (Bayesian posteriors) -- i.e., the entire model is trained end-to-end via stochastic variational inference. We compare the projections of COVID-19 fatalities by our model with other models listed by the Center for Disease Control (CDC), and provide scenario analyses for various lockdown and reopening strategies highlighting their impact on COVID-19 fatalities.

</details>

<details>

<summary>2020-06-03 21:43:28 - You say Normalizing Flows I see Bayesian Networks</summary>

- *Antoine Wehenkel, Gilles Louppe*

- `2006.00866v2` - [abs](http://arxiv.org/abs/2006.00866v2) - [pdf](http://arxiv.org/pdf/2006.00866v2)

> Normalizing flows have emerged as an important family of deep neural networks for modelling complex probability distributions. In this note, we revisit their coupling and autoregressive transformation layers as probabilistic graphical models and show that they reduce to Bayesian networks with a pre-defined topology and a learnable density at each node. From this new perspective, we provide three results. First, we show that stacking multiple transformations in a normalizing flow relaxes independence assumptions and entangles the model distribution. Second, we show that a fundamental leap of capacity emerges when the depth of affine flows exceeds 3 transformation layers. Third, we prove the non-universality of the affine normalizing flow, regardless of its depth.

</details>

<details>

<summary>2020-06-03 21:55:42 - Designing over uncertain outcomes with stochastic sampling Bayesian optimization</summary>

- *Peter D. Tonner, Daniel V. Samarov, A. Gilad Kusne*

- `1911.02106v2` - [abs](http://arxiv.org/abs/1911.02106v2) - [pdf](http://arxiv.org/pdf/1911.02106v2)

> Optimization is becoming increasingly common in scientific and engineering domains. Oftentimes, these problems involve various levels of stochasticity or uncertainty in generating proposed solutions. Therefore, optimization in these scenarios must consider this stochasticity to properly guide the design of future experiments. Here, we adapt Bayesian optimization to handle uncertain outcomes, proposing a new framework called stochastic sampling Bayesian optimization (SSBO). We show that the bounds on expected regret for an upper confidence bound search in SSBO resemble those of earlier Bayesian optimization approaches, with added penalties due to the stochastic generation of inputs. Additionally, we adapt existing batch optimization techniques to properly limit the myopic decision making that can arise when selecting multiple instances before feedback. Finally, we show that SSBO techniques properly optimize a set of standard optimization problems as well as an applied problem inspired by bioengineering.

</details>

<details>

<summary>2020-06-04 02:25:37 - Representation Bayesian Risk Decompositions and Multi-Source Domain Adaptation</summary>

- *Xi Wu, Yang Guo, Jiefeng Chen, Yingyu Liang, Somesh Jha, Prasad Chalasani*

- `2004.10390v2` - [abs](http://arxiv.org/abs/2004.10390v2) - [pdf](http://arxiv.org/pdf/2004.10390v2)

> We consider representation learning (hypothesis class $\mathcal{H} = \mathcal{F}\circ\mathcal{G}$) where training and test distributions can be different. Recent studies provide hints and failure examples for domain invariant representation learning, a common approach for this problem, but the explanations provided are somewhat different and do not provide a unified picture. In this paper, we provide new decompositions of risk which give finer-grained explanations and clarify potential generalization issues. For Single-Source Domain Adaptation, we give an exact decomposition (an equality) of the target risk, via a natural hybrid argument, as sum of three factors: (1) source risk, (2) representation conditional label divergence, and (3) representation covariate shift. We derive a similar decomposition for the Multi-Source case. These decompositions reveal factors (2) and (3) as the precise reasons for failure to generalize. For example, we demonstrate that domain adversarial neural networks (DANN) attempt to regularize for (3) but miss (2), while a recent technique Invariant Risk Minimization (IRM) attempts to account for (2) but does not consider (3). We also verify our observations experimentally.

</details>

<details>

<summary>2020-06-04 04:13:44 - AdaptSPEC-X: Covariate Dependent Spectral Modeling of Multiple Nonstationary Time Series</summary>

- *Michael Bertolacci, Ori Rosen, Edward Cripps, Sally Cripps*

- `1908.06622v2` - [abs](http://arxiv.org/abs/1908.06622v2) - [pdf](http://arxiv.org/pdf/1908.06622v2)

> We present a method for the joint analysis of a panel of possibly nonstationary time series. The approach is Bayesian and uses a covariate-dependent infinite mixture model to incorporate multiple time series, with mixture components parameterized by a time varying mean and log spectrum. The mixture components are based on AdaptSPEC, a nonparametric model which adaptively divides the time series into an unknown number of segments and estimates the local log spectra by smoothing splines. We extend AdaptSPEC to handle missing values, a common feature of time series which can cause difficulties for nonparametric spectral methods. A second extension is to allow for a time varying mean. Covariates, assumed to be time-independent, are incorporated via the mixture weights using the logistic stick breaking process. The model can estimate time varying means and spectra at observed and unobserved covariate values, allowing for predictive inference. Estimation is performed by Markov chain Monte Carlo (MCMC) methods, combining data augmentation, reversible jump, and Riemann manifold Hamiltonian Monte Carlo techniques. We evaluate the methodology using simulated data, and describe applications to Australian rainfall data and measles incidence in the US. Software implementing the method proposed in this paper is available in the R package BayesSpec.

</details>

<details>

<summary>2020-06-04 14:32:53 - Empirical priors for prediction in sparse high-dimensional linear regression</summary>

- *Ryan Martin, Yiqi Tang*

- `1903.00961v2` - [abs](http://arxiv.org/abs/1903.00961v2) - [pdf](http://arxiv.org/pdf/1903.00961v2)

> In this paper we adopt the familiar sparse, high-dimensional linear regression model and focus on the important but often overlooked task of prediction. In particular, we consider a new empirical Bayes framework that incorporates data in the prior in two ways: one is to center the prior for the non-zero regression coefficients and the other is to provide some additional regularization. We show that, in certain settings, the asymptotic concentration of the proposed empirical Bayes posterior predictive distribution is very fast, and we establish a Bernstein--von Mises theorem which ensures that the derived empirical Bayes prediction intervals achieve the targeted frequentist coverage probability. The empirical prior has a convenient conjugate form, so posterior computations are relatively simple and fast. Finally, our numerical results demonstrate the proposed method's strong finite-sample performance in terms of prediction accuracy, uncertainty quantification, and computation time compared to existing Bayesian methods.

</details>

<details>

<summary>2020-06-04 15:26:07 - Quantifying Point-Prediction Uncertainty in Neural Networks via Residual Estimation with an I/O Kernel</summary>

- *Xin Qiu, Elliot Meyerson, Risto Miikkulainen*

- `1906.00588v5` - [abs](http://arxiv.org/abs/1906.00588v5) - [pdf](http://arxiv.org/pdf/1906.00588v5)

> Neural Networks (NNs) have been extensively used for a wide spectrum of real-world regression tasks, where the goal is to predict a numerical outcome such as revenue, effectiveness, or a quantitative result. In many such tasks, the point prediction is not enough: the uncertainty (i.e. risk or confidence) of that prediction must also be estimated. Standard NNs, which are most often used in such tasks, do not provide uncertainty information. Existing approaches address this issue by combining Bayesian models with NNs, but these models are hard to implement, more expensive to train, and usually do not predict as accurately as standard NNs. In this paper, a new framework (RIO) is developed that makes it possible to estimate uncertainty in any pretrained standard NN. The behavior of the NN is captured by modeling its prediction residuals with a Gaussian Process, whose kernel includes both the NN's input and its output. The framework is evaluated in twelve real-world datasets, where it is found to (1) provide reliable estimates of uncertainty, (2) reduce the error of the point predictions, and (3) scale well to large datasets. Given that RIO can be applied to any standard NN without modifications to model architecture or training pipeline, it provides an important ingredient for building real-world NN applications.

</details>

<details>

<summary>2020-06-04 21:02:29 - A combinatorial conjecture from PAC-Bayesian machine learning</summary>

- *M. Younsi, A. Lacasse*

- `2006.01387v2` - [abs](http://arxiv.org/abs/2006.01387v2) - [pdf](http://arxiv.org/pdf/2006.01387v2)

> We present a proof of a combinatorial conjecture from the second author's Ph.D. thesis. The proof relies on binomial and multinomial sums identities. We also discuss the relevance of the conjecture in the context of PAC-Bayesian machine learning.

</details>

<details>

<summary>2020-06-05 05:34:35 - Bayesian Sparse Covariance Structure Analysis for Correlated Count Data</summary>

- *Sho Ichigozaki, Takahiro Kawashima, Hayaru Shouno*

- `2006.03241v1` - [abs](http://arxiv.org/abs/2006.03241v1) - [pdf](http://arxiv.org/pdf/2006.03241v1)

> In this paper, we propose a Bayesian Graphical LASSO for correlated countable data and apply it to spatial crime data. In the proposed model, we assume a Gaussian Graphical Model for the latent variables which dominate the potential risks of crimes. To evaluate the proposed model, we determine optimal hyperparameters which represent samples better. We apply the proposed model for estimation of the sparse inverse covariance of the latent variable and evaluate the partial correlation coefficients. Finally, we illustrate the results on crime spots data and consider the estimated latent variables and the partial correlation coefficients of the sparse inverse covariance.

</details>

<details>

<summary>2020-06-05 08:39:17 - Inflation Dynamics of Financial Shocks</summary>

- *Olli Palmén*

- `2006.03301v1` - [abs](http://arxiv.org/abs/2006.03301v1) - [pdf](http://arxiv.org/pdf/2006.03301v1)

> We study the effects of financial shocks on the United States economy by using a Bayesian structural vector autoregressive (SVAR) model that exploits the non-normalities in the data. We use this method to uniquely identify the model and employ inequality constraints to single out financial shocks. The results point to the existence of two distinct financial shocks that have opposing effects on inflation, which supports the idea that financial shocks are transmitted to the real economy through both demand and supply side channels.

</details>

<details>

<summary>2020-06-05 09:32:27 - fbst: An R package for the Full Bayesian Significance Test for testing a sharp null hypothesis against its alternative via the e-value</summary>

- *Riko Kelter*

- `2006.03332v1` - [abs](http://arxiv.org/abs/2006.03332v1) - [pdf](http://arxiv.org/pdf/2006.03332v1)

> Hypothesis testing is a central statistical method in psychology and the cognitive sciences. However, the problems of null hypothesis significance testing (NHST) and p-values have been debated widely, but few attractive alternatives exist. This article introduces the fbst R package, which implements the Full Bayesian Significance Test (FBST) to test a sharp null hypothesis against its alternative via the e-value. The statistical theory of the FBST has been introduced by Pereira et al. (1999) more than two decades ago and since then, the FBST has shown to be a Bayesian alternative to NHST and p-values with both theoretical and practical highly appealing properties. The algorithm provided in the fbst package is applicable to any Bayesian model as long as the posterior distribution can be obtained at least numerically. The core function of the package provides the Bayesian evidence against the null hypothesis, the e-value. Additionally, p-values based on asymptotic arguments can be computed and rich visualisations for communication and interpretation of the results can be produced. Three examples of frequently used statistical procedures in the cognitive sciences are given in this paper which demonstrate how to apply the FBST in practice using the fbst package. Based on the success of the FBST in statistical science, the fbst package should be of interest to a broad range of researchers in psychology and the cognitive sciences and hopefully will encourage researchers to consider the FBST as a possible alternative when conducting hypothesis tests of a sharp null hypothesis.

</details>

<details>

<summary>2020-06-05 09:34:38 - The Full Bayesian Significance Test and the e-value -- Foundations, theory and application in the cognitive sciences</summary>

- *Riko Kelter, Julio Michael Stern*

- `2006.03334v1` - [abs](http://arxiv.org/abs/2006.03334v1) - [pdf](http://arxiv.org/pdf/2006.03334v1)

> Hypothesis testing is a central statistical method in psychological research and the cognitive sciences. While the problems of null hypothesis significance testing (NHST) have been debated widely, few attractive alternatives exist. In this paper, we provide a tutorial on the Full Bayesian Significance Test (FBST) and the e-value, which is a fully Bayesian alternative to traditional significance tests which rely on p-values. The FBST is an advanced methodological procedure which can be applied to several areas. In this tutorial, we showcase with two examples of widely used statistical methods in psychological research how the FBST can be used in practice, provide researchers with explicit guidelines on how to conduct it and make available R-code to reproduce all results. The FBST is an innovative method which has clearly demonstrated to perform better than frequentist significance testing. However, to our best knowledge, it has not been used so far in the psychological sciences and should be of wide interest to a broad range of researchers in psychology and the cognitive sciences.

</details>

<details>

<summary>2020-06-05 14:16:49 - Learning Bayesian Networks that enable full propagation of evidence</summary>

- *Anthony Constantinou*

- `2004.04571v2` - [abs](http://arxiv.org/abs/2004.04571v2) - [pdf](http://arxiv.org/pdf/2004.04571v2)

> This paper builds on recent developments in Bayesian network (BN) structure learning under the controversial assumption that the input variables are dependent. This assumption can be viewed as a learning constraint geared towards cases where the input variables are known or assumed to be dependent. It addresses the problem of learning multiple disjoint subgraphs that do not enable full propagation of evidence. This problem is highly prevalent in cases where the sample size of the input data is low with respect to the dimensionality of the model, which is often the case when working with real data. The paper presents a novel hybrid structure learning algorithm, called SaiyanH, that addresses this issue. The results show that this constraint helps the algorithm to estimate the number of true edges with higher accuracy compared to the state-of-the-art. Out of the 13 algorithms investigated, the results rank SaiyanH 4th in reconstructing the true DAG, with accuracy scores lower by 8.1% (F1), 10.2% (BSF), and 19.5% (SHD) compared to the top ranked algorithm, and higher by 75.5% (F1), 118% (BSF), and 4.3% (SHD) compared to the bottom ranked algorithm. Overall, the results suggest that the proposed algorithm discovers satisfactorily accurate connected DAGs in cases where other algorithms produce multiple disjoint subgraphs that often underfit the true graph.

</details>

<details>

<summary>2020-06-05 23:02:10 - Health Indicator Forecasting for Improving Remaining Useful Life Estimation</summary>

- *Qiyao Wang, Ahmed Farahat, Chetan Gupta, Haiyan Wang*

- `2006.03729v1` - [abs](http://arxiv.org/abs/2006.03729v1) - [pdf](http://arxiv.org/pdf/2006.03729v1)

> Prognostics is concerned with predicting the future health of the equipment and any potential failures. With the advances in the Internet of Things (IoT), data-driven approaches for prognostics that leverage the power of machine learning models are gaining popularity. One of the most important categories of data-driven approaches relies on a predefined or learned health indicator to characterize the equipment condition up to the present time and make inference on how it is likely to evolve in the future. In these approaches, health indicator forecasting that constructs the health indicator curve over the lifespan using partially observed measurements (i.e., health indicator values within an initial period) plays a key role. Existing health indicator forecasting algorithms, such as the functional Empirical Bayesian approach, the regression-based formulation, a naive scenario matching based on the nearest neighbor, have certain limitations. In this paper, we propose a new `generative + scenario matching' algorithm for health indicator forecasting. The key idea behind the proposed approach is to first non-parametrically fit the underlying health indicator curve with a continuous Gaussian Process using a sample of run-to-failure health indicator curves. The proposed approach then generates a rich set of random curves from the learned distribution, attempting to obtain all possible variations of the target health condition evolution process over the system's lifespan. The health indicator extrapolation for a piece of functioning equipment is inferred as the generated curve that has the highest matching level within the observed period. Our experimental results show the superiority of our algorithm over the other state-of-the-art methods.

</details>

<details>

<summary>2020-06-06 03:30:47 - Reinforcement Learning in Factored MDPs: Oracle-Efficient Algorithms and Tighter Regret Bounds for the Non-Episodic Setting</summary>

- *Ziping Xu, Ambuj Tewari*

- `2002.02302v2` - [abs](http://arxiv.org/abs/2002.02302v2) - [pdf](http://arxiv.org/pdf/2002.02302v2)

> We study reinforcement learning in non-episodic factored Markov decision processes (FMDPs). We propose two near-optimal and oracle-efficient algorithms for FMDPs. Assuming oracle access to an FMDP planner, they enjoy a Bayesian and a frequentist regret bound respectively, both of which reduce to the near-optimal bound $\widetilde{O}(DS\sqrt{AT})$ for standard non-factored MDPs. We propose a tighter connectivity measure, factored span, for FMDPs and prove a lower bound that depends on the factored span rather than the diameter $D$. In order to decrease the gap between lower and upper bounds, we propose an adaptation of the REGAL.C algorithm whose regret bound depends on the factored span. Our oracle-efficient algorithms outperform previously proposed near-optimal algorithms on computer network administration simulations.

</details>

<details>

<summary>2020-06-06 18:04:35 - Sparse representation for damage identification of structural systems</summary>

- *Zhao Chen, Hao Sun*

- `2006.03929v1` - [abs](http://arxiv.org/abs/2006.03929v1) - [pdf](http://arxiv.org/pdf/2006.03929v1)

> Identifying damage of structural systems is typically characterized as an inverse problem which might be ill-conditioned due to aleatory and epistemic uncertainties induced by measurement noise and modeling error. Sparse representation can be used to perform inverse analysis for the case of sparse damage. In this paper, we propose a novel two-stage sensitivity analysis-based framework for both model updating and sparse damage identification. Specifically, an $\ell_2$ Bayesian learning method is firstly developed for updating the intact model and uncertainty quantification so as to set forward a baseline for damage detection. A sparse representation pipeline built on a quasi-$\ell_0$ method, e.g., Sequential Threshold Least Squares (STLS) regression, is then presented for damage localization and quantification. Additionally, Bayesian optimization together with cross validation is developed to heuristically learn hyperparameters from data, which saves the computational cost of hyperparameter tuning and produces more reliable identification result. The proposed framework is verified by three examples, including a 10-story shear-type building, a complex truss structure, and a shake table test of an eight-story steel frame. Results show that the proposed approach is capable of both localizing and quantifying structural damage with high accuracy.

</details>

<details>

<summary>2020-06-07 13:27:56 - Regularised Zero-Variance Control Variates for High-Dimensional Variance Reduction</summary>

- *Leah F. South, Chris J. Oates, Antonietta Mira, Christopher Drovandi*

- `1811.05073v5` - [abs](http://arxiv.org/abs/1811.05073v5) - [pdf](http://arxiv.org/pdf/1811.05073v5)

> Zero-variance control variates (ZV-CV) are a post-processing method to reduce the variance of Monte Carlo estimators of expectations using the derivatives of the log target. Once the derivatives are available, the only additional computational effort lies in solving a linear regression problem. Significant variance reductions have been achieved with this method in low dimensional examples, but the number of covariates in the regression rapidly increases with the dimension of the target. In this paper, we present compelling empirical evidence that the use of penalised regression techniques in the selection of high-dimensional control variates provides performance gains over the classical least squares method. Another type of regularisation based on using subsets of derivatives, or a priori regularisation as we refer to it in this paper, is also proposed to reduce computational and storage requirements. Several examples showing the utility and limitations of regularised ZV-CV for Bayesian inference are given. The methods proposed in this paper are accessible through the R package ZVCV.

</details>

<details>

<summary>2020-06-07 15:38:35 - Uncertainty-Aware Deep Classifiers using Generative Models</summary>

- *Murat Sensoy, Lance Kaplan, Federico Cerutti, Maryam Saleki*

- `2006.04183v1` - [abs](http://arxiv.org/abs/2006.04183v1) - [pdf](http://arxiv.org/pdf/2006.04183v1)

> Deep neural networks are often ignorant about what they do not know and overconfident when they make uninformed predictions. Some recent approaches quantify classification uncertainty directly by training the model to output high uncertainty for the data samples close to class boundaries or from the outside of the training distribution. These approaches use an auxiliary data set during training to represent out-of-distribution samples. However, selection or creation of such an auxiliary data set is non-trivial, especially for high dimensional data such as images. In this work we develop a novel neural network model that is able to express both aleatoric and epistemic uncertainty to distinguish decision boundary and out-of-distribution regions of the feature space. To this end, variational autoencoders and generative adversarial networks are incorporated to automatically generate out-of-distribution exemplars for training. Through extensive analysis, we demonstrate that the proposed approach provides better estimates of uncertainty for in- and out-of-distribution samples, and adversarial examples on well-known data sets against state-of-the-art approaches including recent Bayesian approaches for neural networks and anomaly detection methods.

</details>

<details>

<summary>2020-06-07 18:48:43 - Bayesian Hidden Physics Models: Uncertainty Quantification for Discovery of Nonlinear Partial Differential Operators from Data</summary>

- *Steven Atkinson*

- `2006.04228v1` - [abs](http://arxiv.org/abs/2006.04228v1) - [pdf](http://arxiv.org/pdf/2006.04228v1)

> What do data tell us about physics-and what don't they tell us? There has been a surge of interest in using machine learning models to discover governing physical laws such as differential equations from data, but current methods lack uncertainty quantification to communicate their credibility. This work addresses this shortcoming from a Bayesian perspective. We introduce a novel model comprising "leaf" modules that learn to represent distinct experiments' spatiotemporal functional data as neural networks and a single "root" module that expresses a nonparametric distribution over their governing nonlinear differential operator as a Gaussian process. Automatic differentiation is used to compute the required partial derivatives from the leaf functions as inputs to the root. Our approach quantifies the reliability of the learned physics in terms of a posterior distribution over operators and propagates this uncertainty to solutions of novel initial-boundary value problem instances. Numerical experiments demonstrate the method on several nonlinear PDEs.

</details>

<details>

<summary>2020-06-08 00:28:41 - Randomised Gaussian Process Upper Confidence Bound for Bayesian Optimisation</summary>

- *Julian Berk, Sunil Gupta, Santu Rana, Svetha Venkatesh*

- `2006.04296v1` - [abs](http://arxiv.org/abs/2006.04296v1) - [pdf](http://arxiv.org/pdf/2006.04296v1)

> In order to improve the performance of Bayesian optimisation, we develop a modified Gaussian process upper confidence bound (GP-UCB) acquisition function. This is done by sampling the exploration-exploitation trade-off parameter from a distribution. We prove that this allows the expected trade-off parameter to be altered to better suit the problem without compromising a bound on the function's Bayesian regret. We also provide results showing that our method achieves better performance than GP-UCB in a range of real-world and synthetic problems.

</details>

<details>

<summary>2020-06-08 06:51:09 - Sparse Spectrum Gaussian Process for Bayesian Optimization</summary>

- *Ang Yang, Cheng Li, Santu Rana, Sunil Gupta, Svetha Venkatesh*

- `1906.08898v2` - [abs](http://arxiv.org/abs/1906.08898v2) - [pdf](http://arxiv.org/pdf/1906.08898v2)

> We propose a novel sparse spectrum approximation of Gaussian process (GP) tailored for Bayesian optimization. Whilst the current sparse spectrum methods provide desired approximations for regression problems, it is observed that this particular form of sparse approximations generates an overconfident GP, i.e. it produces less epistemic uncertainty than the original GP. Since the balance between predictive mean and the predictive variance is the key determinant to the success of Bayesian optimization, the current sparse spectrum methods are less suitable for it. We derive a new regularized marginal likelihood for finding the optimal frequencies to fix this over-confidence issue, particularly for Bayesian optimization. The regularizer trades off the accuracy in the model fitting with a targeted increase in the predictive variance of the resultant GP. Specifically, we use the entropy of the global maximum distribution from the posterior GP as the regularizer that needs to be maximized. Since this distribution cannot be calculated analytically, we first propose a Thompson sampling based approach and then a more efficient sequential Monte Carlo based approach to estimate it. Later, we also show that the Expected Improvement acquisition function can be used as a proxy for the maximum distribution, thus making the whole process further efficient. Experiments show considerable improvement to Bayesian optimization convergence rate over the vanilla sparse spectrum method and over a full GP when its covariance matrix is ill-conditioned due to the presence of a large number of observations.

</details>

<details>

<summary>2020-06-08 13:01:37 - A Variational View on Bootstrap Ensembles as Bayesian Inference</summary>

- *Dimitrios Milios, Pietro Michiardi, Maurizio Filippone*

- `2006.04548v1` - [abs](http://arxiv.org/abs/2006.04548v1) - [pdf](http://arxiv.org/pdf/2006.04548v1)

> In this paper, we employ variational arguments to establish a connection between ensemble methods for Neural Networks and Bayesian inference. We consider an ensemble-based scheme where each model/particle corresponds to a perturbation of the data by means of parametric bootstrap and a perturbation of the prior. We derive conditions under which any optimization steps of the particles makes the associated distribution reduce its divergence to the posterior over model parameters. Such conditions do not require any particular form for the approximation and they are purely geometrical, giving insights on the behavior of the ensemble on a number of interesting models such as Neural Networks with ReLU activations. Experiments confirm that ensemble methods can be a valid alternative to approximate Bayesian inference; the theoretical developments in the paper seek to explain this behavior.

</details>

<details>

<summary>2020-06-08 13:16:02 - A Survey of Bayesian Statistical Approaches for Big Data</summary>

- *Farzana Jahan, Insha Ullah, Kerrie L Mengersen*

- `2006.04565v1` - [abs](http://arxiv.org/abs/2006.04565v1) - [pdf](http://arxiv.org/pdf/2006.04565v1)

> The modern era is characterised as an era of information or Big Data. This has motivated a huge literature on new methods for extracting information and insights from these data. A natural question is how these approaches differ from those that were available prior to the advent of Big Data. We present a review of published studies that present Bayesian statistical approaches specifically for Big Data and discuss the reported and perceived benefits of these approaches. We conclude by addressing the question of whether focusing only on improving computational algorithms and infrastructure will be enough to face the challenges of Big Data.

</details>

<details>

<summary>2020-06-08 14:05:28 - BVAR-Connect: A Variational Bayes Approach to Multi-Subject Vector Autoregressive Models for Inference on Brain Connectivity Networks</summary>

- *Jeong Hwan Kook, Kelly A. Vaughn, Dana M. DeMaster, Linda Ewing-Cobbs, Marina Vannucci*

- `2006.04608v1` - [abs](http://arxiv.org/abs/2006.04608v1) - [pdf](http://arxiv.org/pdf/2006.04608v1)

> In this paper we propose BVAR-connect, a variational inference approach to a Bayesian multi-subject vector autoregressive (VAR) model for inference on effective brain connectivity based on resting-state functional MRI data. The modeling framework uses a Bayesian variable selection approach that flexibly integrates multi-modal data, in particular structural diffusion tensor imaging (DTI) data, into the prior construction. The variational inference approach we develop allows scalability of the methods and results in the ability to estimate subject- and group-level brain connectivity networks over whole-brain parcellations of the data. We provide a brief description of a user-friendly MATLAB GUI released for public use. We assess performance on simulated data, where we show that the proposed inference method can achieve comparable accuracy to the sampling-based Markov Chain Monte Carlo approach but at a much lower computational cost. We also address the case of subject groups with imbalanced sample sizes. Finally, we illustrate the methods on resting-state functional MRI and structural DTI data on children with a history of traumatic injury.

</details>

<details>

<summary>2020-06-08 18:34:26 - Incorporating historical information to improve phase I clinical trial designs</summary>

- *Yanhong Zhou, J. Jack Lee, Shunguang Wang, Stuart Bailey, Ying Yuan*

- `2004.12972v3` - [abs](http://arxiv.org/abs/2004.12972v3) - [pdf](http://arxiv.org/pdf/2004.12972v3)

> Incorporating historical data or real-world evidence has a great potential to improve the efficiency of phase I clinical trials and to accelerate drug development. For model-based designs, such as the continuous reassessment method (CRM), this can be conveniently carried out by specifying a "skeleton," i.e., the prior estimate of dose limiting toxicity (DLT) probability at each dose. In contrast, little work has been done to incorporate historical data or real-world evidence into model-assisted designs, such as the Bayesian optimal interval (BOIN), keyboard, and modified toxicity probability interval (mTPI) designs. This has led to the misconception that model-assisted designs cannot incorporate prior information. In this paper, we propose a unified framework that allows for incorporating historical data or real-world evidence into model-assisted designs. The proposed approach uses the well-established "skeleton" approach, combined with the concept of prior effective sample size, thus it is easy to understand and use. More importantly, our approach maintains the hallmark of model-assisted designs: simplicity---the dose escalation/de-escalation rule can be tabulated prior to the trial conduct. Extensive simulation studies show that the proposed method can effectively incorporate prior information to improve the operating characteristics of model-assisted designs, similarly to model-based designs.

</details>

<details>

<summary>2020-06-08 20:00:36 - Wat zei je? Detecting Out-of-Distribution Translations with Variational Transformers</summary>

- *Tim Z. Xiao, Aidan N. Gomez, Yarin Gal*

- `2006.08344v1` - [abs](http://arxiv.org/abs/2006.08344v1) - [pdf](http://arxiv.org/pdf/2006.08344v1)

> We detect out-of-training-distribution sentences in Neural Machine Translation using the Bayesian Deep Learning equivalent of Transformer models. For this we develop a new measure of uncertainty designed specifically for long sequences of discrete random variables -- i.e. words in the output sentence. Our new measure of uncertainty solves a major intractability in the naive application of existing approaches on long sentences. We use our new measure on a Transformer model trained with dropout approximate inference. On the task of German-English translation using WMT13 and Europarl, we show that with dropout uncertainty our measure is able to identify when Dutch source sentences, sentences which use the same word types as German, are given to the model instead of German.

</details>

<details>

<summary>2020-06-09 07:31:21 - Isotropic SGD: a Practical Approach to Bayesian Posterior Sampling</summary>

- *Giulio Franzese, Rosa Candela, Dimitrios Milios, Maurizio Filippone, Pietro Michiardi*

- `2006.05087v1` - [abs](http://arxiv.org/abs/2006.05087v1) - [pdf](http://arxiv.org/pdf/2006.05087v1)

> In this work we define a unified mathematical framework to deepen our understanding of the role of stochastic gradient (SG) noise on the behavior of Markov chain Monte Carlo sampling (SGMCMC) algorithms.   Our formulation unlocks the design of a novel, practical approach to posterior sampling, which makes the SG noise isotropic using a fixed learning rate that we determine analytically, and that requires weaker assumptions than existing algorithms. In contrast, the common traits of existing \sgmcmc algorithms is to approximate the isotropy condition either by drowning the gradients in additive noise (annealing the learning rate) or by making restrictive assumptions on the \sg noise covariance and the geometry of the loss landscape.   Extensive experimental validations indicate that our proposal is competitive with the state-of-the-art on \sgmcmc, while being much more practical to use.

</details>

<details>

<summary>2020-06-09 07:35:48 - Differentiable Bandit Exploration</summary>

- *Craig Boutilier, Chih-Wei Hsu, Branislav Kveton, Martin Mladenov, Csaba Szepesvari, Manzil Zaheer*

- `2002.06772v2` - [abs](http://arxiv.org/abs/2002.06772v2) - [pdf](http://arxiv.org/pdf/2002.06772v2)

> Exploration policies in Bayesian bandits maximize the average reward over problem instances drawn from some distribution $\mathcal{P}$. In this work, we learn such policies for an unknown distribution $\mathcal{P}$ using samples from $\mathcal{P}$. Our approach is a form of meta-learning and exploits properties of $\mathcal{P}$ without making strong assumptions about its form. To do this, we parameterize our policies in a differentiable way and optimize them by policy gradients, an approach that is general and easy to implement. We derive effective gradient estimators and introduce novel variance reduction techniques. We also analyze and experiment with various bandit policy classes, including neural networks and a novel softmax policy. The latter has regret guarantees and is a natural starting point for our optimization. Our experiments show the versatility of our approach. We also observe that neural network policies can learn implicit biases expressed only through the sampled instances.

</details>

<details>

<summary>2020-06-09 13:08:40 - Detecting structural perturbations from time series with deep learning</summary>

- *Edward Laurence, Charles Murphy, Guillaume St-Onge, Xavier Roy-Pomerleau, Vincent Thibeault*

- `2006.05232v1` - [abs](http://arxiv.org/abs/2006.05232v1) - [pdf](http://arxiv.org/pdf/2006.05232v1)

> Small disturbances can trigger functional breakdowns in complex systems. A challenging task is to infer the structural cause of a disturbance in a networked system, soon enough to prevent a catastrophe. We present a graph neural network approach, borrowed from the deep learning paradigm, to infer structural perturbations from functional time series. We show our data-driven approach outperforms typical reconstruction methods while meeting the accuracy of Bayesian inference. We validate the versatility and performance of our approach with epidemic spreading, population dynamics, and neural dynamics, on various network structures: random networks, scale-free networks, 25 real food-web systems, and the C. Elegans connectome. Moreover, we report that our approach is robust to data corruption. This work uncovers a practical avenue to study the resilience of real-world complex systems.

</details>

<details>

<summary>2020-06-09 18:29:23 - Random Hypervolume Scalarizations for Provable Multi-Objective Black Box Optimization</summary>

- *Daniel Golovin, Qiuyi Zhang*

- `2006.04655v2` - [abs](http://arxiv.org/abs/2006.04655v2) - [pdf](http://arxiv.org/pdf/2006.04655v2)

> Single-objective black box optimization (also known as zeroth-order optimization) is the process of minimizing a scalar objective $f(x)$, given evaluations at adaptively chosen inputs $x$. In this paper, we consider multi-objective optimization, where $f(x)$ outputs a vector of possibly competing objectives and the goal is to converge to the Pareto frontier. Quantitatively, we wish to maximize the standard hypervolume indicator metric, which measures the dominated hypervolume of the entire set of chosen inputs. In this paper, we introduce a novel scalarization function, which we term the hypervolume scalarization, and show that drawing random scalarizations from an appropriately chosen distribution can be used to efficiently approximate the hypervolume indicator metric. We utilize this connection to show that Bayesian optimization with our scalarization via common acquisition functions, such as Thompson Sampling or Upper Confidence Bound, provably converges to the whole Pareto frontier by deriving tight hypervolume regret bounds on the order of $\widetilde{O}(\sqrt{T})$. Furthermore, we highlight the general utility of our scalarization framework by showing that any provably convergent single-objective optimization process can be effortlessly converted to a multi-objective optimization process with provable convergence guarantees.

</details>

<details>

<summary>2020-06-09 18:49:32 - A generalized Bayes framework for probabilistic clustering</summary>

- *Tommaso Rigon, Amy H. Herring, David B. Dunson*

- `2006.05451v1` - [abs](http://arxiv.org/abs/2006.05451v1) - [pdf](http://arxiv.org/pdf/2006.05451v1)

> Loss-based clustering methods, such as k-means and its variants, are standard tools for finding groups in data. However, the lack of quantification of uncertainty in the estimated clusters is a disadvantage. Model-based clustering based on mixture models provides an alternative, but such methods face computational problems and large sensitivity to the choice of kernel. This article proposes a generalized Bayes framework that bridges between these two paradigms through the use of Gibbs posteriors. In conducting Bayesian updating, the log likelihood is replaced by a loss function for clustering, leading to a rich family of clustering methods. The Gibbs posterior represents a coherent updating of Bayesian beliefs without needing to specify a likelihood for the data, and can be used for characterizing uncertainty in clustering. We consider losses based on Bregman divergence and pairwise similarities, and develop efficient deterministic algorithms for point estimation along with sampling algorithms for uncertainty quantification. Several existing clustering algorithms, including k-means, can be interpreted as generalized Bayes estimators under our framework, and hence we provide a method of uncertainty quantification for these approaches.

</details>

<details>

<summary>2020-06-09 18:52:47 - Robust Bayesian variable selection for gene-environment interactions</summary>

- *Jie Ren, Fei Zhou, Xiaoxi Li, Shuangge Ma, Yu Jiang, Cen Wu*

- `2006.05455v1` - [abs](http://arxiv.org/abs/2006.05455v1) - [pdf](http://arxiv.org/pdf/2006.05455v1)

> Gene-environment (G$\times$E) interactions have important implications to elucidate the etiology of complex diseases beyond the main genetic and environmental effects. Outliers and data contamination in disease phenotypes of G$\times$E studies have been commonly encountered, leading to the development of a broad spectrum of robust regularization methods. Nevertheless, within the Bayesian framework, the issue has not been taken care of in existing studies. We develop a fully Bayesian robust variable selection method for G$\times$E interaction studies. The proposed Bayesian method can effectively accommodate heavy-tailed errors and outliers in the response variable while conducting variable selection by accounting for structural sparsity. In particular, for the robust sparse group selection, the spike-and-slab priors have been imposed on both individual and group levels to identify important main and interaction effects robustly. An efficient Gibbs sampler has been developed to facilitate fast computation. Extensive simulation studies and analysis of both the diabetes data with SNP measurements from the Nurses' Health Study and TCGA melanoma data with gene expression measurements demonstrate the superior performance of the proposed method over multiple competing alternatives.

</details>

<details>

<summary>2020-06-09 20:14:03 - Linear Models are Most Favorable among Generalized Linear Models</summary>

- *Kuan-Yun Lee, Thomas A. Courtade*

- `2006.05492v1` - [abs](http://arxiv.org/abs/2006.05492v1) - [pdf](http://arxiv.org/pdf/2006.05492v1)

> We establish a nonasymptotic lower bound on the $L_2$ minimax risk for a class of generalized linear models. It is further shown that the minimax risk for the canonical linear model matches this lower bound up to a universal constant. Therefore, the canonical linear model may be regarded as most favorable among the considered class of generalized linear models (in terms of minimax risk). The proof makes use of an information-theoretic Bayesian Cram\'er-Rao bound for log-concave priors, established by Aras et al. (2019).

</details>

<details>

<summary>2020-06-09 20:38:25 - Cross-entropy-based importance sampling with failure-informed dimension reduction for rare event simulation</summary>

- *Felipe Uribe, Iason Papaioannou, Youssef M. Marzouk, Daniel Straub*

- `2006.05496v1` - [abs](http://arxiv.org/abs/2006.05496v1) - [pdf](http://arxiv.org/pdf/2006.05496v1)

> The estimation of rare event or failure probabilities in high dimensions is of interest in many areas of science and technology. We consider problems where the rare event is expressed in terms of a computationally costly numerical model. Importance sampling with the cross-entropy method offers an efficient way to address such problems provided that a suitable parametric family of biasing densities is employed. Although some existing parametric distribution families are designed to perform efficiently in high dimensions, their applicability within the cross-entropy method is limited to problems with dimension of O(1e2). In this work, rather than directly building sampling densities in high dimensions, we focus on identifying the intrinsic low-dimensional structure of the rare event simulation problem. To this end, we exploit a connection between rare event simulation and Bayesian inverse problems. This allows us to adapt dimension reduction techniques from Bayesian inference to construct new, effectively low-dimensional, biasing distributions within the cross-entropy method. In particular, we employ the approach in [47], as it enables control of the error in the approximation of the optimal biasing distribution. We illustrate our method using two standard high-dimensional reliability benchmark problems and one structural mechanics application involving random fields.

</details>

<details>

<summary>2020-06-09 21:31:21 - Adversarial Canonical Correlation Analysis</summary>

- *Benjamin Dutton*

- `2005.10349v2` - [abs](http://arxiv.org/abs/2005.10349v2) - [pdf](http://arxiv.org/pdf/2005.10349v2)

> Canonical Correlation Analysis (CCA) is a statistical technique used to extract common information from multiple data sources or views. It has been used in various representation learning problems, such as dimensionality reduction, word embedding, and clustering. Recent work has given CCA probabilistic footing in a deep learning context and uses a variational lower bound for the data log likelihood to estimate model parameters. Alternatively, adversarial techniques have arisen in recent years as a powerful alternative to variational Bayesian methods in autoencoders. In this work, we explore straightforward adversarial alternatives to recent work in Deep Variational CCA (VCCA and VCCA-Private) we call ACCA and ACCA-Private and show how these approaches offer a stronger and more flexible way to match the approximate posteriors coming from encoders to much larger classes of priors than the VCCA and VCCA-Private models. This allows new priors for what constitutes a good representation, such as disentangling underlying factors of variation, to be more directly pursued. We offer further analysis on the multi-level disentangling properties of VCCA-Private and ACCA-Private through the use of a newly designed dataset we call Tangled MNIST. We also design a validation criteria for these models that is theoretically grounded, task-agnostic, and works well in practice. Lastly, we fill a minor research gap by deriving an additional variational lower bound for VCCA that allows the representation to use view-specific information from both input views.

</details>

<details>

<summary>2020-06-10 08:32:39 - Bayesian Experience Reuse for Learning from Multiple Demonstrators</summary>

- *Michael Gimelfarb, Scott Sanner, Chi-Guhn Lee*

- `2006.05725v1` - [abs](http://arxiv.org/abs/2006.05725v1) - [pdf](http://arxiv.org/pdf/2006.05725v1)

> Learning from demonstrations (LfD) improves the exploration efficiency of a learning agent by incorporating demonstrations from experts. However, demonstration data can often come from multiple experts with conflicting goals, making it difficult to incorporate safely and effectively in online settings. We address this problem in the static and dynamic optimization settings by modelling the uncertainty in source and target task functions using normal-inverse-gamma priors, whose corresponding posteriors are, respectively, learned from demonstrations and target data using Bayesian neural networks with shared features. We use this learned belief to derive a quadratic programming problem whose solution yields a probability distribution over the expert models. Finally, we propose Bayesian Experience Reuse (BERS) to sample demonstrations in accordance with this distribution and reuse them directly in new tasks. We demonstrate the effectiveness of this approach for static optimization of smooth functions, and transfer learning in a high-dimensional supply chain problem with cost uncertainty.

</details>

<details>

<summary>2020-06-10 09:34:56 - A different approach for choosing a threshold in peaks over threshold</summary>

- *Andréhette Verster, Lizanne Raubenheimer*

- `2006.05748v1` - [abs](http://arxiv.org/abs/2006.05748v1) - [pdf](http://arxiv.org/pdf/2006.05748v1)

> Abstract In Extreme Value methodology the choice of threshold plays an important role in efficient modelling of observations exceeding the threshold. The threshold must be chosen high enough to ensure an unbiased extreme value index but choosing the threshold too high results in uncontrolled variances. This paper investigates a generalized model that can assist in the choice of optimal threshold values in the \gamma positive domain. A Bayesian approach is considered by deriving a posterior distribution for the unknown generalized parameter. Using the properties of the posterior distribution allows for a method to choose an optimal threshold without visual inspection.

</details>

<details>

<summary>2020-06-10 12:16:39 - Understanding and adjusting the selection bias from a proof-of-concept study to a more confirmatory study</summary>

- *Yongming Qu, Yu Du, Ying Zhang, Lei Shen*

- `2006.05795v1` - [abs](http://arxiv.org/abs/2006.05795v1) - [pdf](http://arxiv.org/pdf/2006.05795v1)

> It has long been noticed that the efficacy observed in small early phase studies is generally better than that observed in later larger studies. Historically, the inflation of the efficacy results from early proof-of-concept studies is either ignored, or adjusted empirically using a frequentist or Bayesian approach. In this article, we systematically explained the underlying reason for the inflation of efficacy results in small early phase studies from the perspectives of measurement error models and selection bias. A systematic method was built to adjust the early phase study results from both frequentist and Bayesian perspectives. A hierarchical model was proposed to estimate the distribution of the efficacy for a portfolio of compounds, which can serve as the prior distribution for the Bayesian approach. We showed through theory that the systematic adjustment provides an unbiased estimator for the true mean efficacy for a portfolio of compounds. The adjustment was applied to paired data for the efficacy in early small and later larger studies for a set of compounds in diabetes and immunology. After the adjustment, the bias in the early phase small studies seems to be diminished.

</details>

<details>

<summary>2020-06-10 15:00:24 - Projected Stein Variational Gradient Descent</summary>

- *Peng Chen, Omar Ghattas*

- `2002.03469v2` - [abs](http://arxiv.org/abs/2002.03469v2) - [pdf](http://arxiv.org/pdf/2002.03469v2)

> The curse of dimensionality is a longstanding challenge in Bayesian inference in high dimensions. In this work, we propose a projected Stein variational gradient descent (pSVGD) method to overcome this challenge by exploiting the fundamental property of intrinsic low dimensionality of the data informed subspace stemming from ill-posedness of such problems. We adaptively construct the subspace using a gradient information matrix of the log-likelihood, and apply pSVGD to the much lower-dimensional coefficients of the parameter projection. The method is demonstrated to be more accurate and efficient than SVGD. It is also shown to be more scalable with respect to the number of parameters, samples, data points, and processor cores via experiments with parameters dimensions ranging from the hundreds to the tens of thousands.

</details>

<details>

<summary>2020-06-10 18:12:44 - Convergence of Pseudo-Bayes Factors in Forward and Inverse Regression Problems</summary>

- *Debashis Chatterjee, Sourabh Bhattacharya*

- `2006.06020v1` - [abs](http://arxiv.org/abs/2006.06020v1) - [pdf](http://arxiv.org/pdf/2006.06020v1)

> In the Bayesian literature on model comparison, Bayes factors play the leading role. In the classical statistical literature, model selection criteria are often devised used cross-validation ideas. Amalgamating the ideas of Bayes factor and cross-validation Geisser and Eddy (1979) created the pseudo-Bayes factor. The usage of cross-validation inculcates several theoretical advantages, computational simplicity and numerical stability in Bayes factors as the marginal density of the entire dataset is replaced with products of cross-validation densities of individual data points.   However, the popularity of pseudo-Bayes factors is still negligible in comparison with Bayes factors, with respect to both theoretical investigations and practical applications. In this article, we establish almost sure exponential convergence of pseudo-Bayes factors for large samples under a general setup consisting of dependent data and model misspecifications. We particularly focus on general parametric and nonparametric regression setups in both forward and inverse contexts.   We illustrate our theoretical results with various examples, providing explicit calculations. We also supplement our asymptotic theory with simulation experiments in small sample situations of Poisson log regression and geometric logit and probit regression, additionally addressing the variable selection problem. We consider both linear and nonparametric regression modeled by Gaussian processes for our purposes. Our simulation results provide quite interesting insights into the usage of pseudo-Bayes factors in forward and inverse setups.

</details>

<details>

<summary>2020-06-11 06:48:49 - Is being an only child harmful to psychological health?: Evidence from an instrumental variable analysis of China's One-Child Policy</summary>

- *Shuxi Zeng, Fan Li, Peng Ding*

- `2005.09130v2` - [abs](http://arxiv.org/abs/2005.09130v2) - [pdf](http://arxiv.org/pdf/2005.09130v2)

> This paper evaluates the effects of being an only child in a family on psychological health, leveraging data on the One-Child Policy in China. We use an instrumental variable approach to address the potential unmeasured confounding between the fertility decision and psychological health, where the instrumental variable is an index on the intensity of the implementation of the One-Child Policy. We establish an analytical link between the local instrumental variable approach and principal stratification to accommodate the continuous instrumental variable. Within the principal stratification framework, we postulate a Bayesian hierarchical model to infer various causal estimands of policy interest while adjusting for the clustering data structure. We apply the method to the data from the China Family Panel Studies and find small but statistically significant negative effects of being an only child on self-reported psychological health for some subpopulations. Our analysis reveals treatment effect heterogeneity with respect to both observed and unobserved characteristics. In particular, urban males suffer the most from being only children, and the negative effect has larger magnitude if the families were more resistant to the One-Child Policy. We also conduct sensitivity analysis to assess the key instrumental variable assumption.

</details>

<details>

<summary>2020-06-11 07:23:53 - Robust Approximate Bayesian Inference with Synthetic Likelihood</summary>

- *David T. Frazier, Christopher Drovandi*

- `1904.04551v3` - [abs](http://arxiv.org/abs/1904.04551v3) - [pdf](http://arxiv.org/pdf/1904.04551v3)

> Bayesian synthetic likelihood (BSL) is now an established method for conducting approximate Bayesian inference in models where, due to the intractability of the likelihood function, exact Bayesian approaches are either infeasible or computationally too demanding. Implicit in the application of BSL is the assumption that the data generating process (DGP) can produce simulated summary statistics that capture the behaviour of the observed summary statistics. We demonstrate that if this compatibility between the actual and assumed DGP is not satisfied, i.e., if the model is misspecified, BSL can yield unreliable parameter inference. To circumvent this issue, we propose a new BSL approach that can detect the presence of model misspecification, and simultaneously deliver useful inferences even under significant model misspecification. Two simulated and two real data examples demonstrate the performance of this new approach to BSL, and document its superior accuracy over standard BSL when the assumed model is misspecified.

</details>

<details>

<summary>2020-06-11 12:14:54 - Fiducial and Posterior Sampling</summary>

- *Gunnar Taraldsen, Bo H. Lindqvist*

- `2006.09975v1` - [abs](http://arxiv.org/abs/2006.09975v1) - [pdf](http://arxiv.org/pdf/2006.09975v1)

> The fiducial coincides with the posterior in a group model equipped with the right Haar prior. This result is here generalized. For this the underlying probability space of Kolmogorov is replaced by a $\sigma$-finite measure space and fiducial theory is presented within this frame. Examples are presented that demonstrate that this also gives good alternatives to existing Bayesian sampling methods. It is proved that the results provided here for fiducial models imply that the theory of invariant measures for groups cannot be generalized directly to loops: There exist a smooth one-dimensional loop where an invariant measure does not exist.   Keywords: Conditional sampling, Improper prior, Haar prior, Sufficient statistic, Quasi-group

</details>

<details>

<summary>2020-06-11 14:59:45 - Bayesian MIDAS Penalized Regressions: Estimation, Selection, and Prediction</summary>

- *Matteo Mogliani, Anna Simoni*

- `1903.08025v3` - [abs](http://arxiv.org/abs/1903.08025v3) - [pdf](http://arxiv.org/pdf/1903.08025v3)

> We propose a new approach to mixed-frequency regressions in a high-dimensional environment that resorts to Group Lasso penalization and Bayesian techniques for estimation and inference. In particular, to improve the prediction properties of the model and its sparse recovery ability, we consider a Group Lasso with a spike-and-slab prior. Penalty hyper-parameters governing the model shrinkage are automatically tuned via an adaptive MCMC algorithm. We establish good frequentist asymptotic properties of the posterior of the in-sample and out-of-sample prediction error, we recover the optimal posterior contraction rate, and we show optimality of the posterior predictive density. Simulations show that the proposed models have good selection and forecasting performance in small samples, even when the design matrix presents cross-correlation. When applied to forecasting U.S. GDP, our penalized regressions can outperform many strong competitors. Results suggest that financial variables may have some, although very limited, short-term predictive content.

</details>

<details>

<summary>2020-06-11 15:07:52 - A Bridge between Cross-validation Bayes Factors and Geometric Intrinsic Bayes Factors</summary>

- *Yekun Wang, Luis Pericchi*

- `2006.06495v1` - [abs](http://arxiv.org/abs/2006.06495v1) - [pdf](http://arxiv.org/pdf/2006.06495v1)

> Model Selections in Bayesian Statistics are primarily made with statistics known as Bayes Factors, which are directly related to Posterior Probabilities of models. Bayes Factors require a careful assessment of prior distributions as in the Intrinsic Priors of Berger and Pericchi (1996a) and integration over the parameter space, which may be highly dimensional. Recently researchers have been proposing alternatives to Bayes Factors that require neither integration nor specification of priors. These developments are still in a very early stage and are known as Prior-free Bayes Factors, Cross-Validation Bayes Factors (CVBF), and Bayesian "Stacking." This kind of method and Intrinsic Bayes Factor (IBF) both avoid the specification of prior. However, this Prior-free Bayes factor might need a careful choice of a training sample size. In this article, a way of choosing training sample sizes for the Prior-free Bayes factor based on Geometric Intrinsic Bayes Factors (GIBFs) is proposed and studied. We present essential examples with a different number of parameters and study the statistical behavior both numerically and theoretically to explain the ideas for choosing a feasible training sample size for Prior-free Bayes Factors. We put forward the "Bridge Rule" as an assignment of a training sample size for CVBF's that makes them close to Geometric IBFs. We conclude that even though tractable Geometric IBFs are preferable, CVBF's, using the Bridge Rule, are useful and economical approximations to Bayes Factors.

</details>

<details>

<summary>2020-06-11 15:51:11 - Fast increased fidelity approximate Gibbs samplers for Bayesian Gaussian process regression</summary>

- *Kelly R. Moran, Matthew W. Wheeler*

- `2006.06537v1` - [abs](http://arxiv.org/abs/2006.06537v1) - [pdf](http://arxiv.org/pdf/2006.06537v1)

> The use of Gaussian processes (GPs) is supported by efficient sampling algorithms, a rich methodological literature, and strong theoretical grounding. However, due to their prohibitive computation and storage demands, the use of exact GPs in Bayesian models is limited to problems containing at most several thousand observations. Sampling requires matrix operations that scale at $\mathcal{O}(n^3),$ where $n$ is the number of unique inputs. Storage of individual matrices scales at $\mathcal{O}(n^2),$ and can quickly overwhelm the resources of most modern computers. To overcome these bottlenecks, we develop a sampling algorithm using $\mathcal{H}$ matrix approximation of the matrices comprising the GP posterior covariance. These matrices can approximate the true conditional covariance matrix within machine precision and allow for sampling algorithms that scale at $\mathcal{O}(n \ \mbox{log}^2 n)$ time and storage demands scaling at $\mathcal{O}(n \ \mbox{log} \ n).$ We also describe how these algorithms can be used as building blocks to model higher dimensional surfaces at $\mathcal{O}(d \ n \ \mbox{log}^2 n)$, where $d$ is the dimension of the surface under consideration, using tensor products of one-dimensional GPs. Though various scalable processes have been proposed for approximating Bayesian GP inference when $n$ is large, to our knowledge, none of these methods show that the approximation's Kullback-Leibler divergence to the true posterior can be made arbitrarily small and may be no worse than the approximation provided by finite computer arithmetic. We describe $\mathcal{H}-$matrices, give an efficient Gibbs sampler using these matrices for one-dimensional GPs, offer a proposed extension to higher dimensional surfaces, and investigate the performance of this fast increased fidelity approximate GP, FIFA-GP, using both simulated and real data sets.

</details>

<details>

<summary>2020-06-12 07:44:25 - An efficient application of Bayesian optimization to an industrial MDO framework for aircraft design</summary>

- *Remy Priem, Hugo Gagnon, Ian Chittick, Stephane Dufresne, Youssef Diouane, Nathalie Bartoli*

- `2006.08434v1` - [abs](http://arxiv.org/abs/2006.08434v1) - [pdf](http://arxiv.org/pdf/2006.08434v1)

> The multi-level, multi-disciplinary and multi-fidelity optimization framework developed at Bombardier Aviation has shown great results to explore efficient and competitive aircraft configurations. This optimization framework has been developed within the Isight software, the latter offers a set of ready-to-use optimizers. Unfortunately, the computational effort required by the Isight optimizers can be prohibitive with respect to the requirements of an industrial context. In this paper, a constrained Bayesian optimization optimizer, namely the super efficient global optimization with mixture of experts, is used to reduce the optimization computational effort. The obtained results showed significant improvements compared to two of the popular Isight optimizers. The capabilities of the tested constrained Bayesian optimization solver are demonstrated on Bombardier research aircraft configuration study cases.

</details>

<details>

<summary>2020-06-12 09:39:29 - Approximate Inference for Spectral Mixture Kernel</summary>

- *Yohan Jung, Kyungwoo Song, Jinkyoo Park*

- `2006.07036v1` - [abs](http://arxiv.org/abs/2006.07036v1) - [pdf](http://arxiv.org/pdf/2006.07036v1)

> A spectral mixture (SM) kernel is a flexible kernel used to model any stationary covariance function. Although it is useful in modeling data, the learning of the SM kernel is generally difficult because optimizing a large number of parameters for the SM kernel typically induces an over-fitting, particularly when a gradient-based optimization is used. Also, a longer training time is required. To improve the training, we propose an approximate Bayesian inference for the SM kernel. Specifically, we employ the variational distribution of the spectral points to approximate SM kernel with a random Fourier feature. We optimize the variational parameters by applying a sampling-based variational inference to the derived evidence lower bound (ELBO) estimator constructed from the approximate kernel. To improve the inference, we further propose two additional strategies: (1) a sampling strategy of spectral points to estimate the ELBO estimator reliably and thus its associated gradient, and (2) an approximate natural gradient to accelerate the convergence of the parameters. The proposed inference combined with two strategies accelerates the convergence of the parameters and leads to better optimal parameters.

</details>

<details>

<summary>2020-06-12 10:58:10 - Seemingly Unrelated Regression with Measurement Error: Estimation via Markov chain Monte Carlo and Mean Field Variational Bayes Approximation</summary>

- *Georges Bresson, Anoop Chaturvedi, Mohammad Arshad Rahman, Shalabh*

- `2006.07074v1` - [abs](http://arxiv.org/abs/2006.07074v1) - [pdf](http://arxiv.org/pdf/2006.07074v1)

> Linear regression with measurement error in the covariates is a heavily studied topic, however, the statistics/econometrics literature is almost silent to estimating a multi-equation model with measurement error. This paper considers a seemingly unrelated regression model with measurement error in the covariates and introduces two novel estimation methods: a pure Bayesian algorithm (based on Markov chain Monte Carlo techniques) and its mean field variational Bayes (MFVB) approximation. The MFVB method has the added advantage of being computationally fast and can handle big data. An issue pertinent to measurement error models is parameter identification, and this is resolved by employing a prior distribution on the measurement error variance. The methods are shown to perform well in multiple simulation studies, where we analyze the impact on posterior estimates arising due to different values of reliability ratio or variance of the true unobserved quantity used in the data generating process. The paper further implements the proposed algorithms in an application drawn from the health literature and shows that modeling measurement error in the data can improve model fitting.

</details>

<details>

<summary>2020-06-12 11:59:17 - Structural Regularization</summary>

- *Jiaming Mao, Zhesheng Zheng*

- `2004.12601v4` - [abs](http://arxiv.org/abs/2004.12601v4) - [pdf](http://arxiv.org/pdf/2004.12601v4)

> We propose a novel method for modeling data by using structural models based on economic theory as regularizers for statistical models. We show that even if a structural model is misspecified, as long as it is informative about the data-generating mechanism, our method can outperform both the (misspecified) structural model and un-structural-regularized statistical models. Our method permits a Bayesian interpretation of theory as prior knowledge and can be used both for statistical prediction and causal inference. It contributes to transfer learning by showing how incorporating theory into statistical modeling can significantly improve out-of-domain predictions and offers a way to synthesize reduced-form and structural approaches for causal effect estimation. Simulation experiments demonstrate the potential of our method in various settings, including first-price auctions, dynamic models of entry and exit, and demand estimation with instrumental variables. Our method has potential applications not only in economics, but in other scientific disciplines whose theoretical models offer important insight but are subject to significant misspecification concerns.

</details>

<details>

<summary>2020-06-12 23:46:46 - Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers</summary>

- *Divyat Mahajan, Chenhao Tan, Amit Sharma*

- `1912.03277v3` - [abs](http://arxiv.org/abs/1912.03277v3) - [pdf](http://arxiv.org/pdf/1912.03277v3)

> To construct interpretable explanations that are consistent with the original ML model, counterfactual examples---showing how the model's output changes with small perturbations to the input---have been proposed. This paper extends the work in counterfactual explanations by addressing the challenge of feasibility of such examples. For explanations of ML models in critical domains such as healthcare and finance, counterfactual examples are useful for an end-user only to the extent that perturbation of feature inputs is feasible in the real world. We formulate the problem of feasibility as preserving causal relationships among input features and present a method that uses (partial) structural causal models to generate actionable counterfactuals. When feasibility constraints cannot be easily expressed, we consider an alternative mechanism where people can label generated CF examples on feasibility: whether it is feasible to intervene and realize the candidate CF example from the original input. To learn from this labelled feasibility data, we propose a modified variational auto encoder loss for generating CF examples that optimizes for feasibility as people interact with its output. Our experiments on Bayesian networks and the widely used ''Adult-Income'' dataset show that our proposed methods can generate counterfactual explanations that better satisfy feasibility constraints than existing methods.. Code repository can be accessed here: \textit{https://github.com/divyat09/cf-feasibility}

</details>

<details>

<summary>2020-06-13 02:28:03 - Bayesian inference of infected patients in group testing with prevalence estimation</summary>

- *Ayaka Sakata*

- `2004.13667v2` - [abs](http://arxiv.org/abs/2004.13667v2) - [pdf](http://arxiv.org/pdf/2004.13667v2)

> Group testing is a method of identifying infected patients by performing tests on a pool of specimens collected from patients. For the case in which the test returns a false result with finite probability, we propose Bayesian inference and a corresponding belief propagation (BP) algorithm to identify the infected patients from the results of tests performed on the pool. We show that the true-positive rate is improved by taking into account the credible interval of a point estimate of each patient. Further, the prevalence and the error probability in the test are estimated by combining an expectation-maximization method with the BP algorithm. As another approach, we introduce a hierarchical Bayes model to identify the infected patients and estimate the prevalence. By comparing these methods, we formulate a guide for practical usage.

</details>

<details>

<summary>2020-06-13 03:23:46 - Bayesian Calibration of Computer Models with Informative Failures</summary>

- *Peter W. Marcy, Curtis B. Storlie*

- `2006.07546v1` - [abs](http://arxiv.org/abs/2006.07546v1) - [pdf](http://arxiv.org/pdf/2006.07546v1)

> There are many practical difficulties in the calibration of computer models to experimental data. One such complication is the fact that certain combinations of the calibration inputs can cause the code to output data lacking fundamental properties, or even to produce no output at all. In many cases the researchers want or need to exclude the possibility of these "failures" within their analyses. We propose a Bayesian (meta-)model in which the posterior distribution for the calibration parameters naturally excludes regions of the input space corresponding to failed runs. That is, we define a statistical selection model to rigorously couple the disjoint problems of binary classification and computer model calibration. We demonstrate our methodology using data from a carbon capture experiment in which the numerics of the computational fluid dynamics are prone to instability.

</details>

<details>

<summary>2020-06-13 12:03:28 - Compromise-free Bayesian neural networks</summary>

- *Kamran Javid, Will Handley, Mike Hobson, Anthony Lasenby*

- `2004.12211v3` - [abs](http://arxiv.org/abs/2004.12211v3) - [pdf](http://arxiv.org/pdf/2004.12211v3)

> We conduct a thorough analysis of the relationship between the out-of-sample performance and the Bayesian evidence (marginal likelihood) of Bayesian neural networks (BNNs), as well as looking at the performance of ensembles of BNNs, both using the Boston housing dataset. Using the state-of-the-art in nested sampling, we numerically sample the full (non-Gaussian and multimodal) network posterior and obtain numerical estimates of the Bayesian evidence, considering network models with up to 156 trainable parameters. The networks have between zero and four hidden layers, either $\tanh$ or $ReLU$ activation functions, and with and without hierarchical priors. The ensembles of BNNs are obtained by determining the posterior distribution over networks, from the posterior samples of individual BNNs re-weighted by the associated Bayesian evidence values. There is good correlation between out-of-sample performance and evidence, as well as a remarkable symmetry between the evidence versus model size and out-of-sample performance versus model size planes. Networks with $ReLU$ activation functions have consistently higher evidences than those with $\tanh$ functions, and this is reflected in their out-of-sample performance. Ensembling over architectures acts to further improve performance relative to the individual BNNs.

</details>

<details>

<summary>2020-06-13 14:28:19 - Structure learning for CTBN's via penalized maximum likelihood methods</summary>

- *Maryia Shpak, Błażej Miasojedow, Wojciech Rejchel*

- `2006.07648v1` - [abs](http://arxiv.org/abs/2006.07648v1) - [pdf](http://arxiv.org/pdf/2006.07648v1)

> The continuous-time Bayesian networks (CTBNs) represent a class of stochastic processes, which can be used to model complex phenomena, for instance, they can describe interactions occurring in living processes, in social science models or in medicine. The literature on this topic is usually focused on the case when the dependence structure of a system is known and we are to determine conditional transition intensities (parameters of the network). In the paper, we study the structure learning problem, which is a more challenging task and the existing research on this topic is limited. The approach, which we propose, is based on a penalized likelihood method. We prove that our algorithm, under mild regularity conditions, recognizes the dependence structure of the graph with high probability. We also investigate the properties of the procedure in numerical studies to demonstrate its effectiveness.

</details>

<details>

<summary>2020-06-13 15:35:06 - Bayesian causal inference with some invalid instrumental variables</summary>

- *Gyuhyeong Goh, Jisang Yu*

- `2006.07663v1` - [abs](http://arxiv.org/abs/2006.07663v1) - [pdf](http://arxiv.org/pdf/2006.07663v1)

> In observational studies, instrumental variables estimation is greatly utilized to identify causal effects. One of the key conditions for the instrumental variables estimator to be consistent is the exclusion restriction, which indicates that instruments affect the outcome of interest only via the exposure variable of interest. We propose a likelihood-free Bayesian approach to make consistent inferences about the causal effect when there are some invalid instruments in a way that they violate the exclusion restriction condition. Asymptotic properties of the proposed Bayes estimator, including consistency and normality, are established. A simulation study demonstrates that the proposed Bayesian method produces consistent point estimators and valid credible intervals with correct coverage rates for Gaussian and non-Gaussian data with some invalid instruments. We also demonstrate the proposed method through the real data application.

</details>

<details>

<summary>2020-06-14 11:32:44 - GP3: A Sampling-based Analysis Framework for Gaussian Processes</summary>

- *Armin Lederer, Markus Kessler, Sandra Hirche*

- `2006.07871v1` - [abs](http://arxiv.org/abs/2006.07871v1) - [pdf](http://arxiv.org/pdf/2006.07871v1)

> Although machine learning is increasingly applied in control approaches, only few methods guarantee certifiable safety, which is necessary for real world applications. These approaches typically rely on well-understood learning algorithms, which allow formal theoretical analysis. Gaussian process regression is a prominent example among those methods, which attracts growing attention due to its strong Bayesian foundations. Even though many problems regarding the analysis of Gaussian processes have a similar structure, specific approaches are typically tailored for them individually, without strong focus on computational efficiency. Thereby, the practical applicability and performance of these approaches is limited. In order to overcome this issue, we propose a novel framework called GP3, general purpose computation on graphics processing units for Gaussian processes, which allows to solve many of the existing problems efficiently. By employing interval analysis, local Lipschitz constants are computed in order to extend properties verified on a grid to continuous state spaces. Since the computation is completely parallelizable, the computational benefits of GPU processing are exploited in combination with multi-resolution sampling in order to allow high resolution analysis.

</details>

<details>

<summary>2020-06-14 13:42:14 - High-resolution Bayesian mapping of landslide hazard with unobserved trigger event</summary>

- *Thomas Opitz, Haakon Bakka, Raphaël Huser, Luigi Lombardo*

- `2006.07902v1` - [abs](http://arxiv.org/abs/2006.07902v1) - [pdf](http://arxiv.org/pdf/2006.07902v1)

> Statistical models for landslide hazard enable mapping of risk factors and landslide occurrence intensity by using geomorphological covariates available at high spatial resolution. However, the spatial distribution of the triggering event (e.g., precipitation or earthquakes) is often not directly observed. In this paper, we develop Bayesian spatial hierarchical models for point patterns of landslide occurrences using different types of log-Gaussian Cox processes. Starting from a competitive baseline model that captures the unobserved precipitation trigger through a spatial random effect at slope unit resolution, we explore novel complex model structures that take clusters of events arising at small spatial scales into account, as well as nonlinear or spatially-varying covariate effects. For a 2009 event of around 4000 precipitation-triggered landslides in Sicily, Italy, we show how to fit our proposed models efficiently using the integrated nested Laplace approximation (INLA), and rigorously compare the performance of our models both from a statistical and applied perspective. In this context, we argue that model comparison should not be based on a single criterion, and that different models of various complexity may provide insights into complementary aspects of the same applied problem. In our application, our models are found to have mostly the same spatial predictive performance, implying that key to successful prediction is the inclusion of a slope-unit resolved random effect capturing the precipitation trigger. Interestingly, a parsimonious formulation of space-varying slope effects reflects a physical interpretation of the precipitation trigger: in subareas with weak trigger, the slope steepness is shown to be mostly irrelevant.

</details>

<details>

<summary>2020-06-14 18:49:31 - Near-Optimal Glimpse Sequences for Improved Hard Attention Neural Network Training</summary>

- *William Harvey, Michael Teng, Frank Wood*

- `1906.05462v2` - [abs](http://arxiv.org/abs/1906.05462v2) - [pdf](http://arxiv.org/pdf/1906.05462v2)

> Hard visual attention is a promising approach to reduce the computational burden of modern computer vision methodologies. Hard attention mechanisms are typically non-differentiable. They can be trained with reinforcement learning but the high-variance training this entails hinders more widespread application. We show how hard attention for image classification can be framed as a Bayesian optimal experimental design (BOED) problem. From this perspective, the optimal locations to attend to are those which provide the greatest expected reduction in the entropy of the classification distribution. We introduce methodology from the BOED literature to approximate this optimal behaviour, and use it to generate `near-optimal' sequences of attention locations. We then show how to use such sequences to partially supervise, and therefore speed up, the training of a hard attention mechanism. Although generating these sequences is computationally expensive, they can be reused by any other networks later trained on the same task.

</details>

<details>

<summary>2020-06-14 23:01:10 - A Nonparametric Bayesian Model for Sparse Dynamic Multigraphs</summary>

- *Elahe Ghalebi, Hamidreza Mahyar, Radu Grosu, Graham W. Taylor, Sinead A. Williamson*

- `1910.05098v2` - [abs](http://arxiv.org/abs/1910.05098v2) - [pdf](http://arxiv.org/pdf/1910.05098v2)

> As the availability and importance of temporal interaction data--such as email communication--increases, it becomes increasingly important to understand the underlying structure that underpins these interactions. Often these interactions form a multigraph, where we might have multiple interactions between two entities. Such multigraphs tend to be sparse yet structured, and their distribution often evolves over time. Existing statistical models with interpretable parameters can capture some, but not all, of these properties. We propose a dynamic nonparametric model for interaction multigraphs that combines the sparsity of edge-exchangeable multigraphs with dynamic clustering patterns that tend to reinforce recent behavioral patterns. We show that our method yields improved held-out likelihood over stationary variants, and impressive predictive performance against a range of state-of-the-art dynamic graph models.

</details>

<details>

<summary>2020-06-15 01:08:33 - Repeated Communication with Private Lying Cost</summary>

- *Harry Pei*

- `2006.08069v1` - [abs](http://arxiv.org/abs/2006.08069v1) - [pdf](http://arxiv.org/pdf/2006.08069v1)

> I study repeated communication games between a patient sender and a sequence of receivers. The sender has persistent private information about his psychological cost of lying, and in every period, can privately observe the realization of an i.i.d. state before communication takes place. I characterize every type of sender's highest equilibrium payoff. When the highest lying cost in the support of the receivers' prior belief approaches the sender's benefit from lying, every type's highest equilibrium payoff in the repeated communication game converges to his equilibrium payoff in a one-shot Bayesian persuasion game. I also show that in every sender-optimal equilibrium, no type of sender mixes between telling the truth and lying at every history. When there exist ethical types whose lying costs outweigh their benefits, I provide necessary and sufficient conditions for all non-ethical type senders to attain their optimal commitment payoffs. I identify an outside option effect through which the possibility of being ethical decreases every non-ethical type's payoff.

</details>

<details>

<summary>2020-06-15 03:44:27 - Epidemiologically and Socio-economically Optimal Policies via Bayesian Optimization</summary>

- *Amit Chandak, Debojyoti Dey, Bhaskar Mukhoty, Purushottam Kar*

- `2005.11257v2` - [abs](http://arxiv.org/abs/2005.11257v2) - [pdf](http://arxiv.org/pdf/2005.11257v2)

> Mass public quarantining, colloquially known as a lock-down, is a non-pharmaceutical intervention to check spread of disease. This paper presents ESOP (Epidemiologically and Socio-economically Optimal Policies), a novel application of active machine learning techniques using Bayesian optimization, that interacts with an epidemiological model to arrive at lock-down schedules that optimally balance public health benefits and socio-economic downsides of reduced economic activity during lock-down periods. The utility of ESOP is demonstrated using case studies with VIPER (Virus-Individual-Policy-EnviRonment), a stochastic agent-based simulator that this paper also proposes. However, ESOP is flexible enough to interact with arbitrary epidemiological simulators in a black-box manner, and produce schedules that involve multiple phases of lock-downs.

</details>

<details>

<summary>2020-06-15 03:55:38 - Sparse Gaussian Process Based On Hat Basis Functions</summary>

- *Wenqi Fang, Huiyun Li, Hui Huang, Shaobo Dang, Zhejun Huang, Zheng Wang*

- `2006.08117v1` - [abs](http://arxiv.org/abs/2006.08117v1) - [pdf](http://arxiv.org/pdf/2006.08117v1)

> Gaussian process is one of the most popular non-parametric Bayesian methodologies for modeling the regression problem. It is completely determined by its mean and covariance functions. And its linear property makes it relatively straightforward to solve the prediction problem. Although Gaussian process has been successfully applied in many fields, it is still not enough to deal with physical systems that satisfy inequality constraints. This issue has been addressed by the so-called constrained Gaussian process in recent years. In this paper, we extend the core ideas of constrained Gaussian process. According to the range of training or test data, we redefine the hat basis functions mentioned in the constrained Gaussian process. Based on hat basis functions, we propose a new sparse Gaussian process method to solve the unconstrained regression problem. Similar to the exact Gaussian process and Gaussian process with Fully Independent Training Conditional approximation, our method obtains satisfactory approximate results on open-source datasets or analytical functions. In terms of performance, the proposed method reduces the overall computational complexity from $O(n^{3})$ computation in exact Gaussian process to $O(nm^{2})$ with $m$ hat basis functions and $n$ training data points.

</details>

<details>

<summary>2020-06-15 07:01:05 - Equations defining probability tree models</summary>

- *Eliana Duarte, Christiane Görgen*

- `1802.04511v2` - [abs](http://arxiv.org/abs/1802.04511v2) - [pdf](http://arxiv.org/pdf/1802.04511v2)

> Coloured probability tree models are statistical models coding conditional independence between events depicted in a tree graph. They are more general than the very important class of context-specific Bayesian networks. In this paper, we study the algebraic properties of their ideal of model invariants. The generators of this ideal can be easily read from the tree graph and have a straightforward interpretation in terms of the underlying model: they are differences of odds ratios coming from conditional probabilities. One of the key findings in this analysis is that the tree is a convenient tool for understanding the exact algebraic way in which the sum-to-1 conditions on the parameter space translate into the sum-to-one conditions on the joint probabilities of the statistical model. This enables us to identify necessary and sufficient graphical conditions for a staged tree model to be a toric variety intersected with a probability simplex.

</details>

<details>

<summary>2020-06-15 13:49:52 - Root Cause Analysis in Lithium-Ion Battery Production with FMEA-Based Large-Scale Bayesian Network</summary>

- *Michael Kirchhof, Klaus Haas, Thomas Kornas, Sebastian Thiede, Mario Hirz, Christoph Herrmann*

- `2006.03610v2` - [abs](http://arxiv.org/abs/2006.03610v2) - [pdf](http://arxiv.org/pdf/2006.03610v2)

> The production of lithium-ion battery cells is characterized by a high degree of complexity due to numerous cause-effect relationships between process characteristics. Knowledge about the multi-stage production is spread among several experts, rendering tasks as failure analysis challenging. In this paper, a new method is presented that includes expert knowledge acquisition in production ramp-up by combining Failure Mode and Effects Analysis (FMEA) with a Bayesian Network. Special algorithms are presented that help detect and resolve inconsistencies between the expert-provided parameters which are bound to occur when collecting knowledge from several process experts. We show the effectiveness of this holistic method by building up a large scale, cross-process Bayesian Failure Network in lithium-ion battery production and its application for root cause analysis.

</details>

<details>

<summary>2020-06-15 14:00:15 - Discrete Statistical Models with Rational Maximum Likelihood Estimator</summary>

- *Eliana Duarte, Orlando Marigliano, Bernd Sturmfels*

- `1903.06110v2` - [abs](http://arxiv.org/abs/1903.06110v2) - [pdf](http://arxiv.org/pdf/1903.06110v2)

> A discrete statistical model is a subset of a probability simplex. Its maximum likelihood estimator (MLE) is a retraction from that simplex onto the model. We characterize all models for which this retraction is a rational function. This is a contribution via real algebraic geometry which rests on results due to Huh and Kapranov on Horn uniformization. We present an algorithm for constructing models with rational MLE, and we demonstrate it on a range of instances. Our focus lies on models familiar to statisticians, like Bayesian networks, decomposable graphical models, and staged trees.

</details>

<details>

<summary>2020-06-15 14:41:35 - Learning Credal Sum-Product Networks</summary>

- *Amelie Levray, Vaishak Belle*

- `1901.05847v2` - [abs](http://arxiv.org/abs/1901.05847v2) - [pdf](http://arxiv.org/pdf/1901.05847v2)

> Probabilistic representations, such as Bayesian and Markov networks, are fundamental to much of statistical machine learning. Thus, learning probabilistic representations directly from data is a deep challenge, the main computational bottleneck being inference that is intractable. Tractable learning is a powerful new paradigm that attempts to learn distributions that support efficient probabilistic querying. By leveraging local structure, representations such as sum-product networks (SPNs) can capture high tree-width models with many hidden layers, essentially a deep architecture, while still admitting a range of probabilistic queries to be computable in time polynomial in the network size. While the progress is impressive, numerous data sources are incomplete, and in the presence of missing data, structure learning methods nonetheless revert to single distributions without characterizing the loss in confidence. In recent work, credal sum-product networks, an imprecise extension of sum-product networks, were proposed to capture this robustness angle. In this work, we are interested in how such representations can be learnt and thus study how the computational machinery underlying tractable learning and inference can be generalized for imprecise probabilities.

</details>

<details>

<summary>2020-06-15 15:38:35 - Bayesian Semiparametric Longitudinal Drift-Diffusion Mixed Models for Tone Learning in Adults</summary>

- *Giorgio Paulon, Fernando Llanos, Bharath Chandrasekaran, Abhra Sarkar*

- `1912.02774v5` - [abs](http://arxiv.org/abs/1912.02774v5) - [pdf](http://arxiv.org/pdf/1912.02774v5)

> Understanding how adult humans learn non-native speech categories such as tone information has shed novel insights into the mechanisms underlying experience-dependent brain plasticity. Scientists have traditionally examined these questions using longitudinal learning experiments under a multi-category decision making paradigm. Drift-diffusion processes are popular in such contexts for their ability to mimic underlying neural mechanisms. Motivated by these problems, we develop a novel Bayesian semiparametric inverse Gaussian drift-diffusion mixed model for multi-alternative decision making in longitudinal settings. We design a Markov chain Monte Carlo algorithm for posterior computation. We evaluate the method's empirical performances through synthetic experiments. Applied to our motivating longitudinal tone learning study, the method provides novel insights into how the biologically interpretable model parameters evolve with learning, differ between input-response tone combinations, and differ between well and poorly performing adults.

</details>

<details>

<summary>2020-06-15 19:03:35 - Unbiased Markov chain Monte Carlo for intractable target distributions</summary>

- *Lawrence Middleton, George Deligiannidis, Arnaud Doucet, Pierre E. Jacob*

- `1807.08691v3` - [abs](http://arxiv.org/abs/1807.08691v3) - [pdf](http://arxiv.org/pdf/1807.08691v3)

> Performing numerical integration when the integrand itself cannot be evaluated point-wise is a challenging task that arises in statistical analysis, notably in Bayesian inference for models with intractable likelihood functions. Markov chain Monte Carlo (MCMC) algorithms have been proposed for this setting, such as the pseudo-marginal method for latent variable models and the exchange algorithm for a class of undirected graphical models. As with any MCMC algorithm, the resulting estimators are justified asymptotically in the limit of the number of iterations, but exhibit a bias for any fixed number of iterations due to the Markov chains starting outside of stationarity. This "burn-in" bias is known to complicate the use of parallel processors for MCMC computations. We show how to use coupling techniques to generate unbiased estimators in finite time, building on recent advances for generic MCMC algorithms. We establish the theoretical validity of some of these procedures by extending existing results to cover the case of polynomially ergodic Markov chains. The efficiency of the proposed estimators is compared with that of standard MCMC estimators, with theoretical arguments and numerical experiments including state space models and Ising models.

</details>

<details>

<summary>2020-06-15 19:49:24 - Black-Box Optimization with Local Generative Surrogates</summary>

- *Sergey Shirobokov, Vladislav Belavin, Michael Kagan, Andrey Ustyuzhanin, Atılım Güneş Baydin*

- `2002.04632v2` - [abs](http://arxiv.org/abs/2002.04632v2) - [pdf](http://arxiv.org/pdf/2002.04632v2)

> We propose a novel method for gradient-based optimization of black-box simulators using differentiable local surrogate models. In fields such as physics and engineering, many processes are modeled with non-differentiable simulators with intractable likelihoods. Optimization of these forward models is particularly challenging, especially when the simulator is stochastic. To address such cases, we introduce the use of deep generative models to iteratively approximate the simulator in local neighborhoods of the parameter space. We demonstrate that these local surrogates can be used to approximate the gradient of the simulator, and thus enable gradient-based optimization of simulator parameters. In cases where the dependence of the simulator on the parameter space is constrained to a low dimensional submanifold, we observe that our method attains minima faster than baseline methods, including Bayesian optimization, numerical optimization, and approaches using score function gradient estimators.

</details>

<details>

<summary>2020-06-15 22:22:56 - Deep Autoencoding Topic Model with Scalable Hybrid Bayesian Inference</summary>

- *Hao Zhang, Bo Chen, Yulai Cong, Dandan Guo, Hongwei Liu, Mingyuan Zhou*

- `2006.08804v1` - [abs](http://arxiv.org/abs/2006.08804v1) - [pdf](http://arxiv.org/pdf/2006.08804v1)

> To build a flexible and interpretable model for document analysis, we develop deep autoencoding topic model (DATM) that uses a hierarchy of gamma distributions to construct its multi-stochastic-layer generative network. In order to provide scalable posterior inference for the parameters of the generative network, we develop topic-layer-adaptive stochastic gradient Riemannian MCMC that jointly learns simplex-constrained global parameters across all layers and topics, with topic and layer specific learning rates. Given a posterior sample of the global parameters, in order to efficiently infer the local latent representations of a document under DATM across all stochastic layers, we propose a Weibull upward-downward variational encoder that deterministically propagates information upward via a deep neural network, followed by a Weibull distribution based stochastic downward generative model. To jointly model documents and their associated labels, we further propose supervised DATM that enhances the discriminative power of its latent representations. The efficacy and scalability of our models are demonstrated on both unsupervised and supervised learning tasks on big corpora.

</details>

<details>

<summary>2020-06-15 23:17:53 - A Cramér-Rao Type Bound for Bayesian Risk with Bregman Loss</summary>

- *Alex Dytso, Michael Fauß, H. Vincent Poor*

- `2001.10982v2` - [abs](http://arxiv.org/abs/2001.10982v2) - [pdf](http://arxiv.org/pdf/2001.10982v2)

> A general class of Bayesian lower bounds when the underlying loss function is a Bregman divergence is demonstrated. This class can be considered as an extension of the Weinstein--Weiss family of bounds for the mean squared error and relies on finding a variational characterization of Bayesian risk. The approach allows for the derivation of a version of the Cram\'er--Rao bound that is specific to a given Bregman divergence. The new generalization of the Cram\'er--Rao bound reduces to the classical one when the loss function is taken to be the Euclidean norm. The effectiveness of the new bound is evaluated in the Poisson noise setting and the Binomial noise setting.

</details>

<details>

<summary>2020-06-16 00:41:18 - On the Mathematical Theory of Ensemble (Linear-Gaussian) Kalman-Bucy Filtering</summary>

- *Adrian N. Bishop, Pierre Del Moral*

- `2006.08843v1` - [abs](http://arxiv.org/abs/2006.08843v1) - [pdf](http://arxiv.org/pdf/2006.08843v1)

> The purpose of this review is to present a comprehensive overview of the theory of ensemble Kalman-Bucy filtering for linear-Gaussian signal models. We present a system of equations that describe the flow of individual particles and the flow of the sample covariance and the sample mean in continuous-time ensemble filtering. We consider these equations and their characteristics in a number of popular ensemble Kalman filtering variants. Given these equations, we study their asymptotic convergence to the optimal Bayesian filter. We also study in detail some non-asymptotic time-uniform fluctuation, stability, and contraction results on the sample covariance and sample mean (or sample error track). We focus on testable signal/observation model conditions, and we accommodate fully unstable (latent) signal models. We discuss the relevance and importance of these results in characterising the filter's behaviour, e.g. it's signal tracking performance, and we contrast these results with those in classical studies of stability in Kalman-Bucy filtering. We provide intuition for how these results extend to nonlinear signal models and comment on their consequence on some typical filter behaviours seen in practice, e.g. catastrophic divergence.

</details>

<details>

<summary>2020-06-16 02:48:04 - On Local Optimizers of Acquisition Functions in Bayesian Optimization</summary>

- *Jungtaek Kim, Seungjin Choi*

- `1901.08350v4` - [abs](http://arxiv.org/abs/1901.08350v4) - [pdf](http://arxiv.org/pdf/1901.08350v4)

> Bayesian optimization is a sample-efficient method for finding a global optimum of an expensive-to-evaluate black-box function. A global solution is found by accumulating a pair of query point and its function value, repeating these two procedures: (i) modeling a surrogate function; (ii) maximizing an acquisition function to determine where next to query. Convergence guarantees are only valid when the global optimizer of the acquisition function is found at each round and selected as the next query point. In practice, however, local optimizers of an acquisition function are also used, since searching for the global optimizer is often a non-trivial or time-consuming task. In this paper we consider three popular acquisition functions, PI, EI, and GP-UCB induced by Gaussian process regression. Then we present a performance analysis on the behavior of local optimizers of those acquisition functions, in terms of {\em instantaneous regrets} over global optimizers. We also introduce an analysis, allowing a local optimization method to start from multiple different initial conditions. Numerical experiments confirm the validity of our theoretical analysis.

</details>

<details>

<summary>2020-06-16 04:29:32 - On parametric tests of relativity with false degrees of freedom</summary>

- *Alvin J. K. Chua, Michele Vallisneri*

- `2006.08918v1` - [abs](http://arxiv.org/abs/2006.08918v1) - [pdf](http://arxiv.org/pdf/2006.08918v1)

> General relativity can be tested by comparing the binary-inspiral signals found in LIGO--Virgo data against waveform models that are augmented with artificial degrees of freedom. This approach suffers from a number of logical and practical pitfalls. 1) It is difficult to ascribe meaning to the stringency of the resultant constraints. 2) It is doubtful that the Bayesian model comparison of relativity against these artificial models can offer actual validation for the former. 3) It is unknown to what extent these tests might detect alternative theories of gravity for which there are no computed waveforms; conversely, when waveforms are available, tests that employ them will be superior.

</details>

<details>

<summary>2020-06-16 08:33:51 - A joint bayesian space-time model to integrate spatially misaligned air pollution data in R-INLA</summary>

- *Chiara Forlani, Samir Bhatt, Michela Cameletti, Elias Krainski, Marta Blangiardo*

- `2006.08988v1` - [abs](http://arxiv.org/abs/2006.08988v1) - [pdf](http://arxiv.org/pdf/2006.08988v1)

> In air pollution studies, dispersion models provide estimates of concentration at grid level covering the entire spatial domain, and are then calibrated against measurements from monitoring stations. However, these different data sources are misaligned in space and time. If misalignment is not considered, it can bias the predictions. We aim at demonstrating how the combination of multiple data sources, such as dispersion model outputs, ground observations and covariates, leads to more accurate predictions of air pollution at grid level. We consider nitrogen dioxide (NO2) concentration in Greater London and surroundings for the years 2007-2011, and combine two different dispersion models. Different sets of spatial and temporal effects are included in order to obtain the best predictive capability. Our proposed model is framed in between calibration and Bayesian melding techniques for data fusion red. Unlike other examples, we jointly model the response (concentration level at monitoring stations) and the dispersion model outputs on different scales, accounting for the different sources of uncertainty. Our spatio-temporal model allows us to reconstruct the latent fields of each model component, and to predict daily pollution concentrations. We compare the predictive capability of our proposed model with other established methods to account for misalignment (e.g. bilinear interpolation), showing that in our case study the joint model is a better alternative.

</details>

<details>

<summary>2020-06-16 08:54:42 - How Much Can I Trust You? -- Quantifying Uncertainties in Explaining Neural Networks</summary>

- *Kirill Bykov, Marina M. -C. Höhne, Klaus-Robert Müller, Shinichi Nakajima, Marius Kloft*

- `2006.09000v1` - [abs](http://arxiv.org/abs/2006.09000v1) - [pdf](http://arxiv.org/pdf/2006.09000v1)

> Explainable AI (XAI) aims to provide interpretations for predictions made by learning machines, such as deep neural networks, in order to make the machines more transparent for the user and furthermore trustworthy also for applications in e.g. safety-critical areas. So far, however, no methods for quantifying uncertainties of explanations have been conceived, which is problematic in domains where a high confidence in explanations is a prerequisite. We therefore contribute by proposing a new framework that allows to convert any arbitrary explanation method for neural networks into an explanation method for Bayesian neural networks, with an in-built modeling of uncertainties. Within the Bayesian framework a network's weights follow a distribution that extends standard single explanation scores and heatmaps to distributions thereof, in this manner translating the intrinsic network model uncertainties into a quantification of explanation uncertainties. This allows us for the first time to carve out uncertainties associated with a model explanation and subsequently gauge the appropriate level of explanation confidence for a user (using percentiles). We demonstrate the effectiveness and usefulness of our approach extensively in various experiments, both qualitatively and quantitatively.

</details>

<details>

<summary>2020-06-16 14:09:42 - Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates</summary>

- *Adil Salim, Dmitry Kovalev, Peter Richtárik*

- `1905.11768v2` - [abs](http://arxiv.org/abs/1905.11768v2) - [pdf](http://arxiv.org/pdf/1905.11768v2)

> We propose a new algorithm---Stochastic Proximal Langevin Algorithm (SPLA)---for sampling from a log concave distribution. Our method is a generalization of the Langevin algorithm to potentials expressed as the sum of one stochastic smooth term and multiple stochastic nonsmooth terms. In each iteration, our splitting technique only requires access to a stochastic gradient of the smooth term and a stochastic proximal operator for each of the nonsmooth terms. We establish nonasymptotic sublinear and linear convergence rates under convexity and strong convexity of the smooth term, respectively, expressed in terms of the KL divergence and Wasserstein distance. We illustrate the efficiency of our sampling technique through numerical simulations on a Bayesian learning task.

</details>

<details>

<summary>2020-06-16 21:35:12 - A Bayesian incorporated linear non-Gaussian acyclic model for multiple directed graph estimation to study brain emotion circuit development in adolescence</summary>

- *Aiying Zhang, Gemeng Zhang, Biao Cai, Tony W. Wilson, Julia M. Stephen, Vince D. Calhoun, Yu-Ping Wang*

- `2006.12618v1` - [abs](http://arxiv.org/abs/2006.12618v1) - [pdf](http://arxiv.org/pdf/2006.12618v1)

> Emotion perception is essential to affective and cognitive development which involves distributed brain circuits. The ability of emotion identification begins in infancy and continues to develop throughout childhood and adolescence. Understanding the development of brain's emotion circuitry may help us explain the emotional changes observed during adolescence. Our previous study delineated the trajectory of brain functional connectivity (FC) from late childhood to early adulthood during emotion identification tasks. In this work, we endeavour to deepen our understanding from association to causation. We proposed a Bayesian incorporated linear non-Gaussian acyclic model (BiLiNGAM), which incorporated our previous association model into the prior estimation pipeline. In particular, it can jointly estimate multiple directed acyclic graphs (DAGs) for multiple age groups at different developmental stages. Simulation results indicated more stable and accurate performance over various settings, especially when the sample size was small (high-dimensional cases). We then applied to the analysis of real data from the Philadelphia Neurodevelopmental Cohort (PNC). This included 855 individuals aged 8-22 years who were divided into five different adolescent stages. Our network analysis revealed the development of emotion-related intra- and inter- modular connectivity and pinpointed several emotion-related hubs. We further categorized the hubs into two types: in-hubs and out-hubs, as the center of receiving and distributing information. Several unique developmental hub structures and group-specific patterns were also discovered. Our findings help provide a causal understanding of emotion development in the human brain.

</details>

<details>

<summary>2020-06-17 05:21:00 - PAC-Bayesian Generalization Bounds for MultiLayer Perceptrons</summary>

- *Xinjie Lan, Xin Guo, Kenneth E. Barner*

- `2006.08888v2` - [abs](http://arxiv.org/abs/2006.08888v2) - [pdf](http://arxiv.org/pdf/2006.08888v2)

> We study PAC-Bayesian generalization bounds for Multilayer Perceptrons (MLPs) with the cross entropy loss. Above all, we introduce probabilistic explanations for MLPs in two aspects: (i) MLPs formulate a family of Gibbs distributions, and (ii) minimizing the cross-entropy loss for MLPs is equivalent to Bayesian variational inference, which establish a solid probabilistic foundation for studying PAC-Bayesian bounds on MLPs. Furthermore, based on the Evidence Lower Bound (ELBO), we prove that MLPs with the cross entropy loss inherently guarantee PAC- Bayesian generalization bounds, and minimizing PAC-Bayesian generalization bounds for MLPs is equivalent to maximizing the ELBO. Finally, we validate the proposed PAC-Bayesian generalization bound on benchmark datasets.

</details>

<details>

<summary>2020-06-17 07:19:00 - MUCE: Bayesian Hierarchical Modeling for the Design and Analysis of Phase 1b Multiple Expansion Cohort Trials</summary>

- *Jiaying Lyu, Tianjian Zhou, Shijie Yuan, Wentian Guo, Yuan Ji*

- `2006.07785v2` - [abs](http://arxiv.org/abs/2006.07785v2) - [pdf](http://arxiv.org/pdf/2006.07785v2)

> We propose a multiple cohort expansion (MUCE) approach as a design or analysis method for phase 1b multiple expansion cohort trials, which are novel first-in-human studies conducted following phase 1a dose escalation. The MUCE design is based on a class of Bayesian hierarchical models that adaptively borrow information across arms. Statistical inference is directly based on the posterior probability of each arm being efficacious, facilitating the decision making that decides which arm to select for further testing.

</details>

<details>

<summary>2020-06-17 14:35:14 - GPIRT: A Gaussian Process Model for Item Response Theory</summary>

- *JBrandon Duck-Mayr, Roman Garnett, Jacob M. Montgomery*

- `2006.09900v1` - [abs](http://arxiv.org/abs/2006.09900v1) - [pdf](http://arxiv.org/pdf/2006.09900v1)

> The goal of item response theoretic (IRT) models is to provide estimates of latent traits from binary observed indicators and at the same time to learn the item response functions (IRFs) that map from latent trait to observed response. However, in many cases observed behavior can deviate significantly from the parametric assumptions of traditional IRT models. Nonparametric IRT models overcome these challenges by relaxing assumptions about the form of the IRFs, but standard tools are unable to simultaneously estimate flexible IRFs and recover ability estimates for respondents. We propose a Bayesian nonparametric model that solves this problem by placing Gaussian process priors on the latent functions defining the IRFs. This allows us to simultaneously relax assumptions about the shape of the IRFs while preserving the ability to estimate latent traits. This in turn allows us to easily extend the model to further tasks such as active learning. GPIRT therefore provides a simple and intuitive solution to several longstanding problems in the IRT literature.

</details>

<details>

<summary>2020-06-17 14:51:11 - Bayesian active learning for production, a systematic study and a reusable library</summary>

- *Parmida Atighehchian, Frédéric Branchaud-Charron, Alexandre Lacoste*

- `2006.09916v1` - [abs](http://arxiv.org/abs/2006.09916v1) - [pdf](http://arxiv.org/pdf/2006.09916v1)

> Active learning is able to reduce the amount of labelling effort by using a machine learning model to query the user for specific inputs.   While there are many papers on new active learning techniques, these techniques rarely satisfy the constraints of a real-world project. In this paper, we analyse the main drawbacks of current active learning techniques and we present approaches to alleviate them. We do a systematic study on the effects of the most common issues of real-world datasets on the deep active learning process: model convergence, annotation error, and dataset imbalance. We derive two techniques that can speed up the active learning loop such as partial uncertainty sampling and larger query size. Finally, we present our open-source Bayesian active learning library, BaaL.

</details>

<details>

<summary>2020-06-17 17:25:09 - Bayesian semiparametric analysis of multivariate continuous responses, with variable selection</summary>

- *Georgios Papageorgiou, Benjamin C. Marshall*

- `1905.08393v2` - [abs](http://arxiv.org/abs/1905.08393v2) - [pdf](http://arxiv.org/pdf/1905.08393v2)

> This article presents an approach to Bayesian semiparametric inference for Gaussian multivariate response regression. We are motivated by various small and medium dimensional problems from the physical and social sciences. The statistical challenges revolve around dealing with the unknown mean and variance functions and in particular, the correlation matrix. To tackle these problems, we have developed priors over the smooth functions and a Markov chain Monte Carlo algorithm for inference and model selection. Specifically, Dirichlet process mixtures of Gaussian distributions are used as the basis for a cluster-inducing prior over the elements of the correlation matrix. The smooth, multidimensional means and variances are represented using radial basis function expansions. The complexity of the model, in terms of variable selection and smoothness, is then controlled by spike-slab priors. A simulation study is presented, demonstrating performance as the response dimension increases. Finally, the model is fit to a number of real world datasets. An R package, scripts for replicating synthetic and real data examples, and a detailed description of the MCMC sampler are available in the supplementary materials online.

</details>

<details>

<summary>2020-06-18 04:04:22 - Small Area Estimation of Health Outcomes</summary>

- *Jon Wakefield, Taylor Okonek, Jon Pedersen*

- `2006.10266v1` - [abs](http://arxiv.org/abs/2006.10266v1) - [pdf](http://arxiv.org/pdf/2006.10266v1)

> Small area estimation (SAE) entails estimating characteristics of interest for domains, often geographical areas, in which there may be few or no samples available. SAE has a long history and a wide variety of methods have been suggested, from a bewildering range of philosophical standpoints. We describe design-based and model-based approaches and models that are specified at the area-level and at the unit-level, focusing on health applications and fully Bayesian spatial models. The use of auxiliary information is a key ingredient for successful inference when response data are sparse and we discuss a number of approaches that allow the inclusion of covariate data. SAE for HIV prevalence, using data collected from a Demographic Health Survey in Malawi in 2015-2016, is used to illustrate a number of techniques. The potential use of SAE techniques for outcomes related to COVID-19 is discussed.

</details>

<details>

<summary>2020-06-18 11:08:59 - Bayesian Changepoint Analysis</summary>

- *Tobias Siems*

- `2006.10428v1` - [abs](http://arxiv.org/abs/2006.10428v1) - [pdf](http://arxiv.org/pdf/2006.10428v1)

> In my PhD thesis, we elaborate upon Bayesian changepoint analysis, whereby our focus is on three big topics: approximate sampling via MCMC, exact inference and uncertainty quantification. Besides, modeling matters are discussed in an ongoing fashion. Our findings are underpinned through several changepoint examples with a focus on a well-log drilling data.

</details>

<details>

<summary>2020-06-18 13:57:01 - Infinite attention: NNGP and NTK for deep attention networks</summary>

- *Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, Roman Novak*

- `2006.10540v1` - [abs](http://arxiv.org/abs/2006.10540v1) - [pdf](http://arxiv.org/pdf/2006.10540v1)

> There is a growing amount of literature on the relationship between wide neural networks (NNs) and Gaussian processes (GPs), identifying an equivalence between the two for a variety of NN architectures. This equivalence enables, for instance, accurate approximation of the behaviour of wide Bayesian NNs without MCMC or variational approximations, or characterisation of the distribution of randomly initialised wide NNs optimised by gradient descent without ever running an optimiser. We provide a rigorous extension of these results to NNs involving attention layers, showing that unlike single-head attention, which induces non-Gaussian behaviour, multi-head attention architectures behave as GPs as the number of heads tends to infinity. We further discuss the effects of positional encodings and layer normalisation, and propose modifications of the attention mechanism which lead to improved results for both finite and infinitely wide NNs. We evaluate attention kernels empirically, leading to a moderate improvement upon the previous state-of-the-art on CIFAR-10 for GPs without trainable kernels and advanced data preprocessing. Finally, we introduce new features to the Neural Tangents library (Novak et al., 2020) allowing applications of NNGP/NTK models, with and without attention, to variable-length sequences, with an example on the IMDb reviews dataset.

</details>

<details>

<summary>2020-06-18 14:38:05 - Robust parametric modeling of Alzheimer's disease progression</summary>

- *Mostafa Mehdipour Ghazi, Mads Nielsen, Akshay Pai, Marc Modat, M. Jorge Cardoso, Sébastien Ourselin, Lauge Sørensen*

- `1908.05338v3` - [abs](http://arxiv.org/abs/1908.05338v3) - [pdf](http://arxiv.org/pdf/1908.05338v3)

> Quantitative characterization of disease progression using longitudinal data can provide long-term predictions for the pathological stages of individuals. This work studies the robust modeling of Alzheimer's disease progression using parametric methods. The proposed method linearly maps the individual's age to a disease progression score (DPS) and jointly fits constrained generalized logistic functions to the longitudinal dynamics of biomarkers as functions of the DPS using M-estimation. Robustness of the estimates is quantified using bootstrapping via Monte Carlo resampling, and the estimated inflection points of the fitted functions are used to temporally order the modeled biomarkers in the disease course. Kernel density estimation is applied to the obtained DPSs for clinical status classification using a Bayesian classifier. Different M-estimators and logistic functions, including a novel type proposed in this study, called modified Stannard, are evaluated on the data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) for robust modeling of volumetric MRI and PET biomarkers, CSF measurements, as well as cognitive tests. The results show that the modified Stannard function fitted using the logistic loss achieves the best modeling performance with an average normalized MAE of 0.991 across all biomarkers and bootstraps. Applied to the ADNI test set, this model achieves a multiclass AUC of 0.934 in clinical status classification. The obtained results for the proposed model outperform almost all state-of-the-art results in predicting biomarker values and classifying clinical status. Finally, the experiments show that the proposed model, trained using abundant ADNI data, generalizes well to data from the National Alzheimer's Coordinating Center (NACC) with an average normalized MAE of 1.182 and a multiclass AUC of 0.929.

</details>

<details>

<summary>2020-06-18 15:41:11 - Local Competition and Uncertainty for Adversarial Robustness in Deep Learning</summary>

- *Antonios Alexos, Konstantinos P. Panousis, Sotirios Chatzis*

- `2006.10620v1` - [abs](http://arxiv.org/abs/2006.10620v1) - [pdf](http://arxiv.org/pdf/2006.10620v1)

> This work attempts to address adversarial robustness of deep networks by means of novel learning arguments. Specifically, inspired from results in neuroscience, we propose a local competition principle as a means of adversarially-robust deep learning. We argue that novel local winner-takes-all (LWTA) nonlinearities, combined with posterior sampling schemes, can greatly improve the adversarial robustness of traditional deep networks against difficult adversarial attack schemes. We combine these LWTA arguments with tools from the field of Bayesian non-parametrics, specifically the stick-breaking construction of the Indian Buffet Process, to flexibly account for the inherent uncertainty in data-driven modeling. As we experimentally show, the new proposed model achieves high robustness to adversarial perturbations on MNIST and CIFAR10 datasets. Our model achieves state-of-the-art results in powerful white-box attacks, while at the same time retaining its benign accuracy to a high degree. Equally importantly, our approach achieves this result while requiring far less trainable model parameters than the existing state-of-the-art.

</details>

<details>

<summary>2020-06-18 16:15:01 - Revisiting clustering as matrix factorisation on the Stiefel manifold</summary>

- *Stéphane Chrétien, Benjamin Guedj*

- `1903.04479v2` - [abs](http://arxiv.org/abs/1903.04479v2) - [pdf](http://arxiv.org/pdf/1903.04479v2)

> This paper studies clustering for possibly high dimensional data (e.g. images, time series, gene expression data, and many other settings), and rephrase it as low rank matrix estimation in the PAC-Bayesian framework. Our approach leverages the well known Burer-Monteiro factorisation strategy from large scale optimisation, in the context of low rank estimation. Moreover, our Burer-Monteiro factors are shown to lie on a Stiefel manifold. We propose a new generalized Bayesian estimator for this problem and prove novel prediction bounds for clustering. We also devise a componentwise Langevin sampler on the Stiefel manifold to compute this estimator.

</details>

<details>

<summary>2020-06-19 02:06:15 - Probabilistic Safety for Bayesian Neural Networks</summary>

- *Matthew Wicker, Luca Laurenti, Andrea Patane, Marta Kwiatkowska*

- `2004.10281v2` - [abs](http://arxiv.org/abs/2004.10281v2) - [pdf](http://arxiv.org/pdf/2004.10281v2)

> We study probabilistic safety for Bayesian Neural Networks (BNNs) under adversarial input perturbations. Given a compact set of input points, $T \subseteq \mathbb{R}^m$, we study the probability w.r.t. the BNN posterior that all the points in $T$ are mapped to the same region $S$ in the output space. In particular, this can be used to evaluate the probability that a network sampled from the BNN is vulnerable to adversarial attacks. We rely on relaxation techniques from non-convex optimization to develop a method for computing a lower bound on probabilistic safety for BNNs, deriving explicit procedures for the case of interval and linear function propagation techniques. We apply our methods to BNNs trained on a regression task, airborne collision avoidance, and MNIST, empirically showing that our approach allows one to certify probabilistic safety of BNNs with millions of parameters.

</details>

<details>

<summary>2020-06-19 03:40:36 - Efficient Rollout Strategies for Bayesian Optimization</summary>

- *Eric Hans Lee, David Eriksson, Bolong Cheng, Michael McCourt, David Bindel*

- `2002.10539v3` - [abs](http://arxiv.org/abs/2002.10539v3) - [pdf](http://arxiv.org/pdf/2002.10539v3)

> Bayesian optimization (BO) is a class of sample-efficient global optimization methods, where a probabilistic model conditioned on previous observations is used to determine future evaluations via the optimization of an acquisition function. Most acquisition functions are myopic, meaning that they only consider the impact of the next function evaluation. Non-myopic acquisition functions consider the impact of the next $h$ function evaluations and are typically computed through rollout, in which $h$ steps of BO are simulated. These rollout acquisition functions are defined as $h$-dimensional integrals, and are expensive to compute and optimize. We show that a combination of quasi-Monte Carlo, common random numbers, and control variates significantly reduce the computational burden of rollout. We then formulate a policy-search based approach that removes the need to optimize the rollout acquisition function. Finally, we discuss the qualitative behavior of rollout policies in the setting of multi-modal objectives and model error.

</details>

<details>

<summary>2020-06-19 03:56:27 - Bayesian Optimization with Missing Inputs</summary>

- *Phuc Luong, Dang Nguyen, Sunil Gupta, Santu Rana, Svetha Venkatesh*

- `2006.10948v1` - [abs](http://arxiv.org/abs/2006.10948v1) - [pdf](http://arxiv.org/pdf/2006.10948v1)

> Bayesian optimization (BO) is an efficient method for optimizing expensive black-box functions. In real-world applications, BO often faces a major problem of missing values in inputs. The missing inputs can happen in two cases. First, the historical data for training BO often contain missing values. Second, when performing the function evaluation (e.g. computing alloy strength in a heat treatment process), errors may occur (e.g. a thermostat stops working) leading to an erroneous situation where the function is computed at a random unknown value instead of the suggested value. To deal with this problem, a common approach just simply skips data points where missing values happen. Clearly, this naive method cannot utilize data efficiently and often leads to poor performance. In this paper, we propose a novel BO method to handle missing inputs. We first find a probability distribution of each missing value so that we can impute the missing value by drawing a sample from its distribution. We then develop a new acquisition function based on the well-known Upper Confidence Bound (UCB) acquisition function, which considers the uncertainty of imputed values when suggesting the next point for function evaluation. We conduct comprehensive experiments on both synthetic and real-world applications to show the usefulness of our method.

</details>

<details>

<summary>2020-06-19 09:44:01 - Bayesian analysis of mixture autoregressive models covering the complete parameter space</summary>

- *Davide Ravagli, Georgi N. Boshnakov*

- `2006.11041v1` - [abs](http://arxiv.org/abs/2006.11041v1) - [pdf](http://arxiv.org/pdf/2006.11041v1)

> Mixture autoregressive (MAR) models provide a flexible way to model time series with predictive distributions which depend on the recent history of the process and are able to accommodate asymmetry and multimodality. Bayesian inference for such models offers the additional advantage of incorporating the uncertainty in the estimated models into the predictions. We introduce a new way of sampling from the posterior distribution of the parameters of MAR models which allows for covering the complete parameter space of the models, unlike previous approaches. We also propose a relabelling algorithm to deal a posteriori with label switching. We apply our new method to simulated and real datasets, discuss the accuracy and performance of our new method, as well as its advantages over previous studies. The idea of density forecasting using MCMC output is also introduced.

</details>

<details>

<summary>2020-06-19 14:26:46 - Stratification as a general variance reduction method for Markov chain Monte Carlo</summary>

- *Aaron R. Dinner, Erik Thiede, Brian Van Koten, Jonathan Weare*

- `1705.08445v3` - [abs](http://arxiv.org/abs/1705.08445v3) - [pdf](http://arxiv.org/pdf/1705.08445v3)

> The Eigenvector Method for Umbrella Sampling (EMUS) belongs to a popular class of methods in statistical mechanics which adapt the principle of stratified survey sampling to the computation of free energies. We develop a detailed theoretical analysis of EMUS. Based on this analysis, we show that EMUS is an efficient general method for computing averages over arbitrary target distributions. In particular, we show that EMUS can be dramatically more efficient than direct MCMC when the target distribution is multimodal or when the goal is to compute tail probabilities. To illustrate these theoretical results, we present a tutorial application of the method to a problem from Bayesian statistics.

</details>

<details>

<summary>2020-06-19 17:05:45 - Distortion estimates for approximate Bayesian inference</summary>

- *Hanwen Xing, Geoff K. Nicholls, Jeong Eun Lee*

- `2006.11228v1` - [abs](http://arxiv.org/abs/2006.11228v1) - [pdf](http://arxiv.org/pdf/2006.11228v1)

> Current literature on posterior approximation for Bayesian inference offers many alternative methods. Does our chosen approximation scheme work well on the observed data? The best existing generic diagnostic tools treating this kind of question by looking at performance averaged over data space, or otherwise lack diagnostic detail. However, if the approximation is bad for most data, but good at the observed data, then we may discard a useful approximation. We give graphical diagnostics for posterior approximation at the observed data. We estimate a "distortion map" that acts on univariate marginals of the approximate posterior to move them closer to the exact posterior, without recourse to the exact posterior.

</details>

<details>

<summary>2020-06-20 00:22:49 - Mitigating Bias in Online Microfinance Platforms: A Case Study on Kiva.org</summary>

- *Soumajyoti Sarkar, Hamidreza Alvari*

- `2006.12995v1` - [abs](http://arxiv.org/abs/2006.12995v1) - [pdf](http://arxiv.org/pdf/2006.12995v1)

> Over the last couple of decades in the lending industry, financial disintermediation has occurred on a global scale. Traditionally, even for small supply of funds, banks would act as the conduit between the funds and the borrowers. It has now been possible to overcome some of the obstacles associated with such supply of funds with the advent of online platforms like Kiva, Prosper, LendingClub. Kiva for example, works with Micro Finance Institutions (MFIs) in developing countries to build Internet profiles of borrowers with a brief biography, loan requested, loan term, and purpose. Kiva, in particular, allows lenders to fund projects in different sectors through group or individual funding. Traditional research studies have investigated various factors behind lender preferences purely from the perspective of loan attributes and only until recently have some cross-country cultural preferences been investigated. In this paper, we investigate lender perceptions of economic factors of the borrower countries in relation to their preferences towards loans associated with different sectors. We find that the influence from economic factors and loan attributes can have substantially different roles to play for different sectors in achieving faster funding. We formally investigate and quantify the hidden biases prevalent in different loan sectors using recent tools from causal inference and regression models that rely on Bayesian variable selection methods. We then extend these models to incorporate fairness constraints based on our empirical analysis and find that such models can still achieve near comparable results with respect to baseline regression models.

</details>

<details>

<summary>2020-06-20 14:12:55 - Calibration of Model Uncertainty for Dropout Variational Inference</summary>

- *Max-Heinrich Laves, Sontje Ihler, Karl-Philipp Kortmann, Tobias Ortmaier*

- `2006.11584v1` - [abs](http://arxiv.org/abs/2006.11584v1) - [pdf](http://arxiv.org/pdf/2006.11584v1)

> The model uncertainty obtained by variational Bayesian inference with Monte Carlo dropout is prone to miscalibration. In this paper, different logit scaling methods are extended to dropout variational inference to recalibrate model uncertainty. Expected uncertainty calibration error (UCE) is presented as a metric to measure miscalibration. The effectiveness of recalibration is evaluated on CIFAR-10/100 and SVHN for recent CNN architectures. Experimental results show that logit scaling considerably reduce miscalibration by means of UCE. Well-calibrated uncertainty enables reliable rejection of uncertain predictions and robust detection of out-of-distribution data.

</details>

<details>

<summary>2020-06-20 14:53:03 - Nonasymptotic Laplace approximation under model misspecification</summary>

- *Anirban Bhattacharya, Debdeep Pati*

- `2005.07844v2` - [abs](http://arxiv.org/abs/2005.07844v2) - [pdf](http://arxiv.org/pdf/2005.07844v2)

> We present non-asymptotic two-sided bounds to the log-marginal likelihood in Bayesian inference. The classical Laplace approximation is recovered as the leading term. Our derivation permits model misspecification and allows the parameter dimension to grow with the sample size. We do not make any assumptions about the asymptotic shape of the posterior, and instead require certain regularity conditions on the likelihood ratio and that the posterior to be sufficiently concentrated.

</details>

<details>

<summary>2020-06-20 17:07:15 - Bayesian Mean-parameterized Nonnegative Binary Matrix Factorization</summary>

- *Alberto Lumbreras, Louis Filstroff, Cédric Févotte*

- `1812.06866v3` - [abs](http://arxiv.org/abs/1812.06866v3) - [pdf](http://arxiv.org/pdf/1812.06866v3)

> Binary data matrices can represent many types of data such as social networks, votes, or gene expression. In some cases, the analysis of binary matrices can be tackled with nonnegative matrix factorization (NMF), where the observed data matrix is approximated by the product of two smaller nonnegative matrices. In this context, probabilistic NMF assumes a generative model where the data is usually Bernoulli-distributed. Often, a link function is used to map the factorization to the $[0,1]$ range, ensuring a valid Bernoulli mean parameter. However, link functions have the potential disadvantage to lead to uninterpretable models. Mean-parameterized NMF, on the contrary, overcomes this problem. We propose a unified framework for Bayesian mean-parameterized nonnegative binary matrix factorization models (NBMF). We analyze three models which correspond to three possible constraints that respect the mean-parametrization without the need for link functions. Furthermore, we derive a novel collapsed Gibbs sampler and a collapsed variational algorithm to infer the posterior distribution of the factors. Next, we extend the proposed models to a nonparametric setting where the number of used latent dimensions is automatically driven by the observed data. We analyze the performance of our NBMF methods in multiple datasets for different tasks such as dictionary learning and prediction of missing data. Experiments show that our methods provide similar or superior results than the state of the art, while automatically detecting the number of relevant components.

</details>

<details>

<summary>2020-06-21 02:58:21 - Physics-Informed Probabilistic Learning of Linear Embeddings of Non-linear Dynamics With Guaranteed Stability</summary>

- *Shaowu Pan, Karthik Duraisamy*

- `1906.03663v5` - [abs](http://arxiv.org/abs/1906.03663v5) - [pdf](http://arxiv.org/pdf/1906.03663v5)

> The Koopman operator has emerged as a powerful tool for the analysis of nonlinear dynamical systems as it provides coordinate transformations to globally linearize the dynamics. While recent deep learning approaches have been useful in extracting the Koopman operator from a data-driven perspective, several challenges remain. In this work, we formalize the problem of learning the continuous-time Koopman operator with deep neural networks in a measure-theoretic framework. Our approach induces two types of models: differential and recurrent form, the choice of which depends on the availability of the governing equations and data. We then enforce a structural parameterization that renders the realization of the Koopman operator provably stable. A new autoencoder architecture is constructed, such that only the residual of the dynamic mode decomposition is learned. Finally, we employ mean-field variational inference (MFVI) on the aforementioned framework in a hierarchical Bayesian setting to quantify uncertainties in the characterization and prediction of the dynamics of observables. The framework is evaluated on a simple polynomial system, the Duffing oscillator, and an unstable cylinder wake flow with noisy measurements.

</details>

<details>

<summary>2020-06-21 11:21:55 - Additive Tree-Structured Covariance Function for Conditional Parameter Spaces in Bayesian Optimization</summary>

- *Xingchen Ma, Matthew B. Blaschko*

- `2006.11771v1` - [abs](http://arxiv.org/abs/2006.11771v1) - [pdf](http://arxiv.org/pdf/2006.11771v1)

> Bayesian optimization (BO) is a sample-efficient global optimization algorithm for black-box functions which are expensive to evaluate. Existing literature on model based optimization in conditional parameter spaces are usually built on trees. In this work, we generalize the additive assumption to tree-structured functions and propose an additive tree-structured covariance function, showing improved sample-efficiency, wider applicability and greater flexibility. Furthermore, by incorporating the structure information of parameter spaces and the additive assumption in the BO loop, we develop a parallel algorithm to optimize the acquisition function and this optimization can be performed in a low dimensional space. We demonstrate our method on an optimization benchmark function, as well as on a neural network model compression problem, and experimental results show our approach significantly outperforms the current state of the art for conditional parameter optimization including SMAC, TPE and Jenatton et al. (2017).

</details>

<details>

<summary>2020-06-21 18:17:57 - Electoral David vs Goliath: How does the Spatial Concentration of Electors affect District-based Elections?</summary>

- *Adway Mitra*

- `2006.11865v1` - [abs](http://arxiv.org/abs/2006.11865v1) - [pdf](http://arxiv.org/pdf/2006.11865v1)

> Many democratic countries use district-based elections where there is a "seat" for each district in the governing body. In each district, the party whose candidate gets the maximum number of votes wins the corresponding seat. The result of the election is decided based on the number of seats won by the different parties. The electors (voters) can cast their votes only in the district of their residence. Thus, locations of the electors and boundaries of the districts may severely affect the election result even if the proportion of popular support (number of electors) of different parties remains unchanged. This has led to significant amount of research on whether the districts may be redrawn or electors may be moved to maximize seats for a particular party. In this paper, we frame the spatial distribution of electors in a probabilistic setting, and explore different models to capture the intra-district polarization of electors in favour of a party, or the spatial concentration of supporters of different parties. Our models are inspired by elections in India, where supporters of different parties tend to be concentrated in certain districts. We show with extensive simulations that our model can capture different statistical properties of real elections held in India. We frame parameter estimation problems to fit our models to the observed election results. Since analytical calculation of the likelihood functions are infeasible for our complex models, we use Likelihood-free Inference methods under the Approximate Bayesian Computation framework. Since this approach is highly time-consuming, we explore how supervised regression using Logistic Regression or Deep Neural Networks can be used to speed it up. We also explore how the election results can change by varying the spatial distributions of the voters, even when the proportions of popular support of the parties remain constant.

</details>

<details>

<summary>2020-06-21 23:35:34 - Optimal design of large-scale Bayesian linear inverse problems under reducible model uncertainty: good to know what you don't know</summary>

- *Alen Alexanderian, Noemi Petra, Georg Stadler, Isaac Sunseri*

- `2006.11939v1` - [abs](http://arxiv.org/abs/2006.11939v1) - [pdf](http://arxiv.org/pdf/2006.11939v1)

> We consider optimal design of infinite-dimensional Bayesian linear inverse problems governed by partial differential equations that contain secondary reducible model uncertainties, in addition to the uncertainty in the inversion parameters. By reducible uncertainties we refer to parametric uncertainties that can be reduced through parameter inference. We seek experimental designs that minimize the posterior uncertainty in the primary parameters, while accounting for the uncertainty in secondary parameters. We accomplish this by deriving a marginalized A-optimality criterion and developing an efficient computational approach for its optimization. We illustrate our approach for estimating an uncertain time-dependent source in a contaminant transport model with an uncertain initial state as secondary uncertainty. Our results indicate that accounting for additional model uncertainty in the experimental design process is crucial.

</details>

<details>

<summary>2020-06-22 00:33:52 - Disentangled Sticky Hierarchical Dirichlet Process Hidden Markov Model</summary>

- *Ding Zhou, Yuanjun Gao, Liam Paninski*

- `2004.03019v2` - [abs](http://arxiv.org/abs/2004.03019v2) - [pdf](http://arxiv.org/pdf/2004.03019v2)

> The Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) has been used widely as a natural Bayesian nonparametric extension of the classical Hidden Markov Model for learning from sequential and time-series data. A sticky extension of the HDP-HMM has been proposed to strengthen the self-persistence probability in the HDP-HMM. However, the sticky HDP-HMM entangles the strength of the self-persistence prior and transition prior together, limiting its expressiveness. Here, we propose a more general model: the disentangled sticky HDP-HMM (DS-HDP-HMM). We develop novel Gibbs sampling algorithms for efficient inference in this model. We show that the disentangled sticky HDP-HMM outperforms the sticky HDP-HMM and HDP-HMM on both synthetic and real data, and apply the new approach to analyze neural data and segment behavioral video data.

</details>

<details>

<summary>2020-06-22 03:17:10 - Bayesian Quadrature Optimization for Probability Threshold Robustness Measure</summary>

- *Shogo Iwazaki, Yu Inatsu, Ichiro Takeuchi*

- `2006.11986v1` - [abs](http://arxiv.org/abs/2006.11986v1) - [pdf](http://arxiv.org/pdf/2006.11986v1)

> In many product development problems, the performance of the product is governed by two types of parameters called design parameter and environmental parameter. While the former is fully controllable, the latter varies depending on the environment in which the product is used. The challenge of such a problem is to find the design parameter that maximizes the probability that the performance of the product will meet the desired requisite level given the variation of the environmental parameter. In this paper, we formulate this practical problem as active learning (AL) problems and propose efficient algorithms with theoretically guaranteed performance. Our basic idea is to use Gaussian Process (GP) model as the surrogate model of the product development process, and then to formulate our AL problems as Bayesian Quadrature Optimization problems for probabilistic threshold robustness (PTR) measure. We derive credible intervals for the PTR measure and propose AL algorithms for the optimization and level set estimation of the PTR measure. We clarify the theoretical properties of the proposed algorithms and demonstrate their efficiency in both synthetic and real-world product development problems.

</details>

<details>

<summary>2020-06-22 06:30:15 - Bayesian Neural Networks: An Introduction and Survey</summary>

- *Ethan Goan, Clinton Fookes*

- `2006.12024v1` - [abs](http://arxiv.org/abs/2006.12024v1) - [pdf](http://arxiv.org/pdf/2006.12024v1)

> Neural Networks (NNs) have provided state-of-the-art results for many challenging machine learning tasks such as detection, regression and classification across the domains of computer vision, speech recognition and natural language processing. Despite their success, they are often implemented in a frequentist scheme, meaning they are unable to reason about uncertainty in their predictions. This article introduces Bayesian Neural Networks (BNNs) and the seminal research regarding their implementation. Different approximate inference methods are compared, and used to highlight where future research can improve on current methods.

</details>

<details>

<summary>2020-06-22 09:31:55 - MUMBO: MUlti-task Max-value Bayesian Optimization</summary>

- *Henry B. Moss, David S. Leslie, Paul Rayson*

- `2006.12093v1` - [abs](http://arxiv.org/abs/2006.12093v1) - [pdf](http://arxiv.org/pdf/2006.12093v1)

> We propose MUMBO, the first high-performing yet computationally efficient acquisition function for multi-task Bayesian optimization. Here, the challenge is to perform efficient optimization by evaluating low-cost functions somehow related to our true target function. This is a broad class of problems including the popular task of multi-fidelity optimization. However, while information-theoretic acquisition functions are known to provide state-of-the-art Bayesian optimization, existing implementations for multi-task scenarios have prohibitive computational requirements. Previous acquisition functions have therefore been suitable only for problems with both low-dimensional parameter spaces and function query costs sufficiently large to overshadow very significant optimization overheads. In this work, we derive a novel multi-task version of entropy search, delivering robust performance with low computational overheads across classic optimization challenges and multi-task hyper-parameter tuning. MUMBO is scalable and efficient, allowing multi-task Bayesian optimization to be deployed in problems with rich parameter and fidelity spaces.

</details>

<details>

<summary>2020-06-22 13:25:32 - Differentiable PAC-Bayes Objectives with Partially Aggregated Neural Networks</summary>

- *Felix Biggs, Benjamin Guedj*

- `2006.12228v1` - [abs](http://arxiv.org/abs/2006.12228v1) - [pdf](http://arxiv.org/pdf/2006.12228v1)

> We make three related contributions motivated by the challenge of training stochastic neural networks, particularly in a PAC-Bayesian setting: (1) we show how averaging over an ensemble of stochastic neural networks enables a new class of \emph{partially-aggregated} estimators; (2) we show that these lead to provably lower-variance gradient estimates for non-differentiable signed-output networks; (3) we reformulate a PAC-Bayesian bound for these networks to derive a directly optimisable, differentiable objective and a generalisation guarantee, without using a surrogate loss or loosening the bound. This bound is twice as tight as that of Letarte et al. (2019) on a similar network type. We show empirically that these innovations make training easier and lead to competitive guarantees.

</details>

<details>

<summary>2020-06-22 15:59:32 - Don't Wait, Just Weight: Improving Unsupervised Representations by Learning Goal-Driven Instance Weights</summary>

- *Linus Ericsson, Henry Gouk, Timothy M. Hospedales*

- `2006.12360v1` - [abs](http://arxiv.org/abs/2006.12360v1) - [pdf](http://arxiv.org/pdf/2006.12360v1)

> In the absence of large labelled datasets, self-supervised learning techniques can boost performance by learning useful representations from unlabelled data, which is often more readily available. However, there is often a domain shift between the unlabelled collection and the downstream target problem data. We show that by learning Bayesian instance weights for the unlabelled data, we can improve the downstream classification accuracy by prioritising the most useful instances. Additionally, we show that the training time can be reduced by discarding unnecessary datapoints. Our method, BetaDataWeighter is evaluated using the popular self-supervised rotation prediction task on STL-10 and Visual Decathlon. We compare to related instance weighting schemes, both hand-designed heuristics and meta-learning, as well as conventional self-supervised learning. BetaDataWeighter achieves both the highest average accuracy and rank across datasets, and on STL-10 it prunes up to 78% of unlabelled images without significant loss in accuracy, corresponding to over 50% reduction in training time.

</details>

<details>

<summary>2020-06-22 16:19:10 - Recursive Estimation for Sparse Gaussian Process Regression</summary>

- *Manuel Schürch, Dario Azzimonti, Alessio Benavoli, Marco Zaffalon*

- `1905.11711v2` - [abs](http://arxiv.org/abs/1905.11711v2) - [pdf](http://arxiv.org/pdf/1905.11711v2)

> Gaussian Processes (GPs) are powerful kernelized methods for non-parameteric regression used in many applications. However, their use is limited to a few thousand of training samples due to their cubic time complexity. In order to scale GPs to larger datasets, several sparse approximations based on so-called inducing points have been proposed in the literature. In this work we investigate the connection between a general class of sparse inducing point GP regression methods and Bayesian recursive estimation which enables Kalman Filter like updating for online learning. The majority of previous work has focused on the batch setting, in particular for learning the model parameters and the position of the inducing points, here instead we focus on training with mini-batches. By exploiting the Kalman filter formulation, we propose a novel approach that estimates such parameters by recursively propagating the analytical gradients of the posterior over mini-batches of the data. Compared to state of the art methods, our method keeps analytic updates for the mean and covariance of the posterior, thus reducing drastically the size of the optimization problem. We show that our method achieves faster convergence and superior performance compared to state of the art sequential Gaussian Process regression on synthetic GP as well as real-world data with up to a million of data samples.

</details>

<details>

<summary>2020-06-23 03:23:47 - On Compression Principle and Bayesian Optimization for Neural Networks</summary>

- *Michael Tetelman*

- `2006.12714v1` - [abs](http://arxiv.org/abs/2006.12714v1) - [pdf](http://arxiv.org/pdf/2006.12714v1)

> Finding methods for making generalizable predictions is a fundamental problem of machine learning. By looking into similarities between the prediction problem for unknown data and the lossless compression we have found an approach that gives a solution. In this paper we propose a compression principle that states that an optimal predictive model is the one that minimizes a total compressed message length of all data and model definition while guarantees decodability. Following the compression principle we use Bayesian approach to build probabilistic models of data and network definitions. A method to approximate Bayesian integrals using a sequence of variational approximations is implemented as an optimizer for hyper-parameters: Bayesian Stochastic Gradient Descent (BSGD). Training with BSGD is completely defined by setting only three parameters: number of epochs, the size of the dataset and the size of the minibatch, which define a learning rate and a number of iterations. We show that dropout can be used for a continuous dimensionality reduction that allows to find optimal network dimensions as required by the compression principle.

</details>

<details>

<summary>2020-06-23 04:56:51 - Neural Clustering Processes</summary>

- *Ari Pakman, Yueqi Wang, Catalin Mitelut, JinHyung Lee, Liam Paninski*

- `1901.00409v4` - [abs](http://arxiv.org/abs/1901.00409v4) - [pdf](http://arxiv.org/pdf/1901.00409v4)

> Probabilistic clustering models (or equivalently, mixture models) are basic building blocks in countless statistical models and involve latent random variables over discrete spaces. For these models, posterior inference methods can be inaccurate and/or very slow. In this work we introduce deep network architectures trained with labeled samples from any generative model of clustered datasets. At test time, the networks generate approximate posterior samples of cluster labels for any new dataset of arbitrary size. We develop two complementary approaches to this task, requiring either O(N) or O(K) network forward passes per dataset, where N is the dataset size and K the number of clusters. Unlike previous approaches, our methods sample the labels of all the data points from a well-defined posterior, and can learn nonparametric Bayesian posteriors since they do not limit the number of mixture components. As a scientific application, we present a novel approach to neural spike sorting for high-density multielectrode arrays.

</details>

<details>

<summary>2020-06-23 15:51:53 - Bayesian hierarchical weighting adjustment and survey inference</summary>

- *Yajuan Si, Rob Trangucci, Jonah Sol Gabry, Andrew Gelman*

- `1707.08220v2` - [abs](http://arxiv.org/abs/1707.08220v2) - [pdf](http://arxiv.org/pdf/1707.08220v2)

> We combine Bayesian prediction and weighted inference as a unified approach to survey inference. The general principles of Bayesian analysis imply that models for survey outcomes should be conditional on all variables that affect the probability of inclusion. We incorporate the weighting variables under the framework of multilevel regression and poststratification, as a byproduct generating model-based weights after smoothing. We investigate deep interactions and introduce structured prior distributions for smoothing and stability of estimates. The computation is done via Stan and implemented in the open source R package "rstanarm" ready for public use. Simulation studies illustrate that model-based prediction and weighting inference outperform classical weighting. We apply the proposal to the New York Longitudinal Study of Wellbeing. The new approach generates robust weights and increases efficiency for finite population inference, especially for subsets of the population.

</details>

<details>

<summary>2020-06-23 16:57:46 - The principles of adaptation in organisms and machines II: Thermodynamics of the Bayesian brain</summary>

- *Hideaki Shimazaki*

- `2006.13158v1` - [abs](http://arxiv.org/abs/2006.13158v1) - [pdf](http://arxiv.org/pdf/2006.13158v1)

> This article reviews how organisms learn and recognize the world through the dynamics of neural networks from the perspective of Bayesian inference, and introduces a view on how such dynamics is described by the laws for the entropy of neural activity, a paradigm that we call thermodynamics of the Bayesian brain. The Bayesian brain hypothesis sees the stimulus-evoked activity of neurons as an act of constructing the Bayesian posterior distribution based on the generative model of the external world that an organism possesses. A closer look at the stimulus-evoked activity at early sensory cortices reveals that feedforward connections initially mediate the stimulus-response, which is later modulated by input from recurrent connections. Importantly, not the initial response, but the delayed modulation expresses animals' cognitive states such as awareness and attention regarding the stimulus. Using a simple generative model made of a spiking neural population, we reproduce the stimulus-evoked dynamics with the delayed feedback modulation as the process of the Bayesian inference that integrates the stimulus evidence and a prior knowledge with time-delay. We then introduce a thermodynamic view on this process based on the laws for the entropy of neural activity. This view elucidates that the process of the Bayesian inference works as the recently-proposed information-theoretic engine (neural engine, an analogue of a heat engine in thermodynamics), which allows us to quantify the perceptual capacity expressed in the delayed modulation in terms of entropy.

</details>

<details>

<summary>2020-06-23 17:30:48 - On the convergence complexity of Gibbs samplers for a family of simple Bayesian random effects models</summary>

- *Bryant Davis, James P. Hobert*

- `2004.14330v2` - [abs](http://arxiv.org/abs/2004.14330v2) - [pdf](http://arxiv.org/pdf/2004.14330v2)

> The emergence of big data has led to so-called convergence complexity analysis, which is the study of how Markov chain Monte Carlo (MCMC) algorithms behave as the sample size, $n$, and/or the number of parameters, $p$, in the underlying data set increase. This type of analysis is often quite challenging, in part because existing results for fixed $n$ and $p$ are simply not sharp enough to yield good asymptotic results. One of the first convergence complexity results for an MCMC algorithm on a continuous state space is due to Yang and Rosenthal (2019), who established a mixing time result for a Gibbs sampler (for a simple Bayesian random effects model) that was introduced and studied by Rosenthal (1996). The asymptotic behavior of the spectral gap of this Gibbs sampler is, however, still unknown. We use a recently developed simulation technique (Qin et. al., 2019) to provide substantial numerical evidence that the gap is bounded away from 0 as $n \rightarrow \infty$. We also establish a pair of rigorous convergence complexity results for two different Gibbs samplers associated with a generalization of the random effects model considered by Rosenthal (1996). Our results show that, under strong regularity conditions, the spectral gaps of these Gibbs samplers converge to 1 as the sample size increases.

</details>

<details>

<summary>2020-06-23 21:10:55 - Non-Parametric Graph Learning for Bayesian Graph Neural Networks</summary>

- *Soumyasundar Pal, Saber Malekmohammadi, Florence Regol, Yingxue Zhang, Yishi Xu, Mark Coates*

- `2006.13335v1` - [abs](http://arxiv.org/abs/2006.13335v1) - [pdf](http://arxiv.org/pdf/2006.13335v1)

> Graphs are ubiquitous in modelling relational structures. Recent endeavours in machine learning for graph-structured data have led to many architectures and learning algorithms. However, the graph used by these algorithms is often constructed based on inaccurate modelling assumptions and/or noisy data. As a result, it fails to represent the true relationships between nodes. A Bayesian framework which targets posterior inference of the graph by considering it as a random quantity can be beneficial. In this paper, we propose a novel non-parametric graph model for constructing the posterior distribution of graph adjacency matrices. The proposed model is flexible in the sense that it can effectively take into account the output of graph-based learning algorithms that target specific tasks. In addition, model inference scales well to large graphs. We demonstrate the advantages of this model in three different problem settings: node classification, link prediction and recommendation.

</details>

<details>

<summary>2020-06-24 08:53:07 - Why bigger is not always better: on finite and infinite neural networks</summary>

- *Laurence Aitchison*

- `1910.08013v3` - [abs](http://arxiv.org/abs/1910.08013v3) - [pdf](http://arxiv.org/pdf/1910.08013v3)

> Recent work has argued that neural networks can be understood theoretically by taking the number of channels to infinity, at which point the outputs become Gaussian process (GP) distributed. However, we note that infinite Bayesian neural networks lack a key facet of the behaviour of real neural networks: the fixed kernel, determined only by network hyperparameters, implies that they cannot do any form of representation learning. The lack of representation or equivalently kernel learning leads to less flexibility and hence worse performance, giving a potential explanation for the inferior performance of infinite networks observed in the literature (e.g. Novak et al. 2019). We give analytic results characterising the prior over representations and representation learning in finite deep linear networks. We show empirically that the representations in SOTA architectures such as ResNets trained with SGD are much closer to those suggested by our deep linear results than by the corresponding infinite network. This motivates the introduction of a new class of network: infinite networks with bottlenecks, which inherit the theoretical tractability of infinite networks while at the same time allowing representation learning.

</details>

<details>

<summary>2020-06-24 09:52:44 - Predictive Inference Based on Markov Chain Monte Carlo Output</summary>

- *Fabian Krüger, Sebastian Lerch, Thordis L. Thorarinsdottir, Tilmann Gneiting*

- `1608.06802v6` - [abs](http://arxiv.org/abs/1608.06802v6) - [pdf](http://arxiv.org/pdf/1608.06802v6)

> In Bayesian inference, predictive distributions are typically in the form of samples generated via Markov chain Monte Carlo (MCMC) or related algorithms. In this paper, we conduct a systematic analysis of how to make and evaluate probabilistic forecasts from such simulation output. Based on proper scoring rules, we develop a notion of consistency that allows to assess the adequacy of methods for estimating the stationary distribution underlying the simulation output. We then provide asymptotic results that account for the salient features of Bayesian posterior simulators, and derive conditions under which choices from the literature satisfy our notion of consistency. Importantly, these conditions depend on the scoring rule being used, such that the choices of approximation method and scoring rule are intertwined. While the logarithmic rule requires fairly stringent conditions, the continuous ranked probability score (CRPS) yields consistent approximations under minimal assumptions. These results are illustrated in a simulation study and an economic data example. Overall, mixture-of-parameters approximations which exploit the parametric structure of Bayesian models perform particularly well. Under the CRPS, the empirical distribution function is a simple and appealing alternative option.

</details>

<details>

<summary>2020-06-24 10:25:27 - Simple and Scalable Parallelized Bayesian Optimization</summary>

- *Masahiro Nomura*

- `2006.13600v1` - [abs](http://arxiv.org/abs/2006.13600v1) - [pdf](http://arxiv.org/pdf/2006.13600v1)

> In recent years, leveraging parallel and distributed computational resources has become essential to solve problems of high computational cost. Bayesian optimization (BO) has shown attractive results in those expensive-to-evaluate problems such as hyperparameter optimization of machine learning algorithms. While many parallel BO methods have been developed to search efficiently utilizing these computational resources, these methods assumed synchronous settings or were not scalable. In this paper, we propose a simple and scalable BO method for asynchronous parallel settings. Experiments are carried out with a benchmark function and hyperparameter optimization of multi-layer perceptrons, which demonstrate the promising performance of the proposed method.

</details>

<details>

<summary>2020-06-24 10:57:33 - Robustness of Bayesian Neural Networks to Gradient-Based Attacks</summary>

- *Ginevra Carbone, Matthew Wicker, Luca Laurenti, Andrea Patane, Luca Bortolussi, Guido Sanguinetti*

- `2002.04359v3` - [abs](http://arxiv.org/abs/2002.04359v3) - [pdf](http://arxiv.org/pdf/2002.04359v3)

> Vulnerability to adversarial attacks is one of the principal hurdles to the adoption of deep learning in safety-critical applications. Despite significant efforts, both practical and theoretical, the problem remains open. In this paper, we analyse the geometry of adversarial attacks in the large-data, overparametrized limit for Bayesian Neural Networks (BNNs). We show that, in the limit, vulnerability to gradient-based attacks arises as a result of degeneracy in the data distribution, i.e., when the data lies on a lower-dimensional submanifold of the ambient space. As a direct consequence, we demonstrate that in the limit BNN posteriors are robust to gradient-based adversarial attacks. Experimental results on the MNIST and Fashion MNIST datasets with BNNs trained with Hamiltonian Monte Carlo and Variational Inference support this line of argument, showing that BNNs can display both high accuracy and robustness to gradient based adversarial attacks.

</details>

<details>

<summary>2020-06-24 12:00:05 - On Bayesian Search for the Feasible Space Under Computationally Expensive Constraints</summary>

- *Alma Rahat, Michael Wood*

- `2004.11055v2` - [abs](http://arxiv.org/abs/2004.11055v2) - [pdf](http://arxiv.org/pdf/2004.11055v2)

> We are often interested in identifying the feasible subset of a decision space under multiple constraints to permit effective design exploration. If determining feasibility required computationally expensive simulations, the cost of exploration would be prohibitive. Bayesian search is data-efficient for such problems: starting from a small dataset, the central concept is to use Bayesian models of constraints with an acquisition function to locate promising solutions that may improve predictions of feasibility when the dataset is augmented. At the end of this sequential active learning approach with a limited number of expensive evaluations, the models can accurately predict the feasibility of any solution obviating the need for full simulations. In this paper, we propose a novel acquisition function that combines the probability that a solution lies at the boundary between feasible and infeasible spaces (representing exploitation) and the entropy in predictions (representing exploration). Experiments confirmed the efficacy of the proposed function.

</details>

<details>

<summary>2020-06-24 13:18:48 - Social Network Mediation Analysis: a Latent Space Approach</summary>

- *Haiyan Liu, Ick Hoon Jin, Zhiyong Zhang, Ying Yuan*

- `1810.03751v2` - [abs](http://arxiv.org/abs/1810.03751v2) - [pdf](http://arxiv.org/pdf/1810.03751v2)

> Social networks contain data on both actor attributes and social connections among them. Such connections reflect the dependence among social actors, which is important for individual's mental health and social development. To investigate the potential mediation role of a social network, we propose a mediation model with a social network as a mediator. In the model, dependence among actors is accounted by a few mutually orthogonal latent dimensions. The scores on these dimensions are directly involved in the intervention process between an independent variable and a dependent variable. Because all the latent dimensions are equivalent in terms of their relationship to social networks, it is hardly to name them. The intervening effect through an individual dimension is thus of little practical interest. Therefore, we would rather focus on the mediation effect of a network. Although the scores are not unique, we rigorously articulate that the proposed network mediation effect is still well-defined. To estimate the model, we adopt a Bayesian estimation method. This modeling framework and the Bayesian estimation method is evaluated through a simulation study under representative conditions. Its usefulness is demonstrated through an empirical application to a college friendship network.

</details>

<details>

<summary>2020-06-24 14:13:58 - Bayesian Online Prediction of Change Points</summary>

- *Diego Agudelo-España, Sebastian Gomez-Gonzalez, Stefan Bauer, Bernhard Schölkopf, Jan Peters*

- `1902.04524v2` - [abs](http://arxiv.org/abs/1902.04524v2) - [pdf](http://arxiv.org/pdf/1902.04524v2)

> Online detection of instantaneous changes in the generative process of a data sequence generally focuses on retrospective inference of such change points without considering their future occurrences. We extend the Bayesian Online Change Point Detection algorithm to also infer the number of time steps until the next change point (i.e., the residual time). This enables to handle observation models which depend on the total segment duration, which is useful to model data sequences with temporal scaling. The resulting inference algorithm for segment detection can be deployed in an online fashion, and we illustrate applications to synthetic and to two medical real-world data sets.

</details>

<details>

<summary>2020-06-24 14:29:17 - Dynamic Bayesian Neural Networks</summary>

- *Lorenzo Rimella, Nick Whiteley*

- `2004.06963v2` - [abs](http://arxiv.org/abs/2004.06963v2) - [pdf](http://arxiv.org/pdf/2004.06963v2)

> We define an evolving in time Bayesian neural network called a Hidden Markov neural network. The weights of a feed-forward neural network are modelled with the hidden states of a Hidden Markov model, whose observed process is given by the available data. A filtering algorithm is used to learn a variational approximation to the evolving in time posterior over the weights. Training is pursued through a sequential version of Bayes by Backprop Blundell et al. 2015, which is enriched with a stronger regularization technique called variational DropConnect. The experiments test variational DropConnect on MNIST and display the performance of Hidden Markov neural networks on time series.

</details>

<details>

<summary>2020-06-24 15:00:22 - Dynamic Population Estimation Using Anonymized Mobility Data</summary>

- *Xiang Liu, Philo Pöllmann*

- `2006.13786v1` - [abs](http://arxiv.org/abs/2006.13786v1) - [pdf](http://arxiv.org/pdf/2006.13786v1)

> Fine population distribution both in space and in time is crucial for epidemic management, disaster prevention,urban planning and more. Human mobility data have a great potential for mapping population distribution at a high level of spatiotemporal resolution. Power law models are the most popular ones for mapping mobility data to population. However,they fail to provide consistent estimations under different spatial and temporal resolutions, i.e. they have to be recalibrated whenever the spatial or temporal partitioning scheme changes. We propose a Bayesian model for dynamic population estimation using static census data and anonymized mobility data. Our model gives consistent population estimations under different spatial and temporal resolutions.

</details>

<details>

<summary>2020-06-24 15:10:43 - Bayesian Sampling Bias Correction: Training with the Right Loss Function</summary>

- *L. Le Folgoc, V. Baltatzis, A. Alansary, S. Desai, A. Devaraj, S. Ellis, O. E. Martinez Manzanera, F. Kanavati, A. Nair, J. Schnabel, B. Glocker*

- `2006.13798v1` - [abs](http://arxiv.org/abs/2006.13798v1) - [pdf](http://arxiv.org/pdf/2006.13798v1)

> We derive a family of loss functions to train models in the presence of sampling bias. Examples are when the prevalence of a pathology differs from its sampling rate in the training dataset, or when a machine learning practioner rebalances their training dataset. Sampling bias causes large discrepancies between model performance in the lab and in more realistic settings. It is omnipresent in medical imaging applications, yet is often overlooked at training time or addressed on an ad-hoc basis. Our approach is based on Bayesian risk minimization. For arbitrary likelihood models we derive the associated bias corrected loss for training, exhibiting a direct connection to information gain. The approach integrates seamlessly in the current paradigm of (deep) learning using stochastic backpropagation and naturally with Bayesian models. We illustrate the methodology on case studies of lung nodule malignancy grading.

</details>

<details>

<summary>2020-06-25 01:11:02 - Robust and Efficient Approximate Bayesian Computation: A Minimum Distance Approach</summary>

- *David T. Frazier*

- `2006.14126v1` - [abs](http://arxiv.org/abs/2006.14126v1) - [pdf](http://arxiv.org/pdf/2006.14126v1)

> In many instances, the application of approximate Bayesian methods is hampered by two practical features: 1) the requirement to project the data down to low-dimensional summary, including the choice of this projection, which ultimately yields inefficient inference; 2) a possible lack of robustness to deviations from the underlying model structure. Motivated by these efficiency and robustness concerns, we construct a new Bayesian method that can deliver efficient estimators when the underlying model is well-specified, and which is simultaneously robust to certain forms of model misspecification. This new approach bypasses the calculation of summaries by considering a norm between empirical and simulated probability measures. For specific choices of the norm, we demonstrate that this approach can deliver point estimators that are as efficient as those obtained using exact Bayesian inference, while also simultaneously displaying robustness to deviations from the underlying model assumptions.

</details>

<details>

<summary>2020-06-25 03:57:04 - Deep Ensembles: A Loss Landscape Perspective</summary>

- *Stanislav Fort, Huiyi Hu, Balaji Lakshminarayanan*

- `1912.02757v2` - [abs](http://arxiv.org/abs/1912.02757v2) - [pdf](http://arxiv.org/pdf/1912.02757v2)

> Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.

</details>

<details>

<summary>2020-06-25 08:04:48 - Green Machine Learning via Augmented Gaussian Processes and Multi-Information Source Optimization</summary>

- *Antonio Candelieri, Riccardo Perego, Francesco Archetti*

- `2006.14233v1` - [abs](http://arxiv.org/abs/2006.14233v1) - [pdf](http://arxiv.org/pdf/2006.14233v1)

> Searching for accurate Machine and Deep Learning models is a computationally expensive and awfully energivorous process. A strategy which has been gaining recently importance to drastically reduce computational time and energy consumed is to exploit the availability of different information sources, with different computational costs and different "fidelity", typically smaller portions of a large dataset. The multi-source optimization strategy fits into the scheme of Gaussian Process based Bayesian Optimization. An Augmented Gaussian Process method exploiting multiple information sources (namely, AGP-MISO) is proposed. The Augmented Gaussian Process is trained using only "reliable" information among available sources. A novel acquisition function is defined according to the Augmented Gaussian Process. Computational results are reported related to the optimization of the hyperparameters of a Support Vector Machine (SVM) classifier using two sources: a large dataset - the most expensive one - and a smaller portion of it. A comparison with a traditional Bayesian Optimization approach to optimize the hyperparameters of the SVM classifier on the large dataset only is reported.

</details>

<details>

<summary>2020-06-25 10:02:34 - Uncertainty in Neural Relational Inference Trajectory Reconstruction</summary>

- *Vasileios Karavias, Ben Day, Pietro Liò*

- `2006.13666v2` - [abs](http://arxiv.org/abs/2006.13666v2) - [pdf](http://arxiv.org/pdf/2006.13666v2)

> Neural networks used for multi-interaction trajectory reconstruction lack the ability to estimate the uncertainty in their outputs, which would be useful to better analyse and understand the systems they model. In this paper we extend the Factorised Neural Relational Inference model to output both a mean and a standard deviation for each component of the phase space vector, which together with an appropriate loss function, can account for uncertainty. A variety of loss functions are investigated including ideas from convexification and a Bayesian treatment of the problem. We show that the physical meaning of the variables is important when considering the uncertainty and demonstrate the existence of pathological local minima that are difficult to avoid during training.

</details>

<details>

<summary>2020-06-25 10:11:04 - Novel Change of Measure Inequalities with Applications to PAC-Bayesian Bounds and Monte Carlo Estimation</summary>

- *Yuki Ohnishi, Jean Honorio*

- `2002.10678v2` - [abs](http://arxiv.org/abs/2002.10678v2) - [pdf](http://arxiv.org/pdf/2002.10678v2)

> We introduce several novel change of measure inequalities for two families of divergences: $f$-divergences and $\alpha$-divergences. We show how the variational representation for $f$-divergences leads to novel change of measure inequalities. We also present a multiplicative change of measure inequality for $\alpha$-divergences and a generalized version of Hammersley-Chapman-Robbins inequality. Finally, we present several applications of our change of measure inequalities, including PAC-Bayesian bounds for various classes of losses and non-asymptotic intervals for Monte Carlo estimates.

</details>

<details>

<summary>2020-06-25 13:18:18 - Automatic Tuning of Stochastic Gradient Descent with Bayesian Optimisation</summary>

- *Victor Picheny, Vincent Dutordoir, Artem Artemev, Nicolas Durrande*

- `2006.14376v1` - [abs](http://arxiv.org/abs/2006.14376v1) - [pdf](http://arxiv.org/pdf/2006.14376v1)

> Many machine learning models require a training procedure based on running stochastic gradient descent. A key element for the efficiency of those algorithms is the choice of the learning rate schedule. While finding good learning rates schedules using Bayesian optimisation has been tackled by several authors, adapting it dynamically in a data-driven way is an open question. This is of high practical importance to users that need to train a single, expensive model. To tackle this problem, we introduce an original probabilistic model for traces of optimisers, based on latent Gaussian processes and an auto-/regressive formulation, that flexibly adjusts to abrupt changes of behaviours induced by new learning rate values. As illustrated, this model is well-suited to tackle a set of problems: first, for the on-line adaptation of the learning rate for a cold-started run; then, for tuning the schedule for a set of similar tasks (in a classical BO setup), as well as warm-starting it for a new task.

</details>

<details>

<summary>2020-06-25 22:17:20 - Asynchronous Multi Agent Active Search</summary>

- *Ramina Ghods, Arundhati Banerjee, Jeff Schneider*

- `2006.14718v1` - [abs](http://arxiv.org/abs/2006.14718v1) - [pdf](http://arxiv.org/pdf/2006.14718v1)

> Active search refers to the problem of efficiently locating targets in an unknown environment by actively making data-collection decisions, and has many applications including detecting gas leaks, radiation sources or human survivors of disasters using aerial and/or ground robots (agents). Existing active search methods are in general only amenable to a single agent, or if they extend to multi agent they require a central control system to coordinate the actions of all agents. However, such control systems are often impractical in robotics applications. In this paper, we propose two distinct active search algorithms called SPATS (Sparse Parallel Asynchronous Thompson Sampling) and LATSI (LAplace Thompson Sampling with Information gain) that allow for multiple agents to independently make data-collection decisions without a central coordinator. Throughout we consider that targets are sparsely located around the environment in keeping with compressive sensing assumptions and its applicability in real world scenarios. Additionally, while most common search algorithms assume that agents can sense the entire environment (e.g. compressive sensing) or sense point-wise (e.g. Bayesian Optimization) at all times, we make a realistic assumption that each agent can only sense a contiguous region of space at a time. We provide simulation results as well as theoretical analysis to demonstrate the efficacy of our proposed algorithms.

</details>

<details>

<summary>2020-06-26 02:55:24 - PAC-Bayesian Bound for the Conditional Value at Risk</summary>

- *Zakaria Mhammedi, Benjamin Guedj, Robert C. Williamson*

- `2006.14763v1` - [abs](http://arxiv.org/abs/2006.14763v1) - [pdf](http://arxiv.org/pdf/2006.14763v1)

> Conditional Value at Risk (CVaR) is a family of "coherent risk measures" which generalize the traditional mathematical expectation. Widely used in mathematical finance, it is garnering increasing interest in machine learning, e.g., as an alternate approach to regularization, and as a means for ensuring fairness. This paper presents a generalization bound for learning algorithms that minimize the CVaR of the empirical loss. The bound is of PAC-Bayesian type and is guaranteed to be small when the empirical CVaR is small. We achieve this by reducing the problem of estimating CVaR to that of merely estimating an expectation. This then enables us, as a by-product, to obtain concentration inequalities for CVaR even when the random variable in question is unbounded.

</details>

<details>

<summary>2020-06-26 05:47:45 - GINNs: Graph-Informed Neural Networks for Multiscale Physics</summary>

- *Eric J. Hall, Søren Taverniers, Markos A. Katsoulakis, Daniel M. Tartakovsky*

- `2006.14807v1` - [abs](http://arxiv.org/abs/2006.14807v1) - [pdf](http://arxiv.org/pdf/2006.14807v1)

> We introduce the concept of a Graph-Informed Neural Network (GINN), a hybrid approach combining deep learning with probabilistic graphical models (PGMs) that acts as a surrogate for physics-based representations of multiscale and multiphysics systems. GINNs address the twin challenges of removing intrinsic computational bottlenecks in physics-based models and generating large data sets for estimating probability distributions of quantities of interest (QoIs) with a high degree of confidence. Both the selection of the complex physics learned by the NN and its supervised learning/prediction are informed by the PGM, which includes the formulation of structured priors for tunable control variables (CVs) to account for their mutual correlations and ensure physically sound CV and QoI distributions. GINNs accelerate the prediction of QoIs essential for simulation-based decision-making where generating sufficient sample data using physics-based models alone is often prohibitively expensive. Using a real-world application grounded in supercapacitor-based energy storage, we describe the construction of GINNs from a Bayesian network-embedded homogenized model for supercapacitor dynamics, and demonstrate their ability to produce kernel density estimates of relevant non-Gaussian, skewed QoIs with tight confidence intervals.

</details>

<details>

<summary>2020-06-26 08:15:29 - Stochastic Online Optimization using Kalman Recursion</summary>

- *Joseph de Vilmarest, Olivier Wintenberger*

- `2002.03636v2` - [abs](http://arxiv.org/abs/2002.03636v2) - [pdf](http://arxiv.org/pdf/2002.03636v2)

> We study the Extended Kalman Filter in constant dynamics, offering a bayesian perspective of stochastic optimization. We obtain high probability bounds on the cumulative excess risk in an unconstrained setting. In order to avoid any projection step we propose a two-phase analysis. First, for linear and logistic regressions, we prove that the algorithm enters a local phase where the estimate stays in a small region around the optimum. We provide explicit bounds with high probability on this convergence time. Second, for generalized linear regressions, we provide a martingale analysis of the excess risk in the local phase, improving existing ones in bounded stochastic optimization. The EKF appears as a parameter-free online algorithm with O(d^2) cost per iteration that optimally solves some unconstrained optimization problems.

</details>

<details>

<summary>2020-06-26 10:21:35 - Stochastic Differential Equations with Variational Wishart Diffusions</summary>

- *Martin Jørgensen, Marc Peter Deisenroth, Hugh Salimbeni*

- `2006.14895v1` - [abs](http://arxiv.org/abs/2006.14895v1) - [pdf](http://arxiv.org/pdf/2006.14895v1)

> We present a Bayesian non-parametric way of inferring stochastic differential equations for both regression tasks and continuous-time dynamical modelling. The work has high emphasis on the stochastic part of the differential equation, also known as the diffusion, and modelling it by means of Wishart processes. Further, we present a semi-parametric approach that allows the framework to scale to high dimensions. This successfully lead us onto how to model both latent and auto-regressive temporal systems with conditional heteroskedastic noise. We provide experimental evidence that modelling diffusion often improves performance and that this randomness in the differential equation can be essential to avoid overfitting.

</details>

<details>

<summary>2020-06-26 13:50:19 - Unlabelled Data Improves Bayesian Uncertainty Calibration under Covariate Shift</summary>

- *Alex J. Chan, Ahmed M. Alaa, Zhaozhi Qian, Mihaela van der Schaar*

- `2006.14988v1` - [abs](http://arxiv.org/abs/2006.14988v1) - [pdf](http://arxiv.org/pdf/2006.14988v1)

> Modern neural networks have proven to be powerful function approximators, providing state-of-the-art performance in a multitude of applications. They however fall short in their ability to quantify confidence in their predictions - this is crucial in high-stakes applications that involve critical decision-making. Bayesian neural networks (BNNs) aim at solving this problem by placing a prior distribution over the network's parameters, thereby inducing a posterior distribution that encapsulates predictive uncertainty. While existing variants of BNNs based on Monte Carlo dropout produce reliable (albeit approximate) uncertainty estimates over in-distribution data, they tend to exhibit over-confidence in predictions made on target data whose feature distribution differs from the training data, i.e., the covariate shift setup. In this paper, we develop an approximate Bayesian inference scheme based on posterior regularisation, wherein unlabelled target data are used as "pseudo-labels" of model confidence that are used to regularise the model's loss on labelled source data. We show that this approach significantly improves the accuracy of uncertainty quantification on covariate-shifted data sets, with minimal modification to the underlying model architecture. We demonstrate the utility of our method in the context of transferring prognostic models of prostate cancer across globally diverse populations.

</details>

<details>

<summary>2020-06-26 16:02:28 - On the Convergence of the Laplace Approximation and Noise-Level-Robustness of Laplace-based Monte Carlo Methods for Bayesian Inverse Problems</summary>

- *Claudia Schillings, Björn Sprungk, Philipp Wacker*

- `1901.03958v4` - [abs](http://arxiv.org/abs/1901.03958v4) - [pdf](http://arxiv.org/pdf/1901.03958v4)

> The Bayesian approach to inverse problems provides a rigorous framework for the incorporation and quantification of uncertainties in measurements, parameters and models. We are interested in designing numerical methods which are robust w.r.t. the size of the observational noise, i.e., methods which behave well in case of concentrated posterior measures. The concentration of the posterior is a highly desirable situation in practice, since it relates to informative or large data. However, it can pose a computational challenge for numerical methods based on the prior or reference measure. We propose to employ the Laplace approximation of the posterior as the base measure for numerical integration in this context. The Laplace approximation is a Gaussian measure centered at the maximum a-posteriori estimate and with covariance matrix depending on the logposterior density. We discuss convergence results of the Laplace approximation in terms of the Hellinger distance and analyze the efficiency of Monte Carlo methods based on it. In particular, we show that Laplace-based importance sampling and Laplace-based quasi-Monte-Carlo methods are robust w.r.t. the concentration of the posterior for large classes of posterior distributions and integrands whereas prior-based importance sampling and plain quasi-Monte Carlo are not. Numerical experiments are presented to illustrate the theoretical findings.

</details>

<details>

<summary>2020-06-26 16:15:49 - Continual Learning from the Perspective of Compression</summary>

- *Xu He, Min Lin*

- `2006.15078v1` - [abs](http://arxiv.org/abs/2006.15078v1) - [pdf](http://arxiv.org/pdf/2006.15078v1)

> Connectionist models such as neural networks suffer from catastrophic forgetting. In this work, we study this problem from the perspective of information theory and define forgetting as the increase of description lengths of previous data when they are compressed with a sequentially learned model. In addition, we show that continual learning approaches based on variational posterior approximation and generative replay can be considered as approximations to two prequential coding methods in compression, namely, the Bayesian mixture code and maximum likelihood (ML) plug-in code. We compare these approaches in terms of both compression and forgetting and empirically study the reasons that limit the performance of continual learning methods based on variational posterior approximation. To address these limitations, we propose a new continual learning method that combines ML plug-in and Bayesian mixture codes.

</details>

<details>

<summary>2020-06-26 20:47:17 - Fault Detection and Identification using Bayesian Recurrent Neural Networks</summary>

- *Weike Sun, Antonio R. C. Paiva, Peng Xu, Anantha Sundaram, Richard D. Braatz*

- `1911.04386v2` - [abs](http://arxiv.org/abs/1911.04386v2) - [pdf](http://arxiv.org/pdf/1911.04386v2)

> In processing and manufacturing industries, there has been a large push to produce higher quality products and ensure maximum efficiency of processes. This requires approaches to effectively detect and resolve disturbances to ensure optimal operations. While the control system can compensate for many types of disturbances, there are changes to the process which it still cannot handle adequately. It is therefore important to further develop monitoring systems to effectively detect and identify those faults such that they can be quickly resolved by operators. In this paper, a novel probabilistic fault detection and identification method is proposed which adopts a newly developed deep learning approach using Bayesian recurrent neural networks~(BRNNs) with variational dropout. The BRNN model is general and can model complex nonlinear dynamics. Moreover, compared to traditional statistic-based data-driven fault detection and identification methods, the proposed BRNN-based method yields uncertainty estimates which allow for simultaneous fault detection of chemical processes, direct fault identification, and fault propagation analysis. The outstanding performance of this method is demonstrated and contrasted to (dynamic) principal component analysis, which are widely applied in the industry, in the benchmark Tennessee Eastman process~(TEP) and a real chemical manufacturing dataset.

</details>

<details>

<summary>2020-06-27 13:05:43 - Surrogate-assisted Bayesian inversion for landscape and basin evolution models</summary>

- *Rohitash Chandra, Danial Azam, Arpit Kapoor, R. Dietmar Müller*

- `1812.08655v3` - [abs](http://arxiv.org/abs/1812.08655v3) - [pdf](http://arxiv.org/pdf/1812.08655v3)

> The complex and computationally expensive nature of landscape evolution models pose significant challenges in the inference and optimisation of unknown parameters. Bayesian inference provides a methodology for estimation and uncertainty quantification of unknown model parameters. In our previous work, we developed parallel tempering Bayeslands as a framework for parameter estimation and uncertainty quantification for the Badlands landscape evolution model. Parallel tempering Bayeslands features high-performance computing with dozens of processing cores running in parallel to enhance computational efficiency. Although we use parallel computing, the procedure remains computationally challenging since thousands of samples need to be drawn and evaluated. \textcolor{black}{In large-scale landscape and basin evolution problems, a single model evaluation can take from several minutes to hours, and in some instances, even days. Surrogate-assisted optimisation has been used for several computationally expensive engineering problems which motivate its use in optimisation and inference of complex geoscientific models.} The use of surrogate models can speed up parallel tempering Bayeslands by developing computationally inexpensive models to mimic expensive ones. In this paper, we apply surrogate-assisted parallel tempering where that surrogate mimics a landscape evolution model by estimating the likelihood function from the model. \textcolor{black}{We employ a neural network-based surrogate model that learns from the history of samples generated. } The entire framework is developed in a parallel computing infrastructure to take advantage of parallelism. The results show that the proposed methodology is effective in lowering the overall computational cost significantly while retaining the quality of solutions.

</details>

<details>

<summary>2020-06-27 16:31:26 - Bayesian Reinforcement Learning via Deep, Sparse Sampling</summary>

- *Divya Grover, Debabrota Basu, Christos Dimitrakakis*

- `1902.02661v4` - [abs](http://arxiv.org/abs/1902.02661v4) - [pdf](http://arxiv.org/pdf/1902.02661v4)

> We address the problem of Bayesian reinforcement learning using efficient model-based online planning. We propose an optimism-free Bayes-adaptive algorithm to induce deeper and sparser exploration with a theoretical bound on its performance relative to the Bayes optimal policy, with a lower computational complexity. The main novelty is the use of a candidate policy generator, to generate long-term options in the planning tree (over beliefs), which allows us to create much sparser and deeper trees. Experimental results on different environments show that in comparison to the state-of-the-art, our algorithm is both computationally more efficient, and obtains significantly higher reward in discrete environments.

</details>

<details>

<summary>2020-06-27 21:10:21 - Frequentist Uncertainty in Recurrent Neural Networks via Blockwise Influence Functions</summary>

- *Ahmed M. Alaa, Mihaela van der Schaar*

- `2006.13707v2` - [abs](http://arxiv.org/abs/2006.13707v2) - [pdf](http://arxiv.org/pdf/2006.13707v2)

> Recurrent neural networks (RNNs) are instrumental in modelling sequential and time-series data. Yet, when using RNNs to inform decision-making, predictions by themselves are not sufficient; we also need estimates of predictive uncertainty. Existing approaches for uncertainty quantification in RNNs are based predominantly on Bayesian methods; these are computationally prohibitive, and require major alterations to the RNN architecture and training. Capitalizing on ideas from classical jackknife resampling, we develop a frequentist alternative that: (a) does not interfere with model training or compromise its accuracy, (b) applies to any RNN architecture, and (c) provides theoretical coverage guarantees on the estimated uncertainty intervals. Our method derives predictive uncertainty from the variability of the (jackknife) sampling distribution of the RNN outputs, which is estimated by repeatedly deleting blocks of (temporally-correlated) training data, and collecting the predictions of the RNN re-trained on the remaining data. To avoid exhaustive re-training, we utilize influence functions to estimate the effect of removing training data blocks on the learned RNN parameters. Using data from a critical care setting, we demonstrate the utility of uncertainty quantification in sequential decision-making.

</details>

<details>

<summary>2020-06-27 22:41:47 - A strong law of large numbers for scrambled net integration</summary>

- *Art B. Owen, Daniel Rudolf*

- `2002.07859v3` - [abs](http://arxiv.org/abs/2002.07859v3) - [pdf](http://arxiv.org/pdf/2002.07859v3)

> This article provides a strong law of large numbers for integration on digital nets randomized by a nested uniform scramble. The motivating problem is optimization over some variables of an integral over others, arising in Bayesian optimization. This strong law requires that the integrand have a finite moment of order $p$ for some $p>1$. Previously known results implied a strong law only for Riemann integrable functions. Previous general weak laws of large numbers for scrambled nets require a square integrable integrand. We generalize from $L^2$ to $L^p$ for $p>1$ via the Riesz-Thorin interpolation theorem

</details>

<details>

<summary>2020-06-28 03:53:14 - Bayesian Sparsification Methods for Deep Complex-valued Networks</summary>

- *Ivan Nazarov, Evgeny Burnaev*

- `2003.11413v2` - [abs](http://arxiv.org/abs/2003.11413v2) - [pdf](http://arxiv.org/pdf/2003.11413v2)

> With continual miniaturization ever more applications of deep learning can be found in embedded systems, where it is common to encounter data with natural complex domain representation. To this end we extend Sparse Variational Dropout to complex-valued neural networks and verify the proposed Bayesian technique by conducting a large numerical study of the performance-compression trade-off of C-valued networks on two tasks: image recognition on MNIST-like and CIFAR10 datasets and music transcription on MusicNet. We replicate the state-of-the-art result by Trabelsi et al. [2018] on MusicNet with a complex-valued network compressed by 50-100x at a small performance penalty.

</details>

<details>

<summary>2020-06-28 05:31:53 - Sampler Design for Implicit Feedback Data by Noisy-label Robust Learning</summary>

- *Wenhui Yu, Zheng Qin*

- `2007.07204v1` - [abs](http://arxiv.org/abs/2007.07204v1) - [pdf](http://arxiv.org/pdf/2007.07204v1)

> Implicit feedback data is extensively explored in recommendation as it is easy to collect and generally applicable. However, predicting users' preference on implicit feedback data is a challenging task since we can only observe positive (voted) samples and unvoted samples. It is difficult to distinguish between the negative samples and unlabeled positive samples from the unvoted ones. Existing works, such as Bayesian Personalized Ranking (BPR), sample unvoted items as negative samples uniformly, therefore suffer from a critical noisy-label issue. To address this gap, we design an adaptive sampler based on noisy-label robust learning for implicit feedback data.   To formulate the issue, we first introduce Bayesian Point-wise Optimization (BPO) to learn a model, e.g., Matrix Factorization (MF), by maximum likelihood estimation. We predict users' preferences with the model and learn it by maximizing likelihood of observed data labels, i.e., a user prefers her positive samples and has no interests in her unvoted samples. However, in reality, a user may have interests in some of her unvoted samples, which are indeed positive samples mislabeled as negative ones. We then consider the risk of these noisy labels, and propose a Noisy-label Robust BPO (NBPO). NBPO also maximizes the observation likelihood while connects users' preference and observed labels by the likelihood of label flipping based on the Bayes' theorem. In NBPO, a user prefers her true positive samples and shows no interests in her true negative samples, hence the optimization quality is dramatically improved. Extensive experiments on two public real-world datasets show the significant improvement of our proposed optimization methods.

</details>

<details>

<summary>2020-06-28 07:52:35 - Scalable Bayesian Multiple Changepoint Detection via Auxiliary Uniformization</summary>

- *Lu Shaochuan*

- `2006.15532v1` - [abs](http://arxiv.org/abs/2006.15532v1) - [pdf](http://arxiv.org/pdf/2006.15532v1)

> By attaching auxiliary event times to the chronologically ordered observations, we formulate the Bayesian multiple changepoint problem of discrete-time observations into that of continuous-time ones. A version of forward-filtering backward-sampling (FFBS) algorithm is proposed for the simulation of changepoints within a collapsed Gibbs sampling scheme. Ideally, both the computational cost and memory cost of the FFBS algorithm can be quadratically scaled down to the number of changepoints, instead of the number of observations, which is otherwise prohibitive for a long sequence of observations. The new formulation allows the number of changepoints accrue unboundedly upon the arrivals of new data. Also, a time-varying changepoint recurrence rate across different segments is assumed to characterize diverse scales of run lengths of changepoints. We then suggest a continuous-time Viterbi algorithm for obtaining the Maximum A Posteriori (MAP) estimates of changepoints. We demonstrate the methods through simulation studies and real data analysis.

</details>

<details>

<summary>2020-06-28 15:28:18 - Non-Stationary Multi-layered Gaussian Priors for Bayesian Inversion</summary>

- *Muhammad Emzir, Sari Lasanen, Zenith Purisha, Lassi Roininen, Simo Särkkä*

- `2006.15634v1` - [abs](http://arxiv.org/abs/2006.15634v1) - [pdf](http://arxiv.org/pdf/2006.15634v1)

> In this article, we study Bayesian inverse problems with multi-layered Gaussian priors. We first describe the conditionally Gaussian layers in terms of a system of stochastic partial differential equations. We build the computational inference method using a finite-dimensional Galerkin method. We show that the proposed approximation has a convergence-in-probability property to the solution of the original multi-layered model. We then carry out Bayesian inference using the preconditioned Crank--Nicolson algorithm which is modified to work with multi-layered Gaussian fields. We show via numerical experiments in signal deconvolution and computerized X-ray tomography problems that the proposed method can offer both smoothing and edge preservation at the same time.

</details>

<details>

<summary>2020-06-28 15:40:23 - Prior constraints of well-posedness in stochastic inversion problems of computer models</summary>

- *Nicolas Bousquet, Mélanie Blazère, Thomas Cerbelaud*

- `1806.03440v2` - [abs](http://arxiv.org/abs/1806.03440v2) - [pdf](http://arxiv.org/pdf/1806.03440v2)

> Stochastic inversion problems are typically encountered when it is wanted to quantify the uncertainty affecting the inputs of computer models. They consist in estimating input distributions from noisy, observable outputs, and such problems are increasingly examined in Bayesian contexts where the targeted inputs are affected by a mixture of aleatory and epistemic uncertainties. While they are characterized by identifiability conditions, well-posedness constraints of "signal to noise" have to be took into account within the definition of the model, prior to inference. In addition to numeric conditioning notions and regularization techniques used in inverse problems, this article proposes and investigates a novel interpretation of well-posedness, in the context of parametric uncertainty quantification and global sensitivity analysis, based on the degradation of Fisher information. It offers an explicitation of such prior constraints considering linear or linearizable operators, this linearization being either local (based on differentiability) or variational. Simulated experiments indicate that, when injected into the modeling process, these constraints can limit the influence of measurement or process noise on the estimation of the input distribution, and let hope for future extensions in a full non-linear framework, for example through the use of linear Gaussian mixtures.

</details>

<details>

<summary>2020-06-28 16:17:03 - Variational Autoencoding of PDE Inverse Problems</summary>

- *Daniel J. Tait, Theodoros Damoulas*

- `2006.15641v1` - [abs](http://arxiv.org/abs/2006.15641v1) - [pdf](http://arxiv.org/pdf/2006.15641v1)

> Specifying a governing physical model in the presence of missing physics and recovering its parameters are two intertwined and fundamental problems in science. Modern machine learning allows one to circumvent these, via emulators and surrogates, but in doing so disregards prior knowledge and physical laws that are especially important for small data regimes, interpretability, and decision making. In this work we fold the mechanistic model into a flexible data-driven surrogate to arrive at a physically structured decoder network. This provides accelerated inference for the Bayesian inverse problem, and can act as a drop-in regulariser that encodes a-priori physical information. We employ the variational form of the PDE problem and introduce stochastic local approximations as a form of model based data augmentation. We demonstrate both the accuracy and increased computational efficiency of the framework on real world settings and structured spatial processes.

</details>

<details>

<summary>2020-06-28 21:13:55 - A review of Bayesian perspectives on sample size derivation for confirmatory trials</summary>

- *Kevin Kunzmann, Michael J. Grayling, Kim May Lee, David S. Robertson, Kaspar Rufibach, James M. S. Wason*

- `2006.15715v1` - [abs](http://arxiv.org/abs/2006.15715v1) - [pdf](http://arxiv.org/pdf/2006.15715v1)

> Sample size derivation is a crucial element of the planning phase of any confirmatory trial. A sample size is typically derived based on constraints on the maximal acceptable type I error rate and a minimal desired power. Here, power depends on the unknown true effect size. In practice, power is typically calculated either for the smallest relevant effect size or a likely point alternative. The former might be problematic if the minimal relevant effect is close to the null, thus requiring an excessively large sample size. The latter is dubious since it does not account for the a priori uncertainty about the likely alternative effect size. A Bayesian perspective on the sample size derivation for a frequentist trial naturally emerges as a way of reconciling arguments about the relative a priori plausibility of alternative effect sizes with ideas based on the relevance of effect sizes. Many suggestions as to how such `hybrid' approaches could be implemented in practice have been put forward in the literature. However, key quantities such as assurance, probability of success, or expected power are often defined in subtly different ways in the literature. Starting from the traditional and entirely frequentist approach to sample size derivation, we derive consistent definitions for the most commonly used `hybrid' quantities and highlight connections, before discussing and demonstrating their use in the context of sample size derivation for clinical trials.

</details>

<details>

<summary>2020-06-28 22:37:13 - B-SCST: Bayesian Self-Critical Sequence Training for Image Captioning</summary>

- *Shashank Bujimalla, Mahesh Subedar, Omesh Tickoo*

- `2004.02435v2` - [abs](http://arxiv.org/abs/2004.02435v2) - [pdf](http://arxiv.org/pdf/2004.02435v2)

> Bayesian deep neural networks (DNNs) can provide a mathematically grounded framework to quantify uncertainty in predictions from image captioning models. We propose a Bayesian variant of policy-gradient based reinforcement learning training technique for image captioning models to directly optimize non-differentiable image captioning quality metrics such as CIDEr-D. We extend the well-known Self-Critical Sequence Training (SCST) approach for image captioning models by incorporating Bayesian inference, and refer to it as B-SCST. The "baseline" for the policy-gradients in B-SCST is generated by averaging predictive quality metrics (CIDEr-D) of the captions drawn from the distribution obtained using a Bayesian DNN model. We infer this predictive distribution using Monte Carlo (MC) dropout approximate variational inference. We show that B-SCST improves CIDEr-D scores on Flickr30k, MS COCO and VizWiz image captioning datasets, compared to the SCST approach. We also provide a study of uncertainty quantification for the predicted captions, and demonstrate that it correlates well with the CIDEr-D scores. To our knowledge, this is the first such analysis, and it can improve the interpretability of image captioning model outputs, which is critical for practical applications.

</details>

<details>

<summary>2020-06-29 02:17:18 - Efficient Nonmyopic Bayesian Optimization via One-Shot Multi-Step Trees</summary>

- *Shali Jiang, Daniel R. Jiang, Maximilian Balandat, Brian Karrer, Jacob R. Gardner, Roman Garnett*

- `2006.15779v1` - [abs](http://arxiv.org/abs/2006.15779v1) - [pdf](http://arxiv.org/pdf/2006.15779v1)

> Bayesian optimization is a sequential decision making framework for optimizing expensive-to-evaluate black-box functions. Computing a full lookahead policy amounts to solving a highly intractable stochastic dynamic program. Myopic approaches, such as expected improvement, are often adopted in practice, but they ignore the long-term impact of the immediate decision. Existing nonmyopic approaches are mostly heuristic and/or computationally expensive. In this paper, we provide the first efficient implementation of general multi-step lookahead Bayesian optimization, formulated as a sequence of nested optimization problems within a multi-step scenario tree. Instead of solving these problems in a nested way, we equivalently optimize all decision variables in the full tree jointly, in a ``one-shot'' fashion. Combining this with an efficient method for implementing multi-step Gaussian process ``fantasization,'' we demonstrate that multi-step expected improvement is computationally tractable and exhibits performance superior to existing methods on a wide range of benchmarks.

</details>

<details>

<summary>2020-06-29 02:58:25 - Bayesian Low Rank Tensor Ring Model for Image Completion</summary>

- *Zhen Long, Ce Zhu, Jiani Liu, Yipeng Liu*

- `2007.01055v1` - [abs](http://arxiv.org/abs/2007.01055v1) - [pdf](http://arxiv.org/pdf/2007.01055v1)

> Low rank tensor ring model is powerful for image completion which recovers missing entries in data acquisition and transformation. The recently proposed tensor ring (TR) based completion algorithms generally solve the low rank optimization problem by alternating least squares method with predefined ranks, which may easily lead to overfitting when the unknown ranks are set too large and only a few measurements are available. In this paper, we present a Bayesian low rank tensor ring model for image completion by automatically learning the low rank structure of data. A multiplicative interaction model is developed for the low-rank tensor ring decomposition, where core factors are enforced to be sparse by assuming their entries obey Student-T distribution. Compared with most of the existing methods, the proposed one is free of parameter-tuning, and the TR ranks can be obtained by Bayesian inference. Numerical Experiments, including synthetic data, color images with different sizes and YaleFace dataset B with respect to one pose, show that the proposed approach outperforms state-of-the-art ones, especially in terms of recovery accuracy.

</details>

<details>

<summary>2020-06-29 03:04:18 - Statistical Foundation of Variational Bayes Neural Networks</summary>

- *Shrijita Bhattacharya, Tapabrata Maiti*

- `2006.15786v1` - [abs](http://arxiv.org/abs/2006.15786v1) - [pdf](http://arxiv.org/pdf/2006.15786v1)

> Despite the popularism of Bayesian neural networks in recent years, its use is somewhat limited in complex and big data situations due to the computational cost associated with full posterior evaluations. Variational Bayes (VB) provides a useful alternative to circumvent the computational cost and time complexity associated with the generation of samples from the true posterior using Markov Chain Monte Carlo (MCMC) techniques. The efficacy of the VB methods is well established in machine learning literature. However, its potential broader impact is hindered due to a lack of theoretical validity from a statistical perspective. However there are few results which revolve around the theoretical properties of VB, especially in non-parametric problems. In this paper, we establish the fundamental result of posterior consistency for the mean-field variational posterior (VP) for a feed-forward artificial neural network model. The paper underlines the conditions needed to guarantee that the VP concentrates around Hellinger neighborhoods of the true density function. Additionally, the role of the scale parameter and its influence on the convergence rates has also been discussed. The paper mainly relies on two results (1) the rate at which the true posterior grows (2) the rate at which the KL-distance between the posterior and variational posterior grows. The theory provides a guideline of building prior distributions for Bayesian NN models along with an assessment of accuracy of the corresponding VB implementation.

</details>

<details>

<summary>2020-06-29 03:21:38 - Probabilistic Classification Vector Machine for Multi-Class Classification</summary>

- *Shengfei Lyu, Xing Tian, Yang Li, Bingbing Jiang, Huanhuan Chen*

- `2006.15791v1` - [abs](http://arxiv.org/abs/2006.15791v1) - [pdf](http://arxiv.org/pdf/2006.15791v1)

> The probabilistic classification vector machine (PCVM) synthesizes the advantages of both the support vector machine and the relevant vector machine, delivering a sparse Bayesian solution to classification problems. However, the PCVM is currently only applicable to binary cases. Extending the PCVM to multi-class cases via heuristic voting strategies such as one-vs-rest or one-vs-one often results in a dilemma where classifiers make contradictory predictions, and those strategies might lose the benefits of probabilistic outputs. To overcome this problem, we extend the PCVM and propose a multi-class probabilistic classification vector machine (mPCVM). Two learning algorithms, i.e., one top-down algorithm and one bottom-up algorithm, have been implemented in the mPCVM. The top-down algorithm obtains the maximum a posteriori (MAP) point estimates of the parameters based on an expectation-maximization algorithm, and the bottom-up algorithm is an incremental paradigm by maximizing the marginal likelihood. The superior performance of the mPCVMs, especially when the investigated problem has a large number of classes, is extensively evaluated on synthetic and benchmark data sets.

</details>

<details>

<summary>2020-06-29 07:07:29 - Statistical inference of assortative community structures</summary>

- *Lizhi Zhang, Tiago P. Peixoto*

- `2006.14493v3` - [abs](http://arxiv.org/abs/2006.14493v3) - [pdf](http://arxiv.org/pdf/2006.14493v3)

> We develop a principled methodology to infer assortative communities in networks based on a nonparametric Bayesian formulation of the planted partition model. We show that this approach succeeds in finding statistically significant assortative modules in networks, unlike alternatives such as modularity maximization, which systematically overfits both in artificial as well as in empirical examples. In addition, we show that our method is not subject to a resolution limit, and can uncover an arbitrarily large number of communities, as long as there is statistical evidence for them. Our formulation is amenable to model selection procedures, which allow us to compare it to more general approaches based on the stochastic block model, and in this way reveal whether assortativity is in fact the dominating large-scale mixing pattern. We perform this comparison with several empirical networks, and identify numerous cases where the network's assortativity is exaggerated by traditional community detection methods, and we show how a more faithful degree of assortativity can be identified.

</details>

<details>

<summary>2020-06-29 08:12:09 - Burglary in London: Insights from Statistical Heterogeneous Spatial Point Processes</summary>

- *Jan Povala, Seppo Virtanen, Mark Girolami*

- `1910.05212v3` - [abs](http://arxiv.org/abs/1910.05212v3) - [pdf](http://arxiv.org/pdf/1910.05212v3)

> To obtain operational insights regarding the crime of burglary in London we consider the estimation of effects of covariates on the intensity of spatial point patterns. By taking into account localised properties of criminal behaviour, we propose a spatial extension to model-based clustering methods from the mixture modelling literature. The proposed Bayesian model is a finite mixture of Poisson generalised linear models such that each location is probabilistically assigned to one of the clusters. Each cluster is characterised by the regression coefficients which we subsequently use to interpret the localised effects of the covariates. Using a blocking structure of the study region, our approach allows specifying spatial dependence between nearby locations. We estimate the proposed model using Markov Chain Monte Carlo methods and provide a Python implementation.

</details>

<details>

<summary>2020-06-29 08:24:57 - Propagation for Dynamic Continuous Time Chain Event Graphs</summary>

- *Aditi Shenvi, Jim Q. Smith*

- `2006.15865v1` - [abs](http://arxiv.org/abs/2006.15865v1) - [pdf](http://arxiv.org/pdf/2006.15865v1)

> Chain Event Graphs (CEGs) are a family of event-based graphical models that represent context-specific conditional independences typically exhibited by asymmetric state space problems. The class of continuous time dynamic CEGs (CT-DCEGs) provides a factored representation of longitudinally evolving trajectories of a process in continuous time. Temporal evidence in a CT-DCEG introduces dependence between its transition and holding time distributions. We present a tractable exact inferential scheme analogous to the scheme in Kj{\ae}rulff (1992) for discrete Dynamic Bayesian Networks (DBNs) which employs standard junction tree inference by "unrolling" the DBN. To enable this scheme, we present an extension of the standard CEG propagation algorithm (Thwaites et al., 2008). Interestingly, the CT-DCEG benefits from simplification of its graph on observing compatible evidence while preserving the still relevant symmetries within the asymmetric network. Our results indicate that the CT-DCEG is preferred to DBNs and continuous time BNs under contexts involving significant asymmetry and a natural total ordering of the process evolution.

</details>

<details>

<summary>2020-06-29 10:44:06 - Multi-fidelity modeling with different input domain definitions using Deep Gaussian Processes</summary>

- *Ali Hebbal, Loic Brevault, Mathieu Balesdent, El-Ghazali Talbi, Nouredine Melab*

- `2006.15924v1` - [abs](http://arxiv.org/abs/2006.15924v1) - [pdf](http://arxiv.org/pdf/2006.15924v1)

> Multi-fidelity approaches combine different models built on a scarce but accurate data-set (high-fidelity data-set), and a large but approximate one (low-fidelity data-set) in order to improve the prediction accuracy. Gaussian Processes (GPs) are one of the popular approaches to exhibit the correlations between these different fidelity levels. Deep Gaussian Processes (DGPs) that are functional compositions of GPs have also been adapted to multi-fidelity using the Multi-Fidelity Deep Gaussian process model (MF-DGP). This model increases the expressive power compared to GPs by considering non-linear correlations between fidelities within a Bayesian framework. However, these multi-fidelity methods consider only the case where the inputs of the different fidelity models are defined over the same domain of definition (e.g., same variables, same dimensions). However, due to simplification in the modeling of the low-fidelity, some variables may be omitted or a different parametrization may be used compared to the high-fidelity model. In this paper, Deep Gaussian Processes for multi-fidelity (MF-DGP) are extended to the case where a different parametrization is used for each fidelity. The performance of the proposed multifidelity modeling technique is assessed on analytical test cases and on structural and aerodynamic real physical problems.

</details>

<details>

<summary>2020-06-29 13:00:28 - Being Bayesian about Categorical Probability</summary>

- *Taejong Joo, Uijung Chung, Min-Gwan Seo*

- `2002.07965v2` - [abs](http://arxiv.org/abs/2002.07965v2) - [pdf](http://arxiv.org/pdf/2002.07965v2)

> Neural networks utilize the softmax as a building block in classification tasks, which contains an overconfidence problem and lacks an uncertainty representation ability. As a Bayesian alternative to the softmax, we consider a random variable of a categorical probability over class labels. In this framework, the prior distribution explicitly models the presumed noise inherent in the observed label, which provides consistent gains in generalization performance in multiple challenging tasks. The proposed method inherits advantages of Bayesian approaches that achieve better uncertainty estimation and model calibration. Our method can be implemented as a plug-and-play loss function with negligible computational overhead compared to the softmax with the cross-entropy loss function.

</details>

<details>

<summary>2020-06-29 13:36:52 - Discriminative Jackknife: Quantifying Uncertainty in Deep Learning via Higher-Order Influence Functions</summary>

- *Ahmed M. Alaa, Mihaela van der Schaar*

- `2007.13481v1` - [abs](http://arxiv.org/abs/2007.13481v1) - [pdf](http://arxiv.org/pdf/2007.13481v1)

> Deep learning models achieve high predictive accuracy across a broad spectrum of tasks, but rigorously quantifying their predictive uncertainty remains challenging. Usable estimates of predictive uncertainty should (1) cover the true prediction targets with high probability, and (2) discriminate between high- and low-confidence prediction instances. Existing methods for uncertainty quantification are based predominantly on Bayesian neural networks; these may fall short of (1) and (2) -- i.e., Bayesian credible intervals do not guarantee frequentist coverage, and approximate posterior inference undermines discriminative accuracy. In this paper, we develop the discriminative jackknife (DJ), a frequentist procedure that utilizes influence functions of a model's loss functional to construct a jackknife (or leave-one-out) estimator of predictive confidence intervals. The DJ satisfies (1) and (2), is applicable to a wide range of deep learning models, is easy to implement, and can be applied in a post-hoc fashion without interfering with model training or compromising its accuracy. Experiments demonstrate that DJ performs competitively compared to existing Bayesian and non-Bayesian regression baselines.

</details>

<details>

<summary>2020-06-29 16:09:49 - Dueling Posterior Sampling for Preference-Based Reinforcement Learning</summary>

- *Ellen R. Novoseller, Yibing Wei, Yanan Sui, Yisong Yue, Joel W. Burdick*

- `1908.01289v4` - [abs](http://arxiv.org/abs/1908.01289v4) - [pdf](http://arxiv.org/pdf/1908.01289v4)

> In preference-based reinforcement learning (RL), an agent interacts with the environment while receiving preferences instead of absolute feedback. While there is increasing research activity in preference-based RL, the design of formal frameworks that admit tractable theoretical analysis remains an open challenge. Building upon ideas from preference-based bandit learning and posterior sampling in RL, we present DUELING POSTERIOR SAMPLING (DPS), which employs preference-based posterior sampling to learn both the system dynamics and the underlying utility function that governs the preference feedback. As preference feedback is provided on trajectories rather than individual state-action pairs, we develop a Bayesian approach for the credit assignment problem, translating preferences to a posterior distribution over state-action reward models. We prove an asymptotic Bayesian no-regret rate for DPS with a Bayesian linear regression credit assignment model. This is the first regret guarantee for preference-based RL to our knowledge. We also discuss possible avenues for extending the proof methodology to other credit assignment models. Finally, we evaluate the approach empirically, showing competitive performance against existing baselines.

</details>

<details>

<summary>2020-06-29 20:57:20 - Bayesian Sparse learning with preconditioned stochastic gradient MCMC and its applications</summary>

- *Yating Wang, Wei Deng, Lin Guang*

- `2006.16376v1` - [abs](http://arxiv.org/abs/2006.16376v1) - [pdf](http://arxiv.org/pdf/2006.16376v1)

> In this work, we propose a Bayesian type sparse deep learning algorithm. The algorithm utilizes a set of spike-and-slab priors for the parameters in the deep neural network. The hierarchical Bayesian mixture will be trained using an adaptive empirical method. That is, one will alternatively sample from the posterior using preconditioned stochastic gradient Langevin Dynamics (PSGLD), and optimize the latent variables via stochastic approximation. The sparsity of the network is achieved while optimizing the hyperparameters with adaptive searching and penalizing. A popular SG-MCMC approach is Stochastic gradient Langevin dynamics (SGLD). However, considering the complex geometry in the model parameter space in non-convex learning, updating parameters using a universal step size in each component as in SGLD may cause slow mixing. To address this issue, we apply a computationally manageable preconditioner in the updating rule, which provides a step-size parameter to adapt to local geometric properties. Moreover, by smoothly optimizing the hyperparameter in the preconditioning matrix, our proposed algorithm ensures a decreasing bias, which is introduced by ignoring the correction term in preconditioned SGLD. According to the existing theoretical framework, we show that the proposed algorithm can asymptotically converge to the correct distribution with a controllable bias under mild conditions. Numerical tests are performed on both synthetic regression problems and learning the solutions of elliptic PDE, which demonstrate the accuracy and efficiency of present work.

</details>

<details>

<summary>2020-06-30 02:42:06 - On the derivation of the renewal equation from an age-dependent branching process: an epidemic modelling perspective</summary>

- *Swapnil Mishra, Tresnia Berah, Thomas A. Mellan, H. Juliette T. Unwin, Michaela A Vollmer, Kris V Parag, Axel Gandy, Seth Flaxman, Samir Bhatt*

- `2006.16487v1` - [abs](http://arxiv.org/abs/2006.16487v1) - [pdf](http://arxiv.org/pdf/2006.16487v1)

> Renewal processes are a popular approach used in modelling infectious disease outbreaks. In a renewal process, previous infections give rise to future infections. However, while this formulation seems sensible, its application to infectious disease can be difficult to justify from first principles. It has been shown from the seminal work of Bellman and Harris that the renewal equation arises as the expectation of an age-dependent branching process. In this paper we provide a detailed derivation of the original Bellman Harris process. We introduce generalisations, that allow for time-varying reproduction numbers and the accounting of exogenous events, such as importations. We show how inference on the renewal equation is easy to accomplish within a Bayesian hierarchical framework. Using off the shelf MCMC packages, we fit to South Korea COVID-19 case data to estimate reproduction numbers and importations. Our derivation provides the mathematical fundamentals and assumptions underpinning the use of the renewal equation for modelling outbreaks.

</details>

<details>

<summary>2020-06-30 03:27:22 - Policy Gradient Optimization of Thompson Sampling Policies</summary>

- *Seungki Min, Ciamac C. Moallemi, Daniel J. Russo*

- `2006.16507v1` - [abs](http://arxiv.org/abs/2006.16507v1) - [pdf](http://arxiv.org/pdf/2006.16507v1)

> We study the use of policy gradient algorithms to optimize over a class of generalized Thompson sampling policies. Our central insight is to view the posterior parameter sampled by Thompson sampling as a kind of pseudo-action. Policy gradient methods can then be tractably applied to search over a class of sampling policies, which determine a probability distribution over pseudo-actions (i.e., sampled parameters) as a function of observed data. We also propose and compare policy gradient estimators that are specialized to Bayesian bandit problems. Numerical experiments demonstrate that direct policy search on top of Thompson sampling automatically corrects for some of the algorithm's known shortcomings and offers meaningful improvements even in long horizon problems where standard Thompson sampling is extremely effective.

</details>

<details>

<summary>2020-06-30 08:16:06 - Model-based Asynchronous Hyperparameter and Neural Architecture Search</summary>

- *Aaron Klein, Louis C. Tiao, Thibaut Lienart, Cedric Archambeau, Matthias Seeger*

- `2003.10865v2` - [abs](http://arxiv.org/abs/2003.10865v2) - [pdf](http://arxiv.org/pdf/2003.10865v2)

> We introduce a model-based asynchronous multi-fidelity method for hyperparameter and neural architecture search that combines the strengths of asynchronous Hyperband and Gaussian process-based Bayesian optimization. At the heart of our method is a probabilistic model that can simultaneously reason across hyperparameters and resource levels, and supports decision-making in the presence of pending evaluations. We demonstrate the effectiveness of our method on a wide range of challenging benchmarks, for tabular data, image classification and language modelling, and report substantial speed-ups over current state-of-the-art methods. Our new methods, along with asynchronous baselines, are implemented in a distributed framework which will be open sourced along with this publication.

</details>

<details>

<summary>2020-06-30 10:17:29 - Approximate Bayesian computation via the energy statistic</summary>

- *Hien D. Nguyen, Julyan Arbel, Hongliang Lü, Florence Forbes*

- `1905.05884v2` - [abs](http://arxiv.org/abs/1905.05884v2) - [pdf](http://arxiv.org/pdf/1905.05884v2)

> Approximate Bayesian computation (ABC) has become an essential part of the Bayesian toolbox for addressing problems in which the likelihood is prohibitively expensive or entirely unknown, making it intractable. ABC defines a pseudo-posterior by comparing observed data with simulated data, traditionally based on some summary statistics, the elicitation of which is regarded as a key difficulty. Recently, using data discrepancy measures has been proposed in order to bypass the construction of summary statistics. Here we propose to use the importance-sampling ABC (IS-ABC) algorithm relying on the so-called two-sample energy statistic. We establish a new asymptotic result for the case where both the observed sample size and the simulated data sample size increase to infinity, which highlights to what extent the data discrepancy measure impacts the asymptotic pseudo-posterior. The result holds in the broad setting of IS-ABC methodologies, thus generalizing previous results that have been established only for rejection ABC algorithms. Furthermore, we propose a consistent V-statistic estimator of the energy statistic, under which we show that the large sample result holds, and prove that the rejection ABC algorithm, based on the energy statistic, generates pseudo-posterior distributions that achieves convergence to the correct limits, when implemented with rejection thresholds that converge to zero, in the finite sample setting. Our proposed energy statistic based ABC algorithm is demonstrated on a variety of models, including a Gaussian mixture, a moving-average model of order two, a bivariate beta and a multivariate $g$-and-$k$ distribution. We find that our proposed method compares well with alternative discrepancy measures.

</details>

<details>

<summary>2020-06-30 10:54:06 - R2-B2: Recursive Reasoning-Based Bayesian Optimization for No-Regret Learning in Games</summary>

- *Zhongxiang Dai, Yizhou Chen, Kian Hsiang Low, Patrick Jaillet, Teck-Hua Ho*

- `2006.16679v1` - [abs](http://arxiv.org/abs/2006.16679v1) - [pdf](http://arxiv.org/pdf/2006.16679v1)

> This paper presents a recursive reasoning formalism of Bayesian optimization (BO) to model the reasoning process in the interactions between boundedly rational, self-interested agents with unknown, complex, and costly-to-evaluate payoff functions in repeated games, which we call Recursive Reasoning-Based BO (R2-B2). Our R2-B2 algorithm is general in that it does not constrain the relationship among the payoff functions of different agents and can thus be applied to various types of games such as constant-sum, general-sum, and common-payoff games. We prove that by reasoning at level 2 or more and at one level higher than the other agents, our R2-B2 agent can achieve faster asymptotic convergence to no regret than that without utilizing recursive reasoning. We also propose a computationally cheaper variant of R2-B2 called R2-B2-Lite at the expense of a weaker convergence guarantee. The performance and generality of our R2-B2 algorithm are empirically demonstrated using synthetic games, adversarial machine learning, and multi-agent reinforcement learning.

</details>

<details>

<summary>2020-06-30 11:46:57 - Bayesian optimization with local search</summary>

- *Yuzhou Gao, Tengchao Yu, Jinglai Li*

- `1911.09159v3` - [abs](http://arxiv.org/abs/1911.09159v3) - [pdf](http://arxiv.org/pdf/1911.09159v3)

> Global optimization finds applications in a wide range of real world problems. The multi-start methods are a popular class of global optimization techniques, which are based on the ideas of conducting local searches at multiple starting points. In this work we propose a new multi-start algorithm where the starting points are determined in a Bayesian optimization framework. Specifically, the method can be understood as to construct a new function by conducting local searches of the original objective function, where the new function attains the same global optima as the original one. Bayesian optimization is then applied to find the global optima of the new local search defined function.

</details>

<details>

<summary>2020-06-30 16:18:16 - Tomographic Auto-Encoder: Unsupervised Bayesian Recovery of Corrupted Data</summary>

- *Francesco Tonolini, Pablo G. Moreno, Andreas Damianou, Roderick Murray-Smith*

- `2006.16938v1` - [abs](http://arxiv.org/abs/2006.16938v1) - [pdf](http://arxiv.org/pdf/2006.16938v1)

> We propose a new probabilistic method for unsupervised recovery of corrupted data. Given a large ensemble of degraded samples, our method recovers accurate posteriors of clean values, allowing the exploration of the manifold of possible reconstructed data and hence characterising the underlying uncertainty. In this setting, direct application of classical variational methods often gives rise to collapsed densities that do not adequately explore the solution space. Instead, we derive our novel reduced entropy condition approximate inference method that results in rich posteriors. We test our model in a data recovery task under the common setting of missing values and noise, demonstrating superior performance to existing variational methods for imputation and de-noising with different real data sets. We further show higher classification accuracy after imputation, proving the advantage of propagating uncertainty to downstream tasks with our model.

</details>

<details>

<summary>2020-06-30 16:42:04 - Bayesian inference for high-dimensional nonstationary Gaussian processes</summary>

- *Mark D. Risser, Daniel Turek*

- `1910.14101v2` - [abs](http://arxiv.org/abs/1910.14101v2) - [pdf](http://arxiv.org/pdf/1910.14101v2)

> In spite of the diverse literature on nonstationary spatial modeling and approximate Gaussian process (GP) methods, there are no general approaches for conducting fully Bayesian inference for moderately sized nonstationary spatial data sets on a personal laptop. For statisticians and data scientists who wish to learn about spatially-referenced data and conduct posterior inference and prediction with appropriate uncertainty quantification, the lack of such approaches and corresponding software is a significant limitation. In this paper, we develop methodology for implementing formal Bayesian inference for a general class of nonstationary GPs. Our novel approach uses pre-existing frameworks for characterizing nonstationarity in a new way that is applicable for small to moderately sized data sets via modern GP likelihood approximations. Posterior sampling is implemented using flexible MCMC methods, with nonstationary posterior prediction conducted as a post-processing step. We demonstrate our novel methods on two data sets, ranging from several hundred to several thousand locations, and compare our methodology with related statistical methods that provide off-the-shelf software. All of our methods are implemented in the freely available BayesNSGP software package for R.

</details>

<details>

<summary>2020-06-30 17:29:07 - When and where: estimating the date and location of introduction for exotic pests and pathogens</summary>

- *Trevor J. Hefley, Robin E. Russell, Anne E. Ballmann, Haoyu Zhang*

- `2006.16982v1` - [abs](http://arxiv.org/abs/2006.16982v1) - [pdf](http://arxiv.org/pdf/2006.16982v1)

> A fundamental question during the outbreak of a novel disease or invasion of an exotic pest is: At what location and date was it first introduced? With this information, future introductions can be anticipated and perhaps avoided. Point process models are commonly used for mapping species distribution and disease occurrence. If the time and location of introductions were known, then point process models could be used to map and understand the factors that influence introductions; however, rarely is the process of introduction directly observed. We propose embedding a point process within hierarchical Bayesian models commonly used to understand the spatio-temporal dynamics of invasion. Including a point process within a hierarchical Bayesian model enables inference regarding the location and date of introduction from indirect observation of the process such as species or disease occurrence records. We illustrate our approach using disease surveillance data collected to monitor white-nose syndrome, which is a fungal disease that threatens many North American species of bats. We use our model and surveillance data to estimate the location and date that the pathogen was introduced into the United States. Finally, we compare forecasts from our model to forecasts obtained from state-of-the-art regression-based statistical and machine learning methods. Our results show that the pathogen causing white-nose syndrome was most likely introduced into the United States 4 years prior to the first detection, but there is a moderate level of uncertainty in this estimate. The location of introduction could be up to 510 km east of the location of first discovery, but our results indicate that there is a relatively high probability the location of first detection could be the location of introduction.

</details>

<details>

<summary>2020-06-30 21:07:06 - Generalized propensity score approach to causal inference with spatial interference</summary>

- *Andrew Giffin, Brian Reich, Shu Yang, Ana Rappold*

- `2007.00106v1` - [abs](http://arxiv.org/abs/2007.00106v1) - [pdf](http://arxiv.org/pdf/2007.00106v1)

> Many spatial phenomena exhibit treatment interference where treatments at one location may affect the response at other locations. Because interference violates the stable unit treatment value assumption, standard methods for causal inference do not apply. We propose a new causal framework to recover direct and spill-over effects in the presence of spatial interference, taking into account that treatments at nearby locations are more influential than treatments at locations further apart. Under the no unmeasured confounding assumption, we show that a generalized propensity score is sufficient to remove all measured confounding. To reduce dimensionality issues, we propose a Bayesian spline-based regression model accounting for a sufficient set of variables for the generalized propensity score. A simulation study demonstrates the accuracy and coverage properties. We apply the method to estimate the causal effect of wildland fires on air pollution in the Western United States over 2005--2018.

</details>

<details>

<summary>2020-06-30 22:59:55 - Bayesian Graph Neural Networks with Adaptive Connection Sampling</summary>

- *Arman Hasanzadeh, Ehsan Hajiramezanali, Shahin Boluki, Mingyuan Zhou, Nick Duffield, Krishna Narayanan, Xiaoning Qian*

- `2006.04064v3` - [abs](http://arxiv.org/abs/2006.04064v3) - [pdf](http://arxiv.org/pdf/2006.04064v3)

> We propose a unified framework for adaptive connection sampling in graph neural networks (GNNs) that generalizes existing stochastic regularization methods for training GNNs. The proposed framework not only alleviates over-smoothing and over-fitting tendencies of deep GNNs, but also enables learning with uncertainty in graph analytic tasks with GNNs. Instead of using fixed sampling rates or hand-tuning them as model hyperparameters in existing stochastic regularization methods, our adaptive connection sampling can be trained jointly with GNN model parameters in both global and local fashions. GNN training with adaptive connection sampling is shown to be mathematically equivalent to an efficient approximation of training Bayesian GNNs. Experimental results with ablation studies on benchmark datasets validate that adaptively learning the sampling rate given graph training data is the key to boost the performance of GNNs in semi-supervised node classification, less prone to over-smoothing and over-fitting with more robust prediction.

</details>


## 2020-07

<details>

<summary>2020-07-01 03:16:58 - Optimal Experimental Design for Mathematical Models of Hematopoiesis</summary>

- *Luis Martinez Lomeli, Abdon Iniguez, Babak Shahbaba, John S Lowengrub, Vladimir Minin*

- `2004.09065v2` - [abs](http://arxiv.org/abs/2004.09065v2) - [pdf](http://arxiv.org/pdf/2004.09065v2)

> The hematopoietic system has a highly regulated and complex structure in which cells are organized to successfully create and maintain new blood cells. Feedback regulation is crucial to tightly control this system, but the specific mechanisms by which control is exerted are not completely understood. In this work, we aim to uncover the underlying mechanisms in hematopoiesis by conducting perturbation experiments, where animal subjects are exposed to an external agent in order to observe the system response and evolution. Developing a proper experimental design for these studies is an extremely challenging task. To address this issue, we have developed a novel Bayesian framework for optimal design of perturbation experiments. We model the numbers of hematopoietic stem and progenitor cells in mice that are exposed to a low dose of radiation. We use a differential equations model that accounts for feedback and feedforward regulation. A significant obstacle is that the experimental data are not longitudinal, rather each data point corresponds to a different animal. This model is embedded in a hierarchical framework with latent variables that capture unobserved cellular population levels. We select the optimum design based on the amount of information gain, measured by the Kullback-Leibler divergence between the probability distributions before and after observing the data. We evaluate our approach using synthetic and experimental data. We show that a proper design can lead to better estimates of model parameters even with relatively few subjects. Additionally, we demonstrate that the model parameters show a wide range of sensitivities to design options. Our method should allow scientists to find the optimal design by focusing on their specific parameters of interest and provide insight to hematopoiesis. Our approach can be extended to more complex models where latent components are used.

</details>

<details>

<summary>2020-07-01 03:18:38 - A benchmark study on reliable molecular supervised learning via Bayesian learning</summary>

- *Doyeong Hwang, Grace Lee, Hanseok Jo, Seyoul Yoon, Seongok Ryu*

- `2006.07021v2` - [abs](http://arxiv.org/abs/2006.07021v2) - [pdf](http://arxiv.org/pdf/2006.07021v2)

> Virtual screening aims to find desirable compounds from chemical library by using computational methods. For this purpose with machine learning, model outputs that can be interpreted as predictive probability will be beneficial, in that a high prediction score corresponds to high probability of correctness. In this work, we present a study on the prediction performance and reliability of graph neural networks trained with the recently proposed Bayesian learning algorithms. Our work shows that Bayesian learning algorithms allow well-calibrated predictions for various GNN architectures and classification tasks. Also, we show the implications of reliable predictions on virtual screening, where Bayesian learning may lead to higher success in finding hit compounds.

</details>

<details>

<summary>2020-07-01 04:43:38 - Popper's falsification and corroboration from the statistical perspectives</summary>

- *Youngjo Lee, Yudi Pawitan*

- `2007.00238v1` - [abs](http://arxiv.org/abs/2007.00238v1) - [pdf](http://arxiv.org/pdf/2007.00238v1)

> The role of probability appears unchallenged as the key measure of uncertainty, used among other things for practical induction in the empirical sciences. Yet, Popper was emphatic in his rejection of inductive probability and of the logical probability of hypotheses; furthermore, for him, the degree of corroboration cannot be a probability. Instead he proposed a deductive method of testing. In many ways this dialectic tension has many parallels in statistics, with the Bayesians on logico-inductive side vs the non-Bayesians or the frequentists on the other side. Simplistically Popper seems to be on the frequentist side, but recent synthesis on the non-Bayesian side might direct the Popperian views to a more nuanced destination. Logical probability seems perfectly suited to measure partial evidence or support, so what can we use if we are to reject it? For the past 100 years, statisticians have also developed a related concept called likelihood, which has played a central role in statistical modelling and inference. Remarkably, this Fisherian concept of uncertainty is largely unknown or at least severely under-appreciated in non-statistical literature. As a measure of corroboration, the likelihood satisfies the Popperian requirement that it is not a probability. Our aim is to introduce the likelihood and its recent extension via a discussion of two well-known logical fallacies in order to highlight that its lack of recognition may have led to unnecessary confusion in our discourse about falsification and corroboration of hypotheses. We highlight the 100 years of development of likelihood concepts. The year 2021 will mark the 100-year anniversary of the likelihood, so with this paper we wish it a long life and increased appreciation in non-statistical literature.

</details>

<details>

<summary>2020-07-01 10:31:16 - Can Global Optimization Strategy Outperform Myopic Strategy for Bayesian Parameter Estimation?</summary>

- *Juanping Zhu, Hairong Gu*

- `2007.00373v1` - [abs](http://arxiv.org/abs/2007.00373v1) - [pdf](http://arxiv.org/pdf/2007.00373v1)

> Bayesian adaptive inference is widely used in psychophysics to estimate psychometric parameters. Most applications used myopic one-step ahead strategy which only optimizes the immediate utility. The widely held expectation is that global optimization strategies that explicitly optimize over some horizon can largely improve the performance of the myopic strategy. With limited studies that compared myopic and global strategies, the expectation was not challenged and researchers are still investing heavily to achieve global optimization. Is that really worthwhile? This paper provides a discouraging answer based on experimental simulations comparing the performance improvement and computation burden between global and myopic strategies in parameter estimation of multiple models. The finding is that the added horizon in global strategies has negligible contributions to the improvement of optimal global utility other than the most immediate next steps (of myopic strategy). Mathematical recursion is derived to prove that the contribution of utility improvement of each added horizon step diminishes fast as that step moves further into the future.

</details>

<details>

<summary>2020-07-01 11:47:15 - Sequential Bayesian optimal experimental design for structural reliability analysis</summary>

- *Christian Agrell, Kristina Rognlien Dahl*

- `2007.00402v1` - [abs](http://arxiv.org/abs/2007.00402v1) - [pdf](http://arxiv.org/pdf/2007.00402v1)

> Structural reliability analysis is concerned with estimation of the probability of a critical event taking place, described by $P(g(\textbf{X}) \leq 0)$ for some $n$-dimensional random variable $\textbf{X}$ and some real-valued function $g$. In many applications the function $g$ is practically unknown, as function evaluation involves time consuming numerical simulation or some other form of experiment that is expensive to perform. The problem we address in this paper is how to optimally design experiments, in a Bayesian decision theoretic fashion, when the goal is to estimate the probability $P(g(\textbf{X}) \leq 0)$ using a minimal amount of resources. As opposed to existing methods that have been proposed for this purpose, we consider a general structural reliability model given in hierarchical form. We therefore introduce a general formulation of the experimental design problem, where we distinguish between the uncertainty related to the random variable $\textbf{X}$ and any additional epistemic uncertainty that we want to reduce through experimentation. The effectiveness of a design strategy is evaluated through a measure of residual uncertainty, and efficient approximation of this quantity is crucial if we want to apply algorithms that search for an optimal strategy. The method we propose is based on importance sampling combined with the unscented transform for epistemic uncertainty propagation. We implement this for the myopic (one-step look ahead) alternative, and demonstrate the effectiveness through a series of numerical experiments.

</details>

<details>

<summary>2020-07-01 13:25:21 - Sequential Cooperative Bayesian Inference</summary>

- *Junqi Wang, Pei Wang, Patrick Shafto*

- `2002.05706v3` - [abs](http://arxiv.org/abs/2002.05706v3) - [pdf](http://arxiv.org/pdf/2002.05706v3)

> Cooperation is often implicitly assumed when learning from other agents. Cooperation implies that the agent selecting the data, and the agent learning from the data, have the same goal, that the learner infer the intended hypothesis. Recent models in human and machine learning have demonstrated the possibility of cooperation. We seek foundational theoretical results for cooperative inference by Bayesian agents through sequential data. We develop novel approaches analyzing consistency, rate of convergence and stability of Sequential Cooperative Bayesian Inference (SCBI). Our analysis of the effectiveness, sample efficiency and robustness show that cooperation is not only possible in specific instances but theoretically well-founded in general. We discuss implications for human-human and human-machine cooperation.

</details>

<details>

<summary>2020-07-01 19:16:51 - Inferential Induction: A Novel Framework for Bayesian Reinforcement Learning</summary>

- *Hannes Eriksson, Emilio Jorge, Christos Dimitrakakis, Debabrota Basu, Divya Grover*

- `2002.03098v2` - [abs](http://arxiv.org/abs/2002.03098v2) - [pdf](http://arxiv.org/pdf/2002.03098v2)

> Bayesian reinforcement learning (BRL) offers a decision-theoretic solution for reinforcement learning. While "model-based" BRL algorithms have focused either on maintaining a posterior distribution on models or value functions and combining this with approximate dynamic programming or tree search, previous Bayesian "model-free" value function distribution approaches implicitly make strong assumptions or approximations. We describe a novel Bayesian framework, Inferential Induction, for correctly inferring value function distributions from data, which leads to the development of a new class of BRL algorithms. We design an algorithm, Bayesian Backwards Induction, with this framework. We experimentally demonstrate that the proposed algorithm is competitive with respect to the state of the art.

</details>

<details>

<summary>2020-07-01 22:40:40 - Bayesian Multivariate Quantile Regression Using Dependent Dirichlet Process Prior</summary>

- *Indrabati Bhattacharya, Subhashis Ghosal*

- `2007.00797v1` - [abs](http://arxiv.org/abs/2007.00797v1) - [pdf](http://arxiv.org/pdf/2007.00797v1)

> In this article, we consider a non-parametric Bayesian approach to multivariate quantile regression. The collection of related conditional distributions of a response vector Y given a univariate covariate X is modeled using a Dependent Dirichlet Process (DDP) prior. The DDP is used to introduce dependence across x. As the realizations from a Dirichlet process prior are almost surely discrete, we need to convolve it with a kernel. To model the error distribution as flexibly as possible, we use a countable mixture of multidimensional normal distributions as our kernel. For posterior computations, we use a truncated stick-breaking representation of the DDP. This approximation enables us to deal with only a finitely number of parameters. We use a Block Gibbs sampler for estimating the model parameters. We illustrate our method with simulation studies and real data applications. Finally, we provide a theoretical justification for the proposed method through posterior consistency. Our proposed procedure is new even when the response is univariate.

</details>

<details>

<summary>2020-07-02 04:30:47 - ε-BMC: A Bayesian Ensemble Approach to Epsilon-Greedy Exploration in Model-Free Reinforcement Learning</summary>

- *Michael Gimelfarb, Scott Sanner, Chi-Guhn Lee*

- `2007.00869v1` - [abs](http://arxiv.org/abs/2007.00869v1) - [pdf](http://arxiv.org/pdf/2007.00869v1)

> Resolving the exploration-exploitation trade-off remains a fundamental problem in the design and implementation of reinforcement learning (RL) algorithms. In this paper, we focus on model-free RL using the epsilon-greedy exploration policy, which despite its simplicity, remains one of the most frequently used forms of exploration. However, a key limitation of this policy is the specification of $\varepsilon$. In this paper, we provide a novel Bayesian perspective of $\varepsilon$ as a measure of the uniformity of the Q-value function. We introduce a closed-form Bayesian model update based on Bayesian model combination (BMC), based on this new perspective, which allows us to adapt $\varepsilon$ using experiences from the environment in constant time with monotone convergence guarantees. We demonstrate that our proposed algorithm, $\varepsilon$-\texttt{BMC}, efficiently balances exploration and exploitation on different problems, performing comparably or outperforming the best tuned fixed annealing schedules and an alternative data-dependent $\varepsilon$ adaptation scheme proposed in the literature.

</details>

<details>

<summary>2020-07-02 07:35:49 - BOSH: Bayesian Optimization by Sampling Hierarchically</summary>

- *Henry B. Moss, David S. Leslie, Paul Rayson*

- `2007.00939v1` - [abs](http://arxiv.org/abs/2007.00939v1) - [pdf](http://arxiv.org/pdf/2007.00939v1)

> Deployments of Bayesian Optimization (BO) for functions with stochastic evaluations, such as parameter tuning via cross validation and simulation optimization, typically optimize an average of a fixed set of noisy realizations of the objective function. However, disregarding the true objective function in this manner finds a high-precision optimum of the wrong function. To solve this problem, we propose Bayesian Optimization by Sampling Hierarchically (BOSH), a novel BO routine pairing a hierarchical Gaussian process with an information-theoretic framework to generate a growing pool of realizations as the optimization progresses. We demonstrate that BOSH provides more efficient and higher-precision optimization than standard BO across synthetic benchmarks, simulation optimization, reinforcement learning and hyper-parameter tuning tasks.

</details>

<details>

<summary>2020-07-02 12:01:43 - Spatio-Temporal Change of Support Modeling with R</summary>

- *Andrew M. Raim, Scott H. Holan, Jonathan R. Bradley, Christopher K. Wikle*

- `1904.12092v3` - [abs](http://arxiv.org/abs/1904.12092v3) - [pdf](http://arxiv.org/pdf/1904.12092v3)

> Spatio-temporal change of support methods are designed for statistical analysis on spatial and temporal domains which can differ from those of the observed data. Previous work introduced a parsimonious class of Bayesian hierarchical spatio-temporal models, which we refer to as STCOS, for the case of Gaussian outcomes. Application of STCOS methodology from this literature requires a level of proficiency with spatio-temporal methods and statistical computing which may be a hurdle for potential users. The present work seeks to bridge this gap by guiding readers through STCOS computations. We focus on the R computing environment because of its popularity, free availability, and high quality contributed packages. The stcos package is introduced to facilitate computations for the STCOS model. A motivating application is the American Community Survey (ACS), an ongoing survey administered by the U.S. Census Bureau that measures key socioeconomic and demographic variables for various populations in the United States. The STCOS methodology offers a principled approach to compute model-based estimates and associated measures of uncertainty for ACS variables on customized geographies and/or time periods. We present a detailed case study with ACS data as a guide for change of support analysis in R, and as a foundation which can be customized to other applications.

</details>

<details>

<summary>2020-07-02 13:03:09 - Accurate Characterization of Non-Uniformly Sampled Time Series using Stochastic Differential Equations</summary>

- *Stijn de Waele*

- `2007.01073v1` - [abs](http://arxiv.org/abs/2007.01073v1) - [pdf](http://arxiv.org/pdf/2007.01073v1)

> Non-uniform sampling arises when an experimenter does not have full control over the sampling characteristics of the process under investigation. Moreover, it is introduced intentionally in algorithms such as Bayesian optimization and compressive sensing. We argue that Stochastic Differential Equations (SDEs) are especially well-suited for characterizing second order moments of such time series. We introduce new initial estimates for the numerical optimization of the likelihood, based on incremental estimation and initialization from autoregressive models. Furthermore, we introduce model truncation as a purely data-driven method to reduce the order of the estimated model based on the SDE likelihood. We show the increased accuracy achieved with the new estimator in simulation experiments, covering all challenging circumstances that may be encountered in characterizing a non-uniformly sampled time series. Finally, we apply the new estimator to experimental rainfall variability data.

</details>

<details>

<summary>2020-07-02 13:47:36 - Adversarial Neural Pruning with Latent Vulnerability Suppression</summary>

- *Divyam Madaan, Jinwoo Shin, Sung Ju Hwang*

- `1908.04355v4` - [abs](http://arxiv.org/abs/1908.04355v4) - [pdf](http://arxiv.org/pdf/1908.04355v4)

> Despite the remarkable performance of deep neural networks on various computer vision tasks, they are known to be susceptible to adversarial perturbations, which makes it challenging to deploy them in real-world safety-critical applications. In this paper, we conjecture that the leading cause of adversarial vulnerability is the distortion in the latent feature space, and provide methods to suppress them effectively. Explicitly, we define \emph{vulnerability} for each latent feature and then propose a new loss for adversarial learning, \emph{Vulnerability Suppression (VS)} loss, that aims to minimize the feature-level vulnerability during training. We further propose a Bayesian framework to prune features with high vulnerability to reduce both vulnerability and loss on adversarial samples. We validate our \emph{Adversarial Neural Pruning with Vulnerability Suppression (ANP-VS)} method on multiple benchmark datasets, on which it not only obtains state-of-the-art adversarial robustness but also improves the performance on clean examples, using only a fraction of the parameters used by the full network. Further qualitative analysis suggests that the improvements come from the suppression of feature-level vulnerability.

</details>

<details>

<summary>2020-07-02 19:16:47 - Continuous-Time Bayesian Networks with Clocks</summary>

- *Nicolai Engelmann, Dominik Linzner, Heinz Koeppl*

- `2007.00347v2` - [abs](http://arxiv.org/abs/2007.00347v2) - [pdf](http://arxiv.org/pdf/2007.00347v2)

> Structured stochastic processes evolving in continuous time present a widely adopted framework to model phenomena occurring in nature and engineering. However, such models are often chosen to satisfy the Markov property to maintain tractability. One of the more popular of such memoryless models are Continuous Time Bayesian Networks (CTBNs). In this work, we lift its restriction to exponential survival times to arbitrary distributions. Current extensions achieve this via auxiliary states, which hinder tractability. To avoid that, we introduce a set of node-wise clocks to construct a collection of graph-coupled semi-Markov chains. We provide algorithms for parameter and structure inference, which make use of local dependencies and conduct experiments on synthetic data and a data-set generated through a benchmark tool for gene regulatory networks. In doing so, we point out advantages compared to current CTBN extensions.

</details>

<details>

<summary>2020-07-02 20:14:34 - Epidemiology of exposure to mixtures: we cant be casual about causality when using or testing methods</summary>

- *Thomas F. Webster, Marc G. Weisskopf*

- `2007.01370v1` - [abs](http://arxiv.org/abs/2007.01370v1) - [pdf](http://arxiv.org/pdf/2007.01370v1)

> Background: There is increasing interest in approaches for analyzing the effect of exposure mixtures on health. A key issue is how to simultaneously analyze often highly collinear components of the mixture, which can create problems such as confounding by co-exposure and co-exposure amplification bias (CAB). Evaluation of novel mixtures methods, typically using synthetic data, is critical to their ultimate utility. Objectives: This paper aims to answer two questions. How do causal models inform the interpretation of statistical models and the creation of synthetic data used to test them? Are novel mixtures methods susceptible to CAB? Methods: We use directed acyclic graphs (DAGs) and linear models to derive closed form solutions for model parameters to examine how underlying causal assumptions affect the interpretation of model results. Results: The same beta coefficients estimated by a statistical model can have different interpretations depending on the assumed causal structure. Similarly, the method used to simulate data can have implications for the underlying DAG (and vice versa), and therefore the identification of the parameter being estimated with an analytic approach. We demonstrate that methods that can reproduce results of linear regression, such as Bayesian kernel machine regression and the new quantile g-computation approach, will be subject to CAB. However, under some conditions, estimates of an overall effect of the mixture is not subject to CAB and even has reduced uncontrolled bias. Discussion: Just as DAGs encode a priori subject matter knowledge allowing identification of variable control needed to block analytic bias, we recommend explicitly identifying DAGs underlying synthetic data created to test statistical mixtures approaches. Estimates of the total effect of a mixture is an important but relatively underexplored topic that warrants further investigation.

</details>

<details>

<summary>2020-07-02 20:44:07 - Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints</summary>

- *C. P. Andriotis, K. G. Papakonstantinou*

- `2007.01380v1` - [abs](http://arxiv.org/abs/2007.01380v1) - [pdf](http://arxiv.org/pdf/2007.01380v1)

> Determination of inspection and maintenance policies for minimizing long-term risks and costs in deteriorating engineering environments constitutes a complex optimization problem. Major computational challenges include the (i) curse of dimensionality, due to exponential scaling of state/action set cardinalities with the number of components; (ii) curse of history, related to exponentially growing decision-trees with the number of decision-steps; (iii) presence of state uncertainties, induced by inherent environment stochasticity and variability of inspection/monitoring measurements; (iv) presence of constraints, pertaining to stochastic long-term limitations, due to resource scarcity and other infeasible/undesirable system responses. In this work, these challenges are addressed within a joint framework of constrained Partially Observable Markov Decision Processes (POMDP) and multi-agent Deep Reinforcement Learning (DRL). POMDPs optimally tackle (ii)-(iii), combining stochastic dynamic programming with Bayesian inference principles. Multi-agent DRL addresses (i), through deep function parametrizations and decentralized control assumptions. Challenge (iv) is herein handled through proper state augmentation and Lagrangian relaxation, with emphasis on life-cycle risk-based constraints and budget limitations. The underlying algorithmic steps are provided, and the proposed framework is found to outperform well-established policy baselines and facilitate adept prescription of inspection and intervention actions, in cases where decisions must be made in the most resource- and risk-aware manner.

</details>

<details>

<summary>2020-07-02 21:00:15 - Semiparametric Bayesian Inference for the Transmission Dynamics of COVID-19 with a State-Space Model</summary>

- *Tianjian Zhou, Yuan Ji*

- `2006.05581v2` - [abs](http://arxiv.org/abs/2006.05581v2) - [pdf](http://arxiv.org/pdf/2006.05581v2)

> The outbreak of Coronavirus Disease 2019 (COVID-19) is an ongoing pandemic affecting over 200 countries and regions. Inference about the transmission dynamics of COVID-19 can provide important insights into the speed of disease spread and the effects of mitigation policies. We develop a novel Bayesian approach to such inference based on a probabilistic compartmental model using data of daily confirmed COVID-19 cases. In particular, we consider a probabilistic extension of the classical susceptible-infectious-recovered model, which takes into account undocumented infections and allows the epidemiological parameters to vary over time. We estimate the disease transmission rate via a Gaussian process prior, which captures nonlinear changes over time without the need of specific parametric assumptions. We utilize a parallel-tempering Markov chain Monte Carlo algorithm to efficiently sample from the highly correlated posterior space. Predictions for future observations are done by sampling from their posterior predictive distributions. Performance of the proposed approach is assessed using simulated datasets. Finally, our approach is applied to COVID-19 data from four states of the United States: Washington, New York, California, and Illinois. An R package BaySIR is made available at https://github.com/tianjianzhou/BaySIR for the public to conduct independent analysis or reproduce the results in this paper.

</details>

<details>

<summary>2020-07-02 22:18:12 - How Good is the Bayes Posterior in Deep Neural Networks Really?</summary>

- *Florian Wenzel, Kevin Roth, Bastiaan S. Veeling, Jakub Świątkowski, Linh Tran, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, Sebastian Nowozin*

- `2002.02405v2` - [abs](http://arxiv.org/abs/2002.02405v2) - [pdf](http://arxiv.org/pdf/2002.02405v2)

> During the past five years the Bayesian deep learning community has developed increasingly accurate and efficient approximate inference procedures that allow for Bayesian inference in deep neural networks. However, despite this algorithmic progress and the promise of improved uncertainty quantification and sample efficiency there are---as of early 2020---no publicized deployments of Bayesian neural networks in industrial practice. In this work we cast doubt on the current understanding of Bayes posteriors in popular deep neural networks: we demonstrate through careful MCMC sampling that the posterior predictive induced by the Bayes posterior yields systematically worse predictions compared to simpler methods including point estimates obtained from SGD. Furthermore, we demonstrate that predictive performance is improved significantly through the use of a "cold posterior" that overcounts evidence. Such cold posteriors sharply deviate from the Bayesian paradigm but are commonly used as heuristic in Bayesian deep learning papers. We put forward several hypotheses that could explain cold posteriors and evaluate the hypotheses through experiments. Our work questions the goal of accurate posterior approximations in Bayesian deep learning: If the true Bayes posterior is poor, what is the use of more accurate approximations? Instead, we argue that it is timely to focus on understanding the origin of the improved performance of cold posteriors.

</details>

<details>

<summary>2020-07-03 03:44:58 - BAGEL: A Bayesian Graphical Model for Inferring Drug Effect Longitudinally on Depression in People with HIV</summary>

- *Yuliang Li, Yang Ni, Leah H. Rubin, Amanda B. Spence, Yanxun Xu*

- `2007.01484v1` - [abs](http://arxiv.org/abs/2007.01484v1) - [pdf](http://arxiv.org/pdf/2007.01484v1)

> Access and adherence to antiretroviral therapy (ART) has transformed the face of HIV infection from a fatal to a chronic disease. However, ART is also known for its side effects. Studies have reported that ART is associated with depressive symptomatology. Large-scale HIV clinical databases with individuals' longitudinal depression records, ART medications, and clinical characteristics offer researchers unprecedented opportunities to study the effects of ART drugs on depression over time. We develop BAGEL, a Bayesian graphical model to investigate longitudinal effects of ART drugs on a range of depressive symptoms while adjusting for participants' demographic, behavior, and clinical characteristics, and taking into account the heterogeneous population through a Bayesian nonparametric prior. We evaluate BAGEL through simulation studies. Application to a dataset from the Women's Interagency HIV Study yields interpretable and clinically useful results. BAGEL not only can improve our understanding of ART drugs effects on disparate depression symptoms, but also has clinical utility in guiding informed and effective treatment selection to facilitate precision medicine in HIV.

</details>

<details>

<summary>2020-07-03 03:58:53 - Transformations in Semi-Parametric Bayesian Synthetic Likelihood</summary>

- *Jacob W. Priddle, Christopher Drovandi*

- `2007.01485v1` - [abs](http://arxiv.org/abs/2007.01485v1) - [pdf](http://arxiv.org/pdf/2007.01485v1)

> Bayesian synthetic likelihood (BSL) is a popular method for performing approximate Bayesian inference when the likelihood function is intractable. In synthetic likelihood methods, the likelihood function is approximated parametrically via model simulations, and then standard likelihood-based techniques are used to perform inference. The Gaussian synthetic likelihood estimator has become ubiquitous in BSL literature, primarily for its simplicity and ease of implementation. However, it is often too restrictive and may lead to poor posterior approximations. Recently, a more flexible semi-parametric Bayesian synthetic likelihood (semiBSL) estimator has been introduced, which is significantly more robust to irregularly distributed summary statistics. In this work, we propose a number of extensions to semiBSL. First, we consider even more flexible estimators of the marginal distributions using transformation kernel density estimation. Second, we propose whitening semiBSL (wsemiBSL) -- a method to significantly improve the computational efficiency of semiBSL. wsemiBSL uses an approximate whitening transformation to decorrelate summary statistics at each algorithm iteration. The methods developed herein significantly improve the versatility and efficiency of BSL algorithms.

</details>

<details>

<summary>2020-07-03 13:30:50 - Stochastic Variational Bayesian Inference for a Nonlinear Forward Model</summary>

- *Michael A. Chappell, Martin S. Craig, Mark W. Woolrich*

- `2007.01675v1` - [abs](http://arxiv.org/abs/2007.01675v1) - [pdf](http://arxiv.org/pdf/2007.01675v1)

> Variational Bayes (VB) has been used to facilitate the calculation of the posterior distribution in the context of Bayesian inference of the parameters of nonlinear models from data. Previously an analytical formulation of VB has been derived for nonlinear model inference on data with additive gaussian noise as an alternative to nonlinear least squares. Here a stochastic solution is derived that avoids some of the approximations required of the analytical formulation, offering a solution that can be more flexibly deployed for nonlinear model inference problems. The stochastic VB solution was used for inference on a biexponential toy case and the algorithmic parameter space explored, before being deployed on real data from a magnetic resonance imaging study of perfusion. The new method was found to achieve comparable parameter recovery to the analytic solution and be competitive in terms of computational speed despite being reliant on sampling.

</details>

<details>

<summary>2020-07-03 14:40:56 - Qualitative Analysis of Monte Carlo Dropout</summary>

- *Ronald Seoh*

- `2007.01720v1` - [abs](http://arxiv.org/abs/2007.01720v1) - [pdf](http://arxiv.org/pdf/2007.01720v1)

> In this report, we present qualitative analysis of Monte Carlo (MC) dropout method for measuring model uncertainty in neural network (NN) models. We first consider the sources of uncertainty in NNs, and briefly review Bayesian Neural Networks (BNN), the group of Bayesian approaches to tackle uncertainties in NNs. After presenting mathematical formulation of MC dropout, we proceed to suggesting potential benefits and associated costs for using MC dropout in typical NN models, with the results from our experiments.

</details>

<details>

<summary>2020-07-03 21:04:51 - An Early Warning Approach to Monitor COVID-19 Activity with Multiple Digital Traces in Near Real-Time</summary>

- *Nicole E. Kogan, Leonardo Clemente, Parker Liautaud, Justin Kaashoek, Nicholas B. Link, Andre T. Nguyen, Fred S. Lu, Peter Huybers, Bernd Resch, Clemens Havas, Andreas Petutschnig, Jessica Davis, Matteo Chinazzi, Backtosch Mustafa, William P. Hanage, Alessandro Vespignani, Mauricio Santillana*

- `2007.00756v2` - [abs](http://arxiv.org/abs/2007.00756v2) - [pdf](http://arxiv.org/pdf/2007.00756v2)

> Non-pharmaceutical interventions (NPIs) have been crucial in curbing COVID-19 in the United States (US). Consequently, relaxing NPIs through a phased re-opening of the US amid still-high levels of COVID-19 susceptibility could lead to new epidemic waves. This calls for a COVID-19 early warning system. Here we evaluate multiple digital data streams as early warning indicators of increasing or decreasing state-level US COVID-19 activity between January and June 2020. We estimate the timing of sharp changes in each data stream using a simple Bayesian model that calculates in near real-time the probability of exponential growth or decay. Analysis of COVID-19-related activity on social network microblogs, Internet searches, point-of-care medical software, and a metapopulation mechanistic model, as well as fever anomalies captured by smart thermometer networks, shows exponential growth roughly 2-3 weeks prior to comparable growth in confirmed COVID-19 cases and 3-4 weeks prior to comparable growth in COVID-19 deaths across the US over the last 6 months. We further observe exponential decay in confirmed cases and deaths 5-6 weeks after implementation of NPIs, as measured by anonymized and aggregated human mobility data from mobile phones. Finally, we propose a combined indicator for exponential growth in multiple data streams that may aid in developing an early warning system for future COVID-19 outbreaks. These efforts represent an initial exploratory framework, and both continued study of the predictive power of digital indicators as well as further development of the statistical approach are needed.

</details>

<details>

<summary>2020-07-04 20:52:20 - Learning Behavioral Representations from Wearable Sensors</summary>

- *Nazgol Tavabi, Homa Hosseinmardi, Jennifer L. Villatte, Andrés Abeliuk, Shrikanth Narayanan, Emilio Ferrara, Kristina Lerman*

- `1911.06959v2` - [abs](http://arxiv.org/abs/1911.06959v2) - [pdf](http://arxiv.org/pdf/1911.06959v2)

> Continuous collection of physiological data from wearable sensors enables temporal characterization of individual behaviors. Understanding the relation between an individual's behavioral patterns and psychological states can help identify strategies to improve quality of life. One challenge in analyzing physiological data is extracting the underlying behavioral states from the temporal sensor signals and interpreting them. Here, we use a non-parametric Bayesian approach to model sensor data from multiple people and discover the dynamic behaviors they share. We apply this method to data collected from sensors worn by a population of hospital workers and show that the learned states can cluster participants into meaningful groups and better predict their cognitive and psychological states. This method offers a way to learn interpretable compact behavioral representations from multivariate sensor signals.

</details>

<details>

<summary>2020-07-05 01:36:50 - Geographically Weighted Regression Analysis for Spatial Economics Data: a Bayesian Recourse</summary>

- *Zhihua Ma, Yishu Xue, Guanyu Hu*

- `2007.02222v1` - [abs](http://arxiv.org/abs/2007.02222v1) - [pdf](http://arxiv.org/pdf/2007.02222v1)

> The geographically weighted regression (GWR) is a well-known statistical approach to explore spatial non-stationarity of the regression relationship in spatial data analysis. In this paper, we discuss a Bayesian recourse of GWR. Bayesian variable selection based on spike-and-slab prior, bandwidth selection based on range prior, and model assessment using a modified deviance information criterion and a modified logarithm of pseudo-marginal likelihood are fully discussed in this paper. Usage of the graph distance in modeling areal data is also introduced. Extensive simulation studies are carried out to examine the empirical performance of the proposed methods with both small and large number of location scenarios, and comparison with the classical frequentist GWR is made. The performance of variable selection and estimation of the proposed methodology under different circumstances are satisfactory. We further apply the proposed methodology in analysis of a province-level macroeconomic data of 30 selected provinces in China. The estimation and variable selection results reveal insights about China's economy that are convincing and agree with previous studies and facts.

</details>

<details>

<summary>2020-07-05 02:50:52 - Bayesian Hierarchical Spatial Regression Models for Spatial Data in the Presence of Missing Covariates with Applications</summary>

- *Zhihua Ma, Guanyu Hu, Ming-Hui Chen*

- `2007.02228v1` - [abs](http://arxiv.org/abs/2007.02228v1) - [pdf](http://arxiv.org/pdf/2007.02228v1)

> In many applications, survey data are collected from different survey centers in different regions. It happens that in some circumstances, response variables are completely observed while the covariates have missing values. In this paper, we propose a joint spatial regression model for the response variable and missing covariates via a sequence of one-dimensional conditional spatial regression models. We further construct a joint spatial model for missing covariate data mechanisms. The properties of the proposed models are examined and a Markov chain Monte Carlo sampling algorithm is used to sample from the posterior distribution. In addition, the Bayesian model comparison criteria, the modified Deviance Information Criterion (mDIC) and the modified Logarithm of the Pseudo-Marginal Likelihood (mLPML), are developed to assess the fit of spatial regression models for spatial data. Extensive simulation studies are carried out to examine empirical performance of the proposed methods. We further apply the proposed methodology to analyze a real data set from a Chinese Health and Nutrition Survey (CHNS) conducted in 2011.

</details>

<details>

<summary>2020-07-05 10:11:30 - Confidence intervals with maximal average power</summary>

- *Christian Bartels, Johanna Mielke, Ekkehard Glimm*

- `1905.03981v3` - [abs](http://arxiv.org/abs/1905.03981v3) - [pdf](http://arxiv.org/pdf/1905.03981v3)

> We propose a frequentist testing procedure that maintains a defined coverage and is optimal in the sense that it gives maximal power to detect deviations from a null hypothesis when the alternative to the null hypothesis is sampled from a pre-specified distribution (the prior distribution). Selecting a prior distribution allows to tune the decision rule. This leads to an increased power, if the true data generating distribution happens to be compatible with the prior. It comes at the cost of losing power, if the data generating distribution or the observed data are incompatible with the prior. We illustrate the proposed approach for a binomial experiment, which is sufficiently simple such that the decision sets can be illustrated in figures, which should facilitate an intuitive understanding. The potential beyond the simple example will be discussed: the approach is generic in that the test is defined based on the likelihood function and the prior only. It is comparatively simple to implement and efficient to execute, since it does not rely on Minimax optimization. Conceptually it is interesting to note that for constructing the testing procedure the Bayesian posterior probability distribution is used.

</details>

<details>

<summary>2020-07-05 17:04:41 - Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs</summary>

- *Meng Qu, Tianyu Gao, Louis-Pascal A. C. Xhonneux, Jian Tang*

- `2007.02387v1` - [abs](http://arxiv.org/abs/2007.02387v1) - [pdf](http://arxiv.org/pdf/2007.02387v1)

> This paper studies few-shot relation extraction, which aims at predicting the relation for a pair of entities in a sentence by training with a few labeled examples in each relation. To more effectively generalize to new relations, in this paper we study the relationships between different relations and propose to leverage a global relation graph. We propose a novel Bayesian meta-learning approach to effectively learn the posterior distribution of the prototype vectors of relations, where the initial prior of the prototype vectors is parameterized with a graph neural network on the global relation graph. Moreover, to effectively optimize the posterior distribution of the prototype vectors, we propose to use the stochastic gradient Langevin dynamics, which is related to the MAML algorithm but is able to handle the uncertainty of the prototype vectors. The whole framework can be effectively and efficiently optimized in an end-to-end fashion. Experiments on two benchmark datasets prove the effectiveness of our proposed approach against competitive baselines in both the few-shot and zero-shot settings.

</details>

<details>

<summary>2020-07-05 19:05:09 - The k-tied Normal Distribution: A Compact Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks</summary>

- *Jakub Swiatkowski, Kevin Roth, Bastiaan S. Veeling, Linh Tran, Joshua V. Dillon, Jasper Snoek, Stephan Mandt, Tim Salimans, Rodolphe Jenatton, Sebastian Nowozin*

- `2002.02655v2` - [abs](http://arxiv.org/abs/2002.02655v2) - [pdf](http://arxiv.org/pdf/2002.02655v2)

> Variational Bayesian Inference is a popular methodology for approximating posterior distributions over Bayesian neural network weights. Recent work developing this class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, here we share a curious experimental finding that suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian neural networks trained using Gaussian mean-field variational inference, we find that the posterior standard deviations consistently exhibit strong low-rank structure after convergence. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing the models' performance. Furthermore, we find that such factorized parameterizations improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence.

</details>

<details>

<summary>2020-07-06 09:59:20 - Adversarial Risk Analysis (Overview)</summary>

- *David Banks, Víctor Gallego, Roi Naveiro, David Ríos Insua*

- `2007.02613v1` - [abs](http://arxiv.org/abs/2007.02613v1) - [pdf](http://arxiv.org/pdf/2007.02613v1)

> Adversarial risk analysis (ARA) is a relatively new area of research that informs decision-making when facing intelligent opponents and uncertain outcomes. It enables an analyst to express her Bayesian beliefs about an opponent's utilities, capabilities, probabilities and the type of strategic calculation that the opponent is using. Within that framework, the analyst then solves the problem from the perspective of the opponent while placing subjective probability distributions on all unknown quantities. This produces a distribution over the actions of the opponent that permits the analyst to maximize her expected utility. This overview covers conceptual, modeling, computational and applied issues in ARA.

</details>

<details>

<summary>2020-07-06 13:03:31 - A review of spatial causal inference methods for environmental and epidemiological applications</summary>

- *Brian J Reich, Shu Yang, Yawen Guan, Andrew B Giffin, Matthew J Miller, Ana G Rappold*

- `2007.02714v1` - [abs](http://arxiv.org/abs/2007.02714v1) - [pdf](http://arxiv.org/pdf/2007.02714v1)

> The scientific rigor and computational methods of causal inference have had great impacts on many disciplines, but have only recently begun to take hold in spatial applications. Spatial casual inference poses analytic challenges due to complex correlation structures and interference between the treatment at one location and the outcomes at others. In this paper, we review the current literature on spatial causal inference and identify areas of future work. We first discuss methods that exploit spatial structure to account for unmeasured confounding variables. We then discuss causal analysis in the presence of spatial interference including several common assumptions used to reduce the complexity of the interference patterns under consideration. These methods are extended to the spatiotemporal case where we compare and contrast the potential outcomes framework with Granger causality, and to geostatistical analyses involving spatial random fields of treatments and responses. The methods are introduced in the context of observational environmental and epidemiological studies, and are compared using both a simulation study and analysis of the effect of ambient air pollution on COVID-19 mortality rate. Code to implement many of the methods using the popular Bayesian software OpenBUGS is provided.

</details>

<details>

<summary>2020-07-06 15:34:03 - Solving Bayesian Network Structure Learning Problem with Integer Linear Programming</summary>

- *Ronald Seoh*

- `2007.02829v1` - [abs](http://arxiv.org/abs/2007.02829v1) - [pdf](http://arxiv.org/pdf/2007.02829v1)

> This dissertation investigates integer linear programming (ILP) formulation of Bayesian Network structure learning problem. We review the definition and key properties of Bayesian network and explain score metrics used to measure how well certain Bayesian network structure fits the dataset. We outline the integer linear programming formulation based on the decomposability of score metrics. In order to ensure acyclicity of the structure, we add ``cluster constraints'' developed specifically for Bayesian network, in addition to cycle constraints applicable to directed acyclic graphs in general. Since there would be exponential number of these constraints if we specify them fully, we explain the methods to add them as cutting planes without declaring them all in the initial model. Also, we develop a heuristic algorithm that finds a feasible solution based on the idea of sink node on directed acyclic graphs. We implemented the ILP formulation and cutting planes as a \textsf{Python} package, and present the results of experiments with different settings on reference datasets.

</details>

<details>

<summary>2020-07-06 16:17:13 - Wide Neural Networks with Bottlenecks are Deep Gaussian Processes</summary>

- *Devanshu Agrawal, Theodore Papamarkou, Jacob Hinkle*

- `2001.00921v3` - [abs](http://arxiv.org/abs/2001.00921v3) - [pdf](http://arxiv.org/pdf/2001.00921v3)

> There has recently been much work on the "wide limit" of neural networks, where Bayesian neural networks (BNNs) are shown to converge to a Gaussian process (GP) as all hidden layers are sent to infinite width. However, these results do not apply to architectures that require one or more of the hidden layers to remain narrow. In this paper, we consider the wide limit of BNNs where some hidden layers, called "bottlenecks", are held at finite width. The result is a composition of GPs that we term a "bottleneck neural network Gaussian process" (bottleneck NNGP). Although intuitive, the subtlety of the proof is in showing that the wide limit of a composition of networks is in fact the composition of the limiting GPs. We also analyze theoretically a single-bottleneck NNGP, finding that the bottleneck induces dependence between the outputs of a multi-output network that persists through extreme post-bottleneck depths, and prevents the kernel of the network from losing discriminative power at extreme post-bottleneck depths.

</details>

<details>

<summary>2020-07-06 16:27:02 - Learning the Markov order of paths in a network</summary>

- *Luka V. Petrović, Ingo Scholtes*

- `2007.02861v1` - [abs](http://arxiv.org/abs/2007.02861v1) - [pdf](http://arxiv.org/pdf/2007.02861v1)

> We study the problem of learning the Markov order in categorical sequences that represent paths in a network, i.e. sequences of variable lengths where transitions between states are constrained to a known graph. Such data pose challenges for standard Markov order detection methods and demand modelling techniques that explicitly account for the graph constraint. Adopting a multi-order modelling framework for paths, we develop a Bayesian learning technique that (i) more reliably detects the correct Markov order compared to a competing method based on the likelihood ratio test, (ii) requires considerably less data compared to methods using AIC or BIC, and (iii) is robust against partial knowledge of the underlying constraints. We further show that a recently published method that uses a likelihood ratio test has a tendency to overfit the true Markov order of paths, which is not the case for our Bayesian technique. Our method is important for data scientists analyzing patterns in categorical sequence data that are subject to (partially) known constraints, e.g. sequences with forbidden words, mobility trajectories and click stream data, or sequence data in bioinformatics. Addressing the key challenge of model selection, our work is further relevant for the growing body of research that emphasizes the need for higher-order models in network analysis.

</details>

<details>

<summary>2020-07-06 16:54:31 - Bayesian Learning from Sequential Data using Gaussian Processes with Signature Covariances</summary>

- *Csaba Toth, Harald Oberhauser*

- `1906.08215v2` - [abs](http://arxiv.org/abs/1906.08215v2) - [pdf](http://arxiv.org/pdf/1906.08215v2)

> We develop a Bayesian approach to learning from sequential data by using Gaussian processes (GPs) with so-called signature kernels as covariance functions. This allows to make sequences of different length comparable and to rely on strong theoretical results from stochastic analysis. Signatures capture sequential structure with tensors that can scale unfavourably in sequence length and state space dimension. To deal with this, we introduce a sparse variational approach with inducing tensors. We then combine the resulting GP with LSTMs and GRUs to build larger models that leverage the strengths of each of these approaches and benchmark the resulting GPs on multivariate time series (TS) classification datasets. Code available at https://github.com/tgcsaba/GPSig.

</details>

<details>

<summary>2020-07-06 21:26:04 - Kernel Stein Generative Modeling</summary>

- *Wei-Cheng Chang, Chun-Liang Li, Youssef Mroueh, Yiming Yang*

- `2007.03074v1` - [abs](http://arxiv.org/abs/2007.03074v1) - [pdf](http://arxiv.org/pdf/2007.03074v1)

> We are interested in gradient-based Explicit Generative Modeling where samples can be derived from iterative gradient updates based on an estimate of the score function of the data distribution. Recent advances in Stochastic Gradient Langevin Dynamics (SGLD) demonstrates impressive results with energy-based models on high-dimensional and complex data distributions. Stein Variational Gradient Descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate a given distribution, based on functional gradient descent that decreases the KL divergence. SVGD has promising results on several Bayesian inference applications. However, applying SVGD on high dimensional problems is still under-explored. The goal of this work is to study high dimensional inference with SVGD. We first identify key challenges in practical kernel SVGD inference in high-dimension. We propose noise conditional kernel SVGD (NCK-SVGD), that works in tandem with the recently introduced Noise Conditional Score Network estimator. NCK is crucial for successful inference with SVGD in high dimension, as it adapts the kernel to the noise level of the score estimate. As we anneal the noise, NCK-SVGD targets the real data distribution. We then extend the annealed SVGD with an entropic regularization. We show that this offers a flexible control between sample quality and diversity, and verify it empirically by precision and recall evaluations. The NCK-SVGD produces samples comparable to GANs and annealed SGLD on computer vision benchmarks, including MNIST and CIFAR-10.

</details>

<details>

<summary>2020-07-07 09:17:17 - Single Shot MC Dropout Approximation</summary>

- *Kai Brach, Beate Sick, Oliver Dürr*

- `2007.03293v1` - [abs](http://arxiv.org/abs/2007.03293v1) - [pdf](http://arxiv.org/pdf/2007.03293v1)

> Deep neural networks (DNNs) are known for their high prediction performance, especially in perceptual tasks such as object recognition or autonomous driving. Still, DNNs are prone to yield unreliable predictions when encountering completely new situations without indicating their uncertainty. Bayesian variants of DNNs (BDNNs), such as MC dropout BDNNs, do provide uncertainty measures. However, BDNNs are slow during test time because they rely on a sampling approach. Here we present a single shot MC dropout approximation that preserves the advantages of BDNNs without being slower than a DNN. Our approach is to analytically approximate for each layer in a fully connected network the expected value and the variance of the MC dropout signal. We evaluate our approach on different benchmark datasets and a simulated toy example. We demonstrate that our single shot MC dropout approximation resembles the point estimate and the uncertainty estimate of the predictive distribution that is achieved with an MC approach, while being fast enough for real-time deployments of BDNNs.

</details>

<details>

<summary>2020-07-07 09:32:11 - qgam: Bayesian non-parametric quantile regression modelling in R</summary>

- *Matteo Fasiolo, Simon N. Wood, Margaux Zaffran, Raphaël Nedellec, Yannig Goude*

- `2007.03303v1` - [abs](http://arxiv.org/abs/2007.03303v1) - [pdf](http://arxiv.org/pdf/2007.03303v1)

> Generalized additive models (GAMs) are flexible non-linear regression models, which can be fitted efficiently using the approximate Bayesian methods provided by the mgcv R package. While the GAM methods provided by mgcv are based on the assumption that the response distribution is modelled parametrically, here we discuss more flexible methods that do not entail any parametric assumption. In particular, this article introduces the qgam package, which is an extension of mgcv providing fast calibrated Bayesian methods for fitting quantile GAMs (QGAMs) in R. QGAMs are based on a smooth version of the pinball loss of Koenker (2005), rather than on a likelihood function, hence jointly achieving satisfactory accuracy of the quantile point estimates and coverage of the corresponding credible intervals requires adopting the specialized Bayesian fitting framework of Fasiolo, Wood, Zaffran, Nedellec, and Goude (2020b). Here we detail how this framework is implemented in qgam and we provide examples illustrating how the package should be used in practice.

</details>

<details>

<summary>2020-07-07 14:49:27 - Quantum Expectation-Maximization for Gaussian Mixture Models</summary>

- *Iordanis Kerenidis, Alessandro Luongo, Anupam Prakash*

- `1908.06657v2` - [abs](http://arxiv.org/abs/1908.06657v2) - [pdf](http://arxiv.org/pdf/1908.06657v2)

> The Expectation-Maximization (EM) algorithm is a fundamental tool in unsupervised machine learning. It is often used as an efficient way to solve Maximum Likelihood (ML) estimation problems, especially for models with latent variables. It is also the algorithm of choice to fit mixture models: generative models that represent unlabelled points originating from $k$ different processes, as samples from $k$ multivariate distributions. In this work we define and use a quantum version of EM to fit a Gaussian Mixture Model. Given quantum access to a dataset of $n$ vectors of dimension $d$, our algorithm has convergence and precision guarantees similar to the classical algorithm, but the runtime is only polylogarithmic in the number of elements in the training set, and is polynomial in other parameters - as the dimension of the feature space, and the number of components in the mixture. We generalize further the algorithm in two directions. First, we show how to fit any mixture model of probability distributions in the exponential family. Then, we show how to use this algorithm to compute the Maximum a Posteriori (MAP) estimate of a mixture model: the Bayesian approach to likelihood estimation problems. We discuss the performance of the algorithm on a dataset that is expected to be classified successfully by this algorithm, arguing that on those cases we can give strong guarantees on the runtime.

</details>

<details>

<summary>2020-07-07 21:20:00 - Statistical design considerations for trials that study multiple indications</summary>

- *Alexander M. Kaizer, Joseph S. Koopmeiners, Nan Chen, Brian P. Hobbs*

- `2007.03792v1` - [abs](http://arxiv.org/abs/2007.03792v1) - [pdf](http://arxiv.org/pdf/2007.03792v1)

> Breakthroughs in cancer biology have defined new research programs emphasizing the development of therapies that target specific pathways in tumor cells. Innovations in clinical trial design have followed with master protocols defined by inclusive eligibility criteria and evaluations of multiple therapies and/or histologies. Consequently, characterization of subpopulation heterogeneity has become central to the formulation and selection of a study design. However, this transition to master protocols has led to challenges in identifying the optimal trial design and proper calibration of hyperparameters. We often evaluate a range of null and alternative scenarios, however there has been little guidance on how to synthesize the potentially disparate recommendations for what may be optimal. This may lead to the selection of suboptimal designs and statistical methods that do not fully accommodate the subpopulation heterogeneity. This article proposes novel optimization criteria for calibrating and evaluating candidate statistical designs of master protocols in the presence of the potential for treatment effect heterogeneity among enrolled patient subpopulations. The framework is applied to demonstrate the statistical properties of conventional study designs when treatments offer heterogeneous benefit as well as identify optimal designs devised to monitor the potential for heterogeneity among patients with differing clinical indications using Bayesian modeling.

</details>

<details>

<summary>2020-07-08 02:16:21 - An Information-Theoretic Analysis for Thompson Sampling with Many Actions</summary>

- *Shi Dong, Benjamin Van Roy*

- `1805.11845v4` - [abs](http://arxiv.org/abs/1805.11845v4) - [pdf](http://arxiv.org/pdf/1805.11845v4)

> Information-theoretic Bayesian regret bounds of Russo and Van Roy capture the dependence of regret on prior uncertainty. However, this dependence is through entropy, which can become arbitrarily large as the number of actions increases. We establish new bounds that depend instead on a notion of rate-distortion. Among other things, this allows us to recover through information-theoretic arguments a near-optimal bound for the linear bandit. We also offer a bound for the logistic bandit that dramatically improves on the best previously available, though this bound depends on an information-theoretic statistic that we have only been able to quantify via computation.

</details>

<details>

<summary>2020-07-08 03:53:30 - A Bayesian Nonparametric Test for Assessing Multivariate Normality</summary>

- *Luai Al-Labadi, Forough Fazeli Asl, Zahra Saberi*

- `1904.02415v4` - [abs](http://arxiv.org/abs/1904.02415v4) - [pdf](http://arxiv.org/pdf/1904.02415v4)

> In this paper, a novel Bayesian nonparametric test for assessing multivariate normal models is presented. While there are extensive frequentist and graphical methods for testing multivariate normality, it is challenging to find Bayesian counterparts. The proposed approach is based on the use of the Dirichlet process and Mahalanobis distance. More precisely, the Mahalanobis distance is employed as a good technique to transform the $m$-variate problem into a univariate problem. Then the Dirichlet process is used as a prior on the distribution of the Mahalanobis distance. The concentration of the distribution of the distance between the posterior process and the chi-square distribution with $m$ degrees of freedom is compared to the concentration of the distribution of the distance between the prior process and the chi-square distribution with $m$ degrees of freedom via a relative belief ratio. The distance between the Dirichlet process and the chi-square distribution is established based on the Anderson-Darling distance. Key theoretical results of the approach are derived. The procedure is illustrated through several examples, in which the proposed approach shows excellent performance.

</details>

<details>

<summary>2020-07-08 07:17:28 - Non-parametric Models for Non-negative Functions</summary>

- *Ulysse Marteau-Ferey, Francis Bach, Alessandro Rudi*

- `2007.03926v1` - [abs](http://arxiv.org/abs/2007.03926v1) - [pdf](http://arxiv.org/pdf/2007.03926v1)

> Linear models have shown great effectiveness and flexibility in many fields such as machine learning, signal processing and statistics. They can represent rich spaces of functions while preserving the convexity of the optimization problems where they are used, and are simple to evaluate, differentiate and integrate. However, for modeling non-negative functions, which are crucial for unsupervised learning, density estimation, or non-parametric Bayesian methods, linear models are not applicable directly. Moreover, current state-of-the-art models like generalized linear models either lead to non-convex optimization problems, or cannot be easily integrated. In this paper we provide the first model for non-negative functions which benefits from the same good properties of linear models. In particular, we prove that it admits a representer theorem and provide an efficient dual formulation for convex problems. We study its representation power, showing that the resulting space of functions is strictly richer than that of generalized linear models. Finally we extend the model and the theoretical results to functions with outputs in convex cones. The paper is complemented by an experimental evaluation of the model showing its effectiveness in terms of formulation, algorithmic derivation and practical results on the problems of density estimation, regression with heteroscedastic errors, and multiple quantile regression.

</details>

<details>

<summary>2020-07-08 09:03:55 - Deep Graph Random Process for Relational-Thinking-Based Speech Recognition</summary>

- *Hengguan Huang, Fuzhao Xue, Hao Wang, Ye Wang*

- `2007.02126v2` - [abs](http://arxiv.org/abs/2007.02126v2) - [pdf](http://arxiv.org/pdf/2007.02126v2)

> Lying at the core of human intelligence, relational thinking is characterized by initially relying on innumerable unconscious percepts pertaining to relations between new sensory signals and prior knowledge, consequently becoming a recognizable concept or object through coupling and transformation of these percepts. Such mental processes are difficult to model in real-world problems such as in conversational automatic speech recognition (ASR), as the percepts (if they are modelled as graphs indicating relationships among utterances) are supposed to be innumerable and not directly observable. In this paper, we present a Bayesian nonparametric deep learning method called deep graph random process (DGP) that can generate an infinite number of probabilistic graphs representing percepts. We further provide a closed-form solution for coupling and transformation of these percept graphs for acoustic modeling. Our approach is able to successfully infer relations among utterances without using any relational data during training. Experimental evaluations on ASR tasks including CHiME-2 and CHiME-5 demonstrate the effectiveness and benefits of our method.

</details>

<details>

<summary>2020-07-08 09:15:22 - Bayesian Regression Using a Prior on the Model Fit: The R2-D2 Shrinkage Prior</summary>

- *Yan Dora Zhang, Brian P. Naughton, Howard D. Bondell, Brian J. Reich*

- `1609.00046v3` - [abs](http://arxiv.org/abs/1609.00046v3) - [pdf](http://arxiv.org/pdf/1609.00046v3)

> Prior distributions for high-dimensional linear regression require specifying a joint distribution for the unobserved regression coefficients, which is inherently difficult. We instead propose a new class of shrinkage priors for linear regression via specifying a prior first on the model fit, in particular, the coefficient of determination, and then distributing through to the coefficients in a novel way. The proposed method compares favourably to previous approaches in terms of both concentration around the origin and tail behavior, which leads to improved performance both in posterior contraction and in empirical performance. The limiting behavior of the proposed prior is $1/x$, both around the origin and in the tails. This behavior is optimal in the sense that it simultaneously lies on the boundary of being an improper prior both in the tails and around the origin. None of the existing shrinkage priors obtain this behavior in both regions simultaneously. We also demonstrate that our proposed prior leads to the same near-minimax posterior contraction rate as the spike-and-slab prior.

</details>

<details>

<summary>2020-07-08 10:21:56 - Accelerated Sparse Bayesian Learning via Screening Test and Its Applications</summary>

- *Yiping Jiang, Tianshi Chen*

- `2007.04006v1` - [abs](http://arxiv.org/abs/2007.04006v1) - [pdf](http://arxiv.org/pdf/2007.04006v1)

> In high-dimensional settings, sparse structures are critical for efficiency in term of memory and computation complexity. For a linear system, to find the sparsest solution provided with an over-complete dictionary of features directly is typically NP-hard, and thus alternative approximate methods should be considered. In this paper, our choice for alternative method is sparse Bayesian learning, which, as empirical Bayesian approaches, uses a parameterized prior to encourage sparsity in solution, rather than the other methods with fixed priors such as LASSO. Screening test, however, aims at quickly identifying a subset of features whose coefficients are guaranteed to be zero in the optimal solution, and then can be safely removed from the complete dictionary to obtain a smaller, more easily solved problem. Next, we solve the smaller problem, after which the solution of the original problem can be recovered by padding the smaller solution with zeros. The performance of the proposed method will be examined on various data sets and applications.

</details>

<details>

<summary>2020-07-08 15:12:53 - A Bayesian Redesign of the First Probability/Statistics Course</summary>

- *Jim Albert*

- `2007.04180v1` - [abs](http://arxiv.org/abs/2007.04180v1) - [pdf](http://arxiv.org/pdf/2007.04180v1)

> The traditional calculus-based introduction to statistical inference consists of a semester of probability followed by a semester of frequentist inference. Cobb (2015) challenges the statistical education community to rethink the undergraduate statistics curriculum. In particular, he suggests that we should focus on two goals: making fundamental concepts accessible and minimizing prerequisites to research. Using five underlying principles of Cobb, we describe a new calculus-based introduction to statistics based on simulation-based Bayesian computation.

</details>

<details>

<summary>2020-07-08 18:34:12 - Generalised Bayes Updates with $f$-divergences through Probabilistic Classifiers</summary>

- *Owen Thomas, Henri Pesonen, Jukka Corander*

- `2007.04358v1` - [abs](http://arxiv.org/abs/2007.04358v1) - [pdf](http://arxiv.org/pdf/2007.04358v1)

> A stream of algorithmic advances has steadily increased the popularity of the Bayesian approach as an inference paradigm, both from the theoretical and applied perspective. Even with apparent successes in numerous application fields, a rising concern is the robustness of Bayesian inference in the presence of model misspecification, which may lead to undesirable extreme behavior of the posterior distributions for large sample sizes. Generalized belief updating with a loss function represents a central principle to making Bayesian inference more robust and less vulnerable to deviations from the assumed model. Here we consider such updates with $f$-divergences to quantify a discrepancy between the assumed statistical model and the probability distribution which generated the observed data. Since the latter is generally unknown, estimation of the divergence may be viewed as an intractable problem. We show that the divergence becomes accessible through the use of probabilistic classifiers that can leverage an estimate of the ratio of two probability distributions even when one or both of them is unknown. We demonstrate the behavior of generalized belief updates for various specific choices under the $f$-divergence family. We show that for specific divergence functions such an approach can even improve on methods evaluating the correct model likelihood function analytically.

</details>

<details>

<summary>2020-07-08 19:04:41 - Anchored Bayesian Gaussian Mixture Models</summary>

- *Deborah Kunkel, Mario Peruggia*

- `1805.08304v4` - [abs](http://arxiv.org/abs/1805.08304v4) - [pdf](http://arxiv.org/pdf/1805.08304v4)

> Finite mixtures are a flexible modeling tool for irregularly shaped densities and samples from heterogeneous populations. When modeling with mixtures using an exchangeable prior on the component features, the component labels are arbitrary and are indistinguishable in posterior analysis. This makes it impossible to attribute any meaningful interpretation to the marginal posterior distributions of the component features. We propose a model in which a small number of observations are assumed to arise from some of the labeled component densities. The resulting model is not exchangeable, allowing inference on the component features without post-processing. Our method assigns meaning to the component labels at the modeling stage and can be justified as a data-dependent informative prior on the labelings. We show that our method produces interpretable results, often (but not always) similar to those resulting from relabeling algorithms, with the added benefit that the marginal inferences originate directly from a well specified probability model rather than a post hoc manipulation. We provide asymptotic results leading to practical guidelines for model selection that are motivated by maximizing prior information about the class labels and demonstrate our method on real and simulated data.

</details>

<details>

<summary>2020-07-08 22:25:29 - Robust Bayesian Classification Using an Optimistic Score Ratio</summary>

- *Viet Anh Nguyen, Nian Si, Jose Blanchet*

- `2007.04458v1` - [abs](http://arxiv.org/abs/2007.04458v1) - [pdf](http://arxiv.org/pdf/2007.04458v1)

> We build a Bayesian contextual classification model using an optimistic score ratio for robust binary classification when there is limited information on the class-conditional, or contextual, distribution. The optimistic score searches for the distribution that is most plausible to explain the observed outcomes in the testing sample among all distributions belonging to the contextual ambiguity set which is prescribed using a limited structural constraint on the mean vector and the covariance matrix of the underlying contextual distribution. We show that the Bayesian classifier using the optimistic score ratio is conceptually attractive, delivers solid statistical guarantees and is computationally tractable. We showcase the power of the proposed optimistic score ratio classifier on both synthetic and empirical data.

</details>

<details>

<summary>2020-07-08 22:51:28 - URSABench: Comprehensive Benchmarking of Approximate Bayesian Inference Methods for Deep Neural Networks</summary>

- *Meet P. Vadera, Adam D. Cobb, Brian Jalaian, Benjamin M. Marlin*

- `2007.04466v1` - [abs](http://arxiv.org/abs/2007.04466v1) - [pdf](http://arxiv.org/pdf/2007.04466v1)

> While deep learning methods continue to improve in predictive accuracy on a wide range of application domains, significant issues remain with other aspects of their performance including their ability to quantify uncertainty and their robustness. Recent advances in approximate Bayesian inference hold significant promise for addressing these concerns, but the computational scalability of these methods can be problematic when applied to large-scale models. In this paper, we describe initial work on the development ofURSABench(the Uncertainty, Robustness, Scalability, and Accu-racy Benchmark), an open-source suite of bench-marking tools for comprehensive assessment of approximate Bayesian inference methods with a focus on deep learning-based classification tasks

</details>

<details>

<summary>2020-07-09 05:48:08 - Bayesian Hierarchical Modeling on Covariance Valued Data</summary>

- *Satwik Acharyya, Zhengwu Zhang, Anirban Bhattacharya, Debdeep Pati*

- `1811.00724v4` - [abs](http://arxiv.org/abs/1811.00724v4) - [pdf](http://arxiv.org/pdf/1811.00724v4)

> Analysis of structural and functional connectivity (FC) of human brains is of pivotal importance for diagnosis of cognitive ability. The Human Connectome Project (HCP) provides an excellent source of neural data across different regions of interest (ROIs) of the living human brain. Individual specific data were available from an existing analysis (Dai et al., 2017) in the form of time varying covariance matrices representing the brain activity as the subjects perform a specific task. As a preliminary objective of studying the heterogeneity of brain connectomics across the population, we develop a probabilistic model for a sample of covariance matrices using a scaled Wishart distribution. We stress here that our data units are available in the form of covariance matrices, and we use the Wishart distribution to create our likelihood function rather than its more common usage as a prior on covariance matrices. Based on empirical explorations suggesting the data matrices to have low effective rank, we further model the center of the Wishart distribution using an orthogonal factor model type decomposition. We encourage shrinkage towards a low rank structure through a novel shrinkage prior and discuss strategies to sample from the posterior distribution using a combination of Gibbs and slice sampling. We extend our modeling framework to a dynamic setting to detect change points. The efficacy of the approach is explored in various simulation settings and exemplified on several case studies including our motivating HCP data. We extend our modeling framework to a dynamic setting to detect change points.

</details>

<details>

<summary>2020-07-09 10:01:32 - Resource Aware Multifidelity Active Learning for Efficient Optimization</summary>

- *Francesco Grassi, Giorgio Manganini, Michele Garraffa, Laura Mainini*

- `2007.04674v1` - [abs](http://arxiv.org/abs/2007.04674v1) - [pdf](http://arxiv.org/pdf/2007.04674v1)

> Traditional methods for black box optimization require a considerable number of evaluations which can be time consuming, unpractical, and often unfeasible for many engineering applications that rely on accurate representations and expensive models to evaluate. Bayesian Optimization (BO) methods search for the global optimum by progressively (actively) learning a surrogate model of the objective function along the search path. Bayesian optimization can be accelerated through multifidelity approaches which leverage multiple black-box approximations of the objective functions that can be computationally cheaper to evaluate, but still provide relevant information to the search task. Further computational benefits are offered by the availability of parallel and distributed computing architectures whose optimal usage is an open opportunity within the context of active learning. This paper introduces the Resource Aware Active Learning (RAAL) strategy, a multifidelity Bayesian scheme to accelerate the optimization of black box functions. At each optimization step, the RAAL procedure computes the set of best sample locations and the associated fidelity sources that maximize the information gain to acquire during the parallel/distributed evaluation of the objective function, while accounting for the limited computational budget. The scheme is demonstrated for a variety of benchmark problems and results are discussed for both single fidelity and multifidelity settings. In particular we observe that the RAAL strategy optimally seeds multiple points at each iteration allowing for a major speed up of the optimization task.

</details>

<details>

<summary>2020-07-09 10:05:46 - Training Restricted Boltzmann Machines with Binary Synapses using the Bayesian Learning Rule</summary>

- *Xiangming Meng*

- `2007.04676v1` - [abs](http://arxiv.org/abs/2007.04676v1) - [pdf](http://arxiv.org/pdf/2007.04676v1)

> Restricted Boltzmann machines (RBMs) with low-precision synapses are much appealing with high energy efficiency. However, training RBMs with binary synapses is challenging due to the discrete nature of synapses. Recently Huang proposed one efficient method to train RBMs with binary synapses by using a combination of gradient ascent and the message passing algorithm under the variational inference framework. However, additional heuristic clipping operation is needed. In this technical note, inspired from Huang's work , we propose one alternative optimization method using the Bayesian learning rule, which is one natural gradient variational inference method. As opposed to Huang's method, we update the natural parameters of the variational symmetric Bernoulli distribution rather than the expectation parameters. Since the natural parameters take values in the entire real domain, no additional clipping is needed. Interestingly, the algorithm in \cite{huang2019data} could be viewed as one first-order approximation of the proposed algorithm, which justifies its efficacy with heuristic clipping.

</details>

<details>

<summary>2020-07-09 10:30:42 - The FMRIB Variational Bayesian Inference Tutorial II: Stochastic Variational Bayes</summary>

- *Michael A. Chappell, Mark W. Woolrich*

- `2007.02725v2` - [abs](http://arxiv.org/abs/2007.02725v2) - [pdf](http://arxiv.org/pdf/2007.02725v2)

> Bayesian methods have proved powerful in many applications for the inference of model parameters from data. These methods are based on Bayes' theorem, which itself is deceptively simple. However, in practice the computations required are intractable even for simple cases. Hence methods for Bayesian inference have historically either been significantly approximate, e.g., the Laplace approximation, or achieve samples from the exact solution at significant computational expense, e.g., Markov Chain Monte Carlo methods. Since around the year 2000 so-called Variational approaches to Bayesian inference have been increasingly deployed. In its most general form Variational Bayes (VB) involves approximating the true posterior probability distribution via another more 'manageable' distribution, the aim being to achieve as good an approximation as possible. In the original FMRIB Variational Bayes tutorial we documented an approach to VB based that took a 'mean field' approach to forming the approximate posterior, required the conjugacy of prior and likelihood, and exploited the Calculus of Variations, to derive an iterative series of update equations, akin to Expectation Maximisation. In this tutorial we revisit VB, but now take a stochastic approach to the problem that potentially circumvents some of the limitations imposed by the earlier methodology. This new approach bears a lot of similarity to, and has benefited from, computational methods applied to machine learning algorithms. Although, what we document here is still recognisably Bayesian inference in the classic sense, and not an attempt to use machine learning as a black-box to solve the inference problem.

</details>

<details>

<summary>2020-07-09 11:18:01 - Bayesian Modeling of COVID-19 Positivity Rate -- the Indiana experience</summary>

- *Ben Boukai, Jiayue Wang*

- `2007.06541v1` - [abs](http://arxiv.org/abs/2007.06541v1) - [pdf](http://arxiv.org/pdf/2007.06541v1)

> In this short technical report we model, within the Bayesian framework, the rate of positive tests reported by the the State of Indiana, accounting also for the substantial variability (and overdispeartion) in the daily count of the tests performed. The approach we take, results with a simple procedure for prediction, a posteriori, of this rate of 'positivity' and allows for an easy and a straightforward adaptation by any agency tracking daily results of COVID-19 tests. The numerical results provided herein were obtained via an updatable R Markdown document.

</details>

<details>

<summary>2020-07-09 15:21:28 - Invertible Zero-Shot Recognition Flows</summary>

- *Yuming Shen, Jie Qin, Lei Huang*

- `2007.04873v1` - [abs](http://arxiv.org/abs/2007.04873v1) - [pdf](http://arxiv.org/pdf/2007.04873v1)

> Deep generative models have been successfully applied to Zero-Shot Learning (ZSL) recently. However, the underlying drawbacks of GANs and VAEs (e.g., the hardness of training with ZSL-oriented regularizers and the limited generation quality) hinder the existing generative ZSL models from fully bypassing the seen-unseen bias. To tackle the above limitations, for the first time, this work incorporates a new family of generative models (i.e., flow-based models) into ZSL. The proposed Invertible Zero-shot Flow (IZF) learns factorized data embeddings (i.e., the semantic factors and the non-semantic ones) with the forward pass of an invertible flow network, while the reverse pass generates data samples. This procedure theoretically extends conventional generative flows to a factorized conditional scheme. To explicitly solve the bias problem, our model enlarges the seen-unseen distributional discrepancy based on negative sample-based distance measurement. Notably, IZF works flexibly with either a naive Bayesian classifier or a held-out trainable one for zero-shot recognition. Experiments on widely-adopted ZSL benchmarks demonstrate the significant performance gain of IZF over existing methods, in both classic and generalized settings.

</details>

<details>

<summary>2020-07-09 17:37:35 - Bayesian Computation in Dynamic Latent Factor Models</summary>

- *Isaac Lavine, Andrew Cron, Mike West*

- `2007.04956v1` - [abs](http://arxiv.org/abs/2007.04956v1) - [pdf](http://arxiv.org/pdf/2007.04956v1)

> Bayesian computation for filtering and forecasting analysis is developed for a broad class of dynamic models. The ability to scale-up such analyses in non-Gaussian, nonlinear multivariate time series models is advanced through the introduction of a novel copula construction in sequential filtering of coupled sets of dynamic generalized linear models. The new copula approach is integrated into recently introduced multiscale models in which univariate time series are coupled via nonlinear forms involving dynamic latent factors representing cross-series relationships. The resulting methodology offers dramatic speed-up in online Bayesian computations for sequential filtering and forecasting in this broad, flexible class of multivariate models. Two examples in nonlinear models for very heterogeneous time series of non-negative counts demonstrate massive computational efficiencies relative to existing simulation-based methods, while defining similar filtering and forecasting outcomes.

</details>

<details>

<summary>2020-07-09 21:18:44 - The COVID-19 Pandemic, Community Mobility and the Effectiveness of Non-pharmaceutical Interventions: The United States of America, February to May 2020</summary>

- *Ian E. Fellows, Rachel B. Slayton, Avi J. Hakim*

- `2007.12644v1` - [abs](http://arxiv.org/abs/2007.12644v1) - [pdf](http://arxiv.org/pdf/2007.12644v1)

> Background: The impact of individual non-pharmaceutical interventions (NPI) such as state-wide stay-at-home orders, school closures and gathering size limitations, on the COVID-19 epidemic is unknown. Understanding the impact that above listed NPI have on disease transmission is critical for policy makers, particularly as case counts increase again in some areas.   Methods: Using a Bayesian framework, we reconstructed the incidence and time-varying reproductive number (Rt) curves to investigate the relationship between Rt, individual mobility as measured by Google Community Mobility Reports, and NPI.   Results: We found a strong relationship between reproductive number and mobility, with each 10% drop in mobility being associated with an expected 10.2% reduction in Rt compared to baseline. The effects of limitations on the size of gatherings, school and business closures, and stay-at-home orders were dominated by the trend over time, which was associated with a 48% decrease in the reproductive number, adjusting for the NPI.   Conclusions: We found that the decrease in mobility associated with time may be due to individuals changing their behavior in response to perceived risk or external factors.

</details>

<details>

<summary>2020-07-10 00:17:39 - Space-Time Smoothing of Demographic and Health Indicators using the R Package SUMMER</summary>

- *Zehang Richard Li, Bryan D Martin, Tracy Qi Dong, Geir-Arne Fuglstad, John Paige, Andrea Riebler, Samuel Clark, Jon Wakefield*

- `2007.05117v1` - [abs](http://arxiv.org/abs/2007.05117v1) - [pdf](http://arxiv.org/pdf/2007.05117v1)

> The increasing availability of complex survey data, and the continued need for estimates of demographic and health indicators at a fine spatial and temporal scale, which leads to issues of data sparsity, has led to the need for spatio-temporal smoothing methods that acknowledge the manner in which the data were collected. The open source R package SUMMER implements a variety of methods for spatial or spatio-temporal smoothing of survey data. The emphasis is on small-area estimation. We focus primarily on indicators in a low and middle-income countries context. Our methods are particularly useful for data from Demographic Health Surveys and Multiple Indicator Cluster Surveys. We build upon functions within the survey package, and use INLA for fast Bayesian computation. This paper includes a brief overview of these methods and illustrates the workflow of accessing and processing surveys, estimating subnational child mortality rates, and visualizing results with both simulated data and DHS surveys.

</details>

<details>

<summary>2020-07-10 01:55:02 - Revisiting One-vs-All Classifiers for Predictive Uncertainty and Out-of-Distribution Detection in Neural Networks</summary>

- *Shreyas Padhy, Zachary Nado, Jie Ren, Jeremiah Liu, Jasper Snoek, Balaji Lakshminarayanan*

- `2007.05134v1` - [abs](http://arxiv.org/abs/2007.05134v1) - [pdf](http://arxiv.org/pdf/2007.05134v1)

> Accurate estimation of predictive uncertainty in modern neural networks is critical to achieve well calibrated predictions and detect out-of-distribution (OOD) inputs. The most promising approaches have been predominantly focused on improving model uncertainty (e.g. deep ensembles and Bayesian neural networks) and post-processing techniques for OOD detection (e.g. ODIN and Mahalanobis distance). However, there has been relatively little investigation into how the parametrization of the probabilities in discriminative classifiers affects the uncertainty estimates, and the dominant method, softmax cross-entropy, results in misleadingly high confidences on OOD data and under covariate shift. We investigate alternative ways of formulating probabilities using (1) a one-vs-all formulation to capture the notion of "none of the above", and (2) a distance-based logit representation to encode uncertainty as a function of distance to the training manifold. We show that one-vs-all formulations can improve calibration on image classification tasks, while matching the predictive performance of softmax without incurring any additional training or test-time complexity.

</details>

<details>

<summary>2020-07-10 07:46:23 - Incertitudes et mesures</summary>

- *Romain Legrand*

- `2007.06701v1` - [abs](http://arxiv.org/abs/2007.06701v1) - [pdf](http://arxiv.org/pdf/2007.06701v1)

> Educational guide focused on the statistical treatment of measurement uncertainties. The conditions of application of current practices are detailed and precised: mean values, central limit theorem, linear regression. The last two chapters are devoted to an introduction to the Bayesian inference and a series of application cases: machine failure date, elimination of a background noise, linear adjustment with elimination of outliers.

</details>

<details>

<summary>2020-07-10 15:14:43 - Characteristics of Monte Carlo Dropout in Wide Neural Networks</summary>

- *Joachim Sicking, Maram Akila, Tim Wirtz, Sebastian Houben, Asja Fischer*

- `2007.05434v1` - [abs](http://arxiv.org/abs/2007.05434v1) - [pdf](http://arxiv.org/pdf/2007.05434v1)

> Monte Carlo (MC) dropout is one of the state-of-the-art approaches for uncertainty estimation in neural networks (NNs). It has been interpreted as approximately performing Bayesian inference. Based on previous work on the approximation of Gaussian processes by wide and deep neural networks with random weights, we study the limiting distribution of wide untrained NNs under dropout more rigorously and prove that they as well converge to Gaussian processes for fixed sets of weights and biases. We sketch an argument that this property might also hold for infinitely wide feed-forward networks that are trained with (full-batch) gradient descent. The theory is contrasted by an empirical analysis in which we find correlations and non-Gaussian behaviour for the pre-activations of finite width NNs. We therefore investigate how (strongly) correlated pre-activations can induce non-Gaussian behavior in NNs with strongly correlated weights.

</details>

<details>

<summary>2020-07-10 15:16:41 - Bayesian inference, model selection and likelihood estimation using fast rejection sampling: the Conway-Maxwell-Poisson distribution</summary>

- *Alan Benson, Nial Friel*

- `1709.03471v2` - [abs](http://arxiv.org/abs/1709.03471v2) - [pdf](http://arxiv.org/pdf/1709.03471v2)

> Bayesian inference for models with intractable likelihood functions represents a challenging suite of problems in modern statistics. In this work we analyse the Conway-Maxwell-Poisson (COM-Poisson) distribution, a two parameter generalisation of the Poisson distribution. COM-Poisson regression modelling allows the flexibility to model dispersed count data as part of a generalised linear model (GLM) with a COM-Poisson response, where exogenous covariates control the mean and dispersion level of the response. The major difficulty with COM-Poisson regression is that the likelihood function contains multiple intractable normalising constants and is not amenable to standard inference and MCMC techniques. Recent work by Chanialidis et al. (2017) has seen the development of a sampler to draw random variates from the COM-Poisson likelihood using a rejection sampling algorithm. We provide a new rejection sampler for the COM-Poisson distribution which significantly reduces the CPU time required to perform inference for COM-Poisson regression models. A novel extension of this work shows that for any intractable likelihood function with an associated rejection sampler it is possible to construct unbiased estimators of the intractable likelihood which proves useful for model selection or for use within pseudo-marginal MCMC algorithms (Andrieu and Roberts, 2009). We demonstrate all of these methods on a real-world dataset of takeover bids.

</details>

<details>

<summary>2020-07-10 17:26:20 - Estimation of COVID-19 spread curves integrating global data and borrowing information</summary>

- *Se Yoon Lee, Bowen Lei, Bani K. Mallick*

- `2005.00662v5` - [abs](http://arxiv.org/abs/2005.00662v5) - [pdf](http://arxiv.org/pdf/2005.00662v5)

> Currently, novel coronavirus disease 2019 (COVID-19) is a big threat to global health. The rapid spread of the virus has created pandemic, and countries all over the world are struggling with a surge in COVID-19 infected cases. There are no drugs or other therapeutics approved by the US Food and Drug Administration to prevent or treat COVID-19: information on the disease is very limited and scattered even if it exists. This motivates the use of data integration, combining data from diverse sources and eliciting useful information with a unified view of them. In this paper, we propose a Bayesian hierarchical model that integrates global data for real-time prediction of infection trajectory for multiple countries. Because the proposed model takes advantage of borrowing information across multiple countries, it outperforms an existing individual country-based model. As fully Bayesian way has been adopted, the model provides a powerful predictive tool endowed with uncertainty quantification. Additionally, a joint variable selection technique has been integrated into the proposed modeling scheme, which aimed to identify possible country-level risk factors for severe disease due to COVID-19.

</details>

<details>

<summary>2020-07-10 18:00:03 - Climate & BCG: Effects on COVID-19 Death Growth Rates</summary>

- *Chris Finlay, Bruce A. Bassett*

- `2007.05542v1` - [abs](http://arxiv.org/abs/2007.05542v1) - [pdf](http://arxiv.org/pdf/2007.05542v1)

> Multiple studies have suggested the spread of COVID-19 is affected by factors such as climate, BCG vaccinations, pollution and blood type. We perform a joint study of these factors using the death growth rates of 40 regions worldwide with both machine learning and Bayesian methods. We find weak, non-significant (< 3$\sigma$) evidence for temperature and relative humidity as factors in the spread of COVID-19 but little or no evidence for BCG vaccination prevalence or $\text{PM}_{2.5}$ pollution. The only variable detected at a statistically significant level (>3$\sigma$) is the rate of positive COVID-19 tests, with higher positive rates correlating with higher daily growth of deaths.

</details>

<details>

<summary>2020-07-10 21:25:42 - Bayesian Profiling Multiple Imputation for Missing Electronic Health Records</summary>

- *Yajuan Si, Mari Palta, Maureen Smith*

- `1906.00042v2` - [abs](http://arxiv.org/abs/1906.00042v2) - [pdf](http://arxiv.org/pdf/1906.00042v2)

> Electronic health records (EHRs) are increasingly used for clinical and comparative effectiveness research, but suffer from missing data. Motivated by health services research on diabetes care, we seek to increase the quality of EHRs by focusing on missing values of longitudinal glycosylated hemoglobin (A1c), a key risk factor for diabetes complications and adverse events. Under the framework of multiple imputation (MI), we propose an individualized Bayesian latent profiling approach to capture A1c measurement trajectories subject to missingness. The proposed method is applied to EHRs of adult patients with diabetes in a large academic Midwestern health system between 2003 and 2013 and had Medicare A and B coverage. We combine MI inferences to evaluate the association of A1c levels with the incidence of acute adverse health events and examine patient heterogeneity across identified patient profiles. We investigate different missingness mechanisms and perform imputation diagnostics. Our approach is computationally efficient and fits flexible models that provide useful clinical insights.

</details>

<details>

<summary>2020-07-11 11:22:16 - Frequentism-as-model</summary>

- *Christian Hennig*

- `2007.05748v1` - [abs](http://arxiv.org/abs/2007.05748v1) - [pdf](http://arxiv.org/pdf/2007.05748v1)

> Most statisticians are aware that probability models interpreted in a frequentist manner are not really true in objective reality, but only idealisations. I argue that this is often ignored when actually applying frequentist methods and interpreting the results, and that keeping up the awareness for the essential difference between reality and models can lead to a more appropriate use and interpretation of frequentist models and methods, called frequentism-as-model. This is elaborated showing connections to existing work, appreciating the special role of i.i.d. models and subject matter knowledge, giving an account of how and under what conditions models that are not true can be useful, giving detailed interpretations of tests and confidence intervals, confronting their implicit compatibility logic with the inverse probability logic of Bayesian inference, re-interpreting the role of model assumptions, appreciating robustness, and the role of ``interpretative equivalence'' of models. Epistemic (often referred to as Bayesian) probability shares the issue that its models are only idealisations and not really true for modelling reasoning about uncertainty, meaning that it does not have an essential advantage over frequentism, as is often claimed. Bayesian statistics can be combined with frequentism-as-model, leading to what Gelman and Hennig (2017) call ``falsificationist Bayes''.

</details>

<details>

<summary>2020-07-12 03:23:27 - A Probabilistic Approach to Identifying Run Scoring Advantage in the Order of Playing Cricket</summary>

- *Manar D. Samad, Sumen Sen*

- `2007.05894v1` - [abs](http://arxiv.org/abs/2007.05894v1) - [pdf](http://arxiv.org/pdf/2007.05894v1)

> In the game of cricket, the result of coin toss is assumed to be one of the determinants of match outcome. The decision to bat first after winning the toss is often taken to make the best use of superior pitch conditions and set a big target for the opponent. However, the opponent may fail to show their natural batting performance in the second innings due to a number of factors, including deteriorated pitch conditions and excessive pressure of chasing a high target score. The advantage of batting first has been highlighted in the literature and expert opinions, however, the effect of batting and bowling order on match outcome has not been investigated well enough to recommend a solution to any potential bias. This study proposes a probability theory-based model to study venue-specific scoring and chasing characteristics of teams under different match outcomes. A total of 1117 one-day international matches held in ten popular venues are analyzed to show substantially high scoring advantage and likelihood when the winning team bat in the first innings. Results suggest that the same 'bat-first' winning team is very unlikely to score or chase such a high score if they were to bat in the second innings. Therefore, the coin toss decision may favor one team over the other. A Bayesian model is proposed to revise the target score for each venue such that the winning and scoring likelihood is equal regardless of the toss decision. The data and source codes have been shared publicly for future research in creating competitive match outcomes by eliminating the advantage of batting order in run scoring.

</details>

<details>

<summary>2020-07-12 07:42:51 - On the Likelihood of Local Projection Models</summary>

- *Masahiro Tanaka*

- `2005.12620v2` - [abs](http://arxiv.org/abs/2005.12620v2) - [pdf](http://arxiv.org/pdf/2005.12620v2)

> A local projection model is defined by a set of linear regressions that account for the associations between exogenous variables and an endogenous variable observed at different time points. While it is standard practice to separately estimate individual regressions using the ordinary least squares estimator, some recent studies treat a local projection model as a multivariate regression with correlated errors, i.e., seemingly unrelated regressions, and propose Bayesian and non-Bayesian methods to improve the estimation accuracy. However, it is not clear how and when this way of treatment of local projection models is justified. The primary purpose of this paper is to fill this gap by showing that the likelihood of local projection models can be analytically derived from a stationary vector moving average process. By means of numerical experiments, we confirm that this treatment of local projections is tenable for finite samples.

</details>

<details>

<summary>2020-07-12 13:59:25 - State Space Expectation Propagation: Efficient Inference Schemes for Temporal Gaussian Processes</summary>

- *William J. Wilkinson, Paul E. Chang, Michael Riis Andersen, Arno Solin*

- `2007.05994v1` - [abs](http://arxiv.org/abs/2007.05994v1) - [pdf](http://arxiv.org/pdf/2007.05994v1)

> We formulate approximate Bayesian inference in non-conjugate temporal and spatio-temporal Gaussian process models as a simple parameter update rule applied during Kalman smoothing. This viewpoint encompasses most inference schemes, including expectation propagation (EP), the classical (Extended, Unscented, etc.) Kalman smoothers, and variational inference. We provide a unifying perspective on these algorithms, showing how replacing the power EP moment matching step with linearisation recovers the classical smoothers. EP provides some benefits over the traditional methods via introduction of the so-called cavity distribution, and we combine these benefits with the computational efficiency of linearisation, providing extensive empirical analysis demonstrating the efficacy of various algorithms under this unifying framework. We provide a fast implementation of all methods in JAX.

</details>

<details>

<summary>2020-07-12 20:52:55 - BaCOUn: Bayesian Classifers with Out-of-Distribution Uncertainty</summary>

- *Théo Guénais, Dimitris Vamvourellis, Yaniv Yacoby, Finale Doshi-Velez, Weiwei Pan*

- `2007.06096v1` - [abs](http://arxiv.org/abs/2007.06096v1) - [pdf](http://arxiv.org/pdf/2007.06096v1)

> Traditional training of deep classifiers yields overconfident models that are not reliable under dataset shift. We propose a Bayesian framework to obtain reliable uncertainty estimates for deep classifiers. Our approach consists of a plug-in "generator" used to augment the data with an additional class of points that lie on the boundary of the training data, followed by Bayesian inference on top of features that are trained to distinguish these "out-of-distribution" points.

</details>

<details>

<summary>2020-07-12 23:22:26 - The Dependent Dirichlet Process and Related Models</summary>

- *Fernand A. Quintana, Peter Mueller, Alejandro Jara, Steven N. MacEachern*

- `2007.06129v1` - [abs](http://arxiv.org/abs/2007.06129v1) - [pdf](http://arxiv.org/pdf/2007.06129v1)

> Standard regression approaches assume that some finite number of the response distribution characteristics, such as location and scale, change as a (parametric or nonparametric) function of predictors. However, it is not always appropriate to assume a location/scale representation, where the error distribution has unchanging shape over the predictor space. In fact, it often happens in applied research that the distribution of responses under study changes with predictors in ways that cannot be reasonably represented by a finite dimensional functional form. This can seriously affect the answers to the scientific questions of interest, and therefore more general approaches are indeed needed. This gives rise to the study of fully nonparametric regression models. We review some of the main Bayesian approaches that have been employed to define probability models where the complete response distribution may vary flexibly with predictors. We focus on developments based on modifications of the Dirichlet process, historically termed dependent Dirichlet processes, and some of the extensions that have been proposed to tackle this general problem using nonparametric approaches.

</details>

<details>

<summary>2020-07-12 23:57:22 - Uncertainty Quantification for Sparse Deep Learning</summary>

- *Yuexi Wang, Veronika Ročková*

- `2002.11815v2` - [abs](http://arxiv.org/abs/2002.11815v2) - [pdf](http://arxiv.org/pdf/2002.11815v2)

> Deep learning methods continue to have a decided impact on machine learning, both in theory and in practice. Statistical theoretical developments have been mostly concerned with approximability or rates of estimation when recovering infinite dimensional objects (curves or densities). Despite the impressive array of available theoretical results, the literature has been largely silent about uncertainty quantification for deep learning. This paper takes a step forward in this important direction by taking a Bayesian point of view. We study Gaussian approximability of certain aspects of posterior distributions of sparse deep ReLU architectures in non-parametric regression. Building on tools from Bayesian non-parametrics, we provide semi-parametric Bernstein-von Mises theorems for linear and quadratic functionals, which guarantee that implied Bayesian credible regions have valid frequentist coverage. Our results provide new theoretical justifications for (Bayesian) deep learning with ReLU activation functions, highlighting their inferential potential.

</details>

<details>

<summary>2020-07-13 03:27:45 - Model Fusion with Kullback--Leibler Divergence</summary>

- *Sebastian Claici, Mikhail Yurochkin, Soumya Ghosh, Justin Solomon*

- `2007.06168v1` - [abs](http://arxiv.org/abs/2007.06168v1) - [pdf](http://arxiv.org/pdf/2007.06168v1)

> We propose a method to fuse posterior distributions learned from heterogeneous datasets. Our algorithm relies on a mean field assumption for both the fused model and the individual dataset posteriors and proceeds using a simple assign-and-average approach. The components of the dataset posteriors are assigned to the proposed global model components by solving a regularized variant of the assignment problem. The global components are then updated based on these assignments by their mean under a KL divergence. For exponential family variational distributions, our formulation leads to an efficient non-parametric algorithm for computing the fused model. Our algorithm is easy to describe and implement, efficient, and competitive with state-of-the-art on motion capture analysis, topic modeling, and federated learning of Bayesian neural networks.

</details>

<details>

<summary>2020-07-13 05:55:02 - Amortized Bayesian Inference for Models of Cognition</summary>

- *Stefan T. Radev, Andreas Voss, Eva Marie Wieschen, Paul-Christian Bürkner*

- `2005.03899v3` - [abs](http://arxiv.org/abs/2005.03899v3) - [pdf](http://arxiv.org/pdf/2005.03899v3)

> As models of cognition grow in complexity and number of parameters, Bayesian inference with standard methods can become intractable, especially when the data-generating model is of unknown analytic form. Recent advances in simulation-based inference using specialized neural network architectures circumvent many previous problems of approximate Bayesian computation. Moreover, due to the properties of these special neural network estimators, the effort of training the networks via simulations amortizes over subsequent evaluations which can re-use the same network for multiple datasets and across multiple researchers. However, these methods have been largely underutilized in cognitive science and psychology so far, even though they are well suited for tackling a wide variety of modeling problems. With this work, we provide a general introduction to amortized Bayesian parameter estimation and model comparison and demonstrate the applicability of the proposed methods on a well-known class of intractable response-time models.

</details>

<details>

<summary>2020-07-13 13:43:48 - Synthetic Aperture Radar Image Formation with Uncertainty Quantification</summary>

- *Victor Churchill, Anne Gelb*

- `2007.06380v1` - [abs](http://arxiv.org/abs/2007.06380v1) - [pdf](http://arxiv.org/pdf/2007.06380v1)

> Synthetic aperture radar (SAR) is a day or night any-weather imaging modality that is an important tool in remote sensing. Most existing SAR image formation methods result in a maximum a posteriori image which approximates the reflectivity of an unknown ground scene. This single image provides no quantification of the certainty with which the features in the estimate should be trusted. In addition, finding the mode is generally not the best way to interrogate a posterior. This paper addresses these issues by introducing a sampling framework to SAR image formation. A hierarchical Bayesian model is constructed using conjugate priors that directly incorporate coherent imaging and the problematic speckle phenomenon which is known to degrade image quality. Samples of the resulting posterior as well as parameters governing speckle and noise are obtained using a Gibbs sampler. These samples may then be used to compute estimates, and also to derive other statistics like variance which aid in uncertainty quantification. The latter information is particularly important in SAR, where ground truth images even for synthetically-created examples are typically unknown. An example result using real-world data shows that the sampling-based approach introduced here to SAR image formation provides parameter-free estimates with improved contrast and significantly reduced speckle, as well as unprecedented uncertainty quantification information.

</details>

<details>

<summary>2020-07-13 14:28:25 - Equilibrium Refinement in Finite Evidence Games</summary>

- *Shaofei Jiang*

- `2007.06403v1` - [abs](http://arxiv.org/abs/2007.06403v1) - [pdf](http://arxiv.org/pdf/2007.06403v1)

> Evidence games study situations where a sender persuades a receiver by selectively disclosing hard evidence about an unknown state of the world. Evidence games often have multiple equilibria. Hart et al. (2017) propose to focus on truth-leaning equilibria, i.e., perfect Bayesian equilibria where the sender prefers disclosing truthfully when indifferent, and the receiver takes off-path disclosure at face value. They show that a truth-leaning equilibrium is an equilibrium of a perturbed game where the sender has an infinitesimal reward for truth-telling. We show that, when the receiver's action space is finite, truth-leaning equilibrium may fail to exist, and it is not equivalent to equilibrium of the perturbed game. To restore existence, we introduce a disturbed game with a small uncertainty about the receiver's payoff. A purifiable equilibrium is a truth-leaning equilibrium in an infinitesimally disturbed game. It exists and features a simple characterization. A truth-leaning equilibrium that is also purifiable is an equilibrium of the perturbed game.

</details>

<details>

<summary>2020-07-13 21:39:18 - Dynamics of coordinate ascent variational inference: A case study in 2D Ising models</summary>

- *Sean Plummer, Debdeep Pati, Anirban Bhattacharya*

- `2007.06715v1` - [abs](http://arxiv.org/abs/2007.06715v1) - [pdf](http://arxiv.org/pdf/2007.06715v1)

> Variational algorithms have gained prominence over the past two decades as a scalable computational environment for Bayesian inference. In this article, we explore tools from the dynamical systems literature to study convergence of coordinate ascent algorithms for mean field variational inference. Focusing on the Ising model defined on two nodes, we fully characterize the dynamics of the sequential coordinate ascent algorithm and its parallel version. We observe that in the regime where the objective function is convex, both the algorithms are stable and exhibit convergence to the unique fixed point. Our analyses reveal interesting {\em discordances} between these two versions of the algorithm in the region when the objective function is non-convex. In fact, the parallel version exhibits a periodic oscillatory behavior which is absent in the sequential version. Drawing intuition from the Markov chain Monte Carlo literature, we {\em empirically} show that a parameter expansion of the Ising model, popularly called as the Edward--Sokal coupling, leads to an enlargement of the regime of convergence to the global optima.

</details>

<details>

<summary>2020-07-14 02:19:25 - Verification of ML Systems via Reparameterization</summary>

- *Jean-Baptiste Tristan, Joseph Tassarotti, Koundinya Vajjha, Michael L. Wick, Anindya Banerjee*

- `2007.06776v1` - [abs](http://arxiv.org/abs/2007.06776v1) - [pdf](http://arxiv.org/pdf/2007.06776v1)

> As machine learning is increasingly used in essential systems, it is important to reduce or eliminate the incidence of serious bugs. A growing body of research has developed machine learning algorithms with formal guarantees about performance, robustness, or fairness. Yet, the analysis of these algorithms is often complex, and implementing such systems in practice introduces room for error. Proof assistants can be used to formally verify machine learning systems by constructing machine checked proofs of correctness that rule out such bugs. However, reasoning about probabilistic claims inside of a proof assistant remains challenging. We show how a probabilistic program can be automatically represented in a theorem prover using the concept of \emph{reparameterization}, and how some of the tedious proofs of measurability can be generated automatically from the probabilistic program. To demonstrate that this approach is broad enough to handle rather different types of machine learning systems, we verify both a classic result from statistical learning theory (PAC-learnability of decision stumps) and prove that the null model used in a Bayesian hypothesis test satisfies a fairness criterion called demographic parity.

</details>

<details>

<summary>2020-07-14 03:56:17 - Kernel-based Approximate Bayesian Inference for Exponential Family Random Graph Models</summary>

- *Fan Yin, Carter T. Butts*

- `2004.08064v2` - [abs](http://arxiv.org/abs/2004.08064v2) - [pdf](http://arxiv.org/pdf/2004.08064v2)

> Bayesian inference for exponential family random graph models (ERGMs) is a doubly-intractable problem because of the intractability of both the likelihood and posterior normalizing factor. Auxiliary variable based Markov Chain Monte Carlo (MCMC) methods for this problem are asymptotically exact but computationally demanding, and are difficult to extend to modified ERGM families. In this work, we propose a kernel-based approximate Bayesian computation algorithm for fitting ERGMs. By employing an adaptive importance sampling technique, we greatly improve the efficiency of the sampling step. Though approximate, our easily parallelizable approach is yields comparable accuracy to state-of-the-art methods with substantial improvements in compute time on multi-core hardware. Our approach also flexibly accommodates both algorithmic enhancements (including improved learning algorithms for estimating conditional expectations) and extensions to non-standard cases such as inference from non-sufficient statistics. We demonstrate the performance of this approach on two well-known network data sets, comparing its accuracy and efficiency with results obtained using the approximate exchange algorithm. Our tests show a wallclock time advantage of up to 50% with five cores, and the ability to fit models in 1/5th the time at 30 cores; further speed enhancements are possible when more cores are available.

</details>

<details>

<summary>2020-07-14 08:50:00 - Dynkin games with incomplete and asymmetric information</summary>

- *Tiziano De Angelis, Erik Ekström, Kristoffer Glover*

- `1810.07674v5` - [abs](http://arxiv.org/abs/1810.07674v5) - [pdf](http://arxiv.org/pdf/1810.07674v5)

> We study the value and the optimal strategies for a two-player zero-sum optimal stopping game with incomplete and asymmetric information. In our Bayesian set-up, the drift of the underlying diffusion process is unknown to one player (incomplete information feature), but known to the other one (asymmetric information feature). We formulate the problem and reduce it to a fully Markovian setup where the uninformed player optimises over stopping times and the informed one uses randomised stopping times in order to hide their informational advantage. Then we provide a general verification result which allows us to find the value of the game and players' optimal strategies by solving suitable quasi-variational inequalities with some non-standard constraints. Finally, we study an example with linear payoffs, in which an explicit solution of the corresponding quasi-variational inequalities can be obtained.

</details>

<details>

<summary>2020-07-14 12:23:18 - Do Online Courses Provide an Equal Educational Value Compared to In-Person Classroom Teaching? Evidence from US Survey Data using Quantile Regression</summary>

- *Manini Ojha, Mohammad Arshad Rahman*

- `2007.06994v1` - [abs](http://arxiv.org/abs/2007.06994v1) - [pdf](http://arxiv.org/pdf/2007.06994v1)

> Education has traditionally been classroom-oriented with a gradual growth of online courses in recent times. However, the outbreak of the COVID-19 pandemic has dramatically accelerated the shift to online classes. Associated with this learning format is the question: what do people think about the educational value of an online course compared to a course taken in-person in a classroom? This paper addresses the question and presents a Bayesian quantile analysis of public opinion using a nationally representative survey data from the United States. Our findings show that previous participation in online courses and full-time employment status favor the educational value of online courses. We also find that the older demographic and females have a greater propensity for online education. In contrast, highly educated individuals have a lower willingness towards online education vis-\`a-vis traditional classes. Besides, covariate effects show heterogeneity across quantiles which cannot be captured using probit or logit models.

</details>

<details>

<summary>2020-07-14 15:45:28 - Causal Inference using Gaussian Processes with Structured Latent Confounders</summary>

- *Sam Witty, Kenta Takatsu, David Jensen, Vikash Mansinghka*

- `2007.07127v1` - [abs](http://arxiv.org/abs/2007.07127v1) - [pdf](http://arxiv.org/pdf/2007.07127v1)

> Latent confounders---unobserved variables that influence both treatment and outcome---can bias estimates of causal effects. In some cases, these confounders are shared across observations, e.g. all students taking a course are influenced by the course's difficulty in addition to any educational interventions they receive individually. This paper shows how to semiparametrically model latent confounders that have this structure and thereby improve estimates of causal effects. The key innovations are a hierarchical Bayesian model, Gaussian processes with structured latent confounders (GP-SLC), and a Monte Carlo inference algorithm for this model based on elliptical slice sampling. GP-SLC provides principled Bayesian uncertainty estimates of individual treatment effect with minimal assumptions about the functional forms relating confounders, covariates, treatment, and outcome. Finally, this paper shows GP-SLC is competitive with or more accurate than widely used causal inference techniques on three benchmark datasets, including the Infant Health and Development Program and a dataset showing the effect of changing temperatures on state-wide energy consumption across New England.

</details>

<details>

<summary>2020-07-14 16:56:06 - Towards Credit-Fraud Detection via Sparsely Varying Gaussian Approximations</summary>

- *Harshit Sharma, Harsh K. Gandhi, Apoorv Jain*

- `2007.07181v1` - [abs](http://arxiv.org/abs/2007.07181v1) - [pdf](http://arxiv.org/pdf/2007.07181v1)

> Fraudulent activities are an expensive problem for many financial institutions, costing billions of dollars to corporations annually. More commonly occurring activities in this regard are credit card frauds. In this context, the credit card fraud detection concept has been developed over the lines of incorporating the uncertainty in our prediction system to ensure better judgment in such a crucial task. We propose to use a sparse Gaussian classification method to work with the large data-set and use the concept of pseudo or inducing inputs. We perform the same with different sets of kernels and the different number of inducing data points to show the best accuracy was obtained with the selection of RBF kernel with a higher number of inducing points. Our approach was able to work over large financial data given the stochastic nature of our method employed and also good test accuracy with low variance over the prediction suggesting confidence and robustness in our model. Using the methodologies of Bayesian learning techniques with the incorporated inducing points phenomenon, are successfully able to obtain a healthy accuracy and a high confidence score.

</details>

<details>

<summary>2020-07-14 19:21:32 - Implicit Generative Modeling for Efficient Exploration</summary>

- *Neale Ratzlaff, Qinxun Bai, Li Fuxin, Wei Xu*

- `1911.08017v3` - [abs](http://arxiv.org/abs/1911.08017v3) - [pdf](http://arxiv.org/pdf/1911.08017v3)

> Efficient exploration remains a challenging problem in reinforcement learning, especially for those tasks where rewards from environments are sparse. A commonly used approach for exploring such environments is to introduce some "intrinsic" reward. In this work, we focus on model uncertainty estimation as an intrinsic reward for efficient exploration. In particular, we introduce an implicit generative modeling approach to estimate a Bayesian uncertainty of the agent's belief of the environment dynamics. Each random draw from our generative model is a neural network that instantiates the dynamic function, hence multiple draws would approximate the posterior, and the variance in the future prediction based on this posterior is used as an intrinsic reward for exploration. We design a training algorithm for our generative model based on the amortized Stein Variational Gradient Descent. In experiments, we compare our implementation with state-of-the-art intrinsic reward-based exploration approaches, including two recent approaches based on an ensemble of dynamic models. In challenging exploration tasks, our implicit generative model consistently outperforms competing approaches regarding data efficiency in exploration.

</details>

<details>

<summary>2020-07-14 21:25:39 - Streaming Probabilistic Deep Tensor Factorization</summary>

- *Shikai Fang, Zheng Wang, Zhimeng Pan, Ji Liu, Shandian Zhe*

- `2007.07367v1` - [abs](http://arxiv.org/abs/2007.07367v1) - [pdf](http://arxiv.org/pdf/2007.07367v1)

> Despite the success of existing tensor factorization methods, most of them conduct a multilinear decomposition, and rarely exploit powerful modeling frameworks, like deep neural networks, to capture a variety of complicated interactions in data. More important, for highly expressive, deep factorization, we lack an effective approach to handle streaming data, which are ubiquitous in real-world applications. To address these issues, we propose SPIDER, a Streaming ProbabilistIc Deep tEnsoR factorization method. We first use Bayesian neural networks (NNs) to construct a deep tensor factorization model. We assign a spike-and-slab prior over the NN weights to encourage sparsity and prevent overfitting. We then use Taylor expansions and moment matching to approximate the posterior of the NN output and calculate the running model evidence, based on which we develop an efficient streaming posterior inference algorithm in the assumed-density-filtering and expectation propagation framework. Our algorithm provides responsive incremental updates for the posterior of the latent factors and NN weights upon receiving new tensor entries, and meanwhile select and inhibit redundant/useless weights. We show the advantages of our approach in four real-world applications.

</details>

<details>

<summary>2020-07-15 07:07:28 - Bayesian Variational Autoencoders for Unsupervised Out-of-Distribution Detection</summary>

- *Erik Daxberger, José Miguel Hernández-Lobato*

- `1912.05651v3` - [abs](http://arxiv.org/abs/1912.05651v3) - [pdf](http://arxiv.org/pdf/1912.05651v3)

> Despite their successes, deep neural networks may make unreliable predictions when faced with test data drawn from a distribution different to that of the training data, constituting a major problem for AI safety. While this has recently motivated the development of methods to detect such out-of-distribution (OoD) inputs, a robust solution is still lacking. We propose a new probabilistic, unsupervised approach to this problem based on a Bayesian variational autoencoder model, which estimates a full posterior distribution over the decoder parameters using stochastic gradient Markov chain Monte Carlo, instead of fitting a point estimate. We describe how information-theoretic measures based on this posterior can then be used to detect OoD inputs both in input space and in the model's latent space. We empirically demonstrate the effectiveness of our proposed approach.

</details>

<details>

<summary>2020-07-15 14:50:20 - Scalable Bayesian modeling for smoothing disease risks in large spatial data sets</summary>

- *E. Orozco-Acosta, A. Adin, M. D. Ugarte*

- `2007.07724v1` - [abs](http://arxiv.org/abs/2007.07724v1) - [pdf](http://arxiv.org/pdf/2007.07724v1)

> Several methods have been proposed in the spatial statistics literature for the analysis of big data sets in continuous domains. However, new methods for analyzing high-dimensional areal data are still scarce. Here, we propose a scalable Bayesian modeling approach for smoothing mortality (or incidence) risks in high-dimensional data, that is, when the number of small areas is very large. The method is implemented in the R add-on package bigDM. Model fitting and inference is based on the idea of "divide and conquer" and use integrated nested Laplace approximations and numerical integration. We analyze the proposal's empirical performance in a comprehensive simulation study that consider two model-free settings. Finally, the methodology is applied to analyze male colorectal cancer mortality in Spanish municipalities showing its benefits with regard to the standard approach in terms of goodness of fit and computational time.

</details>

<details>

<summary>2020-07-15 17:12:52 - A Bayesian Multiple Testing Paradigm for Model Selection in Inverse Regression Problems</summary>

- *Debashis Chatterjee, Sourabh Bhattacharya*

- `2007.07847v1` - [abs](http://arxiv.org/abs/2007.07847v1) - [pdf](http://arxiv.org/pdf/2007.07847v1)

> In this article, we propose a novel Bayesian multiple testing formulation for model and variable selection in inverse setups, judiciously embedding the idea of inverse reference distributions proposed by Bhattacharya (2013) in a mixture framework consisting of the competing models. We develop the theory and methods in the general context encompassing parametric and nonparametric competing models, dependent data, as well as misspecifications. Our investigation shows that asymptotically the multiple testing procedure almost surely selects the best possible inverse model that minimizes the minimum Kullback-Leibler divergence from the true model. We also show that the error rates, namely, versions of the false discovery rate and the false non-discovery rate converge to zero almost surely as the sample size goes to infinity. Asymptotic {\alpha}-control of versions of the false discovery rate and its impact on the convergence of false non-discovery rate versions, are also investigated.   Our simulation experiments involve small sample based selection among inverse Poisson log regression and inverse geometric logit and probit regression, where the regressions are either linear or based on Gaussian processes. Additionally, variable selection is also considered. Our multiple testing results turn out to be very encouraging in the sense of selecting the best models in all the non-misspecified and misspecified cases.

</details>

<details>

<summary>2020-07-15 20:36:30 - Faster Uncertainty Quantification for Inverse Problems with Conditional Normalizing Flows</summary>

- *Ali Siahkoohi, Gabrio Rizzuti, Philipp A. Witte, Felix J. Herrmann*

- `2007.07985v1` - [abs](http://arxiv.org/abs/2007.07985v1) - [pdf](http://arxiv.org/pdf/2007.07985v1)

> In inverse problems, we often have access to data consisting of paired samples $(x,y)\sim p_{X,Y}(x,y)$ where $y$ are partial observations of a physical system, and $x$ represents the unknowns of the problem. Under these circumstances, we can employ supervised training to learn a solution $x$ and its uncertainty from the observations $y$. We refer to this problem as the "supervised" case. However, the data $y\sim p_{Y}(y)$ collected at one point could be distributed differently than observations $y'\sim p_{Y}'(y')$, relevant for a current set of problems. In the context of Bayesian inference, we propose a two-step scheme, which makes use of normalizing flows and joint data to train a conditional generator $q_{\theta}(x|y)$ to approximate the target posterior density $p_{X|Y}(x|y)$. Additionally, this preliminary phase provides a density function $q_{\theta}(x|y)$, which can be recast as a prior for the "unsupervised" problem, e.g.~when only the observations $y'\sim p_{Y}'(y')$, a likelihood model $y'|x$, and a prior on $x'$ are known. We then train another invertible generator with output density $q'_{\phi}(x|y')$ specifically for $y'$, allowing us to sample from the posterior $p_{X|Y}'(x|y')$. We present some synthetic results that demonstrate considerable training speedup when reusing the pretrained network $q_{\theta}(x|y')$ as a warm start or preconditioning for approximating $p_{X|Y}'(x|y')$, instead of learning from scratch. This training modality can be interpreted as an instance of transfer learning. This result is particularly relevant for large-scale inverse problems that employ expensive numerical simulations.

</details>

<details>

<summary>2020-07-16 00:38:18 - Heterogeneity Learning for SIRS model: an Application to the COVID-19</summary>

- *Guanyu Hu, Junxian Geng*

- `2007.08047v1` - [abs](http://arxiv.org/abs/2007.08047v1) - [pdf](http://arxiv.org/pdf/2007.08047v1)

> We propose a Bayesian Heterogeneity Learning approach for Susceptible-Infected-Removal-Susceptible (SIRS) model that allows underlying clustering patterns for transmission rate, recovery rate, and loss of immunity rate for the latest coronavirus (COVID-19) among different regions. Our proposed method provides simultaneously inference on parameter estimation and clustering information which contains both number of clusters and cluster configurations. Specifically, our key idea is to formulates the SIRS model into a hierarchical form and assign the Mixture of Finite mixtures priors for heterogeneity learning. The properties of the proposed models are examined and a Markov chain Monte Carlo sampling algorithm is used to sample from the posterior distribution. Extensive simulation studies are carried out to examine empirical performance of the proposed methods. We further apply the proposed methodology to analyze the state level COVID-19 data in U.S.

</details>

<details>

<summary>2020-07-16 02:45:49 - Solving Bayesian Risk Optimization via Nested Stochastic Gradient Estimation</summary>

- *Sait Cakmak, Di Wu, Enlu Zhou*

- `2007.05860v2` - [abs](http://arxiv.org/abs/2007.05860v2) - [pdf](http://arxiv.org/pdf/2007.05860v2)

> In this paper, we aim to solve Bayesian Risk Optimization (BRO), which is a recently proposed framework that formulates simulation optimization under input uncertainty. In order to efficiently solve the BRO problem, we derive nested stochastic gradient estimators and propose corresponding stochastic approximation algorithms. We show that our gradient estimators are asymptotically unbiased and consistent, and that the algorithms converge asymptotically. We demonstrate the empirical performance of the algorithms on a two-sided market model. Our estimators are of independent interest in extending the literature of stochastic gradient estimation to the case of nested risk functions.

</details>

<details>

<summary>2020-07-16 15:26:23 - The role of collider bias in understanding statistics on racially biased policing</summary>

- *Norman Fenton, Martin Neil, Steven Frazier*

- `2007.08406v1` - [abs](http://arxiv.org/abs/2007.08406v1) - [pdf](http://arxiv.org/pdf/2007.08406v1)

> Contradictory conclusions have been made about whether unarmed blacks are more likely to be shot by police than unarmed whites using the same data. The problem is that, by relying only on data of 'police encounters', there is the possibility that genuine bias can be hidden. We provide a causal Bayesian network model to explain this bias, which is called collider bias or Berkson's paradox, and show how the different conclusions arise from the same model and data. We also show that causal Bayesian networks provide the ideal formalism for considering alternative hypotheses and explanations of bias.

</details>

<details>

<summary>2020-07-16 19:33:53 - A Bayesian nonparametric test for conditional independence</summary>

- *Onur Teymur, Sarah Filippi*

- `1910.11219v2` - [abs](http://arxiv.org/abs/1910.11219v2) - [pdf](http://arxiv.org/pdf/1910.11219v2)

> This article introduces a Bayesian nonparametric method for quantifying the relative evidence in a dataset in favour of the dependence or independence of two variables conditional on a third. The approach uses Polya tree priors on spaces of conditional probability densities, accounting for uncertainty in the form of the underlying distributions in a nonparametric way. The Bayesian perspective provides an inherently symmetric probability measure of conditional dependence or independence, a feature particularly advantageous in causal discovery and not employed in existing procedures of this type.

</details>

<details>

<summary>2020-07-17 01:12:38 - Incremental Bayesian tensor learning for structural monitoring data imputation and response forecasting</summary>

- *Pu Ren, Xinyu Chen, Lijun Sun, Hao Sun*

- `2007.00790v3` - [abs](http://arxiv.org/abs/2007.00790v3) - [pdf](http://arxiv.org/pdf/2007.00790v3)

> There has been increased interest in missing sensor data imputation, which is ubiquitous in the field of structural health monitoring (SHM) due to discontinuous sensing caused by sensor malfunction. To address this fundamental issue, this paper presents an incremental Bayesian tensor learning method for reconstruction of spatiotemporal missing data in SHM and forecasting of structural response. In particular, a spatiotemporal tensor is first constructed followed by Bayesian tensor factorization that extracts latent features for missing data imputation. To enable structural response forecasting based on incomplete sensing data, the tensor decomposition is further integrated with vector autoregression in an incremental learning scheme. The performance of the proposed approach is validated on continuous field-sensing data (including strain and temperature records) of a concrete bridge, based on the assumption that strain time histories are highly correlated to temperature recordings. The results indicate that the proposed probabilistic tensor learning approach is accurate and robust even in the presence of large rates of random missing, structured missing and their combination. The effect of rank selection on the imputation and prediction performance is also investigated. The results show that a better estimation accuracy can be achieved with a higher rank for random missing whereas a lower rank for structured missing.

</details>

<details>

<summary>2020-07-17 02:50:16 - Bayesian Analysis of Rank Data with Covariates and Heterogeneous Rankers</summary>

- *Xinran Li, Dingdong Yi, Jun S. Liu*

- `1607.06051v3` - [abs](http://arxiv.org/abs/1607.06051v3) - [pdf](http://arxiv.org/pdf/1607.06051v3)

> Data in the form of ranking lists are frequently encountered, and combining ranking results from different sources can potentially generate a better ranking list and help understand behaviors of the rankers. Of interest here are the rank data under the following settings: (i) covariate information available for the ranked entities; (ii) rankers of varying qualities or having different opinions; and (iii) incomplete ranking lists for non-overlapping subgroups. We review some key ideas built around the Thurstone model family by researchers in the past few decades and provide a unifying approach for Bayesian Analysis of Rank data with Covariates (BARC) and its extensions in handling heterogeneous rankers. With this Bayesian framework, we can study rankers' varying quality, cluster rankers' heterogeneous opinions, and measure the corresponding uncertainties. To enable an efficient Bayesian inference, we advocate a parameter-expanded Gibbs sampler to sample from the target posterior distribution. The posterior samples also result in a Bayesian aggregated ranking list, with credible intervals quantifying its uncertainty. We investigate and compare performances of the proposed methods and other rank aggregation methods in both simulation studies and two real-data examples.

</details>

<details>

<summary>2020-07-17 06:03:32 - Modeling Stochastic Microscopic Traffic Behaviors: a Physics Regularized Gaussian Process Approach</summary>

- *Yun Yuan, Qinzheng Wang, Xianfeng Terry Yang*

- `2007.10109v1` - [abs](http://arxiv.org/abs/2007.10109v1) - [pdf](http://arxiv.org/pdf/2007.10109v1)

> Modeling stochastic traffic behaviors at the microscopic level, such as car-following and lane-changing, is a crucial task to understand the interactions between individual vehicles in traffic streams. Leveraging a recently developed theory named physics regularized Gaussian process (PRGP), this study presents a stochastic microscopic traffic model that can capture the randomness and measure errors in the real world. Physical knowledge from classical car-following models is converted as physics regularizers, in the form of shadow Gaussian process (GP), of a multivariate PRGP for improving the modeling accuracy. More specifically, a Bayesian inference algorithm is developed to estimate the mean and kernel of GPs, and an enhanced latent force model is formulated to encode physical knowledge into stochastic processes. Also, based on the posterior regularization inference framework, an efficient stochastic optimization algorithm is developed to maximize the evidence lower-bound of the system likelihood. To evaluate the performance of the proposed models, this study conducts empirical studies on real-world vehicle trajectories from the NGSIM dataset. Since one unique feature of the proposed framework is the capability of capturing both car-following and lane-changing behaviors with one single model, numerical tests are carried out with two separated datasets, one contains lane-changing maneuvers and the other doesn't. The results show the proposed method outperforms the previous influential methods in estimation precision.

</details>

<details>

<summary>2020-07-17 07:24:50 - Incorporating compositional heterogeneity into Lie Markov models for phylogenetic inference</summary>

- *Naomi E. Hannaford, Sarah E. Heaps, Tom M. W. Nye, Tom A. Williams, T. Martin Embley*

- `2007.08511v2` - [abs](http://arxiv.org/abs/2007.08511v2) - [pdf](http://arxiv.org/pdf/2007.08511v2)

> Phylogenetics uses alignments of molecular sequence data to learn about evolutionary trees. Substitutions in sequences are modelled through a continuous-time Markov process, characterised by an instantaneous rate matrix, which standard models assume is time-reversible and stationary. These assumptions are biologically questionable and induce a likelihood function which is invariant to a tree's root position. This hampers inference because a tree's biological interpretation depends critically on where it is rooted. Relaxing both assumptions, we introduce a model whose likelihood can distinguish between rooted trees. The model is non-stationary, with step changes in the instantaneous rate matrix at each speciation event. Exploiting recent theoretical work, each rate matrix belongs to a non-reversible family of Lie Markov models. These models are closed under matrix multiplication, so our extension offers the conceptually appealing property that a tree and all its sub-trees could have arisen from the same family of non-stationary models.   We adopt a Bayesian approach, describe an MCMC algorithm for posterior inference and provide software. The biological insight that our model can provide is illustrated through an analysis in which non-reversible but stationary, and non-stationary but reversible models cannot identify a plausible root.

</details>

<details>

<summary>2020-07-17 10:26:49 - Convergence of Gaussian Process Regression with Estimated Hyper-parameters and Applications in Bayesian Inverse Problems</summary>

- *Aretha L Teckentrup*

- `1909.00232v3` - [abs](http://arxiv.org/abs/1909.00232v3) - [pdf](http://arxiv.org/pdf/1909.00232v3)

> This work is concerned with the convergence of Gaussian process regression. A particular focus is on hierarchical Gaussian process regression, where hyper-parameters appearing in the mean and covariance structure of the Gaussian process emulator are a-priori unknown, and are learnt from the data, along with the posterior mean and covariance. We work in the framework of empirical Bayes, where a point estimate of the hyper-parameters is computed, using the data, and then used within the standard Gaussian process prior to posterior update. We provide a convergence analysis that (i) holds for any continuous function $f$ to be emulated; and (ii) shows that convergence of Gaussian process regression is unaffected by the additional learning of hyper-parameters from data, and is guaranteed in a wide range of scenarios. As the primary motivation for the work is the use of Gaussian process regression to approximate the data likelihood in Bayesian inverse problems, we provide a bound on the error introduced in the Bayesian posterior distribution in this context.

</details>

<details>

<summary>2020-07-17 13:12:22 - Bayesian hierarchical models for the prediction of the driver flow and passenger waiting times in a stochastic carpooling service</summary>

- *Panayotis Papoutsis, Bertrand Michel, Anne Philippe, Tarn Duong*

- `2007.08962v1` - [abs](http://arxiv.org/abs/2007.08962v1) - [pdf](http://arxiv.org/pdf/2007.08962v1)

> Carpooling is an integral component in smart carbon-neutral cities, in particular to facilitate homework commuting. We study an innovative carpooling service developed by the start-up Ecov which specialises in homework commutes in peri-urban and rural regions. When a passenger makes a carpooling request, a designated driver is not assigned as in a traditional carpooling service; rather the passenger waits for the first driver, from a population of non-professional drivers who are already en route, to arrive. We propose a two-stage Bayesian hierarchical model to overcome the considerable difficulties, due to the sparsely observed driver and passenger data from an embryonic stochastic carpooling service, to deliver high-quality predictions of driver flow and passenger waiting times. The first stage focuses on the driver flow, whose predictions are aggregated at the daily level to compensate the data sparsity. The second stage processes this single daily driver flow into sub-daily (e.g. hourly) predictions of the passenger waiting times. We demonstrate that our model mostly outperforms frequentist and non-hierarchical Bayesian methods for observed data from operational carpooling service in Lyon, France and we also validated our model on simulated data.

</details>

<details>

<summary>2020-07-17 13:41:05 - PAC-Bayesian Contrastive Unsupervised Representation Learning</summary>

- *Kento Nozawa, Pascal Germain, Benjamin Guedj*

- `1910.04464v4` - [abs](http://arxiv.org/abs/1910.04464v4) - [pdf](http://arxiv.org/pdf/1910.04464v4)

> Contrastive unsupervised representation learning (CURL) is the state-of-the-art technique to learn representations (as a set of features) from unlabelled data. While CURL has collected several empirical successes recently, theoretical understanding of its performance was still missing. In a recent work, Arora et al. (2019) provide the first generalisation bounds for CURL, relying on a Rademacher complexity. We extend their framework to the flexible PAC-Bayes setting, allowing us to deal with the non-iid setting. We present PAC-Bayesian generalisation bounds for CURL, which are then used to derive a new representation learning algorithm. Numerical experiments on real-life datasets illustrate that our algorithm achieves competitive accuracy, and yields non-vacuous generalisation bounds.

</details>

<details>

<summary>2020-07-17 14:02:13 - Alleviating Privacy Attacks via Causal Learning</summary>

- *Shruti Tople, Amit Sharma, Aditya Nori*

- `1909.12732v4` - [abs](http://arxiv.org/abs/1909.12732v4) - [pdf](http://arxiv.org/pdf/1909.12732v4)

> Machine learning models, especially deep neural networks have been shown to be susceptible to privacy attacks such as membership inference where an adversary can detect whether a data point was used for training a black-box model. Such privacy risks are exacerbated when a model's predictions are used on an unseen data distribution. To alleviate privacy attacks, we demonstrate the benefit of predictive models that are based on the causal relationships between input features and the outcome. We first show that models learnt using causal structure generalize better to unseen data, especially on data from different distributions than the train distribution. Based on this generalization property, we establish a theoretical link between causality and privacy: compared to associational models, causal models provide stronger differential privacy guarantees and are more robust to membership inference attacks. Experiments on simulated Bayesian networks and the colored-MNIST dataset show that associational models exhibit upto 80% attack accuracy under different test distributions and sample sizes whereas causal models exhibit attack accuracy close to a random guess.

</details>

<details>

<summary>2020-07-17 15:04:19 - Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks</summary>

- *Agustinus Kristiadi, Matthias Hein, Philipp Hennig*

- `2002.10118v2` - [abs](http://arxiv.org/abs/2002.10118v2) - [pdf](http://arxiv.org/pdf/2002.10118v2)

> The point estimates of ReLU classification networks---arguably the most widely used neural network architecture---have been shown to yield arbitrarily high confidence far away from the training data. This architecture, in conjunction with a maximum a posteriori estimation scheme, is thus not calibrated nor robust. Approximate Bayesian inference has been empirically demonstrated to improve predictive uncertainty in neural networks, although the theoretical analysis of such Bayesian approximations is limited. We theoretically analyze approximate Gaussian distributions on the weights of ReLU networks and show that they fix the overconfidence problem. Furthermore, we show that even a simplistic, thus cheap, Bayesian approximation, also fixes these issues. This indicates that a sufficient condition for a calibrated uncertainty on a ReLU network is "to be a bit Bayesian". These theoretical results validate the usage of last-layer Bayesian approximation and motivate a range of a fidelity-cost trade-off. We further validate these findings empirically via various standard experiments using common deep ReLU networks and Laplace approximations.

</details>

<details>

<summary>2020-07-17 17:12:38 - Estimating COVID-19 cases and reproduction number in Mexico</summary>

- *Michelle Anzarut, Luis Felipe González, Sonia Mendizábal, María Teresa Ortiz*

- `2007.09117v1` - [abs](http://arxiv.org/abs/2007.09117v1) - [pdf](http://arxiv.org/pdf/2007.09117v1)

> In this report we fit a semi-mechanistic Bayesian hierarchical model to describe the Mexican COVID-19 epidemic. We obtain two epidemiological measures: the number of infections and the reproduction number. Estimations are based on death data. Hence, we expect our estimates to be more accurate than the attack rates estimated from the reported number of cases.

</details>

<details>

<summary>2020-07-17 18:29:49 - Network Learning Approaches to study World Happiness</summary>

- *Siddharth Dixit, Meghna Chaudhary, Niteesh Sahni*

- `2007.09181v1` - [abs](http://arxiv.org/abs/2007.09181v1) - [pdf](http://arxiv.org/pdf/2007.09181v1)

> The United Nations in its 2011 resolution declared the pursuit of happiness a fundamental human goal and proposed public and economic policies centered around happiness. In this paper we used 2 types of computational strategies viz. Predictive Modelling and Bayesian Networks (BNs) to model the processed historical happiness index data of 156 nations published by UN since 2012. We attacked the problem of prediction using General Regression Neural Networks (GRNNs) and show that it out performs other state of the art predictive models. To understand causal links amongst key features that have been proven to have a significant impact on world happiness, we first used a manual discretization scheme to discretize continuous variables into 3 levels viz. Low, Medium and High. A consensus World Happiness BN structure was then fixed after amalgamating information by learning 10000 different BNs using bootstrapping. Lastly, exact inference through conditional probability queries was used on this BN to unravel interesting relationships among the important features affecting happiness which would be useful in policy making.

</details>

<details>

<summary>2020-07-17 20:28:06 - Latent Dirichlet Analysis of Categorical Survey Responses</summary>

- *Evan Munro, Serena Ng*

- `1910.04883v3` - [abs](http://arxiv.org/abs/1910.04883v3) - [pdf](http://arxiv.org/pdf/1910.04883v3)

> Beliefs are important determinants of an individual's choices and economic outcomes, so understanding how they comove and differ across individuals is of considerable interest. Researchers often rely on surveys that report individual beliefs as qualitative data. We propose using a Bayesian hierarchical latent class model to analyze the comovements and observed heterogeneity in categorical survey responses. We show that the statistical model corresponds to an economic structural model of information acquisition, which guides interpretation and estimation of the model parameters. An algorithm based on stochastic optimization is proposed to estimate a model for repeated surveys when responses follow a dynamic structure and conjugate priors are not appropriate. Guidance on selecting the number of belief types is also provided. Two examples are considered. The first shows that there is information in the Michigan survey responses beyond the consumer sentiment index that is officially published. The second shows that belief types constructed from survey responses can be used in a subsequent analysis to estimate heterogeneous returns to education.

</details>

<details>

<summary>2020-07-17 21:31:26 - Global estimation of unintended pregnancy and abortion using a Bayesian hierarchical random walk model</summary>

- *Jonathan Marc Bearak, Anna Popinchalk, Bela Ganatra, Ann-Beth Moller, Özge Tunçalp, Cynthia Beavin, Lorraine Kwok, Leontine Alkema*

- `2007.09246v1` - [abs](http://arxiv.org/abs/2007.09246v1) - [pdf](http://arxiv.org/pdf/2007.09246v1)

> Unintended pregnancy and abortion estimates are needed to inform and motivate investment in global health programmes and policies. Variability in the availability and reliability of data poses challenges for producing estimates. We developed a Bayesian model that simultaneously estimates incidence of unintended pregnancy and abortion for 195 countries and territories. Our modelling strategy was informed by the proximate determinants of fertility with (i) incidence of unintended pregnancy defined by the number of women (grouped by marital and contraceptive use status) and their respective pregnancy rates, and (ii) abortion incidence defined by group-specific pregnancies and propensities to have an abortion. Hierarchical random walk models are used to estimate country-group-period-specific pregnancy rates and propensities to abort.

</details>

<details>

<summary>2020-07-17 22:15:36 - Inferring Signaling Pathways with Probabilistic Programming</summary>

- *David Merrell, Anthony Gitter*

- `2005.14062v2` - [abs](http://arxiv.org/abs/2005.14062v2) - [pdf](http://arxiv.org/pdf/2005.14062v2)

> Cells regulate themselves via dizzyingly complex biochemical processes called signaling pathways. These are usually depicted as a network, where nodes represent proteins and edges indicate their influence on each other. In order to understand diseases and therapies at the cellular level, it is crucial to have an accurate understanding of the signaling pathways at work. Since signaling pathways can be modified by disease, the ability to infer signaling pathways from condition- or patient-specific data is highly valuable. A variety of techniques exist for inferring signaling pathways. We build on past works that formulate signaling pathway inference as a Dynamic Bayesian Network structure estimation problem on phosphoproteomic time course data. We take a Bayesian approach, using Markov Chain Monte Carlo to estimate a posterior distribution over possible Dynamic Bayesian Network structures. Our primary contributions are (i) a novel proposal distribution that efficiently samples sparse graphs and (ii) the relaxation of common restrictive modeling assumptions. We implement our method, named Sparse Signaling Pathway Sampling, in Julia using the Gen probabilistic programming language. Probabilistic programming is a powerful methodology for building statistical models. The resulting code is modular, extensible, and legible. The Gen language, in particular, allows us to customize our inference procedure for biological graphs and ensure efficient sampling. We evaluate our algorithm on simulated data and the HPN-DREAM pathway reconstruction challenge, comparing our performance against a variety of baseline methods. Our results demonstrate the vast potential for probabilistic programming, and Gen specifically, for biological network inference. Find the full codebase at https://github.com/gitter-lab/ssps

</details>

<details>

<summary>2020-07-18 21:36:31 - Probabilistic Neighbourhood Component Analysis: Sample Efficient Uncertainty Estimation in Deep Learning</summary>

- *Ankur Mallick, Chaitanya Dwivedi, Bhavya Kailkhura, Gauri Joshi, T. Yong-Jin Han*

- `2007.10800v1` - [abs](http://arxiv.org/abs/2007.10800v1) - [pdf](http://arxiv.org/pdf/2007.10800v1)

> While Deep Neural Networks (DNNs) achieve state-of-the-art accuracy in various applications, they often fall short in accurately estimating their predictive uncertainty and, in turn, fail to recognize when these predictions may be wrong. Several uncertainty-aware models, such as Bayesian Neural Network (BNNs) and Deep Ensembles have been proposed in the literature for quantifying predictive uncertainty. However, research in this area has been largely confined to the big data regime. In this work, we show that the uncertainty estimation capability of state-of-the-art BNNs and Deep Ensemble models degrades significantly when the amount of training data is small. To address the issue of accurate uncertainty estimation in the small-data regime, we propose a probabilistic generalization of the popular sample-efficient non-parametric kNN approach. Our approach enables deep kNN classifier to accurately quantify underlying uncertainties in its prediction. We demonstrate the usefulness of the proposed approach by achieving superior uncertainty quantification as compared to state-of-the-art on a real-world application of COVID-19 diagnosis from chest X-Rays. Our code is available at https://github.com/ankurmallick/sample-efficient-uq

</details>

<details>

<summary>2020-07-19 03:35:35 - Model identification for ARMA time series through convolutional neural networks</summary>

- *Wai Hoh Tang, Adrian Röllin*

- `1804.04299v2` - [abs](http://arxiv.org/abs/1804.04299v2) - [pdf](http://arxiv.org/pdf/1804.04299v2)

> In this paper, we use convolutional neural networks to address the problem of model identification for autoregressive moving average time series models. We compare the performance of several neural network architectures, trained on simulated time series, with likelihood based methods, in particular the Akaike and Bayesian information criteria. We find that our neural networks can significantly outperform these likelihood based methods in terms of accuracy and, by orders of magnitude, in terms of speed.

</details>

<details>

<summary>2020-07-19 15:07:43 - Bayesian Predictive Density Estimation for a Chi-squared Model Using Information from a Normal Observation with Unknown Mean and Variance</summary>

- *Yasuyuki Hamura, Tatsuya Kubokawa*

- `2006.07052v2` - [abs](http://arxiv.org/abs/2006.07052v2) - [pdf](http://arxiv.org/pdf/2006.07052v2)

> In this paper, we consider the problem of estimating the density function of a Chi-squared variable on the basis of observations of another Chi-squared variable and a normal variable under the Kullback-Leibler divergence. We assume that these variables have a common unknown scale parameter and that the mean of the normal variable is also unknown. We compare the risk functions of two Bayesian predictive densities: one with respect to a hierarchical shrinkage prior and the other based on a noninformative prior. The hierarchical Bayesian predictive density depends on the normal variable while the Bayesian predictive density based on the noninformative prior does not. Sufficient conditions for the former to dominate the latter are obtained. These predictive densities are compared by simulation.

</details>

<details>

<summary>2020-07-19 15:59:08 - Maximum Likelihood under constraints: Degeneracies and Random Critical Points</summary>

- *Subhro Ghosh, Sanjay Chaudhuri*

- `1910.01396v2` - [abs](http://arxiv.org/abs/1910.01396v2) - [pdf](http://arxiv.org/pdf/1910.01396v2)

> We investigate the problem of semi-parametric maximum likelihood under constraints on summary statistics. Such a procedure results in a discrete probability distribution that maximises the likelihood among all such distributions under the specified constraints (called estimating equations), and is an approximation to the underlying population distribution. The study of such empirical likelihood originates from the seminal work of Owen. We investigate this procedure in the setting of mis-specified (or biased) estimating equations, i.e. when the null hypothesis is not true. We establish that the behaviour of the optimal distribution under such mis-specification differ markedly from their properties under the null, i.e. when the estimating equations are unbiased and correctly specified. This is manifested by certain degeneracies in the optimal distribution which define the likelihood. Such degeneracies are not observed under the null. Furthermore, we establish an anomalous behaviour of the log-likelihood based Wilks statistic, which, unlike under the null, does not exhibit a chi-squared limit. In the Bayesian setting, we rigorously establish the posterior consistency of procedures based on these ideas, where instead of a parametric likelihood, an empirical likelihood is used to define the posterior distribution. In particular, we show that this posterior, as a random probability measure, rapidly converges to the delta measure at the true parameter value. A novel feature of our approach is the investigation of critical points of random functions in the context of such empirical likelihood. In particular, we obtain the location and the mass of the degenerate optimal weights as the leading and sub-leading terms in a canonical expansion of a particular critical point of a random function that is naturally associated with the model.

</details>

<details>

<summary>2020-07-19 17:30:35 - Integration of max-stable processes and Bayesian model averaging to predict extreme climatic events in multi-model ensembles</summary>

- *Yonggwan Shin, Youngsaeng Lee, Juntae Choi, Jeong-Soo Park*

- `2007.09726v1` - [abs](http://arxiv.org/abs/2007.09726v1) - [pdf](http://arxiv.org/pdf/2007.09726v1)

> Projections of changes in extreme climate are sometimes predicted by using multi-model ensemble methods such as Bayesian model averaging (BMA) embedded with the generalized extreme value (GEV) distribution. BMA is a popular method for combining the forecasts of individual simulation models by weighted averaging and characterizing the uncertainty induced by simulating the model structure. This method is referred to as the GEV-embedded BMA. It is, however, based on a point-wise analysis of extreme events, which means it overlooks the spatial dependency between nearby grid cells. Instead of a point-wise model, a spatial extreme model such as the max-stable process (MSP) is often employed to improve precision by considering spatial dependency. We propose an approach that integrates the MSP into BMA, which is referred to as the MSP-BMA herein. The superiority of the proposed method over the GEV-embedded BMA is demonstrated by using extreme rainfall intensity data on the Korean peninsula from Coupled Model Intercomparison Project Phase 5 (CMIP5) multi-models. The reanalysis data called APHRODITE (Asian Precipitation Highly-Resolved Observational Data Integration Towards Evaluation, v1101) and 17 CMIP5 models are examined for 10 grid boxes in Korea. In this example, the MSP-BMA achieves a variance reduction over the GEV-embedded BMA. The bias inflation by MSP-BMA over the GEV-embedded BMA is also discussed. A by-product technical advantage of the MSP-BMA is that tedious `regridding' is not required before and after the analysis while it should be done for the GEV-embedded BMA.

</details>

<details>

<summary>2020-07-19 21:28:11 - Approximate Inference Turns Deep Networks into Gaussian Processes</summary>

- *Mohammad Emtiyaz Khan, Alexander Immer, Ehsan Abedi, Maciej Korzepa*

- `1906.01930v3` - [abs](http://arxiv.org/abs/1906.01930v3) - [pdf](http://arxiv.org/pdf/1906.01930v3)

> Deep neural networks (DNN) and Gaussian processes (GP) are two powerful models with several theoretical connections relating them, but the relationship between their training methods is not well understood. In this paper, we show that certain Gaussian posterior approximations for Bayesian DNNs are equivalent to GP posteriors. This enables us to relate solutions and iterations of a deep-learning algorithm to GP inference. As a result, we can obtain a GP kernel and a nonlinear feature map while training a DNN. Surprisingly, the resulting kernel is the neural tangent kernel. We show kernels obtained on real datasets and demonstrate the use of the GP marginal likelihood to tune hyperparameters of DNNs. Our work aims to facilitate further research on combining DNNs and GPs in practical settings.

</details>

<details>

<summary>2020-07-20 02:29:08 - Bayesian EWMA and CUSUM Control Charts Under Different Loss Functions</summary>

- *Chelsea Mitchell, Abdel-Salam Abdel-Salam, D'Arcy Mays*

- `2007.09844v1` - [abs](http://arxiv.org/abs/2007.09844v1) - [pdf](http://arxiv.org/pdf/2007.09844v1)

> The Exponentially Weighted Moving Average (EWMA) and Cumulative Sum (CUSUM) control charts have been used in profile monitoring to track drift shifts that occur in a monitored process. We construct Bayesian EWMA and Bayesian CUSUM charts informed by posterior and posterior predictive distributions using different loss functions, prior distributions, and likelihood distributions. A simulation study is performed, and the performance of the charts are evaluated via average run length (ARL), standard deviation of the run length (SDRL), average time to signal (ATS), and standard deviation of time to signal (SDTS). A sensitivity analysis is conducted using choices for the smoothing parameter, out-of-control shift size, and hyper-parameters of the distribution. Based on obtained results, we provide recommendations for use of the Bayesian EWMA and Bayesian CUSUM control charts.

</details>

<details>

<summary>2020-07-20 02:35:54 - Estimating heterogeneous effects of continuous exposures using Bayesian tree ensembles: revisiting the impact of abortion rates on crime</summary>

- *Spencer Woody, Carlos M. Carvalho, P. Richard Hahn, Jared S. Murray*

- `2007.09845v1` - [abs](http://arxiv.org/abs/2007.09845v1) - [pdf](http://arxiv.org/pdf/2007.09845v1)

> In estimating the causal effect of a continuous exposure or treatment, it is important to control for all confounding factors. However, most existing methods require parametric specification for how control variables influence the outcome or generalized propensity score, and inference on treatment effects is usually sensitive to this choice. Additionally, it is often the goal to estimate how the treatment effect varies across observed units. To address this gap, we propose a semiparametric model using Bayesian tree ensembles for estimating the causal effect of a continuous treatment of exposure which (i) does not require a priori parametric specification of the influence of control variables, and (ii) allows for identification of effect modification by pre-specified moderators. The main parametric assumption we make is that the effect of the exposure on the outcome is linear, with the steepness of this relationship determined by a nonparametric function of the moderators, and we provide heuristics to diagnose the validity of this assumption. We apply our methods to revisit a 2001 study of how abortion rates affect incidence of crime.

</details>

<details>

<summary>2020-07-20 09:03:02 - Multi-level Training and Bayesian Optimization for Economical Hyperparameter Optimization</summary>

- *Yang Yang, Ke Deng, Michael Zhu*

- `2007.09953v1` - [abs](http://arxiv.org/abs/2007.09953v1) - [pdf](http://arxiv.org/pdf/2007.09953v1)

> Hyperparameters play a critical role in the performances of many machine learning methods. Determining their best settings or Hyperparameter Optimization (HPO) faces difficulties presented by the large number of hyperparameters as well as the excessive training time. In this paper, we develop an effective approach to reducing the total amount of required training time for HPO. In the initialization, the nested Latin hypercube design is used to select hyperparameter configurations for two types of training, which are, respectively, heavy training and light training. We propose a truncated additive Gaussian process model to calibrate approximate performance measurements generated by light training, using accurate performance measurements generated by heavy training. Based on the model, a sequential model-based algorithm is developed to generate the performance profile of the configuration space as well as find optimal ones. Our proposed approach demonstrates competitive performance when applied to optimize synthetic examples, support vector machines, fully connected networks and convolutional neural networks.

</details>

<details>

<summary>2020-07-20 10:27:18 - Bayesian optimization for automatic design of face stimuli</summary>

- *Pedro F. da Costa, Romy Lorenz, Ricardo Pio Monti, Emily Jones, Robert Leech*

- `2007.09989v1` - [abs](http://arxiv.org/abs/2007.09989v1) - [pdf](http://arxiv.org/pdf/2007.09989v1)

> Investigating the cognitive and neural mechanisms involved with face processing is a fundamental task in modern neuroscience and psychology. To date, the majority of such studies have focused on the use of pre-selected stimuli. The absence of personalized stimuli presents a serious limitation as it fails to account for how each individual face processing system is tuned to cultural embeddings or how it is disrupted in disease. In this work, we propose a novel framework which combines generative adversarial networks (GANs) with Bayesian optimization to identify individual response patterns to many different faces. Formally, we employ Bayesian optimization to efficiently search the latent space of state-of-the-art GAN models, with the aim to automatically generate novel faces, to maximize an individual subject's response. We present results from a web-based proof-of-principle study, where participants rated images of themselves generated via performing Bayesian optimization over the latent space of a GAN. We show how the algorithm can efficiently locate an individual's optimal face while mapping out their response across different semantic transformations of a face; inter-individual analyses suggest how the approach can provide rich information about individual differences in face processing.

</details>

<details>

<summary>2020-07-20 14:28:35 - Teaching deep neural networks to localize single molecules for super-resolution microscopy</summary>

- *Artur Speiser, Lucas-Raphael Müller, Ulf Matti, Christopher J. Obara, Wesley R. Legant, Jonas Ries, Jakob H. Macke, Srinivas C. Turaga*

- `1907.00770v2` - [abs](http://arxiv.org/abs/1907.00770v2) - [pdf](http://arxiv.org/pdf/1907.00770v2)

> Single-molecule localization fluorescence microscopy constructs super-resolution images by sequential imaging and computational localization of sparsely activated fluorophores. Accurate and efficient fluorophore localization algorithms are key to the success of this computational microscopy method. We present a novel localization algorithm based on deep learning which significantly improves upon the state of the art. Our contributions are a novel network architecture for simultaneous detection and localization, and new loss function which phrases detection and localization as a Bayesian inference problem, and thus allows the network to provide uncertainty-estimates. In contrast to standard methods which independently process imaging frames, our network architecture uses temporal context from multiple sequentially imaged frames to detect and localize molecules. We demonstrate the power of our method across a variety of datasets, imaging modalities, signal to noise ratios, and fluorophore densities. While existing localization algorithms can achieve optimal localization accuracy at low fluorophore densities, they are confounded by high densities. Our method is the first deep-learning based approach which achieves state-of-the-art on the SMLM2016 challenge. It achieves the best scores on 12 out of 12 data-sets when comparing both detection accuracy and precision, and excels at high densities. Finally, we investigate how unsupervised learning can be used to make the network robust against mismatch between simulated and real data. The lessons learned here are more generally relevant for the training of deep networks to solve challenging Bayesian inverse problems on spatially extended domains in biology and physics.

</details>

<details>

<summary>2020-07-20 14:40:01 - Bayesian Non-Parametric Detection Heterogeneity in Ecological Models</summary>

- *Daniel Turek, Claudia Wehrhahn, Olivier Gimenez*

- `2007.10163v1` - [abs](http://arxiv.org/abs/2007.10163v1) - [pdf](http://arxiv.org/pdf/2007.10163v1)

> Detection heterogeneity is inherent to ecological data, arising from factors such as varied terrain or weather conditions, inconsistent sampling effort, or heterogeneity of individuals themselves. Incorporating additional covariates into a statistical model is one approach for addressing heterogeneity, but is no guarantee that any set of measurable covariates will adequately address the heterogeneity, and the presence of unmodelled heterogeneity has been shown to produce biases in the resulting inferences. Other approaches for addressing heterogeneity include the use of random effects, or finite mixtures of homogeneous subgroups. Here, we present a non-parametric approach for modelling detection heterogeneity for use in a Bayesian hierarchical framework. We employ a Dirichlet process mixture which allows a flexible number of population subgroups without the need to pre-specify this number of subgroups as in a finite mixture. We describe this non-parametric approach, then consider its use for modelling detection heterogeneity in two common ecological motifs: capture-recapture and occupancy modelling. For each, we consider a homogeneous model, finite mixture models, and the non-parametric approach. We compare these approaches using two simulation studies, and observe the non-parametric approach as the most reliable method for addressing varying degrees of heterogeneity. We also present two real-data examples, and compare the inferences resulting from each modelling approach. Analyses are carried out using the \texttt{nimble} package for \texttt{R}, which provides facilities for Bayesian non-parametric models.

</details>

<details>

<summary>2020-07-20 16:47:37 - Practical Bayesian Learning of Neural Networks via Adaptive Optimisation Methods</summary>

- *Samuel Kessler, Arnold Salas, Vincent W. C. Tan, Stefan Zohren, Stephen Roberts*

- `1811.03679v3` - [abs](http://arxiv.org/abs/1811.03679v3) - [pdf](http://arxiv.org/pdf/1811.03679v3)

> We introduce a novel framework for the estimation of the posterior distribution over the weights of a neural network, based on a new probabilistic interpretation of adaptive optimisation algorithms such as AdaGrad and Adam. We demonstrate the effectiveness of our Bayesian Adam method, Badam, by experimentally showing that the learnt uncertainties correctly relate to the weights' predictive capabilities by weight pruning. We also demonstrate the quality of the derived uncertainty measures by comparing the performance of Badam to standard methods in a Thompson sampling setting for multi-armed bandits, where good uncertainty measures are required for an agent to balance exploration and exploitation.

</details>

<details>

<summary>2020-07-20 16:50:25 - A Hierarchical Approach to Scaling Batch Active Search Over Structured Data</summary>

- *Vivek Myers, Peyton Greenside*

- `2007.10263v1` - [abs](http://arxiv.org/abs/2007.10263v1) - [pdf](http://arxiv.org/pdf/2007.10263v1)

> Active search is the process of identifying high-value data points in a large and often high-dimensional parameter space that can be expensive to evaluate. Traditional active search techniques like Bayesian optimization trade off exploration and exploitation over consecutive evaluations, and have historically focused on single or small (<5) numbers of examples evaluated per round. As modern data sets grow, so does the need to scale active search to large data sets and batch sizes. In this paper, we present a general hierarchical framework based on bandit algorithms to scale active search to large batch sizes by maximizing information derived from the unique structure of each dataset. Our hierarchical framework, Hierarchical Batch Bandit Search (HBBS), strategically distributes batch selection across a learned embedding space by facilitating wide exploration of different structural elements within a dataset. We focus our application of HBBS on modern biology, where large batch experimentation is often fundamental to the research process, and demonstrate batch design of biological sequences (protein and DNA). We also present a new Gym environment to easily simulate diverse biological sequences and to enable more comprehensive evaluation of active search methods across heterogeneous data sets. The HBBS framework improves upon standard performance, wall-clock, and scalability benchmarks for batch search by using a broad exploration strategy across coarse partitions and fine-grained exploitation within each partition of structured data.

</details>

<details>

<summary>2020-07-21 01:54:59 - Evaluating the Informativeness of the Besag-York-Mollié CAR Model</summary>

- *Harrison Quick, Guangzi Song, Loni Tabb*

- `2007.10559v1` - [abs](http://arxiv.org/abs/2007.10559v1) - [pdf](http://arxiv.org/pdf/2007.10559v1)

> The use of the conditional autoregressive framework proposed by Besag, York, and Molli\'e (1991; BYM) is ubiquitous in Bayesian disease mapping and spatial epidemiology. While it is understood that Bayesian inference is based on a combination of the information contained in the data and the information contributed by the model, quantifying the contribution of the model relative to the information in the data is often non-trivial. Here, we provide a measure of the contribution of the BYM framework by first considering the simple Poisson-gamma setting in which quantifying the prior's contribution is quite clear. We then propose a relationship between gamma and lognormal priors that we then extend to cover the framework proposed by BYM. Following a brief simulation study in which we illustrate the accuracy of our lognormal approximation of the gamma prior, we analyze a dataset comprised of county-level heart disease-related death data across the United States. In addition to demonstrating the potential for the BYM framework to correspond to a highly informative prior specification, we also illustrate the sensitivity of death rate estimates to changes in the informativeness of the BYM framework.

</details>

<details>

<summary>2020-07-21 04:55:31 - Incentives for Federated Learning: a Hypothesis Elicitation Approach</summary>

- *Yang Liu, Jiaheng Wei*

- `2007.10596v1` - [abs](http://arxiv.org/abs/2007.10596v1) - [pdf](http://arxiv.org/pdf/2007.10596v1)

> Federated learning provides a promising paradigm for collecting machine learning models from distributed data sources without compromising users' data privacy. The success of a credible federated learning system builds on the assumption that the decentralized and self-interested users will be willing to participate to contribute their local models in a trustworthy way. However, without proper incentives, users might simply opt out the contribution cycle, or will be mis-incentivized to contribute spam/false information. This paper introduces solutions to incentivize truthful reporting of a local, user-side machine learning model for federated learning. Our results build on the literature of information elicitation, but focus on the questions of eliciting hypothesis (rather than eliciting human predictions). We provide a scoring rule based framework that incentivizes truthful reporting of local hypotheses at a Bayesian Nash Equilibrium. We study the market implementation, accuracy as well as robustness properties of our proposed solution too. We verify the effectiveness of our methods using MNIST and CIFAR-10 datasets. Particularly we show that by reporting low-quality hypotheses, users will receive decreasing scores (rewards, or payments).

</details>

<details>

<summary>2020-07-21 08:07:09 - Maximum Pairwise Bayes Factors for Covariance Structure Testing</summary>

- *Kyoungjae Lee, Lizhen Lin, David Dunson*

- `1809.03105v3` - [abs](http://arxiv.org/abs/1809.03105v3) - [pdf](http://arxiv.org/pdf/1809.03105v3)

> Hypothesis testing of structure in covariance matrices is of significant importance, but faces great challenges in high-dimensional settings. Although consistent frequentist one-sample covariance tests have been proposed, there is a lack of simple, computationally scalable, and theoretically sound Bayesian testing methods for large covariance matrices. Motivated by this gap and by the need for tests that are powerful against sparse alternatives, we propose a novel testing framework based on the maximum pairwise Bayes factor. Our initial focus is on one-sample covariance testing; the proposed test can {\it optimally} distinguish null and alternative hypotheses in a frequentist asymptotic sense. We then propose diagonal tests and a scalable covariance graph selection procedure that are shown to be consistent. A simulation study evaluates the proposed approach relative to competitors. We illustrate advantages of our graph selection method on a gene expression data set.

</details>

<details>

<summary>2020-07-21 17:42:58 - Disentangling the Gauss-Newton Method and Approximate Inference for Neural Networks</summary>

- *Alexander Immer*

- `2007.11994v1` - [abs](http://arxiv.org/abs/2007.11994v1) - [pdf](http://arxiv.org/pdf/2007.11994v1)

> In this thesis, we disentangle the generalized Gauss-Newton and approximate inference for Bayesian deep learning. The generalized Gauss-Newton method is an optimization method that is used in several popular Bayesian deep learning algorithms. Algorithms that combine the Gauss-Newton method with the Laplace and Gaussian variational approximation have recently led to state-of-the-art results in Bayesian deep learning. While the Laplace and Gaussian variational approximation have been studied extensively, their interplay with the Gauss-Newton method remains unclear. Recent criticism of priors and posterior approximations in Bayesian deep learning further urges the need for a deeper understanding of practical algorithms. The individual analysis of the Gauss-Newton method and Laplace and Gaussian variational approximations for neural networks provides both theoretical insight and new practical algorithms. We find that the Gauss-Newton method simplifies the underlying probabilistic model significantly. In particular, the combination of the Gauss-Newton method with approximate inference can be cast as inference in a linear or Gaussian process model. The Laplace and Gaussian variational approximation can subsequently provide a posterior approximation to these simplified models. This new disentangled understanding of recent Bayesian deep learning algorithms also leads to new methods: first, the connection to Gaussian processes enables new function-space inference algorithms. Second, we present a marginal likelihood approximation of the underlying probabilistic model to tune neural network hyperparameters. Finally, the identified underlying models lead to different methods to compute predictive distributions. In fact, we find that these prediction methods for Bayesian neural networks often work better than the default choice and solve a common issue with the Laplace approximation.

</details>

<details>

<summary>2020-07-21 18:15:08 - A Gradient-based Bilevel Optimization Approach for Tuning Hyperparameters in Machine Learning</summary>

- *Ankur Sinha, Tanmay Khandait, Raja Mohanty*

- `2007.11022v1` - [abs](http://arxiv.org/abs/2007.11022v1) - [pdf](http://arxiv.org/pdf/2007.11022v1)

> Hyperparameter tuning is an active area of research in machine learning, where the aim is to identify the optimal hyperparameters that provide the best performance on the validation set. Hyperparameter tuning is often achieved using naive techniques, such as random search and grid search. However, most of these methods seldom lead to an optimal set of hyperparameters and often get very expensive. In this paper, we propose a bilevel solution method for solving the hyperparameter optimization problem that does not suffer from the drawbacks of the earlier studies. The proposed method is general and can be easily applied to any class of machine learning algorithms. The idea is based on the approximation of the lower level optimal value function mapping, which is an important mapping in bilevel optimization and helps in reducing the bilevel problem to a single level constrained optimization task. The single-level constrained optimization problem is solved using the augmented Lagrangian method. We discuss the theory behind the proposed algorithm and perform extensive computational study on two datasets that confirm the efficiency of the proposed method. We perform a comparative study against grid search, random search and Bayesian optimization techniques that shows that the proposed algorithm is multiple times faster on problems with one or two hyperparameters. The computational gain is expected to be significantly higher as the number of hyperparameters increase. Corresponding to a given hyperparameter most of the techniques in the literature often assume a unique optimal parameter set that minimizes loss on the training set. Such an assumption is often violated by deep learning architectures and the proposed method does not require any such assumption.

</details>

<details>

<summary>2020-07-21 19:50:34 - Analyzing initial stage of COVID-19 transmission through Bayesian time-varying model</summary>

- *Arkaprava Roy, Sayar Karmakar*

- `2004.02281v4` - [abs](http://arxiv.org/abs/2004.02281v4) - [pdf](http://arxiv.org/pdf/2004.02281v4)

> Recent outbreak of the novel coronavirus COVID-19 has affected all of our lives in one way or the other. While medical researchers are working hard to find a cure and doctors/nurses to attend the affected individuals, measures such as `lockdown', `stay-at-home', `social distancing' are being implemented in different parts of the world to curb its further spread. To model the non-stationary spread, we propose a novel time-varying semiparametric AR$(p)$ model for the count valued time-series of newly affected cases, collected every day and also extend it to propose a novel time-varying INGARCH model. Our proposed structures of the models are amenable to Hamiltonian Monte Carlo (HMC) sampling for efficient computation. We substantiate our methods by simulations that show superiority compared to some of the close existing methods. Finally we analyze the daily time series data of newly confirmed cases to study its spread through different government interventions.

</details>

<details>

<summary>2020-07-21 20:40:25 - Automatic Discovery of Privacy-Utility Pareto Fronts</summary>

- *Brendan Avent, Javier Gonzalez, Tom Diethe, Andrei Paleyes, Borja Balle*

- `1905.10862v4` - [abs](http://arxiv.org/abs/1905.10862v4) - [pdf](http://arxiv.org/pdf/1905.10862v4)

> Differential privacy is a mathematical framework for privacy-preserving data analysis. Changing the hyperparameters of a differentially private algorithm allows one to trade off privacy and utility in a principled way. Quantifying this trade-off in advance is essential to decision-makers tasked with deciding how much privacy can be provided in a particular application while maintaining acceptable utility. Analytical utility guarantees offer a rigorous tool to reason about this trade-off, but are generally only available for relatively simple problems. For more complex tasks, such as training neural networks under differential privacy, the utility achieved by a given algorithm can only be measured empirically. This paper presents a Bayesian optimization methodology for efficiently characterizing the privacy--utility trade-off of any differentially private algorithm using only empirical measurements of its utility. The versatility of our method is illustrated on a number of machine learning tasks involving multiple models, optimizers, and datasets.

</details>

<details>

<summary>2020-07-21 23:03:11 - Efficient Graph-Based Active Learning with Probit Likelihood via Gaussian Approximations</summary>

- *Kevin Miller, Hao Li, Andrea L. Bertozzi*

- `2007.11126v1` - [abs](http://arxiv.org/abs/2007.11126v1) - [pdf](http://arxiv.org/pdf/2007.11126v1)

> We present a novel adaptation of active learning to graph-based semi-supervised learning (SSL) under non-Gaussian Bayesian models. We present an approximation of non-Gaussian distributions to adapt previously Gaussian-based acquisition functions to these more general cases. We develop an efficient rank-one update for applying "look-ahead" based methods as well as model retraining. We also introduce a novel "model change" acquisition function based on these approximations that further expands the available collection of active learning acquisition functions for such methods.

</details>

<details>

<summary>2020-07-22 08:19:33 - Noisy Adaptive Group Testing using Bayesian Sequential Experimental Design</summary>

- *Marco Cuturi, Olivier Teboul, Quentin Berthet, Arnaud Doucet, Jean-Philippe Vert*

- `2004.12508v6` - [abs](http://arxiv.org/abs/2004.12508v6) - [pdf](http://arxiv.org/pdf/2004.12508v6)

> When the infection prevalence of a disease is low, Dorfman showed 80 years ago that testing groups of people can prove more efficient than testing people individually. Our goal in this paper is to propose new group testing algorithms that can operate in a noisy setting (tests can be mistaken) to decide adaptively (looking at past results) which groups to test next, with the goal to converge to a good detection, as quickly, and with as few tests as possible. We cast this problem as a Bayesian sequential experimental design problem. Using the posterior distribution of infection status vectors for $n$ patients, given observed tests carried out so far, we seek to form groups that have a maximal utility. We consider utilities such as mutual information, but also quantities that have a more direct relevance to testing, such as the AUC of the ROC curve of the test. Practically, the posterior distributions on $\{0,1\}^n$ are approximated by sequential Monte Carlo (SMC) samplers and the utility maximized by a greedy optimizer. Our procedures show in simulations significant improvements over both adaptive and non-adaptive baselines, and are far more efficient than individual tests when disease prevalence is low. Additionally, we show empirically that loopy belief propagation (LBP), widely regarded as the SoTA decoder to decide whether an individual is infected or not given previous tests, can be unreliable and exhibit oscillatory behavior. Our SMC decoder is more reliable, and can improve the performance of other group testing algorithms.

</details>

<details>

<summary>2020-07-22 12:04:02 - A Fourier State Space Model for Bayesian ODE Filters</summary>

- *Hans Kersting, Maren Mahsereci*

- `2007.09118v2` - [abs](http://arxiv.org/abs/2007.09118v2) - [pdf](http://arxiv.org/pdf/2007.09118v2)

> Gaussian ODE filtering is a probabilistic numerical method to solve ordinary differential equations (ODEs). It computes a Bayesian posterior over the solution from evaluations of the vector field defining the ODE. Its most popular version, which employs an integrated Brownian motion prior, uses Taylor expansions of the mean to extrapolate forward and has the same convergence rates as classical numerical methods. As the solution of many important ODEs are periodic functions (oscillators), we raise the question whether Fourier expansions can also be brought to bear within the framework of Gaussian ODE filtering. To this end, we construct a Fourier state space model for ODEs and a `hybrid' model that combines a Taylor (Brownian motion) and Fourier state space model. We show by experiments how the hybrid model might become useful in cheaply predicting until the end of the time domain.

</details>

<details>

<summary>2020-07-22 12:55:56 - Information Validates the Prior: A Theorem on Bayesian Updating and Applications</summary>

- *Navin Kartik, Frances Lee, Wing Suen*

- `2005.05714v3` - [abs](http://arxiv.org/abs/2005.05714v3) - [pdf](http://arxiv.org/pdf/2005.05714v3)

> We develop a result on expected posteriors for Bayesians with heterogenous priors, dubbed information validates the prior (IVP). Under familiar ordering requirements, Anne expects a (Blackwell) more informative experiment to bring Bob's posterior mean closer to Anne's prior mean. We apply the result in two contexts of games of asymmetric information: voluntary testing or certification, and costly signaling or falsification. IVP can be used to determine how an agent's behavior responds to additional exogenous or endogenous information. We discuss economic implications.

</details>

<details>

<summary>2020-07-22 15:43:36 - SBI -- A toolkit for simulation-based inference</summary>

- *Alvaro Tejero-Cantero, Jan Boelts, Michael Deistler, Jan-Matthis Lueckmann, Conor Durkan, Pedro J. Gonçalves, David S. Greenberg, Jakob H. Macke*

- `2007.09114v2` - [abs](http://arxiv.org/abs/2007.09114v2) - [pdf](http://arxiv.org/pdf/2007.09114v2)

> Scientists and engineers employ stochastic numerical simulators to model empirically observed phenomena. In contrast to purely statistical models, simulators express scientific principles that provide powerful inductive biases, improve generalization to new data or scenarios and allow for fewer, more interpretable and domain-relevant parameters. Despite these advantages, tuning a simulator's parameters so that its outputs match data is challenging. Simulation-based inference (SBI) seeks to identify parameter sets that a) are compatible with prior knowledge and b) match empirical observations. Importantly, SBI does not seek to recover a single 'best' data-compatible parameter set, but rather to identify all high probability regions of parameter space that explain observed data, and thereby to quantify parameter uncertainty. In Bayesian terminology, SBI aims to retrieve the posterior distribution over the parameters of interest. In contrast to conventional Bayesian inference, SBI is also applicable when one can run model simulations, but no formula or algorithm exists for evaluating the probability of data given parameters, i.e. the likelihood. We present $\texttt{sbi}$, a PyTorch-based package that implements SBI algorithms based on neural networks. $\texttt{sbi}$ facilitates inference on black-box simulators for practising scientists and engineers by providing a unified interface to state-of-the-art algorithms together with documentation and tutorials.

</details>

<details>

<summary>2020-07-22 17:21:41 - Deeply Uncertain: Comparing Methods of Uncertainty Quantification in Deep Learning Algorithms</summary>

- *João Caldeira, Brian Nord*

- `2004.10710v3` - [abs](http://arxiv.org/abs/2004.10710v3) - [pdf](http://arxiv.org/pdf/2004.10710v3)

> We present a comparison of methods for uncertainty quantification (UQ) in deep learning algorithms in the context of a simple physical system. Three of the most common uncertainty quantification methods - Bayesian Neural Networks (BNN), Concrete Dropout (CD), and Deep Ensembles (DE) - are compared to the standard analytic error propagation. We discuss this comparison in terms endemic to both machine learning ("epistemic" and "aleatoric") and the physical sciences ("statistical" and "systematic"). The comparisons are presented in terms of simulated experimental measurements of a single pendulum - a prototypical physical system for studying measurement and analysis techniques. Our results highlight some pitfalls that may occur when using these UQ methods. For example, when the variation of noise in the training set is small, all methods predicted the same relative uncertainty independently of the inputs. This issue is particularly hard to avoid in BNN. On the other hand, when the test set contains samples far from the training distribution, we found that no methods sufficiently increased the uncertainties associated to their predictions. This problem was particularly clear for CD. In light of these results, we make some recommendations for usage and interpretation of UQ methods.

</details>

<details>

<summary>2020-07-22 18:04:08 - Region-Referenced Spectral Power Dynamics of EEG Signals: A Hierarchical Modeling Approach</summary>

- *Qian Li, John Shamshoian, Damla Senturk, Catherine Sugar, Shafali Jeste, Charlotte DiStefano, Donatello Telesca*

- `1811.05566v2` - [abs](http://arxiv.org/abs/1811.05566v2) - [pdf](http://arxiv.org/pdf/1811.05566v2)

> Functional brain imaging through electroencephalography (EEG) relies upon the analysis and interpretation of high-dimensional, spatially organized time series. We propose to represent time-localized frequency domain characterizations of EEG data as region-referenced functional data. This representation is coupled with a hierarchical modeling approach to multivariate functional observations. Within this familiar setting, we discuss how several prior models relate to structural assumptions about multivariate covariance operators. An overarching modeling framework, based on infinite factorial decompositions, is finally proposed to balance flexibility and efficiency in estimation. The motivating application stems from a study of implicit auditory learning, in which typically developing (TD) children, and children with autism spectrum disorder (ASD) were exposed to a continuous speech stream. Using the proposed model, we examine differential band power dynamics as brain function is interrogated throughout the duration of a computer-controlled experiment. Our work offers a novel look at previous findings in psychiatry, and provides further insights into the understanding of ASD. Our approach to inference is fully Bayesian and implemented in a highly optimized Rcpp package.

</details>

<details>

<summary>2020-07-22 20:25:07 - Two-Phase Data Synthesis for Income: An Application to the NHIS</summary>

- *Kevin Ros, Henrik Olsson, Jingchen Hu*

- `2006.01686v2` - [abs](http://arxiv.org/abs/2006.01686v2) - [pdf](http://arxiv.org/pdf/2006.01686v2)

> We propose a two-phase synthesis process for synthesizing income, a sensitive variable which is usually highly-skewed and has a number of reported zeros. We consider two forms of a continuous income variable: a binary form, which is modeled and synthesized in phase 1; and a non-negative continuous form, which is modeled and synthesized in phase 2. Bayesian synthesis models are proposed for the two-phase synthesis process, and other synthesis models are readily implementable. We demonstrate our methods with applications to a sample from the National Health Interview Survey (NHIS). Utility and risk profiles of generated synthetic datasets are evaluated and compared to results from a single-phase synthesis process.

</details>

<details>

<summary>2020-07-23 13:53:59 - The semi-Markov beta-Stacy process: a Bayesian non-parametric prior for semi-Markov processes</summary>

- *Andrea Arfè, Stefano Peluso, Pietro Muliere*

- `1812.00260v2` - [abs](http://arxiv.org/abs/1812.00260v2) - [pdf](http://arxiv.org/pdf/1812.00260v2)

> The literature on Bayesian methods for the analysis of discrete-time semi-Markov processes is sparse. In this paper, we introduce the semi-Markov beta-Stacy process, a stochastic process useful for the Bayesian non-parametric analysis of semi-Markov processes. The semi-Markov beta-Stacy process is conjugate with respect to data generated by a semi-Markov process, a property which makes it easy to obtain probabilistic forecasts. Its predictive distributions are characterized by a reinforced random walk on a system of urns.

</details>

<details>

<summary>2020-07-23 14:20:57 - Sensor Clusterization in D-optimal Design in Infinite Dimensional Bayesian Inverse Problems</summary>

- *Yair Daon*

- `2007.12032v1` - [abs](http://arxiv.org/abs/2007.12032v1) - [pdf](http://arxiv.org/pdf/2007.12032v1)

> We investigate the problem of sensor clusterization in optimal experimental design for infinite-dimensional Bayesian inverse problems. We suggest an analytically tractable model for such designs and reason how it may lead to sensor clusterization in the case of iid measurement noise. We also show that in the case of spatially correlated measurement error clusterization does not occur. As a part of the analysis we prove a matrix determinant lemma analog in infinite dimensions, as well as a lemma for calculating derivatives of $\log \det$ of operators.

</details>

<details>

<summary>2020-07-24 12:21:30 - Are low frequency macroeconomic variables important for high frequency electricity prices?</summary>

- *Claudia Foroni, Francesco Ravazzolo, Luca Rossini*

- `2007.13566v1` - [abs](http://arxiv.org/abs/2007.13566v1) - [pdf](http://arxiv.org/pdf/2007.13566v1)

> We analyse the importance of low frequency hard and soft macroeconomic information, respectively the industrial production index and the manufacturing Purchasing Managers' Index surveys, for forecasting high-frequency daily electricity prices in two of the main European markets, Germany and Italy. We do that by means of mixed-frequency models, introducing a Bayesian approach to reverse unrestricted MIDAS models (RU-MIDAS). Despite the general parsimonious structure of standard MIDAS models, the RU-MIDAS has a large set of parameters when several predictors are considered simultaneously and Bayesian inference is useful for imposing parameter restrictions. We study the forecasting accuracy for different horizons (from $1$ day ahead to $28$ days ahead) and by considering different specifications of the models. Results indicate that the macroeconomic low frequency variables are more important for short horizons than for longer horizons. Moreover, accuracy increases by combining hard and soft information, and using only surveys gives less accurate forecasts than using only industrial production data.

</details>

<details>

<summary>2020-07-24 16:17:47 - Bayesian Combinatorial Multi-Study Factor Analysis</summary>

- *Isabella N. Grabski, Roberta De Vito, Lorenzo Trippa, Giovanni Parmigiani*

- `2007.12616v1` - [abs](http://arxiv.org/abs/2007.12616v1) - [pdf](http://arxiv.org/pdf/2007.12616v1)

> Analyzing multiple studies allows leveraging data from a range of sources and populations, but until recently, there have been limited methodologies to approach the joint unsupervised analysis of multiple high-dimensional studies. A recent method, Bayesian Multi-Study Factor Analysis (BMSFA), identifies latent factors common to all studies, as well as latent factors specific to individual studies. However, BMSFA does not allow for partially shared factors, i.e. latent factors shared by more than one but less than all studies. We extend BMSFA by introducing a new method, Tetris, for Bayesian combinatorial multi-study factor analysis, which identifies latent factors that can be shared by any combination of studies. We model the subsets of studies that share latent factors with an Indian Buffet Process. We test our method with an extensive range of simulations, and showcase its utility not only in dimension reduction but also in covariance estimation. Finally, we apply Tetris to high-dimensional gene expression datasets to identify patterns in breast cancer gene expression, both within and across known classes defined by germline mutations.

</details>

<details>

<summary>2020-07-24 17:17:01 - A Generalization of Hierarchical Exchangeability on Trees to Directed Acyclic Graphs</summary>

- *Paul Jung, Jiho Lee, Sam Staton, Hongseok Yang*

- `1812.06282v4` - [abs](http://arxiv.org/abs/1812.06282v4) - [pdf](http://arxiv.org/pdf/1812.06282v4)

> Motivated by the problem of designing inference-friendly Bayesian nonparametric models in probabilistic programming languages, we introduce a general class of partially exchangeable random arrays which generalizes the notion of hierarchical exchangeability introduced in Austin and Panchenko (2014). We say that our partially exchangeable arrays are DAG-exchangeable since their partially exchangeable structure is governed by a collection of Directed Acyclic Graphs. More specifically, such a random array is indexed by $\mathbb{N}^{|V|}$ for some DAG $G=(V,E)$, and its exchangeability structure is governed by the edge set $E$. We prove a representation theorem for such arrays which generalizes the Aldous-Hoover and Austin-Panchenko representation theorems.

</details>

<details>

<summary>2020-07-25 04:16:34 - Graph Gamma Process Generalized Linear Dynamical Systems</summary>

- *Rahi Kalantari, Mingyuan Zhou*

- `2007.12852v1` - [abs](http://arxiv.org/abs/2007.12852v1) - [pdf](http://arxiv.org/pdf/2007.12852v1)

> We introduce graph gamma process (GGP) linear dynamical systems to model real-valued multivariate time series. For temporal pattern discovery, the latent representation under the model is used to decompose the time series into a parsimonious set of multivariate sub-sequences. In each sub-sequence, different data dimensions often share similar temporal patterns but may exhibit distinct magnitudes, and hence allowing the superposition of all sub-sequences to exhibit diverse behaviors at different data dimensions. We further generalize the proposed model by replacing the Gaussian observation layer with the negative binomial distribution to model multivariate count time series. Generated from the proposed GGP is an infinite dimensional directed sparse random graph, which is constructed by taking the logical OR operation of countably infinite binary adjacency matrices that share the same set of countably infinite nodes. Each of these adjacency matrices is associated with a weight to indicate its activation strength, and places a finite number of edges between a finite subset of nodes belonging to the same node community. We use the generated random graph, whose number of nonzero-degree nodes is finite, to define both the sparsity pattern and dimension of the latent state transition matrix of a (generalized) linear dynamical system. The activation strength of each node community relative to the overall activation strength is used to extract a multivariate sub-sequence, revealing the data pattern captured by the corresponding community. On both synthetic and real-world time series, the proposed nonparametric Bayesian dynamic models, which are initialized at random, consistently exhibit good predictive performance in comparison to a variety of baseline models, revealing interpretable latent state transition patterns and decomposing the time series into distinctly behaved sub-sequences.

</details>

<details>

<summary>2020-07-25 08:17:07 - Inference with selection, varying population size and evolving population structure: Application of ABC to a forward-backward coalescent process with interactions</summary>

- *Clotilde Lepers, Sylvain Billiard, Matthieu Porte, Sylvie Méléard, Viet Chi Tran*

- `1910.10201v2` - [abs](http://arxiv.org/abs/1910.10201v2) - [pdf](http://arxiv.org/pdf/1910.10201v2)

> Genetic data are often used to infer demographic history and changes or detect genes under selection. Inferential methods are commonly based on models making various strong assumptions: demography and population structures are supposed \textit{a priori} known, the evolution of the genetic composition of a population does not affect demography nor population structure, and there is no selection nor interaction between and within genetic strains. In this paper, we present a stochastic birth-death model with competitive interactions and asexual reproduction. We develop an inferential procedure for ecological, demographic and genetic parameters. We first show how genetic diversity and genealogies are related to birth and death rates, and to how individuals compete within and between strains. {This leads us to propose an original model of phylogenies, with trait structure and interactions, that allows multiple merging}. Second, we develop an Approximate Bayesian Computation framework to use our model for analyzing genetic data. We apply our procedure to simulated data from a toy model, and to real data by analyzing the genetic diversity of microsatellites on Y-chromosomes sampled from Central Asia human populations in order to test whether different social organizations show significantly different fertility.

</details>

<details>

<summary>2020-07-25 13:55:06 - A sequential test for the drift of a Brownian motion with a possibility to change a decision</summary>

- *Mikhail Zhitlukhin*

- `2007.12936v1` - [abs](http://arxiv.org/abs/2007.12936v1) - [pdf](http://arxiv.org/pdf/2007.12936v1)

> We construct a Bayesian sequential test of two simple hypotheses about the value of the unobservable drift coefficient of a Brownian motion, with a possibility to change the initial decision at subsequent moments of time for some penalty. Such a testing procedure allows to correct the initial decision if it turns out to be wrong. The test is based on observation of the posterior mean process and makes the initial decision and, possibly, changes it later, when this process crosses certain thresholds. The solution of the problem is obtained by reducing it to joint optimal stopping and optimal switching problems.

</details>

<details>

<summary>2020-07-26 00:29:50 - Bayesian Measurement Error Models Using Finite Mixtures of Scale Mixtures of Skew-Normal Distributions</summary>

- *C. R. B. Cabral, N. L. de Souza, J. Leão*

- `2007.13037v1` - [abs](http://arxiv.org/abs/2007.13037v1) - [pdf](http://arxiv.org/pdf/2007.13037v1)

> We present a proposal to deal with the non-normality issue in the context of regression models with measurement errors when both the response and the explanatory variable are observed with error. We extend the normal model by jointly modeling the unobserved covariate and the random errors by a finite mixture of scale mixture of skew-normal distributions. This approach allows us to model data with great flexibility, accommodating skewness, heavy tails, and multi-modality.

</details>

<details>

<summary>2020-07-26 10:34:40 - Bayesian Dynamic Mapping of an Exo-Earth from Photometric Variability</summary>

- *Hajime Kawahara, Kento Masuda*

- `2007.13096v1` - [abs](http://arxiv.org/abs/2007.13096v1) - [pdf](http://arxiv.org/pdf/2007.13096v1)

> Photometric variability of a directly imaged exo-Earth conveys spatial information on its surface and can be used to retrieve a two-dimensional geography and axial tilt of the planet (spin-orbit tomography). In this study, we relax the assumption of the static geography and present a computationally tractable framework for dynamic spin-orbit tomography applicable to the time-varying geography. First, a Bayesian framework of static spin-orbit tomography is revisited using analytic expressions of the Bayesian inverse problem with a Gaussian prior. We then extend this analytic framework to a time-varying one through a Gaussian process in time domain, and present analytic expressions that enable efficient sampling from a full joint posterior distribution of geography, axial tilt, spin rotation period, and hyperparameters in the Gaussian-process priors. Consequently, it only takes 0.3 s for a laptop computer to sample one posterior dynamic map conditioned on the other parameters with 3,072 pixels and 1,024 time grids, for a total of $\sim 3 \times 10^6$ parameters. We applied our dynamic mapping method on a toy model and found that the time-varying geography was accurately retrieved along with the axial-tilt and spin rotation period. In addition, we demonstrated the use of dynamic spin-orbit tomography with a real multi-color light curve of the Earth as observed by the Deep Space Climate Observatory. We found that the resultant snapshots from the dominant component of a principle component analysis roughly captured the large-scale, seasonal variations of the clear-sky and cloudy areas on the Earth.

</details>

<details>

<summary>2020-07-26 14:53:36 - Fully Bayesian Analysis of the Relevance Vector Machine Classification for Imbalanced Data</summary>

- *Wenyang Wang, Dongchu Sun, Zhuoqiong He*

- `2007.13140v1` - [abs](http://arxiv.org/abs/2007.13140v1) - [pdf](http://arxiv.org/pdf/2007.13140v1)

> Relevance Vector Machine (RVM) is a supervised learning algorithm extended from Support Vector Machine (SVM) based on the Bayesian sparsity model. Compared with the regression problem, RVM classification is difficult to be conducted because there is no closed-form solution for the weight parameter posterior. Original RVM classification algorithm used Newton's method in optimization to obtain the mode of weight parameter posterior then approximated it by a Gaussian distribution in Laplace's method. It would work but just applied the frequency methods in a Bayesian framework. This paper proposes a Generic Bayesian approach for the RVM classification. We conjecture that our algorithm achieves convergent estimates of the quantities of interest compared with the nonconvergent estimates of the original RVM classification algorithm. Furthermore, a Fully Bayesian approach with the hierarchical hyperprior structure for RVM classification is proposed, which improves the classification performance, especially in the imbalanced data problem. By the numeric studies, our proposed algorithms obtain high classification accuracy rates. The Fully Bayesian hierarchical hyperprior method outperforms the Generic one for the imbalanced data classification.

</details>

<details>

<summary>2020-07-26 23:24:52 - Multidimensional Bayesian IRT Model for Hierarchical Latent Structures</summary>

- *Juliane Venturelli S. L., Flavio B. Gonçalves, Dalton F. Andrade*

- `2006.09966v2` - [abs](http://arxiv.org/abs/2006.09966v2) - [pdf](http://arxiv.org/pdf/2006.09966v2)

> It is reasonable to consider, in many cases, that individuals' latent traits have a hierarchical structure such that more general traits are a suitable composition of more specific ones. Existing item response models that account for such hierarchical structure feature have considerable limitations in terms of modelling and/or inference. Motivated by those limitations and the importance of the theme, this paper aims at proposing an improved methodology in terms of both modelling and inference to deal with hierarchically structured latent traits in an item response theory context. From a modelling perspective, the proposed methodology allows for genuinely multidimensional items and all of the latent traits in the assumed hierarchical structure are on the same scale. Items are allowed to be dichotomous or of graded response. An efficient MCMC algorithm is carefully devised to sample from the joint posterior distribution of all the unknown quantities of the proposed model. In particular, all the latent trait parameters are jointly sampled from their full conditional distribution in a Gibbs sampling algorithm. The proposed methodology is applied to simulated data and a real dataset concerning the Enem exam in Brazil.

</details>

<details>

<summary>2020-07-27 12:15:20 - Transport map accelerated adaptive importance sampling, and application to inverse problems arising from multiscale stochastic reaction networks</summary>

- *Simon L. Cotter, Ioannis G. Kevrekidis, Paul Russell*

- `1901.11269v2` - [abs](http://arxiv.org/abs/1901.11269v2) - [pdf](http://arxiv.org/pdf/1901.11269v2)

> In many applications, Bayesian inverse problems can give rise to probability distributions which contain complexities due to the Hessian varying greatly across parameter space. This complexity often manifests itself as lower dimensional manifolds on which the likelihood function is invariant, or varies very little. This can be due to trying to infer unobservable parameters, or due to sloppiness in the model which is being used to describe the data. In such a situation, standard sampling methods for characterising the posterior distribution, which do not incorporate information about this structure, will be highly inefficient.   In this paper, we seek to develop an approach to tackle this problem when using adaptive importance sampling methods, by using optimal transport maps to simplify posterior distributions which are concentrated on lower dimensional manifolds. This approach is applicable to a whole range of problems for which Monte Carlo Markov chain (MCMC) methods mix slowly.   We demonstrate the approach by considering inverse problems arising from partially observed stochastic reaction networks. In particular, we consider systems which exhibit multiscale behaviour, but for which only the slow variables in the system are observable. We demonstrate that certain multiscale approximations lead to more consistent approximations of the posterior than others. The use of optimal transport maps stabilises the ensemble transform adaptive importance sampling (ETAIS) method, and allows for efficient sampling with smaller ensemble sizes. This approach allows us to take advantage of the large increases of efficiency when using adaptive importance sampling methods for previously intractable Bayesian inverse problems with complex posterior structure.

</details>

<details>

<summary>2020-07-27 12:24:11 - Bayesian Subspace HMM for the Zerospeech 2020 Challenge</summary>

- *Bolaji Yusuf, Lucas Ondel*

- `2005.09282v2` - [abs](http://arxiv.org/abs/2005.09282v2) - [pdf](http://arxiv.org/pdf/2005.09282v2)

> In this paper we describe our submission to the Zerospeech 2020 challenge, where the participants are required to discover latent representations from unannotated speech, and to use those representations to perform speech synthesis, with synthesis quality used as a proxy metric for the unit quality. In our system, we use the Bayesian Subspace Hidden Markov Model (SHMM) for unit discovery. The SHMM models each unit as an HMM whose parameters are constrained to lie in a low dimensional subspace of the total parameter space which is trained to model phonetic variability. Our system compares favorably with the baseline on the human-evaluated character error rate while maintaining significantly lower unit bitrate.

</details>

<details>

<summary>2020-07-27 13:13:40 - Moment-Matching Graph-Networks for Causal Inference</summary>

- *Michael Park*

- `2007.10507v2` - [abs](http://arxiv.org/abs/2007.10507v2) - [pdf](http://arxiv.org/pdf/2007.10507v2)

> In this note we explore a fully unsupervised deep-learning framework for simulating non-linear structural equation models from observational training data. The main contribution of this note is an architecture for applying moment-matching loss functions to the edges of a causal Bayesian graph, resulting in a generative conditional-moment-matching graph-neural-network. This framework thus enables automated sampling of latent space conditional probability distributions for various graphical interventions, and is capable of generating out-of-sample interventional probabilities that are often faithful to the ground truth distributions well beyond the range contained in the training set. These methods could in principle be used in conjunction with any existing autoencoder that produces a latent space representation containing causal graph structures.

</details>

<details>

<summary>2020-07-27 14:54:11 - Bayesian survival analysis with BUGS</summary>

- *Danilo Alvares, Elena Lázaro, Virgilio Gómez-Rubio, Carmen Armero*

- `2005.05952v2` - [abs](http://arxiv.org/abs/2005.05952v2) - [pdf](http://arxiv.org/pdf/2005.05952v2)

> Survival analysis is one of the most important fields of statistics in medicine and the biological sciences. In addition, the computational advances in the last decades have favoured the use of Bayesian methods in this context, providing a flexible and powerful alternative to the traditional frequentist approach. The objective of this paper is to summarise some of the most popular Bayesian survival models, such as accelerated failure time, proportional hazards, mixture cure, competing risks, frailty, and joint models of longitudinal and survival data. Moreover, an implementation of each presented model is provided using a BUGS syntax that can be run with JAGS from the R programming language. Reference to other Bayesian R-packages are also discussed.

</details>

<details>

<summary>2020-07-27 19:24:03 - Bayesian Neural Tree Models for Nonparametric Regression</summary>

- *Tanujit Chakraborty, Gauri Kamat, Ashis Kumar Chakraborty*

- `1909.00515v2` - [abs](http://arxiv.org/abs/1909.00515v2) - [pdf](http://arxiv.org/pdf/1909.00515v2)

> Frequentist and Bayesian methods differ in many aspects, but share some basic optimal properties. In real-life classification and regression problems, situations exist in which a model based on one of the methods is preferable based on some subjective criterion. Nonparametric classification and regression techniques, such as decision trees and neural networks, have frequentist (classification and regression trees (CART) and artificial neural networks) as well as Bayesian (Bayesian CART and Bayesian neural networks) approaches to learning from data. In this work, we present two hybrid models combining the Bayesian and frequentist versions of CART and neural networks, which we call the Bayesian neural tree (BNT) models. Both models exploit the architecture of decision trees and have lesser number of parameters to tune than advanced neural networks. Such models can simultaneously perform feature selection and prediction, are highly flexible, and generalize well in settings with a limited number of training observations. We study the consistency of the proposed models, and derive the optimal value of an important model parameter. We also provide illustrative examples using a wide variety of real-life regression data sets.

</details>

<details>

<summary>2020-07-27 19:25:49 - The look-elsewhere effect from a unified Bayesian and frequentist perspective</summary>

- *Adrian E. Bayer, Uros Seljak*

- `2007.13821v1` - [abs](http://arxiv.org/abs/2007.13821v1) - [pdf](http://arxiv.org/pdf/2007.13821v1)

> When searching over a large parameter space for anomalies such as events, peaks, objects, or particles, there is a large probability that spurious signals with seemingly high significance will be found. This is known as the look-elsewhere effect and is prevalent throughout cosmology, (astro)particle physics, and beyond. To avoid making false claims of detection, one must account for this effect when assigning the statistical significance of an anomaly. This is typically accomplished by considering the trials factor, which is generally computed numerically via potentially expensive simulations. In this paper we develop a continuous generalization of the Bonferroni and Sidak corrections by applying the Laplace approximation to evaluate the Bayes factor, and in turn relating the trials factor to the prior-to-posterior volume ratio. We use this to define a test statistic whose frequentist properties have a simple interpretation in terms of the global $p$-value, or statistical significance. We apply this method to various physics-based examples and show it to work well for the full range of $p$-values, i.e. in both the asymptotic and non-asymptotic regimes. We also show that this method naturally accounts for other model complexities such as additional degrees of freedom, generalizing Wilks' theorem. This provides a fast way to quantify statistical significance in light of the look-elsewhere effect, without resorting to expensive simulations.

</details>

<details>

<summary>2020-07-28 11:23:40 - Toward Reliable Models for Authenticating Multimedia Content: Detecting Resampling Artifacts With Bayesian Neural Networks</summary>

- *Anatol Maier, Benedikt Lorch, Christian Riess*

- `2007.14132v1` - [abs](http://arxiv.org/abs/2007.14132v1) - [pdf](http://arxiv.org/pdf/2007.14132v1)

> In multimedia forensics, learning-based methods provide state-of-the-art performance in determining origin and authenticity of images and videos. However, most existing methods are challenged by out-of-distribution data, i.e., with characteristics that are not covered in the training set. This makes it difficult to know when to trust a model, particularly for practitioners with limited technical background.   In this work, we make a first step toward redesigning forensic algorithms with a strong focus on reliability. To this end, we propose to use Bayesian neural networks (BNN), which combine the power of deep neural networks with the rigorous probabilistic formulation of a Bayesian framework. Instead of providing a point estimate like standard neural networks, BNNs provide distributions that express both the estimate and also an uncertainty range.   We demonstrate the usefulness of this framework on a classical forensic task: resampling detection. The BNN yields state-of-the-art detection performance, plus excellent capabilities for detecting out-of-distribution samples. This is demonstrated for three pathologic issues in resampling detection, namely unseen resampling factors, unseen JPEG compression, and unseen resampling algorithms. We hope that this proposal spurs further research toward reliability in multimedia forensics.

</details>

<details>

<summary>2020-07-28 14:05:14 - Bayesian Updates Compose Optically</summary>

- *Toby St. Clere Smithe*

- `2006.01631v2` - [abs](http://arxiv.org/abs/2006.01631v2) - [pdf](http://arxiv.org/pdf/2006.01631v2)

> Bayes' rule tells us how to invert a causal process in order to update our beliefs in light of new evidence. If the process is believed to have a complex compositional structure, we may ask whether composing the inversions of the component processes gives the same belief update as the inversion of the whole. We answer this question affirmatively, showing that the relevant compositional structure is precisely that of the lens pattern, and that we can think of Bayesian inversion as a particular instance of a state-dependent morphism in a corresponding fibred category. We define a general notion of (mixed) Bayesian lens, and discuss the (un)lawfulness of these lenses when their contravariant components are exact Bayesian inversions. We prove our main result both abstractly and concretely, for both discrete and continuous states, taking care to illustrate the common structures.

</details>

<details>

<summary>2020-07-28 15:22:52 - Modeling Function-Valued Processes with Nonseparable and/or Nonstationary Covariance Structure</summary>

- *Evandro Konzen, Jian Qing Shi, Zhanfeng Wang*

- `1903.09981v2` - [abs](http://arxiv.org/abs/1903.09981v2) - [pdf](http://arxiv.org/pdf/1903.09981v2)

> We discuss a general Bayesian framework on modeling multidimensional function-valued processes by using a Gaussian process or a heavy-tailed process as a prior, enabling us to handle nonseparable and/or nonstationary covariance structure. The nonstationarity is introduced by a convolution-based approach through a varying anisotropy matrix, whose parameters vary along the input space and are estimated via a local empirical Bayesian method. For the varying matrix, we propose to use a spherical parametrization, leading to unconstrained and interpretable parameters. The unconstrained nature allows the parameters to be modeled as a nonparametric function of time, spatial location or other covariates. The interpretation of the parameters is based on closed-form expressions, providing valuable insights into nonseparable covariance structures. Furthermore, to extract important information in data with complex covariance structure, the Bayesian framework can decompose the function-valued processes using the eigenvalues and eigensurfaces calculated from the estimated covariance structure. The results are demonstrated by simulation studies and by an application to wind intensity data. Supplementary materials for this article are available online.

</details>

<details>

<summary>2020-07-28 16:01:13 - Bayesian Matrix Completion Approach to Causal Inference with Panel Data</summary>

- *Masahiro Tanaka*

- `1911.01287v4` - [abs](http://arxiv.org/abs/1911.01287v4) - [pdf](http://arxiv.org/pdf/1911.01287v4)

> This study proposes a new Bayesian approach to infer binary treatment effects. The approach treats counterfactual untreated outcomes as missing observations and infers them by completing a matrix composed of realized and potential untreated outcomes using a data augmentation technique. We also develop a tailored prior that helps in the identification of parameters and induces the matrix of untreated outcomes to be approximately low rank. Posterior draws are simulated using a Markov Chain Monte Carlo sampler. While the proposed approach is similar to synthetic control methods and other related methods, it has several notable advantages. First, unlike synthetic control methods, the proposed approach does not require stringent assumptions. Second, in contrast to non-Bayesian approaches, the proposed method can quantify uncertainty about inferences in a straightforward and consistent manner. By means of a series of simulation studies, we show that our proposal has a better finite sample performance than that of the existing approaches.

</details>

<details>

<summary>2020-07-28 16:34:33 - Combining Bayesian Optimization and Lipschitz Optimization</summary>

- *Mohamed Osama Ahmed, Sharan Vaswani, Mark Schmidt*

- `1810.04336v2` - [abs](http://arxiv.org/abs/1810.04336v2) - [pdf](http://arxiv.org/pdf/1810.04336v2)

> Bayesian optimization and Lipschitz optimization have developed alternative techniques for optimizing black-box functions. They each exploit a different form of prior about the function. In this work, we explore strategies to combine these techniques for better global optimization. In particular, we propose ways to use the Lipschitz continuity assumption within traditional BO algorithms, which we call Lipschitz Bayesian optimization (LBO). This approach does not increase the asymptotic runtime and in some cases drastically improves the performance (while in the worst-case the performance is similar). Indeed, in a particular setting, we prove that using the Lipschitz information yields the same or a better bound on the regret compared to using Bayesian optimization on its own. Moreover, we propose a simple heuristics to estimate the Lipschitz constant, and prove that a growing estimate of the Lipschitz constant is in some sense ``harmless''. Our experiments on 15 datasets with 4 acquisition functions show that in the worst case LBO performs similar to the underlying BO method while in some cases it performs substantially better. Thompson sampling in particular typically saw drastic improvements (as the Lipschitz information corrected for its well-known ``over-exploration'' phenomenon) and its LBO variant often outperformed other acquisition functions.

</details>

<details>

<summary>2020-07-29 03:59:33 - Bayesian adaptive N-of-1 trials for estimating population and individual treatment effects</summary>

- *S. G. Jagath Senarathne, Antony M. Overstall, James M. McGree*

- `1911.00878v3` - [abs](http://arxiv.org/abs/1911.00878v3) - [pdf](http://arxiv.org/pdf/1911.00878v3)

> This article proposes a novel adaptive design algorithm that can be used to find optimal treatment allocations in N-of-1 clinical trials. This new methodology uses two Laplace approximations to provide a computationally efficient estimate of population and individual random effects within a repeated measures, adaptive design framework. Given the efficiency of this approach, it is also adopted for treatment selection to target the collection of data for the precise estimation of treatment effects. To evaluate this approach, we consider both a simulated and motivating N-of-1 clinical trial from the literature. For each trial, our methods were compared to the multi-armed bandit approach and a randomised N-of-1 trial design in terms of identifying the best treatment for each patient and the information gained about the model parameters. The results show that our new approach selects designs that are highly efficient in achieving each of these objectives. As such, we propose our Laplace-based algorithm as an efficient approach for designing adaptive N-of-1 trials.

</details>

<details>

<summary>2020-07-29 20:27:38 - Where Does Haydn End and Mozart Begin? Composer Classification of String Quartets</summary>

- *Katherine C. Kempfert, Samuel W. K. Wong*

- `1809.05075v3` - [abs](http://arxiv.org/abs/1809.05075v3) - [pdf](http://arxiv.org/pdf/1809.05075v3)

> For centuries, the history and music of Joseph Franz Haydn and Wolfgang Amadeus Mozart have been compared by scholars. Recently, the growing field of music information retrieval (MIR) has offered quantitative analyses to complement traditional qualitative analyses of these composers. In this MIR study, we classify the composer of Haydn and Mozart string quartets based on the content of their scores. Our contribution is an interpretable statistical and machine learning approach that provides high classification accuracies and musical relevance. We develop novel global features that are automatically computed from symbolic data and informed by musicological Haydn-Mozart comparative studies, particularly relating to the sonata form. Several of these proposed features are found to be important for distinguishing between Haydn and Mozart string quartets. Our Bayesian logistic regression model attains leave-one-out classification accuracies over 84%, higher than prior works and providing interpretations that could aid in assessing musicological claims. Overall, our work can help expand the longstanding dialogue surrounding Haydn and Mozart and exemplify the benefit of interpretable machine learning in MIR, with potential applications to music generation and classification of other classical composers.

</details>

<details>

<summary>2020-07-30 00:48:35 - Robust Inference and Model Criticism Using Bagged Posteriors</summary>

- *Jonathan H. Huggins, Jeffrey W. Miller*

- `1912.07104v3` - [abs](http://arxiv.org/abs/1912.07104v3) - [pdf](http://arxiv.org/pdf/1912.07104v3)

> Standard Bayesian inference is known to be sensitive to model misspecification, leading to unreliable uncertainty quantification and poor predictive performance. However, finding generally applicable and computationally feasible methods for robust Bayesian inference under misspecification has proven to be a difficult challenge. An intriguing, easy-to-use, and widely applicable approach is to use bagging on the Bayesian posterior ("BayesBag"); that is, to use the average of posterior distributions conditioned on bootstrapped datasets. In this paper, we develop the asymptotic theory of BayesBag, propose a model-data mismatch index for model criticism using BayesBag, and empirically validate our theory and methodology on synthetic and real-world data in linear regression, sparse logistic regression, and a hierarchical mixed effects model. We find that in the presence of significant misspecification, BayesBag yields more reproducible inferences and has better predictive accuracy than the standard Bayesian posterior; on the other hand, when the model is correctly specified, BayesBag produces superior or equally good results. Overall, our results demonstrate that BayesBag combines the attractive modeling features of standard Bayesian inference with the distributional robustness properties of frequentist methods, providing benefits over both Bayes alone and the bootstrap alone.

</details>

<details>

<summary>2020-07-30 01:17:34 - Skewed link regression models for imbalanced binary response with applications to life insurance</summary>

- *Shuang Yin, Dipak K. Dey, Emiliano A. Valdez, Guojun Gan, Jeyaraj Vadiveloo*

- `2007.15172v1` - [abs](http://arxiv.org/abs/2007.15172v1) - [pdf](http://arxiv.org/pdf/2007.15172v1)

> For a portfolio of life insurance policies observed for a stated period of time, e.g., one year, mortality is typically a rare event. When we examine the outcome of dying or not from such portfolios, we have an imbalanced binary response. The popular logistic and probit regression models can be inappropriate for imbalanced binary response as model estimates may be biased, and if not addressed properly, it can lead to serious adverse predictions. In this paper, we propose the use of skewed link regression models (Generalized Extreme Value, Weibull, and Fre\`chet link models) as more superior models to handle imbalanced binary response. We adopt a fully Bayesian approach for the generalized linear models (GLMs) under the proposed link functions to help better explain the high skewness. To calibrate our proposed Bayesian models, we use a real dataset of death claims experience drawn from a life insurance company's portfolio. Bayesian estimates of parameters were obtained using the Metropolis-Hastings algorithm and for Bayesian model selection and comparison, the Deviance Information Criterion (DIC) statistic has been used. For our mortality dataset, we find that these skewed link models are more superior than the widely used binary models with standard link functions. We evaluate the predictive power of the different underlying models by measuring and comparing aggregated death counts and death benefits.

</details>

<details>

<summary>2020-07-30 06:16:00 - Quantity vs. Quality: On Hyperparameter Optimization for Deep Reinforcement Learning</summary>

- *Lars Hertel, Pierre Baldi, Daniel L. Gillen*

- `2007.14604v2` - [abs](http://arxiv.org/abs/2007.14604v2) - [pdf](http://arxiv.org/pdf/2007.14604v2)

> Reinforcement learning algorithms can show strong variation in performance between training runs with different random seeds. In this paper we explore how this affects hyperparameter optimization when the goal is to find hyperparameter settings that perform well across random seeds. In particular, we benchmark whether it is better to explore a large quantity of hyperparameter settings via pruning of bad performers, or if it is better to aim for quality of collected results by using repetitions. For this we consider the Successive Halving, Random Search, and Bayesian Optimization algorithms, the latter two with and without repetitions. We apply these to tuning the PPO2 algorithm on the Cartpole balancing task and the Inverted Pendulum Swing-up task. We demonstrate that pruning may negatively affect the optimization and that repeated sampling does not help in finding hyperparameter settings that perform better across random seeds. From our experiments we conclude that Bayesian optimization with a noise robust acquisition function is the best choice for hyperparameter optimization in reinforcement learning tasks.

</details>

<details>

<summary>2020-07-31 09:57:27 - Towards personalized computer simulations of breast cancer treatment</summary>

- *Alvaro Köhn-Luque, Xiaoran Lai, Arnoldo Frigessi*

- `2007.15934v1` - [abs](http://arxiv.org/abs/2007.15934v1) - [pdf](http://arxiv.org/pdf/2007.15934v1)

> Cancer pathology is unique to a given individual, and developing personalized diagnostic and treatment protocols are a primary concern. Mathematical modeling and simulation is a promising approach to personalized cancer medicine. Yet, the complexity, heterogeneity and multiscale nature of cancer present severe challenges. One of the major barriers to use mathematical models to predict the outcome of therapeutic regimens in a particular patient lies in their initialization and parameterization in order to reflect individual cancer characteristics accurately. Here we present a study where we used multitype measurements acquired routinely on a single breast tumor, including histopathology, magnetic resonance imaging (MRI), and molecular profiling, to personalize a multiscale hybrid cellular automaton model of breast cancer treated with chemotherapeutic and antiangiogenic agents. We model drug pharmacokinetics and pharmacodynamics at the tumor tissue level but with cellular and subcellular resolution. We simulate those spatio-temporal dynamics in 2D cross-sections of tumor portions over 12-week therapy regimes, resulting in complex and computationally intensive simulations. For such computationally demanding systems, formal statistical inference methods to estimate individual parameters from data have not been feasible in practice to until most recently, after the emergence of machine learning techniques applied to likelihood-free inference methods. Here we use the inference advances provided by Bayesian optimization to fit our model to simulated data of individual patients. In this way, we investigate if some key parameters can be estimated from a series of measurements of cell density in the tumor tissue, as well as how often the measurements need to be taken to allow reliable predictions.

</details>

<details>

<summary>2020-07-31 13:55:11 - The rule of conditional probability is valid in quantum theory [Comment on Gelman & Yao's "Holes in Bayesian Statistics"]</summary>

- *P. G. L. Porta Mana*

- `2007.08160v3` - [abs](http://arxiv.org/abs/2007.08160v3) - [pdf](http://arxiv.org/pdf/2007.08160v3)

> In a recent manuscript, Gelman & Yao (2020) claim that "the usual rules of conditional probability fail in the quantum realm" and that "probability theory isn't true (quantum physics)" and purport to support these statements with the example of a quantum double-slit experiment. The present comment recalls some relevant literature in quantum theory and shows that (i) Gelman & Yao's statements are false; in fact, the quantum example confirms the rules of probability theory; (ii) the particular inequality found in the quantum example can be shown to appear also in very non-quantum examples, such as drawing from an urn; thus there is nothing peculiar to quantum theory in this matter. A couple of wrong or imprecise statements about quantum theory in the cited manuscript are also corrected.

</details>

<details>

<summary>2020-07-31 16:04:08 - HMCNAS: Neural Architecture Search using Hidden Markov Chains and Bayesian Optimization</summary>

- *Vasco Lopes, Luís A. Alexandre*

- `2007.16149v1` - [abs](http://arxiv.org/abs/2007.16149v1) - [pdf](http://arxiv.org/pdf/2007.16149v1)

> Neural Architecture Search has achieved state-of-the-art performance in a variety of tasks, out-performing human-designed networks. However, many assumptions, that require human definition, related with the problems being solved or the models generated are still needed: final model architectures, number of layers to be sampled, forced operations, small search spaces, which ultimately contributes to having models with higher performances at the cost of inducing bias into the system. In this paper, we propose HMCNAS, which is composed of two novel components: i) a method that leverages information about human-designed models to autonomously generate a complex search space, and ii) an Evolutionary Algorithm with Bayesian Optimization that is capable of generating competitive CNNs from scratch, without relying on human-defined parameters or small search spaces. The experimental results show that the proposed approach results in competitive architectures obtained in a very short time. HMCNAS provides a step towards generalizing NAS, by providing a way to create competitive models, without requiring any human knowledge about the specific task.

</details>

<details>

<summary>2020-07-31 17:36:21 - Combining Breast Cancer Risk Prediction Models</summary>

- *Zoe Guan, Theodore Huang, Anne Marie McCarthy, Kevin S. Hughes, Alan Semine, Hajime Uno, Lorenzo Trippa, Giovanni Parmigiani, Danielle Braun*

- `2008.01019v1` - [abs](http://arxiv.org/abs/2008.01019v1) - [pdf](http://arxiv.org/pdf/2008.01019v1)

> Accurate risk stratification is key to reducing cancer morbidity through targeted screening and preventative interventions. Numerous breast cancer risk prediction models have been developed, but they often give predictions with conflicting clinical implications. Integrating information from different models may improve the accuracy of risk predictions, which would be valuable for both clinicians and patients. BRCAPRO and BCRAT are two widely used models based on largely complementary sets of risk factors. BRCAPRO is a Bayesian model that uses detailed family history information to estimate the probability of carrying a BRCA1/2 mutation, as well as future risk of breast and ovarian cancer, based on mutation prevalence and penetrance (age-specific probability of developing cancer given genotype). BCRAT uses a relative hazard model based on first-degree family history and non-genetic risk factors. We consider two approaches for combining BRCAPRO and BCRAT: 1) modifying the penetrance functions in BRCAPRO using relative hazard estimates from BCRAT, and 2) training an ensemble model that takes as input BRCAPRO and BCRAT predictions. We show that the combination models achieve performance gains over BRCAPRO and BCRAT in simulations and data from the Cancer Genetics Network.

</details>

<details>

<summary>2020-07-31 18:37:31 - Cold Posteriors and Aleatoric Uncertainty</summary>

- *Ben Adlam, Jasper Snoek, Samuel L. Smith*

- `2008.00029v1` - [abs](http://arxiv.org/abs/2008.00029v1) - [pdf](http://arxiv.org/pdf/2008.00029v1)

> Recent work has observed that one can outperform exact inference in Bayesian neural networks by tuning the "temperature" of the posterior on a validation set (the "cold posterior" effect). To help interpret this phenomenon, we argue that commonly used priors in Bayesian neural networks can significantly overestimate the aleatoric uncertainty in the labels on many classification datasets. This problem is particularly pronounced in academic benchmarks like MNIST or CIFAR, for which the quality of the labels is high. For the special case of Gaussian process regression, any positive temperature corresponds to a valid posterior under a modified prior, and tuning this temperature is directly analogous to empirical Bayes. On classification tasks, there is no direct equivalence between modifying the prior and tuning the temperature, however reducing the temperature can lead to models which better reflect our belief that one gains little information by relabeling existing examples in the training set. Therefore although cold posteriors do not always correspond to an exact inference procedure, we believe they may often better reflect our true prior beliefs.

</details>

<details>

<summary>2020-07-31 19:06:21 - Model Based Screening Embedded Bayesian Variable Selection for Ultra-high Dimensional Settings</summary>

- *Dongjin Li, Somak Dutta, Vivekananda Roy*

- `2006.07561v2` - [abs](http://arxiv.org/abs/2006.07561v2) - [pdf](http://arxiv.org/pdf/2006.07561v2)

> We develop a Bayesian variable selection method, called SVEN, based on a hierarchical Gaussian linear model with priors placed on the regression coefficients as well as on the model space. Sparsity is achieved by using degenerate spike priors on inactive variables, whereas Gaussian slab priors are placed on the coefficients for the important predictors making the posterior probability of a model available in explicit form (up to a normalizing constant). The strong model selection consistency is shown to be attained when the number of predictors grows nearly exponentially with the sample size and even when the norm of mean effects solely due to the unimportant variables diverge, which is a novel attractive feature. An appealing byproduct of SVEN is the construction of novel model weight adjusted prediction intervals. Embedding a unique model based screening and using fast Cholesky updates, SVEN produces a highly scalable computational framework to explore gigantic model spaces, rapidly identify the regions of high posterior probabilities and make fast inference and prediction. A temperature schedule guided by our model selection consistency derivations is used to further mitigate multimodal posterior distributions. The performance of SVEN is demonstrated through a number of simulation experiments and a real data example from a genome wide association study with over half a million markers.

</details>

<details>

<summary>2020-07-31 22:34:53 - A Generalized Bayesian Approach to Model Calibration</summary>

- *Tony Tohme, Kevin Vanslette, Kamal Youcef-Toumi*

- `1911.11715v3` - [abs](http://arxiv.org/abs/1911.11715v3) - [pdf](http://arxiv.org/pdf/1911.11715v3)

> In model development, model calibration and validation play complementary roles toward learning reliable models. In this article, we expand the Bayesian Validation Metric framework to a general calibration and validation framework by inverting the validation mathematics into a generalized Bayesian method for model calibration and regression. We perform Bayesian regression based on a user's definition of model-data agreement. This allows for model selection on any type of data distribution, unlike Bayesian and standard regression techniques, that "fail" in some cases. We show that our tool is capable of representing and combining least squares, likelihood-based, and Bayesian calibration techniques in a single framework while being able to generalize aspects of these methods. This tool also offers new insights into the interpretation of the predictive envelopes (also known as confidence bands) while giving the analyst more control over these envelopes. We demonstrate the validity of our method by providing three numerical examples to calibrate different models, including a model for energy dissipation in lap joints under impact loading. By calibrating models with respect to the validation metrics one desires a model to ultimately pass, reliability and safety metrics may be integrated into and automatically adopted by the model in the calibration phase.

</details>


## 2020-08

<details>

<summary>2020-08-01 17:14:37 - A Bayesian Mixture Model for Changepoint Estimation Using Ordinal Predictors</summary>

- *Emily Roberts, Lili Zhao*

- `2008.00300v1` - [abs](http://arxiv.org/abs/2008.00300v1) - [pdf](http://arxiv.org/pdf/2008.00300v1)

> In regression models, predictor variables with inherent ordering, such as tumor staging ranging and ECOG performance status, are commonly seen in medical settings. Statistically, it may be difficult to determine the functional form of an ordinal predictor variable. Often, such a variable is dichotomized based on whether it is above or below a certain cutoff. Other methods conveniently treat the ordinal predictor as a continuous variable and assume a linear relationship with the outcome. However, arbitrarily choosing a method may lead to inaccurate inference and treatment. In this paper, we propose a Bayesian mixture model to simultaneously assess the appropriate form of the predictor in regression models by considering the presence of a changepoint through the lens of a threshold detection problem. By using a mixture model framework to consider both dichotomous and linear forms for the variable, the estimate is a weighted average of linear and binary parameterizations. This method is applicable to continuous, binary, and survival outcomes, and easily amenable to penalized regression. We evaluated the proposed method using simulation studies and apply it to two real datasets. We provide JAGS code for easy implementation.

</details>

<details>

<summary>2020-08-01 18:50:20 - Measuring Spatial Allocative Efficiency in Basketball</summary>

- *Nathan Sandholtz, Jacob Mortensen, Luke Bornn*

- `1912.05129v2` - [abs](http://arxiv.org/abs/1912.05129v2) - [pdf](http://arxiv.org/pdf/1912.05129v2)

> Every shot in basketball has an opportunity cost; one player's shot eliminates all potential opportunities from their teammates for that play. For this reason, player-shot efficiency should ultimately be considered relative to the lineup. This aspect of efficiency---the optimal way to allocate shots within a lineup---is the focus of our paper. Allocative efficiency should be considered in a spatial context since the distribution of shot attempts within a lineup is highly dependent on court location. We propose a new metric for spatial allocative efficiency by comparing a player's field goal percentage (FG%) to their field goal attempt (FGA) rate in context of both their four teammates on the court and the spatial distribution of their shots. Leveraging publicly available data provided by the National Basketball Association (NBA), we estimate player FG% at every location in the offensive half court using a Bayesian hierarchical model. Then, by ordering a lineup's estimated FG%s and pairing these rankings with the lineup's empirical FGA rate rankings, we detect areas where the lineup exhibits inefficient shot allocation. Lastly, we analyze the impact that sub-optimal shot allocation has on a team's overall offensive potential, demonstrating that inefficient shot allocation correlates with reduced scoring.

</details>

<details>

<summary>2020-08-01 19:23:34 - Convergence of Sparse Variational Inference in Gaussian Processes Regression</summary>

- *David R. Burt, Carl Edward Rasmussen, Mark van der Wilk*

- `2008.00323v1` - [abs](http://arxiv.org/abs/2008.00323v1) - [pdf](http://arxiv.org/pdf/2008.00323v1)

> Gaussian processes are distributions over functions that are versatile and mathematically convenient priors in Bayesian modelling. However, their use is often impeded for data with large numbers of observations, $N$, due to the cubic (in $N$) cost of matrix operations used in exact inference. Many solutions have been proposed that rely on $M \ll N$ inducing variables to form an approximation at a cost of $\mathcal{O}(NM^2)$. While the computational cost appears linear in $N$, the true complexity depends on how $M$ must scale with $N$ to ensure a certain quality of the approximation. In this work, we investigate upper and lower bounds on how $M$ needs to grow with $N$ to ensure high quality approximations. We show that we can make the KL-divergence between the approximate model and the exact posterior arbitrarily small for a Gaussian-noise regression model with $M\ll N$. Specifically, for the popular squared exponential kernel and $D$-dimensional Gaussian distributed covariates, $M=\mathcal{O}((\log N)^D)$ suffice and a method with an overall computational cost of $\mathcal{O}(N(\log N)^{2D}(\log\log N)^2)$ can be used to perform inference.

</details>

<details>

<summary>2020-08-01 20:47:08 - Application of Bayesian Dynamic Linear Models to Random Allocation Clinical Trials</summary>

- *Albert. H. Lee III, Edward L Boone, Roy T. Sabo, Erin Donahue*

- `2008.00339v1` - [abs](http://arxiv.org/abs/2008.00339v1) - [pdf](http://arxiv.org/pdf/2008.00339v1)

> Random allocation models used in clinical trials aid researchers in determining which of a particular treatment provides the best results by reducing bias between groups. Often however, this determination leaves researchers battling ethical issues of providing patients with unfavorable treatments. Many methods such as Play the Winner and Randomized Play the Winner Rule have historically been utilized to determine patient allocation, however, these methods are prone to the increased assignment of unfavorable treatments. Recently a new Bayesian Method using Decreasingly Informative Priors has been proposed by \citep{sabo2014adaptive}, and later \citep{donahue2020allocation}. Yet this method can be time consuming if MCMC methods are required. We propose the use of a new method which uses Dynamic Linear Model (DLM) \citep{harrison1999bayesian} to increase allocation speed while also decreasing patient allocation samples necessary to identify the more favorable treatment. Furthermore, a sensitivity analysis is conducted on multiple parameters. Finally, a Bayes Factor is calculated to determine the proportion of unused patient budget remaining at a specified cut off and this will be used to determine decisive evidence in favor of the better treatment.

</details>

<details>

<summary>2020-08-02 02:56:30 - Bayesian Optimization for Selecting Efficient Machine Learning Models</summary>

- *Lidan Wang, Franck Dernoncourt, Trung Bui*

- `2008.00386v1` - [abs](http://arxiv.org/abs/2008.00386v1) - [pdf](http://arxiv.org/pdf/2008.00386v1)

> The performance of many machine learning models depends on their hyper-parameter settings. Bayesian Optimization has become a successful tool for hyper-parameter optimization of machine learning algorithms, which aims to identify optimal hyper-parameters during an iterative sequential process. However, most of the Bayesian Optimization algorithms are designed to select models for effectiveness only and ignore the important issue of model training efficiency. Given that both model effectiveness and training time are important for real-world applications, models selected for effectiveness may not meet the strict training time requirements necessary to deploy in a production environment. In this work, we present a unified Bayesian Optimization framework for jointly optimizing models for both prediction effectiveness and training efficiency. We propose an objective that captures the tradeoff between these two metrics and demonstrate how we can jointly optimize them in a principled Bayesian Optimization framework. Experiments on model selection for recommendation tasks indicate models selected this way significantly improves model training efficiency while maintaining strong effectiveness as compared to state-of-the-art Bayesian Optimization algorithms.

</details>

<details>

<summary>2020-08-02 16:03:52 - On Pruning for Score-Based Bayesian Network Structure Learning</summary>

- *Alvaro H. C. Correia, James Cussens, Cassio de Campos*

- `1905.09943v2` - [abs](http://arxiv.org/abs/1905.09943v2) - [pdf](http://arxiv.org/pdf/1905.09943v2)

> Many algorithms for score-based Bayesian network structure learning (BNSL), in particular exact ones, take as input a collection of potentially optimal parent sets for each variable in the data. Constructing such collections naively is computationally intensive since the number of parent sets grows exponentially with the number of variables. Thus, pruning techniques are not only desirable but essential. While good pruning rules exist for the Bayesian Information Criterion (BIC), current results for the Bayesian Dirichlet equivalent uniform (BDeu) score reduce the search space very modestly, hampering the use of the (often preferred) BDeu. We derive new non-trivial theoretical upper bounds for the BDeu score that considerably improve on the state-of-the-art. Since the new bounds are mathematically proven to be tighter than previous ones and at little extra computational cost, they are a promising addition to BNSL methods.

</details>

<details>

<summary>2020-08-02 16:37:01 - Detecting Adversarial Examples for Speech Recognition via Uncertainty Quantification</summary>

- *Sina Däubener, Lea Schönherr, Asja Fischer, Dorothea Kolossa*

- `2005.14611v2` - [abs](http://arxiv.org/abs/2005.14611v2) - [pdf](http://arxiv.org/pdf/2005.14611v2)

> Machine learning systems and also, specifically, automatic speech recognition (ASR) systems are vulnerable against adversarial attacks, where an attacker maliciously changes the input. In the case of ASR systems, the most interesting cases are targeted attacks, in which an attacker aims to force the system into recognizing given target transcriptions in an arbitrary audio sample. The increasing number of sophisticated, quasi imperceptible attacks raises the question of countermeasures. In this paper, we focus on hybrid ASR systems and compare four acoustic models regarding their ability to indicate uncertainty under attack: a feed-forward neural network and three neural networks specifically designed for uncertainty quantification, namely a Bayesian neural network, Monte Carlo dropout, and a deep ensemble. We employ uncertainty measures of the acoustic model to construct a simple one-class classification model for assessing whether inputs are benign or adversarial. Based on this approach, we are able to detect adversarial examples with an area under the receiving operator curve score of more than 0.99. The neural networks for uncertainty quantification simultaneously diminish the vulnerability to the attack, which is reflected in a lower recognition accuracy of the malicious target text in comparison to a standard hybrid ASR system.

</details>

<details>

<summary>2020-08-03 02:44:59 - Adaptive Variational Bayesian Inference for Sparse Deep Neural Network</summary>

- *Jincheng Bai, Qifan Song, Guang Cheng*

- `1910.04355v3` - [abs](http://arxiv.org/abs/1910.04355v3) - [pdf](http://arxiv.org/pdf/1910.04355v3)

> In this work, we focus on variational Bayesian inference on the sparse Deep Neural Network (DNN) modeled under a class of spike-and-slab priors. Given a pre-specified sparse DNN structure, the corresponding variational posterior contraction rate is characterized that reveals a trade-off between the variational error and the approximation error, which are both determined by the network structural complexity (i.e., depth, width and sparsity). However, the optimal network structure, which strikes the balance of the aforementioned trade-off and yields the best rate, is generally unknown in reality. Therefore, our work further develops an {\em adaptive} variational inference procedure that can automatically select a reasonably good (data-dependent) network structure that achieves the best contraction rate, without knowing the optimal network structure. In particular, when the true function is H{\"o}lder smooth, the adaptive variational inference is capable to attain (near-)optimal rate without the knowledge of smoothness level. The above rate still suffers from the curse of dimensionality, and thus motivates the teacher-student setup, i.e., the true function is a sparse DNN model, under which the rate only logarithmically depends on the input dimension.

</details>

<details>

<summary>2020-08-03 06:55:10 - A spatial multinomial logit model for analysing urban expansion</summary>

- *Tamás Krisztin, Philipp Piribauer, Michael Wögerer*

- `2008.00673v1` - [abs](http://arxiv.org/abs/2008.00673v1) - [pdf](http://arxiv.org/pdf/2008.00673v1)

> The paper proposes a Bayesian multinomial logit model to analyse spatial patterns of urban expansion. The specification assumes that the log-odds of each class follow a spatial autoregressive process. Using recent advances in Bayesian computing, our model allows for a computationally efficient treatment of the spatial multinomial logit model. This allows us to assess spillovers between regions and across land use classes. In a series of Monte Carlo studies, we benchmark our model against other competing specifications. The paper also showcases the performance of the proposed specification using European regional data. Our results indicate that spatial dependence plays a key role in land sealing process of cropland and grassland. Moreover, we uncover land sealing spillovers across multiple classes of arable land.

</details>

<details>

<summary>2020-08-03 08:58:18 - Deep Bayesian Bandits: Exploring in Online Personalized Recommendations</summary>

- *Dalin Guo, Sofia Ira Ktena, Ferenc Huszar, Pranay Kumar Myana, Wenzhe Shi, Alykhan Tejani*

- `2008.00727v1` - [abs](http://arxiv.org/abs/2008.00727v1) - [pdf](http://arxiv.org/pdf/2008.00727v1)

> Recommender systems trained in a continuous learning fashion are plagued by the feedback loop problem, also known as algorithmic bias. This causes a newly trained model to act greedily and favor items that have already been engaged by users. This behavior is particularly harmful in personalised ads recommendations, as it can also cause new campaigns to remain unexplored. Exploration aims to address this limitation by providing new information about the environment, which encompasses user preference, and can lead to higher long-term reward. In this work, we formulate a display advertising recommender as a contextual bandit and implement exploration techniques that require sampling from the posterior distribution of click-through-rates in a computationally tractable manner. Traditional large-scale deep learning models do not provide uncertainty estimates by default. We approximate these uncertainty measurements of the predictions by employing a bootstrapped model with multiple heads and dropout units. We benchmark a number of different models in an offline simulation environment using a publicly available dataset of user-ads engagements. We test our proposed deep Bayesian bandits algorithm in the offline simulation and online AB setting with large-scale production traffic, where we demonstrate a positive gain of our exploration model.

</details>

<details>

<summary>2020-08-03 10:08:01 - MCMC for Bayesian uncertainty quantification from time-series data</summary>

- *Philip Maybank, Patrick Peltzer, Uwe Naumann, Ingo Bojak*

- `2005.14281v2` - [abs](http://arxiv.org/abs/2005.14281v2) - [pdf](http://arxiv.org/pdf/2005.14281v2)

> Many problems in science and engineering require uncertainty quantification that accounts for observed data. For example, in computational neuroscience, Neural Population Models (NPMs) are mechanistic models that describe brain physiology in a range of different states. Within computational neuroscience there is growing interest in the inverse problem of inferring NPM parameters from recordings such as the EEG (Electroencephalogram). Uncertainty quantification is essential in this application area in order to infer the mechanistic effect of interventions such as anaesthesia. This paper presents C++ software for Bayesian uncertainty quantification in the parameters of NPMs from approximately stationary data using Markov Chain Monte Carlo (MCMC). Modern MCMC methods require first order (and in some cases higher order) derivatives of the posterior density. The software presented offers two distinct methods of evaluating derivatives: finite differences and exact derivatives obtained through Algorithmic Differentiation (AD). For AD, two different implementations are used: the open source Stan Math Library and the commercially licenced dco/c++ tool distributed by NAG (Numerical Algorithms Group). The use of derivative information in MCMC sampling is demonstrated through a simple example, the noise-driven harmonic oscillator. And different methods for computing derivatives are compared. The software is written in a modular object-oriented way such that it can be extended to derivative based MCMC for other scientific domains.

</details>

<details>

<summary>2020-08-03 15:25:10 - Adaptive Physics-Informed Neural Networks for Markov-Chain Monte Carlo</summary>

- *Mohammad Amin Nabian, Hadi Meidani*

- `2008.01604v1` - [abs](http://arxiv.org/abs/2008.01604v1) - [pdf](http://arxiv.org/pdf/2008.01604v1)

> In this paper, we propose the Adaptive Physics-Informed Neural Networks (APINNs) for accurate and efficient simulation-free Bayesian parameter estimation via Markov-Chain Monte Carlo (MCMC). We specifically focus on a class of parameter estimation problems for which computing the likelihood function requires solving a PDE. The proposed method consists of: (1) constructing an offline PINN-UQ model as an approximation to the forward model; and (2) refining this approximate model on the fly using samples generated from the MCMC sampler. The proposed APINN method constantly refines this approximate model on the fly and guarantees that the approximation error is always less than a user-defined residual error threshold. We numerically demonstrate the performance of the proposed APINN method in solving a parameter estimation problem for a system governed by the Poisson equation.

</details>

<details>

<summary>2020-08-03 16:44:29 - Parametric Copula-GP model for analyzing multidimensional neuronal and behavioral relationships</summary>

- *Nina Kudryashova, Theoklitos Amvrosiadis, Nathalie Dupuy, Nathalie Rochefort, Arno Onken*

- `2008.01007v1` - [abs](http://arxiv.org/abs/2008.01007v1) - [pdf](http://arxiv.org/pdf/2008.01007v1)

> One of the main challenges in current systems neuroscience is the analysis of high-dimensional neuronal and behavioral data that are characterized by different statistics and timescales of the recorded variables. We propose a parametric copula model which separates the statistics of the individual variables from their dependence structure, and escapes the curse of dimensionality by using vine copula constructions. We use a Bayesian framework with Gaussian Process (GP) priors over copula parameters, conditioned on a continuous task-related variable. We validate the model on synthetic data and compare its performance in estimating mutual information against the commonly used non-parametric algorithms.   Our model provides accurate information estimates when the dependencies in the data match the parametric copulas used in our framework. When the exact density estimation with a parametric model is not possible, our Copula-GP model is still able to provide reasonable information estimates, close to the ground truth and comparable to those obtained with a neural network estimator. Finally, we apply our framework to real neuronal and behavioral recordings obtained in awake mice. We demonstrate the ability of our framework to   1) produce accurate and interpretable bivariate models for the analysis of inter-neuronal noise correlations or behavioral modulations;   2) expand to more than 100 dimensions and measure information content in the whole-population statistics. These results demonstrate that the Copula-GP framework is particularly useful for the analysis of complex multidimensional relationships between neuronal, sensory and behavioral data.

</details>

<details>

<summary>2020-08-03 21:34:35 - A Decision-Theoretic Approach for Model Interpretability in Bayesian Framework</summary>

- *Homayun Afrabandpey, Tomi Peltola, Juho Piironen, Aki Vehtari, Samuel Kaski*

- `1910.09358v2` - [abs](http://arxiv.org/abs/1910.09358v2) - [pdf](http://arxiv.org/pdf/1910.09358v2)

> A salient approach to interpretable machine learning is to restrict modeling to simple models. In the Bayesian framework, this can be pursued by restricting the model structure and prior to favor interpretable models. Fundamentally, however, interpretability is about users' preferences, not the data generation mechanism; it is more natural to formulate interpretability as a utility function. In this work, we propose an interpretability utility, which explicates the trade-off between explanation fidelity and interpretability in the Bayesian framework. The method consists of two steps. First, a reference model, possibly a black-box Bayesian predictive model which does not compromise accuracy, is fitted to the training data. Second, a proxy model from an interpretable model family that best mimics the predictive behaviour of the reference model is found by optimizing the interpretability utility function. The approach is model agnostic -- neither the interpretable model nor the reference model are restricted to a certain class of models -- and the optimization problem can be solved using standard tools. Through experiments on real-word data sets, using decision trees as interpretable models and Bayesian additive regression models as reference models, we show that for the same level of interpretability, our approach generates more accurate models than the alternative of restricting the prior. We also propose a systematic way to measure stability of interpretabile models constructed by different interpretability approaches and show that our proposed approach generates more stable models.

</details>

<details>

<summary>2020-08-03 23:45:09 - Convergence Rates for Bayesian Estimation and Testing in Monotone Regression</summary>

- *Moumita Chakraborty, Subhashis Ghosal*

- `2008.01244v1` - [abs](http://arxiv.org/abs/2008.01244v1) - [pdf](http://arxiv.org/pdf/2008.01244v1)

> Shape restrictions such as monotonicity on functions often arise naturally in statistical modeling.   We consider a Bayesian approach to the problem of estimation of a monotone regression function and testing for monotonicity. We construct a prior distribution using piecewise constant functions. For estimation, a prior imposing monotonicity of the heights of these steps is sensible, but the resulting posterior is harder to analyze theoretically. We consider a ``projection-posterior'' approach, where a conjugate normal prior is used, but the monotonicity constraint is imposed on posterior samples by a projection map on the space of monotone functions. We show that the resulting posterior contracts at the optimal rate $n^{-1/3}$ under the $L_1$-metric and at a nearly optimal rate under the empirical $L_p$-metrics for $0<p\le 2$. The projection-posterior approach is also computationally more convenient. We also construct a Bayesian test for the hypothesis of monotonicity using the posterior probability of a shrinking neighborhood of the set of monotone functions. We show that the resulting test has a universal consistency property and obtain the separation rate which ensures that the resulting power function approaches one.

</details>

<details>

<summary>2020-08-04 08:35:25 - Mixed-Variable Bayesian Optimization</summary>

- *Erik Daxberger, Anastasia Makarova, Matteo Turchetta, Andreas Krause*

- `1907.01329v4` - [abs](http://arxiv.org/abs/1907.01329v4) - [pdf](http://arxiv.org/pdf/1907.01329v4)

> The optimization of expensive to evaluate, black-box, mixed-variable functions, i.e. functions that have continuous and discrete inputs, is a difficult and yet pervasive problem in science and engineering. In Bayesian optimization (BO), special cases of this problem that consider fully continuous or fully discrete domains have been widely studied. However, few methods exist for mixed-variable domains and none of them can handle discrete constraints that arise in many real-world applications. In this paper, we introduce MiVaBo, a novel BO algorithm for the efficient optimization of mixed-variable functions combining a linear surrogate model based on expressive feature representations with Thompson sampling. We propose an effective method to optimize its acquisition function, a challenging problem for mixed-variable domains, making MiVaBo the first BO method that can handle complex constraints over the discrete variables. Moreover, we provide the first convergence analysis of a mixed-variable BO algorithm. Finally, we show that MiVaBo is significantly more sample efficient than state-of-the-art mixed-variable BO algorithms on several hyperparameter tuning tasks, including the tuning of deep generative models.

</details>

<details>

<summary>2020-08-04 20:38:42 - AMP Chain Graphs: Minimal Separators and Structure Learning Algorithms</summary>

- *Mohammad Ali Javidian, Marco Valtorta, Pooyan Jamshidi*

- `2002.10870v2` - [abs](http://arxiv.org/abs/2002.10870v2) - [pdf](http://arxiv.org/pdf/2002.10870v2)

> We address the problem of finding a minimal separator in an Andersson-Madigan-Perlman chain graph (AMP CG), namely, finding a set Z of nodes that separates a given nonadjacent pair of nodes such that no proper subset of Z separates that pair. We analyze several versions of this problem and offer polynomial-time algorithms for each. These include finding a minimal separator from a restricted set of nodes, finding a minimal separator for two given disjoint sets, and testing whether a given separator is minimal. To address the problem of learning the structure of AMP CGs from data, we show that the PC-like algorithm (Pena, 2012) is order-dependent, in the sense that the output can depend on the order in which the variables are given. We propose several modifications of the PC-like algorithm that remove part or all of this order-dependence. We also extend the decomposition-based approach for learning Bayesian networks (BNs) proposed by (Xie et al., 2006) to learn AMP CGs, which include BNs as a special case, under the faithfulness assumption. We prove the correctness of our extension using the minimal separator results. Using standard benchmarks and synthetically generated models and data in our experiments demonstrate the competitive performance of our decomposition-based method, called LCD-AMP, in comparison with the (modified versions of) PC-like algorithm. The LCD-AMP algorithm usually outperforms the PC-like algorithm, and our modifications of the PC-like algorithm learn structures that are more similar to the underlying ground truth graphs than the original PC-like algorithm, especially in high-dimensional settings. In particular, we empirically show that the results of both algorithms are more accurate and stabler when the sample size is reasonably large and the underlying graph is sparse.

</details>

<details>

<summary>2020-08-05 01:28:44 - Information Newton's flow: second-order optimization method in probability space</summary>

- *Yifei Wang, Wuchen Li*

- `2001.04341v4` - [abs](http://arxiv.org/abs/2001.04341v4) - [pdf](http://arxiv.org/pdf/2001.04341v4)

> We introduce a framework for Newton's flows in probability space with information metrics, named information Newton's flows. Here two information metrics are considered, including both the Fisher-Rao metric and the Wasserstein-2 metric. A known fact is that overdamped Langevin dynamics correspond to Wasserstein gradient flows of Kullback-Leibler (KL) divergence. Extending this fact to Wasserstein Newton's flows, we derive Newton's Langevin dynamics. We provide examples of Newton's Langevin dynamics in both one-dimensional space and Gaussian families. For the numerical implementation, we design sampling efficient variational methods in affine models and reproducing kernel Hilbert space (RKHS) to approximate Wasserstein Newton's directions. We also establish convergence results of the proposed information Newton's method with approximated directions. Several numerical examples from Bayesian sampling problems are shown to demonstrate the effectiveness of the proposed method.

</details>

<details>

<summary>2020-08-05 01:32:04 - A generalized information criterion for high-dimensional PCA rank selection</summary>

- *Hung Hung, Su-Yun Huang, Ching-Kang Ing*

- `2004.13914v2` - [abs](http://arxiv.org/abs/2004.13914v2) - [pdf](http://arxiv.org/pdf/2004.13914v2)

> Principal component analysis (PCA) is the most commonly used statistical procedure for dimension reduction. An important issue for applying PCA is to determine the rank, which is the number of dominant eigenvalues of the covariance matrix. The Akaike information criterion (AIC) and Bayesian information criterion (BIC) are among the most widely used rank selection methods. Both use the number of free parameters for assessing model complexity. In this work, we adopt the generalized information criterion (GIC) to propose a new method for PCA rank selection under the high-dimensional framework. The GIC model complexity takes into account the sizes of covariance eigenvalues and can be better adaptive to practical applications. Asymptotic properties of GIC are derived and the selection consistency is established under the generalized spiked covariance model.

</details>

<details>

<summary>2020-08-05 06:11:17 - Learning and Sampling of Atomic Interventions from Observations</summary>

- *Arnab Bhattacharyya, Sutanu Gayen, Saravanan Kandasamy, Ashwin Maran, N. V. Vinodchandran*

- `2002.04232v2` - [abs](http://arxiv.org/abs/2002.04232v2) - [pdf](http://arxiv.org/pdf/2002.04232v2)

> We study the problem of efficiently estimating the effect of an intervention on a single variable (atomic interventions) using observational samples in a causal Bayesian network. Our goal is to give algorithms that are efficient in both time and sample complexity in a non-parametric setting.   Tian and Pearl (AAAI `02) have exactly characterized the class of causal graphs for which causal effects of atomic interventions can be identified from observational data. We make their result quantitative. Suppose P is a causal model on a set $\vec{V}$ of n observable variables with respect to a given causal graph G with observable distribution $P$. Let $P_x$ denote the interventional distribution over the observables with respect to an intervention of a designated variable X with x. Assuming that $G$ has bounded in-degree, bounded c-components ($k$), and that the observational distribution is identifiable and satisfies certain strong positivity condition, we give an algorithm that takes $m=\tilde{O}(n\epsilon^{-2})$ samples from $P$ and $O(mn)$ time, and outputs with high probability a description of a distribution $\hat{P}$ such that $d_{\mathrm{TV}}(P_x, \hat{P}) \leq \epsilon$, and:   1. [Evaluation] the description can return in $O(n)$ time the probability $\hat{P}(\vec{v})$ for any assignment $\vec{v}$ to $\vec{V}$   2. [Generation] the description can return an iid sample from $\hat{P}$ in $O(n)$ time.   We also show lower bounds for the sample complexity showing that our sample complexity has an optimal dependence on the parameters $n$ and $\epsilon$, as well as if $k=1$ on the strong positivity parameter.

</details>

<details>

<summary>2020-08-05 16:07:22 - Bayesian Survival Analysis Using Gamma Processes with Adaptive Time Partition</summary>

- *Yi Li, Sumi Seo, Kyu Ha Lee*

- `2008.02204v1` - [abs](http://arxiv.org/abs/2008.02204v1) - [pdf](http://arxiv.org/pdf/2008.02204v1)

> In Bayesian semi-parametric analyses of time-to-event data, non-parametric process priors are adopted for the baseline hazard function or the cumulative baseline hazard function for a given finite partition of the time axis. However, it would be controversial to suggest a general guideline to construct an optimal time partition. While a great deal of research has been done to relax the assumption of the fixed split times for other non-parametric processes, to our knowledge, no methods have been developed for a gamma process prior, which is one of the most widely used in Bayesian survival analysis. In this paper, we propose a new Bayesian framework for proportional hazards models where the cumulative baseline hazard function is modeled a priori by a gamma process. A key feature of the proposed framework is that the number and position of interval cutpoints are treated as random and estimated based on their posterior distributions.

</details>

<details>

<summary>2020-08-05 19:02:14 - Spatiotemporal dynamic of COVID-19 mortality in the city of Sao Paulo, Brazil: shifting the high risk from the best to the worst socio-economic conditions</summary>

- *Patricia Marques Moralejo Bermudi, Camila Lorenz, Breno Souza de Aguiar, Marcelo Antunes Failla, Ligia Vizeu Barrozo, Francisco Chiaravalloti-Neto*

- `2008.02322v1` - [abs](http://arxiv.org/abs/2008.02322v1) - [pdf](http://arxiv.org/pdf/2008.02322v1)

> Currently, Brazil has one of the fastest increasing COVID-19 epidemics in the world, that has caused at least 94 thousand confirmed deaths until now. The city of Sao Paulo is particularly vulnerable because it is the most populous in the country. Analyzing the spatiotemporal dynamics of COVID-19 is important to help the urgent need to integrate better actions to face the pandemic. Thus, this study aimed to analyze the COVID-19 mortality, from March to July 2020, considering the spatio-time architectures, the socio-economic context of the population, and using a fine granular level, in the most populous city in Brazil. For this, we conducted an ecological study, using secondary public data from the mortality information system. We describe mortality rates for each epidemiological week and the entire period by sex and age. We modelled the deaths using spatiotemporal and spatial architectures and Poisson probability distributions in a latent Gaussian Bayesian model approach. We obtained the relative risks for temporal and spatiotemporal trends and socio-economic conditions. To reduce possible sub notification, we considered the confirmed and suspected deaths. Our findings showed an apparent stabilization of the temporal trend, at the end of the period, but that may change in the future. Mortality rate increased with increasing age and was higher in men. The risk of death was greater in areas with the worst social conditions throughout the study period. However, this was not a uniform pattern over time, since we identified a shift from the high risk in the areas with best socio-economic conditions to the worst ones. Our study contributed by emphasizing the importance of geographic screening in areas with a higher risk of death, and, currently, worse socio-economic contexts, as a crucial aspect to reducing disease mortality and health inequities, through integrated public health actions.

</details>

<details>

<summary>2020-08-05 19:29:35 - Bayesian Optimization with Machine Learning Algorithms Towards Anomaly Detection</summary>

- *MohammadNoor Injadat, Fadi Salo, Ali Bou Nassif, Aleksander Essex, Abdallah Shami*

- `2008.02327v1` - [abs](http://arxiv.org/abs/2008.02327v1) - [pdf](http://arxiv.org/pdf/2008.02327v1)

> Network attacks have been very prevalent as their rate is growing tremendously. Both organization and individuals are now concerned about their confidentiality, integrity and availability of their critical information which are often impacted by network attacks. To that end, several previous machine learning-based intrusion detection methods have been developed to secure network infrastructure from such attacks. In this paper, an effective anomaly detection framework is proposed utilizing Bayesian Optimization technique to tune the parameters of Support Vector Machine with Gaussian Kernel (SVM-RBF), Random Forest (RF), and k-Nearest Neighbor (k-NN) algorithms. The performance of the considered algorithms is evaluated using the ISCX 2012 dataset. Experimental results show the effectiveness of the proposed framework in term of accuracy rate, precision, low-false alarm rate, and recall.

</details>

<details>

<summary>2020-08-05 20:11:31 - Bayesian Set of Best Dynamic Treatment Regimes and Sample Size Determination for SMARTs with Binary Outcomes</summary>

- *William J. Artman, Ashkan Ertefaie, Kevin G. Lynch, James R. McKay*

- `2008.02341v1` - [abs](http://arxiv.org/abs/2008.02341v1) - [pdf](http://arxiv.org/pdf/2008.02341v1)

> One of the main goals of sequential, multiple assignment, randomized trials (SMART) is to find the most efficacious design embedded dynamic treatment regimes. The analysis method known as multiple comparisons with the best (MCB) allows comparison between dynamic treatment regimes and identification of a set of optimal regimes in the frequentist setting for continuous outcomes, thereby, directly addressing the main goal of a SMART. In this paper, we develop a Bayesian generalization to MCB for SMARTs with binary outcomes. Furthermore, we show how to choose the sample size so that the inferior embedded DTRs are screened out with a specified power. We compare log-odds between different DTRs using their exact distribution without relying on asymptotic normality in either the analysis or the power calculation. We conduct extensive simulation studies under two SMART designs and illustrate our method's application to the Adaptive Treatment for Alcohol and Cocaine Dependence (ENGAGE) trial.

</details>

<details>

<summary>2020-08-05 22:18:02 - Thompson Sampling via Local Uncertainty</summary>

- *Zhendong Wang, Mingyuan Zhou*

- `1910.13673v3` - [abs](http://arxiv.org/abs/1910.13673v3) - [pdf](http://arxiv.org/pdf/1910.13673v3)

> Thompson sampling is an efficient algorithm for sequential decision making, which exploits the posterior uncertainty to address the exploration-exploitation dilemma. There has been significant recent interest in integrating Bayesian neural networks into Thompson sampling. Most of these methods rely on global variable uncertainty for exploration. In this paper, we propose a new probabilistic modeling framework for Thompson sampling, where local latent variable uncertainty is used to sample the mean reward. Variational inference is used to approximate the posterior of the local variable, and semi-implicit structure is further introduced to enhance its expressiveness. Our experimental results on eight contextual bandit benchmark datasets show that Thompson sampling guided by local uncertainty achieves state-of-the-art performance while having low computational complexity.

</details>

<details>

<summary>2020-08-05 22:28:53 - Bayesian learning of orthogonal embeddings for multi-fidelity Gaussian Processes</summary>

- *Panagiotis Tsilifis, Piyush Pandita, Sayan Ghosh, Valeria Andreoli, Thomas Vandeputte, Liping Wang*

- `2008.02386v1` - [abs](http://arxiv.org/abs/2008.02386v1) - [pdf](http://arxiv.org/pdf/2008.02386v1)

> We present a Bayesian approach to identify optimal transformations that map model input points to low dimensional latent variables. The "projection" mapping consists of an orthonormal matrix that is considered a priori unknown and needs to be inferred jointly with the GP parameters, conditioned on the available training data. The proposed Bayesian inference scheme relies on a two-step iterative algorithm that samples from the marginal posteriors of the GP parameters and the projection matrix respectively, both using Markov Chain Monte Carlo (MCMC) sampling. In order to take into account the orthogonality constraints imposed on the orthonormal projection matrix, a Geodesic Monte Carlo sampling algorithm is employed, that is suitable for exploiting probability measures on manifolds. We extend the proposed framework to multi-fidelity models using GPs including the scenarios of training multiple outputs together. We validate our framework on three synthetic problems with a known lower-dimensional subspace. The benefits of our proposed framework, are illustrated on the computationally challenging three-dimensional aerodynamic optimization of a last-stage blade for an industrial gas turbine, where we study the effect of an 85-dimensional airfoil shape parameterization on two output quantities of interest, specifically on the aerodynamic efficiency and the degree of reaction.

</details>

<details>

<summary>2020-08-06 05:56:51 - Markov Chain Importance Sampling -- a highly efficient estimator for MCMC</summary>

- *Ingmar Schuster, Ilja Klebanov*

- `1805.07179v4` - [abs](http://arxiv.org/abs/1805.07179v4) - [pdf](http://arxiv.org/pdf/1805.07179v4)

> Markov chain (MC) algorithms are ubiquitous in machine learning and statistics and many other disciplines. Typically, these algorithms can be formulated as acceptance rejection methods. In this work we present a novel estimator applicable to these methods, dubbed Markov chain importance sampling (MCIS), which efficiently makes use of rejected proposals. For the unadjusted Langevin algorithm, it provides a novel way of correcting the discretization error. Our estimator satisfies a central limit theorem and improves on error per CPU cycle, often to a large extent. As a by-product it enables estimating the normalizing constant, an important quantity in Bayesian machine learning and statistics.

</details>

<details>

<summary>2020-08-06 07:43:41 - Bayesian Indirect Inference for Models with Intractable Normalizing Functions</summary>

- *Jaewoo Park*

- `2008.02497v1` - [abs](http://arxiv.org/abs/2008.02497v1) - [pdf](http://arxiv.org/pdf/2008.02497v1)

> Inference for doubly intractable distributions is challenging because the intractable normalizing functions of these models include parameters of interest. Previous auxiliary variable MCMC algorithms are infeasible for multi-dimensional models with large data sets because they depend on expensive auxiliary variable simulation at each iteration. We develop a fast Bayesian indirect algorithm by replacing an expensive auxiliary variable simulation from a probability model with a computationally cheap simulation from a surrogate model. We learn the relationship between the surrogate model parameters and the probability model parameters using Gaussian process approximations. We apply our methods to challenging simulated and real data examples, and illustrate that the algorithm addresses both computational and inferential challenges for doubly intractable distributions. Especially for a large social network model with 10 parameters, we show that our method can reduce computing time from about 2 weeks to 5 hours, compared to the previous method. Our method allows practitioners to carry out Bayesian inference for more complex models with larger data sets than before.

</details>

<details>

<summary>2020-08-06 12:46:52 - Batch simulations and uncertainty quantification in Gaussian process surrogate approximate Bayesian computation</summary>

- *Marko Järvenpää, Aki Vehtari, Pekka Marttinen*

- `1910.06121v3` - [abs](http://arxiv.org/abs/1910.06121v3) - [pdf](http://arxiv.org/pdf/1910.06121v3)

> The computational efficiency of approximate Bayesian computation (ABC) has been improved by using surrogate models such as Gaussian processes (GP). In one such promising framework the discrepancy between the simulated and observed data is modelled with a GP which is further used to form a model-based estimator for the intractable posterior. In this article we improve this approach in several ways. We develop batch-sequential Bayesian experimental design strategies to parallellise the expensive simulations. In earlier work only sequential strategies have been used. Current surrogate-based ABC methods also do not fully account the uncertainty due to the limited budget of simulations as they output only a point estimate of the ABC posterior. We propose a numerical method to fully quantify the uncertainty in, for example, ABC posterior moments. We also provide some new analysis on the GP modelling assumptions in the resulting improved framework called Bayesian ABC and discuss its connection to Bayesian quadrature (BQ) and Bayesian optimisation (BO). Experiments with toy and real-world simulation models demonstrate advantages of the proposed techniques.

</details>

<details>

<summary>2020-08-06 21:31:32 - Optimal design of experiments to identify latent behavioral types</summary>

- *Stefano Balietti, Brennan Klein, Christoph Riedl*

- `1807.07024v3` - [abs](http://arxiv.org/abs/1807.07024v3) - [pdf](http://arxiv.org/pdf/1807.07024v3)

> Bayesian optimal experiments that maximize the information gained from collected data are critical to efficiently identify behavioral models. We extend a seminal method for designing Bayesian optimal experiments by introducing two computational improvements that make the procedure tractable: (1) a search algorithm from artificial intelligence that efficiently explores the space of possible design parameters, and (2) a sampling procedure which evaluates each design parameter combination more efficiently. We apply our procedure to a game of imperfect information to evaluate and quantify the computational improvements. We then collect data across five different experimental designs to compare the ability of the optimal experimental design to discriminate among competing behavioral models against the experimental designs chosen by a "wisdom of experts" prediction experiment. We find that data from the experiment suggested by the optimal design approach requires significantly less data to distinguish behavioral models (i.e., test hypotheses) than data from the experiment suggested by experts. Substantively, we find that reinforcement learning best explains human decision-making in the imperfect information game and that behavior is not adequately described by the Bayesian Nash equilibrium. Our procedure is general and computationally efficient and can be applied to dynamically optimize online experiments.

</details>

<details>

<summary>2020-08-07 01:22:17 - A Note on Using Discretized Simulated Data to Estimate Implicit Likelihoods in Bayesian Analyses</summary>

- *M. S. Hamada, T. L. Graves, N. W. Hengartner, D. M. Higdon, A. V. Huzurbazar, E. C. Lawrence, C. D. Linkletter, C. S. Reese, D. W. Scott, R. R. Sitter, R. L. Warr, B. J. Williams*

- `2008.02926v1` - [abs](http://arxiv.org/abs/2008.02926v1) - [pdf](http://arxiv.org/pdf/2008.02926v1)

> This article presents a Bayesian inferential method where the likelihood for a model is unknown but where data can easily be simulated from the model. We discretize simulated (continuous) data to estimate the implicit likelihood in a Bayesian analysis employing a Markov chain Monte Carlo algorithm. Three examples are presented as well as a small study on some of the method's properties.

</details>

<details>

<summary>2020-08-07 06:05:54 - Robust Approximate Bayesian Computation: An Adjustment Approach</summary>

- *David T. Frazier, Christopher Drovandi, Ruben Loaiza-Maya*

- `2008.04099v1` - [abs](http://arxiv.org/abs/2008.04099v1) - [pdf](http://arxiv.org/pdf/2008.04099v1)

> We propose a novel approach to approximate Bayesian computation (ABC) that seeks to cater for possible misspecification of the assumed model. This new approach can be equally applied to rejection-based ABC and to popular regression adjustment ABC. We demonstrate that this new approach mitigates the poor performance of regression adjusted ABC that can eventuate when the model is misspecified. In addition, this new adjustment approach allows us to detect which features of the observed data can not be reliably reproduced by the assumed model. A series of simulated and empirical examples illustrate this new approach.

</details>

<details>

<summary>2020-08-07 12:55:52 - BAT.jl -- A Julia-based tool for Bayesian inference</summary>

- *Oliver Schulz, Frederik Beaujean, Allen Caldwell, Cornelius Grunwald, Vasyl Hafych, Kevin Kröninger, Salvatore La Cagnina, Lars Röhrig, Lolian Shtembari*

- `2008.03132v1` - [abs](http://arxiv.org/abs/2008.03132v1) - [pdf](http://arxiv.org/pdf/2008.03132v1)

> We describe the development of a multi-purpose software for Bayesian statistical inference, BAT.jl, written in the Julia language. The major design considerations and implemented algorithms are summarized here, together with a test suite that ensures the proper functioning of the algorithms. We also give an extended example from the realm of physics that demonstrates the functionalities of BAT.jl.

</details>

<details>

<summary>2020-08-07 17:11:04 - Bayesian causal inference for count potential outcomes</summary>

- *Young Lee, Wicher P. Bergsma, Marie-Abele C. Bind*

- `2008.03271v1` - [abs](http://arxiv.org/abs/2008.03271v1) - [pdf](http://arxiv.org/pdf/2008.03271v1)

> The literature for count modeling provides useful tools to conduct causal inference when outcomes take non-negative integer values. Applied to the potential outcomes framework, we link the Bayesian causal inference literature to statistical models for count data. We discuss the general architectural considerations for constructing the predictive posterior of the missing potential outcomes. Special considerations for estimating average treatment effects are discussed, some generalizing certain relationships and some not yet encountered in the causal inference literature.

</details>

<details>

<summary>2020-08-07 18:00:02 - Complete parameter inference for GW150914 using deep learning</summary>

- *Stephen R. Green, Jonathan Gair*

- `2008.03312v1` - [abs](http://arxiv.org/abs/2008.03312v1) - [pdf](http://arxiv.org/pdf/2008.03312v1)

> The LIGO and Virgo gravitational-wave observatories have detected many exciting events over the past five years. As the rate of detections grows with detector sensitivity, this poses a growing computational challenge for data analysis. With this in mind, in this work we apply deep learning techniques to perform fast likelihood-free Bayesian inference for gravitational waves. We train a neural-network conditional density estimator to model posterior probability distributions over the full 15-dimensional space of binary black hole system parameters, given detector strain data from multiple detectors. We use the method of normalizing flows---specifically, a neural spline normalizing flow---which allows for rapid sampling and density estimation. Training the network is likelihood-free, requiring samples from the data generative process, but no likelihood evaluations. Through training, the network learns a global set of posteriors: it can generate thousands of independent posterior samples per second for any strain data consistent with the prior and detector noise characteristics used for training. By training with the detector noise power spectral density estimated at the time of GW150914, and conditioning on the event strain data, we use the neural network to generate accurate posterior samples consistent with analyses using conventional sampling techniques.

</details>

<details>

<summary>2020-08-08 06:40:05 - On posterior concentration rates for Bayesian quantile regression based on the misspecified asymmetric Laplace likelihood</summary>

- *Karthik Sriram, R. V. Ramamoorthi*

- `1812.03652v3` - [abs](http://arxiv.org/abs/1812.03652v3) - [pdf](http://arxiv.org/pdf/1812.03652v3)

> The asymmetric Laplace density (ALD) is used as a working likelihood for Bayesian quantile regression. Sriram et al.(2013) derived posterior consistency for Bayesian linear quantile regression based on the misspecified ALD. While their paper also argued for $\sqrt{n}-$consistency, Sriram and Ramamoorthi (2017) highlighted that the argument was only valid for a rate less than $\sqrt{n}$. So, the question of $\sqrt{n}-$rate has remained unaddressed, but is necessary to carry out meaningful Bayesian inference based on the ALD. In this paper, we derive results to obtain posterior consistency rates for Bayesian quantile regression based on the misspecified ALD. We derive our results in a slightly general setting where the quantile function can possibly be non-linear. In particular, we give sufficient conditions for $\sqrt{n}-$consistency for the Bayesian linear quantile regression using ALD. We also provide examples of Bayesian non-linear quantile regression.

</details>

<details>

<summary>2020-08-08 22:32:29 - Bayesian-Assisted Inference from Visualized Data</summary>

- *Yea-Seul Kim, Paula Kayongo, Madeleine Grunde-McLaughlin, Jessica Hullman*

- `2008.00142v2` - [abs](http://arxiv.org/abs/2008.00142v2) - [pdf](http://arxiv.org/pdf/2008.00142v2)

> A Bayesian view of data interpretation suggests that a visualization user should update their existing beliefs about a parameter's value in accordance with the amount of information about the parameter value captured by the new observations. Extending recent work applying Bayesian models to understand and evaluate belief updating from visualizations, we show how the predictions of Bayesian inference can be used to guide more rational belief updating. We design a Bayesian inference-assisted uncertainty analogy that numerically relates uncertainty in observed data to the user's subjective uncertainty, and a posterior visualization that prescribes how a user should update their beliefs given their prior beliefs and the observed data. In a pre-registered experiment on 4,800 people, we find that when a newly observed data sample is relatively small (N=158), both techniques reliably improve people's Bayesian updating on average compared to the current best practice of visualizing uncertainty in the observed data. For large data samples (N=5208), where people's updated beliefs tend to deviate more strongly from the prescriptions of a Bayesian model, we find evidence that the effectiveness of the two forms of Bayesian assistance may depend on people's proclivity toward trusting the source of the data. We discuss how our results provide insight into individual processes of belief updating and subjective uncertainty, and how understanding these aspects of interpretation paves the way for more sophisticated interactive visualizations for analysis and communication.

</details>

<details>

<summary>2020-08-09 07:10:30 - On Scalable Particle Markov Chain Monte Carlo</summary>

- *David Gunawan, Chris Carter, Robert Kohn*

- `1804.04359v3` - [abs](http://arxiv.org/abs/1804.04359v3) - [pdf](http://arxiv.org/pdf/1804.04359v3)

> Particle Markov Chain Monte Carlo (PMCMC) is a general approach to carry out Bayesian inference in non-linear and non-Gaussian state space models. Our article shows how to scale up PMCMC in terms of the number of observations and parameters by expressing the target density of the PMCMC in terms of the basic uniform or standard normal random numbers, instead of the particles, used in the sequential Monte Carlo algorithm. Parameters that can be drawn efficiently conditional on the particles are generated by particle Gibbs. All the other parameters are drawn by conditioning on the basic uniform or standard normal random variables; e.g. parameters that are highly correlated with the states, or parameters whose generation is expensive when conditioning on the states. The performance of this hybrid sampler is investigated empirically by applying it to univariate and multivariate stochastic volatility models having both a large number of parameters and a large number of latent states and shows that it is much more efficient than competing PMCMC methods. We also show that the proposed hybrid sampler is ergodic.

</details>

<details>

<summary>2020-08-09 17:16:46 - Bayesian Optimisation over Multiple Continuous and Categorical Inputs</summary>

- *Binxin Ru, Ahsan S. Alvi, Vu Nguyen, Michael A. Osborne, Stephen J Roberts*

- `1906.08878v2` - [abs](http://arxiv.org/abs/1906.08878v2) - [pdf](http://arxiv.org/pdf/1906.08878v2)

> Efficient optimisation of black-box problems that comprise both continuous and categorical inputs is important, yet poses significant challenges. We propose a new approach, Continuous and Categorical Bayesian Optimisation (CoCaBO), which combines the strengths of multi-armed bandits and Bayesian optimisation to select values for both categorical and continuous inputs. We model this mixed-type space using a Gaussian Process kernel, designed to allow sharing of information across multiple categorical variables, each with multiple possible values; this allows CoCaBO to leverage all available data efficiently. We extend our method to the batch setting and propose an efficient selection procedure that dynamically balances exploration and exploitation whilst encouraging batch diversity. We demonstrate empirically that our method outperforms existing approaches on both synthetic and real-world optimisation tasks with continuous and categorical inputs.

</details>

<details>

<summary>2020-08-10 08:34:20 - Automatic Remaining Useful Life Estimation Framework with Embedded Convolutional LSTM as the Backbone</summary>

- *Yexu Zhou, Yuting Gao, Yiran Huang, Michael Hefenbrock, Till Riedel, Michael Beigl*

- `2008.03961v1` - [abs](http://arxiv.org/abs/2008.03961v1) - [pdf](http://arxiv.org/pdf/2008.03961v1)

> An essential task in predictive maintenance is the prediction of the Remaining Useful Life (RUL) through the analysis of multivariate time series. Using the sliding window method, Convolutional Neural Network (CNN) and conventional Recurrent Neural Network (RNN) approaches have produced impressive results on this matter, due to their ability to learn optimized features. However, sequence information is only partially modeled by CNN approaches. Due to the flatten mechanism in conventional RNNs, like Long Short Term Memories (LSTM), the temporal information within the window is not fully preserved. To exploit the multi-level temporal information, many approaches are proposed which combine CNN and RNN models. In this work, we propose a new LSTM variant called embedded convolutional LSTM (ECLSTM). In ECLSTM a group of different 1D convolutions is embedded into the LSTM structure. Through this, the temporal information is preserved between and within windows. Since the hyper-parameters of models require careful tuning, we also propose an automated prediction framework based on the Bayesian optimization with hyperband optimizer, which allows for efficient optimization of the network architecture. Finally, we show the superiority of our proposed ECLSTM approach over the state-of-the-art approaches on several widely used benchmark data sets for RUL Estimation.

</details>

<details>

<summary>2020-08-10 09:13:57 - When words collide: Bayesian meta-analyses of distractor and target properties in the picture-word interference paradigm</summary>

- *Audrey Bürki, F. -Xavier Alario, Shravan Vasishth*

- `2008.03972v1` - [abs](http://arxiv.org/abs/2008.03972v1) - [pdf](http://arxiv.org/pdf/2008.03972v1)

> In the picture-word interference paradigm, participants name pictures while ignoring a written or spoken distractor word. Naming times to the pictures are slowed down by the presence of the distractor word. Various properties of the distractor modulate this slow down, for example naming times are shorter with frequent vs. infrequent distractors. Building on this line of research, the present study investigates in more detail the impact of distractor and target word properties on picture naming times. We report the results of several Bayesian meta-analyses, based on 35 datasets. The aim of the first analysis was to obtain an estimation of the size of the distractor frequency effect, and of its precision, in typical picture-word interference experiments where this variable is not manipulated. The analysis shows that a one-unit increase in log frequency results in response times to the pictures decreasing by about 4ms (95% Credible Interval: [-6, -2]). With the second and third analyses, we show that after accounting for the effect of frequency, two variables known to influence processing times in visual word processing tasks also influence picture naming times: distractor length and orthographic neighborhood. Finally, we found that distractor word frequency and target word frequency interact; the effect of distractor frequency decreases as the frequency of the target word increases. We discuss the theoretical and methodological implications of these findings, as well as the importance of obtaining high-precision estimates of experimental effects.

</details>

<details>

<summary>2020-08-10 16:56:48 - Purely Bayesian counterfactuals versus Newcomb's paradox</summary>

- *Lê Nguyên Hoang*

- `2008.04256v1` - [abs](http://arxiv.org/abs/2008.04256v1) - [pdf](http://arxiv.org/pdf/2008.04256v1)

> This paper proposes a careful separation between an entity's epistemic system and their decision system. Crucially, Bayesian counterfactuals are estimated by the epistemic system; not by the decision system. Based on this remark, I prove the existence of Newcomb-like problems for which an epistemic system necessarily expects the entity to make a counterfactually bad decision. I then address (a slight generalization of) Newcomb's paradox. I solve the specific case where the player believes that the predictor applies Bayes rule with a supset of all the data available to the player. I prove that the counterfactual optimality of the 1-Box strategy depends on the player's prior on the predictor's additional data. If these additional data are not expected to reduce sufficiently the predictor's uncertainty on the player's decision, then the player's epistemic system will counterfactually prefer to 2-Box. But if the predictor's data is believed to make them quasi-omniscient, then 1-Box will be counterfactually preferred. Implications of the analysis are then discussed. More generally, I argue that, to better understand or design an entity, it is useful to clearly separate the entity's epistemic, decision, but also data collection, reward and maintenance systems, whether the entity is human, algorithmic or institutional.

</details>

<details>

<summary>2020-08-11 06:20:43 - Evidence bounds in singular models: probabilistic and variational perspectives</summary>

- *Anirban Bhattacharya, Debdeep Pati, Sean Plummer*

- `2008.04537v1` - [abs](http://arxiv.org/abs/2008.04537v1) - [pdf](http://arxiv.org/pdf/2008.04537v1)

> The marginal likelihood or evidence in Bayesian statistics contains an intrinsic penalty for larger model sizes and is a fundamental quantity in Bayesian model comparison. Over the past two decades, there has been steadily increasing activity to understand the nature of this penalty in singular statistical models, building on pioneering work by Sumio Watanabe. Unlike regular models where the Bayesian information criterion (BIC) encapsulates a first-order expansion of the logarithm of the marginal likelihood, parameter counting gets trickier in singular models where a quantity called the real log canonical threshold (RLCT) summarizes the effective model dimensionality. In this article, we offer a probabilistic treatment to recover non-asymptotic versions of established evidence bounds as well as prove a new result based on the Gibbs variational inequality. In particular, we show that mean-field variational inference correctly recovers the RLCT for any singular model in its canonical or normal form. We additionally exhibit sharpness of our bound by analyzing the dynamics of a general purpose coordinate ascent algorithm (CAVI) popularly employed in variational inference.

</details>

<details>

<summary>2020-08-11 08:49:41 - Particle Gibbs Sampling for Bayesian Phylogenetic inference</summary>

- *Shijia Wang, Liangliang Wang*

- `2004.06363v2` - [abs](http://arxiv.org/abs/2004.06363v2) - [pdf](http://arxiv.org/pdf/2004.06363v2)

> The combinatorial sequential Monte Carlo (CSMC) has been demonstrated to be an efficient complementary method to the standard Markov chain Monte Carlo (MCMC) for Bayesian phylogenetic tree inference using biological sequences. It is appealing to combine the CSMC and MCMC in the framework of the particle Gibbs (PG) sampler to jointly estimate the phylogenetic trees and evolutionary parameters. However, the Markov chain of the particle Gibbs may mix poorly if the underlying SMC suffers from the path degeneracy issue. Some remedies, including the particle Gibbs with ancestor sampling and the interacting particle MCMC, have been proposed to improve the PG. But they either cannot be applied to or remain inefficient for the combinatorial tree space. We introduce a novel CSMC method by proposing a more efficient proposal distribution. It also can be combined into the particle Gibbs sampler framework to infer parameters in the evolutionary model. The new algorithm can be easily parallelized by allocating samples over different computing cores. We validate that the developed CSMC can sample trees more efficiently in various particle Gibbs samplers via numerical experiments. Our implementation is available at https://github.com/liangliangwangsfu/phyloPMCMC

</details>

<details>

<summary>2020-08-11 15:54:39 - The Boomerang Sampler</summary>

- *Joris Bierkens, Sebastiano Grazzi, Kengo Kamatani, Gareth Roberts*

- `2006.13777v2` - [abs](http://arxiv.org/abs/2006.13777v2) - [pdf](http://arxiv.org/pdf/2006.13777v2)

> This paper introduces the Boomerang Sampler as a novel class of continuous-time non-reversible Markov chain Monte Carlo algorithms. The methodology begins by representing the target density as a density, $e^{-U}$, with respect to a prescribed (usually) Gaussian measure and constructs a continuous trajectory consisting of a piecewise elliptical path. The method moves from one elliptical orbit to another according to a rate function which can be written in terms of $U$. We demonstrate that the method is easy to implement and demonstrate empirically that it can out-perform existing benchmark piecewise deterministic Markov processes such as the bouncy particle sampler and the Zig-Zag. In the Bayesian statistics context, these competitor algorithms are of substantial interest in the large data context due to the fact that they can adopt data subsampling techniques which are exact (ie induce no error in the stationary distribution). We demonstrate theoretically and empirically that we can also construct a control-variate subsampling boomerang sampler which is also exact, and which possesses remarkable scaling properties in the large data limit. We furthermore illustrate a factorised version on the simulation of diffusion bridges.

</details>

<details>

<summary>2020-08-11 19:32:41 - An Initial Exploration of Bayesian Model Calibration for Estimating the Composition of Rocks and Soils on Mars</summary>

- *Claire-Alice Hébert, Earl Lawrence, Kary Myers, James P. Colgan, Elizabeth J. Judge*

- `2008.04982v1` - [abs](http://arxiv.org/abs/2008.04982v1) - [pdf](http://arxiv.org/pdf/2008.04982v1)

> The Mars Curiosity rover carries an instrument, ChemCam, designed to measure the composition of surface rocks and soil using laser-induced breakdown spectroscopy (LIBS). The measured spectra from this instrument must be analyzed to identify the component elements in the target sample, as well as their relative proportions. This process, which we call disaggregation, is complicated by so-called matrix effects, which describe nonlinear changes in the relative heights of emission lines as an unknown function of composition due to atomic interactions within the LIBS plasma. In this work we explore the use of the plasma physics code ATOMIC, developed at Los Alamos National Laboratory, for the disaggregation task. ATOMIC has recently been used to model LIBS spectra and can robustly reproduce matrix effects from first principles. The ability of ATOMIC to predict LIBS spectra presents an exciting opportunity to perform disaggregation in a manner not yet tried in the LIBS community, namely via Bayesian model calibration. However, using it directly to solve our inverse problem is computationally intractable due to the large parameter space and the computation time required to produce a single output. Therefore we also explore the use of emulators as a fast solution for this analysis. We discuss a proof of concept Gaussian process emulator for disaggregating two-element compounds of sodium and copper. The training and test datasets were simulated with ATOMIC using a Latin hypercube design. After testing the performance of the emulator, we successfully recover the composition of 25 test spectra with Bayesian model calibration.

</details>

<details>

<summary>2020-08-11 22:24:55 - Variational Bayes under Model Misspecification</summary>

- *Yixin Wang, David M. Blei*

- `1905.10859v2` - [abs](http://arxiv.org/abs/1905.10859v2) - [pdf](http://arxiv.org/pdf/1905.10859v2)

> Variational Bayes (VB) is a scalable alternative to Markov chain Monte Carlo (MCMC) for Bayesian posterior inference. Though popular, VB comes with few theoretical guarantees, most of which focus on well-specified models. However, models are rarely well-specified in practice. In this work, we study VB under model misspecification. We prove the VB posterior is asymptotically normal and centers at the value that minimizes the Kullback-Leibler (KL) divergence to the true data-generating distribution. Moreover, the VB posterior mean centers at the same value and is also asymptotically normal. These results generalize the variational Bernstein--von Mises theorem [29] to misspecified models. As a consequence of these results, we find that the model misspecification error dominates the variational approximation error in VB posterior predictive distributions. It explains the widely observed phenomenon that VB achieves comparable predictive accuracy with MCMC even though VB uses an approximating family. As illustrations, we study VB under three forms of model misspecification, ranging from model over-/under-dispersion to latent dimensionality misspecification. We conduct two simulation studies that demonstrate the theoretical results.

</details>

<details>

<summary>2020-08-12 01:08:08 - Predictive and Causal Implications of using Shapley Value for Model Interpretation</summary>

- *Sisi Ma, Roshan Tourani*

- `2008.05052v1` - [abs](http://arxiv.org/abs/2008.05052v1) - [pdf](http://arxiv.org/pdf/2008.05052v1)

> Shapley value is a concept from game theory. Recently, it has been used for explaining complex models produced by machine learning techniques. Although the mathematical definition of Shapley value is straight-forward, the implication of using it as a model interpretation tool is yet to be described. In the current paper, we analyzed Shapley value in the Bayesian network framework. We established the relationship between Shapley value and conditional independence, a key concept in both predictive and causal modeling. Our results indicate that, eliminating a variable with high Shapley value from a model do not necessarily impair predictive performance, whereas eliminating a variable with low Shapley value from a model could impair performance. Therefore, using Shapley value for feature selection do not result in the most parsimonious and predictively optimal model in the general case. More importantly, Shapley value of a variable do not reflect their causal relationship with the target of interest.

</details>

<details>

<summary>2020-08-12 04:55:18 - A Bayesian Approach to Spherical Factor Analysis for Binary Data</summary>

- *Xingchen Yu, Abel Rodriguez*

- `2008.05109v1` - [abs](http://arxiv.org/abs/2008.05109v1) - [pdf](http://arxiv.org/pdf/2008.05109v1)

> Factor models are widely used across diverse areas of application for purposes that include dimensionality reduction, covariance estimation, and feature engineering. Traditional factor models can be seen as an instance of linear embedding methods that project multivariate observations onto a lower dimensional Euclidean latent space. This paper discusses a new class of geometric embedding models for multivariate binary data in which the embedding space correspond to a spherical manifold, with potentially unknown dimension. The resulting models include traditional factor models as a special case, but provide additional flexibility. Furthermore, unlike other techniques for geometric embedding, the models are easy to interpret, and the uncertainty associated with the latent features can be properly quantified. These advantages are illustrated using both simulation studies and real data on voting records from the U.S. Senate.

</details>

<details>

<summary>2020-08-12 11:32:38 - Stability of doubly-intractable distributions</summary>

- *Michael Habeck, Daniel Rudolf, Björn Sprungk*

- `2004.07310v3` - [abs](http://arxiv.org/abs/2004.07310v3) - [pdf](http://arxiv.org/pdf/2004.07310v3)

> Doubly-intractable distributions appear naturally as posterior distributions in Bayesian inference frameworks whenever the likelihood contains a normalizing function $Z$. Having two such functions $Z$ and $\widetilde Z$ we provide estimates of the total variation and Wasserstein distance of the resulting posterior probability measures. As a consequence this leads to local Lipschitz continuity w.r.t. $Z$. In the more general framework of a random function $\widetilde Z$ we derive bounds on the expected total variation and expected Wasserstein distance. The applicability of the estimates is illustrated within the setting of two representative Monte Carlo recovery scenarios.

</details>

<details>

<summary>2020-08-12 21:11:53 - A few brief notes on the equivalence of two expressions for statistical significance in point source detections</summary>

- *James Theiler*

- `2008.05574v1` - [abs](http://arxiv.org/abs/2008.05574v1) - [pdf](http://arxiv.org/pdf/2008.05574v1)

> The problem of point source detection in Poisson-limited count maps has been addressed by two recent papers [M. Lampton, ApJ 436, 784 (1994); D. E. Alexandreas, et al., Nucl. Instr. Meth. Phys. Res. A 328, 570 (1993)]. Both papers consider the problem of determining whether there are significantly more counts in a source region than would be expected given the number of counts observed in a background region. The arguments in the two papers are quite different (one takes a Bayesian point of view and the other does not), and the suggested formulas for computing p-values appear to be different as well. It is shown here that the expressions provided by the authors of these two articles are in fact equivalent.

</details>

<details>

<summary>2020-08-13 09:50:17 - Integrating uncertainty in deep neural networks for MRI based stroke analysis</summary>

- *Lisa Herzog, Elvis Murina, Oliver Dürr, Susanne Wegener, Beate Sick*

- `2008.06332v1` - [abs](http://arxiv.org/abs/2008.06332v1) - [pdf](http://arxiv.org/pdf/2008.06332v1)

> At present, the majority of the proposed Deep Learning (DL) methods provide point predictions without quantifying the models uncertainty. However, a quantification of the reliability of automated image analysis is essential, in particular in medicine when physicians rely on the results for making critical treatment decisions. In this work, we provide an entire framework to diagnose ischemic stroke patients incorporating Bayesian uncertainty into the analysis procedure. We present a Bayesian Convolutional Neural Network (CNN) yielding a probability for a stroke lesion on 2D Magnetic Resonance (MR) images with corresponding uncertainty information about the reliability of the prediction. For patient-level diagnoses, different aggregation methods are proposed and evaluated, which combine the single image-level predictions. Those methods take advantage of the uncertainty in image predictions and report model uncertainty at the patient-level. In a cohort of 511 patients, our Bayesian CNN achieved an accuracy of 95.33% at the image-level representing a significant improvement of 2% over a non-Bayesian counterpart. The best patient aggregation method yielded 95.89% of accuracy. Integrating uncertainty information about image predictions in aggregation models resulted in higher uncertainty measures to false patient classifications, which enabled to filter critical patient diagnoses that are supposed to be closer examined by a medical doctor. We therefore recommend using Bayesian approaches not only for improved image-level prediction and uncertainty estimation but also for the detection of uncertain aggregations at the patient-level.

</details>

<details>

<summary>2020-08-13 10:10:00 - Maximum likelihood estimation of regularisation parameters in high-dimensional inverse problems: an empirical Bayesian approach. Part II: Theoretical Analysis</summary>

- *Valentin De Bortoli, Alain Durmus, Ana F. Vidal, Marcelo Pereyra*

- `2008.05793v1` - [abs](http://arxiv.org/abs/2008.05793v1) - [pdf](http://arxiv.org/pdf/2008.05793v1)

> This paper presents a detailed theoretical analysis of the three stochastic approximation proximal gradient algorithms proposed in our companion paper [49] to set regularization parameters by marginal maximum likelihood estimation. We prove the convergence of a more general stochastic approximation scheme that includes the three algorithms of [49] as special cases. This includes asymptotic and non-asymptotic convergence results with natural and easily verifiable conditions, as well as explicit bounds on the convergence rates. Importantly, the theory is also general in that it can be applied to other intractable optimisation problems. A main novelty of the work is that the stochastic gradient estimates of our scheme are constructed from inexact proximal Markov chain Monte Carlo samplers. This allows the use of samplers that scale efficiently to large problems and for which we have precise theoretical guarantees.

</details>

<details>

<summary>2020-08-13 15:32:40 - Flexible Modeling of Hurdle Conway-Maxwell-Poisson Distributions with Application to Mining Injuries</summary>

- *Shuang Yin, Dipak K. Dey, Emiliano A. Valdez, Xiaomeng Li*

- `2008.05968v1` - [abs](http://arxiv.org/abs/2008.05968v1) - [pdf](http://arxiv.org/pdf/2008.05968v1)

> While the hurdle Poisson regression is a popular class of models for count data with excessive zeros, the link function in the binary component may be unsuitable for highly imbalanced cases. Ordinary Poisson regression is unable to handle the presence of dispersion. In this paper, we introduce Conway-Maxwell-Poisson (CMP) distribution and integrate use of flexible skewed Weibull link functions as better alternative. We take a fully Bayesian approach to draw inference from the underlying models to better explain skewness and quantify dispersion, with Deviance Information Criteria (DIC) used for model selection. For empirical investigation, we analyze mining injury data for period 2013-2016 from the U.S. Mine Safety and Health Administration (MSHA). The risk factors describing proportions of employee hours spent in each type of mining work are compositional data; the probabilistic principal components analysis (PPCA) is deployed to deal with such covariates. The hurdle CMP regression is additionally adjusted for exposure, measured by the total employee working hours, to make inference on rate of mining injuries; we tested its competitiveness against other models. This can be used as predictive model in the mining workplace to identify features that increase the risk of injuries so that prevention can be implemented.

</details>

<details>

<summary>2020-08-13 16:49:34 - Investigating and Mitigating Degree-Related Biases in Graph Convolutional Networks</summary>

- *Xianfeng Tang, Huaxiu Yao, Yiwei Sun, Yiqi Wang, Jiliang Tang, Charu Aggarwal, Prasenjit Mitra, Suhang Wang*

- `2006.15643v2` - [abs](http://arxiv.org/abs/2006.15643v2) - [pdf](http://arxiv.org/pdf/2006.15643v2)

> Graph Convolutional Networks (GCNs) show promising results for semi-supervised learning tasks on graphs, thus become favorable comparing with other approaches. Despite the remarkable success of GCNs, it is difficult to train GCNs with insufficient supervision. When labeled data are limited, the performance of GCNs becomes unsatisfying for low-degree nodes. While some prior work analyze successes and failures of GCNs on the entire model level, profiling GCNs on individual node level is still underexplored.   In this paper, we analyze GCNs in regard to the node degree distribution. From empirical observation to theoretical proof, we confirm that GCNs are biased towards nodes with larger degrees with higher accuracy on them, even if high-degree nodes are underrepresented in most graphs. We further develop a novel Self-Supervised-Learning Degree-Specific GCN (SL-DSGC) that mitigate the degree-related biases of GCNs from model and data aspects. Firstly, we propose a degree-specific GCN layer that captures both discrepancies and similarities of nodes with different degrees, which reduces the inner model-aspect biases of GCNs caused by sharing the same parameters with all nodes. Secondly, we design a self-supervised-learning algorithm that creates pseudo labels with uncertainty scores on unlabeled nodes with a Bayesian neural network. Pseudo labels increase the chance of connecting to labeled neighbors for low-degree nodes, thus reducing the biases of GCNs from the data perspective. Uncertainty scores are further exploited to weight pseudo labels dynamically in the stochastic gradient descent for SL-DSGC. Experiments on three benchmark datasets show SL-DSGC not only outperforms state-of-the-art self-training/self-supervised-learning GCN methods, but also improves GCN accuracy dramatically for low-degree nodes.

</details>

<details>

<summary>2020-08-14 03:15:23 - Optimal Posteriors for Chi-squared Divergence based PAC-Bayesian Bounds and Comparison with KL-divergence based Optimal Posteriors and Cross-Validation Procedure</summary>

- *Puja Sahu, Nandyala Hemachandra*

- `2008.07330v1` - [abs](http://arxiv.org/abs/2008.07330v1) - [pdf](http://arxiv.org/pdf/2008.07330v1)

> We investigate optimal posteriors for recently introduced \cite{begin2016pac} chi-squared divergence based PAC-Bayesian bounds in terms of nature of their distribution, scalability of computations, and test set performance. For a finite classifier set, we deduce bounds for three distance functions: KL-divergence, linear and squared distances. Optimal posterior weights are proportional to deviations of empirical risks, usually with subset support. For uniform prior, it is sufficient to search among posteriors on classifier subsets ordered by these risks. We show the bound minimization for linear distance as a convex program and obtain a closed-form expression for its optimal posterior. Whereas that for squared distance is a quasi-convex program under a specific condition, and the one for KL-divergence is non-convex optimization (a difference of convex functions). To compute such optimal posteriors, we derive fast converging fixed point (FP) equations. We apply these approaches to a finite set of SVM regularization parameter values to yield stochastic SVMs with tight bounds. We perform a comprehensive performance comparison between our optimal posteriors and known KL-divergence based posteriors on a variety of UCI datasets with varying ranges and variances in risk values, etc. Chi-squared divergence based posteriors have weaker bounds and worse test errors, hinting at an underlying regularization by KL-divergence based posteriors. Our study highlights the impact of divergence function on the performance of PAC-Bayesian classifiers. We compare our stochastic classifiers with cross-validation based deterministic classifier. The latter has better test errors, but ours is more sample robust, has quantifiable generalization guarantees, and is computationally much faster.

</details>

<details>

<summary>2020-08-14 04:47:03 - Bayesian joint inference for multiple directed acyclic graphs</summary>

- *Kyoungjae Lee, Xuan Cao*

- `2008.06190v1` - [abs](http://arxiv.org/abs/2008.06190v1) - [pdf](http://arxiv.org/pdf/2008.06190v1)

> In many applications, data often arise from multiple groups that may share similar characteristics. A joint estimation method that models several groups simultaneously can be more efficient than estimating parameters in each group separately. We focus on unraveling the dependence structures of data based on directed acyclic graphs and propose a Bayesian joint inference method for multiple graphs. To encourage similar dependence structures across all groups, a Markov random field prior is adopted. We establish the joint selection consistency of the fractional posterior in high dimensions, and benefits of the joint inference are shown under the common support assumption. This is the first Bayesian method for joint estimation of multiple directed acyclic graphs. The performance of the proposed method is demonstrated using simulation studies, and it is shown that our joint inference outperforms other competitors. We apply our method to an fMRI data for simultaneously inferring multiple brain functional networks.

</details>

<details>

<summary>2020-08-14 10:37:37 - Continual Learning Using Bayesian Neural Networks</summary>

- *HongLin Li, Payam Barnaghi, Shirin Enshaeifar, Frieder Ganz*

- `1910.04112v2` - [abs](http://arxiv.org/abs/1910.04112v2) - [pdf](http://arxiv.org/pdf/1910.04112v2)

> Continual learning models allow to learn and adapt to new changes and tasks over time. However, in continual and sequential learning scenarios in which the models are trained using different data with various distributions, neural networks tend to forget the previously learned knowledge. This phenomenon is often referred to as catastrophic forgetting. The catastrophic forgetting is an inevitable problem in continual learning models for dynamic environments. To address this issue, we propose a method, called Continual Bayesian Learning Networks (CBLN), which enables the networks to allocate additional resources to adapt to new tasks without forgetting the previously learned tasks. Using a Bayesian Neural Network, CBLN maintains a mixture of Gaussian posterior distributions that are associated with different tasks. The proposed method tries to optimise the number of resources that are needed to learn each task and avoids an exponential increase in the number of resources that are involved in learning multiple tasks. The proposed method does not need to access the past training data and can choose suitable weights to classify the data points during the test time automatically based on an uncertainty criterion. We have evaluated our method on the MNIST and UCR time-series datasets. The evaluation results show that our method can address the catastrophic forgetting problem at a promising rate compared to the state-of-the-art models.

</details>

<details>

<summary>2020-08-14 10:44:18 - Maximum likelihood estimation of regularisation parameters in high-dimensional inverse problems: an empirical Bayesian approach. Part I: Methodology and Experiments</summary>

- *Ana F. Vidal, Valentin De Bortoli, Marcelo Pereyra, Alain Durmus*

- `1911.11709v3` - [abs](http://arxiv.org/abs/1911.11709v3) - [pdf](http://arxiv.org/pdf/1911.11709v3)

> Many imaging problems require solving an inverse problem that is ill-conditioned or ill-posed. Imaging methods typically address this difficulty by regularising the estimation problem to make it well-posed. This often requires setting the value of the so-called regularisation parameters that control the amount of regularisation enforced. These parameters are notoriously difficult to set a priori, and can have a dramatic impact on the recovered estimates. In this work, we propose a general empirical Bayesian method for setting regularisation parameters in imaging problems that are convex w.r.t. the unknown image. Our method calibrates regularisation parameters directly from the observed data by maximum marginal likelihood estimation, and can simultaneously estimate multiple regularisation parameters. Furthermore, the proposed algorithm uses the same basic operators as proximal optimisation algorithms, namely gradient and proximal operators, and it is therefore straightforward to apply to problems that are currently solved by using proximal optimisation techniques. Our methodology is demonstrated with a range of experiments and comparisons with alternative approaches from the literature. The considered experiments include image denoising, non-blind image deconvolution, and hyperspectral unmixing, using synthesis and analysis priors involving the L1, total-variation, total-variation and L1, and total-generalised-variation pseudo-norms. A detailed theoretical analysis of the proposed method is presented in the companion paper arXiv:2008.05793.

</details>

<details>

<summary>2020-08-14 12:10:55 - Projective Preferential Bayesian Optimization</summary>

- *Petrus Mikkola, Milica Todorović, Jari Järvi, Patrick Rinke, Samuel Kaski*

- `2002.03113v4` - [abs](http://arxiv.org/abs/2002.03113v4) - [pdf](http://arxiv.org/pdf/2002.03113v4)

> Bayesian optimization is an effective method for finding extrema of a black-box function. We propose a new type of Bayesian optimization for learning user preferences in high-dimensional spaces. The central assumption is that the underlying objective function cannot be evaluated directly, but instead a minimizer along a projection can be queried, which we call a projective preferential query. The form of the query allows for feedback that is natural for a human to give, and which enables interaction. This is demonstrated in a user experiment in which the user feedback comes in the form of optimal position and orientation of a molecule adsorbing to a surface. We demonstrate that our framework is able to find a global minimum of a high-dimensional black-box function, which is an infeasible task for existing preferential Bayesian optimization frameworks that are based on pairwise comparisons.

</details>

<details>

<summary>2020-08-14 13:35:26 - Bayesian Sparse Mediation Analysis with Targeted Penalization of Natural Indirect Effects</summary>

- *Yanyi Song, Xiang Zhou, Jian Kang, Max T. Aung, Min Zhang, Wei Zhao, Belinda L. Needham, Sharon L. R. Kardia, Yongmei Liu, John D. Meeker, Jennifer A. Smith, Bhramar Mukherjee*

- `2008.06366v1` - [abs](http://arxiv.org/abs/2008.06366v1) - [pdf](http://arxiv.org/pdf/2008.06366v1)

> Causal mediation analysis aims to characterize an exposure's effect on an outcome and quantify the indirect effect that acts through a given mediator or a group of mediators of interest. With the increasing availability of measurements on a large number of potential mediators, like the epigenome or the microbiome, new statistical methods are needed to simultaneously accommodate high-dimensional mediators while directly target penalization of the natural indirect effect (NIE) for active mediator identification. Here, we develop two novel prior models for identification of active mediators in high-dimensional mediation analysis through penalizing NIEs in a Bayesian paradigm. Both methods specify a joint prior distribution on the exposure-mediator effect and mediator-outcome effect with either (a) a four-component Gaussian mixture prior or (b) a product threshold Gaussian prior. By jointly modeling the two parameters that contribute to the NIE, the proposed methods enable penalization on their product in a targeted way. Resultant inference can take into account the four-component composite structure underlying the NIE. We show through simulations that the proposed methods improve both selection and estimation accuracy compared to other competing methods. We applied our methods for an in-depth analysis of two ongoing epidemiologic studies: the Multi-Ethnic Study of Atherosclerosis (MESA) and the LIFECODES birth cohort. The identified active mediators in both studies reveal important biological pathways for understanding disease mechanisms.

</details>

<details>

<summary>2020-08-14 15:04:46 - Bayesian Experimental Design for Implicit Models by Mutual Information Neural Estimation</summary>

- *Steven Kleinegesse, Michael U. Gutmann*

- `2002.08129v3` - [abs](http://arxiv.org/abs/2002.08129v3) - [pdf](http://arxiv.org/pdf/2002.08129v3)

> Implicit stochastic models, where the data-generation distribution is intractable but sampling is possible, are ubiquitous in the natural sciences. The models typically have free parameters that need to be inferred from data collected in scientific experiments. A fundamental question is how to design the experiments so that the collected data are most useful. The field of Bayesian experimental design advocates that, ideally, we should choose designs that maximise the mutual information (MI) between the data and the parameters. For implicit models, however, this approach is severely hampered by the high computational cost of computing posteriors and maximising MI, in particular when we have more than a handful of design variables to optimise. In this paper, we propose a new approach to Bayesian experimental design for implicit models that leverages recent advances in neural MI estimation to deal with these issues. We show that training a neural network to maximise a lower bound on MI allows us to jointly determine the optimal design and the posterior. Simulation studies illustrate that this gracefully extends Bayesian experimental design for implicit models to higher design dimensions.

</details>

<details>

<summary>2020-08-14 16:38:52 - Context-aware learning for generative models</summary>

- *Serafeim Perdikis, Robert Leeb, Ricardo Chavarriaga, José del R. Millán*

- `1507.08272v2` - [abs](http://arxiv.org/abs/1507.08272v2) - [pdf](http://arxiv.org/pdf/1507.08272v2)

> This work studies the class of algorithms for learning with side-information that emerge by extending generative models with embedded context-related variables. Using finite mixture models (FMM) as the prototypical Bayesian network, we show that maximum-likelihood estimation (MLE) of parameters through expectation-maximization (EM) improves over the regular unsupervised case and can approach the performances of supervised learning, despite the absence of any explicit ground truth data labeling. By direct application of the missing information principle (MIP), the algorithms' performances are proven to range between the conventional supervised and unsupervised MLE extremities proportionally to the information content of the contextual assistance provided. The acquired benefits regard higher estimation precision, smaller standard errors, faster convergence rates and improved classification accuracy or regression fitness shown in various scenarios, while also highlighting important properties and differences among the outlined situations. Applicability is showcased with three real-world unsupervised classification scenarios employing Gaussian Mixture Models. Importantly, we exemplify the natural extension of this methodology to any type of generative model by deriving an equivalent context-aware algorithm for variational autoencoders (VAs), thus broadening the spectrum of applicability to unsupervised deep learning with artificial neural networks. The latter is contrasted with a neural-symbolic algorithm exploiting side-information.

</details>

<details>

<summary>2020-08-14 17:23:47 - A Maximin $Φ_{p}$-Efficient Design for Multivariate GLM</summary>

- *Yiou Li, Lulu Kang, Xinwei Deng*

- `2008.06475v1` - [abs](http://arxiv.org/abs/2008.06475v1) - [pdf](http://arxiv.org/pdf/2008.06475v1)

> Experimental designs for a generalized linear model (GLM) often depend on the specification of the model, including the link function, the predictors, and unknown parameters, such as the regression coefficients. To deal with uncertainties of these model specifications, it is important to construct optimal designs with high efficiency under such uncertainties. Existing methods such as Bayesian experimental designs often use prior distributions of model specifications to incorporate model uncertainties into the design criterion. Alternatively, one can obtain the design by optimizing the worst-case design efficiency with respect to uncertainties of model specifications. In this work, we propose a new Maximin $\Phi_p$-Efficient (or Mm-$\Phi_p$ for short) design which aims at maximizing the minimum $\Phi_p$-efficiency under model uncertainties. Based on the theoretical properties of the proposed criterion, we develop an efficient algorithm with sound convergence properties to construct the Mm-$\Phi_p$ design. The performance of the proposed Mm-$\Phi_p$ design is assessed through several numerical examples.

</details>

<details>

<summary>2020-08-14 17:45:09 - Asymptotic Consistency of $α-$Rényi-Approximate Posteriors</summary>

- *Prateek Jaiswal, Vinayak A. Rao, Harsha Honnappa*

- `1902.01902v3` - [abs](http://arxiv.org/abs/1902.01902v3) - [pdf](http://arxiv.org/pdf/1902.01902v3)

> We study the asymptotic consistency properties of $\alpha$-R\'enyi approximate posteriors, a class of variational Bayesian methods that approximate an intractable Bayesian posterior with a member of a tractable family of distributions, the member chosen to minimize the $\alpha$-R\'enyi divergence from the true posterior. Unique to our work is that we consider settings with $\alpha > 1$, resulting in approximations that upperbound the log-likelihood, and consequently have wider spread than traditional variational approaches that minimize the Kullback-Liebler (KL) divergence from the posterior. Our primary result identifies sufficient conditions under which consistency holds, centering around the existence of a 'good' sequence of distributions in the approximating family that possesses, among other properties, the right rate of convergence to a limit distribution. We further characterize the good sequence by demonstrating that a sequence of distributions that converges too quickly cannot be a good sequence. We also extend our analysis to the setting where $\alpha$ equals one, corresponding to the minimizer of the reverse KL divergence, and to models with local latent variables. We also illustrate the existence of good sequence with a number of examples. Our results complement a growing body of work focused on the frequentist properties of variational Bayesian methods.

</details>

<details>

<summary>2020-08-14 18:11:26 - Bayesian Auxiliary Variable Model for Birth Records Data with Qualitative and Quantitative Responses</summary>

- *Xiaoning Kang, Shyam Ranganathan, Lulu Kang, Julia Gohlke, Xinwei Deng*

- `2008.06525v1` - [abs](http://arxiv.org/abs/2008.06525v1) - [pdf](http://arxiv.org/pdf/2008.06525v1)

> Many applications involve data with qualitative and quantitative responses. When there is an association between the two responses, a joint model will provide improved results than modeling them separately. In this paper, we propose a Bayesian method to jointly model such data. The joint model links the qualitative and quantitative responses and can assess their dependency strength via a latent variable. The posterior distributions of parameters are obtained through an efficient MCMC sampling algorithm. The simulation shows that the proposed method can improve the prediction capacity for both responses. We apply the proposed joint model to the birth records data acquired by the Virginia Department of Health and study the mutual dependence between preterm birth of infants and their birth weights.

</details>

<details>

<summary>2020-08-14 19:49:32 - Data-Informed Decomposition for Localized Uncertainty Quantification of Dynamical Systems</summary>

- *Waad Subber, Sayan Ghosh, Piyush Pandita, Yiming Zhang, Liping Wang*

- `2008.06556v1` - [abs](http://arxiv.org/abs/2008.06556v1) - [pdf](http://arxiv.org/pdf/2008.06556v1)

> Industrial dynamical systems often exhibit multi-scale response due to material heterogeneities, operation conditions and complex environmental loadings. In such problems, it is the case that the smallest length-scale of the systems dynamics controls the numerical resolution required to effectively resolve the embedded physics. In practice however, high numerical resolutions is only required in a confined region of the system where fast dynamics or localized material variability are exhibited, whereas a coarser discretization can be sufficient in the rest majority of the system. To this end, a unified computational scheme with uniform spatio-temporal resolutions for uncertainty quantification can be very computationally demanding. Partitioning the complex dynamical system into smaller easier-to-solve problems based of the localized dynamics and material variability can reduce the overall computational cost. However, identifying the region of interest for high-resolution and intensive uncertainty quantification can be a problem dependent. The region of interest can be specified based on the localization features of the solution, user interest, and correlation length of the random material properties. For problems where a region of interest is not evident, Bayesian inference can provide a feasible solution. In this work, we employ a Bayesian framework to update our prior knowledge on the localized region of interest using measurements and system response. To address the computational cost of the Bayesian inference, we construct a Gaussian process surrogate for the forward model. Once, the localized region of interest is identified, we use polynomial chaos expansion to propagate the localization uncertainty. We demonstrate our framework through numerical experiments on a three-dimensional elastodynamic problem.

</details>

<details>

<summary>2020-08-14 20:39:11 - Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors</summary>

- *Michael W. Dusenberry, Ghassen Jerfel, Yeming Wen, Yi-An Ma, Jasper Snoek, Katherine Heller, Balaji Lakshminarayanan, Dustin Tran*

- `2005.07186v2` - [abs](http://arxiv.org/abs/2005.07186v2) - [pdf](http://arxiv.org/pdf/2005.07186v2)

> Bayesian neural networks (BNNs) demonstrate promising success in improving the robustness and uncertainty quantification of modern deep learning. However, they generally struggle with underfitting at scale and parameter efficiency. On the other hand, deep ensembles have emerged as alternatives for uncertainty quantification that, while outperforming BNNs on certain problems, also suffer from efficiency issues. It remains unclear how to combine the strengths of these two approaches and remediate their common issues. To tackle this challenge, we propose a rank-1 parameterization of BNNs, where each weight matrix involves only a distribution on a rank-1 subspace. We also revisit the use of mixture approximate posteriors to capture multiple modes, where unlike typical mixtures, this approach admits a significantly smaller memory increase (e.g., only a 0.4% increase for a ResNet-50 mixture of size 10). We perform a systematic empirical study on the choices of prior, variational posterior, and methods to improve training. For ResNet-50 on ImageNet, Wide ResNet 28-10 on CIFAR-10/100, and an RNN on MIMIC-III, rank-1 BNNs achieve state-of-the-art performance across log-likelihood, accuracy, and calibration on the test sets and out-of-distribution variants.

</details>

<details>

<summary>2020-08-14 20:46:06 - VarFA: A Variational Factor Analysis Framework For Efficient Bayesian Learning Analytics</summary>

- *Zichao Wang, Yi Gu, Andrew Lan, Richard Baraniuk*

- `2005.13107v2` - [abs](http://arxiv.org/abs/2005.13107v2) - [pdf](http://arxiv.org/pdf/2005.13107v2)

> We propose VarFA, a variational inference factor analysis framework that extends existing factor analysis models for educational data mining to efficiently output uncertainty estimation in the model's estimated factors. Such uncertainty information is useful, for example, for an adaptive testing scenario, where additional tests can be administered if the model is not quite certain about a students' skill level estimation. Traditional Bayesian inference methods that produce such uncertainty information are computationally expensive and do not scale to large data sets. VarFA utilizes variational inference which makes it possible to efficiently perform Bayesian inference even on very large data sets. We use the sparse factor analysis model as a case study and demonstrate the efficacy of VarFA on both synthetic and real data sets. VarFA is also very general and can be applied to a wide array of factor analysis models.

</details>

<details>

<summary>2020-08-14 21:47:35 - Knowing The What But Not The Where in Bayesian Optimization</summary>

- *Vu Nguyen, Michael A. Osborne*

- `1905.02685v5` - [abs](http://arxiv.org/abs/1905.02685v5) - [pdf](http://arxiv.org/pdf/1905.02685v5)

> Bayesian optimization has demonstrated impressive success in finding the optimum input x* and output f* = f(x*) = max f(x) of a black-box function f. In some applications, however, the optimum output f* is known in advance and the goal is to find the corresponding optimum input x*. In this paper, we consider a new setting in BO in which the knowledge of the optimum output f* is available. Our goal is to exploit the knowledge about f* to search for the input x* efficiently. To achieve this goal, we first transform the Gaussian process surrogate using the information about the optimum output. Then, we propose two acquisition functions, called confidence bound minimization and expected regret minimization. We show that our approaches work intuitively and give quantitatively better performance against standard BO methods. We demonstrate real applications in tuning a deep reinforcement learning algorithm on the CartPole problem and XGBoost on Skin Segmentation dataset in which the optimum values are publicly available.

</details>

<details>

<summary>2020-08-15 04:39:50 - A Note on Particle Gibbs Method and its Extensions and Variants</summary>

- *Niharika Gauraha*

- `2007.15862v2` - [abs](http://arxiv.org/abs/2007.15862v2) - [pdf](http://arxiv.org/pdf/2007.15862v2)

> High-dimensional state trajectories of state-space models pose challenges for Bayesian inference. Particle Gibbs (PG) methods have been widely used to sample from the posterior of a state space model. Basically, particle Gibbs is a Particle Markov Chain Monte Carlo (PMCMC) algorithm that mimics the Gibbs sampler by drawing model parameters and states from their conditional distributions.   This tutorial provides an introductory view on Particle Gibbs (PG) method and its extensions and variants, and illustrates through several examples of inference in non-linear state space models (SSMs). We also implement PG Samplers in two different programming languages: Python and Rust. Comparison of run-time performance of Python and Rust programs are also provided for various PG methods.

</details>

<details>

<summary>2020-08-15 15:03:46 - Reliable Uncertainties for Bayesian Neural Networks using Alpha-divergences</summary>

- *Hector J. Hortua, Luigi Malago, Riccardo Volpi*

- `2008.06729v1` - [abs](http://arxiv.org/abs/2008.06729v1) - [pdf](http://arxiv.org/pdf/2008.06729v1)

> Bayesian Neural Networks (BNNs) often result uncalibrated after training, usually tending towards overconfidence. Devising effective calibration methods with low impact in terms of computational complexity is thus of central interest. In this paper we present calibration methods for BNNs based on the alpha divergences from Information Geometry. We compare the use of alpha divergence in training and in calibration, and we show how the use in calibration provides better calibrated uncertainty estimates for specific choices of alpha and is more efficient especially for complex network architectures. We empirically demonstrate the advantages of alpha calibration in regression problems involving parameter estimation and inferred correlations between output uncertainties.

</details>

<details>

<summary>2020-08-15 15:09:21 - Enhanced data efficiency using deep neural networks and Gaussian processes for aerodynamic design optimization</summary>

- *S. Ashwin Renganathan, Romit Maulik and, Jai Ahuja*

- `2008.06731v1` - [abs](http://arxiv.org/abs/2008.06731v1) - [pdf](http://arxiv.org/pdf/2008.06731v1)

> Adjoint-based optimization methods are attractive for aerodynamic shape design primarily due to their computational costs being independent of the dimensionality of the input space and their ability to generate high-fidelity gradients that can then be used in a gradient-based optimizer. This makes them very well suited for high-fidelity simulation based aerodynamic shape optimization of highly parametrized geometries such as aircraft wings. However, the development of adjoint-based solvers involve careful mathematical treatment and their implementation require detailed software development. Furthermore, they can become prohibitively expensive when multiple optimization problems are being solved, each requiring multiple restarts to circumvent local optima. In this work, we propose a machine learning enabled, surrogate-based framework that replaces the expensive adjoint solver, without compromising on predicting predictive accuracy. Specifically, we first train a deep neural network (DNN) from training data generated from evaluating the high-fidelity simulation model on a model-agnostic, design of experiments on the geometry shape parameters. The optimum shape may then be computed by using a gradient-based optimizer coupled with the trained DNN. Subsequently, we also perform a gradient-free Bayesian optimization, where the trained DNN is used as the prior mean. We observe that the latter framework (DNN-BO) improves upon the DNN-only based optimization strategy for the same computational cost. Overall, this framework predicts the true optimum with very high accuracy, while requiring far fewer high-fidelity function calls compared to the adjoint-based method. Furthermore, we show that multiple optimization problems can be solved with the same machine learning model with high accuracy, to amortize the offline costs associated with constructing our models.

</details>

<details>

<summary>2020-08-16 04:01:34 - On Global-local Shrinkage Priors for Count Data</summary>

- *Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa*

- `1907.01333v2` - [abs](http://arxiv.org/abs/1907.01333v2) - [pdf](http://arxiv.org/pdf/1907.01333v2)

> Global-local shrinkage prior has been recognized as useful class of priors which can strongly shrink small signals towards prior means while keeping large signals unshrunk. Although such priors have been extensively discussed under Gaussian responses, we intensively encounter count responses in practice in which the previous knowledge of global-local shrinkage priors cannot be directly imported. In this paper, we discuss global-local shrinkage priors for analyzing sequence of counts. We provide sufficient conditions under which the posterior mean keeps the observation as it is for very large signals, known as tail robustness property. Then, we propose tractable priors to meet the derived conditions approximately or exactly and develop an efficient posterior computation algorithm for Bayesian inference. The proposed methods are free from tuning parameters, that is, all the hyperparameters are automatically estimated based on the data. We demonstrate the proposed methods through simulation and an application to a real dataset.

</details>

<details>

<summary>2020-08-16 08:33:19 - Variance reduction for dependent sequences with applications to Stochastic Gradient MCMC</summary>

- *D. Belomestny, L. Iosipoi, E. Moulines, A. Naumov, S. Samsonov*

- `2008.06858v1` - [abs](http://arxiv.org/abs/2008.06858v1) - [pdf](http://arxiv.org/pdf/2008.06858v1)

> In this paper we propose a novel and practical variance reduction approach for additive functionals of dependent sequences. Our approach combines the use of control variates with the minimisation of an empirical variance estimate. We analyse finite sample properties of the proposed method and derive finite-time bounds of the excess asymptotic variance to zero. We apply our methodology to Stochastic Gradient MCMC (SGMCMC) methods for Bayesian inference on large data sets and combine it with existing variance reduction methods for SGMCMC. We present empirical results carried out on a number of benchmark examples showing that our variance reduction method achieves significant improvement as compared to state-of-the-art methods at the expense of a moderate increase of computational overhead.

</details>

<details>

<summary>2020-08-16 13:09:58 - Modeling "Equitable and Sustainable Well-being" (BES) using Bayesian Networks: A Case Study of the Italian regions</summary>

- *Federica Onori, Giovanna Jona Lasinio*

- `2008.06902v1` - [abs](http://arxiv.org/abs/2008.06902v1) - [pdf](http://arxiv.org/pdf/2008.06902v1)

> Measurement of well-being has been a highly debated topic since the end of the last century. While some specific aspects are still open issues, a multidimensional approach as well as the construction of shared and well-rooted systems of indicators are now accepted as the main route to measure this complex phenomenon. A meaningful effort, in this direction, is that of the Italian "Equitable and Sustainable Well-being" (BES) system of indicators, developed by the Italian National Institute of Statistics (ISTAT) and the National Council for Economics and Labour (CNEL). The BES framework comprises a number of atomic indicators measured yearly at the regional level and reflecting the different domains of well-being (e.g. Health, Education, Work \& Life Balance, Environment,...). In this work we aim at dealing with the multidimensionality of the BES system of indicators and try to answer three main research questions: I) What is the structure of the relationships among the BES atomic indicators; II) What is the structure of the relationships among the BES domains; III) To what extent the structure of the relationships reflects the current BES theoretical framework. We address these questions by implementing Bayesian Networks (BNs), a widely accepted class of multivariate statistical models, particularly suitable for handling reasoning with uncertainty. Implementation of a BN results in a set of nodes and a set of conditional independence statements that provide an effective tool to explore associations in a system of variables. In this work, we also suggest two strategies for encoding prior knowledge in the BN estimating algorithm so that the BES theoretical framework can be represented into the network.

</details>

<details>

<summary>2020-08-16 19:38:06 - Bayesian Cycle-Consistent Generative Adversarial Networks via Marginalizing Latent Sampling</summary>

- *Haoran You, Yu Cheng, Tianheng Cheng, Chunliang Li, Pan Zhou*

- `1811.07465v3` - [abs](http://arxiv.org/abs/1811.07465v3) - [pdf](http://arxiv.org/pdf/1811.07465v3)

> Recent techniques built on Generative Adversarial Networks (GANs), such as Cycle-Consistent GANs, are able to learn mappings among different domains built from unpaired datasets, through min-max optimization games between generators and discriminators. However, it remains challenging to stabilize the training process and thus cyclic models fall into mode collapse accompanied by the success of discriminator. To address this problem, we propose an novel Bayesian cyclic model and an integrated cyclic framework for inter-domain mappings. The proposed method motivated by Bayesian GAN explores the full posteriors of cyclic model via sampling latent variables and optimizes the model with maximum a posteriori (MAP) estimation. Hence, we name it Bayesian CycleGAN. In addition, original CycleGAN cannot generate diversified results. But it is feasible for Bayesian framework to diversify generated images by replacing restricted latent variables in inference process. We evaluate the proposed Bayesian CycleGAN on multiple benchmark datasets, including Cityscapes, Maps, and Monet2photo. The proposed method improve the per-pixel accuracy by 15% for the Cityscapes semantic segmentation task within origin framework and improve 20% within the proposed integrated framework, showing better resilience to imbalance confrontation. The diversified results of Monet2Photo style transfer also demonstrate its superiority over original cyclic model. We provide codes for all of our experiments in https://github.com/ranery/Bayesian-CycleGAN.

</details>

<details>

<summary>2020-08-17 01:54:26 - A Spatial Stochastic SIR Model for Transmission Networks with Application to COVID-19 Epidemic in China</summary>

- *Tatsushi Oka, Wei Wei, Dan Zhu*

- `2008.06051v2` - [abs](http://arxiv.org/abs/2008.06051v2) - [pdf](http://arxiv.org/pdf/2008.06051v2)

> Governments around the world have implemented preventive measures against the spread of the coronavirus disease (COVID-19). In this study, we consider a multivariate discrete-time Markov model to analyze the propagation of COVID-19 across 33 provincial regions in China. This approach enables us to evaluate the effect of mobility restriction policies on the spread of the disease. We use data on daily human mobility across regions and apply the Bayesian framework to estimate the proposed model. The results show that the spread of the disease in China was predominately driven by community transmission within regions and the lockdown policy introduced by local governments curbed the spread of the pandemic. Further, we document that Hubei was only the epicenter of the early epidemic stage. Secondary epicenters, such as Beijing and Guangdong, had already become established by late January 2020, and the disease spread out to connected regions. The transmission from these epicenters substantially declined following the introduction of human mobility restrictions across regions.

</details>

<details>

<summary>2020-08-17 04:03:17 - A Common Atom Model for the Bayesian Nonparametric Analysis of Nested Data</summary>

- *Francesco Denti, Federico Camerlenghi, Michele Guindani, Antonietta Mira*

- `2008.07077v1` - [abs](http://arxiv.org/abs/2008.07077v1) - [pdf](http://arxiv.org/pdf/2008.07077v1)

> The use of high-dimensional data for targeted therapeutic interventions requires new ways to characterize the heterogeneity observed across subgroups of a specific population. In particular, models for partially exchangeable data are needed for inference on nested datasets, where the observations are assumed to be organized in different units and some sharing of information is required to learn distinctive features of the units. In this manuscript, we propose a nested Common Atoms Model (CAM) that is particularly suited for the analysis of nested datasets where the distributions of the units are expected to differ only over a small fraction of the observations sampled from each unit. The proposed CAM allows a two-layered clustering at the distributional and observational level and is amenable to scalable posterior inference through the use of a computationally efficient nested slice-sampler algorithm. We further discuss how to extend the proposed modeling framework to handle discrete measurements, and we conduct posterior inference on a real microbiome dataset from a diet swap study to investigate how the alterations in intestinal microbiota composition are associated with different eating habits. We further investigate the performance of our model in capturing true distributional structures in the population by means of a simulation study.

</details>

<details>

<summary>2020-08-17 14:44:40 - Posterior contraction rates for non-parametric state and drift estimation</summary>

- *Sebastian Reich, Paul Rozdeba*

- `2003.09219v2` - [abs](http://arxiv.org/abs/2003.09219v2) - [pdf](http://arxiv.org/pdf/2003.09219v2)

> We consider a combined state and drift estimation problem for the linear stochastic heat equation. The infinite-dimensional Bayesian inference problem is formulated in terms of the Kalman-Bucy filter over an extended state space, and its long-time asymptotic properties are studied. Asymptotic posterior contraction rates in the unknown drift function are the main contribution of this paper. Such rates have been studied before for stationary non-parametric Bayesian inverse problems, and here we demonstrate the consistency of our time-dependent formulation with these previous results building upon scale separation and a slow manifold approximation.

</details>

<details>

<summary>2020-08-17 15:50:40 - Hey Human, If your Facial Emotions are Uncertain, You Should Use Bayesian Neural Networks!</summary>

- *Maryam Matin, Matias Valdenegro-Toro*

- `2008.07426v1` - [abs](http://arxiv.org/abs/2008.07426v1) - [pdf](http://arxiv.org/pdf/2008.07426v1)

> Facial emotion recognition is the task to classify human emotions in face images. It is a difficult task due to high aleatoric uncertainty and visual ambiguity. A large part of the literature aims to show progress by increasing accuracy on this task, but this ignores the inherent uncertainty and ambiguity in the task. In this paper we show that Bayesian Neural Networks, as approximated using MC-Dropout, MC-DropConnect, or an Ensemble, are able to model the aleatoric uncertainty in facial emotion recognition, and produce output probabilities that are closer to what a human expects. We also show that calibration metrics show strange behaviors for this task, due to the multiple classes that can be considered correct, which motivates future work. We believe our work will motivate other researchers to move away from Classical and into Bayesian Neural Networks.

</details>

<details>

<summary>2020-08-17 15:57:13 - Investigating maximum likelihood based training of infinite mixtures for uncertainty quantification</summary>

- *Sina Däubener, Asja Fischer*

- `2008.03209v2` - [abs](http://arxiv.org/abs/2008.03209v2) - [pdf](http://arxiv.org/pdf/2008.03209v2)

> Uncertainty quantification in neural networks gained a lot of attention in the past years. The most popular approaches, Bayesian neural networks (BNNs), Monte Carlo dropout, and deep ensembles have one thing in common: they are all based on some kind of mixture model. While the BNNs build infinite mixture models and are derived via variational inference, the latter two build finite mixtures trained with the maximum likelihood method. In this work we investigate the effect of training an infinite mixture distribution with the maximum likelihood method instead of variational inference. We find that the proposed objective leads to stochastic networks with an increased predictive variance, which improves uncertainty based identification of miss-classification and robustness against adversarial attacks in comparison to a standard BNN with equivalent network structure. The new model also displays higher entropy on out-of-distribution data.

</details>

<details>

<summary>2020-08-17 16:05:52 - On Distance and Kernel Measures of Conditional Independence</summary>

- *Tianhong Sheng, Bharath K. Sriperumbudur*

- `1912.01103v2` - [abs](http://arxiv.org/abs/1912.01103v2) - [pdf](http://arxiv.org/pdf/1912.01103v2)

> Measuring conditional independence is one of the important tasks in statistical inference and is fundamental in causal discovery, feature selection, dimensionality reduction, Bayesian network learning, and others. In this work, we explore the connection between conditional independence measures induced by distances on a metric space and reproducing kernels associated with a reproducing kernel Hilbert space (RKHS). For certain distance and kernel pairs, we show the distance-based conditional independence measures to be equivalent to that of kernel-based measures. On the other hand, we also show that some popular---in machine learning---kernel conditional independence measures based on the Hilbert-Schmidt norm of a certain cross-conditional covariance operator, do not have a simple distance representation, except in some limiting cases. This paper, therefore, shows the distance and kernel measures of conditional independence to be not quite equivalent unlike in the case of joint independence as shown by Sejdinovic et al. (2013).

</details>

<details>

<summary>2020-08-18 00:48:15 - Training Binary Neural Networks using the Bayesian Learning Rule</summary>

- *Xiangming Meng, Roman Bachmann, Mohammad Emtiyaz Khan*

- `2002.10778v4` - [abs](http://arxiv.org/abs/2002.10778v4) - [pdf](http://arxiv.org/pdf/2002.10778v4)

> Neural networks with binary weights are computation-efficient and hardware-friendly, but their training is challenging because it involves a discrete optimization problem. Surprisingly, ignoring the discrete nature of the problem and using gradient-based methods, such as the Straight-Through Estimator, still works well in practice. This raises the question: are there principled approaches which justify such methods? In this paper, we propose such an approach using the Bayesian learning rule. The rule, when applied to estimate a Bernoulli distribution over the binary weights, results in an algorithm which justifies some of the algorithmic choices made by the previous approaches. The algorithm not only obtains state-of-the-art performance, but also enables uncertainty estimation for continual learning to avoid catastrophic forgetting. Our work provides a principled approach for training binary neural networks which justifies and extends existing approaches.

</details>

<details>

<summary>2020-08-18 02:48:43 - Selecting Data Adaptive Learner from Multiple Deep Learners using Bayesian Networks</summary>

- *Shusuke Kobayashi, Susumu Shirayama*

- `2008.07709v1` - [abs](http://arxiv.org/abs/2008.07709v1) - [pdf](http://arxiv.org/pdf/2008.07709v1)

> A method to predict time-series using multiple deep learners and a Bayesian network is proposed. In this study, the input explanatory variables are Bayesian network nodes that are associated with learners. Training data are divided using K-means clustering, and multiple deep learners are trained depending on the cluster. A Bayesian network is used to determine which deep learner is in charge of predicting a time-series. We determine a threshold value and select learners with a posterior probability equal to or greater than the threshold value, which could facilitate more robust prediction. The proposed method is applied to financial time-series data, and the predicted results for the Nikkei 225 index are demonstrated.

</details>

<details>

<summary>2020-08-18 04:17:54 - Efficient Bayesian Structural Equation Modeling in Stan</summary>

- *Edgar C. Merkle, Ellen Fitzsimmons, James Uanhoro, Ben Goodrich*

- `2008.07733v1` - [abs](http://arxiv.org/abs/2008.07733v1) - [pdf](http://arxiv.org/pdf/2008.07733v1)

> Structural equation models comprise a large class of popular statistical models, including factor analysis models, certain mixed models, and extensions thereof. Model estimation is complicated by the fact that we typically have multiple interdependent response variables and multiple latent variables (which may also be called random effects or hidden variables), often leading to slow and inefficient MCMC samples. In this paper, we describe and illustrate a general, efficient approach to Bayesian SEM estimation in Stan, contrasting it with previous implementations in R package blavaan (Merkle & Rosseel, 2018). After describing the approaches in detail, we conduct a practical comparison under multiple scenarios. The comparisons show that the new approach is clearly better. We also discuss ways that the approach may be extended to other models that are of interest to psychometricians.

</details>

<details>

<summary>2020-08-18 06:17:56 - Bayesian network structure learning with causal effects in the presence of latent variables</summary>

- *Kiattikun Chobtham, Anthony C. Constantinou*

- `2005.14381v2` - [abs](http://arxiv.org/abs/2005.14381v2) - [pdf](http://arxiv.org/pdf/2005.14381v2)

> Latent variables may lead to spurious relationships that can be misinterpreted as causal relationships. In Bayesian Networks (BNs), this challenge is known as learning under causal insufficiency. Structure learning algorithms that assume causal insufficiency tend to reconstruct the ancestral graph of a BN, where bi-directed edges represent confounding and directed edges represent direct or ancestral relationships. This paper describes a hybrid structure learning algorithm, called CCHM, which combines the constraint-based part of cFCI with hill-climbing score-based learning. The score-based process incorporates Pearl s do-calculus to measure causal effects and orientate edges that would otherwise remain undirected, under the assumption the BN is a linear Structure Equation Model where data follow a multivariate Gaussian distribution. Experiments based on both randomised and well-known networks show that CCHM improves the state-of-the-art in terms of reconstructing the true ancestral graph.

</details>

<details>

<summary>2020-08-18 09:55:31 - Conditional Flow Variational Autoencoders for Structured Sequence Prediction</summary>

- *Apratim Bhattacharyya, Michael Hanselmann, Mario Fritz, Bernt Schiele, Christoph-Nikolas Straehle*

- `1908.09008v3` - [abs](http://arxiv.org/abs/1908.09008v3) - [pdf](http://arxiv.org/pdf/1908.09008v3)

> Prediction of future states of the environment and interacting agents is a key competence required for autonomous agents to operate successfully in the real world. Prior work for structured sequence prediction based on latent variable models imposes a uni-modal standard Gaussian prior on the latent variables. This induces a strong model bias which makes it challenging to fully capture the multi-modality of the distribution of the future states. In this work, we introduce Conditional Flow Variational Autoencoders (CF-VAE) using our novel conditional normalizing flow based prior to capture complex multi-modal conditional distributions for effective structured sequence prediction. Moreover, we propose two novel regularization schemes which stabilizes training and deals with posterior collapse for stable training and better fit to the target data distribution. Our experiments on three multi-modal structured sequence prediction datasets -- MNIST Sequences, Stanford Drone and HighD -- show that the proposed method obtains state of art results across different evaluation metrics.

</details>

<details>

<summary>2020-08-18 16:10:06 - Extreme data compression while searching for new physics</summary>

- *Alan Heavens, Elena Sellentin, Andrew Jaffe*

- `2006.06706v2` - [abs](http://arxiv.org/abs/2006.06706v2) - [pdf](http://arxiv.org/pdf/2006.06706v2)

> Bringing a high-dimensional dataset into science-ready shape is a formidable challenge that often necessitates data compression. Compression has accordingly become a key consideration for contemporary cosmology, affecting public data releases, and reanalyses searching for new physics. However, data compression optimized for a particular model can suppress signs of new physics, or even remove them altogether. We therefore provide a solution for exploring new physics \emph{during} data compression. In particular, we store additional agnostic compressed data points, selected to enable precise constraints of non-standard physics at a later date. Our procedure is based on the maximal compression of the MOPED algorithm, which optimally filters the data with respect to a baseline model. We select additional filters, based on a generalised principal component analysis, which are carefully constructed to scout for new physics at high precision and speed. We refer to the augmented set of filters as MOPED-PC. They enable an analytic computation of Bayesian evidences that may indicate the presence of new physics, and fast analytic estimates of best-fitting parameters when adopting a specific non-standard theory, without further expensive MCMC analysis. As there may be large numbers of non-standard theories, the speed of the method becomes essential. Should no new physics be found, then our approach preserves the precision of the standard parameters. As a result, we achieve very rapid and maximally precise constraints of standard and non-standard physics, with a technique that scales well to large dimensional datasets.

</details>

<details>

<summary>2020-08-18 17:08:39 - Fast Approximate Bayesian Contextual Cold Start Learning (FAB-COST)</summary>

- *Jack R. McKenzie, Peter A. Appleby, Thomas House, Neil Walton*

- `2008.08038v1` - [abs](http://arxiv.org/abs/2008.08038v1) - [pdf](http://arxiv.org/pdf/2008.08038v1)

> Cold-start is a notoriously difficult problem which can occur in recommendation systems, and arises when there is insufficient information to draw inferences for users or items. To address this challenge, a contextual bandit algorithm -- the Fast Approximate Bayesian Contextual Cold Start Learning algorithm (FAB-COST) -- is proposed, which is designed to provide improved accuracy compared to the traditionally used Laplace approximation in the logistic contextual bandit, while controlling both algorithmic complexity and computational cost. To this end, FAB-COST uses a combination of two moment projection variational methods: Expectation Propagation (EP), which performs well at the cold start, but becomes slow as the amount of data increases; and Assumed Density Filtering (ADF), which has slower growth of computational cost with data size but requires more data to obtain an acceptable level of accuracy. By switching from EP to ADF when the dataset becomes large, it is able to exploit their complementary strengths. The empirical justification for FAB-COST is presented, and systematically compared to other approaches on simulated data. In a benchmark against the Laplace approximation on real data consisting of over $670,000$ impressions from autotrader.co.uk, FAB-COST demonstrates at one point increase of over $16\%$ in user clicks. On the basis of these results, it is argued that FAB-COST is likely to be an attractive approach to cold-start recommendation systems in a variety of contexts.

</details>

<details>

<summary>2020-08-18 22:56:46 - Scalable Combinatorial Bayesian Optimization with Tractable Statistical models</summary>

- *Aryan Deshwal, Syrine Belakaria, Janardhan Rao Doppa*

- `2008.08177v1` - [abs](http://arxiv.org/abs/2008.08177v1) - [pdf](http://arxiv.org/pdf/2008.08177v1)

> We study the problem of optimizing expensive blackbox functions over combinatorial spaces (e.g., sets, sequences, trees, and graphs). BOCS (Baptista and Poloczek, 2018) is a state-of-the-art Bayesian optimization method for tractable statistical models, which performs semi-definite programming based acquisition function optimization (AFO) to select the next structure for evaluation. Unfortunately, BOCS scales poorly for large number of binary and/or categorical variables. Based on recent advances in submodular relaxation (Ito and Fujimaki, 2016) for solving Binary Quadratic Programs, we study an approach referred as Parametrized Submodular Relaxation (PSR) towards the goal of improving the scalability and accuracy of solving AFO problems for BOCS model. PSR approach relies on two key ideas. First, reformulation of AFO problem as submodular relaxation with some unknown parameters, which can be solved efficiently using minimum graph cut algorithms. Second, construction of an optimization problem to estimate the unknown parameters with close approximation to the true objective. Experiments on diverse benchmark problems show significant improvements with PSR for BOCS model. The source code is available at https://github.com/aryandeshwal/Submodular_Relaxation_BOCS .

</details>

<details>

<summary>2020-08-19 08:30:30 - Active pooling design in group testing based on Bayesian posterior prediction</summary>

- *Ayaka Sakata*

- `2007.13323v2` - [abs](http://arxiv.org/abs/2007.13323v2) - [pdf](http://arxiv.org/pdf/2007.13323v2)

> In identifying infected patients in a population, group testing is an effective method to reduce the number of tests and correct the test errors. In the group testing procedure, tests are performed on pools of specimens collected from patients, where the number of pools is lower than that of patients. The performance of group testing heavily depends on the design of pools and algorithms that are used in inferring the infected patients from the test outcomes. In this paper, an adaptive design method of pools based on the predictive distribution is proposed in the framework of Bayesian inference. The proposed method executed using the belief propagation algorithm results in more accurate identification of the infected patients, as compared to the group testing performed on random pools determined in advance.

</details>

<details>

<summary>2020-08-19 08:55:17 - Synergy in fertility forecasting: Improving forecast accuracy through model averaging</summary>

- *Han Lin Shang, Heather Booth*

- `2008.08335v1` - [abs](http://arxiv.org/abs/2008.08335v1) - [pdf](http://arxiv.org/pdf/2008.08335v1)

> Accuracy in fertility forecasting has proved challenging and warrants renewed attention. One way to improve accuracy is to combine the strengths of a set of existing models through model averaging. The model-averaged forecast is derived using empirical model weights that optimise forecast accuracy at each forecast horizon based on historical data. We apply model averaging to fertility forecasting for the first time, using data for 17 countries and six models. Four model-averaging methods are compared: frequentist, Bayesian, model confidence set, and equal weights. We compute individual-model and model-averaged point and interval forecasts at horizons of one to 20 years. We demonstrate gains in average accuracy of 4-23\% for point forecasts and 3-24\% for interval forecasts, with greater gains from the frequentist and equal-weights approaches at longer horizons. Data for England \& Wales are used to illustrate model averaging in forecasting age-specific fertility to 2036. The advantages and further potential of model averaging for fertility forecasting are discussed. As the accuracy of model-averaged forecasts depends on the accuracy of the individual models, there is ongoing need to develop better models of fertility for use in forecasting and model averaging. We conclude that model averaging holds considerable promise for the improvement of fertility forecasting in a systematic way using existing models and warrants further investigation.

</details>

<details>

<summary>2020-08-19 10:06:41 - Spatiotemporal mapping of malaria prevalence in Madagascar using routine surveillance and health survey data</summary>

- *Rohan Arambepola, Suzanne H. Keddie, Emma L. Collins, Katherine A. Twohig, Punam Amratia, Amelia Bertozzi-Villa, Elisabeth G. Chestnutt, Joseph Harris, Justin Millar, Jennifer Rozier, Susan F. Rumisha, Tasmin L. Symons, Camilo Vargas-Ruiz, Mauricette Andriamananjara, Saraha Rabeherisoa, Arsène C. Ratsimbasoa, Rosalind E. Howes, Daniel J. Weiss, Peter W. Gething, Ewan Cameron*

- `2008.08358v1` - [abs](http://arxiv.org/abs/2008.08358v1) - [pdf](http://arxiv.org/pdf/2008.08358v1)

> Malaria transmission in Madagascar is highly heterogeneous, exhibiting spatial, seasonal and long-term trends. Previous efforts to map malaria risk in Madagascar used prevalence data from Malaria Indicator Surveys. These cross-sectional surveys, conducted during the high transmission season most recently in 2013 and 2016, provide nationally representative prevalence data but cover relatively short time frames. Conversely, monthly case data are collected at health facilities but suffer from biases, including incomplete reporting.   We combined survey and case data to make monthly maps of prevalence between 2013 and 2016. Health facility catchments were estimated and incidence surfaces, environmental and socioeconomic covariates, and survey data informed a Bayesian prevalence model. Prevalence estimates were consistently high in the coastal regions and low in the highlands. Prevalence was lowest in 2014 and peaked in 2015, highlighting the importance of estimates between survey years. Seasonality was widely observed. Similar multi-metric approaches may be applicable across sub-Saharan Africa.

</details>

<details>

<summary>2020-08-19 13:53:32 - A Tutorial on Distance Metric Learning: Mathematical Foundations, Algorithms, Experimental Analysis, Prospects and Challenges (with Appendices on Mathematical Background and Detailed Algorithms Explanation)</summary>

- *Juan Luis Suárez-Díaz, Salvador García, Francisco Herrera*

- `1812.05944v3` - [abs](http://arxiv.org/abs/1812.05944v3) - [pdf](http://arxiv.org/pdf/1812.05944v3)

> Distance metric learning is a branch of machine learning that aims to learn distances from the data, which enhances the performance of similarity-based algorithms. This tutorial provides a theoretical background and foundations on this topic and a comprehensive experimental analysis of the most-known algorithms. We start by describing the distance metric learning problem and its main mathematical foundations, divided into three main blocks: convex analysis, matrix analysis and information theory. Then, we will describe a representative set of the most popular distance metric learning methods used in classification. All the algorithms studied in this paper will be evaluated with exhaustive testing in order to analyze their capabilities in standard classification problems, particularly considering dimensionality reduction and kernelization. The results, verified by Bayesian statistical tests, highlight a set of outstanding algorithms. Finally, we will discuss several potential future prospects and challenges in this field. This tutorial will serve as a starting point in the domain of distance metric learning from both a theoretical and practical perspective.

</details>

<details>

<summary>2020-08-19 14:23:38 - Sparse Cholesky covariance parametrization for recovering latent structure in ordered data</summary>

- *Irene Córdoba, Concha Bielza, Pedro Larrañaga, Gherardo Varando*

- `2006.01448v2` - [abs](http://arxiv.org/abs/2006.01448v2) - [pdf](http://arxiv.org/pdf/2006.01448v2)

> The sparse Cholesky parametrization of the inverse covariance matrix can be interpreted as a Gaussian Bayesian network; however its counterpart, the covariance Cholesky factor, has received, with few notable exceptions, little attention so far, despite having a natural interpretation as a hidden variable model for ordered signal data. To fill this gap, in this paper we focus on arbitrary zero patterns in the Cholesky factor of a covariance matrix. We discuss how these models can also be extended, in analogy with Gaussian Bayesian networks, to data where no apparent order is available. For the ordered scenario, we propose a novel estimation method that is based on matrix loss penalization, as opposed to the existing regression-based approaches. The performance of this sparse model for the Cholesky factor, together with our novel estimator, is assessed in a simulation setting, as well as over spatial and temporal real data where a natural ordering arises among the variables. We give guidelines, based on the empirical results, about which of the methods analysed is more appropriate for each setting.

</details>

<details>

<summary>2020-08-19 15:47:32 - Bayesian neural networks and dimensionality reduction</summary>

- *Deborshee Sen, Theodore Papamarkou, David Dunson*

- `2008.08044v2` - [abs](http://arxiv.org/abs/2008.08044v2) - [pdf](http://arxiv.org/pdf/2008.08044v2)

> In conducting non-linear dimensionality reduction and feature learning, it is common to suppose that the data lie near a lower-dimensional manifold. A class of model-based approaches for such problems includes latent variables in an unknown non-linear regression function; this includes Gaussian process latent variable models and variational auto-encoders (VAEs) as special cases. VAEs are artificial neural networks (ANNs) that employ approximations to make computation tractable; however, current implementations lack adequate uncertainty quantification in estimating the parameters, predictive densities, and lower-dimensional subspace, and can be unstable and lack interpretability in practice. We attempt to solve these problems by deploying Markov chain Monte Carlo sampling algorithms (MCMC) for Bayesian inference in ANN models with latent variables. We address issues of identifiability by imposing constraints on the ANN parameters as well as by using anchor points. This is demonstrated on simulated and real data examples. We find that current MCMC sampling schemes face fundamental challenges in neural networks involving latent variables, motivating new research directions.

</details>

<details>

<summary>2020-08-19 19:05:19 - Bayesian Differential Privacy for Machine Learning</summary>

- *Aleksei Triastcyn, Boi Faltings*

- `1901.09697v5` - [abs](http://arxiv.org/abs/1901.09697v5) - [pdf](http://arxiv.org/pdf/1901.09697v5)

> Traditional differential privacy is independent of the data distribution. However, this is not well-matched with the modern machine learning context, where models are trained on specific data. As a result, achieving meaningful privacy guarantees in ML often excessively reduces accuracy. We propose Bayesian differential privacy (BDP), which takes into account the data distribution to provide more practical privacy guarantees. We also derive a general privacy accounting method under BDP, building upon the well-known moments accountant. Our experiments demonstrate that in-distribution samples in classic machine learning datasets, such as MNIST and CIFAR-10, enjoy significantly stronger privacy guarantees than postulated by DP, while models maintain high classification accuracy.

</details>

<details>

<summary>2020-08-20 10:55:20 - No-regret Algorithms for Multi-task Bayesian Optimization</summary>

- *Sayak Ray Chowdhury, Aditya Gopalan*

- `2008.08885v1` - [abs](http://arxiv.org/abs/2008.08885v1) - [pdf](http://arxiv.org/pdf/2008.08885v1)

> We consider multi-objective optimization (MOO) of an unknown vector-valued function in the non-parametric Bayesian optimization (BO) setting, with the aim being to learn points on the Pareto front of the objectives. Most existing BO algorithms do not model the fact that the multiple objectives, or equivalently, tasks can share similarities, and even the few that do lack rigorous, finite-time regret guarantees that capture explicitly inter-task structure. In this work, we address this problem by modelling inter-task dependencies using a multi-task kernel and develop two novel BO algorithms based on random scalarizations of the objectives. Our algorithms employ vector-valued kernel regression as a stepping stone and belong to the upper confidence bound class of algorithms. Under a smoothness assumption that the unknown vector-valued function is an element of the reproducing kernel Hilbert space associated with the multi-task kernel, we derive worst-case regret bounds for our algorithms that explicitly capture the similarities between tasks. We numerically benchmark our algorithms on both synthetic and real-life MOO problems, and show the advantages offered by learning with multi-task kernels.

</details>

<details>

<summary>2020-08-20 14:15:32 - Improving Bayesian Local Spatial Models in Large Data Sets</summary>

- *Amanda Lenzi, Stefano Castruccio, Haavard Rue, Marc G. Genton*

- `1907.06932v2` - [abs](http://arxiv.org/abs/1907.06932v2) - [pdf](http://arxiv.org/pdf/1907.06932v2)

> Environmental processes resolved at a sufficiently small scale in space and time will inevitably display non-stationary behavior. Such processes are both challenging to model and computationally expensive when the data size is large. Instead of modeling the global non-stationarity explicitly, local models can be applied to disjoint regions of the domain. The choice of the size of these regions is dictated by a bias-variance trade-off; large regions will have smaller variance and larger bias, whereas small regions will have higher variance and smaller bias. From both the modeling and computational point of view, small regions are preferable to better accommodate the non-stationarity. However, in practice, large regions are necessary to control the variance. We propose a novel Bayesian three-step approach that allows for smaller regions without compromising the increase of the variance that would follow. We are able to propagate the uncertainty from one step to the next without issues caused by reusing the data. The improvement in inference also results in improved prediction, as our simulated example shows. We illustrate this new approach on a data set of simulated high-resolution wind speed data over Saudi Arabia.

</details>

<details>

<summary>2020-08-20 20:21:40 - A Bayesian time-to-event pharmacokinetic model for sequential phase I dose-escalation trials with multiple schedules</summary>

- *Burak Kürsad Günhan, Sebastian Weber, Abdelkader Seroutou, Tim Friede*

- `1811.09433v3` - [abs](http://arxiv.org/abs/1811.09433v3) - [pdf](http://arxiv.org/pdf/1811.09433v3)

> Phase I dose-escalation trials constitute the first step in investigating the safety of potentially promising drugs in humans. Conventional methods for phase I dose-escalation trials are based on a single treatment schedule only. More recently, however, multiple schedules are more frequently investigated in the same trial. Here, we consider sequential phase I trials, where the trial proceeds with a new schedule (e.g. daily or weekly dosing) once the dose escalation with another schedule has been completed. The aim is to utilize the information from both the completed and the ongoing dose-escalation trial to inform decisions on the dose level for the next dose cohort. For this purpose, we adapted the time-to-event pharmacokinetics (TITE-PK) model, which were originally developed for simultaneous investigation of multiple schedules. TITE-PK integrates information from multiple schedules using a pharmacokinetics (PK) model. In a simulation study, the developed appraoch is compared to the bridging continual reassessment method and the Bayesian logistic regression model using a meta-analytic-prior. TITE-PK results in better performance than comparators in terms of recommending acceptable dose and avoiding overly toxic doses for sequential phase I trials in most of the scenarios considered. Furthermore, better performance of TITE-PK is achieved while requiring similar number of patients in the simulated trials. For the scenarios involving one schedule, TITE-PK displays similar performance with alternatives in terms of acceptable dose recommendations. The \texttt{R} and \texttt{Stan} code for the implementation of an illustrative sequential phase I trial example is publicly available at https://github.com/gunhanb/TITEPK_sequential.

</details>

<details>

<summary>2020-08-21 04:35:06 - Amortized learning of neural causal representations</summary>

- *Nan Rosemary Ke, Jane. X. Wang, Jovana Mitrovic, Martin Szummer, Danilo J. Rezende*

- `2008.09301v1` - [abs](http://arxiv.org/abs/2008.09301v1) - [pdf](http://arxiv.org/pdf/2008.09301v1)

> Causal models can compactly and efficiently encode the data-generating process under all interventions and hence may generalize better under changes in distribution. These models are often represented as Bayesian networks and learning them scales poorly with the number of variables. Moreover, these approaches cannot leverage previously learned knowledge to help with learning new causal models. In order to tackle these challenges, we represent a novel algorithm called \textit{causal relational networks} (CRN) for learning causal models using neural networks. The CRN represent causal models using continuous representations and hence could scale much better with the number of variables. These models also take in previously learned information to facilitate learning of new causal models. Finally, we propose a decoding-based metric to evaluate causal models with continuous representations. We test our method on synthetic data achieving high accuracy and quick adaptation to previously unseen causal models.

</details>

<details>

<summary>2020-08-21 04:59:57 - Focused Bayesian Prediction</summary>

- *Ruben Loaiza-Maya, Gael M. Martin, David T. Frazier*

- `1912.12571v2` - [abs](http://arxiv.org/abs/1912.12571v2) - [pdf](http://arxiv.org/pdf/1912.12571v2)

> We propose a new method for conducting Bayesian prediction that delivers accurate predictions without correctly specifying the unknown true data generating process. A prior is defined over a class of plausible predictive models. After observing data, we update the prior to a posterior over these models, via a criterion that captures a user-specified measure of predictive accuracy. Under regularity, this update yields posterior concentration onto the element of the predictive class that maximizes the expectation of the accuracy measure. In a series of simulation experiments and empirical examples we find notable gains in predictive accuracy relative to conventional likelihood-based prediction.

</details>

<details>

<summary>2020-08-21 10:00:57 - Bayesian optimal control of GHZ states in Rydberg lattices</summary>

- *Rick Mukherjee, Harry Xie, Florian Mintert*

- `2005.05802v2` - [abs](http://arxiv.org/abs/2005.05802v2) - [pdf](http://arxiv.org/pdf/2005.05802v2)

> The ability to prepare non-classical states in a robust manner is essential for quantum sensors beyond the standard quantum limit. We demonstrate that Bayesian optimal control is capable of finding control pulses that drive trapped Rydberg atoms into highly entangled GHZ states. The control sequences have a physically intuitive functionality based on the quasi-integrability of the Ising dynamics. They can be constructed in laboratory experiments resulting in preparation times that scale very favourably with the system size.

</details>

<details>

<summary>2020-08-21 15:18:09 - Variational Inference for Computational Imaging Inverse Problems</summary>

- *Francesco Tonolini, Jack Radford, Alex Turpin, Daniele Faccio, Roderick Murray-Smith*

- `1904.06264v3` - [abs](http://arxiv.org/abs/1904.06264v3) - [pdf](http://arxiv.org/pdf/1904.06264v3)

> Machine learning methods for computational imaging require uncertainty estimation to be reliable in real settings. While Bayesian models offer a computationally tractable way of recovering uncertainty, they need large data volumes to be trained, which in imaging applications implicates prohibitively expensive collections with specific imaging instruments. This paper introduces a novel framework to train variational inference for inverse problems exploiting in combination few experimentally collected data, domain expertise and existing image data sets. In such a way, Bayesian machine learning models can solve imaging inverse problems with minimal data collection efforts. Extensive simulated experiments show the advantages of the proposed framework. The approach is then applied to two real experimental optics settings: holographic image reconstruction and imaging through highly scattering media. In both settings, state of the art reconstructions are achieved with little collection of training data.

</details>

<details>

<summary>2020-08-21 15:28:27 - Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes</summary>

- *Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein*

- `1810.05148v4` - [abs](http://arxiv.org/abs/1810.05148v4) - [pdf](http://arxiv.org/pdf/1810.05148v4)

> There is a previously identified equivalence between wide fully connected neural networks (FCNs) and Gaussian processes (GPs). This equivalence enables, for instance, test set predictions that would have resulted from a fully Bayesian, infinitely wide trained FCN to be computed without ever instantiating the FCN, but by instead evaluating the corresponding GP. In this work, we derive an analogous equivalence for multi-layer convolutional neural networks (CNNs) both with and without pooling layers, and achieve state of the art results on CIFAR10 for GPs without trainable kernels. We also introduce a Monte Carlo method to estimate the GP corresponding to a given neural network architecture, even in cases where the analytic form has too many terms to be computationally feasible.   Surprisingly, in the absence of pooling layers, the GPs corresponding to CNNs with and without weight sharing are identical. As a consequence, translation equivariance, beneficial in finite channel CNNs trained with stochastic gradient descent (SGD), is guaranteed to play no role in the Bayesian treatment of the infinite channel limit - a qualitative difference between the two regimes that is not present in the FCN case. We confirm experimentally, that while in some scenarios the performance of SGD-trained finite CNNs approaches that of the corresponding GPs as the channel count increases, with careful tuning SGD-trained CNNs can significantly outperform their corresponding GPs, suggesting advantages from SGD training compared to fully Bayesian parameter estimation.

</details>

<details>

<summary>2020-08-21 16:22:47 - Differentiable TAN Structure Learning for Bayesian Network Classifiers</summary>

- *Wolfgang Roth, Franz Pernkopf*

- `2008.09566v1` - [abs](http://arxiv.org/abs/2008.09566v1) - [pdf](http://arxiv.org/pdf/2008.09566v1)

> Learning the structure of Bayesian networks is a difficult combinatorial optimization problem. In this paper, we consider learning of tree-augmented naive Bayes (TAN) structures for Bayesian network classifiers with discrete input features. Instead of performing a combinatorial optimization over the space of possible graph structures, the proposed method learns a distribution over graph structures. After training, we select the most probable structure of this distribution. This allows for a joint training of the Bayesian network parameters along with its TAN structure using gradient-based optimization. The proposed method is agnostic to the specific loss and only requires that it is differentiable. We perform extensive experiments using a hybrid generative-discriminative loss based on the discriminative probabilistic margin. Our method consistently outperforms random TAN structures and Chow-Liu TAN structures.

</details>

<details>

<summary>2020-08-21 17:29:08 - A Practical Introduction to Bayesian Estimation of Causal Effects: Parametric and Nonparametric Approaches</summary>

- *Arman Oganisian, Jason A. Roy*

- `2004.07375v2` - [abs](http://arxiv.org/abs/2004.07375v2) - [pdf](http://arxiv.org/pdf/2004.07375v2)

> Substantial advances in Bayesian methods for causal inference have been developed in recent years. We provide an introduction to Bayesian inference for causal effects for practicing statisticians who have some familiarity with Bayesian models and would like an overview of what it can add to causal estimation in practical settings. In the paper, we demonstrate how priors can induce shrinkage and sparsity on parametric models and be used to perform probabilistic sensitivity analyses around causal assumptions. We provide an overview of nonparametric Bayesian estimation and survey their applications in the causal inference literature. Inference in the point-treatment and time-varying treatment settings are considered. For the latter, we explore both static and dynamic treatment regimes. Throughout, we illustrate implementation using off-the-shelf open source software. We hope the reader will walk away with implementation-level knowledge of Bayesian causal inference using both parametric and nonparametric models. All synthetic examples and code used in the paper are publicly available on a companion GitHub repository.

</details>

<details>

<summary>2020-08-21 20:44:33 - Data-Driven Forward Discretizations for Bayesian Inversion</summary>

- *Daniele Bigoni, Yuming Chen, Nicolas Garcia Trillos, Youssef Marzouk, Daniel Sanz-Alonso*

- `2003.07991v2` - [abs](http://arxiv.org/abs/2003.07991v2) - [pdf](http://arxiv.org/pdf/2003.07991v2)

> This paper suggests a framework for the learning of discretizations of expensive forward models in Bayesian inverse problems. The main idea is to incorporate the parameters governing the discretization as part of the unknown to be estimated within the Bayesian machinery. We numerically show that in a variety of inverse problems arising in mechanical engineering, signal processing and the geosciences, the observations contain useful information to guide the choice of discretization.

</details>

<details>

<summary>2020-08-23 04:23:05 - Learning Neural Causal Models from Unknown Interventions</summary>

- *Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard Schölkopf, Michael C. Mozer, Chris Pal, Yoshua Bengio*

- `1910.01075v2` - [abs](http://arxiv.org/abs/1910.01075v2) - [pdf](http://arxiv.org/pdf/1910.01075v2)

> Promising results have driven a recent surge of interest in continuous optimization methods for Bayesian network structure learning from observational data. However, there are theoretical limitations on the identifiability of underlying structures obtained from observational data alone. Interventional data provides much richer information about the underlying data-generating process. However, the extension and application of methods designed for observational data to include interventions is not straightforward and remains an open problem. In this paper we provide a general framework based on continuous optimization and neural networks to create models for the combination of observational and interventional data. The proposed method is even applicable in the challenging and realistic case that the identity of the intervened upon variable is unknown. We examine the proposed method in the setting of graph recovery both de novo and from a partially-known edge set. We establish strong benchmark results on several structure learning tasks, including structure recovery of both synthetic graphs as well as standard graphs from the Bayesian Network Repository.

</details>

<details>

<summary>2020-08-23 16:48:23 - Reward Design for Driver Repositioning Using Multi-Agent Reinforcement Learning</summary>

- *Zhenyu Shou, Xuan Di*

- `2002.06723v3` - [abs](http://arxiv.org/abs/2002.06723v3) - [pdf](http://arxiv.org/pdf/2002.06723v3)

> A large portion of passenger requests is reportedly unserviced, partially due to vacant for-hire drivers' cruising behavior during the passenger seeking process. This paper aims to model the multi-driver repositioning task through a mean field multi-agent reinforcement learning (MARL) approach that captures competition among multiple agents. Because the direct application of MARL to the multi-driver system under a given reward mechanism will likely yield a suboptimal equilibrium due to the selfishness of drivers, this study proposes a reward design scheme with which a more desired equilibrium can be reached. To effectively solve the bilevel optimization problem with upper level as the reward design and the lower level as a multi-agent system, a Bayesian optimization (BO) algorithm is adopted to speed up the learning process. We then apply the bilevel optimization model to two case studies, namely, e-hailing driver repositioning under service charge and multiclass taxi driver repositioning under NYC congestion pricing. In the first case study, the model is validated by the agreement between the derived optimal control from BO and that from an analytical solution. With a simple piecewise linear service charge, the objective of the e-hailing platform can be increased by 8.4%. In the second case study, an optimal toll charge of $5.1 is solved using BO, which improves the objective of city planners by 7.9%, compared to that without any toll charge. Under this optimal toll charge, the number of taxis in the NYC central business district is decreased, indicating a better traffic condition, without substantially increasing the crowdedness of the subway system.

</details>

<details>

<summary>2020-08-24 11:37:53 - Probabilistic Object Classification using CNN ML-MAP layers</summary>

- *G. Melotti, C. Premebida, J. J. Bird, D. R. Faria, N. Gonçalves*

- `2005.14565v2` - [abs](http://arxiv.org/abs/2005.14565v2) - [pdf](http://arxiv.org/pdf/2005.14565v2)

> Deep networks are currently the state-of-the-art for sensory perception in autonomous driving and robotics. However, deep models often generate overconfident predictions precluding proper probabilistic interpretation which we argue is due to the nature of the SoftMax layer. To reduce the overconfidence without compromising the classification performance, we introduce a CNN probabilistic approach based on distributions calculated in the network's Logit layer. The approach enables Bayesian inference by means of ML and MAP layers. Experiments with calibrated and the proposed prediction layers are carried out on object classification using data from the KITTI database. Results are reported for camera ($RGB$) and LiDAR (range-view) modalities, where the new approach shows promising performance compared to SoftMax.

</details>

<details>

<summary>2020-08-24 16:33:54 - SDE-Net: Equipping Deep Neural Networks with Uncertainty Estimates</summary>

- *Lingkai Kong, Jimeng Sun, Chao Zhang*

- `2008.10546v1` - [abs](http://arxiv.org/abs/2008.10546v1) - [pdf](http://arxiv.org/pdf/2008.10546v1)

> Uncertainty quantification is a fundamental yet unsolved problem for deep learning. The Bayesian framework provides a principled way of uncertainty estimation but is often not scalable to modern deep neural nets (DNNs) that have a large number of parameters. Non-Bayesian methods are simple to implement but often conflate different sources of uncertainties and require huge computing resources. We propose a new method for quantifying uncertainties of DNNs from a dynamical system perspective. The core of our method is to view DNN transformations as state evolution of a stochastic dynamical system and introduce a Brownian motion term for capturing epistemic uncertainty. Based on this perspective, we propose a neural stochastic differential equation model (SDE-Net) which consists of (1) a drift net that controls the system to fit the predictive function; and (2) a diffusion net that captures epistemic uncertainty. We theoretically analyze the existence and uniqueness of the solution to SDE-Net. Our experiments demonstrate that the SDE-Net model can outperform existing uncertainty estimation methods across a series of tasks where uncertainty plays a fundamental role.

</details>

<details>

<summary>2020-08-24 17:58:18 - A Hierarchical Bayesian SED Model for Type Ia Supernovae in the Optical to Near-Infrared</summary>

- *Kaisey S. Mandel, Stephen Thorp, Gautham Narayan, Andrew S. Friedman, Arturo Avelino*

- `2008.07538v3` - [abs](http://arxiv.org/abs/2008.07538v3) - [pdf](http://arxiv.org/pdf/2008.07538v3)

> While conventional Type Ia supernova (SN Ia) cosmology analyses rely primarily on rest-frame optical light curves to determine distances, SNe Ia are excellent standard candles in near-infrared (NIR) light, which is significantly less sensitive to dust extinction. A SN Ia spectral energy distribution (SED) model capable of fitting rest-frame NIR observations is necessary to fully leverage current and future SN Ia datasets from ground- and space-based telescopes including HST, LSST, JWST, and RST. We construct a hierarchical Bayesian model for SN Ia SEDs, continuous over time and wavelength, from the optical to NIR ($B$ through $H$, or $0.35 -1.8\, \mu$m). We model the SED as a combination of physically-distinct host galaxy dust and intrinsic spectral components. The distribution of intrinsic SEDs over time and wavelength is modelled with probabilistic functional principal components and the covariance of residual functions. We train the model on a nearby sample of 79 SNe Ia with joint optical and NIR light curves by sampling the global posterior distribution over dust and intrinsic latent variables, SED components, and population hyperparameters. The photometric distances of SNe Ia with NIR data near maximum light obtain a total RMS error of 0.10 mag with our BayeSN model, compared to 0.14 mag with SNooPy and SALT2 for the same sample. Jointly fitting the optical and NIR data of the full sample for a global host dust law, we find $R_V = 2.9 \pm 0.2$, consistent with the Milky Way average.

</details>

<details>

<summary>2020-08-25 01:08:24 - Word2vec Skip-gram Dimensionality Selection via Sequential Normalized Maximum Likelihood</summary>

- *Pham Thuc Hung, Kenji Yamanishi*

- `2008.07720v3` - [abs](http://arxiv.org/abs/2008.07720v3) - [pdf](http://arxiv.org/pdf/2008.07720v3)

> In this paper, we propose a novel information criteria-based approach to select the dimensionality of the word2vec Skip-gram (SG). From the perspective of the probability theory, SG is considered as an implicit probability distribution estimation under the assumption that there exists a true contextual distribution among words. Therefore, we apply information criteria with the aim of selecting the best dimensionality so that the corresponding model can be as close as possible to the true distribution. We examine the following information criteria for the dimensionality selection problem: the Akaike Information Criterion, Bayesian Information Criterion, and Sequential Normalized Maximum Likelihood (SNML) criterion. SNML is the total codelength required for the sequential encoding of a data sequence on the basis of the minimum description length. The proposed approach is applied to both the original SG model and the SG Negative Sampling model to clarify the idea of using information criteria. Additionally, as the original SNML suffers from computational disadvantages, we introduce novel heuristics for its efficient computation. Moreover, we empirically demonstrate that SNML outperforms both BIC and AIC. In comparison with other evaluation methods for word embedding, the dimensionality selected by SNML is significantly closer to the optimal dimensionality obtained by word analogy or word similarity tasks.

</details>

<details>

<summary>2020-08-25 08:37:53 - Posterior contraction for empirical Bayesian approach to inverse problems under non-diagonal assumption</summary>

- *Junxiong Jia, Jigen Peng, Jinghuai Gao*

- `1810.02221v2` - [abs](http://arxiv.org/abs/1810.02221v2) - [pdf](http://arxiv.org/pdf/1810.02221v2)

> We investigate an empirical Bayesian nonparametric approach to a family of linear inverse problems with Gaussian prior and Gaussian noise. We consider a class of Gaussian prior probability measures with covariance operator indexed by a hyperparameter that quantifies regularity. By introducing two auxiliary problems, we construct an empirical Bayes method and prove that this method can automatically select the hyperparameter. In addition, we show that this adaptive Bayes procedure provides optimal contraction rates up to a slowly varying term and an arbitrarily small constant, without knowledge about the regularity index. Our method needs not the prior covariance, noise covariance and forward operator have a common basis in their singular value decomposition, enlarging the application range compared with the existing results.

</details>

<details>

<summary>2020-08-25 17:12:38 - How Ominous is the Future Global Warming Premonition?</summary>

- *Debashis Chatterjee, Sourabh Bhattacharya*

- `2008.11175v1` - [abs](http://arxiv.org/abs/2008.11175v1) - [pdf](http://arxiv.org/pdf/2008.11175v1)

> Global warming, the phenomenon of increasing global average temperature in the recent decades, is receiving wide attention due to its very significant adverse effects on climate. Whether global warming will continue even in the future, is a question that is most important to investigate. In this regard, the so-called general circulation models (GCMs) have attempted to project the future climate, and nearly all of them exhibit alarming rates of global temperature rise in the future.   Although global warming in the current time frame is undeniable, it is important to assess the validity of the future predictions of the GCMs. In this article, we attempt such a study using our recently-developed Bayesian multiple testing paradigm for model selection in inverse regression problems. The model we assume for the global temperature time series is based on Gaussian process emulation of the black box scenario, realistically treating the dynamic evolution of the time series as unknown.   We apply our ideas to datasets available from the Intergovernmental Panel on Climate Change (IPCC) website. The best GCM models selected by our method under different assumptions on future climate change scenarios do not convincingly support the present global warming pattern when only the future predictions are considered known. Using our Gaussian process idea, we also forecast the future temperature time series given the current one. Interestingly, our results do not support drastic future global warming predicted by almost all the GCM models.

</details>

<details>

<summary>2020-08-26 10:07:54 - Variational Metric Scaling for Metric-Based Meta-Learning</summary>

- *Jiaxin Chen, Li-Ming Zhan, Xiao-Ming Wu, Fu-lai Chung*

- `1912.11809v2` - [abs](http://arxiv.org/abs/1912.11809v2) - [pdf](http://arxiv.org/pdf/1912.11809v2)

> Metric-based meta-learning has attracted a lot of attention due to its effectiveness and efficiency in few-shot learning. Recent studies show that metric scaling plays a crucial role in the performance of metric-based meta-learning algorithms. However, there still lacks a principled method for learning the metric scaling parameter automatically. In this paper, we recast metric-based meta-learning from a Bayesian perspective and develop a variational metric scaling framework for learning a proper metric scaling parameter. Firstly, we propose a stochastic variational method to learn a single global scaling parameter. To better fit the embedding space to a given data distribution, we extend our method to learn a dimensional scaling vector to transform the embedding space. Furthermore, to learn task-specific embeddings, we generate task-dependent dimensional scaling vectors with amortized variational inference. Our method is end-to-end without any pre-training and can be used as a simple plug-and-play module for existing metric-based meta-algorithms. Experiments on mini-ImageNet show that our methods can be used to consistently improve the performance of existing metric-based meta-algorithms including prototypical networks and TADAM. The source code can be downloaded from https://github.com/jiaxinchen666/variational-scaling.

</details>

<details>

<summary>2020-08-26 13:38:21 - Combining Shrinkage and Sparsity in Conjugate Vector Autoregressive Models</summary>

- *Niko Hauzenberger, Florian Huber, Luca Onorante*

- `2002.08760v2` - [abs](http://arxiv.org/abs/2002.08760v2) - [pdf](http://arxiv.org/pdf/2002.08760v2)

> Conjugate priors allow for fast inference in large dimensional vector autoregressive (VAR) models but, at the same time, introduce the restriction that each equation features the same set of explanatory variables. This paper proposes a straightforward means of post-processing posterior estimates of a conjugate Bayesian VAR to effectively perform equation-specific covariate selection. Compared to existing techniques using shrinkage alone, our approach combines shrinkage and sparsity in both the VAR coefficients and the error variance-covariance matrices, greatly reducing estimation uncertainty in large dimensions while maintaining computational tractability. We illustrate our approach by means of two applications. The first application uses synthetic data to investigate the properties of the model across different data-generating processes, the second application analyzes the predictive gains from sparsification in a forecasting exercise for US data.

</details>

<details>

<summary>2020-08-26 16:28:48 - How to tune the RBF SVM hyperparameters?: An empirical evaluation of 18 search algorithms</summary>

- *Jacques Wainer, Pablo Fonseca*

- `2008.11655v1` - [abs](http://arxiv.org/abs/2008.11655v1) - [pdf](http://arxiv.org/pdf/2008.11655v1)

> SVM with an RBF kernel is usually one of the best classification algorithms for most data sets, but it is important to tune the two hyperparameters $C$ and $\gamma$ to the data itself. In general, the selection of the hyperparameters is a non-convex optimization problem and thus many algorithms have been proposed to solve it, among them: grid search, random search, Bayesian optimization, simulated annealing, particle swarm optimization, Nelder Mead, and others. There have also been proposals to decouple the selection of $\gamma$ and $C$. We empirically compare 18 of these proposed search algorithms (with different parameterizations for a total of 47 combinations) on 115 real-life binary data sets. We find (among other things) that trees of Parzen estimators and particle swarm optimization select better hyperparameters with only a slight increase in computation time with respect to a grid search with the same number of evaluations. We also find that spending too much computational effort searching the hyperparameters will not likely result in better performance for future data and that there are no significant differences among the different procedures to select the best set of hyperparameters when more than one is found by the search algorithms.

</details>

<details>

<summary>2020-08-26 18:56:07 - The polar-generalized normal distribution</summary>

- *Masoud Faridi, Majid Jafari Khaledi*

- `2008.11765v1` - [abs](http://arxiv.org/abs/2008.11765v1) - [pdf](http://arxiv.org/pdf/2008.11765v1)

> This paper introduces an extension to the normal distribution through the polar method to capture bimodality and asymmetry, which are often observed characteristics of empirical data. The later two features are entirely controlled by a separate scalar parameter. Explicit expressions for the cumulative distribution function, the density function and the moments were derived. The stochastic representation of the distribution facilitates implementing Bayesian estimation via the Markov chain Monte Carlo methods. Some real-life data as well as simulated data are analyzed to illustrate the flexibility of the distribution for modeling asymmetric bimodality.

</details>

<details>

<summary>2020-08-26 22:50:38 - Non-Gaussian processes and neural networks at finite widths</summary>

- *Sho Yaida*

- `1910.00019v2` - [abs](http://arxiv.org/abs/1910.00019v2) - [pdf](http://arxiv.org/pdf/1910.00019v2)

> Gaussian processes are ubiquitous in nature and engineering. A case in point is a class of neural networks in the infinite-width limit, whose priors correspond to Gaussian processes. Here we perturbatively extend this correspondence to finite-width neural networks, yielding non-Gaussian processes as priors. The methodology developed herein allows us to track the flow of preactivation distributions by progressively integrating out random variables from lower to higher layers, reminiscent of renormalization-group flow. We further develop a perturbative procedure to perform Bayesian inference with weakly non-Gaussian priors.

</details>

<details>

<summary>2020-08-27 19:31:39 - An Algorithm for Distributed Bayesian Inference in Generalized Linear Models</summary>

- *Nariankadu D. Shyamalkumar, Sanvesh Srivastava*

- `1911.07947v2` - [abs](http://arxiv.org/abs/1911.07947v2) - [pdf](http://arxiv.org/pdf/1911.07947v2)

> Monte Carlo algorithms, such as Markov chain Monte Carlo (MCMC) and Hamiltonian Monte Carlo (HMC), are routinely used for Bayesian inference in generalized linear models; however, these algorithms are prohibitively slow in massive data settings because they require multiple passes through the full data in every iteration. Addressing this problem, we develop a scalable extension of these algorithms using the divide-and-conquer (D&C) technique that divides the data into a sufficiently large number of subsets, draws parameters in parallel on the subsets using a \textit{powered} likelihood, and produces Monte Carlo draws of the parameter by combining parameter draws obtained from each subset. These combined parameter draws play the role of draws from the original sampling algorithm. Our main contributions are two-fold. First, we demonstrate through diverse simulated and real data analyses that our distributed algorithm is comparable to the current state-of-the-art D&C algorithm in terms of statistical accuracy and computational efficiency. Second, providing theoretical support for our empirical observations, we identify regularity assumptions under which the proposed algorithm leads to asymptotically optimal inference. We illustrate our methodology through normal linear and logistic regressions, where parts of our D&C algorithm are analytically tractable.

</details>

<details>

<summary>2020-08-28 06:57:10 - BLOB : A Probabilistic Model for Recommendation that Combines Organic and Bandit Signals</summary>

- *Otmane Sakhi, Stephen Bonner, David Rohde, Flavian Vasile*

- `2008.12504v1` - [abs](http://arxiv.org/abs/2008.12504v1) - [pdf](http://arxiv.org/pdf/2008.12504v1)

> A common task for recommender systems is to build a pro le of the interests of a user from items in their browsing history and later to recommend items to the user from the same catalog. The users' behavior consists of two parts: the sequence of items that they viewed without intervention (the organic part) and the sequences of items recommended to them and their outcome (the bandit part). In this paper, we propose Bayesian Latent Organic Bandit model (BLOB), a probabilistic approach to combine the 'or-ganic' and 'bandit' signals in order to improve the estimation of recommendation quality. The bandit signal is valuable as it gives direct feedback of recommendation performance, but the signal quality is very uneven, as it is highly concentrated on the recommendations deemed optimal by the past version of the recom-mender system. In contrast, the organic signal is typically strong and covers most items, but is not always relevant to the recommendation task. In order to leverage the organic signal to e ciently learn the bandit signal in a Bayesian model we identify three fundamental types of distances, namely action-history, action-action and history-history distances. We implement a scalable approximation of the full model using variational auto-encoders and the local re-paramerization trick. We show using extensive simulation studies that our method out-performs or matches the value of both state-of-the-art organic-based recommendation algorithms, and of bandit-based methods (both value and policy-based) both in organic and bandit-rich environments.

</details>

<details>

<summary>2020-08-28 07:19:24 - Trading Convergence Rate with Computational Budget in High Dimensional Bayesian Optimization</summary>

- *Hung Tran-The, Sunil Gupta, Santu Rana, Svetha Venkatesh*

- `1911.11950v2` - [abs](http://arxiv.org/abs/1911.11950v2) - [pdf](http://arxiv.org/pdf/1911.11950v2)

> Scaling Bayesian optimisation (BO) to high-dimensional search spaces is a active and open research problems particularly when no assumptions are made on function structure. The main reason is that at each iteration, BO requires to find global maximisation of acquisition function, which itself is a non-convex optimization problem in the original search space. With growing dimensions, the computational budget for this maximisation gets increasingly short leading to inaccurate solution of the maximisation. This inaccuracy adversely affects both the convergence and the efficiency of BO. We propose a novel approach where the acquisition function only requires maximisation on a discrete set of low dimensional subspaces embedded in the original high-dimensional search space. Our method is free of any low dimensional structure assumption on the function unlike many recent high-dimensional BO methods. Optimising acquisition function in low dimensional subspaces allows our method to obtain accurate solutions within limited computational budget. We show that in spite of this convenience, our algorithm remains convergent. In particular, cumulative regret of our algorithm only grows sub-linearly with the number of iterations. More importantly, as evident from our regret bounds, our algorithm provides a way to trade the convergence rate with the number of subspaces used in the optimisation. Finally, when the number of subspaces is "sufficiently large", our algorithm's cumulative regret is at most $\mathcal{O}^{*}(\sqrt{T\gamma_T})$ as opposed to $\mathcal{O}^{*}(\sqrt{DT\gamma_T})$ for the GP-UCB of Srinivas et al. (2012), reducing a crucial factor $\sqrt{D}$ where $D$ being the dimensional number of input space.

</details>

<details>

<summary>2020-08-28 15:23:54 - A Bayesian Statistics Course for Undergraduates: Bayesian Thinking, Computing, and Research</summary>

- *Jingchen Hu*

- `1910.05818v3` - [abs](http://arxiv.org/abs/1910.05818v3) - [pdf](http://arxiv.org/pdf/1910.05818v3)

> We propose a semester-long Bayesian statistics course for undergraduate students with calculus and probability background. We cultivate students' Bayesian thinking with Bayesian methods applied to real data problems. We leverage modern Bayesian computing techniques not only for implementing Bayesian methods, but also to deepen students' understanding of the methods. Collaborative case studies further enrich students' learning and provide experience to solve open-ended applied problems. The course has an emphasis on undergraduate research, where accessible academic journal articles are read, discussed, and critiqued in class. With increased confidence and familiarity, students take the challenge of reading, implementing, and sometimes extending methods in journal articles for their course projects.

</details>

<details>

<summary>2020-08-28 17:52:46 - Deep Historical Borrowing Framework to Prospectively and Simultaneously Synthesize Control Information in Confirmatory Clinical Trials with Multiple Endpoints</summary>

- *Tianyu Zhan, Yiwang Zhou, Ziqian Geng, Yihua Gu, Jian Kang, Li Wang, Xiaohong Huang, Elizabeth H. Slate*

- `2008.12774v1` - [abs](http://arxiv.org/abs/2008.12774v1) - [pdf](http://arxiv.org/pdf/2008.12774v1)

> In current clinical trial development, historical information is receiving more attention as providing value beyond sample size calculation. Meta-analytic-predictive (MAP) priors and robust MAP priors have been proposed for prospectively borrowing historical data on a single endpoint. To simultaneously synthesize control information from multiple endpoints in confirmatory clinical trials, we propose to approximate posterior probabilities from a Bayesian hierarchical model and estimate critical values by deep learning to construct pre-specified decision functions before the trial conduct. Simulation studies and a case study demonstrate that our method additionally preserves power, and has a satisfactory performance under prior-data conflict.

</details>

<details>

<summary>2020-08-29 03:36:20 - hIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems Governed by PDEs; Part I: Deterministic Inversion and Linearized Bayesian Inference</summary>

- *Umberto Villa, Noemi Petra, Omar Ghattas*

- `1909.03948v2` - [abs](http://arxiv.org/abs/1909.03948v2) - [pdf](http://arxiv.org/pdf/1909.03948v2)

> We present an extensible software framework, hIPPYlib, for solution of large-scale deterministic and Bayesian inverse problems governed by partial differential equations (PDEs) with infinite-dimensional parameter fields (which are high-dimensional after discretization). hIPPYlib overcomes the prohibitive nature of Bayesian inversion for this class of problems by implementing state-of-the-art scalable algorithms for PDE-based inverse problems that exploit the structure of the underlying operators, notably the Hessian of the log-posterior. The key property of the algorithms implemented in hIPPYlib is that the solution of the deterministic and linearized Bayesian inverse problem is computed at a cost, measured in linearized forward PDE solves, that is independent of the parameter dimension. The mean of the posterior is approximated by the MAP point, which is found by minimizing the negative log-posterior. This deterministic nonlinear least-squares optimization problem is solved with an inexact matrix-free Newton-CG method. The posterior covariance is approximated by the inverse of the Hessian of the negative log posterior evaluated at the MAP point. This Gaussian approximation is exact when the parameter-to-observable map is linear; otherwise, its logarithm agrees to two derivatives with the log-posterior at the MAP point, and thus it can serve as a proposal for Hessian-based MCMC methods. The construction of the posterior covariance is made tractable by invoking a low-rank approximation of the Hessian of the log-likelihood. Scalable tools for sample generation are also implemented. hIPPYlib makes all of these advanced algorithms easily accessible to domain scientists and provides an environment that expedites the development of new algorithms. hIPPYlib is also a teaching tool to educate researchers and practitioners who are new to inverse problems and the Bayesian inference framework.

</details>

<details>

<summary>2020-08-29 07:15:32 - Privacy-preserving Federated Bayesian Learning of a Generative Model for Imbalanced Classification of Clinical Data</summary>

- *Seok-Ju Hahn, Junghye Lee*

- `1910.08489v3` - [abs](http://arxiv.org/abs/1910.08489v3) - [pdf](http://arxiv.org/pdf/1910.08489v3)

> In clinical research, the lack of events of interest often necessitates imbalanced learning. One approach to resolve this obstacle is data integration or sharing, but due to privacy concerns neither is practical. Therefore, there is an increasing demand for a platform on which an analysis can be performed in a federated environment while maintaining privacy. However, it is quite challenging to develop a federated learning algorithm that can address both privacy-preserving and class imbalanced issues. In this study, we introduce a federated generative model learning platform for generating samples in a data-distributed environment while preserving privacy. We specifically propose approximate Bayesian computation-based Gaussian Mixture Model called 'Federated ABC-GMM', which can oversample data in a minor class by estimating the posterior distribution of model parameters across institutions in a privacy-preserving manner. PhysioNet2012, a dataset for prediction of mortality of patients in an Intensive Care Unit (ICU), was used to verify the performance of the proposed method. Experimental results show that our method boosts classification performance in terms of F1 score up to nearly an ideal situation. It is believed that the proposed method can be a novel alternative to solving class imbalance problems.

</details>

<details>

<summary>2020-08-29 07:19:44 - GRAFFL: Gradient-free Federated Learning of a Bayesian Generative Model</summary>

- *Seok-Ju Hahn, Junghye Lee*

- `2008.12925v1` - [abs](http://arxiv.org/abs/2008.12925v1) - [pdf](http://arxiv.org/pdf/2008.12925v1)

> Federated learning platforms are gaining popularity. One of the major benefits is to mitigate the privacy risks as the learning of algorithms can be achieved without collecting or sharing data. While federated learning (i.e., many based on stochastic gradient algorithms) has shown great promise, there are still many challenging problems in protecting privacy, especially during the process of gradients update and exchange. This paper presents the first gradient-free federated learning framework called GRAFFL for learning a Bayesian generative model based on approximate Bayesian computation. Unlike conventional federated learning algorithms based on gradients, our framework does not require to disassemble a model (i.e., to linear components) or to perturb data (or encryption of data for aggregation) to preserve privacy. Instead, this framework uses implicit information derived from each participating institution to learn posterior distributions of parameters. The implicit information is summary statistics derived from SuffiAE that is a neural network developed in this study to create compressed and linearly separable representations thereby protecting sensitive information from leakage. As a sufficient dimensionality reduction technique, this is proved to provide sufficient summary statistics. We propose the GRAFFL-based Bayesian Gaussian mixture model to serve as a proof-of-concept of the framework. Using several datasets, we demonstrated the feasibility and usefulness of our model in terms of privacy protection and prediction performance (i.e., close to an ideal setting). The trained model as a quasi-global model can generate informative samples involving information from other institutions and enhances data analysis of each institution.

</details>

<details>

<summary>2020-08-29 19:08:39 - Bayesian Updating and Sequential Testing: Overcoming Inferential Limitations of Screening Tests</summary>

- *Jacques Balayla*

- `2006.11641v4` - [abs](http://arxiv.org/abs/2006.11641v4) - [pdf](http://arxiv.org/pdf/2006.11641v4)

> Bayes' Theorem confers inherent limitations on the accuracy of screening tests as a function of disease prevalence. We have shown in previous work that a testing system can tolerate significant drops in prevalence, up until a certain well-defined point known as the $prevalence$ $threshold$, below which the reliability of a positive screening test drops precipitously. Herein, we establish a mathematical model to determine whether sequential testing overcomes the aforementioned Bayesian limitations and thus improves the reliability of screening tests. We show that for a desired positive predictive value of $\rho$ that approaches $k$, the number of positive test iterations $n_i$ needed is: $ n_i =\lim_{\rho \to k}\left\lceil\frac{ln\left[\frac{\rho(\phi-1)}{\phi(\rho-1)}\right]}{ln\left[\frac{a}{1-b}\right]}\right\rceil$ where $n_i$ = number of testing iterations necessary to achieve $\rho$, the desired positive predictive value, a = sensitivity, b = specificity, $\phi$ = disease prevalence and $k$ = constant. Based on the aforementioned derivation, we provide reference tables for the number of test iterations needed to obtain a $\rho(\phi)$ of 50, 75, 95 and 99$\%$ as a function of various levels of sensitivity, specificity and disease prevalence.

</details>

<details>

<summary>2020-08-29 19:16:43 - Loss convergence in a causal Bayesian neural network of retail firm performance</summary>

- *F. Trevor Rogers*

- `2008.13038v1` - [abs](http://arxiv.org/abs/2008.13038v1) - [pdf](http://arxiv.org/pdf/2008.13038v1)

> We extend the empirical results from the structural equation model (SEM) published in the paper Assortment Planning for Retail Buying, Retail Store Operations, and Firm Performance [1] by implementing the directed acyclic graph as a causal Bayesian neural network. Neural network convergence is shown to improve with the removal of the node with the weakest SEM path when variational inference is provided by perturbing weights with Flipout layers, while results from perturbing weights at the output with the Vadam optimizer are inconclusive.

</details>

<details>

<summary>2020-08-30 12:40:59 - Levels and trends in the sex ratio at birth in seven provinces of Nepal between 1980 and 2016 with probabilistic projections to 2050: a Bayesian modeling approach</summary>

- *Fengqing Chao, Samir KC, Hernando Ombao*

- `2007.00437v2` - [abs](http://arxiv.org/abs/2007.00437v2) - [pdf](http://arxiv.org/pdf/2007.00437v2)

> The sex ratio at birth (SRB; ratio of male to female births) in Nepal has been reported without imbalance on the national level. However, the national SRB could mask the disparity within the country. Given the demographic and cultural heterogeneities in Nepal, it is crucial to model Nepal SRB on the subnational level. Prior studies on subnational SRB in Nepal are mostly based on reporting observed values from surveys and census, and no study has provided probabilistic projections. We aim to estimate and project SRB for the seven provinces of Nepal from 1980 to 2050 using a Bayesian modeling approach. We compiled an extensive database on provincial SRB of Nepal, consisting 2001, 2006, 2011, and 2016 Nepal Demographic and Health Surveys and 2011 Census. We adopted a Bayesian hierarchical time series model to estimate and project the provincial SRB, with a focus on modelling the potential SRB imbalance. In 2016, the highest SRB is estimated in Province 5 at 1.102 with a 95% credible interval (1.044, 1.127) and the lowest SRB is in Province 2 at 1.053 (1.035, 1.109). The SRB imbalance probabilities in all provinces are generally low and vary from 16% in Province 2 to 81% in Province 5. SRB imbalances are estimated to have begun at the earliest in 2001 in Province 5 with a 95% credible interval (1992, 2022) and the latest in 2017 (1998, 2040) in Province 2. We project SRB in all provinces to begin converging back to the national baseline in the mid-2030s. Our findings imply that the majority of provinces in Nepal have low risks of SRB imbalance for the period 1980-2016. However, we identify a few provinces with higher probabilities of having SRB inflation. The projected SRB is an important illustration of potential future prenatal sex discrimination and shows the need to monitor SRB in provinces with higher possibilities of SRB imbalance.

</details>

<details>

<summary>2020-08-30 13:55:01 - Bayesian High-dimensional Semi-parametric Inference beyond sub-Gaussian Errors</summary>

- *Kyoungjae Lee, Minwoo Chae, Lizhen Lin*

- `2008.13174v1` - [abs](http://arxiv.org/abs/2008.13174v1) - [pdf](http://arxiv.org/pdf/2008.13174v1)

> We consider a sparse linear regression model with unknown symmetric error under the high-dimensional setting. The true error distribution is assumed to belong to the locally $\beta$-H\"{o}lder class with an exponentially decreasing tail, which does not need to be sub-Gaussian. We obtain posterior convergence rates of the regression coefficient and the error density, which are nearly optimal and adaptive to the unknown sparsity level. Furthermore, we derive the semi-parametric Bernstein-von Mises (BvM) theorem to characterize asymptotic shape of the marginal posterior for regression coefficients. Under the sub-Gaussianity assumption on the true score function, strong model selection consistency for regression coefficients are also obtained, which eventually asserts the frequentist's validity of credible sets.

</details>

<details>

<summary>2020-08-31 04:18:39 - Coarsened mixtures of hierarchical skew normal kernels for flow cytometry analyses</summary>

- *Shai Gorsky, Cliburn Chan, Li Ma*

- `2001.06451v2` - [abs](http://arxiv.org/abs/2001.06451v2) - [pdf](http://arxiv.org/pdf/2001.06451v2)

> Flow cytometry (FCM) is the standard multi-parameter assay for measuring single cell phenotype and functionality. It is commonly used for quantifying the relative frequencies of cell subsets in blood and disaggregated tissues. A typical analysis of FCM data involves cell classification---that is, the identification of cell subgroups in the sample---and comparisons of the cell subgroups across samples or conditions. While modern experiments often necessitate the collection and processing of samples in multiple batches, analysis of FCM data across batches is challenging because differences across samples may occur due to either true biological variation or technical reasons such as antibody lot effects or instrument optics across batches. Thus a critical step in comparative analyses of multi-sample FCM data---yet missing in existing automated methods for analyzing such data---is cross-sample calibration, whose goal is to align corresponding cell subsets across multiple samples in the presence of technical variations, so that biological variations can be meaningfully compared. We introduce a Bayesian nonparametric hierarchical modeling approach for accomplishing both calibration and cell classification simultaneously in a unified probabilistic manner. Three important features of our method make it particularly effective for analyzing multi-sample FCM data: a nonparametric mixture avoids prespecifying the number of cell clusters; a hierarchical skew normal kernel that allows flexibility in the shapes of the cell subsets and cross-sample variation in their locations; and finally the "coarsening" strategy makes inference robust to departures from the model such as heavy-tailness not captured by the skew normal kernels. We demonstrate the merits of our approach in simulated examples and carry out a case study in the analysis of two multi-sample FCM data sets.

</details>

<details>

<summary>2020-08-31 06:55:30 - Compound vectors of subordinators and their associated positive Lévy copulas</summary>

- *Alan Riva Palacio, Fabrizio Leisen*

- `1909.12112v3` - [abs](http://arxiv.org/abs/1909.12112v3) - [pdf](http://arxiv.org/pdf/1909.12112v3)

> L\'evy copulas are an important tool which can be used to build dependent L\'evy processes. In a classical setting, they have been used to model financial applications. In a Bayesian framework they have been employed to introduce dependent nonparametric priors which allow to model heterogeneous data. This paper focuses on introducing a new class of L\'evy copulas based on a class of subordinators recently appeared in the literature, called \textit{Compound Random Measures}. The well-known Clayton L\'evy copula is a special case of this new class. Furthermore, we provide some novel results about the underlying vector of subordinators such as a series representation and relevant moments. The article concludes with an application to a Danish fire dataset.

</details>

<details>

<summary>2020-08-31 10:43:38 - Sparse Portfolio selection via Bayesian Multiple testing</summary>

- *Sourish Das, Rituparna Sen*

- `1705.01407v4` - [abs](http://arxiv.org/abs/1705.01407v4) - [pdf](http://arxiv.org/pdf/1705.01407v4)

> We presented Bayesian portfolio selection strategy, via the $k$ factor asset pricing model. If the market is information efficient, the proposed strategy will mimic the market; otherwise, the strategy will outperform the market. The strategy depends on the selection of a portfolio via Bayesian multiple testing methodologies. We present the "discrete-mixture prior" model and the "hierarchical Bayes model with horseshoe prior." We define the Oracle set and prove that asymptotically the Bayes rule attains the risk of Bayes Oracle up to $O(1)$. Our proposed Bayes Oracle test guarantees statistical power by providing the upper bound of the type-II error. Simulation study indicates that the proposed Bayes oracle test is suitable for the efficient market with few stocks inefficiently priced. However, as the model becomes dense, i.e., the market is highly inefficient, one should not use the Bayes oracle test. The statistical power of the Bayes Oracle portfolio is uniformly better for the $k$-factor model ($k>1$) than the one factor CAPM. We present the empirical study, where we considered the 500 constituent stocks of S\&P 500 from the New York Stock Exchange (NYSE), and S\&P 500 index as the benchmark for thirteen years from the year 2006 to 2018. We showed the out-sample risk and return performance of the four different portfolio selection strategies and compared with the S\&P 500 index as the benchmark market index. Empirical results indicate that it is possible to propose a strategy which can outperform the market.

</details>

<details>

<summary>2020-08-31 19:04:34 - Variable selection in social-environmental data: Sparse regression and tree ensemble machine learning approaches</summary>

- *Elizabeth Handorf, Yinuo Yin, Michael Slifker, Shannon Lynch*

- `2009.00065v1` - [abs](http://arxiv.org/abs/2009.00065v1) - [pdf](http://arxiv.org/pdf/2009.00065v1)

> Objective: Social-environmental data obtained from the U.S. Census is an important resource for understanding health disparities, but rarely is the full dataset utilized for analysis. A barrier to incorporating the full data is a lack of solid recommendations for variable selection, with researchers often hand-selecting a few variables. Thus, we evaluated the ability of empirical machine learning approaches to identify social-environmental factors having a true association with a health outcome.   Materials and Methods: We compared several popular machine learning methods, including penalized regressions (e.g. lasso, elastic net), and tree ensemble methods. Via simulation, we assessed the methods' ability to identify census variables truly associated with binary and continuous outcomes while minimizing false positive results (10 true associations, 1,000 total variables). We applied the most promising method to the full census data (p=14,663 variables) linked to prostate cancer registry data (n=76,186 cases) to identify social-environmental factors associated with advanced prostate cancer.   Results: In simulations, we found that elastic net identified many true-positive variables, while lasso provided good control of false positives. Using a combined measure of accuracy, hierarchical clustering based on Spearman's correlation with sparse group lasso regression performed the best overall. Bayesian Adaptive Regression Trees outperformed other tree ensemble methods, but not the sparse group lasso. In the full dataset, the sparse group lasso successfully identified a subset of variables, three of which replicated earlier findings.   Discussion: This analysis demonstrated the potential of empirical machine learning approaches to identify a small subset of census variables having a true association with the outcome, and that replicate across empiric methods.

</details>

<details>

<summary>2020-08-31 19:33:54 - Extending Deep Knowledge Tracing: Inferring Interpretable Knowledge and Predicting Post-System Performance</summary>

- *Richard Scruggs, Ryan S. Baker, Bruce M. McLaren*

- `1910.12597v2` - [abs](http://arxiv.org/abs/1910.12597v2) - [pdf](http://arxiv.org/pdf/1910.12597v2)

> Recent student knowledge modeling algorithms such as Deep Knowledge Tracing (DKT) and Dynamic Key-Value Memory Networks (DKVMN) have been shown to produce accurate predictions of problem correctness within the same learning system. However, these algorithms do not attempt to directly infer student knowledge. In this paper we present an extension to these algorithms to also infer knowledge. We apply this extension to DKT and DKVMN, resulting in knowledge estimates that correlate better with a posttest than knowledge estimates from Bayesian Knowledge Tracing (BKT), an algorithm designed to infer knowledge, and another classic algorithm, Performance Factors Analysis (PFA). We also apply our extension to correctness predictions from BKT and PFA, finding that knowledge estimates produced with it correlate better with the posttest than BKT and PFA's standard knowledge estimates. These findings are significant since the primary aim of education is to prepare students for later experiences outside of the immediate learning activity.

</details>

<details>

<summary>2020-08-31 20:21:27 - Random Forest (RF) Kernel for Regression, Classification and Survival</summary>

- *Dai Feng, Richard Baumgartner*

- `2009.00089v1` - [abs](http://arxiv.org/abs/2009.00089v1) - [pdf](http://arxiv.org/pdf/2009.00089v1)

> Breiman's random forest (RF) can be interpreted as an implicit kernel generator,where the ensuing proximity matrix represents the data-driven RF kernel. Kernel perspective on the RF has been used to develop a principled framework for theoretical investigation of its statistical properties. However, practical utility of the links between kernels and the RF has not been widely explored and systematically evaluated.Focus of our work is investigation of the interplay between kernel methods and the RF. We elucidate the performance and properties of the data driven RF kernels used by regularized linear models in a comprehensive simulation study comprising of continuous, binary and survival targets. We show that for continuous and survival targets, the RF kernels are competitive to RF in higher dimensional scenarios with larger number of noisy features. For the binary target, the RF kernel and RF exhibit comparable performance. As the RF kernel asymptotically converges to the Laplace kernel, we included it in our evaluation. For most simulation setups, the RF and RFkernel outperformed the Laplace kernel. Nevertheless, in some cases the Laplace kernel was competitive, showing its potential value for applications. We also provide the results from real life data sets for the regression, classification and survival to illustrate how these insights may be leveraged in practice.Finally, we discuss further extensions of the RF kernels in the context of interpretable prototype and landmarking classification, regression and survival. We outline future line of research for kernels furnished by Bayesian counterparts of the RF.

</details>


## 2020-09

<details>

<summary>2020-09-01 02:53:09 - Quantum-Inspired Hamiltonian Monte Carlo for Bayesian Sampling</summary>

- *Ziming Liu, Zheng Zhang*

- `1912.01937v3` - [abs](http://arxiv.org/abs/1912.01937v3) - [pdf](http://arxiv.org/pdf/1912.01937v3)

> Hamiltonian Monte Carlo (HMC) is an efficient Bayesian sampling method that can make distant proposals in the parameter space by simulating a Hamiltonian dynamical system. Despite its popularity in machine learning and data science, HMC is inefficient to sample from spiky and multimodal distributions. Motivated by the energy-time uncertainty relation from quantum mechanics, we propose a Quantum-Inspired Hamiltonian Monte Carlo algorithm (QHMC). This algorithm allows a particle to have a random mass matrix with a probability distribution rather than a fixed mass. We prove the convergence property of QHMC and further show why such a random mass can improve the performance when we sample a broad class of distributions. In order to handle the big training data sets in large-scale machine learning, we develop a stochastic gradient version of QHMC using Nos{\'e}-Hoover thermostat called QSGNHT, and we also provide theoretical justifications about its steady-state distributions. Finally in the experiments, we demonstrate the effectiveness of QHMC and QSGNHT on synthetic examples, bridge regression, image denoising and neural network pruning. The proposed QHMC and QSGNHT can indeed achieve much more stable and accurate sampling results on the test cases.

</details>

<details>

<summary>2020-09-01 04:19:29 - Invited Discussion of "A Unified Framework for De-Duplication and Population Size Estimation"</summary>

- *Jared S. Murray*

- `2009.00217v1` - [abs](http://arxiv.org/abs/2009.00217v1) - [pdf](http://arxiv.org/pdf/2009.00217v1)

> Invited Discussion of "A Unified Framework for De-Duplication and Population Size Estimation", published in Bayesian Analysis. My discussion focuses on two main themes: Providing a more nuanced picture of the costs and benefits of joint models for record linkage and the "downstream task" (i.e. whatever we might want to do with the linked and de-duplicated files), and how we should measure performance.

</details>

<details>

<summary>2020-09-01 04:53:56 - Uncertainty aware Search Framework for Multi-Objective Bayesian Optimization with Constraints</summary>

- *Syrine Belakaria, Aryan Deshwal, Janardhan Rao Doppa*

- `2008.07029v2` - [abs](http://arxiv.org/abs/2008.07029v2) - [pdf](http://arxiv.org/pdf/2008.07029v2)

> We consider the problem of constrained multi-objective (MO) blackbox optimization using expensive function evaluations, where the goal is to approximate the true Pareto set of solutions satisfying a set of constraints while minimizing the number of function evaluations. We propose a novel framework named Uncertainty-aware Search framework for Multi-Objective Optimization with Constraints (USeMOC) to efficiently select the sequence of inputs for evaluation to solve this problem. The selection method of USeMOC consists of solving a cheap constrained MO optimization problem via surrogate models of the true functions to identify the most promising candidates and picking the best candidate based on a measure of uncertainty. We applied this framework to optimize the design of a multi-output switched-capacitor voltage regulator via expensive simulations. Our experimental results show that USeMOC is able to achieve more than 90 % reduction in the number of simulations needed to uncover optimized circuits.

</details>

<details>

<summary>2020-09-01 08:08:46 - JointAI: Joint Analysis and Imputation of Incomplete Data in R</summary>

- *Nicole S. Erler, Dimitris Rizopoulos, Emmanuel M. E. H. Lesaffre*

- `1907.10867v3` - [abs](http://arxiv.org/abs/1907.10867v3) - [pdf](http://arxiv.org/pdf/1907.10867v3)

> Missing data occur in many types of studies and typically complicate the analysis. Multiple imputation, either using joint modelling or the more flexible fully conditional specification approach, are popular and work well in standard settings. In settings involving non-linear associations or interactions, however, incompatibility of the imputation model with the analysis model is an issue often resulting in bias. Similarly, complex outcomes such as longitudinal or survival outcomes cannot be adequately handled by standard implementations. In this paper, we introduce the R package JointAI, which utilizes the Bayesian framework to perform simultaneous analysis and imputation in regression models with incomplete covariates. Using a fully Bayesian joint modelling approach it overcomes the issue of uncongeniality while retaining the attractive flexibility of fully conditional specification multiple imputation by specifying the joint distribution of analysis and imputation models as a sequence of univariate models that can be adapted to the type of variable. JointAI provides functions for Bayesian inference with generalized linear and generalized linear mixed models and extensions thereof as well as survival models and joint models for longitudinal and survival data, that take arguments analogous to corresponding well known functions for the analysis of complete data from base R and other packages. Usage and features of JointAI are described and illustrated using various examples and the theoretical background is outlined.

</details>

<details>

<summary>2020-09-01 13:03:51 - Accounting for correlated horizontal pleiotropy in two-sample Mendelian randomization using correlated instrumental variants</summary>

- *Qing Cheng, Baoluo Sun, Yingcun Xia, Jin Liu*

- `2009.00399v1` - [abs](http://arxiv.org/abs/2009.00399v1) - [pdf](http://arxiv.org/pdf/2009.00399v1)

> Mendelian randomization (MR) is a powerful approach to examine the causal relationships between health risk factors and outcomes from observational studies. Due to the proliferation of genome-wide association studies (GWASs) and abundant fully accessible GWASs summary statistics, a variety of two-sample MR methods for summary data have been developed to either detect or account for horizontal pleiotropy, primarily based on the assumption that the effects of variants on exposure ({\gamma}) and horizontal pleiotropy ({\alpha}) are independent. This assumption is too strict and can be easily violated because of the correlated horizontal pleiotropy (CHP). To account for this CHP, we propose a Bayesian approach, MR-Corr2, that uses the orthogonal projection to reparameterize the bivariate normal distribution for {\gamma} and {\alpha}, and a spike-slab prior to mitigate the impact of CHP. We develop an efficient algorithm with paralleled Gibbs sampling. To demonstrate the advantages of MR-Corr2 over existing methods, we conducted comprehensive simulation studies to compare for both type-I error control and point estimates in various scenarios. By applying MR-Corr2 to study the relationships between pairs in two sets of complex traits, we did not identify the contradictory causal relationship between HDL-c and CAD. Moreover, the results provide a new perspective of the causal network among complex traits. The developed R package and code to reproduce all the results are available at https://github.com/QingCheng0218/MR.Corr2.

</details>

<details>

<summary>2020-09-01 14:28:31 - Adaptive Path Sampling in Metastable Posterior Distributions</summary>

- *Yuling Yao, Collin Cademartori, Aki Vehtari, Andrew Gelman*

- `2009.00471v1` - [abs](http://arxiv.org/abs/2009.00471v1) - [pdf](http://arxiv.org/pdf/2009.00471v1)

> The normalizing constant plays an important role in Bayesian computation, and there is a large literature on methods for computing or approximating normalizing constants that cannot be evaluated in closed form. When the normalizing constant varies by orders of magnitude, methods based on importance sampling can require many rounds of tuning. We present an improved approach using adaptive path sampling, iteratively reducing gaps between the base and target. Using this adaptive strategy, we develop two metastable sampling schemes. They are automated in Stan and require little tuning. For a multimodal posterior density, we equip simulated tempering with a continuous temperature. For a funnel-shaped entropic barrier, we adaptively increase mass in bottleneck regions to form an implicit divide-and-conquer. Both approaches empirically perform better than existing methods for sampling from metastable distributions, including higher accuracy and computation efficiency.

</details>

<details>

<summary>2020-09-01 16:43:07 - Application of the Cox Regression Model for Analysis of Railway Safety Performance</summary>

- *Hendrik Schäbe, Jens Braband*

- `2009.00558v1` - [abs](http://arxiv.org/abs/2009.00558v1) - [pdf](http://arxiv.org/pdf/2009.00558v1)

> The assessment of in-service safety performance is an important task, not only in railways. For example it is important to identify deviations early, in particular possible deterioration of safety performance, so that corrective actions can be applied early. On the other hand the assessment should be fair and objective and rely on sound and proven statistical methods. A popular means for this task is trend analysis. This paper defines a model for trend analysis and compares different approaches, e. g. classical and Bayes approaches, on real data. The examples show that in particular for small sample sizes, e. g. when railway operators shall be assessed, the Bayesian prior may influence the results significantly.

</details>

<details>

<summary>2020-09-01 16:53:29 - Performance-Agnostic Fusion of Probabilistic Classifier Outputs</summary>

- *Jordan F. Masakuna, Simukai W. Utete, Steve Kroon*

- `2009.00565v1` - [abs](http://arxiv.org/abs/2009.00565v1) - [pdf](http://arxiv.org/pdf/2009.00565v1)

> We propose a method for combining probabilistic outputs of classifiers to make a single consensus class prediction when no further information about the individual classifiers is available, beyond that they have been trained for the same task. The lack of relevant prior information rules out typical applications of Bayesian or Dempster-Shafer methods, and the default approach here would be methods based on the principle of indifference, such as the sum or product rule, which essentially weight all classifiers equally. In contrast, our approach considers the diversity between the outputs of the various classifiers, iteratively updating predictions based on their correspondence with other predictions until the predictions converge to a consensus decision. The intuition behind this approach is that classifiers trained for the same task should typically exhibit regularities in their outputs on a new task; the predictions of classifiers which differ significantly from those of others are thus given less credence using our approach. The approach implicitly assumes a symmetric loss function, in that the relative cost of various prediction errors are not taken into account. Performance of the model is demonstrated on different benchmark datasets. Our proposed method works well in situations where accuracy is the performance metric; however, it does not output calibrated probabilities, so it is not suitable in situations where such probabilities are required for further processing.

</details>

<details>

<summary>2020-09-01 20:51:53 - Checking individuals and sampling populations with imperfect tests</summary>

- *Giulio D'Agostini, Alfredo Esposito*

- `2009.04843v1` - [abs](http://arxiv.org/abs/2009.04843v1) - [pdf](http://arxiv.org/pdf/2009.04843v1)

> In the last months, due to the emergency of Covid-19, questions related to the fact of belonging or not to a particular class of individuals (`infected or not infected'), after being tagged as `positive' or `negative' by a test, have never been so popular. Similarly, there has been strong interest in estimating the proportion of a population expected to hold a given characteristics (`having or having had the virus'). Taking the cue from the many related discussions on the media, in addition to those to which we took part, we analyze these questions from a probabilistic perspective (`Bayesian'), considering several effects that play a role in evaluating the probabilities of interest. The resulting paper, written with didactic intent, is rather general and not strictly related to pandemics: the basic ideas of Bayesian inference are introduced and the uncertainties on the performances of the tests are treated using the metrological concepts of `systematics', and are propagated into the quantities of interest following the rules of probability theory; the separation of `statistical' and `systematic' contributions to the uncertainty on the inferred proportion of infectees allows to optimize the sample size; the role of `priors', often overlooked, is stressed, however recommending the use of `flat priors', since the resulting posterior distribution can be `reshaped' by an `informative prior' in a later step; details on the calculations are given, also deriving useful approximated formulae, the tough work being however done with the help of direct Monte Carlo simulations and Markov Chain Monte Carlo, implemented in R and JAGS (relevant code provided in appendix).

</details>

<details>

<summary>2020-09-02 10:01:12 - Evaluating structure learning algorithms with a balanced scoring function</summary>

- *Anthony C. Constantinou*

- `1905.12666v3` - [abs](http://arxiv.org/abs/1905.12666v3) - [pdf](http://arxiv.org/pdf/1905.12666v3)

> Several structure learning algorithms have been proposed towards discovering causal or Bayesian Network (BN) graphs. The validity of these algorithms tends to be evaluated by assessing the relationship between the learnt and the ground truth graph. However, there is no agreed scoring metric to determine this relationship. Moreover, this paper shows that some of the commonly used metrics tend to be biased in favour of graphs that minimise edges. While graphs that are less complex are desirable, some of the metrics favour underfitted graphs, thereby encouraging limited propagation of evidence. This paper proposes the Balanced Scoring Function (BSF) that eliminates this bias by adjusting the reward function based on the difficulty of discovering an edge, or no edge, proportional to their occurrence rate in the ground truth graph. The BSF score can be used in conjunction with other traditional metrics to provide an alternative and unbiased assessment about the capability of a structure learning algorithm in discovering causal or BN graphs.

</details>

<details>

<summary>2020-09-02 10:21:54 - Interim recruitment prediction for multi-centre clinical trials</summary>

- *Szymon Urbas, Chris Sherlock, Paul Metcalfe*

- `1910.07965v4` - [abs](http://arxiv.org/abs/1910.07965v4) - [pdf](http://arxiv.org/pdf/1910.07965v4)

> We introduce a general framework for monitoring, modelling, and predicting the recruitment to multi-centre clinical trials. The work is motivated by overly optimistic and narrow prediction intervals produced by existing time-homogeneous recruitment models for multi-centre recruitment. We first present two tests for detection of decay in recruitment rates, together with a power study. We then introduce a model based on the inhomogeneous Poisson process with monotonically decaying intensity, motivated by recruitment trends observed in oncology trials. The general form of the model permits adaptation to any parametric curve-shape. A general method for constructing sensible parameter priors is provided and Bayesian model averaging is used for making predictions which account for the uncertainty in both the parameters and the model. The validity of the method and its robustness to misspecification are tested using simulated datasets. The new methodology is then applied to oncology trial data, where we make interim accrual predictions, comparing them to those obtained by existing methods, and indicate where unexpected changes in the accrual pattern occur.

</details>

<details>

<summary>2020-09-03 16:03:48 - Sparse Additive Gaussian Process Regression</summary>

- *Hengrui Luo, Giovanni Nattino, Matthew T. Pratola*

- `1908.08864v2` - [abs](http://arxiv.org/abs/1908.08864v2) - [pdf](http://arxiv.org/pdf/1908.08864v2)

> In this paper we introduce a novel model for Gaussian process (GP) regression in the fully Bayesian setting. Motivated by the ideas of sparsification, localization and Bayesian additive modeling, our model is built around a recursive partitioning (RP) scheme. Within each RP partition, a sparse GP (SGP) regression model is fitted. A Bayesian additive framework then combines multiple layers of partitioned SGPs, capturing both global trends and local refinements with efficient computations. The model addresses both the problem of efficiency in fitting a full Gaussian process regression model and the problem of prediction performance associated with a single SGP. Our approach mitigates the issue of pseudo-input selection and avoids the need for complex inter-block correlations in existing methods. The crucial trade-off becomes choosing between many simpler local model components or fewer complex global model components, which the practitioner can sensibly tune. Implementation is via a Metropolis-Hasting Markov chain Monte-Carlo algorithm with Bayesian back-fitting. We compare our model against popular alternatives on simulated and real datasets, and find the performance is competitive, while the fully Bayesian procedure enables the quantification of model uncertainties.

</details>

<details>

<summary>2020-09-03 16:22:42 - Scalable and Efficient Comparison-based Search without Features</summary>

- *Daniyar Chumbalov, Lucas Maystre, Matthias Grossglauser*

- `1905.05049v3` - [abs](http://arxiv.org/abs/1905.05049v3) - [pdf](http://arxiv.org/pdf/1905.05049v3)

> We consider the problem of finding a target object $t$ using pairwise comparisons, by asking an oracle questions of the form \emph{"Which object from the pair $(i,j)$ is more similar to $t$?"}. Objects live in a space of latent features, from which the oracle generates noisy answers. First, we consider the {\em non-blind} setting where these features are accessible. We propose a new Bayesian comparison-based search algorithm with noisy answers; it has low computational complexity yet is efficient in the number of queries. We provide theoretical guarantees, deriving the form of the optimal query and proving almost sure convergence to the target $t$. Second, we consider the \emph{blind} setting, where the object features are hidden from the search algorithm. In this setting, we combine our search method and a new distributional triplet embedding algorithm into one scalable learning framework called \textsc{Learn2Search}. We show that the query complexity of our approach on two real-world datasets is on par with the non-blind setting, which is not achievable using any of the current state-of-the-art embedding methods. Finally, we demonstrate the efficacy of our framework by conducting an experiment with users searching for movie actors.

</details>

<details>

<summary>2020-09-03 19:22:29 - Bayesian Variable Selection for Non-Gaussian Responses: A Marginally Calibrated Copula Approach</summary>

- *Nadja Klein, Michael Stanley Smith*

- `1907.04530v2` - [abs](http://arxiv.org/abs/1907.04530v2) - [pdf](http://arxiv.org/pdf/1907.04530v2)

> We propose a new highly flexible and tractable Bayesian approach to undertake variable selection in non-Gaussian regression models. It uses a copula decomposition for the joint distribution of observations on the dependent variable. This allows the marginal distribution of the dependent variable to be calibrated accurately using a nonparametric or other estimator. The family of copulas employed are `implicit copulas' that are constructed from existing hierarchical Bayesian models widely used for variable selection, and we establish some of their properties. Even though the copulas are high-dimensional, they can be estimated efficiently and quickly using Markov chain Monte Carlo (MCMC). A simulation study shows that when the responses are non-Gaussian the approach selects variables more accurately than contemporary benchmarks. A real data example in the Web Appendix illustrates that accounting for even mild deviations from normality can lead to a substantial increase in accuracy. To illustrate the full potential of our approach we extend it to spatial variable selection for fMRI. Using real data, we show our method allows for voxel-specific marginal calibration of the magnetic resonance signal at over 6,000 voxels, leading to an increase in the quality of the activation maps.

</details>

<details>

<summary>2020-09-04 19:29:10 - Evaluating the relative contribution of data sources in a Bayesian analysis with the application of estimating the size of hard to reach populations</summary>

- *Jacob Parsons, Xiaoyue Niu, Le Bao*

- `2009.02372v1` - [abs](http://arxiv.org/abs/2009.02372v1) - [pdf](http://arxiv.org/pdf/2009.02372v1)

> When using multiple data sources in an analysis, it is important to understand the influence of each data source on the analysis and the consistency of the data sources with each other and the model. We suggest the use of a retrospective value of information framework in order to address such concerns. Value of information methods can be computationally difficult. We illustrate the use of computational methods that allow these methods to be applied even in relatively complicated settings.   In illustrating the proposed methods, we focus on an application in estimating the size of hard to reach populations. Specifically, we consider estimating the number of injection drug users in Ukraine by combining all available data sources spanning over half a decade and numerous sub-national areas in the Ukraine. This application is of interest to public health researchers as this hard to reach population that plays a large role in the spread of HIV. We apply a Bayesian hierarchical model and evaluate the contribution of each data source in terms of absolute influence, expected influence, and level of surprise. Finally we apply value of information methods to inform suggestions on future data collection.

</details>

<details>

<summary>2020-09-05 21:24:04 - Deep-n-Cheap: An Automated Search Framework for Low Complexity Deep Learning</summary>

- *Sourya Dey, Saikrishna C. Kanala, Keith M. Chugg, Peter A. Beerel*

- `2004.00974v3` - [abs](http://arxiv.org/abs/2004.00974v3) - [pdf](http://arxiv.org/pdf/2004.00974v3)

> We present Deep-n-Cheap -- an open-source AutoML framework to search for deep learning models. This search includes both architecture and training hyperparameters, and supports convolutional neural networks and multi-layer perceptrons. Our framework is targeted for deployment on both benchmark and custom datasets, and as a result, offers a greater degree of search space customizability as compared to a more limited search over only pre-existing models from literature. We also introduce the technique of 'search transfer', which demonstrates the generalization capabilities of the models found by our framework to multiple datasets.   Deep-n-Cheap includes a user-customizable complexity penalty which trades off performance with training time or number of parameters. Specifically, our framework results in models offering performance comparable to state-of-the-art while taking 1-2 orders of magnitude less time to train than models from other AutoML and model search frameworks. Additionally, this work investigates and develops various insights regarding the search process. In particular, we show the superiority of a greedy strategy and justify our choice of Bayesian optimization as the primary search methodology over random / grid search.

</details>

<details>

<summary>2020-09-06 04:08:21 - Modeling Stochastic Variability in Multi-Band Time Series Data</summary>

- *Zhirui Hu, Hyungsuk Tak*

- `2005.08049v2` - [abs](http://arxiv.org/abs/2005.08049v2) - [pdf](http://arxiv.org/pdf/2005.08049v2)

> In preparation for the era of the time-domain astronomy with upcoming large-scale surveys, we propose a state-space representation of a multivariate damped random walk process as a tool to analyze irregularly-spaced multi-filter light curves with heteroscedastic measurement errors. We adopt a computationally efficient and scalable Kalman-filtering approach to evaluate the likelihood function, leading to maximum $O(k^3n)$ complexity, where $k$ is the number of available bands and $n$ is the number of unique observation times across the $k$ bands. This is a significant computational advantage over a commonly used univariate Gaussian process that can stack up all multi-band light curves in one vector with maximum $O(k^3n^3)$ complexity. Using such efficient likelihood computation, we provide both maximum likelihood estimates and Bayesian posterior samples of the model parameters. Three numerical illustrations are presented; (i) analyzing simulated five-band light curves for a comparison with independent single-band fits; (ii) analyzing five-band light curves of a quasar obtained from the Sloan Digital Sky Survey (SDSS) Stripe~82 to estimate the short-term variability and timescale; (iii) analyzing gravitationally lensed $g$- and $r$-band light curves of Q0957+561 to infer the time delay. Two R packages, Rdrw and timedelay, are publicly available to fit the proposed models.

</details>

<details>

<summary>2020-09-06 09:46:09 - Conditional Independence in Max-linear Bayesian Networks</summary>

- *Carlos Améndola, Claudia Klüppelberg, Steffen Lauritzen, Ngoc Tran*

- `2002.09233v2` - [abs](http://arxiv.org/abs/2002.09233v2) - [pdf](http://arxiv.org/pdf/2002.09233v2)

> Motivated by extreme value theory, max-linear Bayesian networks have been recently introduced and studied as an alternative to linear structural equation models. However, for max-linear systems the classical independence results for Bayesian networks are far from exhausting valid conditional independence statements. We use tropical linear algebra to derive a compact representation of the conditional distribution given a partial observation, and exploit this to obtain a complete description of all conditional independence relations. In the context-specific case, where conditional independence is queried relative to a specific value of the conditioning variables, we introduce the notion of a source DAG to disclose the valid conditional independence relations. In the context-free case we characterize conditional independence through a modified separation concept, $\ast$-separation, combined with a tropical eigenvalue condition. We also introduce the notion of an impact graph which describes how extreme events spread deterministically through the network and we give a complete characterization of such impact graphs. Our analysis opens up several interesting questions concerning conditional independence and tropical geometry.

</details>

<details>

<summary>2020-09-06 16:52:06 - Dynamic Information Design with Diminishing Sensitivity Over News</summary>

- *Jetlir Duraj, Kevin He*

- `1908.00084v4` - [abs](http://arxiv.org/abs/1908.00084v4) - [pdf](http://arxiv.org/pdf/1908.00084v4)

> A Bayesian agent experiences gain-loss utility each period over changes in belief about future consumption ("news utility"), with diminishing sensitivity over the magnitude of news. We show the agent's preference between an information structure that delivers news gradually and another that resolves all uncertainty at once depends on his consumption ranking of different states. One-shot resolution is better than gradual bad news, but it is not optimal among all information structures (under common functional forms). In a dynamic cheap-talk framework where a benevolent sender communicates the state over multiple periods, the babbling equilibrium is essentially unique without loss aversion. More loss-averse agents may enjoy higher news utility in equilibrium, contrary to the commitment case. We characterize the family of gradual good news equilibria that exist with high enough loss aversion, and find the sender conveys progressively larger pieces of good news. We discuss applications to media competition and game shows.

</details>

<details>

<summary>2020-09-07 10:18:38 - Bayesian shared-parameter models for analysing sardine fishing in the Mediterranean Sea</summary>

- *Gabriel Calvo, Carmen Armero, Maria Grazia Pennino, Luigi Spezia*

- `2009.02992v1` - [abs](http://arxiv.org/abs/2009.02992v1) - [pdf](http://arxiv.org/pdf/2009.02992v1)

> European sardine is experiencing an overfishing around the world. The dynamics of the industrial and artisanal fishing in the Mediterranean Sea from 1970 to 2014 by country was assessed by means of Bayesian joint longitudinal modelling that uses the random effects to generate an association structure between both longitudinal measures. Model selection was based on Bayes factors approximated through the harmonic mean.

</details>

<details>

<summary>2020-09-07 13:24:47 - Iterative Correction of Sensor Degradation and a Bayesian Multi-Sensor Data Fusion Method</summary>

- *Luka Kolar, Rok Šikonja, Lenart Treven*

- `2009.03091v1` - [abs](http://arxiv.org/abs/2009.03091v1) - [pdf](http://arxiv.org/pdf/2009.03091v1)

> We present a novel method for inferring ground-truth signal from multiple degraded signals, affected by different amounts of sensor exposure. The algorithm learns a multiplicative degradation effect by performing iterative corrections of two signals solely from the ratio between them. The degradation function d should be continuous, satisfy monotonicity, and d(0) = 1. We use smoothed monotonic regression method, where we easily incorporate the aforementioned criteria to the fitting part. We include theoretical analysis and prove convergence to the ground-truth signal for the noiseless measurement model. Lastly, we present an approach to fuse the noisy corrected signals using Gaussian processes. We use sparse Gaussian processes that can be utilized for a large number of measurements together with a specialized kernel that enables the estimation of noise values of all sensors. The data fusion framework naturally handles data gaps and provides a simple and powerful method for observing the signal trends on multiple timescales(long-term and short-term signal properties). The viability of correction method is evaluated on a synthetic dataset with known ground-truth signal.

</details>

<details>

<summary>2020-09-07 21:22:19 - Bayesian estimation and prediction for certain mixtures</summary>

- *Aziz LMoudden, Éric Marchand*

- `2005.02879v2` - [abs](http://arxiv.org/abs/2005.02879v2) - [pdf](http://arxiv.org/pdf/2005.02879v2)

> For two vast families of mixture distributions and a given prior, we provide unified representations of posterior and predictive distributions. Model applications presented include bivariate mixtures of Gamma distributions labelled as Kibble-type, non-central Chi-square and F distributions, the distribution of $R^2$ in multiple regression, variance mixture of normal distributions, and mixtures of location-scale exponential distributions including the multivariate Lomax distribution. An emphasis is also placed on analytical representations and the relationships with a host of existing distributions and several hypergeomtric functions of one or two variables.

</details>

<details>

<summary>2020-09-07 22:25:12 - Variational Bayesian Quantization</summary>

- *Yibo Yang, Robert Bamler, Stephan Mandt*

- `2002.08158v2` - [abs](http://arxiv.org/abs/2002.08158v2) - [pdf](http://arxiv.org/pdf/2002.08158v2)

> We propose a novel algorithm for quantizing continuous latent representations in trained models. Our approach applies to deep probabilistic models, such as variational autoencoders (VAEs), and enables both data and model compression. Unlike current end-to-end neural compression methods that cater the model to a fixed quantization scheme, our algorithm separates model design and training from quantization. Consequently, our algorithm enables "plug-and-play" compression with variable rate-distortion trade-off, using a single trained model. Our algorithm can be seen as a novel extension of arithmetic coding to the continuous domain, and uses adaptive quantization accuracy based on estimates of posterior uncertainty. Our experimental results demonstrate the importance of taking into account posterior uncertainties, and show that image compression with the proposed algorithm outperforms JPEG over a wide range of bit rates using only a single standard VAE. Further experiments on Bayesian neural word embeddings demonstrate the versatility of the proposed method.

</details>

<details>

<summary>2020-09-08 06:54:11 - Sequential Subspace Search for Functional Bayesian Optimization Incorporating Experimenter Intuition</summary>

- *Alistair Shilton, Sunil Gupta, Santu Rana, Svetha Venkatesh*

- `2009.03543v1` - [abs](http://arxiv.org/abs/2009.03543v1) - [pdf](http://arxiv.org/pdf/2009.03543v1)

> We propose an algorithm for Bayesian functional optimisation - that is, finding the function to optimise a process - guided by experimenter beliefs and intuitions regarding the expected characteristics (length-scale, smoothness, cyclicity etc.) of the optimal solution encoded into the covariance function of a Gaussian Process. Our algorithm generates a sequence of finite-dimensional random subspaces of functional space spanned by a set of draws from the experimenter's Gaussian Process. Standard Bayesian optimisation is applied on each subspace, and the best solution found used as a starting point (origin) for the next subspace. Using the concept of effective dimensionality, we analyse the convergence of our algorithm and provide a regret bound to show that our algorithm converges in sub-linear time provided a finite effective dimension exists. We test our algorithm in simulated and real-world experiments, namely blind function matching, finding the optimal precipitation-strengthening function for an aluminium alloy, and learning rate schedule optimisation for deep networks.

</details>

<details>

<summary>2020-09-08 13:08:27 - Spatial Bayesian Hierarchical Modelling with Integrated Nested Laplace Approximation</summary>

- *Nicoletta D'Angelo, Antonino Abbruzzo, Giada Adelfio*

- `2009.03712v1` - [abs](http://arxiv.org/abs/2009.03712v1) - [pdf](http://arxiv.org/pdf/2009.03712v1)

> We consider latent Gaussian fields for modelling spatial dependence in the context of both spatial point patterns and areal data, providing two different applications. The inhomogeneous Log-Gaussian Cox Process model is specified to describe a seismic sequence occurred in Greece, resorting to the Stochastic Partial Differential Equations. The Besag-York-Mollie model is fitted for disease mapping of the Covid-19 infection in the North of Italy. These models both belong to the class of Bayesian hierarchical models with latent Gaussian fields whose posterior is not available in closed form. Therefore, the inference is performed with the Integrated Nested Laplace Approximation, which provides accurate and relatively fast analytical approximations to the posterior quantities of interest.

</details>

<details>

<summary>2020-09-08 13:35:55 - A Bayesian response-adaptive dose finding and comparative effectiveness trial</summary>

- *Anna Heath, Maryna Yaskina, Petros Pechlivanoglou, Juan David Rios, Martin Offringa, Terry P Klassen, Naveen Poonai, Eleanor Pullenayegum*

- `2006.06739v2` - [abs](http://arxiv.org/abs/2006.06739v2) - [pdf](http://arxiv.org/pdf/2006.06739v2)

> Aims: Combinations of treatments can offer additional benefit over the treatments individually. However, trials of these combinations are lower priority than the development of novel therapies, which can restrict funding, timelines and patient availability. This paper develops a novel trial design to facilitate the evaluation of novel combination therapies that combines elements of phase II and phase III trials. Methods: This trial uses response adaptive randomisation to increase the information collected about successful novel drug combinations and Bayesian dose-response modelling to undertake a comparative-effectiveness analysis for the most successful dose combination against a relevant comparator. We used simulation methods to evaluate the probability of selecting the correct optimal dose combination, the operating characteristics and predictive power of this design for a trial in pain management and sedation in paediatric emergency departments. Results: With 410 participants, 5 interim updates of the randomisation ratio and a probability of effectiveness of 0.93, 0.88 and 0.83 for the three dose combinations, we have an 83% chance of randomising the largest number of patients to the drug with the highest probability of effectiveness. Based on this adaptive randomisation procedure, the comparative effectiveness analysis has a type I error of less than 5% and a 93% chance of correctly concluding non-inferiority when the probability of effectiveness for the optimal combination therapy is 0.9. In this case, the trial has a 77% chance of meeting its dual aims of dose finding and comparative effectiveness. Finally, the Bayesian predictive power of the trial is over 90%. Conclusion: The proposed trial has high potential to meet the dual study objectives within a feasible level of recruitment, minimising the administrative burden and recruitment time for a trial.

</details>

<details>

<summary>2020-09-08 14:50:12 - Lifelong Generative Modeling</summary>

- *Jason Ramapuram, Magda Gregorova, Alexandros Kalousis*

- `1705.09847v7` - [abs](http://arxiv.org/abs/1705.09847v7) - [pdf](http://arxiv.org/pdf/1705.09847v7)

> Lifelong learning is the problem of learning multiple consecutive tasks in a sequential manner, where knowledge gained from previous tasks is retained and used to aid future learning over the lifetime of the learner. It is essential towards the development of intelligent machines that can adapt to their surroundings. In this work we focus on a lifelong learning approach to unsupervised generative modeling, where we continuously incorporate newly observed distributions into a learned model. We do so through a student-teacher Variational Autoencoder architecture which allows us to learn and preserve all the distributions seen so far, without the need to retain the past data nor the past models. Through the introduction of a novel cross-model regularizer, inspired by a Bayesian update rule, the student model leverages the information learned by the teacher, which acts as a probabilistic knowledge store. The regularizer reduces the effect of catastrophic interference that appears when we learn over sequences of distributions. We validate our model's performance on sequential variants of MNIST, FashionMNIST, PermutedMNIST, SVHN and Celeb-A and demonstrate that our model mitigates the effects of catastrophic interference faced by neural networks in sequential learning scenarios.

</details>

<details>

<summary>2020-09-08 17:43:10 - Qualitative Robust Bayesianism and the Likelihood Principle</summary>

- *Conor Mayo-Wilson, Aditya Saraf*

- `2009.03879v1` - [abs](http://arxiv.org/abs/2009.03879v1) - [pdf](http://arxiv.org/pdf/2009.03879v1)

> We argue that the likelihood principle (LP) and weak law of likelihood (LL) generalize naturally to settings in which experimenters are justified only in making comparative, non-numerical judgments of the form "$A$ given $B$ is more likely than $C$ given $D$." To do so, we first \emph{formulate} qualitative analogs of those theses. Then, using a framework for qualitative conditional probability, just as the characterizes when all Bayesians (regardless of prior) agree that two pieces of evidence are equivalent, so a qualitative/non-numerical version of LP provides sufficient conditions for agreement among experimenters' whose degrees of belief satisfy only very weak "coherence" constraints. We prove a similar result for LL. We conclude by discussing the relevance of results to stopping rules.

</details>

<details>

<summary>2020-09-08 19:00:51 - Modulating Surrogates for Bayesian Optimization</summary>

- *Erik Bodin, Markus Kaiser, Ieva Kazlauskaite, Zhenwen Dai, Neill D. F. Campbell, Carl Henrik Ek*

- `1906.11152v4` - [abs](http://arxiv.org/abs/1906.11152v4) - [pdf](http://arxiv.org/pdf/1906.11152v4)

> Bayesian optimization (BO) methods often rely on the assumption that the objective function is well-behaved, but in practice, this is seldom true for real-world objectives even if noise-free observations can be collected. Common approaches, which try to model the objective as precisely as possible, often fail to make progress by spending too many evaluations modeling irrelevant details. We address this issue by proposing surrogate models that focus on the well-behaved structure in the objective function, which is informative for search, while ignoring detrimental structure that is challenging to model from few observations. First, we demonstrate that surrogate models with appropriate noise distributions can absorb challenging structures in the objective function by treating them as irreducible uncertainty. Secondly, we show that a latent Gaussian process is an excellent surrogate for this purpose, comparing with Gaussian processes with standard noise distributions. We perform numerous experiments on a range of BO benchmarks and find that our approach improves reliability and performance when faced with challenging objective functions.

</details>

<details>

<summary>2020-09-08 19:29:18 - Health-behaviors associated with the growing risk of adolescent suicide attempts: A data-driven cross-sectional study</summary>

- *Zhiyuan Wei, Sayanti Mukherjee*

- `2009.03966v1` - [abs](http://arxiv.org/abs/2009.03966v1) - [pdf](http://arxiv.org/pdf/2009.03966v1)

> Purpose: Identify and examine the associations between health behaviors and increased risk of adolescent suicide attempts, while controlling for socioeconomic and demographic differences. Design: A data-driven analysis using cross-sectional data. Setting: Communities in the state of Montana from 1999 to 2017. Subjects: Selected 22,447 adolescents of whom 1,631 adolescents attempted suicide at least once. Measures: Overall 29 variables (predictors) accounting for psychological behaviors, illegal substances consumption, daily activities at schools and demographic backgrounds, were considered. Analysis: A library of machine learning algorithms along with the traditionally-used logistic regression were used to model and predict suicide attempt risk. Model performances (goodness-of-fit and predictive accuracy) were measured using accuracy, precision, recall and F-score metrics. Results: The non-parametric Bayesian tree ensemble model outperformed all other models, with 80.0% accuracy in goodness-of-fit (F-score:0.802) and 78.2% in predictive accuracy (F-score:0.785). Key health-behaviors identified include: being sad/hopeless, followed by safety concerns at school, physical fighting, inhalant usage, illegal drugs consumption at school, current cigarette usage, and having first sex at an early age (below 15 years of age). Additionally, the minority groups (American Indian/Alaska Natives, Hispanics/Latinos), and females are also found to be highly vulnerable to attempting suicides. Conclusion: Significant contribution of this work is understanding the key health-behaviors and health disparities that lead to higher frequency of suicide attempts among adolescents, while accounting for the non-linearity and complex interactions among the outcome and the exposure variables.

</details>

<details>

<summary>2020-09-08 20:22:04 - Bayesian deep learning for mapping via auxiliary information: a new era for geostatistics?</summary>

- *Charlie Kirkwood, Theo Economou, Nicolas Pugeault*

- `2008.07320v3` - [abs](http://arxiv.org/abs/2008.07320v3) - [pdf](http://arxiv.org/pdf/2008.07320v3)

> For geospatial modelling and mapping tasks, variants of kriging - the spatial interpolation technique developed by South African mining engineer Danie Krige - have long been regarded as the established geostatistical methods. However, kriging and its variants (such as regression kriging, in which auxiliary variables or derivatives of these are included as covariates) are relatively restrictive models and lack capabilities that have been afforded to us in the last decade by deep neural networks. Principal among these is feature learning - the ability to learn filters to recognise task-specific patterns in gridded data such as images. Here we demonstrate the power of feature learning in a geostatistical context, by showing how deep neural networks can automatically learn the complex relationships between point-sampled target variables and gridded auxiliary variables (such as those provided by remote sensing), and in doing so produce detailed maps of chosen target variables. At the same time, in order to cater for the needs of decision makers who require well-calibrated probabilities, we obtain uncertainty estimates via a Bayesian approximation known as Monte Carlo dropout. In our example, we produce a national-scale probabilistic geochemical map from point-sampled assay data, with auxiliary information provided by a terrain elevation grid. Unlike traditional geostatistical approaches, auxiliary variable grids are fed into our deep neural network raw. There is no need to provide terrain derivatives (e.g. slope angles, roughness, etc) because the deep neural network is capable of learning these and arbitrarily more complex derivatives as necessary to maximise predictive performance. We hope our results will raise awareness of the suitability of Bayesian deep learning - and its feature learning capabilities - for large-scale geostatistical applications where uncertainty matters.

</details>

<details>

<summary>2020-09-08 22:28:22 - Bayesian Nonparametric Cost-Effectiveness Analyses: Causal Estimation and Adaptive Subgroup Discovery</summary>

- *Arman Oganisian, Nandita Mitra, Jason Roy*

- `2002.04706v2` - [abs](http://arxiv.org/abs/2002.04706v2) - [pdf](http://arxiv.org/pdf/2002.04706v2)

> Cost-effectiveness analyses (CEAs) are at the center of health economic decision making. While these analyses help policy analysts and economists determine coverage, inform policy, and guide resource allocation, they are statistically challenging for several reasons. Cost and effectiveness are correlated and follow complex joint distributions which are difficult to capture parametrically. Effectiveness (often measured as increased survival time) and accumulated cost tends to be right-censored in many applications. Moreover, CEAs are often conducted using observational data with non-random treatment assignment. Policy-relevant causal estimation therefore requires robust confounding control. Finally, current CEA methods do not address cost-effectiveness heterogeneity in a principled way - often presenting population-averaged estimates even though significant effect heterogeneity may exist. Motivated by these challenges, we develop a nonparametric Bayesian model for joint cost-survival distributions in the presence of censoring. Our approach utilizes a joint Enriched Dirichlet Process prior on the covariate effects of cost and survival time, while using a Gamma Process prior on the baseline survival time hazard. Causal CEA estimands, with policy-relevant interpretations, are identified and estimated via a Bayesian nonparametric g-computation procedure. Finally, we outline how the induced clustering of the Enriched Dirichlet Process can be used to adaptively detect presence of subgroups with different cost-effectiveness profiles. We outline an MCMC procedure for full posterior inference and evaluate frequentist properties via simulations. We use our model to assess the cost-efficacy of chemotherapy versus radiation adjuvant therapy for treating endometrial cancer in the SEER-Medicare database.

</details>

<details>

<summary>2020-09-09 07:05:53 - Bayesian statistical analysis of hydrogeochemical data using point processes: a new tool for source detection in multicomponent fluid mixtures</summary>

- *Christophe Reype, Antonin Richard, Madalina Deaconu, Radu Stoica*

- `2009.04132v1` - [abs](http://arxiv.org/abs/2009.04132v1) - [pdf](http://arxiv.org/pdf/2009.04132v1)

> Hydrogeochemical data may be seen as a point cloud in a multi-dimensional space. Each dimension of this space represents a hydrogeochemical parameter (i.e. salinity, solute concentration, concentration ratio, isotopic composition...). While the composition of many geological fluids is controlled by mixing between multiple sources, a key question related to hydrogeochemical data set is the detection of the sources. By looking at the hydrogeochemical data as spatial data, this paper presents a new solution to the source detection problem that is based on point processes. Results are shown on simulated and real data from geothermal fluids.

</details>

<details>

<summary>2020-09-09 12:44:05 - Simulated Tempering Langevin Monte Carlo II: An Improved Proof using Soft Markov Chain Decomposition</summary>

- *Rong Ge, Holden Lee, Andrej Risteski*

- `1812.00793v3` - [abs](http://arxiv.org/abs/1812.00793v3) - [pdf](http://arxiv.org/pdf/1812.00793v3)

> A key task in Bayesian machine learning is sampling from distributions that are only specified up to a partition function (i.e., constant of proportionality). One prevalent example of this is sampling posteriors in parametric distributions, such as latent-variable generative models. However sampling (even very approximately) can be #P-hard.   Classical results going back to Bakry and \'Emery (1985) on sampling focus on log-concave distributions, and show a natural Markov chain called Langevin diffusion mixes in polynomial time. However, all log-concave distributions are uni-modal, while in practice it is very common for the distribution of interest to have multiple modes. In this case, Langevin diffusion suffers from torpid mixing.   We address this problem by combining Langevin diffusion with simulated tempering. The result is a Markov chain that mixes more rapidly by transitioning between different temperatures of the distribution. We analyze this Markov chain for a mixture of (strongly) log-concave distributions of the same shape. In particular, our technique applies to the canonical multi-modal distribution: a mixture of gaussians (of equal variance). Our algorithm efficiently samples from these distributions given only access to the gradient of the log-pdf.   For the analysis, we introduce novel techniques for proving spectral gaps based on decomposing the action of the generator of the diffusion. Previous approaches rely on decomposing the state space as a partition of sets, while our approach can be thought of as decomposing the stationary measure as a mixture of distributions (a "soft partition").   Additional materials for the paper can be found at http://holdenlee.github.io/Simulated%20tempering%20Langevin%20Monte%20Carlo.html. The proof and results have been improved and generalized from the precursor at arXiv:1710.02736.

</details>

<details>

<summary>2020-09-10 05:02:40 - Bayesian Perceptron: Towards fully Bayesian Neural Networks</summary>

- *Marco F. Huber*

- `2009.01730v2` - [abs](http://arxiv.org/abs/2009.01730v2) - [pdf](http://arxiv.org/pdf/2009.01730v2)

> Artificial neural networks (NNs) have become the de facto standard in machine learning. They allow learning highly nonlinear transformations in a plethora of applications. However, NNs usually only provide point estimates without systematically quantifying corresponding uncertainties. In this paper a novel approach towards fully Bayesian NNs is proposed, where training and predictions of a perceptron are performed within the Bayesian inference framework in closed-form. The weights and the predictions of the perceptron are considered Gaussian random variables. Analytical expressions for predicting the perceptron's output and for learning the weights are provided for commonly used activation functions like sigmoid or ReLU. This approach requires no computationally expensive gradient calculations and further allows sequential learning.

</details>

<details>

<summary>2020-09-10 06:22:06 - NPENAS: Neural Predictor Guided Evolution for Neural Architecture Search</summary>

- *Chen Wei, Chuang Niu, Yiping Tang, Yue Wang, Haihong Hu, Jimin Liang*

- `2003.12857v3` - [abs](http://arxiv.org/abs/2003.12857v3) - [pdf](http://arxiv.org/pdf/2003.12857v3)

> Neural architecture search (NAS) is a promising method for automatically design neural architectures. NAS adopts a search strategy to explore the predefined search space to find outstanding performance architecture with the minimum searching costs. Bayesian optimization and evolutionary algorithms are two commonly used search strategies, but they suffer from computationally expensive, challenge to implement or inefficient exploration ability. In this paper, we propose a neural predictor guided evolutionary algorithm to enhance the exploration ability of EA for NAS (NPENAS) and design two kinds of neural predictors. The first predictor is defined from Bayesian optimization and we propose a graph-based uncertainty estimation network as a surrogate model that is easy to implement and computationally efficient. The second predictor is a graph-based neural network that directly outputs the performance prediction of the input neural architecture. The NPENAS using the two neural predictors are denoted as NPENAS-BO and NPENAS-NP respectively. In addition, we introduce a new random architecture sampling method to overcome the drawbacks of the existing sampling method. Extensive experiments demonstrate the superiority of NPENAS. Quantitative results on three NAS search spaces indicate that both NPENAS-BO and NPENAS-NP outperform most existing NAS algorithms, with NPENAS-BO achieving state-of-the-art performance on NASBench-201 and NPENAS-NP on NASBench-101 and DARTS, respectively.

</details>

<details>

<summary>2020-09-10 10:02:54 - General Robust Bayes Pseudo-Posterior: Exponential Convergence results with Applications</summary>

- *Abhik Ghosh, Tuhin Majumder, Ayanendranath Basu*

- `1708.09692v2` - [abs](http://arxiv.org/abs/1708.09692v2) - [pdf](http://arxiv.org/pdf/1708.09692v2)

> Although Bayesian inference is an immensely popular paradigm among a large segment of scientists including statisticians, most applications consider objective priors and need critical investigations (Efron, 2013, Science). While it has several optimal properties, a major drawback of Bayesian inference is the lack of robustness against data contamination and model misspecification, which becomes pernicious in the use of objective priors. This paper presents the general formulation of a Bayes pseudo-posterior distribution yielding robust inference. Exponential convergence results related to the new pseudo-posterior and the corresponding Bayes estimators are established under the general parametric set-up and illustrations are provided for the independent stationary as well as non-homogeneous models. Several additional details and properties of the procedure are described, including the estimation under fixed-design regression models.

</details>

<details>

<summary>2020-09-10 11:55:44 - Bayesian causal inference in probit graphical models</summary>

- *Federico Castelletti, Guido Consonni*

- `2009.04795v1` - [abs](http://arxiv.org/abs/2009.04795v1) - [pdf](http://arxiv.org/pdf/2009.04795v1)

> We consider a binary response which is potentially affected by a set of continuous variables. Of special interest is the causal effect on the response due to an intervention on a specific variable. The latter can be meaningfully determined on the basis of observational data through suitable assumptions on the data generating mechanism. In particular we assume that the joint distribution obeys the conditional independencies (Markov properties) inherent in a Directed Acyclic Graph (DAG), and the DAG is given a causal interpretation through the notion of interventional distribution. We propose a DAG-probit model where the response is generated by discretization through a random threshold of a continuous latent variable and the latter, jointly with the remaining continuous variables, has a distribution belonging to a zero-mean Gaussian model whose covariance matrix is constrained to satisfy the Markov properties of the DAG. Our model leads to a natural definition of causal effect conditionally on a given DAG. Since the DAG which generates the observations is unknown, we present an efficient MCMC algorithm whose target is the posterior distribution on the space of DAGs, the Cholesky parameters of the concentration matrix, and the threshold linking the response to the latent. Our end result is a Bayesian Model Averaging estimate of the causal effect which incorporates parameter, as well as model, uncertainty. The methodology is assessed using simulation experiments and applied to a gene expression data set originating from breast cancer stem cells.

</details>

<details>

<summary>2020-09-10 13:39:57 - Bayesian state-space modeling for analyzing heterogeneous network effects of US monetary policy</summary>

- *Niko Hauzenberger, Michael Pfarrhofer*

- `1911.06206v3` - [abs](http://arxiv.org/abs/1911.06206v3) - [pdf](http://arxiv.org/pdf/1911.06206v3)

> Understanding disaggregate channels in the transmission of monetary policy is of crucial importance for effectively implementing policy measures. We extend the empirical econometric literature on the role of production networks in the propagation of shocks along two dimensions. First, we allow for industry-specific responses that vary over time, reflecting non-linearities and cross-sectional heterogeneities in direct transmission channels. Second, we allow for time-varying network structures and dependence. This feature captures both variation in the structure of the production network, but also differences in cross-industry demand elasticities. We find that impacts vary substantially over time and the cross-section. Higher-order effects appear to be particularly important in periods of economic and financial uncertainty, often coinciding with tight credit market conditions and financial stress. Differentials in industry-specific responses can be explained by how close the respective industries are to end-consumers.

</details>

<details>

<summary>2020-09-10 16:27:23 - Bayesian cumulative shrinkage for infinite factorizations</summary>

- *Sirio Legramanti, Daniele Durante, David B. Dunson*

- `1902.04349v3` - [abs](http://arxiv.org/abs/1902.04349v3) - [pdf](http://arxiv.org/pdf/1902.04349v3)

> There is a wide variety of models in which the dimension of the parameter space is unknown. For example, in factor analysis the number of latent factors is typically not known and has to be inferred from the observed data. Although classical shrinkage priors are useful in these contexts, increasing shrinkage priors can provide a more effective option, which progressively penalizes expansions with growing complexity. In this article we propose a novel increasing shrinkage prior, named the cumulative shrinkage process, for the parameters controlling the dimension in over-complete formulations. Our construction has broad applicability, simple interpretation, and is based on a sequence of spike and slab distributions which assign increasing mass to the spike as model complexity grows. Using factor analysis as an illustrative example, we show that this formulation has theoretical and practical advantages over current competitors, including an improved ability to recover the model dimension. An adaptive Markov chain Monte Carlo algorithm is proposed, and the methods are evaluated in simulation studies and applied to personality traits data.

</details>

<details>

<summary>2020-09-10 18:44:13 - Spatial hierarchical modeling of threshold exceedances using rate mixtures</summary>

- *Rishikesh Yadav, Raphaël Huser, Thomas Opitz*

- `1912.04571v2` - [abs](http://arxiv.org/abs/1912.04571v2) - [pdf](http://arxiv.org/pdf/1912.04571v2)

> We develop new flexible univariate models for light-tailed and heavy-tailed data, which extend a hierarchical representation of the generalized Pareto (GP) limit for threshold exceedances. These models can accommodate departure from asymptotic threshold stability in finite samples while keeping the asymptotic GP distribution as a special (or boundary) case and can capture the tails and the bulk jointly without losing much flexibility. Spatial dependence is modeled through a latent process, while the data are assumed to be conditionally independent. Focusing on a gamma-gamma model construction, we design penalized complexity priors for crucial model parameters, shrinking our proposed spatial Bayesian hierarchical model toward a simpler reference whose marginal distributions are GP with moderately heavy tails. Our model can be fitted in fairly high dimensions using Markov chain Monte Carlo by exploiting the Metropolis-adjusted Langevin algorithm (MALA), which guarantees fast convergence of Markov chains with efficient block proposals for the latent variables. We also develop an adaptive scheme to calibrate the MALA tuning parameters. Moreover, our model avoids the expensive numerical evaluations of multifold integrals in censored likelihood expressions. We demonstrate our new methodology by simulation and application to a dataset of extreme rainfall events that occurred in Germany. Our fitted gamma-gamma model provides a satisfactory performance and can be successfully used to predict rainfall extremes at unobserved locations.

</details>

<details>

<summary>2020-09-11 02:27:01 - A Bayesian hierarchical model to estimate land surface phenology parameters with harmonized Landsat 8 and Sentinel-2 images</summary>

- *Chad Babcock, Andrew O. Finley, Nathaniel Looker*

- `2009.05203v1` - [abs](http://arxiv.org/abs/2009.05203v1) - [pdf](http://arxiv.org/pdf/2009.05203v1)

> We develop a Bayesian Land Surface Phenology (LSP) model and examine its performance using Enhanced Vegetation Index (EVI) observations derived from the Harmonized Landsat Sentinel-2 (HLS) dataset. Building on previous work, we propose a double logistic function that, once couched within a Bayesian model, yields posterior distributions for all LSP parameters. We assess the efficacy of the Normal, Truncated Normal, and Beta likelihoods to deliver robust LSP parameter estimates. Two case studies are presented and used to explore aspects of the proposed model. The first, conducted over forested pixels within a HLS tile, explores choice of likelihood and space-time varying HLS data availability for long-term average LSP parameter point and uncertainty estimation. The second, conducted on a small area of interest within the HLS tile on an annual time-step, further examines the impact of sample size and choice of likelihood on LSP parameter estimates. Results indicate that while the Truncated Normal and Beta likelihoods are theoretically preferable when the vegetation index is bounded, all three likelihoods performed similarly when the number of index observations is sufficiently large and values are not near the index bounds. Both case studies demonstrate how pixel-level LSP parameter posterior distributions can be used to propagate uncertainty through subsequent analysis. As a companion to this article, we provide an open-source \R package \pkg{rsBayes} and supplementary data and code used to reproduce the analysis results. The proposed model specification and software implementation delivers computationally efficient, statistically robust, and inferentially rich LSP parameter posterior distributions at the pixel-level across massive raster time series datasets.

</details>

<details>

<summary>2020-09-11 09:28:01 - Likelihood-free inference by ratio estimation</summary>

- *Owen Thomas, Ritabrata Dutta, Jukka Corander, Samuel Kaski, Michael U. Gutmann*

- `1611.10242v6` - [abs](http://arxiv.org/abs/1611.10242v6) - [pdf](http://arxiv.org/pdf/1611.10242v6)

> We consider the problem of parametric statistical inference when likelihood computations are prohibitively expensive but sampling from the model is possible. Several so-called likelihood-free methods have been developed to perform inference in the absence of a likelihood function. The popular synthetic likelihood approach infers the parameters by modelling summary statistics of the data by a Gaussian probability distribution. In another popular approach called approximate Bayesian computation, the inference is performed by identifying parameter values for which the summary statistics of the simulated data are close to those of the observed data. Synthetic likelihood is easier to use as no measure of `closeness' is required but the Gaussianity assumption is often limiting. Moreover, both approaches require judiciously chosen summary statistics. We here present an alternative inference approach that is as easy to use as synthetic likelihood but not as restricted in its assumptions, and that, in a natural way, enables automatic selection of relevant summary statistic from a large set of candidates. The basic idea is to frame the problem of estimating the posterior as a problem of estimating the ratio between the data generating distribution and the marginal distribution. This problem can be solved by logistic regression, and including regularising penalty terms enables automatic selection of the summary statistics relevant to the inference task. We illustrate the general theory on canonical examples and employ it to perform inference for challenging stochastic nonlinear dynamical systems and high-dimensional summary statistics.

</details>

<details>

<summary>2020-09-11 10:22:06 - Bayesian estimation of a decreasing density</summary>

- *Geurt Jongbloed, Frank van der Meulen, Lixue Pang*

- `1801.02539v6` - [abs](http://arxiv.org/abs/1801.02539v6) - [pdf](http://arxiv.org/pdf/1801.02539v6)

> Suppose $X_1,\dots, X_n$ is a random sample from a bounded and decreasing density $f_0$ on $[0,\infty)$. We are interested in estimating such $f_0$, with special interest in $f_0(0)$. This problem is encountered in various statistical applications and has gained quite some attention in the statistical literature. It is well known that the maximum likelihood estimator is inconsistent at zero. This has led several authors to propose alternative estimators which are consistent. As any decreasing density can be represented as a scale mixture of uniform densities, a Bayesian estimator is obtained by endowing the mixture distribution with the Dirichlet process prior. Assuming this prior, we derive contraction rates of the posterior density at zero by carefully revising arguments presented in Salomond (2014). Various methods for estimating the density are compared using a simulation study. We apply the Bayesian procedure to the current durations data described in Keiding et al.(2012).

</details>

<details>

<summary>2020-09-11 11:57:38 - Inferring hidden potentials in analytical regions: uncovering crime suspect communities in Medellín</summary>

- *Alejandro Puerta, Andrés Ramírez-Hassan*

- `2009.05360v1` - [abs](http://arxiv.org/abs/2009.05360v1) - [pdf](http://arxiv.org/pdf/2009.05360v1)

> This paper proposes a Bayesian approach to perform inference regarding the size of hidden populations at analytical region using reported statistics. To do so, we propose a specification taking into account one-sided error components and spatial effects within a panel data structure. Our simulation exercises suggest good finite sample performance. We analyze rates of crime suspects living per neighborhood in Medell\'in (Colombia) associated with four crime activities. Our proposal seems to identify hot spots or "crime communities", potential neighborhoods where under-reporting is more severe, and also drivers of crime schools. Statistical evidence suggests a high level of interaction between homicides and drug dealing in one hand, and motorcycle and car thefts on the other hand.

</details>

<details>

<summary>2020-09-11 13:04:28 - Explaining the Decline of Child Mortality in 44 Developing Countries: A Bayesian Extension of Oaxaca Decomposition Methods</summary>

- *Antonio P. Ramos, Martin J. Flores, Leiwen Gao, Patrick Heuveline, Robert E. Weiss*

- `2009.05417v1` - [abs](http://arxiv.org/abs/2009.05417v1) - [pdf](http://arxiv.org/pdf/2009.05417v1)

> We investigate the decline of infant mortality in 42 low and middle income countries (LMIC) using detailed micro data from 84 Demographic and Health Surveys. We estimate infant mortality risk for each infant in our data and develop a novel extension of Oaxaca decomposition to understand the sources of these changes. We find that the decline in infant mortality is due to a declining propensity for parents with given characteristics to experience the death of an infant rather than due to changes in the distributions of these characteristics over time. Our results suggest that technical progress and policy health interventions in the form of public goods are the main drivers of the the recent decline in infant mortality in LMIC.

</details>

<details>

<summary>2020-09-11 13:07:51 - Bayesian Screening: Multi-test Bayesian Optimization Applied to in silico Material Screening</summary>

- *James Hook, Calum Hand, Emma Whitfield*

- `2009.05418v1` - [abs](http://arxiv.org/abs/2009.05418v1) - [pdf](http://arxiv.org/pdf/2009.05418v1)

> We present new multi-test Bayesian optimization models and algorithms for use in large scale material screening applications. Our screening problems are designed around two tests, one expensive and one cheap. This paper differs from other recent work on multi-test Bayesian optimization through use of a flexible model that allows for complex, non-linear relationships between the cheap and expensive test scores. This additional modeling flexibility is essential in the material screening applications which we describe. We demonstrate the power of our new algorithms on a family of synthetic toy problems as well as on real data from two large scale screening studies.

</details>

<details>

<summary>2020-09-11 13:12:00 - Large-scale empirical validation of Bayesian Network structure learning algorithms with noisy data</summary>

- *Anthony C. Constantinou, Yang Liu, Kiattikun Chobtham, Zhigao Guo, Neville K. Kitson*

- `2005.09020v2` - [abs](http://arxiv.org/abs/2005.09020v2) - [pdf](http://arxiv.org/pdf/2005.09020v2)

> Numerous Bayesian Network (BN) structure learning algorithms have been proposed in the literature over the past few decades. Each publication makes an empirical or theoretical case for the algorithm proposed in that publication and results across studies are often inconsistent in their claims about which algorithm is 'best'. This is partly because there is no agreed evaluation approach to determine their effectiveness. Moreover, each algorithm is based on a set of assumptions, such as complete data and causal sufficiency, and tend to be evaluated with data that conforms to these assumptions, however unrealistic these assumptions may be in the real world. As a result, it is widely accepted that synthetic performance overestimates real performance, although to what degree this may happen remains unknown. This paper investigates the performance of 15 structure learning algorithms. We propose a methodology that applies the algorithms to data that incorporates synthetic noise, in an effort to better understand the performance of structure learning algorithms when applied to real data. Each algorithm is tested over multiple case studies, sample sizes, types of noise, and assessed with multiple evaluation criteria. This work involved approximately 10,000 graphs with a total structure learning runtime of seven months. It provides the first large-scale empirical validation of BN structure learning algorithms under different assumptions of data noise. The results suggest that traditional synthetic performance may overestimate real-world performance by anywhere between 10% and more than 50%. They also show that while score-based learning is generally superior to constraint-based learning, a higher fitting score does not necessarily imply a more accurate causal graph. To facilitate comparisons with future studies, we have made all data, raw results, graphs and BN models freely available online.

</details>

<details>

<summary>2020-09-11 13:49:33 - Bayesian Beta-Binomial Prevalence Estimation Using an Imperfect Test</summary>

- *Jonathan Baxter*

- `2009.05446v1` - [abs](http://arxiv.org/abs/2009.05446v1) - [pdf](http://arxiv.org/pdf/2009.05446v1)

> Following [Diggle 2011, Greenland 1995], we give a simple formula for the Bayesian posterior density of a prevalence parameter based on unreliable testing of a population. This problem is of particular importance when the false positive test rate is close to the prevalence in the population being tested. An efficient Monte Carlo algorithm for approximating the posterior density is presented, and applied to estimating the Covid-19 infection rate in Santa Clara county, CA using the data reported in [Bendavid 2020]. We show that the true Bayesian posterior places considerably more mass near zero, resulting in a prevalence estimate of 5,000--70,000 infections (median: 42,000) (2.17% (95CI 0.27%--3.63%)), compared to the estimate of 48,000--81,000 infections derived in [Bendavid 2020] using the delta method.   A demonstration, with code and additional examples, is available at testprev.com.

</details>

<details>

<summary>2020-09-11 19:53:42 - Computationally Efficient Bayesian Unit-Level Models for Non-Gaussian Data Under Informative Sampling</summary>

- *Paul A. Parker, Scott H. Holan, Ryan Janicki*

- `2009.05642v1` - [abs](http://arxiv.org/abs/2009.05642v1) - [pdf](http://arxiv.org/pdf/2009.05642v1)

> Statistical estimates from survey samples have traditionally been obtained via design-based estimators. In many cases, these estimators tend to work well for quantities such as population totals or means, but can fall short as sample sizes become small. In today's "information age," there is a strong demand for more granular estimates. To meet this demand, using a Bayesian pseudo-likelihood, we propose a computationally efficient unit-level modeling approach for non-Gaussian data collected under informative sampling designs. Specifically, we focus on binary and multinomial data. Our approach is both multivariate and multiscale, incorporating spatial dependence at the area-level. We illustrate our approach through an empirical simulation study and through a motivating application to health insurance estimates using the American Community Survey.

</details>

<details>

<summary>2020-09-12 10:25:41 - Cointegration and unit root tests: A fully Bayesian approach</summary>

- *Marcio Alves Diniz, Carlos Alberto de Braganca Pereira, Julio Michael Stern*

- `2006.04499v4` - [abs](http://arxiv.org/abs/2006.04499v4) - [pdf](http://arxiv.org/pdf/2006.04499v4)

> To perform statistical inference for time series, one should be able to assess if they present deterministic or stochastic trends. For univariate analysis one way to detect stochastic trends is to test if the series has unit roots, and for multivariate studies it is often relevant to search for stationary linear relationships between the series, or if they cointegrate. The main goal of this article is to briefly review the shortcomings of unit root and cointegration tests proposed by the Bayesian approach of statistical inference and to show how they can be overcome by the fully Bayesian significance test (FBST), a procedure designed to test sharp or precise hypothesis. We will compare its performance with the most used frequentist alternatives, namely, the Augmented Dickey-Fuller for unit roots and the maximum eigenvalue test for cointegration. Keywords: Time series; Bayesian inference; Hypothesis testing; Unit root; Cointegration.

</details>

<details>

<summary>2020-09-12 20:37:33 - Tracking disease outbreaks from sparse data with Bayesian inference</summary>

- *Bryan Wilder, Michael J. Mina, Milind Tambe*

- `2009.05863v1` - [abs](http://arxiv.org/abs/2009.05863v1) - [pdf](http://arxiv.org/pdf/2009.05863v1)

> The COVID-19 pandemic provides new motivation for a classic problem in epidemiology: estimating the empirical rate of transmission during an outbreak (formally, the time-varying reproduction number) from case counts. While standard methods exist, they work best at coarse-grained national or state scales with abundant data, and struggle to accommodate the partial observability and sparse data common at finer scales (e.g., individual schools or towns). For example, case counts may be sparse when only a small fraction of infections are caught by a testing program. Or, whether an infected individual tests positive may depend on the kind of test and the point in time when they are tested. We propose a Bayesian framework which accommodates partial observability in a principled manner. Our model places a Gaussian process prior over the unknown reproduction number at each time step and models observations sampled from the distribution of a specific testing program. For example, our framework can accommodate a variety of kinds of tests (viral RNA, antibody, antigen, etc.) and sampling schemes (e.g., longitudinal or cross-sectional screening). Inference in this framework is complicated by the presence of tens or hundreds of thousands of discrete latent variables. To address this challenge, we propose an efficient stochastic variational inference method which relies on a novel gradient estimator for the variational objective. Experimental results for an example motivated by COVID-19 show that our method produces an accurate and well-calibrated posterior, while standard methods for estimating the reproduction number can fail badly.

</details>

<details>

<summary>2020-09-13 02:28:32 - Operational Calibration: Debugging Confidence Errors for DNNs in the Field</summary>

- *Zenan Li, Xiaoxing Ma, Chang Xu, Jingwei Xu, Chun Cao, Jian Lü*

- `1910.02352v2` - [abs](http://arxiv.org/abs/1910.02352v2) - [pdf](http://arxiv.org/pdf/1910.02352v2)

> Trained DNN models are increasingly adopted as integral parts of software systems, but they often perform deficiently in the field. A particularly damaging problem is that DNN models often give false predictions with high confidence, due to the unavoidable slight divergences between operation data and training data. To minimize the loss caused by inaccurate confidence, operational calibration, i.e., calibrating the confidence function of a DNN classifier against its operation domain, becomes a necessary debugging step in the engineering of the whole system.   Operational calibration is difficult considering the limited budget of labeling operation data and the weak interpretability of DNN models. We propose a Bayesian approach to operational calibration that gradually corrects the confidence given by the model under calibration with a small number of labeled operation data deliberately selected from a larger set of unlabeled operation data. The approach is made effective and efficient by leveraging the locality of the learned representation of the DNN model and modeling the calibration as Gaussian Process Regression. Comprehensive experiments with various practical datasets and DNN models show that it significantly outperformed alternative methods, and in some difficult tasks it eliminated about 71% to 97% high-confidence (>0.9) errors with only about 10\% of the minimal amount of labeled operation data needed for practical learning techniques to barely work.

</details>

<details>

<summary>2020-09-13 14:13:27 - Clustering of non-Gaussian data by variational Bayes for normal inverse Gaussian mixture models</summary>

- *Takashi Takekawa*

- `2009.06002v1` - [abs](http://arxiv.org/abs/2009.06002v1) - [pdf](http://arxiv.org/pdf/2009.06002v1)

> Finite mixture models, typically Gaussian mixtures, are well known and widely used as model-based clustering. In practical situations, there are many non-Gaussian data that are heavy-tailed and/or asymmetric. Normal inverse Gaussian (NIG) distributions are normal-variance mean which mixing densities are inverse Gaussian distributions and can be used for both haavy-tail and asymmetry. For NIG mixture models, both expectation-maximization method and variational Bayesian (VB) algorithms have been proposed. However, the existing VB algorithm for NIG mixture have a disadvantage that the shape of the mixing density is limited. In this paper, we propose another VB algorithm for NIG mixture that improves on the shortcomings. We also propose an extension of Dirichlet process mixture models to overcome the difficulty in determining the number of clusters in finite mixture models. We evaluated the performance with artificial data and found that it outperformed Gaussian mixtures and existing implementations for NIG mixtures, especially for highly non-normative data.

</details>

<details>

<summary>2020-09-14 07:04:42 - Bayesian Appraisal of Random Series Convergence with Application to Climate Change</summary>

- *Sucharita Roy, Sourabh Bhattacharya*

- `2009.06229v1` - [abs](http://arxiv.org/abs/2009.06229v1) - [pdf](http://arxiv.org/pdf/2009.06229v1)

> Roy and Bhattacharya (2020) provided Bayesian characterization of infinite series, and their most important application, namely, to the Dirichlet series characterizing the (in)famous Riemann Hypothesis, revealed insights that are not in support of the most celebrated conjecture for over 150 years.   In contrast with deterministic series considered by Roy and Bhattacharya (2020), in this article we take up random infinite series for our investigation. Remarkably, our method does not require any simplifying assumption. Albeit the Bayesian characterization theory for random series is no different from that for the deterministic setup, construction of effective upper bounds for partial sums, required for implementation, turns out to be a challenging undertaking in the random setup. In this article, we construct parametric and nonparametric upper bound forms for the partial sums of random infinite series and demonstrate the generality of the latter in comparison to the former. Simulation studies exhibit high accuracy and efficiency of the nonparametric bound in all the setups that we consider.   Finally, exploiting the property that the summands tend to zero in the case of series convergence, we consider application of our nonparametric bound driven Bayesian method to global climate change analysis. Specifically, analyzing the global average temperature record over the years 1850--2016 and Holocene global average temperature reconstruction data 12,000 years before present, we conclude, in spite of the current global warming situation, that global climate dynamics is subject to temporary variability only, the current global warming being an instance, and long term global warming or cooling either in the past or in the future, are highly unlikely.

</details>

<details>

<summary>2020-09-14 08:16:59 - Stein Point Markov Chain Monte Carlo</summary>

- *Wilson Ye Chen, Alessandro Barp, François-Xavier Briol, Jackson Gorham, Mark Girolami, Lester Mackey, Chris. J. Oates*

- `1905.03673v2` - [abs](http://arxiv.org/abs/1905.03673v2) - [pdf](http://arxiv.org/pdf/1905.03673v2)

> An important task in machine learning and statistics is the approximation of a probability measure by an empirical measure supported on a discrete point set. Stein Points are a class of algorithms for this task, which proceed by sequentially minimising a Stein discrepancy between the empirical measure and the target and, hence, require the solution of a non-convex optimisation problem to obtain each new point. This paper removes the need to solve this optimisation problem by, instead, selecting each new point based on a Markov chain sample path. This significantly reduces the computational cost of Stein Points and leads to a suite of algorithms that are straightforward to implement. The new algorithms are illustrated on a set of challenging Bayesian inference problems, and rigorous theoretical guarantees of consistency are established.

</details>

<details>

<summary>2020-09-14 17:55:04 - Multilevel regression with poststratification for the national level Viber/Street poll on the 2020 presidential election in Belarus</summary>

- *Ales Zahorski*

- `2009.06615v1` - [abs](http://arxiv.org/abs/2009.06615v1) - [pdf](http://arxiv.org/pdf/2009.06615v1)

> Independent sociological polls are forbidden in Belarus. Online polls performed without sound scientific rigour do not yield representative results. Yet, both inside and outside Belarus it is of great importance to obtain precise estimates of the ratings of all candidates. These ratings could function as reliable proxies for the election's outcomes. We conduct an independent poll based on the combination of the data collected via Viber and on the streets of Belarus. The Viber and the street data samples consist of almost 45000 and 1150 unique observations respectively. Bayesian regressions with poststratification were build to estimate ratings of the candidates and rates of early voting turnout for the population as a whole and within various focus subgroups. We show that both the officially announced results of the election and early voting rates are highly improbable. With a probability of at least 95%, Sviatlana Tikhanouskaya's rating lies between 75% and 80%, whereas Aliaksandr Lukashenka's rating lies between 13% and 18% and early voting rate predicted by the method ranges from 9% to 13% of those who took part in the election. These results contradict the officially announced outcomes, which are 10.12%, 80.11%, and 49.54% respectively and lie far outside even the 99.9% credible intervals predicted by our model. The only marginal groups of people where the upper bounds of the 99.9% credible intervals of the rating of Lukashenka are above 50% are people older than 60 and uneducated people. For all other marginal subgroups, including rural residents, even the upper bounds of 99.9% credible intervals for Lukashenka are far below 50%. The same is true for the population as a whole. Thus, with a probability of at least 99.9% Lukashenka could not have had enough electoral support to win the 2020 presidential election in Belarus.

</details>

<details>

<summary>2020-09-15 02:47:05 - Assessing variable activity for Bayesian regression trees</summary>

- *Akira Horiguchi, Matthew T. Pratola, Thomas J. Santner*

- `2005.13622v2` - [abs](http://arxiv.org/abs/2005.13622v2) - [pdf](http://arxiv.org/pdf/2005.13622v2)

> Bayesian Additive Regression Trees (BART) are non-parametric models that can capture complex exogenous variable effects. In any regression problem, it is often of interest to learn which variables are most active. Variable activity in BART is usually measured by counting the number of times a tree splits for each variable. Such one-way counts have the advantage of fast computations. Despite their convenience, one-way counts have several issues. They are statistically unjustified, cannot distinguish between main effects and interaction effects, and become inflated when measuring interaction effects. An alternative method well-established in the literature is Sobol' indices, a variance-based global sensitivity analysis technique. However, these indices often require Monte Carlo integration, which can be computationally expensive. This paper provides analytic expressions for Sobol' indices for BART posterior samples. These expressions are easy to interpret and are computationally feasible. Furthermore, we will show a fascinating connection between first-order (main-effects) Sobol' indices and one-way counts. We also introduce a novel ranking method, and use this to demonstrate that the proposed indices preserve the Sobol'-based rank order of variable importance. Finally, we compare these methods using analytic test functions and the En-ROADS climate impacts simulator.

</details>

<details>

<summary>2020-09-15 08:14:13 - Bayesian Inference for Diffusion Processes: Using Higher-Order Approximations for Transition Densities</summary>

- *Susanne Pieschner, Christiane Fuchs*

- `1806.02429v3` - [abs](http://arxiv.org/abs/1806.02429v3) - [pdf](http://arxiv.org/pdf/1806.02429v3)

> Modelling random dynamical systems in continuous time, diffusion processes are a powerful tool in many areas of science. Model parameters can be estimated from time-discretely observed processes using Markov chain Monte Carlo (MCMC) methods that introduce auxiliary data. These methods typically approximate the transition densities of the process numerically, both for calculating the posterior densities and proposing auxiliary data. Here, the Euler-Maruyama scheme is the standard approximation technique. However, the MCMC method is computationally expensive. Using higher-order approximations may accelerate it, but the specific implementation and benefit remain unclear. Hence, we investigate the utilisation and usefulness of higher-order approximations in the example of the Milstein scheme. Our study demonstrates that the MCMC methods based on the Milstein approximation yield good estimation results. However, they are computationally more expensive and can be applied to multidimensional processes only with impractical restrictions. Moreover, the combination of the Milstein approximation and the well-known modified bridge proposal introduces additional numerical challenges.

</details>

<details>

<summary>2020-09-15 13:14:37 - Efficient Bayesian generalized linear models with time-varying coefficients: The walker package in R</summary>

- *Jouni Helske*

- `2009.07063v1` - [abs](http://arxiv.org/abs/2009.07063v1) - [pdf](http://arxiv.org/pdf/2009.07063v1)

> The R package walker extends standard Bayesian general linear models to the case where the effects of the explanatory variables can vary in time. This allows, for example, to model the effects of interventions such as changes in tax policy which gradually increases their effect over time. The Markov chain Monte Carlo algorithms powering the Bayesian inference are based on Hamiltonian Monte Carlo provided by Stan software, using a state space representation of the model to marginalise over the regression coefficients for efficient low-dimensional sampling.

</details>

<details>

<summary>2020-09-15 15:48:22 - Fixed Inducing Points Online Bayesian Calibration for Computer Models with an Application to a Scale-Resolving CFD Simulation</summary>

- *Yu Duan, Matthew Eaton, Michael Bluck*

- `2009.07184v1` - [abs](http://arxiv.org/abs/2009.07184v1) - [pdf](http://arxiv.org/pdf/2009.07184v1)

> This paper proposes a novel fixed inducing points online Bayesian calibration (FIPO-BC) algorithm to efficiently learn the model parameters using a benchmark database. The standard Bayesian calibration (STD-BC) algorithm provides a statistical method to calibrate the parameters of computationally expensive models. However, the STD-BC algorithm scales very badly with the number of data points and lacks online learning capability. The proposed FIPO-BC algorithm greatly improves the computational efficiency and enables the online calibration by executing the calibration on a set of predefined inducing points.   To demonstrate the procedure of the FIPO-BC algorithm, two tests are performed, finding the optimal value and exploring the posterior distribution of 1) the parameter in a simple function, and 2) the high-wave number damping factor in a scale-resolving turbulence model (SAS-SST). The results (such as the calibrated model parameter and its posterior distribution) of FIPO-BC with different inducing points are compared to those of STD-BC. It is found that FIPO-BC and STD-BC can provide very similar results, once the predefined set of inducing point in FIPO-BC is sufficiently fine. But, the FIPO-BC algorithm is at least ten times faster than the STD-BC algorithm. Meanwhile, the online feature of the FIPO-BC allows continuous updating of the calibration outputs and potentially reduces the workload on generating the database.

</details>

<details>

<summary>2020-09-16 02:36:43 - An Extensive Experimental Evaluation of Automated Machine Learning Methods for Recommending Classification Algorithms (Extended Version)</summary>

- *Márcio P. Basgalupp, Rodrigo C. Barros, Alex G. C. de Sá, Gisele L. Pappa, Rafael G. Mantovani, André C. P. L. F. de Carvalho, Alex A. Freitas*

- `2009.07430v1` - [abs](http://arxiv.org/abs/2009.07430v1) - [pdf](http://arxiv.org/pdf/2009.07430v1)

> This paper presents an experimental comparison among four Automated Machine Learning (AutoML) methods for recommending the best classification algorithm for a given input dataset. Three of these methods are based on Evolutionary Algorithms (EAs), and the other is Auto-WEKA, a well-known AutoML method based on the Combined Algorithm Selection and Hyper-parameter optimisation (CASH) approach. The EA-based methods build classification algorithms from a single machine learning paradigm: either decision-tree induction, rule induction, or Bayesian network classification. Auto-WEKA combines algorithm selection and hyper-parameter optimisation to recommend classification algorithms from multiple paradigms. We performed controlled experiments where these four AutoML methods were given the same runtime limit for different values of this limit. In general, the difference in predictive accuracy of the three best AutoML methods was not statistically significant. However, the EA evolving decision-tree induction algorithms has the advantage of producing algorithms that generate interpretable classification models and that are more scalable to large datasets, by comparison with many algorithms from other learning paradigms that can be recommended by Auto-WEKA. We also observed that Auto-WEKA has shown meta-overfitting, a form of overfitting at the meta-learning level, rather than at the base-learning level.

</details>

<details>

<summary>2020-09-16 04:49:47 - Bayesian estimation of trend components within Markovian regime-switching models for wholesale electricity prices: an application to the South Australian wholesale electricity market</summary>

- *Angus Lewis, Nigel Bean, Giang Nguyen*

- `2009.07471v1` - [abs](http://arxiv.org/abs/2009.07471v1) - [pdf](http://arxiv.org/pdf/2009.07471v1)

> We discuss and extend methods for estimating Markovian-Regime-Switching (MRS) and trend models for wholesale electricity prices. We argue the existing methods of trend estimation used in the electricity price modelling literature either require an ambiguous definition of an extreme price, or lead to issues when implementing model selection [23]. The first main contribution of this paper is to design and infer a model which has a model-based definition of extreme prices and permits the use of model selection criteria. Due to the complexity of the MRS models inference is not straightforward. In the existing literature an approximate EM algorithm is used [26]. Another contribution of this paper is to implement exact inference in a Bayesian setting. This also allows the use of posterior predictive checks to assess model fit. We demonstrate the methodologies with South Australian electricity market.

</details>

<details>

<summary>2020-09-16 18:09:26 - A Survival Mediation Model with Bayesian Model Averaging</summary>

- *Jie Zhou, Xun Jiang, H. Amy Xia, Peng Wei, Brian P. Hobbs*

- `2009.07875v1` - [abs](http://arxiv.org/abs/2009.07875v1) - [pdf](http://arxiv.org/pdf/2009.07875v1)

> Determining the extent to which a patient is benefiting from cancer therapy is challenging. Criteria for quantifying the extent of "tumor response" observed within a few cycles of treatment have been established for various types of solid as well as hematologic malignancies. These measures comprise the primary endpoints of phase II trials. Regulatory approvals of new cancer therapies, however, are usually contingent upon the demonstration of superior overall survival with randomized evidence acquired with a phase III trial comparing the novel therapy to an appropriate standard of care treatment. With nearly two thirds of phase III oncology trials failing to achieve statistically significant results, researchers continue to refine and propose new surrogate endpoints. This article presents a Bayesian framework for studying relationships among treatment, patient subgroups, tumor response and survival. Combining classical components of mediation analysis with Bayesian model averaging (BMA), the methodology is robust to model mis-specification among various possible relationships among the observable entities. Posterior inference is demonstrated via application to a randomized controlled phase III trial in metastatic colorectal cancer. Moreover, the article details posterior predictive distributions of survival and statistical metrics for quantifying the extent of direct and indirect, or tumor response mediated, treatment effects.

</details>

<details>

<summary>2020-09-16 21:05:21 - Computationally Efficient Deep Bayesian Unit-Level Modeling of Survey Data under Informative Sampling for Small Area Estimation</summary>

- *Paul A. Parker, Scott H. Holan*

- `2009.07934v1` - [abs](http://arxiv.org/abs/2009.07934v1) - [pdf](http://arxiv.org/pdf/2009.07934v1)

> The topic of deep learning has seen a surge of interest in recent years both within and outside of the field of Statistics. Deep models leverage both nonlinearity and interaction effects to provide superior predictions in many cases when compared to linear or generalized linear models. However, one of the main challenges with deep modeling approaches is quantification of uncertainty. The use of random weight models, such as the popularized "Extreme Learning Machine," offer a potential solution in this regard. In addition to uncertainty quantification, these models are extremely computationally efficient as they do not require optimization through stochastic gradient descent, which is what is typically done for deep learning. We show how the use of random weights in a deep model can fit into a likelihood based framework to allow for uncertainty quantification of the model parameters and any desired estimates. Furthermore, we show how this approach can be used to account for informative sampling of survey data through the use of a pseudo-likelihood. We illustrate the effectiveness of this methodology through simulation and with a real survey data application involving American National Election Studies data.

</details>

<details>

<summary>2020-09-17 03:41:44 - Identification and Estimation of A Rational Inattention Discrete Choice Model with Bayesian Persuasion</summary>

- *Moyu Liao*

- `2009.08045v1` - [abs](http://arxiv.org/abs/2009.08045v1) - [pdf](http://arxiv.org/pdf/2009.08045v1)

> This paper studies the semi-parametric identification and estimation of a rational inattention model with Bayesian persuasion. The identification requires the observation of a cross-section of market-level outcomes. The empirical content of the model can be characterized by three moment conditions. A two-step estimation procedure is proposed to avoid computation complexity in the structural model. In the empirical application, I study the persuasion effect of Fox News in the 2000 presidential election. Welfare analysis shows that persuasion will not influence voters with high school education but will generate higher dispersion in the welfare of voters with a partial college education and decrease the dispersion in the welfare of voters with a bachelors degree.

</details>

<details>

<summary>2020-09-17 05:08:41 - Randomized Reduced Forward Models for Efficient Metropolis--Hastings MCMC, with Application to Subsurface Fluid Flow and Capacitance Tomography</summary>

- *Colin Fox, Tiangang Cui, Markus Neumayer*

- `2009.08782v1` - [abs](http://arxiv.org/abs/2009.08782v1) - [pdf](http://arxiv.org/pdf/2009.08782v1)

> Bayesian modelling and computational inference by Markov chain Monte Carlo (MCMC) is a principled framework for large-scale uncertainty quantification, though is limited in practice by computational cost when implemented in the simplest form that requires simulating an accurate computer model at each iteration of the MCMC. The delayed acceptance Metropolis--Hastings MCMC leverages a reduced model for the forward map to lower the compute cost per iteration, though necessarily reduces statistical efficiency that can, without care, lead to no reduction in the computational cost of computing estimates to a desired accuracy. Randomizing the reduced model for the forward map can dramatically improve computational efficiency, by maintaining the low cost per iteration but also avoiding appreciable loss of statistical efficiency. Randomized maps are constructed by a posteriori adaptive tuning of a randomized and locally-corrected deterministic reduced model. Equivalently, the approximated posterior distribution may be viewed as induced by a modified likelihood function for use with the reduced map, with parameters tuned to optimize the quality of the approximation to the correct posterior distribution. Conditions for adaptive MCMC algorithms allow practical approximations and algorithms that have guaranteed ergodicity for the target distribution. Good statistical and computational efficiencies are demonstrated in examples of calibration of large-scale numerical models of geothermal reservoirs and electrical capacitance tomography.

</details>

<details>

<summary>2020-09-17 06:58:35 - Factor Investing: A Bayesian Hierarchical Approach</summary>

- *Guanhao Feng, Jingyu He*

- `1902.01015v3` - [abs](http://arxiv.org/abs/1902.01015v3) - [pdf](http://arxiv.org/pdf/1902.01015v3)

> This paper investigates asset allocation problems when returns are predictable. We introduce a market-timing Bayesian hierarchical (BH) approach that adopts heterogeneous time-varying coefficients driven by lagged fundamental characteristics. Our approach includes a joint estimation of conditional expected returns and covariance matrix and considers estimation risk for portfolio analysis. The hierarchical prior allows modeling different assets separately while sharing information across assets. We demonstrate the performance of the U.S. equity market. Though the Bayesian forecast is slightly biased, our BH approach outperforms most alternative methods in point and interval prediction. Our BH approach in sector investment for the recent twenty years delivers a 0.92\% average monthly returns and a 0.32\% significant Jensen`s alpha. We also find technology, energy, and manufacturing are important sectors in the past decade, and size, investment, and short-term reversal factors are heavily weighted. Finally, the stochastic discount factor constructed by our BH approach explains most anomalies.

</details>

<details>

<summary>2020-09-17 09:21:46 - Mean-Variance Analysis in Bayesian Optimization under Uncertainty</summary>

- *Shogo Iwazaki, Yu Inatsu, Ichiro Takeuchi*

- `2009.08166v1` - [abs](http://arxiv.org/abs/2009.08166v1) - [pdf](http://arxiv.org/pdf/2009.08166v1)

> We consider active learning (AL) in an uncertain environment in which trade-off between multiple risk measures need to be considered. As an AL problem in such an uncertain environment, we study Mean-Variance Analysis in Bayesian Optimization (MVA-BO) setting. Mean-variance analysis was developed in the field of financial engineering and has been used to make decisions that take into account the trade-off between the average and variance of investment uncertainty. In this paper, we specifically focus on BO setting with an uncertain component and consider multi-task, multi-objective, and constrained optimization scenarios for the mean-variance trade-off of the uncertain component. When the target blackbox function is modeled by Gaussian Process (GP), we derive the bounds of the two risk measures and propose AL algorithm for each of the above three problems based on the risk measure bounds. We show the effectiveness of the proposed AL algorithms through theoretical analysis and numerical experiments.

</details>

<details>

<summary>2020-09-17 13:05:26 - Spike and Slab Pólya tree posterior distributions: adaptive inference</summary>

- *Ismaël Castillo, Romain Mismer*

- `1911.12106v2` - [abs](http://arxiv.org/abs/1911.12106v2) - [pdf](http://arxiv.org/pdf/1911.12106v2)

> In the density estimation model, the question of adaptive inference using P\'olya tree-type prior distributions is considered. A class of prior densities having a tree structure, called spike-and-slab P\'olya trees, is introduced. For this class, two types of results are obtained: first, the Bayesian posterior distribution is shown to converge at the minimax rate for the supremum norm in an adaptive way, for any H\"older regularity of the true density between $0$ and $1$, thereby providing adaptive counterparts to the results for classical P\'olya trees in Castillo (2017). Second, the question of uncertainty quantification is considered. An adaptive nonparametric Bernstein-von Mises theorem is derived. Next, it is shown that, under a self-similarity condition on the true density, certain credible sets from the posterior distribution are adaptive confidence bands, having prescribed coverage level and with a diameter shrinking at optimal rate in the minimax sense.

</details>

<details>

<summary>2020-09-17 15:50:04 - Component-wise approximate Bayesian computation via Gibbs-like steps</summary>

- *Grégoire Clarté, Christian P. Robert, Robin Ryder, Julien Stoehr*

- `1905.13599v5` - [abs](http://arxiv.org/abs/1905.13599v5) - [pdf](http://arxiv.org/pdf/1905.13599v5)

> Approximate Bayesian computation methods are useful for generative models with intractable likelihoods. These methods are however sensitive to the dimension of the parameter space, requiring exponentially increasing resources as this dimension grows. To tackle this difficulty, we explore a Gibbs version of the ABC approach that runs component-wise approximate Bayesian computation steps aimed at the corresponding conditional posterior distributions, and based on summary statistics of reduced dimensions. While lacking the standard justifications for the Gibbs sampler, the resulting Markov chain is shown to converge in distribution under some partial independence conditions. The associated stationary distribution can further be shown to be close to the true posterior distribution and some hierarchical versions of the proposed mechanism enjoy a closed form limiting distribution. Experiments also demonstrate the gain in efficiency brought by the Gibbs version over the standard solution.

</details>

<details>

<summary>2020-09-17 17:05:16 - Utilizing remote sensing data in forest inventory sampling via Bayesian optimization</summary>

- *Jonne Pohjankukka, Sakari Tuominen, Jukka Heikkonen*

- `2009.08420v1` - [abs](http://arxiv.org/abs/2009.08420v1) - [pdf](http://arxiv.org/pdf/2009.08420v1)

> In large-area forest inventories a trade-off between the amount of data to be sampled and the costs of collecting the data is necessary. It is not always possible to have a very large data sample when dealing with sampling-based inventories. It is therefore necessary to optimize the sampling design in order to achieve optimal population parameter estimation. On the contrary, the availability of remote sensing (RS) data correlated with the forest inventory variables is usually much higher. The combination of RS and the sampled field measurement data is often used for improving the forest inventory parameter estimation. In addition, it is also reasonable to study the utilization of RS data in inventory sampling, which can further improve the estimation of forest variables. In this study, we propose a data sampling method based on Bayesian optimization which uses RS data in forest inventory sample selection. The presented method applies the learned functional relationship between the RS and inventory data in new sampling decisions. We evaluate our method by conducting simulated sampling experiments with both synthetic data and measured data from the Aland region in Finland. The proposed method is benchmarked against two baseline methods: simple random sampling and the local pivotal method. The results of the simulated experiments show the best results in terms of MSE values for the proposed method when the functional relationship between RS and inventory data is correctly learned from the available training data.

</details>

<details>

<summary>2020-09-18 19:24:47 - A Bayesian Time-Varying Effect Model for Behavioral mHealth Data</summary>

- *Matthew D. Koslovsky, Emily T. Hebert, Michael S. Businelle, Marina Vannucci*

- `2009.09034v1` - [abs](http://arxiv.org/abs/2009.09034v1) - [pdf](http://arxiv.org/pdf/2009.09034v1)

> The integration of mobile health (mHealth) devices into behavioral health research has fundamentally changed the way researchers and interventionalists are able to collect data as well as deploy and evaluate intervention strategies. In these studies, researchers often collect intensive longitudinal data (ILD) using ecological momentary assessment methods, which aim to capture psychological, emotional, and environmental factors that may relate to a behavioral outcome in near real-time. In order to investigate ILD collected in a novel, smartphone-based smoking cessation study, we propose a Bayesian variable selection approach for time-varying effect models, designed to identify dynamic relations between potential risk factors and smoking behaviors in the critical moments around a quit attempt. We use parameter-expansion and data-augmentation techniques to efficiently explore how the underlying structure of these relations varies over time and across subjects. We achieve deeper insights into these relations by introducing nonparametric priors for regression coefficients that cluster similar effects for risk factors while simultaneously determining their inclusion. Results indicate that our approach is well-positioned to help researchers effectively evaluate, design, and deliver tailored intervention strategies in the critical moments surrounding a quit attempt.

</details>

<details>

<summary>2020-09-18 20:58:02 - Bayesian Causal Inference with Bipartite Record Linkage</summary>

- *Sharmistha Guha, Jerome P. Reiter, Andrea Mercatanti*

- `2002.09119v2` - [abs](http://arxiv.org/abs/2002.09119v2) - [pdf](http://arxiv.org/pdf/2002.09119v2)

> In many scenarios, the observational data needed for causal inferences are spread over two data files. In particular, we consider scenarios where one file includes covariates and the treatment measured on one set of individuals, and a second file includes responses measured on another, partially overlapping set of individuals. In the absence of error free direct identifiers like social security numbers, straightforward merging of separate files is not feasible, so that records must be linked using error-prone variables such as names, birth dates, and demographic characteristics. Typical practice in such situations generally follows a two-stage procedure: first link the two files using a probabilistic linkage technique, then make causal inferences with the linked dataset. This does not propagate uncertainty due to imperfect linkages to the causal inference, nor does it leverage relationships among the study variables to improve the quality of the linkages. We propose a hierarchical model for simultaneous Bayesian inference on probabilistic linkage and causal effects that addresses these deficiencies. Using simulation studies and theoretical arguments, we show the hierarchical model can improve the accuracy of estimated treatment effects, as well as the record linkages, compared to the two-stage modeling option. We illustrate the hierarchical model using a causal study of the effects of debit card possession on household spending.

</details>

<details>

<summary>2020-09-19 14:58:34 - Posterior Averaging Information Criterion</summary>

- *Shouhao Zhou*

- `2009.09248v1` - [abs](http://arxiv.org/abs/2009.09248v1) - [pdf](http://arxiv.org/pdf/2009.09248v1)

> We propose a new model selection method, the posterior averaging information criterion, for Bayesian model assessment from a predictive perspective. The theoretical foundation is built on the Kullback-Leibler divergence to quantify the similarity between the proposed candidate model and the underlying true model. From a Bayesian perspective, our method evaluates the candidate models over the entire posterior distribution in terms of predicting a future independent observation. Without assuming that the true distribution is contained in the candidate models, the new criterion is developed by correcting the asymptotic bias of the posterior mean of the log-likelihood against its expected log-likelihood. It can be generally applied even for Bayesian models with degenerate non-informative prior. The simulation in both normal and binomial settings demonstrates decent small sample performance.

</details>

<details>

<summary>2020-09-19 15:07:48 - Anomaly Detection with HMM Gauge Likelihood Analysis</summary>

- *Boris Lorbeer, Tanja Deutsch, Peter Ruppel, Axel Küpper*

- `1906.06134v2` - [abs](http://arxiv.org/abs/1906.06134v2) - [pdf](http://arxiv.org/pdf/1906.06134v2)

> This paper describes a new method, HMM gauge likelihood analysis, or GLA, of detecting anomalies in discrete time series using Hidden Markov Models and clustering. At the center of the method lies the comparison of subsequences. To achieve this, they first get assigned to their Hidden Markov Models using the Baum-Welch algorithm. Next, those models are described by an approximating representation of the probability distributions they define. Finally, this representation is then analyzed with the help of some clustering technique or other outlier detection tool and anomalies are detected. Clearly, HMMs could be substituted by some other appropriate model, e.g. some other dynamic Bayesian network. Our learning algorithm is unsupervised, so it does not require the labeling of large amounts of data. The usability of this method is demonstrated by applying it to synthetic and real-world syslog data.

</details>

<details>

<summary>2020-09-19 17:59:26 - Density estimation and modeling on symmetric spaces</summary>

- *Didong Li, Yulong Lu, Emmanuel Chevallier, David B. Dunson*

- `2009.01983v3` - [abs](http://arxiv.org/abs/2009.01983v3) - [pdf](http://arxiv.org/pdf/2009.01983v3)

> In many applications, data and/or parameters are supported on non-Euclidean manifolds. It is important to take into account the geometric structure of manifolds in statistical analysis to avoid misleading results. Although there has been a considerable focus on simple and specific manifolds, there is a lack of general and easy-to-implement statistical methods for density estimation and modeling on manifolds. In this article, we consider a very broad class of manifolds: non-compact Riemannian symmetric spaces. For this class, we provide a very general mathematical result for easily calculating volume changes of the exponential and logarithm map between the tangent space and the manifold. This allows one to define statistical models on the tangent space, push these models forward onto the manifold, and easily calculate induced distributions by Jacobians. To illustrate the statistical utility of this theoretical result, we provide a general method to construct distributions on symmetric spaces. In particular, we define the log-Gaussian distribution as an analogue of the multivariate Gaussian distribution in Euclidean space. With these new kernels on symmetric spaces, we also consider the problem of density estimation. Our proposed approach can use any existing density estimation approach designed for Euclidean spaces and push it forward to the manifold with an easy-to-calculate adjustment. We provide theorems showing that the induced density estimators on the manifold inherit the statistical optimality properties of the parent Euclidean density estimator; this holds for both frequentist and Bayesian nonparametric methods. We illustrate the theory and practical utility of the proposed approach on the space of positive definite matrices.

</details>

<details>

<summary>2020-09-19 19:27:35 - Early detection of the advanced persistent threat attack using performance analysis of deep learning</summary>

- *Javad Hassannataj Joloudari, Mojtaba Haderbadi, Amir Mashmool, Mohammad GhasemiGol, Shahab S., Amir Mosavi*

- `2009.10524v1` - [abs](http://arxiv.org/abs/2009.10524v1) - [pdf](http://arxiv.org/pdf/2009.10524v1)

> One of the most common and important destructive attacks on the victim system is Advanced Persistent Threat (APT)-attack. The APT attacker can achieve his hostile goals by obtaining information and gaining financial benefits regarding the infrastructure of a network. One of the solutions to detect a secret APT attack is using network traffic. Due to the nature of the APT attack in terms of being on the network for a long time and the fact that the network may crash because of high traffic, it is difficult to detect this type of attack. Hence, in this study, machine learning methods such as C5.0 decision tree, Bayesian network and deep neural network are used for timely detection and classification of APT-attacks on the NSL-KDD dataset. Moreover, 10-fold cross validation method is used to experiment these models. As a result, the accuracy (ACC) of the C5.0 decision tree, Bayesian network and 6-layer deep learning models is obtained as 95.64%, 88.37% and 98.85%, respectively, and also, in terms of the important criterion of the false positive rate (FPR), the FPR value for the C5.0 decision tree, Bayesian network and 6-layer deep learning models is obtained as 2.56, 10.47 and 1.13, respectively. Other criterions such as sensitivity, specificity, accuracy, false negative rate and F-measure are also investigated for the models, and the experimental results show that the deep learning model with automatic multi-layered extraction of features has the best performance for timely detection of an APT-attack comparing to other classification models.

</details>

<details>

<summary>2020-09-20 07:29:45 - Sequential Ensemble Transform for Bayesian Inverse Problems</summary>

- *Aaron Myers, Alexandre H. Thiery, Kainan Wang, Tan Bui-Thanh*

- `1909.09591v2` - [abs](http://arxiv.org/abs/1909.09591v2) - [pdf](http://arxiv.org/pdf/1909.09591v2)

> We present the Sequential Ensemble Transform (SET) method, an approach for generating approximate samples from a Bayesian posterior distribution. The method explores the posterior distribution by solving a sequence of discrete optimal transport problems to produce a series of transport plans which map prior samples to posterior samples. We prove that the sequence of Dirac mixture distributions produced by the SET method converges weakly to the true posterior as the sample size approaches infinity. Furthermore, our numerical results indicate that, when compared to standard Sequential Monte Carlo (SMC) methods, the SET approach is more robust to the choice of Markov mutation kernels and requires less computational efforts to reach a similar accuracy when used to explore complex posterior distributions. Finally, we describe adaptive schemes that allow to completely automate the use of the SET method.

</details>

<details>

<summary>2020-09-20 13:27:04 - Consistency of Bayesian Inference for Multivariate Max-Stable Distributions</summary>

- *Simone A. Padoan, Stefano Rizzelli*

- `1904.00245v3` - [abs](http://arxiv.org/abs/1904.00245v3) - [pdf](http://arxiv.org/pdf/1904.00245v3)

> Predicting extreme events is important in many applications in risk analysis. The extreme-value theory suggests modelling extremes by max-stable distributions. The Bayesian approach provides a natural framework for statistical prediction. Although various Bayesian inferential procedures have been proposed in the literature of univariate extremes and some for multivariate extremes, the study of their asymptotic properties has been left largely untouched. In this paper we focus on a semiparatric Bayesian method for estimating max-stable distributions in arbitrary dimension. We establish consistency of the pertaining posterior distributions for fairly general, well-specified max-stable models, whose margins can be short-, light- or heavy-tailed. We then extend our consistency results to the case where the data come from a distribution lying in a neighbourhood of a max-stable one, which represents the most realistic inferential setting.

</details>

<details>

<summary>2020-09-20 14:05:04 - Nonparametric Bayesian multi-armed bandits for single cell experiment design</summary>

- *Federico Camerlenghi, Bianca Dumitrascu, Federico Ferrari, Barbara E. Engelhardt, Stefano Favaro*

- `1910.05355v2` - [abs](http://arxiv.org/abs/1910.05355v2) - [pdf](http://arxiv.org/pdf/1910.05355v2)

> The problem of maximizing cell type discovery under budget constraints is a fundamental challenge for the collection and analysis of single-cell RNA-sequencing (scRNA-seq) data. In this paper, we introduce a simple, computationally efficient, and scalable Bayesian nonparametric sequential approach to optimize the budget allocation when designing a large scale experiment for the collection of scRNA-seq data for the purpose of, but not limited to, creating cell atlases. Our approach relies on the following tools: i) a hierarchical Pitman-Yor prior that recapitulates biological assumptions regarding cellular differentiation, and ii) a Thompson sampling multi-armed bandit strategy that balances exploitation and exploration to prioritize experiments across a sequence of trials. Posterior inference is performed by using a sequential Monte Carlo approach, which allows us to fully exploit the sequential nature of our species sampling problem. We empirically show that our approach outperforms state-of-the-art methods and achieves near-Oracle performance on simulated and scRNA-seq data alike. HPY-TS code is available at https://github.com/fedfer/HPYsinglecell.

</details>

<details>

<summary>2020-09-20 14:30:27 - The Significance Filter, the Winner's Curse and the Need to Shrink</summary>

- *Erik van Zwet, Eric Cator*

- `2009.09440v1` - [abs](http://arxiv.org/abs/2009.09440v1) - [pdf](http://arxiv.org/pdf/2009.09440v1)

> The "significance filter" refers to focusing exclusively on statistically significant results. Since frequentist properties such as unbiasedness and coverage are valid only before the data have been observed, there are no guarantees if we condition on significance. In fact, the significance filter leads to overestimation of the magnitude of the parameter, which has been called the "winner's curse". It can also lead to undercoverage of the confidence interval. Moreover, these problems become more severe if the power is low. While these issues clearly deserve our attention, they have been studied only informally and mathematical results are lacking. Here we study them from the frequentist and the Bayesian perspective. We prove that the relative bias of the magnitude is a decreasing function of the power and that the usual confidence interval undercovers when the power is less than 50%. We conclude that failure to apply the appropriate amount of shrinkage can lead to misleading inferences.

</details>

<details>

<summary>2020-09-20 17:23:00 - Bayesian Variable Selection for Gaussian copula regression models</summary>

- *Angelos Alexopoulos, Leonardo Bottolo*

- `1907.08245v2` - [abs](http://arxiv.org/abs/1907.08245v2) - [pdf](http://arxiv.org/pdf/1907.08245v2)

> We develop a novel Bayesian method to select important predictors in regression models with multiple responses of diverse types. A sparse Gaussian copula regression model is used to account for the multivariate dependencies between any combination of discrete and/or continuous responses and their association with a set of predictors. We utilize the parameter expansion for data augmentation strategy to construct a Markov chain Monte Carlo algorithm for the estimation of the parameters and the latent variables of the model. Based on a centered parametrization of the Gaussian latent variables, we design a fixed-dimensional proposal distribution to update jointly the latent binary vectors of important predictors and the corresponding non-zero regression coefficients. For Gaussian responses and for outcomes that can be modeled as a dependent version of a Gaussian response, this proposal leads to a Metropolis-Hastings step that allows an efficient exploration of the predictors' model space. The proposed strategy is tested on simulated data and applied to real data sets in which the responses consist of low-intensity counts, binary, ordinal and continuous variables.

</details>

<details>

<summary>2020-09-20 22:03:39 - Stochastic Gradient Langevin Dynamics Algorithms with Adaptive Drifts</summary>

- *Sehwan Kim, Qifan Song, Faming Liang*

- `2009.09535v1` - [abs](http://arxiv.org/abs/2009.09535v1) - [pdf](http://arxiv.org/pdf/2009.09535v1)

> Bayesian deep learning offers a principled way to address many issues concerning safety of artificial intelligence (AI), such as model uncertainty,model interpretability, and prediction bias. However, due to the lack of efficient Monte Carlo algorithms for sampling from the posterior of deep neural networks (DNNs), Bayesian deep learning has not yet powered our AI system. We propose a class of adaptive stochastic gradient Markov chain Monte Carlo (SGMCMC) algorithms, where the drift function is biased to enhance escape from saddle points and the bias is adaptively adjusted according to the gradient of past samples. We establish the convergence of the proposed algorithms under mild conditions, and demonstrate via numerical examples that the proposed algorithms can significantly outperform the existing SGMCMC algorithms, such as stochastic gradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian Monte Carlo (SGHMC) and preconditioned SGLD, in both simulation and optimization tasks.

</details>

<details>

<summary>2020-09-21 02:41:20 - Modeling Score Distributions and Continuous Covariates: A Bayesian Approach</summary>

- *Mel McCurrie, Hamish Nicholson, Walter J. Scheirer, Samuel Anthony*

- `2009.09583v1` - [abs](http://arxiv.org/abs/2009.09583v1) - [pdf](http://arxiv.org/pdf/2009.09583v1)

> Computer Vision practitioners must thoroughly understand their model's performance, but conditional evaluation is complex and error-prone. In biometric verification, model performance over continuous covariates---real-number attributes of images that affect performance---is particularly challenging to study. We develop a generative model of the match and non-match score distributions over continuous covariates and perform inference with modern Bayesian methods. We use mixture models to capture arbitrary distributions and local basis functions to capture non-linear, multivariate trends. Three experiments demonstrate the accuracy and effectiveness of our approach. First, we study the relationship between age and face verification performance and find previous methods may overstate performance and confidence. Second, we study preprocessing for CNNs and find a highly non-linear, multivariate surface of model performance. Our method is accurate and data efficient when evaluated against previous synthetic methods. Third, we demonstrate the novel application of our method to pedestrian tracking and calculate variable thresholds and expected performance while controlling for multiple covariates.

</details>

<details>

<summary>2020-09-21 05:26:07 - Interactive Steering of Hierarchical Clustering</summary>

- *Weikai Yang, Xiting Wang, Jie Lu, Wenwen Dou, Shixia Liu*

- `2009.09618v1` - [abs](http://arxiv.org/abs/2009.09618v1) - [pdf](http://arxiv.org/pdf/2009.09618v1)

> Hierarchical clustering is an important technique to organize big data for exploratory data analysis. However, existing one-size-fits-all hierarchical clustering methods often fail to meet the diverse needs of different users. To address this challenge, we present an interactive steering method to visually supervise constrained hierarchical clustering by utilizing both public knowledge (e.g., Wikipedia) and private knowledge from users. The novelty of our approach includes 1) automatically constructing constraints for hierarchical clustering using knowledge (knowledge-driven) and intrinsic data distribution (data-driven), and 2) enabling the interactive steering of clustering through a visual interface (user-driven). Our method first maps each data item to the most relevant items in a knowledge base. An initial constraint tree is then extracted using the ant colony optimization algorithm. The algorithm balances the tree width and depth and covers the data items with high confidence. Given the constraint tree, the data items are hierarchically clustered using evolutionary Bayesian rose tree. To clearly convey the hierarchical clustering results, an uncertainty-aware tree visualization has been developed to enable users to quickly locate the most uncertain sub-hierarchies and interactively improve them. The quantitative evaluation and case study demonstrate that the proposed approach facilitates the building of customized clustering trees in an efficient and effective manner.

</details>

<details>

<summary>2020-09-21 10:03:56 - Toroidal diffusions and protein structure evolution</summary>

- *Eduardo García-Portugués, Michael Golden, Michael Sørensen, Kanti V. Mardia, Thomas Hamelryck, Jotun Hein*

- `1804.00285v2` - [abs](http://arxiv.org/abs/1804.00285v2) - [pdf](http://arxiv.org/pdf/1804.00285v2)

> This chapter shows how toroidal diffusions are convenient methodological tools for modelling protein evolution in a probabilistic framework. The chapter addresses the construction of ergodic diffusions with stationary distributions equal to well-known directional distributions, which can be regarded as toroidal analogues of the Ornstein-Uhlenbeck process. The important challenges that arise in the estimation of the diffusion parameters require the consideration of tractable approximate likelihoods and, among the several approaches introduced, the one yielding a specific approximation to the transition density of the wrapped normal process is shown to give the best empirical performance on average. This provides the methodological building block for Evolutionary Torus Dynamic Bayesian Network (ETDBN), a hidden Markov model for protein evolution that emits a wrapped normal process and two continuous-time Markov chains per hidden state. The chapter describes the main features of ETDBN, which allows for both "smooth" conformational changes and "catastrophic" conformational jumps, and several empirical benchmarks. The insights into the relationship between sequence and structure evolution that ETDBN provides are illustrated in a case study.

</details>

<details>

<summary>2020-09-21 14:26:03 - Parameter clustering in Bayesian functional PCA of fMRI data</summary>

- *Nicolo' Margaritella, Vanda Inacio, Ruth King*

- `1904.11758v3` - [abs](http://arxiv.org/abs/1904.11758v3) - [pdf](http://arxiv.org/pdf/1904.11758v3)

> The extraordinary advancements in neuroscientific technology for brain recordings over the last decades have led to increasingly complex spatio-temporal datasets. To reduce oversimplifications, new models have been developed to be able to identify meaningful patterns and new insights within a highly demanding data environment. To this extent, we propose a new model called parameter clustering functional Principal Component Analysis (PCl-fPCA) that merges ideas from Functional Data Analysis and Bayesian nonparametrics to obtain a flexible and computationally feasible signal reconstruction and exploration of spatio-temporal neuroscientific data. In particular, we use a Dirichlet process Gaussian mixture model to cluster functional principal component scores within the standard Bayesian functional PCA framework. This approach captures the spatial dependence structure among smoothed time series (curves) and its interaction with the time domain without imposing a prior spatial structure on the data. Moreover, by moving the mixture from data to functional principal component scores, we obtain a more general clustering procedure, thus allowing a higher level of intricate insight and understanding of the data. We present results from a simulation study showing improvements in curve and correlation reconstruction compared with different Bayesian and frequentist fPCA models and we apply our method to functional Magnetic Resonance Imaging and Electroencephalogram data analyses providing a rich exploration of the spatio-temporal dependence in brain time series.

</details>

<details>

<summary>2020-09-21 17:19:09 - Source Separation with Deep Generative Priors</summary>

- *Vivek Jayaram, John Thickstun*

- `2002.07942v2` - [abs](http://arxiv.org/abs/2002.07942v2) - [pdf](http://arxiv.org/pdf/2002.07942v2)

> Despite substantial progress in signal source separation, results for richly structured data continue to contain perceptible artifacts. In contrast, recent deep generative models can produce authentic samples in a variety of domains that are indistinguishable from samples of the data distribution. This paper introduces a Bayesian approach to source separation that uses generative models as priors over the components of a mixture of sources, and noise-annealed Langevin dynamics to sample from the posterior distribution of sources given a mixture. This decouples the source separation problem from generative modeling, enabling us to directly use cutting-edge generative models as priors. The method achieves state-of-the-art performance for MNIST digit separation. We introduce new methodology for evaluating separation quality on richer datasets, providing quantitative evaluation of separation results on CIFAR-10. We also provide qualitative results on LSUN.

</details>

<details>

<summary>2020-09-21 17:22:06 - Thompson Sampling for Combinatorial Network Optimization in Unknown Environments</summary>

- *Alihan Hüyük, Cem Tekin*

- `1907.04201v3` - [abs](http://arxiv.org/abs/1907.04201v3) - [pdf](http://arxiv.org/pdf/1907.04201v3)

> Influence maximization, adaptive routing, and dynamic spectrum allocation all require choosing the right action from a large set of alternatives. Thanks to the advances in combinatorial optimization, these and many similar problems can be efficiently solved given an environment with known stochasticity. In this paper, we take this one step further and focus on combinatorial optimization in unknown environments. We consider a very general learning framework called combinatorial multi-armed bandit with probabilistically triggered arms and a very powerful Bayesian algorithm called Combinatorial Thompson Sampling (CTS). Under the semi-bandit feedback model and assuming access to an oracle without knowing the expected base arm outcomes beforehand, we show that when the expected reward is Lipschitz continuous in the expected base arm outcomes CTS achieves $O(\sum_{i =1}^m\log T/(p_i\Delta_i))$ regret and $O(\max\{\mathbb{E}[m\sqrt{T\log T/p^*}],\mathbb{E}[m^2/p^*]\})$ Bayesian regret, where $m$ denotes the number of base arms, $p_i$ and $\Delta_i$ denote the minimum non-zero triggering probability and the minimum suboptimality gap of base arm $i$ respectively, $T$ denotes the time horizon, and $p^*$ denotes the overall minimum non-zero triggering probability. We also show that when the expected reward satisfies the triggering probability modulated Lipschitz continuity, CTS achieves $O(\max\{m\sqrt{T\log T},m^2\})$ Bayesian regret, and when triggering probabilities are non-zero for all base arms, CTS achieves $O(1/p^*\log(1/p^*))$ regret independent of the time horizon. Finally, we numerically compare CTS with algorithms based on upper confidence bounds in several networking problems and show that CTS outperforms these algorithms by at least an order of magnitude in majority of the cases.

</details>

<details>

<summary>2020-09-21 17:48:52 - Fast Convergence for Langevin Diffusion with Manifold Structure</summary>

- *Ankur Moitra, Andrej Risteski*

- `2002.05576v2` - [abs](http://arxiv.org/abs/2002.05576v2) - [pdf](http://arxiv.org/pdf/2002.05576v2)

> In this paper, we study the problem of sampling from distributions of the form p(x) \propto e^{-\beta f(x)} for some function f whose values and gradients we can query. This mode of access to f is natural in the scenarios in which such problems arise, for instance sampling from posteriors in parametric Bayesian models. Classical results show that a natural random walk, Langevin diffusion, mixes rapidly when f is convex. Unfortunately, even in simple examples, the applications listed above will entail working with functions f that are nonconvex -- for which sampling from p may in general require an exponential number of queries.   In this paper, we focus on an aspect of nonconvexity relevant for modern machine learning applications: existence of invariances (symmetries) in the function f, as a result of which the distribution p will have manifolds of points with equal probability. First, we give a recipe for proving mixing time bounds for Langevin diffusion as a function of the geometry of these manifolds. Second, we specialize our arguments to classic matrix factorization-like Bayesian inference problems where we get noisy measurements A(XX^T), X \in R^{d \times k} of a low-rank matrix, i.e. f(X) = \|A(XX^T) - b\|^2_2, X \in R^{d \times k}, and \beta the inverse of the variance of the noise. Such functions f are invariant under orthogonal transformations, and include problems like matrix factorization, sensing, completion. Beyond sampling, Langevin dynamics is a popular toy model for studying stochastic gradient descent. Along these lines, we believe that our work is an important first step towards understanding how SGD behaves when there is a high degree of symmetry in the space of parameters the produce the same output.

</details>

<details>

<summary>2020-09-21 21:32:43 - Leveraging Cross Feedback of User and Item Embeddings with Attention for Variational Autoencoder based Collaborative Filtering</summary>

- *Yuan Jin, He Zhao, Ming Liu, Ye Zhu, Lan Du, Longxiang Gao, Wray Buntine*

- `2002.09145v2` - [abs](http://arxiv.org/abs/2002.09145v2) - [pdf](http://arxiv.org/pdf/2002.09145v2)

> Matrix factorization (MF) has been widely applied to collaborative filtering in recommendation systems. Its Bayesian variants can derive posterior distributions of user and item embeddings, and are more robust to sparse ratings. However, the Bayesian methods are restricted by their update rules for the posterior parameters due to the conjugacy of the priors and the likelihood. Variational autoencoders (VAE) can address this issue by capturing complex mappings between the posterior parameters and the data. However, current research on VAEs for collaborative filtering only considers the mappings based on the explicit data information while the implicit embedding information is overlooked. In this paper, we first derive evidence lower bounds (ELBO) for Bayesian MF models from two viewpoints: user-oriented and item-oriented. Based on the ELBOs, we propose a VAE-based Bayesian MF framework. It leverages not only the data but also the embedding information to approximate the user-item joint distribution. As suggested by the ELBOs, the approximation is iterative with cross feedback of user and item embeddings into each other's encoders. More specifically, user embeddings sampled at the previous iteration are fed to the item-side encoders to estimate the posterior parameters for the item embeddings at the current iteration, and vice versa. The estimation also attends to the cross-fed embeddings to further exploit useful information. The decoder then reconstructs the data via the matrix factorization over the currently re-sampled user and item embeddings.

</details>

<details>

<summary>2020-09-22 00:46:40 - Uncertainty-aware Attention Graph Neural Network for Defending Adversarial Attacks</summary>

- *Boyuan Feng, Yuke Wang, Zheng Wang, Yufei Ding*

- `2009.10235v1` - [abs](http://arxiv.org/abs/2009.10235v1) - [pdf](http://arxiv.org/pdf/2009.10235v1)

> With the increasing popularity of graph-based learning, graph neural networks (GNNs) emerge as the essential tool for gaining insights from graphs. However, unlike the conventional CNNs that have been extensively explored and exhaustively tested, people are still worrying about the GNNs' robustness under the critical settings, such as financial services. The main reason is that existing GNNs usually serve as a black-box in predicting and do not provide the uncertainty on the predictions. On the other side, the recent advancement of Bayesian deep learning on CNNs has demonstrated its success of quantifying and explaining such uncertainties to fortify CNN models. Motivated by these observations, we propose UAG, the first systematic solution to defend adversarial attacks on GNNs through identifying and exploiting hierarchical uncertainties in GNNs. UAG develops a Bayesian Uncertainty Technique (BUT) to explicitly capture uncertainties in GNNs and further employs an Uncertainty-aware Attention Technique (UAT) to defend adversarial attacks on GNNs. Intensive experiments show that our proposed defense approach outperforms the state-of-the-art solutions by a significant margin.

</details>

<details>

<summary>2020-09-22 04:52:12 - Computational Techniques for Parameter Estimation of Gravitational Wave Signals</summary>

- *Renate Meyer, Matthew C. Edwards, Patricio Maturana-Russel, Nelson Christensen*

- `2009.10316v1` - [abs](http://arxiv.org/abs/2009.10316v1) - [pdf](http://arxiv.org/pdf/2009.10316v1)

> Since the very first detection of gravitational waves from the coalescence of two black holes in 2015, Bayesian statistical methods have been routinely applied by LIGO and Virgo to extract the signal out of noisy interferometric measurements, obtain point estimates of the physical parameters responsible for producing the signal, and rigorously quantify their uncertainties. Different computational techniques have been devised depending on the source of the gravitational radiation and the gravitational waveform model used. Prominent sources of gravitational waves are binary black hole or neutron star mergers, the only objects that have been observed by detectors to date. But also gravitational waves from core collapse supernovae, rapidly rotating neutron stars, and the stochastic gravitational wave background are in the sensitivity band of the ground-based interferometers and expected to be observable in future observation runs. As nonlinearities of the complex waveforms and the high-dimensional parameter spaces preclude analytic evaluation of the posterior distribution, posterior inference for all these sources relies on computer-intensive simulation techniques such as Markov chain Monte Carlo methods. A review of state-of-the-art Bayesian statistical parameter estimation methods will be given for researchers in this cross-disciplinary area of gravitational wave data analysis.

</details>

<details>

<summary>2020-09-22 11:56:48 - Spatio-temporal modelling of $\text{PM}_{10}$ daily concentrations in Italy using the SPDE approach</summary>

- *Guido Fioravanti, Sara Martino, Michela Cameletti, Giorgio Cattani*

- `2009.10476v1` - [abs](http://arxiv.org/abs/2009.10476v1) - [pdf](http://arxiv.org/pdf/2009.10476v1)

> This paper illustrates the main results of a spatio-temporal interpolation process of $\text{PM}_{10}$ concentrations at daily resolution using a set of 410 monitoring sites, distributed throughout the Italian territory, for the year 2015. The interpolation process is based on a Bayesian hierarchical model where the spatial-component is represented through the Stochastic Partial Differential Equation (SPDE) approach with a lag-1 temporal autoregressive component (AR1). Inference is performed through the Integrated Nested Laplace Approximation (INLA). Our model includes 11 spatial and spatio-temporal predictors, including meteorological variables and Aerosol Optical Depth. As the predictors' impact varies across months, the regression is based on 12 monthly models with the same set of covariates. The predictive model performance has been analyzed using a cross-validation study. Our results show that the predicted and the observed values are well in accordance (correlation range: 0.79 - 0.91; bias: 0.22 - 1.07 $\mu \text{g}/\text{m}^3$; RMSE: 4.9 - 13.9 $\mu \text{g}/\text{m}^3$). The model final output is a set of 365 gridded (1km $\times$ 1km) daily $\text{PM}_{10}$ maps over Italy equipped with an uncertainty measure. The spatial prediction performance shows that the interpolation procedure is able to reproduce the large scale data features without unrealistic artifacts in the generated $\text{PM}_{10}$ surfaces. The paper presents also two illustrative examples of practical applications of our model, exceedance probability and population exposure maps.

</details>

<details>

<summary>2020-09-22 12:02:51 - Application of Dynamic Linear Models to Random Allocation Clinical Trials with Covariates</summary>

- *Albert H. Lee III*

- `2009.11074v1` - [abs](http://arxiv.org/abs/2009.11074v1) - [pdf](http://arxiv.org/pdf/2009.11074v1)

> A recent method using Dynamic Linear Models to improve preferred treatment allocation budget in random allocation models was proposed by Lee, Boone, et al (2020). However this model failed to include the impact covariates such as smoking, gender, etc, had on model performance. The current paper addresses random allocation to treatments using the DLM in Bayesian Adaptive Allocation Models with a single covariate. We show a reduced treatment allocation budget along with a reduced time to locate preferred treatment. Furthermore, a sensitivity analysis is performed on mean and variance parameters and a power analysis is conducted using Bayes Factor. This power analysis is used to determine the proportion of unallocated patient budgets above a specified cutoff value. Additionally a sensitivity analysis is conducted on covariate coefficients.

</details>

<details>

<summary>2020-09-22 12:19:26 - A Bayesian Decision Tree Algorithm</summary>

- *Giuseppe Nuti, Lluís Antoni Jiménez Rugama, Andreea-Ingrid Cross*

- `1901.03214v3` - [abs](http://arxiv.org/abs/1901.03214v3) - [pdf](http://arxiv.org/pdf/1901.03214v3)

> Bayesian Decision Trees are known for their probabilistic interpretability. However, their construction can sometimes be costly. In this article we present a general Bayesian Decision Tree algorithm applicable to both regression and classification problems. The algorithm does not apply Markov Chain Monte Carlo and does not require a pruning step. While it is possible to construct a weighted probability tree space we find that one particular tree, the greedy-modal tree (GMT), explains most of the information contained in the numerical examples. This approach seems to perform similarly to Random Forests.

</details>

<details>

<summary>2020-09-22 13:42:27 - d-blink: Distributed End-to-End Bayesian Entity Resolution</summary>

- *Neil G. Marchant, Andee Kaplan, Daniel N. Elazar, Benjamin I. P. Rubinstein, Rebecca C. Steorts*

- `1909.06039v3` - [abs](http://arxiv.org/abs/1909.06039v3) - [pdf](http://arxiv.org/pdf/1909.06039v3)

> Entity resolution (ER; also known as record linkage or de-duplication) is the process of merging noisy databases, often in the absence of unique identifiers. A major advancement in ER methodology has been the application of Bayesian generative models, which provide a natural framework for inferring latent entities with rigorous quantification of uncertainty. Despite these advantages, existing models are severely limited in practice, as standard inference algorithms scale quadratically in the number of records. While scaling can be managed by fitting the model on separate blocks of the data, such a na\"ive approach may induce significant error in the posterior. In this paper, we propose a principled model for scalable Bayesian ER, called "distributed Bayesian linkage" or d-blink, which jointly performs blocking and ER without compromising posterior correctness. Our approach relies on several key ideas, including: (i) an auxiliary variable representation that induces a partition of the entities and records into blocks; (ii) a method for constructing well-balanced blocks based on k-d trees; (iii) a distributed partially-collapsed Gibbs sampler with improved mixing; and (iv) fast algorithms for performing Gibbs updates. Empirical studies on six data sets---including a case study on the 2010 Decennial Census---demonstrate the scalability and effectiveness of our approach.

</details>

<details>

<summary>2020-09-22 15:24:02 - Bayesian Nonparametric Bivariate Survival Regression for Current Status Data</summary>

- *Giorgio Paulon, Peter Müller, Victor G. Sal Y Rosas*

- `2009.06460v2` - [abs](http://arxiv.org/abs/2009.06460v2) - [pdf](http://arxiv.org/pdf/2009.06460v2)

> We consider nonparametric inference for event time distributions based on current status data. We show that in this scenario conventional mixture priors, including the popular Dirichlet process mixture prior, lead to biologically uninterpretable results as they unnaturally skew the probability mass for the event times toward the extremes of the observed data. Simple assumptions on dependent censoring can fix the problem. We then extend the discussion to bivariate current status data with partial ordering of the two outcomes. In addition to dependent censoring, we also exploit some minimal known structure relating the two event times. We design a Markov chain Monte Carlo algorithm for posterior simulation. Applied to a recurrent infection study, the method provides novel insights into how symptoms-related hospital visits are affected by covariates.

</details>

<details>

<summary>2020-09-22 16:03:04 - Partially Observable Online Change Detection via Smooth-Sparse Decomposition</summary>

- *Jie Guo, Hao Yan, Chen Zhang, Steven Hoi*

- `2009.10645v1` - [abs](http://arxiv.org/abs/2009.10645v1) - [pdf](http://arxiv.org/pdf/2009.10645v1)

> We consider online change detection of high dimensional data streams with sparse changes, where only a subset of data streams can be observed at each sensing time point due to limited sensing capacities. On the one hand, the detection scheme should be able to deal with partially observable data and meanwhile have efficient detection power for sparse changes. On the other, the scheme should be able to adaptively and actively select the most important variables to observe to maximize the detection power. To address these two points, in this paper, we propose a novel detection scheme called CDSSD. In particular, it describes the structure of high dimensional data with sparse changes by smooth-sparse decomposition, whose parameters can be learned via spike-slab variational Bayesian inference. Then the posterior Bayes factor, which incorporates the learned parameters and sparse change information, is formulated as a detection statistic. Finally, by formulating the statistic as the reward of a combinatorial multi-armed bandit problem, an adaptive sampling strategy based on Thompson sampling is proposed. The efficacy and applicability of our method in practice are demonstrated with numerical studies and a real case study.

</details>

<details>

<summary>2020-09-22 21:34:21 - Bayesian Update with Importance Sampling: Required Sample Size</summary>

- *Daniel Sanz-Alonso, Zijian Wang*

- `2009.10831v1` - [abs](http://arxiv.org/abs/2009.10831v1) - [pdf](http://arxiv.org/pdf/2009.10831v1)

> Importance sampling is used to approximate Bayes' rule in many computational approaches to Bayesian inverse problems, data assimilation and machine learning. This paper reviews and further investigates the required sample size for importance sampling in terms of the $\chi^2$-divergence between target and proposal. We develop general abstract theory and illustrate through numerous examples the roles that dimension, noise-level and other model parameters play in approximating the Bayesian update with importance sampling. Our examples also facilitate a new direct comparison of standard and optimal proposals for particle filtering.

</details>

<details>

<summary>2020-09-22 22:14:18 - Hierarchical Bayesian Bootstrap for Heterogeneous Treatment Effect Estimation</summary>

- *Arman Oganisian, Nandita Mitra, Jason Roy*

- `2009.10839v1` - [abs](http://arxiv.org/abs/2009.10839v1) - [pdf](http://arxiv.org/pdf/2009.10839v1)

> A major focus of causal inference is the estimation of heterogeneous average treatment effects (HTE) - average treatment effects within strata of another variable of interest. This involves estimating a stratum-specific regression and integrating it over the distribution of confounders in that stratum - which itself must be estimated. Standard practice in the Bayesian causal literature is to use Rubin's Bayesian bootstrap to estimate these stratum-specific confounder distributions independently. However, this becomes problematic for sparsely populated strata with few unique observed confounder vectors. By construction, the Bayesian bootstrap allocates no prior mass on confounder values unobserved within each stratum - even if these values are observed in other strata and we think they are a priori plausible. We propose causal estimation via a hierarchical Bayesian bootstrap (HBB) prior over the stratum-specific confounder distributions. Based on the Hierarchical Dirichlet Process, the HBB partially pools the stratum-specific confounder distributions by assuming all confounder vectors seen in the overall sample are plausible. In large strata, estimates allocate much of the mass to values seen within the strata, while placing small non-zero mass on unseen values. However, for sparse strata, more weight is given to values unseen in that stratum but seen elsewhere - thus shrinking the distribution towards the marginal. This allows us to borrow information across strata when estimating HTEs - leading to efficiency gains over standard marginalization approaches while avoiding strong parametric modeling assumptions about the confounder distribution when estimating HTEs. Moreover, the HBB is computationally efficient (due to conjugacy) and compatible with arbitrary outcome models.

</details>

<details>

<summary>2020-09-23 11:29:26 - A Bayesian inference approach for determining player abilities in football</summary>

- *Gavin A. Whitaker, Ricardo Silva, Daniel Edwards, Ioannis Kosmidis*

- `1710.00001v2` - [abs](http://arxiv.org/abs/1710.00001v2) - [pdf](http://arxiv.org/pdf/1710.00001v2)

> We consider the task of determining a football player's ability for a given event type, for example, scoring a goal. We propose an interpretable Bayesian model which is fit using variational inference methods. We implement a Poisson model to capture occurrences of event types, from which we infer player abilities. Our approach also allows the visualisation of differences between players, for a specific ability, through the marginal posterior variational densities. We then use these inferred player abilities to extend the Bayesian hierarchical model of Baio and Blangiardo (2010) which captures a team's scoring rate (the rate at which they score goals). We apply the resulting scheme to the English Premier League, capturing player abilities over the 2013/2014 season, before using output from the hierarchical model to predict whether over or under 2.5 goals will be scored in a given game in the 2014/2015 season. This validates our model as a way of providing insights into team formation and the individual success of sports teams.

</details>

<details>

<summary>2020-09-23 15:34:40 - Representation Learning from Limited Educational Data with Crowdsourced Labels</summary>

- *Wentao Wang, Guowei Xu, Wenbiao Ding, Gale Yan Huang, Guoliang Li, Jiliang Tang, Zitao Liu*

- `2009.11222v1` - [abs](http://arxiv.org/abs/2009.11222v1) - [pdf](http://arxiv.org/pdf/2009.11222v1)

> Representation learning has been proven to play an important role in the unprecedented success of machine learning models in numerous tasks, such as machine translation, face recognition and recommendation. The majority of existing representation learning approaches often require a large number of consistent and noise-free labels. However, due to various reasons such as budget constraints and privacy concerns, labels are very limited in many real-world scenarios. Directly applying standard representation learning approaches on small labeled data sets will easily run into over-fitting problems and lead to sub-optimal solutions. Even worse, in some domains such as education, the limited labels are usually annotated by multiple workers with diverse expertise, which yields noises and inconsistency in such crowdsourcing settings. In this paper, we propose a novel framework which aims to learn effective representations from limited data with crowdsourced labels. Specifically, we design a grouping based deep neural network to learn embeddings from a limited number of training samples and present a Bayesian confidence estimator to capture the inconsistency among crowdsourced labels. Furthermore, to expedite the training process, we develop a hard example selection procedure to adaptively pick up training examples that are misclassified by the model. Extensive experiments conducted on three real-world data sets demonstrate the superiority of our framework on learning representations from limited data with crowdsourced labels, comparing with various state-of-the-art baselines. In addition, we provide a comprehensive analysis on each of the main components of our proposed framework and also introduce the promising results it achieved in our real production to fully understand the proposed framework.

</details>

<details>

<summary>2020-09-23 19:55:12 - Learning in Volatile Environments with the Bayes Factor Surprise</summary>

- *Vasiliki Liakoni, Alireza Modirshanechi, Wulfram Gerstner, Johanni Brea*

- `1907.02936v3` - [abs](http://arxiv.org/abs/1907.02936v3) - [pdf](http://arxiv.org/pdf/1907.02936v3)

> Surprise-based learning allows agents to rapidly adapt to non-stationary stochastic environments characterized by sudden changes. We show that exact Bayesian inference in a hierarchical model gives rise to a surprise-modulated trade-off between forgetting old observations and integrating them with the new ones. The modulation depends on a probability ratio, which we call "Bayes Factor Surprise", that tests the prior belief against the current belief. We demonstrate that in several existing approximate algorithms the Bayes Factor Surprise modulates the rate of adaptation to new observations. We derive three novel surprised-based algorithms, one in the family of particle filters, one in the family of variational learning, and the other in the family of message passing, that have constant scaling in observation sequence length and particularly simple update dynamics for any distribution in the exponential family. Empirical results show that these surprise-based algorithms estimate parameters better than alternative approximate approaches and reach levels of performance comparable to computationally more expensive algorithms. The Bayes Factor Surprise is related to but different from Shannon Surprise. In two hypothetical experiments, we make testable predictions for physiological indicators that dissociate the Bayes Factor Surprise from Shannon Surprise. The theoretical insight of casting various approaches as surprise-based learning, as well as the proposed online algorithms, may be applied to the analysis of animal and human behavior, and to reinforcement learning in non-stationary environments.

</details>

<details>

<summary>2020-09-23 22:21:07 - High Dimensional Bayesian Network Classification with Network Global-Local Shrinkage Priors</summary>

- *Sharmistha Guha, Abel Rodriguez*

- `2009.11401v1` - [abs](http://arxiv.org/abs/2009.11401v1) - [pdf](http://arxiv.org/pdf/2009.11401v1)

> This article proposes a novel Bayesian classification framework for networks with labeled nodes. While literature on statistical modeling of network data typically involves analysis of a single network, the recent emergence of complex data in several biological applications, including brain imaging studies, presents a need to devise a network classifier for subjects. This article considers an application from a brain connectome study, where the overarching goal is to classify subjects into two separate groups based on their brain network data, along with identifying influential regions of interest (ROIs) (referred to as nodes). Existing approaches either treat all edge weights as a long vector or summarize the network information with a few summary measures. Both these approaches ignore the full network structure, may lead to less desirable inference in small samples and are not designed to identify significant network nodes. We propose a novel binary logistic regression framework with the network as the predictor and a binary response, the network predictor coefficient being modeled using a novel class global-local shrinkage priors. The framework is able to accurately detect nodes and edges in the network influencing the classification. Our framework is implemented using an efficient Markov Chain Monte Carlo algorithm. Theoretically, we show asymptotically optimal classification for the proposed framework when the number of network edges grows faster than the sample size. The framework is empirically validated by extensive simulation studies and analysis of a brain connectome data.

</details>

<details>

<summary>2020-09-23 22:34:06 - Unfairness Discovery and Prevention For Few-Shot Regression</summary>

- *Chen Zhao, Feng Chen*

- `2009.11406v1` - [abs](http://arxiv.org/abs/2009.11406v1) - [pdf](http://arxiv.org/pdf/2009.11406v1)

> We study fairness in supervised few-shot meta-learning models that are sensitive to discrimination (or bias) in historical data. A machine learning model trained based on biased data tends to make unfair predictions for users from minority groups. Although this problem has been studied before, existing methods mainly aim to detect and control the dependency effect of the protected variables (e.g. race, gender) on target prediction based on a large amount of training data. These approaches carry two major drawbacks that (1) lacking showing a global cause-effect visualization for all variables; (2) lacking generalization of both accuracy and fairness to unseen tasks. In this work, we first discover discrimination from data using a causal Bayesian knowledge graph which not only demonstrates the dependency of the protected variable on target but also indicates causal effects between all variables. Next, we develop a novel algorithm based on risk difference in order to quantify the discriminatory influence for each protected variable in the graph. Furthermore, to protect prediction from unfairness, a fast-adapted bias-control approach in meta-learning is proposed, which efficiently mitigates statistical disparity for each task and it thus ensures independence of protected attributes on predictions based on biased and few-shot data samples. Distinct from existing meta-learning models, group unfairness of tasks are efficiently reduced by leveraging the mean difference between (un)protected groups for regression problems. Through extensive experiments on both synthetic and real-world data sets, we demonstrate that our proposed unfairness discovery and prevention approaches efficiently detect discrimination and mitigate biases on model output as well as generalize both accuracy and fairness to unseen tasks with a limited amount of training samples.

</details>

<details>

<summary>2020-09-23 22:40:27 - Bayesian Hierarchical Models for High-Dimensional Mediation Analysis with Coordinated Selection of Correlated Mediators</summary>

- *Yanyi Song, Xiang Zhou, Jian Kang, Max T. Aung, Min Zhang, Wei Zhao, Belinda L. Needham, Sharon L. R. Kardia, Yongmei Liu, John D. Meeker, Jennifer A. Smith, Bhramar Mukherjee*

- `2009.11409v1` - [abs](http://arxiv.org/abs/2009.11409v1) - [pdf](http://arxiv.org/pdf/2009.11409v1)

> We consider Bayesian high-dimensional mediation analysis to identify among a large set of correlated potential mediators the active ones that mediate the effect from an exposure variable to an outcome of interest. Correlations among mediators are commonly observed in modern data analysis; examples include the activated voxels within connected regions in brain image data, regulatory signals driven by gene networks in genome data and correlated exposure data from the same source. When correlations are present among active mediators, mediation analysis that fails to account for such correlation can be sub-optimal and may lead to a loss of power in identifying active mediators. Building upon a recent high-dimensional mediation analysis framework, we propose two Bayesian hierarchical models, one with a Gaussian mixture prior that enables correlated mediator selection and the other with a Potts mixture prior that accounts for the correlation among active mediators in mediation analysis. We develop efficient sampling algorithms for both methods. Various simulations demonstrate that our methods enable effective identification of correlated active mediators, which could be missed by using existing methods that assume prior independence among active mediators. The proposed methods are applied to the LIFECODES birth cohort and the Multi-Ethnic Study of Atherosclerosis (MESA) and identified new active mediators with important biological implications.

</details>

<details>

<summary>2020-09-24 07:00:46 - Latent Causal Socioeconomic Health Index</summary>

- *F. Swen Kuh, Grace S. Chiu, Anton H. Westveld*

- `2009.12217v1` - [abs](http://arxiv.org/abs/2009.12217v1) - [pdf](http://arxiv.org/pdf/2009.12217v1)

> This research develops a model-based LAtent Causal Socioeconomic Health (LACSH) index at the national level. We build upon the latent health factor index (LHFI) approach that has been used to assess the unobservable ecological/ecosystem health. This framework integratively models the relationship between metrics, the latent health, and the covariates that drive the notion of health. In this paper, the LHFI structure is integrated with spatial modeling and statistical causal modeling, so as to evaluate the impact of a continuous policy variable (mandatory maternity leave days and government's expenditure on healthcare, respectively) on a nation's socioeconomic health, while formally accounting for spatial dependency among the nations. A novel visualization technique for evaluating covariate balance is also introduced for the case of a continuous policy (treatment) variable. We apply our LACSH model to countries around the world using data on various metrics and potential covariates pertaining to different aspects of societal health. The approach is structured in a Bayesian hierarchical framework and results are obtained by Markov chain Monte Carlo techniques.

</details>

<details>

<summary>2020-09-24 10:34:14 - BETS: The dangers of selection bias in early analyses of the coronavirus disease (COVID-19) pandemic</summary>

- *Qingyuan Zhao, Nianqiao Ju, Sergio Bacallado, Rajen D. Shah*

- `2004.07743v4` - [abs](http://arxiv.org/abs/2004.07743v4) - [pdf](http://arxiv.org/pdf/2004.07743v4)

> The coronavirus disease 2019 (COVID-19) has quickly grown from a regional outbreak in Wuhan, China to a global pandemic. Early estimates of the epidemic growth and incubation period of COVID-19 may have been biased due to sample selection. Using detailed case reports from 14 locations in and outside mainland China, we obtained 378 Wuhan-exported cases who left Wuhan before an abrupt travel quarantine. We developed a generative model we call BETS for four key epidemiological events---Beginning of exposure, End of exposure, time of Transmission, and time of Symptom onset (BETS)---and derived explicit formulas to correct for the sample selection. We gave a detailed illustration of why some early and highly influential analyses of the COVID-19 pandemic were severely biased. All our analyses, regardless of which subsample and model were being used, point to an epidemic doubling time of 2 to 2.5 days during the early outbreak in Wuhan. A Bayesian nonparametric analysis further suggests that about 5% of the symptomatic cases may not develop symptoms within 14 days of infection and that men may be much more likely than women to develop symptoms within 2 days of infection.

</details>

<details>

<summary>2020-09-24 16:42:19 - Ensemble Forecasting of the Zika Space-TimeSpread with Topological Data Analysis</summary>

- *Marwah Soliman, Vyacheslav Lyubchich, Yulia R. Gel*

- `2009.13423v1` - [abs](http://arxiv.org/abs/2009.13423v1) - [pdf](http://arxiv.org/pdf/2009.13423v1)

> As per the records of theWorld Health Organization, the first formally reported incidence of Zika virus occurred in Brazil in May 2015. The disease then rapidly spread to other countries in Americas and East Asia, affecting more than 1,000,000 people. Zika virus is primarily transmitted through bites of infected mosquitoes of the species Aedes (Aedes aegypti and Aedes albopictus). The abundance of mosquitoes and, as a result, the prevalence of Zika virus infections are common in areas which have high precipitation, high temperature, and high population density.Nonlinear spatio-temporal dependency of such data and lack of historical public health records make prediction of the virus spread particularly challenging. In this article, we enhance Zika forecasting by introducing the concepts of topological data analysis and, specifically, persistent homology of atmospheric variables, into the virus spread modeling. The topological summaries allow for capturing higher order dependencies among atmospheric variables that otherwise might be unassessable via conventional spatio-temporal modeling approaches based on geographical proximity assessed via Euclidean distance. We introduce a new concept of cumulative Betti numbers and then integrate the cumulative Betti numbers as topological descriptors into three predictive machine learning models: random forest, generalized boosted regression, and deep neural network. Furthermore, to better quantify for various sources of uncertainties, we combine the resulting individual model forecasts into an ensemble of the Zika spread predictions using Bayesian model averaging. The proposed methodology is illustrated in application to forecasting of the Zika space-time spread in Brazil in the year 2018.

</details>

<details>

<summary>2020-09-24 18:42:48 - A Fully Bayesian, Logistic Regression Tracking Algorithm for Mitigating Disparate Misclassification</summary>

- *Martin B. Short, George O. Mohler*

- `2012.00662v1` - [abs](http://arxiv.org/abs/2012.00662v1) - [pdf](http://arxiv.org/pdf/2012.00662v1)

> We develop a fully Bayesian, logistic tracking algorithm with the purpose of providing classification results that are unbiased when applied uniformly to individuals with differing sensitive variable values. Here, we consider bias in the form of differences in false prediction rates between the different sensitive variable groups. Given that the method is fully Bayesian, it is well suited for situations where group parameters or logistic regression coefficients are dynamic quantities. We illustrate our method, in comparison to others, on both simulated datasets and the well-known ProPublica COMPAS dataset.

</details>

<details>

<summary>2020-09-24 22:43:03 - Bayesian Topological Learning for Classifying the Structure of Biological Networks</summary>

- *Vasileios Maroulas, Cassie Putman Micucci, Farzana Nasrin*

- `2009.11974v1` - [abs](http://arxiv.org/abs/2009.11974v1) - [pdf](http://arxiv.org/pdf/2009.11974v1)

> Actin cytoskeleton networks generate local topological signatures due to the natural variations in the number, size, and shape of holes of the networks. Persistent homology is a method that explores these topological properties of data and summarizes them as persistence diagrams. In this work, we analyze and classify these filament networks by transforming them into persistence diagrams whose variability is quantified via a Bayesian framework on the space of persistence diagrams. The proposed generalized Bayesian framework adopts an independent and identically distributed cluster point process characterization of persistence diagrams and relies on a substitution likelihood argument. This framework provides the flexibility to estimate the posterior cardinality distribution of points in a persistence diagram and the posterior spatial distribution simultaneously. We present a closed form of the posteriors under the assumption of Gaussian mixtures and binomials for prior intensity and cardinality respectively. Using this posterior calculation, we implement a Bayes factor algorithm to classify the actin filament networks and benchmark it against several state-of-the-art classification methods.

</details>

<details>

<summary>2020-09-25 01:14:21 - Parameter Restrictions for the Sake of Identification: Is there Utility in Asserting that Perhaps a Restriction Holds?</summary>

- *Paul Gustafson*

- `2009.11993v1` - [abs](http://arxiv.org/abs/2009.11993v1) - [pdf](http://arxiv.org/pdf/2009.11993v1)

> Statistical modeling can involve a tension between assumptions and statistical identification. The law of the observable data may not uniquely determine the value of a target parameter without invoking a key assumption, and, while plausible, this assumption may not be obviously true in the scientific context at hand. Moreover, there are many instances of key assumptions which are untestable, hence we cannot rely on the data to resolve the question of whether the target is legitimately identified. Working in the Bayesian paradigm, we consider the grey zone of situations where a key assumption, in the form of a parameter space restriction, is scientifically reasonable but not incontrovertible for the problem being tackled. Specifically, we investigate statistical properties that ensue if we structure a prior distribution to assert that `maybe' or `perhaps' the assumption holds. Technically this simply devolves to using a mixture prior distribution putting just some prior weight on the assumption, or one of several assumptions, holding. However, while the construct is straightforward, there is very little literature discussing situations where Bayesian model averaging is employed across a mix of fully identified and partially identified models.

</details>

<details>

<summary>2020-09-25 11:04:59 - High-dimensional Bayesian optimization using low-dimensional feature spaces</summary>

- *Riccardo Moriconi, Marc P. Deisenroth, K. S. Sesh Kumar*

- `1902.10675v7` - [abs](http://arxiv.org/abs/1902.10675v7) - [pdf](http://arxiv.org/pdf/1902.10675v7)

> Bayesian optimization (BO) is a powerful approach for seeking the global optimum of expensive black-box functions and has proven successful for fine tuning hyper-parameters of machine learning models. However, BO is practically limited to optimizing 10--20 parameters. To scale BO to high dimensions, we usually make structural assumptions on the decomposition of the objective and\slash or exploit the intrinsic lower dimensionality of the problem, e.g. by using linear projections. We could achieve a higher compression rate with nonlinear projections, but learning these nonlinear embeddings typically requires much data. This contradicts the BO objective of a relatively small evaluation budget. To address this challenge, we propose to learn a low-dimensional feature space jointly with (a) the response surface and (b) a reconstruction mapping. Our approach allows for optimization of BO's acquisition function in the lower-dimensional subspace, which significantly simplifies the optimization problem. We reconstruct the original parameter space from the lower-dimensional subspace for evaluating the black-box function. For meaningful exploration, we solve a constrained optimization problem.

</details>

<details>

<summary>2020-09-25 11:18:17 - Multilevel Gibbs Sampling for Bayesian Regression</summary>

- *Joris Tavernier, Jaak Simm, Adam Arany, Karl Meerbergen, Yves Moreau*

- `2009.12132v1` - [abs](http://arxiv.org/abs/2009.12132v1) - [pdf](http://arxiv.org/pdf/2009.12132v1)

> Bayesian regression remains a simple but effective tool based on Bayesian inference techniques. For large-scale applications, with complicated posterior distributions, Markov Chain Monte Carlo methods are applied. To improve the well-known computational burden of Markov Chain Monte Carlo approach for Bayesian regression, we developed a multilevel Gibbs sampler for Bayesian regression of linear mixed models. The level hierarchy of data matrices is created by clustering the features and/or samples of data matrices. Additionally, the use of correlated samples is investigated for variance reduction to improve the convergence of the Markov Chain. Testing on a diverse set of data sets, speed-up is achieved for almost all of them without significant loss in predictive performance.

</details>

<details>

<summary>2020-09-25 13:17:38 - Mirror Descent and the Information Ratio</summary>

- *Tor Lattimore, András György*

- `2009.12228v1` - [abs](http://arxiv.org/abs/2009.12228v1) - [pdf](http://arxiv.org/pdf/2009.12228v1)

> We establish a connection between the stability of mirror descent and the information ratio by Russo and Van Roy [2014]. Our analysis shows that mirror descent with suitable loss estimators and exploratory distributions enjoys the same bound on the adversarial regret as the bounds on the Bayesian regret for information-directed sampling. Along the way, we develop the theory for information-directed sampling and provide an efficient algorithm for adversarial bandits for which the regret upper bound matches exactly the best known information-theoretic upper bound.

</details>

<details>

<summary>2020-09-25 14:33:59 - Bayesian Testing for Exogenous Partition Structures in Stochastic Block Models</summary>

- *Sirio Legramanti, Tommaso Rigon, Daniele Durante*

- `2009.12267v1` - [abs](http://arxiv.org/abs/2009.12267v1) - [pdf](http://arxiv.org/pdf/2009.12267v1)

> Network data often exhibit block structures characterized by clusters of nodes with similar patterns of edge formation. When such relational data are complemented by additional information on exogenous node partitions, these sources of knowledge are typically included in the model to supervise the cluster assignment mechanism or to improve inference on edge probabilities. Although these solutions are routinely implemented, there is a lack of formal approaches to test if a given external node partition is in line with the endogenous clustering structure encoding stochastic equivalence patterns among the nodes in the network. To fill this gap, we develop a formal Bayesian testing procedure which relies on the calculation of the Bayes factor between a stochastic block model with known grouping structure defined by the exogenous node partition and an infinite relational model that allows the endogenous clustering configurations to be unknown, random and fully revealed by the block-connectivity patterns in the network. A simple Markov chain Monte Carlo method for computing the Bayes factor and quantifying uncertainty in the endogenous groups is proposed. This routine is evaluated in simulations and in an application to study exogenous equivalence structures in brain networks of Alzheimer's patients.

</details>

<details>

<summary>2020-09-25 17:29:48 - Bayesian Nonparametric Multivariate Spatial Mixture Mixed Effects Models with Application to American Community Survey Special Tabulations</summary>

- *Ryan Janicki, Andrew M. Raim, Scott H. Holan, Jerry Maples*

- `2009.12351v1` - [abs](http://arxiv.org/abs/2009.12351v1) - [pdf](http://arxiv.org/pdf/2009.12351v1)

> Leveraging multivariate spatial dependence to improve the precision of estimates using American Community Survey data and other sample survey data has been a topic of recent interest among data-users and federal statistical agencies. One strategy is to use a multivariate spatial mixed effects model with a Gaussian observation model and latent Gaussian process model. In practice, this works well for a wide range of tabulations. Nevertheless, in situations that exhibit heterogeneity among geographies and/or sparsity in the data, the Gaussian assumptions may be problematic and lead to underperformance. To remedy these situations, we propose a multivariate hierarchical Bayesian nonparametric mixed effects spatial mixture model to increase model flexibility. The number of clusters is chosen automatically in a data-driven manner. The effectiveness of our approach is demonstrated through a simulation study and motivating application of special tabulations for American Community Survey data.

</details>

<details>

<summary>2020-09-25 17:39:07 - A Parsimonious Tour of Bayesian Model Uncertainty</summary>

- *Pierre-Alexandre Mattei*

- `1902.05539v2` - [abs](http://arxiv.org/abs/1902.05539v2) - [pdf](http://arxiv.org/pdf/1902.05539v2)

> Modern statistical software and machine learning libraries are enabling semi-automated statistical inference. Within this context, it appears easier and easier to try and fit many models to the data at hand, reversing thereby the Fisherian way of conducting science by collecting data after the scientific hypothesis (and hence the model) has been determined. The renewed goal of the statistician becomes to help the practitioner choose within such large and heterogeneous families of models, a task known as model selection. The Bayesian paradigm offers a systematized way of assessing this problem. This approach, launched by Harold Jeffreys in his 1935 book Theory of Probability, has witnessed a remarkable evolution in the last decades, that has brought about several new theoretical and methodological advances. Some of these recent developments are the focus of this survey, which tries to present a unifying perspective on work carried out by different communities. In particular, we focus on non-asymptotic out-of-sample performance of Bayesian model selection and averaging techniques, and draw connections with penalized maximum likelihood. We also describe recent extensions to wider classes of probabilistic frameworks including high-dimensional, unidentifiable, or likelihood-free models.

</details>

<details>

<summary>2020-09-25 19:15:26 - Why have a Unified Predictive Uncertainty? Disentangling it using Deep Split Ensembles</summary>

- *Utkarsh Sarawgi, Wazeer Zulfikar, Rishab Khincha, Pattie Maes*

- `2009.12406v1` - [abs](http://arxiv.org/abs/2009.12406v1) - [pdf](http://arxiv.org/pdf/2009.12406v1)

> Understanding and quantifying uncertainty in black box Neural Networks (NNs) is critical when deployed in real-world settings such as healthcare. Recent works using Bayesian and non-Bayesian methods have shown how a unified predictive uncertainty can be modelled for NNs. Decomposing this uncertainty to disentangle the granular sources of heteroscedasticity in data provides rich information about its underlying causes. We propose a conceptually simple non-Bayesian approach, deep split ensemble, to disentangle the predictive uncertainties using a multivariate Gaussian mixture model. The NNs are trained with clusters of input features, for uncertainty estimates per cluster. We evaluate our approach on a series of benchmark regression datasets, while also comparing with unified uncertainty methods. Extensive analyses using dataset shits and empirical rule highlight our inherently well-calibrated models. Our work further demonstrates its applicability in a multi-modal setting using a benchmark Alzheimer's dataset and also shows how deep split ensembles can highlight hidden modality-specific biases. The minimal changes required to NNs and the training procedure, and the high flexibility to group features into clusters makes it readily deployable and useful. The source code is available at https://github.com/wazeerzulfikar/deep-split-ensembles

</details>

<details>

<summary>2020-09-26 14:11:51 - Bayesian Restoration of Audio Degraded by Low-Frequency Pulses Modeled via Gaussian Process</summary>

- *Hugo Tremonte de Carvalho, Flávio Rainho Ávila, Luiz Wagner Pereira Biscainho*

- `2005.14181v2` - [abs](http://arxiv.org/abs/2005.14181v2) - [pdf](http://arxiv.org/pdf/2005.14181v2)

> A common defect found when reproducing old vinyl and gramophone recordings with mechanical devices are the long pulses with significant low-frequency content caused by the interaction of the arm-needle system with deep scratches or even breakages on the media surface. Previous approaches to their suppression on digital counterparts of the recordings depend on a prior estimation of the pulse location, usually performed via heuristic methods. This paper proposes a novel Bayesian approach capable of jointly estimating the pulse location; interpolating the almost annihilated signal underlying the strong discontinuity that initiates the pulse; and also estimating the long pulse tail by a simple Gaussian Process, allowing its suppression from the corrupted signal. The posterior distribution for the model parameters as well for the pulse is explored via Markov-Chain Monte Carlo (MCMC) algorithms. Controlled experiments indicate that the proposed method, while requiring significantly less user intervention, achieves perceptual results similar to those of previous approaches and performs well when dealing with naturally degraded signals.

</details>

<details>

<summary>2020-09-26 21:34:01 - Adaptive Non-reversible Stochastic Gradient Langevin Dynamics</summary>

- *Vikram Krishnamurthy, George Yin*

- `2009.12690v1` - [abs](http://arxiv.org/abs/2009.12690v1) - [pdf](http://arxiv.org/pdf/2009.12690v1)

> It is well known that adding any skew symmetric matrix to the gradient of Langevin dynamics algorithm results in a non-reversible diffusion with improved convergence rate. This paper presents a gradient algorithm to adaptively optimize the choice of the skew symmetric matrix. The resulting algorithm involves a non-reversible diffusion algorithm cross coupled with a stochastic gradient algorithm that adapts the skew symmetric matrix. The algorithm uses the same data as the classical Langevin algorithm. A weak convergence proof is given for the optimality of the choice of the skew symmetric matrix. The improved convergence rate of the algorithm is illustrated numerically in Bayesian learning and tracking examples.

</details>

<details>

<summary>2020-09-27 11:33:40 - Multi-task Causal Learning with Gaussian Processes</summary>

- *Virginia Aglietti, Theodoros Damoulas, Mauricio Álvarez, Javier González*

- `2009.12821v1` - [abs](http://arxiv.org/abs/2009.12821v1) - [pdf](http://arxiv.org/pdf/2009.12821v1)

> This paper studies the problem of learning the correlation structure of a set of intervention functions defined on the directed acyclic graph (DAG) of a causal model. This is useful when we are interested in jointly learning the causal effects of interventions on different subsets of variables in a DAG, which is common in field such as healthcare or operations research. We propose the first multi-task causal Gaussian process (GP) model, which we call DAG-GP, that allows for information sharing across continuous interventions and across experiments on different variables. DAG-GP accommodates different assumptions in terms of data availability and captures the correlation between functions lying in input spaces of different dimensionality via a well-defined integral operator. We give theoretical results detailing when and how the DAG-GP model can be formulated depending on the DAG. We test both the quality of its predictions and its calibrated uncertainties. Compared to single-task models, DAG-GP achieves the best fitting performance in a variety of real and synthetic settings. In addition, it helps to select optimal interventions faster than competing approaches when used within sequential decision making frameworks, like active learning or Bayesian optimization.

</details>

<details>

<summary>2020-09-27 14:16:14 - Kernel learning approaches for summarising and combining posterior similarity matrices</summary>

- *Alessandra Cabassi, Sylvia Richardson, Paul D. W. Kirk*

- `2009.12852v1` - [abs](http://arxiv.org/abs/2009.12852v1) - [pdf](http://arxiv.org/pdf/2009.12852v1)

> When using Markov chain Monte Carlo (MCMC) algorithms to perform inference for Bayesian clustering models, such as mixture models, the output is typically a sample of clusterings (partitions) drawn from the posterior distribution. In practice, a key challenge is how to summarise this output. Here we build upon the notion of the posterior similarity matrix (PSM) in order to suggest new approaches for summarising the output of MCMC algorithms for Bayesian clustering models. A key contribution of our work is the observation that PSMs are positive semi-definite, and hence can be used to define probabilistically-motivated kernel matrices that capture the clustering structure present in the data. This observation enables us to employ a range of kernel methods to obtain summary clusterings, and otherwise exploit the information summarised by PSMs. For example, if we have multiple PSMs, each corresponding to a different dataset on a common set of statistical units, we may use standard methods for combining kernels in order to perform integrative clustering. We may moreover embed PSMs within predictive kernel models in order to perform outcome-guided data integration. We demonstrate the performances of the proposed methods through a range of simulation studies as well as two real data applications. R code is available at https://github.com/acabassi/combine-psms.

</details>

<details>

<summary>2020-09-27 14:26:01 - Recurrent Neural Filters: Learning Independent Bayesian Filtering Steps for Time Series Prediction</summary>

- *Bryan Lim, Stefan Zohren, Stephen Roberts*

- `1901.08096v6` - [abs](http://arxiv.org/abs/1901.08096v6) - [pdf](http://arxiv.org/pdf/1901.08096v6)

> Despite the recent popularity of deep generative state space models, few comparisons have been made between network architectures and the inference steps of the Bayesian filtering framework -- with most models simultaneously approximating both state transition and update steps with a single recurrent neural network (RNN). In this paper, we introduce the Recurrent Neural Filter (RNF), a novel recurrent autoencoder architecture that learns distinct representations for each Bayesian filtering step, captured by a series of encoders and decoders. Testing this on three real-world time series datasets, we demonstrate that the decoupled representations learnt not only improve the accuracy of one-step-ahead forecasts while providing realistic uncertainty estimates, but also facilitate multistep prediction through the separation of encoder stages.

</details>

<details>

<summary>2020-09-28 18:49:10 - A hierarchical spatio-temporal model to analyze relative risk variations of COVID-19: a focus on Spain, Italy and Germany</summary>

- *Abdollah Jalilian, Jorge Mateu*

- `2009.13577v1` - [abs](http://arxiv.org/abs/2009.13577v1) - [pdf](http://arxiv.org/pdf/2009.13577v1)

> The novel coronavirus disease (COVID-19) has spread rapidly across the world in a short period of time and with a heterogeneous pattern. Understanding the underlying temporal and spatial dynamics in the spread of COVID-19 can result in informed and timely public health policies. In this paper, we use a spatio-temporal stochastic model to explain the temporal and spatial variations in the daily number of new confirmed cases in Spain, Italy and Germany from late February to mid September 2020. Using a hierarchical Bayesian framework, we found that the temporal trend of the epidemic in the three countries rapidly reached their peaks and slowly started to decline at the beginning of April and then increased and reached their second maximum in August. However decline and increase of the temporal trend seems to be sharper in Spain and smoother in Germany. The spatial heterogeneity of the relative risk of COVID-19 in Spain is also more pronounced than Italy and Germany.

</details>

<details>

<summary>2020-09-28 19:21:00 - Quantile Regression Neural Networks: A Bayesian Approach</summary>

- *Sanket R. Jantre, Shrijita Bhattacharya, Tapabrata Maiti*

- `2009.13591v1` - [abs](http://arxiv.org/abs/2009.13591v1) - [pdf](http://arxiv.org/pdf/2009.13591v1)

> This article introduces a Bayesian neural network estimation method for quantile regression assuming an asymmetric Laplace distribution (ALD) for the response variable. It is shown that the posterior distribution for feedforward neural network quantile regression is asymptotically consistent under a misspecified ALD model. This consistency proof embeds the problem from density estimation domain and uses bounds on the bracketing entropy to derive the posterior consistency over Hellinger neighborhoods. This consistency result is shown in the setting where the number of hidden nodes grow with the sample size. The Bayesian implementation utilizes the normal-exponential mixture representation of the ALD density. The algorithm uses Markov chain Monte Carlo (MCMC) simulation technique - Gibbs sampling coupled with Metropolis-Hastings algorithm. We have addressed the issue of complexity associated with the afore-mentioned MCMC implementation in the context of chain convergence, choice of starting values, and step sizes. We have illustrated the proposed method with simulation studies and real data examples.

</details>

<details>

<summary>2020-09-28 21:25:44 - A General Bayesian Model for Heteroskedastic Data with Fully Conjugate Full-Conditional Distributions</summary>

- *Paul A. Parker, Scott H. Holan, Skye A. Wills*

- `2009.13636v1` - [abs](http://arxiv.org/abs/2009.13636v1) - [pdf](http://arxiv.org/pdf/2009.13636v1)

> Models for heteroskedastic data are relevant in a wide variety of applications ranging from financial time series to environmental statistics. However, the topic of modeling the variance function conditionally has not seen near as much attention as modeling the mean. Volatility models have been used in specific applications, but these models can be difficult to fit in a Bayesian setting due to posterior distributions that are challenging to sample from efficiently. In this work, we introduce a general model for heteroskedastic data. This approach models the conditional variance in a mixed model approach as a function of any desired covariates or random effects. We rely on new distribution theory in order to construct priors that yield fully conjugate full conditional distributions. Thus, our approach can easily be fit via Gibbs sampling. Furthermore, we extend the model to a deep learning approach that can provide highly accurate estimates for time dependent data. We also provide an extension for heavy-tailed data. We illustrate our methodology via three applications. The first application utilizes a high dimensional soil dataset with inherent spatial dependence. The second application involves modeling of asset volatility. The third application focuses on clinical trial data for creatinine.

</details>

<details>

<summary>2020-09-29 07:38:24 - Community Detection in Bipartite Networks with Stochastic Blockmodels</summary>

- *Tzu-Chi Yen, Daniel B. Larremore*

- `2001.11818v2` - [abs](http://arxiv.org/abs/2001.11818v2) - [pdf](http://arxiv.org/pdf/2001.11818v2)

> In bipartite networks, community structures are restricted to being disassortative, in that nodes of one type are grouped according to common patterns of connection with nodes of the other type. This makes the stochastic block model (SBM), a highly flexible generative model for networks with block structure, an intuitive choice for bipartite community detection. However, typical formulations of the SBM do not make use of the special structure of bipartite networks. Here we introduce a Bayesian nonparametric formulation of the SBM and a corresponding algorithm to efficiently find communities in bipartite networks which parsimoniously chooses the number of communities. The biSBM improves community detection results over general SBMs when data are noisy, improves the model resolution limit by a factor of $\sqrt{2}$, and expands our understanding of the complicated optimization landscape associated with community detection tasks. A direct comparison of certain terms of the prior distributions in the biSBM and a related high-resolution hierarchical SBM also reveals a counterintuitive regime of community detection problems, populated by smaller and sparser networks, where nonhierarchical models outperform their more flexible counterpart.

</details>

<details>

<summary>2020-09-29 09:22:46 - Efficient Bayesian Inference of General Gaussian Models on Large Phylogenetic Trees</summary>

- *Paul Bastide, Lam Si Tung Ho, Guy Baele, Philippe Lemey, Marc A Suchard*

- `2003.10336v2` - [abs](http://arxiv.org/abs/2003.10336v2) - [pdf](http://arxiv.org/pdf/2003.10336v2)

> Phylogenetic comparative methods correct for shared evolutionary history among a set of non-independent organisms by modeling sample traits as arising from a diffusion process along on the branches of a possibly unknown history. To incorporate such uncertainty, we present a scalable Bayesian inference framework under a general Gaussian trait evolution model that exploits Hamiltonian Monte Carlo (HMC). HMC enables efficient sampling of the constrained model parameters and takes advantage of the tree structure for fast likelihood and gradient computations, yielding algorithmic complexity linear in the number of observations. This approach encompasses a wide family of stochastic processes, including the general Ornstein-Uhlenbeck (OU) process, with possible missing data and measurement errors. We implement inference tools for a biologically relevant subset of all these models into the BEAST phylogenetic software package and develop model comparison through marginal likelihood estimation. We apply our approach to study the morphological evolution in the superfamilly of Musteloidea (including weasels and allies) as well as the heritability of HIV virulence. This second problem furnishes a new measure of evolutionary heritability that demonstrates its utility through a targeted simulation study.

</details>

<details>

<summary>2020-09-29 16:26:08 - Dynamic sparsity on dynamic regression models</summary>

- *Paloma W. Uribe, Hedibert F. Lopes*

- `2009.14131v1` - [abs](http://arxiv.org/abs/2009.14131v1) - [pdf](http://arxiv.org/pdf/2009.14131v1)

> In the present work, we consider variable selection and shrinkage for the Gaussian dynamic linear regression within a Bayesian framework. In particular, we propose a novel method that allows for time-varying sparsity, based on an extension of spike-and-slab priors for dynamic models. This is done by assigning appropriate Markov switching priors for the time-varying coefficients' variances, extending the previous work of Ishwaran and Rao (2005). Furthermore, we investigate different priors, including the common Inverted gamma prior for the process variances, and other mixture prior distributions such as Gamma priors for both the spike and the slab, which leads to a mixture of Normal-Gammas priors (Griffin ad Brown, 2010) for the coefficients. In this sense, our prior can be view as a dynamic variable selection prior which induces either smoothness (through the slab) or shrinkage towards zero (through the spike) at each time point. The MCMC method used for posterior computation uses Markov latent variables that can assume binary regimes at each time point to generate the coefficients' variances. In that way, our model is a dynamic mixture model, thus, we could use the algorithm of Gerlach et al (2000) to generate the latent processes without conditioning on the states. Finally, our approach is exemplified through simulated examples and a real data application.

</details>

<details>

<summary>2020-09-29 18:04:02 - ParaMonte: A high-performance serial/parallel Monte Carlo simulation library for C, C++, Fortran</summary>

- *Amir Shahmoradi, Fatemeh Bagheri*

- `2009.14229v1` - [abs](http://arxiv.org/abs/2009.14229v1) - [pdf](http://arxiv.org/pdf/2009.14229v1)

> ParaMonte (standing for Parallel Monte Carlo) is a serial and MPI/Coarray-parallelized library of Monte Carlo routines for sampling mathematical objective functions of arbitrary-dimensions, in particular, the posterior distributions of Bayesian models in data science, Machine Learning, and scientific inference. The ParaMonte library has been developed with the design goal of unifying the **automation**, **accessibility**, **high-performance**, **scalability**, and **reproducibility** of Monte Carlo simulations. The current implementation of the library includes **ParaDRAM**, a **Para**llel **D**elyaed-**R**ejection **A**daptive **M**etropolis Markov Chain Monte Carlo sampler, accessible from a wide range of programming languages including C, C++, Fortran, with a unified Application Programming Interface and simulation environment across all supported programming languages. The ParaMonte library is MIT-licensed and is permanently located and maintained at [https://github.com/cdslaborg/paramonte](https://github.com/cdslaborg/paramonte).

</details>

<details>

<summary>2020-09-29 18:32:55 - Enhanced Bayesian Model Updating with Incomplete Modal Information Using Parallel, Interactive and Adaptive Markov Chains</summary>

- *Kai Zhou, Jiong Tang*

- `2009.14249v1` - [abs](http://arxiv.org/abs/2009.14249v1) - [pdf](http://arxiv.org/pdf/2009.14249v1)

> Finite element model updating is challenging because 1) the problem is oftentimes underdetermined while the measurements are limited and/or incomplete; 2) many combinations of parameters may yield responses that are similar with respect to actual measurements; and 3) uncertainties inevitably exist. The aim of this research is to leverage upon computational intelligence through statistical inference to facilitate an enhanced, probabilistic finite element model updating using incomplete modal response measurement. This new framework is built upon efficient inverse identification through optimization, whereas Bayesian inference is employed to account for the effect of uncertainties. To overcome the computational cost barrier, we adopt Markov chain Monte Carlo (MCMC) to characterize the target function/distribution. Instead of using single Markov chain in conventional Bayesian approach, we develop a new sampling theory with multiple parallel, interactive and adaptive Markov chains and incorporate into Bayesian inference. This can harness the collective power of these Markov chains to realize the concurrent search of multiple local optima. The number of required Markov chains and their respective initial model parameters are automatically determined via Monte Carlo simulation-based sample pre-screening followed by K-means clustering analysis. These enhancements can effectively address the aforementioned challenges in finite element model updating. The validity of this framework is systematically demonstrated through case studies.

</details>

<details>

<summary>2020-09-29 20:39:13 - The Illusion of the Illusion of Sparsity: An exercise in prior sensitivity</summary>

- *Bruno Fava, Hedibert F. Lopes*

- `2009.14296v1` - [abs](http://arxiv.org/abs/2009.14296v1) - [pdf](http://arxiv.org/pdf/2009.14296v1)

> The emergence of Big Data raises the question of how to model economic relations when there is a large number of possible explanatory variables. We revisit the issue by comparing the possibility of using dense or sparse models in a Bayesian approach, allowing for variable selection and shrinkage. More specifically, we discuss the results reached by Giannone, Lenza, and Primiceri (2020) through a "Spike-and-Slab" prior, which suggest an "illusion of sparsity" in economic data, as no clear patterns of sparsity could be detected. We make a further revision of the posterior distributions of the model, and propose three experiments to evaluate the robustness of the adopted prior distribution. We find that the pattern of sparsity is sensitive to the prior distribution of the regression coefficients, and present evidence that the model indirectly induces variable selection and shrinkage, which suggests that the "illusion of sparsity" could be, itself, an illusion. Code is available on github.com/bfava/IllusionOfIllusion.

</details>

<details>

<summary>2020-09-30 01:19:22 - Spatial Statistical Models: an overview under the Bayesian Approach</summary>

- *Francisco Louzada, Diego C. Nascimento, Osafu Augustine Egbon*

- `2009.14371v1` - [abs](http://arxiv.org/abs/2009.14371v1) - [pdf](http://arxiv.org/pdf/2009.14371v1)

> Spatial documentation is exponentially increasing given the availability of Big IoT Data, enabled by the devices miniaturization and data storage capacity. Bayesian spatial statistics is a useful statistical tool to determine the dependence structure and hidden patterns over space through prior knowledge and data likelihood. Nevertheless, this modeling class is not well explored as the classification and regression machine learning models given their simplicity and often weak (data) independence supposition. In this manner, this systematic review aimed to unravel the main models presented in the literature in the past 20 years, identify gaps, and research opportunities. Elements such as random fields, spatial domains, prior specification, covariance function, and numerical approximations were discussed. This work explored the two subclasses of spatial smoothing global and local.

</details>

<details>

<summary>2020-09-30 09:56:44 - Bayesian Neural Networks With Maximum Mean Discrepancy Regularization</summary>

- *Jary Pomponi, Simone Scardapane, Aurelio Uncini*

- `2003.00952v2` - [abs](http://arxiv.org/abs/2003.00952v2) - [pdf](http://arxiv.org/pdf/2003.00952v2)

> Bayesian Neural Networks (BNNs) are trained to optimize an entire distribution over their weights instead of a single set, having significant advantages in terms of, e.g., interpretability, multi-task learning, and calibration. Because of the intractability of the resulting optimization problem, most BNNs are either sampled through Monte Carlo methods, or trained by minimizing a suitable Evidence Lower BOund (ELBO) on a variational approximation. In this paper, we propose a variant of the latter, wherein we replace the Kullback-Leibler divergence in the ELBO term with a Maximum Mean Discrepancy (MMD) estimator, inspired by recent work in variational inference. After motivating our proposal based on the properties of the MMD term, we proceed to show a number of empirical advantages of the proposed formulation over the state-of-the-art. In particular, our BNNs achieve higher accuracy on multiple benchmarks, including several image classification tasks. In addition, they are more robust to the selection of a prior over the weights, and they are better calibrated. As a second contribution, we provide a new formulation for estimating the uncertainty on a given prediction, showing it performs in a more robust fashion against adversarial attacks and the injection of noise over their inputs, compared to more classical criteria such as the differential entropy.

</details>

<details>

<summary>2020-09-30 16:02:45 - PAC-Bayes unleashed: generalisation bounds with unbounded losses</summary>

- *Maxime Haddouche, Benjamin Guedj, Omar Rivasplata, John Shawe-Taylor*

- `2006.07279v2` - [abs](http://arxiv.org/abs/2006.07279v2) - [pdf](http://arxiv.org/pdf/2006.07279v2)

> We present new PAC-Bayesian generalisation bounds for learning problems with unbounded loss functions. This extends the relevance and applicability of the PAC-Bayes learning framework, where most of the existing literature focuses on supervised learning problems with a bounded loss function (typically assumed to take values in the interval [0;1]). In order to relax this assumption, we propose a new notion called HYPE (standing for \emph{HYPothesis-dependent rangE}), which effectively allows the range of the loss to depend on each predictor. Based on this new notion we derive a novel PAC-Bayesian generalisation bound for unbounded loss functions, and we instantiate it on a linear regression problem. To make our theory usable by the largest audience possible, we include discussions on actual computation, practicality and limitations of our assumptions.

</details>


## 2020-10

<details>

<summary>2020-10-01 09:26:53 - Spatial 3D Matérn priors for fast whole-brain fMRI analysis</summary>

- *Per Sidén, Finn Lindgren, David Bolin, Anders Eklund, Mattias Villani*

- `1906.10591v2` - [abs](http://arxiv.org/abs/1906.10591v2) - [pdf](http://arxiv.org/pdf/1906.10591v2)

> Bayesian whole-brain functional magnetic resonance imaging (fMRI) analysis with three-dimensional spatial smoothing priors has been shown to produce state-of-the-art activity maps without pre-smoothing the data. The proposed inference algorithms are computationally demanding however, and the proposed spatial priors have several less appealing properties, such as being improper and having infinite spatial range. We propose a statistical inference framework for whole-brain fMRI analysis based on the class of Mat\'ern covariance functions. The framework uses the Gaussian Markov random field (GMRF) representation of possibly anisotropic spatial Mat\'ern fields via the stochastic partial differential equation (SPDE) approach of Lindgren et al. (2011). This allows for more flexible and interpretable spatial priors, while maintaining the sparsity required for fast inference in the high-dimensional whole-brain setting. We develop an accelerated stochastic gradient descent (SGD) optimization algorithm for empirical Bayes (EB) inference of the spatial hyperparameters. Conditionally on the inferred hyperparameters, we make a fully Bayesian treatment of the brain activity. The Mat\'ern prior is applied to both simulated and experimental task-fMRI data and clearly demonstrates that it is a more reasonable choice than the previously used priors, using comparisons of activity maps, prior simulation and cross-validation.

</details>

<details>

<summary>2020-10-01 10:22:15 - Bayesian Policy Search for Stochastic Domains</summary>

- *David Tolpin, Yuan Zhou, Hongseok Yang*

- `2010.00284v1` - [abs](http://arxiv.org/abs/2010.00284v1) - [pdf](http://arxiv.org/pdf/2010.00284v1)

> AI planning can be cast as inference in probabilistic models, and probabilistic programming was shown to be capable of policy search in partially observable domains. Prior work introduces policy search through Markov chain Monte Carlo in deterministic domains, as well as adapts black-box variational inference to stochastic domains, however not in the strictly Bayesian sense. In this work, we cast policy search in stochastic domains as a Bayesian inference problem and provide a scheme for encoding such problems as nested probabilistic programs. We argue that probabilistic programs for policy search in stochastic domains should involve nested conditioning, and provide an adaption of Lightweight Metropolis-Hastings (LMH) for robust inference in such programs. We apply the proposed scheme to stochastic domains and show that policies of similar quality are learned, despite a simpler and more general inference algorithm. We believe that the proposed variant of LMH is novel and applicable to a wider class of probabilistic programs with nested conditioning.

</details>

<details>

<summary>2020-10-01 12:59:20 - Efficient leave-one-out cross-validation for Bayesian non-factorized normal and Student-t models</summary>

- *Paul-Christian Bürkner, Jonah Gabry, Aki Vehtari*

- `1810.10559v5` - [abs](http://arxiv.org/abs/1810.10559v5) - [pdf](http://arxiv.org/pdf/1810.10559v5)

> Cross-validation can be used to measure a model's predictive accuracy for the purpose of model comparison, averaging, or selection. Standard leave-one-out cross-validation (LOO-CV) requires that the observation model can be factorized into simple terms, but a lot of important models in temporal and spatial statistics do not have this property or are inefficient or unstable when forced into a factorized form. We derive how to efficiently compute and validate both exact and approximate LOO-CV for any Bayesian non-factorized model with a multivariate normal or Student-t distribution on the outcome values. We demonstrate the method using lagged simultaneously autoregressive (SAR) models as a case study.

</details>

<details>

<summary>2020-10-01 16:35:15 - Bayesian spatial modelling of terrestrial radiation in Switzerland</summary>

- *Christophe L. Folly, Garyfallos Konstantinoudis, Antonella Mazzei-Abba, Christian Kreis, Benno Bucher, Reinhard Furrer, Ben D. Spycher*

- `2010.00534v1` - [abs](http://arxiv.org/abs/2010.00534v1) - [pdf](http://arxiv.org/pdf/2010.00534v1)

> The geographic variation of terrestrial radiation can be exploited in epidemiological studies of the health effects of protracted low-dose exposure. Various methods have been applied to derive maps of this variation. We aimed to construct a map of terrestrial radiation for Switzerland. We used airborne $\gamma$-spectrometry measurements to model the ambient dose rates from terrestrial radiation through a Bayesian mixed-effects model and conducted inference using Integrated Nested Laplace Approximation (INLA). We predicted higher levels of ambient dose rates in the alpine regions and Ticino compared with the western and northern parts of Switzerland. We provide a map that can be used for exposure assessment in epidemiological studies and as a baseline map for assessing potential contamination.

</details>

<details>

<summary>2020-10-01 23:26:42 - Fast fully-reproducible serial/parallel Monte Carlo and MCMC simulations and visualizations via ParaMonte::Python library</summary>

- *Amir Shahmoradi, Fatemeh Bagheri, Joshua Alexander Osborne*

- `2010.00724v1` - [abs](http://arxiv.org/abs/2010.00724v1) - [pdf](http://arxiv.org/pdf/2010.00724v1)

> ParaMonte::Python (standing for Parallel Monte Carlo in Python) is a serial and MPI-parallelized library of (Markov Chain) Monte Carlo (MCMC) routines for sampling mathematical objective functions, in particular, the posterior distributions of parameters in Bayesian modeling and analysis in data science, Machine Learning, and scientific inference in general. In addition to providing access to fast high-performance serial/parallel Monte Carlo and MCMC sampling routines, the ParaMonte::Python library provides extensive post-processing and visualization tools that aim to automate and streamline the process of model calibration and uncertainty quantification in Bayesian data analysis. Furthermore, the automatically-enabled restart functionality of ParaMonte::Python samplers ensure seamless fully-deterministic into-the-future restart of Monte Carlo simulations, should any interruptions happen. The ParaMonte::Python library is MIT-licensed and is permanently maintained on GitHub at https://github.com/cdslaborg/paramonte/tree/master/src/interface/Python.

</details>

<details>

<summary>2020-10-02 07:01:27 - Convergence Analysis of a Collapsed Gibbs Sampler for Bayesian Vector Autoregressions</summary>

- *Karl Oskar Ekvall, Galin L. Jones*

- `1907.03170v3` - [abs](http://arxiv.org/abs/1907.03170v3) - [pdf](http://arxiv.org/pdf/1907.03170v3)

> We study the convergence properties of a collapsed Gibbs sampler for Bayesian vector autoregressions with predictors, or exogenous variables. The Markov chain generated by our algorithm is shown to be geometrically ergodic regardless of whether the number of observations in the underlying vector autoregression is small or large in comparison to the order and dimension of it. In a convergence complexity analysis, we also give conditions for when the geometric ergodicity is asymptotically stable as the number of observations tends to infinity. Specifically, the geometric convergence rate is shown to be bounded away from unity asymptotically, either almost surely or with probability tending to one, depending on what is assumed about the data generating process. This result is one of the first of its kind for practically relevant Markov chain Monte Carlo algorithms. Our convergence results hold under close to arbitrary model misspecification.

</details>

<details>

<summary>2020-10-02 13:18:27 - BOSS: Bayesian Optimization over String Spaces</summary>

- *Henry B. Moss, Daniel Beck, Javier Gonzalez, David S. Leslie, Paul Rayson*

- `2010.00979v1` - [abs](http://arxiv.org/abs/2010.00979v1) - [pdf](http://arxiv.org/pdf/2010.00979v1)

> This article develops a Bayesian optimization (BO) method which acts directly over raw strings, proposing the first uses of string kernels and genetic algorithms within BO loops. Recent applications of BO over strings have been hindered by the need to map inputs into a smooth and unconstrained latent space. Learning this projection is computationally and data-intensive. Our approach instead builds a powerful Gaussian process surrogate model based on string kernels, naturally supporting variable length inputs, and performs efficient acquisition function maximization for spaces with syntactical constraints. Experiments demonstrate considerably improved optimization over existing approaches across a broad range of constraints, including the popular setting where syntax is governed by a context-free grammar.

</details>

<details>

<summary>2020-10-02 22:02:56 - Causal Inference in high dimensions: A marriage between Bayesian modeling and good frequentist properties</summary>

- *Joseph Antonelli, Georgia Papadogeorgou, Francesca Dominici*

- `1805.04899v7` - [abs](http://arxiv.org/abs/1805.04899v7) - [pdf](http://arxiv.org/pdf/1805.04899v7)

> We introduce a framework for estimating causal effects of binary and continuous treatments in high dimensions. We show how posterior distributions of treatment and outcome models can be used together with doubly robust estimators. We propose an approach to uncertainty quantification for the doubly robust estimator which utilizes posterior distributions of model parameters and (1) results in good frequentist properties in small samples, (2) is based on a single MCMC, and (3) improves over frequentist measures of uncertainty which rely on asymptotic properties. We show that our proposed variance estimation strategy is consistent when both models are correctly specified and that it is conservative in finite samples or when one or both models are misspecified. We consider a flexible framework for modeling the treatment and outcome processes within the Bayesian paradigm that reduces model dependence, accommodates nonlinearity, and achieves dimension reduction of the covariate space. We illustrate the ability of the proposed approach to flexibly estimate causal effects in high dimensions and appropriately quantify uncertainty, and show that it performs well relative to existing approaches. Finally, we estimate the effect of continuous environmental exposures on cholesterol and triglyceride levels. An R package is available at github.com/jantonelli111/DoublyRobustHD.

</details>

<details>

<summary>2020-10-03 01:44:16 - CorrAttack: Black-box Adversarial Attack with Structured Search</summary>

- *Zhichao Huang, Yaowei Huang, Tong Zhang*

- `2010.01250v1` - [abs](http://arxiv.org/abs/2010.01250v1) - [pdf](http://arxiv.org/pdf/2010.01250v1)

> We present a new method for score-based adversarial attack, where the attacker queries the loss-oracle of the target model. Our method employs a parameterized search space with a structure that captures the relationship of the gradient of the loss function. We show that searching over the structured space can be approximated by a time-varying contextual bandits problem, where the attacker takes feature of the associated arm to make modifications of the input, and receives an immediate reward as the reduction of the loss function. The time-varying contextual bandits problem can then be solved by a Bayesian optimization procedure, which can take advantage of the features of the structured action space. The experiments on ImageNet and the Google Cloud Vision API demonstrate that the proposed method achieves the state of the art success rates and query efficiencies for both undefended and defended models.

</details>

<details>

<summary>2020-10-03 03:25:54 - Estimation of causal effects of multiple treatments in healthcare database studies with rare outcomes</summary>

- *Liangyuan Hu, Chenyang Gu*

- `2008.07687v2` - [abs](http://arxiv.org/abs/2008.07687v2) - [pdf](http://arxiv.org/pdf/2008.07687v2)

> The preponderance of large-scale healthcare databases provide abundant opportunities for comparative effectiveness research. Evidence necessary to making informed treatment decisions often relies on comparing effectiveness of multiple treatment options on outcomes of interest observed in a small number of individuals. Causal inference with multiple treatments and rare outcomes is a subject that has been treated sparingly in the literature. This paper designs three sets of simulations, representative of the structure of our healthcare database study, and propose causal analysis strategies for such settings. We investigate and compare the operating characteristics of three types of methods and their variants: Bayesian Additive Regression Trees (BART), regression adjustment on multivariate spline of generalized propensity scores (RAMS) and inverse probability of treatment weighting (IPTW) with multinomial logistic regression or generalized boosted models. Our results suggest that BART and RAMS provide lower bias and mean squared error, and the widely used IPTW methods deliver unfavorable operating characteristics. We illustrate the methods using a case study evaluating the comparative effectiveness of robotic-assisted surgery, video-assisted thoracoscopic surgery and open thoracotomy for treating non-small cell lung cancer.

</details>

<details>

<summary>2020-10-03 14:46:06 - Ramifications of Approximate Posterior Inference for Bayesian Deep Learning in Adversarial and Out-of-Distribution Settings</summary>

- *John Mitros, Arjun Pakrashi, Brian Mac Namee*

- `2009.01798v2` - [abs](http://arxiv.org/abs/2009.01798v2) - [pdf](http://arxiv.org/pdf/2009.01798v2)

> Deep neural networks have been successful in diverse discriminative classification tasks, although, they are poorly calibrated often assigning high probability to misclassified predictions. Potential consequences could lead to trustworthiness and accountability of the models when deployed in real applications, where predictions are evaluated based on their confidence scores. Existing solutions suggest the benefits attained by combining deep neural networks and Bayesian inference to quantify uncertainty over the models' predictions for ambiguous datapoints. In this work we propose to validate and test the efficacy of likelihood based models in the task of out of distribution detection (OoD). Across different datasets and metrics we show that Bayesian deep learning models on certain occasions marginally outperform conventional neural networks and in the event of minimal overlap between in/out distribution classes, even the best models exhibit a reduction in AUC scores in detecting OoD data. Preliminary investigations indicate the potential inherent role of bias due to choices of initialisation, architecture or activation functions. We hypothesise that the sensitivity of neural networks to unseen inputs could be a multi-factor phenomenon arising from the different architectural design choices often amplified by the curse of dimensionality. Furthermore, we perform a study to find the effect of the adversarial noise resistance methods on in and out-of-distribution performance, as well as, also investigate adversarial noise robustness of Bayesian deep learners.

</details>

<details>

<summary>2020-10-03 20:09:18 - Quantifying the Trendiness of Trends</summary>

- *Andreas Kryger Jensen, Claus Thorn Ekstrøm*

- `1912.11848v2` - [abs](http://arxiv.org/abs/1912.11848v2) - [pdf](http://arxiv.org/pdf/1912.11848v2)

> News media often report that the trend of some public health outcome has changed. These statements are frequently based on longitudinal data, and the change in trend is typically found to have occurred at the most recent data collection time point - if no change had occurred the story is less likely to be reported. Such claims may potentially influence public health decisions on a national level.   We propose two measures for quantifying the trendiness of trends. Assuming that reality evolves in continuous time we define what constitutes a trend and a change in trend, and introduce a probabilistic Trend Direction Index. This index has the interpretation of the probability that a latent characteristic has changed monotonicity at any given time conditional on observed data. We also define an index of Expected Trend Instability quantifying the expected number of changes in trend on an interval.   Using a latent Gaussian Process model we show how the Trend Direction Index and the Expected Trend Instability can be estimated in a Bayesian framework and use the methods to analyze the proportion of smokers in Denmark during the last 20 years, and the development of new COVID-19 cases in Italy from February 24th onwards.

</details>

<details>

<summary>2020-10-03 21:58:21 - Bayesian Optimization with Output-Weighted Optimal Sampling</summary>

- *Antoine Blanchard, Themistoklis Sapsis*

- `2004.10599v4` - [abs](http://arxiv.org/abs/2004.10599v4) - [pdf](http://arxiv.org/pdf/2004.10599v4)

> In Bayesian optimization, accounting for the importance of the output relative to the input is a crucial yet challenging exercise, as it can considerably improve the final result but often involves inaccurate and cumbersome entropy estimations. We approach the problem from the perspective of importance-sampling theory, and advocate the use of the likelihood ratio to guide the search algorithm towards regions of the input space where the objective function to be minimized assumes abnormally small values. The likelihood ratio acts as a sampling weight and can be computed at each iteration without severely deteriorating the overall efficiency of the algorithm. In particular, it can be approximated in a way that makes the approach tractable in high dimensions. The "likelihood-weighted" acquisition functions introduced in this work are found to outperform their unweighted counterparts in a number of applications.

</details>

<details>

<summary>2020-10-04 15:16:18 - Online learning of both state and dynamics using ensemble Kalman filters</summary>

- *Marc Bocquet, Alban Farchi, Quentin Malartic*

- `2006.03859v2` - [abs](http://arxiv.org/abs/2006.03859v2) - [pdf](http://arxiv.org/pdf/2006.03859v2)

> The reconstruction of the dynamics of an observed physical system as a surrogate model has been brought to the fore by recent advances in machine learning. To deal with partial and noisy observations in that endeavor, machine learning representations of the surrogate model can be used within a Bayesian data assimilation framework. However, these approaches require to consider long time series of observational data, meant to be assimilated all together. This paper investigates the possibility to learn both the dynamics and the state online, i.e. to update their estimates at any time, in particular when new observations are acquired. The estimation is based on the ensemble Kalman filter (EnKF) family of algorithms using a rather simple representation for the surrogate model and state augmentation. We consider the implication of learning dynamics online through (i) a global EnKF, (i) a local EnKF and (iii) an iterative EnKF and we discuss in each case issues and algorithmic solutions. We then demonstrate numerically the efficiency and assess the accuracy of these methods using one-dimensional, one-scale and two-scale chaotic Lorenz models.

</details>

<details>

<summary>2020-10-04 20:04:34 - Forecasting with Bayesian Grouped Random Effects in Panel Data</summary>

- *Boyuan Zhang*

- `2007.02435v8` - [abs](http://arxiv.org/abs/2007.02435v8) - [pdf](http://arxiv.org/pdf/2007.02435v8)

> In this paper, we estimate and leverage latent constant group structure to generate the point, set, and density forecasts for short dynamic panel data. We implement a nonparametric Bayesian approach to simultaneously identify coefficients and group membership in the random effects which are heterogeneous across groups but fixed within a group. This method allows us to flexibly incorporate subjective prior knowledge on the group structure that potentially improves the predictive accuracy. In Monte Carlo experiments, we demonstrate that our Bayesian grouped random effects (BGRE) estimators produce accurate estimates and score predictive gains over standard panel data estimators. With a data-driven group structure, the BGRE estimators exhibit comparable accuracy of clustering with the Kmeans algorithm and outperform a two-step Bayesian grouped estimator whose group structure relies on Kmeans. In the empirical analysis, we apply our method to forecast the investment rate across a broad range of firms and illustrate that the estimated latent group structure improves forecasts relative to standard panel data estimators.

</details>

<details>

<summary>2020-10-05 13:13:28 - Parameter Optimization using high-dimensional Bayesian Optimization</summary>

- *David Yenicelik*

- `2010.03955v1` - [abs](http://arxiv.org/abs/2010.03955v1) - [pdf](http://arxiv.org/pdf/2010.03955v1)

> In this thesis, I explore the possibilities of conducting Bayesian optimization techniques in high dimensional domains. Although high dimensional domains can be defined to be between hundreds and thousands of dimensions, we will primarily focus on problem settings that occur between two and 20 dimensions. As such, we focus on solutions to practical problems, such as tuning the parameters for an electron accelerator, or for even simpler tasks that can be run and optimized just in time with a standard laptop at hand. Our main contributions are 1.) comparing how the log-likelihood affects the angle-difference in the real projection matrix, and the found matrix matrix, 2.) an extensive analysis of current popular methods including strengths and shortcomings, 3.) a short analysis on how dimensionality reduction techniques can be used for feature selection, and 4.) a novel algorithm called "BORING", which allows for a simple fallback mechanism if the matrix identification fails, as well as taking into consideration "passive" subspaces which provide small perturbations of the function at hand. The main features of BORING are 1.) the possibility to identify the subspace (unlike most other optimization algorithms), and 2.) to provide a much lower penalty to identify the subspace if identification fails, as optimization is still the primary goal.

</details>

<details>

<summary>2020-10-05 15:04:18 - Using Bayesian deep learning approaches for uncertainty-aware building energy surrogate models</summary>

- *Paul Westermann, Ralph Evins*

- `2010.03029v1` - [abs](http://arxiv.org/abs/2010.03029v1) - [pdf](http://arxiv.org/pdf/2010.03029v1)

> Fast machine learning-based surrogate models are trained to emulate slow, high-fidelity engineering simulation models to accelerate engineering design tasks. This introduces uncertainty as the surrogate is only an approximation of the original model.   Bayesian methods can quantify that uncertainty, and deep learning models exist that follow the Bayesian paradigm. These models, namely Bayesian neural networks and Gaussian process models, enable us to give predictions together with an estimate of the model's uncertainty. As a result we can derive uncertainty-aware surrogate models that can automatically suspect unseen design samples that cause large emulation errors. For these samples, the high-fidelity model can be queried instead. This outlines how the Bayesian paradigm allows us to hybridize fast, but approximate, and slow, but accurate models.   In this paper, we train two types of Bayesian models, dropout neural networks and stochastic variational Gaussian Process models, to emulate a complex high dimensional building energy performance simulation problem. The surrogate model processes 35 building design parameters (inputs) to estimate 12 different performance metrics (outputs). We benchmark both approaches, prove their accuracy to be competitive, and show that errors can be reduced by up to 30% when the 10% of samples with the highest uncertainty are transferred to the high-fidelity model.

</details>

<details>

<summary>2020-10-05 15:12:52 - Learned Hardware/Software Co-Design of Neural Accelerators</summary>

- *Zhan Shi, Chirag Sakhuja, Milad Hashemi, Kevin Swersky, Calvin Lin*

- `2010.02075v1` - [abs](http://arxiv.org/abs/2010.02075v1) - [pdf](http://arxiv.org/pdf/2010.02075v1)

> The use of deep learning has grown at an exponential rate, giving rise to numerous specialized hardware and software systems for deep learning. Because the design space of deep learning software stacks and hardware accelerators is diverse and vast, prior work considers software optimizations separately from hardware architectures, effectively reducing the search space. Unfortunately, this bifurcated approach means that many profitable design points are never explored. This paper instead casts the problem as hardware/software co-design, with the goal of automatically identifying desirable points in the joint design space. The key to our solution is a new constrained Bayesian optimization framework that avoids invalid solutions by exploiting the highly constrained features of this design space, which are semi-continuous/semi-discrete. We evaluate our optimization framework by applying it to a variety of neural models, improving the energy-delay product by 18% (ResNet) and 40% (DQN) over hand-tuned state-of-the-art systems, as well as demonstrating strong results on other neural network architectures, such as MLPs and Transformers.

</details>

<details>

<summary>2020-10-05 15:51:18 - Highly Scalable Bayesian Geostatistical Modeling via Meshed Gaussian Processes on Partitioned Domains</summary>

- *Michele Peruzzi, Sudipto Banerjee, Andrew O. Finley*

- `2003.11208v2` - [abs](http://arxiv.org/abs/2003.11208v2) - [pdf](http://arxiv.org/pdf/2003.11208v2)

> We introduce a class of scalable Bayesian hierarchical models for the analysis of massive geostatistical datasets. The underlying idea combines ideas on high-dimensional geostatistics by partitioning the spatial domain and modeling the regions in the partition using a sparsity-inducing directed acyclic graph (DAG). We extend the model over the DAG to a well-defined spatial process, which we call the Meshed Gaussian Process (MGP). A major contribution is the development of a MGPs on tessellated domains, accompanied by a Gibbs sampler for the efficient recovery of spatial random effects. In particular, the cubic MGP (Q-MGP) can harness high-performance computing resources by executing all large-scale operations in parallel within the Gibbs sampler, improving mixing and computing time compared to sequential updating schemes. Unlike some existing models for large spatial data, a Q-MGP facilitates massive caching of expensive matrix operations, making it particularly apt in dealing with spatiotemporal remote-sensing data. We compare Q-MGPs with large synthetic and real world data against state-of-the-art methods. We also illustrate using Normalized Difference Vegetation Index (NDVI) data from the Serengeti park region to recover latent multivariate spatiotemporal random effects at millions of locations. The source code is available at https://github.com/mkln/meshgp.

</details>

<details>

<summary>2020-10-06 09:02:46 - Exploring the Effects of COVID-19 Containment Policies on Crime: An Empirical Analysis of the Short-term Aftermath in Los Angeles</summary>

- *Gian Maria Campedelli, Alberto Aziani, Serena Favarin*

- `2003.11021v3` - [abs](http://arxiv.org/abs/2003.11021v3) - [pdf](http://arxiv.org/pdf/2003.11021v3)

> This work investigates whether and how COVID-19 containment policies had an immediate impact on crime trends in Los Angeles. The analysis is conducted using Bayesian structural time-series and focuses on nine crime categories and on the overall crime count, daily monitored from January 1st 2017 to March 28th 2020. We concentrate on two post-intervention time windows - from March 4th to March 16th and from March 4th to March 28th 2020 - to dynamically assess the short-term effects of mild and strict policies. In Los Angeles, overall crime has significantly decreased, as well as robbery, shoplifting, theft, and battery. No significant effect has been detected for vehicle theft, burglary, assault with a deadly weapon, intimate partner assault, and homicide. Results suggest that, in the first weeks after the interventions are put in place, social distancing impacts more directly on instrumental and less serious crimes. Policy implications are also discussed.

</details>

<details>

<summary>2020-10-06 12:40:45 - State of the Art Survey of Deep Learning and Machine Learning Models for Smart Cities and Urban Sustainability</summary>

- *Saeed Nosratabadi, Amir Mosavi, Ramin Keivani, Sina Ardabili, Farshid Aram*

- `2010.02670v1` - [abs](http://arxiv.org/abs/2010.02670v1) - [pdf](http://arxiv.org/pdf/2010.02670v1)

> Deep learning (DL) and machine learning (ML) methods have recently contributed to the advancement of models in the various aspects of prediction, planning, and uncertainty analysis of smart cities and urban development. This paper presents the state of the art of DL and ML methods used in this realm. Through a novel taxonomy, the advances in model development and new application domains in urban sustainability and smart cities are presented. Findings reveal that five DL and ML methods have been most applied to address the different aspects of smart cities. These are artificial neural networks; support vector machines; decision trees; ensembles, Bayesians, hybrids, and neuro-fuzzy; and deep learning. It is also disclosed that energy, health, and urban transport are the main domains of smart cities that DL and ML methods contributed in to address their problems.

</details>

<details>

<summary>2020-10-06 16:08:58 - Additive Tree-Structured Conditional Parameter Spaces in Bayesian Optimization: A Novel Covariance Function and a Fast Implementation</summary>

- *Xingchen Ma, Matthew B. Blaschko*

- `2010.03171v1` - [abs](http://arxiv.org/abs/2010.03171v1) - [pdf](http://arxiv.org/pdf/2010.03171v1)

> Bayesian optimization (BO) is a sample-efficient global optimization algorithm for black-box functions which are expensive to evaluate. Existing literature on model based optimization in conditional parameter spaces are usually built on trees. In this work, we generalize the additive assumption to tree-structured functions and propose an additive tree-structured covariance function, showing improved sample-efficiency, wider applicability and greater flexibility. Furthermore, by incorporating the structure information of parameter spaces and the additive assumption in the BO loop, we develop a parallel algorithm to optimize the acquisition function and this optimization can be performed in a low dimensional space. We demonstrate our method on an optimization benchmark function, on a neural network compression problem, on pruning pre-trained VGG16 and ResNet50 models as well as on searching activation functions of ResNet20. Experimental results show our approach significantly outperforms the current state of the art for conditional parameter optimization including SMAC, TPE and Jenatton et al. (2017).

</details>

<details>

<summary>2020-10-06 21:49:54 - Sequential Changepoint Detection in Neural Networks with Checkpoints</summary>

- *Michalis K. Titsias, Jakub Sygnowski, Yutian Chen*

- `2010.03053v1` - [abs](http://arxiv.org/abs/2010.03053v1) - [pdf](http://arxiv.org/pdf/2010.03053v1)

> We introduce a framework for online changepoint detection and simultaneous model learning which is applicable to highly parametrized models, such as deep neural networks. It is based on detecting changepoints across time by sequentially performing generalized likelihood ratio tests that require only evaluations of simple prediction score functions. This procedure makes use of checkpoints, consisting of early versions of the actual model parameters, that allow to detect distributional changes by performing predictions on future data. We define an algorithm that bounds the Type I error in the sequential testing procedure. We demonstrate the efficiency of our method in challenging continual learning applications with unknown task changepoints, and show improved performance compared to online Bayesian changepoint detection.

</details>

<details>

<summary>2020-10-06 23:37:28 - Is the Best Better? Bayesian Statistical Model Comparison for Natural Language Processing</summary>

- *Piotr Szymański, Kyle Gorman*

- `2010.03088v1` - [abs](http://arxiv.org/abs/2010.03088v1) - [pdf](http://arxiv.org/pdf/2010.03088v1)

> Recent work raises concerns about the use of standard splits to compare natural language processing models. We propose a Bayesian statistical model comparison technique which uses k-fold cross-validation across multiple data sets to estimate the likelihood that one model will outperform the other, or that the two will produce practically equivalent results. We use this technique to rank six English part-of-speech taggers across two data sets and three evaluation metrics.

</details>

<details>

<summary>2020-10-07 02:15:04 - Bayesian Distance Weighted Discrimination</summary>

- *Eric F. Lock*

- `2010.03111v1` - [abs](http://arxiv.org/abs/2010.03111v1) - [pdf](http://arxiv.org/pdf/2010.03111v1)

> Distance weighted discrimination (DWD) is a linear discrimination method that is particularly well-suited for classification tasks with high-dimensional data. The DWD coefficients minimize an intuitive objective function, which can solved very efficiently using state-of-the-art optimization techniques. However, DWD has not yet been cast into a model-based framework for statistical inference. In this article we show that DWD identifies the mode of a proper Bayesian posterior distribution, that results from a particular link function for the class probabilities and a shrinkage-inducing proper prior distribution on the coefficients. We describe a relatively efficient Markov chain Monte Carlo (MCMC) algorithm to simulate from the true posterior under this Bayesian framework. We show that the posterior is asymptotically normal and derive the mean and covariance matrix of its limiting distribution. Through several simulation studies and an application to breast cancer genomics we demonstrate how the Bayesian approach to DWD can be used to (1) compute well-calibrated posterior class probabilities, (2) assess uncertainty in the DWD coefficients and resulting sample scores, (3) improve power via semi-supervised analysis when not all class labels are available, and (4) automatically determine a penalty tuning parameter within the model-based framework. R code to perform Bayesian DWD is available at https://github.com/lockEF/BayesianDWD .

</details>

<details>

<summary>2020-10-07 14:34:28 - Effects of Model Misspecification on Bayesian Bandits: Case Studies in UX Optimization</summary>

- *Mack Sweeney, Matthew van Adelsberg, Kathryn Laskey, Carlotta Domeniconi*

- `2010.04010v1` - [abs](http://arxiv.org/abs/2010.04010v1) - [pdf](http://arxiv.org/pdf/2010.04010v1)

> Bayesian bandits using Thompson Sampling have seen increasing success in recent years. Yet existing value models (of rewards) are misspecified on many real-world problem. We demonstrate this on the User Experience Optimization (UXO) problem, providing a novel formulation as a restless, sleeping bandit with unobserved confounders plus optional stopping. Our case studies show how common misspecifications can lead to sub-optimal rewards, and we provide model extensions to address these, along with a scientific model building process practitioners can adopt or adapt to solve their own unique problems. To our knowledge, this is the first study showing the effects of overdispersion on bandit explore/exploit efficacy, tying the common notions of under- and over-confidence to over- and under-exploration, respectively. We also present the first model to exploit cointegration in a restless bandit, demonstrating that finite regret and fast and consistent optional stopping are possible by moving beyond simpler windowing, discounting, and drift models.

</details>

<details>

<summary>2020-10-07 16:03:29 - A Seamless Phase I/II Platform Design with a Time-To-Event Efficacy Endpoint for Potential COVID-19 Therapies</summary>

- *Thomas Jaki, Helen Barnett, Andrew Titman, Pavel Mozgunov*

- `2010.06518v1` - [abs](http://arxiv.org/abs/2010.06518v1) - [pdf](http://arxiv.org/pdf/2010.06518v1)

> In the search for effective treatments for COVID-19, initial emphasis has been on re-purposed treatments. To maximise the chances of finding successful treatments, novel treatments that have been developed for this disease in particular, are needed. In this manuscript we describe and evaluate the statistical design of the AGILE platform, an adaptive randomized seamless Phase I/II trial platform that seeks to quickly establish a safe range of doses and investigates treatments for potential efficacy using a Bayesian sequential trial design. Both single agent and combination treatments are considered. We find that the design can identify potential treatments that are safe and efficacious reliably with small to moderate sample sizes.

</details>

<details>

<summary>2020-10-07 17:44:41 - Information Criterion for Boltzmann Approximation Problems</summary>

- *Youngjun Choe, Yen-Chi Chen, Nick Terry*

- `1704.04315v3` - [abs](http://arxiv.org/abs/1704.04315v3) - [pdf](http://arxiv.org/pdf/1704.04315v3)

> This paper considers the problem of approximating a density when it can be evaluated up to a normalizing constant at a limited number of points. We call this problem the Boltzmann approximation (BA) problem. The BA problem is ubiquitous in statistics, such as approximating a posterior density for Bayesian inference and estimating an optimal density for importance sampling. Approximating the density with a parametric model can be cast as a model selection problem. This problem cannot be addressed with traditional approaches that maximize the (marginal) likelihood of a model, for example, using the Akaike information criterion (AIC) or Bayesian information criterion (BIC). We instead aim to minimize the cross-entropy that gauges the deviation of a parametric model from the target density. We propose a novel information criterion called the cross-entropy information criterion (CIC) and prove that the CIC is an asymptotically unbiased estimator of the cross-entropy (up to a multiplicative constant) under some regularity conditions. We propose an iterative method to approximate the target density by minimizing the CIC. We demonstrate that the proposed method selects a parametric model that well approximates the target density.

</details>

<details>

<summary>2020-10-07 17:53:10 - Estimating the Stillbirth Rate for 195 Countries Using A Bayesian Sparse Regression Model with Temporal Smoothing</summary>

- *Zhengfan Wang, Miranda J. Fix, Lucia Hug, Anu Mishra, Danzhen You, Hannah Blencowe, Jon Wakefield, Leontine Alkema*

- `2010.03551v1` - [abs](http://arxiv.org/abs/2010.03551v1) - [pdf](http://arxiv.org/pdf/2010.03551v1)

> Estimation of stillbirth rates globally is complicated because of the paucity of reliable data from countries where most stillbirths occur. We compiled data and developed a Bayesian hierarchical temporal sparse regression model for estimating stillbirth rates for all countries from 2000 to 2019. The model combines covariates with a temporal smoothing process so that estimates are data-driven in country-periods with high-quality data and deter-mined by covariates for country-periods with limited or no data. Horseshoepriors are used to encourage sparseness. The model adjusts observations with alternative stillbirth definitions and accounts for bias in observations that are subject to non-sampling errors. In-sample goodness of fit and out-of-sample validation results suggest that the model is reasonably well calibrated. The model is used by the UN Inter-agency Group for Child Mortality Estimation to monitor the stillbirth rate for all countries.

</details>

<details>

<summary>2020-10-07 18:32:32 - Ensembling geophysical models with Bayesian Neural Networks</summary>

- *Ushnish Sengupta, Matt Amos, J. Scott Hosking, Carl Edward Rasmussen, Matthew Juniper, Paul J. Young*

- `2010.03561v1` - [abs](http://arxiv.org/abs/2010.03561v1) - [pdf](http://arxiv.org/pdf/2010.03561v1)

> Ensembles of geophysical models improve projection accuracy and express uncertainties. We develop a novel data-driven ensembling strategy for combining geophysical models using Bayesian Neural Networks, which infers spatiotemporally varying model weights and bias while accounting for heteroscedastic uncertainties in the observations. This produces more accurate and uncertainty-aware projections without sacrificing interpretability. Applied to the prediction of total column ozone from an ensemble of 15 chemistry-climate models, we find that the Bayesian neural network ensemble (BayNNE) outperforms existing ensembling methods, achieving a 49.4% reduction in RMSE for temporal extrapolation, and a 67.4% reduction in RMSE for polar data voids, compared to a weighted mean. Uncertainty is also well-characterized, with 90.6% of the data points in our extrapolation validation dataset lying within 2 standard deviations and 98.5% within 3 standard deviations.

</details>

<details>

<summary>2020-10-08 03:01:52 - Heteroscedastic Bayesian Optimisation for Stochastic Model Predictive Control</summary>

- *Rel Guzman, Rafael Oliveira, Fabio Ramos*

- `2010.00202v2` - [abs](http://arxiv.org/abs/2010.00202v2) - [pdf](http://arxiv.org/pdf/2010.00202v2)

> Model predictive control (MPC) has been successful in applications involving the control of complex physical systems. This class of controllers leverages the information provided by an approximate model of the system's dynamics to simulate the effect of control actions. MPC methods also present a few hyper-parameters which may require a relatively expensive tuning process by demanding interactions with the physical system. Therefore, we investigate fine-tuning MPC methods in the context of stochastic MPC, which presents extra challenges due to the randomness of the controller's actions. In these scenarios, performance outcomes present noise, which is not homogeneous across the domain of possible hyper-parameter settings, but which varies in an input-dependent way. To address these issues, we propose a Bayesian optimisation framework that accounts for heteroscedastic noise to tune hyper-parameters in control problems. Empirical results on benchmark continuous control tasks and a physical robot support the proposed framework's suitability relative to baselines, which do not take heteroscedasticity into account.

</details>

<details>

<summary>2020-10-08 04:36:28 - Explicit Estimation of Derivatives from Data and Differential Equations by Gaussian Process Regression</summary>

- *Hongqiao Wang, Xiang Zhou*

- `2004.05796v2` - [abs](http://arxiv.org/abs/2004.05796v2) - [pdf](http://arxiv.org/pdf/2004.05796v2)

> In this work, we employ the Bayesian inference framework to solve the problem of estimating the solution and particularly, its derivatives, which satisfy a known differential equation, from the given noisy and scarce observations of the solution data only. To address the key issue of accuracy and robustness of derivative estimation, we use the Gaussian processes to jointly model the solution, the derivatives, and the differential equation. By regarding the linear differential equation as a linear constraint, a Gaussian process regression with constraint method (GPRC) is developed to improve the accuracy of prediction of derivatives. For nonlinear differential equations, we propose a Picard-iteration-like approximation of linearization around the Gaussian process obtained only from data so that our GPRC can be still iteratively applicable. Besides, a product of experts method is applied to ensure the initial or boundary condition is considered to further enhance the prediction accuracy of the derivatives. We present several numerical results to illustrate the advantages of our new method in comparison to the standard data-driven Gaussian process regression.

</details>

<details>

<summary>2020-10-08 06:32:00 - Bayesian Singular Value Regularization via a Cumulative Shrinkage Process</summary>

- *Masahiro Tanaka*

- `2006.06220v4` - [abs](http://arxiv.org/abs/2006.06220v4) - [pdf](http://arxiv.org/pdf/2006.06220v4)

> This study proposes a novel hierarchical prior for inferring possibly low-rank matrices measured with noise. We consider three-component matrix factorization, as in singular value decomposition, and its fully Bayesian inference. The proposed prior is specified by a scale mixture of exponential distributions that has spike and slab components. The weights for the spike/slab parts are inferred using a special prior based on a cumulative shrinkage process. The proposed prior is designed to increasingly aggressively push less important, or essentially redundant, singular values toward zero, leading to more accurate estimates of low-rank matrices. To ensure the parameter identification, we simulate posterior draws from an approximated posterior, in which the constraints are slightly relaxed, using a No-U-Turn sampler. By means of a set of simulation studies, we show that our proposal is competitive with alternative prior specifications and that it does not incur significant additional computational burden. We apply the proposed approach to sectoral industrial production in the United States to analyze the structural change during the Great Moderation period.

</details>

<details>

<summary>2020-10-08 07:59:00 - No increase in COVID-19 mortality after the 2020 primary elections in the USA</summary>

- *Eric M. Feltham, Laura Forastiere, Marcus Alexander, Nicholas A. Christakis*

- `2010.02896v2` - [abs](http://arxiv.org/abs/2010.02896v2) - [pdf](http://arxiv.org/pdf/2010.02896v2)

> We examined the impact of voting on the spread of COVID-19 after the US primary elections held from March 17 to July 11, 2020 (1574 counties across 34 states). We estimated the average effect of treatment on the treated (ATT) using a non-parametric, generalized difference-in-difference estimator with a matching procedure for panel data. Separately, we estimated the time-varying reproduction number $R_t$ using a semi-mechanistic Bayesian hierarchical model at the state level. We found no evidence of a spike in COVID-19 deaths in the period immediately following the primaries. It is possible that elections can be held safely, without necessarily contributing to spreading the epidemic. Appropriate precautionary measures that enforce physical distancing and mask-wearing are likely needed during general elections, with larger turnout or colder weather.

</details>

<details>

<summary>2020-10-08 11:36:45 - Power-Expected-Posterior Priors as Mixtures of g-Priors</summary>

- *Dimitris Fouskakis, Ioannis Ntzoufras*

- `2002.05782v2` - [abs](http://arxiv.org/abs/2002.05782v2) - [pdf](http://arxiv.org/pdf/2002.05782v2)

> One of the main approaches used to construct prior distributions for objective Bayes methods is the concept of random imaginary observations. Under this setup, the expected-posterior prior (EPP) offers several advantages, among which it has a nice and simple interpretation and provides an effective way to establish compatibility of priors among models. In this paper, we study the power-expected posterior prior as a generalization to the EPP in objective Bayesian model selection under normal linear models. We prove that it can be represented as a mixture of $g$-prior, like a wide range of prior distributions under normal linear models, and thus posterior distributions and Bayes factors are derived in closed form, keeping therefore computational tractability. Comparisons with other mixtures of $g$-prior are made and emphasis is given in the posterior distribution of g and its effect on Bayesian model selection and model averaging.

</details>

<details>

<summary>2020-10-08 18:09:09 - MatDRAM: A pure-MATLAB Delayed-Rejection Adaptive Metropolis-Hastings Markov Chain Monte Carlo Sampler</summary>

- *Shashank Kumbhare, Amir Shahmoradi*

- `2010.04190v1` - [abs](http://arxiv.org/abs/2010.04190v1) - [pdf](http://arxiv.org/pdf/2010.04190v1)

> Markov Chain Monte Carlo (MCMC) algorithms are widely used for stochastic optimization, sampling, and integration of mathematical objective functions, in particular, in the context of Bayesian inverse problems and parameter estimation. For decades, the algorithm of choice in MCMC simulations has been the Metropolis-Hastings (MH) algorithm. An advancement over the traditional MH-MCMC sampler is the Delayed-Rejection Adaptive Metropolis (DRAM). In this paper, we present MatDRAM, a stochastic optimization, sampling, and Monte Carlo integration toolbox in MATLAB which implements a variant of the DRAM algorithm for exploring the mathematical objective functions of arbitrary-dimensions, in particular, the posterior distributions of Bayesian models in data science, Machine Learning, and scientific inference. The design goals of MatDRAM include nearly-full automation of MCMC simulations, user-friendliness, fully-deterministic reproducibility, and the restart functionality of simulations. We also discuss the implementation details of a technique to automatically monitor and ensure the diminishing adaptation of the proposal distribution of the DRAM algorithm and a method of efficiently storing the resulting simulated Markov chains. The MatDRAM library is open-source, MIT-licensed, and permanently located and maintained as part of the ParaMonte library at https://github.com/cdslaborg/paramonte.

</details>

<details>

<summary>2020-10-08 23:35:57 - Bayesian biclustering for microbial metagenomic sequencing data via multinomial matrix factorization</summary>

- *Fangting Zhou, Kejun He, Qiwei Li, Robert S. Chapkin, Yang Ni*

- `2005.08361v2` - [abs](http://arxiv.org/abs/2005.08361v2) - [pdf](http://arxiv.org/pdf/2005.08361v2)

> High-throughput sequencing technology provides unprecedented opportunities to quantitatively explore human gut microbiome and its relation to diseases. Microbiome data are compositional, sparse, noisy, and heterogeneous, which pose serious challenges for statistical modeling. We propose an identifiable Bayesian multinomial matrix factorization model to infer overlapping clusters on both microbes and hosts. The proposed method represents the observed over-dispersed zero-inflated count matrix as Dirichlet-multinomial mixtures on which latent cluster structures are built hierarchically. Under the Bayesian framework, the number of clusters is automatically determined and available information from a taxonomic rank tree of microbes is naturally incorporated, which greatly improves the interpretability of our findings. We demonstrate the utility of the proposed approach by comparing to alternative methods in simulations. An application to a human gut microbiome dataset involving patients with inflammatory bowel disease reveals interesting clusters, which contain bacteria families Bacteroidaceae, Bifidobacteriaceae, Enterobacteriaceae, Fusobacteriaceae, Lachnospiraceae, Ruminococcaceae, Pasteurellaceae, and Porphyromonadaceae that are known to be related to the inflammatory bowel disease and its subtypes according to biological literature. Our findings can help generate potential hypotheses for future investigation of the heterogeneity of the human gut microbiome.

</details>

<details>

<summary>2020-10-09 10:19:20 - Causal inference for semi-competing risks data</summary>

- *Daniel Nevo, Malka Gorfine*

- `2010.04485v1` - [abs](http://arxiv.org/abs/2010.04485v1) - [pdf](http://arxiv.org/pdf/2010.04485v1)

> An emerging challenge for time-to-event data is studying semi-competing risks, namely when two event times are of interest: a non-terminal event time (e.g. age at disease diagnosis), and a terminal event time (e.g. age at death). The non-terminal event is observed only if it precedes the terminal event, which may occur before or after the non-terminal event. Studying treatment or intervention effects on the dual event times is complicated because for some units, the non-terminal event may occur under one treatment value but not under the other. Until recently, existing approaches (e.g., the survivor average causal effect) generally disregarded the time-to-event nature of both outcomes. More recent research focused on principal strata effects within time-varying populations under Bayesian approaches. In this paper, we propose alternative non time-varying estimands, based on a single stratification of the population. We present a novel assumption utilizing the time-to-event nature of the data, which is weaker than the often-invoked monotonicity assumption. We derive results on partial identifiability, suggest a sensitivity analysis approach, and give conditions under which full identification is possible. Finally, we present non-parametric and semi-parametric estimation methods for right-censored data.

</details>

<details>

<summary>2020-10-09 16:23:47 - Asymmetric prior in wavelet shrinkage</summary>

- *Alex Rodrigo dos Santos Sousa*

- `2010.04666v1` - [abs](http://arxiv.org/abs/2010.04666v1) - [pdf](http://arxiv.org/pdf/2010.04666v1)

> In bayesian wavelet shrinkage, the already proposed priors to wavelet coefficients are assumed to be symmetric around zero. Although this assumption is reasonable in many applications, it is not general. The present paper proposes the use of an asymmetric shrinkage rule based on the discrete mixture of a point mass function at zero and an asymmetric beta distribution as prior to the wavelet coefficients in a non-parametric regression model. Statistical properties such as bias, variance, classical and bayesian risks of the associated asymmetric rule are provided and performances of the proposed rule are obtained in simulation studies involving artificial asymmetric distributed coefficients and the Donoho-Johnstone test functions. Application in a seismic real dataset is also analyzed.

</details>

<details>

<summary>2020-10-09 16:40:03 - Product risk assessment: a Bayesian network approach</summary>

- *Joshua Hunte, Martin Neil, Norman Fenton*

- `2010.06698v1` - [abs](http://arxiv.org/abs/2010.06698v1) - [pdf](http://arxiv.org/pdf/2010.06698v1)

> Product risk assessment is the overall process of determining whether a product, which could be anything from a type of washing machine to a type of teddy bear, is judged safe for consumers to use. There are several methods used for product risk assessment, including RAPEX, which is the primary method used by regulators in the UK and EU. However, despite its widespread use, we identify several limitations of RAPEX including a limited approach to handling uncertainty and the inability to incorporate causal explanations for using and interpreting test data. In contrast, Bayesian Networks (BNs) are a rigorous, normative method for modelling uncertainty and causality which are already used for risk assessment in domains such as medicine and finance, as well as critical systems generally. This article proposes a BN model that provides an improved systematic method for product risk assessment that resolves the identified limitations with RAPEX. We use our proposed method to demonstrate risk assessments for a teddy bear and a new uncertified kettle for which there is no testing data and the number of product instances is unknown. We show that, while we can replicate the results of the RAPEX method, the BN approach is more powerful and flexible.

</details>

<details>

<summary>2020-10-09 19:33:06 - Bayesian Poisson Log-normal Model with Regularized Time Structure for Mortality Projection of Multi-population</summary>

- *Zhen Liu, Xiaoqian Sun, Leping Liu, Yu-Bo Wang*

- `2010.04775v1` - [abs](http://arxiv.org/abs/2010.04775v1) - [pdf](http://arxiv.org/pdf/2010.04775v1)

> The improvement of mortality projection is a pivotal topic in the diverse branches related to insurance, demography, and public policy. Motivated by the thread of Lee-Carter related models, we propose a Bayesian model to estimate and predict mortality rates for multi-population. This new model features in information borrowing among populations and properly reflecting variations of data. It also provides a solution to a long-time overlooked problem: model selection for dependence structures of population-specific time parameters. By introducing a Dirac spike function, simultaneous model selection and estimation for population-specific time effects can be achieved without much extra computation cost. We use the Japanese mortality data from Human Mortality Database to illustrate the desirable properties of our model.

</details>

<details>

<summary>2020-10-10 16:39:37 - Fast, Optimal, and Targeted Predictions using Parametrized Decision Analysis</summary>

- *Daniel R. Kowal*

- `2006.13107v2` - [abs](http://arxiv.org/abs/2006.13107v2) - [pdf](http://arxiv.org/pdf/2006.13107v2)

> Prediction is critical for decision-making under uncertainty and lends validity to statistical inference. With targeted prediction, the goal is to optimize predictions for specific decision tasks of interest, which we represent via functionals. Although classical decision analysis extracts predictions from a Bayesian model, these predictions are often difficult to interpret and slow to compute. Instead, we design a class of parametrized actions for Bayesian decision analysis that produce optimal, scalable, and simple targeted predictions. For a wide variety of action parametrizations and loss functions--including linear actions with sparsity constraints for targeted variable selection--we derive a convenient representation of the optimal targeted prediction that yields efficient and interpretable solutions. Customized out-of-sample predictive metrics are developed to evaluate and compare among targeted predictors. Through careful use of the posterior predictive distribution, we introduce a procedure that identifies a set of near-optimal, or acceptable targeted predictors, which provide unique insights into the features and level of complexity needed for accurate targeted prediction. Simulations demonstrate excellent prediction, estimation, and variable selection capabilities. Targeted predictions are constructed for physical activity data from the National Health and Nutrition Examination Survey (NHANES) to better predict and understand the characteristics of intraday physical activity.

</details>

<details>

<summary>2020-10-11 12:09:28 - Stochastic Frontier Analysis with Generalized Errors: inference, model comparison and averaging</summary>

- *Kamil Makieła, Błażej Mazur*

- `2003.07150v2` - [abs](http://arxiv.org/abs/2003.07150v2) - [pdf](http://arxiv.org/pdf/2003.07150v2)

> Contribution of this paper lies in the formulation and estimation of a generalized model for stochastic frontier analysis (SFA) that nests virtually all forms used and includes some that have not been considered so far. The model is based on the generalized t distribution for the observation error and the generalized beta distribution of the second kind for the inefficiency-related term. We use this general error structure framework for formal testing, to compare alternative specifications and to conduct model averaging. This allows us to deal with model specification uncertainty, which is one of the main unresolved issues in SFA, and to relax a number of potentially restrictive assumptions embedded within existing SF models. We also develop Bayesian inference methods that are less restrictive compared to the ones used so far and demonstrate feasible approximate alternatives based on maximum likelihood.

</details>

<details>

<summary>2020-10-11 14:28:21 - Should the Ransomware be Paid?</summary>

- *Rui Fang, Maochao Xu, Peng Zhao*

- `2010.06700v1` - [abs](http://arxiv.org/abs/2010.06700v1) - [pdf](http://arxiv.org/pdf/2010.06700v1)

> Ransomware has emerged as one of the most concerned cyber risks in recent years, which has caused millions of dollars monetary loss over the world. It typically demands a certain amount of ransom payment within a limited timeframe to decrypt the encrypted victim's files. This paper explores whether the ransomware should be paid in a novel game-theoretic model from the perspective of Bayesian game. In particular, the new model analyzes the ransom payment strategies within the framework of incomplete information for both hacker and victim. Our results show that there exist pure and randomized Bayesian Nash equilibria under some mild conditions for the hacker and victim. The sufficient conditions that when the ransom should be paid are presented when an organization is compromised by the ransomware attack. We further study how the costs and probabilities of cracking or recovering affect the expected payoffs of the hacker and the victim in the equilibria. In particular, it is found that the backup option for computer files is not always beneficial, which actually depends on the related cost. Moreover, it is discovered that fake ransomware may be more than expected because of the potential high payoffs. Numerical examples are also presented for illustration.

</details>

<details>

<summary>2020-10-11 15:04:34 - Real-time parameter inference in reduced-order flame models with heteroscedastic Bayesian neural network ensembles</summary>

- *Ushnish Sengupta, Maximilian L. Croci, Matthew P. Juniper*

- `2011.02838v1` - [abs](http://arxiv.org/abs/2011.02838v1) - [pdf](http://arxiv.org/pdf/2011.02838v1)

> The estimation of model parameters with uncertainties from observed data is a ubiquitous inverse problem in science and engineering. In this paper, we suggest an inexpensive and easy to implement parameter estimation technique that uses a heteroscedastic Bayesian Neural Network trained using anchored ensembling. The heteroscedastic aleatoric error of the network models the irreducible uncertainty due to parameter degeneracies in our inverse problem, while the epistemic uncertainty of the Bayesian model captures uncertainties which may arise from an input observation's out-of-distribution nature. We use this tool to perform real-time parameter inference in a 6 parameter G-equation model of a ducted, premixed flame from observations of acoustically excited flames. We train our networks on a library of 2.1 million simulated flame videos. Results on the test dataset of simulated flames show that the network recovers flame model parameters, with the correlation coefficient between predicted and true parameters ranging from 0.97 to 0.99, and well-calibrated uncertainty estimates. The trained neural networks are then used to infer model parameters from real videos of a premixed Bunsen flame captured using a high-speed camera in our lab. Re-simulation using inferred parameters shows excellent agreement between the real and simulated flames. Compared to Ensemble Kalman Filter-based tools that have been proposed for this problem in the combustion literature, our neural network ensemble achieves better data-efficiency and our sub-millisecond inference times represent a savings on computational costs by several orders of magnitude. This allows us to calibrate our reduced-order flame model in real-time and predict the thermoacoustic instability behaviour of the flame more accurately.

</details>

<details>

<summary>2020-10-12 16:05:18 - Multifidelity Approximate Bayesian Computation with Sequential Monte Carlo Parameter Sampling</summary>

- *Thomas P. Prescott, Ruth E. Baker*

- `2001.06256v2` - [abs](http://arxiv.org/abs/2001.06256v2) - [pdf](http://arxiv.org/pdf/2001.06256v2)

> Multifidelity approximate Bayesian computation (MF-ABC) is a likelihood-free technique for parameter inference that exploits model approximations to significantly increase the speed of ABC algorithms (Prescott and Baker, 2020). Previous work has considered MF-ABC only in the context of rejection sampling, which does not explore parameter space particularly efficiently. In this work, we integrate the multifidelity approach with the ABC sequential Monte Carlo (ABC-SMC) algorithm into a new MF-ABC-SMC algorithm. We show that the improvements generated by each of ABC-SMC and MF-ABC to the efficiency of generating Monte Carlo samples and estimates from the ABC posterior are amplified when the two techniques are used together.

</details>

<details>

<summary>2020-10-12 19:38:59 - Bayesian semi-parametric G-computation for causal inference in a cohort study with MNAR dropout and death</summary>

- *Maria Josefsson, Michael J. Daniels*

- `1902.10787v4` - [abs](http://arxiv.org/abs/1902.10787v4) - [pdf](http://arxiv.org/pdf/1902.10787v4)

> Causal inference with observational longitudinal data and time-varying exposures is often complicated by time-dependent confounding and attrition. The G-computation formula is one approach for estimating a causal effect in this setting. The parametric modeling approach typically used in practice relies on strong modeling assumptions for valid inference, and moreover depends on an assumption of missing at random, which is not appropriate when the missingness is missing not at random (MNAR) or due to death. In this work we develop a flexible Bayesian semi-parametric G-computation approach for assessing the causal effect on the subpopulation that would survive irrespective of exposure, in a setting with MNAR dropout. The approach is to specify models for the observed data using Bayesian additive regression trees, and then use assumptions with embedded sensitivity parameters to identify and estimate the causal effect. The proposed approach is motivated by a longitudinal cohort study on cognition, health, and aging, and we apply our approach to study the effect of becoming a widow on memory. We also compare our approach to several standard methods.

</details>

<details>

<summary>2020-10-12 22:54:59 - Bayesian Weighted Triplet and Quartet Methods for Species Tree Inference</summary>

- *Andrew Richards, Laura Kubatko*

- `2010.06063v1` - [abs](http://arxiv.org/abs/2010.06063v1) - [pdf](http://arxiv.org/pdf/2010.06063v1)

> Inference of the evolutionary histories of species, commonly represented by a species tree, is complicated by the divergent evolutionary history of different parts of the genome. Different loci on the genome can have different histories from the underlying species tree (and each other) due to processes such as incomplete lineage sorting (ILS), gene duplication and loss, and horizontal gene transfer. The multispecies coalescent is a commonly used model for performing inference on species and gene trees in the presence of ILS. This paper introduces Lily-T and Lily-Q, two new methods for species tree inference under the multispecies coalescent. We then compare them to two frequently used methods, SVDQuartets and ASTRAL, using simulated and empirical data. Both methods generally showed improvement over SVDQuartets, and Lily-Q was superior to Lily-T for most simulation settings. The comparison to ASTRAL was more mixed - Lily-Q tended to be better than ASTRAL when the length of recombination-free loci was short, when the coalescent population parameter {\theta} was small, or when the internal branch lengths were longer.

</details>

<details>

<summary>2020-10-13 01:19:06 - Quasi-maximum Likelihood Inference for Linear Double Autoregressive Models</summary>

- *Hua Liu, Songhua Tan, Qianqian Zhu*

- `2010.06103v1` - [abs](http://arxiv.org/abs/2010.06103v1) - [pdf](http://arxiv.org/pdf/2010.06103v1)

> This paper investigates the quasi-maximum likelihood inference including estimation, model selection and diagnostic checking for linear double autoregressive (DAR) models, where all asymptotic properties are established under only fractional moment of the observed process. We propose a Gaussian quasi-maximum likelihood estimator (G-QMLE) and an exponential quasi-maximum likelihood estimator (E-QMLE) for the linear DAR model, and establish the consistency and asymptotic normality for both estimators. Based on the G-QMLE and E-QMLE, two Bayesian information criteria are proposed for model selection, and two mixed portmanteau tests are constructed to check the adequacy of fitted models. Moreover, we compare the proposed G-QMLE and E-QMLE with the existing doubly weighted quantile regression estimator in terms of the asymptotic efficiency and numerical performance. Simulation studies illustrate the finite-sample performance of the proposed inference tools, and a real example on the Bitcoin return series shows the usefulness of the proposed inference tools.

</details>

<details>

<summary>2020-10-13 01:47:06 - Bayesian inference under small sample size -- A noninformative prior approach</summary>

- *Jingjing He, Xuefei Guan*

- `2010.06110v1` - [abs](http://arxiv.org/abs/2010.06110v1) - [pdf](http://arxiv.org/pdf/2010.06110v1)

> A Bayesian inference method for problems with small samples and sparse data is presented in this paper. A general type of prior ($\propto 1/\sigma^{q}$) is proposed to formulate the Bayesian posterior for inference problems under small sample size. It is shown that this type of prior can represents a broad range of priors such as classical noninformative priors and asymptotically locally invariant priors. It is further shown in this study that such priors can be derived as the limiting states of Normal-Inverse-Gamma conjugate priors, allowing for analytical evaluations of Bayesian posteriors and predictors. The performance of different noninformative priors under small sample size is compared using the global likelihood. The method of Laplace approximation is employed to evaluate the global likelihood. A numerical linear regression problem and a realistic fatigue reliability problem are used to demonstrate the method and identify the optimal noninformative prior. Results indicate the predictor using Jeffreys' prior outperforms others. The advantage of the noninformative Bayesian estimator over the regular least square estimator under small sample size is shown.

</details>

<details>

<summary>2020-10-13 09:45:42 - Optimization of Genomic Classifiers for Clinical Deployment: Evaluation of Bayesian Optimization to Select Predictive Models of Acute Infection and In-Hospital Mortality</summary>

- *Michael B. Mayhew, Elizabeth Tran, Kirindi Choi, Uros Midic, Roland Luethy, Nandita Damaraju, Ljubomir Buturovic*

- `2003.12310v3` - [abs](http://arxiv.org/abs/2003.12310v3) - [pdf](http://arxiv.org/pdf/2003.12310v3)

> Acute infection, if not rapidly and accurately detected, can lead to sepsis, organ failure and even death. Current detection of acute infection as well as assessment of a patient's severity of illness are imperfect. Characterization of a patient's immune response by quantifying expression levels of specific genes from blood represents a potentially more timely and precise means of accomplishing both tasks. Machine learning methods provide a platform to leverage this 'host response' for development of deployment-ready classification models. Prioritization of promising classifiers is dependent, in part, on hyperparameter optimization for which a number of approaches including grid search, random sampling and Bayesian optimization have been shown to be effective. We compare HO approaches for the development of diagnostic classifiers of acute infection and in-hospital mortality from gene expression of 29 diagnostic markers. We take a deployment-centered approach to our comprehensive analysis, accounting for heterogeneity in our multi-study patient cohort with our choices of dataset partitioning and hyperparameter optimization objective as well as assessing selected classifiers in external (as well as internal) validation. We find that classifiers selected by Bayesian optimization for in-hospital mortality can outperform those selected by grid search or random sampling. However, in contrast to previous research: 1) Bayesian optimization is not more efficient in selecting classifiers in all instances compared to grid search or random sampling-based methods and 2) we note marginal gains in classifier performance in only specific circumstances when using a common variant of Bayesian optimization (i.e. automatic relevance determination). Our analysis highlights the need for further practical, deployment-centered benchmarking of HO approaches in the healthcare context.

</details>

<details>

<summary>2020-10-13 10:07:51 - Probabilistic simulation of partial differential equations</summary>

- *Philipp Frank, Torsten A. Enßlin*

- `2010.06583v1` - [abs](http://arxiv.org/abs/2010.06583v1) - [pdf](http://arxiv.org/pdf/2010.06583v1)

> Computer simulations of differential equations require a time discretization, which inhibits to identify the exact solution with certainty. Probabilistic simulations take this into account via uncertainty quantification. The construction of a probabilistic simulation scheme can be regarded as Bayesian filtering by means of probabilistic numerics. Gaussian prior based filters, specifically Gauss-Markov priors, have successfully been applied to simulation of ordinary differential equations (ODEs) and give rise to filtering problems that can be solved efficiently. This work extends this approach to partial differential equations (PDEs) subject to periodic boundary conditions and utilizes continuous Gaussian processes in space and time to arrive at a Bayesian filtering problem structurally similar to the ODE setting. The usage of a process that is Markov in time and statistically homogeneous in space leads to a probabilistic spectral simulation method that allows for an efficient realization. Furthermore, the Bayesian perspective allows the incorporation of methods developed within the context of information field theory such as the estimation of the power spectrum associated with the prior distribution, to be jointly estimated along with the solution of the PDE.

</details>

<details>

<summary>2020-10-13 12:54:29 - Joint Perception and Control as Inference with an Object-based Implementation</summary>

- *Minne Li, Zheng Tian, Pranav Nashikkar, Ian Davies, Ying Wen, Jun Wang*

- `1903.01385v3` - [abs](http://arxiv.org/abs/1903.01385v3) - [pdf](http://arxiv.org/pdf/1903.01385v3)

> Existing model-based reinforcement learning methods often study perception modeling and decision making separately. We introduce joint Perception and Control as Inference (PCI), a general framework to combine perception and control for partially observable environments through Bayesian inference. Based on the fact that object-level inductive biases are critical in human perceptual learning and reasoning, we propose Object-based Perception Control (OPC), an instantiation of PCI which manages to facilitate control using automatic discovered object-based representations. We develop an unsupervised end-to-end solution and analyze the convergence of the perception model update. Experiments in a high-dimensional pixel environment demonstrate the learning effectiveness of our object-based perception control approach. Specifically, we show that OPC achieves good perceptual grouping quality and outperforms several strong baselines in accumulated rewards.

</details>

<details>

<summary>2020-10-13 13:02:27 - Bayesian model selection for unsupervised image deconvolution with structured Gaussian priors</summary>

- *Benjamin Harroué, Jean-François Giovannelli, Marcelo Pereyra*

- `2010.06346v1` - [abs](http://arxiv.org/abs/2010.06346v1) - [pdf](http://arxiv.org/pdf/2010.06346v1)

> This paper considers the objective comparison of stochastic models to solve inverse problems, more specifically image restoration. Most often, model comparison is addressed in a supervised manner, that can be time-consuming and partly arbitrary. Here we adopt an unsupervised Bayesian approach and objectively compare the models based on their posterior probabilities, directly from the data without ground truth available. The probabilities depend on the marginal likelihood or "evidence" of the models and we resort to the Chib approach including a Gibbs sampler. We focus on the family of Gaussian models with circulant covariances and unknown hyperparameters, and compare different types of covariance matrices for the image and noise.

</details>

<details>

<summary>2020-10-13 13:04:11 - Swipe dynamics as a means of authentication: results from a Bayesian unsupervised approach</summary>

- *Parker Lamb, Alexander Millar, Ramon Fuentes*

- `2008.01013v2` - [abs](http://arxiv.org/abs/2008.01013v2) - [pdf](http://arxiv.org/pdf/2008.01013v2)

> The field of behavioural biometrics stands as an appealing alternative to more traditional biometric systems due to the ease of use from a user perspective and potential robustness to presentation attacks. This paper focuses its attention to a specific type of behavioural biometric utilising swipe dynamics, also referred to as touch gestures. In touch gesture authentication, a user swipes across the touchscreen of a mobile device to perform an authentication attempt. A key characteristic of touch gesture authentication and new behavioural biometrics in general is the lack of available data to train and validate models. From a machine learning perspective, this presents the classic curse of dimensionality problem and the methodology presented here focuses on Bayesian unsupervised models as they are well suited to such conditions. This paper presents results from a set of experiments consisting of 38 sessions with labelled victim as well as blind and over-the-shoulder presentation attacks. Three models are compared using this dataset; two single-mode models: a shrunk covariance estimate and a Bayesian Gaussian distribution, as well as a Bayesian non-parametric infinite mixture of Gaussians, modelled as a Dirichlet Process. Equal error rates (EER) for the three models are compared and attention is paid to how these vary across the two single-mode models at differing numbers of enrolment samples.

</details>

<details>

<summary>2020-10-13 14:13:35 - Batch-Incremental Triplet Sampling for Training Triplet Networks Using Bayesian Updating Theorem</summary>

- *Milad Sikaroudi, Benyamin Ghojogh, Fakhri Karray, Mark Crowley, H. R. Tizhoosh*

- `2007.05610v2` - [abs](http://arxiv.org/abs/2007.05610v2) - [pdf](http://arxiv.org/pdf/2007.05610v2)

> Variants of Triplet networks are robust entities for learning a discriminative embedding subspace. There exist different triplet mining approaches for selecting the most suitable training triplets. Some of these mining methods rely on the extreme distances between instances, and some others make use of sampling. However, sampling from stochastic distributions of data rather than sampling merely from the existing embedding instances can provide more discriminative information. In this work, we sample triplets from distributions of data rather than from existing instances. We consider a multivariate normal distribution for the embedding of each class. Using Bayesian updating and conjugate priors, we update the distributions of classes dynamically by receiving the new mini-batches of training data. The proposed triplet mining with Bayesian updating can be used with any triplet-based loss function, e.g., triplet-loss or Neighborhood Component Analysis (NCA) loss. Accordingly, Our triplet mining approaches are called Bayesian Updating Triplet (BUT) and Bayesian Updating NCA (BUNCA), depending on which loss function is being used. Experimental results on two public datasets, namely MNIST and histopathology colorectal cancer (CRC), substantiate the effectiveness of the proposed triplet mining method.

</details>

<details>

<summary>2020-10-13 14:51:41 - Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels</summary>

- *Massimiliano Patacchiola, Jack Turner, Elliot J. Crowley, Michael O'Boyle, Amos Storkey*

- `1910.05199v4` - [abs](http://arxiv.org/abs/1910.05199v4) - [pdf](http://arxiv.org/pdf/1910.05199v4)

> Recently, different machine learning methods have been introduced to tackle the challenging few-shot learning scenario that is, learning from a small labeled dataset related to a specific task. Common approaches have taken the form of meta-learning: learning to learn on the new problem given the old. Following the recognition that meta-learning is implementing learning in a multi-level model, we present a Bayesian treatment for the meta-learning inner loop through the use of deep kernels. As a result we can learn a kernel that transfers to new tasks; we call this Deep Kernel Transfer (DKT). This approach has many advantages: is straightforward to implement as a single optimizer, provides uncertainty quantification, and does not require estimation of task-specific parameters. We empirically demonstrate that DKT outperforms several state-of-the-art algorithms in few-shot classification, and is the state of the art for cross-domain adaptation and regression. We conclude that complex meta-learning routines can be replaced by a simpler Bayesian model without loss of accuracy.

</details>

<details>

<summary>2020-10-13 21:41:49 - Transportation Interventions Reshaping NYC Commute: the Probabilistic Simulation Framework Assessing the Impacts of Ridesharing and Manhattan Congestion Surcharge</summary>

- *Devashish Khulbe, Chaogui Kang, Stanislav Sobolevsky*

- `2010.06588v1` - [abs](http://arxiv.org/abs/2010.06588v1) - [pdf](http://arxiv.org/pdf/2010.06588v1)

> Understanding holistic impact of planned transportation solutions and interventions on urban systems is challenged by their complexity but critical for decision making. The cornerstone for such impact assessments is estimating the transportation mode-shift resulting from the intervention. And while transportation planning has well-established models for the mode-choice assessment such as the nested multinomial logit model, an individual choice simulation could be better suited for addressing the mode-shift allowing to consistently account for individual preferences. In addition, no model perfectly represents the reality while the available ground truth data on the actual transportation choices needed to infer the model is often incomplete or inconsistent. The present paper addresses those challenges by offering an individual mode-choice and mode-shift simulation model and the Bayesian inference framework. It accounts for uncertainties in the data as well as the model estimate and translates them into uncertainties of the resulting mode-shift and the impacts. The framework is evaluated on the two intervention cases: introducing ride-sharing for-hire-vehicles in NYC as well the recent introduction of the Manhattan Congestion Surcharge. Being successfully evaluated on the cases above, the framework can be used for assessing mode-shift and resulting economic, social and environmental implications for any future urban transportation solutions and policies being considered by decision-makers or transportation companies.

</details>

<details>

<summary>2020-10-14 01:46:56 - Flexible mean field variational inference using mixtures of non-overlapping exponential families</summary>

- *Jeffrey P. Spence*

- `2010.06768v1` - [abs](http://arxiv.org/abs/2010.06768v1) - [pdf](http://arxiv.org/pdf/2010.06768v1)

> Sparse models are desirable for many applications across diverse domains as they can perform automatic variable selection, aid interpretability, and provide regularization. When fitting sparse models in a Bayesian framework, however, analytically obtaining a posterior distribution over the parameters of interest is intractable for all but the simplest cases. As a result practitioners must rely on either sampling algorithms such as Markov chain Monte Carlo or variational methods to obtain an approximate posterior. Mean field variational inference is a particularly simple and popular framework that is often amenable to analytically deriving closed-form parameter updates. When all distributions in the model are members of exponential families and are conditionally conjugate, optimization schemes can often be derived by hand. Yet, I show that using standard mean field variational inference can fail to produce sensible results for models with sparsity-inducing priors, such as the spike-and-slab. Fortunately, such pathological behavior can be remedied as I show that mixtures of exponential family distributions with non-overlapping support form an exponential family. In particular, any mixture of a diffuse exponential family and a point mass at zero to model sparsity forms an exponential family. Furthermore, specific choices of these distributions maintain conditional conjugacy. I use two applications to motivate these results: one from statistical genetics that has connections to generalized least squares with a spike-and-slab prior on the regression coefficients; and sparse probabilistic principal component analysis. The theoretical results presented here are broadly applicable beyond these two examples.

</details>

<details>

<summary>2020-10-14 01:58:34 - Scaling Hamiltonian Monte Carlo Inference for Bayesian Neural Networks with Symmetric Splitting</summary>

- *Adam D. Cobb, Brian Jalaian*

- `2010.06772v1` - [abs](http://arxiv.org/abs/2010.06772v1) - [pdf](http://arxiv.org/pdf/2010.06772v1)

> Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) approach that exhibits favourable exploration properties in high-dimensional models such as neural networks. Unfortunately, HMC has limited use in large-data regimes and little work has explored suitable approaches that aim to preserve the entire Hamiltonian. In our work, we introduce a new symmetric integration scheme for split HMC that does not rely on stochastic gradients. We show that our new formulation is more efficient than previous approaches and is easy to implement with a single GPU. As a result, we are able to perform full HMC over common deep learning architectures using entire data sets. In addition, when we compare with stochastic gradient MCMC, we show that our method achieves better performance in both accuracy and uncertainty quantification. Our approach demonstrates HMC as a feasible option when considering inference schemes for large-scale machine learning problems.

</details>

<details>

<summary>2020-10-14 05:52:42 - Wasserstein-based methods for convergence complexity analysis of MCMC with applications</summary>

- *Qian Qin, James P. Hobert*

- `1810.08826v3` - [abs](http://arxiv.org/abs/1810.08826v3) - [pdf](http://arxiv.org/pdf/1810.08826v3)

> Over the last 25 years, techniques based on drift and minorization (d&m) have been mainstays in the convergence analysis of MCMC algorithms. However, results presented herein suggest that d&m may be less useful in the emerging area of convergence complexity analysis, which is the study of how the convergence behavior of Monte Carlo Markov chains scale with sample size, $n$, and/or number of covariates, $p$. The problem appears to be that minorization can become a serious liability as dimension increases. Alternative methods for constructing convergence rate bounds (with respect to total variation distance) that do not require minorization are investigated. Based on Wasserstein distances and random mappings, these methods can produce bounds that are substantially more robust to increasing dimension than those based on d&m. The Wasserstein-based bounds are used to develop strong convergence complexity results for MCMC algorithms used in Bayesian probit regression and random effects models in the challenging asymptotic regime where $n$ and $p$ are both large.

</details>

<details>

<summary>2020-10-14 12:14:38 - Projection Predictive Inference for Generalized Linear and Additive Multilevel Models</summary>

- *Alejandro Catalina, Paul-Christian Bürkner, Aki Vehtari*

- `2010.06994v1` - [abs](http://arxiv.org/abs/2010.06994v1) - [pdf](http://arxiv.org/pdf/2010.06994v1)

> Projection predictive inference is a decision theoretic Bayesian approach that decouples model estimation from decision making. Given a reference model previously built including all variables present in the data, projection predictive inference projects its posterior onto a constrained space of a subset of variables. Variable selection is then performed by sequentially adding relevant variables until predictive performance is satisfactory. Previously, projection predictive inference has been demonstrated only for generalized linear models (GLMs) and Gaussian processes (GPs) where it showed superior performance to competing variable selection procedures. In this work, we extend projection predictive inference to support variable and structure selection for generalized linear multilevel models (GLMMs) and generalized additive multilevel models (GAMMs). Our simulative and real-word experiments demonstrate that our method can drastically reduce the model complexity required to reach reference predictive performance and achieve good frequency properties.

</details>

<details>

<summary>2020-10-14 14:57:21 - Theoretical bounds on estimation error for meta-learning</summary>

- *James Lucas, Mengye Ren, Irene Kameni, Toniann Pitassi, Richard Zemel*

- `2010.07140v1` - [abs](http://arxiv.org/abs/2010.07140v1) - [pdf](http://arxiv.org/pdf/2010.07140v1)

> Machine learning models have traditionally been developed under the assumption that the training and test distributions match exactly. However, recent success in few-shot learning and related problems are encouraging signs that these models can be adapted to more realistic settings where train and test distributions differ. Unfortunately, there is severely limited theoretical support for these algorithms and little is known about the difficulty of these problems. In this work, we provide novel information-theoretic lower-bounds on minimax rates of convergence for algorithms that are trained on data from multiple sources and tested on novel data. Our bounds depend intuitively on the information shared between sources of data, and characterize the difficulty of learning in this setting for arbitrary algorithms. We demonstrate these bounds on a hierarchical Bayesian model of meta-learning, computing both upper and lower bounds on parameter estimation via maximum-a-posteriori inference.

</details>

<details>

<summary>2020-10-14 18:29:54 - How Close and How Much? Linking Health Outcomes to Built Environment Spatial Distributions</summary>

- *Adam Peterson, Veronica Berrocal, Emma Sanchez-Vaznaugh, Brisa Sanchez*

- `2010.07348v1` - [abs](http://arxiv.org/abs/2010.07348v1) - [pdf](http://arxiv.org/pdf/2010.07348v1)

> Built environment features (BEFs) refer to aspects of the human constructed environment, which may in turn support or restrict health related behaviors and thus impact health. In this paper we are interested in understanding whether the spatial distribution and quantity of fast food restaurants (FFRs) influence the risk of obesity in schoolchildren. To achieve this goal, we propose a two-stage Bayesian hierarchical modeling framework. In the first stage, examining the position of FFRs relative to that of some reference locations - in our case, schools - we model the distances of FFRs from these reference locations as realizations of Inhomogenous Poisson processes (IPP). With the goal of identifying representative spatial patterns of exposure to FFRs, we model the intensity functions of the IPPs using a Bayesian non-parametric viewpoint and specifying a Nested Dirichlet Process prior. The second stage model relates exposure patterns to obesity, offering two different approaches to accommodate uncertainty in the exposure patterns estimated in the first stage: in the first approach the odds of obesity at the school level is regressed on cluster indicators, each representing a major pattern of exposure to FFRs. In the second, we employ Bayesian Kernel Machine regression to relate the odds of obesity to the multivariate vector reporting the degree of similarity of a given school to all other schools. Our analysis on the influence of patterns of FFR occurrence on obesity among Californian schoolchildren has indicated that, in 2010, among schools that are consistently assigned to a cluster, there is a lower odds of obesity amongst 9th graders who attend schools with most distant FFR occurrences in a 1-mile radius as compared to others.

</details>

<details>

<summary>2020-10-14 18:41:54 - Exploring the Uncertainty Properties of Neural Networks' Implicit Priors in the Infinite-Width Limit</summary>

- *Ben Adlam, Jaehoon Lee, Lechao Xiao, Jeffrey Pennington, Jasper Snoek*

- `2010.07355v1` - [abs](http://arxiv.org/abs/2010.07355v1) - [pdf](http://arxiv.org/pdf/2010.07355v1)

> Modern deep learning models have achieved great success in predictive accuracy for many data modalities. However, their application to many real-world tasks is restricted by poor uncertainty estimates, such as overconfidence on out-of-distribution (OOD) data and ungraceful failing under distributional shift. Previous benchmarks have found that ensembles of neural networks (NNs) are typically the best calibrated models on OOD data. Inspired by this, we leverage recent theoretical advances that characterize the function-space prior of an ensemble of infinitely-wide NNs as a Gaussian process, termed the neural network Gaussian process (NNGP). We use the NNGP with a softmax link function to build a probabilistic model for multi-class classification and marginalize over the latent Gaussian outputs to sample from the posterior. This gives us a better understanding of the implicit prior NNs place on function space and allows a direct comparison of the calibration of the NNGP and its finite-width analogue. We also examine the calibration of previous approaches to classification with the NNGP, which treat classification problems as regression to the one-hot labels. In this case the Bayesian posterior is exact, and we compare several heuristics to generate a categorical distribution over classes. We find these methods are well calibrated under distributional shift. Finally, we consider an infinite-width final layer in conjunction with a pre-trained embedding. This replicates the important practical use case of transfer learning and allows scaling to significantly larger datasets. As well as achieving competitive predictive accuracy, this approach is better calibrated than its finite width analogue.

</details>

<details>

<summary>2020-10-14 22:53:16 - Modelling reporting delays for disease surveillance data</summary>

- *Leonardo Bastos, Theodoros Economou, Marcelo Gomes, Daniel Villela, Flavio Coelho, Oswaldo Cruz, Oliver Stoner, Trevor Bailey, Claudia Codeço*

- `1709.09150v3` - [abs](http://arxiv.org/abs/1709.09150v3) - [pdf](http://arxiv.org/pdf/1709.09150v3)

> One difficulty for real-time tracking of epidemics is related to reporting delay. The reporting delay may be due to laboratory confirmation, logistic problems, infrastructure difficulties and so on. The ability to correct the available information as quickly as possible is crucial, in terms of decision making such as issuing warnings to the public and local authorities. A Bayesian hierarchical modelling approach is proposed as a flexible way of correcting the reporting delays and to quantify the associated uncertainty. Implementation of the model is fast, due to the use of the integrated nested Laplace approximation (INLA). The approach is illustrated on dengue fever incidence data in Rio de Janeiro, and Severe Acute Respiratory Illness (SARI) data in Paran\'a state, Brazil.

</details>

<details>

<summary>2020-10-15 01:30:33 - Detecting conflicting summary statistics in likelihood-free inference</summary>

- *Yinan Mao, Xueou Wang, David J. Nott, Michael Evans*

- `2010.07465v1` - [abs](http://arxiv.org/abs/2010.07465v1) - [pdf](http://arxiv.org/pdf/2010.07465v1)

> Bayesian likelihood-free methods implement Bayesian inference using simulation of data from the model to substitute for intractable likelihood evaluations. Most likelihood-free inference methods replace the full data set with a summary statistic before performing Bayesian inference, and the choice of this statistic is often difficult. The summary statistic should be low-dimensional for computational reasons, while retaining as much information as possible about the parameter. Using a recent idea from the interpretable machine learning literature, we develop some regression-based diagnostic methods which are useful for detecting when different parts of a summary statistic vector contain conflicting information about the model parameters. Conflicts of this kind complicate summary statistic choice, and detecting them can be insightful about model deficiencies and guide model improvement. The diagnostic methods developed are based on regression approaches to likelihood-free inference, in which the regression model estimates the posterior density using summary statistics as features. Deletion and imputation of part of the summary statistic vector within the regression model can remove conflicts and approximate posterior distributions for summary statistic subsets. A larger than expected change in the estimated posterior density following deletion and imputation can indicate a conflict in which inferences of interest are affected. The usefulness of the new methods is demonstrated in a number of real examples.

</details>

<details>

<summary>2020-10-15 07:24:16 - Infinite-dimensional gradient-based descent for alpha-divergence minimisation</summary>

- *Kamélia Daudel, Randal Douc, François Portier*

- `2005.10618v2` - [abs](http://arxiv.org/abs/2005.10618v2) - [pdf](http://arxiv.org/pdf/2005.10618v2)

> This paper introduces the $(\alpha, \Gamma)$-descent, an iterative algorithm which operates on measures and performs $\alpha$-divergence minimisation in a Bayesian framework. This gradient-based procedure extends the commonly-used variational approximation by adding a prior on the variational parameters in the form of a measure. We prove that for a rich family of functions $\Gamma$, this algorithm leads at each step to a systematic decrease in the $\alpha$-divergence and derive convergence results. Our framework recovers the Entropic Mirror Descent algorithm and provides an alternative algorithm that we call the Power Descent. Moreover, in its stochastic formulation, the $(\alpha, \Gamma)$-descent allows to optimise the mixture weights of any given mixture model without any information on the underlying distribution of the variational parameters. This renders our method compatible with many choices of parameters updates and applicable to a wide range of Machine Learning tasks. We demonstrate empirically on both toy and real-world examples the benefit of using the Power descent and going beyond the Entropic Mirror Descent framework, which fails as the dimension grows.

</details>

<details>

<summary>2020-10-15 10:23:17 - Bayesian Dynamic Fused LASSO</summary>

- *Kaoru Irie*

- `1905.12275v2` - [abs](http://arxiv.org/abs/1905.12275v2) - [pdf](http://arxiv.org/pdf/1905.12275v2)

> The new class of Markov processes is proposed to realize the flexible shrinkage effects for the dynamic models. The transition density of the new process consists of two penalty functions, similarly to Bayesian fused LASSO in its functional form, that shrink the current state variable to its previous value and zero. The normalizing constant of the density, which is not ignorable in the posterior computation, is shown to be essentially the log-geometric mixture of double-exponential densities. This process comprises the state equation of the dynamic regression models, which is shown to be conditionally Gaussian and linear in state variables and utilize the forward filtering and backward sampling in posterior computation by Gibbs sampler. The problem of overshrinkage that is inherent in lasso is moderated by considering the hierarchical extension, which can even realize the shrinkage of horseshoe priors marginally. The new prior is compared with the standard double-exponential prior in the estimation of and prediction by the dynamic linear models for illustration. It is also applied to the time-varying vector autoregressive models for the US macroeconomic data, where we examine the (dis)similarity of the additional shrinkage effect to dynamic variable selection or, specifically, the latent threshold models.

</details>

<details>

<summary>2020-10-15 21:09:17 - Fundamental Linear Algebra Problem of Gaussian Inference</summary>

- *Timothy D Barfoot*

- `2010.08022v1` - [abs](http://arxiv.org/abs/2010.08022v1) - [pdf](http://arxiv.org/pdf/2010.08022v1)

> Underlying many Bayesian inference techniques that seek to approximate the posterior as a Gaussian distribution is a fundamental linear algebra problem that must be solved for both the mean and key entries of the covariance. Even when the true posterior is not Gaussian (e.g., in the case of nonlinear measurement functions) we can use variational schemes that repeatedly solve this linear algebra problem at each iteration. In most cases, the question is not whether a solution to this problem exists, but rather how we can exploit problem-specific structure to find it efficiently. Our contribution is to clearly state the Fundamental Linear Algebra Problem of Gaussian Inference (FLAPOGI) and to provide a novel presentation (using Kronecker algebra) of the not-so-well-known result of Takahashi et al. (1973) that makes it possible to solve for key entries of the covariance matrix. We first provide a global solution and then a local version that can be implemented using local message passing amongst a collection of agents calculating in parallel. Contrary to belief propagation, our local scheme is guaranteed to converge in both the mean and desired covariance quantities to the global solution even when the underlying factor graph is loopy; in the case of synchronous updates, we provide a bound on the number of iterations required for convergence. Compared to belief propagation, this guaranteed convergence comes at the cost of additional storage, calculations, and communication links in the case of loops; however, we show how these can be automatically constructed on the fly using only local information.

</details>

<details>

<summary>2020-10-15 23:18:33 - Minimum Uncertainty Based Detection of Adversaries in Deep Neural Networks</summary>

- *Fatemeh Sheikholeslami, Swayambhoo Jain, Georgios B. Giannakis*

- `1904.02841v2` - [abs](http://arxiv.org/abs/1904.02841v2) - [pdf](http://arxiv.org/pdf/1904.02841v2)

> Despite their unprecedented performance in various domains, utilization of Deep Neural Networks (DNNs) in safety-critical environments is severely limited in the presence of even small adversarial perturbations. The present work develops a randomized approach to detecting such perturbations based on minimum uncertainty metrics that rely on sampling at the hidden layers during the DNN inference stage. Inspired by Bayesian approaches to uncertainty estimation, the sampling probabilities are designed for effective detection of the adversarially corrupted inputs. Being modular, the novel detector of adversaries can be conveniently employed by any pre-trained DNN at no extra training overhead. Selecting which units to sample per hidden layer entails quantifying the amount of DNN output uncertainty, where the overall uncertainty is expressed in terms of its layer-wise components - what also promotes scalability. Sampling probabilities are then sought by minimizing uncertainty measures layer-by-layer, leading to a novel convex optimization problem that admits an exact solver with superlinear convergence rate. By simplifying the objective function, low-complexity approximate solvers are also developed. In addition to valuable insights, these approximations link the novel approach with state-of-the-art randomized adversarial detectors. The effectiveness of the novel detectors in the context of competing alternatives is highlighted through extensive tests for various types of adversarial attacks with variable levels of strength.

</details>

<details>

<summary>2020-10-16 00:07:48 - Learning Social Networks from Text Data using Covariate Information</summary>

- *Xiaoyi Yang, Nynke M. D. Niezink, Rebecca Nugent*

- `2010.08076v1` - [abs](http://arxiv.org/abs/2010.08076v1) - [pdf](http://arxiv.org/pdf/2010.08076v1)

> Describing and characterizing the impact of historical figures can be challenging, but unraveling their social structures perhaps even more so. Historical social network analysis methods can help and may also illuminate people who have been overlooked by historians but turn out to be influential social connection points. Text data, such as biographies, can be a useful source of information about the structure of historical social networks but can also introduce challenges in identifying links. The Local Poisson Graphical Lasso model leverages the number of co-mentions in the text to measure relationships between people and uses a conditional independence structure to model a social network. This structure will reduce the tendency to overstate the relationship between "friends of friends", but given the historical high frequency of common names, without additional distinguishing information, we can still introduce incorrect links. In this work, we extend the Local Poisson Graphical Lasso model with a (multiple) penalty structure that incorporates covariates giving increased link probabilities to people with shared covariate information. We propose both greedy and Bayesian approaches to estimate the penalty parameters. We present results on data simulated with characteristics of historical networks and show that this type of penalty structure can improve network recovery as measured by precision and recall. We also illustrate the approach on biographical data of individuals who lived in early modern Britain, targeting the period from 1500 to 1575.

</details>

<details>

<summary>2020-10-16 07:39:40 - Practical Bayesian Optimization with Threshold-Guided Marginal Likelihood Maximization</summary>

- *Jungtaek Kim, Seungjin Choi*

- `1905.07540v2` - [abs](http://arxiv.org/abs/1905.07540v2) - [pdf](http://arxiv.org/pdf/1905.07540v2)

> We propose a practical Bayesian optimization method using Gaussian process regression, of which the marginal likelihood is maximized where the number of model selection steps is guided by a pre-defined threshold. Since Bayesian optimization consumes a large portion of its execution time in finding the optimal free parameters for Gaussian process regression, our simple, but straightforward method is able to mitigate the time complexity and speed up the overall Bayesian optimization procedure. Finally, the experimental results show that our method is effective to reduce the execution time in most of cases, with less loss of optimization quality.

</details>

<details>

<summary>2020-10-16 12:35:49 - Multi-fidelity data fusion for the approximation of scalar functions with low intrinsic dimensionality through active subspaces</summary>

- *Francesco Romor, Marco Tezzele, Gianluigi Rozza*

- `2010.08349v1` - [abs](http://arxiv.org/abs/2010.08349v1) - [pdf](http://arxiv.org/pdf/2010.08349v1)

> Gaussian processes are employed for non-parametric regression in a Bayesian setting. They generalize linear regression, embedding the inputs in a latent manifold inside an infinite-dimensional reproducing kernel Hilbert space. We can augment the inputs with the observations of low-fidelity models in order to learn a more expressive latent manifold and thus increment the model's accuracy. This can be realized recursively with a chain of Gaussian processes with incrementally higher fidelity. We would like to extend these multi-fidelity model realizations to case studies affected by a high-dimensional input space but with low intrinsic dimensionality. In this cases physical supported or purely numerical low-order models are still affected by the curse of dimensionality when queried for responses. When the model's gradient information is provided, the presence of an active subspace can be exploited to design low-fidelity response surfaces and thus enable Gaussian process multi-fidelity regression, without the need to perform new simulations. This is particularly useful in the case of data scarcity. In this work we present a multi-fidelity approach involving active subspaces and we test it on two different high-dimensional benchmarks.

</details>

<details>

<summary>2020-10-16 13:19:03 - Fast Bayesian Estimation of Spatial Count Data Models</summary>

- *Prateek Bansal, Rico Krueger, Daniel J. Graham*

- `2007.03681v2` - [abs](http://arxiv.org/abs/2007.03681v2) - [pdf](http://arxiv.org/pdf/2007.03681v2)

> Spatial count data models are used to explain and predict the frequency of phenomena such as traffic accidents in geographically distinct entities such as census tracts or road segments. These models are typically estimated using Bayesian Markov chain Monte Carlo (MCMC) simulation methods, which, however, are computationally expensive and do not scale well to large datasets. Variational Bayes (VB), a method from machine learning, addresses the shortcomings of MCMC by casting Bayesian estimation as an optimisation problem instead of a simulation problem. Considering all these advantages of VB, a VB method is derived for posterior inference in negative binomial models with unobserved parameter heterogeneity and spatial dependence. P\'olya-Gamma augmentation is used to deal with the non-conjugacy of the negative binomial likelihood and an integrated non-factorised specification of the variational distribution is adopted to capture posterior dependencies. The benefits of the proposed approach are demonstrated in a Monte Carlo study and an empirical application on estimating youth pedestrian injury counts in census tracts of New York City. The VB approach is around 45 to 50 times faster than MCMC on a regular eight-core processor in a simulation and an empirical study, while offering similar estimation and predictive accuracy. Conditional on the availability of computational resources, the embarrassingly parallel architecture of the proposed VB method can be exploited to further accelerate its estimation by up to 20 times.

</details>

<details>

<summary>2020-10-16 14:48:02 - Belief functions induced by random fuzzy sets: A general framework for representing uncertain and fuzzy evidence</summary>

- *Thierry Denoeux*

- `2004.11638v2` - [abs](http://arxiv.org/abs/2004.11638v2) - [pdf](http://arxiv.org/pdf/2004.11638v2)

> We revisit Zadeh's notion of "evidence of the second kind" and show that it provides the foundation for a general theory of epistemic random fuzzy sets, which generalizes both the Dempster-Shafer theory of belief functions and possibility theory. In this perspective, Dempster-Shafer theory deals with belief functions generated by random sets, while possibility theory deals with belief functions induced by fuzzy sets. The more general theory allows us to represent and combine evidence that is both uncertain and fuzzy. We demonstrate the application of this formalism to statistical inference, and show that it makes it possible to reconcile the possibilistic interpretation of likelihood with Bayesian inference.

</details>

<details>

<summary>2020-10-16 16:53:09 - Analysis of professional basketball field goal attempts via a Bayesian matrix clustering approach</summary>

- *Fan Yin, Guanyu Hu, Weining Shen*

- `2010.08495v1` - [abs](http://arxiv.org/abs/2010.08495v1) - [pdf](http://arxiv.org/pdf/2010.08495v1)

> We propose a Bayesian nonparametric matrix clustering approach to analyze the latent heterogeneity structure in the shot selection data collected from professional basketball players in the National Basketball Association (NBA). The proposed method adopts a mixture of finite mixtures framework and fully utilizes the spatial information via a mixture of matrix normal distribution representation. We propose an efficient Markov chain Monte Carlo algorithm for posterior sampling that allows simultaneous inference on both the number of clusters and the cluster configurations. We also establish large sample convergence properties for the posterior distribution. The excellent empirical performance of the proposed method is demonstrated via simulation studies and an application to shot chart data from selected players in the 2017 18 NBA regular season.

</details>

<details>

<summary>2020-10-18 12:40:16 - Using mobility data in the design of optimal lockdown strategies for the COVID-19 pandemic</summary>

- *Ritabrata Dutta, Susana Gomes, Dante Kalise, Lorenzo Pacchiardi*

- `2006.16059v2` - [abs](http://arxiv.org/abs/2006.16059v2) - [pdf](http://arxiv.org/pdf/2006.16059v2)

> A mathematical model for the COVID-19 pandemic spread, which integrates age-structured Susceptible-Exposed-Infected-Recovered-Deceased dynamics with real mobile phone data accounting for the population mobility, is presented. The dynamical model adjustment is performed via Approximate Bayesian Computation. Optimal lockdown and exit strategies are determined based on nonlinear model predictive control, constrained to public-health and socio-economic factors. Through an extensive computational validation of the methodology, it is shown that it is possible to compute robust exit strategies with realistic reduced mobility values to inform public policy making, and we exemplify the applicability of the methodology using datasets from England and France. Code implementing the described experiments is available at https://github.com/OptimalLockdown.

</details>

<details>

<summary>2020-10-18 13:53:29 - Estimating high-resolution Red Sea surface temperature hotspots, using a low-rank semiparametric spatial model</summary>

- *Arnab Hazra, Raphaël Huser*

- `1912.05657v3` - [abs](http://arxiv.org/abs/1912.05657v3) - [pdf](http://arxiv.org/pdf/1912.05657v3)

> In this work, we estimate extreme sea surface temperature (SST) hotspots, i.e., high threshold exceedance regions, for the Red Sea, a vital region of high biodiversity. We analyze high-resolution satellite-derived SST data comprising daily measurements at 16703 grid cells across the Red Sea over the period 1985-2015. We propose a semiparametric Bayesian spatial mixed-effects linear model with a flexible mean structure to capture spatially-varying trend and seasonality, while the residual spatial variability is modeled through a Dirichlet process mixture (DPM) of low-rank spatial Student-$t$ processes (LTPs). By specifying cluster-specific parameters for each LTP mixture component, the bulk of the SST residuals influence tail inference and hotspot estimation only moderately. Our proposed model has a nonstationary mean, covariance and tail dependence, and posterior inference can be drawn efficiently through Gibbs sampling. In our application, we show that the proposed method outperforms some natural parametric and semiparametric alternatives. Moreover, we show how hotspots can be identified and we estimate extreme SST hotspots for the whole Red Sea, projected until the year 2100, based on the Representative Concentration Pathways 4.5 and 8.5. The estimated 95\% credible region for joint high threshold exceedances include large areas covering major endangered coral reefs in the southern Red Sea.

</details>

<details>

<summary>2020-10-18 16:17:46 - Deep Residual Autoencoders for Expectation Maximization-inspired Dictionary Learning</summary>

- *Bahareh Tolooshams, Sourav Dey, Demba Ba*

- `1904.08827v3` - [abs](http://arxiv.org/abs/1904.08827v3) - [pdf](http://arxiv.org/pdf/1904.08827v3)

> We introduce a neural-network architecture, termed the constrained recurrent sparse autoencoder (CRsAE), that solves convolutional dictionary learning problems, thus establishing a link between dictionary learning and neural networks. Specifically, we leverage the interpretation of the alternating-minimization algorithm for dictionary learning as an approximate Expectation-Maximization algorithm to develop autoencoders that enable the simultaneous training of the dictionary and regularization parameter (ReLU bias). The forward pass of the encoder approximates the sufficient statistics of the E-step as the solution to a sparse coding problem, using an iterative proximal gradient algorithm called FISTA. The encoder can be interpreted either as a recurrent neural network or as a deep residual network, with two-sided ReLU non-linearities in both cases. The M-step is implemented via a two-stage back-propagation. The first stage relies on a linear decoder applied to the encoder and a norm-squared loss. It parallels the dictionary update step in dictionary learning. The second stage updates the regularization parameter by applying a loss function to the encoder that includes a prior on the parameter motivated by Bayesian statistics. We demonstrate in an image-denoising task that CRsAE learns Gabor-like filters, and that the EM-inspired approach for learning biases is superior to the conventional approach. In an application to recordings of electrical activity from the brain, we demonstrate that CRsAE learns realistic spike templates and speeds up the process of identifying spike times by 900x compared to algorithms based on convex optimization.

</details>

<details>

<summary>2020-10-18 22:59:37 - DAGs with No Fears: A Closer Look at Continuous Optimization for Learning Bayesian Networks</summary>

- *Dennis Wei, Tian Gao, Yue Yu*

- `2010.09133v1` - [abs](http://arxiv.org/abs/2010.09133v1) - [pdf](http://arxiv.org/pdf/2010.09133v1)

> This paper re-examines a continuous optimization framework dubbed NOTEARS for learning Bayesian networks. We first generalize existing algebraic characterizations of acyclicity to a class of matrix polynomials. Next, focusing on a one-parameter-per-edge setting, it is shown that the Karush-Kuhn-Tucker (KKT) optimality conditions for the NOTEARS formulation cannot be satisfied except in a trivial case, which explains a behavior of the associated algorithm. We then derive the KKT conditions for an equivalent reformulation, show that they are indeed necessary, and relate them to explicit constraints that certain edges be absent from the graph. If the score function is convex, these KKT conditions are also sufficient for local minimality despite the non-convexity of the constraint. Informed by the KKT conditions, a local search post-processing algorithm is proposed and shown to substantially and universally improve the structural Hamming distance of all tested algorithms, typically by a factor of 2 or more. Some combinations with local search are both more accurate and more efficient than the original NOTEARS.

</details>

<details>

<summary>2020-10-18 23:09:39 - Bayesian joint modeling of chemical structure and dose response curves</summary>

- *Kelly R. Moran, David Dunson, Matthew W. Wheeler, Amy H. Herring*

- `1912.12228v2` - [abs](http://arxiv.org/abs/1912.12228v2) - [pdf](http://arxiv.org/pdf/1912.12228v2)

> Today there are approximately 85,000 chemicals regulated under the Toxic Substances Control Act, with around 2,000 new chemicals introduced each year. It is impossible to screen all of these chemicals for potential toxic effects either via full organism in vivo studies or in vitro high-throughput screening (HTS) programs. Toxicologists face the challenge of choosing which chemicals to screen, and predicting the toxicity of as-yet-unscreened chemicals. Our goal is to describe how variation in chemical structure relates to variation in toxicological response to enable in silico toxicity characterization designed to meet both of these challenges. With our Bayesian partially Supervised Sparse and Smooth Factor Analysis ($\text{BS}^3\text{FA}$) model, we learn a distance between chemicals targeted to toxicity, rather than one based on molecular structure alone. Our model also enables the prediction of chemical dose-response profiles based on chemical structure (that is, without in vivo or in vitro testing) by taking advantage of a large database of chemicals that have already been tested for toxicity in HTS programs. We show superior simulation performance in distance learning and modest to large gains in predictive ability compared to existing methods. Results from the high-throughput screening data application elucidate the relationship between chemical structure and a toxicity-relevant high-throughput assay. An R package for $\text{BS}^3\text{FA}$ is available online at https://github.com/kelrenmor/bs3fa.

</details>

<details>

<summary>2020-10-18 23:16:55 - Bayesian Hierarchical Factor Regression Models to Infer Cause of Death From Verbal Autopsy Data</summary>

- *Kelly R. Moran, Elizabeth L. Turner, David Dunson, Amy H. Herring*

- `1908.07632v2` - [abs](http://arxiv.org/abs/1908.07632v2) - [pdf](http://arxiv.org/pdf/1908.07632v2)

> In low-resource settings where vital registration of death is not routine it is often of critical interest to determine and study the cause of death (COD) for individuals and the cause-specific mortality fraction (CSMF) for populations. Post-mortem autopsies, considered the gold standard for COD assignment, are often difficult or impossible to implement due to deaths occurring outside the hospital, expense, and/or cultural norms. For this reason, Verbal Autopsies (VAs) are commonly conducted, consisting of a questionnaire administered to next of kin recording demographic information, known medical conditions, symptoms, and other factors for the decedent. This article proposes a novel class of hierarchical factor regression models that avoid restrictive assumptions of standard methods, allow both the mean and covariance to vary with COD category, and can include covariate information on the decedent, region, or events surrounding death. Taking a Bayesian approach to inference, this work develops an MCMC algorithm and validates the FActor Regression for Verbal Autopsy (FARVA) model in simulation experiments. An application of FARVA to real VA data shows improved goodness-of-fit and better predictive performance in inferring COD and CSMF over competing methods. Code and a user manual are made available at https://github.com/kelrenmor/farva.

</details>

<details>

<summary>2020-10-19 00:17:53 - Posterior Distribution for the Number of Clusters in Dirichlet Process Mixture Models</summary>

- *Chiao-Yu Yang, Eric Xia, Nhat Ho, Michael I. Jordan*

- `1905.09959v2` - [abs](http://arxiv.org/abs/1905.09959v2) - [pdf](http://arxiv.org/pdf/1905.09959v2)

> Dirichlet process mixture models (DPMM) play a central role in Bayesian nonparametrics, with applications throughout statistics and machine learning. DPMMs are generally used in clustering problems where the number of clusters is not known in advance, and the posterior distribution is treated as providing inference for this number. Recently, however, it has been shown that the DPMM is inconsistent in inferring the true number of components in certain cases. This is an asymptotic result, and it would be desirable to understand whether it holds with finite samples, and to more fully understand the full posterior. In this work, we provide a rigorous study for the posterior distribution of the number of clusters in DPMM under different prior distributions on the parameters and constraints on the distributions of the data. We provide novel lower bounds on the ratios of probabilities between $s+1$ clusters and $s$ clusters when the prior distributions on parameters are chosen to be Gaussian or uniform distributions.

</details>

<details>

<summary>2020-10-19 05:36:39 - Semi-supervised Embedding Learning for High-dimensional Bayesian Optimization</summary>

- *Jingfan Chen, Guanghui Zhu, Chunfeng Yuan, Yihua Huang*

- `2005.14601v3` - [abs](http://arxiv.org/abs/2005.14601v3) - [pdf](http://arxiv.org/pdf/2005.14601v3)

> Bayesian optimization is a broadly applied methodology to optimize the expensive black-box function. Despite its success, it still faces the challenge from the high-dimensional search space. To alleviate this problem, we propose a novel Bayesian optimization framework (termed SILBO), which finds a low-dimensional space to perform Bayesian optimization iteratively through semi-supervised dimension reduction. SILBO incorporates both labeled points and unlabeled points acquired from the acquisition function to guide the embedding space learning. To accelerate the learning procedure, we present a randomized method for generating the projection matrix. Furthermore, to map from the low-dimensional space to the high-dimensional original space, we propose two mapping strategies: $\text{SILBO}_{FZ}$ and $\text{SILBO}_{FX}$ according to the evaluation overhead of the objective function. Experimental results on both synthetic function and hyperparameter optimization tasks demonstrate that SILBO outperforms the existing state-of-the-art high-dimensional Bayesian optimization methods.

</details>

<details>

<summary>2020-10-19 05:54:29 - Variational Bayesian Monte Carlo with Noisy Likelihoods</summary>

- *Luigi Acerbi*

- `2006.08655v3` - [abs](http://arxiv.org/abs/2006.08655v3) - [pdf](http://arxiv.org/pdf/2006.08655v3)

> Variational Bayesian Monte Carlo (VBMC) is a recently introduced framework that uses Gaussian process surrogates to perform approximate Bayesian inference in models with black-box, non-cheap likelihoods. In this work, we extend VBMC to deal with noisy log-likelihood evaluations, such as those arising from simulation-based models. We introduce new `global' acquisition functions, such as expected information gain (EIG) and variational interquantile range (VIQR), which are robust to noise and can be efficiently evaluated within the VBMC setting. In a novel, challenging, noisy-inference benchmark comprising of a variety of models with real datasets from computational and cognitive neuroscience, VBMC+VIQR achieves state-of-the-art performance in recovering the ground-truth posteriors and model evidence. In particular, our method vastly outperforms `local' acquisition functions and other surrogate-based inference methods while keeping a small algorithmic cost. Our benchmark corroborates VBMC as a general-purpose technique for sample-efficient black-box Bayesian inference also with noisy models.

</details>

<details>

<summary>2020-10-19 09:07:57 - Bayesian Inference for Optimal Transport with Stochastic Cost</summary>

- *Anton Mallasto, Markus Heinonen, Samuel Kaski*

- `2010.09327v1` - [abs](http://arxiv.org/abs/2010.09327v1) - [pdf](http://arxiv.org/pdf/2010.09327v1)

> In machine learning and computer vision, optimal transport has had significant success in learning generative models and defining metric distances between structured and stochastic data objects, that can be cast as probability measures. The key element of optimal transport is the so called lifting of an \emph{exact} cost (distance) function, defined on the sample space, to a cost (distance) between probability measures over the sample space. However, in many real life applications the cost is \emph{stochastic}: e.g., the unpredictable traffic flow affects the cost of transportation between a factory and an outlet. To take this stochasticity into account, we introduce a Bayesian framework for inferring the optimal transport plan distribution induced by the stochastic cost, allowing for a principled way to include prior information and to model the induced stochasticity on the transport plans. Additionally, we tailor an HMC method to sample from the resulting transport plan posterior distribution.

</details>

<details>

<summary>2020-10-19 19:06:00 - ABC-Di: Approximate Bayesian Computation for Discrete Data</summary>

- *Ilze Amanda Auzina, Jakub M. Tomczak*

- `2010.09790v1` - [abs](http://arxiv.org/abs/2010.09790v1) - [pdf](http://arxiv.org/pdf/2010.09790v1)

> Many real-life problems are represented as a black-box, i.e., the internal workings are inaccessible or a closed-form mathematical expression of the likelihood function cannot be defined. For continuous random variables likelihood-free inference problems can be solved by a group of methods under the name of Approximate Bayesian Computation (ABC). However, a similar approach for discrete random variables is yet to be formulated. Here, we aim to fill this research gap. We propose to use a population-based MCMC ABC framework. Further, we present a valid Markov kernel, and propose a new kernel that is inspired by Differential Evolution. We assess the proposed approach on a problem with the known likelihood function, namely, discovering the underlying diseases based on a QMR-DT Network, and three likelihood-free inference problems: (i) the QMR-DT Network with the unknown likelihood function, (ii) learning binary neural network, and (iii) Neural Architecture Search. The obtained results indicate the high potential of the proposed framework and the superiority of the new Markov kernel.

</details>

<details>

<summary>2020-10-19 20:34:27 - A Bayesian Hierarchical Network for Combining Heterogeneous Data Sources in Medical Diagnoses</summary>

- *Claire Donnat, Nina Miolane, Freddy Bunbury, Jack Kreindler*

- `2007.13847v2` - [abs](http://arxiv.org/abs/2007.13847v2) - [pdf](http://arxiv.org/pdf/2007.13847v2)

> Computer-Aided Diagnosis has shown stellar performance in providing accurate medical diagnoses across multiple testing modalities (medical images, electrophysiological signals, etc.). While this field has typically focused on fully harvesting the signal provided by a single (and generally extremely reliable) modality, fewer efforts have utilized imprecise data lacking reliable ground truth labels. In this unsupervised, noisy setting, the robustification and quantification of the diagnosis uncertainty become paramount, thus posing a new challenge: how can we combine multiple sources of information -- often themselves with vastly varying levels of precision and uncertainty -- to provide a diagnosis estimate with confidence bounds? Motivated by a concrete application in antibody testing, we devise a Stochastic Expectation-Maximization algorithm that allows the principled integration of heterogeneous, and potentially unreliable, data types. Our Bayesian formalism is essential in (a) flexibly combining these heterogeneous data sources and their corresponding levels of uncertainty, (b) quantifying the degree of confidence associated with a given diagnostic, and (c) dealing with the missing values that typically plague medical data. We quantify the potential of this approach on simulated data, and showcase its practicality by deploying it on a real COVID-19 immunity study.

</details>

<details>

<summary>2020-10-19 20:42:18 - Can I Trust My Fairness Metric? Assessing Fairness with Unlabeled Data and Bayesian Inference</summary>

- *Disi Ji, Padhraic Smyth, Mark Steyvers*

- `2010.09851v1` - [abs](http://arxiv.org/abs/2010.09851v1) - [pdf](http://arxiv.org/pdf/2010.09851v1)

> We investigate the problem of reliably assessing group fairness when labeled examples are few but unlabeled examples are plentiful. We propose a general Bayesian framework that can augment labeled data with unlabeled data to produce more accurate and lower-variance estimates compared to methods based on labeled data alone. Our approach estimates calibrated scores for unlabeled examples in each group using a hierarchical latent variable model conditioned on labeled examples. This in turn allows for inference of posterior distributions with associated notions of uncertainty for a variety of group fairness metrics. We demonstrate that our approach leads to significant and consistent reductions in estimation error across multiple well-known fairness datasets, sensitive attributes, and predictive models. The results show the benefits of using both unlabeled data and Bayesian inference in terms of assessing whether a prediction model is fair or not.

</details>

<details>

<summary>2020-10-19 23:41:30 - Bayesian Group Learning for Shot Selection of Professional Basketball Players</summary>

- *Guanyu Hu, Hou-Cheng Yang, Yishu Xue*

- `2006.07513v3` - [abs](http://arxiv.org/abs/2006.07513v3) - [pdf](http://arxiv.org/pdf/2006.07513v3)

> In this paper, we develop a group learning approach to analyze the underlying heterogeneity structure of shot selection among professional basketball players in the NBA. We propose a mixture of finite mixtures (MFM) model to capture the heterogeneity of shot selection among different players based on Log Gaussian Cox process (LGCP). Our proposed method can simultaneously estimate the number of groups and group configurations. An efficient Markov Chain Monte Carlo (MCMC) algorithm is developed for our proposed model. Simulation studies have been conducted to demonstrate its performance. Ultimately, our proposed learning approach is further illustrated in analyzing shot charts of several players in the NBA's 2017-2018 regular season.

</details>

<details>

<summary>2020-10-20 03:54:20 - Robust Asynchronous and Network-Independent Cooperative Learning</summary>

- *Eduardo Mojica-Nava, David Yanguas-Rojas, César A. Uribe*

- `2010.09993v1` - [abs](http://arxiv.org/abs/2010.09993v1) - [pdf](http://arxiv.org/pdf/2010.09993v1)

> We consider the model of cooperative learning via distributed non-Bayesian learning, where a network of agents tries to jointly agree on a hypothesis that best described a sequence of locally available observations. Building upon recently proposed weak communication network models, we propose a robust cooperative learning rule that allows asynchronous communications, message delays, unpredictable message losses, and directed communication among nodes. We show that our proposed learning dynamics guarantee that all agents in the network will have an asymptotic exponential decay of their beliefs on the wrong hypothesis, indicating that the beliefs of all agents will concentrate on the optimal hypotheses. Numerical experiments provide evidence on a number of network setups.

</details>

<details>

<summary>2020-10-20 20:30:55 - Bayesian Attention Modules</summary>

- *Xinjie Fan, Shujian Zhang, Bo Chen, Mingyuan Zhou*

- `2010.10604v1` - [abs](http://arxiv.org/abs/2010.10604v1) - [pdf](http://arxiv.org/pdf/2010.10604v1)

> Attention modules, as simple and effective tools, have not only enabled deep neural networks to achieve state-of-the-art results in many domains, but also enhanced their interpretability. Most current models use deterministic attention modules due to their simplicity and ease of optimization. Stochastic counterparts, on the other hand, are less popular despite their potential benefits. The main reason is that stochastic attention often introduces optimization issues or requires significant model changes. In this paper, we propose a scalable stochastic version of attention that is easy to implement and optimize. We construct simplex-constrained attention distributions by normalizing reparameterizable distributions, making the training process differentiable. We learn their parameters in a Bayesian framework where a data-dependent prior is introduced for regularization. We apply the proposed stochastic attention modules to various attention-based models, with applications to graph node classification, visual question answering, image captioning, machine translation, and language understanding. Our experiments show the proposed method brings consistent improvements over the corresponding baselines.

</details>

<details>

<summary>2020-10-21 00:14:30 - Continuous Meta-Learning without Tasks</summary>

- *James Harrison, Apoorva Sharma, Chelsea Finn, Marco Pavone*

- `1912.08866v2` - [abs](http://arxiv.org/abs/1912.08866v2) - [pdf](http://arxiv.org/pdf/1912.08866v2)

> Meta-learning is a promising strategy for learning to efficiently learn within new tasks, using data gathered from a distribution of tasks. However, the meta-learning literature thus far has focused on the task segmented setting, where at train-time, offline data is assumed to be split according to the underlying task, and at test-time, the algorithms are optimized to learn in a single task. In this work, we enable the application of generic meta-learning algorithms to settings where this task segmentation is unavailable, such as continual online learning with a time-varying task. We present meta-learning via online changepoint analysis (MOCA), an approach which augments a meta-learning algorithm with a differentiable Bayesian changepoint detection scheme. The framework allows both training and testing directly on time series data without segmenting it into discrete tasks. We demonstrate the utility of this approach on a nonlinear meta-regression benchmark as well as two meta-image-classification benchmarks.

</details>

<details>

<summary>2020-10-21 00:23:25 - Variational Inference for Graph Convolutional Networks in the Absence of Graph Data and Adversarial Settings</summary>

- *Pantelis Elinas, Edwin V. Bonilla, Louis Tiao*

- `1906.01852v5` - [abs](http://arxiv.org/abs/1906.01852v5) - [pdf](http://arxiv.org/pdf/1906.01852v5)

> We propose a framework that lifts the capabilities of graph convolutional networks (GCNs) to scenarios where no input graph is given and increases their robustness to adversarial attacks. We formulate a joint probabilistic model that considers a prior distribution over graphs along with a GCN-based likelihood and develop a stochastic variational inference algorithm to estimate the graph posterior and the GCN parameters jointly. To address the problem of propagating gradients through latent variables drawn from discrete distributions, we use their continuous relaxations known as Concrete distributions. We show that, on real datasets, our approach can outperform state-of-the-art Bayesian and non-Bayesian graph neural network algorithms on the task of semi-supervised classification in the absence of graph data and when the network structure is subjected to adversarial perturbations.

</details>

<details>

<summary>2020-10-21 03:21:13 - A Bayesian Hidden Semi-Markov Model with Covariate-Dependent State Duration Parameters for High-Frequency Data from Wearable Devices</summary>

- *Shirley Rojas-Salazar, Erin M. Schliep, Christopher K. Wikle, Matthew Hawkey*

- `2010.10739v1` - [abs](http://arxiv.org/abs/2010.10739v1) - [pdf](http://arxiv.org/pdf/2010.10739v1)

> Data collected by wearable devices in sports provide valuable information about an athlete's behavior such as their activity, performance, and ability. These time series data can be studied with approaches such as hidden Markov and semi-Markov models (HMM and HSMM) for varied purposes including activity recognition and event detection. HSMMs extend the HMM by explicitly modeling the time spent in each state. In a discrete-time HSMM, the duration in each state can be modeled with a zero-truncated Poisson distribution, where the duration parameter may be state-specific but constant in time. We extend the HSMM by allowing the state-specific duration parameters to vary in time and model them as a function of known covariates derived from the wearable device and observed over a period of time leading up to a state transition. In addition, we propose a data subsampling approach given that high-frequency data from wearable devices can violate the conditional independence assumption of the HSMM. We apply the model to wearable device data collected on a soccer referee in a Major League Soccer game. We model the referee's physiological response to the game demands and identify important time-varying effects of these demands associated with the duration in each state.

</details>

<details>

<summary>2020-10-21 07:11:36 - Dirichlet-tree multinomial mixtures for clustering microbiome compositions</summary>

- *Jialiang Mao, Li Ma*

- `2008.00400v2` - [abs](http://arxiv.org/abs/2008.00400v2) - [pdf](http://arxiv.org/pdf/2008.00400v2)

> Studying the human microbiome has gained substantial interest in recent years, and a common task in the analysis of these data is to cluster microbiome compositions into subtypes. This subdivision of samples into subgroups serves as an intermediary step in achieving personalized diagnosis and treatment. In applying existing clustering methods to modern microbiome studies including the American Gut Project (AGP) data, we found that this seemingly standard task, however, is very challenging in the microbiome composition context due to several key features of such data. Standard distance-based clustering algorithms generally do not produce reliable results as they do not take into account the heterogeneity of the cross-sample variability among the bacterial taxa, while existing model-based approaches do not allow sufficient flexibility for the identification of complex within-cluster variation from cross-cluster variation. Direct applications of such methods generally lead to overly dispersed clusters in the AGP data and such phenomenon is common for other microbiome data. To overcome these challenges, we introduce Dirichlet-tree multinomial mixtures (DTMM) as a Bayesian generative model for clustering amplicon sequencing data in microbiome studies. DTMM models the microbiome population with a mixture of Dirichlet-tree kernels that utilizes the phylogenetic tree to offer a more flexible covariance structure in characterizing within-cluster variation, and it provides a means for identifying a subset of signature taxa that distinguish the clusters. We perform extensive simulation studies to evaluate the performance of DTMM and compare it to state-of-the-art model-based and distance-based clustering methods in the microbiome context. Finally, we report a case study on the fecal data from the AGP to identify compositional clusters among individuals with inflammatory bowel disease and diabetes.

</details>

<details>

<summary>2020-10-21 15:05:58 - Generalized Bayesian Filtering via Sequential Monte Carlo</summary>

- *Ayman Boustati, Ömer Deniz Akyildiz, Theodoros Damoulas, Adam M. Johansen*

- `2002.09998v2` - [abs](http://arxiv.org/abs/2002.09998v2) - [pdf](http://arxiv.org/pdf/2002.09998v2)

> We introduce a framework for inference in general state-space hidden Markov models (HMMs) under likelihood misspecification. In particular, we leverage the loss-theoretic perspective of Generalized Bayesian Inference (GBI) to define generalised filtering recursions in HMMs, that can tackle the problem of inference under model misspecification. In doing so, we arrive at principled procedures for robust inference against observation contamination by utilising the $\beta$-divergence. Operationalising the proposed framework is made possible via sequential Monte Carlo methods (SMC), where most standard particle methods, and their associated convergence results, are readily adapted to the new setting. We apply our approach to object tracking and Gaussian process regression problems, and observe improved performance over both standard filtering algorithms and other robust filters.

</details>

<details>

<summary>2020-10-21 15:56:55 - Predictive Complexity Priors</summary>

- *Eric Nalisnick, Jonathan Gordon, José Miguel Hernández-Lobato*

- `2006.10801v3` - [abs](http://arxiv.org/abs/2006.10801v3) - [pdf](http://arxiv.org/pdf/2006.10801v3)

> Specifying a Bayesian prior is notoriously difficult for complex models such as neural networks. Reasoning about parameters is made challenging by the high-dimensionality and over-parameterization of the space. Priors that seem benign and uninformative can have unintuitive and detrimental effects on a model's predictions. For this reason, we propose predictive complexity priors: a functional prior that is defined by comparing the model's predictions to those of a reference model. Although originally defined on the model outputs, we transfer the prior to the model parameters via a change of variables. The traditional Bayesian workflow can then proceed as usual. We apply our predictive complexity prior to high-dimensional regression, reasoning over neural network depth, and sharing of statistical strength for few-shot learning.

</details>

<details>

<summary>2020-10-21 17:50:23 - Validating Bayesian Inference Algorithms with Simulation-Based Calibration</summary>

- *Sean Talts, Michael Betancourt, Daniel Simpson, Aki Vehtari, Andrew Gelman*

- `1804.06788v2` - [abs](http://arxiv.org/abs/1804.06788v2) - [pdf](http://arxiv.org/pdf/1804.06788v2)

> Verifying the correctness of Bayesian computation is challenging. This is especially true for complex models that are common in practice, as these require sophisticated model implementations and algorithms. In this paper we introduce \emph{simulation-based calibration} (SBC), a general procedure for validating inferences from Bayesian algorithms capable of generating posterior samples. This procedure not only identifies inaccurate computation and inconsistencies in model implementations but also provides graphical summaries that can indicate the nature of the problems that arise. We argue that SBC is a critical part of a robust Bayesian workflow, as well as being a useful tool for those developing computational algorithms and statistical software.

</details>

<details>

<summary>2020-10-21 19:38:57 - Convex Polytope Trees</summary>

- *Mohammadreza Armandpour, Mingyuan Zhou*

- `2010.11266v1` - [abs](http://arxiv.org/abs/2010.11266v1) - [pdf](http://arxiv.org/pdf/2010.11266v1)

> A decision tree is commonly restricted to use a single hyperplane to split the covariate space at each of its internal nodes. It often requires a large number of nodes to achieve high accuracy, hurting its interpretability. In this paper, we propose convex polytope trees (CPT) to expand the family of decision trees by an interpretable generalization of their decision boundary. The splitting function at each node of CPT is based on the logical disjunction of a community of differently weighted probabilistic linear decision-makers, which also geometrically corresponds to a convex polytope in the covariate space. We use a nonparametric Bayesian prior at each node to infer the community's size, encouraging simpler decision boundaries by shrinking the number of polytope facets. We develop a greedy method to efficiently construct CPT and scalable end-to-end training algorithms for the tree parameters when the tree structure is given. We empirically demonstrate the efficiency of CPT over existing state-of-the-art decision trees in several real-world classification and regression tasks from diverse domains.

</details>

<details>

<summary>2020-10-21 22:06:08 - Integrated causal-predictive machine learning models for tropical cyclone epidemiology</summary>

- *Rachel C. Nethery, Nina Katz-Christy, Marianthi-Anna Kioumourtzoglou, Robbie M. Parks, Andrea Schumacher, G. Brooke Anderson*

- `2010.11330v1` - [abs](http://arxiv.org/abs/2010.11330v1) - [pdf](http://arxiv.org/pdf/2010.11330v1)

> Strategic preparedness has been shown to reduce the adverse health impacts of hurricanes and tropical storms, referred to collectively as tropical cyclones (TCs), but its protective impact could be enhanced by a more comprehensive and rigorous characterization of TC epidemiology. To generate the insights and tools necessary for high-precision TC preparedness, we develop and apply a novel Bayesian machine learning approach that standardizes estimation of historic TC health impacts, discovers common patterns and sources of heterogeneity in those health impacts, and enables identification of communities at highest health risk for future TCs. The model integrates (1) a causal inference component to quantify the immediate health impacts of recent historic TCs at high spatial resolution and (2) a predictive component that captures how TC meteorological features and socioeconomic/demographic characteristics of impacted communities are associated with health impacts. We apply it to a rich data platform containing detailed historic TC exposure information and Medicare claims data. The health outcomes used in our analyses are all-cause mortality and cardiovascular- and respiratory-related hospitalizations. We report a high degree of heterogeneity in the acute health impacts of historic TCs at both the TC level and the community level, with substantial increases in respiratory hospitalizations, on average, during a two-week period surrounding TCs. TC sustained windspeeds are found to be the primary driver of increased mortality and respiratory risk. Our modeling approach has broader utility for predicting the health impacts of many types of extreme climate events.

</details>

<details>

<summary>2020-10-22 00:15:06 - Statistical Guarantees and Algorithmic Convergence Issues of Variational Boosting</summary>

- *Biraj Subhra Guha, Anirban Bhattacharya, Debdeep Pati*

- `2010.09540v2` - [abs](http://arxiv.org/abs/2010.09540v2) - [pdf](http://arxiv.org/pdf/2010.09540v2)

> We provide statistical guarantees for Bayesian variational boosting by proposing a novel small bandwidth Gaussian mixture variational family. We employ a functional version of Frank-Wolfe optimization as our variational algorithm and study frequentist properties of the iterative boosting updates. Comparisons are drawn to the recent literature on boosting, describing how the choice of the variational family and the discrepancy measure affect both convergence and finite-sample statistical properties of the optimization routine. Specifically, we first demonstrate stochastic boundedness of the boosting iterates with respect to the data generating distribution. We next integrate this within our algorithm to provide an explicit convergence rate, ending with a result on the required number of boosting updates.

</details>

<details>

<summary>2020-10-22 00:27:23 - Bayes-optimal Methods for Finding the Source of a Cascade</summary>

- *Anirudh Sridhar, H. Vincent Poor*

- `2001.11942v3` - [abs](http://arxiv.org/abs/2001.11942v3) - [pdf](http://arxiv.org/pdf/2001.11942v3)

> We study the problem of estimating the source of a network cascade. The cascade starts from a single vertex at time 0 and spreads over time, but only a noisy version of the propagation is observable. The goal is then to design a stopping time and estimator that will estimate the source well while ensuring the cost of the cascade to the system is not too large. We rigorously formulate a Bayesian approach to the problem. If vertices can be labelled by vectors in Euclidean space (which is natural in geo-spatial networks), the optimal estimator is the conditional mean estimator, and we derive an explicit form for the optimal stopping time under minimal assumptions on the cascade dynamics. We study the performance of the optimal stopping time on lattices, and show that a computationally efficient but suboptimal stopping time which compares the posterior variance to a threshold has near-optimal performance.

</details>

<details>

<summary>2020-10-22 01:18:23 - Variance reduction for Random Coordinate Descent-Langevin Monte Carlo</summary>

- *Zhiyan Ding, Qin Li*

- `2006.06068v4` - [abs](http://arxiv.org/abs/2006.06068v4) - [pdf](http://arxiv.org/pdf/2006.06068v4)

> Sampling from a log-concave distribution function is one core problem that has wide applications in Bayesian statistics and machine learning. While most gradient free methods have slow convergence rate, the Langevin Monte Carlo (LMC) that provides fast convergence requires the computation of gradients. In practice one uses finite-differencing approximations as surrogates, and the method is expensive in high-dimensions.   A natural strategy to reduce computational cost in each iteration is to utilize random gradient approximations, such as random coordinate descent (RCD) or simultaneous perturbation stochastic approximation (SPSA). We show by a counter-example that blindly applying RCD does not achieve the goal in the most general setting. The high variance induced by the randomness means a larger number of iterations are needed, and this balances out the saving in each iteration.   We then introduce a new variance reduction approach, termed Randomized Coordinates Averaging Descent (RCAD), and incorporate it with both overdamped and underdamped LMC. The methods are termed RCAD-O-LMC and RCAD-U-LMC respectively. The methods still sit in the random gradient approximation framework, and thus the computational cost in each iteration is low. However, by employing RCAD, the variance is reduced, so the methods converge within the same number of iterations as the classical overdamped and underdamped LMC. This leads to a computational saving overall.

</details>

<details>

<summary>2020-10-22 05:26:44 - Nonasymptotic estimates for Stochastic Gradient Langevin Dynamics under local conditions in nonconvex optimization</summary>

- *Ying Zhang, Ömer Deniz Akyildiz, Theodoros Damoulas, Sotirios Sabanis*

- `1910.02008v4` - [abs](http://arxiv.org/abs/1910.02008v4) - [pdf](http://arxiv.org/pdf/1910.02008v4)

> Within the context of empirical risk minimization, see Raginsky, Rakhlin, and Telgarsky (2017), we are concerned with a non-asymptotic analysis of sampling algorithms used in optimization. In particular, we obtain non-asymptotic error bounds for a popular class of algorithms called Stochastic Gradient Langevin Dynamics (SGLD). These results are derived in Wasserstein-1 and Wasserstein-2 distances in the absence of log-concavity of the target distribution. More precisely, the stochastic gradient $H(\theta, x)$ is assumed to be locally Lipschitz continuous in both variables, and furthermore, the dissipativity condition is relaxed by removing its uniform dependence in $x$. This relaxation allows us to present two key paradigms within the framework of scalable posterior sampling for Bayesian inference and of nonconvex optimization; namely, examples from minibatch logistic regression and from variational inference are given by providing theoretical guarantees for the sampling behaviour of the algorithm.

</details>

<details>

<summary>2020-10-22 07:22:40 - BayReL: Bayesian Relational Learning for Multi-omics Data Integration</summary>

- *Ehsan Hajiramezanali, Arman Hasanzadeh, Nick Duffield, Krishna R Narayanan, Xiaoning Qian*

- `2010.05895v3` - [abs](http://arxiv.org/abs/2010.05895v3) - [pdf](http://arxiv.org/pdf/2010.05895v3)

> High-throughput molecular profiling technologies have produced high-dimensional multi-omics data, enabling systematic understanding of living systems at the genome scale. Studying molecular interactions across different data types helps reveal signal transduction mechanisms across different classes of molecules. In this paper, we develop a novel Bayesian representation learning method that infers the relational interactions across multi-omics data types. Our method, Bayesian Relational Learning (BayReL) for multi-omics data integration, takes advantage of a priori known relationships among the same class of molecules, modeled as a graph at each corresponding view, to learn view-specific latent variables as well as a multi-partite graph that encodes the interactions across views. Our experiments on several real-world datasets demonstrate enhanced performance of BayReL in inferring meaningful interactions compared to existing baselines.

</details>

<details>

<summary>2020-10-22 10:08:28 - Learning under Model Misspecification: Applications to Variational and Ensemble methods</summary>

- *Andres R. Masegosa*

- `1912.08335v5` - [abs](http://arxiv.org/abs/1912.08335v5) - [pdf](http://arxiv.org/pdf/1912.08335v5)

> Virtually any model we use in machine learning to make predictions does not perfectly represent reality. So, most of the learning happens under model misspecification. In this work, we present a novel analysis of the generalization performance of Bayesian model averaging under model misspecification and i.i.d. data using a new family of second-order PAC-Bayes bounds. This analysis shows, in simple and intuitive terms, that Bayesian model averaging provides suboptimal generalization performance when the model is misspecified. In consequence, we provide strong theoretical arguments showing that Bayesian methods are not optimal for learning predictive models, unless the model class is perfectly specified. Using novel second-order PAC-Bayes bounds, we derive a new family of Bayesian-like algorithms, which can be implemented as variational and ensemble methods. The output of these algorithms is a new posterior distribution, different from the Bayesian posterior, which induces a posterior predictive distribution with better generalization performance. Experiments with Bayesian neural networks illustrate these findings.

</details>

<details>

<summary>2020-10-22 11:03:48 - Using Large Ensembles of Control Variates for Variational Inference</summary>

- *Tomas Geffner, Justin Domke*

- `1810.12482v2` - [abs](http://arxiv.org/abs/1810.12482v2) - [pdf](http://arxiv.org/pdf/1810.12482v2)

> Variational inference is increasingly being addressed with stochastic optimization. In this setting, the gradient's variance plays a crucial role in the optimization procedure, since high variance gradients lead to poor convergence. A popular approach used to reduce gradient's variance involves the use of control variates. Despite the good results obtained, control variates developed for variational inference are typically looked at in isolation. In this paper we clarify the large number of control variates that are available by giving a systematic view of how they are derived. We also present a Bayesian risk minimization framework in which the quality of a procedure for combining control variates is quantified by its effect on optimization convergence rates, which leads to a very simple combination rule. Results show that combining a large number of control variates this way significantly improves the convergence of inference over using the typical gradient estimators or a reduced number of control variates.

</details>

<details>

<summary>2020-10-22 12:46:28 - Statistical methods for linking geostatistical maps and transmission models: Application to lymphatic filariasis in East Africa</summary>

- *Panayiota Touloupou, Renata Retkute, T Deirdre Hollingsworth, Simon E. F. Spencer*

- `2010.12383v1` - [abs](http://arxiv.org/abs/2010.12383v1) - [pdf](http://arxiv.org/pdf/2010.12383v1)

> Infectious diseases remain one of the major causes of human mortality and suffering. Mathematical models have been established as an important tool for capturing the features that drive the spread of the disease, predicting the progression of an epidemic and hence guiding the development of strategies to control it. Another important area of epidemiological interest is the development of geostatistical methods for the analysis of data from spatially referenced prevalence surveys. Maps of prevalence are useful, not only for enabling a more precise disease risk stratification, but also for guiding the planning of more reliable spatial control programmes by identifying affected areas. Despite the methodological advances that have been made in each area independently, efforts to link transmission models and geostatistical maps have been limited. Motivated by this fact, we developed a Bayesian approach that combines fine-scale geostatistical maps of disease prevalence with transmission models to provide quantitative, spatially explicit projections of the current and future impact of control programs against a disease. These estimates can then be used at a local level to identify the effectiveness of suggested intervention schemes and allow investigation of alternative strategies. The methodology has been applied to lymphatic filariasis in East Africa to provide estimates of the impact of different intervention strategies against the disease.

</details>

<details>

<summary>2020-10-22 13:17:36 - Deep active inference agents using Monte-Carlo methods</summary>

- *Zafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, Karl Friston*

- `2006.04176v2` - [abs](http://arxiv.org/abs/2006.04176v2) - [pdf](http://arxiv.org/pdf/2006.04176v2)

> Active inference is a Bayesian framework for understanding biological intelligence. The underlying theory brings together perception and action under one single imperative: minimizing free energy. However, despite its theoretical utility in explaining intelligence, computational implementations have been restricted to low-dimensional and idealized situations. In this paper, we present a neural architecture for building deep active inference agents operating in complex, continuous state-spaces using multiple forms of Monte-Carlo (MC) sampling. For this, we introduce a number of techniques, novel to active inference. These include: i) selecting free-energy-optimal policies via MC tree search, ii) approximating this optimal policy distribution via a feed-forward `habitual' network, iii) predicting future parameter belief updates using MC dropouts and, finally, iv) optimizing state transition precision (a high-end form of attention). Our approach enables agents to learn environmental dynamics efficiently, while maintaining task performance, in relation to reward-based counterparts. We illustrate this in a new toy environment, based on the dSprites data-set, and demonstrate that active inference agents automatically create disentangled representations that are apt for modeling state transitions. In a more complex Animal-AI environment, our agents (using the same neural architecture) are able to simulate future state transitions and actions (i.e., plan), to evince reward-directed navigation - despite temporary suspension of visual input. These results show that deep active inference - equipped with MC methods - provides a flexible framework to develop biologically-inspired intelligent agents, with applications in both machine learning and cognitive science.

</details>

<details>

<summary>2020-10-22 16:45:20 - Modular Meta-Learning with Shrinkage</summary>

- *Yutian Chen, Abram L. Friesen, Feryal Behbahani, Arnaud Doucet, David Budden, Matthew W. Hoffman, Nando de Freitas*

- `1909.05557v4` - [abs](http://arxiv.org/abs/1909.05557v4) - [pdf](http://arxiv.org/pdf/1909.05557v4)

> Many real-world problems, including multi-speaker text-to-speech synthesis, can greatly benefit from the ability to meta-learn large models with only a few task-specific components. Updating only these task-specific modules then allows the model to be adapted to low-data tasks for as many steps as necessary without risking overfitting. Unfortunately, existing meta-learning methods either do not scale to long adaptation or else rely on handcrafted task-specific architectures. Here, we propose a meta-learning approach that obviates the need for this often sub-optimal hand-selection. In particular, we develop general techniques based on Bayesian shrinkage to automatically discover and learn both task-specific and general reusable modules. Empirically, we demonstrate that our method discovers a small set of meaningful task-specific modules and outperforms existing meta-learning approaches in domains like few-shot text-to-speech that have little task data and long adaptation horizons. We also show that existing meta-learning methods including MAML, iMAML, and Reptile emerge as special cases of our method.

</details>

<details>

<summary>2020-10-22 18:30:08 - Re-Examining Linear Embeddings for High-Dimensional Bayesian Optimization</summary>

- *Benjamin Letham, Roberto Calandra, Akshara Rai, Eytan Bakshy*

- `2001.11659v2` - [abs](http://arxiv.org/abs/2001.11659v2) - [pdf](http://arxiv.org/pdf/2001.11659v2)

> Bayesian optimization (BO) is a popular approach to optimize expensive-to-evaluate black-box functions. A significant challenge in BO is to scale to high-dimensional parameter spaces while retaining sample efficiency. A solution considered in existing literature is to embed the high-dimensional space in a lower-dimensional manifold, often via a random linear embedding. In this paper, we identify several crucial issues and misconceptions about the use of linear embeddings for BO. We study the properties of linear embeddings from the literature and show that some of the design choices in current approaches adversely impact their performance. We show empirically that properly addressing these issues significantly improves the efficacy of linear embeddings for BO on a range of problems, including learning a gait policy for robot locomotion.

</details>

<details>

<summary>2020-10-23 02:05:37 - Accelerating Metropolis-Hastings with Lightweight Inference Compilation</summary>

- *Feynman Liang, Nimar Arora, Nazanin Tehrani, Yucen Li, Michael Tingley, Erik Meijer*

- `2010.12128v1` - [abs](http://arxiv.org/abs/2010.12128v1) - [pdf](http://arxiv.org/pdf/2010.12128v1)

> In order to construct accurate proposers for Metropolis-Hastings Markov Chain Monte Carlo, we integrate ideas from probabilistic graphical models and neural networks in an open-source framework we call Lightweight Inference Compilation (LIC). LIC implements amortized inference within an open-universe declarative probabilistic programming language (PPL). Graph neural networks are used to parameterize proposal distributions as functions of Markov blankets, which during "compilation" are optimized to approximate single-site Gibbs sampling distributions. Unlike prior work in inference compilation (IC), LIC forgoes importance sampling of linear execution traces in favor of operating directly on Bayesian networks. Through using a declarative PPL, the Markov blankets of nodes (which may be non-static) are queried at inference-time to produce proposers Experimental results show LIC can produce proposers which have less parameters, greater robustness to nuisance random variables, and improved posterior sampling in a Bayesian logistic regression and $n$-schools inference application.

</details>

<details>

<summary>2020-10-23 04:20:57 - Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization</summary>

- *Samuel Daulton, Maximilian Balandat, Eytan Bakshy*

- `2006.05078v3` - [abs](http://arxiv.org/abs/2006.05078v3) - [pdf](http://arxiv.org/pdf/2006.05078v3)

> In many real-world scenarios, decision makers seek to efficiently optimize multiple competing objectives in a sample-efficient fashion. Multi-objective Bayesian optimization (BO) is a common approach, but many of the best-performing acquisition functions do not have known analytic gradients and suffer from high computational overhead. We leverage recent advances in programming models and hardware acceleration for multi-objective BO using Expected Hypervolume Improvement (EHVI)---an algorithm notorious for its high computational complexity. We derive a novel formulation of q-Expected Hypervolume Improvement (qEHVI), an acquisition function that extends EHVI to the parallel, constrained evaluation setting. qEHVI is an exact computation of the joint EHVI of q new candidate points (up to Monte-Carlo (MC) integration error). Whereas previous EHVI formulations rely on gradient-free acquisition optimization or approximated gradients, we compute exact gradients of the MC estimator via auto-differentiation, thereby enabling efficient and effective optimization using first-order and quasi-second-order methods. Our empirical evaluation demonstrates that qEHVI is computationally tractable in many practical scenarios and outperforms state-of-the-art multi-objective BO algorithms at a fraction of their wall time.

</details>

<details>

<summary>2020-10-23 08:55:05 - Domain Adaptation as a Problem of Inference on Graphical Models</summary>

- *Kun Zhang, Mingming Gong, Petar Stojanov, Biwei Huang, Qingsong Liu, Clark Glymour*

- `2002.03278v4` - [abs](http://arxiv.org/abs/2002.03278v4) - [pdf](http://arxiv.org/pdf/2002.03278v4)

> This paper is concerned with data-driven unsupervised domain adaptation, where it is unknown in advance how the joint distribution changes across domains, i.e., what factors or modules of the data distribution remain invariant or change across domains. To develop an automated way of domain adaptation with multiple source domains, we propose to use a graphical model as a compact way to encode the change property of the joint distribution, which can be learned from data, and then view domain adaptation as a problem of Bayesian inference on the graphical models. Such a graphical model distinguishes between constant and varied modules of the distribution and specifies the properties of the changes across domains, which serves as prior knowledge of the changing modules for the purpose of deriving the posterior of the target variable $Y$ in the target domain. This provides an end-to-end framework of domain adaptation, in which additional knowledge about how the joint distribution changes, if available, can be directly incorporated to improve the graphical representation. We discuss how causality-based domain adaptation can be put under this umbrella. Experimental results on both synthetic and real data demonstrate the efficacy of the proposed framework for domain adaptation. The code is available at https://github.com/mgong2/DA_Infer .

</details>

<details>

<summary>2020-10-23 09:47:00 - Forecasting With Factor-Augmented Quantile Autoregressions: A Model Averaging Approach</summary>

- *Anthoulla Phella*

- `2010.12263v1` - [abs](http://arxiv.org/abs/2010.12263v1) - [pdf](http://arxiv.org/pdf/2010.12263v1)

> This paper considers forecasts of the growth and inflation distributions of the United Kingdom with factor-augmented quantile autoregressions under a model averaging framework. We investigate model combinations across models using weights that minimise the Akaike Information Criterion (AIC), the Bayesian Information Criterion (BIC), the Quantile Regression Information Criterion (QRIC) as well as the leave-one-out cross validation criterion. The unobserved factors are estimated by principal components of a large panel with N predictors over T periods under a recursive estimation scheme. We apply the aforementioned methods to the UK GDP growth and CPI inflation rate. We find that, on average, for GDP growth, in terms of coverage and final prediction error, the equal weights or the weights obtained by the AIC and BIC perform equally well but are outperformed by the QRIC and the Jackknife approach on the majority of the quantiles of interest. In contrast, the naive QAR(1) model of inflation outperforms all model averaging methodologies.

</details>

<details>

<summary>2020-10-23 11:00:22 - Richter b-value maps from local moments of seismicity</summary>

- *M. Holschneider, K. Ferrat, G. Zöller, Ch. Molkenthin*

- `2010.12298v1` - [abs](http://arxiv.org/abs/2010.12298v1) - [pdf](http://arxiv.org/pdf/2010.12298v1)

> We develop a technique to estimate spatially varying seismicity patterns. It is based on a Gaussian approximation of the underlying Poisson Process. A link function is used to estimate local moments of the seismicity from observed catalogues. These are modeled by a nonstationary Gaussian field. We construct a prior based on the local distribution of seismic faults. This allows us to incorporate geological information into the Bayesian inversion of the observed seismicity. In this paper we limit ourselve to the $b$-value field for which we compute the posterior expectations as well as the uncertainties. The technique however may be applied to other seismically relevant parameters like Omori $c$ and $p$-values.

</details>

<details>

<summary>2020-10-23 11:31:38 - Algorithmic recourse under imperfect causal knowledge: a probabilistic approach</summary>

- *Amir-Hossein Karimi, Julius von Kügelgen, Bernhard Schölkopf, Isabel Valera*

- `2006.06831v3` - [abs](http://arxiv.org/abs/2006.06831v3) - [pdf](http://arxiv.org/pdf/2006.06831v3)

> Recent work has discussed the limitations of counterfactual explanations to recommend actions for algorithmic recourse, and argued for the need of taking causal relationships between features into consideration. Unfortunately, in practice, the true underlying structural causal model is generally unknown. In this work, we first show that it is impossible to guarantee recourse without access to the true structural equations. To address this limitation, we propose two probabilistic approaches to select optimal actions that achieve recourse with high probability given limited causal knowledge (e.g., only the causal graph). The first captures uncertainty over structural equations under additive Gaussian noise, and uses Bayesian model averaging to estimate the counterfactual distribution. The second removes any assumptions on the structural equations by instead computing the average effect of recourse actions on individuals similar to the person who seeks recourse, leading to a novel subpopulation-based interventional notion of recourse. We then derive a gradient-based procedure for selecting optimal recourse actions, and empirically show that the proposed approaches lead to more reliable recommendations under imperfect causal knowledge than non-probabilistic baselines.

</details>

<details>

<summary>2020-10-23 15:18:44 - Geostatistical models for zero-inflated data and extreme values</summary>

- *Soraia Pereira, Raquel Menezes, Maria Manuel Angélico, Tiago Marques*

- `2010.12474v1` - [abs](http://arxiv.org/abs/2010.12474v1) - [pdf](http://arxiv.org/pdf/2010.12474v1)

> Understanding the spatial distribution of animals, during all their life phases, as well as how the distributions are influenced by environmental covariates, is a fundamental requirement for the effective management of animal populations. Several geostatistical models have been proposed in the literature, however often the data structure presents an excess of zeros and extreme values, which can lead to unreliable estimates when these are ignored in the modelling process. To deal with these issues, we propose a point-referenced zero-inflated model to model the probability of presence together with the positive observations and a point-referenced generalised Pareto model for the extremes. Finally, we combine the results of these two models to get the spatial predictions of the variable of interest. We follow a Bayesian approach and the inference is made using the package R-INLA in the software R. Our proposed methodology was illustrated through the analysis of the spatial distribution of sardine eggs density (eggs/$m^3$). The results showed that the combined model for zero-inflated and extreme values improved the spatial prediction accuracy. Accordingly, our conclusion is that it is relevant to consider the data structure in the modelling process. Also, the hierarchical model considered can be widely applicable in many ecological problems and even in other contexts.

</details>

<details>

<summary>2020-10-23 16:42:35 - The Wasserstein Impact Measure (WIM): a generally applicable, practical tool for quantifying prior impact in Bayesian statistics</summary>

- *Fatemeh Ghaderinezhad, Christophe Ley, Ben Serrien*

- `2010.12522v1` - [abs](http://arxiv.org/abs/2010.12522v1) - [pdf](http://arxiv.org/pdf/2010.12522v1)

> The prior distribution is a crucial building block in Bayesian analysis, and its choice will impact the subsequent inference. It is therefore important to have a convenient way to quantify this impact, as such a measure of prior impact will help us to choose between two or more priors in a given situation. A recently proposed approach consists in determining the Wasserstein distance between posteriors resulting from two distinct priors, revealing how close or distant they are. In particular, if one prior is the uniform/flat prior, this distance leads to a genuine measure of prior impact for the other prior. While highly appealing and successful from a theoretical viewpoint, this proposal suffers from practical limitations: it requires prior distributions to be nested, posterior distributions should not be of a too complex form, in most considered settings the exact distance was not computed but sharp upper and lower bounds were proposed, and the proposal so far is restricted to scalar parameter settings. In this paper, we overcome all these limitations by introducing a practical version of this theoretical approach, namely the Wasserstein Impact Measure (WIM). In three simulated scenarios, we will compare the WIM to the theoretical Wasserstein approach, as well as to two competitor prior impact measures from the literature. We finally illustrate the versatility of the WIM by applying it on two datasets.

</details>

<details>

<summary>2020-10-23 17:16:49 - On the Expressiveness of Approximate Inference in Bayesian Neural Networks</summary>

- *Andrew Y. K. Foong, David R. Burt, Yingzhen Li, Richard E. Turner*

- `1909.00719v4` - [abs](http://arxiv.org/abs/1909.00719v4) - [pdf](http://arxiv.org/pdf/1909.00719v4)

> While Bayesian neural networks (BNNs) hold the promise of being flexible, well-calibrated statistical models, inference often requires approximations whose consequences are poorly understood. We study the quality of common variational methods in approximating the Bayesian predictive distribution. For single-hidden layer ReLU BNNs, we prove a fundamental limitation in function-space of two of the most commonly used distributions defined in weight-space: mean-field Gaussian and Monte Carlo dropout. We find there are simple cases where neither method can have substantially increased uncertainty in between well-separated regions of low uncertainty. We provide strong empirical evidence that exact inference does not have this pathology, hence it is due to the approximation and not the model. In contrast, for deep networks, we prove a universality result showing that there exist approximate posteriors in the above classes which provide flexible uncertainty estimates. However, we find empirically that pathologies of a similar form as in the single-hidden layer case can persist when performing variational inference in deeper networks. Our results motivate careful consideration of the implications of approximate inference methods in BNNs.

</details>

<details>

<summary>2020-10-24 06:30:45 - Private Outsourced Bayesian Optimization</summary>

- *Dmitrii Kharkovskii, Zhongxiang Dai, Bryan Kian Hsiang Low*

- `2010.12799v1` - [abs](http://arxiv.org/abs/2010.12799v1) - [pdf](http://arxiv.org/pdf/2010.12799v1)

> This paper presents the private-outsourced-Gaussian process-upper confidence bound (PO-GP-UCB) algorithm, which is the first algorithm for privacy-preserving Bayesian optimization (BO) in the outsourced setting with a provable performance guarantee. We consider the outsourced setting where the entity holding the dataset and the entity performing BO are represented by different parties, and the dataset cannot be released non-privately. For example, a hospital holds a dataset of sensitive medical records and outsources the BO task on this dataset to an industrial AI company. The key idea of our approach is to make the BO performance of our algorithm similar to that of non-private GP-UCB run using the original dataset, which is achieved by using a random projection-based transformation that preserves both privacy and the pairwise distances between inputs. Our main theoretical contribution is to show that a regret bound similar to that of the standard GP-UCB algorithm can be established for our PO-GP-UCB algorithm. We empirically evaluate the performance of our PO-GP-UCB algorithm with synthetic and real-world datasets.

</details>

<details>

<summary>2020-10-24 11:53:00 - Variational Bayesian Unlearning</summary>

- *Quoc Phong Nguyen, Bryan Kian Hsiang Low, Patrick Jaillet*

- `2010.12883v1` - [abs](http://arxiv.org/abs/2010.12883v1) - [pdf](http://arxiv.org/pdf/2010.12883v1)

> This paper studies the problem of approximately unlearning a Bayesian model from a small subset of the training data to be erased. We frame this problem as one of minimizing the Kullback-Leibler divergence between the approximate posterior belief of model parameters after directly unlearning from erased data vs. the exact posterior belief from retraining with remaining data. Using the variational inference (VI) framework, we show that it is equivalent to minimizing an evidence upper bound which trades off between fully unlearning from erased data vs. not entirely forgetting the posterior belief given the full data (i.e., including the remaining data); the latter prevents catastrophic unlearning that can render the model useless. In model training with VI, only an approximate (instead of exact) posterior belief given the full data can be obtained, which makes unlearning even more challenging. We propose two novel tricks to tackle this challenge. We empirically demonstrate our unlearning methods on Bayesian models such as sparse Gaussian process and logistic regression using synthetic and real-world datasets.

</details>

<details>

<summary>2020-10-24 12:10:27 - Nearly Optimal Variational Inference for High Dimensional Regression with Shrinkage Priors</summary>

- *Jincheng Bai, Qifan Song, Guang Cheng*

- `2010.12887v1` - [abs](http://arxiv.org/abs/2010.12887v1) - [pdf](http://arxiv.org/pdf/2010.12887v1)

> We propose a variational Bayesian (VB) procedure for high-dimensional linear model inferences with heavy tail shrinkage priors, such as student-t prior. Theoretically, we establish the consistency of the proposed VB method and prove that under the proper choice of prior specifications, the contraction rate of the VB posterior is nearly optimal. It justifies the validity of VB inference as an alternative of Markov Chain Monte Carlo (MCMC) sampling. Meanwhile, comparing to conventional MCMC methods, the VB procedure achieves much higher computational efficiency, which greatly alleviates the computing burden for modern machine learning applications such as massive data analysis. Through numerical studies, we demonstrate that the proposed VB method leads to shorter computing time, higher estimation accuracy, and lower variable selection error than competitive sparse Bayesian methods.

</details>

<details>

<summary>2020-10-24 13:28:11 - Is SGD a Bayesian sampler? Well, almost</summary>

- *Chris Mingard, Guillermo Valle-Pérez, Joar Skalse, Ard A. Louis*

- `2006.15191v2` - [abs](http://arxiv.org/abs/2006.15191v2) - [pdf](http://arxiv.org/pdf/2006.15191v2)

> Overparameterised deep neural networks (DNNs) are highly expressive and so can, in principle, generate almost any function that fits a training dataset with zero error. The vast majority of these functions will perform poorly on unseen data, and yet in practice DNNs often generalise remarkably well. This success suggests that a trained DNN must have a strong inductive bias towards functions with low generalisation error. Here we empirically investigate this inductive bias by calculating, for a range of architectures and datasets, the probability $P_{SGD}(f\mid S)$ that an overparameterised DNN, trained with stochastic gradient descent (SGD) or one of its variants, converges on a function $f$ consistent with a training set $S$. We also use Gaussian processes to estimate the Bayesian posterior probability $P_B(f\mid S)$ that the DNN expresses $f$ upon random sampling of its parameters, conditioned on $S$.   Our main findings are that $P_{SGD}(f\mid S)$ correlates remarkably well with $P_B(f\mid S)$ and that $P_B(f\mid S)$ is strongly biased towards low-error and low complexity functions. These results imply that strong inductive bias in the parameter-function map (which determines $P_B(f\mid S)$), rather than a special property of SGD, is the primary explanation for why DNNs generalise so well in the overparameterised regime.   While our results suggest that the Bayesian posterior $P_B(f\mid S)$ is the first order determinant of $P_{SGD}(f\mid S)$, there remain second order differences that are sensitive to hyperparameter tuning. A function probability picture, based on $P_{SGD}(f\mid S)$ and/or $P_B(f\mid S)$, can shed new light on the way that variations in architecture or hyperparameter settings such as batch size, learning rate, and optimiser choice, affect DNN performance.

</details>

<details>

<summary>2020-10-24 16:51:14 - Bayesian Deep Ensembles via the Neural Tangent Kernel</summary>

- *Bobby He, Balaji Lakshminarayanan, Yee Whye Teh*

- `2007.05864v2` - [abs](http://arxiv.org/abs/2007.05864v2) - [pdf](http://arxiv.org/pdf/2007.05864v2)

> We explore the link between deep ensembles and Gaussian processes (GPs) through the lens of the Neural Tangent Kernel (NTK): a recent development in understanding the training dynamics of wide neural networks (NNs). Previous work has shown that even in the infinite width limit, when NNs become GPs, there is no GP posterior interpretation to a deep ensemble trained with squared error loss. We introduce a simple modification to standard deep ensembles training, through addition of a computationally-tractable, randomised and untrainable function to each ensemble member, that enables a posterior interpretation in the infinite width limit. When ensembled together, our trained NNs give an approximation to a posterior predictive distribution, and we prove that our Bayesian deep ensembles make more conservative predictions than standard deep ensembles in the infinite width limit. Finally, using finite width NNs we demonstrate that our Bayesian deep ensembles faithfully emulate the analytic posterior predictive when available, and can outperform standard deep ensembles in various out-of-distribution settings, for both regression and classification tasks.

</details>

<details>

<summary>2020-10-24 21:41:21 - Implicit Variational Inference: the Parameter and the Predictor Space</summary>

- *Yann Pequignot, Mathieu Alain, Patrick Dallaire, Alireza Yeganehparast, Pascal Germain, Josée Desharnais, François Laviolette*

- `2010.12995v1` - [abs](http://arxiv.org/abs/2010.12995v1) - [pdf](http://arxiv.org/pdf/2010.12995v1)

> Having access to accurate confidence levels along with the predictions allows to determine whether making a decision is worth the risk. Under the Bayesian paradigm, the posterior distribution over parameters is used to capture model uncertainty, a valuable information that can be translated into predictive uncertainty. However, computing the posterior distribution for high capacity predictors, such as neural networks, is generally intractable, making approximate methods such as variational inference a promising alternative. While most methods perform inference in the space of parameters, we explore the benefits of carrying inference directly in the space of predictors. Relying on a family of distributions given by a deep generative neural network, we present two ways of carrying variational inference: one in \emph{parameter space}, one in \emph{predictor space}. Importantly, the latter requires us to choose a distribution of inputs, therefore allowing us at the same time to explicitly address the question of \emph{out-of-distribution} uncertainty. We explore from various perspectives the implications of working in the predictor space induced by neural networks as opposed to the parameter space, focusing mainly on the quality of uncertainty estimation for data lying outside of the training distribution. We compare posterior approximations obtained with these two methods to several standard methods and present results showing that variational approximations learned in the predictor space distinguish themselves positively from those trained in the parameter space.

</details>

<details>

<summary>2020-10-25 04:28:55 - Handling the Positive-Definite Constraint in the Bayesian Learning Rule</summary>

- *Wu Lin, Mark Schmidt, Mohammad Emtiyaz Khan*

- `2002.10060v13` - [abs](http://arxiv.org/abs/2002.10060v13) - [pdf](http://arxiv.org/pdf/2002.10060v13)

> The Bayesian learning rule is a natural-gradient variational inference method, which not only contains many existing learning algorithms as special cases but also enables the design of new algorithms. Unfortunately, when variational parameters lie in an open constraint set, the rule may not satisfy the constraint and requires line-searches which could slow down the algorithm. In this work, we address this issue for positive-definite constraints by proposing an improved rule that naturally handles the constraints. Our modification is obtained by using Riemannian gradient methods, and is valid when the approximation attains a \emph{block-coordinate natural parameterization} (e.g., Gaussian distributions and their mixtures). We propose a principled way to derive Riemannian gradients and retractions from scratch. Our method outperforms existing methods without any significant increase in computation. Our work makes it easier to apply the rule in the presence of positive-definite constraints in parameter spaces.

</details>

<details>

<summary>2020-10-25 05:15:13 - Statistical optimality and stability of tangent transform algorithms in logit models</summary>

- *Indrajit Ghosh, Anirban Bhattacharya, Debdeep Pati*

- `2010.13039v1` - [abs](http://arxiv.org/abs/2010.13039v1) - [pdf](http://arxiv.org/pdf/2010.13039v1)

> A systematic approach to finding variational approximation in an otherwise intractable non-conjugate model is to exploit the general principle of convex duality by minorizing the marginal likelihood that renders the problem tractable. While such approaches are popular in the context of variational inference in non-conjugate Bayesian models, theoretical guarantees on statistical optimality and algorithmic convergence are lacking. Focusing on logistic regression models, we provide mild conditions on the data generating process to derive non-asymptotic upper bounds to the risk incurred by the variational optima. We demonstrate that these assumptions can be completely relaxed if one considers a slight variation of the algorithm by raising the likelihood to a fractional power. Next, we utilize the theory of dynamical systems to provide convergence guarantees for such algorithms in logistic and multinomial logit regression. In particular, we establish local asymptotic stability of the algorithm without any assumptions on the data-generating process. We explore a special case involving a semi-orthogonal design under which a global convergence is obtained. The theory is further illustrated using several numerical studies.

</details>

<details>

<summary>2020-10-25 22:30:00 - Hamiltonian Monte Carlo using an adjoint-differentiated Laplace approximation: Bayesian inference for latent Gaussian models and beyond</summary>

- *Charles C. Margossian, Aki Vehtari, Daniel Simpson, Raj Agrawal*

- `2004.12550v4` - [abs](http://arxiv.org/abs/2004.12550v4) - [pdf](http://arxiv.org/pdf/2004.12550v4)

> Gaussian latent variable models are a key class of Bayesian hierarchical models with applications in many fields. Performing Bayesian inference on such models can be challenging as Markov chain Monte Carlo algorithms struggle with the geometry of the resulting posterior distribution and can be prohibitively slow. An alternative is to use a Laplace approximation to marginalize out the latent Gaussian variables and then integrate out the remaining hyperparameters using dynamic Hamiltonian Monte Carlo, a gradient-based Markov chain Monte Carlo sampler. To implement this scheme efficiently, we derive a novel adjoint method that propagates the minimal information needed to construct the gradient of the approximate marginal likelihood. This strategy yields a scalable differentiation method that is orders of magnitude faster than state of the art differentiation techniques when the hyperparameters are high dimensional. We prototype the method in the probabilistic programming framework Stan and test the utility of the embedded Laplace approximation on several models, including one where the dimension of the hyperparameter is $\sim$6,000. Depending on the cases, the benefits can include an alleviation of the geometric pathologies that frustrate Hamiltonian Monte Carlo and a dramatic speed-up.

</details>

<details>

<summary>2020-10-26 02:56:53 - Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness</summary>

- *Jeremiah Zhe Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, Balaji Lakshminarayanan*

- `2006.10108v2` - [abs](http://arxiv.org/abs/2006.10108v2) - [pdf](http://arxiv.org/pdf/2006.10108v2)

> Bayesian neural networks (BNN) and deep ensembles are principled approaches to estimate the predictive uncertainty of a deep learning model. However their practicality in real-time, industrial-scale applications are limited due to their heavy memory and inference cost. This motivates us to study principled approaches to high-quality uncertainty estimation that require only a single deep neural network (DNN). By formalizing the uncertainty quantification as a minimax learning problem, we first identify input distance awareness, i.e., the model's ability to quantify the distance of a testing example from the training data in the input space, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs, by adding a weight normalization step during training and replacing the output layer with a Gaussian process. On a suite of vision and language understanding tasks and on modern architectures (Wide-ResNet and BERT), SNGP is competitive with deep ensembles in prediction, calibration and out-of-domain detection, and outperforms the other single-model approaches.

</details>

<details>

<summary>2020-10-26 03:16:54 - Scalable Bayesian Optimization with Sparse Gaussian Process Models</summary>

- *Ang Yang*

- `2010.13301v1` - [abs](http://arxiv.org/abs/2010.13301v1) - [pdf](http://arxiv.org/pdf/2010.13301v1)

> This thesis focuses on Bayesian optimization with the improvements coming from two aspects:(i) the use of derivative information to accelerate the optimization convergence; and (ii) the consideration of scalable GPs for handling massive data.

</details>

<details>

<summary>2020-10-26 06:18:08 - A Comprehensive Overview and Survey of Recent Advances in Meta-Learning</summary>

- *Huimin Peng*

- `2004.11149v7` - [abs](http://arxiv.org/abs/2004.11149v7) - [pdf](http://arxiv.org/pdf/2004.11149v7)

> This article reviews meta-learning also known as learning-to-learn which seeks rapid and accurate model adaptation to unseen tasks with applications in highly automated AI, few-shot learning, natural language processing and robotics. Unlike deep learning, meta-learning can be applied to few-shot high-dimensional datasets and considers further improving model generalization to unseen tasks. Deep learning is focused upon in-sample prediction and meta-learning concerns model adaptation for out-of-sample prediction. Meta-learning can continually perform self-improvement to achieve highly autonomous AI. Meta-learning may serve as an additional generalization block complementary for original deep learning model. Meta-learning seeks adaptation of machine learning models to unseen tasks which are vastly different from trained tasks. Meta-learning with coevolution between agent and environment provides solutions for complex tasks unsolvable by training from scratch. Meta-learning methodology covers a wide range of great minds and thoughts. We briefly introduce meta-learning methodologies in the following categories: black-box meta-learning, metric-based meta-learning, layered meta-learning and Bayesian meta-learning framework. Recent applications concentrate upon the integration of meta-learning with other machine learning framework to provide feasible integrated problem solutions. We briefly present recent meta-learning advances and discuss potential future research directions.

</details>

<details>

<summary>2020-10-26 09:47:39 - BayCANN: Streamlining Bayesian Calibration with Artificial Neural Network Metamodeling</summary>

- *Hawre Jalal, Fernando Alarid-Escudero*

- `2010.13452v1` - [abs](http://arxiv.org/abs/2010.13452v1) - [pdf](http://arxiv.org/pdf/2010.13452v1)

> Purpose: Bayesian calibration is theoretically superior to standard direct-search algorithm because it can reveal the full joint posterior distribution of the calibrated parameters. However, to date, Bayesian calibration has not been used often in health decision sciences due to practical and computational burdens. In this paper we propose to use artificial neural networks (ANN) as one solution to these limitations.   Methods: Bayesian Calibration using Artificial Neural Networks (BayCANN) involves (1) training an ANN metamodel on a sample of model inputs and outputs, and (2) then calibrating the trained ANN metamodel instead of the full model in a probabilistic programming language to obtain the posterior joint distribution of the calibrated parameters. We demonstrate BayCANN by calibrating a natural history model of colorectal cancer to adenoma prevalence and cancer incidence data. In addition, we compare the efficiency and accuracy of BayCANN against performing a Bayesian calibration directly on the simulation model using an incremental mixture importance sampling (IMIS) algorithm.   Results: BayCANN was generally more accurate than IMIS in recovering the "true" parameter values. The ratio of the absolute ANN deviation from the truth compared to IMIS for eight out of the nine calibrated parameters were less than one indicating that BayCANN was more accurate than IMIS. In addition, BayCANN took about 15 minutes total compared to the IMIS method which took 80 minutes.   Conclusions: In our case study, BayCANN was more accurate than IMIS and was five-folds faster. Because BayCANN does not depend on the structure of the simulation model, it can be adapted to models of various levels of complexity with minor changes to its structure. We provide BayCANN's open-source implementation in R.

</details>

<details>

<summary>2020-10-26 09:53:06 - Robust Bayesian Inference for Discrete Outcomes with the Total Variation Distance</summary>

- *Jeremias Knoblauch, Lara Vomfell*

- `2010.13456v1` - [abs](http://arxiv.org/abs/2010.13456v1) - [pdf](http://arxiv.org/pdf/2010.13456v1)

> Models of discrete-valued outcomes are easily misspecified if the data exhibit zero-inflation, overdispersion or contamination. Without additional knowledge about the existence and nature of this misspecification, model inference and prediction are adversely affected. Here, we introduce a robust discrepancy-based Bayesian approach using the Total Variation Distance (TVD). In the process, we address and resolve two challenges: First, we study convergence and robustness properties of a computationally efficient estimator for the TVD between a parametric model and the data-generating mechanism. Second, we provide an efficient inference method adapted from Lyddon et al. (2019) which corresponds to formulating an uninformative nonparametric prior directly over the data-generating mechanism. Lastly, we empirically demonstrate that our approach is robust and significantly improves predictive performance on a range of simulated and real world data.

</details>

<details>

<summary>2020-10-26 11:45:19 - Scalable Bayesian neural networks by layer-wise input augmentation</summary>

- *Trung Trinh, Samuel Kaski, Markus Heinonen*

- `2010.13498v1` - [abs](http://arxiv.org/abs/2010.13498v1) - [pdf](http://arxiv.org/pdf/2010.13498v1)

> We introduce implicit Bayesian neural networks, a simple and scalable approach for uncertainty representation in deep learning. Standard Bayesian approach to deep learning requires the impractical inference of the posterior distribution over millions of parameters. Instead, we propose to induce a distribution that captures the uncertainty over neural networks by augmenting each layer's inputs with latent variables. We present appropriate input distributions and demonstrate state-of-the-art performance in terms of calibration, robustness and uncertainty characterisation over large-scale, multi-million parameter image classification tasks.

</details>

<details>

<summary>2020-10-26 11:55:32 - Continuous-Time Birth-Death MCMC for Bayesian Regression Tree Models</summary>

- *Reza Mohammadi, Matthew Pratola, Maurits Kaptein*

- `1904.09339v2` - [abs](http://arxiv.org/abs/1904.09339v2) - [pdf](http://arxiv.org/pdf/1904.09339v2)

> Decision trees are flexible models that are well suited for many statistical regression problems. In a Bayesian framework for regression trees, Markov Chain Monte Carlo (MCMC) search algorithms are required to generate samples of tree models according to their posterior probabilities. The critical component of such an MCMC algorithm is to construct good Metropolis-Hastings steps for updating the tree topology. However, such algorithms frequently suffering from local mode stickiness and poor mixing. As a result, the algorithms are slow to converge. Hitherto, authors have primarily used discrete-time birth/death mechanisms for Bayesian (sums of) regression tree models to explore the model space. These algorithms are efficient only if the acceptance rate is high which is not always the case. Here we overcome this issue by developing a new search algorithm which is based on a continuous-time birth-death Markov process. This search algorithm explores the model space by jumping between parameter spaces corresponding to different tree structures. In the proposed algorithm, the moves between models are always accepted which can dramatically improve the convergence and mixing properties of the MCMC algorithm. We provide theoretical support of the algorithm for Bayesian regression tree models and demonstrate its performance.

</details>

<details>

<summary>2020-10-26 12:08:15 - Measure Transport with Kernel Stein Discrepancy</summary>

- *Matthew A. Fisher, Tui Nolan, Matthew M. Graham, Dennis Prangle, Chris J. Oates*

- `2010.11779v2` - [abs](http://arxiv.org/abs/2010.11779v2) - [pdf](http://arxiv.org/pdf/2010.11779v2)

> Measure transport underpins several recent algorithms for posterior approximation in the Bayesian context, wherein a transport map is sought to minimise the Kullback--Leibler divergence (KLD) from the posterior to the approximation. The KLD is a strong mode of convergence, requiring absolute continuity of measures and placing restrictions on which transport maps can be permitted. Here we propose to minimise a kernel Stein discrepancy (KSD) instead, requiring only that the set of transport maps is dense in an $L^2$ sense and demonstrating how this condition can be validated. The consistency of the associated posterior approximation is established and empirical results suggest that KSD is competitive and more flexible alternative to KLD for measure transport.

</details>

<details>

<summary>2020-10-26 13:59:38 - Function Optimization with Posterior Gaussian Derivative Process</summary>

- *Sucharita Roy, Sourabh Bhattacharya*

- `2010.13591v1` - [abs](http://arxiv.org/abs/2010.13591v1) - [pdf](http://arxiv.org/pdf/2010.13591v1)

> In this article, we propose and develop a novel Bayesian algorithm for optimization of functions whose first and second partial derivatives are known. The basic premise is the Gaussian process representation of the function which induces a first derivative process that is also Gaussian. The Bayesian posterior solutions of the derivative process set equal to zero, given data consisting of suitable choices of input points in the function domain and their function values, emulate the stationary points of the function, which can be fine-tuned by setting restrictions on the prior in terms of the first and second derivatives of the objective function. These observations motivate us to propose a general and effective algorithm for function optimization that attempts to get closer to the true optima adaptively with in-built iterative stages. We provide theoretical foundation to this algorithm, proving almost sure convergence to the true optima as the number of iterative stages tends to infinity. The theoretical foundation hinges upon our proofs of almost sure uniform convergence of the posteriors associated with Gaussian and Gaussian derivative processes to the underlying function and its derivatives in appropriate fixed-domain infill asymptotics setups; rates of convergence are also available. We also provide Bayesian characterization of the number of optima using information inherent in our optimization algorithm. We illustrate our Bayesian optimization algorithm with five different examples involving maxima, minima, saddle points and even inconclusiveness. Our examples range from simple, one-dimensional problems to challenging 50 and 100-dimensional problems.

</details>

<details>

<summary>2020-10-26 16:19:57 - Cognitive Constructivism and the Epistemic Significance of Sharp Statistical Hypotheses in Natural Sciences</summary>

- *J. M. Stern*

- `1006.5471v8` - [abs](http://arxiv.org/abs/1006.5471v8) - [pdf](http://arxiv.org/pdf/1006.5471v8)

> This book presents our case in defense of a constructivist epistemological framework and the use of compatible statistical theory and inference tools. The basic metaphor of decision theory is the maximization of a gambler's expected fortune, according to his own subjective utility, prior beliefs an learned experiences. This metaphor has proven to be very useful, leading the development of Bayesian statistics since its XX-th century revival, rooted on the work of de Finetti, Savage and others. The basic metaphor presented in this text, as a foundation for cognitive constructivism, is that of an eigen-solution, and the verification of its objective epistemic status. The FBST - Full Bayesian Significance Test - is the cornerstone of a set of statistical tolls conceived to assess the epistemic value of such eigen-solutions, according to their four essential attributes, namely, sharpness, stability, separability and composability. We believe that this alternative perspective, complementary to the one ofered by decision theory, can provide powerful insights and make pertinent contributions in the context of scientific research.

</details>

<details>

<summary>2020-10-26 17:24:20 - Fast classification rates without standard margin assumptions</summary>

- *Olivier Bousquet, Nikita Zhivotovskiy*

- `1910.12756v2` - [abs](http://arxiv.org/abs/1910.12756v2) - [pdf](http://arxiv.org/pdf/1910.12756v2)

> We consider the classical problem of learning rates for classes with finite VC dimension. It is well known that fast learning rates up to $O\left(\frac{d}{n}\right)$ are achievable by the empirical risk minimization algorithm (ERM) if low noise or margin assumptions are satisfied. These usually require the optimal Bayes classifier to be in the class, and it has been shown that when this is not the case, the fast rates cannot be achieved even in the noise free case. In this paper, we further investigate the question of the fast rates under the misspecification, when the Bayes classifier is not in the class (also called the agnostic setting).   First, we consider classification with a reject option, namely Chow's reject option model, and show that by slightly lowering the impact of hard instances, a learning rate of order $O\left(\frac{d}{n}\log \frac{n}{d}\right)$ is always achievable in the agnostic setting by a specific learning algorithm. Similar results were only known under special versions of margin assumptions. We also show that the performance of the proposed algorithm is never worse than the performance of ERM.   Based on those results, we derive the necessary and sufficient conditions for classification (without a reject option) with fast rates in the agnostic setting achievable by improper learners. This simultaneously extends the work of Massart and N\'{e}d\'{e}lec (Ann. of Statistics, 2006), which studied this question in the case where the Bayesian optimal rule belongs to the class, and the work of Ben-David and Urner (COLT, 2014), which allows the misspecification but is limited to the no noise setting. Our result also provides the first general setup in statistical learning theory in which an improper learning algorithm may significantly improve the learning rate for non-convex losses.

</details>

<details>

<summary>2020-10-26 17:39:05 - Meaningful uncertainties from deep neural network surrogates of large-scale numerical simulations</summary>

- *Gemma J. Anderson, Jim A. Gaffney, Brian K. Spears, Peer-Timo Bremer, Rushil Anirudh, Jayaraman J. Thiagarajan*

- `2010.13749v1` - [abs](http://arxiv.org/abs/2010.13749v1) - [pdf](http://arxiv.org/pdf/2010.13749v1)

> Large-scale numerical simulations are used across many scientific disciplines to facilitate experimental development and provide insights into underlying physical processes, but they come with a significant computational cost. Deep neural networks (DNNs) can serve as highly-accurate surrogate models, with the capacity to handle diverse datatypes, offering tremendous speed-ups for prediction and many other downstream tasks. An important use-case for these surrogates is the comparison between simulations and experiments; prediction uncertainty estimates are crucial for making such comparisons meaningful, yet standard DNNs do not provide them. In this work we define the fundamental requirements for a DNN to be useful for scientific applications, and demonstrate a general variational inference approach to equip predictions of scalar and image data from a DNN surrogate model trained on inertial confinement fusion simulations with calibrated Bayesian uncertainties. Critically, these uncertainties are interpretable, meaningful and preserve physics-correlations in the predicted quantities.

</details>

<details>

<summary>2020-10-26 17:59:11 - Bayesian Multivariate Probability of Success Using Historical Data with Strict Control of Family-wise Error Rate</summary>

- *Ethan M. Alt, Matthew A. Psioda, Joseph G. Ibrahim*

- `2010.13774v1` - [abs](http://arxiv.org/abs/2010.13774v1) - [pdf](http://arxiv.org/pdf/2010.13774v1)

> Given the cost and duration of phase III and phase IV clinical trials, the development of statistical methods for go/no-go decisions is vital. In this paper, we introduce a Bayesian methodology to compute the probability of success based on the current data of a treatment regimen for the multivariate linear model. Our approach utilizes a Bayesian seemingly unrelated regression model, which allows for multiple endpoints to be modeled jointly even if the covariates between the endpoints are different. Correlations between endpoints are explicitly modeled. This Bayesian joint modeling approach unifies single and multiple testing procedures under a single framework. We develop an approach to multiple testing that asymptotically guarantees strict family-wise error rate control, and is more powerful than frequentist approaches to multiplicity. The method effectively yields those of Ibrahim et al. and Chuang-Stein as special cases, and, to our knowledge, is the only method that allows for robust sample size determination for multiple endpoints and/or hypotheses and the only method that provides strict family-wise type I error control in the presence of multiplicity.

</details>

<details>

<summary>2020-10-26 21:58:59 - Bayesian Fusion of Data Partitioned Particle Estimates</summary>

- *Caleb Miller, Michael D. Schneider, Jem N. Corcoran, Jason Bernstein*

- `2010.13921v1` - [abs](http://arxiv.org/abs/2010.13921v1) - [pdf](http://arxiv.org/pdf/2010.13921v1)

> We present a Bayesian data fusion method to approximate a posterior distribution from an ensemble of particle estimates that only have access to subsets of the data. Our approach relies on approximate probabilistic inference of model parameters through Monte Carlo methods, followed by an update and resample scheme related to multiple importance sampling to combine information from the initial estimates. We show the method is convergent in the particle limit and directly suited to application on multi-sensor data fusion problems by demonstrating efficacy on a multi-sensor Keplerian orbit determination problem and a bearings-only tracking problem.

</details>

<details>

<summary>2020-10-27 10:50:08 - Estimation and uncertainty quantification for extreme quantile regions</summary>

- *Boris Beranger, Simone A. Padoan, Scott A. Sisson*

- `1904.08251v4` - [abs](http://arxiv.org/abs/1904.08251v4) - [pdf](http://arxiv.org/pdf/1904.08251v4)

> Estimation of extreme quantile regions, spaces in which future extreme events can occur with a given low probability, even beyond the range of the observed data, is an important task in the analysis of extremes. Existing methods to estimate such regions are available, but do not provide any measures of estimation uncertainty. We develop univariate and bivariate schemes for estimating extreme quantile regions under the Bayesian paradigm that outperforms existing approaches and provides natural measures of quantile region estimate uncertainty. We examine the method's performance in controlled simulation studies. We illustrate the applicability of the proposed method by analysing high bivariate quantiles for pairs of pollutants, conditionally on different temperature gradations, recorded in Milan, Italy.

</details>

<details>

<summary>2020-10-27 11:27:24 - Bayesian Bits: Unifying Quantization and Pruning</summary>

- *Mart van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen Blankevoort, Max Welling*

- `2005.07093v3` - [abs](http://arxiv.org/abs/2005.07093v3) - [pdf](http://arxiv.org/pdf/2005.07093v3)

> We introduce Bayesian Bits, a practical method for joint mixed precision quantization and pruning through gradient based optimization. Bayesian Bits employs a novel decomposition of the quantization operation, which sequentially considers doubling the bit width. At each new bit width, the residual error between the full precision value and the previously rounded value is quantized. We then decide whether or not to add this quantized residual error for a higher effective bit width and lower quantization noise. By starting with a power-of-two bit width, this decomposition will always produce hardware-friendly configurations, and through an additional 0-bit option, serves as a unified view of pruning and quantization. Bayesian Bits then introduces learnable stochastic gates, which collectively control the bit width of the given tensor. As a result, we can obtain low bit solutions by performing approximate inference over the gates, with prior distributions that encourage most of them to be switched off. We experimentally validate our proposed method on several benchmark datasets and show that we can learn pruned, mixed precision networks that provide a better trade-off between accuracy and efficiency than their static bit width equivalents.

</details>

<details>

<summary>2020-10-27 16:35:16 - SubTSBR to tackle high noise and outliers for data-driven discovery of differential equations</summary>

- *Sheng Zhang, Guang Lin*

- `1907.07788v4` - [abs](http://arxiv.org/abs/1907.07788v4) - [pdf](http://arxiv.org/pdf/1907.07788v4)

> Data-driven discovery of differential equations has been an emerging research topic. We propose a novel algorithm subsampling-based threshold sparse Bayesian regression (SubTSBR) to tackle high noise and outliers. The subsampling technique is used for improving the accuracy of the Bayesian learning algorithm. It has two parameters: subsampling size and the number of subsamples. When the subsampling size increases with fixed total sample size, the accuracy of our algorithm goes up and then down. When the number of subsamples increases, the accuracy of our algorithm keeps going up. We demonstrate how to use our algorithm step by step and compare our algorithm with threshold sparse Bayesian regression (TSBR) for the discovery of differential equations. We show that our algorithm produces better results. We also discuss the merits of discovering differential equations from data and demonstrate how to discover models with random initial and boundary condition as well as models with bifurcations. The numerical examples are: (1) predator-prey model with noise, (2) shallow water equations with outliers, (3) heat diffusion with random initial and boundary condition, and (4) fish-harvesting problem with bifurcations.

</details>

<details>

<summary>2020-10-27 18:41:52 - PClean: Bayesian Data Cleaning at Scale with Domain-Specific Probabilistic Programming</summary>

- *Alexander K. Lew, Monica Agrawal, David Sontag, Vikash K. Mansinghka*

- `2007.11838v4` - [abs](http://arxiv.org/abs/2007.11838v4) - [pdf](http://arxiv.org/pdf/2007.11838v4)

> Data cleaning can be naturally framed as probabilistic inference in a generative model, combining a prior distribution over ground-truth databases with a likelihood that models the noisy channel by which the data are filtered and corrupted to yield incomplete, dirty, and denormalized datasets. Based on this view, we present PClean, a probabilistic programming language for leveraging dataset-specific knowledge to clean and normalize dirty data. PClean is powered by three modeling and inference contributions: (1) a non-parametric model of relational database instances, customizable via probabilistic programs, (2) a sequential Monte Carlo inference algorithm that exploits the model's structure, and (3) near-optimal SMC proposals and blocked Gibbs rejuvenation moves constructed on a per-dataset basis. We show empirically that short (< 50-line) PClean programs can be faster and more accurate than generic PPL inference on multiple data-cleaning benchmarks; perform comparably in terms of accuracy and runtime to state-of-the-art data-cleaning systems (unlike generic PPL inference given the same runtime); and scale to real-world datasets with millions of records.

</details>

<details>

<summary>2020-10-27 20:36:08 - Information theoretic limits of learning a sparse rule</summary>

- *Clément Luneau, Jean Barbier, Nicolas Macris*

- `2006.11313v2` - [abs](http://arxiv.org/abs/2006.11313v2) - [pdf](http://arxiv.org/pdf/2006.11313v2)

> We consider generalized linear models in regimes where the number of nonzero components of the signal and accessible data points are sublinear with respect to the size of the signal. We prove a variational formula for the asymptotic mutual information per sample when the system size grows to infinity. This result allows us to derive an expression for the minimum mean-square error (MMSE) of the Bayesian estimator when the signal entries have a discrete distribution with finite support. We find that, for such signals and suitable vanishing scalings of the sparsity and sampling rate, the MMSE is nonincreasing piecewise constant. In specific instances the MMSE even displays an all-or-nothing phase transition, that is, the MMSE sharply jumps from its maximum value to zero at a critical sampling rate. The all-or-nothing phenomenon has previously been shown to occur in high-dimensional linear regression. Our analysis goes beyond the linear case and applies to learning the weights of a perceptron with general activation function in a teacher-student scenario. In particular, we discuss an all-or-nothing phenomenon for the generalization error with a sublinear set of training examples.

</details>

<details>

<summary>2020-10-27 23:16:26 - Integration of AI and mechanistic modeling in generative adversarial networks for stochastic inverse problems</summary>

- *Jaimit Parikh, James Kozloski, Viatcheslav Gurev*

- `2009.08267v2` - [abs](http://arxiv.org/abs/2009.08267v2) - [pdf](http://arxiv.org/pdf/2009.08267v2)

> Stochastic inverse problems (SIP) address the behavior of a set of objects of the same kind but with variable properties, such as a population of cells. Using a population of mechanistic models from a single parametric family, SIP explains population variability by transferring real-world observations into the latent space of model parameters. Previous research in SIP focused on solving the parameter inference problem for a single population using Markov chain Monte Carlo methods. Here we extend SIP to address multiple related populations simultaneously. Specifically, we simulate control and treatment populations in experimental protocols by discovering two related latent spaces of model parameters. Instead of taking a Bayesian approach, our two-population SIP is reformulated as the constrained-optimization problem of finding distributions of model parameters. To minimize the divergence between distributions of experimental observations and model outputs, we developed novel deep learning models based on generative adversarial networks (GANs) which have the structure of our underlying constrained-optimization problem. The flexibility of GANs allowed us to build computationally scalable solutions and tackle complex model input parameter inference scenarios, which appear routinely in physics, biophysics, economics and other areas, and which can not be handled with existing methods. Specifically, we demonstrate two scenarios of parameter inference over a control population and a treatment population whose treatment either selectively affects only a subset of model parameters with some uncertainty or has a deterministic effect on all model parameters.

</details>

<details>

<summary>2020-10-28 00:06:05 - Bayesian Algorithms for Decentralized Stochastic Bandits</summary>

- *Anusha Lalitha, Andrea Goldsmith*

- `2010.10569v2` - [abs](http://arxiv.org/abs/2010.10569v2) - [pdf](http://arxiv.org/pdf/2010.10569v2)

> We study a decentralized cooperative multi-agent multi-armed bandit problem with $K$ arms and $N$ agents connected over a network. In our model, each arm's reward distribution is same for all agents, and rewards are drawn independently across agents and over time steps. In each round, agents choose an arm to play and subsequently send a message to their neighbors. The goal is to minimize cumulative regret averaged over the entire network. We propose a decentralized Bayesian multi-armed bandit framework that extends single-agent Bayesian bandit algorithms to the decentralized setting. Specifically, we study an information assimilation algorithm that can be combined with existing Bayesian algorithms, and using this, we propose a decentralized Thompson Sampling algorithm and decentralized Bayes-UCB algorithm. We analyze the decentralized Thompson Sampling algorithm under Bernoulli rewards and establish a problem-dependent upper bound on the cumulative regret. We show that regret incurred scales logarithmically over the time horizon with constants that match those of an optimal centralized agent with access to all observations across the network. Our analysis also characterizes the cumulative regret in terms of the network structure. Through extensive numerical studies, we show that our extensions of Thompson Sampling and Bayes-UCB incur lesser cumulative regret than the state-of-art algorithms inspired by the Upper Confidence Bound algorithm. We implement our proposed decentralized Thompson Sampling under gossip protocol, and over time-varying networks, where each communication link has a fixed probability of failure.

</details>

<details>

<summary>2020-10-28 10:06:40 - Modeling European regional FDI flows using a Bayesian spatial Poisson interaction model</summary>

- *Tamás Krisztin, Philipp Piribauer*

- `2010.14856v1` - [abs](http://arxiv.org/abs/2010.14856v1) - [pdf](http://arxiv.org/pdf/2010.14856v1)

> This paper presents an empirical study of spatial origin and destination effects of European regional FDI dyads. Recent regional studies primarily focus on locational determinants, but ignore bilateral origin- and intervening factors, as well as associated spatial dependence. This paper fills this gap by using observations on interregional FDI flows within a spatially augmented Poisson interaction model. We explicitly distinguish FDI activities between three different stages of the value chain. Our results provide important insights on drivers of regional FDI activities, both from origin and destination perspectives. We moreover show that spatial dependence plays a key role in both dimensions.

</details>

<details>

<summary>2020-10-28 10:42:04 - Bayesian Methods for Semi-supervised Text Annotation</summary>

- *Kristian Miok, Gregor Pirs, Marko Robnik-Sikonja*

- `2010.14872v1` - [abs](http://arxiv.org/abs/2010.14872v1) - [pdf](http://arxiv.org/pdf/2010.14872v1)

> Human annotations are an important source of information in the development of natural language understanding approaches. As under the pressure of productivity annotators can assign different labels to a given text, the quality of produced annotations frequently varies. This is especially the case if decisions are difficult, with high cognitive load, requires awareness of broader context, or careful consideration of background knowledge. To alleviate the problem, we propose two semi-supervised methods to guide the annotation process: a Bayesian deep learning model and a Bayesian ensemble method. Using a Bayesian deep learning method, we can discover annotations that cannot be trusted and might require reannotation. A recently proposed Bayesian ensemble method helps us to combine the annotators' labels with predictions of trained models. According to the results obtained from three hate speech detection experiments, the proposed Bayesian methods can improve the annotations and prediction performance of BERT models.

</details>

<details>

<summary>2020-10-28 10:48:23 - Inference of a universal social scale and segregation measures using social connectivity kernels</summary>

- *Till Hoffmann, Nick S. Jones*

- `2008.05337v2` - [abs](http://arxiv.org/abs/2008.05337v2) - [pdf](http://arxiv.org/pdf/2008.05337v2)

> How people connect with one another is a fundamental question in the social sciences, and the resulting social networks can have a profound impact on our daily lives. Blau offered a powerful explanation: people connect with one another based on their positions in a social space. Yet a principled measure of social distance, allowing comparison within and between societies, remains elusive. We use the connectivity kernel of conditionally-independent edge models to develop a family of segregation statistics with desirable properties: they offer an intuitive and universal characteristic scale on social space (facilitating comparison across datasets and societies), are applicable to multivariate and mixed node attributes, and capture segregation at the level of individuals, pairs of individuals, and society as a whole. We show that the segregation statistics can induce a metric on Blau space (a space spanned by the attributes of the members of society) and provide maps of two societies. Under a Bayesian paradigm, we infer the parameters of the connectivity kernel from eleven ego-network datasets collected in four surveys in the United Kingdom and United States. The importance of different dimensions of Blau space is similar across time and location, suggesting a macroscopically stable social fabric. Physical separation and age differences have the most significant impact on segregation within friendship networks with implications for intergenerational mixing and isolation in later stages of life.

</details>

<details>

<summary>2020-10-28 16:27:55 - Jump Markov Chains and Rejection-Free Metropolis Algorithms</summary>

- *J. S. Rosenthal, A. Dote, K. Dabiri, H. Tamura, S. Chen, A. Sheikholeslami*

- `1910.13316v3` - [abs](http://arxiv.org/abs/1910.13316v3) - [pdf](http://arxiv.org/pdf/1910.13316v3)

> We consider versions of the Metropolis algorithm which avoid the inefficiency of rejections. We first illustrate that a natural Uniform Selection Algorithm might not converge to the correct distribution. We then analyse the use of Markov jump chains which avoid successive repetitions of the same state. After exploring the properties of jump chains, we show how they can exploit parallelism in computer hardware to produce more efficient samples. We apply our results to the Metropolis algorithm, to Parallel Tempering, to a Bayesian model, to a two-dimensional ferromagnetic 4 x 4 Ising model, and to a pseudo-marginal MCMC algorithm.

</details>

<details>

<summary>2020-10-28 20:13:01 - Space-Time Covid-19 Bayesian SIR modeling in South Carolina</summary>

- *Andrew B. Lawson, Joanne Kim*

- `2010.15207v1` - [abs](http://arxiv.org/abs/2010.15207v1) - [pdf](http://arxiv.org/pdf/2010.15207v1)

> The Covid-19 pandemic has spread across the world since the beginning of 2020. Many regions have experienced its effects. The state of South Carolina in the USA has seen cases since early March 2020 and a primary peak in early April 2020. A lockdown was imposed on April 6th but lifting of restrictions started on April 24th. The daily case and death data as reported by NCHS (deaths) via the New York Times GitHUB repository have been analyzed and approaches to modeling of the data are presented. Prediction is also considered and the role of asymptomatic transmission is assessed as a latent unobserved effect. Two different time periods are examined and one step prediction is provided.

</details>

<details>

<summary>2020-10-29 00:48:55 - The P-T Probability Framework for Semantic Communication, Falsification, Confirmation, and Bayesian Reasoning</summary>

- *Chenguang Lu*

- `2011.00992v1` - [abs](http://arxiv.org/abs/2011.00992v1) - [pdf](http://arxiv.org/pdf/2011.00992v1)

> Many researchers want to unify probability and logic by defining logical probability or probabilistic logic reasonably. This paper tries to unify statistics and logic so that we can use both statistical probability and logical probability at the same time. For this purpose, this paper proposes the P-T probability framework, which is assembled with Shannon's statistical probability framework for communication, Kolmogorov's probability axioms for logical probability, and Zadeh's membership functions used as truth functions. Two kinds of probabilities are connected by an extended Bayes' theorem, with which we can convert a likelihood function and a truth function from one to another. Hence, we can train truth functions (in logic) by sampling distributions (in statistics). This probability framework was developed in the author's long-term studies on semantic information, statistical learning, and color vision. This paper first proposes the P-T probability framework and explains different probabilities in it by its applications to semantic information theory. Then, this framework and the semantic information methods are applied to statistical learning, statistical mechanics, hypothesis evaluation (including falsification), confirmation, and Bayesian reasoning. Theoretical applications illustrate the reasonability and practicability of this framework. This framework is helpful for interpretable AI. To interpret neural networks, we need further study.

</details>

<details>

<summary>2020-10-29 03:42:30 - Subgroup-based Rank-1 Lattice Quasi-Monte Carlo</summary>

- *Yueming Lyu, Yuan Yuan, Ivor W. Tsang*

- `2011.06446v1` - [abs](http://arxiv.org/abs/2011.06446v1) - [pdf](http://arxiv.org/pdf/2011.06446v1)

> Quasi-Monte Carlo (QMC) is an essential tool for integral approximation, Bayesian inference, and sampling for simulation in science, etc. In the QMC area, the rank-1 lattice is important due to its simple operation, and nice properties for point set construction. However, the construction of the generating vector of the rank-1 lattice is usually time-consuming because of an exhaustive computer search. To address this issue, we propose a simple closed-form rank-1 lattice construction method based on group theory. Our method reduces the number of distinct pairwise distance values to generate a more regular lattice. We theoretically prove a lower and an upper bound of the minimum pairwise distance of any non-degenerate rank-1 lattice. Empirically, our methods can generate a near-optimal rank-1 lattice compared with the Korobov exhaustive search regarding the $l_1$-norm and $l_2$-norm minimum distance. Moreover, experimental results show that our method achieves superior approximation performance on benchmark integration test problems and kernel approximation problems.

</details>

<details>

<summary>2020-10-29 08:01:27 - Bayes Extended Estimators for Curved Exponential Families</summary>

- *Michiko Okudo, Fumiyasu Komaki*

- `1906.07514v2` - [abs](http://arxiv.org/abs/1906.07514v2) - [pdf](http://arxiv.org/pdf/1906.07514v2)

> The Bayesian predictive density has complex representation and does not belong to any finite-dimensional statistical model except for in limited situations. In this paper, we introduce its simple approximate representation employing its projection onto a finite-dimensional exponential family. Its theoretical properties are established parallelly to those of the Bayesian predictive density when the model belongs to curved exponential families. It is also demonstrated that the projection asymptotically coincides with the plugin density with the posterior mean of the expectation parameter of the exponential family, which we refer to as the Bayes extended estimator. Information-geometric correspondence indicates that the Bayesian predictive density can be represented as the posterior mean of the infinite-dimensional exponential family. The Kullback--Leibler risk performance of the approximation is demonstrated by numerical simulations and it indicates that the posterior mean of the expectation parameter approaches the Bayesian predictive density as the dimension of the exponential family increases. It also suggests that approximation by projection onto an exponential family of reasonable size is practically advantageous with respect to risk performance and computational cost.

</details>

<details>

<summary>2020-10-29 14:47:17 - Bayesian Shrinkage Estimation of Negative Multinomial Parameter Vectors</summary>

- *Yasuyuki Hamura, Tatsuya Kubokawa*

- `2001.09602v2` - [abs](http://arxiv.org/abs/2001.09602v2) - [pdf](http://arxiv.org/pdf/2001.09602v2)

> The negative multinomial distribution is a multivariate generalization of the negative binomial distribution. In this paper, we consider the problem of estimating an unknown matrix of probabilities on the basis of observations of negative multinomial variables under the standardized squared error loss. First, a general sufficient condition for a shrinkage estimator to dominate the UMVU estimator is derived and an empirical Bayes estimator satisfying the condition is constructed. Next, a hierarchical shrinkage prior is introduced, an associated Bayes estimator is shown to dominate the UMVU estimator under some conditions, and some remarks about posterior computation are presented. Finally, shrinkage estimators and the UMVU estimator are compared by simulation.

</details>

<details>

<summary>2020-10-30 03:05:33 - Representable Markov Categories and Comparison of Statistical Experiments in Categorical Probability</summary>

- *Tobias Fritz, Tomáš Gonda, Paolo Perrone, Eigil Fjeldgren Rischel*

- `2010.07416v2` - [abs](http://arxiv.org/abs/2010.07416v2) - [pdf](http://arxiv.org/pdf/2010.07416v2)

> Markov categories are a recent categorical approach to the mathematical foundations of probability and statistics. Here, this approach is advanced by stating and proving equivalent conditions for second-order stochastic dominance, a widely used way of comparing probability distributions by their spread. Furthermore, we lay foundation for the theory of comparing statistical experiments within Markov categories by stating and proving the classical Blackwell-Sherman-Stein Theorem. Our version not only offers new insight into the proof, but its abstract nature also makes the result more general, automatically specializing to the standard Blackwell-Sherman-Stein Theorem in measure-theoretic probability as well as a Bayesian version that involves prior-dependent garbling. Along the way, we define and characterize representable Markov categories, within which one can talk about Markov kernels to or from spaces of distributions. We do so by exploring the relation between Markov categories and Kleisli categories of probability monads.

</details>

<details>

<summary>2020-10-30 07:09:41 - Inverse Rational Control with Partially Observable Continuous Nonlinear Dynamics</summary>

- *Minhae Kwon, Saurabh Daptardar, Paul Schrater, Xaq Pitkow*

- `2009.12576v2` - [abs](http://arxiv.org/abs/2009.12576v2) - [pdf](http://arxiv.org/pdf/2009.12576v2)

> A fundamental question in neuroscience is how the brain creates an internal model of the world to guide actions using sequences of ambiguous sensory information. This is naturally formulated as a reinforcement learning problem under partial observations, where an agent must estimate relevant latent variables in the world from its evidence, anticipate possible future states, and choose actions that optimize total expected reward. This problem can be solved by control theory, which allows us to find the optimal actions for a given system dynamics and objective function. However, animals often appear to behave suboptimally. Why? We hypothesize that animals have their own flawed internal model of the world, and choose actions with the highest expected subjective reward according to that flawed model. We describe this behavior as rational but not optimal. The problem of Inverse Rational Control (IRC) aims to identify which internal model would best explain an agent's actions. Our contribution here generalizes past work on Inverse Rational Control which solved this problem for discrete control in partially observable Markov decision processes. Here we accommodate continuous nonlinear dynamics and continuous actions, and impute sensory observations corrupted by unknown noise that is private to the animal. We first build an optimal Bayesian agent that learns an optimal policy generalized over the entire model space of dynamics and subjective rewards using deep reinforcement learning. Crucially, this allows us to compute a likelihood over models for experimentally observable action trajectories acquired from a suboptimal agent. We then find the model parameters that maximize the likelihood using gradient ascent.

</details>

<details>

<summary>2020-10-30 15:36:25 - Machine-Learning the Sato--Tate Conjecture</summary>

- *Yang-Hui He, Kyu-Hwan Lee, Thomas Oliver*

- `2010.01213v2` - [abs](http://arxiv.org/abs/2010.01213v2) - [pdf](http://arxiv.org/pdf/2010.01213v2)

> We apply some of the latest techniques from machine-learning to the arithmetic of hyperelliptic curves. More precisely we show that, with impressive accuracy and confidence (between 99 and 100 percent precision), and in very short time (matter of seconds on an ordinary laptop), a Bayesian classifier can distinguish between Sato-Tate groups given a small number of Euler factors for the L-function. Our observations are in keeping with the Sato-Tate conjecture for curves of low genus. For elliptic curves, this amounts to distinguishing generic curves (with Sato-Tate group SU(2)) from those with complex multiplication. In genus 2, a principal component analysis is observed to separate the generic Sato-Tate group USp(4) from the non-generic groups. Furthermore in this case, for which there are many more non-generic possibilities than in the case of elliptic curves, we demonstrate an accurate characterisation of several Sato-Tate groups with the same identity component. Throughout, our observations are verified using known results from the literature and the data available in the LMFDB. The results in this paper suggest that a machine can be trained to learn the Sato-Tate distributions and may be able to classify curves much more efficiently than the methods available in the literature.

</details>

<details>

<summary>2020-10-30 16:54:40 - Industrial Forecasting with Exponentially Smoothed Recurrent Neural Networks</summary>

- *Matthew F Dixon*

- `2004.04717v2` - [abs](http://arxiv.org/abs/2004.04717v2) - [pdf](http://arxiv.org/pdf/2004.04717v2)

> Time series modeling has entered an era of unprecedented growth in the size and complexity of data which require new modeling approaches. While many new general purpose machine learning approaches have emerged, they remain poorly understand and irreconcilable with more traditional statistical modeling approaches. We present a general class of exponential smoothed recurrent neural networks (RNNs) which are well suited to modeling non-stationary dynamical systems arising in industrial applications. In particular, we analyze their capacity to characterize the non-linear partial autocorrelation structure of time series and directly capture dynamic effects such as seasonality and trends. Application of exponentially smoothed RNNs to forecasting electricity load, weather data, and stock prices highlight the efficacy of exponential smoothing of the hidden state for multi-step time series forecasting. The results also suggest that popular, but more complicated neural network architectures originally designed for speech processing, such as LSTMs and GRUs, are likely over-engineered for industrial forecasting and light-weight exponentially smoothed architectures, trained in a fraction of the time, capture the salient features while being superior and more robust than simple RNNs and ARIMA models. Additionally uncertainty quantification of the exponential smoothed recurrent neural networks, provided by Bayesian estimation, is shown to provide improved coverage.

</details>

<details>

<summary>2020-10-30 23:11:51 - Sensor-based localization of epidemic sources on human mobility networks</summary>

- *Jun Li, Juliane Manitz, Enrico Bertuzzo, Eric D. Kolaczyk*

- `2011.00138v1` - [abs](http://arxiv.org/abs/2011.00138v1) - [pdf](http://arxiv.org/pdf/2011.00138v1)

> We investigate the source detection problem in epidemiology, which is one of the most important issues for control of epidemics. Mathematically, we reformulate the problem as one of identifying the relevant component in a multivariate Gaussian mixture model. Focusing on the study of cholera and diseases with similar modes of transmission, we calibrate the parameters of our mixture model using human mobility networks within a stochastic, spatially explicit epidemiological model for waterborne disease. Furthermore, we adopt a Bayesian perspective, so that prior information on source location can be incorporated (e.g., reflecting the impact of local conditions). Posterior-based inference is performed, which permits estimates in the form of either individual locations or regions. Importantly, our estimator only requires first-arrival times of the epidemic by putative observers, typically located only at a small proportion of nodes. The proposed method is demonstrated within the context of the 2000-2002 cholera outbreak in the KwaZulu-Natal province of South Africa.

</details>

<details>

<summary>2020-10-31 11:32:12 - Growth of Random Trees by Leaf Attachment</summary>

- *Nomvelo Sibisi*

- `2010.05589v3` - [abs](http://arxiv.org/abs/2010.05589v3) - [pdf](http://arxiv.org/pdf/2010.05589v3)

> We study the growth of a time-ordered rooted tree by probabilistic attachment of new vertices to leaves. We construct a likelihood function of the leaves based on the connectivity of the tree. We take such connectivity to be induced by the merging of directed ordered paths from leaves to the root. Combining the likelihood with an assigned prior distribution leads to a posterior leaf distribution from which we sample attachment points for new vertices. We present computational examples of such Bayesian tree growth. Although the discussion is generic, the initial motivation for the paper is the concept of a distributed ledger, which may be regarded as a time-ordered random tree that grows by probabilistic leaf attachment.

</details>

<details>

<summary>2020-10-31 17:32:05 - Bayesian Analysis of Static Light Scattering Data for Globular Proteins</summary>

- *Fan Yin, Domarin Khago, Rachel W. Martin, Carter T. Butts*

- `2011.00321v1` - [abs](http://arxiv.org/abs/2011.00321v1) - [pdf](http://arxiv.org/pdf/2011.00321v1)

> Static light scattering is a popular physical chemistry technique that enables calculation of physical attributes such as the radius of gyration and the second virial coefficient for a macromolecule (e.g., a polymer or a protein) in solution. The second virial coefficient is a physical quantity that characterizes the magnitude and sign of pairwise interactions between particles, and hence is related to aggregation propensity, a property of considerable scientific and practical interest. Estimating the second virial coefficient from experimental data is challenging due both to the degree of precision required and the complexity of the error structure involved. In contrast to conventional approaches based on heuristic OLS estimates, Bayesian inference for the second virial coefficient allows explicit modeling of error processes, incorporation of prior information, and the ability to directly test competing physical models. Here, we introduce a fully Bayesian model for static light scattering experiments on small-particle systems, with joint inference for concentration, index of refraction, oligomer size, and the second virial coefficient. We apply our proposed model to study the aggregation behavior of hen egg-white lysozyme and human gammaS-crystallin using in-house experimental data. Based on these observations, we also perform a simulation study on the primary drivers of uncertainty in this family of experiments, showing in particular the potential for improved monitoring and control of concentration to aid inference.

</details>

<details>

<summary>2020-10-31 20:28:35 - Optimizing Ensemble Weights and Hyperparameters of Machine Learning Models for Regression Problems</summary>

- *Mohsen Shahhosseini, Guiping Hu, Hieu Pham*

- `1908.05287v6` - [abs](http://arxiv.org/abs/1908.05287v6) - [pdf](http://arxiv.org/pdf/1908.05287v6)

> Aggregating multiple learners through an ensemble of models aim to make better predictions by capturing the underlying distribution of the data more accurately. Different ensembling methods, such as bagging, boosting, and stacking/blending, have been studied and adopted extensively in research and practice. While bagging and boosting focus more on reducing variance and bias, respectively, stacking approaches target both by finding the optimal way to combine base learners. In stacking with the weighted average, ensembles are created from weighted averages of multiple base learners. It is known that tuning hyperparameters of each base learner inside the ensemble weight optimization process can produce better performing ensembles. To this end, an optimization-based nested algorithm that considers tuning hyperparameters as well as finding the optimal weights to combine ensembles (Generalized Weighted Ensemble with Internally Tuned Hyperparameters (GEM-ITH)) is designed. Besides, Bayesian search was used to speed-up the optimizing process, and a heuristic was implemented to generate diverse and well-performing base learners. The algorithm is shown to be generalizable to real data sets through analyses with ten publicly available data sets.

</details>


## 2020-11

<details>

<summary>2020-11-01 12:10:41 - Nondiagonal Mixture of Dirichlet Network Distributions for Analyzing a Stock Ownership Network</summary>

- *Wenning Zhang, Ryohei Hisano, Takaaki Ohnishi, Takayuki Mizuno*

- `2009.04446v2` - [abs](http://arxiv.org/abs/2009.04446v2) - [pdf](http://arxiv.org/pdf/2009.04446v2)

> Block modeling is widely used in studies on complex networks. The cornerstone model is the stochastic block model (SBM), widely used over the past decades. However, the SBM is limited in analyzing complex networks as the model is, in essence, a random graph model that cannot reproduce the basic properties of many complex networks, such as sparsity and heavy-tailed degree distribution. In this paper, we provide an edge exchangeable block model that incorporates such basic features and simultaneously infers the latent block structure of a given complex network. Our model is a Bayesian nonparametric model that flexibly estimates the number of blocks and takes into account the possibility of unseen nodes. Using one synthetic dataset and one real-world stock ownership dataset, we show that our model outperforms state-of-the-art SBMs for held-out link prediction tasks.

</details>

<details>

<summary>2020-11-01 12:38:20 - Sub-linear Regret Bounds for Bayesian Optimisation in Unknown Search Spaces</summary>

- *Hung Tran-The, Sunil Gupta, Santu Rana, Huong Ha, Svetha Venkatesh*

- `2009.02539v4` - [abs](http://arxiv.org/abs/2009.02539v4) - [pdf](http://arxiv.org/pdf/2009.02539v4)

> Bayesian optimisation is a popular method for efficient optimisation of expensive black-box functions. Traditionally, BO assumes that the search space is known. However, in many problems, this assumption does not hold. To this end, we propose a novel BO algorithm which expands (and shifts) the search space over iterations based on controlling the expansion rate thought a hyperharmonic series. Further, we propose another variant of our algorithm that scales to high dimensions. We show theoretically that for both our algorithms, the cumulative regret grows at sub-linear rates. Our experiments with synthetic and real-world optimisation tasks demonstrate the superiority of our algorithms over the current state-of-the-art methods for Bayesian optimisation in unknown search space.

</details>

<details>

<summary>2020-11-01 18:46:44 - A general modelling framework for open wildlife populations based on the Polya Tree prior</summary>

- *Alex Diana, Eleni Matechou, Jim Griffin, Todd Arnold, Richard Griffiths, John Pickering, Simone Tenan, Stefano Volponi*

- `2011.00591v1` - [abs](http://arxiv.org/abs/2011.00591v1) - [pdf](http://arxiv.org/pdf/2011.00591v1)

> Wildlife monitoring for open populations can be performed using a number of different survey methods. Each survey method gives rise to a type of data and, in the last five decades, a large number of associated statistical models have been developed for analysing these data. Although these models have been parameterised and fitted using different approaches, they have all been designed to model the pattern with which individuals enter and exit the population and to estimate the population size. However, existing approaches rely on a predefined model structure and complexity, either by assuming that parameters are specific to sampling occasions, or by employing parametric curves. Instead, we propose a novel Bayesian nonparametric framework for modelling entry and exit patterns based on the Polya Tree (PT) prior for densities. Our Bayesian non-parametric approach avoids overfitting when inferring entry and exit patterns while simultaneously allowing more flexibility than is possible using parametric curves. We apply our new framework to capture-recapture, count and ring-recovery data and we introduce the replicated PT prior for defining classes of models for these data. Additionally, we define the Hierarchical Logistic PT prior for jointly modelling related data and we consider the Optional PT prior for modelling long time series of data. We demonstrate our new approach using five different case studies on birds, amphibians and insects.

</details>

<details>

<summary>2020-11-01 22:03:26 - Screening for an Infectious Disease as a Problem in Stochastic Control</summary>

- *Jakub Marecek*

- `2011.00635v1` - [abs](http://arxiv.org/abs/2011.00635v1) - [pdf](http://arxiv.org/pdf/2011.00635v1)

> There has been much recent interest in screening populations for an infectious disease. Here, we present a stochastic-control model, wherein the optimum screening policy is provably difficult to find, but wherein Thompson sampling has provably optimal performance guarantees in the form of Bayesian regret. Thompson sampling seems applicable especially to diseases, for which we do not understand the dynamics well, such as to the super-spreading COVID-19.

</details>

<details>

<summary>2020-11-02 02:22:48 - Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference</summary>

- *Bang An, Jie Lyu, Zhenyi Wang, Chunyuan Li, Changwei Hu, Fei Tan, Ruiyi Zhang, Yifan Hu, Changyou Chen*

- `2009.09364v2` - [abs](http://arxiv.org/abs/2009.09364v2) - [pdf](http://arxiv.org/pdf/2009.09364v2)

> The neural attention mechanism plays an important role in many natural language processing applications. In particular, the use of multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. Without explicit constraining, however, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model's representation power. In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model's expressiveness. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on various tasks.

</details>

<details>

<summary>2020-11-02 10:11:36 - The Probabilistic Backbone of Data-Driven Complex Networks: An example in Climate</summary>

- *Catharina Graafland, José M. Gutiérrez, Juan M. López, Diego Pazó, Miguel A. Rodríguez*

- `1912.03758v2` - [abs](http://arxiv.org/abs/1912.03758v2) - [pdf](http://arxiv.org/pdf/1912.03758v2)

> Correlation Networks (CNs) inherently suffer from redundant information in their network topology. Bayesian Networks (BNs), on the other hand, include only non-redundant information (from a probabilistic perspective) resulting in a sparse topology from which generalizable physical features can be extracted. We advocate the use of BNs to construct data-driven complex networks as they can be regarded as the probabilistic backbone of the underlying complex system. Results are illustrated at the hand of a global climate dataset.

</details>

<details>

<summary>2020-11-02 11:12:18 - Approximate Bayesian Computation for Finite Mixture Models</summary>

- *Umberto Simola, Jessi Cisewski-Kehe, Robert L. Wolpert*

- `1803.10031v2` - [abs](http://arxiv.org/abs/1803.10031v2) - [pdf](http://arxiv.org/pdf/1803.10031v2)

> Finite mixture models are used in statistics and other disciplines, but inference for mixture models is challenging due, in part, to the multimodality of the likelihood function and the so-called label switching problem. We propose extensions of the Approximate Bayesian Computation-Population Monte Carlo (ABC-PMC) algorithm as an alternative framework for inference on finite mixture models. There are several decisions to make when implementing an ABC-PMC algorithm for finite mixture models, including the selection of the kernels used for moving the particles through the iterations, how to address the label switching problem, and the choice of informative summary statistics. Examples are presented to demonstrate the performance of the proposed ABC-PMC algorithm for mixture modeling. The performance of the proposed method is evaluated in a simulation study and for the popular recessional velocity galaxy data.

</details>

<details>

<summary>2020-11-02 13:37:57 - Sample-efficient reinforcement learning using deep Gaussian processes</summary>

- *Charles Gadd, Markus Heinonen, Harri Lähdesmäki, Samuel Kaski*

- `2011.01226v1` - [abs](http://arxiv.org/abs/2011.01226v1) - [pdf](http://arxiv.org/pdf/2011.01226v1)

> Reinforcement learning provides a framework for learning to control which actions to take towards completing a task through trial-and-error. In many applications observing interactions is costly, necessitating sample-efficient learning. In model-based reinforcement learning efficiency is improved by learning to simulate the world dynamics. The challenge is that model inaccuracies rapidly accumulate over planned trajectories. We introduce deep Gaussian processes where the depth of the compositions introduces model complexity while incorporating prior knowledge on the dynamics brings smoothness and structure. Our approach is able to sample a Bayesian posterior over trajectories. We demonstrate highly improved early sample-efficiency over competing methods. This is shown across a number of continuous control tasks, including the half-cheetah whose contact dynamics have previously posed an insurmountable problem for earlier sample-efficient Gaussian process based models.

</details>

<details>

<summary>2020-11-02 15:28:47 - BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search</summary>

- *Colin White, Willie Neiswanger, Yash Savani*

- `1910.11858v3` - [abs](http://arxiv.org/abs/1910.11858v3) - [pdf](http://arxiv.org/pdf/1910.11858v3)

> Over the past half-decade, many methods have been considered for neural architecture search (NAS). Bayesian optimization (BO), which has long had success in hyperparameter optimization, has recently emerged as a very promising strategy for NAS when it is coupled with a neural predictor. Recent work has proposed different instantiations of this framework, for example, using Bayesian neural networks or graph convolutional networks as the predictive model within BO. However, the analyses in these papers often focus on the full-fledged NAS algorithm, so it is difficult to tell which individual components of the framework lead to the best performance.   In this work, we give a thorough analysis of the "BO + neural predictor" framework by identifying five main components: the architecture encoding, neural predictor, uncertainty calibration method, acquisition function, and acquisition optimization strategy. We test several different methods for each component and also develop a novel path-based encoding scheme for neural architectures, which we show theoretically and empirically scales better than other encodings. Using all of our analyses, we develop a final algorithm called BANANAS, which achieves state-of-the-art performance on NAS search spaces. We adhere to the NAS research checklist (Lindauer and Hutter 2019) to facilitate best practices, and our code is available at https://github.com/naszilla/naszilla.

</details>

<details>

<summary>2020-11-02 15:40:17 - Bayesian inference of heterogeneous epidemic models: Application to COVID-19 spread accounting for long-term care facilities</summary>

- *Peng Chen, Keyi Wu, Omar Ghattas*

- `2011.01058v1` - [abs](http://arxiv.org/abs/2011.01058v1) - [pdf](http://arxiv.org/pdf/2011.01058v1)

> We propose a high dimensional Bayesian inference framework for learning heterogeneous dynamics of a COVID-19 model, with a specific application to the dynamics and severity of COVID-19 inside and outside long-term care (LTC) facilities. We develop a heterogeneous compartmental model that accounts for the heterogeneity of the time-varying spread and severity of COVID-19 inside and outside LTC facilities, which is characterized by time-dependent stochastic processes and time-independent parameters in $\sim$1500 dimensions after discretization. To infer these parameters, we use reported data on the number of confirmed, hospitalized, and deceased cases with suitable post-processing in both a deterministic inversion approach with appropriate regularization as a first step, followed by Bayesian inversion with proper prior distributions. To address the curse of dimensionality and the ill-posedness of the high-dimensional inference problem, we propose use of a dimension-independent projected Stein variational gradient descent method, and demonstrate the intrinsic low-dimensionality of the inverse problem. We present inference results with quantified uncertainties for both New Jersey and Texas, which experienced different epidemic phases and patterns. Moreover, we also present forecasting and validation results based on the empirical posterior samples of our inference for the future trajectory of COVID-19.

</details>

<details>

<summary>2020-11-02 16:39:33 - Bayesian sample size determination using commensurate priors to leverage pre-experimental data</summary>

- *Haiyan Zheng, Thomas Jaki, James M. S. Wason*

- `2011.01106v1` - [abs](http://arxiv.org/abs/2011.01106v1) - [pdf](http://arxiv.org/pdf/2011.01106v1)

> This paper develops Bayesian sample size formulae for experiments comparing two groups. We assume the experimental data will be analysed in the Bayesian framework, where pre-experimental information from multiple sources can be represented into robust priors. In particular, such robust priors account for preliminary belief about the pairwise commensurability between parameters that underpin the historical and new experiments, to permit flexible borrowing of information. Averaged over the probability space of the new experimental data, appropriate sample sizes are found according to criteria that control certain aspects of the posterior distribution, such as the coverage probability or length of a defined density region. Our Bayesian methodology can be applied to circumstances where the common variance in the new experiment is known or unknown. Exact solutions are available based on most of the criteria considered for Bayesian sample size determination, while a search procedure is described in cases for which there are no closed-form expressions. We illustrate the application of our Bayesian sample size formulae in the setting of designing a clinical trial. Hypothetical data examples, motivated by a rare-disease trial with elicitation of expert prior opinion, and a comprehensive performance evaluation of the proposed methodology are presented.

</details>

<details>

<summary>2020-11-02 17:33:13 - Computer Model Emulation with High-Dimensional Functional Output in Large-Scale Observing System Uncertainty Experiments</summary>

- *Pulong Ma, Anirban Mondal, Bledar Konomi, Jonathan Hobbs, Joon Song, Emily Kang*

- `1911.09274v2` - [abs](http://arxiv.org/abs/1911.09274v2) - [pdf](http://arxiv.org/pdf/1911.09274v2)

> Observing system uncertainty experiments (OSUEs) have been recently proposed as a cost-effective way to perform probabilistic assessment of retrievals for NASA's Orbiting Carbon Observatory-2 (OCO-2) mission. One important component in the OCO-2 retrieval algorithm is a full-physics forward model that describes the mathematical relationship between atmospheric variables such as carbon dioxide and radiances measured by the remote sensing instrument. This forward model is complicated and computationally expensive but large-scale OSUEs require evaluation of this model numerous times, which makes it infeasible for comprehensive experiments. To tackle this issue, we develop a statistical emulator to facilitate large-scale OSUEs in the OCO-2 mission with independent emulation. Within each distinct spectral band, the emulator represents radiances output at irregular wavelengths via a linear combination of basis functions and random coefficients. These random coefficients are then modeled with nearest-neighbor Gaussian processes with built-in input dimension reduction via active subspace. The proposed emulator reduces dimensionality in both input space and output space, so that fast computation is achieved within a fully Bayesian inference framework. Validation experiments demonstrate that this emulator outperforms other competing statistical methods and a reduced order model that approximates the full-physics forward model.

</details>

<details>

<summary>2020-11-02 17:56:42 - Objective Bayesian Analysis of a Cokriging Model for Hierarchical Multifidelity Codes</summary>

- *Pulong Ma*

- `1910.10225v4` - [abs](http://arxiv.org/abs/1910.10225v4) - [pdf](http://arxiv.org/pdf/1910.10225v4)

> Autoregressive cokriging models have been widely used to emulate multiple computer models with different levels of fidelity. The dependence structures are modeled via Gaussian processes at each level of fidelity, where covariance structures are often parameterized up to a few parameters. The predictive distributions typically require intensive Monte Carlo approximations in previous works. This article derives new closed-form formulas to compute the means and variances of predictive distributions in autoregressive cokriging models that only depend on correlation parameters. For parameter estimation, we consider objective Bayesian analysis of such autoregressive cokriging models. We show that common choices of prior distributions, such as the constant prior and inverse correlation prior, typically lead to improper posteriors. We also develop several objective priors such as the independent reference prior and the independent Jeffreys prior that are shown to yield proper posterior distributions. This development is illustrated with a borehole function in an eight-dimensional input space and applied to an engineering application in a six-dimensional input space.

</details>

<details>

<summary>2020-11-02 18:17:03 - Assessing racial inequality in COVID-19 testing with Bayesian threshold tests</summary>

- *Emma Pierson*

- `2011.01179v1` - [abs](http://arxiv.org/abs/2011.01179v1) - [pdf](http://arxiv.org/pdf/2011.01179v1)

> There are racial disparities in the COVID-19 test positivity rate, suggesting that minorities may be under-tested. Here, drawing on the literature on statistically assessing racial disparities in policing, we 1) illuminate a statistical flaw, known as infra-marginality, in using the positivity rate as a metric for assessing racial disparities in under-testing; 2) develop a new type of Bayesian threshold test to measure disparities in COVID-19 testing and 3) apply the test to measure racial disparities in testing thresholds in a real-world COVID-19 dataset.

</details>

<details>

<summary>2020-11-02 21:21:40 - Practical Bayesian Optimization of Objectives with Conditioning Variables</summary>

- *Michael Pearce, Janis Klaise, Matthew Groves*

- `2002.09996v2` - [abs](http://arxiv.org/abs/2002.09996v2) - [pdf](http://arxiv.org/pdf/2002.09996v2)

> Bayesian optimization is a class of data efficient model based algorithms typically focused on global optimization. We consider the more general case where a user is faced with multiple problems that each need to be optimized conditional on a state variable, for example given a range of cities with different patient distributions, we optimize the ambulance locations conditioned on patient distribution. Given partitions of CIFAR-10, we optimize CNN hyperparameters for each partition. Similarity across objectives boosts optimization of each objective in two ways: in modelling by data sharing across objectives, and also in acquisition by quantifying how a single point on one objective can provide benefit to all objectives. For this we propose a framework for conditional optimization: ConBO. This can be built on top of a range of acquisition functions and we propose a new Hybrid Knowledge Gradient acquisition function. The resulting method is intuitive and theoretically grounded, performs either similar to or significantly better than recently published works on a range of problems, and is easily parallelized to collect a batch of points.

</details>

<details>

<summary>2020-11-03 08:47:27 - Bayesian inference for spline-based hidden Markov models</summary>

- *Sida Chen, Bärbel Finkenstädt Rand*

- `2011.01567v1` - [abs](http://arxiv.org/abs/2011.01567v1) - [pdf](http://arxiv.org/pdf/2011.01567v1)

> B-spline-based hidden Markov models (HMMs), where the emission densities are specified as mixtures of normalized B-spline basis functions, offer a more flexible modelling approach to data than conventional parametric HMMs. We introduce a fully Bayesian framework for inference in these nonparametric models where the number of states may be unknown along with other model parameters. We propose the use of a trans-dimensional Markov chain inference algorithm to identify a parsimonious knot configuration of the B-splines while model selection regarding the number of states can be performed within a parallel sampling framework. The feasibility and efficiency of our proposed methodology is shown in a simulation study. Its explorative use for real data is demonstrated for activity acceleration data in animals, i.e. whitetip-sharks. The flexibility of a Bayesian approach allows us to extend the modelling framework in a straightforward way and we demonstrate this by developing a hierarchical conditional HMM to analyse human accelerator activity data to focus on studying small movements and/or inactivity during sleep.

</details>

<details>

<summary>2020-11-03 14:19:59 - Recommendations for Bayesian hierarchical model specifications for case-control studies in mental health</summary>

- *Vincent Valton, Toby Wise, Oliver J. Robinson*

- `2011.01725v1` - [abs](http://arxiv.org/abs/2011.01725v1) - [pdf](http://arxiv.org/pdf/2011.01725v1)

> Hierarchical model fitting has become commonplace for case-control studies of cognition and behaviour in mental health. However, these techniques require us to formalise assumptions about the data-generating process at the group level, which may not be known. Specifically, researchers typically must choose whether to assume all subjects are drawn from a common population, or to model them as deriving from separate populations. These assumptions have profound implications for computational psychiatry, as they affect the resulting inference (latent parameter recovery) and may conflate or mask true group-level differences. To test these assumptions we ran systematic simulations on synthetic multi-group behavioural data from a commonly used multi-armed bandit task (reinforcement learning task). We then examined recovery of group differences in latent parameter space under the two commonly used generative modelling assumptions: (1) modelling groups under a common shared group-level prior (assuming all participants are generated from a common distribution, and are likely to share common characteristics); (2) modelling separate groups based on symptomatology or diagnostic labels, resulting in separate group-level priors. We evaluated the robustness of these approaches to variations in data quality and prior specifications on a variety of metrics. We found that fitting groups separately (assumptions 2), provided the most accurate and robust inference across all conditions. Our results suggest that when dealing with data from multiple clinical groups, researchers should analyse patient and control groups separately as it provides the most accurate and robust recovery of the parameters of interest.

</details>

<details>

<summary>2020-11-03 14:48:54 - Overlapping-sample Mendelian randomisation with multiple exposures: A Bayesian approach</summary>

- *Linyi Zou, Hui Guo, Carlo Berzuini*

- `2007.10183v2` - [abs](http://arxiv.org/abs/2007.10183v2) - [pdf](http://arxiv.org/pdf/2007.10183v2)

> Background: Mendelian randomization (MR) has been widely applied to causal inference in medical research. It uses genetic variants as instrumental variables (IVs) to investigate putative causal relationship between an exposure and an outcome. Traditional MR methods have dominantly focussed on a two-sample setting in which IV-exposure association study and IV-outcome association study are independent. However, it is not uncommon that participants from the two studies fully overlap (one-sample) or partly overlap (overlapping-sample). Methods: We proposed a method that is applicable to all the three sample settings. In essence, we converted a two- or overlapping- sample problem to a one-sample problem where data of some or all of the individuals were incomplete. Assume that all individuals were drawn from the same population and unmeasured data were missing at random. Then the unobserved data were treated au pair with the model parameters as unknown quantities, and thus, could be imputed iteratively conditioning on the observed data and estimated parameters using Markov chain Monte Carlo. We generalised our model to allow for pleiotropy and multiple exposures and assessed its performance by a number of simulations using four metrics: mean, standard deviation, coverage and power. Results: Higher sample overlapping rate and stronger instruments led to estimates with higher precision and power. Pleiotropy had a notably negative impact on the estimates. Nevertheless, overall the coverages were high and our model performed well in all the sample settings. Conclusions: Our model offers the flexibility of being applicable to any of the sample settings, which is an important addition to the MR literature which has restricted to one- or two- sample scenarios. Given the nature of Bayesian inference, it can be easily extended to more complex MR analysis in medical research.

</details>

<details>

<summary>2020-11-03 15:42:21 - Bayesian Computing in the Undergraduate Statistics Curriculum</summary>

- *Jim Albert, Jingchen Hu*

- `2002.09716v3` - [abs](http://arxiv.org/abs/2002.09716v3) - [pdf](http://arxiv.org/pdf/2002.09716v3)

> Bayesian statistics has gained great momentum since the computational developments of the 1990s. Gradually, advances in Bayesian methodology and software have made Bayesian techniques much more accessible to applied statisticians and, in turn, have potentially transformed Bayesian education at the undergraduate level. This article provides an overview on the various options for implementing Bayesian computational methods motivated to achieve particular learning outcomes. The advantages and disadvantages of each computational method are described based on the authors' experience in using these methods in the classroom. The goal is to present guidance on the choice of computation for the instructors who are introducing Bayesian methods in their undergraduate statistics curriculum.

</details>

<details>

<summary>2020-11-03 15:59:50 - Bayesian Workflow</summary>

- *Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, Martin Modrák*

- `2011.01808v1` - [abs](http://arxiv.org/abs/2011.01808v1) - [pdf](http://arxiv.org/pdf/2011.01808v1)

> The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.

</details>

<details>

<summary>2020-11-03 17:45:26 - Multinomial Sampling for Hierarchical Change-Point Detection</summary>

- *Lorena Romero-Medrano, Pablo Moreno-Muñoz, Antonio Artés-Rodríguez*

- `2007.12420v2` - [abs](http://arxiv.org/abs/2007.12420v2) - [pdf](http://arxiv.org/pdf/2007.12420v2)

> Bayesian change-point detection, together with latent variable models, allows to perform segmentation over high-dimensional time-series. We assume that change-points lie on a lower-dimensional manifold where we aim to infer subsets of discrete latent variables. For this model, full inference is computationally unfeasible and pseudo-observations based on point-estimates are used instead. However, if estimation is not certain enough, change-point detection gets affected. To circumvent this problem, we propose a multinomial sampling methodology that improves the detection rate and reduces the delay while keeping complexity stable and inference analytically tractable. Our experiments show results that outperform the baseline method and we also provide an example oriented to a human behavior study.

</details>

<details>

<summary>2020-11-03 20:56:13 - Bayesian Variational Optimization for Combinatorial Spaces</summary>

- *Tony C. Wu, Daniel Flam-Shepherd, Alán Aspuru-Guzik*

- `2011.02004v1` - [abs](http://arxiv.org/abs/2011.02004v1) - [pdf](http://arxiv.org/pdf/2011.02004v1)

> This paper focuses on Bayesian Optimization in combinatorial spaces. In many applications in the natural science. Broad applications include the study of molecules, proteins, DNA, device structures and quantum circuit designs, a on optimization over combinatorial categorical spaces is needed to find optimal or pareto-optimal solutions. However, only a limited amount of methods have been proposed to tackle this problem. Many of them depend on employing Gaussian Process for combinatorial Bayesian Optimizations. Gaussian Processes suffer from scalability issues for large data sizes as their scaling is cubic with respect to the number of data points. This is often impractical for optimizing large search spaces. Here, we introduce a variational Bayesian optimization method that combines variational optimization and continuous relaxations to the optimization of the acquisition function for Bayesian optimization. Critically, this method allows for gradient-based optimization and has the capability of optimizing problems with large data size and data dimensions. We have shown the performance of our method is comparable to state-of-the-art methods while maintaining its scalability advantages. We also applied our method in molecular optimization.

</details>

<details>

<summary>2020-11-04 01:17:35 - Bayesian Optimization of Risk Measures</summary>

- *Sait Cakmak, Raul Astudillo, Peter Frazier, Enlu Zhou*

- `2007.05554v3` - [abs](http://arxiv.org/abs/2007.05554v3) - [pdf](http://arxiv.org/pdf/2007.05554v3)

> We consider Bayesian optimization of objective functions of the form $\rho[ F(x, W) ]$, where $F$ is a black-box expensive-to-evaluate function and $\rho$ denotes either the VaR or CVaR risk measure, computed with respect to the randomness induced by the environmental random variable $W$. Such problems arise in decision making under uncertainty, such as in portfolio optimization and robust systems design. We propose a family of novel Bayesian optimization algorithms that exploit the structure of the objective function to substantially improve sampling efficiency. Instead of modeling the objective function directly as is typical in Bayesian optimization, these algorithms model $F$ as a Gaussian process, and use the implied posterior on the objective function to decide which points to evaluate. We demonstrate the effectiveness of our approach in a variety of numerical experiments.

</details>

<details>

<summary>2020-11-04 17:10:03 - Nonparametric Multiple Change Point Detection for Non-Stationary Times Series</summary>

- *Zixiang Guan, Gemai Chen*

- `1901.03036v3` - [abs](http://arxiv.org/abs/1901.03036v3) - [pdf](http://arxiv.org/pdf/1901.03036v3)

> This article considers a nonparametric method for detecting change points in non-stationary time series. The proposed method will divide the time series into several segments so that between two adjacent segments, the normalized spectral density functions are different. The theory is based on the assumption that within each segment, time series is a linear process, which means that our method works not only for classic time series models, e.g., causal and invertible ARMA process, but also preserves good performance for non-invertible moving average process. We show that our estimations for change points are consistent. Also, a Bayesian information criterion is applied to estimate the member of change points consistently. Simulation results as well as empirical results will be presented.

</details>

<details>

<summary>2020-11-05 00:12:51 - Dependence on a collection of Poisson random variables</summary>

- *Luis E. Nieto-Barajas*

- `2005.10306v2` - [abs](http://arxiv.org/abs/2005.10306v2) - [pdf](http://arxiv.org/pdf/2005.10306v2)

> We propose two novel ways of introducing dependence among Poisson counts through the use of latent variables in a three levels hierarchical model. Marginal distributions of the random variables of interest are Poisson with strict stationarity as special case. Order--$p$ dependence is described in detail for a temporal sequence of random variables, however spatial or spatio-temporal dependencies are also possible. A full Bayesian inference of the models is described and performance of the models is illustrated with a numerical analysis of maternal mortality in Mexico. Extensions to cope with overdispersion are also discussed.

</details>

<details>

<summary>2020-11-05 00:49:19 - Projected Pólya Tree</summary>

- *Luis Nieto-Barajas, Gabriel Nuñez-Antonio*

- `1902.06020v3` - [abs](http://arxiv.org/abs/1902.06020v3) - [pdf](http://arxiv.org/pdf/1902.06020v3)

> One way of defining probability distributions for circular variables (directions in two dimensions) is to radially project probability distributions, originally defined on $\mathbb{R}^2$, to the unit circle. Projected distributions have proved to be useful in the study of circular and directional data. Although any bivariate distribution can be used to produce a projected circular model, these distributions are typically parametric. In this article we consider a bivariate P\'olya tree on $\mathbb{R}^2$ and project it to the unit circle to define a new Bayesian nonparametric model for circular data. We study the properties of the proposed model, obtain its posterior characterisation and show its performance with simulated and real datasets.

</details>

<details>

<summary>2020-11-05 07:05:16 - All your loss are belong to Bayes</summary>

- *Christian Walder, Richard Nock*

- `2006.04633v2` - [abs](http://arxiv.org/abs/2006.04633v2) - [pdf](http://arxiv.org/pdf/2006.04633v2)

> Loss functions are a cornerstone of machine learning and the starting point of most algorithms. Statistics and Bayesian decision theory have contributed, via properness, to elicit over the past decades a wide set of admissible losses in supervised learning, to which most popular choices belong (logistic, square, Matsushita, etc.). Rather than making a potentially biased ad hoc choice of the loss, there has recently been a boost in efforts to fit the loss to the domain at hand while training the model itself. The key approaches fit a canonical link, a function which monotonically relates the closed unit interval to R and can provide a proper loss via integration. In this paper, we rely on a broader view of proper composite losses and a recent construct from information geometry, source functions, whose fitting alleviates constraints faced by canonical links. We introduce a trick on squared Gaussian Processes to obtain a random process whose paths are compliant source functions with many desirable properties in the context of link estimation. Experimental results demonstrate substantial improvements over the state of the art.

</details>

<details>

<summary>2020-11-06 03:54:01 - Error control in the numerical posterior distribution in the Bayesian UQ analysis of a semilinear evolution PDE</summary>

- *Maria L. Daza-Torres, J. Cricelio Montesinos-López, Marcos A. Capistrán, J. Andrés Christen, Heikki Haario*

- `2001.04977v2` - [abs](http://arxiv.org/abs/2001.04977v2) - [pdf](http://arxiv.org/pdf/2001.04977v2)

> We elaborate on results obtained in \cite{christen2018} for controlling the numerical posterior error for Bayesian UQ problems, now considering forward maps arising from the solution of a semilinear evolution partial differential equation. Results in \cite{christen2018} demand an estimate for the absolute global error (AGE) of the numeric forward map. Our contribution is a numerical method for computing the AGE for semilinear evolution PDEs and shows the potential applicability of \cite{christen2018} in this important wide range family of PDEs. Numerical examples are given to illustrate the efficiency of the proposed method, obtaining numerical posterior distributions for unknown parameters that are nearly identical to the corresponding theoretical posterior, by keeping their Bayes factor close to 1.

</details>

<details>

<summary>2020-11-06 06:40:05 - Fast and Simple Natural-Gradient Variational Inference with Mixture of Exponential-family Approximations</summary>

- *Wu Lin, Mohammad Emtiyaz Khan, Mark Schmidt*

- `1906.02914v3` - [abs](http://arxiv.org/abs/1906.02914v3) - [pdf](http://arxiv.org/pdf/1906.02914v3)

> Natural-gradient methods enable fast and simple algorithms for variational inference, but due to computational difficulties, their use is mostly limited to \emph{minimal} exponential-family (EF) approximations. In this paper, we extend their application to estimate \emph{structured} approximations such as mixtures of EF distributions. Such approximations can fit complex, multimodal posterior distributions and are generally more accurate than unimodal EF approximations. By using a \emph{minimal conditional-EF} representation of such approximations, we derive simple natural-gradient updates. Our empirical results demonstrate a faster convergence of our natural-gradient method compared to black-box gradient-based methods with reparameterization gradients. Our work expands the scope of natural gradients for Bayesian inference and makes them more widely applicable than before.

</details>

<details>

<summary>2020-11-06 07:53:50 - Bayesian Neural Networks</summary>

- *Tom Charnock, Laurence Perreault-Levasseur, François Lanusse*

- `2006.01490v2` - [abs](http://arxiv.org/abs/2006.01490v2) - [pdf](http://arxiv.org/pdf/2006.01490v2)

> In recent times, neural networks have become a powerful tool for the analysis of complex and abstract data models. However, their introduction intrinsically increases our uncertainty about which features of the analysis are model-related and which are due to the neural network. This means that predictions by neural networks have biases which cannot be trivially distinguished from being due to the true nature of the creation and observation of data or not. In order to attempt to address such issues we discuss Bayesian neural networks: neural networks where the uncertainty due to the network can be characterised. In particular, we present the Bayesian statistical framework which allows us to categorise uncertainty in terms of the ingrained randomness of observing certain data and the uncertainty from our lack of knowledge about how data can be created and observed. In presenting such techniques we show how errors in prediction by neural networks can be obtained in principle, and provide the two favoured methods for characterising these errors. We will also describe how both of these methods have substantial pitfalls when put into practice, highlighting the need for other statistical techniques to truly be able to do inference when using neural networks.

</details>

<details>

<summary>2020-11-06 11:02:05 - Bayesian Regression and Classification Using Gaussian Process Priors Indexed by Probability Density Functions</summary>

- *A. Fradi, Y. Feunteun, C. Samir, M. Baklouti, F. Bachoc, J-M. Loubes*

- `2011.03282v1` - [abs](http://arxiv.org/abs/2011.03282v1) - [pdf](http://arxiv.org/pdf/2011.03282v1)

> In this paper, we introduce the notion of Gaussian processes indexed by probability density functions for extending the Mat\'ern family of covariance functions. We use some tools from information geometry to improve the efficiency and the computational aspects of the Bayesian learning model. We particularly show how a Bayesian inference with a Gaussian process prior (covariance parameters estimation and prediction) can be put into action on the space of probability density functions. Our framework has the capacity of classifiying and infering on data observations that lie on nonlinear subspaces. Extensive experiments on multiple synthetic, semi-synthetic and real data demonstrate the effectiveness and the efficiency of the proposed methods in comparison with current state-of-the-art methods.

</details>

<details>

<summary>2020-11-06 18:46:17 - A Bayesian Functional Data Model for Surveys Collected under Informative Sampling with Application to Mortality Estimation using NHANES</summary>

- *Paul A. Parker, Scott H. Holan*

- `2011.03515v1` - [abs](http://arxiv.org/abs/2011.03515v1) - [pdf](http://arxiv.org/pdf/2011.03515v1)

> Functional data are often extremely high-dimensional and exhibit strong dependence structures but can often prove valuable for both prediction and inference. The literature on functional data analysis is well developed; however, there has been very little work involving functional data in complex survey settings. Motivated by physical activity monitor data from the National Health and Nutrition Examination Survey (NHANES), we develop a Bayesian model for functional covariates that can properly account for the survey design. Our approach is intended for non-Gaussian data and can be applied in multivariate settings. In addition, we make use of a variety of Bayesian modeling techniques to ensure that the model is fit in a computationally efficient manner. We illustrate the value of our approach through an empirical simulation study as well as an example of mortality estimation using NHANES data.

</details>

<details>

<summary>2020-11-06 19:05:55 - On Worst-case Regret of Linear Thompson Sampling</summary>

- *Nima Hamidi, Mohsen Bayati*

- `2006.06790v2` - [abs](http://arxiv.org/abs/2006.06790v2) - [pdf](http://arxiv.org/pdf/2006.06790v2)

> In this paper, we consider the worst-case regret of Linear Thompson Sampling (LinTS) for the linear bandit problem. \citet{russo2014learning} show that the Bayesian regret of LinTS is bounded above by $\widetilde{\mathcal{O}}(d\sqrt{T})$ where $T$ is the time horizon and $d$ is the number of parameters. While this bound matches the minimax lower-bounds for this problem up to logarithmic factors, the existence of a similar worst-case regret bound is still unknown. The only known worst-case regret bound for LinTS, due to \cite{agrawal2013thompson,abeille2017linear}, is $\widetilde{\mathcal{O}}(d\sqrt{dT})$ which requires the posterior variance to be inflated by a factor of $\widetilde{\mathcal{O}}(\sqrt{d})$. While this bound is far from the minimax optimal rate by a factor of $\sqrt{d}$, in this paper we show that it is the best possible one can get, settling an open problem stated in \cite{russo2018tutorial}. Specifically, we construct examples to show that, without the inflation, LinTS can incur linear regret up to time $\exp(\Omega(d))$. We then demonstrate that, under mild conditions, a slightly modified version of LinTS requires only an $\widetilde{\mathcal{O}}(1)$ inflation where the constant depends on the diversity of the optimal arm.

</details>

<details>

<summary>2020-11-06 19:23:52 - Learning Asymmetric and Local Features in Multi-Dimensional Data through Wavelets with Recursive Partitioning</summary>

- *Meng Li, Li Ma*

- `1711.00789v5` - [abs](http://arxiv.org/abs/1711.00789v5) - [pdf](http://arxiv.org/pdf/1711.00789v5)

> Effective learning of asymmetric and local features in images and other data observed on multi-dimensional grids is a challenging objective critical for a wide range of image processing applications involving biomedical and natural images. It requires methods that are sensitive to local details while fast enough to handle massive numbers of images of ever increasing sizes. We introduce a probabilistic model-based framework that achieves these objectives by incorporating adaptivity into discrete wavelet transforms (DWT) through Bayesian hierarchical modeling, thereby allowing wavelet bases to adapt to the geometric structure of the data while maintaining the high computational scalability of wavelet methods---linear in the sample size (e.g., the resolution of an image). We derive a recursive representation of the Bayesian posterior model which leads to an exact message passing algorithm to complete learning and inference. While our framework is applicable to a range of problems including multi-dimensional signal processing, compression, and structural learning, we illustrate its work and evaluate its performance in the context of image reconstruction using real images from the ImageNet database, two widely used benchmark datasets, and a dataset from retinal optical coherence tomography and compare its performance to state-of-the-art methods based on basis transforms and deep learning.

</details>

<details>

<summary>2020-11-07 23:28:15 - Towards unique and unbiased causal effect estimation from data with hidden variables</summary>

- *Debo Cheng, Jiuyong Li, Lin Liu, Kui Yu, Thuc Duy Lee, Jixue Liu*

- `2002.10091v2` - [abs](http://arxiv.org/abs/2002.10091v2) - [pdf](http://arxiv.org/pdf/2002.10091v2)

> Causal effect estimation from observational data is a crucial but challenging task. Currently, only a limited number of data-driven causal effect estimation methods are available. These methods either provide only a bound estimation of the causal effect of a treatment on the outcome, or generate a unique estimation of the causal effect, but making strong assumptions on data and having low efficiency. In this paper, we identify a practical problem setting and propose an approach to achieving unique and unbiased estimation of causal effects from data with hidden variables. For the approach, we have developed the theorems to support the discovery of the proper covariate sets for confounding adjustment (adjustment sets). Based on the theorems, two algorithms are proposed for finding the proper adjustment sets from data with hidden variables to obtain unbiased and unique causal effect estimation. Experiments with synthetic datasets generated using five benchmark Bayesian networks and four real-world datasets have demonstrated the efficiency and effectiveness of the proposed algorithms, indicating the practicability of the identified problem setting and the potential of the proposed approach in real-world applications.

</details>

<details>

<summary>2020-11-08 04:16:15 - Bayesian Robust Optimization for Imitation Learning</summary>

- *Daniel S. Brown, Scott Niekum, Marek Petrik*

- `2007.12315v3` - [abs](http://arxiv.org/abs/2007.12315v3) - [pdf](http://arxiv.org/pdf/2007.12315v3)

> One of the main challenges in imitation learning is determining what action an agent should take when outside the state distribution of the demonstrations. Inverse reinforcement learning (IRL) can enable generalization to new states by learning a parameterized reward function, but these approaches still face uncertainty over the true reward function and corresponding optimal policy. Existing safe imitation learning approaches based on IRL deal with this uncertainty using a maxmin framework that optimizes a policy under the assumption of an adversarial reward function, whereas risk-neutral IRL approaches either optimize a policy for the mean or MAP reward function. While completely ignoring risk can lead to overly aggressive and unsafe policies, optimizing in a fully adversarial sense is also problematic as it can lead to overly conservative policies that perform poorly in practice. To provide a bridge between these two extremes, we propose Bayesian Robust Optimization for Imitation Learning (BROIL). BROIL leverages Bayesian reward function inference and a user specific risk tolerance to efficiently optimize a robust policy that balances expected return and conditional value at risk. Our empirical results show that BROIL provides a natural way to interpolate between return-maximizing and risk-minimizing behaviors and outperforms existing risk-sensitive and risk-neutral inverse reinforcement learning algorithms. Code is available at https://github.com/dsbrown1331/broil.

</details>

<details>

<summary>2020-11-08 07:36:49 - Asymptotic Convergence of Thompson Sampling</summary>

- *Cem Kalkanli, Ayfer Ozgur*

- `2011.03917v1` - [abs](http://arxiv.org/abs/2011.03917v1) - [pdf](http://arxiv.org/pdf/2011.03917v1)

> Thompson sampling has been shown to be an effective policy across a variety of online learning tasks. Many works have analyzed the finite time performance of Thompson sampling, and proved that it achieves a sub-linear regret under a broad range of probabilistic settings. However its asymptotic behavior remains mostly underexplored. In this paper, we prove an asymptotic convergence result for Thompson sampling under the assumption of a sub-linear Bayesian regret, and show that the actions of a Thompson sampling agent provide a strongly consistent estimator of the optimal action. Our results rely on the martingale structure inherent in Thompson sampling.

</details>

<details>

<summary>2020-11-08 15:30:03 - Inference under Superspreading: Determinants of SARS-CoV-2 Transmission in Germany</summary>

- *Patrick W. Schmidt*

- `2011.04002v1` - [abs](http://arxiv.org/abs/2011.04002v1) - [pdf](http://arxiv.org/pdf/2011.04002v1)

> Superspreading complicates the study of SARS-CoV-2 transmission. I propose a model for aggregated case data that accounts for superspreading and improves statistical inference. In a Bayesian framework, the model is estimated on German data featuring over 60,000 cases with date of symptom onset and age group. Several factors were associated with a strong reduction in transmission: public awareness rising, testing and tracing, information on local incidence, and high temperature. Immunity after infection, school and restaurant closures, stay-at-home orders, and mandatory face covering were associated with a smaller reduction in transmission. The data suggests that public distancing rules increased transmission in young adults. Information on local incidence was associated with a reduction in transmission of up to 44% (95%-CI: [40%, 48%]), which suggests a prominent role of behavioral adaptations to local risk of infection. Testing and tracing reduced transmission by 15% (95%-CI: [9%,20%]), where the effect was strongest among the elderly. Extrapolating weather effects, I estimate that transmission increases by 53% (95%-CI: [43%, 64%]) in colder seasons.

</details>

<details>

<summary>2020-11-09 01:43:03 - Mixture of Finite Mixtures Model for Basket Trial</summary>

- *Junxian Geng, Guanyu Hu*

- `2011.04135v1` - [abs](http://arxiv.org/abs/2011.04135v1) - [pdf](http://arxiv.org/pdf/2011.04135v1)

> With the recent paradigm shift from cytotoxic drugs to new generation of target therapy and immuno-oncology therapy during oncology drug developments, patients with various cancer (sub)types may be eligible to participate in a basket trial if they have the same molecular target. Bayesian hierarchical modeling (BHM) are widely used in basket trial data analysis, where they adaptively borrow information among different cohorts (subtypes) rather than fully pool the data together or doing stratified analysis based on each cohort. Those approaches, however, may have the risk of over shrinkage estimation because of the invalidated exchangeable assumption. We propose a two-step procedure to find the balance between pooled and stratified analysis. In the first step, we treat it as a clustering problem by grouping cohorts into clusters that share the similar treatment effect. In the second step, we use shrinkage estimator from BHM to estimate treatment effects for cohorts within each cluster under exchangeable assumption. For clustering part, we adapt the mixture of finite mixtures (MFM) approach to have consistent estimate of the number of clusters. We investigate the performance of our proposed method in simulation studies and apply this method to Vemurafenib basket trial data analysis.

</details>

<details>

<summary>2020-11-09 02:26:34 - Bayesian bandwidth estimation for local linear fitting in nonparametric regression models</summary>

- *Han Lin Shang, Xibin Zhang*

- `2011.04155v1` - [abs](http://arxiv.org/abs/2011.04155v1) - [pdf](http://arxiv.org/pdf/2011.04155v1)

> This paper presents a Bayesian sampling approach to bandwidth estimation for the local linear estimator of the regression function in a nonparametric regression model. In the Bayesian sampling approach, the error density is approximated by a location-mixture density of Gaussian densities with means the individual errors and variance a constant parameter. This mixture density has the form of a kernel density estimator of errors and is referred to as the kernel-form error density (c.f., Zhang et al., 2014). While Zhang et al. (2014) use the local constant (also known as the Nadaraya- Watson) estimator to estimate the regression function, we extend this to the local linear estimator, which produces more accurate estimation. The proposed investigation is motivated by the lack of data-driven methods for simultaneously choosing bandwidths in the local linear estimator of the regression function and kernel-form error density. Treating bandwidths as parameters, we derive an approximate (pseudo) likelihood and a posterior. A simulation study shows that the proposed bandwidth estimation outperforms the rule-of-thumb and cross-validation methods under the criterion of integrated squared errors. The proposed bandwidth estimation method is validated through a nonparametric regression model involving firm ownership concentration, and a model involving state-price density estimation.

</details>

<details>

<summary>2020-11-09 10:25:11 - $β$-Cores: Robust Large-Scale Bayesian Data Summarization in the Presence of Outliers</summary>

- *Dionysis Manousakas, Cecilia Mascolo*

- `2008.13600v2` - [abs](http://arxiv.org/abs/2008.13600v2) - [pdf](http://arxiv.org/pdf/2008.13600v2)

> Modern machine learning applications should be able to address the intrinsic challenges arising over inference on massive real-world datasets, including scalability and robustness to outliers. Despite the multiple benefits of Bayesian methods (such as uncertainty-aware predictions, incorporation of experts knowledge, and hierarchical modeling), the quality of classic Bayesian inference depends critically on whether observations conform with the assumed data generating model, which is impossible to guarantee in practice. In this work, we propose a variational inference method that, in a principled way, can simultaneously scale to large datasets, and robustify the inferred posterior with respect to the existence of outliers in the observed data. Reformulating Bayes theorem via the $\beta$-divergence, we posit a robustified pseudo-Bayesian posterior as the target of inference. Moreover, relying on the recent formulations of Riemannian coresets for scalable Bayesian inference, we propose a sparse variational approximation of the robustified posterior and an efficient stochastic black-box algorithm to construct it. Overall our method allows releasing cleansed data summaries that can be applied broadly in scenarios including structured data corruption. We illustrate the applicability of our approach in diverse simulated and real datasets, and various statistical models, including Gaussian mean inference, logistic and neural linear regression, demonstrating its superiority to existing Bayesian summarization methods in the presence of outliers.

</details>

<details>

<summary>2020-11-09 16:13:47 - Scalable Approximate Bayesian Computation for Growing Network Models via Extrapolated and Sampled Summaries</summary>

- *Louis Raynal, Sixing Chen, Antonietta Mira, Jukka-Pekka Onnela*

- `2011.04532v1` - [abs](http://arxiv.org/abs/2011.04532v1) - [pdf](http://arxiv.org/pdf/2011.04532v1)

> Approximate Bayesian computation (ABC) is a simulation-based likelihood-free method applicable to both model selection and parameter estimation. ABC parameter estimation requires the ability to forward simulate datasets from a candidate model, but because the sizes of the observed and simulated datasets usually need to match, this can be computationally expensive. Additionally, since ABC inference is based on comparisons of summary statistics computed on the observed and simulated data, using computationally expensive summary statistics can lead to further losses in efficiency. ABC has recently been applied to the family of mechanistic network models, an area that has traditionally lacked tools for inference and model choice. Mechanistic models of network growth repeatedly add nodes to a network until it reaches the size of the observed network, which may be of the order of millions of nodes. With ABC, this process can quickly become computationally prohibitive due to the resource intensive nature of network simulations and evaluation of summary statistics. We propose two methodological developments to enable the use of ABC for inference in models for large growing networks. First, to save time needed for forward simulating model realizations, we propose a procedure to extrapolate (via both least squares and Gaussian processes) summary statistics from small to large networks. Second, to reduce computation time for evaluating summary statistics, we use sample-based rather than census-based summary statistics. We show that the ABC posterior obtained through this approach, which adds two additional layers of approximation to the standard ABC, is similar to a classic ABC posterior. Although we deal with growing network models, both extrapolated summaries and sampled summaries are expected to be relevant in other ABC settings where the data are generated incrementally.

</details>

<details>

<summary>2020-11-09 17:30:24 - Bayesian Reconstruction of Fourier Pairs</summary>

- *Felipe Tobar, Lerko Araya-Hernández, Pablo Huijse, Petar M. Djurić*

- `2011.04585v1` - [abs](http://arxiv.org/abs/2011.04585v1) - [pdf](http://arxiv.org/pdf/2011.04585v1)

> In a number of data-driven applications such as detection of arrhythmia, interferometry or audio compression, observations are acquired indistinctly in the time or frequency domains: temporal observations allow us to study the spectral content of signals (e.g., audio), while frequency-domain observations are used to reconstruct temporal/spatial data (e.g., MRI). Classical approaches for spectral analysis rely either on i) a discretisation of the time and frequency domains, where the fast Fourier transform stands out as the \textit{de facto} off-the-shelf resource, or ii) stringent parametric models with closed-form spectra. However, the general literature fails to cater for missing observations and noise-corrupted data. Our aim is to address the lack of a principled treatment of data acquired indistinctly in the temporal and frequency domains in a way that is robust to missing or noisy observations, and that at the same time models uncertainty effectively. To achieve this aim, we first define a joint probabilistic model for the temporal and spectral representations of signals, to then perform a Bayesian model update in the light of observations, thus jointly reconstructing the complete (latent) time and frequency representations. The proposed model is analysed from a classical spectral analysis perspective, and its implementation is illustrated through intuitive examples. Lastly, we show that the proposed model is able to perform joint time and frequency reconstruction of real-world audio, healthcare and astronomy signals, while successfully dealing with missing data and handling uncertainty (noise) naturally against both classical and modern approaches for spectral estimation.

</details>

<details>

<summary>2020-11-09 19:22:36 - Classified Regression for Bayesian Optimization: Robot Learning with Unknown Penalties</summary>

- *Alonso Marco, Dominik Baumann, Philipp Hennig, Sebastian Trimpe*

- `1907.10383v2` - [abs](http://arxiv.org/abs/1907.10383v2) - [pdf](http://arxiv.org/pdf/1907.10383v2)

> Learning robot controllers by minimizing a black-box objective cost using Bayesian optimization (BO) can be time-consuming and challenging. It is very often the case that some roll-outs result in failure behaviors, causing premature experiment detention. In such cases, the designer is forced to decide on heuristic cost penalties because the acquired data is often scarce, or not comparable with that of the stable policies. To overcome this, we propose a Bayesian model that captures exactly what we know about the cost of unstable controllers prior to data collection: Nothing, except that it should be a somewhat large number. The resulting Bayesian model, approximated with a Gaussian process, predicts high cost values in regions where failures are likely to occur. In this way, the model guides the BO exploration toward regions of stability. We demonstrate the benefits of the proposed model in several illustrative and statistical synthetic benchmarks, and also in experiments on a real robotic platform. In addition, we propose and experimentally validate a new BO method to account for unknown constraints. Such method is an extension of Max-Value Entropy Search, a recent information-theoretic method, to solve unconstrained global optimization problems.

</details>

<details>

<summary>2020-11-09 21:14:22 - Deep Bayesian Nonparametric Factor Analysis</summary>

- *Arunesh Mittal, Paul Sajda, John Paisley*

- `2011.04770v1` - [abs](http://arxiv.org/abs/2011.04770v1) - [pdf](http://arxiv.org/pdf/2011.04770v1)

> We propose a deep generative factor analysis model with beta process prior that can approximate complex non-factorial distributions over the latent codes. We outline a stochastic EM algorithm for scalable inference in a specific instantiation of this model and present some preliminary results.

</details>

<details>

<summary>2020-11-09 23:30:04 - A Fast Linear Regression via SVD and Marginalization</summary>

- *Philip Greengard, Andrew Gelman, Aki Vehtari*

- `2011.04829v1` - [abs](http://arxiv.org/abs/2011.04829v1) - [pdf](http://arxiv.org/pdf/2011.04829v1)

> We describe a numerical scheme for evaluating the posterior moments of Bayesian linear regression models with partial pooling of the coefficients. The principal analytical tool of the evaluation is a change of basis from coefficient space to the space of singular vectors of the matrix of predictors. After this change of basis and an analytical integration, we reduce the problem of finding moments of a density over k + m dimensions, to finding moments of an m-dimensional density, where k is the number of coefficients and k + m is the dimension of the posterior. Moments can then be computed using, for example, MCMC, the trapezoid rule, or adaptive Gaussian quadrature. An evaluation of the SVD of the matrix of predictors is the dominant computational cost and is performed once during the precomputation stage. We demonstrate numerical results of the algorithm. The scheme described in this paper generalizes naturally to multilevel and multi-group hierarchical regression models where normal-normal parameters appear.

</details>

<details>

<summary>2020-11-10 01:44:22 - Using flexible noise models to avoid noise model misspecification in inference of differential equation time series models</summary>

- *Richard Creswell, Ben Lambert, Chon Lok Lei, Martin Robinson, David Gavaghan*

- `2011.04854v1` - [abs](http://arxiv.org/abs/2011.04854v1) - [pdf](http://arxiv.org/pdf/2011.04854v1)

> When modelling time series, it is common to decompose observed variation into a "signal" process, the process of interest, and "noise", representing nuisance factors that obfuscate the signal. To separate signal from noise, assumptions must be made about both parts of the system. If the signal process is incorrectly specified, our predictions using this model may generalise poorly; similarly, if the noise process is incorrectly specified, we can attribute too much or too little observed variation to the signal. With little justification, independent Gaussian noise is typically chosen, which defines a statistical model that is simple to implement but often misstates system uncertainty and may underestimate error autocorrelation. There are a range of alternative noise processes available but, in practice, none of these may be entirely appropriate, as actual noise may be better characterised as a time-varying mixture of these various types. Here, we consider systems where the signal is modelled with ordinary differential equations and present classes of flexible noise processes that adapt to a system's characteristics. Our noise models include a multivariate normal kernel where Gaussian processes allow for non-stationary persistence and variance, and nonparametric Bayesian models that partition time series into distinct blocks of separate noise structures. Across the scenarios we consider, these noise processes faithfully reproduce true system uncertainty: that is, parameter estimate uncertainty when doing inference using the correct noise model. The models themselves and the methods for fitting them are scalable to large datasets and could help to ensure more appropriate quantification of uncertainty in a host of time series models.

</details>

<details>

<summary>2020-11-10 07:13:09 - Efficient MCMC Sampling for Bayesian Matrix Factorization by Breaking Posterior Symmetries</summary>

- *Saibal De, Hadi Salehi, Alex Gorodetsky*

- `2006.04295v3` - [abs](http://arxiv.org/abs/2006.04295v3) - [pdf](http://arxiv.org/pdf/2006.04295v3)

> Bayesian low-rank matrix factorization techniques have become an essential tool for relational data analysis and matrix completion. A standard approach is to assign zero-mean Gaussian priors on the columns or rows of factor matrices to create a conjugate system. This choice of prior leads to simple implementations; however it also causes symmetries in the posterior distribution that can severely reduce the efficiency of Markov-chain Monte-Carlo (MCMC) sampling approaches. In this paper, we propose a simple modification to the prior choice that provably breaks these symmetries and maintains/improves accuracy. Specifically, we provide conditions that the Gaussian prior mean and covariance must satisfy so the posterior does not exhibit invariances that yield sampling difficulties. For example, we show that using non-zero linearly independent prior means significantly lowers the autocorrelation of MCMC samples, and can also lead to lower reconstruction errors.

</details>

<details>

<summary>2020-11-10 08:29:39 - Neural Networks with Recurrent Generative Feedback</summary>

- *Yujia Huang, James Gornet, Sihui Dai, Zhiding Yu, Tan Nguyen, Doris Y. Tsao, Anima Anandkumar*

- `2007.09200v2` - [abs](http://arxiv.org/abs/2007.09200v2) - [pdf](http://arxiv.org/pdf/2007.09200v2)

> Neural networks are vulnerable to input perturbations such as additive noise and adversarial attacks. In contrast, human perception is much more robust to such perturbations. The Bayesian brain hypothesis states that human brains use an internal generative model to update the posterior beliefs of the sensory input. This mechanism can be interpreted as a form of self-consistency between the maximum a posteriori (MAP) estimation of an internal generative model and the external environment. Inspired by such hypothesis, we enforce self-consistency in neural networks by incorporating generative recurrent feedback. We instantiate this design on convolutional neural networks (CNNs). The proposed framework, termed Convolutional Neural Networks with Feedback (CNN-F), introduces a generative feedback with latent variables to existing CNN architectures, where consistent predictions are made through alternating MAP inference under a Bayesian framework. In the experiments, CNN-F shows considerably improved adversarial robustness over conventional feedforward CNNs on standard benchmarks.

</details>

<details>

<summary>2020-11-10 15:19:07 - On-the-fly Closed-loop Autonomous Materials Discovery via Bayesian Active Learning</summary>

- *A. Gilad Kusne, Heshan Yu, Changming Wu, Huairuo Zhang, Jason Hattrick-Simpers, Brian DeCost, Suchismita Sarker, Corey Oses, Cormac Toher, Stefano Curtarolo, Albert V. Davydov, Ritesh Agarwal, Leonid A. Bendersky, Mo Li, Apurva Mehta, Ichiro Takeuchi*

- `2006.06141v2` - [abs](http://arxiv.org/abs/2006.06141v2) - [pdf](http://arxiv.org/pdf/2006.06141v2)

> Active learning - the field of machine learning (ML) dedicated to optimal experiment design, has played a part in science as far back as the 18th century when Laplace used it to guide his discovery of celestial mechanics [1]. In this work we focus a closed-loop, active learning-driven autonomous system on another major challenge, the discovery of advanced materials against the exceedingly complex synthesis-processes-structure-property landscape. We demonstrate autonomous research methodology (i.e. autonomous hypothesis definition and evaluation) that can place complex, advanced materials in reach, allowing scientists to fail smarter, learn faster, and spend less resources in their studies, while simultaneously improving trust in scientific results and machine learning tools. Additionally, this robot science enables science-over-the-network, reducing the economic impact of scientists being physically separated from their labs. We used the real-time closed-loop, autonomous system for materials exploration and optimization (CAMEO) at the synchrotron beamline to accelerate the fundamentally interconnected tasks of rapid phase mapping and property optimization, with each cycle taking seconds to minutes, resulting in the discovery of a novel epitaxial nanocomposite phase-change memory material.

</details>

<details>

<summary>2020-11-10 22:42:17 - Further Inference on Categorical Data -- A Bayesian Approach</summary>

- *Samyajoy Pal, Prof. Dr. Christian Heumann, Dr. M. Subbiah*

- `2002.06439v2` - [abs](http://arxiv.org/abs/2002.06439v2) - [pdf](http://arxiv.org/pdf/2002.06439v2)

> Three different inferential problems related to a two dimensional categorical data from a Bayesian perspective have been discussed in this article. Conjugate prior distribution with symmetric and asymmetric hyper parameters are considered. Newly conceived asymmetric prior is based on perceived preferences of categories. An extension of test of independence by introducing a notion of measuring association between the parameters has been shown using correlation matrix. Probabilities of different parametric combinations have been estimated from the posterior distribution using closed form integration, Monte-Carlo integration and MCMC methods to draw further inference from categorical data. Bayesian computation is done using R programming language and illustrated with appropriate data sets. Study has highlighted the application of Bayesian inference exploiting the distributional form of underlying parameters.

</details>

<details>

<summary>2020-11-10 23:20:21 - Holes in Bayesian Statistics</summary>

- *Andrew Gelman, Yuling Yao*

- `2002.06467v2` - [abs](http://arxiv.org/abs/2002.06467v2) - [pdf](http://arxiv.org/pdf/2002.06467v2)

> Every philosophy has holes, and it is the responsibility of proponents of a philosophy to point out these problems. Here are a few holes in Bayesian data analysis: (1) the usual rules of conditional probability fail in the quantum realm, (2) flat or weak priors lead to terrible inferences about things we care about, (3) subjective priors are incoherent, (4) Bayesian decision picks the wrong model, (5) Bayes factors fail in the presence of flat or weak priors, (6) for Cantorian reasons we need to check our models, but this destroys the coherence of Bayesian inference. Some of the problems of Bayesian statistics arise from people trying to do things they shouldn't be trying to do, but other holes are not so easily patched. In particular, it may be a good idea to avoid flat, weak, or conventional priors, but such advice, if followed, would go against the vast majority of Bayesian practice and requires us to confront the fundamental incoherence of Bayesian inference. This does not mean that we think Bayesian inference is a bad idea, but it does mean that there is a tension between Bayesian logic and Bayesian workflow which we believe can only be resolved by considering Bayesian logic as a tool, a way of revealing inevitable misfits and incoherences in our model assumptions, rather than as an end in itself.

</details>

<details>

<summary>2020-11-11 05:14:31 - A Bayesian Nonparametric model for textural pattern heterogeneity</summary>

- *Xiao Li, Michele Guindani, Chaan S. Ng, Brian P. Hobbs*

- `2011.05548v1` - [abs](http://arxiv.org/abs/2011.05548v1) - [pdf](http://arxiv.org/pdf/2011.05548v1)

> Cancer radiomics is an emerging discipline promising to elucidate lesion phenotypes and tumor heterogeneity through patterns of enhancement, texture, morphology, and shape. The prevailing technique for image texture analysis relies on the construction and synthesis of Gray-Level Co-occurrence Matrices (GLCM). Practice currently reduces the structured count data of a GLCM to reductive and redundant summary statistics for which analysis requires variable selection and multiple comparisons for each application, thus limiting reproducibility. In this article, we develop a Bayesian multivariate probabilistic framework for the analysis and unsupervised clustering of a sample of GLCM objects. By appropriately accounting for skewness and zero-inflation of the observed counts and simultaneously adjusting for existing spatial autocorrelation at nearby cells, the methodology facilitates estimation of texture pattern distributions within the GLCM lattice itself. The techniques are applied to cluster images of adrenal lesions obtained from CT scans with and without administration of contrast. We further assess whether the resultant subtypes are clinically oriented by investigating their correspondence with pathological diagnoses. Additionally, we compare performance to a class of machine-learning approaches currently used in cancer radiomics with simulation studies.

</details>

<details>

<summary>2020-11-11 09:34:49 - Disentangling Community-level Changes in Crime Trends During the COVID-19 Pandemic in Chicago</summary>

- *Gian Maria Campedelli, Serena Favarin, Alberto Aziani, Alex R. Piquero*

- `2011.05658v1` - [abs](http://arxiv.org/abs/2011.05658v1) - [pdf](http://arxiv.org/pdf/2011.05658v1)

> Recent studies exploiting city-level time series have shown that, around the world, several crimes declined after COVID-19 containment policies have been put in place. Using data at the community-level in Chicago, this work aims to advance our understanding on how public interventions affected criminal activities at a finer spatial scale. The analysis relies on a two-step methodology. First, it estimates the community-wise causal impact of social distancing and shelter-in-place policies adopted in Chicago via Structural Bayesian Time-Series across four crime categories (i.e., burglary, assault, narcotics-related offenses, and robbery). Once the models detected the direction, magnitude and significance of the trend changes, Firth's Logistic Regression is used to investigate the factors associated to the statistically significant crime reduction found in the first step of the analyses. Statistical results first show that changes in crime trends differ across communities and crime types. This suggests that beyond the results of aggregate models lies a complex picture characterized by diverging patterns. Second, regression models provide mixed findings regarding the correlates associated with significant crime reduction: several relations have opposite directions across crimes with population being the only factor that is stably and positively associated with significant crime reduction.

</details>

<details>

<summary>2020-11-11 12:41:52 - Multi-Loss Sub-Ensembles for Accurate Classification with Uncertainty Estimation</summary>

- *Omer Achrack, Raizy Kellerman, Ouriel Barzilay*

- `2010.01917v2` - [abs](http://arxiv.org/abs/2010.01917v2) - [pdf](http://arxiv.org/pdf/2010.01917v2)

> Deep neural networks (DNNs) have made a revolution in numerous fields during the last decade. However, in tasks with high safety requirements, such as medical or autonomous driving applications, providing an assessment of the models reliability can be vital. Uncertainty estimation for DNNs has been addressed using Bayesian methods, providing mathematically founded models for reliability assessment. These model are computationally expensive and generally impractical for many real-time use cases. Recently, non-Bayesian methods were proposed to tackle uncertainty estimation more efficiently. We propose an efficient method for uncertainty estimation in DNNs achieving high accuracy. We simulate the notion of multi-task learning on single-task problems by producing parallel predictions from similar models differing by their loss. This multi-loss approach allows one-phase training for single-task learning with uncertainty estimation. We keep our inference time relatively low by leveraging the advantage proposed by the Deep-Sub-Ensembles method. The novelty of this work resides in the proposed accurate variational inference with a simple and convenient training procedure, while remaining competitive in terms of computational time. We conduct experiments on SVHN, CIFAR10, CIFAR100 as well as Image-Net using different architectures. Our results show improved accuracy on the classification task and competitive results on several uncertainty measures.

</details>

<details>

<summary>2020-11-11 17:56:09 - Bayesian Wavelet Shrinkage with Beta Priors</summary>

- *Alex Rodrigo dos Santos Sousa, Nancy Lopes Garcia, Branislav Vidakovic*

- `1907.06606v3` - [abs](http://arxiv.org/abs/1907.06606v3) - [pdf](http://arxiv.org/pdf/1907.06606v3)

> In wavelet shrinkage and thresholding, most of the standard techniques do not consider information that wavelet coefficients might be bounded, although information about bounded energy in signals can be readily available. To address this, we present a Bayesian approach for shrinkage of bounded wavelet coefficients in the context of non-parametric regression. We propose the use of a zero-contaminated beta distribution with a support symmetric around zero as the prior distribution for the location parameter in the wavelet domain in models with additive gaussian errors. The hyperparameters of the proposed model are closely related to the shrinkage level, which facilitates their elicitation and interpretation. For signals with a low signal-to-noise ratio, the associated Bayesian shrinkage rules provide significant improvement in performance in simulation studies when compared with standard techniques.

</details>

<details>

<summary>2020-11-11 19:35:16 - On the Formalism of The Screening Paradox</summary>

- *Jacques Balayla*

- `2011.06032v1` - [abs](http://arxiv.org/abs/2011.06032v1) - [pdf](http://arxiv.org/pdf/2011.06032v1)

> Bayes' Theorem imposes inevitable limitations on the accuracy of screening tests by tying the test's predictive value to the disease prevalence. The aforementioned limitation is independent of the adequacy and make-up of the test and thus implies inherent Bayesian limitations to the screening process itself. As per the WHO's $Wilson-Jungner$ criteria, one of the prerequisite steps before undertaking screening is to ensure that a treatment for the condition screened exists. However, in so doing, a paradox, henceforth termed the screening paradox, ensues. If a disease process is screened for and subsequently treated, its prevalence would drop in the population, which as per Bayes' theorem, would make the tests' predictive value drop in return. Put another way, a very powerful screening test would, by performing and succeeding at the very task it was developed to do, paradoxically reduce its ability to correctly identify individuals with the disease it screens for in the future. Where $J$ is Youden's statistic (sensitivity [$a$] + specificity [$b$] - 1), and $\phi$ is the prevalence, the ratio of positive predictive values at subsequent time $k$, $\rho(\phi_{k})$, over the original $\rho(\phi_{0})$ at $t_0$ is given by:   $\zeta(\phi_{0},k) = \frac{\rho(\phi_{k})}{\rho(\phi_{0})} =\frac{\phi_k(1-b)+J\phi_0\phi_k}{\phi_0(1-b)+J\phi_0\phi_k}$   In this manuscript, we explore the mathematical model which formalizes said screening paradox and explore its implications for population level screening programs. In particular, we define the number of positive test iterations (PTI) needed to reverse the effects of the paradox as follows:   $n_{i\phi_e}=\left\lceil\frac{ln\left[\frac{\omega\phi_e\phi_k-\omega\phi_e}{\omega\phi_e\phi_k-\phi_k}\right]}{2ln\omega}\right\rceil$   where $\omega$ is the square root of the positive likelihood ratio (LR+).

</details>

<details>

<summary>2020-11-11 19:57:00 - Bayesian inference for transportation origin-destination matrices: the Poisson-inverse Gaussian and other Poisson mixtures</summary>

- *Konstantinos Perrakis, Dimitris Karlis, Mario Cools, Davy Janssens*

- `2011.06045v1` - [abs](http://arxiv.org/abs/2011.06045v1) - [pdf](http://arxiv.org/pdf/2011.06045v1)

> In this paper we present Poisson mixture approaches for origin-destination (OD) modeling in transportation analysis. We introduce covariate-based models which incorporate different transport modeling phases and also allow for direct probabilistic inference on link traffic based on Bayesian predictions. Emphasis is placed on the Poisson-inverse Gaussian as an alternative to the commonly-used Poisson-gamma and Poisson-lognormal models. We present a first full Bayesian formulation and demonstrate that the Poisson-inverse Gaussian is particularly suited for OD analysis due to desirable marginal and hierarchical properties. In addition, the integrated nested Laplace approximation (INLA) is considered as an alternative to Markov chain Monte Carlo and the two methodologies are compared under specific modeling assumptions. The case study is based on 2001 Belgian census data and focuses on a large, sparsely-distributed OD matrix containing trip information for 308 Flemish municipalities.

</details>

<details>

<summary>2020-11-12 00:54:48 - Efficient in-situ image and video compression through probabilistic image representation</summary>

- *Rongjie Liu, Meng Li, Li Ma*

- `1912.05622v3` - [abs](http://arxiv.org/abs/1912.05622v3) - [pdf](http://arxiv.org/pdf/1912.05622v3)

> Fast and effective image compression for multi-dimensional images has become increasingly important for efficient storage and transfer of massive amounts of high-resolution images and videos. Desirable properties in compression methods include (1) high reconstruction quality at a wide range of compression rates while preserving key local details, (2) computational scalability, (3) applicability to a variety of different image/video types and of different dimensions, (4) progressive transmission, and (5) ease of tuning. We present such a method for multi-dimensional image compression called Compression via Adaptive Recursive Partitioning (CARP). CARP uses an optimal permutation of the image pixels inferred from a Bayesian probabilistic model on recursive partitions of the image to reduce its effective dimensionality, achieving a parsimonious representation that preserves information. CARP uses a multi-layer Bayesian hierarchical model to achieve in-situ compression along with self-tuning and regularization, with just one single parameter to be specified by the user to achieve the desired compression rate. Extensive numerical experiments using a variety of datasets including 2D still images, real-life YouTube videos, and surveillance videos show that CARP dominates the state-of-the-art image/video compression approaches---including JPEG, JPEG2000, BPG, MPEG4, HEVC and a neural network-based method---for all of these different image types and on nearly all of the individual images and videos over some methods.

</details>

<details>

<summary>2020-11-12 01:29:31 - De-randomized PAC-Bayes Margin Bounds: Applications to Non-convex and Non-smooth Predictors</summary>

- *Arindam Banerjee, Tiancong Chen, Yingxue Zhou*

- `2002.09956v3` - [abs](http://arxiv.org/abs/2002.09956v3) - [pdf](http://arxiv.org/pdf/2002.09956v3)

> In spite of several notable efforts, explaining the generalization of deterministic non-smooth deep nets, e.g., ReLU-nets, has remained challenging. Existing approaches for deterministic non-smooth deep nets typically need to bound the Lipschitz constant of such deep nets but such bounds are quite large, may even increase with the training set size yielding vacuous generalization bounds. In this paper, we present a new family of de-randomized PAC-Bayes margin bounds for deterministic non-convex and non-smooth predictors, e.g., ReLU-nets. Unlike PAC-Bayes, which applies to Bayesian predictors, the de-randomized bounds apply to deterministic predictors like ReLU-nets. A specific instantiation of the bound depends on a trade-off between the (weighted) distance of the trained weights from the initialization and the effective curvature (`flatness') of the trained predictor.   To get to these bounds, we first develop a de-randomization argument for non-convex but smooth predictors, e.g., linear deep networks (LDNs), which connects the performance of the deterministic predictor with a Bayesian predictor. We then consider non-smooth predictors which for any given input realized as a smooth predictor, e.g., ReLU-nets become some LDNs for any given input, but the realized smooth predictors can be different for different inputs. For such non-smooth predictors, we introduce a new PAC-Bayes analysis which takes advantage of the smoothness of the realized predictors, e.g., LDN, for a given input, and avoids dependency on the Lipschitz constant of the non-smooth predictor. After careful de-randomization, we get a bound for the deterministic non-smooth predictor. We also establish non-uniform sample complexity results based on such bounds. Finally, we present extensive empirical results of our bounds over changing training set size and randomness in labels.

</details>

<details>

<summary>2020-11-12 16:33:17 - Maximum Likelihood Estimation of Spatially Varying Coefficient Models for Large Data with an Application to Real Estate Price Prediction</summary>

- *Jakob A. Dambon, Fabio Sigrist, Reinhard Furrer*

- `2001.08089v5` - [abs](http://arxiv.org/abs/2001.08089v5) - [pdf](http://arxiv.org/pdf/2001.08089v5)

> In regression models for spatial data, it is often assumed that the marginal effects of covariates on the response are constant over space. In practice, this assumption might often be questionable. In this article, we show how a Gaussian process-based spatially varying coefficient (SVC) model can be estimated using maximum likelihood estimation (MLE). In addition, we present an approach that scales to large data by applying covariance tapering. We compare our methodology to existing methods such as a Bayesian approach using the stochastic partial differential equation (SPDE) link, geographically weighted regression (GWR), and eigenvector spatial filtering (ESF) in both a simulation study and an application where the goal is to predict prices of real estate apartments in Switzerland. The results from both the simulation study and application show that the MLE approach results in increased predictive accuracy and more precise estimates. Since we use a model-based approach, we can also provide predictive variances. In contrast to existing model-based approaches, our method scales better to data where both the number of spatial points is large and the number of spatially varying covariates is moderately-sized, e.g., above ten.

</details>

<details>

<summary>2020-11-12 19:29:16 - Bayesian nonparametric modelling of sequential discoveries</summary>

- *Alessandro Zito, Tommaso Rigon, Otso Ovaskainen, David Dunson*

- `2011.06629v1` - [abs](http://arxiv.org/abs/2011.06629v1) - [pdf](http://arxiv.org/pdf/2011.06629v1)

> We aim at modelling the appearance of distinct tags in a sequence of labelled objects. Common examples of this type of data include words in a corpus or distinct species in a sample. These sequential discoveries are often summarised via accumulation curves, which count the number of distinct entities observed in an increasingly large set of objects. We propose a novel Bayesian nonparametric method for species sampling modelling by directly specifying the probability of a new discovery, therefore allowing for flexible specifications. The asymptotic behavior and finite sample properties of such an approach are extensively studied. Interestingly, our enlarged class of sequential processes includes highly tractable special cases. We present a subclass of models characterized by appealing theoretical and computational properties. Moreover, due to strong connections with logistic regression models, the latter subclass can naturally account for covariates. We finally test our proposal on both synthetic and real data, with special emphasis on a large fungal biodiversity study in Finland.

</details>

<details>

<summary>2020-11-12 21:24:26 - On a Variational Approximation based Empirical Likelihood ABC Method</summary>

- *Sanjay Chaudhuri, Subhroshekhar Ghosh, David J. Nott, Kim Cuc Pham*

- `2011.07721v1` - [abs](http://arxiv.org/abs/2011.07721v1) - [pdf](http://arxiv.org/pdf/2011.07721v1)

> Many scientifically well-motivated statistical models in natural, engineering, and environmental sciences are specified through a generative process. However, in some cases, it may not be possible to write down the likelihood for these models analytically. Approximate Bayesian computation (ABC) methods allow Bayesian inference in such situations. The procedures are nonetheless typically computationally intensive. Recently, computationally attractive empirical likelihood-based ABC methods have been suggested in the literature. All of these methods rely on the availability of several suitable analytically tractable estimating equations, and this is sometimes problematic. We propose an easy-to-use empirical likelihood ABC method in this article. First, by using a variational approximation argument as a motivation, we show that the target log-posterior can be approximated as a sum of an expected joint log-likelihood and the differential entropy of the data generating density. The expected log-likelihood is then estimated by an empirical likelihood where the only inputs required are a choice of summary statistic, it's observed value, and the ability to simulate the chosen summary statistics for any parameter value under the model. The differential entropy is estimated from the simulated summaries using traditional methods. Posterior consistency is established for the method, and we discuss the bounds for the required number of simulated summaries in detail. The performance of the proposed method is explored in various examples.

</details>

<details>

<summary>2020-11-13 08:13:18 - Health improvement framework for planning actionable treatment process using surrogate Bayesian model</summary>

- *Kazuki Nakamura, Ryosuke Kojima, Eiichiro Uchino, Koichi Murashita, Ken Itoh, Shigeyuki Nakaji, Yasushi Okuno*

- `2010.16087v2` - [abs](http://arxiv.org/abs/2010.16087v2) - [pdf](http://arxiv.org/pdf/2010.16087v2)

> Clinical decision making regarding treatments based on personal characteristics leads to effective health improvements. Machine learning (ML) has been the primary concern of diagnosis support according to comprehensive patient information. However, the remaining prominent issue is the development of objective treatment processes in clinical situations. This study proposes a novel framework to plan treatment processes in a data-driven manner. A key point of the framework is the evaluation of the "actionability" for personal health improvements by using a surrogate Bayesian model in addition to a high-performance nonlinear ML model. We first evaluated the framework from the viewpoint of its methodology using a synthetic dataset. Subsequently, the framework was applied to an actual health checkup dataset comprising data from 3,132 participants, to improve systolic blood pressure values at the individual level. We confirmed that the computed treatment processes are actionable and consistent with clinical knowledge for lowering blood pressure. These results demonstrate that our framework could contribute toward decision making in the medical field, providing clinicians with deeper insights.

</details>

<details>

<summary>2020-11-13 16:15:58 - Informed Bayesian Inference for the A/B Test</summary>

- *Quentin F. Gronau, K. N. Akash Raj, Eric-Jan Wagenmakers*

- `1905.02068v6` - [abs](http://arxiv.org/abs/1905.02068v6) - [pdf](http://arxiv.org/pdf/1905.02068v6)

> Booming in business and a staple analysis in medical trials, the A/B test assesses the effect of an intervention or treatment by comparing its success rate with that of a control condition. Across many practical applications, it is desirable that (1) evidence can be obtained in favor of the null hypothesis that the treatment is ineffective; (2) evidence can be monitored as the data accumulate; (3) expert prior knowledge can be taken into account. Most existing approaches do not fulfill these desiderata. Here we describe a Bayesian A/B procedure based on Kass and Vaidyanathan (1992) that allows one to monitor the evidence for the hypotheses that the treatment has either a positive effect, a negative effect, or, crucially, no effect. Furthermore, this approach enables one to incorporate expert knowledge about the relative prior plausibility of the rival hypotheses and about the expected size of the effect, given that it is non-zero. To facilitate the wider adoption of this Bayesian procedure we developed the abtest package in R. We illustrate the package options and the associated statistical results with a fictitious business example and a real data medical example.

</details>

<details>

<summary>2020-11-13 17:45:08 - Learning in a Small/Big World</summary>

- *Benson Tsz Kin Leung*

- `2009.11917v7` - [abs](http://arxiv.org/abs/2009.11917v7) - [pdf](http://arxiv.org/pdf/2009.11917v7)

> Savage (1972) lays down the foundation of Bayesian decision theory, but asserts that it is not applicable in big worlds where the environment is complex. Using the theory of finite automaton to model belief formation, this paper studies the characteristics of optimal learning behavior in small and big worlds, where the complexity of the environment is low and high, respectively, relative to the cognitive ability of the decision maker. Confirming Savage's claim, optimal learning behavior is closed to Bayesian in small worlds but significantly different in big worlds. In addition, I show that in big worlds, the optimal learning behavior could exhibit a wide range of well-documented non-Bayesian learning behavior, including the use of heuristic, correlation neglect, persistent over-confidence, inattentive learning, and other behaviors of model simplification or misspecification. These results establish a clear and testable relationship between the prominence of non-Bayesian learning behavior, complexity and cognitive ability.

</details>

<details>

<summary>2020-11-13 19:25:14 - Optimal post-selection inference for sparse signals: a nonparametric empirical-Bayes approach</summary>

- *Spencer Woody, Oscar Hernan Madrid Padilla, James G. Scott*

- `1810.11042v3` - [abs](http://arxiv.org/abs/1810.11042v3) - [pdf](http://arxiv.org/pdf/1810.11042v3)

> Many recently developed Bayesian methods have focused on sparse signal detection. However, much less work has been done addressing the natural follow-up question: how to make valid inferences for the magnitude of those signals after selection. Ordinary Bayesian credible intervals suffer from selection bias, owing to the fact that the target of inference is chosen adaptively. Existing Bayesian approaches for correcting this bias produce credible intervals with poor frequentist properties, while existing frequentist approaches require sacrificing the benefits of shrinkage typical in Bayesian methods, resulting in confidence intervals that are needlessly wide. We address this gap by proposing a nonparametric empirical-Bayes approach for constructing optimal selection-adjusted confidence sets. Our method produces confidence sets that are as short as possible on average, while both adjusting for selection and maintaining exact frequentist coverage uniformly over the parameter space. Our main theoretical result establishes an important consistency property of our procedure: that under mild conditions, it asymptotically converges to the results of an oracle-Bayes analysis in which the prior distribution of signal sizes is known exactly. Across a series of examples, the method outperforms existing frequentist techniques for post-selection inference, producing confidence sets that are notably shorter but with the same coverage guarantee.

</details>

<details>

<summary>2020-11-14 11:43:44 - A Framework for Eliciting, Incorporating, and Disciplining Identification Beliefs in Linear Models</summary>

- *Francis J. DiTraglia, Camilo Garcia-Jimeno*

- `2011.07276v1` - [abs](http://arxiv.org/abs/2011.07276v1) - [pdf](http://arxiv.org/pdf/2011.07276v1)

> To estimate causal effects from observational data, an applied researcher must impose beliefs. The instrumental variables exclusion restriction, for example, represents the belief that the instrument has no direct effect on the outcome of interest. Yet beliefs about instrument validity do not exist in isolation. Applied researchers often discuss the likely direction of selection and the potential for measurement error in their articles but lack formal tools for incorporating this information into their analyses. Failing to use all relevant information not only leaves money on the table; it runs the risk of leading to a contradiction in which one holds mutually incompatible beliefs about the problem at hand. To address these issues, we first characterize the joint restrictions relating instrument invalidity, treatment endogeneity, and non-differential measurement error in a workhorse linear model, showing how beliefs over these three dimensions are mutually constrained by each other and the data. Using this information, we propose a Bayesian framework to help researchers elicit their beliefs, incorporate them into estimation, and ensure their mutual coherence. We conclude by illustrating our framework in a number of examples drawn from the empirical microeconomics literature.

</details>

<details>

<summary>2020-11-14 18:53:24 - Bayesian recurrent state space model for rs-fMRI</summary>

- *Arunesh Mittal, Scott Linderman, John Paisley, Paul Sajda*

- `2011.07365v1` - [abs](http://arxiv.org/abs/2011.07365v1) - [pdf](http://arxiv.org/pdf/2011.07365v1)

> We propose a hierarchical Bayesian recurrent state space model for modeling switching network connectivity in resting state fMRI data. Our model allows us to uncover shared network patterns across disease conditions. We evaluate our method on the ADNI2 dataset by inferring latent state patterns corresponding to altered neural circuits in individuals with Mild Cognitive Impairment (MCI). In addition to states shared across healthy and individuals with MCI, we discover latent states that are predominantly observed in individuals with MCI. Our model outperforms current state of the art deep learning method on ADNI2 dataset.

</details>

<details>

<summary>2020-11-14 20:09:41 - An Expectation Maximization Framework for Yule-Simon Preferential Attachment Models</summary>

- *Lucas Roberts, Denisa Roberts*

- `1710.08511v4` - [abs](http://arxiv.org/abs/1710.08511v4) - [pdf](http://arxiv.org/pdf/1710.08511v4)

> In this paper we develop an Expectation Maximization(EM) algorithm to estimate the parameter of a Yule-Simon distribution. The Yule-Simon distribution exhibits the "rich get richer" effect whereby an 80-20 type of rule tends to dominate. These distributions are ubiquitous in industrial settings. The EM algorithm presented provides both frequentist and Bayesian estimates of the $\lambda$ parameter. By placing the estimation method within the EM framework we are able to derive Standard errors of the resulting estimate. Additionally, we prove convergence of the Yule-Simon EM algorithm and study the rate of convergence. An explicit, closed form solution for the rate of convergence of the algorithm is given. Applications including graph node degree distribution estimation are listed.

</details>

<details>

<summary>2020-11-15 00:18:13 - MixTwice: large-scale hypothesis testing for peptide arrays by variance mixing</summary>

- *Zihao Zheng, Aisha M. Mergaert, Irene M. Ong, Miriam A. Shelef, Michael A. Newton*

- `2011.07420v1` - [abs](http://arxiv.org/abs/2011.07420v1) - [pdf](http://arxiv.org/pdf/2011.07420v1)

> Peptide microarrays have emerged as a powerful technology in immunoproteomics as they provide a tool to measure the abundance of different antibodies in patient serum samples. The high dimensionality and small sample size of many experiments challenge conventional statistical approaches, including those aiming to control the false discovery rate (FDR). Motivated by limitations in reproducibility and power of current methods, we advance an empirical Bayesian tool that computes local false discovery rate statistics and local false sign rate statistics when provided with data on estimated effects and estimated standard errors from all the measured peptides. As the name suggests, the \verb+MixTwice+ tool involves the estimation of two mixing distributions, one on underlying effects and one on underlying variance parameters. Constrained optimization techniques provide for model fitting of mixing distributions under weak shape constraints (unimodality of the effect distribution). Numerical experiments show that \verb+MixTwice+ can accurately estimate generative parameters and powerfully identify non-null peptides. In a peptide array study of rheumatoid arthritis (RA), \verb+MixTwice+ recovers meaningful peptide markers in one case where the signal is weak, and has strong reproducibility properties in one case where the signal is strong. \verb+MixTwice+ is available as an R software package. \href{https://github.com/wiscstatman/MixTwice}{https://github.com/wiscstatman/MixTwice}

</details>

<details>

<summary>2020-11-15 03:27:54 - Efficient Variational Inference for Sparse Deep Learning with Theoretical Guarantee</summary>

- *Jincheng Bai, Qifan Song, Guang Cheng*

- `2011.07439v1` - [abs](http://arxiv.org/abs/2011.07439v1) - [pdf](http://arxiv.org/pdf/2011.07439v1)

> Sparse deep learning aims to address the challenge of huge storage consumption by deep neural networks, and to recover the sparse structure of target functions. Although tremendous empirical successes have been achieved, most sparse deep learning algorithms are lacking of theoretical support. On the other hand, another line of works have proposed theoretical frameworks that are computationally infeasible. In this paper, we train sparse deep neural networks with a fully Bayesian treatment under spike-and-slab priors, and develop a set of computationally efficient variational inferences via continuous relaxation of Bernoulli distribution. The variational posterior contraction rate is provided, which justifies the consistency of the proposed variational Bayes method. Notably, our empirical results demonstrate that this variational procedure provides uncertainty quantification in terms of Bayesian predictive distribution and is also capable to accomplish consistent variable selection by training a sparse multi-layer neural network.

</details>

<details>

<summary>2020-11-15 09:30:56 - Nonparametric Bayesian estimation of a concave distribution function with mixed interval censored data</summary>

- *Geurt Jongbloed, Frank van der Meulen, Lixue Pang*

- `1909.00625v2` - [abs](http://arxiv.org/abs/1909.00625v2) - [pdf](http://arxiv.org/pdf/1909.00625v2)

> Assume we observe a finite number of inspection times together with information on whether a specific event has occurred before each of these times. Suppose replicated measurements are available on multiple event times. The set of inspection times, including the number of inspections, may be different for each event. This is known as mixed case interval censored data. We consider Bayesian estimation of the distribution function of the event time while assuming it is concave. We provide sufficient conditions on the prior such that the resulting procedure is consistent from the Bayesian point of view. We also provide computational methods for drawing from the posterior and illustrate the performance of the Bayesian method in both a simulation study and two real datasets.

</details>

<details>

<summary>2020-11-15 23:38:26 - Online Label Aggregation: A Variational Bayesian Approach</summary>

- *Chi Hong, Amirmasoud Ghiassi, Yichi Zhou, Robert Birke, Lydia Y. Chen*

- `1807.07291v2` - [abs](http://arxiv.org/abs/1807.07291v2) - [pdf](http://arxiv.org/pdf/1807.07291v2)

> Noisy labeled data is more a norm than a rarity for crowd sourced contents. It is effective to distill noise and infer correct labels through aggregation results from crowd workers. To ensure the time relevance and overcome slow responses of workers, online label aggregation is increasingly requested, calling for solutions that can incrementally infer true label distribution via subsets of data items. In this paper, we propose a novel online label aggregation framework, BiLA, which employs variational Bayesian inference method and designs a novel stochastic optimization scheme for incremental training. BiLA is flexible to accommodate any generating distribution of labels by the exact computation of its posterior distribution. We also derive the convergence bound of the proposed optimizer. We compare BiLA with the state of the art based on minimax entropy, neural networks and expectation maximization algorithms, on synthetic and real-world data sets. Our evaluation results on various online scenarios show that BiLA can effectively infer the true labels, with an error rate reduction of at least 10 to 1.5 percent points for synthetic and real-world datasets, respectively.

</details>

<details>

<summary>2020-11-16 11:12:59 - Bayesian approach to inverse scattering with topological priors</summary>

- *Ana Carpio, Sergei Iakunin, Georg Stadler*

- `2003.09318v2` - [abs](http://arxiv.org/abs/2003.09318v2) - [pdf](http://arxiv.org/pdf/2003.09318v2)

> We propose a Bayesian inference framework to estimate uncertainties in inverse scattering problems. Given the observed data, the forward model and their uncertainties, we find the posterior distribution over a finite parameter field representing the objects. To construct the prior distribution we use a topological sensitivity analysis. We demonstrate the approach on the Bayesian solution of 2D inverse problems in light and acoustic holography with synthetic data. Statistical information on objects such as their center location, diameter size, orientation, as well as material properties, are extracted by sampling the posterior distribution. Assuming the number of objects known, comparison of the results obtained by Markov Chain Monte Carlo sampling and by sampling a Gaussian distribution found by linearization about the maximum a posteriori estimate show reasonable agreement. The latter procedure has low computational cost, which makes it an interesting tool for uncertainty studies in 3D. However, MCMC sampling provides a more complete picture of the posterior distribution and yields multi-modal posterior distributions for problems with larger measurement noise. When the number of objects is unknown, we devise a stochastic model selection framework.

</details>

<details>

<summary>2020-11-16 13:38:38 - Posterior analysis of $n$ in the binomial $(n,p)$ problem with both parameters unknown -- with applications to quantitative nanoscopy</summary>

- *Johannes Schmidt-Hieber, Laura Fee Schneider, Thomas Staudt, Andrea Krajina, Timo Aspelmeier, Axel Munk*

- `1809.02443v3` - [abs](http://arxiv.org/abs/1809.02443v3) - [pdf](http://arxiv.org/pdf/1809.02443v3)

> Estimation of the population size $n$ from $k$ i.i.d.\ binomial observations with unknown success probability $p$ is relevant to a multitude of applications and has a long history. Without additional prior information this is a notoriously difficult task when $p$ becomes small, and the Bayesian approach becomes particularly useful. For a large class of priors, we establish posterior contraction and a Bernstein-von Mises type theorem in a setting where $p\rightarrow0$ and $n\rightarrow\infty$ as $k\to\infty$. Furthermore, we suggest a new class of Bayesian estimators for $n$ and provide a comprehensive simulation study in which we investigate their performance. To showcase the advantages of a Bayesian approach on real data, we also benchmark our estimators in a novel application from super-resolution microscopy.

</details>

<details>

<summary>2020-11-16 16:35:46 - A semiparametric spatiotemporal Bayesian model for the bulk and extremes of the Fosberg Fire Weather Index</summary>

- *Arnab Hazra, Brian J. Reich, Benjamin A. Shaby, Ana-Maria Staicu*

- `1812.11699v2` - [abs](http://arxiv.org/abs/1812.11699v2) - [pdf](http://arxiv.org/pdf/1812.11699v2)

> Large wildfires pose a major environmental concern, and precise maps of fire risk can improve disaster relief planning. Fosberg Fire Weather Index (FFWI) is often used to measure wildfire risk; FFWI exhibits non-Gaussian marginal distributions as well as strong spatiotemporal extremal dependence and thus, modeling FFWI using geostatistical models like Gaussian processes is questionable. Extreme value theory (EVT)-driven models like max-stable processes are theoretically appealing but are computationally demanding and applicable only for threshold exceedances or block maxima. Disaster management policies often consider moderate-to-extreme quantiles of climate parameters and hence, joint modeling of the bulk and the tail of the data is required. In this paper, we consider a Dirichlet process mixture of spatial skew-t processes that can flexibly model the bulk as well as the tail. The proposed model has nonstationary mean and covariance structure, and also nonzero spatiotemporal extremal dependence. A simulation study demonstrates that the proposed model has better spatial prediction performance compared to some competing models. We develop spatial maps of FFWI medians and extremes, and discuss the wildfire risk throughout the Santa Ana region of California.

</details>

<details>

<summary>2020-11-16 18:33:06 - Denoising Score-Matching for Uncertainty Quantification in Inverse Problems</summary>

- *Zaccharie Ramzi, Benjamin Remy, Francois Lanusse, Jean-Luc Starck, Philippe Ciuciu*

- `2011.08698v1` - [abs](http://arxiv.org/abs/2011.08698v1) - [pdf](http://arxiv.org/pdf/2011.08698v1)

> Deep neural networks have proven extremely efficient at solving a wide rangeof inverse problems, but most often the uncertainty on the solution they provideis hard to quantify. In this work, we propose a generic Bayesian framework forsolving inverse problems, in which we limit the use of deep neural networks tolearning a prior distribution on the signals to recover. We adopt recent denoisingscore matching techniques to learn this prior from data, and subsequently use it aspart of an annealed Hamiltonian Monte-Carlo scheme to sample the full posteriorof image inverse problems. We apply this framework to Magnetic ResonanceImage (MRI) reconstruction and illustrate how this approach not only yields highquality reconstructions but can also be used to assess the uncertainty on particularfeatures of a reconstructed image.

</details>

<details>

<summary>2020-11-17 04:14:53 - Ultimate Pólya Gamma Samplers -- Efficient MCMC for possibly imbalanced binary and categorical data</summary>

- *Sylvia Frühwirth-Schnatter, Gregor Zens, Helga Wagner*

- `2011.06898v3` - [abs](http://arxiv.org/abs/2011.06898v3) - [pdf](http://arxiv.org/pdf/2011.06898v3)

> Modeling binary and categorical data is one of the most commonly encountered tasks of applied statisticians and econometricians. While Bayesian methods in this context have been available for decades now, they often require a high level of familiarity with Bayesian statistics or suffer from issues such as low sampling efficiency. To contribute to the accessibility of Bayesian models for binary and categorical data, we introduce novel latent variable representations based on P\'olya Gamma random variables for a range of commonly encountered discrete choice models. From these latent variable representations, new Gibbs sampling algorithms for binary, binomial and multinomial logistic regression models are derived. All models allow for a conditionally Gaussian likelihood representation, rendering extensions to more complex modeling frameworks such as state space models straight-forward. However, sampling efficiency may still be an issue in these data augmentation based estimation frameworks. To counteract this, MCMC boosting strategies are developed and discussed in detail. The merits of our approach are illustrated through extensive simulations and a real data application.

</details>

<details>

<summary>2020-11-17 12:35:02 - DS-UI: Dual-Supervised Mixture of Gaussian Mixture Models for Uncertainty Inference</summary>

- *Jiyang Xie, Zhanyu Ma, Jing-Hao Xue, Guoqiang Zhang, Jun Guo*

- `2011.08595v1` - [abs](http://arxiv.org/abs/2011.08595v1) - [pdf](http://arxiv.org/pdf/2011.08595v1)

> This paper proposes a dual-supervised uncertainty inference (DS-UI) framework for improving Bayesian estimation-based uncertainty inference (UI) in deep neural network (DNN)-based image recognition. In the DS-UI, we combine the classifier of a DNN, i.e., the last fully-connected (FC) layer, with a mixture of Gaussian mixture models (MoGMM) to obtain an MoGMM-FC layer. Unlike existing UI methods for DNNs, which only calculate the means or modes of the DNN outputs' distributions, the proposed MoGMM-FC layer acts as a probabilistic interpreter for the features that are inputs of the classifier to directly calculate the probability density of them for the DS-UI. In addition, we propose a dual-supervised stochastic gradient-based variational Bayes (DS-SGVB) algorithm for the MoGMM-FC layer optimization. Unlike conventional SGVB and optimization algorithms in other UI methods, the DS-SGVB not only models the samples in the specific class for each Gaussian mixture model (GMM) in the MoGMM, but also considers the negative samples from other classes for the GMM to reduce the intra-class distances and enlarge the inter-class margins simultaneously for enhancing the learning ability of the MoGMM-FC layer in the DS-UI. Experimental results show the DS-UI outperforms the state-of-the-art UI methods in misclassification detection. We further evaluate the DS-UI in open-set out-of-domain/-distribution detection and find statistically significant improvements. Visualizations of the feature spaces demonstrate the superiority of the DS-UI.

</details>

<details>

<summary>2020-11-17 15:36:35 - VIB is Half Bayes</summary>

- *Alexander A Alemi, Warren R Morningstar, Ben Poole, Ian Fischer, Joshua V Dillon*

- `2011.08711v1` - [abs](http://arxiv.org/abs/2011.08711v1) - [pdf](http://arxiv.org/pdf/2011.08711v1)

> In discriminative settings such as regression and classification there are two random variables at play, the inputs X and the targets Y. Here, we demonstrate that the Variational Information Bottleneck can be viewed as a compromise between fully empirical and fully Bayesian objectives, attempting to minimize the risks due to finite sampling of Y only. We argue that this approach provides some of the benefits of Bayes while requiring only some of the work.

</details>

<details>

<summary>2020-11-18 06:34:49 - Gradient-EM Bayesian Meta-learning</summary>

- *Yayi Zou, Xiaoqi Lu*

- `2006.11764v2` - [abs](http://arxiv.org/abs/2006.11764v2) - [pdf](http://arxiv.org/pdf/2006.11764v2)

> Bayesian meta-learning enables robust and fast adaptation to new tasks with uncertainty assessment. The key idea behind Bayesian meta-learning is empirical Bayes inference of hierarchical model. In this work, we extend this framework to include a variety of existing methods, before proposing our variant based on gradient-EM algorithm. Our method improves computational efficiency by avoiding back-propagation computation in the meta-update step, which is exhausting for deep neural networks. Furthermore, it provides flexibility to the inner-update optimization procedure by decoupling it from meta-update. Experiments on sinusoidal regression, few-shot image classification, and policy-based reinforcement learning show that our method not only achieves better accuracy with less computation cost, but is also more robust to uncertainty.

</details>

<details>

<summary>2020-11-18 13:18:10 - Identification of Dominant Features in Spatial Data</summary>

- *Roman Flury, Florian Gerber, Bernhard Schmid, Reinhard Furrer*

- `2006.07183v2` - [abs](http://arxiv.org/abs/2006.07183v2) - [pdf](http://arxiv.org/pdf/2006.07183v2)

> Dominant features of spatial data are connected structures or patterns that emerge from location-based variation and manifest at specific scales or resolutions. To identify dominant features, we propose a sequential application of multiresolution decomposition and variogram function estimation. Multiresolution decomposition separates data into additive components, and in this way enables the recognition of their dominant features. A dedicated multiresolution decomposition method is developed for arbitrary gridded spatial data, where the underlying model includes a precision and spatial-weight matrix to capture spatial correlation. The data are separated into their components by smoothing on different scales, such that larger scales have longer spatial correlation ranges. Moreover, our model can handle missing values, which is often useful in applications. Variogram function estimation can be used to describe properties in spatial data. Such functions are therefore estimated for each component to determine its effective range, which assesses the width-extent of the dominant feature. Finally, Bayesian analysis enables the inference of identified dominant features and to judge whether these are credibly different. The efficient implementation of the method relies mainly on a sparse-matrix data structure and algorithms. By applying the method to simulated data we demonstrate its applicability and theoretical soundness. In disciplines that use spatial data, this method can lead to new insights, as we exemplify by identifying the dominant features in a forest dataset. In that application, the width-extents of the dominant features have an ecological interpretation, namely the species interaction range, and their estimates support the derivation of ecosystem properties such as biodiversity indices.

</details>

<details>

<summary>2020-11-18 15:28:30 - Two-way sparsity for time-varying networks, with applications in genomics</summary>

- *Thomas E. Bartlett, Ioannis Kosmidis, Ricardo Silva*

- `1802.08114v6` - [abs](http://arxiv.org/abs/1802.08114v6) - [pdf](http://arxiv.org/pdf/1802.08114v6)

> We propose a novel way of modelling time-varying networks, by inducing two-way sparsity on local models of node connectivity. This two-way sparsity separately promotes sparsity across time and sparsity across variables (within time). Separation of these two types of sparsity is achieved through a novel prior structure, which draws on ideas from the Bayesian lasso and from copula modelling. We provide an efficient implementation of the proposed model via a Gibbs sampler, and we apply the model to data from neural development. In doing so, we demonstrate that the proposed model is able to identify changes in genomic network structure that match current biological knowledge. Such changes in genomic network structure can then be used by neuro-biologists to identify potential targets for further experimental investigation.

</details>

<details>

<summary>2020-11-18 15:52:17 - Bounds for the weight of external data in shrinkage estimation</summary>

- *Christian Röver, Tim Friede*

- `2004.02525v3` - [abs](http://arxiv.org/abs/2004.02525v3) - [pdf](http://arxiv.org/pdf/2004.02525v3)

> Shrinkage estimation in a meta-analysis framework may be used to facilitate dynamical borrowing of information. This framework might be used to analyze a new study in the light of previous data, which might differ in their design (e.g., a randomized controlled trial (RCT) and a clinical registry). We show how the common study weights arise in effect and shrinkage estimation, and how these may be generalized to the case of Bayesian meta-analysis. Next we develop simple ways to compute bounds on the weights, so that the contribution of the external evidence may be assessed a priori. These considerations are illustrated and discussed using numerical examples, including applications in the treatment of Creutzfeldt-Jakob disease and in fetal monitoring to prevent the occurrence of metabolic acidosis. The target study's contribution to the resulting estimate is shown to be bounded below. Therefore, concerns of evidence being easily overwhelmed by external data are largely unwarranted.

</details>

<details>

<summary>2020-11-18 17:42:01 - Understanding Variational Inference in Function-Space</summary>

- *David R. Burt, Sebastian W. Ober, Adrià Garriga-Alonso, Mark van der Wilk*

- `2011.09421v1` - [abs](http://arxiv.org/abs/2011.09421v1) - [pdf](http://arxiv.org/pdf/2011.09421v1)

> Recent work has attempted to directly approximate the `function-space' or predictive posterior distribution of Bayesian models, without approximating the posterior distribution over the parameters. This is appealing in e.g. Bayesian neural networks, where we only need the former, and the latter is hard to represent. In this work, we highlight some advantages and limitations of employing the Kullback-Leibler divergence in this setting. For example, we show that minimizing the KL divergence between a wide class of parametric distributions and the posterior induced by a (non-degenerate) Gaussian process prior leads to an ill-defined objective function. Then, we propose (featurized) Bayesian linear regression as a benchmark for `function-space' inference methods that directly measures approximation quality. We apply this methodology to assess aspects of the objective function and inference scheme considered in Sun, Zhang, Shi, and Grosse (2018), emphasizing the quality of approximation to Bayesian inference as opposed to predictive performance.

</details>

<details>

<summary>2020-11-18 18:13:57 - Assessment of System-Level Cyber Attack Vulnerability for Connected and Autonomous Vehicles Using Bayesian Networks</summary>

- *Gurcan Comert, Mashrur Chowdhury, David M. Nicol*

- `2011.09436v1` - [abs](http://arxiv.org/abs/2011.09436v1) - [pdf](http://arxiv.org/pdf/2011.09436v1)

> This study presents a methodology to quantify vulnerability of cyber attacks and their impacts based on probabilistic graphical models for intelligent transportation systems under connected and autonomous vehicles framework. Cyber attack vulnerabilities from various types and their impacts are calculated for intelligent signals and cooperative adaptive cruise control (CACC) applications based on the selected performance measures. Numerical examples are given that show impact of vulnerabilities in terms of average intersection queue lengths, number of stops, average speed, and delays. At a signalized network with and without redundant systems, vulnerability can increase average queues and delays by $3\%$ and $15\%$ and $4\%$ and $17\%$, respectively. For CACC application, impact levels reach to $50\%$ delay difference on average when low amount of speed information is perturbed. When significantly different speed characteristics are inserted by an attacker, delay difference increases beyond $100\%$ of normal traffic conditions.

</details>

<details>

<summary>2020-11-19 00:11:27 - Variational Bayes Neural Network: Posterior Consistency, Classification Accuracy and Computational Challenges</summary>

- *Shrijita Bhattacharya, Zihuan Liu, Tapabrata Maiti*

- `2011.09592v1` - [abs](http://arxiv.org/abs/2011.09592v1) - [pdf](http://arxiv.org/pdf/2011.09592v1)

> Bayesian neural network models (BNN) have re-surged in recent years due to the advancement of scalable computations and its utility in solving complex prediction problems in a wide variety of applications. Despite the popularity and usefulness of BNN, the conventional Markov Chain Monte Carlo based implementation suffers from high computational cost, limiting the use of this powerful technique in large scale studies. The variational Bayes inference has become a viable alternative to circumvent some of the computational issues. Although the approach is popular in machine learning, its application in statistics is somewhat limited. This paper develops a variational Bayesian neural network estimation methodology and related statistical theory. The numerical algorithms and their implementational are discussed in detail. The theory for posterior consistency, a desirable property in nonparametric Bayesian statistics, is also developed. This theory provides an assessment of prediction accuracy and guidelines for characterizing the prior distributions and variational family. The loss of using a variational posterior over the true posterior has also been quantified. The development is motivated by an important biomedical engineering application, namely building predictive tools for the transition from mild cognitive impairment to Alzheimer's disease. The predictors are multi-modal and may involve complex interactive relations.

</details>

<details>

<summary>2020-11-19 04:06:32 - Belief Error and Non-Bayesian Social Learning: Experimental Evidence</summary>

- *Boğaçhan Çelen, Sen Geng, Huihui Li*

- `2011.09640v1` - [abs](http://arxiv.org/abs/2011.09640v1) - [pdf](http://arxiv.org/pdf/2011.09640v1)

> This paper experimentally studies whether individuals hold a first-order belief that others apply Bayes' rule to incorporate private information into their beliefs, which is a fundamental assumption in many Bayesian and non-Bayesian social learning models. We design a novel experimental setting in which the first-order belief assumption implies that social information is equivalent to private information. Our main finding is that participants' reported reservation prices of social information are significantly lower than those of private information, which provides evidence that casts doubt on the first-order belief assumption. We also build a novel belief error model in which participants form a random posterior belief with a Bayesian posterior belief kernel to explain the experimental findings. A structural estimation of the model suggests that participants' sophisticated consideration of others' belief error and their exaggeration of the error both contribute to the difference in reservation prices.

</details>

<details>

<summary>2020-11-19 10:00:45 - $R^*$: A robust MCMC convergence diagnostic with uncertainty using decision tree classifiers</summary>

- *Ben Lambert, Aki Vehtari*

- `2003.07900v5` - [abs](http://arxiv.org/abs/2003.07900v5) - [pdf](http://arxiv.org/pdf/2003.07900v5)

> Markov chain Monte Carlo (MCMC) has transformed Bayesian model inference over the past three decades: mainly because of this, Bayesian inference is now a workhorse of applied scientists. Under general conditions, MCMC sampling converges asymptotically to the posterior distribution, but this provides no guarantees about its performance in finite time. The predominant method for monitoring convergence is to run multiple chains and monitor individual chains' characteristics and compare these to the population as a whole: if within-chain and between-chain summaries are comparable, then this is taken to indicate that the chains have converged to a common stationary distribution. Here, we introduce a new method for diagnosing convergence based on how well a machine learning classifier model can successfully discriminate the individual chains. We call this convergence measure $R^*$. In contrast to the predominant $\widehat{R}$, $R^*$ is a single statistic across all parameters that indicates lack of mixing, although individual variables' importance for this metric can also be determined. Additionally, $R^*$ is not based on any single characteristic of the sampling distribution; instead it uses all the information in the chain, including that given by the joint sampling distribution, which is currently largely overlooked by existing approaches. We recommend calculating $R^*$ using two different machine learning classifiers - gradient-boosted regression trees and random forests - which each work well in models of different dimensions. Because each of these methods outputs a classification probability, as a byproduct, we obtain uncertainty in $R^*$. The method is straightforward to implement and could be a complementary additional check on MCMC convergence for applied analyses.

</details>

<details>

<summary>2020-11-19 15:06:11 - Variational Bayes for high-dimensional linear regression with sparse priors</summary>

- *Kolyan Ray, Botond Szabo*

- `1904.07150v3` - [abs](http://arxiv.org/abs/1904.07150v3) - [pdf](http://arxiv.org/pdf/1904.07150v3)

> We study a mean-field spike and slab variational Bayes (VB) approximation to Bayesian model selection priors in sparse high-dimensional linear regression. Under compatibility conditions on the design matrix, oracle inequalities are derived for the mean-field VB approximation, implying that it converges to the sparse truth at the optimal rate and gives optimal prediction of the response vector. The empirical performance of our algorithm is studied, showing that it works comparably well as other state-of-the-art Bayesian variable selection methods. We also numerically demonstrate that the widely used coordinate-ascent variational inference (CAVI) algorithm can be highly sensitive to the parameter updating order, leading to potentially poor performance. To mitigate this, we propose a novel prioritized updating scheme that uses a data-driven updating order and performs better in simulations. The variational algorithm is implemented in the R package 'sparsevb'.

</details>

<details>

<summary>2020-11-19 22:49:14 - Functional central limit theorems for stick-breaking priors</summary>

- *Yaozhong Hu, Junxi Zhang*

- `2011.10138v1` - [abs](http://arxiv.org/abs/2011.10138v1) - [pdf](http://arxiv.org/pdf/2011.10138v1)

> We obtain the empirical strong law of large numbers, empirical Glivenko-Cantelli theorem, central limit theorem, functional central limit theorem for various nonparametric Bayesian priors which include the Dirichlet process with general stick-breaking weights, the Poisson-Dirichlet process, the normalized inverse Gaussian process, the normalized generalized gamma process, and the generalized Dirichlet process. For the Dirichlet process with general stick-breaking weights, we introduce two general conditions such that the central limit theorem and functional central limit theorem hold. Except in the case of the generalized Dirichlet process, since the finite dimensional distributions of these processes are either hard to obtain or are complicated to use even they are available, we use the method of moments to obtain the convergence results. For the generalized Dirichlet process we use its finite dimensional marginal distributions to obtain the asymptotics although the computations are highly technical.

</details>

<details>

<summary>2020-11-20 07:50:15 - A Semi-Parametric Bayesian Generalized Least Squares Estimator</summary>

- *Ruochen Wu, Melvyn Weeks*

- `2011.10252v1` - [abs](http://arxiv.org/abs/2011.10252v1) - [pdf](http://arxiv.org/pdf/2011.10252v1)

> In this paper we propose a semi-parametric Bayesian Generalized Least Squares estimator. In a generic GLS setting where each error is a vector, parametric GLS maintains the assumption that each error vector has the same covariance matrix. In reality however, the observations are likely to be heterogeneous regarding their distributions. To cope with such heterogeneity, a Dirichlet process prior is introduced for the covariance matrices of the errors, leading to the error distribution being a mixture of a variable number of normal distributions. Our methods let the number of normal components be data driven. Two specific cases are then presented: the semi-parametric Bayesian Seemingly Unrelated Regression (SUR) for equation systems; as well as the Random Effects Model (REM) and Correlated Random Effects Model (CREM) for panel data. A series of simulation experiments is designed to explore the performance of our methods. The results demonstrate that our methods obtain smaller posterior standard deviations than the parametric Bayesian GLS. We then apply our semi-parametric Bayesian SUR and REM/CREM methods to empirical examples.

</details>

<details>

<summary>2020-11-21 06:43:23 - A Bayesian framework for patient-level partitioned survival cost-utility analysis</summary>

- *Andrea Gabrio*

- `2011.10732v1` - [abs](http://arxiv.org/abs/2011.10732v1) - [pdf](http://arxiv.org/pdf/2011.10732v1)

> Patient-level health economic data collected alongside clinical trials are an important component of the process of technology appraisal, with a view to informing resource allocation decisions. For end of life treatments, such as cancer treatments, modelling of cost-effectiveness/utility data may involve some form of partitioned survival analysis, where measures of health-related quality of life and survival time for both pre- and post-progression periods are combined to generate some aggregate measure of clinical benefits (e.g. quality-adjusted survival). In addition, resource use data are often collected from health records on different services from which different cost components are obtained (e.g. treatment, hospital or adverse events costs). A critical problem in these analyses is that both effectiveness and cost data present some complexities, including non-normality, spikes, and missingness, that should be addressed using appropriate methods. Bayesian modelling provides a powerful tool which has become more and more popular in the recent health economics and statistical literature to jointly handle these issues in a relatively easy way. This paper presents a general Bayesian framework that takes into account the complex relationships of trial-based partitioned survival cost-utility data, potentially providing a more adequate evidence for policymakers to inform the decision-making process. Our approach is motivated by, and applied to, a working example based on data from a trial assessing the cost-effectiveness of a new treatment for patients with advanced non-small-cell lung cancer.

</details>

<details>

<summary>2020-11-21 14:05:49 - AutoWeka4MCPS-AVATAR: Accelerating Automated Machine Learning Pipeline Composition and Optimisation</summary>

- *Tien-Dung Nguyen, Bogdan Gabrys, Katarzyna Musial*

- `2011.11846v1` - [abs](http://arxiv.org/abs/2011.11846v1) - [pdf](http://arxiv.org/pdf/2011.11846v1)

> Automated machine learning pipeline (ML) composition and optimisation aim at automating the process of finding the most promising ML pipelines within allocated resources (i.e., time, CPU and memory). Existing methods, such as Bayesian-based and genetic-based optimisation, which are implemented in Auto-Weka, Auto-sklearn and TPOT, evaluate pipelines by executing them. Therefore, the pipeline composition and optimisation of these methods frequently require a tremendous amount of time that prevents them from exploring complex pipelines to find better predictive models. To further explore this research challenge, we have conducted experiments showing that many of the generated pipelines are invalid in the first place, and attempting to execute them is a waste of time and resources. To address this issue, we propose a novel method to evaluate the validity of ML pipelines, without their execution, using a surrogate model (AVATAR). The AVATAR generates a knowledge base by automatically learning the capabilities and effects of ML algorithms on datasets' characteristics. This knowledge base is used for a simplified mapping from an original ML pipeline to a surrogate model which is a Petri net based pipeline. Instead of executing the original ML pipeline to evaluate its validity, the AVATAR evaluates its surrogate model constructed by capabilities and effects of the ML pipeline components and input/output simplified mappings. Evaluating this surrogate model is less resource-intensive than the execution of the original pipeline. As a result, the AVATAR enables the pipeline composition and optimisation methods to evaluate more pipelines by quickly rejecting invalid pipelines. We integrate the AVATAR into the sequential model-based algorithm configuration (SMAC). Our experiments show that when SMAC employs AVATAR, it finds better solutions than on its own.

</details>

<details>

<summary>2020-11-22 09:12:56 - Inferring the unknown parameters in Differential Equation by Gaussian Process Regression with Constraint</summary>

- *Ying Zhou, Hongqiao Wang*

- `2011.10971v1` - [abs](http://arxiv.org/abs/2011.10971v1) - [pdf](http://arxiv.org/pdf/2011.10971v1)

> Differential Equation (DE) is a commonly used modeling method in various scientific subjects such as finance and biology. The parameters in DE models often have interesting scientific interpretations, but their values are often unknown and need to be estimated from the measurements of the DE. In this work, we propose a Bayesian inference framework to solve the problem of estimating the parameters of the DE model, from the given noisy and scarce observations of the solution only. A key issue in this problem is to robustly estimate the derivatives of a function from noisy observations of only the function values at given location points, under the assumption of a physical model in the form of differential equation governing the function and its derivatives. To address the key issue, we use the Gaussian Process Regression with Constraint (GPRC) method which jointly model the solution, the derivatives, and the parametric differential equation, to estimate the solution and its derivatives. For nonlinear differential equations, a Picard-iteration-like approximation of linearization method is used so that the GPRC can be still iteratively applicable. A new potential which combines the data and equation information, is proposed and used in the likelihood for our inference. With numerical examples, we illustrate that the proposed method has competitive performance against existing approaches for estimating the unknown parameters in DEs.

</details>

<details>

<summary>2020-11-23 01:46:09 - Information-Theoretic Multi-Objective Bayesian Optimization with Continuous Approximations</summary>

- *Syrine Belakaria, Aryan Deshwal, Janardhan Rao Doppa*

- `2009.05700v3` - [abs](http://arxiv.org/abs/2009.05700v3) - [pdf](http://arxiv.org/pdf/2009.05700v3)

> Many real-world applications involve black-box optimization of multiple objectives using continuous function approximations that trade-off accuracy and resource cost of evaluation. For example, in rocket launching research, we need to find designs that trade-off return-time and angular distance using continuous-fidelity simulators (e.g., varying tolerance parameter to trade-off simulation time and accuracy) for design evaluations. The goal is to approximate the optimal Pareto set by minimizing the cost for evaluations. In this paper, we propose a novel approach referred to as information-Theoretic Multi-Objective Bayesian Optimization with Continuous Approximations (iMOCA)} to solve this problem. The key idea is to select the sequence of input and function approximations for multiple objectives which maximize the information gain per unit cost for the optimal Pareto front. Our experiments on diverse synthetic and real-world benchmarks show that iMOCA significantly improves over existing single-fidelity methods.

</details>

<details>

<summary>2020-11-23 02:20:58 - Max-value Entropy Search for Multi-Objective Bayesian Optimization with Constraints</summary>

- *Syrine Belakaria, Aryan Deshwal, Janardhan Rao Doppa*

- `2009.01721v2` - [abs](http://arxiv.org/abs/2009.01721v2) - [pdf](http://arxiv.org/pdf/2009.01721v2)

> We consider the problem of constrained multi-objective blackbox optimization using expensive function evaluations, where the goal is to approximate the true Pareto set of solutions satisfying a set of constraints while minimizing the number of function evaluations. For example, in aviation power system design applications, we need to find the designs that trade-off total energy and the mass while satisfying specific thresholds for motor temperature and voltage of cells. This optimization requires performing expensive computational simulations to evaluate designs. In this paper, we propose a new approach referred as {\em Max-value Entropy Search for Multi-objective Optimization with Constraints (MESMOC)} to solve this problem. MESMOC employs an output-space entropy based acquisition function to efficiently select the sequence of inputs for evaluation to uncover high-quality pareto-set solutions while satisfying constraints.   We apply MESMOC to two real-world engineering design applications to demonstrate its effectiveness over state-of-the-art algorithms.

</details>

<details>

<summary>2020-11-23 02:32:49 - Bayesian Nonparametric Estimation for Point Processes with Spatial Homogeneity: A Spatial Analysis of NBA Shot Locations</summary>

- *Fan Yin, Jieying Jiao, Guanyu Hu, Jun Yan*

- `2011.11178v1` - [abs](http://arxiv.org/abs/2011.11178v1) - [pdf](http://arxiv.org/pdf/2011.11178v1)

> Basketball shot location data provide valuable summary information regarding players to coaches, sports analysts, fans, statisticians, as well as players themselves. Represented by spatial points, such data are naturally analyzed with spatial point process models. We present a novel nonparametric Bayesian method for learning the underlying intensity surface built upon a combination of Dirichlet process and Markov random field. Our method has the advantage of effectively encouraging local spatial homogeneity when estimating a globally heterogeneous intensity surface. Posterior inferences are performed with an efficient Markov chain Monte Carlo (MCMC) algorithm. Simulation studies show that the inferences are accurate and that the method is superior compared to the competing methods. Application to the shot location data of $20$ representative NBA players in the 2017-2018 regular season offers interesting insights about the shooting patterns of these players. A comparison against the competing method shows that the proposed method can effectively incorporate spatial contiguity into the estimation of intensity surfaces.

</details>

<details>

<summary>2020-11-23 09:22:10 - Parallel tempering as a mechanism for facilitating inference in hierarchical hidden Markov models</summary>

- *Giada Sacchi, Ben Swallow*

- `2011.10088v2` - [abs](http://arxiv.org/abs/2011.10088v2) - [pdf](http://arxiv.org/pdf/2011.10088v2)

> The study of animal behavioural states inferred through hidden Markov models and similar state switching models has seen a significant increase in popularity in recent years. The ability to account for varying levels of behavioural scale has become possible through hierarchical hidden Markov models, but additional levels lead to higher complexity and increased correlation between model components. Maximum likelihood approaches to inference using the EM algorithm and direct optimisation of likelihoods are more frequently used, with Bayesian approaches being less favoured due to computational demands. Given these demands, it is vital that efficient estimation algorithms are developed when Bayesian methods are preferred. We study the use of various approaches to improve convergence times and mixing in Markov chain Monte Carlo methods applied to hierarchical hidden Markov models, including parallel tempering as an inference facilitation mechanism. The method shows promise for analysing complex stochastic models with high levels of correlation between components, but our results show that it requires careful tuning in order to maximise that potential.

</details>

<details>

<summary>2020-11-23 13:27:07 - Sampling Techniques in Bayesian Target Encoding</summary>

- *Michael Larionov*

- `2006.01317v2` - [abs](http://arxiv.org/abs/2006.01317v2) - [pdf](http://arxiv.org/pdf/2006.01317v2)

> Target encoding is an effective encoding technique of categorical variables and is often used in machine learning systems for processing tabular data sets with mixed numeric and categorical variables. Recently en enhanced version of this encoding technique was proposed by using conjugate Bayesian modeling. This paper presents a further development of Bayesian encoding method by using sampling techniques, which helps in extracting information from intra-category distribution of the target variable, improves generalization and reduces target leakage.

</details>

<details>

<summary>2020-11-23 14:16:36 - Walsh-Hadamard Variational Inference for Bayesian Deep Learning</summary>

- *Simone Rossi, Sebastien Marmin, Maurizio Filippone*

- `1905.11248v2` - [abs](http://arxiv.org/abs/1905.11248v2) - [pdf](http://arxiv.org/pdf/1905.11248v2)

> Over-parameterized models, such as DeepNets and ConvNets, form a class of models that are routinely adopted in a wide variety of applications, and for which Bayesian inference is desirable but extremely challenging. Variational inference offers the tools to tackle this challenge in a scalable way and with some degree of flexibility on the approximation, but for over-parameterized models this is challenging due to the over-regularization property of the variational objective. Inspired by the literature on kernel methods, and in particular on structured approximations of distributions of random matrices, this paper proposes Walsh-Hadamard Variational Inference (WHVI), which uses Walsh-Hadamard-based factorization strategies to reduce the parameterization and accelerate computations, thus avoiding over-regularization issues with the variational objective. Extensive theoretical and empirical analyses demonstrate that WHVI yields considerable speedups and model reductions compared to other techniques to carry out approximate inference for over-parameterized models, and ultimately show how advances in kernel methods can be translated into advances in approximate Bayesian inference.

</details>

<details>

<summary>2020-11-23 15:41:46 - Bayesian dose-regimen assessment in early phase oncology incorporating pharmacokinetics and pharmacodynamics</summary>

- *Emma Gerard, Sarah Zohar, Hoai-Thu Thai, Christelle Lorenzato, Marie-Karelle Riviere, Moreno Ursino*

- `2011.11480v1` - [abs](http://arxiv.org/abs/2011.11480v1) - [pdf](http://arxiv.org/pdf/2011.11480v1)

> Phase I dose-finding trials in oncology seek to find the maximum tolerated dose (MTD) of a drug under a specific schedule. Evaluating drug-schedules aims at improving treatment safety while maintaining efficacy. However, while we can reasonably assume that toxicity increases with the dose for cytotoxic drugs, the relationship between toxicity and multiple schedules remains elusive. We proposed a Bayesian dose-regimen assessment method (DRtox) using pharmacokinetics/pharmacodynamics (PK/PD) information to estimate the maximum tolerated dose-regimen (MTD-regimen), at the end of the dose-escalation stage of a trial to be recommended for the next phase. We modeled the binary toxicity via a PD endpoint and estimated the dose-regimen toxicity relationship through the integration of a dose-regimen PD model and a PD toxicity model. For the dose-regimen PD model, we considered nonlinear mixed-effects models, and for the PD toxicity model, we proposed the following two Bayesian approaches: a logistic model and a hierarchical model. We evaluated the operating characteristics of the DRtox through simulation studies under various scenarios. The results showed that our method outperforms traditional model-based designs demonstrating a higher percentage of correctly selecting the MTD-regimen. Moreover, the inclusion of PK/PD information in the DRtox helped provide more precise estimates for the entire dose-regimen toxicity curve; therefore the DRtox may recommend alternative untested regimens for expansion cohorts. The DRtox should be applied at the end of the dose-escalation stage of an ongoing trial for patients with relapsed or refractory acute myeloid leukemia (NCT03594955) once all toxicity and PK/PD data are collected.

</details>

<details>

<summary>2020-11-23 17:12:34 - On a Bayesian Approach to Malware Detection and Classification through $n$-gram Profiles</summary>

- *José A. Perusquía, Jim E. Griffin, Cristiano Villa*

- `2011.11558v1` - [abs](http://arxiv.org/abs/2011.11558v1) - [pdf](http://arxiv.org/pdf/2011.11558v1)

> Detecting and correctly classifying malicious executables has become one of the major concerns in cyber security, especially because traditional detection systems have become less effective with the increasing number and danger of threats found nowadays. One way to differentiate benign from malicious executables is to leverage on their hexadecimal representation by creating a set of binary features that completely characterise each executable. In this paper we present a novel supervised learning Bayesian nonparametric approach for binary matrices, that provides an effective probabilistic approach for malware detection. Moreover, and due to the model's flexible assumptions, we are able to use it in a multi-class framework where the interest relies in classifying malware into known families. Finally, a generalisation of the model which provides a deeper understanding of the behaviour across groups for each feature is also developed.

</details>

<details>

<summary>2020-11-23 20:27:40 - Adversarial Evaluation of Autonomous Vehicles in Lane-Change Scenarios</summary>

- *Baiming Chen, Xiang Chen, Wu Qiong, Liang Li*

- `2004.06531v2` - [abs](http://arxiv.org/abs/2004.06531v2) - [pdf](http://arxiv.org/pdf/2004.06531v2)

> Autonomous vehicles must be comprehensively evaluated before deployed in cities and highways. However, most existing evaluation approaches for autonomous vehicles are static and lack adaptability, so they are usually inefficient in generating challenging scenarios for tested vehicles. In this paper, we propose an adaptive evaluation framework to efficiently evaluate autonomous vehicles in adversarial environments generated by deep reinforcement learning. Considering the multimodal nature of dangerous scenarios, we use ensemble models to represent different local optimums for diversity. We then utilize a nonparametric Bayesian method to cluster the adversarial policies. The proposed method is validated in a typical lane-change scenario that involves frequent interactions between the ego vehicle and the surrounding vehicles. Results show that the adversarial scenarios generated by our method significantly degrade the performance of the tested vehicles. We also illustrate different patterns of generated adversarial environments, which can be used to infer the weaknesses of the tested vehicles.

</details>

<details>

<summary>2020-11-24 00:04:09 - Bayesian Landmark-based Shape Analysis of Tumor Pathology Images</summary>

- *Cong Zhang, Guanghua Xiao, Chul Moon, Min Chen, Qiwei Li*

- `2012.01149v1` - [abs](http://arxiv.org/abs/2012.01149v1) - [pdf](http://arxiv.org/pdf/2012.01149v1)

> Medical imaging is a form of technology that has revolutionized the medical field in the past century. In addition to radiology imaging of tumor tissues, digital pathology imaging, which captures histological details in high spatial resolution, is fast becoming a routine clinical procedure for cancer diagnosis support and treatment planning. Recent developments in deep-learning methods facilitate the segmentation of tumor regions at almost the cellular level from digital pathology images. The traditional shape features that were developed for characterizing tumor boundary roughness in radiology are not applicable. Reliable statistical approaches to modeling tumor shape in pathology images are in urgent need. In this paper, we consider the problem of modeling a tumor boundary with a closed polygonal chain. A Bayesian landmark-based shape analysis (BayesLASA) model is proposed to partition the polygonal chain into mutually exclusive segments to quantify the boundary roughness piecewise. Our fully Bayesian inference framework provides uncertainty estimates of both the number and locations of landmarks. The BayesLASA outperforms a recently developed landmark detection model for planar elastic curves in terms of accuracy and efficiency. We demonstrate how this model-based analysis can lead to sharper inferences than ordinary approaches through a case study on the 246 pathology images from 143 non-small cell lung cancer patients. The case study shows that the heterogeneity of tumor boundary roughness predicts patient prognosis (p-value < 0.001). This statistical methodology not only presents a new model for characterizing a digitized object's shape features by using its landmarks, but also provides a new perspective for understanding the role of tumor surface in cancer progression.

</details>

<details>

<summary>2020-11-24 14:50:28 - Pareto-efficient Acquisition Functions for Cost-Aware Bayesian Optimization</summary>

- *Gauthier Guinet, Valerio Perrone, Cédric Archambeau*

- `2011.11456v2` - [abs](http://arxiv.org/abs/2011.11456v2) - [pdf](http://arxiv.org/pdf/2011.11456v2)

> Bayesian optimization (BO) is a popular method to optimize expensive black-box functions. It efficiently tunes machine learning algorithms under the implicit assumption that hyperparameter evaluations cost approximately the same. In reality, the cost of evaluating different hyperparameters, be it in terms of time, dollars or energy, can span several orders of magnitude of difference. While a number of heuristics have been proposed to make BO cost-aware, none of these have been proven to work robustly. In this work, we reformulate cost-aware BO in terms of Pareto efficiency and introduce the cost Pareto Front, a mathematical object allowing us to highlight the shortcomings of commonly used acquisition functions. Based on this, we propose a novel Pareto-efficient adaptation of the expected improvement. On 144 real-world black-box function optimization problems we show that our Pareto-efficient acquisition functions significantly outperform previous solutions, bringing up to 50% speed-ups while providing finer control over the cost-accuracy trade-off. We also revisit the common choice of Gaussian process cost models, showing that simple, low-variance cost models predict training times effectively.

</details>

<details>

<summary>2020-11-24 15:01:22 - Foundations of Bayesian Learning from Synthetic Data</summary>

- *Harrison Wilde, Jack Jewson, Sebastian Vollmer, Chris Holmes*

- `2011.08299v2` - [abs](http://arxiv.org/abs/2011.08299v2) - [pdf](http://arxiv.org/pdf/2011.08299v2)

> There is significant growth and interest in the use of synthetic data as an enabler for machine learning in environments where the release of real data is restricted due to privacy or availability constraints. Despite a large number of methods for synthetic data generation, there are comparatively few results on the statistical properties of models learnt on synthetic data, and fewer still for situations where a researcher wishes to augment real data with another party's synthesised data. We use a Bayesian paradigm to characterise the updating of model parameters when learning in these settings, demonstrating that caution should be taken when applying conventional learning algorithms without appropriate consideration of the synthetic data generating process and learning task. Recent results from general Bayesian updating support a novel and robust approach to Bayesian synthetic-learning founded on decision theory that outperforms standard approaches across repeated experiments on supervised learning and inference problems.

</details>

<details>

<summary>2020-11-24 15:13:32 - Shrinkage in the Time-Varying Parameter Model Framework Using the R Package shrinkTVP</summary>

- *Peter Knaus, Angela Bitto-Nemling, Annalisa Cadonna, Sylvia Frühwirth-Schnatter*

- `1907.07065v3` - [abs](http://arxiv.org/abs/1907.07065v3) - [pdf](http://arxiv.org/pdf/1907.07065v3)

> Time-varying parameter (TVP) models are widely used in time series analysis to flexibly deal with processes which gradually change over time. However, the risk of overfitting in TVP models is well known. This issue can be dealt with using appropriate global-local shrinkage priors, which pull time-varying parameters towards static ones. In this paper, we introduce the R package shrinkTVP (Knaus, Bitto-Nemling, Cadonna, and Fr\"uhwirth-Schnatter 2019), which provides a fully Bayesian implementation of shrinkage priors for TVP models, taking advantage of recent developments in the literature, in particular that of Bitto and Fr\"uhwirth-Schnatter (2019). The package shrinkTVP allows for posterior simulation of the parameters through an efficient Markov Chain Monte Carlo (MCMC) scheme. Moreover, summary and visualization methods, as well as the possibility of assessing predictive performance through log predictive density scores (LPDSs), are provided. The computationally intensive tasks have been implemented in C++ and interfaced with R. The paper includes a brief overview of the models and shrinkage priors implemented in the package. Furthermore, core functionalities are illustrated, both with simulated and real data.

</details>

<details>

<summary>2020-11-24 16:37:04 - Deep Evidential Regression</summary>

- *Alexander Amini, Wilko Schwarting, Ava Soleimany, Daniela Rus*

- `1910.02600v2` - [abs](http://arxiv.org/abs/1910.02600v2) - [pdf](http://arxiv.org/pdf/1910.02600v2)

> Deterministic neural networks (NNs) are increasingly being deployed in safety critical domains, where calibrated, robust, and efficient measures of uncertainty are crucial. In this paper, we propose a novel method for training non-Bayesian NNs to estimate a continuous target as well as its associated evidence in order to learn both aleatoric and epistemic uncertainty. We accomplish this by placing evidential priors over the original Gaussian likelihood function and training the NN to infer the hyperparameters of the evidential distribution. We additionally impose priors during training such that the model is regularized when its predicted evidence is not aligned with the correct output. Our method does not rely on sampling during inference or on out-of-distribution (OOD) examples for training, thus enabling efficient and scalable uncertainty learning. We demonstrate learning well-calibrated measures of uncertainty on various benchmarks, scaling to complex computer vision tasks, as well as robustness to adversarial and OOD test samples.

</details>

<details>

<summary>2020-11-24 21:27:10 - Elastic $k$-means clustering of functional data for posterior exploration, with an application to inference on acute respiratory infection dynamics</summary>

- *Xiao Zang, Sebastian Kurtek, Oksana Chkrebtii, J. Derek Tucker*

- `2011.12397v1` - [abs](http://arxiv.org/abs/2011.12397v1) - [pdf](http://arxiv.org/pdf/2011.12397v1)

> We propose a new method for clustering of functional data using a $k$-means framework. We work within the elastic functional data analysis framework, which allows for decomposition of the overall variation in functional data into amplitude and phase components. We use the amplitude component to partition functions into shape clusters using an automated approach. To select an appropriate number of clusters, we additionally propose a novel Bayesian Information Criterion defined using a mixture model on principal components estimated using functional Principal Component Analysis. The proposed method is motivated by the problem of posterior exploration, wherein samples obtained from Markov chain Monte Carlo algorithms are naturally represented as functions. We evaluate our approach using a simulated dataset, and apply it to a study of acute respiratory infection dynamics in San Luis Potos\'{i}, Mexico.

</details>

<details>

<summary>2020-11-25 01:13:39 - EI-MTD:Moving Target Defense for Edge Intelligence against Adversarial Attacks</summary>

- *Yaguan Qian, Qiqi Shao, Jiamin Wang, Xiang Lin, Yankai Guo, Zhaoquan Gu, Bin Wang, Chunming Wu*

- `2009.10537v3` - [abs](http://arxiv.org/abs/2009.10537v3) - [pdf](http://arxiv.org/pdf/2009.10537v3)

> With the boom of edge intelligence, its vulnerability to adversarial attacks becomes an urgent problem. The so-called adversarial example can fool a deep learning model on the edge node to misclassify. Due to the property of transferability, the adversary can easily make a black-box attack using a local substitute model. Nevertheless, the limitation of resource of edge nodes cannot afford a complicated defense mechanism as doing on the cloud data center. To overcome the challenge, we propose a dynamic defense mechanism, namely EI-MTD. It first obtains robust member models with small size through differential knowledge distillation from a complicated teacher model on the cloud data center. Then, a dynamic scheduling policy based on a Bayesian Stackelberg game is applied to the choice of a target model for service. This dynamic defense can prohibit the adversary from selecting an optimal substitute model for black-box attacks. Our experimental result shows that this dynamic scheduling can effectively protect edge intelligence against adversarial attacks under the black-box setting.

</details>

<details>

<summary>2020-11-25 02:13:43 - Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS</summary>

- *Han Shi, Renjie Pi, Hang Xu, Zhenguo Li, James T. Kwok, Tong Zhang*

- `1911.09336v4` - [abs](http://arxiv.org/abs/1911.09336v4) - [pdf](http://arxiv.org/pdf/1911.09336v4)

> Neural Architecture Search (NAS) has shown great potentials in finding better neural network designs. Sample-based NAS is the most reliable approach which aims at exploring the search space and evaluating the most promising architectures. However, it is computationally very costly. As a remedy, the one-shot approach has emerged as a popular technique for accelerating NAS using weight-sharing. However, due to the weight-sharing of vastly different networks, the one-shot approach is less reliable than the sample-based approach. In this work, we propose BONAS (Bayesian Optimized Neural Architecture Search), a sample-based NAS framework which is accelerated using weight-sharing to evaluate multiple related architectures simultaneously. Specifically, we apply Graph Convolutional Network predictor as a surrogate model for Bayesian Optimization to select multiple related candidate models in each iteration. We then apply weight-sharing to train multiple candidate models simultaneously. This approach not only accelerates the traditional sample-based approach significantly, but also keeps its reliability. This is because weight-sharing among related architectures are more reliable than those in the one-shot approach. Extensive experiments are conducted to verify the effectiveness of our method over many competing algorithms.

</details>

<details>

<summary>2020-11-25 03:51:49 - Combinatorial 3D Shape Generation via Sequential Assembly</summary>

- *Jungtaek Kim, Hyunsoo Chung, Jinhwi Lee, Minsu Cho, Jaesik Park*

- `2004.07414v2` - [abs](http://arxiv.org/abs/2004.07414v2) - [pdf](http://arxiv.org/pdf/2004.07414v2)

> Sequential assembly with geometric primitives has drawn attention in robotics and 3D vision since it yields a practical blueprint to construct a target shape. However, due to its combinatorial property, a greedy method falls short of generating a sequence of volumetric primitives. To alleviate this consequence induced by a huge number of feasible combinations, we propose a combinatorial 3D shape generation framework. The proposed framework reflects an important aspect of human generation processes in real life -- we often create a 3D shape by sequentially assembling unit primitives with geometric constraints. To find the desired combination regarding combination evaluations, we adopt Bayesian optimization, which is able to exploit and explore efficiently the feasible regions constrained by the current primitive placements. An evaluation function conveys global structure guidance for an assembly process and stability in terms of gravity and external forces simultaneously. Experimental results demonstrate that our method successfully generates combinatorial 3D shapes and simulates more realistic generation processes. We also introduce a new dataset for combinatorial 3D shape generation. All the codes are available at \url{https://github.com/POSTECH-CVLab/Combinatorial-3D-Shape-Generation}.

</details>

<details>

<summary>2020-11-25 10:32:12 - Post-Processed Posteriors for Banded Covariances</summary>

- *Kwangmin Lee, Kyoungjae Lee, Jaeyong Lee*

- `2011.12627v1` - [abs](http://arxiv.org/abs/2011.12627v1) - [pdf](http://arxiv.org/pdf/2011.12627v1)

> We consider Bayesian inference of banded covariance matrices and propose a post-processed posterior. The post-processing of the posterior consists of two steps. In the first step, posterior samples are obtained from the conjugate inverse-Wishart posterior which does not satisfy any structural restrictions. In the second step, the posterior samples are transformed to satisfy the structural restriction through a post-processing function. The conceptually straightforward procedure of the post-processed posterior makes its computation efficient and can render interval estimators of functionals of covariance matrices. We show that it has nearly optimal minimax rates for banded covariances among all possible pairs of priors and post-processing functions. Furthermore, we prove that the expected coverage probability of the $(1-\alpha)100\%$ highest posterior density region of the post-processed posterior is asymptotically $1-\alpha$ with respect to a conventional posterior distribution. It implies that the highest posterior density region of the post-processed posterior is, on average, a credible set of a conventional posterior. The advantages of the post-processed posterior are demonstrated by a simulation study and a real data analysis.

</details>

<details>

<summary>2020-11-25 10:51:24 - Design of Experiments for Verifying Biomolecular Networks</summary>

- *Ruby Sedgwick, John Goertz, Molly Stevens, Ruth Misener, Mark van der Wilk*

- `2011.10575v2` - [abs](http://arxiv.org/abs/2011.10575v2) - [pdf](http://arxiv.org/pdf/2011.10575v2)

> There is a growing trend in molecular and synthetic biology of using mechanistic (non machine learning) models to design biomolecular networks. Once designed, these networks need to be validated by experimental results to ensure the theoretical network correctly models the true system. However, these experiments can be expensive and time consuming. We propose a design of experiments approach for validating these networks efficiently. Gaussian processes are used to construct a probabilistic model of the discrepancy between experimental results and the designed response, then a Bayesian optimization strategy used to select the next sample points. We compare different design criteria and develop a stopping criterion based on a metric that quantifies this discrepancy over the whole surface, and its uncertainty. We test our strategy on simulated data from computer models of biochemical processes.

</details>

<details>

<summary>2020-11-25 14:18:40 - Surrogate-based Bayesian Comparison of Computationally Expensive Models: Application to Microbially Induced Calcite Precipitation</summary>

- *Stefania Scheurer, Aline Schäfer Rodrigues Silva, Farid Mohammadi, Johannes Hommel, Sergey Oladyshkin, Bernd Flemisch, Wolfgang Nowak*

- `2011.12756v1` - [abs](http://arxiv.org/abs/2011.12756v1) - [pdf](http://arxiv.org/pdf/2011.12756v1)

> Geochemical processes in subsurface reservoirs affected by microbial activity change the material properties of porous media. This is a complex biogeochemical process in subsurface reservoirs that currently contains strong conceptual uncertainty. This means, several modeling approaches describing the biogeochemical process are plausible and modelers face the uncertainty of choosing the most appropriate one. Once observation data becomes available, a rigorous Bayesian model selection accompanied by a Bayesian model justifiability analysis could be employed to choose the most appropriate model, i.e. the one that describes the underlying physical processes best in the light of the available data. However, biogeochemical modeling is computationally very demanding because it conceptualizes different phases, biomass dynamics, geochemistry, precipitation and dissolution in porous media. Therefore, the Bayesian framework cannot be based directly on the full computational models as this would require too many expensive model evaluations. To circumvent this problem, we suggest performing both Bayesian model selection and justifiability analysis after constructing surrogates for the competing biogeochemical models. Here, we use the arbitrary polynomial chaos expansion. We account for the approximation error in the Bayesian analysis by introducing novel correction factors for the resulting model weights. Thereby, we extend the Bayesian justifiability analysis and assess model similarities for computationally expensive models. We demonstrate the method on a representative scenario for microbially induced calcite precipitation in a porous medium. Our extension of the justifiability analysis provides a suitable approach for the comparison of computationally demanding models and gives an insight on the necessary amount of data for a reliable model performance.

</details>

<details>

<summary>2020-11-25 21:37:44 - Greedy inference with structure-exploiting lazy maps</summary>

- *Michael C. Brennan, Daniele Bigoni, Olivier Zahm, Alessio Spantini, Youssef Marzouk*

- `1906.00031v3` - [abs](http://arxiv.org/abs/1906.00031v3) - [pdf](http://arxiv.org/pdf/1906.00031v3)

> We propose a framework for solving high-dimensional Bayesian inference problems using \emph{structure-exploiting} low-dimensional transport maps or flows. These maps are confined to a low-dimensional subspace (hence, lazy), and the subspace is identified by minimizing an upper bound on the Kullback--Leibler divergence (hence, structured). Our framework provides a principled way of identifying and exploiting low-dimensional structure in an inference problem. It focuses the expressiveness of a transport map along the directions of most significant discrepancy from the posterior, and can be used to build deep compositions of lazy maps, where low-dimensional projections of the parameters are iteratively transformed to match the posterior. We prove weak convergence of the generated sequence of distributions to the posterior, and we demonstrate the benefits of the framework on challenging inference problems in machine learning and differential equations, using inverse autoregressive flows and polynomial maps as examples of the underlying density estimators.

</details>

<details>

<summary>2020-11-26 01:17:57 - A Scalable Partitioned Approach to Model Massive Nonstationary Non-Gaussian Spatial Datasets</summary>

- *Benjamin Seiyon Lee, Jaewoo Park*

- `2011.13083v1` - [abs](http://arxiv.org/abs/2011.13083v1) - [pdf](http://arxiv.org/pdf/2011.13083v1)

> Nonstationary non-Gaussian spatial data are common in many disciplines, including climate science, ecology, epidemiology, and social sciences. Examples include count data on disease incidence and binary satellite data on cloud mask (cloud/no-cloud). Modeling such datasets as stationary spatial processes can be unrealistic since they are collected over large heterogeneous domains (i.e., spatial behavior differs across subregions). Although several approaches have been developed for nonstationary spatial models, these have focused primarily on Gaussian responses. In addition, fitting nonstationary models for large non-Gaussian datasets is computationally prohibitive. To address these challenges, we propose a scalable algorithm for modeling such data by leveraging parallel computing in modern high-performance computing systems. We partition the spatial domain into disjoint subregions and fit locally nonstationary models using a carefully curated set of spatial basis functions. Then, we combine the local processes using a novel neighbor-based weighting scheme. Our approach scales well to massive datasets (e.g., 1 million samples) and can be implemented in nimble, a popular software environment for Bayesian hierarchical modeling. We demonstrate our method to simulated examples and two large real-world datasets pertaining to infectious diseases and remote sensing.

</details>

<details>

<summary>2020-11-26 10:36:55 - Exact posterior distributions of wide Bayesian neural networks</summary>

- *Jiri Hron, Yasaman Bahri, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein*

- `2006.10541v2` - [abs](http://arxiv.org/abs/2006.10541v2) - [pdf](http://arxiv.org/pdf/2006.10541v2)

> Recent work has shown that the prior over functions induced by a deep Bayesian neural network (BNN) behaves as a Gaussian process (GP) as the width of all layers becomes large. However, many BNN applications are concerned with the BNN function space posterior. While some empirical evidence of the posterior convergence was provided in the original works of Neal (1996) and Matthews et al. (2018), it is limited to small datasets or architectures due to the notorious difficulty of obtaining and verifying exactness of BNN posterior approximations. We provide the missing theoretical proof that the exact BNN posterior converges (weakly) to the one induced by the GP limit of the prior. For empirical validation, we show how to generate exact samples from a finite BNN on a small dataset via rejection sampling.

</details>

<details>

<summary>2020-11-26 14:11:58 - Fast Bayesian Deconvolution using Simple Reversible Jump Moves</summary>

- *Koki Okajima, Kenji Nagata, Masato Okada*

- `2011.13301v1` - [abs](http://arxiv.org/abs/2011.13301v1) - [pdf](http://arxiv.org/pdf/2011.13301v1)

> We propose a Markov chain Monte Carlo-based deconvolution method designed to estimate the number of peaks in spectral data, along with the optimal parameters of each radial basis function. Assuming cases where the number of peaks is unknown, and a sweep simulation on all candidate models is computationally unrealistic, the proposed method efficiently searches over the probable candidates via trans-dimensional moves assisted by annealing effects from replica exchange Monte Carlo moves. Through simulation using synthetic data, the proposed method demonstrates its advantages over conventional sweep simulations, particularly in model selection problems. Application to a set of olivine reflectance spectral data with varying forsterite and fayalite mixture ratios reproduced results obtained from previous mineralogical research, indicating that our method is applicable to deconvolution on real data sets.

</details>

<details>

<summary>2020-11-27 08:12:18 - Distributed Variational Bayesian Algorithms Over Sensor Networks</summary>

- *Junhao Hua, Chunguang Li*

- `2011.13600v1` - [abs](http://arxiv.org/abs/2011.13600v1) - [pdf](http://arxiv.org/pdf/2011.13600v1)

> Distributed inference/estimation in Bayesian framework in the context of sensor networks has recently received much attention due to its broad applicability. The variational Bayesian (VB) algorithm is a technique for approximating intractable integrals arising in Bayesian inference. In this paper, we propose two novel distributed VB algorithms for general Bayesian inference problem, which can be applied to a very general class of conjugate-exponential models. In the first approach, the global natural parameters at each node are optimized using a stochastic natural gradient that utilizes the Riemannian geometry of the approximation space, followed by an information diffusion step for cooperation with the neighbors. In the second method, a constrained optimization formulation for distributed estimation is established in natural parameter space and solved by alternating direction method of multipliers (ADMM). An application of the distributed inference/estimation of a Bayesian Gaussian mixture model is then presented, to evaluate the effectiveness of the proposed algorithms. Simulations on both synthetic and real datasets demonstrate that the proposed algorithms have excellent performance, which are almost as good as the corresponding centralized VB algorithm relying on all data available in a fusion center.

</details>

<details>

<summary>2020-11-27 14:06:25 - Uncertainty-driven ensembles of deep architectures for multiclass classification. Application to COVID-19 diagnosis in chest X-ray images</summary>

- *Juan E. Arco, A. Ortiz, J. Ramirez, F. J. Martinez-Murcia, Yu-Dong Zhang, Juan M. Gorriz*

- `2011.14894v1` - [abs](http://arxiv.org/abs/2011.14894v1) - [pdf](http://arxiv.org/pdf/2011.14894v1)

> Respiratory diseases kill million of people each year. Diagnosis of these pathologies is a manual, time-consuming process that has inter and intra-observer variability, delaying diagnosis and treatment. The recent COVID-19 pandemic has demonstrated the need of developing systems to automatize the diagnosis of pneumonia, whilst Convolutional Neural Network (CNNs) have proved to be an excellent option for the automatic classification of medical images. However, given the need of providing a confidence classification in this context it is crucial to quantify the reliability of the model's predictions. In this work, we propose a multi-level ensemble classification system based on a Bayesian Deep Learning approach in order to maximize performance while quantifying the uncertainty of each classification decision. This tool combines the information extracted from different architectures by weighting their results according to the uncertainty of their predictions. Performance of the Bayesian network is evaluated in a real scenario where simultaneously differentiating between four different pathologies: control vs bacterial pneumonia vs viral pneumonia vs COVID-19 pneumonia. A three-level decision tree is employed to divide the 4-class classification into three binary classifications, yielding an accuracy of 98.06% and overcoming the results obtained by recent literature. The reduced preprocessing needed for obtaining this high performance, in addition to the information provided about the reliability of the predictions evidence the applicability of the system to be used as an aid for clinicians.

</details>

<details>

<summary>2020-11-27 15:39:59 - Rank Bounds for Approximating Gaussian Densities in the Tensor-Train Format</summary>

- *Paul B. Rohrbach, Sergey Dolgov, Lars Grasedyck, Robert Scheichl*

- `2001.08187v2` - [abs](http://arxiv.org/abs/2001.08187v2) - [pdf](http://arxiv.org/pdf/2001.08187v2)

> Low-rank tensor approximations have shown great potential for uncertainty quantification in high dimensions, for example, to build surrogate models that can be used to speed up large-scale inference problems (Eigel et al., Inverse Problems 34, 2018; Dolgov et al., Statistics & Computing 30, 2020). The feasibility and efficiency of such approaches depends critically on the rank that is necessary to represent or approximate the underlying distribution. In this paper, a-priori rank bounds for approximations in the functional tensor-train representation for the case of Gaussian models are developed. It is shown that under suitable conditions on the precision matrix, the Gaussian density can be approximated to high accuracy without suffering from an exponential growth of complexity as the dimension increases. These results provide a rigorous justification of the suitability and the limitations of low-rank tensor methods in a simple but important model case. Numerical experiments confirm that the rank bounds capture the qualitative behavior of the rank structure when varying the parameters of the precision matrix and the accuracy of the approximation. Finally, the practical relevance of the theoretical results is demonstrated in the context of a Bayesian filtering problem.

</details>

<details>

<summary>2020-11-27 15:53:36 - Comparison of Bayesian Nonparametric Density Estimation Methods</summary>

- *Adel Bedoui, Ori Rosen*

- `2011.13800v1` - [abs](http://arxiv.org/abs/2011.13800v1) - [pdf](http://arxiv.org/pdf/2011.13800v1)

> In this paper, we propose a nonparametric Bayesian approach for Lindsey and penalized Gaussian mixtures methods. We compare these methods with the Dirichlet process mixture model. Our approach is a Bayesian nonparametric method not based solely on a parametric family of probability distributions. Thus, the fitted models are more robust to model misspecification. Also, with the Bayesian approach, we have the entire posterior distribution of our parameter of interest; it can be summarized through credible intervals, mean, median, standard deviation, quantiles, etc. The Lindsey, penalized Gaussian mixtures, and Dirichlet process mixture methods are reviewed. The estimations are performed via Markov chain Monte Carlo (MCMC) methods. The penalized Gaussian mixtures method is implemented via Hamiltonian Monte Carlo (HMC). We show that under certain regularity conditions, and as n increases, the posterior distribution of the weights converges to a Normal distribution. Simulation results and data analysis are reported.

</details>

<details>

<summary>2020-11-27 19:11:56 - Equivalence of Convergence Rates of Posterior Distributions and Bayes Estimators for Functions and Nonparametric Functionals</summary>

- *Zejian Liu, Meng Li*

- `2011.13967v1` - [abs](http://arxiv.org/abs/2011.13967v1) - [pdf](http://arxiv.org/pdf/2011.13967v1)

> We study the posterior contraction rates of a Bayesian method with Gaussian process priors in nonparametric regression and its plug-in property for differential operators. For a general class of kernels, we establish convergence rates of the posterior measure of the regression function and its derivatives, which are both minimax optimal up to a logarithmic factor for functions in certain classes. Our calculation shows that the rate-optimal estimation of the regression function and its derivatives share the same choice of hyperparameter, indicating that the Bayes procedure remarkably adapts to the order of derivatives and enjoys a generalized plug-in property that extends real-valued functionals to function-valued functionals. This leads to a practically simple method for estimating the regression function and its derivatives, whose finite sample performance is assessed using simulations.   Our proof shows that, under certain conditions, to any convergence rate of Bayes estimators there corresponds the same convergence rate of the posterior distributions (i.e., posterior contraction rate), and vice versa. This equivalence holds for a general class of Gaussian processes and covers the regression function and its derivative functionals, under both the $L_2$ and $L_{\infty}$ norms. In addition to connecting these two fundamental large sample properties in Bayesian and non-Bayesian regimes, such equivalence enables a new routine to establish posterior contraction rates by calculating convergence rates of nonparametric point estimators.   At the core of our argument is an operator-theoretic framework for kernel ridge regression and equivalent kernel techniques. We derive a range of sharp non-asymptotic bounds that are pivotal in establishing convergence rates of nonparametric point estimators and the equivalence theory, which may be of independent interest.

</details>

<details>

<summary>2020-11-28 02:07:27 - Covariate Distribution Aware Meta-learning</summary>

- *Amrith Setlur, Saket Dingliwal, Barnabas Poczos*

- `2007.02523v3` - [abs](http://arxiv.org/abs/2007.02523v3) - [pdf](http://arxiv.org/pdf/2007.02523v3)

> Meta-learning has proven to be successful for few-shot learning across the regression, classification, and reinforcement learning paradigms. Recent approaches have adopted Bayesian interpretations to improve gradient-based meta-learners by quantifying the uncertainty of the post-adaptation estimates. Most of these works almost completely ignore the latent relationship between the covariate distribution $(p(x))$ of a task and the corresponding conditional distribution $p(y|x)$. In this paper, we identify the need to explicitly model the meta-distribution over the task covariates in a hierarchical Bayesian framework. We begin by introducing a graphical model that leverages the samples from the marginal $p(x)$ to better infer the posterior over the optimal parameters of the conditional distribution $(p(y|x))$ for each task. Based on this model we propose a computationally feasible meta-learning algorithm by introducing meaningful relaxations in our final objective. We demonstrate the gains of our algorithm over initialization based meta-learning baselines on popular classification benchmarks. Finally, to understand the potential benefit of modeling task covariates we further evaluate our method on a synthetic regression dataset.

</details>

<details>

<summary>2020-11-29 08:53:43 - A Variational Approach for Learning from Positive and Unlabeled Data</summary>

- *Hui Chen, Fangqing Liu, Yin Wang, Liyue Zhao, Hao Wu*

- `1906.00642v6` - [abs](http://arxiv.org/abs/1906.00642v6) - [pdf](http://arxiv.org/pdf/1906.00642v6)

> Learning binary classifiers only from positive and unlabeled (PU) data is an important and challenging task in many real-world applications, including web text classification, disease gene identification and fraud detection, where negative samples are difficult to verify experimentally. Most recent PU learning methods are developed based on the conventional misclassification risk of the supervised learning type, and they require to solve the intractable risk estimation problem by approximating the negative data distribution or the class prior. In this paper, we introduce a variational principle for PU learning that allows us to quantitatively evaluate the modeling error of the Bayesian classifier directly from given data. This leads to a loss function which can be efficiently calculated without any intermediate step or model, and a variational learning method can then be employed to optimize the classifier under general conditions. In addition, the discriminative performance and numerical stability of the variational PU learning method can be further improved by incorporating a margin maximizing loss function. We illustrate the effectiveness of the proposed variational method on a number of benchmark examples.

</details>

<details>

<summary>2020-11-29 19:21:55 - On the effectiveness of the European Central Bank's conventional and unconventional policies under uncertainty</summary>

- *Niko Hauzenberger, Michael Pfarrhofer, Anna Stelzer*

- `2011.14424v1` - [abs](http://arxiv.org/abs/2011.14424v1) - [pdf](http://arxiv.org/pdf/2011.14424v1)

> In this paper, we investigate the effectiveness of conventional and unconventional monetary policy measures by the European Central Bank (ECB) conditional on the prevailing level of uncertainty. To obtain exogenous variation in central bank policy, we rely on high-frequency surprises in financial market data for the euro area (EA) around policy announcement dates. We trace the dynamic effects of shocks to the short-term policy rate, forward guidance and quantitative easing on several key macroeconomic and financial quantities alongside survey-based measures of expectations. For this purpose, we propose a Bayesian smooth-transition vector autoregression (ST-VAR). Our results suggest that transmission channels are impaired when uncertainty is elevated. While conventional monetary policy is less effective during such periods, and sometimes also forward guidance, quantitative easing measures seem to work comparatively well in uncertain times.

</details>

<details>

<summary>2020-11-30 04:03:43 - Quasi-Bayesian Inference for Production Frontiers</summary>

- *Xiaobin Liu, Thomas Tao Yang, Yichong Zhang*

- `1709.08846v3` - [abs](http://arxiv.org/abs/1709.08846v3) - [pdf](http://arxiv.org/pdf/1709.08846v3)

> We propose a quasi-Bayesian method to conduct inference for the production frontier. This approach combines multiple first-stage extreme quantile estimates by the quasi-Bayesian method to produce the point estimate and confidence interval for the production frontier. We show the asymptotic properties of the proposed estimator and the validity of the inference procedure. The finite sample performance of our method is illustrated through simulations and an empirical application.

</details>

<details>

<summary>2020-11-30 11:41:16 - Global estimation and scenario-based projections of sex ratio at birth and missing female births using a Bayesian hierarchical time series mixture model</summary>

- *Fengqing Chao, Patrick Gerland, Alex R. Cook, Leontine Alkema*

- `2006.07101v2` - [abs](http://arxiv.org/abs/2006.07101v2) - [pdf](http://arxiv.org/pdf/2006.07101v2)

> The sex ratio at birth (SRB) is defined as the ratio of male to female live births. The SRB imbalance in parts of the world over the past several decades is a direct consequence of sex-selective abortion, driven by the co-existence of son preference, readily available technology of prenatal sex determination, and fertility decline. Estimation and projection of the degree of SRB imbalance is complicated because of variability in SRB reference levels and because of the uncertainty associated with SRB observations. We develop Bayesian hierarchical time series mixture models for SRB estimation and scenario-based projections for all countries from 1950 to 2100. We model the SRB regional and national reference levels, and the fluctuation around national reference levels. We identify countries at risk of SRB imbalances and model both (i) the absence or presence of sex ratio transitions in such countries and, if present, (ii) the transition process. The transition model of SRB imbalance captures three stages (increase, stagnation and convergence back to SRB baselines). The model identifies countries with statistical evidence of SRB inflation in a fully Bayesian approach. The scenario-based SRB projections are based on the sex ratio transition model with varying assumptions regarding the occurrence of a sex ratio transition in at-risk countries. Projections are used to quantify the future burden of missing female births due to sex-selective abortions under different scenarios.

</details>

<details>

<summary>2020-11-30 15:53:43 - Towards constraining warm dark matter with stellar streams through neural simulation-based inference</summary>

- *Joeri Hermans, Nilanjan Banik, Christoph Weniger, Gianfranco Bertone, Gilles Louppe*

- `2011.14923v1` - [abs](http://arxiv.org/abs/2011.14923v1) - [pdf](http://arxiv.org/pdf/2011.14923v1)

> A statistical analysis of the observed perturbations in the density of stellar streams can in principle set stringent contraints on the mass function of dark matter subhaloes, which in turn can be used to constrain the mass of the dark matter particle. However, the likelihood of a stellar density with respect to the stream and subhaloes parameters involves solving an intractable inverse problem which rests on the integration of all possible forward realisations implicitly defined by the simulation model. In order to infer the subhalo abundance, previous analyses have relied on Approximate Bayesian Computation (ABC) together with domain-motivated but handcrafted summary statistics. Here, we introduce a likelihood-free Bayesian inference pipeline based on Amortised Approximate Likelihood Ratios (AALR), which automatically learns a mapping between the data and the simulator parameters and obviates the need to handcraft a possibly insufficient summary statistic. We apply the method to the simplified case where stellar streams are only perturbed by dark matter subhaloes, thus neglecting baryonic substructures, and describe several diagnostics that demonstrate the effectiveness of the new method and the statistical quality of the learned estimator.

</details>

<details>

<summary>2020-11-30 16:39:32 - AutoBayes: Automated Bayesian Graph Exploration for Nuisance-Robust Inference</summary>

- *Andac Demir, Toshiaki Koike-Akino, Ye Wang, Deniz Erdogmus*

- `2007.01255v3` - [abs](http://arxiv.org/abs/2007.01255v3) - [pdf](http://arxiv.org/pdf/2007.01255v3)

> Learning data representations that capture task-related features, but are invariant to nuisance variations remains a key challenge in machine learning. We introduce an automated Bayesian inference framework, called AutoBayes, that explores different graphical models linking classifier, encoder, decoder, estimator and adversarial network blocks to optimize nuisance-invariant machine learning pipelines. AutoBayes also enables learning disentangled representations, where the latent variable is split into multiple pieces to impose various relationships with the nuisance variation and task labels. We benchmark the framework on several public datasets, and provide analysis of its capability for subject-transfer learning with/without variational modeling and adversarial training. We demonstrate a significant performance improvement with ensemble learning across explored graphical models.

</details>

<details>

<summary>2020-11-30 17:37:42 - A Markov Chain Monte-Carlo Approach to Dose-Response Optimization Using Probabilistic Programming (RStan)</summary>

- *Dorsa Mohammadi Arezooji*

- `2011.15034v1` - [abs](http://arxiv.org/abs/2011.15034v1) - [pdf](http://arxiv.org/pdf/2011.15034v1)

> A hierarchical logistic regression Bayesian model is proposed and implemented in R to model the probability of patient improvement corresponding to any given dosage of a certain drug. RStan is used to obtain samples from the posterior distributions via Markov Chain Monte-Carlo (MCMC) sampling. The effects of selecting different families of prior distributions are examined and finally, the posterior distributions are compared across RStan, and two other environments, namely PyMC, and AgenaRisk.

</details>

<details>

<summary>2020-11-30 18:32:34 - Use of Bayesian Nonparametric methods for Estimating the Measurements in High Clutter</summary>

- *Bahman Moraffah, Christ Richmond, Raha Moraffah, Antonia Papandreou-Suppappola*

- `2012.09785v1` - [abs](http://arxiv.org/abs/2012.09785v1) - [pdf](http://arxiv.org/pdf/2012.09785v1)

> Robust tracking of a target in a clutter environment is an important and challenging task. In recent years, the nearest neighbor methods and probabilistic data association filters were proposed. However, the performance of these methods diminishes as the number of measurements increases. In this paper, we propose a robust generative approach to effectively model multiple sensor measurements for tracking a moving target in an environment with high clutter. We assume a time-dependent number of measurements that include sensor observations with unknown origin, some of which may only contain clutter with no additional information. We robustly and accurately estimate the trajectory of the moving target in a high clutter environment with an unknown number of clutters by employing Bayesian nonparametric modeling. In particular, we employ a class of joint Bayesian nonparametric models to construct the joint prior distribution of target and clutter measurements such that the conditional distributions follow a Dirichlet process. The marginalized Dirichlet process prior of the target measurements is then used in a Bayesian tracker to estimate the dynamically-varying target state. We show through experiments that the tracking performance and effectiveness of our proposed framework are increased by suppressing high clutter measurements. In addition, we show that our proposed method outperforms existing methods such as nearest neighbor and probability data association filters.

</details>

<details>

<summary>2020-11-30 21:52:07 - Fast Matrix Square Roots with Applications to Gaussian Processes and Bayesian Optimization</summary>

- *Geoff Pleiss, Martin Jankowiak, David Eriksson, Anil Damle, Jacob R. Gardner*

- `2006.11267v2` - [abs](http://arxiv.org/abs/2006.11267v2) - [pdf](http://arxiv.org/pdf/2006.11267v2)

> Matrix square roots and their inverses arise frequently in machine learning, e.g., when sampling from high-dimensional Gaussians $\mathcal{N}(\mathbf 0, \mathbf K)$ or whitening a vector $\mathbf b$ against covariance matrix $\mathbf K$. While existing methods typically require $O(N^3)$ computation, we introduce a highly-efficient quadratic-time algorithm for computing $\mathbf K^{1/2} \mathbf b$, $\mathbf K^{-1/2} \mathbf b$, and their derivatives through matrix-vector multiplication (MVMs). Our method combines Krylov subspace methods with a rational approximation and typically achieves $4$ decimal places of accuracy with fewer than $100$ MVMs. Moreover, the backward pass requires little additional computation. We demonstrate our method's applicability on matrices as large as $50,\!000 \times 50,\!000$ - well beyond traditional methods - with little approximation error. Applying this increased scalability to variational Gaussian processes, Bayesian optimization, and Gibbs sampling results in more powerful models with higher accuracy.

</details>


## 2020-12

<details>

<summary>2020-12-01 09:10:10 - On The Gaussian Approximation To Bayesian Posterior Distributions</summary>

- *Christoph Fuhrmann, Hanns Ludwig Harney, Klaus Harney, Andreas Müller*

- `2012.00748v1` - [abs](http://arxiv.org/abs/2012.00748v1) - [pdf](http://arxiv.org/pdf/2012.00748v1)

> The present article derives the minimal number $N$ of observations needed to consider a Bayesian posterior distribution as Gaussian. Two examples are presented. Within one of them, a chi-squared distribution, the observable $x$ as well as the parameter $\xi$ are defined all over the real axis, in the other one, the binomial distribution, the observable $x$ is an entire number while the parameter $\xi$ is defined on a finite interval of the real axis. The required minimal $N$ is high in the first case and low for the binomial model. In both cases the precise definition of the measure $\mu$ on the scale of $\xi$ is crucial.

</details>

<details>

<summary>2020-12-01 10:28:04 - Nowcasting in a Pandemic using Non-Parametric Mixed Frequency VARs</summary>

- *Florian Huber, Gary Koop, Luca Onorante, Michael Pfarrhofer, Josef Schreiner*

- `2008.12706v3` - [abs](http://arxiv.org/abs/2008.12706v3) - [pdf](http://arxiv.org/pdf/2008.12706v3)

> This paper develops Bayesian econometric methods for posterior inference in non-parametric mixed frequency VARs using additive regression trees. We argue that regression tree models are ideally suited for macroeconomic nowcasting in the face of extreme observations, for instance those produced by the COVID-19 pandemic of 2020. This is due to their flexibility and ability to model outliers. In an application involving four major euro area countries, we find substantial improvements in nowcasting performance relative to a linear mixed frequency VAR.

</details>

<details>

<summary>2020-12-01 11:05:34 - Systematic errors in estimates of $R_t$ from symptomatic cases in the presence of observation bias</summary>

- *Guido Sanguinetti*

- `2012.02105v1` - [abs](http://arxiv.org/abs/2012.02105v1) - [pdf](http://arxiv.org/pdf/2012.02105v1)

> We consider the problem of estimating the reproduction number $R_t$ of an epidemic for populations where the probability of detection of cases depends on a known covariate. We argue that in such cases the normal empirical estimator can fail when the prevalence of cases among groups changes with time. We propose a Bayesian strategy to resolve the problem, as well as a simple solution in the case of large number of cases. We illustrate the issue and its solution on a simple yet realistic simulation study, and discuss the general relevance of the issue to the current covid19 pandemic.

</details>

<details>

<summary>2020-12-01 13:10:00 - Improved Variational Bayesian Phylogenetic Inference with Normalizing Flows</summary>

- *Cheng Zhang*

- `2012.00459v1` - [abs](http://arxiv.org/abs/2012.00459v1) - [pdf](http://arxiv.org/pdf/2012.00459v1)

> Variational Bayesian phylogenetic inference (VBPI) provides a promising general variational framework for efficient estimation of phylogenetic posteriors. However, the current diagonal Lognormal branch length approximation would significantly restrict the quality of the approximating distributions. In this paper, we propose a new type of VBPI, VBPI-NF, as a first step to empower phylogenetic posterior estimation with deep learning techniques. By handling the non-Euclidean branch length space of phylogenetic models with carefully designed permutation equivariant transformations, VBPI-NF uses normalizing flows to provide a rich family of flexible branch length distributions that generalize across different tree topologies. We show that VBPI-NF significantly improves upon the vanilla VBPI on a benchmark of challenging real data Bayesian phylogenetic inference problems. Further investigation also reveals that the structured parameterization in those permutation equivariant transformations can provide additional amortization benefit.

</details>

<details>

<summary>2020-12-01 14:42:13 - Towards a Kernel based Uncertainty Decomposition Framework for Data and Models</summary>

- *Rishabh Singh, Jose C. Principe*

- `2001.11495v4` - [abs](http://arxiv.org/abs/2001.11495v4) - [pdf](http://arxiv.org/pdf/2001.11495v4)

> This paper introduces a new framework for quantifying predictive uncertainty for both data and models that relies on projecting the data into a Gaussian reproducing kernel Hilbert space (RKHS) and transforming the data probability density function (PDF) in a way that quantifies the flow of its gradient as a topological potential field quantified at all points in the sample space. This enables the decomposition of the PDF gradient flow by formulating it as a moment decomposition problem using operators from quantum physics, specifically the Schrodinger's formulation. We experimentally show that the higher order modes systematically cluster the different tail regions of the PDF, thereby providing unprecedented discriminative resolution of data regions having high epistemic uncertainty. In essence, this approach decomposes local realizations of the data PDF in terms of uncertainty moments. We apply this framework as a surrogate tool for predictive uncertainty quantification of point-prediction neural network models, overcoming various limitations of conventional Bayesian based uncertainty quantification methods. Experimental comparisons with some established methods illustrate performance advantages exhibited by our framework.

</details>

<details>

<summary>2020-12-01 15:35:19 - BayesTime: Bayesian Functional Principal Components for Sparse Longitudinal Data</summary>

- *Lingjing Jiang, Yuan Zhong, Chris Elrod, Loki Natarajan, Rob Knight, Wesley K. Thompson*

- `2012.00579v1` - [abs](http://arxiv.org/abs/2012.00579v1) - [pdf](http://arxiv.org/pdf/2012.00579v1)

> Modeling non-linear temporal trajectories is of fundamental interest in many application areas, such as in longitudinal microbiome analysis. Many existing methods focus on estimating mean trajectories, but it is also often of value to assess temporal patterns of individual subjects. Sparse principal components analysis (SFPCA) serves as a useful tool for assessing individual variation in non-linear trajectories; however its application to real data often requires careful model selection criteria and diagnostic tools. Here, we propose a Bayesian approach to SFPCA, which allows users to use the efficient leave-one-out cross-validation (LOO) with Pareto-smoothed importance sampling (PSIS) for model selection, and to utilize the estimated shape parameter from PSIS-LOO and also the posterior predictive checks for graphical model diagnostics. This Bayesian implementation thus enables careful application of SFPCA to a wide range of longitudinal data applications.

</details>

<details>

<summary>2020-12-01 17:31:24 - Bayesian classification for dating archaeological sites via projectile points</summary>

- *Carmen Armero, Gonzalo García-Donato, Joaquín Jiménez-Puerto, Salvador Pardo-Gordó, Joan Bernabeu*

- `2012.00657v1` - [abs](http://arxiv.org/abs/2012.00657v1) - [pdf](http://arxiv.org/pdf/2012.00657v1)

> Dating is a key element for archaeologists. We propose a Bayesian approach to provide chronology to sites that have neither radiocarbon dating nor clear stratigraphy and whose only information comes from lithic arrowheads. This classifier is based on the Dirichlet-multinomial inferential process and posterior predictive distributions. The procedure is applied to predict the period of a set of undated sites located in the east of the Iberian Peninsula during the IVth and IIIrd millennium cal. BC.

</details>

<details>

<summary>2020-12-01 18:32:32 - Measuring and assessing economic uncertainty</summary>

- *Oscar Claveria*

- `2012.00722v1` - [abs](http://arxiv.org/abs/2012.00722v1) - [pdf](http://arxiv.org/pdf/2012.00722v1)

> This paper evaluates the dynamic response of economic activity to shocks in uncertainty as percieved by agents.The study focuses on the comparison between the perception of economic uncertainty by manufacturers and consumers.Since uncertainty is not directly observable, we approximate it using the geometric discrepancy indicator of Claveria et al.(2019).This approach allows us quantifying the proportion of disagreement in business and consumer expectations of eleven European countries and the Euro Area.First, we compute three independent indices of discrepancy corresponding to three dimensions of uncertainty (economic, inflation and employment) and we average them to obtain aggregate disagreement measures for businesses and for consumers.Next, we use a bivariate Bayesian vector autoregressive framework to estimate the impulse response functions to innovations in disagreement in every country.We find that the effect on economic activity of shocks to the perception of uncertainty differ markedly between manufacturers and consumers.On the one hand, shocks to consumer discrepancy tend to be of greater magnitude and duration than those to manufacturer discrepancy.On the other hand, innovations in disagreement between the two collectives have an opposite effect on economic activity:shocks to manufacturer discrepancy lead to a decrease in economic activity, as opposed to shocks to consumer discrepancy.This finding is of particular relevance to researchers when using cross-sectional dispersion of survey-based expectations, since the effect on economic growth of shocks to disagreement depend on the type of agent.

</details>

<details>

<summary>2020-12-01 19:28:55 - BayesFlow: Learning complex stochastic models with invertible neural networks</summary>

- *Stefan T. Radev, Ulf K. Mertens, Andreass Voss, Lynton Ardizzone, Ullrich Köthe*

- `2003.06281v4` - [abs](http://arxiv.org/abs/2003.06281v4) - [pdf](http://arxiv.org/pdf/2003.06281v4)

> Estimating the parameters of mathematical models is a common problem in almost all branches of science. However, this problem can prove notably difficult when processes and model descriptions become increasingly complex and an explicit likelihood function is not available. With this work, we propose a novel method for globally amortized Bayesian inference based on invertible neural networks which we call BayesFlow. The method uses simulation to learn a global estimator for the probabilistic mapping from observed data to underlying model parameters. A neural network pre-trained in this way can then, without additional training or optimization, infer full posteriors on arbitrary many real datasets involving the same model family. In addition, our method incorporates a summary network trained to embed the observed data into maximally informative summary statistics. Learning summary statistics from data makes the method applicable to modeling scenarios where standard inference techniques with hand-crafted summary statistics fail. We demonstrate the utility of BayesFlow on challenging intractable models from population dynamics, epidemiology, cognitive science and ecology. We argue that BayesFlow provides a general framework for building amortized Bayesian parameter estimation machines for any forward model from which data can be simulated.

</details>

<details>

<summary>2020-12-01 22:55:11 - A Programmable Approach to Neural Network Compression</summary>

- *Vinu Joseph, Saurav Muralidharan, Animesh Garg, Michael Garland, Ganesh Gopalakrishnan*

- `1911.02497v2` - [abs](http://arxiv.org/abs/1911.02497v2) - [pdf](http://arxiv.org/pdf/1911.02497v2)

> Deep neural networks (DNNs) frequently contain far more weights, represented at a higher precision, than are required for the specific task which they are trained to perform. Consequently, they can often be compressed using techniques such as weight pruning and quantization that reduce both the model size and inference time without appreciable loss in accuracy. However, finding the best compression strategy and corresponding target sparsity for a given DNN, hardware platform, and optimization objective currently requires expensive, frequently manual, trial-and-error experimentation. In this paper, we introduce a programmable system for model compression called Condensa. Users programmatically compose simple operators, in Python, to build more complex and practically interesting compression strategies. Given a strategy and user-provided objective (such as minimization of running time), Condensa uses a novel Bayesian optimization-based algorithm to automatically infer desirable sparsities. Our experiments on four real-world DNNs demonstrate memory footprint and hardware runtime throughput improvements of 188x and 2.59x, respectively, using at most ten samples per search. We have released a reference implementation of Condensa at https://github.com/NVlabs/condensa.

</details>

<details>

<summary>2020-12-02 03:10:28 - Spatial Multivariate Trees for Big Data Bayesian Regression</summary>

- *Michele Peruzzi, David B. Dunson*

- `2012.00943v1` - [abs](http://arxiv.org/abs/2012.00943v1) - [pdf](http://arxiv.org/pdf/2012.00943v1)

> High resolution geospatial data are challenging because standard geostatistical models based on Gaussian processes are known to not scale to large data sizes. While progress has been made towards methods that can be computed more efficiently, considerably less attention has been devoted to big data methods that allow the description of complex relationships between several outcomes recorded at high resolutions by different sensors. Our Bayesian multivariate regression models based on spatial multivariate trees (SpamTrees) achieve scalability via conditional independence assumptions on latent random effects following a treed directed acyclic graph. Information-theoretic arguments and considerations on computational efficiency guide the construction of the tree and the related efficient sampling algorithms in imbalanced multivariate settings. In addition to simulated data examples, we illustrate SpamTrees using a large climate data set which combines satellite data with land-based station data. Source code is available at https://github.com/mkln/spamtree

</details>

<details>

<summary>2020-12-02 05:08:52 - Perturbed factor analysis: Accounting for group differences in exposure profiles</summary>

- *Arkaprava Roy, Isaac Lavine, Amy H. Herring, David B. Dunson*

- `1910.03021v3` - [abs](http://arxiv.org/abs/1910.03021v3) - [pdf](http://arxiv.org/pdf/1910.03021v3)

> In this article, we investigate group differences in phthalate exposure profiles using NHANES data. Phthalates are a family of industrial chemicals used in plastics and as solvents. There is increasing evidence of adverse health effects of exposure to phthalates on reproduction and neuro-development, and concern about racial disparities in exposure. We would like to identify a single set of low-dimensional factors summarizing exposure to different chemicals, while allowing differences across groups. Improving on current multi-group additive factor models, we propose a class of Perturbed Factor Analysis (PFA) models that assume a common factor structure after perturbing the data via multiplication by a group-specific matrix. Bayesian inference algorithms are defined using a matrix normal hierarchical model for the perturbation matrices. The resulting model is just as flexible as current approaches in allowing arbitrarily large differences across groups but has substantial advantages that we illustrate in simulation studies. Applying PFA to NHANES data, we learn common factors summarizing exposures to phthalates, while showing clear differences across groups.

</details>

<details>

<summary>2020-12-02 09:06:39 - Variable fusion for Bayesian linear regression via spike-and-slab priors</summary>

- *Shengyi Wu, Kaito Shimamura, Kohei Yoshikawa, Kazuaki Murayama, Shuichi Kawano*

- `2003.13299v3` - [abs](http://arxiv.org/abs/2003.13299v3) - [pdf](http://arxiv.org/pdf/2003.13299v3)

> In linear regression models, fusion of coefficients is used to identify predictors having similar relationships with a response. This is called variable fusion. This paper presents a novel variable fusion method in terms of Bayesian linear regression models. We focus on hierarchical Bayesian models based on a spike-and-slab prior approach. A spike-and-slab prior is tailored to perform variable fusion. To obtain estimates of the parameters, we develop a Gibbs sampler for the parameters. Simulation studies and a real data analysis show that our proposed method achieves better performance than previous methods.

</details>

<details>

<summary>2020-12-02 14:13:35 - Hierarchical spline for time series forecasting: An application to Naval ship engine failure rate</summary>

- *Hyunji Moon, Jinwoo Choi*

- `2012.01224v1` - [abs](http://arxiv.org/abs/2012.01224v1) - [pdf](http://arxiv.org/pdf/2012.01224v1)

> Predicting equipment failure is important because it could improve availability and cut down the operating budget. Previous literature has attempted to model failure rate with bathtub-formed function, Weibull distribution, Bayesian network, or AHP. But these models perform well with a sufficient amount of data and could not incorporate the two salient characteristics; imbalanced category and sharing structure. Hierarchical model has the advantage of partial pooling. The proposed model is based on Bayesian hierarchical B-spline. Time series of the failure rate of 99 Republic of Korea Naval ships are modeled hierarchically, where each layer corresponds to ship engine, engine type, and engine archetype. As a result of the analysis, the suggested model predicted the failure rate of an entire lifetime accurately in multiple situational conditions, such as prior knowledge of the engine.

</details>

<details>

<summary>2020-12-03 07:54:00 - Approximate Bayesian inference for a spatial point process model exhibiting regularity and random aggregation</summary>

- *Ninna Vihrs, Jesper Møller, Alan E. Gelfand*

- `2003.10490v3` - [abs](http://arxiv.org/abs/2003.10490v3) - [pdf](http://arxiv.org/pdf/2003.10490v3)

> In this paper, we propose a doubly stochastic spatial point process model with both aggregation and repulsion. This model combines the ideas behind Strauss processes and log Gaussian Cox processes. The likelihood for this model is not expressible in closed form but it is easy to simulate realisations under the model. We therefore explain how to use approximate Bayesian computation (ABC) to carry out statistical inference for this model. We suggest a method for model validation based on posterior predictions and global envelopes. We illustrate the ABC procedure and model validation approach using both simulated point patterns and a real data example.

</details>

<details>

<summary>2020-12-03 08:19:40 - Sub-Weibull distributions: generalizing sub-Gaussian and sub-Exponential properties to heavier-tailed distributions</summary>

- *Mariia Vladimirova, Stephane Girard, Hien Nguyen, Julyan Arbel*

- `1905.04955v4` - [abs](http://arxiv.org/abs/1905.04955v4) - [pdf](http://arxiv.org/pdf/1905.04955v4)

> We propose the notion of sub-Weibull distributions, which are characterised by tails lighter than (or equally light as) the right tail of a Weibull distribution. This novel class generalises the sub-Gaussian and sub-Exponential families to potentially heavier-tailed distributions. Sub-Weibull distributions are parameterized by a positive tail index $\theta$ and reduce to sub-Gaussian distributions for $\theta=1/2$ and to sub-Exponential distributions for $\theta=1$. A characterisation of the sub-Weibull property based on moments and on the moment generating function is provided and properties of the class are studied. An estimation procedure for the tail parameter is proposed and is applied to an example stemming from Bayesian deep learning.

</details>

<details>

<summary>2020-12-03 17:06:11 - A Note on Bayesian Modeling Specification of Censored Data in JAGS</summary>

- *Xinyue Qi, Shouhao Zhou, Martyn Plummer*

- `2012.02074v1` - [abs](http://arxiv.org/abs/2012.02074v1) - [pdf](http://arxiv.org/pdf/2012.02074v1)

> Just Another Gibbs Sampling (JAGS) is a convenient tool to draw posterior samples using Markov Chain Monte Carlo for Bayesian modeling. However, the built-in function dinterval() to model censored data misspecifies the computation of deviance function, which may limit its usage to perform likelihood based model comparison. To establish an automatic approach to specify the correct deviance function in JAGS, we propose a simple alternative modeling strategy to implement Bayesian model selection for analysis of censored outcomes. The proposed approach is applicable to a broad spectrum of data types, which include survival data and many other right-, left- and interval-censored Bayesian model structures.

</details>

<details>

<summary>2020-12-03 18:48:46 - Filtering and improved Uncertainty Quantification in the dynamic estimation of effective reproduction numbers</summary>

- *Marcos A. Capistrán, Antonio Capella, J. Andrés Christen*

- `2012.02168v1` - [abs](http://arxiv.org/abs/2012.02168v1) - [pdf](http://arxiv.org/pdf/2012.02168v1)

> The effective reproduction number $R_t$ measures an infectious disease's transmissibility as the number of secondary infections in one reproduction time in a population having both susceptible and non-susceptible hosts. Current approaches do not quantify the uncertainty correctly in estimating $R_t$, as expected by the observed variability in contagion patterns. We elaborate on the Bayesian estimation of $R_t$ by improving on the Poisson sampling model of Cori et al. (2013). By adding an autoregressive latent process, we build a Dynamic Linear Model on the log of observed $R_t$s, resulting in a filtering type Bayesian inference. We use a conjugate analysis, and all calculations are explicit. Results show an improved uncertainty quantification on the estimation of $R_t$'s, with a reliable method that could safely be used by non-experts and within other forecasting systems. We illustrate our approach with recent data from the current COVID19 epidemic in Mexico.

</details>

<details>

<summary>2020-12-03 22:04:35 - Bayesian hierarchical space-time models to improve multispecies assessment by combining observations from disparate fish surveys</summary>

- *Chibuzor C. Nnanatu, Murray S. A. Thompson, Michael A. Spence, Elena Couce, Jeroen van der Kooij, Christopher P. Lynam*

- `2012.02196v1` - [abs](http://arxiv.org/abs/2012.02196v1) - [pdf](http://arxiv.org/pdf/2012.02196v1)

> Many wild species affected by human activities require multiple surveys with differing designs to capture behavioural response to wide ranging habitat conditions and map and quantify them. While data from for example intersecting but disparate fish surveys using different gear, are widely available, differences in design and methodology often limit their integration. Novel statistical approaches which can draw on observations from diverse sources could enhance our understanding of multiple species distributions simultaneously and thus provide vital evidence needed to conserve their populations and biodiversity at large. Using a novel Bayesian hierarchical binomial-lognormal hurdle modelling approach within the INLA-SPDE framework, we combined and analysed acoustic and bottom trawl survey data for herring, sprat and northeast Atlantic mackerel in the North Sea. These models were implemented using INLA-SPDE techniques. By accounting for gear-specific efficiencies across surveys in addition to increased spatial coverage, we gained larger statistical power with greatly minimised uncertainties in estimation. Our statistical approach provides a methodological development to improve the evidence base for multispecies assessment and marine ecosystem-based management. And on a broader scale, it could be readily applied where disparate biological surveys and sampling methods intersect, e.g. to provide information on biodiversity patterns using global datasets of species distributions.

</details>

<details>

<summary>2020-12-04 03:12:09 - Optimal Bayesian hierarchical model to accelerate the development of tissue-agnostic drugs and basket trials</summary>

- *Liyun Jiang, Lei Nie, Fangrong Yan, Ying Yuan*

- `2012.02378v1` - [abs](http://arxiv.org/abs/2012.02378v1) - [pdf](http://arxiv.org/pdf/2012.02378v1)

> Tissue-agnostic trials enroll patients based on their genetic biomarkers, not tumor type, in an attempt to determine if a new drug can successfully treat disease conditions based on biomarkers. The Bayesian hierarchical model (BHM) provides an attractive approach to design phase II tissue-agnostic trials by allowing information borrowing across multiple disease types. In this article, we elucidate two intrinsic and inevitable issues that may limit the use of BHM to tissue-agnostic trials: sensitivity to the prior specification of the shrinkage parameter and the competing "interest" among disease types in increasing power and controlling type I error. To address these issues, we propose the optimal BHM (OBHM) approach. With OBHM, we first specify a flexible utility function to quantify the tradeoff between type I error and power across disease type based on the study objectives, and then we select the prior of the shrinkage parameter to optimize the utility function of clinical and regulatory interest. OBMH effectively balances type I and II errors, addresses the sensitivity of the prior selection, and reduces the "unwarranted" subjectivity in the prior selection. Simulation study shows that the resulting OBHM and its extensions, clustered OBHM (COBHM) and adaptive OBHM (AOBHM), have desirable operating characteristics, outperforming some existing methods with better balanced power and type I error control. Our method provides a systematic, rigorous way to apply BHM and solve the common problem of blindingly using a non-informative inverse-gamma prior (with a large variance) or priors arbitrarily chosen that may lead to pathological statistical properties.

</details>

<details>

<summary>2020-12-04 16:19:37 - Bayesian Active Learning for Wearable Stress and Affect Detection</summary>

- *Abhijith Ragav, Gautham Krishna Gudur*

- `2012.02702v1` - [abs](http://arxiv.org/abs/2012.02702v1) - [pdf](http://arxiv.org/pdf/2012.02702v1)

> In the recent past, psychological stress has been increasingly observed in humans, and early detection is crucial to prevent health risks. Stress detection using on-device deep learning algorithms has been on the rise owing to advancements in pervasive computing. However, an important challenge that needs to be addressed is handling unlabeled data in real-time via suitable ground truthing techniques (like Active Learning), which should help establish affective states (labels) while also selecting only the most informative data points to query from an oracle. In this paper, we propose a framework with capabilities to represent model uncertainties through approximations in Bayesian Neural Networks using Monte-Carlo (MC) Dropout. This is combined with suitable acquisition functions for active learning. Empirical results on a popular stress and affect detection dataset experimented on a Raspberry Pi 2 indicate that our proposed framework achieves a considerable efficiency boost during inference, with a substantially low number of acquired pool points during active learning across various acquisition functions. Variation Ratios achieves an accuracy of 90.38% which is comparable to the maximum test accuracy achieved while training on about 40% lesser data.

</details>

<details>

<summary>2020-12-04 17:05:12 - Distributed Classification of Urban Congestion Using VANET</summary>

- *Al Mallah Ranwa, Farooq Bilal, Quintero Alejandro*

- `1904.12685v2` - [abs](http://arxiv.org/abs/1904.12685v2) - [pdf](http://arxiv.org/pdf/1904.12685v2)

> Vehicular Ad-hoc NETworks (VANET) can efficiently detect traffic congestion, but detection is not enough because congestion can be further classified as recurrent and non-recurrent congestion (NRC). In particular, NRC in an urban network is mainly caused by incidents, workzones, special events and adverse weather. We propose a framework for the real-time distributed classification of congestion into its components on a heterogeneous urban road network using VANET. We present models built on an understanding of the spatial and temporal causality measures and trained on synthetic data extended from a real case study of Cologne. Our performance evaluation shows a predictive accuracy of 87.63\% for the deterministic Classification Tree (CT), 88.83\% for the Naive Bayesian classifier (NB), 89.51\% for Random Forest (RF) and 89.17\% for the boosting technique. This framework can assist transportation agencies in reducing urban congestion by developing effective congestion mitigation strategies knowing the root causes of congestion.

</details>

<details>

<summary>2020-12-04 20:12:01 - Bayesian modelling for spatially misaligned health areal data: a multiple membership approach</summary>

- *Marco Gramatica, Peter Congdon, Silvia Liverani*

- `2004.05334v3` - [abs](http://arxiv.org/abs/2004.05334v3) - [pdf](http://arxiv.org/pdf/2004.05334v3)

> Diabetes prevalence is on the rise in the UK, and for public health strategy, estimation of relative disease risk and subsequent mapping is important. We consider an application to London data on diabetes prevalence and mortality. In order to improve the estimation of relative risks we analyse jointly prevalence and mortality data to ensure borrowing strength over the two outcomes. The available data involves two spatial frameworks, areas (middle level super output areas, MSOAs), and general practices (GPs) recruiting patients from several areas. This raises a spatial misalignment issue that we deal with by employing the multiple membership principle. Specifically we translate area spatial effects to explain GP practice prevalence according to proportions of GP populations resident in different areas. A sparse implementation in Stan of both the MCAR and GMCAR allows the comparison of these bivariate priors as well as exploring the different implications for the mapping patterns for both outcomes. The necessary causal precedence of diabetes prevalence over mortality allows a specific conditionality assumption in the GMCAR, not always present in the context of disease mapping.

</details>

<details>

<summary>2020-12-05 15:07:07 - Bayesian Median Autoregression for Robust Time Series Forecasting</summary>

- *Zijian Zeng, Meng Li*

- `2001.01116v2` - [abs](http://arxiv.org/abs/2001.01116v2) - [pdf](http://arxiv.org/pdf/2001.01116v2)

> We develop a Bayesian median autoregressive (BayesMAR) model for time series forecasting. The proposed method utilizes time-varying quantile regression at the median, favorably inheriting the robustness of median regression in contrast to the widely used mean-based methods. Motivated by a working Laplace likelihood approach in Bayesian quantile regression, BayesMAR adopts a parametric model bearing the same structure as autoregressive models by altering the Gaussian error to Laplace, leading to a simple, robust, and interpretable modeling strategy for time series forecasting. We estimate model parameters by Markov chain Monte Carlo. Bayesian model averaging is used to account for model uncertainty, including the uncertainty in the autoregressive order, in addition to a Bayesian model selection approach. The proposed methods are illustrated using simulations and real data applications. An application to U.S. macroeconomic data forecasting shows that BayesMAR leads to favorable and often superior predictive performance compared to the selected mean-based alternatives under various loss functions that encompass both point and probabilistic forecasts. The proposed methods are generic and can be used to complement a rich class of methods that build on autoregressive models.

</details>

<details>

<summary>2020-12-05 21:41:15 - Computing Bayes: Bayesian Computation from 1763 to the 21st Century</summary>

- *Gael M. Martin, David T. Frazier, Christian P. Robert*

- `2004.06425v2` - [abs](http://arxiv.org/abs/2004.06425v2) - [pdf](http://arxiv.org/pdf/2004.06425v2)

> The Bayesian statistical paradigm uses the language of probability to express uncertainty about the phenomena that generate observed data. Probability distributions thus characterize Bayesian analysis, with the rules of probability used to transform prior probability distributions for all unknowns - parameters, latent variables, models - into posterior distributions, subsequent to the observation of data. Conducting Bayesian analysis requires the evaluation of integrals in which these probability distributions appear. Bayesian computation is all about evaluating such integrals in the typical case where no analytical solution exists. This paper takes the reader on a chronological tour of Bayesian computation over the past two and a half centuries. Beginning with the one-dimensional integral first confronted by Bayes in 1763, through to recent problems in which the unknowns number in the millions, we place all computational problems into a common framework, and describe all computational methods using a common notation. The aim is to help new researchers in particular - and more generally those interested in adopting a Bayesian approach to empirical work - make sense of the plethora of computational techniques that are now on offer; understand when and why different methods are useful; and see the links that do exist, between them all.

</details>

<details>

<summary>2020-12-06 08:46:31 - A Bayesian approach to deconvolution in well test analysis</summary>

- *Themistoklis Botsas, Jonathan A. Cumming, Ian H. Jermyn*

- `2012.03217v1` - [abs](http://arxiv.org/abs/2012.03217v1) - [pdf](http://arxiv.org/pdf/2012.03217v1)

> In petroleum well test analysis, deconvolution is used to obtain information about the reservoir system. This information is contained in the response function, which can be estimated by solving an inverse problem in the pressure and flow rate measurements. Our Bayesian approach to this problem is based upon a parametric physical model of reservoir behaviour, derived from the solution for fluid flow in a general class of reservoirs. This permits joint parametric Bayesian inference for both the reservoir parameters and the true pressure and rate values, which is essential due to the typical levels of observation error. Using a set of flexible priors for the reservoir parameters to restrict the solution space to physical behaviours, samples from the posterior are generated using MCMC. Summaries and visualisations of the reservoir parameters' posterior, response, and true pressure and rate values can be produced, interpreted, and model selection can be performed. The method is validated through a synthetic application, and applied to a field data set. The results are comparable to the state of the art solution, but through our method we gain access to system parameters, we can incorporate prior knowledge that excludes non-physical results, and we can quantify parameter uncertainty.

</details>

<details>

<summary>2020-12-06 17:15:55 - Bayesian Modeling of Spatial Molecular Profiling Data via Gaussian Process</summary>

- *Qiwei Li, Minzhe Zhang, Yang Xie, Guanghua Xiao*

- `2012.03326v1` - [abs](http://arxiv.org/abs/2012.03326v1) - [pdf](http://arxiv.org/pdf/2012.03326v1)

> The location, timing, and abundance of gene expression (both mRNA and proteins) within a tissue define the molecular mechanisms of cell functions. Recent technology breakthroughs in spatial molecular profiling, including imaging-based technologies and sequencing-based technologies, have enabled the comprehensive molecular characterization of single cells while preserving their spatial and morphological contexts. This new bioinformatics scenario calls for effective and robust computational methods to identify genes with spatial patterns. We represent a novel Bayesian hierarchical model to analyze spatial transcriptomics data, with several unique characteristics. It models the zero-inflated and over-dispersed counts by deploying a zero-inflated negative binomial model that greatly increases model stability and robustness. Besides, the Bayesian inference framework allows us to borrow strength in parameter estimation in a de novo fashion. As a result, the proposed model shows competitive performances in accuracy and robustness over existing methods in both simulation studies and two real data applications. The related R/C++ source code is available at https://github.com/Minzhe/BOOST-GP.

</details>

<details>

<summary>2020-12-07 01:44:38 - High-dimensional Multivariate Geostatistics: A Bayesian Matrix-Normal Approach</summary>

- *Lu Zhang, Sudipto Banerjee, Andrew O. Finley*

- `2003.10051v5` - [abs](http://arxiv.org/abs/2003.10051v5) - [pdf](http://arxiv.org/pdf/2003.10051v5)

> Joint modeling of spatially-oriented dependent variables is commonplace in the environmental sciences, where scientists seek to estimate the relationships among a set of environmental outcomes accounting for dependence among these outcomes and the spatial dependence for each outcome. Such modeling is now sought for massive data sets with variables measured at a very large number of locations. Bayesian inference, while attractive for accommodating uncertainties through hierarchical structures, can become computationally onerous for modeling massive spatial data sets because of its reliance on iterative estimation algorithms. This manuscript develops a conjugate Bayesian framework for analyzing multivariate spatial data using analytically tractable posterior distributions that obviate iterative algorithms. We discuss differences between modeling the multivariate response itself as a spatial process and that of modeling a latent process in a hierarchical model. We illustrate the computational and inferential benefits of these models using simulation studies and analysis of a Vegetation Index data set with spatially dependent observations numbering in the millions.

</details>

<details>

<summary>2020-12-07 07:51:23 - Adaptive Local Bayesian Optimization Over Multiple Discrete Variables</summary>

- *Taehyeon Kim, Jaeyeon Ahn, Nakyil Kim, Seyoung Yun*

- `2012.03501v1` - [abs](http://arxiv.org/abs/2012.03501v1) - [pdf](http://arxiv.org/pdf/2012.03501v1)

> In the machine learning algorithms, the choice of the hyperparameter is often an art more than a science, requiring labor-intensive search with expert experience. Therefore, automation on hyperparameter optimization to exclude human intervention is a great appeal, especially for the black-box functions. Recently, there have been increasing demands of solving such concealed tasks for better generalization, though the task-dependent issue is not easy to solve. The Black-Box Optimization challenge (NeurIPS 2020) required competitors to build a robust black-box optimizer across different domains of standard machine learning problems. This paper describes the approach of team KAIST OSI in a step-wise manner, which outperforms the baseline algorithms by up to +20.39%. We first strengthen the local Bayesian search under the concept of region reliability. Then, we design a combinatorial kernel for a Gaussian process kernel. In a similar vein, we combine the methodology of Bayesian and multi-armed bandit,(MAB) approach to select the values with the consideration of the variable types; the real and integer variables are with Bayesian, while the boolean and categorical variables are with MAB. Empirical evaluations demonstrate that our method outperforms the existing methods across different tasks.

</details>

<details>

<summary>2020-12-07 08:35:45 - Exploring the Predictability of Cryptocurrencies via Bayesian Hidden Markov Models</summary>

- *Constandina Koki, Stefanos Leonardos, Georgios Piliouras*

- `2011.03741v2` - [abs](http://arxiv.org/abs/2011.03741v2) - [pdf](http://arxiv.org/pdf/2011.03741v2)

> In this paper, we consider a variety of multi-state Hidden Markov models for predicting and explaining the Bitcoin, Ether and Ripple returns in the presence of state (regime) dynamics. In addition, we examine the effects of several financial, economic and cryptocurrency specific predictors on the cryptocurrency return series. Our results indicate that the Non-Homogeneous Hidden Markov (NHHM) model with four states has the best one-step-ahead forecasting performance among all competing models for all three series. The dominance of the predictive densities over the single regime random walk model relies on the fact that the states capture alternating periods with distinct return characteristics. In particular, the four state NHHM model distinguishes bull, bear and calm regimes for the Bitcoin series, and periods with different profit and risk magnitudes for the Ether and Ripple series. Also, conditionally on the hidden states, it identifies predictors with different linear and non-linear effects on the cryptocurrency returns. These empirical findings provide important insight for portfolio management and policy implementation.

</details>

<details>

<summary>2020-12-07 08:45:57 - On spike-and-slab priors for Bayesian equation discovery of nonlinear dynamical systems via sparse linear regression</summary>

- *Rajdip Nayek, Ramon Fuentes, Keith Worden, Elizabeth J. Cross*

- `2012.01937v2` - [abs](http://arxiv.org/abs/2012.01937v2) - [pdf](http://arxiv.org/pdf/2012.01937v2)

> This paper presents the use of spike-and-slab (SS) priors for discovering governing differential equations of motion of nonlinear structural dynamic systems. The problem of discovering governing equations is cast as that of selecting relevant variables from a predetermined dictionary of basis variables and solved via sparse Bayesian linear regression. The SS priors, which belong to a class of discrete-mixture priors and are known for their strong sparsifying (or shrinkage) properties, are employed to induce sparse solutions and select relevant variables. Three different variants of SS priors are explored for performing Bayesian equation discovery. As the posteriors with SS priors are analytically intractable, a Markov chain Monte Carlo (MCMC)-based Gibbs sampler is employed for drawing posterior samples of the model parameters; the posterior samples are used for variable selection and parameter estimation in equation discovery. The proposed algorithm has been applied to four systems of engineering interest, which include a baseline linear system, and systems with cubic stiffness, quadratic viscous damping, and Coulomb damping. The results demonstrate the effectiveness of the SS priors in identifying the presence and type of nonlinearity in the system. Additionally, comparisons with the Relevance Vector Machine (RVM) - that uses a Student's-t prior - indicate that the SS priors can achieve better model selection consistency, reduce false discoveries, and derive models that have superior predictive accuracy. Finally, the Silverbox experimental benchmark is used to validate the proposed methodology.

</details>

<details>

<summary>2020-12-07 11:21:13 - Mapping Leaf Area Index with a Smartphone and Gaussian Processes</summary>

- *Manuel Campos-Taberner, Franciso Javier García-Haro, Álvaro Moreno, María Amparo Gilabert, Sergio Sánchez-Ruiz, Beatriz Martínez, Gustau Camps-Valls*

- `2012.04596v1` - [abs](http://arxiv.org/abs/2012.04596v1) - [pdf](http://arxiv.org/pdf/2012.04596v1)

> Leaf area index (LAI) is a key biophysical parameter used to determine foliage cover and crop growth in environmental studies. Smartphones are nowadays ubiquitous sensor devices with high computational power, moderate cost, and high-quality sensors. A smartphone app, called PocketLAI, was recently presented and tested for acquiring ground LAI estimates. In this letter, we explore the use of state-of-the-art nonlinear Gaussian process regression (GPR) to derive spatially explicit LAI estimates over rice using ground data from PocketLAI and Landsat 8 imagery. GPR has gained popularity in recent years because of their solid Bayesian foundations that offers not only high accuracy but also confidence intervals for the retrievals. We show the first LAI maps obtained with ground data from a smartphone combined with advanced machine learning. This work compares LAI predictions and confidence intervals of the retrievals obtained with PocketLAI to those obtained with classical instruments, such as digital hemispheric photography (DHP) and LI-COR LAI-2000. This letter shows that all three instruments got comparable result but the PocketLAI is far cheaper. The proposed methodology hence opens a wide range of possible applications at moderate cost.

</details>

<details>

<summary>2020-12-07 11:44:11 - Physics-Aware Gaussian Processes in Remote Sensing</summary>

- *Gustau Camps-Valls, Luca Martino, Daniel H. Svendsen, Manuel Campos-Taberner, Jordi Muñoz-Marí, Valero Laparra, David Luengo, Francisco Javier García-Haro*

- `2012.07986v1` - [abs](http://arxiv.org/abs/2012.07986v1) - [pdf](http://arxiv.org/pdf/2012.07986v1)

> Earth observation from satellite sensory data poses challenging problems, where machine learning is currently a key player. In recent years, Gaussian Process (GP) regression has excelled in biophysical parameter estimation tasks from airborne and satellite observations. GP regression is based on solid Bayesian statistics and generally yields efficient and accurate parameter estimates. However, GPs are typically used for inverse modeling based on concurrent observations and in situ measurements only. Very often a forward model encoding the well-understood physical relations between the state vector and the radiance observations is available though and could be useful to improve predictions and understanding. In this work, we review three GP models that respect and learn the physics of the underlying processes in the context of both forward and inverse modeling. After reviewing the traditional application of GPs for parameter retrieval, we introduce a Joint GP (JGP) model that combines in situ measurements and simulated data in a single GP model. Then, we present a latent force model (LFM) for GP modeling that encodes ordinary differential equations to blend data-driven modeling and physical constraints of the system governing equations. The LFM performs multi-output regression, adapts to the signal characteristics, is able to cope with missing data in the time series, and provides explicit latent functions that allow system analysis and evaluation. Finally, we present an Automatic Gaussian Process Emulator (AGAPE) that approximates the forward physical model using concepts from Bayesian optimization and at the same time builds an optimally compact look-up-table for inversion. We give empirical evidence of the performance of these models through illustrative examples of vegetation monitoring and atmospheric modeling.

</details>

<details>

<summary>2020-12-07 13:58:58 - Bayesian model for early dose-finding in phase I trials with multiple treatment courses</summary>

- *Moreno Ursino, Lucie Biard, Sylvie Chevret*

- `2012.03700v1` - [abs](http://arxiv.org/abs/2012.03700v1) - [pdf](http://arxiv.org/pdf/2012.03700v1)

> Dose-finding clinical trials in oncology aim to determine the maximum tolerated dose (MTD) of a new drug, generally defined by the proportion of patients with short-term dose-limiting toxicities (DLTs). Model-based approaches for such phase I oncology trials have been widely designed and are mostly restricted to the DLTs occurring during the first cycle of treatment, although patients continue to receive treatment for multiple cycles. We aim to estimate the probability of DLTs over sequences of treatment cycles via a Bayesian cumulative modeling approach, where the probability of DLT is modeled taking into account the cumulative effect of the administered drug and the DLT cycle of occurrence. We propose a design, called DICE (Dose-fInding CumulativE), for dose escalation and de-escalation according to previously observed toxicities, which aims at finding the MTD sequence (MTS). We performed an extensive simulation study comparing this approach to the time-to-event continual reassessment method (TITE-CRM) and to a benchmark. In general, our approach achieved a better or comparable percentage of correct MTS selection. Moreover, we investigated the DICE prediction ability.

</details>

<details>

<summary>2020-12-07 14:35:32 - The linear conditional expectation in Hilbert space</summary>

- *Ilja Klebanov, Björn Sprungk, T. J. Sullivan*

- `2008.12070v2` - [abs](http://arxiv.org/abs/2008.12070v2) - [pdf](http://arxiv.org/pdf/2008.12070v2)

> The linear conditional expectation (LCE) provides a best linear (or rather, affine) estimate of the conditional expectation and hence plays an important r\^ole in approximate Bayesian inference, especially the Bayes linear approach. This article establishes the analytical properties of the LCE in an infinite-dimensional Hilbert space context. In addition, working in the space of affine Hilbert--Schmidt operators, we establish a regularisation procedure for this LCE. As an important application, we obtain a simple alternative derivation and intuitive justification of the conditional mean embedding formula, a concept widely used in machine learning to perform the conditioning of random variables by embedding them into reproducing kernel Hilbert spaces.

</details>

<details>

<summary>2020-12-07 16:23:02 - Ratio of counts vs ratio of rates in Poisson processes</summary>

- *Giulio D'Agostini*

- `2012.04455v1` - [abs](http://arxiv.org/abs/2012.04455v1) - [pdf](http://arxiv.org/pdf/2012.04455v1)

> The often debated issue of `ratios of small numbers of events' is approached from a probabilistic perspective, making a clear distinction between the predictive problem (forecasting numbers of events we might count under well stated assumptions, and therefore of their ratios) and inferential problem (learning about the relevant parameters of the related probability distribution, in the light of the observed number of events). The quantities of interests and their relations are visualized in a graphical model (`Bayesian network'), very useful to understand how to approach the problem following the rules of probability theory. In this paper, written with didactic intent, we discuss in detail the basic ideas, however giving some hints of how real life complications, like (uncertain) efficiencies and possible background and systematics, can be included in the analysis, as well as the possibility that the ratio of rates might depend on some physical quantity. The simple models considered in this paper allow to obtain, under reasonable assumptions, closed expressions for the rates and their ratios. Monte Carlo methods are also used, both to cross check the exact results and to evaluate by sampling the ratios of counts in the cases in which large number approximation does not hold. In particular it is shown how to make approximate inferences using a Markov Chain Monte Carlo using JAGS/rjags. Some examples of R and JAGS code are provided.

</details>

<details>

<summary>2020-12-07 16:26:54 - Online neural connectivity estimation with ensemble stimulation</summary>

- *Anne Draelos, Eva A. Naumann, John M. Pearson*

- `2007.13911v2` - [abs](http://arxiv.org/abs/2007.13911v2) - [pdf](http://arxiv.org/pdf/2007.13911v2)

> One of the primary goals of systems neuroscience is to relate the structure of neural circuits to their function, yet patterns of connectivity are difficult to establish when recording from large populations in behaving organisms. Many previous approaches have attempted to estimate functional connectivity between neurons using statistical modeling of observational data, but these approaches rely heavily on parametric assumptions and are purely correlational. Recently, however, holographic photostimulation techniques have made it possible to precisely target selected ensembles of neurons, offering the possibility of establishing direct causal links. Here, we propose a method based on noisy group testing that drastically increases the efficiency of this process in sparse networks. By stimulating small ensembles of neurons, we show that it is possible to recover binarized network connectivity with a number of tests that grows only logarithmically with population size under minimal statistical assumptions. Moreover, we prove that our approach, which reduces to an efficiently solvable convex optimization problem, can be related to Variational Bayesian inference on the binary connection weights, and we derive rigorous bounds on the posterior marginals. This allows us to extend our method to the streaming setting, where continuously updated posteriors allow for optional stopping, and we demonstrate the feasibility of inferring connectivity for networks of up to tens of thousands of neurons online. Finally, we show how our work can be theoretically linked to compressed sensing approaches, and compare results for connectivity inference in different settings.

</details>

<details>

<summary>2020-12-08 00:53:34 - Adaptive Sampling for Estimating Distributions: A Bayesian Upper Confidence Bound Approach</summary>

- *Dhruva Kartik, Neeraj Sood, Urbashi Mitra, Tara Javidi*

- `2012.04137v1` - [abs](http://arxiv.org/abs/2012.04137v1) - [pdf](http://arxiv.org/pdf/2012.04137v1)

> The problem of adaptive sampling for estimating probability mass functions (pmf) uniformly well is considered. Performance of the sampling strategy is measured in terms of the worst-case mean squared error. A Bayesian variant of the existing upper confidence bound (UCB) based approaches is proposed. It is shown analytically that the performance of this Bayesian variant is no worse than the existing approaches. The posterior distribution on the pmfs in the Bayesian setting allows for a tighter computation of upper confidence bounds which leads to significant performance gains in practice. Using this approach, adaptive sampling protocols are proposed for estimating SARS-CoV-2 seroprevalence in various groups such as location and ethnicity. The effectiveness of this strategy is discussed using data obtained from a seroprevalence survey in Los Angeles county.

</details>

<details>

<summary>2020-12-08 06:49:00 - Efficient Numerical Algorithms for the Generalized Langevin Equation</summary>

- *Benedict Leimkuhler, Matthias Sachs*

- `2012.04245v1` - [abs](http://arxiv.org/abs/2012.04245v1) - [pdf](http://arxiv.org/pdf/2012.04245v1)

> We study the design and implementation of numerical methods to solve the generalized Langevin equation (GLE) focusing on canonical sampling properties of numerical integrators. For this purpose, we cast the GLE in an extended phase space formulation and derive a family of splitting methods which generalize existing Langevin dynamics integration methods. We show exponential convergence in law and the validity of a central limit theorem for the Markov chains obtained via these integration methods, and we show that the dynamics of a suggested integration scheme is consistent with asymptotic limits of the exact dynamics and can reproduce (in the short memory limit) a superconvergence property for the analogous splitting of underdamped Langevin dynamics. We then apply our proposed integration method to several model systems, including a Bayesian inference problem. We demonstrate in numerical experiments that our method outperforms other proposed GLE integration schemes in terms of the accuracy of sampling. Moreover, using a parameterization of the memory kernel in the GLE as proposed by Ceriotti et al [9], our experiments indicate that the obtained GLE-based sampling scheme outperforms state-of-the-art sampling schemes based on underdamped Langevin dynamics in terms of robustness and efficiency.

</details>

<details>

<summary>2020-12-08 11:29:57 - Efficient Automatic CASH via Rising Bandits</summary>

- *Yang Li, Jiawei Jiang, Jinyang Gao, Yingxia Shao, Ce Zhang, Bin Cui*

- `2012.04371v1` - [abs](http://arxiv.org/abs/2012.04371v1) - [pdf](http://arxiv.org/pdf/2012.04371v1)

> The Combined Algorithm Selection and Hyperparameter optimization (CASH) is one of the most fundamental problems in Automatic Machine Learning (AutoML). The existing Bayesian optimization (BO) based solutions turn the CASH problem into a Hyperparameter Optimization (HPO) problem by combining the hyperparameters of all machine learning (ML) algorithms, and use BO methods to solve it. As a result, these methods suffer from the low-efficiency problem due to the huge hyperparameter space in CASH. To alleviate this issue, we propose the alternating optimization framework, where the HPO problem for each ML algorithm and the algorithm selection problem are optimized alternately. In this framework, the BO methods are used to solve the HPO problem for each ML algorithm separately, incorporating a much smaller hyperparameter space for BO methods. Furthermore, we introduce Rising Bandits, a CASH-oriented Multi-Armed Bandits (MAB) variant, to model the algorithm selection in CASH. This framework can take the advantages of both BO in solving the HPO problem with a relatively small hyperparameter space and the MABs in accelerating the algorithm selection. Moreover, we further develop an efficient online algorithm to solve the Rising Bandits with provably theoretical guarantees. The extensive experiments on 30 OpenML datasets demonstrate the superiority of the proposed approach over the competitive baselines.

</details>

<details>

<summary>2020-12-08 18:09:22 - Recursive Two-Step Lookahead Expected Payoff for Time-Dependent Bayesian Optimization</summary>

- *S. Ashwin Renganathan, Jeffrey Larson, Stefan Wild*

- `2006.08037v2` - [abs](http://arxiv.org/abs/2006.08037v2) - [pdf](http://arxiv.org/pdf/2006.08037v2)

> We propose a novel Bayesian method to solve the maximization of a time-dependent expensive-to-evaluate oracle. We are interested in the decision that maximizes the oracle at a finite time horizon, when relatively few noisy evaluations can be performed before the horizon. Our recursive, two-step lookahead expected payoff ($\texttt{r2LEY}$) acquisition function makes nonmyopic decisions at every stage by maximizing the estimated expected value of the oracle at the horizon. $\texttt{r2LEY}$ circumvents the evaluation of the expensive multistep (more than two steps) lookahead acquisition function by recursively optimizing a two-step lookahead acquisition function at every stage; unbiased estimators of this latter function and its gradient are utilized for efficient optimization. $\texttt{r2LEY}$ is shown to exhibit natural exploration properties far from the time horizon, enabling accurate emulation of the oracle, which is exploited in the final decision made at the horizon. To demonstrate the utility of $\texttt{r2LEY}$, we compare it with time-dependent extensions of popular myopic acquisition functions via both synthetic and real-world datasets.

</details>

<details>

<summary>2020-12-08 18:42:26 - A Bayesian Fisher-EM algorithm for discriminative Gaussian subspace clustering</summary>

- *Nicolas Jouvin, Charles Bouveyron, Pierre Latouche*

- `2012.04620v1` - [abs](http://arxiv.org/abs/2012.04620v1) - [pdf](http://arxiv.org/pdf/2012.04620v1)

> High-dimensional data clustering has become and remains a challenging task for modern statistics and machine learning, with a wide range of applications. We consider in this work the powerful discriminative latent mixture model, and we extend it to the Bayesian framework. Modeling data as a mixture of Gaussians in a low-dimensional discriminative subspace, a Gaussian prior distribution is introduced over the latent group means and a family of twelve submodels are derived considering different covariance structures. Model inference is done with a variational EM algorithm, while the discriminative subspace is estimated via a Fisher-step maximizing an unsupervised Fisher criterion. An empirical Bayes procedure is proposed for the estimation of the prior hyper-parameters, and an integrated classification likelihood criterion is derived for selecting both the number of clusters and the submodel. The performances of the resulting Bayesian Fisher-EM algorithm are investigated in two thorough simulated scenarios, regarding both dimensionality as well as noise and assessing its superiority with respect to state-of-the-art Gaussian subspace clustering models. In addition to standard real data benchmarks, an application to single image denoising is proposed, displaying relevant results. This work comes with a reference implementation for the R software in the FisherEM package accompanying the paper.

</details>

<details>

<summary>2020-12-08 19:31:38 - BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization</summary>

- *Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gordon Wilson, Eytan Bakshy*

- `1910.06403v3` - [abs](http://arxiv.org/abs/1910.06403v3) - [pdf](http://arxiv.org/pdf/1910.06403v3)

> Bayesian optimization provides sample-efficient global optimization for a broad range of applications, including automatic machine learning, engineering, physics, and experimental design. We introduce BoTorch, a modern programming framework for Bayesian optimization that combines Monte-Carlo (MC) acquisition functions, a novel sample average approximation optimization approach, auto-differentiation, and variance reduction techniques. BoTorch's modular design facilitates flexible specification and optimization of probabilistic models written in PyTorch, simplifying implementation of new acquisition functions. Our approach is backed by novel theoretical convergence results and made practical by a distinctive algorithmic foundation that leverages fast predictive distributions, hardware acceleration, and deterministic optimization. We also propose a novel "one-shot" formulation of the Knowledge Gradient, enabled by a combination of our theoretical and software contributions. In experiments, we demonstrate the improved sample efficiency of BoTorch relative to other popular libraries.

</details>

<details>

<summary>2020-12-08 20:14:08 - Diversity inducing Information Bottleneck in Model Ensembles</summary>

- *Samarth Sinha, Homanga Bharadhwaj, Anirudh Goyal, Hugo Larochelle, Animesh Garg, Florian Shkurti*

- `2003.04514v3` - [abs](http://arxiv.org/abs/2003.04514v3) - [pdf](http://arxiv.org/pdf/2003.04514v3)

> Although deep learning models have achieved state-of-the-art performance on a number of vision tasks, generalization over high dimensional multi-modal data, and reliable predictive uncertainty estimation are still active areas of research. Bayesian approaches including Bayesian Neural Nets (BNNs) do not scale well to modern computer vision tasks, as they are difficult to train, and have poor generalization under dataset-shift. This motivates the need for effective ensembles which can generalize and give reliable uncertainty estimates. In this paper, we target the problem of generating effective ensembles of neural networks by encouraging diversity in prediction. We explicitly optimize a diversity inducing adversarial loss for learning the stochastic latent variables and thereby obtain diversity in the output predictions necessary for modeling multi-modal data. We evaluate our method on benchmark datasets: MNIST, CIFAR100, TinyImageNet and MIT Places 2, and compared to the most competitive baselines show significant improvements in classification accuracy, under a shift in the data distribution and in out-of-distribution detection. Code will be released in this url https://github.com/rvl-lab-utoronto/dibs

</details>

<details>

<summary>2020-12-08 22:11:35 - Bayesian Inference for Polycrystalline Materials</summary>

- *James Matuk, Oksana Chkrebtii, Stephen Niezgoda*

- `2012.04765v1` - [abs](http://arxiv.org/abs/2012.04765v1) - [pdf](http://arxiv.org/pdf/2012.04765v1)

> Polycrystalline materials, such as metals, are comprised of heterogeneously oriented crystals. Observed crystal orientations are modelled as a sample from an orientation distribution function (ODF), which determines a variety of material properties and is therefore of great interest to practitioners. Observations consist of quaternions, 4-dimensional unit vectors reflecting both orientation and rotation of a single crystal. Thus, an ODF must account for known crystal symmetries as well as satisfy the unit length constraint. A popular method for estimating ODFs non-parametrically is symmetrized kernel density estimation. However, disadvantages of this approach include difficulty in interpreting results quantitatively, as well as in quantifying uncertainty in the ODF. We propose to use a mixture of symmetric Bingham distributions as a flexible parametric ODF model, inferring the number of mixture components, the mixture weights, and scale and location parameters based on crystal orientation data. Furthermore, our Bayesian approach allows for structured uncertainty quantification of the parameters of interest. We discuss details of the sampling methodology and conclude with analyses of various orientation datasets, interpretations of parameters of interest, and comparison with kernel density estimation methods.

</details>

<details>

<summary>2020-12-09 07:52:44 - Robust Sparse Bayesian Infinite Factor Models</summary>

- *Jaejoon Lee, Jaeyong Lee*

- `2012.04315v2` - [abs](http://arxiv.org/abs/2012.04315v2) - [pdf](http://arxiv.org/pdf/2012.04315v2)

> Most of previous works and applications of Bayesian factor model have assumed the normal likelihood regardless of its validity. We propose a Bayesian factor model for heavy-tailed high-dimensional data based on multivariate Student-$t$ likelihood to obtain better covariance estimation. We use multiplicative gamma process shrinkage prior and factor number adaptation scheme proposed in Bhattacharya & Dunson [Biometrika (2011) 291-306]. Since a naive Gibbs sampler for the proposed model suffers from slow mixing, we propose a Markov Chain Monte Carlo algorithm where fast mixing of Hamiltonian Monte Carlo is exploited for some parameters in proposed model. Simulation results illustrate the gain in performance of covariance estimation for heavy-tailed high-dimensional data. We also provide a theoretical result that the posterior of the proposed model is weakly consistent under reasonable conditions. We conclude the paper with the application of proposed factor model on breast cancer metastasis prediction given DNA signature data of cancer cell.

</details>

<details>

<summary>2020-12-09 13:53:18 - Deep Mixtures of Unigrams for uncovering Topics in Textual Data</summary>

- *Cinzia Viroli, Laura Anderlucci*

- `1902.06615v2` - [abs](http://arxiv.org/abs/1902.06615v2) - [pdf](http://arxiv.org/pdf/1902.06615v2)

> Mixtures of Unigrams are one of the simplest and most efficient tools for clustering textual data, as they assume that documents related to the same topic have similar distributions of terms, naturally described by Multinomials. When the classification task is particularly challenging, such as when the document-term matrix is high-dimensional and extremely sparse, a more composite representation can provide better insight on the grouping structure. In this work, we developed a deep version of mixtures of Unigrams for the unsupervised classification of very short documents with a large number of terms, by allowing for models with further deeper latent layers; the proposal is derived in a Bayesian framework. The behaviour of the Deep Mixtures of Unigrams is empirically compared with that of other traditional and state-of-the-art methods, namely $k$-means with cosine distance, $k$-means with Euclidean distance on data transformed according to Semantic Analysis, Partition Around Medoids, Mixture of Gaussians on semantic-based transformed data, hierarchical clustering according to Ward's method with cosine dissimilarity, Latent Dirichlet Allocation, Mixtures of Unigrams estimated via the EM algorithm, Spectral Clustering and Affinity Propagation clustering. The performance is evaluated in terms of both correct classification rate and Adjusted Rand Index. Simulation studies and real data analysis prove that going deep in clustering such data highly improves the classification accuracy.

</details>

<details>

<summary>2020-12-09 15:00:23 - Generalization bounds for deep learning</summary>

- *Guillermo Valle-Pérez, Ard A. Louis*

- `2012.04115v2` - [abs](http://arxiv.org/abs/2012.04115v2) - [pdf](http://arxiv.org/pdf/2012.04115v2)

> Generalization in deep learning has been the topic of much recent theoretical and empirical research. Here we introduce desiderata for techniques that predict generalization errors for deep learning models in supervised learning. Such predictions should 1) scale correctly with data complexity; 2) scale correctly with training set size; 3) capture differences between architectures; 4) capture differences between optimization algorithms; 5) be quantitatively not too far from the true error (in particular, be non-vacuous); 6) be efficiently computable; and 7) be rigorous. We focus on generalization error upper bounds, and introduce a categorisation of bounds depending on assumptions on the algorithm and data. We review a wide range of existing approaches, from classical VC dimension to recent PAC-Bayesian bounds, commenting on how well they perform against the desiderata.   We next use a function-based picture to derive a marginal-likelihood PAC-Bayesian bound. This bound is, by one definition, optimal up to a multiplicative constant in the asymptotic limit of large training sets, as long as the learning curve follows a power law, which is typically found in practice for deep learning problems. Extensive empirical analysis demonstrates that our marginal-likelihood PAC-Bayes bound fulfills desiderata 1-3 and 5. The results for 6 and 7 are promising, but not yet fully conclusive, while only desideratum 4 is currently beyond the scope of our bound. Finally, we comment on why this function-based bound performs significantly better than current parameter-based PAC-Bayes bounds.

</details>

<details>

<summary>2020-12-09 19:13:32 - Hard and Soft EM in Bayesian Network Learning from Incomplete Data</summary>

- *Andrea Ruggieri, Francesco Stranieri, Fabio Stella, Marco Scutari*

- `2012.05269v1` - [abs](http://arxiv.org/abs/2012.05269v1) - [pdf](http://arxiv.org/pdf/2012.05269v1)

> Incomplete data are a common feature in many domains, from clinical trials to industrial applications. Bayesian networks (BNs) are often used in these domains because of their graphical and causal interpretations. BN parameter learning from incomplete data is usually implemented with the Expectation-Maximisation algorithm (EM), which computes the relevant sufficient statistics ("soft EM") using belief propagation. Similarly, the Structural Expectation-Maximisation algorithm (Structural EM) learns the network structure of the BN from those sufficient statistics using algorithms designed for complete data. However, practical implementations of parameter and structure learning often impute missing data ("hard EM") to compute sufficient statistics instead of using belief propagation, for both ease of implementation and computational speed. In this paper, we investigate the question: what is the impact of using imputation instead of belief propagation on the quality of the resulting BNs? From a simulation study using synthetic data and reference BNs, we find that it is possible to recommend one approach over the other in several scenarios based on the characteristics of the data. We then use this information to build a simple decision tree to guide practitioners in choosing the EM algorithm best suited to their problem.

</details>

<details>

<summary>2020-12-10 01:53:20 - PoolTestR: An R package for estimating prevalence and regression modelling with pooled samples</summary>

- *Angus McLure, Ben O'Neill, Helen Mayfield, Colleen Lau, Brady McPherson*

- `2012.05405v1` - [abs](http://arxiv.org/abs/2012.05405v1) - [pdf](http://arxiv.org/pdf/2012.05405v1)

> Pooled testing (also known as group testing), where diagnostic tests are performed on pooled samples, has broad applications in the surveillance of diseases in animals and humans. An increasingly common use case is molecular xenomonitoring (MX), where surveillance of vector-borne diseases is conducted by capturing and testing large numbers of vectors (e.g. mosquitoes). The R package PoolTestR was developed to meet the needs of increasingly large and complex molecular xenomonitoring surveys but can be applied to analyse any data involving pooled testing. PoolTestR includes simple and flexible tools to estimate prevalence and fit fixed- and mixed-effect generalised linear models for pooled data in frequentist and Bayesian frameworks. Mixed-effect models allow users to account for the hierarchical sampling designs that are often employed in surveys, including MX. We demonstrate the utility of PoolTestR by applying it to a large synthetic dataset that emulates a MX survey with a hierarchical sampling design.

</details>

<details>

<summary>2020-12-10 05:29:15 - Multi-Fidelity Bayesian Optimization via Deep Neural Networks</summary>

- *Shibo Li, Wei Xing, Mike Kirby, Shandian Zhe*

- `2007.03117v4` - [abs](http://arxiv.org/abs/2007.03117v4) - [pdf](http://arxiv.org/pdf/2007.03117v4)

> Bayesian optimization (BO) is a popular framework to optimize black-box functions. In many applications, the objective function can be evaluated at multiple fidelities to enable a trade-off between the cost and accuracy. To reduce the optimization cost, many multi-fidelity BO methods have been proposed. Despite their success, these methods either ignore or over-simplify the strong, complex correlations across the fidelities, and hence can be inefficient in estimating the objective function. To address this issue, we propose Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) that can flexibly capture all kinds of complicated relationships between the fidelities to improve the objective function estimation and hence the optimization performance. We use sequential, fidelity-wise Gauss-Hermite quadrature and moment-matching to fulfill a mutual information-based acquisition function, which is computationally tractable and efficient. We show the advantages of our method in both synthetic benchmark datasets and real-world applications in engineering design.

</details>

<details>

<summary>2020-12-10 15:27:06 - Estimating Seroprevalence of SARS-CoV-2 in Ohio: A Bayesian Multilevel Poststratification Approach with Multiple Diagnostic Tests</summary>

- *David Kline, Zehang Li, Yue Chu, Jon Wakefield, William C. Miller, Abigail Norris Turner, Samuel J Clark*

- `2011.09033v2` - [abs](http://arxiv.org/abs/2011.09033v2) - [pdf](http://arxiv.org/pdf/2011.09033v2)

> Globally the SARS-CoV-2 coronavirus has infected more than 59 million people and killed more than 1.39 million. Designing and monitoring interventions to slow and stop the spread of the virus require knowledge of how many people have been and are currently infected, where they live, and how they interact. The first step is an accurate assessment of the population prevalence of past infections. There are very few population-representative prevalence studies of the SARS-CoV-2 coronavirus, and only two American states -- Indiana and Connecticut -- have reported probability-based sample surveys that characterize state-wide prevalence of the SARS-CoV-2 coronavirus. One of the difficulties is the fact that the tests to detect and characterize SARS-CoV-2 coronavirus antibodies are new, not well characterized, and generally function poorly. During July, 2020, a survey representing all adults in the State of Ohio in the United States collected biomarkers and information on protective behavior related to the SARS-CoV-2 coronavirus. Several features of the survey make it difficult to estimate past prevalence: 1) a low response rate, 2) very low number of positive cases, and 3) the fact that multiple, poor quality serological tests were used to detect SARS-CoV-2 antibodies. We describe a new Bayesian approach for analyzing the biomarker data that simultaneously addresses these challenges and characterizes the potential effect of selective response. The model does not require survey sample weights, accounts for multiple, imperfect antibody test results, and characterizes uncertainty related to the sample survey and the multiple, imperfect, potentially correlated tests.

</details>

<details>

<summary>2020-12-10 20:48:43 - Bayesian nonstationary and nonparametric covariance estimation for large spatial data</summary>

- *Brian Kidd, Matthias Katzfuss*

- `2012.05967v1` - [abs](http://arxiv.org/abs/2012.05967v1) - [pdf](http://arxiv.org/pdf/2012.05967v1)

> In spatial statistics, it is often assumed that the spatial field of interest is stationary and its covariance has a simple parametric form, but these assumptions are not appropriate in many applications. Given replicate observations of a Gaussian spatial field, we propose nonstationary and nonparametric Bayesian inference on the spatial dependence. Instead of estimating the quadratic (in the number of spatial locations) entries of the covariance matrix, the idea is to infer a near-linear number of nonzero entries in a sparse Cholesky factor of the precision matrix. Our prior assumptions are motivated by recent results on the exponential decay of the entries of this Cholesky factor for Matern-type covariances under a specific ordering scheme. Our methods are highly scalable and parallelizable. We conduct numerical comparisons and apply our methodology to climate-model output, enabling statistical emulation of an expensive physical model.

</details>

<details>

<summary>2020-12-10 20:48:55 - Power prior models for treatment effect estimation in a small n, sequential, multiple assignment, randomized trial</summary>

- *Yan-Cheng Chao, Thomas M. Braun, Roy N. Tamura, Kelley M. Kidwell*

- `2012.05968v1` - [abs](http://arxiv.org/abs/2012.05968v1) - [pdf](http://arxiv.org/pdf/2012.05968v1)

> A small n, sequential, multiple assignment, randomized trial (snSMART) is a small sample, two-stage design where participants receive up to two treatments sequentially, but the second treatment depends on response to the first treatment. The treatment effect of interest in an snSMART is the first-stage response rate, but outcomes from both stages can be used to obtain more information from a small sample. A novel way to incorporate the outcomes from both stages applies power prior models, in which first stage outcomes from an snSMART are regarded as the primary data and second stage outcomes are regarded as supplemental. We apply existing power prior models to snSMART data, and we also develop new extensions of power prior models. All methods are compared to each other and to the Bayesian joint stage model (BJSM) via simulation studies. By comparing the biases and the efficiency of the response rate estimates among all proposed power prior methods, we suggest application of Fisher's exact test or the Bhattacharyya's overlap measure to an snSMART to estimate the treatment effect in an snSMART, which both have performance mostly as good or better than the BJSM. We describe the situations where each of these suggested approaches is preferred.

</details>

<details>

<summary>2020-12-11 09:11:55 - Bayesian Variable Selection for Single Index Logistic Model</summary>

- *Yinrui Sun, Hangjin Jiang*

- `2012.06199v1` - [abs](http://arxiv.org/abs/2012.06199v1) - [pdf](http://arxiv.org/pdf/2012.06199v1)

> In the era of big data, variable selection is a key technology for handling high-dimensional problems with a small sample size but a large number of covariables. Different variable selection methods were proposed for different models, such as linear model, logistic model and generalized linear model. However, fewer works focused on variable selection for single index models, especially, for single index logistic model, due to the difficulty arose from the unknown link function and the slow mixing rate of MCMC algorithm for traditional logistic model. In this paper, we proposed a Bayesian variable selection procedure for single index logistic model by taking the advantage of Gaussian process and data augmentation. Numerical results from simulations and real data analysis show the advantage of our method over the state of arts.

</details>

<details>

<summary>2020-12-11 13:10:55 - Bayesian semiparametric modelling of phase-varying point processes</summary>

- *Bastian Galasso, Yoav Zemel, Miguel de Carvalho*

- `1812.09607v2` - [abs](http://arxiv.org/abs/1812.09607v2) - [pdf](http://arxiv.org/pdf/1812.09607v2)

> We propose a Bayesian semiparametric approach for registration of multiple point processes. Our approach entails modelling the mean measures of the phase-varying point processes with a Bernstein-Dirichlet prior, which induces a prior on the space of all warp functions. Theoretical results on the support of the induced priors are derived, and posterior consistency is obtained under mild conditions. Numerical experiments suggest a good performance of the proposed methods, and a climatology real-data example is used to showcase how the method can be employed in practice.

</details>

<details>

<summary>2020-12-11 15:08:40 - Implicit Feedback Deep Collaborative Filtering Product Recommendation System</summary>

- *Karthik Raja Kalaiselvi Bhaskar, Deepa Kundur, Yuri Lawryshyn*

- `2009.08950v2` - [abs](http://arxiv.org/abs/2009.08950v2) - [pdf](http://arxiv.org/pdf/2009.08950v2)

> In this paper, several Collaborative Filtering (CF) approaches with latent variable methods were studied using user-item interactions to capture important hidden variations of the sparse customer purchasing behaviours. The latent factors are used to generalize the purchasing pattern of the customers and to provide product recommendations. CF with Neural Collaborative Filtering(NCF) was shown to produce the highest Normalized Discounted Cumulative Gain (NDCG) performance on the real-world proprietary dataset provided by a large parts supply company. Different hyperparameters were tested using Bayesian Optimization (BO) for applicability in the CF framework. External data sources like click-data and metrics like Clickthrough Rate (CTR) were reviewed for potential extensions to the work presented. The work shown in this paper provides techniques the Company can use to provide product recommendations to enhance revenues, attract new customers, and gain advantages over competitors.

</details>

<details>

<summary>2020-12-11 16:53:20 - On the UMVUE and Closed-Form Bayes Estimator for $Pr(X<Y<Z)$ and its Generalizations</summary>

- *Tau Raphael Rasethuntsa*

- `2012.06487v1` - [abs](http://arxiv.org/abs/2012.06487v1) - [pdf](http://arxiv.org/pdf/2012.06487v1)

> This article considers the parametric estimation of $Pr(X<Y<Z)$ and its generalizations based on several well-known one-parameter and two-parameter continuous distributions. It is shown that for some one-parameter distributions and when there is a common known parameter in some two-parameter distributions, the uniformly minimum variance unbiased estimator can be expressed as a linear combination of the Appell hypergeometric function of the first type, $F_{1}$ and the hypergeometric functions $_{2}F_{1}$ and $_{3}F_{2}.$ The Bayes estimator based on conjugate gamma priors and Jefferys' non-informative priors under the squared error loss function is also given as a linear combination of $_{2}F_{1}$ and $F_{1}.$ Alternatively, a convergent infinite series form of the Bayes estimator involving the $F_{1}$ function is also proposed. In model generalizations and extensions, it is further shown that the UMVUE can be expressed as a linear combination of a Lauricella series, $F_{D}^{(n)},$ and the generalized hypergeometric function, $_{p}F_{q},$ which are generalizations of $F_{1}$ and $_{2}F_{1}$ respectively. The generalized closed-form Bayes estimator is also given as a convergent infinite series involving $F_{D}^{(n)}.$ To gauge the performances of the UMVUE and the closed-form Bayes estimator for $P$ against other well-known estimators, maximum likelihood estimates, Lindley approximation estimates and Markov Chain Monte Carlo estimates for $P$ are also computed. Additionally, asymptotic confidence intervals and Bayesian highest probability density credible intervals are also constructed.

</details>

<details>

<summary>2020-12-12 02:43:18 - Variable Selection Consistency of Gaussian Process Regression</summary>

- *Sheng Jiang, Surya T. Tokdar*

- `1912.05738v2` - [abs](http://arxiv.org/abs/1912.05738v2) - [pdf](http://arxiv.org/pdf/1912.05738v2)

> Bayesian nonparametric regression under a rescaled Gaussian process prior offers smoothness-adaptive function estimation with near minimax-optimal error rates. Hierarchical extensions of this approach, equipped with stochastic variable selection, are known to also adapt to the unknown intrinsic dimension of a sparse true regression function. But it remains unclear if such extensions offer variable selection consistency, i.e., if the true subset of important variables could be consistently learned from the data. It is shown here that variable consistency may indeed be achieved with such models at least when the true regression function has finite smoothness to induce a polynomially larger penalty on inclusion of false positive predictors. Our result covers the high dimensional asymptotic setting where the predictor dimension is allowed to grow with the sample size. The proof utilizes Schwartz theory to establish that the posterior probability of wrong selection vanishes asymptotically. A necessary and challenging technical development involves providing sharp upper and lower bounds to small ball probabilities at all rescaling levels of the Gaussian process prior, a result that could be of independent interest.

</details>

<details>

<summary>2020-12-12 13:49:37 - A simulation study to compare 210Pb dating data analyses</summary>

- *Marco A Aquino-López, Nicole K. Sanderson, Maarten Blaauw, Joan-Albert Sanchez-Cabeza, Ana Carolina Ruiz-Fernandez, J Andrés Christen Marco A Aquino-López, Nicole K. Sanderson, Maarten Blaauw, Joan-Albert Sanchez-Cabeza, Ana Carolina Ruiz-Fernandez, J Andrés Christen*

- `2012.06819v1` - [abs](http://arxiv.org/abs/2012.06819v1) - [pdf](http://arxiv.org/pdf/2012.06819v1)

> The increasing interest in understanding anthropogenic impacts on the environment have led to a considerable number of studies focusing on sedimentary records for the last $\sim$ 100 - 200 years. Dating this period is often complicated by the poor resolution and large errors associated with radiocarbon (14C) ages, which is the most popular dating technique. To improve age-depth model resolution for the recent period, sediment dating with lead-210 ($^{210}$Pb) is widely used as it provides absolute and continuous dates for the last $\sim$ 100 - 150 years. The $^{210}$Pb dating method has traditionally relied on the Constant Rate of Supply (CRS, also known as Constant Flux - CF) model which uses the radioactive decay equation as an age-depth relationship resulting in a restrictive model to approximate dates. In this work, we compare the classical approach to $^{210}$Pb dating (CRS) and its Bayesian alternative (\textit{Plum}). To do so, we created simulated $^{210}$Pb profiles following three different sedimentation processes, complying with the assumptions imposed by the CRS model, and analysed them using both approaches. Results indicate that the CRS model does not capture the true values even with a high dating resolution for the sediment, nor improves does its accuracy improve as more information is available. On the other hand, the Bayesian alternative (\textit{Plum}) provides consistently more accurate results even with few samples, and its accuracy and precision constantly improves as more information is available.

</details>

<details>

<summary>2020-12-12 22:16:29 - Continuous-time Markov-switching GARCH Process with Robust and Efficient State Path and Volatility Estimation</summary>

- *Yinan Li, Fang Liu*

- `1906.07313v2` - [abs](http://arxiv.org/abs/1906.07313v2) - [pdf](http://arxiv.org/pdf/1906.07313v2)

> We propose a continuous-time Markov-switching generalized autoregressive conditional heteroskedasticity (COMS-GARCH) process for handling irregularly spaced time series (TS) with multiple volatilities states. We employ a Gibbs sampler in the Bayesian framework to estimate the COMS-GARCH model parameters, the latent state path and volatilities. To improve the inferential robustness and computational efficiency for obtaining the maximum a posteriori estimates for the state path and volatilities, we suggest a multi-path sampling scheme and incorporate the Bernoulli noise injection in the computational algorithm. We provide theoretical justifications for the improved stability and robustness with the Bernoulli noise injection through the concept of ensemble learning and the low sensitivity of the objective function to external perturbation in the TS. We apply the proposed COMS-GARCH process and the computational procedure to simulated TS, a real currency exchange rate TS, and a real blood volume amplitude TS. The empirical results demonstrate that the COMS-GARCH process and the computational procedure are able to predict volatility regimes and volatilities in a TS with satisfactory accuracy.

</details>

<details>

<summary>2020-12-13 06:54:50 - Modeling the social media relationships of Irish politicians using a generalized latent space stochastic blockmodel</summary>

- *Tin Lok James Ng, Thomas Brendan Murphy, Ted Westling, Tyler H. McCormick, Bailey K. Fosdick*

- `1807.06063v2` - [abs](http://arxiv.org/abs/1807.06063v2) - [pdf](http://arxiv.org/pdf/1807.06063v2)

> D\'ail \'Eireann is the principal chamber of the Irish parliament. The 31st D\'ail \'Eireann is the principal chamber of the Irish parliament. The 31st D\'ail was in session from March 11th, 2011 to February 6th, 2016. Many of the members of the D\'ail were active on social media and many were Twitter users who followed other members of the D\'ail. The pattern of following amongst these politicians provides insights into political alignment within the D\'ail. We propose a new model, called the generalized latent space stochastic blockmodel, which extends and generalizes both the latent space model and the stochastic blockmodel to study social media connections between members of the D\'ail. The probability of an edge between two nodes in a network depends on their respective class labels as well as latent positions in an unobserved latent space. The proposed model is capable of representing transitivity, clustering, as well as disassortative mixing. A Bayesian method with Markov chain Monte Carlo sampling is proposed for estimation of model parameters. Model selection is performed using the WAIC criterion and models of different number of classes or dimensions of latent space are compared. We use the model to study Twitter following relationships of members of the D\'ail and interpret structure found in these relationships. We find that the following relationships amongst politicians is mainly driven by past and present political party membership. We also find that the modeling outputs are informative when studying voting within the D\'ail.

</details>

<details>

<summary>2020-12-13 11:18:52 - Uncertainty Estimation in Deep Neural Networks for Point Cloud Segmentation in Factory Planning</summary>

- *Christina Petschnigg, Juergen Pilz*

- `2012.07038v1` - [abs](http://arxiv.org/abs/2012.07038v1) - [pdf](http://arxiv.org/pdf/2012.07038v1)

> The digital factory provides undoubtedly a great potential for future production systems in terms of efficiency and effectivity. A key aspect on the way to realize the digital copy of a real factory is the understanding of complex indoor environments on the basis of 3D data. In order to generate an accurate factory model including the major components, i.e. building parts, product assets and process details, the 3D data collected during digitalization can be processed with advanced methods of deep learning. In this work, we propose a fully Bayesian and an approximate Bayesian neural network for point cloud segmentation. This allows us to analyze how different ways of estimating uncertainty in these networks improve segmentation results on raw 3D point clouds. We achieve superior model performance for both, the Bayesian and the approximate Bayesian model compared to the frequentist one. This performance difference becomes even more striking when incorporating the networks' uncertainty in their predictions. For evaluation we use the scientific data set S3DIS as well as a data set, which was collected by the authors at a German automotive production plant. The methods proposed in this work lead to more accurate segmentation results and the incorporation of uncertainty information makes this approach especially applicable to safety critical applications.

</details>

<details>

<summary>2020-12-13 23:27:48 - Explanation from Specification</summary>

- *Harish Naik, György Turán*

- `2012.07179v1` - [abs](http://arxiv.org/abs/2012.07179v1) - [pdf](http://arxiv.org/pdf/2012.07179v1)

> Explainable components in XAI algorithms often come from a familiar set of models, such as linear models or decision trees. We formulate an approach where the type of explanation produced is guided by a specification. Specifications are elicited from the user, possibly using interaction with the user and contributions from other areas. Areas where a specification could be obtained include forensic, medical, and scientific applications. Providing a menu of possible types of specifications in an area is an exploratory knowledge representation and reasoning task for the algorithm designer, aiming at understanding the possibilities and limitations of efficiently computable modes of explanations. Two examples are discussed: explanations for Bayesian networks using the theory of argumentation, and explanations for graph neural networks. The latter case illustrates the possibility of having a representation formalism available to the user for specifying the type of explanation requested, for example, a chemical query language for classifying molecules. The approach is motivated by a theory of explanation in the philosophy of science, and it is related to current questions in the philosophy of science on the role of machine learning.

</details>

<details>

<summary>2020-12-14 02:00:21 - Late 19th-Century Navigational Uncertainties and Their Influence on Sea Surface Temperature Estimates</summary>

- *Chenguang Dai, Duo Chan, Peter Huybers, Natesh Pillai*

- `1910.04843v2` - [abs](http://arxiv.org/abs/1910.04843v2) - [pdf](http://arxiv.org/pdf/1910.04843v2)

> Accurate estimates of historical changes in sea surface temperatures (SSTs) and their uncertainties are important for documenting and understanding historical changes in climate. A source of uncertainty that has not previously been quantified in historical SST estimates stems from position errors. A Bayesian inference framework is proposed for quantifying errors in reported positions and their implications on SST estimates. The analysis framework is applied to data from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS3.0) in 1885, a time when astronomical and chronometer estimation of position was common, but predating the use of radio signals. Focus is upon a subset of 943 ship tracks from ICOADS3.0 that report their position every two hours to a precision of 0.01{\deg} longitude and latitude. These data are interpreted as positions determined by dead reckoning that are periodically updated by celestial correction techniques. The posterior medians of uncertainties in celestial correction are 33.1 km (0.30{\deg} on the equator) in longitude and 24.4 km (0.22{\deg}) in latitude, respectively. The posterior medians of two-hourly dead reckoning uncertainties are 19.2% for ship speed and 13.2{\deg} for ship heading, leading to random position uncertainties with median 0.18{\deg} (20 km on the equator) in longitude and 0.15{\deg} (17 km) in latitude. Reported ship tracks also contain systematic position uncertainties relating to precursor dead-reckoning positions not being updated after obtaining celestial position estimates, indicating that more accurate positions can be provided for SST observations. Finally, we translate position errors into SST uncertainties by sampling an ensemble of SSTs from the Multi-scale Ultra-high resolution Sea Surface Temperature (MURSST) data set.

</details>

<details>

<summary>2020-12-14 03:31:42 - Efficient Learning for Clustering and Optimizing Context-Dependent Designs</summary>

- *Haidong Li, Henry Lam, Yijie Peng*

- `2012.05591v2` - [abs](http://arxiv.org/abs/2012.05591v2) - [pdf](http://arxiv.org/pdf/2012.05591v2)

> We consider a simulation optimization problem for a context-dependent decision-making. A Gaussian mixture model is proposed to capture the performance clustering phenomena of context-dependent designs. Under a Bayesian framework, we develop a dynamic sampling policy to efficiently learn both the global information of each cluster and local information of each design for selecting the best designs in all contexts. The proposed sampling policy is proved to be consistent and achieve the asymptotically optimal sampling ratio. Numerical experiments show that the proposed sampling policy significantly improves the efficiency in context-dependent simulation optimization.

</details>

<details>

<summary>2020-12-14 03:45:43 - Misspecified Beliefs about Time Lags</summary>

- *Yingkai Li, Harry Pei*

- `2012.07238v1` - [abs](http://arxiv.org/abs/2012.07238v1) - [pdf](http://arxiv.org/pdf/2012.07238v1)

> We examine the long-term behavior of a Bayesian agent who has a misspecified belief about the time lag between actions and feedback, and learns about the payoff consequences of his actions over time. Misspecified beliefs about time lags result in attribution errors, which have no long-term effect when the agent's action converges, but can lead to arbitrarily large long-term inefficiencies when his action cycles. Our proof uses concentration inequalities to bound the frequency of action switches, which are useful to study learning problems with history dependence. We apply our methods to study a policy choice game between a policy-maker who has a correctly specified belief about the time lag and the public who has a misspecified belief.

</details>

<details>

<summary>2020-12-14 05:35:29 - Variational State and Parameter Estimation</summary>

- *Jarrad Courts, Johannes Hendriks, Adrian Wills, Thomas Schön, Brett Ninness*

- `2012.07269v1` - [abs](http://arxiv.org/abs/2012.07269v1) - [pdf](http://arxiv.org/pdf/2012.07269v1)

> This paper considers the problem of computing Bayesian estimates of both states and model parameters for nonlinear state-space models. Generally, this problem does not have a tractable solution and approximations must be utilised. In this work, a variational approach is used to provide an assumed density which approximates the desired, intractable, distribution. The approach is deterministic and results in an optimisation problem of a standard form. Due to the parametrisation of the assumed density selected first- and second-order derivatives are readily available which allows for efficient solutions. The proposed method is compared against state-of-the-art Hamiltonian Monte Carlo in two numerical examples.

</details>

<details>

<summary>2020-12-14 06:22:53 - Vector operations for accelerating expensive Bayesian computations -- a tutorial guide</summary>

- *David J. Warne, Scott A. Sisson, Christopher Drovandi*

- `1902.09046v3` - [abs](http://arxiv.org/abs/1902.09046v3) - [pdf](http://arxiv.org/pdf/1902.09046v3)

> Many applications in Bayesian statistics are extremely computationally intensive. However, they are often inherently parallel, making them prime targets for modern massively parallel processors. Multi-core and distributed computing is widely applied in the Bayesian community, however, very little attention has been given to fine-grain parallelisation using single instruction multiple data (SIMD) operations that are available on most modern commodity CPUs and is the basis of GPGPU computing. In this work, we practically demonstrate, using standard programming libraries, the utility of the SIMD approach for several topical Bayesian applications. We show that SIMD can improve the floating point arithmetic performance resulting in up to $6\times$ improvement in serial algorithm performance. Importantly, these improvements are multiplicative to any gains achieved through multi-core processing. We illustrate the potential of SIMD for accelerating Bayesian computations and provide the reader with techniques for exploiting modern massively parallel processing environments using standard tools.

</details>

<details>

<summary>2020-12-14 14:16:45 - Variational Auto-encoder Based Bayesian Poisson Tensor Factorization for Sparse and Imbalanced Count Data</summary>

- *Yuan Jin, Ming Liu, Yunfeng Li, Ruohua Xu, Lan Du, Longxiang Gao, Yong Xiang*

- `1910.05570v2` - [abs](http://arxiv.org/abs/1910.05570v2) - [pdf](http://arxiv.org/pdf/1910.05570v2)

> Non-negative tensor factorization models enable predictive analysis on count data. Among them, Bayesian Poisson-Gamma models can derive full posterior distributions of latent factors and are less sensitive to sparse count data. However, current inference methods for these Bayesian models adopt restricted update rules for the posterior parameters. They also fail to share the update information to better cope with the data sparsity. Moreover, these models are not endowed with a component that handles the imbalance in count data values. In this paper, we propose a novel variational auto-encoder framework called VAE-BPTF which addresses the above issues. It uses multi-layer perceptron networks to encode and share complex update information. The encoded information is then reweighted per data instance to penalize common data values before aggregated to compute the posterior parameters for the latent factors. Under synthetic data evaluation, VAE-BPTF tended to recover the right number of latent factors and posterior parameter values. It also outperformed current models in both reconstruction errors and latent factor (semantic) coherence across five real-world datasets. Furthermore, the latent factors inferred by VAE-BPTF are perceived to be meaningful and coherent under a qualitative analysis.

</details>

<details>

<summary>2020-12-14 16:06:35 - A mathematical model of national-level food system sustainability</summary>

- *Conor Goold, Simone Pfuderer, William H. M. James, Nik Lomax, Fiona Smith, Lisa M. Collins*

- `2012.08355v1` - [abs](http://arxiv.org/abs/2012.08355v1) - [pdf](http://arxiv.org/pdf/2012.08355v1)

> The global food system faces various endogeneous and exogeneous, biotic and abiotic risk factors, including a rising human population, higher population densities, price volatility and climate change. Quantitative models play an important role in understanding food systems' expected responses to shocks and stresses. Here, we present a stylised mathematical model of a national-level food system that incorporates domestic supply of a food commodity, international trade, consumer demand, and food commodity price. We derive a critical compound parameter signalling when domestic supply will become unsustainable and the food system entirely dependent on imports, which results in higher commodity prices, lower consumer demand and lower inventory levels. Using Bayesian estimation, we apply the dynamic food systems model to infer the sustainability of the UK pork industry. We find that the UK pork industry is currently sustainable but because the industry is dependent on imports to meet demand, a decrease in self-sufficiency below 50% (current levels are 60-65%) would lead it close to the critical boundary signalling its collapse. Our model provides a theoretical foundation for future work to determine more complex causal drivers of food system vulnerability.

</details>

<details>

<summary>2020-12-15 09:38:22 - Disentangling the socio-ecological drivers behind illegal fishing in a small-scale fishery managed by a TURF system</summary>

- *Silvia de Juan, Maria Dulce Subida, Andres Ospina-Alvarez, Ainara Aguilar, Miriam Fernandez*

- `2012.08970v1` - [abs](http://arxiv.org/abs/2012.08970v1) - [pdf](http://arxiv.org/pdf/2012.08970v1)

> A substantial increase in illegal extraction of the benthic resources in central Chile is likely driven by an interplay of numerous socio-economic local factors that threatens the success of the fisheries management areas (MA) system. To assess this problem, the exploitation state of a commercially important benthic resource (i.e., keyhole limpet) in the MAs was related with socio-economic drivers of the small-scale fisheries. The potential drivers of illegal extraction included rebound effect of fishing effort displacement by MAs, level of enforcement, distance to surveillance authorities, wave exposure and land-based access to the MA, and alternative economic activities in the fishing village. The exploitation state of limpets was assessed by the proportion of the catch that is below the minimum legal size, with high proportions indicating a poor state, and by the relative median size of limpets fished within the MAs in comparison with neighbouring OA areas, with larger relative sizes in the MA indicating a good state. A Bayesian-Belief Network approach was adopted to assess the effects of potential drivers of illegal fishing on the status of the benthic resource in the MAs. Results evidenced the absence of a direct link between the level of enforcement and the status of the resource, with other socio-economic (e.g., alternative economic activities in the village) and context variables (e.g., fishing effort or distance to surveillance authorities) playing important roles. Scenario analysis explored variables that are susceptible to be managed, evidencing that BBN is a powerful approach to explore the role of multiple external drivers, and their impact on marine resources, in complex small-scale fisheries.

</details>

<details>

<summary>2020-12-15 14:47:48 - Asymptotic Behavior of Free Energy When Optimal Probability Distribution Is Not Unique</summary>

- *Shuya Nagayasu, Sumio Watanabe*

- `2012.08338v1` - [abs](http://arxiv.org/abs/2012.08338v1) - [pdf](http://arxiv.org/pdf/2012.08338v1)

> Bayesian inference is a widely used statistical method. The free energy and generalization loss, which are used to estimate the accuracy of Bayesian inference, are known to be small in singular models that do not have a unique optimal parameter. However, their characteristics are not yet known when there are multiple optimal probability distributions. In this paper, we theoretically derive the asymptotic behaviors of the generalization loss and free energy in the case that the optimal probability distributions are not unique and show that they contain asymptotically different terms from those of the conventional asymptotic analysis.

</details>

<details>

<summary>2020-12-16 02:52:16 - Monte Carlo Approximation of Bayes Factors via Mixing with Surrogate Distributions</summary>

- *Chenguang Dai, Jun S. Liu*

- `1909.05922v2` - [abs](http://arxiv.org/abs/1909.05922v2) - [pdf](http://arxiv.org/pdf/1909.05922v2)

> By mixing the target posterior distribution with a surrogate distribution, of which the normalizing constant is tractable, we propose a method for estimating the marginal likelihood using the Wang-Landau algorithm. We show that a faster convergence of the proposed method can be achieved via the momentum acceleration. Two implementation strategies are detailed: (i) facilitating global jumps between the posterior and surrogate distributions via the Multiple-try Metropolis; (ii) constructing the surrogate via the variational approximation. When a surrogate is difficult to come by, we describe a new jumping mechanism for general reversible jump Markov chain Monte Carlo algorithms, which combines the Multiple-try Metropolis and a directional sampling algorithm. We illustrate the proposed methods on several statistical models, including the Log-Gaussian Cox process, the Bayesian Lasso, the logistic regression, and the g-prior Bayesian variable selection.

</details>

<details>

<summary>2020-12-16 10:28:43 - An Asymptotically Optimal Multi-Armed Bandit Algorithm and Hyperparameter Optimization</summary>

- *Yimin Huang, Yujun Li, Hanrong Ye, Zhenguo Li, Zhihua Zhang*

- `2007.05670v2` - [abs](http://arxiv.org/abs/2007.05670v2) - [pdf](http://arxiv.org/pdf/2007.05670v2)

> The evaluation of hyperparameters, neural architectures, or data augmentation policies becomes a critical model selection problem in advanced deep learning with a large hyperparameter search space. In this paper, we propose an efficient and robust bandit-based algorithm called Sub-Sampling (SS) in the scenario of hyperparameter search evaluation. It evaluates the potential of hyperparameters by the sub-samples of observations and is theoretically proved to be optimal under the criterion of cumulative regret. We further combine SS with Bayesian Optimization and develop a novel hyperparameter optimization algorithm called BOSS. Empirical studies validate our theoretical arguments of SS and demonstrate the superior performance of BOSS on a number of applications, including Neural Architecture Search (NAS), Data Augmentation (DA), Object Detection (OD), and Reinforcement Learning (RL).

</details>

<details>

<summary>2020-12-16 15:14:05 - Deep Bayesian Quadrature Policy Optimization</summary>

- *Akella Ravi Tej, Kamyar Azizzadenesheli, Mohammad Ghavamzadeh, Anima Anandkumar, Yisong Yue*

- `2006.15637v3` - [abs](http://arxiv.org/abs/2006.15637v3) - [pdf](http://arxiv.org/pdf/2006.15637v3)

> We study the problem of obtaining accurate policy gradient estimates using a finite number of samples. Monte-Carlo methods have been the default choice for policy gradient estimation, despite suffering from high variance in the gradient estimates. On the other hand, more sample efficient alternatives like Bayesian quadrature methods have received little attention due to their high computational complexity. In this work, we propose deep Bayesian quadrature policy gradient (DBQPG), a computationally efficient high-dimensional generalization of Bayesian quadrature, for policy gradient estimation. We show that DBQPG can substitute Monte-Carlo estimation in policy gradient methods, and demonstrate its effectiveness on a set of continuous control benchmarks. In comparison to Monte-Carlo estimation, DBQPG provides (i) more accurate gradient estimates with a significantly lower variance, (ii) a consistent improvement in the sample complexity and average return for several deep policy gradient algorithms, and, (iii) the uncertainty in gradient estimation that can be incorporated to further improve the performance.

</details>

<details>

<summary>2020-12-16 16:44:55 - Bayes-TrEx: a Bayesian Sampling Approach to Model Transparency by Example</summary>

- *Serena Booth, Yilun Zhou, Ankit Shah, Julie Shah*

- `2002.10248v4` - [abs](http://arxiv.org/abs/2002.10248v4) - [pdf](http://arxiv.org/pdf/2002.10248v4)

> Post-hoc explanation methods are gaining popularity for interpreting, understanding, and debugging neural networks. Most analyses using such methods explain decisions in response to inputs drawn from the test set. However, the test set may have few examples that trigger some model behaviors, such as high-confidence failures or ambiguous classifications. To address these challenges, we introduce a flexible model inspection framework: Bayes-TrEx. Given a data distribution, Bayes-TrEx finds in-distribution examples with a specified prediction confidence. We demonstrate several use cases of Bayes-TrEx, including revealing highly confident (mis)classifications, visualizing class boundaries via ambiguous examples, understanding novel-class extrapolation behavior, and exposing neural network overconfidence. We use Bayes-TrEx to study classifiers trained on CLEVR, MNIST, and Fashion-MNIST, and we show that this framework enables more flexible holistic model analysis than just inspecting the test set. Code is available at https://github.com/serenabooth/Bayes-TrEx.

</details>

<details>

<summary>2020-12-17 04:08:42 - A General Class of Transfer Learning Regression without Implementation Cost</summary>

- *Shunya Minami, Song Liu, Stephen Wu, Kenji Fukumizu, Ryo Yoshida*

- `2006.13228v2` - [abs](http://arxiv.org/abs/2006.13228v2) - [pdf](http://arxiv.org/pdf/2006.13228v2)

> We propose a novel framework that unifies and extends existing methods of transfer learning (TL) for regression. To bridge a pretrained source model to the model on a target task, we introduce a density-ratio reweighting function, which is estimated through the Bayesian framework with a specific prior distribution. By changing two intrinsic hyperparameters and the choice of the density-ratio model, the proposed method can integrate three popular methods of TL: TL based on cross-domain similarity regularization, a probabilistic TL using the density-ratio estimation, and fine-tuning of pretrained neural networks. Moreover, the proposed method can benefit from its simple implementation without any additional cost; the regression model can be fully trained using off-the-shelf libraries for supervised learning in which the original output variable is simply transformed to a new output variable. We demonstrate its simplicity, generality, and applicability using various real data applications.

</details>

<details>

<summary>2020-12-17 04:28:06 - Success Stories from a Democratized Experimentation Platform</summary>

- *Eskil Forsell, Julie Beckley, Simon Ejdemyr, Veronica Hannan, Andy Rhines, Martin Tingley, Matthew Wardrop, Jeffrey Wong*

- `2012.10403v1` - [abs](http://arxiv.org/abs/2012.10403v1) - [pdf](http://arxiv.org/pdf/2012.10403v1)

> We demonstrate the effectiveness of democratization and efficient computation as key concepts of our experimentation platform (XP) by presenting four new models supported by the platform: 1) Weighted least squares, 2) Quantile bootstrapping, 3) Bayesian shrinkage, and 4) Dynamic treatment effects. Each model is motivated by a specific business problem but is generalizable and extensible. The modular structure of our platform allows independent innovation on statistical and computational methods. In practice, a technical symbiosis is created where increasingly advanced user contributions inspire innovations to the software that in turn enable further methodological improvements. This cycle adds further value to how the XP contributes to business solutions.

</details>

<details>

<summary>2020-12-17 07:44:22 - Maximum Entropy competes with Maximum Likelihood</summary>

- *A. E. Allahverdyan, N. H. Martirosyan*

- `2012.09430v1` - [abs](http://arxiv.org/abs/2012.09430v1) - [pdf](http://arxiv.org/pdf/2012.09430v1)

> Maximum entropy (MAXENT) method has a large number of applications in theoretical and applied machine learning, since it provides a convenient non-parametric tool for estimating unknown probabilities. The method is a major contribution of statistical physics to probabilistic inference. However, a systematic approach towards its validity limits is currently missing. Here we study MAXENT in a Bayesian decision theory set-up, i.e. assuming that there exists a well-defined prior Dirichlet density for unknown probabilities, and that the average Kullback-Leibler (KL) distance can be employed for deciding on the quality and applicability of various estimators. These allow to evaluate the relevance of various MAXENT constraints, check its general applicability, and compare MAXENT with estimators having various degrees of dependence on the prior, viz. the regularized maximum likelihood (ML) and the Bayesian estimators. We show that MAXENT applies in sparse data regimes, but needs specific types of prior information. In particular, MAXENT can outperform the optimally regularized ML provided that there are prior rank correlations between the estimated random quantity and its probabilities.

</details>

<details>

<summary>2020-12-17 09:43:00 - To BAN or not to BAN: Bayesian Attention Networks for Reliable Hate Speech Detection</summary>

- *Kristian Miok, Blaz Skrlj, Daniela Zaharie, Marko Robnik-Sikonja*

- `2007.05304v7` - [abs](http://arxiv.org/abs/2007.05304v7) - [pdf](http://arxiv.org/pdf/2007.05304v7)

> Hate speech is an important problem in the management of user-generated content. To remove offensive content or ban misbehaving users, content moderators need reliable hate speech detectors. Recently, deep neural networks based on the transformer architecture, such as the (multilingual) BERT model, achieve superior performance in many natural language classification tasks, including hate speech detection. So far, these methods have not been able to quantify their output in terms of reliability. We propose a Bayesian method using Monte Carlo dropout within the attention layers of the transformer models to provide well-calibrated reliability estimates. We evaluate and visualize the results of the proposed approach on hate speech detection problems in several languages. Additionally, we test if affective dimensions can enhance the information extracted by the BERT model in hate speech classification. Our experiments show that Monte Carlo dropout provides a viable mechanism for reliability estimation in transformer networks. Used within the BERT model, it ofers state-of-the-art classification performance and can detect less trusted predictions. Also, it was observed that affective dimensions extracted using sentic computing methods can provide insights toward interpretation of emotions involved in hate speech. Our approach not only improves the classification performance of the state-of-the-art multilingual BERT model but the computed reliability scores also significantly reduce the workload in an inspection of ofending cases and reannotation campaigns. The provided visualization helps to understand the borderline outcomes.

</details>

<details>

<summary>2020-12-17 11:42:45 - Two-stage Circular-circular Regression with Zero-inflation: Application to Medical Sciences</summary>

- *Jayant Jha, Prajamitra Bhuyan*

- `1901.05178v5` - [abs](http://arxiv.org/abs/1901.05178v5) - [pdf](http://arxiv.org/pdf/1901.05178v5)

> This paper considers the modeling of zero-inflated circular measurements concerning real case studies from medical sciences. Circular-circular regression models have been discussed in the statistical literature and illustrated with various real-life applications. However, there are no models to deal with zero-inflated response as well as a covariate simultaneously. The Mobius transformation based two-stage circular-circular regression model is proposed, and the Bayesian estimation of the model parameters is suggested using the MCMC algorithm. Simulation results show the superiority of the performance of the proposed method over the existing competitors. The method is applied to analyse real datasets on astigmatism due to cataract surgery and abnormal gait related to orthopaedic impairment. The methodology proposed can assist in efficient decision making during treatment or post-operative care.

</details>

<details>

<summary>2020-12-17 12:20:16 - Are we Forgetting about Compositional Optimisers in Bayesian Optimisation?</summary>

- *Antoine Grosnit, Alexander I. Cowen-Rivers, Rasul Tutunov, Ryan-Rhys Griffiths, Jun Wang, Haitham Bou-Ammar*

- `2012.08240v2` - [abs](http://arxiv.org/abs/2012.08240v2) - [pdf](http://arxiv.org/pdf/2012.08240v2)

> Bayesian optimisation presents a sample-efficient methodology for global optimisation. Within this framework, a crucial performance-determining subroutine is the maximisation of the acquisition function, a task complicated by the fact that acquisition functions tend to be non-convex and thus nontrivial to optimise. In this paper, we undertake a comprehensive empirical study of approaches to maximise the acquisition function. Additionally, by deriving novel, yet mathematically equivalent, compositional forms for popular acquisition functions, we recast the maximisation task as a compositional optimisation problem, allowing us to benefit from the extensive literature in this field. We highlight the empirical advantages of the compositional approach to acquisition function maximisation across 3958 individual experiments comprising synthetic optimisation tasks as well as tasks from Bayesmark. Given the generality of the acquisition function maximisation subroutine, we posit that the adoption of compositional optimisers has the potential to yield performance improvements across all domains in which Bayesian optimisation is currently being applied.

</details>

<details>

<summary>2020-12-17 12:26:53 - Second Order PAC-Bayesian Bounds for the Weighted Majority Vote</summary>

- *Andrés R. Masegosa, Stephan S. Lorenzen, Christian Igel, Yevgeny Seldin*

- `2007.13532v2` - [abs](http://arxiv.org/abs/2007.13532v2) - [pdf](http://arxiv.org/pdf/2007.13532v2)

> We present a novel analysis of the expected risk of weighted majority vote in multiclass classification. The analysis takes correlation of predictions by ensemble members into account and provides a bound that is amenable to efficient minimization, which yields improved weighting for the majority vote. We also provide a specialized version of our bound for binary classification, which allows to exploit additional unlabeled data for tighter risk estimation. In experiments, we apply the bound to improve weighting of trees in random forests and show that, in contrast to the commonly used first order bound, minimization of the new bound typically does not lead to degradation of the test error of the ensemble.

</details>

<details>

<summary>2020-12-17 14:03:58 - Adaptive Variable Selection for Sequential Prediction in Multivariate Dynamic Models</summary>

- *Isaac Lavine, Michael Lindon, Mike West*

- `1906.06580v2` - [abs](http://arxiv.org/abs/1906.06580v2) - [pdf](http://arxiv.org/pdf/1906.06580v2)

> We discuss Bayesian model uncertainty analysis and forecasting in sequential dynamic modeling of multivariate time series. The perspective is that of a decision-maker with a specific forecasting objective that guides thinking about relevant models. Based on formal Bayesian decision-theoretic reasoning, we develop a time-adaptive approach to exploring, weighting, combining and selecting models that differ in terms of predictive variables included. The adaptivity allows for changes in the sets of favored models over time, and is guided by the specific forecasting goals. A synthetic example illustrates how decision-guided variable selection differs from traditional Bayesian model uncertainty analysis and standard model averaging. An applied study in one motivating application of long-term macroeconomic forecasting highlights the utility of the new approach in terms of improving predictions as well as its ability to identify and interpret different sets of relevant models over time with respect to specific, defined forecasting goals.

</details>

<details>

<summary>2020-12-17 18:53:03 - Bayesian semiparametric modelling of covariance matrices for multivariate longitudinal data</summary>

- *Georgios Papageorgiou*

- `2012.09833v1` - [abs](http://arxiv.org/abs/2012.09833v1) - [pdf](http://arxiv.org/pdf/2012.09833v1)

> The article develops marginal models for multivariate longitudinal responses. Overall, the model consists of five regression submodels, one for the mean and four for the covariance matrix, with the latter resulting by considering various matrix decompositions. The decompositions that we employ are intuitive, easy to understand, and they do not rely on any assumptions such as the presence of an ordering among the multivariate responses. The regression submodels are semiparametric, with unknown functions represented by basis function expansions. We use spike-slap priors for the regression coefficients to achieve variable selection and function regularization, and to obtain parameter estimates that account for model uncertainty. An efficient Markov chain Monte Carlo algorithm for posterior sampling is developed. The simulation studies presented investigate the effects of priors on posteriors, the gains that one may have when considering multivariate longitudinal analyses instead of univariate ones, and whether these gains can counteract the negative effects of missing data. We apply the methods on a highly unbalanced longitudinal dataset with four responses observed over of period of 20 years

</details>

<details>

<summary>2020-12-17 21:48:13 - Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences</summary>

- *Daniel S. Brown, Russell Coleman, Ravi Srinivasan, Scott Niekum*

- `2002.09089v4` - [abs](http://arxiv.org/abs/2002.09089v4) - [pdf](http://arxiv.org/pdf/2002.09089v4)

> Bayesian reward learning from demonstrations enables rigorous safety and uncertainty analysis when performing imitation learning. However, Bayesian reward learning methods are typically computationally intractable for complex control problems. We propose Bayesian Reward Extrapolation (Bayesian REX), a highly efficient Bayesian reward learning algorithm that scales to high-dimensional imitation learning problems by pre-training a low-dimensional feature encoding via self-supervised tasks and then leveraging preferences over demonstrations to perform fast Bayesian inference. Bayesian REX can learn to play Atari games from demonstrations, without access to the game score and can generate 100,000 samples from the posterior over reward functions in only 5 minutes on a personal laptop. Bayesian REX also results in imitation learning performance that is competitive with or better than state-of-the-art methods that only learn point estimates of the reward function. Finally, Bayesian REX enables efficient high-confidence policy evaluation without having access to samples of the reward function. These high-confidence performance bounds can be used to rank the performance and risk of a variety of evaluation policies and provide a way to detect reward hacking behaviors.

</details>

<details>

<summary>2020-12-17 23:21:53 - High Dimensional Level Set Estimation with Bayesian Neural Network</summary>

- *Huong Ha, Sunil Gupta, Santu Rana, Svetha Venkatesh*

- `2012.09973v1` - [abs](http://arxiv.org/abs/2012.09973v1) - [pdf](http://arxiv.org/pdf/2012.09973v1)

> Level Set Estimation (LSE) is an important problem with applications in various fields such as material design, biotechnology, machine operational testing, etc. Existing techniques suffer from the scalability issue, that is, these methods do not work well with high dimensional inputs. This paper proposes novel methods to solve the high dimensional LSE problems using Bayesian Neural Networks. In particular, we consider two types of LSE problems: (1) \textit{explicit} LSE problem where the threshold level is a fixed user-specified value, and, (2) \textit{implicit} LSE problem where the threshold level is defined as a percentage of the (unknown) maximum of the objective function. For each problem, we derive the corresponding theoretic information based acquisition function to sample the data points so as to maximally increase the level set accuracy. Furthermore, we also analyse the theoretical time complexity of our proposed acquisition functions, and suggest a practical methodology to efficiently tune the network hyper-parameters to achieve high model accuracy. Numerical experiments on both synthetic and real-world datasets show that our proposed method can achieve better results compared to existing state-of-the-art approaches.

</details>

<details>

<summary>2020-12-18 05:00:02 - On default priors for robust Bayesian estimation with divergences</summary>

- *Tomoyuki Nakagawa, Shintaro Hashimoto*

- `2004.13991v3` - [abs](http://arxiv.org/abs/2004.13991v3) - [pdf](http://arxiv.org/pdf/2004.13991v3)

> This paper presents objective priors for robust Bayesian estimation against outliers based on divergences. The minimum $\gamma$-divergence estimator is well-known to work well estimation against heavy contamination. The robust Bayesian methods by using quasi-posterior distributions based on divergences have been also proposed in recent years. In objective Bayesian framework, the selection of default prior distributions under such quasi-posterior distributions is an important problem. In this study, we provide some properties of reference and moment matching priors under the quasi-posterior distribution based on the $\gamma$-divergence. In particular, we show that the proposed priors are approximately robust under the condition on the contamination distribution without assuming any conditions on the contamination ratio. Some simulation studies are also presented.

</details>

<details>

<summary>2020-12-18 05:39:19 - Context-dependent Ranking and Selection under a Bayesian Framework</summary>

- *Haidong Li, Henry Lam, Zhe Liang, Yijie Peng*

- `2012.05577v2` - [abs](http://arxiv.org/abs/2012.05577v2) - [pdf](http://arxiv.org/pdf/2012.05577v2)

> We consider a context-dependent ranking and selection problem. The best design is not universal but depends on the contexts. Under a Bayesian framework, we develop a dynamic sampling scheme for context-dependent optimization (DSCO) to efficiently learn and select the best designs in all contexts. The proposed sampling scheme is proved to be consistent. Numerical experiments show that the proposed sampling scheme significantly improves the efficiency in context-dependent ranking and selection.

</details>

<details>

<summary>2020-12-18 09:29:30 - Bayesian nonparametric panel Markov-switching GARCH models</summary>

- *Roberto Casarin, Mauro Costantini, Anthony Osuntuyi*

- `2012.10124v1` - [abs](http://arxiv.org/abs/2012.10124v1) - [pdf](http://arxiv.org/pdf/2012.10124v1)

> This paper introduces a new model for panel data with Markov-switching GARCH effects. The model incorporates a series-specific hidden Markov chain process that drives the GARCH parameters. To cope with the high-dimensionality of the parameter space, the paper exploits the cross-sectional clustering of the series by first assuming a soft parameter pooling through a hierarchical prior distribution with two-step procedure, and then introducing clustering effects in the parameter space through a nonparametric prior distribution. The model and the proposed inference are evaluated through a simulation experiment. The results suggest that the inference is able to recover the true value of the parameters and the number of groups in each regime. An empirical application to 78 assets of the SP\&100 index from $6^{th}$ January 2000 to $3^{rd}$ October 2020 is also carried out by using a two-regime Markov switching GARCH model. The findings shows the presence of 2 and 3 clusters among the constituents in the first and second regime, respectively.

</details>

<details>

<summary>2020-12-18 10:06:55 - MASSIVE: Tractable and Robust Bayesian Learning of Many-Dimensional Instrumental Variable Models</summary>

- *Ioan Gabriel Bucur, Tom Claassen, Tom Heskes*

- `2012.10141v1` - [abs](http://arxiv.org/abs/2012.10141v1) - [pdf](http://arxiv.org/pdf/2012.10141v1)

> The recent availability of huge, many-dimensional data sets, like those arising from genome-wide association studies (GWAS), provides many opportunities for strengthening causal inference. One popular approach is to utilize these many-dimensional measurements as instrumental variables (instruments) for improving the causal effect estimate between other pairs of variables. Unfortunately, searching for proper instruments in a many-dimensional set of candidates is a daunting task due to the intractable model space and the fact that we cannot directly test which of these candidates are valid, so most existing search methods either rely on overly stringent modeling assumptions or fail to capture the inherent model uncertainty in the selection process. We show that, as long as at least some of the candidates are (close to) valid, without knowing a priori which ones, they collectively still pose enough restrictions on the target interaction to obtain a reliable causal effect estimate. We propose a general and efficient causal inference algorithm that accounts for model uncertainty by performing Bayesian model averaging over the most promising many-dimensional instrumental variable models, while at the same time employing weaker assumptions regarding the data generating process. We showcase the efficiency, robustness and predictive performance of our algorithm through experimental results on both simulated and real-world data.

</details>

<details>

<summary>2020-12-18 11:01:52 - Inferring the Direction of a Causal Link and Estimating Its Effect via a Bayesian Mendelian Randomization Approach</summary>

- *Ioan Gabriel Bucur, Tom Claassen, Tom Heskes*

- `2012.10167v1` - [abs](http://arxiv.org/abs/2012.10167v1) - [pdf](http://arxiv.org/pdf/2012.10167v1)

> The use of genetic variants as instrumental variables - an approach known as Mendelian randomization - is a popular epidemiological method for estimating the causal effect of an exposure (phenotype, biomarker, risk factor) on a disease or health-related outcome from observational data. Instrumental variables must satisfy strong, often untestable assumptions, which means that finding good genetic instruments among a large list of potential candidates is challenging. This difficulty is compounded by the fact that many genetic variants influence more than one phenotype through different causal pathways, a phenomenon called horizontal pleiotropy. This leads to errors not only in estimating the magnitude of the causal effect but also in inferring the direction of the putative causal link. In this paper, we propose a Bayesian approach called BayesMR that is a generalization of the Mendelian randomization technique in which we allow for pleiotropic effects and, crucially, for the possibility of reverse causation. The output of the method is a posterior distribution over the target causal effect, which provides an immediate and easily interpretable measure of the uncertainty in the estimation. More importantly, we use Bayesian model averaging to determine how much more likely the inferred direction is relative to the reverse direction.

</details>

<details>

<summary>2020-12-18 12:50:36 - Learning Hamiltonian Monte Carlo in R</summary>

- *Samuel Thomas, Wanzhu Tu*

- `2006.16194v2` - [abs](http://arxiv.org/abs/2006.16194v2) - [pdf](http://arxiv.org/pdf/2006.16194v2)

> Hamiltonian Monte Carlo (HMC) is a powerful tool for Bayesian computation. In comparison with the traditional Metropolis-Hastings algorithm, HMC offers greater computational efficiency, especially in higher dimensional or more complex modeling situations. To most statisticians, however, the idea of HMC comes from a less familiar origin, one that is based on the theory of classical mechanics. Its implementation, either through Stan or one of its derivative programs, can appear opaque to beginners. A lack of understanding of the inner working of HMC, in our opinion, has hindered its application to a broader range of statistical problems. In this article, we review the basic concepts of HMC in a language that is more familiar to statisticians, and we describe an HMC implementation in R, one of the most frequently used statistical software environments. We also present hmclearn, an R package for learning HMC. This package contains a general-purpose HMC function for data analysis. We illustrate the use of this package in common statistical models. In doing so, we hope to promote this powerful computational tool for wider use. Example code for common statistical models is presented as supplementary material for online publication.

</details>

<details>

<summary>2020-12-18 15:00:07 - VARCLUST: clustering variables using dimensionality reduction</summary>

- *Piotr Sobczyk, Stanislaw Wilczynski, Malgorzata Bogdan, Piotr Graczyk, Julie Josse, Fabien Panloup, Valérie Seegers, Mateusz Staniak*

- `2011.06501v2` - [abs](http://arxiv.org/abs/2011.06501v2) - [pdf](http://arxiv.org/pdf/2011.06501v2)

> VARCLUST algorithm is proposed for clustering variables under the assumption that variables in a given cluster are linear combinations of a small number of hidden latent variables, corrupted by the random noise. The entire clustering task is viewed as the problem of selection of the statistical model, which is defined by the number of clusters, the partition of variables into these clusters and the 'cluster dimensions', i.e. the vector of dimensions of linear subspaces spanning each of the clusters. The optimal model is selected using the approximate Bayesian criterion based on the Laplace approximations and using a non-informative uniform prior on the number of clusters. To solve the problem of the search over a huge space of possible models we propose an extension of the ClustOfVar algorithm which was dedicated to subspaces of dimension only 1, and which is similar in structure to the $K$-centroid algorithm. We provide a complete methodology with theoretical guarantees, extensive numerical experimentations, complete data analyses and implementation. Our algorithm assigns variables to appropriate clusterse based on the consistent Bayesian Information Criterion (BIC), and estimates the dimensionality of each cluster by the PEnalized SEmi-integrated Likelihood Criterion (PESEL), whose consistency we prove. Additionally, we prove that each iteration of our algorithm leads to an increase of the Laplace approximation to the model posterior probability and provide the criterion for the estimation of the number of clusters. Numerical comparisons with other algorithms show that VARCLUST may outperform some popular machine learning tools for sparse subspace clustering. We also report the results of real data analysis including TCGA breast cancer data and meteorological data. The proposed method is implemented in the publicly available R package varclust.

</details>

<details>

<summary>2020-12-18 22:23:26 - Identifying latent groups in spatial panel data using a Markov random field constrained product partition model</summary>

- *Tianyu Pan, Guanyu Hu, Weining Shen*

- `2012.10541v1` - [abs](http://arxiv.org/abs/2012.10541v1) - [pdf](http://arxiv.org/pdf/2012.10541v1)

> Understanding the heterogeneity over spatial locations is an important problem that has been widely studied in many applications such as economics and environmental science. In this paper, we focus on regression models for spatial panel data analysis, where repeated measurements are collected over time at various spatial locations. We propose a novel class of nonparametric priors that combines Markov random field (MRF) with the product partition model (PPM), and show that the resulting prior, called by MRF-PPM, is capable of identifying the latent group structure among the spatial locations while efficiently utilizing the spatial dependence information. We derive a closed-form conditional distribution for the proposed prior and introduce a new way to compute the marginal likelihood that renders efficient Bayesian inference. We further study the theoretical properties of the proposed MRF-PPM prior and show a clustering consistency result for the posterior distribution. We demonstrate the excellent empirical performance of our method via extensive simulation studies and applications to a US precipitation data and a California median household income data study.

</details>

<details>

<summary>2020-12-19 13:49:41 - Top-$k$ Ranking Bayesian Optimization</summary>

- *Quoc Phong Nguyen, Sebastian Tay, Bryan Kian Hsiang Low, Patrick Jaillet*

- `2012.10688v1` - [abs](http://arxiv.org/abs/2012.10688v1) - [pdf](http://arxiv.org/pdf/2012.10688v1)

> This paper presents a novel approach to top-$k$ ranking Bayesian optimization (top-$k$ ranking BO) which is a practical and significant generalization of preferential BO to handle top-$k$ ranking and tie/indifference observations. We first design a surrogate model that is not only capable of catering to the above observations, but is also supported by a classic random utility model. Another equally important contribution is the introduction of the first information-theoretic acquisition function in BO with preferential observation called multinomial predictive entropy search (MPES) which is flexible in handling these observations and optimized for all inputs of a query jointly. MPES possesses superior performance compared with existing acquisition functions that select the inputs of a query one at a time greedily. We empirically evaluate the performance of MPES using several synthetic benchmark functions, CIFAR-$10$ dataset, and SUSHI preference dataset.

</details>

<details>

<summary>2020-12-19 14:22:48 - An Information-Theoretic Framework for Unifying Active Learning Problems</summary>

- *Quoc Phong Nguyen, Bryan Kian Hsiang Low, Patrick Jaillet*

- `2012.10695v1` - [abs](http://arxiv.org/abs/2012.10695v1) - [pdf](http://arxiv.org/pdf/2012.10695v1)

> This paper presents an information-theoretic framework for unifying active learning problems: level set estimation (LSE), Bayesian optimization (BO), and their generalized variant. We first introduce a novel active learning criterion that subsumes an existing LSE algorithm and achieves state-of-the-art performance in LSE problems with a continuous input domain. Then, by exploiting the relationship between LSE and BO, we design a competitive information-theoretic acquisition function for BO that has interesting connections to upper confidence bound and max-value entropy search (MES). The latter connection reveals a drawback of MES which has important implications on not only MES but also on other MES-based acquisition functions. Finally, our unifying information-theoretic framework can be applied to solve a generalized problem of LSE and BO involving multiple level sets in a data-efficient manner. We empirically evaluate the performance of our proposed algorithms using synthetic benchmark functions, a real-world dataset, and in hyperparameter tuning of machine learning models.

</details>

<details>

<summary>2020-12-19 16:52:58 - (Decision and regression) tree ensemble based kernels for regression and classification</summary>

- *Dai Feng, Richard Baumgartner*

- `2012.10737v1` - [abs](http://arxiv.org/abs/2012.10737v1) - [pdf](http://arxiv.org/pdf/2012.10737v1)

> Tree based ensembles such as Breiman's random forest (RF) and Gradient Boosted Trees (GBT) can be interpreted as implicit kernel generators, where the ensuing proximity matrix represents the data-driven tree ensemble kernel. Kernel perspective on the RF has been used to develop a principled framework for theoretical investigation of its statistical properties. Recently, it has been shown that the kernel interpretation is germane to other tree-based ensembles e.g. GBTs. However, practical utility of the links between kernels and the tree ensembles has not been widely explored and systematically evaluated.   Focus of our work is investigation of the interplay between kernel methods and the tree based ensembles including the RF and GBT. We elucidate the performance and properties of the RF and GBT based kernels in a comprehensive simulation study comprising of continuous and binary targets. We show that for continuous targets, the RF/GBT kernels are competitive to their respective ensembles in higher dimensional scenarios, particularly in cases with larger number of noisy features. For the binary target, the RF/GBT kernels and their respective ensembles exhibit comparable performance. We provide the results from real life data sets for regression and classification to show how these insights may be leveraged in practice. Overall, our results support the tree ensemble based kernels as a valuable addition to the practitioner's toolbox.   Finally, we discuss extensions of the tree ensemble based kernels for survival targets, interpretable prototype and landmarking classification and regression. We outline future line of research for kernels furnished by Bayesian counterparts of the frequentist tree ensembles.

</details>

<details>

<summary>2020-12-20 01:26:00 - Non-reversible jump algorithms for Bayesian nested model selection</summary>

- *Philippe Gagnon, Arnaud Doucet*

- `1911.01340v3` - [abs](http://arxiv.org/abs/1911.01340v3) - [pdf](http://arxiv.org/pdf/1911.01340v3)

> Non-reversible Markov chain Monte Carlo methods often outperform their reversible counterparts in terms of asymptotic variance of ergodic averages and mixing properties. Lifting the state-space (Chen et al., 1999; Diaconis et al., 2000) is a generic technique for constructing such samplers. The idea is to think of the random variables we want to generate as position variables and to associate to them direction variables so as to design Markov chains which do not have the diffusive behaviour often exhibited by reversible schemes. In this paper, we explore the benefits of using such ideas in the context of Bayesian model choice for nested models, a class of models for which the model indicator variable is an ordinal random variable. By lifting this model indicator variable, we obtain non-reversible jump algorithms, a non-reversible version of the popular reversible jump algorithms introduced by Green (1995). This simple algorithmic modification provides samplers which can empirically outperform their reversible counterparts at no extra computational cost. The code to reproduce all experiments is available online.

</details>

<details>

<summary>2020-12-20 03:46:06 - Cluster Prediction for Opinion Dynamics from Partial Observations</summary>

- *Zehong Zhang, Fei Lu*

- `2007.02006v3` - [abs](http://arxiv.org/abs/2007.02006v3) - [pdf](http://arxiv.org/pdf/2007.02006v3)

> We present a Bayesian approach to predict the clustering of opinions for a system of interacting agents from partial observations. The Bayesian formulation overcomes the unobservability of the system and quantifies the uncertainty in the prediction. We characterize the clustering by the posterior of the clusters' sizes and centers, and we represent the posterior by samples. To overcome the challenge in sampling the high-dimensional posterior, we introduce an auxiliary implicit sampling (AIS) algorithm using two-step observations. Numerical results show that the AIS algorithm leads to accurate predictions of the sizes and centers for the leading clusters, in both cases of noiseless and noisy observations. In particular, the centers are predicted with high success rates, but the sizes exhibit a considerable uncertainty that is sensitive to observation noise and the observation ratio.

</details>

<details>

<summary>2020-12-20 23:26:11 - Semiparametric Regression for Dual Population Mortality</summary>

- *Gary Venter, Şule Şahin*

- `1912.04406v3` - [abs](http://arxiv.org/abs/1912.04406v3) - [pdf](http://arxiv.org/pdf/1912.04406v3)

> Parameter shrinkage applied optimally can always reduce error and projection variances from those of maximum likelihood estimation. Many variables that actuaries use are on numerical scales, like age or year, which require parameters at each point. Rather than shrinking these towards zero, nearby parameters are better shrunk towards each other. Semiparametric regression is a statistical discipline for building curves across parameter classes using shrinkage methodology. It is similar to but more parsimonious than cubic splines. We introduce it in the context of Bayesian shrinkage and apply it to joint mortality modeling for related populations. Bayesian shrinkage of slope changes of linear splines is an approach to semiparametric modeling that evolved in the actuarial literature. It has some theoretical and practical advantages, like closed-form curves, direct and transparent determination of degree of shrinkage and of placing knots for the splines, and quantifying goodness of fit. It is also relatively easy to apply to the many nonlinear models that arise in actuarial work. We find that it compares well to a more complex state-of-the-art statistical spline shrinkage approach on a popular example from that literature.

</details>

<details>

<summary>2020-12-21 04:44:57 - A Bayesian State-Space Approach to Mapping Directional Brain Networks</summary>

- *Huazhang Li, Yaotian Wang, Guofen Yan, Yinge Sun, Seiji Tanabe, Chang-Chia Liu, Mark Quigg, Tingting Zhang*

- `2012.11114v1` - [abs](http://arxiv.org/abs/2012.11114v1) - [pdf](http://arxiv.org/pdf/2012.11114v1)

> The human brain is a directional network system of brain regions involving directional connectivity. Seizures are a directional network phenomenon as abnormal neuronal activities start from a seizure onset zone (SOZ) and propagate to otherwise healthy regions. To localize the SOZ of an epileptic patient, clinicians use iEEG to record the patient's intracranial brain activity in many small regions. iEEG data are high-dimensional multivariate time series. We build a state-space multivariate autoregression (SSMAR) for iEEG data to model the underlying directional brain network. To produce scientifically interpretable network results, we incorporate into the SSMAR the scientific knowledge that the underlying brain network tends to have a cluster structure. Specifically, we assign to the SSMAR parameters a stochastic-blockmodel-motivated prior, which reflects the cluster structure. We develop a Bayesian framework to estimate the SSMAR, infer directional connections, and identify clusters for the unobserved network edges. The new method is robust to violations of model assumptions and outperforms existing network methods. By applying the new method to an epileptic patient's iEEG data, we reveal seizure initiation and propagation in the patient's brain network. Our method can also accurately localize the SOZ. Overall, this paper provides a tool to study the human brain network.

</details>

<details>

<summary>2020-12-21 07:43:22 - A Pólya-Gamma Sampler for a Generalized Logistic Regression</summary>

- *Luciana Dalla Valle, Fabrizio Leisen, Luca Rossini, Weixuan Zhu*

- `1909.02989v3` - [abs](http://arxiv.org/abs/1909.02989v3) - [pdf](http://arxiv.org/pdf/1909.02989v3)

> In this paper we introduce a novel Bayesian data augmentation approach for estimating the parameters of the generalised logistic regression model. We propose a P\'olya-Gamma sampler algorithm that allows us to sample from the exact posterior distribution, rather than relying on approximations. A simulation study illustrates the flexibility and accuracy of the proposed approach to capture heavy and light tails in binary response data of different dimensions. The methodology is applied to two different real datasets, where we demonstrate that the P\'olya-Gamma sampler provides more precise estimates than the empirical likelihood method, outperforming approximate approaches.

</details>

<details>

<summary>2020-12-21 14:02:19 - A comparison of learning rate selection methods in generalized Bayesian inference</summary>

- *Pei-Shien Wu, Ryan Martin*

- `2012.11349v1` - [abs](http://arxiv.org/abs/2012.11349v1) - [pdf](http://arxiv.org/pdf/2012.11349v1)

> Generalized Bayes posterior distributions are formed by putting a fractional power on the likelihood before combining with the prior via Bayes's formula. This fractional power, which is often viewed as a remedy for potential model misspecification bias, is called the learning rate, and a number of data-driven learning rate selection methods have been proposed in the recent literature. Each of these proposals has a different focus, a different target they aim to achieve, which makes them difficult to compare. In this paper, we provide a direct head-to-head comparison of these learning rate selection methods in various misspecified model scenarios, in terms of several relevant metrics, in particular, coverage probability of the generalized Bayes credible regions. In some examples all the methods perform well, while in others the misspecification is too severe to be overcome, but we find that the so-called generalized posterior calibration algorithm tends to outperform the others in terms of credible region coverage probability.

</details>

<details>

<summary>2020-12-21 15:15:10 - An Approach to Gender Pay Equity Analysis Using Bayesian Hierarchical Regression</summary>

- *Diana Cesar*

- `2012.11411v1` - [abs](http://arxiv.org/abs/2012.11411v1) - [pdf](http://arxiv.org/pdf/2012.11411v1)

> Diversity and inclusion, or D and I, is a topic that sparks the interest of companies, research groups, and individuals alike. Recently in the United States, renewed focus has been placed on fair and equitable pay practices, which are a key component of promoting diversity in the workplace. Despite the increased demand for reliable pay equity analysis, the challenges of conducting this type of analysis on industry data have not been adequately addressed. This paper explains a few limitations of current approaches to pay equity analysis by gender and improves on them with a Bayesian hierarchical regression model. Using global workforce data from a large U.S. semiconductor company, Micron Technology, Inc., the paper demonstrates how the model provides a holistic view of gender pay equity across the organization, while overcoming issues more common in industry data, such as small sample size and poor gender representation. When compared to a prior analysis of Micron's U.S. workforce, this approach decreased the amount of manual review required, enabling decision makers to finalize pay adjustments across a workforce of 31,738 people within four weeks of receiving preliminary model results.

</details>

<details>

<summary>2020-12-21 17:20:30 - A PAC-Bayesian Perspective on Structured Prediction with Implicit Loss Embeddings</summary>

- *Théophile Cantelobre, Benjamin Guedj, María Pérez-Ortiz, John Shawe-Taylor*

- `2012.03780v2` - [abs](http://arxiv.org/abs/2012.03780v2) - [pdf](http://arxiv.org/pdf/2012.03780v2)

> Many practical machine learning tasks can be framed as Structured prediction problems, where several output variables are predicted and considered interdependent. Recent theoretical advances in structured prediction have focused on obtaining fast rates convergence guarantees, especially in the Implicit Loss Embedding (ILE) framework. PAC-Bayes has gained interest recently for its capacity of producing tight risk bounds for predictor distributions. This work proposes a novel PAC-Bayes perspective on the ILE Structured prediction framework. We present two generalization bounds, on the risk and excess risk, which yield insights into the behavior of ILE predictors. Two learning algorithms are derived from these bounds. The algorithms are implemented and their behavior analyzed, with source code available at \url{https://github.com/theophilec/PAC-Bayes-ILE-Structured-Prediction}.

</details>

<details>

<summary>2020-12-22 10:46:37 - Learning Structures in Earth Observation Data with Gaussian Processes</summary>

- *Fernando Mateo, Jordi Munoz-Mari, Valero Laparra, Jochem Verrelst, Gustau Camps-Valls*

- `2012.11922v1` - [abs](http://arxiv.org/abs/2012.11922v1) - [pdf](http://arxiv.org/pdf/2012.11922v1)

> Gaussian Processes (GPs) has experienced tremendous success in geoscience in general and for bio-geophysical parameter retrieval in the last years. GPs constitute a solid Bayesian framework to formulate many function approximation problems consistently. This paper reviews the main theoretical GP developments in the field. We review new algorithms that respect the signal and noise characteristics, that provide feature rankings automatically, and that allow applicability of associated uncertainty intervals to transport GP models in space and time. All these developments are illustrated in the field of geoscience and remote sensing at a local and global scales through a set of illustrative examples.

</details>

<details>

<summary>2020-12-22 15:26:28 - Bayesian structural equation modeling for data from multiple cohorts</summary>

- *Khue-Dung Dang, Louise M. Ryan, Tugba Akkaya-Hocagil, Richard J. Cook, Gale A. Richardson, Nancy L. Day, Claire D. Coles, Heather Carmichael Olson, Sandra W. Jacobson, Joseph L. Jacobson*

- `2012.12085v1` - [abs](http://arxiv.org/abs/2012.12085v1) - [pdf](http://arxiv.org/pdf/2012.12085v1)

> While it is well known that high levels of prenatal alcohol exposure (PAE) result in significant cognitive deficits in children, the exact nature of the dose response is less well understood. In particular, there is a pressing need to identify the levels of PAE associated with an increased risk of clinically significant adverse effects. To address this issue, data have been combined from six longitudinal birth cohort studies in the United States that assessed the effects of PAE on cognitive outcomes measured from early school age through adolescence. Structural equation models (SEMs) are commonly used to capture the association among multiple observed outcomes in order to characterise the underlying variable of interest (in this case, cognition) and then relate it to PAE. However, it was not possible to apply classic SEM software in our context because different outcomes were measured in the six studies. In this paper we show how a Bayesian approach can be used to fit a multi-group multi-level structural model that maps cognition to a broad range of observed variables measured at multiple ages. These variables map to several different cognitive subdomains and are examined in relation to PAE after adjusting for confounding using propensity scores. The model also tests the possibility of a change point in the dose-response function.

</details>

<details>

<summary>2020-12-22 18:31:57 - Accelerating Uncertainty Quantification of Groundwater Flow Modelling Using a Deep Neural Network Proxy</summary>

- *Mikkel B. Lykkegaard, Tim J. Dodwell, David Moxey*

- `2007.00400v2` - [abs](http://arxiv.org/abs/2007.00400v2) - [pdf](http://arxiv.org/pdf/2007.00400v2)

> Quantifying the uncertainty in model parameters and output is a critical component in model-driven decision support systems for groundwater management. This paper presents a novel algorithmic approach which fuses Markov Chain Monte Carlo (MCMC) and Machine Learning methods to accelerate uncertainty quantification for groundwater flow models. We formulate the governing mathematical model as a Bayesian inverse problem, considering model parameters as a random process with an underlying probability distribution. MCMC allows us to sample from this distribution, but it comes with some limitations: it can be prohibitively expensive when dealing with costly likelihood functions, subsequent samples are often highly correlated, and the standard Metropolis-Hastings algorithm suffers from the curse of dimensionality. This paper designs a Metropolis-Hastings proposal which exploits a deep neural network (DNN) approximation of a groundwater flow model, to significantly accelerate MCMC sampling. We modify a delayed acceptance (DA) model hierarchy, whereby proposals are generated by running short subchains using an inexpensive DNN approximation, resulting in a decorrelation of subsequent fine model proposals. Using a simple adaptive error model, we estimate and correct the bias of the DNN approximation with respect to the posterior distribution on-the-fly. The approach is tested on two synthetic examples; a isotropic two-dimensional problem, and an anisotropic three-dimensional problem. The results show that the cost of uncertainty quantification can be reduced by up to 50% compared to single-level MCMC, depending on the precomputation cost and accuracy of the employed DNN.

</details>

<details>

<summary>2020-12-22 22:27:30 - Modelling a novel Coronavirus (COVID-19): A stochastic SEIR-HCD approach, with real-time parameter estimation & forecasting for Scotland</summary>

- *Jonathan Wells, Chris Robertson, Vincent Marmara, Alan Yeung, Adam Kleczkowski*

- `2012.12390v1` - [abs](http://arxiv.org/abs/2012.12390v1) - [pdf](http://arxiv.org/pdf/2012.12390v1)

> Faced with the 2020 SARS-CoV2 epidemic, public health officials have been seeking models that could be used to predict not only the number of new cases but also the levels of hospitalisation, critical care and deaths. In this paper we present a stochastic compartmental model capable of real-time monitoring and forecasting of the pandemic incorporating multiple streams of real-world data, reported cases, testing intensity, deaths, hospitalisations and critical care occupancy. Model parameters are estimated via a Bayesian particle filtering technique. The model successfully tracks the key variables (reported cases, critical care and deaths) throughout the two waves (March-June and September-November 2020) of the COVID-19 outbreak in Scotland. The model hospitalisation predictions in Summer 2020 are consistently lower than the recorded data, but consistent with the change to the reporting criteria by the Health Protection Scotland on 15th September. Most parameter estimates were constant over the two waves, but the infection rate and consequently the reproductive number decrease in the later stages of the first wave and increase again from July 2020. The death rates are initially high but decrease over Summer 2020 before rising again in November. The model can also be used to provide short-term predictions. We show that the 2-week predictability is very good for the period from March to June 2020, even at early stages of the pandemic. The model has been slower to pick up the increase in the case numbers in September 2020 but forecasting improves again in the later stages of the epidemic.

</details>

<details>

<summary>2020-12-22 22:30:58 - Fast Algorithms and Theory for High-Dimensional Bayesian Varying Coefficient Models</summary>

- *Ray Bai, Mary R. Boland, Yong Chen*

- `1907.06477v8` - [abs](http://arxiv.org/abs/1907.06477v8) - [pdf](http://arxiv.org/pdf/1907.06477v8)

> Nonparametric varying coefficient (NVC) models are useful for modeling time-varying effects on responses that are measured repeatedly. In this paper, we introduce the nonparametric varying coefficient spike-and-slab lasso (NVC-SSL) for Bayesian estimation and variable selection in NVC models. The NVC-SSL simultaneously selects and estimates the significant varying coefficients, while also accounting for temporal correlations. Our model can be implemented using a computationally efficient expectation-maximization (EM) algorithm. We also employ a simple method to make our model robust to misspecification of the temporal correlation structure. In contrast to frequentist approaches, little is known about the large-sample properties for Bayesian NVC models when the dimension of the covariates $p$ grows much faster than sample size $n$. In this paper, we derive posterior contraction rates for the NVC-SSL model when $p \gg n$ under both correct specification and misspecification of the temporal correlation structure. Thus, our results are derived under weaker assumptions than those seen in other high-dimensional NVC models which assume independent and identically distributed (iid) random errors. Finally, we illustrate our methodology through simulation studies and data analysis. Our method is implemented in the publicly available R package NVCSSL.

</details>

<details>

<summary>2020-12-23 02:16:54 - Towards Automated Satellite Conjunction Management with Bayesian Deep Learning</summary>

- *Francesco Pinto, Giacomo Acciarini, Sascha Metz, Sarah Boufelja, Sylvester Kaczmarek, Klaus Merz, José A. Martinez-Heras, Francesca Letizia, Christopher Bridges, Atılım Güneş Baydin*

- `2012.12450v1` - [abs](http://arxiv.org/abs/2012.12450v1) - [pdf](http://arxiv.org/pdf/2012.12450v1)

> After decades of space travel, low Earth orbit is a junkyard of discarded rocket bodies, dead satellites, and millions of pieces of debris from collisions and explosions. Objects in high enough altitudes do not re-enter and burn up in the atmosphere, but stay in orbit around Earth for a long time. With a speed of 28,000 km/h, collisions in these orbits can generate fragments and potentially trigger a cascade of more collisions known as the Kessler syndrome. This could pose a planetary challenge, because the phenomenon could escalate to the point of hindering future space operations and damaging satellite infrastructure critical for space and Earth science applications. As commercial entities place mega-constellations of satellites in orbit, the burden on operators conducting collision avoidance manoeuvres will increase. For this reason, development of automated tools that predict potential collision events (conjunctions) is critical. We introduce a Bayesian deep learning approach to this problem, and develop recurrent neural network architectures (LSTMs) that work with time series of conjunction data messages (CDMs), a standard data format used by the space community. We show that our method can be used to model all CDM features simultaneously, including the time of arrival of future CDMs, providing predictions of conjunction event evolution with associated uncertainties.

</details>

<details>

<summary>2020-12-23 16:24:44 - Physics-inspired forms of the Bayesian Cramér-Rao bound</summary>

- *Mankei Tsang*

- `2007.04849v4` - [abs](http://arxiv.org/abs/2007.04849v4) - [pdf](http://arxiv.org/pdf/2007.04849v4)

> Using differential geometry, I derive a form of the Bayesian Cram\'er-Rao bound that remains invariant under reparametrization. With the invariant formulation at hand, I find the optimal and naturally invariant bound among the Gill-Levit family of bounds. By assuming that the prior probability density is the square of a wavefunction, I also express the bounds in terms of functionals that are quadratic with respect to the wavefunction and its gradient. The problem of finding an unfavorable prior to tighten the bound for minimax estimation is shown, in a special case, to be equivalent to finding the ground state of a Schr\"odinger equation, with the Fisher information playing the role of the potential. To illustrate the theory, two quantum estimation problems, namely, optomechanical waveform estimation and subdiffraction incoherent optical imaging, are discussed.

</details>

<details>

<summary>2020-12-24 03:56:44 - High-Dimensional Bayesian Optimization via Tree-Structured Additive Models</summary>

- *Eric Han, Ishank Arora, Jonathan Scarlett*

- `2012.13088v1` - [abs](http://arxiv.org/abs/2012.13088v1) - [pdf](http://arxiv.org/pdf/2012.13088v1)

> Bayesian Optimization (BO) has shown significant success in tackling expensive low-dimensional black-box optimization problems. Many optimization problems of interest are high-dimensional, and scaling BO to such settings remains an important challenge. In this paper, we consider generalized additive models in which low-dimensional functions with overlapping subsets of variables are composed to model a high-dimensional target function. Our goal is to lower the computational resources required and facilitate faster model learning by reducing the model complexity while retaining the sample-efficiency of existing methods. Specifically, we constrain the underlying dependency graphs to tree structures in order to facilitate both the structure learning and optimization of the acquisition function. For the former, we propose a hybrid graph learning algorithm based on Gibbs sampling and mutation. In addition, we propose a novel zooming-based algorithm that permits generalized additive models to be employed more efficiently in the case of continuous domains. We demonstrate and discuss the efficacy of our approach via a range of experiments on synthetic functions and real-world datasets.

</details>

<details>

<summary>2020-12-24 05:19:03 - Bayesian prognostic covariate adjustment</summary>

- *David Walsh, Alejandro Schuler, Diana Hall, Jon Walsh, Charles Fisher*

- `2012.13112v1` - [abs](http://arxiv.org/abs/2012.13112v1) - [pdf](http://arxiv.org/pdf/2012.13112v1)

> Historical data about disease outcomes can be integrated into the analysis of clinical trials in many ways. We build on existing literature that uses prognostic scores from a predictive model to increase the efficiency of treatment effect estimates via covariate adjustment. Here we go further, utilizing a Bayesian framework that combines prognostic covariate adjustment with an empirical prior distribution learned from the predictive performances of the prognostic model on past trials. The Bayesian approach interpolates between prognostic covariate adjustment with strict type I error control when the prior is diffuse, and a single-arm trial when the prior is sharply peaked. This method is shown theoretically to offer a substantial increase in statistical power, while limiting the type I error rate under reasonable conditions. We demonstrate the utility of our method in simulations and with an analysis of a past Alzheimer's disease clinical trial.

</details>

<details>

<summary>2020-12-24 12:40:11 - On Batch Normalisation for Approximate Bayesian Inference</summary>

- *Jishnu Mukhoti, Puneet K. Dokania, Philip H. S. Torr, Yarin Gal*

- `2012.13220v1` - [abs](http://arxiv.org/abs/2012.13220v1) - [pdf](http://arxiv.org/pdf/2012.13220v1)

> We study batch normalisation in the context of variational inference methods in Bayesian neural networks, such as mean-field or MC Dropout. We show that batch-normalisation does not affect the optimum of the evidence lower bound (ELBO). Furthermore, we study the Monte Carlo Batch Normalisation (MCBN) algorithm, proposed as an approximate inference technique parallel to MC Dropout, and show that for larger batch sizes, MCBN fails to capture epistemic uncertainty. Finally, we provide insights into what is required to fix this failure, namely having to view the mini-batch size as a variational parameter in MCBN. We comment on the asymptotics of the ELBO with respect to this variational parameter, showing that as dataset size increases towards infinity, the batch-size must increase towards infinity as well for MCBN to be a valid approximate inference technique.

</details>

<details>

<summary>2020-12-26 00:53:46 - An adequacy approach for deciding the number of clusters for OTRIMLE robust Gaussian mixture based clustering</summary>

- *Christian Hennig, Pietro Coretto*

- `2009.00921v2` - [abs](http://arxiv.org/abs/2009.00921v2) - [pdf](http://arxiv.org/pdf/2009.00921v2)

> We introduce a new approach to deciding the number of clusters. The approach is applied to Optimally Tuned Robust Improper Maximum Likelihood Estimation (OTRIMLE; Coretto and Hennig 2016) of a Gaussian mixture model allowing for observations to be classified as "noise", but it can be applied to other clustering methods as well. The quality of a clustering is assessed by a statistic $Q$ that measures how close the within-cluster distributions are to elliptical unimodal distributions that have the only mode in the mean. This nonparametric measure allows for non-Gaussian clusters as long as they have a good quality according to $Q$. The simplicity of a model is assessed by a measure $S$ that prefers a smaller number of clusters unless additional clusters can reduce the estimated noise proportion substantially. The simplest model is then chosen that is adequate for the data in the sense that its observed value of $Q$ is not significantly larger than what is expected for data truly generated from the fitted model, as can be assessed by parametric bootstrap. The approach is compared with model-based clustering using the Bayesian Information Criterion (BIC) and the Integrated Complete Likelihood (ICL) in a simulation study and on two datasets of scientific interest. Keywords: parametric bootstrap; noise component; unimodality; model-based clustering

</details>

<details>

<summary>2020-12-26 07:59:48 - Graph signal denoising using $t$-shrinkage priors</summary>

- *Sayantan Banerjee, Weining Shen*

- `2012.13696v1` - [abs](http://arxiv.org/abs/2012.13696v1) - [pdf](http://arxiv.org/pdf/2012.13696v1)

> We study the graph signal denoising problem by estimating a piecewise constant signal over an undirected graph. We propose a new Bayesian approach that first converts a general graph to a chain graph via the depth-first search algorithm, and then imposes a heavy-tailed $t$-shrinkage prior on the differences between consecutive signals over the induced chain graph. We show that the posterior computation can be conveniently conducted by fully exploring the conjugacy structure in the model. We also derive the posterior contraction rate for the proposed estimator, and show that this rate is optimal up to a logarithmic factor, besides automatically adapting to the unknown edge sparsity level of the graph. We demonstrate the excellent empirical performance of the proposed method via extensive simulation studies and applications to stock market data.

</details>

<details>

<summary>2020-12-26 12:49:42 - Accelerated MM Algorithms for Ranking Scores Inference from Comparison Data</summary>

- *Milan Vojnovic, Seyoung Yun, Kaifang Zhou*

- `1901.00150v3` - [abs](http://arxiv.org/abs/1901.00150v3) - [pdf](http://arxiv.org/pdf/1901.00150v3)

> In this paper, we study a popular method for inference of the Bradley-Terry model parameters, namely the MM algorithm, for maximum likelihood estimation and maximum a posteriori probability estimation. This class of models includes the Bradley-Terry model of paired comparisons, the Rao-Kupper model of paired comparisons allowing for tie outcomes, the Luce choice model, and the Plackett-Luce ranking model. We establish tight characterizations of the convergence rate for the MM algorithm, and show that it is essentially equivalent to that of a gradient descent algorithm. For the maximum likelihood estimation, the convergence is shown to be linear with the rate crucially determined by the algebraic connectivity of the matrix of item pair co-occurrences in observed comparison data. For the Bayesian inference, the convergence rate is also shown to be linear, with the rate determined by a parameter of the prior distribution in a way that can make the convergence arbitrarily slow for small values of this parameter. We propose a simple modification of the classical MM algorithm that avoids the observed slow convergence issue and accelerates the convergence. The key component of the accelerated MM algorithm is a parameter rescaling performed at each iteration step that is carefully chosen based on theoretical analysis and characterisation of the convergence rate.   Our experimental results, performed on both synthetic and real-world data, demonstrate the identified slow convergence issue of the classic MM algorithm, and show that significant efficiency gains can be obtained by our new proposed method.

</details>

<details>

<summary>2020-12-26 13:13:15 - Renormalized Normalized Maximum Likelihood and Three-Part Code Criteria For Learning Gaussian Networks</summary>

- *Borzou Alipourfard, Jean X. Gao*

- `1810.08749v2` - [abs](http://arxiv.org/abs/1810.08749v2) - [pdf](http://arxiv.org/pdf/1810.08749v2)

> Score based learning (SBL) is a promising approach for learning Bayesian networks in the discrete domain. However, when employing SBL in the continuous domain, one is either forced to move the problem to the discrete domain or use metrics such as BIC/AIC, and these approaches are often lacking. Discretization can have an undesired impact on the accuracy of the results, and BIC/AIC can fall short of achieving the desired accuracy. In this paper, we introduce two new scoring metrics for scoring Bayesian networks in the continuous domain: the three-part minimum description length and the renormalized normalized maximum likelihood metric. We rely on the minimum description length principle in formulating these metrics. The metrics proposed are free of hyperparameters, decomposable, and are asymptotically consistent. We evaluate our solution by studying the convergence rate of the learned graph to the generating network and, also, the structural hamming distance of the learned graph to the generating network. Our evaluations show that the proposed metrics outperform their competitors, the BIC/AIC metrics. Furthermore, using the proposed RNML metric, SBL will have the fastest rate of convergence with the smallest structural hamming distance to the generating network.

</details>

<details>

<summary>2020-12-26 16:10:54 - Population Quasi-Monte Carlo</summary>

- *Chaofan Huang, V. Roshan Joseph, Simon Mak*

- `2012.13769v1` - [abs](http://arxiv.org/abs/2012.13769v1) - [pdf](http://arxiv.org/pdf/2012.13769v1)

> Monte Carlo methods are widely used for approximating complicated, multidimensional integrals for Bayesian inference. Population Monte Carlo (PMC) is an important class of Monte Carlo methods, which utilizes a population of proposals to generate weighted samples that approximate the target distribution. The generic PMC framework iterates over three steps: samples are simulated from a set of proposals, weights are assigned to such samples to correct for mismatch between the proposal and target distributions, and the proposals are then adapted via resampling from the weighted samples. When the target distribution is expensive to evaluate, the PMC has its computational limitation since the convergence rate is $\mathcal{O}(N^{-1/2})$. To address this, we propose in this paper a new Population Quasi-Monte Carlo (PQMC) framework, which integrates Quasi-Monte Carlo ideas within the sampling and adaptation steps of PMC. A key novelty in PQMC is the idea of importance support points resampling, a deterministic method for finding an "optimal" subsample from the weighted proposal samples. Moreover, within the PQMC framework, we develop an efficient covariance adaptation strategy for multivariate normal proposals. Lastly, a new set of correction weights is introduced for the weighted PMC estimator to improve the efficiency from the standard PMC estimator. We demonstrate the improved empirical convergence of PQMC over PMC in extensive numerical simulations and a friction drilling application.

</details>

<details>

<summary>2020-12-26 19:30:35 - A new class of generative classifiers based on staged tree models</summary>

- *Federico Carli, Manuele Leonelli, Gherardo Varando*

- `2012.13798v1` - [abs](http://arxiv.org/abs/2012.13798v1) - [pdf](http://arxiv.org/pdf/2012.13798v1)

> Generative models for classification use the joint probability distribution of the class variable and the features to construct a decision rule. Among generative models, Bayesian networks and naive Bayes classifiers are the most commonly used and provide a clear graphical representation of the relationship among all variables. However, these have the disadvantage of highly restricting the type of relationships that could exist, by not allowing for context-specific independences. Here we introduce a new class of generative classifiers, called staged tree classifiers, which formally account for context-specific independence. They are constructed by a partitioning of the vertices of an event tree from which conditional independence can be formally read. The naive staged tree classifier is also defined, which extends the classic naive Bayes classifier whilst retaining the same complexity. An extensive simulation study shows that the classification accuracy of staged tree classifiers is competitive with those of state-of-the-art classifiers. An applied analysis to predict the fate of the passengers of the Titanic highlights the insights that the new class of generative classifiers can give.

</details>

<details>

<summary>2020-12-28 03:47:08 - Objective Bayesian Analysis for the Differential Entropy of the Gamma Distribution</summary>

- *Eduardo Ramos, Osafu A. Egbon, Pedro L. Ramos, Francisco A. Rodrigues, Francisco Louzada*

- `2012.14081v1` - [abs](http://arxiv.org/abs/2012.14081v1) - [pdf](http://arxiv.org/pdf/2012.14081v1)

> The use of entropy related concepts goes from physics, such as in statistical mechanics, to evolutionary biology. The Shannon entropy is a measure used to quantify the amount of information in a system, and its estimation is usually made under the frequentist approach. In the present paper, we introduce an fully objective Bayesian analysis to obtain this measure's posterior distribution. Notably, we consider the Gamma distribution, which describes many natural phenomena in physics, engineering, and biology. We reparametrize the model in terms of entropy, and different objective priors are derived, such as Jeffreys prior, reference prior, and matching priors. Since the obtained priors are improper, we prove that the obtained posterior distributions are proper and their respective posterior means are finite. An intensive simulation study is conducted to select the prior that returns better results in terms of bias, mean square error, and coverage probabilities. The proposed approach is illustrated in two datasets, where the first one is related to the Achaemenid dynasty reign period, and the second data describes the time to failure of an electronic component in the sugarcane harvest machine.

</details>

<details>

<summary>2020-12-29 05:31:41 - A Survey of Parameter and State Estimation in Queues</summary>

- *Azam Asanjarani, Yoni Nazarathy, Peter Taylor*

- `2012.14614v1` - [abs](http://arxiv.org/abs/2012.14614v1) - [pdf](http://arxiv.org/pdf/2012.14614v1)

> We present a broad literature survey of parameter and state estimation for queueing systems. Our approach is based on various inference activities, queueing models, observations schemes, and statistical methods. We categorize these into branches of research that we call estimation paradigms. These include: the classical sampling approach, inverse problems, inference for non-interacting systems, inference with discrete sampling, inference with queueing fundamentals, queue inference engine problems, Bayesian approaches, online prediction, implicit models, and control, design, and uncertainty quantification. For each of these estimation paradigms, we outline the principles and ideas, while surveying key references. We also present various simple numerical experiments. In addition to some key references mentioned here, a periodically-updated comprehensive list of references dealing with parameter and state estimation of queues will be kept in an accompanying annotated bibliography.

</details>

<details>

<summary>2020-12-29 09:00:05 - Semi-Mechanistic Bayesian Modeling of COVID-19 with Renewal Processes</summary>

- *Samir Bhatt, Neil Ferguson, Seth Flaxman, Axel Gandy, Swapnil Mishra, James A. Scott*

- `2012.00394v2` - [abs](http://arxiv.org/abs/2012.00394v2) - [pdf](http://arxiv.org/pdf/2012.00394v2)

> We propose a general Bayesian approach to modeling epidemics such as COVID-19. The approach grew out of specific analyses conducted during the pandemic, in particular an analysis concerning the effects of non-pharmaceutical interventions (NPIs) in reducing COVID-19 transmission in 11 European countries. The model parameterizes the time varying reproduction number $R_t$ through a regression framework in which covariates can e.g be governmental interventions or changes in mobility patterns. This allows a joint fit across regions and partial pooling to share strength. This innovation was critical to our timely estimates of the impact of lockdown and other NPIs in the European epidemics, whose validity was borne out by the subsequent course of the epidemic. Our framework provides a fully generative model for latent infections and observations deriving from them, including deaths, cases, hospitalizations, ICU admissions and seroprevalence surveys. One issue surrounding our model's use during the COVID-19 pandemic is the confounded nature of NPIs and mobility. We use our framework to explore this issue. We have open sourced an R package epidemia implementing our approach in Stan. Versions of the model are used by New York State, Tennessee and Scotland to estimate the current situation and make policy decisions.

</details>

<details>

<summary>2020-12-29 10:09:46 - The impact of Climate on Economic and Financial Cycles: A Markov-switching Panel Approach</summary>

- *Monica Billio, Roberto Casarin, Enrica De Cian, Malcolm Mistry, Anthony Osuntuyi*

- `2012.14693v1` - [abs](http://arxiv.org/abs/2012.14693v1) - [pdf](http://arxiv.org/pdf/2012.14693v1)

> This paper examines the impact of climate shocks on 13 European economies analysing jointly business and financial cycles, in different phases and disentangling the effects for different sector channels. A Bayesian Panel Markov-switching framework is proposed to jointly estimate the impact of extreme weather events on the economies as well as the interaction between business and financial cycles. Results from the empirical analysis suggest that extreme weather events impact asymmetrically across the different phases of the economy and heterogeneously across the EU countries. Moreover, we highlight how the manufacturing output, a component of the industrial production index, constitutes the main channel through which climate shocks impact the EU economies.

</details>

<details>

<summary>2020-12-30 14:44:20 - Combining probability distributions: Extending the logarithmic pooling approach</summary>

- *Luiz Max de Carvalho, Daniel A. M. Villela, Flavio Codeco Coelho, Leonardo Soares Bastos*

- `1502.04206v2` - [abs](http://arxiv.org/abs/1502.04206v2) - [pdf](http://arxiv.org/pdf/1502.04206v2)

> Combining distributions is an important issue in decision theory and Bayesian inference. Logarithmic pooling is a popular method to aggregate expert opinions by using a set of weights that reflect the reliability of each information source. However, the resulting pooled distribution depends heavily on set of weights given to each opinion/prior and thus careful consideration must be given to the choice of weights. In this paper we review and extend the statistical theory of logarithmic pooling, focusing on the assignment of the weights using a hierarchical prior distribution. We explore several statistical applications, such as the estimation of survival probabilities, meta-analysis and Bayesian melding of deterministic models of population growth and epidemics. We show that it is possible learn the weights from data, although identifiability issues may arise for some configurations of priors and data. Furthermore, we show how the hierarchical approach leads to posterior distributions that are able to accommodate prior-data conflict in complex models.

</details>

<details>

<summary>2020-12-30 20:00:44 - A Bayesian marked spatial point processes model for basketball shot chart</summary>

- *Jieying Jiao, Guanyu Hu, Jun Yan*

- `1908.05745v3` - [abs](http://arxiv.org/abs/1908.05745v3) - [pdf](http://arxiv.org/pdf/1908.05745v3)

> The success rate of a basketball shot may be higher at locations where a player makes more shots. For a marked spatial point process, this means that the mark and the intensity are associated. We propose a Bayesian joint model for the mark and the intensity of marked point processes, where the intensity is incorporated in the mark model as a covariate. Inferences are done with a Markov chain Monte Carlo algorithm. Two Bayesian model comparison criteria, the Deviance Information Criterion and the Logarithm of the Pseudo-Marginal Likelihood, were used to assess the model. The performances of the proposed methods were examined in extensive simulation studies. The proposed methods were applied to the shot charts of four players (Curry, Harden, Durant, and James) in the 2017--2018 regular season of the National Basketball Association to analyze their shot intensity in the field and the field goal percentage in detail. Application to the top 50 most frequent shooters in the season suggests that the field goal percentage and the shot intensity are positively associated for a majority of the players. The fitted parameters were used as inputs in a secondary analysis to cluster the players into different groups.

</details>

<details>

<summary>2020-12-30 20:35:43 - Bayesian Active Learning With Abstention Feedbacks</summary>

- *Cuong V. Nguyen, Lam Si Tung Ho, Huan Xu, Vu Dinh, Binh Nguyen*

- `1906.02179v2` - [abs](http://arxiv.org/abs/1906.02179v2) - [pdf](http://arxiv.org/pdf/1906.02179v2)

> We study pool-based active learning with abstention feedbacks where a labeler can abstain from labeling a queried example with some unknown abstention rate. This is an important problem with many useful applications. We take a Bayesian approach to the problem and develop two new greedy algorithms that learn both the classification problem and the unknown abstention rate at the same time. These are achieved by simply incorporating the estimated average abstention rate into the greedy criteria. We prove that both algorithms have near-optimality guarantees: they respectively achieve a ${(1-\frac{1}{e})}$ constant factor approximation of the optimal expected or worst-case value of a useful utility function. Our experiments show the algorithms perform well in various practical scenarios.

</details>

<details>

<summary>2020-12-31 17:57:04 - Multivariate mixed membership modeling: Inferring domain-specific risk profiles</summary>

- *Massimiliano Russo, Burton H. Singer, David B. Dunson*

- `1901.05191v3` - [abs](http://arxiv.org/abs/1901.05191v3) - [pdf](http://arxiv.org/pdf/1901.05191v3)

> Characterizing the shared memberships of individuals in a classification scheme poses severe interpretability issues, even when using a moderate number of classes (say 4). Mixed membership models quantify this phenomenon, but they typically focus on goodness-of-fit more than on interpretable inference. To achieve a good numerical fit, these models may in fact require many extreme profiles, making the results difficult to interpret. We introduce a new class of multivariate mixed membership models that, when variables can be partitioned into subject-matter based domains, can provide a good fit to the data using fewer profiles than standard formulations. The proposed model explicitly accounts for the blocks of variables corresponding to the distinct domains along with a cross-domain correlation structure, which provides new information about shared membership of individuals in a complex classification scheme. We specify a multivariate logistic normal distribution for the membership vectors, which allows easy introduction of auxiliary information leveraging a latent multivariate logistic regression. A Bayesian approach to inference, relying on P\'olya gamma data augmentation, facilitates efficient posterior computation via Markov Chain Monte Carlo. We apply this methodology to a spatially explicit study of malaria risk over time on the Brazilian Amazon frontier.

</details>

<details>

<summary>2020-12-31 18:38:58 - Fairness in Machine Learning</summary>

- *Luca Oneto, Silvia Chiappa*

- `2012.15816v1` - [abs](http://arxiv.org/abs/2012.15816v1) - [pdf](http://arxiv.org/pdf/2012.15816v1)

> Machine learning based systems are reaching society at large and in many aspects of everyday life. This phenomenon has been accompanied by concerns about the ethical issues that may arise from the adoption of these technologies. ML fairness is a recently established area of machine learning that studies how to ensure that biases in the data and model inaccuracies do not lead to models that treat individuals unfavorably on the basis of characteristics such as e.g. race, gender, disabilities, and sexual or political orientation. In this manuscript, we discuss some of the limitations present in the current reasoning about fairness and in methods that deal with it, and describe some work done by the authors to address them. More specifically, we show how causal Bayesian networks can play an important role to reason about and deal with fairness, especially in complex unfairness scenarios. We describe how optimal transport theory can be used to develop methods that impose constraints on the full shapes of distributions corresponding to different sensitive attributes, overcoming the limitation of most approaches that approximate fairness desiderata by imposing constraints on the lower order moments or other functions of those distributions. We present a unified framework that encompasses methods that can deal with different settings and fairness criteria, and that enjoys strong theoretical guarantees. We introduce an approach to learn fair representations that can generalize to unseen tasks. Finally, we describe a technique that accounts for legal restrictions about the use of sensitive attributes.

</details>

