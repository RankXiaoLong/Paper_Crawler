# 2016

## TOC

- [2016-01](#2016-01)
- [2016-02](#2016-02)
- [2016-03](#2016-03)
- [2016-04](#2016-04)
- [2016-05](#2016-05)
- [2016-06](#2016-06)
- [2016-07](#2016-07)
- [2016-08](#2016-08)
- [2016-09](#2016-09)
- [2016-10](#2016-10)
- [2016-11](#2016-11)
- [2016-12](#2016-12)

## 2016-01

<details>

<summary>2016-01-02 01:55:07 - The Reduced-Order Hybrid Monte Carlo Sampling Smoother</summary>

- *Ahmed Attia, Razvan Stefanescu, Adrian Sandu*

- `1601.00129v1` - [abs](http://arxiv.org/abs/1601.00129v1) - [pdf](http://arxiv.org/pdf/1601.00129v1)

> Hybrid Monte-Carlo (HMC) sampling smoother is a fully non-Gaussian four-dimensional data assimilation algorithm that works by directly sampling the posterior distribution formulated in the Bayesian framework. The smoother in its original formulation is computationally expensive due to the intrinsic requirement of running the forward and adjoint models repeatedly. Here we present computationally efficient versions of the HMC sampling smoother based on reduced-order approximations of the underlying model dynamics. The schemes developed herein are tested numerically using the shallow-water equations model on Cartesian coordinates. The results reveal that the reduced-order versions of the smoother are capable of accurately capturing the posterior probability density, while being significantly faster than the original full order formulation.

</details>

<details>

<summary>2016-01-05 10:55:02 - Probabilistic Programming with Gaussian Process Memoization</summary>

- *Ulrich Schaechtle, Ben Zinberg, Alexey Radul, Kostas Stathis, Vikash K. Mansinghka*

- `1512.05665v2` - [abs](http://arxiv.org/abs/1512.05665v2) - [pdf](http://arxiv.org/pdf/1512.05665v2)

> Gaussian Processes (GPs) are widely used tools in statistics, machine learning, robotics, computer vision, and scientific computation. However, despite their popularity, they can be difficult to apply; all but the simplest classification or regression applications require specification and inference over complex covariance functions that do not admit simple analytical posteriors. This paper shows how to embed Gaussian processes in any higher-order probabilistic programming language, using an idiom based on memoization, and demonstrates its utility by implementing and extending classic and state-of-the-art GP applications. The interface to Gaussian processes, called gpmem, takes an arbitrary real-valued computational process as input and returns a statistical emulator that automatically improve as the original process is invoked and its input-output behavior is recorded. The flexibility of gpmem is illustrated via three applications: (i) robust GP regression with hierarchical hyper-parameter learning, (ii) discovering symbolic expressions from time-series data by fully Bayesian structure learning over kernels generated by a stochastic grammar, and (iii) a bandit formulation of Bayesian optimization with automatic inference and action selection. All applications share a single 50-line Python library and require fewer than 20 lines of probabilistic code each.

</details>

<details>

<summary>2016-01-05 11:06:52 - Exploring dependence between categorical variables: benefits and limitations of using variable selection within Bayesian clustering in relation to log-linear modelling with interaction terms</summary>

- *Michail Papathomas, Sylvia Richardson*

- `1401.7214v3` - [abs](http://arxiv.org/abs/1401.7214v3) - [pdf](http://arxiv.org/pdf/1401.7214v3)

> This manuscript is concerned with relating two approaches that can be used to explore complex dependence structures between categorical variables, namely Bayesian partitioning of the covariate space incorporating a variable selection procedure that highlights the covariates that drive the clustering, and log-linear modelling with interaction terms. We derive theoretical results on this relation and discuss if they can be employed to assist log-linear model determination, demonstrating advantages and limitations with simulated and real data sets. The main advantage concerns sparse contingency tables. Inferences from clustering can potentially reduce the number of covariates considered and, subsequently, the number of competing log-linear models, making the exploration of the model space feasible. Variable selection within clustering can inform on marginal independence in general, thus allowing for a more efficient exploration of the log-linear model space. However, we show that the clustering structure is not informative on the existence of interactions in a consistent manner. This work is of interest to those who utilize log-linear models, as well as practitioners such as epidemiologists that use clustering models to reduce the dimensionality in the data and to reveal interesting patterns on how covariates combine.

</details>

<details>

<summary>2016-01-05 16:44:38 - Too good to be true: when overwhelming evidence fails to convince</summary>

- *Lachlan J. Gunn, FranÃ§ois Chapeau-Blondeau, Mark McDonnell, Bruce Davis, Andrew Allison, Derek Abbott*

- `1601.00900v1` - [abs](http://arxiv.org/abs/1601.00900v1) - [pdf](http://arxiv.org/pdf/1601.00900v1)

> Is it possible for a large sequence of measurements or observations, which support a hypothesis, to counterintuitively decrease our confidence? Can unanimous support be too good to be true? The assumption of independence is often made in good faith, however rarely is consideration given to whether a systemic failure has occurred.   Taking this into account can cause certainty in a hypothesis to decrease as the evidence for it becomes apparently stronger. We perform a probabilistic Bayesian analysis of this effect with examples based on (i) archaeological evidence, (ii) weighing of legal evidence, and (iii) cryptographic primality testing.   We find that even with surprisingly low systemic failure rates high confidence is very difficult to achieve and in particular we find that certain analyses of cryptographically-important numerical tests are highly optimistic, underestimating their false-negative rate by as much as a factor of $2^{80}$.

</details>

<details>

<summary>2016-01-05 17:15:37 - The high-conductance state enables neural sampling in networks of LIF neurons</summary>

- *Mihai A. Petrovici, Ilja Bytschok, Johannes Bill, Johannes Schemmel, Karlheinz Meier*

- `1601.00909v1` - [abs](http://arxiv.org/abs/1601.00909v1) - [pdf](http://arxiv.org/pdf/1601.00909v1)

> The apparent stochasticity of in-vivo neural circuits has long been hypothesized to represent a signature of ongoing stochastic inference in the brain. More recently, a theoretical framework for neural sampling has been proposed, which explains how sample-based inference can be performed by networks of spiking neurons. One particular requirement of this approach is that the neural response function closely follows a logistic curve.   Analytical approaches to calculating neural response functions have been the subject of many theoretical studies. In order to make the problem tractable, particular assumptions regarding the neural or synaptic parameters are usually made. However, biologically significant activity regimes exist which are not covered by these approaches: Under strong synaptic bombardment, as is often the case in cortex, the neuron is shifted into a high-conductance state (HCS) characterized by a small membrane time constant. In this regime, synaptic time constants and refractory periods dominate membrane dynamics.   The core idea of our approach is to separately consider two different "modes" of spiking dynamics: burst spiking and transient quiescence, in which the neuron does not spike for longer periods. We treat the former by propagating the PDF of the effective membrane potential from spike to spike within a burst, while using a diffusion approximation for the latter. We find that our prediction of the neural response function closely matches simulation data. Moreover, in the HCS scenario, we show that the neural response function becomes symmetric and can be well approximated by a logistic function, thereby providing the correct dynamics in order to perform neural sampling. We hereby provide not only a normative framework for Bayesian inference in cortex, but also powerful applications of low-power, accelerated neuromorphic systems to relevant machine learning tasks.

</details>

<details>

<summary>2016-01-05 20:11:39 - Bayesian Graphical Models for Multivariate Functional Data</summary>

- *Hongxiao Zhu, Nate Strawn, David B. Dunson*

- `1411.4158v3` - [abs](http://arxiv.org/abs/1411.4158v3) - [pdf](http://arxiv.org/pdf/1411.4158v3)

> Graphical models express conditional independence relationships among variables. Although methods for vector-valued data are well established, functional data graphical models remain underdeveloped. We introduce a notion of conditional independence between random functions, and construct a framework for Bayesian inference of undirected, decomposable graphs in the multivariate functional data context. This framework is based on extending Markov distributions and hyper Markov laws from random variables to random processes, providing a principled alternative to naive application of multivariate methods to discretized functional data. Markov properties facilitate the composition of likelihoods and priors according to the decomposition of a graph. Our focus is on Gaussian process graphical models using orthogonal basis expansions. We propose a hyper-inverse-Wishart-process prior for the covariance kernels of the infinite coefficient sequences of the basis expansion, establish existence, uniqueness, strong hyper Markov property, and conjugacy. Stochastic search Markov chain Monte Carlo algorithms are developed for posterior inference, assessed through simulations, and applied to a study of brain activity and alcoholism.

</details>

<details>

<summary>2016-01-06 12:54:38 - Adaptive Bayesian density regression for high-dimensional data</summary>

- *Weining Shen, Subhashis Ghosal*

- `1403.2695v2` - [abs](http://arxiv.org/abs/1403.2695v2) - [pdf](http://arxiv.org/pdf/1403.2695v2)

> Density regression provides a flexible strategy for modeling the distribution of a response variable $Y$ given predictors $\mathbf{X}=(X_1,\ldots,X_p)$ by letting that the conditional density of $Y$ given $\mathbf{X}$ as a completely unknown function and allowing its shape to change with the value of $\mathbf{X}$. The number of predictors $p$ may be very large, possibly much larger than the number of observations $n$, but the conditional density is assumed to depend only on a much smaller number of predictors, which are unknown. In addition to estimation, the goal is also to select the important predictors which actually affect the true conditional density. We consider a nonparametric Bayesian approach to density regression by constructing a random series prior based on tensor products of spline functions. The proposed prior also incorporates the issue of variable selection. We show that the posterior distribution of the conditional density contracts adaptively at the truth nearly at the optimal oracle rate, determined by the unknown sparsity and smoothness levels, even in the ultra high-dimensional settings where $p$ increases exponentially with $n$. The result is also extended to the anisotropic case where the degree of smoothness can vary in different directions, and both random and deterministic predictors are considered. We also propose a technique to calculate posterior moments of the conditional density function without requiring Markov chain Monte Carlo methods.

</details>

<details>

<summary>2016-01-06 14:06:13 - An intuitive Bayesian spatial model for disease mapping that accounts for scaling</summary>

- *Andrea Riebler, Sigrunn H. SÃ¸rbye, Daniel Simpson, HÃ¥vard Rue*

- `1601.01180v1` - [abs](http://arxiv.org/abs/1601.01180v1) - [pdf](http://arxiv.org/pdf/1601.01180v1)

> In recent years, disease mapping studies have become a routine application within geographical epidemiology and are typically analysed within a Bayesian hierarchical model formulation. A variety of model formulations for the latent level have been proposed but all come with inherent issues. In the classical BYM model, the spatially structured component cannot be seen independently from the unstructured component. This makes prior definitions for the hyperparameters of the two random effects challenging. There are alternative model formulations that address this confounding, however, the issue on how to choose interpretable hyperpriors is still unsolved. Here, we discuss a recently proposed parameterisation of the BYM model that leads to improved parameter control as the hyperparameters can be seen independently from each other. Furthermore, the need for a scaled spatial component is addressed, which facilitates assignment of interpretable hyperpriors and make these transferable between spatial applications with different graph structures. We provide implementation details for the new model formulation which preserve sparsity properties, and we investigate systematically the model performance and compare it to existing parameterisations. Through a simulation study, we show that the new model performs well, both showing good learning abilities and good shrinkage behaviour. In terms of model choice criteria, the proposed model performs at least equally well as existing parameterisations, but only the new formulation offers parameters that are interpretable and hyperpriors that have a clear meaning.

</details>

<details>

<summary>2016-01-08 06:42:29 - Asymptotic optimality of myopic information-based strategies for Bayesian adaptive estimation</summary>

- *Janne V. Kujala*

- `1506.05483v2` - [abs](http://arxiv.org/abs/1506.05483v2) - [pdf](http://arxiv.org/pdf/1506.05483v2)

> This paper presents a general asymptotic theory of sequential Bayesian estimation giving results for the strongest, almost sure convergence. We show that under certain smoothness conditions on the probability model, the greedy information gain maximization algorithm for adaptive Bayesian estimation is asymptotically optimal in the sense that the determinant of the posterior covariance in a certain neighborhood of the true parameter value is asymptotically minimal. Using this result, we also obtain an asymptotic expression for the posterior entropy based on a novel definition of almost sure convergence on "most trials" (meaning that the convergence holds on a fraction of trials that converges to one). Then, we extend the results to a recently published framework, which generalizes the usual adaptive estimation setting by allowing different trial placements to be associated with different, random costs of observation. For this setting, the author has proposed the heuristic of maximizing the expected information gain divided by the expected cost of that placement. In this paper, we show that this myopic strategy satisfies an analogous asymptotic optimality result when the convergence of the posterior distribution is considered as a function of the total cost (as opposed to the number of observations).

</details>

<details>

<summary>2016-01-08 17:35:30 - Bayesian Spatial Binary Classification</summary>

- *Candace Berrett, Catherine A. Calder*

- `1406.3647v4` - [abs](http://arxiv.org/abs/1406.3647v4) - [pdf](http://arxiv.org/pdf/1406.3647v4)

> In analyses of spatially-referenced data, researchers often have one of two goals: to quantify relationships between a response variable and covariates while accounting for residual spatial dependence or to predict the value of a response variable at unobserved locations. In this second case, when the response variable is categorical, prediction can be viewed as a classification problem. Many classification methods either ignore response-variable/covariate relationships and rely only on spatially proximate observations for classification, or they ignore spatial dependence and use only the covariates for classification. The Bayesian spatial generalized linear (mixed) model offers a tool to accommodate both spatial and covariate sources of information in classification problems. In this paper, we formally define spatial classification rules based on these models. We also take a close look at two of these models that have been proposed in the literature, namely the probit versions of the spatial generalized linear model (SGLM) and the Bayesian spatial generalized linear mixed model (SGLMM). We describe the implications of the seemingly slight differences between these models for spatial classification and explore the issue of robustness to model misspecification through a simulation study. We also provide an overview of alternatives to the SGLM/SGLMM-based classifiers and illustrate the various methods using satellite-derived land cover data from Southeast Asia.

</details>

<details>

<summary>2016-01-10 14:37:17 - Bayesian linear regression with skew-symmetric error distributions with applications to survival analysis</summary>

- *Francisco J. Rubio, Marc G. Genton*

- `1601.02224v1` - [abs](http://arxiv.org/abs/1601.02224v1) - [pdf](http://arxiv.org/pdf/1601.02224v1)

> We study Bayesian linear regression models with skew-symmetric scale mixtures of normal error distributions. These kinds of models can be used to capture departures from the usual assumption of normality of the errors in terms of heavy tails and asymmetry. We propose a general non-informative prior structure for these regression models and show that the corresponding posterior distribution is proper under mild conditions. We extend these propriety results to cases where the response variables are censored. The latter scenario is of interest in the context of accelerated failure time models, which are relevant in survival analysis. We present a simulation study that demonstrates good frequentist properties of the posterior credible intervals associated to the proposed priors. This study also sheds some light on the trade-off between increased model flexibility and the risk of over-fitting. We illustrate the performance of the proposed models with real data. Although we focus on models with univariate response variables, we also present some extensions to the multivariate case in the Supporting Web Material.

</details>

<details>

<summary>2016-01-10 16:01:22 - Bayesian Optimization in a Billion Dimensions via Random Embeddings</summary>

- *Ziyu Wang, Frank Hutter, Masrour Zoghi, David Matheson, Nando de Freitas*

- `1301.1942v2` - [abs](http://arxiv.org/abs/1301.1942v2) - [pdf](http://arxiv.org/pdf/1301.1942v2)

> Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high-dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple, has important invariance properties, and applies to domains with both categorical and continuous variables. We present a thorough theoretical analysis of REMBO. Empirical results confirm that REMBO can effectively solve problems with billions of dimensions, provided the intrinsic dimensionality is low. They also show that REMBO achieves state-of-the-art performance in optimizing the 47 discrete parameters of a popular mixed integer linear programming solver.

</details>

<details>

<summary>2016-01-10 19:23:27 - A Sufficient Statistics Construction of Bayesian Nonparametric Exponential Family Conjugate Models</summary>

- *Robert Finn, Brian Kulis*

- `1601.02257v1` - [abs](http://arxiv.org/abs/1601.02257v1) - [pdf](http://arxiv.org/pdf/1601.02257v1)

> Conjugate pairs of distributions over infinite dimensional spaces are prominent in statistical learning theory, particularly due to the widespread adoption of Bayesian nonparametric methodologies for a host of models and applications. Much of the existing literature in the learning community focuses on processes possessing some form of computationally tractable conjugacy as is the case for the beta and gamma processes (and, via normalization, the Dirichlet process). For these processes, proofs of conjugacy and requisite derivation of explicit computational formulae for posterior density parameters are idiosyncratic to the stochastic process in question. As such, Bayesian Nonparametric models are currently available for a limited number of conjugate pairs, e.g. the Dirichlet-multinomial and beta-Bernoulli process pairs. In each of these above cases the likelihood process belongs to the class of discrete exponential family distributions. The exclusion of continuous likelihood distributions from the known cases of Bayesian Nonparametric Conjugate models stands as a disparity in the researcher's toolbox.   In this paper we first address the problem of obtaining a general construction of prior distributions over infinite dimensional spaces possessing distributional properties amenable to conjugacy. Second, we bridge the divide between the discrete and continuous likelihoods by illustrating a canonical construction for stochastic processes whose Levy measure densities are from positive exponential families, and then demonstrate that these processes in fact form the prior, likelihood, and posterior in a conjugate family. Our canonical construction subsumes known computational formulae for posterior density parameters in the cases where the likelihood is from a discrete distribution belonging to an exponential family.

</details>

<details>

<summary>2016-01-11 02:34:47 - Localisation of a source of biochemical agent dispersion using binary measurements</summary>

- *Branko Ristic, Ajith Gunatilaka, Ralph Gailis*

- `1601.02304v1` - [abs](http://arxiv.org/abs/1601.02304v1) - [pdf](http://arxiv.org/pdf/1601.02304v1)

> Using the measurements collected at a number of known locations by a moving binary sensor, characterised by an unknown threshold, the problem is to estimate the parameters of a biochemical source, continuously releasing material into the atmosphere. The solution is formulated in the Bayesian framework using a dispersion model of Poisson distributed particle encounters in a turbulent flow. The method is implemented using the importance sampling technique and successfully validated with three experimental datasets under different wind conditions.

</details>

<details>

<summary>2016-01-11 14:37:58 - Modeling Multivariate Mixed-Response Functional Data</summary>

- *Beth A. Tidemann-Miller, Brian J. Reich, Ana-Maria Staicu*

- `1601.02461v1` - [abs](http://arxiv.org/abs/1601.02461v1) - [pdf](http://arxiv.org/pdf/1601.02461v1)

> We propose a Bayesian modeling framework for jointly analyzing multiple functional responses of different types (e.g. binary and continuous data). Our approach is based on a multivariate latent Gaussian process and models the dependence among the functional responses through the dependence of the latent process. Our framework easily accommodates additional covariates. We offer a way to estimate the multivariate latent covariance, allowing for implementation of multivariate functional principal components analysis (FPCA) to specify basis expansions and simplify computation. We demonstrate our method through both simulation studies and an application to real data from a periodontal study.

</details>

<details>

<summary>2016-01-12 07:36:53 - Excellence networks in science: A Web-based application based on Bayesian multilevel logistic regression (BMLR) for the identification of institutions collaborating successfully</summary>

- *Lutz Bornmann, Moritz Stefaner, Felix de Moya Anegon, Ruediger Mutz*

- `1508.03950v4` - [abs](http://arxiv.org/abs/1508.03950v4) - [pdf](http://arxiv.org/pdf/1508.03950v4)

> In this study we present an application which can be accessed via www.excellence-networks.net and which represents networks of scientific institutions worldwide. The application is based on papers (articles, reviews and conference papers) published between 2007 and 2011. It uses (network) data, on which the SCImago Institutions Ranking is based (Scopus data from Elsevier). Using this data, institutional networks have been estimated with statistical models (Bayesian multilevel logistic regression, BMLR) for a number of Scopus subject areas. Within single subject areas, we have investigated and visualized how successfully overall an institution (reference institution) has collaborated (compared to all the other institutions in a subject area), and with which other institutions (network institutions) a reference institution has collaborated particularly successfully. The "best paper rate" (statistically estimated) was used as an indicator for evaluating the collaboration success of an institution. This gives the proportion of highly cited papers from an institution, and is considered generally as an indicator for measuring impact in bibliometrics.

</details>

<details>

<summary>2016-01-12 12:03:22 - A new Bayesian regression model for counts in medicine</summary>

- *Hamed Haselimashhadi, Veronica Vinciotti, Keming Yu*

- `1601.02820v1` - [abs](http://arxiv.org/abs/1601.02820v1) - [pdf](http://arxiv.org/pdf/1601.02820v1)

> Discrete data are collected in many application areas and are often characterised by highly skewed and power-lawlike distributions. An example of this, which is considered in this paper, is the number of visits to a specialist, often taken as a measure of demand in healthcare. A discrete Weibull regression model was recently proposed for regression problems with a discrete response and it was shown to possess two important features: the ability to capture over and under-dispersion simultaneously and a closed-form analytical expression of the quantiles of the conditional distribution. In this paper, we propose the first Bayesian implementation of a discrete Weibull regression model. The implementation considers a novel parameterization, where both parameters of the discrete Weibull distribution can be made dependent on the predictors. In addition, prior distributions can be imposed that encourage parameter shrinkage and that lead to variable selection. As with Bayesian procedures, the full posterior distribution of the parameters is returned, from which credible intervals can be readily calculated. A simulation study and the analysis of four real datasets of medical records show promises for the wide applicability of this approach to the analysis of count data. The method is implemented in the R package BDWreg.

</details>

<details>

<summary>2016-01-13 02:44:36 - Blind Image Denoising via Dependent Dirichlet Process Tree</summary>

- *Fengyuan Zhu, Guangyong Chen, Jianye Hao, Pheng-Ann Heng*

- `1601.03117v1` - [abs](http://arxiv.org/abs/1601.03117v1) - [pdf](http://arxiv.org/pdf/1601.03117v1)

> Most existing image denoising approaches assumed the noise to be homogeneous white Gaussian distributed with known intensity. However, in real noisy images, the noise models are usually unknown beforehand and can be much more complex. This paper addresses this problem and proposes a novel blind image denoising algorithm to recover the clean image from noisy one with the unknown noise model. To model the empirical noise of an image, our method introduces the mixture of Gaussian distribution, which is flexible enough to approximate different continuous distributions. The problem of blind image denoising is reformulated as a learning problem. The procedure is to first build a two-layer structural model for noisy patches and consider the clean ones as latent variable. To control the complexity of the noisy patch model, this work proposes a novel Bayesian nonparametric prior called "Dependent Dirichlet Process Tree" to build the model. Then, this study derives a variational inference algorithm to estimate model parameters and recover clean patches. We apply our method on synthesis and real noisy images with different noise models. Comparing with previous approaches, ours achieves better performance. The experimental results indicate the efficiency of the proposed algorithm to cope with practical image denoising tasks.

</details>

<details>

<summary>2016-01-13 04:20:09 - Online Prediction of Dyadic Data with Heterogeneous Matrix Factorization</summary>

- *Guangyong Chen, Fengyuan Zhu, Pheng Ann Heng*

- `1601.03124v1` - [abs](http://arxiv.org/abs/1601.03124v1) - [pdf](http://arxiv.org/pdf/1601.03124v1)

> Dyadic Data Prediction (DDP) is an important problem in many research areas. This paper develops a novel fully Bayesian nonparametric framework which integrates two popular and complementary approaches, discrete mixed membership modeling and continuous latent factor modeling into a unified Heterogeneous Matrix Factorization~(HeMF) model, which can predict the unobserved dyadics accurately. The HeMF can determine the number of communities automatically and exploit the latent linear structure for each bicluster efficiently. We propose a Variational Bayesian method to estimate the parameters and missing data. We further develop a novel online learning approach for Variational inference and use it for the online learning of HeMF, which can efficiently cope with the important large-scale DDP problem. We evaluate the performance of our method on the EachMoive, MovieLens and Netflix Prize collaborative filtering datasets. The experiment shows that, our model outperforms state-of-the-art methods on all benchmarks. Compared with Stochastic Gradient Method (SGD), our online learning approach achieves significant improvement on the estimation accuracy and robustness.

</details>

<details>

<summary>2016-01-13 18:58:42 - Moment conditions and Bayesian nonparametrics</summary>

- *Luke Bornn, Neil Shephard, Reza Solgi*

- `1507.08645v2` - [abs](http://arxiv.org/abs/1507.08645v2) - [pdf](http://arxiv.org/pdf/1507.08645v2)

> Models phrased though moment conditions are central to much of modern inference. Here these moment conditions are embedded within a nonparametric Bayesian setup. Handling such a model is not probabilistically straightforward as the posterior has support on a manifold. We solve the relevant issues, building new probability and computational tools using Hausdorff measures to analyze them on real and simulated data. These new methods which involve simulating on a manifold can be applied widely, including providing Bayesian analysis of quasi-likelihoods, linear and nonlinear regression, missing data and hierarchical models.

</details>

<details>

<summary>2016-01-13 23:59:52 - On Asymptotic Optimality in Sequential Changepoint Detection: Non-iid Case</summary>

- *Alexander G. Tartakovsky*

- `1510.03827v2` - [abs](http://arxiv.org/abs/1510.03827v2) - [pdf](http://arxiv.org/pdf/1510.03827v2)

> We consider a sequential Bayesian changepoint detection problem for a general stochastic model, assuming that the observed data may be dependent and non-identically distributed and the prior distribution of the change point is arbitrary, not necessarily geometric. Tartakovsky and Veeravalli (2004) developed a general asymptotic theory of changepoint detection in the non-iid case and discrete time, and Baron and Tartakovsky (2006) in continuous time assuming certain stability of the log-likelihood ratio process. This stability property was formulated in terms of the r-quick convergence of the normalized log-likelihood ratio process to a positive and finite number, which can be interpreted as the limiting Kullback-Leibler information between the "change" and "no change" hypotheses. In these papers, it was conjectured that the r-quick convergence can be relaxed in the r-complete convergence, which is typically much easier to verify in particular examples. In the present paper, we justify this conjecture by showing that the Shiryaev change detection procedure is nearly optimal, minimizing asymptotically (as the probability of false alarm vanishes) the moments of the delay to detection up to order r whenever r-complete convergence holds. We also study asymptotic properties of the Shiryaev-Roberts detection procedure in the Bayesian context.

</details>

<details>

<summary>2016-01-14 04:40:17 - Bayesian Inference of Arrival Rate and Substitution Behavior from Sales Transaction Data with Stockouts</summary>

- *Benjamin Letham, Lydia M. Letham, Cynthia Rudin*

- `1502.04243v3` - [abs](http://arxiv.org/abs/1502.04243v3) - [pdf](http://arxiv.org/pdf/1502.04243v3)

> When an item goes out of stock, sales transaction data no longer reflect the original customer demand, since some customers leave with no purchase while others substitute alternative products for the one that was out of stock. Here we develop a Bayesian hierarchical model for inferring the underlying customer arrival rate and choice model from sales transaction data and the corresponding stock levels. The model uses a nonhomogeneous Poisson process to allow the arrival rate to vary throughout the day, and allows for a variety of choice models. Model parameters are inferred using a stochastic gradient MCMC algorithm that can scale to large transaction databases. We fit the model to data from a local bakery and show that it is able to make accurate out-of-sample predictions, and to provide actionable insight into lost cookie sales.

</details>

<details>

<summary>2016-01-15 23:01:42 - Goodness-of-fit statistics for approximate Bayesian computation</summary>

- *Louisiane Lemaire, Flora Jay, I-Hung Lee, Katalin CsillÃ©ry, Michael G. B. Blum*

- `1601.04096v1` - [abs](http://arxiv.org/abs/1601.04096v1) - [pdf](http://arxiv.org/pdf/1601.04096v1)

> Approximate Bayesian computation is a statistical framework that uses numerical simulations to calibrate and compare models. Instead of computing likelihood functions, Approximate Bayesian computation relies on numerical simulations, which makes it applicable to complex models in ecology and evolution. As usual for statistical modeling, evaluating goodness-of-fit is a fundamental step for Approximate Bayesian Computation. Here, we introduce a goodness-of-fit approach based on hypothesis-testing. We introduce two test statistics based on the mean distance between numerical summaries of the data and simulated ones. One test statistic relies on summaries simulated with the prior predictive distribution whereas the other one relies on simulations from the posterior predictive distribution. For different coalescent models, we find that the statistics are well calibrated, meaning that the type I error can be controlled. However, the statistical power of the two statistics is extremely variable across models ranging from 20% to 100%. The difference of power between the two statistics is negligible in models of demographic inference but substantial in an additional and purely statistical example. When analyzing resequencing data to evaluate models of human demography, the two statistics confirm that an out-of-Africa bottleneck cannot be rejected for Asiatic and European data. We also consider two speciation models in the context of a butterfly species complex. One goodness-of-fit statistic indicates a poor fit for both models, and the numerical summaries causing the poor fit were identified using posterior predictive checks. Statistical tests for goodness-of-fit should foster evaluation of model fit in Approximate Bayesian Computation. The test statistic based on simulations from the prior predictive distribution is implemented in the gfit function of the R abc package.

</details>

<details>

<summary>2016-01-17 05:20:19 - On-line Bayesian System Identification</summary>

- *Diego Romeres, Giulia Prando, Gianluigi Pillonetto, Alessandro Chiuso*

- `1601.04251v1` - [abs](http://arxiv.org/abs/1601.04251v1) - [pdf](http://arxiv.org/pdf/1601.04251v1)

> We consider an on-line system identification setting, in which new data become available at given time steps. In order to meet real-time estimation requirements, we propose a tailored Bayesian system identification procedure, in which the hyper-parameters are still updated through Marginal Likelihood maximization, but after only one iteration of a suitable iterative optimization algorithm. Both gradient methods and the EM algorithm are considered for the Marginal Likelihood optimization. We compare this "1-step" procedure with the standard one, in which the optimization method is run until convergence to a local minimum. The experiments we perform confirm the effectiveness of the approach we propose.

</details>

<details>

<summary>2016-01-17 11:40:11 - A TV-Gaussian prior for infinite-dimensional Bayesian inverse problems and its numerical implementations</summary>

- *Zhewei Yao, Zixi Hu, Jinglai Li*

- `1510.05239v2` - [abs](http://arxiv.org/abs/1510.05239v2) - [pdf](http://arxiv.org/pdf/1510.05239v2)

> Many scientific and engineering problems require to perform Bayesian inferences in function spaces, in which the unknowns are of infinite dimension. In such problems, choosing an appropriate prior distribution is an important task. In particular we consider problems where the function to infer is subject to sharp jumps which render the commonly used Gaussian measures unsuitable. On the other hand, the so-called total variation (TV) prior can only be defined in a finite dimensional setting, and does not lead to a well-defined posterior measure in function spaces. In this work we present a TV-Gaussian (TG) prior to address such problems, where the TV term is used to detect sharp jumps of the function, and the Gaussian distribution is used as a reference measure so that it results in a well-defined posterior measure in the function space. We also present an efficient Markov Chain Monte Carlo (MCMC) algorithm to draw samples from the posterior distribution of the TG prior. With numerical examples we demonstrate the performance of the TG prior and the efficiency of the proposed MCMC algorithm.

</details>

<details>

<summary>2016-01-18 13:17:08 - Probabilistic Line Searches for Stochastic Optimization</summary>

- *Maren Mahsereci, Philipp Hennig*

- `1502.02846v4` - [abs](http://arxiv.org/abs/1502.02846v4) - [pdf](http://arxiv.org/pdf/1502.02846v4)

> In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.

</details>

<details>

<summary>2016-01-18 20:42:07 - Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference</summary>

- *Yarin Gal, Zoubin Ghahramani*

- `1506.02158v6` - [abs](http://arxiv.org/abs/1506.02158v6) - [pdf](http://arxiv.org/pdf/1506.02158v6)

> Convolutional neural networks (CNNs) work well on large datasets. But labelled data is hard to collect, and in some applications larger amounts of data are not available. The problem then is how to use CNNs with small data -- as CNNs overfit quickly. We present an efficient Bayesian CNN, offering better robustness to over-fitting on small data than traditional approaches. This is by placing a probability distribution over the CNN's kernels. We approximate our model's intractable posterior with Bernoulli variational distributions, requiring no additional model parameters.   On the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity, while highlighting a negative result in the field. We show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10.

</details>

<details>

<summary>2016-01-19 20:55:10 - Understanding Past Population Dynamics: Bayesian Coalescent-Based Modeling with Covariates</summary>

- *Mandev S. Gill, Philippe Lemey, Shannon N. Bennett, Roman Biek, Marc A. Suchard*

- `1601.05078v1` - [abs](http://arxiv.org/abs/1601.05078v1) - [pdf](http://arxiv.org/pdf/1601.05078v1)

> Effective population size characterizes the genetic variability in a population and is a parameter of paramount importance in population genetics. Kingman's coalescent process enables inference of past population dynamics directly from molecular sequence data, and researchers have developed a number of flexible coalescent-based models for Bayesian nonparametric estimation of the effective population size as a function of time. A major goal of demographic reconstruction is understanding the association between the effective population size and potential explanatory factors. Building upon Bayesian nonparametric coalescent-based approaches, we introduce a flexible framework that incorporates time-varying covariates through Gaussian Markov random fields. To approximate the posterior distribution, we adapt efficient Markov chain Monte Carlo algorithms designed for highly structured Gaussian models. Incorporating covariates into the demographic inference framework enables the modeling of associations between the effective population size and covariates while accounting for uncertainty in population histories. Furthermore, it can lead to more precise estimates of population dynamics. We apply our model to four examples. We reconstruct the demographic history of raccoon rabies in North America and find a significant association with the spatiotemporal spread of the outbreak. Next, we examine the effective population size trajectory of the DENV-4 virus in Puerto Rico along with viral isolate count data and find similar cyclic patterns. We compare the population history of the HIV-1 CRF02_AG clade in Cameroon with HIV incidence and prevalence data and find that the effective population size is more reflective of incidence rate. Finally, we explore the hypothesis that the population dynamics of musk ox during the Late Quaternary period were related to climate change.

</details>

<details>

<summary>2016-01-19 22:28:10 - Adaptive Bayesian Estimation of Conditional Densities</summary>

- *Andriy Norets, Debdeep Pati*

- `1408.5355v3` - [abs](http://arxiv.org/abs/1408.5355v3) - [pdf](http://arxiv.org/pdf/1408.5355v3)

> We consider a non-parametric Bayesian model for conditional densities. The model is a finite mixture of normal distributions with covariate dependent multinomial logit mixing probabilities. A prior for the number of mixture components is specified on positive integers. The marginal distribution of covariates is not modeled. We study asymptotic frequentist behavior of the posterior in this model. Specifically, we show that when the true conditional density has a certain smoothness level, then the posterior contraction rate around the truth is equal up to a log factor to the frequentist minimax rate of estimation. An extension to the case when the covariate space is unbounded is also established. As our result holds without a priori knowledge of the smoothness level of the true density, the established posterior contraction rates are adaptive. Moreover, we show that the rate is not affected by inclusion of irrelevant covariates in the model. In Monte Carlo simulations, a version of the model compares favorably to a cross-validated kernel conditional density estimator.

</details>

<details>

<summary>2016-01-20 19:48:24 - Bayesian inference of natural selection from allele frequency time series</summary>

- *Joshua G. Schraiber, Steven N. Evans, Montgomery Slatkin*

- `1601.05388v1` - [abs](http://arxiv.org/abs/1601.05388v1) - [pdf](http://arxiv.org/pdf/1601.05388v1)

> The advent of accessible ancient DNA technology now allows the direct ascertainment of allele frequencies in ancestral populations, thereby enabling the use of allele frequency time series to detect and estimate natural selection. Such direct observations of allele frequency dynamics are expected to be more powerful than inferences made using patterns of linked neutral variation obtained from modern individuals. We develop a Bayesian method to make use of allele frequency time series data and infer the parameters of general diploid selection, along with allele age, in non-equilibrium populations. We introduce a novel path augmentation approach, in which we use Markov chain Monte Carlo to integrate over the space of allele frequency trajectories consistent with the observed data. Using simulations, we show that this approach has good power to estimate selection coefficients and allele age. Moreover, when applying our approach to data on horse coat color, we find that ignoring a relevant demographic history can significantly bias the results of inference. Our approach is made available in a C++ software package.

</details>

<details>

<summary>2016-01-20 23:33:41 - Bayesian model comparison with un-normalised likelihoods</summary>

- *Richard G. Everitt, Adam M. Johansen, Ellen Rowing, Melina Evdemon-Hogan*

- `1504.00298v3` - [abs](http://arxiv.org/abs/1504.00298v3) - [pdf](http://arxiv.org/pdf/1504.00298v3)

> Models for which the likelihood function can be evaluated only up to a parameter-dependent unknown normalising constant, such as Markov random field models, are used widely in computer science, statistical physics, spatial statistics, and network analysis. However, Bayesian analysis of these models using standard Monte Carlo methods is not possible due to the intractability of their likelihood functions. Several methods that permit exact, or close to exact, simulation from the posterior distribution have recently been developed. However, estimating the evidence and Bayes' factors (BFs) for these models remains challenging in general. This paper describes new random weight importance sampling and sequential Monte Carlo methods for estimating BFs that use simulation to circumvent the evaluation of the intractable likelihood, and compares them to existing methods. In some cases we observe an advantage in the use of biased weight estimates. An initial investigation into the theoretical and empirical properties of this class of methods is presented. Some support for the use of biased estimates is presented, but we advocate caution in the use of such estimates.

</details>

<details>

<summary>2016-01-21 14:24:56 - Rate exact Bayesian adaptation with modified block priors</summary>

- *Chao Gao, Harrison H. Zhou*

- `1312.3937v3` - [abs](http://arxiv.org/abs/1312.3937v3) - [pdf](http://arxiv.org/pdf/1312.3937v3)

> A novel block prior is proposed for adaptive Bayesian estimation. The prior does not depend on the smoothness of the function or the sample size. It puts sufficient prior mass near the true signal and automatically concentrates on its effective dimension. A rate-optimal posterior contraction is obtained in a general framework, which includes density estimation, white noise model, Gaussian sequence model, Gaussian regression and spectral density estimation.

</details>

<details>

<summary>2016-01-24 23:33:02 - Sequential Hypothesis Test with Online Usage-Constrained Sensor Selection</summary>

- *Shang Li, Xiaoou Li, Xiaodong Wang, Jingchen Liu*

- `1601.06447v1` - [abs](http://arxiv.org/abs/1601.06447v1) - [pdf](http://arxiv.org/pdf/1601.06447v1)

> This work investigates the sequential hypothesis testing problem with online sensor selection and sensor usage constraints. That is, in a sensor network, the fusion center sequentially acquires samples by selecting one "most informative" sensor at each time until a reliable decision can be made. In particular, the sensor selection is carried out in the online fashion since it depends on all the previous samples at each time. Our goal is to develop the sequential test (i.e., stopping rule and decision function) and sensor selection strategy that minimize the expected sample size subject to the constraints on the error probabilities and sensor usages. To this end, we first recast the usage-constrained formulation into a Bayesian optimal stopping problem with different sampling costs for the usage-contrained sensors. The Bayesian problem is then studied under both finite- and infinite-horizon setups, based on which, the optimal solution to the original usage-constrained problem can be readily established. Moreover, by capitalizing on the structures of the optimal solution, a lower bound is obtained for the optimal expected sample size. In addition, we also propose algorithms to approximately evaluate the parameters in the optimal sequential test so that the sensor usage and error probability constraints are satisfied. Finally, numerical experiments are provided to illustrate the theoretical findings, and compare with the existing methods.

</details>

<details>

<summary>2016-01-25 09:53:13 - Meta-analysis of few small studies in orphan diseases</summary>

- *Tim Friede, Christian RÃ¶ver, Simon Wandel, Beat Neuenschwander*

- `1601.06533v1` - [abs](http://arxiv.org/abs/1601.06533v1) - [pdf](http://arxiv.org/pdf/1601.06533v1)

> Meta-analyses in orphan diseases and small populations generally face particular problems including small numbers of studies, small study sizes, and heterogeneity of results. However, the heterogeneity is difficult to estimate if only very few studies are included. Motivated by a systematic review in immunosuppression following liver transplantation in children we investigate the properties of a range of commonly used frequentist and Bayesian procedures in extensive simulation studies. Furthermore, the consequences for interval estimation of the common treatment effect in random effects meta-analysis are assessed. The Bayesian credibility intervals using weakly informative priors for the between-trial heterogeneity exhibited coverage probabilities in excess of the nominal level for a range of scenarios considered. However, they tended to be shorter than those obtained by the Knapp-Hartung method, which were also conservative. In contrast, methods based on normal quantiles exhibited coverages well below the nominal levels in many scenarios. With very few studies, the performance of the Bayesian credibility intervals is of course sensitive to the specification of the prior for the between trial heterogeneity. In conclusion, the use of weakly informative priors as exemplified by half-normal priors (with scale 0.5 or 1.0) for log odds ratios is recommended for applications in rare diseases.

</details>

<details>

<summary>2016-01-25 14:58:41 - Bayesian Estimation of Bipartite Matchings for Record Linkage</summary>

- *Mauricio Sadinle*

- `1601.06630v1` - [abs](http://arxiv.org/abs/1601.06630v1) - [pdf](http://arxiv.org/pdf/1601.06630v1)

> The bipartite record linkage task consists of merging two disparate datafiles containing information on two overlapping sets of entities. This is non-trivial in the absence of unique identifiers and it is important for a wide variety of applications given that it needs to be solved whenever we have to combine information from different sources. Most statistical techniques currently used for record linkage are derived from a seminal paper by Fellegi and Sunter (1969). These techniques usually assume independence in the matching statuses of record pairs to derive estimation procedures and optimal point estimators. We argue that this independence assumption is unreasonable and instead target a bipartite matching between the two datafiles as our parameter of interest. Bayesian implementations allow us to quantify uncertainty on the matching decisions and derive a variety of point estimators using different loss functions. We propose partial Bayes estimates that allow uncertain parts of the bipartite matching to be left unresolved. We evaluate our approach to record linkage using a variety of challenging scenarios and show that it outperforms the traditional methodology. We illustrate the advantages of our methods merging two datafiles on casualties from the civil war of El Salvador.

</details>

<details>

<summary>2016-01-25 16:02:50 - Time-Varying Gaussian Process Bandit Optimization</summary>

- *Ilija Bogunovic, Jonathan Scarlett, Volkan Cevher*

- `1601.06650v1` - [abs](http://arxiv.org/abs/1601.06650v1) - [pdf](http://arxiv.org/pdf/1601.06650v1)

> We consider the sequential Bayesian optimization problem with bandit feedback, adopting a formulation that allows for the reward function to vary with time. We model the reward function using a Gaussian process whose evolution obeys a simple Markov model. We introduce two natural extensions of the classical Gaussian process upper confidence bound (GP-UCB) algorithm. The first, R-GP-UCB, resets GP-UCB at regular intervals. The second, TV-GP-UCB, instead forgets about old data in a smooth fashion. Our main contribution comprises of novel regret bounds for these algorithms, providing an explicit characterization of the trade-off between the time horizon and the rate at which the function varies. We illustrate the performance of the algorithms on both synthetic and real data, and we find the gradual forgetting of TV-GP-UCB to perform favorably compared to the sharp resetting of R-GP-UCB. Moreover, both algorithms significantly outperform classical GP-UCB, since it treats stale and fresh data equally.

</details>

<details>

<summary>2016-01-25 16:07:54 - Testing for Causality in Continuous Time Bayesian Network Models of High-Frequency Data</summary>

- *Jonas Hallgren, Timo Koski*

- `1601.06651v1` - [abs](http://arxiv.org/abs/1601.06651v1) - [pdf](http://arxiv.org/pdf/1601.06651v1)

> Continuous time Bayesian networks are investigated with a special focus on their ability to express causality. A framework is presented for doing inference in these networks. The central contributions are a representation of the intensity matrices for the networks and the introduction of a causality measure. A new model for high-frequency financial data is presented. It is calibrated to market data and by the new causality measure it performs better than older models.

</details>

<details>

<summary>2016-01-26 10:05:50 - Posterior contraction rates for deconvolution of Dirichlet-Laplace mixtures</summary>

- *Fengnan Gao, Aad van der Vaart*

- `1507.07412v2` - [abs](http://arxiv.org/abs/1507.07412v2) - [pdf](http://arxiv.org/pdf/1507.07412v2)

> We study nonparametric Bayesian inference with location mixtures of the Laplace density and a Dirichlet process prior on the mixing distribution. We derive a contraction rate of the corresponding posterior distribution, both for the mixing distribution relative to the Wasserstein metric and for the mixed density relative to the Hellinger and $L_q$ metrics.

</details>

<details>

<summary>2016-01-27 11:44:23 - Convergence Analysis of the Data Augmentation Algorithm for Bayesian Linear Regression with Non-Gaussian Errors</summary>

- *James P. Hobert, Yeun Ji Jung, Kshitij Khare, Qian Qin*

- `1506.03113v2` - [abs](http://arxiv.org/abs/1506.03113v2) - [pdf](http://arxiv.org/pdf/1506.03113v2)

> Gaussian errors are sometimes inappropriate in a multivariate linear regression setting because, for example, the data contain outliers. In such situations, it is often assumed that the error density is a scale mixture of multivariate normal densities that takes the form $f(\varepsilon) = \int_0^\infty |\Sigma|^{-\frac{1}{2}} u^{\frac{d}{2}} \, \phi_d \big( \Sigma^{-\frac{1}{2}} \sqrt{u} \, \varepsilon \big) \, h(u) \, du$, where $d$ is the dimension of the response, $\phi_d(\cdot)$ is the standard $d$-variate normal density, $\Sigma$ is an unknown $d \times d$ positive definite scale matrix, and $h(\cdot)$ is some fixed mixing density. Combining this alternative regression model with a default prior on the unknown parameters results in a highly intractable posterior density. Fortunately, there is a simple data augmentation (DA) algorithm and a corresponding Haar PX-DA algorithm that can be used to explore this posterior. This paper provides conditions (on $h$) for geometric ergodicity of the Markov chains underlying these Markov chain Monte Carlo (MCMC) algorithms. These results are extremely important from a practical standpoint because geometric ergodicity guarantees the existence of the central limit theorems that form the basis of all the standard methods of calculating valid asymptotic standard errors for MCMC-based estimators. The main result is that, if $h$ converges to 0 at the origin at an appropriate rate, and $\int_0^\infty u^{\frac{d}{2}} \, h(u) \, du < \infty$, then the DA and Haar PX-DA Markov chains are both geometrically ergodic. This result is quite far-reaching. For example, it implies the geometric ergodicity of the DA and Haar PX-DA Markov chains whenever $h$ is generalized inverse Gaussian, log-normal, inverted gamma (with shape parameter larger than $d/2$), or Fr\'{e}chet (with shape parameter larger than $d/2$).

</details>

<details>

<summary>2016-01-27 12:41:56 - On Bayesian quantile regression and outliers</summary>

- *Bruno Santos, Heleno Bolfarine*

- `1601.07344v1` - [abs](http://arxiv.org/abs/1601.07344v1) - [pdf](http://arxiv.org/pdf/1601.07344v1)

> In this work we discuss the progress of Bayesian quantile regression models since their first proposal and we discuss the importance of all parameters involved in the inference process. Using a representation of the asymmetric Laplace distribution as a mixture of a normal and an exponential distribution, we discuss the relevance of the presence of a scale parameter to control for the variance in the model. Besides that we consider the posterior distribution of the latent variable present in the mixture representation to showcase outlying observations given the Bayesian quantile regression fits, where we compare the posterior distribution for each latent variable with the others. We illustrate these results with simulation studies and also with data about Gini indexes in Brazilian states from years with census information.

</details>

<details>

<summary>2016-01-28 01:40:56 - Assessing differences in legislators' revealed preferences: A case study on the 107th U.S. Senate</summary>

- *Chelsea Lofland, Abel Rodriguez, Scott Moser*

- `1601.07617v1` - [abs](http://arxiv.org/abs/1601.07617v1) - [pdf](http://arxiv.org/pdf/1601.07617v1)

> Roll call data are widely used to assess legislators' preferences and ideology, as well as test theories of legislative behavior. In particular, roll call data is often used to determine whether the revealed preferences of legislators are affected by outside forces such as party pressure, minority status or procedural rules. This paper describes a Bayesian hierarchical model that extends existing spatial voting models to test sharp hypotheses about differences in preferences the using posterior probabilities associated with such hypotheses. We use our model to investigate the effect of the change of party majority status during the 107th U.S. Senate on the revealed preferences of senators. This analysis provides evidence that change in party affiliation might affect the revealed preferences of legislators, but provides no evidence about the effect of majority status on the revealed preferences of legislators.

</details>

<details>

<summary>2016-01-28 02:24:53 - Bayesian inference in non-Markovian state-space models with applications to fractional order systems</summary>

- *Pierre E. Jacob, S. M. Mahdi Alavi, Adam Mahdi, Stephen J. Payne, David A. Howey*

- `1601.07622v1` - [abs](http://arxiv.org/abs/1601.07622v1) - [pdf](http://arxiv.org/pdf/1601.07622v1)

> Battery impedance spectroscopy models are given by fractional order (FO) differential equations. In the discrete-time domain, they give rise to state-space models where the latent process is not Markovian. Parameter estimation for these models is therefore challenging, especially for non-commensurate FO models. In this paper, we propose a Bayesian approach to identify the parameters of generic FO systems. The computational challenge is tackled with particle Markov chain Monte Carlo methods, with an implementation specifically designed for the non-Markovian setting. The approach is then applied to estimate the parameters of a battery non-commensurate FO equivalent circuit model. Extensive simulations are provided to study the practical identifiability of model parameters and their sensitivity to the choice of prior distributions, the number of observations, the magnitude of the input signal and the measurement noise.

</details>

<details>

<summary>2016-01-28 14:11:48 - A General Framework for Updating Belief Distributions</summary>

- *Pier Giovanni Bissiri, Chris Holmes, Stephen Walker*

- `1306.6430v2` - [abs](http://arxiv.org/abs/1306.6430v2) - [pdf](http://arxiv.org/pdf/1306.6430v2)

> We propose a framework for general Bayesian inference. We argue that a valid update of a prior belief distribution to a posterior can be made for parameters which are connected to observations through a loss function rather than the traditional likelihood function, which is recovered under the special case of using self information loss. Modern application areas make it is increasingly challenging for Bayesians to attempt to model the true data generating mechanism. Moreover, when the object of interest is low dimensional, such as a mean or median, it is cumbersome to have to achieve this via a complete model for the whole data distribution. More importantly, there are settings where the parameter of interest does not directly index a family of density functions and thus the Bayesian approach to learning about such parameters is currently regarded as problematic. Our proposed framework uses loss-functions to connect information in the data to functionals of interest. The updating of beliefs then follows from a decision theoretic approach involving cumulative loss functions. Importantly, the procedure coincides with Bayesian updating when a true likelihood is known, yet provides coherent subjective inference in much more general settings. Connections to other inference frameworks are highlighted.

</details>

<details>

<summary>2016-01-29 08:53:57 - Multivariate spatio-temporal models for high-dimensional areal data with application to Longitudinal Employer-Household Dynamics</summary>

- *Jonathan R. Bradley, Scott H. Holan, Christopher K. Wikle*

- `1503.00982v2` - [abs](http://arxiv.org/abs/1503.00982v2) - [pdf](http://arxiv.org/pdf/1503.00982v2)

> Many data sources report related variables of interest that are also referenced over geographic regions and time; however, there are relatively few general statistical methods that one can readily use that incorporate these multivariate spatio-temporal dependencies. Additionally, many multivariate spatio-temporal areal data sets are extremely high dimensional, which leads to practical issues when formulating statistical models. For example, we analyze Quarterly Workforce Indicators (QWI) published by the US Census Bureau's Longitudinal Employer-Household Dynamics (LEHD) program. QWIs are available by different variables, regions, and time points, resulting in millions of tabulations. Despite their already expansive coverage, by adopting a fully Bayesian framework, the scope of the QWIs can be extended to provide estimates of missing values along with associated measures of uncertainty. Motivated by the LEHD, and other applications in federal statistics, we introduce the multivariate spatio-temporal mixed effects model (MSTM), which can be used to efficiently model high-dimensional multivariate spatio-temporal areal data sets. The proposed MSTM extends the notion of Moran's I basis functions to the multivariate spatio-temporal setting. This extension leads to several methodological contributions, including extremely effective dimension reduction, a dynamic linear model for multivariate spatio-temporal areal processes, and the reduction of a high-dimensional parameter space using a novel parameter model.

</details>

<details>

<summary>2016-01-29 09:20:39 - A Robust UCB Scheme for Active Learning in Regression from Strategic Crowds</summary>

- *Divya Padmanabhan, Satyanath Bhat, Dinesh Garg, Shirish Shevade, Y. Narahari*

- `1601.06750v2` - [abs](http://arxiv.org/abs/1601.06750v2) - [pdf](http://arxiv.org/pdf/1601.06750v2)

> We study the problem of training an accurate linear regression model by procuring labels from multiple noisy crowd annotators, under a budget constraint. We propose a Bayesian model for linear regression in crowdsourcing and use variational inference for parameter estimation. To minimize the number of labels crowdsourced from the annotators, we adopt an active learning approach. In this specific context, we prove the equivalence of well-studied criteria of active learning like entropy minimization and expected error reduction. Interestingly, we observe that we can decouple the problems of identifying an optimal unlabeled instance and identifying an annotator to label it. We observe a useful connection between the multi-armed bandit framework and the annotator selection in active learning. Due to the nature of the distribution of the rewards on the arms, we use the Robust Upper Confidence Bound (UCB) scheme with truncated empirical mean estimator to solve the annotator selection problem. This yields provable guarantees on the regret. We further apply our model to the scenario where annotators are strategic and design suitable incentives to induce them to put in their best efforts.

</details>

<details>

<summary>2016-01-29 11:10:09 - Bayesian analysis of traffic flow on interstate I-55: The LWR model</summary>

- *Nicholas Polson, Vadim Sokolov*

- `1409.6034v4` - [abs](http://arxiv.org/abs/1409.6034v4) - [pdf](http://arxiv.org/pdf/1409.6034v4)

> Transportation departments take actions to manage traffic flow and reduce travel times based on estimated current and projected traffic conditions. Travel time estimates and forecasts require information on traffic density which are combined with a model to project traffic flow such as the Lighthill-Whitham-Richards (LWR) model. We develop a particle filtering and learning algorithm to estimate the current traffic density state and the LWR parameters. These inputs are related to the so-called fundamental diagram, which describes the relationship between traffic flow and density. We build on existing methodology by allowing real-time updating of the posterior uncertainty for the critical density and capacity parameters. Our methodology is applied to traffic flow data from interstate highway I-55 in Chicago. We provide a real-time data analysis of how to learn the drop in capacity as a result of a major traffic accident. Our algorithm allows us to accurately assess the uncertainty of the current traffic state at shock waves, where the uncertainty is a mixture distribution. We show that Bayesian learning can correct the estimation bias that is present in the model with fixed parameters.

</details>

<details>

<summary>2016-01-29 13:29:30 - Lymphangiogenesis and carcinoma in the uterine cervix: Joint and hierarchical models for random cluster sizes and continuous outcomes</summary>

- *T. R. Fanshawe, C. M. Chapman, T. Crick*

- `1601.08097v1` - [abs](http://arxiv.org/abs/1601.08097v1) - [pdf](http://arxiv.org/pdf/1601.08097v1)

> Although the lymphatic system is clearly linked to the metastasis of most human carcinomas, the mechanisms by which lymphangiogenesis occurs in response to the presence of carcinoma remain unclear. Hierarchical models are presented to investigate the properties of lymphatic vessel production in 2997 fields taken from 20 individuals with invasive carcinoma, 21 individuals with cervical intraepithelial neoplasia and 21 controls. Such data demonstrate a high degree of correlation within tumour samples from the same individual. Joint hierarchical models utilising shared random effects are discussed and fitted in a Bayesian framework to allow for the correlation between two key outcome measures: a random cluster size (the number of lymphatic vessels in a tissue sample) and a continuous outcome (vessel size). Results show that invasive carcinoma samples are associated with increased production of smaller and more irregularly-shaped lymphatic vessels and suggest a mechanistic link between carcinoma of the cervix and lymphangiogenesis.

</details>

<details>

<summary>2016-01-29 18:39:26 - Spatial Bayesian hierarchical modeling of precipitation extremes over a large domain</summary>

- *Cameron Bracken, Balaji Rajagopalan, Linyin Cheng, Will Kleiber, Subhrendu Gangopadhyay*

- `1512.08560v2` - [abs](http://arxiv.org/abs/1512.08560v2) - [pdf](http://arxiv.org/pdf/1512.08560v2)

> We propose a Bayesian hierarchical model for spatial extremes on a large domain. In the data layer a Gaussian elliptical copula having generalized extreme value (GEV) marginals is applied. Spatial dependence in the GEV parameters are captured with a latent spatial regression with spatially varying coefficients. Using a composite likelihood approach, we are able to efficiently incorporate a large precipitation dataset, which includes stations with missing data. The model is demonstrated by application to fall precipitation extremes at approximately 2600 stations covering the western United States, -125E to -100E longitude and 30N to 50N latitude. The hierarchical model provides GEV parameters on a $1/8$th degree grid and consequently maps of return levels and associated uncertainty. The model results indicate that return levels vary coherently both spatially and across seasons, providing information about the space-time variations of risk of extreme precipitation in the western US, helpful for infrastructure planning.

</details>

<details>

<summary>2016-01-30 01:25:37 - A Scalable Blocked Gibbs Sampling Algorithm For Gaussian And Poisson Regression Models</summary>

- *Nicholas A. Johnson, Frank O. Kuehnel, Ali Nasiri Amini*

- `1602.00047v1` - [abs](http://arxiv.org/abs/1602.00047v1) - [pdf](http://arxiv.org/pdf/1602.00047v1)

> Markov Chain Monte Carlo (MCMC) methods are a popular technique in Bayesian statistical modeling. They have long been used to obtain samples from posterior distributions, but recent research has focused on the scalability of these techniques for large problems. We do not develop new sampling methods but instead describe a blocked Gibbs sampler which is sufficiently scalable to accomodate many interesting problems. The sampler we describe applies to a restricted subset of the Generalized Linear Mixed-effects Models (GLMM's); this subset includes Poisson and Gaussian regression models. The blocked Gibbs sampling steps jointly update a prior variance parameter along with all of the random effects underneath it. We also discuss extensions such as flexible prior distributions.

</details>

<details>

<summary>2016-01-31 06:07:54 - Bayesian stochastic volatility models for high-frequency data</summary>

- *Georgi Dinolov, Abel Rodriguez, Hongyun Wang*

- `1602.00202v1` - [abs](http://arxiv.org/abs/1602.00202v1) - [pdf](http://arxiv.org/pdf/1602.00202v1)

> We formulate a discrete-time Bayesian stochastic volatility model for high-frequency stock-market data that directly accounts for microstructure noise, and outline a Markov chain Monte Carlo algorithm for parameter estimation. The methods described in this paper are designed to be coherent across all sampling timescales, with the goal of estimating the latent log-volatility signal from data collected at arbitrarily short sampling periods. In keeping with this goal, we carefully develop a method for eliciting priors. The empirical results derived from both simulated and real data show that directly accounting for microstructure in a state-space formulation allows for well-calibrated estimates of the log-volatility process driving prices.

</details>

<details>

<summary>2016-01-31 06:39:07 - Binomial and Multinomial Proportions: Accurate Estimation and Reliable Assessment of Accuracy</summary>

- *Jonathan Malcolm Friedman*

- `1602.00207v1` - [abs](http://arxiv.org/abs/1602.00207v1) - [pdf](http://arxiv.org/pdf/1602.00207v1)

> Misestimates of $\sigma_{P_o}$, the \emph{uncertainty} in $P_o$ from a 2-state Bayes equation used for binary classification, apparently arose from $\hat{\sigma}_{p_i}$, the uncertainty in underlying pdfs estimated from experimental $b$-bin histograms. To address this, several Bayesian estimator pairs $(\hat{p}_i, \hat{\sigma}_{p_i})$ were compared for agreement between nominal confidence level ($\xi$) and calculated coverage values ($C$). Large $\xi$-to-$C$ inconsistency for large $b$ and $ p_i \gg \frac{1}{b}$ arises for all multinomial estimators since priors downweight low likelihood, high $p_i$ values. To improve $\xi$-to-$C$ matching, $(\xi-C)^2$ was minimized against $\alpha_0$ in a more general prior pdf ($\mathcal{B}[\alpha_0,(b-1)\alpha_0;x]$) to obtain $(\hat{p_i})_{\xi\leftrightarrow C}$. This improved matching for $b=2$, but for $b>2$, $\xi$-to-$C$ matching by $(\hat{p_i})_{\xi\leftrightarrow C}$ required an effective value "$b=2$" and renormalization, and this reduced $\hat{p}_i$-to-$p_i$ matching. Better $\hat{p}_i$-to-$p_i$ matching came from the original multinomial estimators, a new discrete-domain estimator $\hat{p}(n_i,N)$, or an earlier \emph{joint} estimator, $(\hat{p_i})_{\bowtie}$ that co-adjusted all estimates $p_i$ for James-Stein shrinkage to a mean vector. Best simultaneous $\xi$-to-$C$ and $\hat{p}_i$-to-$p_i$ matching came by \emph{de-noising} initial estimates of underlying pdfs. For $b=100$, $N<12800$, de-noised $\hat{p}$ needed $\approx 10\times$ fewer observations to achieve $\hat{p}_i$-to-$p_i$ matching equivalent to that found for $\hat{p}(n_i,N)$, $(\hat{p_i})_{\bowtie}$ or the original multinomial $\hat{p}_i$. De-noising each different type of initial estimate yielded similarly high accuracy in Monte-Carlo tests.

</details>

<details>

<summary>2016-01-31 10:02:14 - Image Denoising with Kernels based on Natural Image Relations</summary>

- *Valero Laparra, Juan GutiÃ©rrez, Gustavo Camps-Valls, JesÃºs Malo*

- `1602.00217v1` - [abs](http://arxiv.org/abs/1602.00217v1) - [pdf](http://arxiv.org/pdf/1602.00217v1)

> A successful class of image denoising methods is based on Bayesian approaches working in wavelet representations. However, analytical estimates can be obtained only for particular combinations of analytical models of signal and noise, thus precluding its straightforward extension to deal with other arbitrary noise sources. In this paper, we propose an alternative non-explicit way to take into account the relations among natural image wavelet coefficients for denoising: we use support vector regression (SVR) in the wavelet domain to enforce these relations in the estimated signal. Since relations among the coefficients are specific to the signal, the regularization property of SVR is exploited to remove the noise, which does not share this feature. The specific signal relations are encoded in an anisotropic kernel obtained from mutual information measures computed on a representative image database. Training considers minimizing the Kullback-Leibler divergence (KLD) between the estimated and actual probability functions of signal and noise in order to enforce similarity. Due to its non-parametric nature, the method can eventually cope with different noise sources without the need of an explicit re-formulation, as it is strictly necessary under parametric Bayesian formalisms. Results under several noise levels and noise sources show that: (1) the proposed method outperforms conventional wavelet methods that assume coefficient independence, (2) it is similar to state-of-the-art methods that do explicitly include these relations when the noise source is Gaussian, and (3) it gives better numerical and visual performance when more complex, realistic noise sources are considered. Therefore, the proposed machine learning approach can be seen as a more flexible (model-free) alternative to the explicit description of wavelet coefficient relations for image denoising.

</details>

<details>

<summary>2016-01-31 13:36:20 - Statistical methods for linguistic research: Foundational Ideas - Part II</summary>

- *Bruno Nicenboim, Shravan Vasishth*

- `1602.00245v1` - [abs](http://arxiv.org/abs/1602.00245v1) - [pdf](http://arxiv.org/pdf/1602.00245v1)

> We provide an introductory review of Bayesian data analytical methods, with a focus on applications for linguistics, psychology, psycholinguistics, and cognitive science. The empirically oriented researcher will benefit from making Bayesian methods part of their statistical toolkit due to the many advantages of this framework, among them easier interpretation of results relative to research hypotheses, and flexible model specification. We present an informal introduction to the foundational ideas behind Bayesian data analysis, using, as an example, a linear mixed models analysis of data from a typical psycholinguistics experiment. We discuss hypothesis testing using the Bayes factor, and model selection using cross-validation. We close with some examples illustrating the flexibility of model specification in the Bayesian framework. Suggestions for further reading are also provided.

</details>

<details>

<summary>2016-01-31 23:46:36 - Efficient moment calculations for variance components in large unbalanced crossed random effects models</summary>

- *Katelyn Gao, Art B. Owen*

- `1602.00346v1` - [abs](http://arxiv.org/abs/1602.00346v1) - [pdf](http://arxiv.org/pdf/1602.00346v1)

> Large crossed data sets, described by generalized linear mixed models, have become increasingly common and provide challenges for statistical analysis. At very large sizes it becomes desirable to have the computational costs of estimation, inference and prediction (both space and time) grow at most linearly with sample size. Both traditional maximum likelihood estimation and numerous Markov chain Monte Carlo Bayesian algorithms take superlinear time in order to obtain good parameter estimates. We propose moment based algorithms that, with at most linear cost, estimate variance components, measure the uncertainties of those estimates, and generate shrinkage based predictions for missing observations. When run on simulated normally distributed data, our algorithm performs competitively with maximum likelihood methods.

</details>


## 2016-02

<details>

<summary>2016-02-02 08:53:06 - Fast unsupervised Bayesian image segmentation with adaptive spatial regularisation</summary>

- *Marcelo Pereyra, Steve McLaughlin*

- `1502.01400v3` - [abs](http://arxiv.org/abs/1502.01400v3) - [pdf](http://arxiv.org/pdf/1502.01400v3)

> This paper presents a new Bayesian estimation technique for hidden Potts-Markov random fields with unknown regularisation parameters, with application to fast unsupervised K-class image segmentation. The technique is derived by first removing the regularisation parameter from the Bayesian model by marginalisation, followed by a small-variance-asymptotic (SVA) analysis in which the spatial regularisation and the integer-constrained terms of the Potts model are decoupled. The evaluation of this SVA Bayesian estimator is then relaxed into a problem that can be computed efficiently by iteratively solving a convex total-variation denoising problem and a least-squares clustering (K-means) problem, both of which can be solved straightforwardly, even in high-dimensions, and with parallel computing techniques. This leads to a fast fully unsupervised Bayesian image segmentation methodology in which the strength of the spatial regularisation is adapted automatically to the observed image during the inference procedure, and that can be easily applied in large 2D and 3D scenarios or in applications requiring low computing times. Experimental results on real images, as well as extensive comparisons with state-of-the-art algorithms, confirm that the proposed methodology offer extremely fast convergence and produces accurate segmentation results, with the important additional advantage of self-adjusting regularisation parameters.

</details>

<details>

<summary>2016-02-02 09:49:42 - Dynamic Model Averaging for Bayesian Quantile Regression</summary>

- *Mauro Bernardi, Roberto Casarin, Bertrand Maillet, Lea Petrella*

- `1602.00856v1` - [abs](http://arxiv.org/abs/1602.00856v1) - [pdf](http://arxiv.org/pdf/1602.00856v1)

> We propose a general dynamic model averaging (DMA) approach based on Markov-Chain Monte Carlo for the sequential combination and estimation of quantile regression models with time-varying parameters. The efficiency and the effectiveness of the proposed DMA approach and the MCMC algorithm are shown through simulation studies and applications to macro-economics and finance.

</details>

<details>

<summary>2016-02-03 10:19:56 - Frequentist tests for Bayesian models</summary>

- *L. B. Lucy*

- `1511.02363v3` - [abs](http://arxiv.org/abs/1511.02363v3) - [pdf](http://arxiv.org/pdf/1511.02363v3)

> Analogues of the frequentist chi-square and F tests are proposed for testing goodness-of-fit and consistency for Bayesian models. Simple examples exhibit these tests' detection of inconsistency between consecutive experiments with identical parameters, when the first experiment provides the prior for the second. In a related analysis, a quantitative measure is derived for judging the degree of tension between two different experiments with partially overlapping parameter vectors.

</details>

<details>

<summary>2016-02-03 11:28:31 - Frequentistic approximations to Bayesian prevision of exchangeable random elements</summary>

- *Donato Michele Cifarelli, Emanuele Dolera, Eugenio Regazzini*

- `1602.01269v1` - [abs](http://arxiv.org/abs/1602.01269v1) - [pdf](http://arxiv.org/pdf/1602.01269v1)

> Given a sequence \xi_1, \xi_2,... of X-valued, exchangeable random elements, let q(\xi^(n)) and p_m(\xi^(n)) stand for posterior and predictive distribution, respectively, given \xi^(n) = (\xi_1,..., \xi_n). We provide an upper bound for limsup b_n d_[[X]](q(\xi^(n)), \delta_\empiricn) and limsup b_n d_[X^m](p_m(\xi^(n)), \empiricn^m), where \empiricn is the empirical measure, b_n is a suitable sequence of positive numbers increasing to +\infty, d_[[X]] and d_[X^m] denote distinguished weak probability distances on [[X]] and [X^m], respectively, with the proviso that [S] denotes the space of all probability measures on S. A characteristic feature of our work is that the aforesaid bounds are established under the law of the \xi_n's, unlike the more common literature on Bayesian consistency, where they are studied with respect to product measures (p_0)^\infty, as p_0 varies among the admissible determinations of a random probability measure.

</details>

<details>

<summary>2016-02-04 09:49:21 - Bayesian Intent Prediction in Object Tracking Using Bridging Distributions</summary>

- *Bashar I. Ahmad, James K. Murphy, Patrick M. Langdon, Simon J. Godsill*

- `1508.06115v3` - [abs](http://arxiv.org/abs/1508.06115v3) - [pdf](http://arxiv.org/pdf/1508.06115v3)

> In several application areas, such as human computer interaction, surveillance and defence, determining the intent of a tracked object enables systems to aid the user/operator and facilitate effective, possibly automated, decision making. In this paper, we propose a probabilistic inference approach that permits the prediction, well in advance, of the intended destination of a tracked object and its future trajectory. Within the framework introduced here, the observed partial track of the object is modeled as being part of a Markov bridge terminating at its destination, since the target path, albeit random, must end at the intended endpoint. This captures the underlying long term dependencies in the trajectory, as dictated by the object intent. By determining the likelihood of the partial track being drawn from a particular constructed bridge, the probability of each of a number of possible destinations is evaluated. These bridges can also be employed to produce refined estimates of the latent system state (e.g. object position, velocity, etc.), predict its future values (up until reaching the designated endpoint) and estimate the time of arrival. This is shown to lead to a low complexity Kalman-filter-based implementation of the inference routine, where any linear Gaussian motion model, including the destination reverting ones, can be applied. Free hand pointing gestures data collected in an instrumented vehicle and synthetic trajectories of a vessel heading towards multiple possible harbours are utilised to demonstrate the effectiveness of the proposed approach.

</details>

<details>

<summary>2016-02-04 12:01:15 - Bayesian estimation of discrete Burr distribution with two parameters</summary>

- *Halaleh Kamari, Hossein Bevrani, Kaniav Kamary*

- `1501.05876v3` - [abs](http://arxiv.org/abs/1501.05876v3) - [pdf](http://arxiv.org/pdf/1501.05876v3)

> So far, various techniques have been implemented for generating discrete distributions based on continuous distributions. The characteristics and properties of this kind of probability distributions have been studied. Furthermore, the estimation of related parameters have been computed trough classical methods. However, a few studies addressed the parameter estimate issue of these distributions through Bayesian methods. This is essentially because of the complexity of the model whatever the number of parameter is and the fact that in general they contain a large number of parameters to be estimated. This paper deals with computing Bayes estimate of the parameters of discrete Burr distribution with two parameters. Since the resulting posterior distribution of the parameters is not standard, we apply Metropolis-Hastings algorithm to simulate from the posterior density.

</details>

<details>

<summary>2016-02-04 12:02:45 - Bayesian Nonparametric System Reliability using Sets of Priors</summary>

- *Gero Walter, Louis J. M. Aslett, Frank P. A. Coolen*

- `1602.01650v1` - [abs](http://arxiv.org/abs/1602.01650v1) - [pdf](http://arxiv.org/pdf/1602.01650v1)

> An imprecise Bayesian nonparametric approach to system reliability with multiple types of components is developed. This allows modelling partial or imperfect prior knowledge on component failure distributions in a flexible way through bounds on the functioning probability. Given component level test data these bounds are propagated to bounds on the posterior predictive distribution for the functioning probability of a new system containing components exchangeable with those used in testing. The method further enables identification of prior-data conflict at the system level based on component level test data. New results on first-order stochastic dominance for the Beta-Binomial distribution make the technique computationally tractable. Our methodological contributions can be immediately used in applications by reliability practitioners as we provide easy to use software tools.

</details>

<details>

<summary>2016-02-04 14:36:31 - Regularized brain reading with shrinkage and smoothing</summary>

- *Leila Wehbe, Aaditya Ramdas, Rebecca C. Steorts, Cosma Rohilla Shalizi*

- `1401.6595v3` - [abs](http://arxiv.org/abs/1401.6595v3) - [pdf](http://arxiv.org/pdf/1401.6595v3)

> Functional neuroimaging measures how the brain responds to complex stimuli. However, sample sizes are modest, noise is substantial, and stimuli are high dimensional. Hence, direct estimates are inherently imprecise and call for regularization. We compare a suite of approaches which regularize via shrinkage: ridge regression, the elastic net (a generalization of ridge regression and the lasso), and a hierarchical Bayesian model based on small area estimation (SAE). We contrast regularization with spatial smoothing and combinations of smoothing and shrinkage. All methods are tested on functional magnetic resonance imaging (fMRI) data from multiple subjects participating in two different experiments related to reading, for both predicting neural response to stimuli and decoding stimuli from responses. Interestingly, when the regularization parameters are chosen by cross-validation independently for every voxel, low/high regularization is chosen in voxels where the classification accuracy is high/low, indicating that the regularization intensity is a good tool for identification of relevant voxels for the cognitive task. Surprisingly, all the regularization methods work about equally well, suggesting that beating basic smoothing and shrinkage will take not only clever methods, but also careful modeling.

</details>

<details>

<summary>2016-02-04 15:13:28 - Graph_sampler: a simple tool for fully Bayesian analyses of DAG-models</summary>

- *Sagnik Datta, Ghislaine Gayraud, Eric Leclerc, Frederic Y. Bois*

- `1505.07228v2` - [abs](http://arxiv.org/abs/1505.07228v2) - [pdf](http://arxiv.org/pdf/1505.07228v2)

> Bayesian networks (BNs) are widely used graphical models usable to draw statistical inference about Directed acyclic graphs (DAGs). We presented here Graph_sampler a fast free C language software for structural inference on BNs. Graph_sampler uses a fully Bayesian approach in which the marginal likelihood of the data and prior information about the network structure are considered. This new software can handle both the continuous as well discrete data and based on the data type two different models are formulated. The software also provides a wide variety of structure priors which can be informative or uninformative. We proposed a new and much faster jumping kernel strategy in the Metropolis-Hastings algorithm. The source C code distributed is very compact, fast, uses low memory and disk storage. We performed out several analyses based on different simulated data sets and synthetic as well as real networks to discuss the performance of Graph_sampler.

</details>

<details>

<summary>2016-02-06 13:47:34 - A Tractable Fully Bayesian Method for the Stochastic Block Model</summary>

- *Kohei Hayashi, Takuya Konishi, Tatsuro Kawamoto*

- `1602.02256v1` - [abs](http://arxiv.org/abs/1602.02256v1) - [pdf](http://arxiv.org/pdf/1602.02256v1)

> The stochastic block model (SBM) is a generative model revealing macroscopic structures in graphs. Bayesian methods are used for (i) cluster assignment inference and (ii) model selection for the number of clusters. In this paper, we study the behavior of Bayesian inference in the SBM in the large sample limit. Combining variational approximation and Laplace's method, a consistent criterion of the fully marginalized log-likelihood is established. Based on that, we derive a tractable algorithm that solves tasks (i) and (ii) concurrently, obviating the need for an outer loop to check all model candidates. Our empirical and theoretical results demonstrate that our method is scalable in computation, accurate in approximation, and concise in model selection.

</details>

<details>

<summary>2016-02-06 19:16:06 - MPBART - Multinomial Probit Bayesian Additive Regression Trees</summary>

- *Bereket P. Kindo, Hao Wang, Edsel A. PeÃ±a*

- `1309.7821v2` - [abs](http://arxiv.org/abs/1309.7821v2) - [pdf](http://arxiv.org/pdf/1309.7821v2)

> This article proposes Multinomial Probit Bayesian Additive Regression Trees (MPBART) as a multinomial probit extension of BART - Bayesian Additive Regression Trees (Chipman et al (2010)). MPBART is flexible to allow inclusion of predictors that describe the observed units as well as the available choice alternatives. Through two simulation studies and four real data examples, we show that MPBART exhibits very good predictive performance in comparison to other discrete choice and multiclass classification methods. To implement MPBART, we have developed an R package mpbart available freely from CRAN repositories.

</details>

<details>

<summary>2016-02-07 01:28:00 - Empirical bayes formulation of the elastic net and mixed-norm models: application to the eeg inverse problem</summary>

- *Deirel Paz-Linares, Mayrim Vega-HernÃ¡ndez, Pedro A. Rojas-LÃ³pez, Pedro A. ValdÃ©s-Sosa, Eduardo MartÃ­nez-Montes*

- `1601.06749v2` - [abs](http://arxiv.org/abs/1601.06749v2) - [pdf](http://arxiv.org/pdf/1601.06749v2)

> The estimation of EEG generating sources constitutes an Inverse Problem (IP) in Neuroscience. This is an ill-posed problem, due to the non-uniqueness of the solution, and many kinds of prior information have been used to constrain it. A combination of smoothness (L2 norm-based) and sparseness (L1 norm-based) constraints is a flexible approach that have been pursued by important examples such as the Elastic Net (ENET) and mixed-norm (MXN) models. The former is used to find solutions with a small number of smooth non-zero patches, while the latter imposes sparseness and smoothness simultaneously along different dimensions of the spatio-temporal matrix solutions. Both models have been addressed within the penalized regression approach, where the regularization parameters are selected heuristically, leading usually to non-optimal solutions. The existing Bayesian formulation of ENET allows hyperparameter learning, but using computationally intensive Monte Carlo/Expectation Maximization methods. In this work we attempt to solve the EEG IP using a Bayesian framework for models based on mixtures of L1/L2 norms penalization functions (Laplace/Normal priors) such as ENET and MXN. We propose a Sparse Bayesian Learning algorithm based on combining the Empirical Bayes and the iterative coordinate descent procedures to estimate both the parameters and hyperparameters. Using simple but realistic simulations we found that our methods are able to recover complicated source setups more accurately and with a more robust variable selection than the ENET and LASSO solutions using classical algorithms. We also solve the EEG IP using data coming from a visual attention experiment, finding more interpretable neurophysiological patterns with our methods, as compared with other known methods such as LORETA, ENET and LASSO FUSION using the classical regularization approach.

</details>

<details>

<summary>2016-02-07 19:53:24 - Interpretable Selection and Visualization of Features and Interactions Using Bayesian Forests</summary>

- *Viktoriya Krakovna, Jiong Du, Jun S. Liu*

- `1506.02371v4` - [abs](http://arxiv.org/abs/1506.02371v4) - [pdf](http://arxiv.org/pdf/1506.02371v4)

> It is becoming increasingly important for machine learning methods to make predictions that are interpretable as well as accurate. In many practical applications, it is of interest which features and feature interactions are relevant to the prediction task. We present a novel method, Selective Bayesian Forest Classifier, that strikes a balance between predictive power and interpretability by simultaneously performing classification, feature selection, feature interaction detection and visualization. It builds parsimonious yet flexible models using tree-structured Bayesian networks, and samples an ensemble of such models using Markov chain Monte Carlo. We build in feature selection by dividing the trees into two groups according to their relevance to the outcome of interest. Our method performs competitively on classification and feature selection benchmarks in low and high dimensions, and includes a visualization tool that provides insight into relevant features and interactions.

</details>

<details>

<summary>2016-02-07 23:09:46 - A Bayesian spatial temporal mixtures approach to kinetic parametric images in dynamic Positron Emission Tomography</summary>

- *Wanchuang Zhu, Jinsong Ouyang, Yothin Rakvongthai, N. J. Guehl, D. W. Wooten, G. El Fakhri, M. D. Normandin, Yanan Fan*

- `1409.7454v3` - [abs](http://arxiv.org/abs/1409.7454v3) - [pdf](http://arxiv.org/pdf/1409.7454v3)

> We present a fully Bayesian statistical approach to the problem of compartmental modelling in the context of Positron Emission Tomography. We cluster homogeneous region of interest and perform kinetic parameter estimation simultaneously. A mixture modelling approach is adopted, incorporating both spatial and temporal information based on reconstructed dynamic PET image. Our modelling approach is flexible, and provides uncertainty estimates for the estimated kinetic parameters. Crucially, the proposed method allows us to determine the unknown number of clusters, which has a great impact on resulting estimated kinetic parameters. We demonstrate our method on simulated dynamic Myocardial PET data, and show that our method is superior to standard curve-fitting approach.

</details>

<details>

<summary>2016-02-08 05:23:26 - Overfitting hidden Markov models with an unknown number of states</summary>

- *ZoÃ© van Havre, Judith Rousseau, Nicole White, Kerrie Mengersen*

- `1602.02466v1` - [abs](http://arxiv.org/abs/1602.02466v1) - [pdf](http://arxiv.org/pdf/1602.02466v1)

> This paper presents new theory and methodology for the Bayesian estimation of overfitted hidden Markov models, with finite state space. The goal is then to achieve posterior emptying of extra states. A prior configuration is constructed which favours configurations where the hidden Markov chain remains ergodic although it empties out some of the states. Asymptotic posterior convergence rates are proven theoretically, and demonstrated with a large sample simulation. The problem of overfitted HMMs is then considered in the context of smaller sample sizes, and due to computational and mixing issues two alternative prior structures are studied, one commonly used in practice, and a mixture of the two priors. The Prior Parallel Tempering approach of van Havre (2015) is also extended to HMMs to allow MCMC estimation of the complex posterior space. A replicate simulation study and an in-depth exploration is performed to compare the three priors with hyperparameters chosen according to the asymptotic constraints alongside less informative alternatives.

</details>

<details>

<summary>2016-02-08 12:12:05 - Just Another Gibbs Additive Modeller: Interfacing JAGS and mgcv</summary>

- *Simon N Wood*

- `1602.02539v1` - [abs](http://arxiv.org/abs/1602.02539v1) - [pdf](http://arxiv.org/pdf/1602.02539v1)

> The BUGS language offers a very flexible way of specifying complex statistical models for the purposes of Gibbs sampling, while its JAGS variant offers very convenient R integration via the rjags package. However, including smoothers in JAGS models can involve some quite tedious coding, especially for multivariate or adaptive smoothers. Further, if an additive smooth structure is required then some care is needed, in order to centre smooths appropriately, and to find appropriate starting values. R package mgcv implements a wide range of smoothers, all in a manner appropriate for inclusion in JAGS code, and automates centring and other smooth setup tasks. The purpose of this note is to describe an interface between mgcv and JAGS, based around an R function, `jagam', which takes a generalized additive model (GAM) as specified in mgcv and automatically generates the JAGS model code and data required for inference about the model via Gibbs sampling. Although the auto-generated JAGS code can be run as is, the expectation is that the user would wish to modify it in order to add complex stochastic model components readily specified in JAGS. A simple interface is also provided for visualisation and further inference about the estimated smooth components using standard mgcv functionality. The methods described here will be un-necessarily inefficient if all that is required is fully Bayesian inference about a standard GAM, rather than the full flexibility of JAGS. In that case the BayesX package would be more efficient.

</details>

<details>

<summary>2016-02-08 15:20:47 - Hidden Gibbs random fields model selection using Block Likelihood Information Criterion</summary>

- *Julien Stoehr, Jean-Michel Marin, Pierre Pudlo*

- `1602.02606v1` - [abs](http://arxiv.org/abs/1602.02606v1) - [pdf](http://arxiv.org/pdf/1602.02606v1)

> Performing model selection between Gibbs random fields is a very challenging task. Indeed, due to the Markovian dependence structure, the normalizing constant of the fields cannot be computed using standard analytical or numerical methods. Furthermore, such unobserved fields cannot be integrated out and the likelihood evaluztion is a doubly intractable problem. This forms a central issue to pick the model that best fits an observed data. We introduce a new approximate version of the Bayesian Information Criterion. We partition the lattice into continuous rectangular blocks and we approximate the probability measure of the hidden Gibbs field by the product of some Gibbs distributions over the blocks. On that basis, we estimate the likelihood and derive the Block Likelihood Information Criterion (BLIC) that answers model choice questions such as the selection of the dependency structure or the number of latent states. We study the performances of BLIC for those questions. In addition, we present a comparison with ABC algorithms to point out that the novel criterion offers a better trade-off between time efficiency and reliable results.

</details>

<details>

<summary>2016-02-08 17:02:41 - The Longevity of Lava Dome Eruptions</summary>

- *Robert L. Wolpert, Sarah E. Ogburn, Eliza S. Calder*

- `1512.08495v2` - [abs](http://arxiv.org/abs/1512.08495v2) - [pdf](http://arxiv.org/pdf/1512.08495v2)

> Understanding the duration of past, on-going and future volcanic eruptions is an important scientific goal and a key societal need. We present a new methodology for forecasting the duration of on-going and future lava dome eruptions based on a database (DomeHaz) recently compiled by the authors. The database includes duration and composition for 177 such eruptions, with "eruption" defined as the period encompassing individual episodes of dome growth along with associated quiescent periods during which extrusion pauses but unrest continues. In a key finding we show that probability distributions for dome eruption durations are both heavy-tailed and composition-dependent. We construct Objective Bayes statistical models featuring heavy-tailed Generalized Pareto distributions with composition-specific parameters to make forecasts about the durations of new and on-going eruptions that depend on both eruption duration-to-date and composition. Our Bayesian predictive distributions reflect both uncertainty about model parameter values (epistemic uncertainty) and the natural variability of the geologic processes (aleatoric uncertainty). The results are illustrated by presenting likely trajectories for fourteen dome-building eruptions on-going in 2015. Full representation of the uncertainty is presented for two key eruptions, Soufri{\'{e}}re Hills Volcano in Montserrat (10--139 years, median 35yr) and Sinabung, Indonesia (1--17 years, median 4yr). Uncertainties are high, but, importantly, quantifiable. This work provides for the first time a quantitative and transferable method and rationale on which to base long-term planning decisions for lava dome forming volcanoes, with wide potential use and transferability to forecasts of other types of eruptions and other adverse events across the geohazard spectrum.

</details>

<details>

<summary>2016-02-09 01:56:57 - Machine Learning Model of the Swift/BAT Trigger Algorithm for Long GRB Population Studies</summary>

- *Philip B Graff, Amy Y Lien, John G Baker, Takanori Sakamoto*

- `1509.01228v2` - [abs](http://arxiv.org/abs/1509.01228v2) - [pdf](http://arxiv.org/pdf/1509.01228v2)

> To draw inferences about gamma-ray burst (GRB) source populations based on Swift observations, it is essential to understand the detection efficiency of the Swift burst alert telescope (BAT). This study considers the problem of modeling the Swift/BAT triggering algorithm for long GRBs, a computationally expensive procedure, and models it using machine learning algorithms. A large sample of simulated GRBs from Lien 2014 is used to train various models: random forests, boosted decision trees (with AdaBoost), support vector machines, and artificial neural networks. The best models have accuracies of $\gtrsim97\%$ ($\lesssim 3\%$ error), which is a significant improvement on a cut in GRB flux which has an accuracy of $89.6\%$ ($10.4\%$ error). These models are then used to measure the detection efficiency of Swift as a function of redshift $z$, which is used to perform Bayesian parameter estimation on the GRB rate distribution. We find a local GRB rate density of $n_0 \sim 0.48^{+0.41}_{-0.23} \ {\rm Gpc}^{-3} {\rm yr}^{-1}$ with power-law indices of $n_1 \sim 1.7^{+0.6}_{-0.5}$ and $n_2 \sim -5.9^{+5.7}_{-0.1}$ for GRBs above and below a break point of $z_1 \sim 6.8^{+2.8}_{-3.2}$. This methodology is able to improve upon earlier studies by more accurately modeling Swift detection and using this for fully Bayesian model fitting. The code used in this is analysis is publicly available online (https://github.com/PBGraff/SwiftGRB_PEanalysis).

</details>

<details>

<summary>2016-02-09 03:43:21 - Toward Optimal Feature Selection in Naive Bayes for Text Categorization</summary>

- *Bo Tang, Steven Kay, Haibo He*

- `1602.02850v1` - [abs](http://arxiv.org/abs/1602.02850v1) - [pdf](http://arxiv.org/pdf/1602.02850v1)

> Automated feature selection is important for text categorization to reduce the feature size and to speed up the learning process of classifiers. In this paper, we present a novel and efficient feature selection framework based on the Information Theory, which aims to rank the features with their discriminative capacity for classification. We first revisit two information measures: Kullback-Leibler divergence and Jeffreys divergence for binary hypothesis testing, and analyze their asymptotic properties relating to type I and type II errors of a Bayesian classifier. We then introduce a new divergence measure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure multi-distribution divergence for multi-class classification. Based on the JMH-divergence, we develop two efficient feature selection methods, termed maximum discrimination ($MD$) and $MD-\chi^2$ methods, for text categorization. The promising results of extensive experiments demonstrate the effectiveness of the proposed approaches.

</details>

<details>

<summary>2016-02-09 08:43:16 - A stochastic space-time model for intermittent precipitation occurrences</summary>

- *Ying Sun, Michael L. Stein*

- `1602.02902v1` - [abs](http://arxiv.org/abs/1602.02902v1) - [pdf](http://arxiv.org/pdf/1602.02902v1)

> Modeling a precipitation field is challenging due to its intermittent and highly scale-dependent nature. Motivated by the features of high-frequency precipitation data from a network of rain gauges, we propose a threshold space-time $t$ random field (tRF) model for 15-minute precipitation occurrences. This model is constructed through a space-time Gaussian random field (GRF) with random scaling varying along time or space and time. It can be viewed as a generalization of the purely spatial tRF, and has a hierarchical representation that allows for Bayesian interpretation. Developing appropriate tools for evaluating precipitation models is a crucial part of the model-building process, and we focus on evaluating whether models can produce the observed conditional dry and rain probabilities given that some set of neighboring sites all have rain or all have no rain. These conditional probabilities show that the proposed space-time model has noticeable improvements in some characteristics of joint rainfall occurrences for the data we have considered.

</details>

<details>

<summary>2016-02-09 08:58:47 - Infinite-dimensional statistical manifolds based on a balanced chart</summary>

- *Nigel J. Newton*

- `1308.3602v2` - [abs](http://arxiv.org/abs/1308.3602v2) - [pdf](http://arxiv.org/pdf/1308.3602v2)

> We develop a family of infinite-dimensional Banach manifolds of measures on an abstract measurable space, employing charts that are "balanced" between the density and log-density functions. The manifolds, $(\tilde{M}_{\lambda},\lambda\in [2,\infty))$, retain many of the features of finite-dimensional information geometry; in particular, the $\alpha$-divergences are of class $C^{\lceil\lambda\rceil-1}$, enabling the definition of the Fisher metric and $\alpha$-derivatives of particular classes of vector fields. Manifolds of probability measures, $(M_{\lambda},\lambda\in [2,\infty))$, based on centred versions of the charts are shown to be $C^{\lceil\lambda \rceil-1}$-embedded submanifolds of the $\tilde{M}_{\lambda}$. The Fisher metric is a pseudo-Riemannian metric on $\tilde{M}_{\lambda}$. However, when restricted to finite-dimensional embedded submanifolds it becomes a Riemannian metric, allowing the full development of the geometry of $\alpha$-covariant derivatives. $\tilde{M}_{\lambda}$ and $M_{\lambda}$ provide natural settings for the study and comparison of approximations to posterior distributions in problems of Bayesian estimation.

</details>

<details>

<summary>2016-02-09 16:02:00 - Bayesian nonparametric image segmentation using a generalized Swendsen-Wang algorithm</summary>

- *Richard Yi Da Xu, Francois Caron, Arnaud Doucet*

- `1602.03048v1` - [abs](http://arxiv.org/abs/1602.03048v1) - [pdf](http://arxiv.org/pdf/1602.03048v1)

> Unsupervised image segmentation aims at clustering the set of pixels of an image into spatially homogeneous regions. We introduce here a class of Bayesian nonparametric models to address this problem. These models are based on a combination of a Potts-like spatial smoothness component and a prior on partitions which is used to control both the number and size of clusters. This class of models is flexible enough to include the standard Potts model and the more recent Potts-Dirichlet Process model \cite{Orbanz2008}. More importantly, any prior on partitions can be introduced to control the global clustering structure so that it is possible to penalize small or large clusters if necessary. Bayesian computation is carried out using an original generalized Swendsen-Wang algorithm. Experiments demonstrate that our method is competitive in terms of RAND\ index compared to popular image segmentation methods, such as mean-shift, and recent alternative Bayesian nonparametric models.

</details>

<details>

<summary>2016-02-11 09:30:04 - Bayesian Filtering of Smooth Signals: Application to Altimetry</summary>

- *Abderrahim Halimi, Gerald S. Buller, Steve McLaughlin, Paul Honeine*

- `1602.03649v1` - [abs](http://arxiv.org/abs/1602.03649v1) - [pdf](http://arxiv.org/pdf/1602.03649v1)

> This paper presents a novel Bayesian strategy for the estimation of smooth signals corrupted by Gaussian noise. The method assumes a smooth evolution of a succession of continuous signals that can have a numerical or an analytical expression with respect to some parameters. The Bayesian model proposed takes into account the Gaussian properties of the noise and the smooth evolution of the successive signals. In addition, a gamma Markov random field prior is assigned to the signal energies and to the noise variances to account for their known properties. The resulting posterior distribution is maximized using a fast coordinate descent algorithm whose parameters are updated by analytical expressions. The proposed algorithm is tested on satellite altimetric data demonstrating good denoising results on both synthetic and real signals. The proposed algorithm is also shown to improve the quality of the altimetric parameters when combined with a parameter estimation strategy.

</details>

<details>

<summary>2016-02-11 10:09:08 - A randomized maximum a posterior method for posterior sampling of high dimensional nonlinear Bayesian inverse problems</summary>

- *Kainan Wang, Tan Bui-Thanh, Omar Ghattas*

- `1602.03658v1` - [abs](http://arxiv.org/abs/1602.03658v1) - [pdf](http://arxiv.org/pdf/1602.03658v1)

> We present a randomized maximum a posteriori (rMAP) method for generating approximate samples of posteriors in high dimensional Bayesian inverse problems governed by large-scale forward problems. We derive the rMAP approach by: 1) casting the problem of computing the MAP point as a stochastic optimization problem; 2) interchanging optimization and expectation; and 3) approximating the expectation with a Monte Carlo method. For a specific randomized data and prior mean, rMAP reduces to the maximum likelihood approach (RML). It can also be viewed as an iterative stochastic Newton method. An analysis of the convergence of the rMAP samples is carried out for both linear and nonlinear inverse problems. Each rMAP sample requires solution of a PDE-constrained optimization problem; to solve these problems, we employ a state-of-the-art trust region inexact Newton conjugate gradient method with sensitivity-based warm starts. An approximate Metropolization approach is presented to reduce the bias in rMAP samples. Various numerical methods will be presented to demonstrate the potential of the rMAP approach in posterior sampling of nonlinear Bayesian inverse problems in high dimensions.

</details>

<details>

<summary>2016-02-11 15:46:47 - Stochastic partial differential equation based modelling of large space-time data sets</summary>

- *Fabio Sigrist, Hans R. KÃ¼nsch, Werner A. Stahel*

- `1204.6118v6` - [abs](http://arxiv.org/abs/1204.6118v6) - [pdf](http://arxiv.org/pdf/1204.6118v6)

> Increasingly larger data sets of processes in space and time ask for statistical models and methods that can cope with such data. We show that the solution of a stochastic advection-diffusion partial differential equation provides a flexible model class for spatio-temporal processes which is computationally feasible also for large data sets. The Gaussian process defined through the stochastic partial differential equation has in general a nonseparable covariance structure. Furthermore, its parameters can be physically interpreted as explicitly modeling phenomena such as transport and diffusion that occur in many natural processes in diverse fields ranging from environmental sciences to ecology. In order to obtain computationally efficient statistical algorithms we use spectral methods to solve the stochastic partial differential equation. This has the advantage that approximation errors do not accumulate over time, and that in the spectral space the computational cost grows linearly with the dimension, the total computational costs of Bayesian or frequentist inference being dominated by the fast Fourier transform. The proposed model is applied to postprocessing of precipitation forecasts from a numerical weather prediction model for northern Switzerland. In contrast to the raw forecasts from the numerical model, the postprocessed forecasts are calibrated and quantify prediction uncertainty. Moreover, they outperform the raw forecasts, in the sense that they have a lower mean absolute error.

</details>

<details>

<summary>2016-02-11 19:47:24 - A Statistical Framework for Single Subject Design with an Application in Post-stroke Rehabilitation</summary>

- *Ying Lu, Marc Scott, Preeti Raghavan*

- `1602.03855v1` - [abs](http://arxiv.org/abs/1602.03855v1) - [pdf](http://arxiv.org/pdf/1602.03855v1)

> This paper proposes a practical yet novel solution to a longstanding statistical testing problem regarding single subject design. In particular, we aim to resolve an important clinical question: does a new patient behave the same as one from a healthy population? This question cannot be answered using the traditional single subject design when only test subject information is used, nor can it be satisfactorily resolved by comparing a single-subject's data with the mean value of a healthy population without proper assessment of the impact of between and within subject variability. Here, we use Bayesian posterior predictive draws based on a training set of healthy subjects to generate a template null distribution of the statistic of interest to test whether the test subject belongs to the healthy population. This method also provides an estimate of the error rate associated with the decision and provides a confidence interval for the point estimate of interest. Taken together, this information will enable clinicians to conduct evidence-based clinical decision making by directly comparing the observed measures with a precalculated null distribution for such measures. Simulation studies show that the proposed test performs satisfactorily under controlled conditions.

</details>

<details>

<summary>2016-02-12 01:14:57 - A regional compound Poisson process for hurricane and tropical storm damage</summary>

- *Simon Mak, Derek Bingham, Yi Lu*

- `1602.03940v1` - [abs](http://arxiv.org/abs/1602.03940v1) - [pdf](http://arxiv.org/pdf/1602.03940v1)

> In light of intense hurricane activity along the U.S. Atlantic coast, attention has turned to understanding both the economic impact and behaviour of these storms. The compound Poisson-lognormal process has been proposed as a model for aggregate storm damage, but does not shed light on regional analysis since storm path data are not used. In this paper, we propose a fully Bayesian regional prediction model which uses conditional autoregressive (CAR) models to account for both storm paths and spatial patterns for storm damage. When fitted to historical data, the analysis from our model both confirms previous findings and reveals new insights on regional storm tendencies. Posterior predictive samples can also be used for pricing regional insurance premiums, which we illustrate using three different risk measures.

</details>

<details>

<summary>2016-02-12 03:08:12 - The Automatic Statistician: A Relational Perspective</summary>

- *Yunseong Hwang, Anh Tong, Jaesik Choi*

- `1511.08343v2` - [abs](http://arxiv.org/abs/1511.08343v2) - [pdf](http://arxiv.org/pdf/1511.08343v2)

> Gaussian Processes (GPs) provide a general and analytically tractable way of modeling complex time-varying, nonparametric functions. The Automatic Bayesian Covariance Discovery (ABCD) system constructs natural-language description of time-series data by treating unknown time-series data nonparametrically using GP with a composite covariance kernel function. Unfortunately, learning a composite covariance kernel with a single time-series data set often results in less informative kernel that may not give qualitative, distinctive descriptions of data. We address this challenge by proposing two relational kernel learning methods which can model multiple time-series data sets by finding common, shared causes of changes. We show that the relational kernel learning methods find more accurate models for regression problems on several real-world data sets; US stock data, US house price index data and currency exchange rate data.

</details>

<details>

<summary>2016-02-12 07:34:33 - Opportunistic Detection Rules: Finite and Asymptotic Analysis</summary>

- *Wenyi Zhang, George V. Moustakides, H. Vincent Poor*

- `1602.03969v1` - [abs](http://arxiv.org/abs/1602.03969v1) - [pdf](http://arxiv.org/pdf/1602.03969v1)

> Opportunistic detection rules (ODRs) are variants of fixed-sample-size detection rules in which the statistician is allowed to make an early decision on the alternative hypothesis opportunistically based on the sequentially observed samples. From a sequential decision perspective, ODRs are also mixtures of one-sided and truncated sequential detection rules. Several results regarding ODRs are established in this paper. In the finite regime, the maximum sample size is modeled either as a fixed finite number, or a geometric random variable with a fixed finite mean. For both cases, the corresponding Bayesian formulations are investigated. The former case is a slight variation of the well-known finite-length sequential hypothesis testing procedure in the literature, whereas the latter case is new, for which the Bayesian optimal ODR is shown to be a sequence of likelihood ratio threshold tests with two different thresholds: a running threshold, which is determined by solving a stationary state equation, is used when future samples are still available, and a terminal threshold (simply the ratio between the priors scaled by costs) is used when the statistician reaches the final sample and thus has to make a decision immediately. In the asymptotic regime, the tradeoff among the exponents of the (false alarm and miss) error probabilities and the normalized expected stopping time under the alternative hypothesis is completely characterized and proved to be tight, via an information-theoretic argument. Within the tradeoff region, one noteworthy fact is that the performance of the Stein-Chernoff Lemma is attainable by ODRs.

</details>

<details>

<summary>2016-02-12 10:27:06 - Online Low-Rank Subspace Learning from Incomplete Data: A Bayesian View</summary>

- *Paris V. Giampouras, Athanasios A. Rontogiannis, Konstantinos E. Themelis, Konstantinos D. Koutroumbas*

- `1602.03670v2` - [abs](http://arxiv.org/abs/1602.03670v2) - [pdf](http://arxiv.org/pdf/1602.03670v2)

> Extracting the underlying low-dimensional space where high-dimensional signals often reside has long been at the center of numerous algorithms in the signal processing and machine learning literature during the past few decades. At the same time, working with incomplete (partly observed) large scale datasets has recently been commonplace for diverse reasons. This so called {\it big data era} we are currently living calls for devising online subspace learning algorithms that can suitably handle incomplete data. Their envisaged objective is to {\it recursively} estimate the unknown subspace by processing streaming data sequentially, thus reducing computational complexity, while obviating the need for storing the whole dataset in memory. In this paper, an online variational Bayes subspace learning algorithm from partial observations is presented. To account for the unawareness of the true rank of the subspace, commonly met in practice, low-rankness is explicitly imposed on the sought subspace data matrix by exploiting sparse Bayesian learning principles. Moreover, sparsity, {\it simultaneously} to low-rankness, is favored on the subspace matrix by the sophisticated hierarchical Bayesian scheme that is adopted. In doing so, the proposed algorithm becomes adept in dealing with applications whereby the underlying subspace may be also sparse, as, e.g., in sparse dictionary learning problems. As shown, the new subspace tracking scheme outperforms its state-of-the-art counterparts in terms of estimation accuracy, in a variety of experiments conducted on simulated and real data.

</details>

<details>

<summary>2016-02-12 10:30:27 - Bayesian smoothing of dipoles in Magneto-/Electro-encephalography</summary>

- *Valentina Vivaldi, Alberto Sorrentino*

- `1602.04003v1` - [abs](http://arxiv.org/abs/1602.04003v1) - [pdf](http://arxiv.org/pdf/1602.04003v1)

> We describe a novel method for dynamic estimation of multi-dipole states from Magneto/Electro-encephalography (M/EEG) time series. The new approach builds on the recent development of particle filters for M/EEG; these algorithms approximate, with samples and weights, the posterior distribution of the neural sources at time t given the data up to time t. However, for off-line inference purposes it is preferable to work with the smoothing distribution, i.e. the distribution for the neural sources at time t conditioned on the whole time series. In this study, we use a Monte Carlo algorithm to approximate the smoothing distribution for a time-varying set of current dipoles. We show, using numerical simulations, that the estimates provided by the smoothing distribution are more accurate than those provided by the filtering distribution, particularly at the appearance of the source. We validate the proposed algorithm using an experimental dataset recorded from an epileptic patient. Improved localization of the source onset can be particularly relevant in source modeling of epileptic patients, where the source onset brings information on the epileptogenic zone.

</details>

<details>

<summary>2016-02-12 17:32:39 - Deep Gaussian Processes for Regression using Approximate Expectation Propagation</summary>

- *Thang D. Bui, Daniel HernÃ¡ndez-Lobato, Yingzhen Li, JosÃ© Miguel HernÃ¡ndez-Lobato, Richard E. Turner*

- `1602.04133v1` - [abs](http://arxiv.org/abs/1602.04133v1) - [pdf](http://arxiv.org/pdf/1602.04133v1)

> Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic models and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. This paper develops a new approximate Bayesian learning scheme that enables DGPs to be applied to a range of medium to large scale regression problems for the first time. The new method uses an approximate Expectation Propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning. We evaluate the new method for non-linear regression on eleven real-world datasets, showing that it always outperforms GP regression and is almost always better than state-of-the-art deterministic and sampling-based approximate inference methods for Bayesian neural networks. As a by-product, this work provides a comprehensive analysis of six approximate Bayesian methods for training neural networks.

</details>

<details>

<summary>2016-02-12 23:04:13 - A Model-Based Approach for Analog Spatio-Temporal Dynamic Forecasting</summary>

- *Patrick L. McDermott, Christopher K. Wikle*

- `1506.06169v2` - [abs](http://arxiv.org/abs/1506.06169v2) - [pdf](http://arxiv.org/pdf/1506.06169v2)

> Analog forecasting has been applied in a variety of fields for predicting future states of complex nonlinear systems that require flexible forecasting methods. Past analog methods have almost exclu- sively been used in an empirical framework without the structure of a model-based approach. We propose a Bayesian model framework for analog forecasting, building upon previous analog methods but accounting for parameter uncertainty. Thus, unlike traditional analog forecasting methods, the use of Bayesian modeling allows one to rigorously quantify uncertainty to obtain realistic posterior predictive distributions. The model is applied to the long-lead time forecasting of mid-May averaged soil moisture anomalies in Iowa over a high-resolution grid of spatial locations. Sea Surface Tem- perature (SST) is used to find past time periods with similar trajectories to the current pre-forecast period. The analog model is developed on projection coefficients from a basis expansion of the soil moisture and SST fields. Separate models are constructed for locations falling in each Iowa Crop Reporting District (CRD) and the forecasting ability of the proposed model is compared against a variety of alternative methods and metrics.

</details>

<details>

<summary>2016-02-13 03:21:30 - Cascaded High Dimensional Histograms: A Generative Approach to Density Estimation</summary>

- *Siong Thye Goh, Cynthia Rudin*

- `1510.06779v3` - [abs](http://arxiv.org/abs/1510.06779v3) - [pdf](http://arxiv.org/pdf/1510.06779v3)

> We present tree- and list- structured density estimation methods for high dimensional binary/categorical data. Our density estimation models are high dimensional analogies to variable bin width histograms. In each leaf of the tree (or list), the density is constant, similar to the flat density within the bin of a histogram. Histograms, however, cannot easily be visualized in higher dimensions, whereas our models can. The accuracy of histograms fades as dimensions increase, whereas our models have priors that help with generalization. Our models are sparse, unlike high-dimensional histograms. We present three generative models, where the first one allows the user to specify the number of desired leaves in the tree within a Bayesian prior. The second model allows the user to specify the desired number of branches within the prior. The third model returns lists (rather than trees) and allows the user to specify the desired number of rules and the length of rules within the prior. Our results indicate that the new approaches yield a better balance between sparsity and accuracy of density estimates than other methods for this task.

</details>

<details>

<summary>2016-02-14 21:00:42 - Bayesian model choice via mixture distributions with application to epidemics and population process models</summary>

- *Philip D. O'Neill, Theodore Kypraios*

- `1411.7888v2` - [abs](http://arxiv.org/abs/1411.7888v2) - [pdf](http://arxiv.org/pdf/1411.7888v2)

> We describe a new method for evaluating Bayes factors. The key idea is to introduce a hypermodel in which the competing models are components of a mixture distribution. Inference for the mixing probabilities then yields estimates of the Bayes factors. Our motivation is the setting where the observed data are a partially observed realisation of a stochastic population process, although the methods have far wider applicability. The methods allow for missing data and for parameters to be shared between models. Illustrative examples including epidemics, population processes and regression models are given, showing that the methods are competitive compared to existing approaches.

</details>

<details>

<summary>2016-02-15 20:55:57 - DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression</summary>

- *Jovana Mitrovic, Dino Sejdinovic, Yee Whye Teh*

- `1602.04805v1` - [abs](http://arxiv.org/abs/1602.04805v1) - [pdf](http://arxiv.org/pdf/1602.04805v1)

> Performing exact posterior inference in complex generative models is often difficult or impossible due to an expensive to evaluate or intractable likelihood function. Approximate Bayesian computation (ABC) is an inference framework that constructs an approximation to the true likelihood based on the similarity between the observed and simulated data as measured by a predefined set of summary statistics. Although the choice of appropriate problem-specific summary statistics crucially influences the quality of the likelihood approximation and hence also the quality of the posterior sample in ABC, there are only few principled general-purpose approaches to the selection or construction of such summary statistics. In this paper, we develop a novel framework for this task using kernel-based distribution regression. We model the functional relationship between data distributions and the optimal choice (with respect to a loss function) of summary statistics using kernel-based distribution regression. We show that our approach can be implemented in a computationally and statistically efficient way using the random Fourier features framework for large-scale kernel learning. In addition to that, our framework shows superior performance when compared to related methods on toy and real-world problems.

</details>

<details>

<summary>2016-02-16 05:20:14 - Bayesian generalized fused lasso modeling via NEG distribution</summary>

- *Kaito Shimamura, Masao Ueki, Shuichi Kawano, Sadanori Konishi*

- `1602.04910v1` - [abs](http://arxiv.org/abs/1602.04910v1) - [pdf](http://arxiv.org/pdf/1602.04910v1)

> The fused lasso penalizes a loss function by the $L_1$ norm for both the regression coefficients and their successive differences to encourage sparsity of both. In this paper, we propose a Bayesian generalized fused lasso modeling based on a normal-exponential-gamma (NEG) prior distribution. The NEG prior is assumed into the difference of successive regression coefficients. The proposed method enables us to construct a more versatile sparse model than the ordinary fused lasso by using a flexible regularization term. We also propose a sparse fused algorithm to produce exact sparse solutions. Simulation studies and real data analyses show that the proposed method has superior performance to the ordinary fused lasso.

</details>

<details>

<summary>2016-02-16 14:10:57 - An introduction to sampling via measure transport</summary>

- *Youssef Marzouk, Tarek Moselhy, Matthew Parno, Alessio Spantini*

- `1602.05023v1` - [abs](http://arxiv.org/abs/1602.05023v1) - [pdf](http://arxiv.org/pdf/1602.05023v1)

> We present the fundamentals of a measure transport approach to sampling. The idea is to construct a deterministic coupling---i.e., a transport map---between a complex "target" probability measure of interest and a simpler reference measure. Given a transport map, one can generate arbitrarily many independent and unweighted samples from the target simply by pushing forward reference samples through the map. We consider two different and complementary scenarios: first, when only evaluations of the unnormalized target density are available, and second, when the target distribution is known only through a finite collection of samples. We show that in both settings the desired transports can be characterized as the solutions of variational problems. We then address practical issues associated with the optimization--based construction of transports: choosing finite-dimensional parameterizations of the map, enforcing monotonicity, quantifying the error of approximate transports, and refining approximate transports by enriching the corresponding approximation spaces. Approximate transports can also be used to "Gaussianize" complex distributions and thus precondition conventional asymptotically exact sampling schemes. We place the measure transport approach in broader context, describing connections with other optimization--based samplers, with inference and density estimation schemes using optimal transport, and with alternative transformation--based approaches to simulation. We also sketch current work aimed at the construction of transport maps in high dimensions, exploiting essential features of the target distribution (e.g., conditional independence, low-rank structure). The approaches and algorithms presented here have direct applications to Bayesian computation and to broader problems of stochastic simulation.

</details>

<details>

<summary>2016-02-16 20:14:40 - A Bayes interpretation of stacking for M-complete and M-open settings</summary>

- *Tri Le, Bertrand Clarke*

- `1602.05162v1` - [abs](http://arxiv.org/abs/1602.05162v1) - [pdf](http://arxiv.org/pdf/1602.05162v1)

> In M-open problems where no true model can be conceptualized, it is common to back off from modeling and merely seek good prediction. Even in M-complete problems, taking a predictive approach can be very useful. Stacking is a model averaging procedure that gives a composite predictor by combining individual predictors from a list of models using weights that optimize a cross-validation criterion. We show that the stacking weights also asymptotically minimize a posterior expected loss. Hence we formally provide a Bayesian justification for cross-validation. Often the weights are constrained to be positive and sum to one. For greater generality, we omit the positivity constraint and relax the `sum to one' constraint.   A key question is `What predictors should be in the average?' We first verify that the stacking error depends only on the span of the models. Then we propose using bootstrap samples from the data to generate empirical basis elements that can be used to form models. We use this in two computed examples to give stacking predictors that are (i) data driven, (ii) optimal with respect to the number of component predictors, and (iii) optimal with respect to the weight each predictor gets.

</details>

<details>

<summary>2016-02-16 22:07:01 - The Use of a Single Pseudo-Sample in Approximate Bayesian Computation</summary>

- *Luke Bornn, Natesh Pillai, Aaron Smith, Dawn Woodard*

- `1404.6298v5` - [abs](http://arxiv.org/abs/1404.6298v5) - [pdf](http://arxiv.org/pdf/1404.6298v5)

> We analyze the computational efficiency of approximate Bayesian computation (ABC), which approximates a likelihood function by drawing pseudo-samples from the associated model. For the rejection sampling version of ABC, it is known that multiple pseudo-samples cannot substantially increase (and can substantially decrease) the efficiency of the algorithm as compared to employing a high-variance estimate based on a single pseudo-sample. We show that this conclusion also holds for a Markov chain Monte Carlo version of ABC, implying that it is unnecessary to tune the number of pseudo-samples used in ABC-MCMC. This conclusion is in contrast to particle MCMC methods, for which increasing the number of particles can provide large gains in computational efficiency.

</details>

<details>

<summary>2016-02-17 17:16:45 - Bayesian identification of bacterial strains from sequencing data</summary>

- *Aravind Sankar, Brandon Malone, Sion Bayliss, Ben Pascoe, Guillaume MÃ©ric, Matthew D. Hitchings, Samuel K. Sheppard, Edward J. Feil, Jukka Corander, Antti Honkela*

- `1511.06546v2` - [abs](http://arxiv.org/abs/1511.06546v2) - [pdf](http://arxiv.org/pdf/1511.06546v2)

> Rapidly assaying the diversity of a bacterial species present in a sample obtained from a hospital patient or an evironmental source has become possible after recent technological advances in DNA sequencing. For several applications it is important to accurately identify the presence and estimate relative abundances of the target organisms from short sequence reads obtained from a sample. This task is particularly challenging when the set of interest includes very closely related organisms, such as different strains of pathogenic bacteria, which can vary considerably in terms of virulence, resistance and spread. Using advanced Bayesian statistical modelling and computation techniques we introduce a novel pipeline for bacterial identification that is shown to outperform the currently leading pipeline for this purpose. Our approach enables fast and accurate sequence-based identification of bacterial strains while using only modest computational resources. Hence it provides a useful tool for a wide spectrum of applications, including rapid clinical diagnostics to distinguish among closely related strains causing nosocomial infections. The software implementation is available at https://github.com/PROBIC/BIB

</details>

<details>

<summary>2016-02-17 19:51:34 - Continuous Monitoring of A/B Tests without Pain: Optional Stopping in Bayesian Testing</summary>

- *Alex Deng, Jiannan Lu, Shouyuan Chen*

- `1602.05549v1` - [abs](http://arxiv.org/abs/1602.05549v1) - [pdf](http://arxiv.org/pdf/1602.05549v1)

> A/B testing is one of the most successful applications of statistical theory in modern Internet age. One problem of Null Hypothesis Statistical Testing (NHST), the backbone of A/B testing methodology, is that experimenters are not allowed to continuously monitor the result and make decision in real time. Many people see this restriction as a setback against the trend in the technology toward real time data analytics. Recently, Bayesian Hypothesis Testing, which intuitively is more suitable for real time decision making, attracted growing interest as an alternative to NHST. While corrections of NHST for the continuous monitoring setting are well established in the existing literature and known in A/B testing community, the debate over the issue of whether continuous monitoring is a proper practice in Bayesian testing exists among both academic researchers and general practitioners. In this paper, we formally prove the validity of Bayesian testing with continuous monitoring when proper stopping rules are used, and illustrate the theoretical results with concrete simulation illustrations. We point out common bad practices where stopping rules are not proper and also compare our methodology to NHST corrections. General guidelines for researchers and practitioners are also provided.

</details>

<details>

<summary>2016-02-17 22:31:44 - Shared kernel Bayesian screening</summary>

- *Eric F. Lock, David B. Dunson*

- `1311.0307v3` - [abs](http://arxiv.org/abs/1311.0307v3) - [pdf](http://arxiv.org/pdf/1311.0307v3)

> This article concerns testing for equality of distribution between groups. We focus on screening variables with shared distributional features such as common support, modes and patterns of skewness. We propose a Bayesian testing method using kernel mixtures, which improves performance by borrowing information across the different variables and groups through shared kernels and a common probability of group differences. The inclusion of shared kernels in a finite mixture, with Dirichlet priors on the weights, leads to a simple framework for testing that scales well for high-dimensional data. We provide closed asymptotic forms for the posterior probability of equivalence in two groups and prove consistency under model misspecification. The method is applied to DNA methylation array data from a breast cancer study, and compares favorably to competitors when type I error is estimated via permutation.

</details>

<details>

<summary>2016-02-18 00:04:24 - A multiscale strategy for Bayesian inference using transport maps</summary>

- *Matthew Parno, Tarek Moselhy, Youssef Marzouk*

- `1507.07024v2` - [abs](http://arxiv.org/abs/1507.07024v2) - [pdf](http://arxiv.org/pdf/1507.07024v2)

> In many inverse problems, model parameters cannot be precisely determined from observational data. Bayesian inference provides a mechanism for capturing the resulting parameter uncertainty, but typically at a high computational cost. This work introduces a multiscale decomposition that exploits conditional independence across scales, when present in certain classes of inverse problems, to decouple Bayesian inference into two stages: (1) a computationally tractable coarse-scale inference problem; and (2) a mapping of the low-dimensional coarse-scale posterior distribution into the original high-dimensional parameter space. This decomposition relies on a characterization of the non-Gaussian joint distribution of coarse- and fine-scale quantities via optimal transport maps. We demonstrate our approach on a sequence of inverse problems arising in subsurface flow, using the multiscale finite element method to discretize the steady state pressure equation. We compare the multiscale strategy with full-dimensional Markov chain Monte Carlo on a problem of moderate dimension (100 parameters) and then use it to infer a conductivity field described by over 10,000 parameters.

</details>

<details>

<summary>2016-02-20 10:08:18 - Stratified Bayesian Optimization</summary>

- *Saul Toscano-Palmerin, Peter I. Frazier*

- `1602.02338v2` - [abs](http://arxiv.org/abs/1602.02338v2) - [pdf](http://arxiv.org/pdf/1602.02338v2)

> We consider derivative-free black-box global optimization of expensive noisy functions, when most of the randomness in the objective is produced by a few influential scalar random inputs. We present a new Bayesian global optimization algorithm, called Stratified Bayesian Optimization (SBO), which uses this strong dependence to improve performance. Our algorithm is similar in spirit to stratification, a technique from simulation, which uses strong dependence on a categorical representation of the random input to reduce variance. We demonstrate in numerical experiments that SBO outperforms state-of-the-art Bayesian optimization benchmarks that do not leverage this dependence.

</details>

<details>

<summary>2016-02-20 15:54:08 - Posterior Predictive P-values with Fisher Randomization Tests in Noncompliance Settings: Test Statistics vs Discrepancy Variables</summary>

- *Laura Forastiere, Fabrizia Mealli, Luke Miratrix*

- `1511.00521v3` - [abs](http://arxiv.org/abs/1511.00521v3) - [pdf](http://arxiv.org/pdf/1511.00521v3)

> In randomized experiments with noncompliance, tests may focus on compliers rather than on the overall sample. Rubin (1998) put forth such a method, and argued that testing for the complier average causal effect and averaging permutation based p-values over the posterior distribution of the compliance status could increase power, as compared to general intent-to-treat tests. The general scheme is to repeatedly do a two-step process of imputing missing compliance statuses and conducting a permutation test with the completed data. In this paper, we explore this idea further, comparing the use of discrepancy measures, which depend on unknown but imputed parameters, to classical test statistics and exploring different approaches for imputing the unknown compliance statuses. We also examine consequences of model misspecification in the imputation step, and discuss to what extent this additional modeling undercuts the permutation test's model independence. We find that, especially for discrepancy measures, modeling choices can impact both power and validity. In particular, imputing missing compliance statuses assuming the null can radically reduce power, but not doing so can jeopardize validity. Fortunately, covariates predictive of compliance status can mitigate these results. Finally, we compare this overall approach to Bayesian model-based tests, that is tests that are directly derived from posterior credible intervals, under both correct and incorrect model specification. We find that adding the permutation step in an otherwise Bayesian approach improves robustness to model specification without substantial loss of power.

</details>

<details>

<summary>2016-02-21 03:56:08 - Efficient functional ANOVA through wavelet-domain Markov groves</summary>

- *Li Ma, Jacopo Soriano*

- `1602.03990v2` - [abs](http://arxiv.org/abs/1602.03990v2) - [pdf](http://arxiv.org/pdf/1602.03990v2)

> We introduce a wavelet-domain functional analysis of variance (fANOVA) method based on a Bayesian hierarchical model. The factor effects are modeled through a spike-and-slab mixture at each location-scale combination along with a normal-inverse-Gamma (NIG) conjugate setup for the coefficients and errors. A graphical model called the Markov grove (MG) is designed to jointly model the spike-and-slab statuses at all location-scale combinations, which incorporates the clustering of each factor effect in the wavelet-domain thereby allowing borrowing of strength across location and scale. The posterior of this NIG-MG model is analytically available through a pyramid algorithm of the same computational complexity as Mallat's pyramid algorithm for discrete wavelet transform, i.e., linear in both the number of observations and the number of locations. Posterior probabilities of factor contributions can also be computed through pyramid recursion, and exact samples from the posterior can be drawn without MCMC. We investigate the performance of our method through extensive simulation and show that it outperforms existing wavelet-domain fANOVA methods in a variety of common settings. We apply the method to analyzing the orthosis data.

</details>

<details>

<summary>2016-02-21 06:27:40 - Learning, Visualizing, and Exploiting a Model for the Intrinsic Value of a Batted Ball</summary>

- *Glenn Healey*

- `1603.00050v1` - [abs](http://arxiv.org/abs/1603.00050v1) - [pdf](http://arxiv.org/pdf/1603.00050v1)

> We present an algorithm for learning the intrinsic value of a batted ball in baseball. This work addresses the fundamental problem of separating the value of a batted ball at contact from factors such as the defense, weather, and ballpark that can affect its observed outcome. The algorithm uses a Bayesian model to construct a continuous mapping from a vector of batted ball parameters to an intrinsic measure defined as the expected value of a linear weights representation for run value. A kernel method is used to build nonparametric estimates for the component probability density functions in Bayes theorem from a set of over one hundred thousand batted ball measurements recorded by the HITf/x system during the 2014 major league baseball (MLB) season. Cross-validation is used to determine the optimal vector of smoothing parameters for the density estimates. Properties of the mapping are visualized by considering reduced-dimension subsets of the batted ball parameter space. We use the mapping to derive statistics for intrinsic quality of contact for batters and pitchers which have the potential to improve the accuracy of player models and forecasting systems. We also show that the new approach leads to a simple automated measure of contact-adjusted defense and provides insight into the impact of environmental variables on batted balls.

</details>

<details>

<summary>2016-02-21 16:25:27 - Predictive Entropy Search for Multi-objective Bayesian Optimization</summary>

- *Daniel HernÃ¡ndez-Lobato, JosÃ© Miguel HernÃ¡ndez-Lobato, Amar Shah, Ryan P. Adams*

- `1511.05467v3` - [abs](http://arxiv.org/abs/1511.05467v3) - [pdf](http://arxiv.org/pdf/1511.05467v3)

> We present PESMO, a Bayesian method for identifying the Pareto set of multi-objective optimization problems, when the functions are expensive to evaluate. The central idea of PESMO is to choose evaluation points so as to maximally reduce the entropy of the posterior distribution over the Pareto set. Critically, the PESMO multi-objective acquisition function can be decomposed as a sum of objective-specific acquisition functions, which enables the algorithm to be used in \emph{decoupled} scenarios in which the objectives can be evaluated separately and perhaps with different costs. This decoupling capability also makes it possible to identify difficult objectives that require more evaluations. PESMO also offers gains in efficiency, as its cost scales linearly with the number of objectives, in comparison to the exponential cost of other methods. We compare PESMO with other related methods for multi-objective Bayesian optimization on synthetic and real-world problems. The results show that PESMO produces better recommendations with a smaller number of evaluations of the objectives, and that a decoupled evaluation can lead to improvements in performance, particularly when the number of objectives is large.

</details>

<details>

<summary>2016-02-22 03:10:56 - Statistical Mechanics of High-Dimensional Inference</summary>

- *Madhu Advani, Surya Ganguli*

- `1601.04650v2` - [abs](http://arxiv.org/abs/1601.04650v2) - [pdf](http://arxiv.org/pdf/1601.04650v2)

> To model modern large-scale datasets, we need efficient algorithms to infer a set of $P$ unknown model parameters from $N$ noisy measurements. What are fundamental limits on the accuracy of parameter inference, given finite signal-to-noise ratios, limited measurements, prior information, and computational tractability requirements? How can we combine prior information with measurements to achieve these limits? Classical statistics gives incisive answers to these questions as the measurement density $\alpha = \frac{N}{P}\rightarrow \infty$. However, these classical results are not relevant to modern high-dimensional inference problems, which instead occur at finite $\alpha$. We formulate and analyze high-dimensional inference as a problem in the statistical physics of quenched disorder. Our analysis uncovers fundamental limits on the accuracy of inference in high dimensions, and reveals that widely cherished inference algorithms like maximum likelihood (ML) and maximum-a posteriori (MAP) inference cannot achieve these limits. We further find optimal, computationally tractable algorithms that can achieve these limits. Intriguingly, in high dimensions, these optimal algorithms become computationally simpler than MAP and ML, while still outperforming them. For example, such optimal algorithms can lead to as much as a 20% reduction in the amount of data to achieve the same performance relative to MAP. Moreover, our analysis reveals simple relations between optimal high dimensional inference and low dimensional scalar Bayesian inference, insights into the nature of generalization and predictive power in high dimensions, information theoretic limits on compressed sensing, phase transitions in quadratic inference, and connections to central mathematical objects in convex optimization theory and random matrix theory.

</details>

<details>

<summary>2016-02-22 20:32:19 - Efficient Bayesian estimation and uncertainty quantification in ordinary differential equation models</summary>

- *Prithwish Bhaumik, Subhashis Ghosal*

- `1411.1166v2` - [abs](http://arxiv.org/abs/1411.1166v2) - [pdf](http://arxiv.org/pdf/1411.1166v2)

> Often the regression function is specified by a system of ordinary differential equations (ODEs) involving some unknown parameters. Typically analytical solution of the ODEs is not available, and hence likelihood evaluation at many parameter values by numerical solution of equations may be computationally prohibitive. Bhaumik and Ghosal (2015) considered a Bayesian two-step approach by embedding the model in a larger nonparametric regression model, where a prior is put through a random series based on B-spline basis functions. A posterior on the parameter is induced from the regression function by minimizing an integrated weighted squared distance between the derivative of the regression function and the derivative suggested by the ODEs. Although this approach is computationally fast, the Bayes estimator is not asymptotically efficient. In this paper we suggest a modification of the two-step method by directly considering the distance between the function in the nonparametric model and that obtained from a four stage Runge-Kutta (RK4) method. We also study the asymptotic behavior of the posterior distribution of the parameter based on an approximate likelihood obtained from an RK4 numerical solution of the ODEs. We establish a Bernstein-von Mises theorem for both methods which assures that Bayesian uncertainty quantification matches with the frequentist one and the Bayes estimator is asymptotically efficient.

</details>

<details>

<summary>2016-02-22 21:32:41 - Modelling collinear and spatially correlated data</summary>

- *Silvia Liverani, Aurore Lavigne, Marta Blangiardo*

- `1602.06972v1` - [abs](http://arxiv.org/abs/1602.06972v1) - [pdf](http://arxiv.org/pdf/1602.06972v1)

> In this work we present a statistical approach to distinguish and interpret the complex relationship between several predictors and a response variable at the small area level, in the presence of i) high correlation between the predictors and ii) spatial correlation for the response. Covariates which are highly correlated create collinearity problems when used in a standard multiple regression model. Many methods have been proposed in the literature to address this issue. A very common approach is to create an index which aggregates all the highly correlated variables of interest. For example, it is well known that there is a relationship between social deprivation measured through the Multiple Deprivation Index (IMD) and air pollution; this index is then used as a confounder in assessing the effect of air pollution on health outcomes (e.g. respiratory hospital admissions or mortality). However it would be more informative to look specifically at each domain of the IMD and at its relationship with air pollution to better understand its role as a confounder in the epidemiological analyses. In this paper we illustrate how the complex relationships between the domains of IMD and air pollution can be deconstructed and analysed using profile regression, a Bayesian non-parametric model for clustering responses and covariates simultaneously. Moreover, we include an intrinsic spatial conditional autoregressive (ICAR) term to account for the spatial correlation of the response variable.

</details>

<details>

<summary>2016-02-23 02:50:34 - Fast sampling in a linear-Gaussian inverse problem</summary>

- *Colin Fox, Richard A. Norton*

- `1507.01614v2` - [abs](http://arxiv.org/abs/1507.01614v2) - [pdf](http://arxiv.org/pdf/1507.01614v2)

> We solve the inverse problem of deblurring a pixelized image of Jupiter using regularized deconvolution and by sample-based Bayesian inference. By efficiently sampling the marginal posterior distribution for hyperparameters, then the full conditional for the deblurred image, we find that we can evaluate the posterior mean faster than regularized inversion, when selection of the regularizing parameter is considered. To our knowledge, this is the first demonstration of sampling and inference that takes less compute time than regularized inversion in an inverse problems. Comparison to random-walk Metropolis-Hastings and block Gibbs MCMC shows that marginal then conditional sampling also outperforms these more common sampling algorithms, having better scaling with problem size. When problem-specific computations are feasible the asymptotic cost of an independent sample is one linear solve, implying that sample-based Bayesian inference may be performed directly over function spaces, when that limit exists.

</details>

<details>

<summary>2016-02-23 08:37:39 - On dimension reduction in Gaussian filters</summary>

- *Antti Solonen, Tiangang Cui, Janne Hakkarainen, Youssef Marzouk*

- `1508.06452v3` - [abs](http://arxiv.org/abs/1508.06452v3) - [pdf](http://arxiv.org/pdf/1508.06452v3)

> A priori dimension reduction is a widely adopted technique for reducing the computational complexity of stationary inverse problems. In this setting, the solution of an inverse problem is parameterized by a low-dimensional basis that is often obtained from the truncated Karhunen-Loeve expansion of the prior distribution. For high-dimensional inverse problems equipped with smoothing priors, this technique can lead to drastic reductions in parameter dimension and significant computational savings.   In this paper, we extend the concept of a priori dimension reduction to non-stationary inverse problems, in which the goal is to sequentially infer the state of a dynamical system. Our approach proceeds in an offline-online fashion. We first identify a low-dimensional subspace in the state space before solving the inverse problem (the offline phase), using either the method of "snapshots" or regularized covariance estimation. Then this subspace is used to reduce the computational complexity of various filtering algorithms - including the Kalman filter, extended Kalman filter, and ensemble Kalman filter - within a novel subspace-constrained Bayesian prediction-and-update procedure (the online phase). We demonstrate the performance of our new dimension reduction approach on various numerical examples. In some test cases, our approach reduces the dimensionality of the original problem by orders of magnitude and yields up to two orders of magnitude in computational savings.

</details>

<details>

<summary>2016-02-24 08:08:05 - Max-Margin Nonparametric Latent Feature Models for Link Prediction</summary>

- *Jun Zhu, Jiaming Song, Bei Chen*

- `1602.07428v1` - [abs](http://arxiv.org/abs/1602.07428v1) - [pdf](http://arxiv.org/pdf/1602.07428v1)

> Link prediction is a fundamental task in statistical network analysis. Recent advances have been made on learning flexible nonparametric Bayesian latent feature models for link prediction. In this paper, we present a max-margin learning method for such nonparametric latent feature relational models. Our approach attempts to unite the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction. It inherits the advances of nonparametric Bayesian methods to infer the unknown latent social dimension, while for discriminative link prediction, it adopts the max-margin learning principle by minimizing a hinge-loss using the linear expectation operator, without dealing with a highly nonlinear link likelihood function. For posterior inference, we develop an efficient stochastic variational inference algorithm under a truncated mean-field assumption. Our methods can scale up to large-scale real networks with millions of entities and tens of millions of positive links. We also provide a full Bayesian formulation, which can avoid tuning regularization hyper-parameters. Experimental results on a diverse range of real datasets demonstrate the benefits inherited from max-margin learning and Bayesian nonparametric inference.

</details>

<details>

<summary>2016-02-24 19:16:36 - A Variational Algorithm for Bayesian Variable Selection</summary>

- *Xichen Huang, Jin Wang, Feng Liang*

- `1602.07640v1` - [abs](http://arxiv.org/abs/1602.07640v1) - [pdf](http://arxiv.org/pdf/1602.07640v1)

> There has been an intense development on the estimation of a sparse regression coefficient vector in statistics, machine learning and related fields. In this paper, we focus on the Bayesian approach to this problem, where sparsity is incorporated by the so-called spike-and-slab prior on the coefficients. Instead of replying on MCMC for posterior inference, we propose a fast and scalable algorithm based on variational approximation to the posterior distribution. The updating scheme employed by our algorithm is different from the one proposed by Carbonetto and Stephens (2012). Those changes seem crucial for us to show that our algorithm can achieve asymptotic consistency even when the feature dimension diverges exponentially fast with the sample size. Empirical results have demonstrated the effectiveness and efficiency of the proposed algorithm.

</details>

<details>

<summary>2016-02-24 20:01:19 - Recurrent Gaussian Processes</summary>

- *CÃ©sar Lincoln C. Mattos, Zhenwen Dai, Andreas Damianou, Jeremy Forth, Guilherme A. Barreto, Neil D. Lawrence*

- `1511.06644v6` - [abs](http://arxiv.org/abs/1511.06644v6) - [pdf](http://arxiv.org/pdf/1511.06644v6)

> We define Recurrent Gaussian Processes (RGP) models, a general family of Bayesian nonparametric models with recurrent GP priors which are able to learn dynamical patterns from sequential data. Similar to Recurrent Neural Networks (RNNs), RGPs can have different formulations for their internal states, distinct inference methods and be extended with deep structures. In such context, we propose a novel deep RGP model whose autoregressive states are latent, thereby performing representation and dynamical learning simultaneously. To fully exploit the Bayesian nature of the RGP model we develop the Recurrent Variational Bayes (REVARB) framework, which enables efficient inference and strong regularization through coherent propagation of uncertainty across the RGP layers and states. We also introduce a RGP extension where variational parameters are greatly reduced by being reparametrized through RNN-based sequential recognition models. We apply our model to the tasks of nonlinear system identification and human motion modeling. The promising obtained results indicate that our RGP model maintains its highly flexibility while being able to avoid overfitting and being applicable even when larger datasets are not available.

</details>

<details>

<summary>2016-02-25 09:42:46 - Learning Gaussian Graphical Models With Fractional Marginal Pseudo-likelihood</summary>

- *Janne LeppÃ¤-aho, Johan Pensar, Teemu Roos, Jukka Corander*

- `1602.07863v1` - [abs](http://arxiv.org/abs/1602.07863v1) - [pdf](http://arxiv.org/pdf/1602.07863v1)

> We propose a Bayesian approximate inference method for learning the dependence structure of a Gaussian graphical model. Using pseudo-likelihood, we derive an analytical expression to approximate the marginal likelihood for an arbitrary graph structure without invoking any assumptions about decomposability. The majority of the existing methods for learning Gaussian graphical models are either restricted to decomposable graphs or require specification of a tuning parameter that may have a substantial impact on learned structures. By combining a simple sparsity inducing prior for the graph structures with a default reference prior for the model parameters, we obtain a fast and easily applicable scoring function that works well for even high-dimensional data. We demonstrate the favourable performance of our approach by large-scale comparisons against the leading methods for learning non-decomposable Gaussian graphical models. A theoretical justification for our method is provided by showing that it yields a consistent estimator of the graph structure.

</details>

<details>

<summary>2016-02-26 19:41:21 - Objective Bayesian Analysis for the Lomax Distribution</summary>

- *Paulo Ferreira, Jhon Gonzales, Vera Tomazella, Ricardo Ehlers, Francisco Louzada, Eveliny Silva*

- `1602.08450v1` - [abs](http://arxiv.org/abs/1602.08450v1) - [pdf](http://arxiv.org/pdf/1602.08450v1)

> In this paper we propose to make Bayesian inferences for the parameters of the Lomax distribution using non-informative priors, namely the Jeffreys prior and the reference prior. We assess Bayesian estimation through a Monte Carlo study with 500 simulated data sets. To evaluate the possible impact of prior specification on estimation, two criteria were considered: the bias and square root of the mean square error. The developed procedures are illustrated on a real data set.

</details>

<details>

<summary>2016-02-27 09:11:24 - Bayesian Quantile Regression for Ordinal Longitudinal Data</summary>

- *Rahim Alhamzawi*

- `1603.00297v1` - [abs](http://arxiv.org/abs/1603.00297v1) - [pdf](http://arxiv.org/pdf/1603.00297v1)

> Since the pioneering work by Koenker and Bassett (1978), quantile regression models and its applications have become increasingly popular and important for research in many areas. In this paper, a random effects ordinal quantile regression model is proposed for analysis of longitudinal data with ordinal outcome of interest. An efficient Gibbs sampling algorithm was derived for fitting the model to the data based on a location scale mixture representation of the skewed double exponential distribution. The proposed approach is illustrated using simulated data and a real data example. This is the first work to discuss quantile regression for analysis of longitudinal data with ordinal outcome.

</details>

<details>

<summary>2016-02-27 17:27:49 - A Bayesian baseline for belief in uncommon events</summary>

- *V. Palonen*

- `1602.07836v2` - [abs](http://arxiv.org/abs/1602.07836v2) - [pdf](http://arxiv.org/pdf/1602.07836v2)

> The plausibility of uncommon events and miracles based on testimony of such an event has been much discussed. When analyzing the probabilities involved, it has mostly been assumed that the common events can be taken as data in the calculations. However, we usually have only testimonies for the common events. While this difference does not have a significant effect on the inductive part of the inference, it has a large influence on how one should view the reliability of testimonies. In this work, a full Bayesian solution is given for the more realistic case, where one has a large number of testimonies for a common event and one testimony for an uncommon event. It is seen that, in order for there to be a large amount of testimonies for a common event, the testimonies will probably be quite reliable. For this reason, because the testimonies are quite reliable based on the testimonies for the common events, the probability for the uncommon event, given a testimony for it, is also higher. Hence, one should be more open-minded when considering the plausibility of uncommon events.

</details>

<details>

<summary>2016-02-29 17:23:18 - Bayesian estimation of airborne fugitive emissions using a Gaussian plume model</summary>

- *Bamdad Hosseini, John M. Stockie*

- `1602.09053v1` - [abs](http://arxiv.org/abs/1602.09053v1) - [pdf](http://arxiv.org/pdf/1602.09053v1)

> A new method is proposed for estimating the rate of fugitive emissions of particulate matter from multiple time-dependent sources via measurements of deposition and concentration. We cast this source inversion problem within the Bayesian framework, and use a forward model based on a Gaussian plume solution. We present three alternate models for constructing the prior distribution on the emission rates as functions of time. Next, we present an industrial case study in which our framework is applied to estimate the rate of fugitive emissions of lead particulates from a smelter in Trail, British Columbia, Canada. The Bayesian framework not only provides an approximate solution to the inverse problem, but also quantifies the uncertainty in the solution. Using this information we perform an uncertainty propagation study in order to assess the impact of the estimated sources on the area surrounding the industrial site.

</details>

<details>

<summary>2016-02-29 20:02:13 - Multi Snapshot Sparse Bayesian Learning for DOA Estimation</summary>

- *Peter Gerstoft, Christoph F. MecklenbrÃ¤uker, Angeliki Xenaki*

- `1602.09120v1` - [abs](http://arxiv.org/abs/1602.09120v1) - [pdf](http://arxiv.org/pdf/1602.09120v1)

> The directions of arrival (DOA) of plane waves are estimated from multi-snapshot sensor array data using Sparse Bayesian Learning (SBL). The prior source amplitudes is assumed independent zero-mean complex Gaussian distributed with hyperparameters the unknown variances (i.e. the source powers). For a complex Gaussian likelihood with hyperparameter the unknown noise variance, the corresponding Gaussian posterior distribution is derived. For a given number of DOAs, the hyperparameters are automatically selected by maximizing the evidence and promote sparse DOA estimates. The SBL scheme for DOA estimation is discussed and evaluated competitively against LASSO ($\ell_1$-regularization), conventional beamforming, and MUSIC

</details>

<details>

<summary>2016-02-29 21:34:58 - Variational Auto-encoded Deep Gaussian Processes</summary>

- *Zhenwen Dai, Andreas Damianou, Javier GonzÃ¡lez, Neil Lawrence*

- `1511.06455v2` - [abs](http://arxiv.org/abs/1511.06455v2) - [pdf](http://arxiv.org/pdf/1511.06455v2)

> We develop a scalable deep non-parametric generative model by augmenting deep Gaussian processes with a recognition model. Inference is performed in a novel scalable variational framework where the variational posterior distributions are reparametrized through a multilayer perceptron. The key aspect of this reformulation is that it prevents the proliferation of variational parameters which otherwise grow linearly in proportion to the sample size. We derive a new formulation of the variational lower bound that allows us to distribute most of the computation in a way that enables to handle datasets of the size of mainstream deep learning tasks. We show the efficacy of the method on a variety of challenges including deep unsupervised learning and deep Bayesian optimization.

</details>

<details>

<summary>2016-02-29 22:21:42 - Modeling the Infectiousness of Twitter Hashtags</summary>

- *Jonathan Skaza, Brian Blais*

- `1603.00074v1` - [abs](http://arxiv.org/abs/1603.00074v1) - [pdf](http://arxiv.org/pdf/1603.00074v1)

> This study applies dynamical and statistical modeling techniques to quantify the proliferation and popularity of trending hashtags on Twitter. Using time-series data reflecting actual tweets in New York City and San Francisco, we present estimates for the dynamics (i.e., rates of infection and recovery) of several hundred trending hashtags using an epidemic modeling framework coupled with Bayesian Markov Chain Monte Carlo (MCMC) methods. This methodological strategy is an extension of techniques traditionally used to model the spread of infectious disease. We demonstrate that in some models, hashtags can be grouped by infectiousness, possibly providing a method for quantifying the trendiness of a topic.

</details>


## 2016-03

<details>

<summary>2016-03-01 16:48:56 - Analyzing Non-proportional Hazards: Use of the MRH Package</summary>

- *Yolanda Hagar, Vanja Dukic*

- `1603.00351v1` - [abs](http://arxiv.org/abs/1603.00351v1) - [pdf](http://arxiv.org/pdf/1603.00351v1)

> In this manuscript we demonstrate the analysis of right-censored survival outcomes using the MRH package in R. The MRH package implements the multi-resolution hazard (MRH) model, which is a Polya-tree based, Bayesian semi-parametric method for flexible estimation of the hazard rate and covariate effects. The package allows for covariates to be included under the proportional and non-proportional hazards assumption, and for robust estimation of the hazard rate in periods of sparsely observed failures via a "pruning" tool.

</details>

<details>

<summary>2016-03-01 18:35:04 - On the Design and use of Ensembles of Multi-model Simulations for Forecasting</summary>

- *Sarah Higgins, Hailiang Du, Leonard A. Smith*

- `1603.00393v1` - [abs](http://arxiv.org/abs/1603.00393v1) - [pdf](http://arxiv.org/pdf/1603.00393v1)

> Probability forecasting is common in the geosciences, the finance sector, and elsewhere. It is sometimes the case that one has multiple probability-forecasts for the same target. How is the information in these multiple forecast systems best "combined"? Assuming stationary, then in the limit of a very large forecast-outcome archive, each model-based probability density function can be weighted to form a "multi-model forecast" which will, in expectation, provide the most information. In the case that one of the forecast systems yields a probability distribution which reflects the distribution from which the outcome will be drawn, then Bayesian Model Averaging will identify this model as the number of forecast-outcome pairs goes to infinity. In many applications, like those of seasonal forecasting, data are precious: the archive is often limited to fewer than $2^6$ entries. And no perfect model is in hand. In this case, it is shown that forming a single "multi-model probability forecast" can be expected to prove misleading. These issues are investigated using probability forecasts of a simple mathematical system, which allows most limiting behaviours to be quantified.

</details>

<details>

<summary>2016-03-01 23:36:04 - Bayesian representation learning with oracle constraints</summary>

- *Theofanis Karaletsos, Serge Belongie, Gunnar RÃ¤tsch*

- `1506.05011v4` - [abs](http://arxiv.org/abs/1506.05011v4) - [pdf](http://arxiv.org/pdf/1506.05011v4)

> Representation learning systems typically rely on massive amounts of labeled data in order to be trained to high accuracy. Recently, high-dimensional parametric models like neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Thus, \emph{oracles} or \emph{human-in-the-loop systems}, for example crowdsourcing, are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. In this work we propose to combine \emph{generative unsupervised feature learning} with a \emph{probabilistic treatment of oracle information like triplets} in order to transfer implicit privileged oracle knowledge into explicit nonlinear Bayesian latent factor models of the observations. We use a fast variational algorithm to learn the joint model and demonstrate applicability to a well-known image dataset. We show how implicit triplet information can provide rich information to learn representations that outperform previous metric learning approaches as well as generative models without this side-information in a variety of predictive tasks. In addition, we illustrate that the proposed approach compartmentalizes the latent spaces semantically which allows interpretation of the latent variables.

</details>

<details>

<summary>2016-03-08 09:57:31 - Doubly Decomposing Nonparametric Tensor Regression</summary>

- *Masaaki Imaizumi, Kohei Hayashi*

- `1506.05967v3` - [abs](http://arxiv.org/abs/1506.05967v3) - [pdf](http://arxiv.org/pdf/1506.05967v3)

> Nonparametric extension of tensor regression is proposed. Nonlinearity in a high-dimensional tensor space is broken into simple local functions by incorporating low-rank tensor decomposition. Compared to naive nonparametric approaches, our formulation considerably improves the convergence rate of estimation while maintaining consistency with the same function class under specific conditions. To estimate local functions, we develop a Bayesian estimator with the Gaussian process prior. Experimental results show its theoretical properties and high performance in terms of predicting a summary statistic of a real complex network.

</details>

<details>

<summary>2016-03-08 12:02:59 - A Bayesian non-parametric method for clustering high-dimensional binary data</summary>

- *Tapesh Santra*

- `1603.02494v1` - [abs](http://arxiv.org/abs/1603.02494v1) - [pdf](http://arxiv.org/pdf/1603.02494v1)

> In many real life problems, objects are described by large number of binary features. For instance, documents are characterized by presence or absence of certain keywords; cancer patients are characterized by presence or absence of certain mutations etc. In such cases, grouping together similar objects/profiles based on such high dimensional binary features is desirable, but challenging. Here, I present a Bayesian non parametric algorithm for clustering high dimensional binary data. It uses a Dirichlet Process (DP) mixture model and simulated annealing to not only cluster binary data, but also find optimal number of clusters in the data. The performance of the algorithm was evaluated and compared with other algorithms using simulated datasets. It outperformed all other clustering methods that were tested in the simulation studies. It was also used to cluster real datasets arising from document analysis, handwritten image analysis and cancer research. It successfully divided a set of documents based on their topics, hand written images based on different styles of writing digits and identified tissue and mutation specificity of chemotherapy treatments.

</details>

<details>

<summary>2016-03-09 03:38:11 - Inference on Self-Exciting Jumps in Prices and Volatility using High Frequency Measures</summary>

- *Worapree Maneesoonthorn, Catherine S. Forbes, Gael M. Martin*

- `1401.3911v3` - [abs](http://arxiv.org/abs/1401.3911v3) - [pdf](http://arxiv.org/pdf/1401.3911v3)

> Dynamic jumps in the price and volatility of an asset are modelled using a joint Hawkes process in conjunction with a bivariate jump diffusion. A state space representation is used to link observed returns, plus nonparametric measures of integrated volatility and price jumps, to the specified model components; with Bayesian inference conducted using a Markov chain Monte Carlo algorithm. An evaluation of marginal likelihoods for the proposed model relative to a large number of alternative models, including some that have featured in the literature, is provided. An extensive empirical investigation is undertaken using data on the S&P500 market index over the 1996 to 2014 period, with substantial support for dynamic jump intensities - including in terms of predictive accuracy - documented.

</details>

<details>

<summary>2016-03-09 15:59:07 - Nonparametric Bayesian Double Articulation Analyzer for Direct Language Acquisition from Continuous Speech Signals</summary>

- *Tadahiro Taniguchi, Ryo Nakashima, Shogo Nagasaka*

- `1506.06646v2` - [abs](http://arxiv.org/abs/1506.06646v2) - [pdf](http://arxiv.org/pdf/1506.06646v2)

> Human infants can discover words directly from unsegmented speech signals without any explicitly labeled data. In this paper, we develop a novel machine learning method called nonparametric Bayesian double articulation analyzer (NPB-DAA) that can directly acquire language and acoustic models from observed continuous speech signals. For this purpose, we propose an integrative generative model that combines a language model and an acoustic model into a single generative model called the "hierarchical Dirichlet process hidden language model" (HDP-HLM). The HDP-HLM is obtained by extending the hierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by Johnson et al. An inference procedure for the HDP-HLM is derived using the blocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure enables the simultaneous and direct inference of language and acoustic models from continuous speech signals. Based on the HDP-HLM and its inference procedure, we developed a novel double articulation analyzer. By assuming HDP-HLM as a generative model of observed time series data, and by inferring latent variables of the model, the method can analyze latent double articulation structure, i.e., hierarchically organized latent words and phonemes, of the data in an unsupervised manner. The novel unsupervised double articulation analyzer is called NPB-DAA.   The NPB-DAA can automatically estimate double articulation structure embedded in speech signals. We also carried out two evaluation experiments using synthetic data and actual human continuous speech signals representing Japanese vowel sequences. In the word acquisition and phoneme categorization tasks, the NPB-DAA outperformed a conventional double articulation analyzer (DAA) and baseline automatic speech recognition system whose acoustic model was trained in a supervised manner.

</details>

<details>

<summary>2016-03-10 18:34:41 - Efficient Bayesian experimentation using an expected information gain lower bound</summary>

- *Panagiotis Tsilifis, Roger G. Ghanem, Paris Hajali*

- `1506.00053v2` - [abs](http://arxiv.org/abs/1506.00053v2) - [pdf](http://arxiv.org/pdf/1506.00053v2)

> Experimental design is crucial for inference where limitations in the data collection procedure are present due to cost or other restrictions. Optimal experimental designs determine parameters that in some appropriate sense make the data the most informative possible. In a Bayesian setting this is translated to updating to the best possible posterior. Information theoretic arguments have led to the formation of the expected information gain as a design criterion. This can be evaluated mainly by Monte Carlo sampling and maximized by using stochastic approximation methods, both known for being computationally expensive tasks. We propose a framework where a lower bound of the expected information gain is used as an alternative design criterion. In addition to alleviating the computational burden, this also addresses issues concerning estimation bias. The problem of permeability inference in a large contaminated area is used to demonstrate the validity of our approach where we employ the massively parallel version of the multiphase multicomponent simulator TOUGH2 to simulate contaminant transport and a Polynomial Chaos approximation of the forward model that further accelerates the objective function evaluations. The proposed methodology is demonstrated to a setting where field measurements are available.

</details>

<details>

<summary>2016-03-10 20:04:27 - P-values: misunderstood and misused</summary>

- *Bertie Vidgen, Taha Yasseri*

- `1601.06805v2` - [abs](http://arxiv.org/abs/1601.06805v2) - [pdf](http://arxiv.org/pdf/1601.06805v2)

> P-values are widely used in both the social and natural sciences to quantify the statistical significance of observed results. The recent surge of big data research has made the p-value an even more popular tool to test the significance of a study. However, substantial literature has been produced critiquing how p-values are used and understood. In this paper we review this recent critical literature, much of which is routed in the life sciences, and consider its implications for social scientific research. We provide a coherent picture of what the main criticisms are, and draw together and disambiguate common themes. In particular, we explain how the False Discovery Rate is calculated, and how this differs from a p-value. We also make explicit the Bayesian nature of many recent criticisms, a dimension that is often underplayed or ignored. We conclude by identifying practical steps to help remediate some of the concerns identified. We recommend that (i) far lower significance levels are used, such as $0.01$ or $0.001$, and (ii) p-values are interpreted contextually, and situated within both the findings of the individual study and the broader field of inquiry (through, for example, meta-analyses).

</details>

<details>

<summary>2016-03-13 23:45:27 - Multi-Target Tracking Using A Randomized Hypothesis Generation Technique</summary>

- *W. Faber, S. Chakravorty, Islam I. Hussein*

- `1603.04096v1` - [abs](http://arxiv.org/abs/1603.04096v1) - [pdf](http://arxiv.org/pdf/1603.04096v1)

> In this paper, we present a randomized version of the finite set statistics (FISST) Bayesian recursions for multi-object tracking problems. We propose a hypothesis level derivation of the FISST equations that shows that the multi-object tracking problem may be considered as a finite state space Bayesian filtering problem, albeit with a growing state space. We further show that the FISST and Multi-Hypothesis Tracking (MHT) methods for multi-target tracking are essentially the same. We propose a randomized scheme, termed randomized FISST (R-FISST), where we sample the highly likely hypotheses using Markov Chain Monte Carlo (MCMC) methods which allows us to keep the problem computationally tractable. We apply the R-FISST technique to a fifty-object birth and death Space Situational Awareness (SSA) tracking and detection problem. We also compare the R-FISST technique to the Hypothesis Oriented Multiple Hypothesis Tracking (HOMHT) method using an SSA example.

</details>

<details>

<summary>2016-03-14 08:08:18 - Data assimilation for massive autonomous systems based on second-order adjoint method</summary>

- *Shin-ichi Ito, Hiromichi Nagao, Akinori Yamanaka, Yuhki Tsukada, Toshiyuki Koyama, Masayuki Kano, Junya Inoue*

- `1603.04160v1` - [abs](http://arxiv.org/abs/1603.04160v1) - [pdf](http://arxiv.org/pdf/1603.04160v1)

> Data assimilation (DA) is a fundamental computational technique that integrates numerical simulation models and observation data on the basis of Bayesian statistics. Originally developed for meteorology, especially weather forecasting, DA is now an accepted technique in various scientific fields. One key issue that remains controversial is the implementation of DA in massive simulation models under limited computation time and resources. In this paper, we propose an adjoint-based DA method for massive autonomous models that produces optimum estimates and their uncertainties within practical computation time and resource constraints. The uncertainties are given as several diagonal components of an inverse Hessian matrix, which is the covariance matrix of a normal distribution that approximates the target posterior probability density function in the neighborhood of the optimum. Conventional algorithms for deriving the inverse Hessian matrix require $O(CN^2+N^3)$ computations and $O(N^2)$ memory, where $N$ is the number of degrees of freedom of a given autonomous system and $C$ is the number of computations needed to simulate time series of suitable length. The proposed method using a second-order adjoint method allows us to directly evaluate the diagonal components of the inverse Hessian matrix without computing all of its components. This drastically reduces the number of computations to $O(C)$ and the amount of memory to $O(N)$ for each diagonal component. The proposed method is validated through numerical tests using a massive two-dimensional Kobayashi's phase-field model. We confirm that the proposed method correctly reproduces the parameter and initial state assumed in advance, and successfully evaluates the uncertainty of the parameter. Such information regarding uncertainty is valuable, as it can be used to optimize the design of experiments.

</details>

<details>

<summary>2016-03-14 09:02:13 - The Normal Law Under Linear Restrictions: Simulation and Estimation via Minimax Tilting</summary>

- *Z. I. Botev*

- `1603.04166v1` - [abs](http://arxiv.org/abs/1603.04166v1) - [pdf](http://arxiv.org/pdf/1603.04166v1)

> Simulation from the truncated multivariate normal distribution in high dimensions is a recurrent problem in statistical computing, and is typically only feasible using approximate MCMC sampling. In this article we propose a minimax tilting method for exact iid simulation from the truncated multivariate normal distribution. The new methodology provides both a method for simulation and an efficient estimator to hitherto intractable Gaussian integrals. We prove that the estimator possesses a rare vanishing relative error asymptotic property. Numerical experiments suggest that the proposed scheme is accurate in a wide range of setups for which competing estimation schemes fail. We give an application to exact iid simulation from the Bayesian posterior of the probit regression model.

</details>

<details>

<summary>2016-03-14 17:51:05 - An Ensemble EM Algorithm for Bayesian Variable Selection</summary>

- *Jin Wang, Feng Liang, Yuan Ji*

- `1603.04360v1` - [abs](http://arxiv.org/abs/1603.04360v1) - [pdf](http://arxiv.org/pdf/1603.04360v1)

> We study the Bayesian approach to variable selection in the context of linear regression. Motivated by a recent work by Rockova and George (2014), we propose an EM algorithm that returns the MAP estimate of the set of relevant variables. Due to its particular updating scheme, our algorithm can be implemented efficiently without inverting a large matrix in each iteration and therefore can scale up with big data. We also show that the MAP estimate returned by our EM algorithm achieves variable selection consistency even when $p$ diverges with $n$. In practice, our algorithm could get stuck with local modes, a common problem with EM algorithms. To address this issue, we propose an ensemble EM algorithm, in which we repeatedly apply the EM algorithm on a subset of the samples with a subset of the covariates, and then aggregate the variable selection results across those bootstrap replicates. Empirical studies have demonstrated the superior performance of the ensemble EM algorithm.

</details>

<details>

<summary>2016-03-14 23:13:45 - Optimal designs for dose response curves with common parameters</summary>

- *Chrystel Feller, Kirsten Schorning, Holger Dette, Georgina Bermann, BjÃ¶rn Bornkamp*

- `1603.04500v1` - [abs](http://arxiv.org/abs/1603.04500v1) - [pdf](http://arxiv.org/pdf/1603.04500v1)

> A common problem in Phase II clinical trials is the comparison of dose response curves corresponding to different treatment groups. If the effect of the dose level is described by parametric regression models and the treatments differ in the administration frequency (but not in the sort of drug) a reasonable assumption is that the regression models for the different treatments share common parameters. This paper develops optimal design theory for the comparison of different regression models with common parameters. We derive upper bounds on the number of support points of admissible designs, and explicit expressions for $D$-optimal designs are derived for frequently used dose response models with a common location parameter. If the location and scale parameter in the different models coincide, minimally supported designs are determined and sufficient conditions for their optimality in the class of all designs derived. The results are illustrated in a dose-finding study comparing monthly and weekly administration.

</details>

<details>

<summary>2016-03-15 10:45:15 - Accelerating a hybrid continuum-atomistic fluidic model with on-the-fly machine learning</summary>

- *David Stephenson, James R Kermode, Duncan A Lockerby*

- `1603.04628v1` - [abs](http://arxiv.org/abs/1603.04628v1) - [pdf](http://arxiv.org/pdf/1603.04628v1)

> We present a hybrid continuum-atomistic scheme which combines molecular dynamics (MD) simulations with on-the-fly machine learning techniques for the accurate and efficient prediction of multiscale fluidic systems. By using a Gaussian process as a surrogate model for the computationally expensive MD simulations, we use Bayesian inference to predict the system behaviour at the atomistic scale, purely by consideration of the macroscopic inputs and outputs. Whenever the uncertainty of this prediction is greater than a predetermined acceptable threshold, a new MD simulation is performed to continually augment the database, which is never required to be complete. This provides a substantial enhancement to the current generation of hybrid methods, which often require many similar atomistic simulations to be performed, discarding information after it is used once.   We apply our hybrid scheme to nano-confined unsteady flow through a high-aspect-ratio converging-diverging channel, and make comparisons between the new scheme and full MD simulations for a range of uncertainty thresholds and initial databases. For low thresholds, our hybrid solution is highly accurate\,---\,within the thermal noise of a full MD simulation. As the uncertainty threshold is raised, the accuracy of our scheme decreases and the computational speed-up increases (relative to a full MD simulation), enabling the compromise between precision and efficiency to be tuned. The speed-up of our hybrid solution ranges from an order of magnitude, with no initial database, to cases where an extensive initial database ensures no new MD simulations are required.

</details>

<details>

<summary>2016-03-16 10:57:08 - A Bayesian binary classification approach to pure tone audiometry</summary>

- *Marco Cox, Bert de Vries*

- `1511.08670v2` - [abs](http://arxiv.org/abs/1511.08670v2) - [pdf](http://arxiv.org/pdf/1511.08670v2)

> The pure tone hearing threshold is usually estimated from responses to stimuli at a set of standard frequencies. This paper describes a probabilistic approach to the estimation problem in which the hearing threshold is modelled as a smooth continuous function of frequency using a Gaussian process. This allows sampling at any frequency and reduces the number of required measurements. The Gaussian process is combined with a probabilistic response model to account for uncertainty in the responses. The resulting full model can be interpreted as a two-dimensional binary classifier for stimuli, and provides uncertainty bands on the estimated threshold curve. The optimal next stimulus is determined based on an information theoretic criterion. This leads to a robust adaptive estimation method that can be applied to fully automate the hearing threshold estimation process.

</details>

<details>

<summary>2016-03-16 14:11:36 - Bayesian Uncertainty Management in Temporal Dependence of Extremes</summary>

- *Thomas Lugrin, Anthony C. Davison, Jonathan A. Tawn*

- `1512.01169v2` - [abs](http://arxiv.org/abs/1512.01169v2) - [pdf](http://arxiv.org/pdf/1512.01169v2)

> Both marginal and dependence features must be described when modelling the extremes of a stationary time series. There are standard approaches to marginal modelling, but long- and short-range dependence of extremes may both appear. In applications, an assumption of long-range independence often seems reasonable, but short-range dependence, i.e., the clustering of extremes, needs attention. The extremal index $0<\theta\le 1$ is a natural limiting measure of clustering, but for wide classes of dependent processes, including all stationary Gaussian processes, it cannot distinguish dependent processes from independent processes with $\theta=1$. Eastoe and Tawn (2012) exploit methods from multivariate extremes to treat the subasymptotic extremal dependence structure of stationary time series, covering both $0<\theta<1$ and $\theta=1$, through the introduction of a threshold-based extremal index. Inference for their dependence models uses an inefficient stepwise procedure that has various weaknesses and has no reliable assessment of uncertainty. We overcome these issues using a Bayesian semiparametric approach. Simulations and the analysis of a UK daily river flow time series show that the new approach provides improved efficiency for estimating properties of functionals of clusters.

</details>

<details>

<summary>2016-03-16 19:29:17 - PAC-Bayesian bounds for the Gram matrix and least squares regression with a random design</summary>

- *Olivier Catoni*

- `1603.05229v1` - [abs](http://arxiv.org/abs/1603.05229v1) - [pdf](http://arxiv.org/pdf/1603.05229v1)

> The topics dicussed in this paper take their origin inthe estimation of the Gram matrix of a random vector from a sample made of n independent copies. They comprise the estimation of the covariance matrix and the study of least squares regression with a random design. We propose four types of results, based on non-asymptotic PAC-Bayesian generalization bounds: a new robust estimator of the Gram matrix and of the covariance matrix, new results on the empirical Gram matrix, new robust least squares estimators and new results on the ordinary least squares estimator, including its exact rate of convergence under polynomial moment assumptions.

</details>

<details>

<summary>2016-03-17 15:03:10 - Tracking multiple moving objects in images using Markov Chain Monte Carlo</summary>

- *Lan Jiang, Sumeetpal S. Singh*

- `1603.05522v1` - [abs](http://arxiv.org/abs/1603.05522v1) - [pdf](http://arxiv.org/pdf/1603.05522v1)

> A new Bayesian state and parameter learning algorithm for multiple target tracking (MTT) models with image observations is proposed. Specifically, a Markov chain Monte Carlo algorithm is designed to sample from the posterior distribution of the unknown number of targets, their birth and death times, states and model parameters, which constitutes the complete solution to the tracking problem. The conventional approach is to pre-process the images to extract point observations and then perform tracking. We model the image generation process directly to avoid potential loss of information when extracting point observations. Numerical examples show that our algorithm has improved tracking performance over commonly used techniques, for both synthetic examples and real florescent microscopy data, especially in the case of dim targets with overlapping illuminated regions.

</details>

<details>

<summary>2016-03-18 16:47:11 - Approximating Likelihood Ratios with Calibrated Discriminative Classifiers</summary>

- *Kyle Cranmer, Juan Pavez, Gilles Louppe*

- `1506.02169v2` - [abs](http://arxiv.org/abs/1506.02169v2) - [pdf](http://arxiv.org/pdf/1506.02169v2)

> In many fields of science, generalized likelihood ratio tests are established tools for statistical inference. At the same time, it has become increasingly common that a simulator (or generative model) is used to describe complex processes that tie parameters $\theta$ of an underlying theory and measurement apparatus to high-dimensional observations $\mathbf{x}\in \mathbb{R}^p$. However, simulator often do not provide a way to evaluate the likelihood function for a given observation $\mathbf{x}$, which motivates a new class of likelihood-free inference algorithms. In this paper, we show that likelihood ratios are invariant under a specific class of dimensionality reduction maps $\mathbb{R}^p \mapsto \mathbb{R}$. As a direct consequence, we show that discriminative classifiers can be used to approximate the generalized likelihood ratio statistic when only a generative model for the data is available. This leads to a new machine learning-based approach to likelihood-free inference that is complementary to Approximate Bayesian Computation, and which does not require a prior on the model parameters. Experimental results on artificial problems with known exact likelihoods illustrate the potential of the proposed method.

</details>

<details>

<summary>2016-03-19 18:30:02 - The Computational Power of Dynamic Bayesian Networks</summary>

- *Joshua BrulÃ©*

- `1603.06125v1` - [abs](http://arxiv.org/abs/1603.06125v1) - [pdf](http://arxiv.org/pdf/1603.06125v1)

> This paper considers the computational power of constant size, dynamic Bayesian networks. Although discrete dynamic Bayesian networks are no more powerful than hidden Markov models, dynamic Bayesian networks with continuous random variables and discrete children of continuous parents are capable of performing Turing-complete computation. With modified versions of existing algorithms for belief propagation, such a simulation can be carried out in real time. This result suggests that dynamic Bayesian networks may be more powerful than previously considered. Relationships to causal models and recurrent neural networks are also discussed.

</details>

<details>

<summary>2016-03-19 19:15:17 - Spatial Clustering of Curves with Functional Covariates: A Bayesian Partitioning Model with Application to Spectra Radiance in Climate Study</summary>

- *Zhen Zhang, Chae Young Lim, Tapabrata Maiti, Seiji Kato*

- `1604.00059v1` - [abs](http://arxiv.org/abs/1604.00059v1) - [pdf](http://arxiv.org/pdf/1604.00059v1)

> In climate change study, the infrared spectral signatures of climate change have recently been conceptually adopted, and widely applied to identifying and attributing atmospheric composition change. We propose a Bayesian hierarchical model for spatial clustering of the high-dimensional functional data based on the effects of functional covariates and local features. We couple the functional mixed-effects model with a generalized spatial partitioning method for: (1) producing spatially contiguous clusters for the high-dimensional spatio-functional data; (2) improving the computational efficiency via parallel computing over subregions or multi-level partitions; and (3) capturing the near-boundary ambiguity and uncertainty for data-driven partitions. We propose a generalized partitioning method which puts less constraints on the shape of spatial clusters. Dimension reduction in the parameter space is also achieved via Bayesian wavelets to alleviate the increasing model complexity introduced by clusters. The model well captures the regional effects of the atmospheric and cloud properties on the spectral radiance measurements. The results elaborate the importance of exploiting spatially contiguous partitions for identifying regional effects and small-scale variability.

</details>

<details>

<summary>2016-03-19 19:40:07 - Cauchy difference priors for edge-preserving Bayesian inversion with an application to X-ray tomography</summary>

- *Markku Markkanen, Lassi Roininen, Janne M J Huttunen, Sari Lasanen*

- `1603.06135v1` - [abs](http://arxiv.org/abs/1603.06135v1) - [pdf](http://arxiv.org/pdf/1603.06135v1)

> We study Cauchy-distributed difference priors for edge-preserving Bayesian statistical inverse problems. On the contrary to the well-known total variation priors, one-dimensional Cauchy priors are non-Gaussian priors also in the discretization limit. Cauchy priors have independent and identically distributed increments. One-dimensional Cauchy and Gaussian random walks are special cases of L\'evy $\alpha$-stable random walks with $\alpha=1$ and $\alpha=2$, respectively. Both random walks can be written in closed-form, and as priors, they provide smoothing and edge-preserving properties. We briefly discuss also continuous and discrete L\'evy $\alpha$-stable random walks, and generalize the methodology to two-dimensional priors.   We apply the developed algorithm to one-dimensional deconvolution and two-dimensional X-ray tomography problems. We compute conditional mean estimates with single-component Metropolis-Hastings and maximum a posteriori estimates with Gauss-Newton-type optimization method. We compare the proposed tomography reconstruction method to filtered back-projection estimate and conditional mean estimates with Gaussian and total variation priors.

</details>

<details>

<summary>2016-03-20 22:48:09 - Bayesian correction for covariate measurement error: a frequentist evaluation and comparison with regression calibration</summary>

- *Jonathan W. Bartlett, Ruth H. Keogh*

- `1603.06284v1` - [abs](http://arxiv.org/abs/1603.06284v1) - [pdf](http://arxiv.org/pdf/1603.06284v1)

> Bayesian approaches for handling covariate measurement error are well established, and yet arguably are still relatively little used by researchers. For some this is likely due to unfamiliarity or disagreement with the Bayesian inferential paradigm. For others a contributory factor is the inability of standard statistical packages to perform such Bayesian analyses. In this paper we first give an overview of the Bayesian approach to handling covariate measurement error, and contrast it with regression calibration (RC), arguably the most commonly adopted approach. We then argue why the Bayesian approach has a number of statistical advantages compared to RC, and demonstrate that implementing the Bayesian approach is usually quite feasible for the analyst. Next we describe the closely related maximum likelihood and multiple imputation approaches, and explain why we believe the Bayesian approach to generally be preferable. We then empirically compare the frequentist properties of RC and the Bayesian approach through simulation studies. The flexibility of the Bayesian approach to handle both measurement error and missing data is then illustrated through an analysis of data from the Third National Health and Nutrition Examination Survey.

</details>

<details>

<summary>2016-03-21 12:30:02 - Sharp sup-norm Bayesian curve estimation</summary>

- *Catia Scricciolo*

- `1603.06408v1` - [abs](http://arxiv.org/abs/1603.06408v1) - [pdf](http://arxiv.org/pdf/1603.06408v1)

> Sup-norm curve estimation is a fundamental statistical problem and, in principle, a premise for the construction of confidence bands for infinite-dimensional parameters. In a Bayesian framework, the issue of whether the sup-norm-concentration- of-posterior-measure approach proposed by Gin\'e and Nickl (2011), which involves solving a testing problem exploiting concentration properties of kernel and projection-type density estimators around their expectations, can yield minimax-optimal rates is herein settled in the affirmative beyond conjugate-prior settings obtaining sharp rates for common prior-model pairs like random histograms, Dirichlet Gaussian or Laplace mixtures, which can be employed for density, regression or quantile estimation.

</details>

<details>

<summary>2016-03-21 21:18:48 - Sensor Selection for Estimation with Correlated Measurement Noise</summary>

- *Sijia Liu, Sundeep Prabhakar Chepuri, Makan Fardad, Engin Masazade, Geert Leus, Pramod K. Varshney*

- `1508.03690v2` - [abs](http://arxiv.org/abs/1508.03690v2) - [pdf](http://arxiv.org/pdf/1508.03690v2)

> In this paper, we consider the problem of sensor selection for parameter estimation with correlated measurement noise. We seek optimal sensor activations by formulating an optimization problem, in which the estimation error, given by the trace of the inverse of the Bayesian Fisher information matrix, is minimized subject to energy constraints. Fisher information has been widely used as an effective sensor selection criterion. However, existing information-based sensor selection methods are limited to the case of uncorrelated noise or weakly correlated noise due to the use of approximate metrics. By contrast, here we derive the closed form of the Fisher information matrix with respect to sensor selection variables that is valid for any arbitrary noise correlation regime, and develop both a convex relaxation approach and a greedy algorithm to find near-optimal solutions. We further extend our framework of sensor selection to solve the problem of sensor scheduling, where a greedy algorithm is proposed to determine non-myopic (multi-time step ahead) sensor schedules. Lastly, numerical results are provided to illustrate the effectiveness of our approach, and to reveal the effect of noise correlation on estimation performance.

</details>

<details>

<summary>2016-03-22 14:55:29 - Patterns of Scalable Bayesian Inference</summary>

- *Elaine Angelino, Matthew James Johnson, Ryan P. Adams*

- `1602.05221v2` - [abs](http://arxiv.org/abs/1602.05221v2) - [pdf](http://arxiv.org/pdf/1602.05221v2)

> Datasets are growing not just in size but in complexity, creating a demand for rich models and quantification of uncertainty. Bayesian methods are an excellent fit for this demand, but scaling Bayesian inference is a challenge. In response to this challenge, there has been considerable recent work based on varying assumptions about model structure, underlying computational resources, and the importance of asymptotic correctness. As a result, there is a zoo of ideas with few clear overarching principles.   In this paper, we seek to identify unifying principles, patterns, and intuitions for scaling Bayesian inference. We review existing work on utilizing modern computing resources with both MCMC and variational approximation techniques. From this taxonomy of ideas, we characterize the general principles that have proven successful for designing scalable inference procedures and comment on the path forward.

</details>

<details>

<summary>2016-03-22 16:54:36 - Stopping criteria for boosting automatic experimental design using real-time fMRI with Bayesian optimization</summary>

- *Romy Lorenz, Ricardo P Monti, Ines R Violante, Aldo A Faisal, Christoforos Anagnostopoulos, Robert Leech, Giovanni Montana*

- `1511.07827v2` - [abs](http://arxiv.org/abs/1511.07827v2) - [pdf](http://arxiv.org/pdf/1511.07827v2)

> Bayesian optimization has been proposed as a practical and efficient tool through which to tune parameters in many difficult settings. Recently, such techniques have been combined with real-time fMRI to propose a novel framework which turns on its head the conventional functional neuroimaging approach. This closed-loop method automatically designs the optimal experiment to evoke a desired target brain pattern. One of the challenges associated with extending such methods to real-time brain imaging is the need for adequate stopping criteria, an aspect of Bayesian optimization which has received limited attention. In light of high scanning costs and limited attentional capacities of subjects an accurate and reliable stopping criteria is essential. In order to address this issue we propose and empirically study the performance of two stopping criteria.

</details>

<details>

<summary>2016-03-22 18:28:09 - Edge-exchangeable graphs and sparsity</summary>

- *Tamara Broderick, Diana Cai*

- `1603.06898v1` - [abs](http://arxiv.org/abs/1603.06898v1) - [pdf](http://arxiv.org/pdf/1603.06898v1)

> A known failing of many popular random graph models is that the Aldous-Hoover Theorem guarantees these graphs are dense with probability one; that is, the number of edges grows quadratically with the number of nodes. This behavior is considered unrealistic in observed graphs. We define a notion of edge exchangeability for random graphs in contrast to the established notion of infinite exchangeability for random graphs --- which has traditionally relied on exchangeability of nodes (rather than edges) in a graph. We show that, unlike node exchangeability, edge exchangeability encompasses models that are known to provide a projective sequence of random graphs that circumvent the Aldous-Hoover Theorem and exhibit sparsity, i.e., sub-quadratic growth of the number of edges with the number of nodes. We show how edge-exchangeability of graphs relates naturally to existing notions of exchangeability from clustering (a.k.a. partitions) and other familiar combinatorial structures.

</details>

<details>

<summary>2016-03-23 11:28:15 - Comparison of Bayesian predictive methods for model selection</summary>

- *Juho Piironen, Aki Vehtari*

- `1503.08650v4` - [abs](http://arxiv.org/abs/1503.08650v4) - [pdf](http://arxiv.org/pdf/1503.08650v4)

> The goal of this paper is to compare several widely used Bayesian model selection methods in practical model selection problems, highlight their differences and give recommendations about the preferred approaches. We focus on the variable subset selection for regression and classification and perform several numerical experiments using both simulated and real world data. The results show that the optimization of a utility estimate such as the cross-validation (CV) score is liable to finding overfitted models due to relatively high variance in the utility estimates when the data is scarce. This can also lead to substantial selection induced bias and optimism in the performance evaluation for the selected model. From a predictive viewpoint, best results are obtained by accounting for model uncertainty by forming the full encompassing model, such as the Bayesian model averaging solution over the candidate models. If the encompassing model is too complex, it can be robustly simplified by the projection method, in which the information of the full model is projected onto the submodels. This approach is substantially less prone to overfitting than selection based on CV-score. Overall, the projection method appears to outperform also the maximum a posteriori model and the selection of the most probable variables. The study also demonstrates that the model selection can greatly benefit from using cross-validation outside the searching process both for guiding the model size selection and assessing the predictive performance of the finally selected model.

</details>

<details>

<summary>2016-03-23 13:37:43 - A Bayesian information criterion for singular models</summary>

- *Mathias Drton, Martyn Plummer*

- `1309.0911v3` - [abs](http://arxiv.org/abs/1309.0911v3) - [pdf](http://arxiv.org/pdf/1309.0911v3)

> We consider approximate Bayesian model choice for model selection problems that involve models whose Fisher-information matrices may fail to be invertible along other competing submodels. Such singular models do not obey the regularity conditions underlying the derivation of Schwarz's Bayesian information criterion (BIC) and the penalty structure in BIC generally does not reflect the frequentist large-sample behavior of their marginal likelihood. While large-sample theory for the marginal likelihood of singular models has been developed recently, the resulting approximations depend on the true parameter value and lead to a paradox of circular reasoning. Guided by examples such as determining the number of components of mixture models, the number of factors in latent factor models or the rank in reduced-rank regression, we propose a resolution to this paradox and give a practical extension of BIC for singular model selection problems.

</details>

<details>

<summary>2016-03-24 03:58:32 - Penalized Weighted Least Squares for Outlier Detection and Robust Regression</summary>

- *Xiaoli Gao, Yixin Fang*

- `1603.07427v1` - [abs](http://arxiv.org/abs/1603.07427v1) - [pdf](http://arxiv.org/pdf/1603.07427v1)

> To conduct regression analysis for data contaminated with outliers, many approaches have been proposed for simultaneous outlier detection and robust regression, so is the approach proposed in this manuscript. This new approach is called "penalized weighted least squares" (PWLS). By assigning each observation an individual weight and incorporating a lasso-type penalty on the log-transformation of the weight vector, the PWLS is able to perform outlier detection and robust regression simultaneously. A Bayesian point-of-view of the PWLS is provided, and it is showed that the PWLS can be seen as an example of M-estimation. Two methods are developed for selecting the tuning parameter in the PWLS. The performance of the PWLS is demonstrated via simulations and real applications.

</details>

<details>

<summary>2016-03-24 14:26:50 - Borrowing strengh in hierarchical Bayes: Posterior concentration of the Dirichlet base measure</summary>

- *XuanLong Nguyen*

- `1301.0802v4` - [abs](http://arxiv.org/abs/1301.0802v4) - [pdf](http://arxiv.org/pdf/1301.0802v4)

> This paper studies posterior concentration behavior of the base probability measure of a Dirichlet measure, given observations associated with the sampled Dirichlet processes, as the number of observations tends to infinity. The base measure itself is endowed with another Dirichlet prior, a construction known as the hierarchical Dirichlet processes (Teh et al. [J. Amer. Statist. Assoc. 101 (2006) 1566-1581]). Convergence rates are established in transportation distances (i.e., Wasserstein metrics) under various conditions on the geometry of the support of the true base measure. As a consequence of the theory, we demonstrate the benefit of "borrowing strength" in the inference of multiple groups of data - a powerful insight often invoked to motivate hierarchical modeling. In certain settings, the gain in efficiency due to the latent hierarchy can be dramatic, improving from a standard nonparametric rate to a parametric rate of convergence. Tools developed include transportation distances for nonparametric Bayesian hierarchies of random measures, the existence of tests for Dirichlet measures, and geometric properties of the support of Dirichlet measures.

</details>

<details>

<summary>2016-03-27 02:34:02 - Regularization Parameter Selection for a Bayesian Multi-Level Group Lasso Regression Model with Application to Imaging Genomics</summary>

- *Farouk S. Nathoo, Keelin Greenlaw, Mary Lesperance*

- `1603.08163v1` - [abs](http://arxiv.org/abs/1603.08163v1) - [pdf](http://arxiv.org/pdf/1603.08163v1)

> We investigate the choice of tuning parameters for a Bayesian multi-level group lasso model developed for the joint analysis of neuroimaging and genetic data. The regression model we consider relates multivariate phenotypes consisting of brain summary measures (volumetric and cortical thickness values) to single nucleotide polymorphism (SNPs) data and imposes penalization at two nested levels, the first corresponding to genes and the second corresponding to SNPs. Associated with each level in the penalty is a tuning parameter which corresponds to a hyperparameter in the hierarchical Bayesian formulation. Following previous work on Bayesian lassos we consider the estimation of tuning parameters through either hierarchical Bayes based on hyperpriors and Gibbs sampling or through empirical Bayes based on maximizing the marginal likelihood using a Monte Carlo EM algorithm. For the specific model under consideration we find that these approaches can lead to severe overshrinkage of the regression parameter estimates in the high-dimensional setting or when the genetic effects are weak. We demonstrate these problems through simulation examples and study an approximation to the marginal likelihood which sheds light on the cause of this problem. We then suggest an alternative approach based on the widely applicable information criterion (WAIC), an asymptotic approximation to leave-one-out cross-validation that can be computed conveniently within an MCMC framework.

</details>

<details>

<summary>2016-03-28 18:05:44 - Bayesian modeling of networks in complex business intelligence problems</summary>

- *Daniele Durante, Sally Paganin, Bruno Scarpa, David B. Dunson*

- `1510.00646v2` - [abs](http://arxiv.org/abs/1510.00646v2) - [pdf](http://arxiv.org/pdf/1510.00646v2)

> Complex network data problems are increasingly common in many fields of application. Our motivation is drawn from strategic marketing studies monitoring customer choices of specific products, along with co-subscription networks encoding multiple purchasing behavior. Data are available for several agencies within the same insurance company, and our goal is to efficiently exploit co-subscription networks to inform targeted advertising of cross-sell strategies to currently mono-product customers. We address this goal by developing a Bayesian hierarchical model, which clusters agencies according to common mono-product customer choices and co-subscription networks. Within each cluster, we efficiently model customer behavior via a cluster-dependent mixture of latent eigenmodels. This formulation provides key information on mono-product customer choices and multiple purchasing behavior within each cluster, informing targeted cross-sell strategies. We develop simple algorithms for tractable inference, and assess performance in simulations and an application to business intelligence.

</details>

<details>

<summary>2016-03-29 13:30:30 - Nonparametric Bayesian posterior contraction rates for discretely observed scalar diffusions</summary>

- *Richard Nickl, Jakob SÃ¶hl*

- `1510.05526v2` - [abs](http://arxiv.org/abs/1510.05526v2) - [pdf](http://arxiv.org/pdf/1510.05526v2)

> We consider nonparametric Bayesian inference in a reflected diffusion model $dX_t = b (X_t)dt + \sigma(X_t) dW_t,$ with discretely sampled observations $X_0, X_\Delta, \dots, X_{n\Delta}$. We analyse the nonlinear inverse problem corresponding to the `low frequency sampling' regime where $\Delta>0$ is fixed and $n \to \infty$. A general theorem is proved that gives conditions for prior distributions $\Pi$ on the diffusion coefficient $\sigma$ and the drift function $b$ that ensure minimax optimal contraction rates of the posterior distribution over H\"older-Sobolev smoothness classes. These conditions are verified for natural examples of nonparametric random wavelet series priors. For the proofs we derive new concentration inequalities for empirical processes arising from discretely observed diffusions that are of independent interest.

</details>

<details>

<summary>2016-03-29 22:41:17 - Towards Practical Bayesian Parameter and State Estimation</summary>

- *Yusuf Bugra Erol, Yi Wu, Lei Li, Stuart Russell*

- `1603.08988v1` - [abs](http://arxiv.org/abs/1603.08988v1) - [pdf](http://arxiv.org/pdf/1603.08988v1)

> Joint state and parameter estimation is a core problem for dynamic Bayesian networks. Although modern probabilistic inference toolkits make it relatively easy to specify large and practically relevant probabilistic models, the silver bullet---an efficient and general online inference algorithm for such problems---remains elusive, forcing users to write special-purpose code for each application. We propose a novel blackbox algorithm -- a hybrid of particle filtering for state variables and assumed density filtering for parameter variables. It has following advantages: (a) it is efficient due to its online nature, and (b) it is applicable to both discrete and continuous parameter spaces . On a variety of toy and real models, our system is able to generate more accurate results within a fixed computation budget. This preliminary evidence indicates that the proposed approach is likely to be of practical use.

</details>

<details>

<summary>2016-03-30 00:11:38 - Uniform convergence over time of a nested particle filtering scheme for recursive parameter estimation in state--space Markov models</summary>

- *Dan Crisan, Joaquin Miguez*

- `1603.09005v1` - [abs](http://arxiv.org/abs/1603.09005v1) - [pdf](http://arxiv.org/pdf/1603.09005v1)

> We analyse the performance of a recursive Monte Carlo method for the Bayesian estimation of the static parameters of a discrete--time state--space Markov model. The algorithm employs two layers of particle filters to approximate the posterior probability distribution of the model parameters. In particular, the first layer yields an empirical distribution of samples on the parameter space, while the filters in the second layer are auxiliary devices to approximate the (analytically intractable) likelihood of the parameters. This approach relates the this algorithm to the recent sequential Monte Carlo square (SMC$^2$) method, which provides a {\em non-recursive} solution to the same problem. In this paper, we investigate the approximation, via the proposed scheme, of integrals of real bounded functions with respect to the posterior distribution of the system parameters. Under assumptions related to the compactness of the parameter support and the stability and continuity of the sequence of posterior distributions for the state--space model, we prove that the $L_p$ norms of the approximation errors vanish asymptotically (as the number of Monte Carlo samples generated by the algorithm increases) and uniformly over time. We also prove that, under the same assumptions, the proposed scheme can asymptotically identify the parameter values for a class of models. We conclude the paper with a numerical example that illustrates the uniform convergence results by exploring the accuracy and stability of the proposed algorithm operating with long sequences of observations.

</details>

<details>

<summary>2016-03-30 06:21:42 - Robustness of Bayesian Pool-based Active Learning Against Prior Misspecification</summary>

- *Nguyen Viet Cuong, Nan Ye, Wee Sun Lee*

- `1603.09050v1` - [abs](http://arxiv.org/abs/1603.09050v1) - [pdf](http://arxiv.org/pdf/1603.09050v1)

> We study the robustness of active learning (AL) algorithms against prior misspecification: whether an algorithm achieves similar performance using a perturbed prior as compared to using the true prior. In both the average and worst cases of the maximum coverage setting, we prove that all $\alpha$-approximate algorithms are robust (i.e., near $\alpha$-approximate) if the utility is Lipschitz continuous in the prior. We further show that robustness may not be achieved if the utility is non-Lipschitz. This suggests we should use a Lipschitz utility for AL if robustness is required. For the minimum cost setting, we can also obtain a robustness result for approximate AL algorithms. Our results imply that many commonly used AL algorithms are robust against perturbed priors. We then propose the use of a mixture prior to alleviate the problem of prior misspecification. We analyze the robustness of the uniform mixture prior and show experimentally that it performs reasonably well in practice.

</details>

<details>

<summary>2016-03-31 02:23:24 - A Stratified Analysis of Bayesian Optimization Methods</summary>

- *Ian Dewancker, Michael McCourt, Scott Clark, Patrick Hayes, Alexandra Johnson, George Ke*

- `1603.09441v1` - [abs](http://arxiv.org/abs/1603.09441v1) - [pdf](http://arxiv.org/pdf/1603.09441v1)

> Empirical analysis serves as an important complement to theoretical analysis for studying practical Bayesian optimization. Often empirical insights expose strengths and weaknesses inaccessible to theoretical analysis. We define two metrics for comparing the performance of Bayesian optimization methods and propose a ranking mechanism for summarizing performance within various genres or strata of test functions. These test functions serve to mimic the complexity of hyperparameter optimization problems, the most prominent application of Bayesian optimization, but with a closed form which allows for rapid evaluation and more predictable behavior. This offers a flexible and efficient way to investigate functions with specific properties of interest, such as oscillatory behavior or an optimum on the domain boundary.

</details>


## 2016-04

<details>

<summary>2016-04-01 07:18:49 - Censored and shifted gamma distribution based EMOS model for probabilistic quantitative precipitation forecasting</summary>

- *SÃ¡ndor Baran, DÃ³ra Nemoda*

- `1512.04068v3` - [abs](http://arxiv.org/abs/1512.04068v3) - [pdf](http://arxiv.org/pdf/1512.04068v3)

> Recently all major weather prediction centres provide forecast ensembles of different weather quantities which are obtained from multiple runs of numerical weather prediction models with various initial conditions and model parametrizations. However, ensemble forecasts often show an underdispersive character and may also be biased, so that some post-processing is needed to account for these deficiencies. Probably the most popular modern post-processing techniques are the ensemble model output statistics (EMOS) and the Bayesian model averaging (BMA) which provide estimates of the density of the predictable weather quantity.   In the present work an EMOS method for calibrating ensemble forecasts of precipitation accumulation is proposed, where the predictive distribution follows a censored and shifted gamma (CSG) law with parameters depending on the ensemble members. The CSG EMOS model is tested on ensemble forecasts of 24 h precipitation accumulation of the eight-member University of Washington mesoscale ensemble and on the 11 member ensemble produced by the operational Limited Area Model Ensemble Prediction System of the Hungarian Meteorological Service. The predictive performance of the new EMOS approach is compared with the fit of the raw ensemble, the generalized extreme value (GEV) distribution based EMOS model and the gamma BMA method. According to the results, the proposed CSG EMOS model slightly outperforms the GEV EMOS approach in terms of calibration of probabilistic and accuracy of point forecasts and shows significantly better predictive skill that the raw ensemble and the BMA model.

</details>

<details>

<summary>2016-04-01 14:58:03 - On an adaptive preconditioned Crank-Nicolson MCMC algorithm for infinite dimensional Bayesian inferences</summary>

- *Zixi Hu, Zhewei Yao, Jinglai Li*

- `1511.05838v3` - [abs](http://arxiv.org/abs/1511.05838v3) - [pdf](http://arxiv.org/pdf/1511.05838v3)

> Many scientific and engineering problems require to perform Bayesian inferences for unknowns of infinite dimension. In such problems, many standard Markov Chain Monte Carlo (MCMC) algorithms become arbitrary slow under the mesh refinement, which is referred to as being dimension dependent. To this end, a family of dimensional independent MCMC algorithms, known as the preconditioned Crank-Nicolson (pCN) methods, were proposed to sample the infinite dimensional parameters. In this work we develop an adaptive version of the pCN algorithm, where the covariance operator of the proposal distribution is adjusted based on sampling history to improve the simulation efficiency. We show that the proposed algorithm satisfies an important ergodicity condition under some mild assumptions. Finally we provide numerical examples to demonstrate the performance of the proposed method.

</details>

<details>

<summary>2016-04-01 19:37:31 - Inferring network structure in non-normal and mixed discrete-continuous genomic data</summary>

- *Anindya Bhadra, Arvind Rao, Veerabhadran Baladandayuthapani*

- `1604.00376v1` - [abs](http://arxiv.org/abs/1604.00376v1) - [pdf](http://arxiv.org/pdf/1604.00376v1)

> Inferring dependence structure through undirected graphs is crucial for uncovering the major modes of multivariate interaction among high-dimensional genomic markers that are potentially associated with cancer. Traditionally, conditional independence has been studied using sparse Gaussian graphical models for continuous data and sparse Ising models for discrete data. However, there are two clear situations when these approaches are inadequate. The first occurs when the data are continuous but display non-normal marginal behavior such as heavy tails or skewness, rendering an assumption of normality inappropriate. The second occurs when a part of the data is ordinal or discrete (e.g., presence or absence of a mutation) and the other part is continuous (e.g., expression levels of genes or proteins). In this case, the existing Bayesian approaches typically employ a latent variable framework for the discrete part that precludes inferring conditional independence among the data that are actually observed. The current article overcomes these two challenges in a unified framework using Gaussian scale mixtures. Our framework is able to handle continuous data that are not normal and data that are of mixed continuous and discrete nature, while still being able to infer a sparse conditional sign independence structure among the observed data. Extensive performance comparison in simulations with alternative techniques and an analysis of a real cancer genomics data set demonstrate the effectiveness of the proposed approach.

</details>

<details>

<summary>2016-04-02 23:00:07 - Online EM for Functional Data</summary>

- *Florian Maire, Eric Moulines, Sidonie Lefebvre*

- `1604.00570v1` - [abs](http://arxiv.org/abs/1604.00570v1) - [pdf](http://arxiv.org/pdf/1604.00570v1)

> A novel approach to perform unsupervised sequential learning for functional data is proposed. Our goal is to extract reference shapes (referred to as templates) from noisy, deformed and censored realizations of curves and images. Our model generalizes the Bayesian dense deformable template model (Allassonni\`ere et al., 2007), a hierarchical model in which the template is the function to be estimated and the deformation is a nuisance, assumed to be random with a known prior distribution. The templates are estimated using a Monte Carlo version of the online Expectation-Maximization algorithm, extending the work from Capp\'e and Moulines (2009). Our sequential inference framework is significantly more computationally efficient than equivalent batch learning algorithms, especially when the missing data is high-dimensional. Some numerical illustrations on curve registration problem and templates extraction from images are provided to support our findings.

</details>

<details>

<summary>2016-04-02 23:08:31 - Spatio-temporal Modelling of Temperature Fields in the Pacific Northwest</summary>

- *Camila M. Casquilho-Resende, Nhu D. Le, James V. Zidek*

- `1604.00572v1` - [abs](http://arxiv.org/abs/1604.00572v1) - [pdf](http://arxiv.org/pdf/1604.00572v1)

> The importance of modelling temperature fields goes beyond the need to understand a region's climate and serves too as a starting point for understanding their socioeconomic, and health consequences. The topography of the study region contributes much to the complexity of modelling these fields and demands flexible spatio-temporal models that are able to handle nonstationarity and changes in trend. In this paper, we develop a flexible stochastic spatio-temporal model for daily temperatures in the Pacific Northwest, and describe a methodology for performing Bayesian spatial prediction. A novel aspect of this model, an extension of the spatio-temporal model proposed in Le and Zidek (1992), is its incorporation of site-specific features of a spatio-temporal field in its spatio-temporal mean. Due to the often surprising Pacific Northwestern weather, the analysis reported in the paper shows the need to incorporate spatio-temporal interactions in that mean in order to understand the rapid changes in temperature observed in nearby locations and to get approximately stationary residuals for higher level analysis. No structure is assumed for the spatial covariance matrix of these residuals, thus allowing the model to capture any nonstationary spatial structures remaining in those residuals.

</details>

<details>

<summary>2016-04-03 19:56:59 - Statistically-estimated tree composition for the northeastern United States at the time of Euro-American settlement</summary>

- *Christopher J. Paciorek, Simon J. Goring, Andrew L. Thurman, Charles V. Cogbill, John W. Williams, David J. Mladenoff, Jody A. Peters, Jun Zhu, Jason S. McLachlan*

- `1508.07509v2` - [abs](http://arxiv.org/abs/1508.07509v2) - [pdf](http://arxiv.org/pdf/1508.07509v2)

> We present a gridded 8 km-resolution data product of the estimated composition of tree taxa at the time of Euro-American settlement of the northeastern United States and the statistical methodology used to produce the product from trees recorded by land surveyors. Composition is defined as the proportion of stems larger than approximately 20 cm diameter at breast height for 22 tree taxa, generally at the genus level. The data come from settlement-era public survey records that are transcribed and then aggregated spatially, giving count data. The domain is divided into two regions, eastern (Maine to Ohio) and midwestern (Indiana to Minnesota). Public Land Survey point data in the midwestern region (ca. 0.8-km resolution) are aggregated to a regular 8 km grid, while data in the eastern region, from Town Proprietor Surveys, are aggregated at the township level in irregularly-shaped local administrative units. The product is based on a Bayesian statistical model fit to the count data that estimates composition on a regular 8 km grid across the entire domain. The statistical model is designed to handle data from both the regular grid and the irregularly-shaped townships and allows us to estimate composition at locations with no data and to smooth over noise caused by limited counts in locations with data. The model also allows us to quantify uncertainty in our composition estimates, making the product suitable for applications employing data assimilation. We expect this data product to be useful for understanding the state of vegetation in the northeastern United States prior to large-scale Euro-American settlement. In addition to specific regional questions, the data product can also serve as a baseline against which to investigate how forests and ecosystems change after intensive settlement. The data product is available at the NIS data portal as version 1.0.

</details>

<details>

<summary>2016-04-04 20:59:56 - On Bayesian robust regression with diverging number of predictors</summary>

- *Daniel Nevo, Ya'acov Ritov*

- `1507.02074v2` - [abs](http://arxiv.org/abs/1507.02074v2) - [pdf](http://arxiv.org/pdf/1507.02074v2)

> This paper concerns the robust regression model when the number of predictors and the number of observations grow in a similar rate. Theory for M-estimators in this regime has been recently developed by several authors [El Karoui et al., 2013, Bean et al., 2013, Donoho and Montanari, 2013].   Motivated by the inability of M-estimators to successfully estimate the Euclidean norm of the coefficient vector, we consider a Bayesian framework for this model. We suggest a two-component mixture of normals prior for the coefficients and develop a Gibbs sampler procedure for sampling from relevant posterior distributions, while utilizing a scale mixture of normal representation for the error distribution . Unlike M-estimators, the proposed Bayes estimator is consistent in the Euclidean norm sense. Simulation results demonstrate the superiority of the Bayes estimator over traditional estimation methods.

</details>

<details>

<summary>2016-04-04 21:06:40 - Bayesian Local Extrema Splines</summary>

- *Matthew W. Wheeler, David B. Dunson, Amy H. Herring*

- `1604.01064v1` - [abs](http://arxiv.org/abs/1604.01064v1) - [pdf](http://arxiv.org/pdf/1604.01064v1)

> We consider the problem of shape restricted nonparametric regression on a closed set X ?\in R; where it is reasonable to assume the function has no more than H local extrema interior to X: Following a Bayesian approach we develop a nonparametric prior over a novel class of local extrema splines. This approach is shown to be consistent when modeling any continuously differentiable function within the class of functions considered, and is used to develop methods for hypothesis testing on the shape of the curve. Sampling algorithms are developed, and the method is applied in simulation studies and data examples where the shape of the curve is of interest.

</details>

<details>

<summary>2016-04-04 21:49:22 - A Dynamic Bayesian Network Model for Inventory Level Estimation in Retail Marketing</summary>

- *Luis I. Reyes-Castro, Andres G. Abad*

- `1604.01075v1` - [abs](http://arxiv.org/abs/1604.01075v1) - [pdf](http://arxiv.org/pdf/1604.01075v1)

> Many retailers today employ inventory management systems based on Re-Order Point Policies, most of which rely on the assumption that all decreases in product inventory levels result from product sales. Unfortunately, it usually happens that small but random quantities of the product get lost, stolen or broken without record as time passes, e.g., as a consequence of shoplifting. This is usual for retailers handling large varieties of inexpensive products, e.g., grocery stores. In turn, over time these discrepancies lead to stock freezing problems, i.e., situations where the system believes the stock is above the re-order point but the actual stock is at zero, and so no replenishments or sales occur. Motivated by these issues, we model the interaction between sales, losses, replenishments and inventory levels as a Dynamic Bayesian Network (DBN), where the inventory levels are unobserved (i.e., hidden) variables we wish to estimate. We present an Expectation-Maximization (EM) algorithm to estimate the parameters of the sale and loss distributions, which relies on solving a one-dimensional dynamic program for the E-step and on solving two separate one-dimensional nonlinear programs for the M-step.

</details>

<details>

<summary>2016-04-04 22:25:05 - Genetic variant selection: learning across traits and sites</summary>

- *Laurel Stell, Chiara Sabatti*

- `1504.00946v4` - [abs](http://arxiv.org/abs/1504.00946v4) - [pdf](http://arxiv.org/pdf/1504.00946v4)

> We consider resequencing studies of associated loci and the problem of prioritizing sequence variants for functional follow-up. Working within the multivariate linear regression framework helps us to account for correlation across variants, and adopting a Bayesian approach naturally leads to posterior probabilities that incorporate all information about the variants' function. We describe two novel prior distributions that facilitate learning the role of each variant by borrowing evidence across phenotypes and across mutations in the same gene. We illustrate their potential advantages with simulations and re-analyzing a dataset of sequencing variants.

</details>

<details>

<summary>2016-04-05 14:21:14 - Bayesian Estimation of the Threshold of a Generalised Pareto Distribution for Heavy-Tailed Observations</summary>

- *Cristiano Villa*

- `1604.01268v1` - [abs](http://arxiv.org/abs/1604.01268v1) - [pdf](http://arxiv.org/pdf/1604.01268v1)

> In this paper, we discuss a method to define prior distributions for the threshold of a generalised Pareto distribution, in particular when its applications are directed to heavy-tailed data. We propose to assign prior probabilities to the order statistics of a given set of observations. In other words, we assume that the threshold coincides to one of the data points. We show two ways of defining a prior: by assigning equal mass to each order statistic, that is a uniform prior, and by considering the worth that every order statistic has in representing the true threshold. Both proposed priors represent a scenario of minimal information, and we study their adequacy through simulation exercises and by analysing two applications from insurance and from finance.

</details>

<details>

<summary>2016-04-05 17:53:59 - Bayesian Optimization with Exponential Convergence</summary>

- *Kenji Kawaguchi, Leslie Pack Kaelbling, TomÃ¡s Lozano-PÃ©rez*

- `1604.01348v1` - [abs](http://arxiv.org/abs/1604.01348v1) - [pdf](http://arxiv.org/pdf/1604.01348v1)

> This paper presents a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the delta-cover sampling. Most Bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing Bayesian optimization method with exponential convergence requires access to the delta-cover sampling, which was considered to be impractical. Our approach eliminates both requirements and achieves an exponential convergence rate.

</details>

<details>

<summary>2016-04-05 22:35:22 - Analysis of distributional variation through multi-scale Beta-Binomial modeling</summary>

- *Li Ma, Jacopo Soriano*

- `1604.01443v1` - [abs](http://arxiv.org/abs/1604.01443v1) - [pdf](http://arxiv.org/pdf/1604.01443v1)

> Many statistical analyses involve the comparison of multiple data sets collected under different conditions in order to identify the difference in the underlying distributions. A common challenge in multi-sample comparison is the presence of various confounders, or extraneous causes other than the conditions of interest that also contribute to the difference across the distributions. They result in false findings, i.e., identified differences that are not replicable in follow-up investigations. We consider an ANOVA approach to addressing this issue in multi-sample comparison---by collecting replicate data sets under each condition, thereby allowing the identification of the interesting distributional variation from the extraneous ones. We introduce a multi-scale Bayesian hierarchical model for the analysis of distributional variation (ANDOVA) under this design, based on a collection of Beta-Binomial tests targeting variations of different scales at different locations across the sample space. Instead treating the tests independently, the model employs a graphical structure to introduce dependency among the individual tests thereby allowing borrowing of strength among them. We derive efficient inference recipe through a combination of numerical integration and message passing, and evaluate the ability of our method to effectively address ANDOVA through extensive simulation. We utilize our method to analyze a DNase-seq data set for identifying differences in transcriptional factor binding.

</details>

<details>

<summary>2016-04-06 04:13:51 - Bayesian Estimators for Small Area Models Shrinking Both Means and Variances</summary>

- *Shonosuke Sugasawa, Hiromasa Tamae, Tatsuya Kubokawa*

- `1507.05179v3` - [abs](http://arxiv.org/abs/1507.05179v3) - [pdf](http://arxiv.org/pdf/1507.05179v3)

> For small area estimation of area-level data, the Fay-Herriot model is extensively used as a model based method. In the Fay-Herriot model, it is conventionally assumed that the sampling variances are known whereas estimators of sampling variances are used in practice. Thus, the settings of knowing sampling variances are unrealistic and several methods are proposed to overcome this problem. In this paper, we assume the situation where the direct estimators of the sampling variances are available as well as the sample means. Using these information, we propose a Bayesian yet objective method producing shrinkage estimation of both means and variances in the Fay-Herriot model. We consider the hierarchical structure for the sampling variances and we set uniform prior on model parameters to keep objectivity of the proposed model. For validity of the posterior inference, we show under mild conditions that the posterior distribution is proper and has finite variances. We investigate the numerical performance through simulation and empirical studies.

</details>

<details>

<summary>2016-04-06 04:46:59 - Fast Approximate Inference for Arbitrarily Large Semiparametric Regression Models via Message Passing</summary>

- *M. P. Wand*

- `1602.07412v2` - [abs](http://arxiv.org/abs/1602.07412v2) - [pdf](http://arxiv.org/pdf/1602.07412v2)

> We show how the notion of message passing can be used to streamline the algebra and computer coding for fast approximate inference in large Bayesian semiparametric regression models. In particular, this approach is amenable to handling arbitrarily large models of particular types once a set of primitive operations is established. The approach is founded upon a message passing formulation of mean field variational Bayes that utilizes factor graph representations of statistical models. The underlying principles apply to general Bayesian hierarchical models although we focus on semiparametric regression. The notion of factor graph fragments is introduced and is shown to facilitate compartmentalization of the required algebra and coding. The resultant algorithms have ready-to-implement closed form expressions and allow a broad class of arbitrarily large semiparametric regression models to be handled. Ongoing software projects such as Infer.NET and Stan support variational-type inference for particular model classes. This article is not concerned with software packages per se and focuses on the underlying tenets of scalable variational inference algorithms.

</details>

<details>

<summary>2016-04-06 20:01:28 - Safe Probability</summary>

- *Peter GrÃ¼nwald*

- `1604.01785v1` - [abs](http://arxiv.org/abs/1604.01785v1) - [pdf](http://arxiv.org/pdf/1604.01785v1)

> We formalize the idea of probability distributions that lead to reliable predictions about some, but not all aspects of a domain. The resulting notion of `safety' provides a fresh perspective on foundational issues in statistics, providing a middle ground between imprecise probability and multiple-prior models on the one hand and strictly Bayesian approaches on the other. It also allows us to formalize fiducial distributions in terms of the set of random variables that they can safely predict, thus taking some of the sting out of the fiducial idea. By restricting probabilistic inference to safe uses, one also automatically avoids paradoxes such as the Monty Hall problem. Safety comes in a variety of degrees, such as "validity" (the strongest notion), "calibration", "confidence safety" and "unbiasedness" (almost the weakest notion).

</details>

<details>

<summary>2016-04-07 08:39:29 - Robust Modeling Using Non-Elliptically Contoured Multivariate t Distributions</summary>

- *Zhichao Jiang, Peng Ding*

- `1604.01919v1` - [abs](http://arxiv.org/abs/1604.01919v1) - [pdf](http://arxiv.org/pdf/1604.01919v1)

> Models based on multivariate t distributions are widely applied to analyze data with heavy tails. However, all the marginal distributions of the multivariate t distributions are restricted to have the same degrees of freedom, making these models unable to describe different marginal heavy-tailedness. We generalize the traditional multivariate t distributions to non-elliptically contoured multivariate t distributions, allowing for different marginal degrees of freedom. We apply the non-elliptically contoured multivariate t distributions to three widely-used models: the Heckman selection model with different degrees of freedom for selection and outcome equations, the multivariate Robit model with different degrees of freedom for marginal responses, and the linear mixed-effects model with different degrees of freedom for random effects and within-subject errors. Based on the Normal mixture representation of our t distribution, we propose efficient Bayesian inferential procedures for the model parameters based on data augmentation and parameter expansion. We show via simulation studies and real examples that the conclusions are sensitive to the existence of different marginal heavy-tailedness.

</details>

<details>

<summary>2016-04-07 12:56:50 - A Property of the Kullback--Leibler Divergence for Location-scale Models</summary>

- *Cristiano Villa*

- `1604.01983v1` - [abs](http://arxiv.org/abs/1604.01983v1) - [pdf](http://arxiv.org/pdf/1604.01983v1)

> In this paper, we discuss a property of the Kullback--Leibler divergence measured between two models of the family of the location-scale distributions. We show that, if model $M_1$ and model $M_2$ are represented by location-scale distributions, then the minimum Kullback--Leibler divergence from $M_1$ to $M_2$, with respect to the parameters of $M_2$, is independent from the value of the parameters of $M_1$. Furthermore, we show that the property holds for models that can be transformed into location-scale distributions. We illustrate a possible application of the property in objective Bayesian model selection.

</details>

<details>

<summary>2016-04-07 13:40:18 - CopulaDTA: An R Package for Copula Based Bivariate Beta-Binomial Models for Diagnostic Test Accuracy Studies in a Bayesian Framework</summary>

- *Victoria N Nyaga, Marc Arbyn, Marc Aerts*

- `1604.01996v1` - [abs](http://arxiv.org/abs/1604.01996v1) - [pdf](http://arxiv.org/pdf/1604.01996v1)

> The current statistical procedures implemented in statistical software packages for pooling of diagnostic test accuracy data include hSROC regression and the bivariate random-effects meta-analysis model (BRMA). However, these models do not report the overall mean but rather the mean for a central study with random-effect equal to zero and have difficulties estimating the correlation between sensitivity and specificity when the number of studies in the meta-analysis is small and/or when the between-study variance is relatively large. This tutorial on advanced statistical methods for meta-analysis of diagnostic accuracy studies discusses and demonstrates Bayesian modeling using CopulaDTA package in R to fit different models to obtain the meta-analytic parameter estimates. The focus is on the joint modelling of sensitivity and specificity using copula based bivariate beta distribution. Essentially, we extend the work of Nikoloulopoulos by: i) presenting the Bayesian approach which offers flexibility and ability to perform complex statistical modelling even with small data sets and ii) including covariate information, and iii) providing an easy to use code. The statistical methods are illustrated by re-analysing data of two published meta-analyses. Modelling sensitivity and specificity using the bivariate beta distribution provides marginal as well as study-specific parameter estimates as opposed to using bivariate normal distribution (e.g., in BRMA) which only yields study-specific parameter estimates. Moreover, copula based models offer greater flexibility in modelling different correlation structures in contrast to the normal distribution which allows for only one correlation structure.

</details>

<details>

<summary>2016-04-09 10:19:06 - Some comments about James Watson's and Chris Holmes' "Approximate Models and Robust Decisions": Nonparametric Bayesian clay for robust decision bricks</summary>

- *Christian P. Robert, Judith Rousseau*

- `1603.09088v2` - [abs](http://arxiv.org/abs/1603.09088v2) - [pdf](http://arxiv.org/pdf/1603.09088v2)

> This note discusses Watson and Holmes (2016) and their pro- posals towards more robust Bayesian decisions. While we acknowledge and commend the authors for setting new and all-encompassing prin- ciples of Bayesian robustness, and we appreciate the strong anchoring of those within a decision-theoretic referential, we remain uncertain as to which extent such principles can be applied outside binary de- cisions. We also wonder at the ultimate relevance of Kullback-Leibler neighbourhoods to characterise robustness and favour extensions along non-parametric axes.

</details>

<details>

<summary>2016-04-10 03:28:34 - A New Family of Bounded Divergence Measures and Application to Signal Detection</summary>

- *Shivakumar Jolad, Ahmed Roman, Mahesh C. Shastry, Mihir Gadgil, Ayanendranath Basu*

- `1201.0418v9` - [abs](http://arxiv.org/abs/1201.0418v9) - [pdf](http://arxiv.org/pdf/1201.0418v9)

> We introduce a new one-parameter family of divergence measures, called bounded Bhattacharyya distance (BBD) measures, for quantifying the dissimilarity between probability distributions. These measures are bounded, symmetric and positive semi-definite and do not require absolute continuity. In the asymptotic limit, BBD measure approaches the squared Hellinger distance. A generalized BBD measure for multiple distributions is also introduced. We prove an extension of a theorem of Bradt and Karlin for BBD relating Bayes error probability and divergence ranking. We show that BBD belongs to the class of generalized Csiszar f-divergence and derive some properties such as curvature and relation to Fisher Information. For distributions with vector valued parameters, the curvature matrix is related to the Fisher-Rao metric. We derive certain inequalities between BBD and well known measures such as Hellinger and Jensen-Shannon divergence. We also derive bounds on the Bayesian error probability. We give an application of these measures to the problem of signal detection where we compare two monochromatic signals buried in white noise and differing in frequency and amplitude.

</details>

<details>

<summary>2016-04-10 12:44:59 - Stability and Structural Properties of Gene Regulation Networks with Coregulation Rules</summary>

- *Jonathan H. Warrell, Musa M. Mhlanga*

- `1602.08753v2` - [abs](http://arxiv.org/abs/1602.08753v2) - [pdf](http://arxiv.org/pdf/1602.08753v2)

> Coregulation of the expression of groups of genes has been extensively demonstrated empirically in bacterial and eukaryotic systems. Such coregulation can arise through the use of shared regulatory motifs, which allow the coordinated expression of modules (and module groups) of functionally related genes across the genome. Coregulation can also arise through the physical association of multi-gene complexes through chromosomal looping, which are then transcribed together. We present a general formalism for modeling coregulation rules in the framework of Random Boolean Networks (RBN), and develop specific models for transcription factor networks with modular structure (including module groups, and multi-input modules (MIM) with autoregulation) and multi-gene complexes (including hierarchical differentiation between multi-gene complex members). We develop a mean-field approach to analyse the stability of large networks incorporating coregulation, and show that autoregulated MIM and hierarchical gene-complex models can achieve greater stability than networks without coregulation whose rules have matching activation frequency. We provide further analysis of the stability of small networks of both kinds through simulations. We also characterize several general properties of the transients and attractors in the hierarchical coregulation model, and show using simulations that the steady-state distribution factorizes hierarchically as a Bayesian network in a Markov Jump Process analogue of the RBN model.

</details>

<details>

<summary>2016-04-10 13:38:06 - An adaptive independence sampler MCMC algorithm for infinite dimensional Bayesian inferences</summary>

- *Zhe Feng, Jinglai Li*

- `1508.03283v2` - [abs](http://arxiv.org/abs/1508.03283v2) - [pdf](http://arxiv.org/pdf/1508.03283v2)

> Many scientific and engineering problems require to perform Bayesian inferences in function spaces, in which the unknowns are of infinite dimension. In such problems, many standard Markov Chain Monte Carlo (MCMC) algorithms become arbitrary slow under the mesh refinement, which is referred to as being dimension dependent. In this work we develop an independence sampler based MCMC method for the infinite dimensional Bayesian inferences. We represent the proposal distribution as a mixture of a finite number of specially parametrized Gaussian measures. We show that under the chosen parametrization, the resulting MCMC algorithm is dimension independent. We also design an efficient adaptive algorithm to adjust the parameter values of the mixtures from the previous samples. Finally we provide numerical examples to demonstrate the efficiency and robustness of the proposed method, even for problems with multimodal posterior distributions.

</details>

<details>

<summary>2016-04-12 00:47:55 - Estimating an NBA player's impact on his team's chances of winning</summary>

- *Sameer K. Deshpande, Shane T. Jensen*

- `1604.03186v1` - [abs](http://arxiv.org/abs/1604.03186v1) - [pdf](http://arxiv.org/pdf/1604.03186v1)

> Traditional NBA player evaluation metrics are based on scoring differential or some pace-adjusted linear combination of box score statistics like points, rebounds, assists, etc. These measures treat performances with the outcome of the game still in question (e.g. tie score with five minutes left) in exactly the same way as they treat performances with the outcome virtually decided (e.g. when one team leads by 30 points with one minute left). Because they ignore the context in which players perform, these measures can result in misleading estimates of how players help their teams win. We instead use a win probability framework for evaluating the impact NBA players have on their teams' chances of winning. We propose a Bayesian linear regression model to estimate an individual player's impact, after controlling for the other players on the court. We introduce several posterior summaries to derive rank-orderings of players within their team and across the league. This allows us to identify highly paid players with low impact relative to their teammates, as well as players whose high impact is not captured by existing metrics.

</details>

<details>

<summary>2016-04-12 01:14:38 - Scalar-on-Image Regression via the Soft-Thresholded Gaussian Process</summary>

- *Jian Kang, Brian J. Reich, Ana-Maria Staicu*

- `1604.03192v1` - [abs](http://arxiv.org/abs/1604.03192v1) - [pdf](http://arxiv.org/pdf/1604.03192v1)

> The focus of this work is on spatial variable selection for scalar-on-image regression. We propose a new class of Bayesian nonparametric models, soft-thresholded Gaussian processes and develop the efficient posterior computation algorithms. Theoretically, soft-thresholded Gaussian processes provide large prior support for the spatially varying coefficients that enjoy piecewise smoothness, sparsity and continuity, characterizing the important features of imaging data. Also, under some mild regularity conditions, the soft-thresholded Gaussian process leads to the posterior consistency for both parameter estimation and variable selection for scalar-on-image regression, even when the number of true predictors is larger than the sample size. The proposed method is illustrated via simulations, compared numerically with existing alternatives and applied to Electroencephalography (EEG) study of alcoholism.

</details>

<details>

<summary>2016-04-12 09:05:28 - Bayesian Constrained-Model Selection for Factor Analytic Modeling</summary>

- *Carel F. W. Peeters*

- `1603.05882v2` - [abs](http://arxiv.org/abs/1603.05882v2) - [pdf](http://arxiv.org/pdf/1603.05882v2)

> My dissertation revolves around Bayesian approaches towards constrained statistical inference in the factor analysis (FA) model. Two interconnected types of restricted-model selection are considered. These types have a natural connection to selection problems in the exploratory FA (EFA) and confirmatory FA (CFA) model and are termed Type I and Type II model selection. Type I constrained-model selection is taken to mean the determination of the appropriate dimensionality of a model. This type of constrained-model selection connects with EFA in the sense of selecting the optimal dimensionality of the latent vector. Type II model selection is taken to mean the determination of appropriate inequality, order or shape restrictions on the parameter space. The dissertation connects Type II constrained-model selection to CFA by focusing on the determination of linear inequality constraints as expressions of the direction and (relative) strength of factor loadings. The figures accompanying this article are taken from the slides of my Division 5 Awards Symposium Invited address at the APA 2015 Annual Convention in Toronto. These slides can be retrieved from \url{https://github.com/CFWP/ConventionTalk}.

</details>

<details>

<summary>2016-04-12 15:11:06 - Competition and extinction explain the evolution of diversity in American automobiles</summary>

- *Erik Gjesfjeld, Jonathan Chang, Daniele Silvestro, Christopher Kelty, Michael Alfaro*

- `1604.00055v2` - [abs](http://arxiv.org/abs/1604.00055v2) - [pdf](http://arxiv.org/pdf/1604.00055v2)

> One of the most remarkable aspects of our species is that while we show surprisingly little genetic diversity, we demonstrate astonishing amounts of cultural diversity. Perhaps most impressive is the diversity of our technologies, broadly defined as all the physical objects we produce and the skills we use to produce them. Despite considerable focus on the evolution of technology by social scientists and philosophers, there have been few attempts to systematically quantify technological diversity and therefore the dynamics of technological change remain poorly understood. Here we show a novel Bayesian model for examining technological diversification adopted from paleontological analysis of occurrence data. We use this framework to estimate the tempo of diversification in American car and truck models produced between 1896 and 2014 and to test the relative importance of competition and extrinsic factors in shaping changes in macroevolutionary rates. Our results identify a four-fold decrease in the origination and extinction rates of car models and a negative net diversification rate over the last thirty years. We also demonstrate that competition played a more significant role in car model diversification than either changes in oil prices or gross domestic product. Together our analyses provide a set of tools that can enhance current research on technological and cultural evolution by providing a flexible and quantitative framework for exploring the dynamics of diversification.

</details>

<details>

<summary>2016-04-12 23:24:00 - A Nonparametric Bayesian Technique for High-Dimensional Regression</summary>

- *Subharup Guha, Veerabhadran Baladandayuthapani*

- `1604.03615v1` - [abs](http://arxiv.org/abs/1604.03615v1) - [pdf](http://arxiv.org/pdf/1604.03615v1)

> This paper proposes a nonparametric Bayesian framework called VariScan for simultaneous clustering, variable selection, and prediction in high-throughput regression settings. Poisson-Dirichlet processes are utilized to detect lower-dimensional latent clusters of covariates. An adaptive nonlinear prediction model is constructed for the response, achieving a balance between model parsimony and flexibility. Contrary to conventional belief, cluster detection is shown to be aposteriori consistent for a general class of models as the number of covariates and subjects grows. Simulation studies and data analyses demonstrate that VariScan often outperforms several well-known statistical methods.

</details>

<details>

<summary>2016-04-13 08:48:35 - Bayesian approach to LR assessment in case of rare type match: careful derivation and limits</summary>

- *Giulia Cereda*

- `1502.02406v9` - [abs](http://arxiv.org/abs/1502.02406v9) - [pdf](http://arxiv.org/pdf/1502.02406v9)

> The likelihood ratio (LR) is largely used to evaluate the relative weight of forensic data regarding two hypotheses and for its assessment Bayesian methods are widespread in the forensic field. However, the Bayesian `recipe' for the LR presented in most of literature consists in plugging-in Bayesian estimates of the involved nuisance parameters into a frequentist-defined LR: frequentist and Bayesian methods are thus mixed, giving rise to solutions obtained by hybrid reasoning. This paper provides the derivation of a proper Bayesian approach to assess LR for the `rare type match problem', the situation in which the expert wants to evaluate a match between the profile of a suspect and that of a trace from the crime scene, and this profile has never been observed before in the database of reference. Bayesian LR assessment using the two most popular Bayesian models (beta-binomial and Dirichlet-multinomial) is discussed and compared to corresponding plug-in versions.

</details>

<details>

<summary>2016-04-13 09:33:22 - Bayesian inference in hierarchical models by combining independent posteriors</summary>

- *Ritabrata Dutta, Paul Blomstedt, Samuel Kaski*

- `1603.09272v2` - [abs](http://arxiv.org/abs/1603.09272v2) - [pdf](http://arxiv.org/pdf/1603.09272v2)

> Hierarchical models are versatile tools for joint modeling of data sets arising from different, but related, sources. Fully Bayesian inference may, however, become computationally prohibitive if the source-specific data models are complex, or if the number of sources is very large. To facilitate computation, we propose an approach, where inference is first made independently for the parameters of each data set, whereupon the obtained posterior samples are used as observed data in a substitute hierarchical model, based on a scaled likelihood function. Compared to direct inference in a full hierarchical model, the approach has the advantage of being able to speed up convergence by breaking down the initial large inference problem into smaller individual subproblems with better convergence properties. Moreover it enables parallel processing of the possibly complex inferences of the source-specific parameters, which may otherwise create a computational bottleneck if processed jointly as part of a hierarchical model. The approach is illustrated with both simulated and real data.

</details>

<details>

<summary>2016-04-13 11:24:30 - Quantifying uncertainties on excursion sets under a Gaussian random field prior</summary>

- *Dario Azzimonti, Julien Bect, ClÃ©ment Chevalier, David Ginsbourger*

- `1501.03659v2` - [abs](http://arxiv.org/abs/1501.03659v2) - [pdf](http://arxiv.org/pdf/1501.03659v2)

> We focus on the problem of estimating and quantifying uncertainties on the excursion set of a function under a limited evaluation budget. We adopt a Bayesian approach where the objective function is assumed to be a realization of a Gaussian random field. In this setting, the posterior distribution on the objective function gives rise to a posterior distribution on excursion sets. Several approaches exist to summarize the distribution of such sets based on random closed set theory. While the recently proposed Vorob'ev approach exploits analytical formulae, further notions of variability require Monte Carlo estimators relying on Gaussian random field conditional simulations. In the present work we propose a method to choose Monte Carlo simulation points and obtain quasi-realizations of the conditional field at fine designs through affine predictors. The points are chosen optimally in the sense that they minimize the posterior expected distance in measure between the excursion set and its reconstruction. The proposed method reduces the computational costs due to Monte Carlo simulations and enables the computation of quasi-realizations on fine designs in large dimensions. We apply this reconstruction approach to obtain realizations of an excursion set on a fine grid which allow us to give a new measure of uncertainty based on the distance transform of the excursion set. Finally we present a safety engineering test case where the simulation method is employed to compute a Monte Carlo estimate of a contour line.

</details>

<details>

<summary>2016-04-14 03:42:20 - Non-separable Dynamic Nearest-Neighbor Gaussian Process Models for Large spatio-temporal Data With an Application to Particulate Matter Analysis</summary>

- *Abhirup Datta, Sudipto Banerjee, Andrew O. Finley, Nicholas A. S. Hamm, Martijn Schaap*

- `1510.07130v3` - [abs](http://arxiv.org/abs/1510.07130v3) - [pdf](http://arxiv.org/pdf/1510.07130v3)

> Particulate matter (PM) is a class of malicious environmental pollutants known to be detrimental to human health. Regulatory efforts aimed at curbing PM levels in different countries often require high resolution space-time maps that can identify red-flag regions exceeding statutory concentration limits. Continuous spatio-temporal Gaussian Process (GP) models can deliver maps depicting predicted PM levels and quantify predictive uncertainty. However, GP based approaches are usually thwarted by computational challenges posed by large datasets. We construct a novel class of scalable Dynamic Nearest Neighbor Gaussian Process (DNNGP) models that can provide a sparse approximation to any spatio-temporal GP (e.g., with non-separable covariance structures). The DNNGP we develop here can be used as a sparsity-inducing prior for spatio-temporal random effects in any Bayesian hierarchical model to deliver full posterior inference. Storage and memory requirements for a DNNGP model are linear in the size of the dataset thereby delivering massive scalability without sacrificing inferential richness. Extensive numerical studies reveal that the DNNGP provides substantially superior approximations to the underlying process than low rank approximations. Finally, we use the DNNGP to analyze a massive air quality dataset to substantially improve predictions of PM levels across Europe in conjunction with the LOTOS-EUROS chemistry transport models (CTMs).

</details>

<details>

<summary>2016-04-14 15:42:03 - 1-bit Matrix Completion: PAC-Bayesian Analysis of a Variational Approximation</summary>

- *Vincent Cottet, Pierre Alquier*

- `1604.04191v1` - [abs](http://arxiv.org/abs/1604.04191v1) - [pdf](http://arxiv.org/pdf/1604.04191v1)

> Due to challenging applications such as collaborative filtering, the matrix completion problem has been widely studied in the past few years. Different approaches rely on different structure assumptions on the matrix in hand. Here, we focus on the completion of a (possibly) low-rank matrix with binary entries, the so-called 1-bit matrix completion problem. Our approach relies on tools from machine learning theory: empirical risk minimization and its convex relaxations. We propose an algorithm to compute a variational approximation of the pseudo-posterior. Thanks to the convex relaxation, the corresponding minimization problem is bi-convex, and thus the method behaves well in practice. We also study the performance of this variational approximation through PAC-Bayesian learning bounds. On the contrary to previous works that focused on upper bounds on the estimation error of M with various matrix norms, we are able to derive from this analysis a PAC bound on the prediction error of our algorithm.   We focus essentially on convex relaxation through the hinge loss, for which we present the complete analysis, a complete simulation study and a test on the MovieLens data set. However, we also discuss a variational approximation to deal with the logistic loss.

</details>

<details>

<summary>2016-04-14 23:42:08 - Sparse Kalman Filtering Approaches to Covariance Estimation from High Frequency Data in the Presence of Jumps</summary>

- *Michael Ho, Jack Xin*

- `1602.02185v2` - [abs](http://arxiv.org/abs/1602.02185v2) - [pdf](http://arxiv.org/pdf/1602.02185v2)

> Estimation of the covariance matrix of asset returns from high frequency data is complicated by asynchronous returns, market mi- crostructure noise and jumps. One technique for addressing both asynchronous returns and market microstructure is the Kalman-EM (KEM) algorithm. However the KEM approach assumes log-normal prices and does not address jumps in the return process which can corrupt estimation of the covariance matrix.   In this paper we extend the KEM algorithm to price models that include jumps. We propose two sparse Kalman filtering approaches to this problem. In the first approach we develop a Kalman Expectation Conditional Maximization (KECM) algorithm to determine the un- known covariance as well as detecting the jumps. For this algorithm we consider Laplace and the spike and slab jump models, both of which promote sparse estimates of the jumps. In the second method we take a Bayesian approach and use Gibbs sampling to sample from the posterior distribution of the covariance matrix under the spike and slab jump model. Numerical results using simulated data show that each of these approaches provide for improved covariance estima- tion relative to the KEM method in a variety of settings where jumps occur.

</details>

<details>

<summary>2016-04-15 07:39:12 - Computationally Efficient Bayesian Learning of Gaussian Process State Space Models</summary>

- *Andreas Svensson, Arno Solin, Simo SÃ¤rkkÃ¤, Thomas B. SchÃ¶n*

- `1506.02267v2` - [abs](http://arxiv.org/abs/1506.02267v2) - [pdf](http://arxiv.org/pdf/1506.02267v2)

> Gaussian processes allow for flexible specification of prior assumptions of unknown dynamics in state space models. We present a procedure for efficient Bayesian learning in Gaussian process state space models, where the representation is formed by projecting the problem onto a set of approximate eigenfunctions derived from the prior covariance structure. Learning under this family of models can be conducted using a carefully crafted particle MCMC algorithm. This scheme is computationally efficient and yet allows for a fully Bayesian treatment of the problem. Compared to conventional system identification tools or existing learning methods, we show competitive performance and reliable quantification of uncertainties in the model.

</details>

<details>

<summary>2016-04-15 11:21:27 - Bayesian linear regression with Student-t assumptions</summary>

- *Chaobing Song, Shu-Tao Xia*

- `1604.04434v1` - [abs](http://arxiv.org/abs/1604.04434v1) - [pdf](http://arxiv.org/pdf/1604.04434v1)

> As an automatic method of determining model complexity using the training data alone, Bayesian linear regression provides us a principled way to select hyperparameters. But one often needs approximation inference if distribution assumption is beyond Gaussian distribution. In this paper, we propose a Bayesian linear regression model with Student-t assumptions (BLRS), which can be inferred exactly. In this framework, both conjugate prior and expectation maximization (EM) algorithm are generalized. Meanwhile, we prove that the maximum likelihood solution is equivalent to the standard Bayesian linear regression with Gaussian assumptions (BLRG). The $q$-EM algorithm for BLRS is nearly identical to the EM algorithm for BLRG. It is showed that $q$-EM for BLRS can converge faster than EM for BLRG for the task of predicting online news popularity.

</details>

<details>

<summary>2016-04-17 21:16:37 - Regularizing Solutions to the MEG Inverse Problem Using Space-Time Separable Covariance Functions</summary>

- *Arno Solin, Pasi JylÃ¤nki, Jaakko KauramÃ¤ki, Tom Heskes, Marcel A. J. van Gerven, Simo SÃ¤rkkÃ¤*

- `1604.04931v1` - [abs](http://arxiv.org/abs/1604.04931v1) - [pdf](http://arxiv.org/pdf/1604.04931v1)

> In magnetoencephalography (MEG) the conventional approach to source reconstruction is to solve the underdetermined inverse problem independently over time and space. Here we present how the conventional approach can be extended by regularizing the solution in space and time by a Gaussian process (Gaussian random field) model. Assuming a separable covariance function in space and time, the computational complexity of the proposed model becomes (without any further assumptions or restrictions) $\mathcal{O}(t^3 + n^3 + m^2n)$, where $t$ is the number of time steps, $m$ is the number of sources, and $n$ is the number of sensors. We apply the method to both simulated and empirical data, and demonstrate the efficiency and generality of our Bayesian source reconstruction approach which subsumes various classical approaches in the literature.

</details>

<details>

<summary>2016-04-17 22:14:13 - The Variational Gaussian Process</summary>

- *Dustin Tran, Rajesh Ranganath, David M. Blei*

- `1511.06499v4` - [abs](http://arxiv.org/abs/1511.06499v4) - [pdf](http://arxiv.org/pdf/1511.06499v4)

> Variational inference is a powerful tool for approximate inference, and it has been recently applied for representation learning with deep generative models. We develop the variational Gaussian process (VGP), a Bayesian nonparametric variational family, which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by auto-encoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW.

</details>

<details>

<summary>2016-04-17 23:13:50 - Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor Analysis</summary>

- *Andreas Damianou, Neil D. Lawrence, Carl Henrik Ek*

- `1604.04939v1` - [abs](http://arxiv.org/abs/1604.04939v1) - [pdf](http://arxiv.org/pdf/1604.04939v1)

> Factor analysis aims to determine latent factors, or traits, which summarize a given data set. Inter-battery factor analysis extends this notion to multiple views of the data. In this paper we show how a nonlinear, nonparametric version of these models can be recovered through the Gaussian process latent variable model. This gives us a flexible formalism for multi-view learning where the latent variables can be used both for exploratory purposes and for learning representations that enable efficient inference for ambiguous estimation tasks. Learning is performed in a Bayesian manner through the formulation of a variational compression scheme which gives a rigorous lower bound on the log likelihood. Our Bayesian framework provides strong regularization during training, allowing the structure of the latent space to be determined efficiently and automatically. We demonstrate this by producing the first (to our knowledge) published results of learning from dozens of views, even when data is scarce. We further show experimental results on several different types of multi-view data sets and for different kinds of tasks, including exploratory data analysis, generation, ambiguity modelling through latent priors and classification.

</details>

<details>

<summary>2016-04-18 17:46:23 - Chained Gaussian Processes</summary>

- *Alan D. Saul, James Hensman, Aki Vehtari, Neil D. Lawrence*

- `1604.05263v1` - [abs](http://arxiv.org/abs/1604.05263v1) - [pdf](http://arxiv.org/pdf/1604.05263v1)

> Gaussian process models are flexible, Bayesian non-parametric approaches to regression. Properties of multivariate Gaussians mean that they can be combined linearly in the manner of additive models and via a link function (like in generalized linear models) to handle non-Gaussian data. However, the link function formalism is restrictive, link functions are always invertible and must convert a parameter of interest to a linear combination of the underlying processes. There are many likelihoods and models where a non-linear combination is more appropriate. We term these more general models Chained Gaussian Processes: the transformation of the GPs to the likelihood parameters will not generally be invertible, and that implies that linearisation would only be possible with multiple (localized) links, i.e. a chain. We develop an approximate inference procedure for Chained GPs that is scalable and applicable to any factorized likelihood. We demonstrate the approximation on a range of likelihood functions.

</details>

<details>

<summary>2016-04-19 09:04:31 - Boosting Bayesian Parameter Inference of Nonlinear Stochastic Differential Equation Models by Hamiltonian Scale Separation</summary>

- *Carlo Albert, Simone Ulzega, Ruedi Stoop*

- `1509.05305v2` - [abs](http://arxiv.org/abs/1509.05305v2) - [pdf](http://arxiv.org/pdf/1509.05305v2)

> Parameter inference is a fundamental problem in data-driven modeling. Given observed data that is believed to be a realization of some parameterized model, the aim is to find parameter values that are able to explain the observed data. In many situations, the dominant sources of uncertainty must be included into the model, for making reliable predictions. This naturally leads to stochastic models. Stochastic models render parameter inference much harder, as the aim then is to find a distribution of likely parameter values. In Bayesian statistics, which is a consistent framework for data-driven learning, this so-called posterior distribution can be used to make probabilistic predictions. We propose a novel, exact and very efficient approach for generating posterior parameter distributions, for stochastic differential equation models calibrated to measured time-series. The algorithm is inspired by re-interpreting the posterior distribution as a statistical mechanics partition function of an object akin to a polymer, where the measurements are mapped on heavier beads compared to those of the simulated data. To arrive at distribution samples, we employ a Hamiltonian Monte Carlo approach combined with a multiple time-scale integration. A separation of time scales naturally arises if either the number of measurement points or the number of simulation points becomes large. Furthermore, at least for 1D problems, we can decouple the harmonic modes between measurement points and solve the fastest part of their dynamics analytically. Our approach is applicable to a wide range of inference problems and is highly parallelizable.

</details>

<details>

<summary>2016-04-19 17:28:07 - Sequential Monte Carlo Smoothing with Parameter Estimation</summary>

- *Biao Yang, Jonathan R. Stroud, Gabriel Huerta*

- `1604.05658v1` - [abs](http://arxiv.org/abs/1604.05658v1) - [pdf](http://arxiv.org/pdf/1604.05658v1)

> We propose two new Bayesian smoothing methods for general state-space models with unknown parameters. The first approach is based on the particle learning and smoothing algorithm, but with an adjustment in the backward resampling weights. The second is a new method combining sequential parameter learning and smoothing algorithms for general state-space models. This method is straightforward but effective, and we find it is the best existing Sequential Monte Carlo algorithm to solve the joint Bayesian smoothing problem. We first illustrate the methods on three benchmark models using simulated data, and then apply them to a stochastic volatility model for daily S&P 500 index returns during the financial crisis.

</details>

<details>

<summary>2016-04-19 17:30:45 - Objective Bayesian Analysis of the Yule-Simon Distribution with Applications</summary>

- *Fabrizio Leisen, Luca Rossini, Cristiano Villa*

- `1604.05661v1` - [abs](http://arxiv.org/abs/1604.05661v1) - [pdf](http://arxiv.org/pdf/1604.05661v1)

> The Yule-Simon distribution is usually employed in the analysis of frequency data. As the Bayesian literature, so far, ignored this distribution, here we show the derivation of two objective priors for the parameter of the Yule-Simon distribution. In particular, we discuss the Jeffreys prior and a loss-based prior, which has recently appeared in the literature. We illustrate the performance of the derived priors through a simulation study and the analysis of real datasets.

</details>

<details>

<summary>2016-04-20 16:22:01 - Constructive Preference Elicitation by Setwise Max-margin Learning</summary>

- *Stefano Teso, Andrea Passerini, Paolo Viappiani*

- `1604.06020v1` - [abs](http://arxiv.org/abs/1604.06020v1) - [pdf](http://arxiv.org/pdf/1604.06020v1)

> In this paper we propose an approach to preference elicitation that is suitable to large configuration spaces beyond the reach of existing state-of-the-art approaches. Our setwise max-margin method can be viewed as a generalization of max-margin learning to sets, and can produce a set of "diverse" items that can be used to ask informative queries to the user. Moreover, the approach can encourage sparsity in the parameter space, in order to favor the assessment of utility towards combinations of weights that concentrate on just few features. We present a mixed integer linear programming formulation and show how our approach compares favourably with Bayesian preference elicitation alternatives and easily scales to realistic datasets.

</details>

<details>

<summary>2016-04-20 17:22:12 - Qualitative Robustness in Bayesian Inference</summary>

- *Houman Owhadi, Clint Scovel*

- `1411.3984v3` - [abs](http://arxiv.org/abs/1411.3984v3) - [pdf](http://arxiv.org/pdf/1411.3984v3)

> The practical implementation of Bayesian inference requires numerical approximation when closed-form expressions are not available. What types of accuracy (convergence) of the numerical approximations guarantee robustness and what types do not? In particular, is the recursive application of Bayes' rule robust when subsequent data or posteriors are approximated? When the prior is the push forward of a distribution by the map induced by the solution of a PDE, in which norm should that solution be approximated? Motivated by such questions, we investigate the sensitivity of the distribution of posterior distributions (i.e. posterior distribution-valued random variables, randomized through the data) with respect to perturbations of the prior and data generating distributions in the limit when the number of data points grows towards infinity.

</details>

<details>

<summary>2016-04-21 10:23:53 - Sparse group factor analysis for biclustering of multiple data sources</summary>

- *Kerstin Bunte, Eemeli LeppÃ¤aho, Inka Saarinen, Samuel Kaski*

- `1512.08808v2` - [abs](http://arxiv.org/abs/1512.08808v2) - [pdf](http://arxiv.org/pdf/1512.08808v2)

> Motivation: Modelling methods that find structure in data are necessary with the current large volumes of genomic data, and there have been various efforts to find subsets of genes exhibiting consistent patterns over subsets of treatments. These biclustering techniques have focused on one data source, often gene expression data. We present a Bayesian approach for joint biclustering of multiple data sources, extending a recent method Group Factor Analysis (GFA) to have a biclustering interpretation with additional sparsity assumptions. The resulting method enables data-driven detection of linear structure present in parts of the data sources. Results: Our simulation studies show that the proposed method reliably infers bi-clusters from heterogeneous data sources. We tested the method on data from the NCI-DREAM drug sensitivity prediction challenge, resulting in an excellent prediction accuracy. Moreover, the predictions are based on several biclusters which provide insight into the data sources, in this case on gene expression, DNA methylation, protein abundance, exome sequence, functional connectivity fingerprints and drug sensitivity.

</details>

<details>

<summary>2016-04-22 03:05:18 - Variational inference for rare variant detection in deep, heterogeneous next-generation sequencing data</summary>

- *Fan Zhang, Patrick Flaherty*

- `1604.04280v2` - [abs](http://arxiv.org/abs/1604.04280v2) - [pdf](http://arxiv.org/pdf/1604.04280v2)

> The detection of rare variants is important for understanding the genetic heterogeneity in mixed samples. Recently, next-generation sequencing (NGS) technologies have enabled the identification of single nucleotide variants (SNVs) in mixed samples with high resolution. Yet, the noise inherent in the biological processes involved in next-generation sequencing necessitates the use of statistical methods to identify true rare variants. We propose a novel Bayesian statistical model and a variational expectation-maximization (EM) algorithm to estimate non-reference allele frequency (NRAF) and identify SNVs in heterogeneous cell populations. We demonstrate that our variational EM algorithm has comparable sensitivity and specificity compared with a Markov Chain Monte Carlo (MCMC) sampling inference algorithm, and is more computationally efficient on tests of low coverage ($27\times$ and $298\times$) data. Furthermore, we show that our model with a variational EM inference algorithm has higher specificity than many state-of-the-art algorithms. In an analysis of a directed evolution longitudinal yeast data set, we are able to identify a time-series trend in non-reference allele frequency and detect novel variants that have not yet been reported. Our model also detects the emergence of a beneficial variant earlier than was previously shown, and a pair of concomitant variants.

</details>

<details>

<summary>2016-04-22 14:06:32 - Optimal scaling of the Random Walk Metropolis algorithm under Lp mean differentiability</summary>

- *Alain Durmus, Sylvain Le Corff, Eric Moulines, Gareth O. Roberts*

- `1604.06664v1` - [abs](http://arxiv.org/abs/1604.06664v1) - [pdf](http://arxiv.org/pdf/1604.06664v1)

> This paper considers the optimal scaling problem for high-dimensional random walk Metropolis algorithms for densities which are differentiable in Lp mean but which may be irregular at some points (like the Laplace density for example) and/or are supported on an interval. Our main result is the weak convergence of the Markov chain (appropriately rescaled in time and space) to a Langevin diffusion process as the dimension d goes to infinity. Because the log-density might be non-differentiable, the limiting diffusion could be singular. The scaling limit is established under assumptions which are much weaker than the one used in the original derivation of [6]. This result has important practical implications for the use of random walk Metropolis algorithms in Bayesian frameworks based on sparsity inducing priors.

</details>

<details>

<summary>2016-04-22 17:44:00 - Posteriors, conjugacy, and exponential families for completely random measures</summary>

- *Tamara Broderick, Ashia C. Wilson, Michael I. Jordan*

- `1410.6843v2` - [abs](http://arxiv.org/abs/1410.6843v2) - [pdf](http://arxiv.org/pdf/1410.6843v2)

> We demonstrate how to calculate posteriors for general CRM-based priors and likelihoods for Bayesian nonparametric models. We further show how to represent Bayesian nonparametric priors as a sequence of finite draws using a size-biasing approach---and how to represent full Bayesian nonparametric models via finite marginals. Motivated by conjugate priors based on exponential family representations of likelihoods, we introduce a notion of exponential families for CRMs, which we call exponential CRMs. This construction allows us to specify automatic Bayesian nonparametric conjugate priors for exponential CRM likelihoods. We demonstrate that our exponential CRMs allow particularly straightforward recipes for size-biased and marginal representations of Bayesian nonparametric models. Along the way, we prove that the gamma process is a conjugate prior for the Poisson likelihood process and the beta prime process is a conjugate prior for a process we call the odds Bernoulli process. We deliver a size-biased representation of the gamma process and a marginal representation of the gamma process coupled with a Poisson likelihood process.

</details>

<details>

<summary>2016-04-25 01:35:31 - Learning to Select Pre-Trained Deep Representations with Bayesian Evidence Framework</summary>

- *Yong-Deok Kim, Taewoong Jang, Bohyung Han, Seungjin Choi*

- `1506.02565v4` - [abs](http://arxiv.org/abs/1506.02565v4) - [pdf](http://arxiv.org/pdf/1506.02565v4)

> We propose a Bayesian evidence framework to facilitate transfer learning from pre-trained deep convolutional neural networks (CNNs). Our framework is formulated on top of a least squares SVM (LS-SVM) classifier, which is simple and fast in both training and testing, and achieves competitive performance in practice. The regularization parameters in LS-SVM is estimated automatically without grid search and cross-validation by maximizing evidence, which is a useful measure to select the best performing CNN out of multiple candidates for transfer learning; the evidence is optimized efficiently by employing Aitken's delta-squared process, which accelerates convergence of fixed point update. The proposed Bayesian evidence framework also provides a good solution to identify the best ensemble of heterogeneous CNNs through a greedy algorithm. Our Bayesian evidence framework for transfer learning is tested on 12 visual recognition datasets and illustrates the state-of-the-art performance consistently in terms of prediction accuracy and modeling efficiency.

</details>

<details>

<summary>2016-04-25 09:19:22 - On the Use of Penalty MCMC for Differential Privacy</summary>

- *Sinan YÄ±ldÄ±rÄ±m*

- `1604.07177v1` - [abs](http://arxiv.org/abs/1604.07177v1) - [pdf](http://arxiv.org/pdf/1604.07177v1)

> We view the penalty algorithm of Ceperley and Dewing (1999), a Markov chain Monte Carlo (MCMC) algorithm for Bayesian inference, in the context of data privacy. Specifically, we study differential privacy of the penalty algorithm and advocate its use for data privacy. We show that in the simple model of independent observations the algorithm has desirable convergence and privacy properties that scale with data size. Two special cases are also investigated and privacy preserving schemes are proposed for those cases: (i) Data are distributed among several data owners who are interested in the inference of a common parameter while preserving their data privacy. (ii) The data likelihood belongs to an exponential family.

</details>

<details>

<summary>2016-04-26 00:48:43 - Scalable Bayesian Variable Selection for Structured High-dimensional Data</summary>

- *Changgee Chang, Suprateek Kundu, Qi Long*

- `1604.07264v2` - [abs](http://arxiv.org/abs/1604.07264v2) - [pdf](http://arxiv.org/pdf/1604.07264v2)

> Variable selection for structured covariates lying on an underlying known graph is a problem motivated by practical applications, and has been a topic of increasing interest. However, most of the existing methods may not be scalable to high dimensional settings involving tens of thousands of variables lying on known pathways such as the case in genomics studies. We propose an adaptive Bayesian shrinkage approach which incorporates prior network information by smoothing the shrinkage parameters for connected variables in the graph, so that the corresponding coefficients have a similar degree of shrinkage. We fit our model via a computationally efficient expectation maximization algorithm which scalable to high dimensional settings (p~100,000). Theoretical properties for fixed as well as increasing dimensions are established, even when the number of variables increases faster than the sample size. We demonstrate the advantages of our approach in terms of variable selection, prediction, and computational scalability via a simulation study, and apply the method to a cancer genomics study.

</details>

<details>

<summary>2016-04-26 09:03:00 - Spectral likelihood expansions for Bayesian inference</summary>

- *Joseph B. Nagel, Bruno Sudret*

- `1506.07564v2` - [abs](http://arxiv.org/abs/1506.07564v2) - [pdf](http://arxiv.org/pdf/1506.07564v2)

> A spectral approach to Bayesian inference is presented. It pursues the emulation of the posterior probability density. The starting point is a series expansion of the likelihood function in terms of orthogonal polynomials. From this spectral likelihood expansion all statistical quantities of interest can be calculated semi-analytically. The posterior is formally represented as the product of a reference density and a linear combination of polynomial basis functions. Both the model evidence and the posterior moments are related to the expansion coefficients. This formulation avoids Markov chain Monte Carlo simulation and allows one to make use of linear least squares instead. The pros and cons of spectral Bayesian inference are discussed and demonstrated on the basis of simple applications from classical statistics and inverse modeling.

</details>

<details>

<summary>2016-04-26 22:31:01 - A Model-based Semi-Supervised Clustering Methodology</summary>

- *Jordan Yoder, Carey E. Priebe*

- `1412.4841v2` - [abs](http://arxiv.org/abs/1412.4841v2) - [pdf](http://arxiv.org/pdf/1412.4841v2)

> We consider an extension of model-based clustering to the semi-supervised case, where some of the data are pre-labeled. We provide a derivation of the Bayesian Information Criterion (BIC) approximation to the Bayes factor in this setting. We then use the BIC to the select number of clusters and the variables useful for clustering. We demonstrate the efficacy of this adaptation of the model-based clustering paradigm through two simulation examples and a fly larvae behavioral dataset in which lines of neurons are clustered into behavioral groups.

</details>

<details>

<summary>2016-04-27 09:56:26 - Penalized complexity priors for degrees of freedom in Bayesian P-splines</summary>

- *Massimo Ventrucci, HÃ¥vard Rue*

- `1511.05748v2` - [abs](http://arxiv.org/abs/1511.05748v2) - [pdf](http://arxiv.org/pdf/1511.05748v2)

> Bayesian P-splines assume an intrinsic Gaussian Markov random field prior on the spline coefficients, conditional on a precision hyper-parameter $\tau$. Prior elicitation of $\tau$ is difficult. To overcome this issue we aim to building priors on an interpretable property of the model, indicating the complexity of the smooth function to be estimated. Following this idea, we propose Penalized Complexity (PC) priors for the number of effective degrees of freedom. We present the general ideas behind the construction of these new PC priors, describe their properties and show how to implement them in P-splines for Gaussian data.

</details>

<details>

<summary>2016-04-27 14:25:17 - Scalable Bayesian nonparametric measures for exploring pairwise dependence via Dirichlet Process Mixtures</summary>

- *Sarah Filippi, Chris C. Holmes, Luis E. Nieto-Barajas*

- `1604.08085v1` - [abs](http://arxiv.org/abs/1604.08085v1) - [pdf](http://arxiv.org/pdf/1604.08085v1)

> In this article we propose novel Bayesian nonparametric methods using Dirichlet Process Mixture (DPM) models for detecting pairwise dependence between random variables while accounting for uncertainty in the form of the underlying distributions. A key criteria is that the procedures should scale to large data sets. In this regard we find that the formal calculation of the Bayes factor for a dependent-vs.-independent DPM joint probability measure is not feasible computationally. To address this we present Bayesian diagnostic measures for characterising evidence against a "null model" of pairwise independence. In simulation studies, as well as for a real data analysis, we show that our approach provides a useful tool for the exploratory nonparametric Bayesian analysis of large multivariate data sets.

</details>

<details>

<summary>2016-04-27 15:07:48 - An ABC interpretation of the multiple auxiliary variable method</summary>

- *Dennis Prangle, Richard G. Everitt*

- `1604.08102v1` - [abs](http://arxiv.org/abs/1604.08102v1) - [pdf](http://arxiv.org/pdf/1604.08102v1)

> We show that the auxiliary variable method (M{\o}ller et al., 2006; Murray et al., 2006) for inference of Markov random fields can be viewed as an approximate Bayesian computation method for likelihood estimation.

</details>

<details>

<summary>2016-04-27 21:09:43 - Scalable Discrete Sampling as a Multi-Armed Bandit Problem</summary>

- *Yutian Chen, Zoubin Ghahramani*

- `1506.09039v3` - [abs](http://arxiv.org/abs/1506.09039v3) - [pdf](http://arxiv.org/pdf/1506.09039v3)

> Drawing a sample from a discrete distribution is one of the building components for Monte Carlo methods. Like other sampling algorithms, discrete sampling suffers from the high computational burden in large-scale inference problems. We study the problem of sampling a discrete random variable with a high degree of dependency that is typical in large-scale Bayesian inference and graphical models, and propose an efficient approximate solution with a subsampling approach. We make a novel connection between the discrete sampling and Multi-Armed Bandits problems with a finite reward population and provide three algorithms with theoretical guarantees. Empirical evaluations show the robustness and efficiency of the approximate algorithms in both synthetic and real-world large-scale problems.

</details>

<details>

<summary>2016-04-28 06:32:27 - Sequential Bayesian optimal experimental design via approximate dynamic programming</summary>

- *Xun Huan, Youssef M. Marzouk*

- `1604.08320v1` - [abs](http://arxiv.org/abs/1604.08320v1) - [pdf](http://arxiv.org/pdf/1604.08320v1)

> The design of multiple experiments is commonly undertaken via suboptimal strategies, such as batch (open-loop) design that omits feedback or greedy (myopic) design that does not account for future effects. This paper introduces new strategies for the optimal design of sequential experiments. First, we rigorously formulate the general sequential optimal experimental design (sOED) problem as a dynamic program. Batch and greedy designs are shown to result from special cases of this formulation. We then focus on sOED for parameter inference, adopting a Bayesian formulation with an information theoretic design objective. To make the problem tractable, we develop new numerical approaches for nonlinear design with continuous parameter, design, and observation spaces. We approximate the optimal policy by using backward induction with regression to construct and refine value function approximations in the dynamic program. The proposed algorithm iteratively generates trajectories via exploration and exploitation to improve approximation accuracy in frequently visited regions of the state space. Numerical results are verified against analytical solutions in a linear-Gaussian setting. Advantages over batch and greedy design are then demonstrated on a nonlinear source inversion problem where we seek an optimal policy for sequential sensing.

</details>

<details>

<summary>2016-04-28 09:13:24 - Investigation of the widely applicable Bayesian information criterion</summary>

- *N. Friel, J. P. McKeone, C. J. Oates, A. N. Pettitt*

- `1501.05447v2` - [abs](http://arxiv.org/abs/1501.05447v2) - [pdf](http://arxiv.org/pdf/1501.05447v2)

> The widely applicable Bayesian information criterion (WBIC) is a simple and fast approximation to the model evidence that has received little practical consideration. WBIC uses the fact that the log evidence can be written as an expectation, with respect to a powered posterior proportional to the likelihood raised to a power $t^*\in{(0,1)}$, of the log deviance. Finding this temperature value $t^*$ is generally an intractable problem. We find that for a particular tractable statistical model that the mean squared error of an optimally-tuned version of WBIC with correct temperature $t^*$ is lower than an optimally-tuned version of thermodynamic integration (power posteriors). However in practice WBIC uses the a canonical choice of $t=1/\log(n)$. Here we investigate the performance of WBIC in practice, for a range of statistical models, both regular models and singular models such as latent variable models or those with a hierarchical structure for which BIC cannot provide an adequate solution. Our findings are that, generally WBIC performs adequately when one uses informative priors, but it can systematically overestimate the evidence, particularly for small sample sizes.

</details>

<details>

<summary>2016-04-28 17:23:42 - A Probabilistic Adaptive Search System for Exploring the Face Space</summary>

- *Andres G. Abad, Luis I. Reyes Castro*

- `1604.08524v1` - [abs](http://arxiv.org/abs/1604.08524v1) - [pdf](http://arxiv.org/pdf/1604.08524v1)

> Face recall is a basic human cognitive process performed routinely, e.g., when meeting someone and determining if we have met that person before. Assisting a subject during face recall by suggesting candidate faces can be challenging. One of the reasons is that the search space - the face space - is quite large and lacks structure. A commercial application of face recall is facial composite systems - such as Identikit, PhotoFIT, and CD-FIT - where a witness searches for an image of a face that resembles his memory of a particular offender. The inherent uncertainty and cost in the evaluation of the objective function, the large size and lack of structure of the search space, and the unavailability of the gradient concept makes this problem inappropriate for traditional optimization methods. In this paper we propose a novel evolutionary approach for searching the face space that can be used as a facial composite system. The approach is inspired by methods of Bayesian optimization and differs from other applications in the use of the skew-normal distribution as its acquisition function. This choice of acquisition function provides greater granularity, with regularized, conservative, and realistic results.

</details>

<details>

<summary>2016-04-29 00:12:38 - Bayesian Genome- and Epigenome-wide Association Studies with Gene Level Dependence</summary>

- *Eric F. Lock, David B. Dunson*

- `1604.08654v1` - [abs](http://arxiv.org/abs/1604.08654v1) - [pdf](http://arxiv.org/pdf/1604.08654v1)

> High-throughput genetic and epigenetic data are often screened for associations with an observed phenotype. For example, one may wish to test hundreds of thousands of genetic variants, or DNA methylation sites, for an association with disease status. These genomic variables can naturally be grouped by the gene they encode, among other criteria. However, standard practice in such applications is independent screening with a universal correction for multiplicity. We propose a Bayesian approach in which the prior probability of an association for a given genomic variable depends on its gene, and the gene-specific probabilities are modeled nonparametrically. This hierarchical model allows for appropriate gene and genome-wide multiplicity adjustments, and can be incorporated into a variety of Bayesian association screening methodologies with negligible increase in computational complexity. We describe an application to screening for differences in DNA methylation between lower grade glioma and glioblastoma multiforme tumor samples from The Cancer Genome Atlas. Software is available via the package BayesianScreening for R at https://github.com/lockEF/BayesianScreening .

</details>

<details>

<summary>2016-04-29 14:32:51 - Joint Dispersion Model with a Flexible Link</summary>

- *Rui Martins*

- `1604.08853v1` - [abs](http://arxiv.org/abs/1604.08853v1) - [pdf](http://arxiv.org/pdf/1604.08853v1)

> The objective is to model longitudinal and survival data jointly taking into account the dependence between the two responses in a real HIV/AIDS dataset using a shared parameter approach inside a Bayesian framework. We propose a linear mixed effects dispersion model to adjust the CD4 longitudinal biomarker data with a between-individual heterogeneity in the mean and variance. In doing so we are relaxing the usual assumption of a common variance for the longitudinal residuals. A hazard regression model is considered in addition to model the time since HIV/AIDS diagnostic until failure, being the coefficients, accounting for the linking between the longitudinal and survival processes, time-varying. This flexibility is specified using Penalized Splines and allows the relationship to vary in time. Because heteroscedasticity may be related with the survival, the standard deviation is considered as a covariate in the hazard model, thus enabling to study the effect of the CD4 counts' stability on the survival. The proposed framework outperforms the most used joint models, highlighting the importance in correctly taking account the individual heterogeneity for the measurement errors variance and the evolution of the disease over time in bringing new insights to better understand this biomarker-survival relation.

</details>

<details>

<summary>2016-04-30 01:42:29 - Scalable posterior approximations for large-scale Bayesian inverse problems via likelihood-informed parameter and state reduction</summary>

- *Tiangang Cui, Youssef M. Marzouk, Karen E. Willcox*

- `1510.06053v2` - [abs](http://arxiv.org/abs/1510.06053v2) - [pdf](http://arxiv.org/pdf/1510.06053v2)

> Two major bottlenecks to the solution of large-scale Bayesian inverse problems are the scaling of posterior sampling algorithms to high-dimensional parameter spaces and the computational cost of forward model evaluations. Yet incomplete or noisy data, the state variation and parameter dependence of the forward model, and correlations in the prior collectively provide useful structure that can be exploited for dimension reduction in this setting--both in the parameter space of the inverse problem and in the state space of the forward model. To this end, we show how to jointly construct low-dimensional subspaces of the parameter space and the state space in order to accelerate the Bayesian solution of the inverse problem. As a byproduct of state dimension reduction, we also show how to identify low-dimensional subspaces of the data in problems with high-dimensional observations. These subspaces enable approximation of the posterior as a product of two factors: (i) a projection of the posterior onto a low-dimensional parameter subspace, wherein the original likelihood is replaced by an approximation involving a reduced model; and (ii) the marginal prior distribution on the high-dimensional complement of the parameter subspace. We present and compare several strategies for constructing these subspaces using only a limited number of forward and adjoint model simulations. The resulting posterior approximations can rapidly be characterized using standard sampling techniques, e.g., Markov chain Monte Carlo. Two numerical examples demonstrate the accuracy and efficiency of our approach: inversion of an integral equation in atmospheric remote sensing, where the data dimension is very high; and the inference of a heterogeneous transmissivity field in a groundwater system, which involves a partial differential equation forward model with high dimensional state and parameters.

</details>


## 2016-05

<details>

<summary>2016-05-03 16:23:20 - A Bayesian Nonparametric Markovian Model for Nonstationary Time Series</summary>

- *Maria DeYoreo, Athanasios Kottas*

- `1601.04331v3` - [abs](http://arxiv.org/abs/1601.04331v3) - [pdf](http://arxiv.org/pdf/1601.04331v3)

> Stationary time series models built from parametric distributions are, in general, limited in scope due to the assumptions imposed on the residual distribution and autoregression relationship. We present a modeling approach for univariate time series data, which makes no assumptions of stationarity, and can accommodate complex dynamics and capture nonstandard distributions. The model for the transition density arises from the conditional distribution implied by a Bayesian nonparametric mixture of bivariate normals. This implies a flexible autoregressive form for the conditional transition density, defining a time-homogeneous, nonstationary, Markovian model for real-valued data indexed in discrete-time. To obtain a more computationally tractable algorithm for posterior inference, we utilize a square-root-free Cholesky decomposition of the mixture kernel covariance matrix. Results from simulated data suggest the model is able to recover challenging transition and predictive densities. We also illustrate the model on time intervals between eruptions of the Old Faithful geyser. Extensions to accommodate higher order structure and to develop a state-space model are also discussed.

</details>

<details>

<summary>2016-05-04 05:01:36 - A note on Bayesian wavelet-based estimation of scaling</summary>

- *Minkyoung Kang, Brani Vidakovic*

- `1605.01146v1` - [abs](http://arxiv.org/abs/1605.01146v1) - [pdf](http://arxiv.org/pdf/1605.01146v1)

> A number of phenomena in various fields such as geology, atmospheric sciences, economics, to list a few, can be modeled as a fractional Brownian motion indexed by Hurst exponent $H$. This exponent is related to the degree of regularity and self-similarity present in the signal, and it often captures important characteristics useful in various applications. Given its importance, a number of methods have been developed for the estimation of the Hurst exponent. Typically, the proposed methods do not utilize prior information about scaling of a signal. Some signals are known to possess a theoretical value of the Hurst exponent, which motivates us to propose a Bayesian approach that incorporates this information via a suitable elicited prior distribution on $H$.   This significantly improves the accuracy of the estimation, as we demonstrate by simulations. Moreover, the proposed method is robust to small misspecifications of the prior location. The proposed method is applied to a turbulence time series for which Hurst exponent is theoretically known by Kolmogorov's K41 theory.

</details>

<details>

<summary>2016-05-04 08:52:10 - Linear Bandit algorithms using the Bootstrap</summary>

- *Nandan Sudarsanam, Balaraman Ravindran*

- `1605.01185v1` - [abs](http://arxiv.org/abs/1605.01185v1) - [pdf](http://arxiv.org/pdf/1605.01185v1)

> This study presents two new algorithms for solving linear stochastic bandit problems. The proposed methods use an approach from non-parametric statistics called bootstrapping to create confidence bounds. This is achieved without making any assumptions about the distribution of noise in the underlying system. We present the X-Random and X-Fixed bootstrap bandits which correspond to the two well-known approaches for conducting bootstraps on models, in the literature. The proposed methods are compared to other popular solutions for linear stochastic bandit problems, namely, OFUL, LinUCB and Thompson Sampling. The comparisons are carried out using a simulation study on a hierarchical probability meta-model, built from published data of experiments, which are run on real systems. The model representing the response surfaces is conceptualized as a Bayesian Network which is presented with varying degrees of noise for the simulations. One of the proposed methods, X-Random bootstrap, performs better than the baselines in-terms of cumulative regret across various degrees of noise and different number of trials. In certain settings the cumulative regret of this method is less than half of the best baseline. The X-Fixed bootstrap performs comparably in most situations and particularly well when the number of trials is low. The study concludes that these algorithms could be a preferred alternative for solving linear bandit problems, especially when the distribution of the noise in the system is unknown.

</details>

<details>

<summary>2016-05-05 20:29:47 - Intrinsic Bayesian Analysis for Occupancy Models</summary>

- *Daniel Taylor-Rodriguez, Andrew Womack, Claudio Fuentes, Nikolay Bliznyuk*

- `1508.07403v2` - [abs](http://arxiv.org/abs/1508.07403v2) - [pdf](http://arxiv.org/pdf/1508.07403v2)

> Occupancy models are typically used to determine the probability of a species being present at a given site while accounting for imperfect detection. The survey data underlying these models often include information on several predictors that could potentially characterize habitat suitability and species detectability. Because these variables might not all be relevant, model selection techniques are necessary in this context. In practice, model selection is performed using the Akaike Information Criterion (AIC), as few other alternatives are available. This paper builds an objective Bayesian variable selection framework for occupancy models through the intrinsic prior methodology. The procedure incorporates priors on the model space that account for test multiplicity and respect the polynomial hierarchy of the predictors when higher-order terms are considered. The methodology is implemented using a stochastic search algorithm that is able to thoroughly explore large spaces of occupancy models. The proposed strategy is entirely automatic and provides control of false positives without sacrificing the discovery of truly meaningful covariates. The performance of the method is evaluated and compared to AIC through a simulation study. The method is illustrated on two datasets previously studied in the literature.

</details>

<details>

<summary>2016-05-05 22:56:13 - Provable Bayesian Inference via Particle Mirror Descent</summary>

- *Bo Dai, Niao He, Hanjun Dai, Le Song*

- `1506.03101v3` - [abs](http://arxiv.org/abs/1506.03101v3) - [pdf](http://arxiv.org/pdf/1506.03101v3)

> Bayesian methods are appealing in their flexibility in modeling complex data and ability in capturing uncertainty in parameters. However, when Bayes' rule does not result in tractable closed-form, most approximate inference algorithms lack either scalability or rigorous guarantees. To tackle this challenge, we propose a simple yet provable algorithm, \emph{Particle Mirror Descent} (PMD), to iteratively approximate the posterior density. PMD is inspired by stochastic functional mirror descent where one descends in the density space using a small batch of data points at each iteration, and by particle filtering where one uses samples to approximate a function. We prove result of the first kind that, with $m$ particles, PMD provides a posterior density estimator that converges in terms of $KL$-divergence to the true posterior in rate $O(1/\sqrt{m})$. We demonstrate competitive empirical performances of PMD compared to several approximate inference algorithms in mixture models, logistic regression, sparse Gaussian processes and latent Dirichlet allocation on large scale datasets.

</details>

<details>

<summary>2016-05-06 11:01:32 - Flexible objective Bayesian linear regression with applications in survival analysis</summary>

- *F. J. Rubio, K. Yu*

- `1605.01889v1` - [abs](http://arxiv.org/abs/1605.01889v1) - [pdf](http://arxiv.org/pdf/1605.01889v1)

> We study objective Bayesian inference for linear regression models with residual errors distributed according to the class of two-piece scale mixtures of normal distributions. These models allow for capturing departures from the usual assumption of normality of the errors in terms of heavy tails, asymmetry, and certain types of heteroscedasticity. We propose a general noninformative, scale-invariant, prior structure and provide sufficient conditions for the propriety of the posterior distribution of the model parameters, which cover cases when the response variables are censored. These results allow us to apply the proposed models in the context of survival analysis. This paper represents an extension to the Bayesian framework of the models proposed in Rubio and Hong (2015). We present a simulation study that shows good frequentist properties of the posterior credible intervals as well as point estimators associated to the proposed priors. We illustrate the performance of these models with real data in the context of survival analysis of cancer patients.

</details>

<details>

<summary>2016-05-06 21:47:36 - Distributed Learning with Infinitely Many Hypotheses</summary>

- *Angelia NediÄ, Alex Olshevsky, CÃ©sar Uribe*

- `1605.02105v1` - [abs](http://arxiv.org/abs/1605.02105v1) - [pdf](http://arxiv.org/pdf/1605.02105v1)

> We consider a distributed learning setup where a network of agents sequentially access realizations of a set of random variables with unknown distributions. The network objective is to find a parametrized distribution that best describes their joint observations in the sense of the Kullback-Leibler divergence. Apart from recent efforts in the literature, we analyze the case of countably many hypotheses and the case of a continuum of hypotheses. We provide non-asymptotic bounds for the concentration rate of the agents' beliefs around the correct hypothesis in terms of the number of agents, the network parameters, and the learning abilities of the agents. Additionally, we provide a novel motivation for a general set of distributed Non-Bayesian update rules as instances of the distributed stochastic mirror descent algorithm.

</details>

<details>

<summary>2016-05-09 07:26:11 - P-splines with derivative based penalties and tensor product smoothing of unevenly distributed data</summary>

- *Simon N. Wood*

- `1605.02446v1` - [abs](http://arxiv.org/abs/1605.02446v1) - [pdf](http://arxiv.org/pdf/1605.02446v1)

> The P-splines of Eilers and Marx (1996) combine a B-spline basis with a discrete quadratic penalty on the basis coefficients, to produce a reduced rank spline like smoother. P-splines have three properties that make them very popular as reduced rank smoothers: i) the basis and the penalty are sparse, enabling efficient computation, especially for Bayesian stochastic simulation; ii) it is possible to flexibly `mix-and-match' the order of B-spline basis and penalty, rather than the order of penalty controlling the order of the basis as in spline smoothing; iii) it is very easy to set up the B-spline basis functions and penalties. The discrete penalties are somewhat less interpretable in terms of function shape than the traditional derivative based spline penalties, but tend towards penalties proportional to traditional spline penalties in the limit of large basis size. However part of the point of P-splines is not to use a large basis size. In addition the spline basis functions arise from solving functional optimization problems involving derivative based penalties, so moving to discrete penalties for smoothing may not always be desirable. The purpose of this note is to point out that the three properties of basis-penalty sparsity, mix-and-match penalization and ease of setup are readily obtainable with B-splines subject to derivative based penalization. The penalty setup typically requires a few lines of code, rather than the two lines typically required for P-splines, but this one off disadvantage seems to be the only one associated with using derivative based penalties. As an example application, it is shown how basis-penalty sparsity enables efficient computation with tensor product smoothers of scattered data.

</details>

<details>

<summary>2016-05-09 07:34:53 - A Bayesian approach to constrained single- and multi-objective optimization</summary>

- *Paul Feliot, Julien Bect, Emmanuel Vazquez*

- `1510.00503v3` - [abs](http://arxiv.org/abs/1510.00503v3) - [pdf](http://arxiv.org/pdf/1510.00503v3)

> This article addresses the problem of derivative-free (single- or multi-objective) optimization subject to multiple inequality constraints. Both the objective and constraint functions are assumed to be smooth, non-linear and expensive to evaluate. As a consequence, the number of evaluations that can be used to carry out the optimization is very limited, as in complex industrial design optimization problems. The method we propose to overcome this difficulty has its roots in both the Bayesian and the multi-objective optimization literatures. More specifically, an extended domination rule is used to handle objectives and constraints in a unified way, and a corresponding expected hyper-volume improvement sampling criterion is proposed. This new criterion is naturally adapted to the search of a feasible point when none is available, and reduces to existing Bayesian sampling criteria---the classical Expected Improvement (EI) criterion and some of its constrained/multi-objective extensions---as soon as at least one feasible point is available. The calculation and optimization of the criterion are performed using Sequential Monte Carlo techniques. In particular, an algorithm similar to the subset simulation method, which is well known in the field of structural reliability, is used to estimate the criterion. The method, which we call BMOO (for Bayesian Multi-Objective Optimization), is compared to state-of-the-art algorithms for single- and multi-objective constrained optimization.

</details>

<details>

<summary>2016-05-09 12:41:27 - Gaussian process modeling for stochastic multi-fidelity simulators, with application to fire safety</summary>

- *RÃ©mi Stroh, Julien Bect, SÃ©verine Demeyer, Nicolas Fischer, Emmanuel Vazquez*

- `1605.02561v1` - [abs](http://arxiv.org/abs/1605.02561v1) - [pdf](http://arxiv.org/pdf/1605.02561v1)

> To assess the possibility of evacuating a building in case of a fire, a standard method consists in simulating the propagation of fire, using finite difference methods and takes into account the random behavior of the fire, so that the result of a simulation is non-deterministic. The mesh fineness tunes the quality of the numerical model, and its computational cost. Depending on the mesh fineness, one simulation can last anywhere from a few minutes to several weeks. In this article, we focus on predicting the behavior of the fire simulator at fine meshes, using cheaper results, at coarser meshes. In the literature of the design and analysis of computer experiments, such a problem is referred to as multi-fidelity prediction. Our contribution is to extend to the case of stochastic simulators the Bayesian multi-fidelity model proposed by Picheny and Ginsbourger (2013) and Tuo et al. (2014).

</details>

<details>

<summary>2016-05-09 18:06:02 - A Bayesian Search for the Higgs Particle</summary>

- *Shirin Golchi, Richard Lockhart*

- `1501.02226v2` - [abs](http://arxiv.org/abs/1501.02226v2) - [pdf](http://arxiv.org/pdf/1501.02226v2)

> The statistical procedure used in the search for the Higgs boson is investigated in this paper. A Bayesian hierarchical model is proposed that uses the information provided by the theory in the analysis of the data generated by the particle detectors. In addition, we develop a Bayesian decision making procedure that combines the two steps of the current method (discovery and exclusion) into one and can be calibrated to satisfy frequency theory error rate requirements. .

</details>

<details>

<summary>2016-05-11 05:52:08 - Practical Bayesian Tomography</summary>

- *Christopher Granade, Joshua Combes, D. G. Cory*

- `1509.03770v2` - [abs](http://arxiv.org/abs/1509.03770v2) - [pdf](http://arxiv.org/pdf/1509.03770v2)

> In recent years, Bayesian methods have been proposed as a solution to a wide range of issues in quantum state and process tomography. State-of-the-art Bayesian tomography solutions suffer from three problems: numerical intractability, a lack of informative prior distributions, and an inability to track time-dependent processes. Here, we address all three problems. First, we use modern statistical methods, as pioneered by Husz\'ar and Houlsby and by Ferrie, to make Bayesian tomography numerically tractable. Our approach allows for practical computation of Bayesian point and region estimators for quantum states and channels. Second, we propose the first priors on quantum states and channels that allow for including useful experimental insight. Finally, we develop a method that allows tracking of time-dependent states and estimates the drift and diffusion processes affecting a state. We provide source code and animated visual examples for our methods.

</details>

<details>

<summary>2016-05-11 07:03:58 - Model Selection Principles in Misspecified Models</summary>

- *Jinchi Lv, Jun S. Liu*

- `1005.5483v2` - [abs](http://arxiv.org/abs/1005.5483v2) - [pdf](http://arxiv.org/pdf/1005.5483v2)

> Model selection is of fundamental importance to high dimensional modeling featured in many contemporary applications. Classical principles of model selection include the Kullback-Leibler divergence principle and the Bayesian principle, which lead to the Akaike information criterion and Bayesian information criterion when models are correctly specified. Yet model misspecification is unavoidable when we have no knowledge of the true model or when we have the correct family of distributions but miss some true predictor. In this paper, we propose a family of semi-Bayesian principles for model selection in misspecified models, which combine the strengths of the two well-known principles. We derive asymptotic expansions of the semi-Bayesian principles in misspecified generalized linear models, which give the new semi-Bayesian information criteria (SIC). A specific form of SIC admits a natural decomposition into the negative maximum quasi-log-likelihood, a penalty on model dimensionality, and a penalty on model misspecification directly. Numerical studies demonstrate the advantage of the newly proposed SIC methodology for model selection in both correctly specified and misspecified models.

</details>

<details>

<summary>2016-05-11 09:10:11 - On Bayesian Asymptotics in Stochastic Differential Equations with Random Effects</summary>

- *Trisha Maitra, Sourabh Bhattacharya*

- `1407.3971v3` - [abs](http://arxiv.org/abs/1407.3971v3) - [pdf](http://arxiv.org/pdf/1407.3971v3)

> Delattre et al. (2013) investigated asymptotic properties of the maximum likelihood estimator of the population parameters of the random effects associated with n independent stochastic differential equations (SDEs) assuming that the SDEs are independent and identical (iid).   In this article, we consider the Bayesian approach to learning about the population parameters, and prove consistency and asymptotic normality of the corresponding posterior distribution in the iid set-up as well as when the SDEs are independent but non-identical.

</details>

<details>

<summary>2016-05-11 15:16:52 - Nonparametric hierarchical Bayesian quantiles</summary>

- *Luke Bornn, Neil Shephard, Reza Solgi*

- `1605.03471v1` - [abs](http://arxiv.org/abs/1605.03471v1) - [pdf](http://arxiv.org/pdf/1605.03471v1)

> Here we develop a method for performing nonparametric Bayesian inference on quantiles. Relying on geometric measure theory and employing a Hausdorff base measure, we are able to specify meaningful priors for the quantile while treating the distribution of the data otherwise nonparametrically. We further extend the method to a hierarchical model for quantiles of subpopulations, linking subgroups together solely through their quantiles. Our approach is computationally straightforward, allowing for censored and noisy data. We demonstrate the proposed methodology on simulated data and an applied problem from sports statistics, where it is observed to stabilize and improve inference and prediction.

</details>

<details>

<summary>2016-05-12 13:44:51 - A Bayesian nonparametric approach to testing for dependence between random variables</summary>

- *Sarah Filippi, Chris Holmes*

- `1506.00829v2` - [abs](http://arxiv.org/abs/1506.00829v2) - [pdf](http://arxiv.org/pdf/1506.00829v2)

> Nonparametric and nonlinear measures of statistical dependence between pairs of random variables are important tools in modern data analysis. In particular the emergence of large data sets can now support the relaxation of linearity assumptions implicit in traditional association scores such as correlation. Here we describe a Bayesian nonparametric procedure that leads to a tractable, explicit and analytic quantification of the relative evidence for dependence vs independence. Our approach uses Polya tree priors on the space of probability measures which can then be embedded within a decision theoretic test for dependence. Polya tree priors can accommodate known uncertainty in the form of the underlying sampling distribution and provides an explicit posterior probability measure of both dependence and independence. Well known advantages of having an explicit probability measure include: easy comparison of evidence across different studies; encoding prior information; quantifying changes in dependence across different experimental conditions, and; the integration of results within formal decision analysis.

</details>

<details>

<summary>2016-05-13 06:16:33 - Bayesian $D$-optimal designs for error-in-variables models</summary>

- *Maria Konstantinou, Holger Dette*

- `1605.04055v1` - [abs](http://arxiv.org/abs/1605.04055v1) - [pdf](http://arxiv.org/pdf/1605.04055v1)

> Bayesian optimality criteria provide a robust design strategy to parameter misspecification. We develop an approximate design theory for Bayesian $D$-optimality for non-linear regression models with covariates subject to measurement errors. Both maximum likelihood and least squares estimation are studied and explicit characterisations of the Bayesian $D$-optimal saturated designs for the Michaelis-Menten, Emax and exponential regression models are provided. Several data examples are considered for the case of no preference for specific parameter values, where Bayesian $D$-optimal saturated designs are calculated using the uniform prior and compared to several other designs, including the corresponding locally $D$-optimal designs, which are often used in practice.

</details>

<details>

<summary>2016-05-13 10:38:28 - Fast methods for training Gaussian processes on large data sets</summary>

- *Christopher J. Moore, Alvin J. K. Chua, Christopher P. L. Berry, Jonathan R. Gair*

- `1604.01250v2` - [abs](http://arxiv.org/abs/1604.01250v2) - [pdf](http://arxiv.org/pdf/1604.01250v2)

> Gaussian process regression (GPR) is a non-parametric Bayesian technique for interpolating or fitting data. The main barrier to further uptake of this powerful tool rests in the computational costs associated with the matrices which arise when dealing with large data sets. Here, we derive some simple results which we have found useful for speeding up the learning stage in the GPR algorithm, and especially for performing Bayesian model comparison between different covariance functions. We apply our techniques to both synthetic and real data and quantify the speed-up relative to using nested sampling to numerically evaluate model evidences.

</details>

<details>

<summary>2016-05-13 15:31:03 - High Dimensional Bayesian Optimisation and Bandits via Additive Models</summary>

- *Kirthevasan Kandasamy, Jeff Schneider, Barnabas Poczos*

- `1503.01673v3` - [abs](http://arxiv.org/abs/1503.01673v3) - [pdf](http://arxiv.org/pdf/1503.01673v3)

> Bayesian Optimisation (BO) is a technique used in optimising a $D$-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on $D$ even though the function depends on all $D$ dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.

</details>

<details>

<summary>2016-05-13 16:38:03 - Unbiased Bayesian Inference for Population Markov Jump Processes via Random Truncations</summary>

- *Anastasis Georgoulas, Jane Hillston, Guido Sanguinetti*

- `1509.08327v2` - [abs](http://arxiv.org/abs/1509.08327v2) - [pdf](http://arxiv.org/pdf/1509.08327v2)

> We consider continuous time Markovian processes where populations of individual agents interact stochastically according to kinetic rules. Despite the increasing prominence of such models in fields ranging from biology to smart cities, Bayesian inference for such systems remains challenging, as these are continuous time, discrete state systems with potentially infinite state-space. Here we propose a novel efficient algorithm for joint state / parameter posterior sampling in population Markov Jump processes. We introduce a class of pseudo-marginal sampling algorithms based on a random truncation method which enables a principled treatment of infinite state spaces. Extensive evaluation on a number of benchmark models shows that this approach achieves considerable savings compared to state of the art methods, retaining accuracy and fast convergence. We also present results on a synthetic biology data set showing the potential for practical usefulness of our work.

</details>

<details>

<summary>2016-05-14 14:41:13 - Simple trees in complex forests: Growing Take The Best by Approximate Bayesian Computation</summary>

- *Eric Schulz, Maarten Speekenbrink, BjÃ¶rn Meder*

- `1605.01598v2` - [abs](http://arxiv.org/abs/1605.01598v2) - [pdf](http://arxiv.org/pdf/1605.01598v2)

> How can heuristic strategies emerge from smaller building blocks? We propose Approximate Bayesian Computation as a computational solution to this problem. As a first proof of concept, we demonstrate how a heuristic decision strategy such as Take The Best (TTB) can be learned from smaller, probabilistically updated building blocks. Based on a self-reinforcing sampling scheme, different building blocks are combined and, over time, tree-like non-compensatory heuristics emerge. This new algorithm, coined Approximately Bayesian Computed Take The Best (ABC-TTB), is able to recover a data set that was generated by TTB, leads to sensible inferences about cue importance and cue directions, can outperform traditional TTB, and allows to trade-off performance and computational effort explicitly.

</details>

<details>

<summary>2016-05-15 00:24:31 - Default Bayesian analysis with global-local shrinkage priors</summary>

- *Anindya Bhadra, Jyotishka Datta, Nicholas G. Polson, Brandon T. Willard*

- `1510.03516v2` - [abs](http://arxiv.org/abs/1510.03516v2) - [pdf](http://arxiv.org/pdf/1510.03516v2)

> We provide a framework for assessing the default nature of a prior distribution using the property of regular variation, which we study for global-local shrinkage priors. In particular, we demonstrate the horseshoe priors, originally designed to handle sparsity, also possess regular variation and thus are appropriate for default Bayesian analysis. To illustrate our methodology, we solve a problem of non-informative priors due to Efron (1973), who showed standard flat non-informative priors in high-dimensional normal means model can be highly informative for nonlinear parameters of interest. We consider four such problems and show global-local shrinkage priors such as the horseshoe and horseshoe+ perform as Efron (1973) requires in each case. We find the reason for this lies in the ability of the global-local shrinkage priors to separate a low-dimensional signal embedded in high-dimensional noise, even for nonlinear functions.

</details>

<details>

<summary>2016-05-16 17:40:12 - Social Learning and Distributed Hypothesis Testing</summary>

- *Anusha Lalitha, Tara Javidi, Anand Sarwate*

- `1410.4307v5` - [abs](http://arxiv.org/abs/1410.4307v5) - [pdf](http://arxiv.org/pdf/1410.4307v5)

> This paper considers a problem of distributed hypothesis testing and social learning. Individual nodes in a network receive noisy local (private) observations whose distribution is parameterized by a discrete parameter (hypotheses). The conditional distributions are known locally at the nodes, but the true parameter/hypothesis is not known. An update rule is analyzed in which nodes first perform a Bayesian update of their belief (distribution estimate) of the parameter based on their local observation, communicate these updates to their neighbors, and then perform a "non-Bayesian" linear consensus using the log-beliefs of their neighbors. In this paper we show that under mild assumptions, the belief of any node in any incorrect hypothesis converges to zero exponentially fast, and we characterize the exponential rate of learning which is given in terms of the network structure and the divergences between the observations' distributions. Our main result is the concentration property established on the rate of convergence.

</details>

<details>

<summary>2016-05-18 04:01:05 - Methods for Bayesian Variable Selection with Binary Response Data using the EM Algorithm</summary>

- *Patrick McDermott, John Snyder, Rebecca Willison*

- `1605.05429v1` - [abs](http://arxiv.org/abs/1605.05429v1) - [pdf](http://arxiv.org/pdf/1605.05429v1)

> High-dimensional Bayesian variable selection problems are often solved using computationally expensive Markov Chain Montle Carlo (MCMC) techniques. Recently, a Bayesian variable selection technique was developed for continuous data using the EM algorithm called EMVS. We extend the EMVS method to binary data by proposing both a logistic and probit extension. To preserve the computational speed of EMVS we also implemented the Stochastic Dual Coordinate Descent (SDCA) algorithm. Further, we conduct two extensive simulation studies to show the computational speed of both methods. These simulation studies reveal the power of both methods to quickly identify the correct sparse model. When these EMVS methods are compared to Stochastic Search Variable Selection (SSVS), the EMVS methods surpass SSVS both in terms of computational speed and correctly identifying significant variables. Finally, we illustrate the effectiveness of both methods on two well-known gene expression datasets. Our results mirror the results of previous examinations of these datasets with far less computational cost.

</details>

<details>

<summary>2016-05-18 13:52:01 - Bayesian inference for diffusion driven mixed-effects models</summary>

- *Gavin A. Whitaker, Andrew Golightly, Richard J. Boys, Chris Sherlock*

- `1507.06807v2` - [abs](http://arxiv.org/abs/1507.06807v2) - [pdf](http://arxiv.org/pdf/1507.06807v2)

> Stochastic differential equations (SDEs) provide a natural framework for modelling intrinsic stochasticity inherent in many continuous-time physical processes. When such processes are observed in multiple individuals or experimental units, SDE driven mixed-effects models allow the quantification of between (as well as within) individual variation. Performing Bayesian inference for such models, using discrete time data that may be incomplete and subject to measurement error is a challenging problem and is the focus of this paper. We extend a recently proposed MCMC scheme to include the SDE driven mixed-effects framework. Fundamental to our approach is the development of a novel construct that allows for efficient sampling of conditioned SDEs that may exhibit nonlinear dynamics between observation times. We apply the resulting scheme to synthetic data generated from a simple SDE model of orange tree growth, and real data consisting of observations on aphid numbers recorded under a variety of different treatment regimes. In addition, we provide a systematic comparison of our approach with an inference scheme based on a tractable approximation of the SDE, that is, the linear noise approximation.

</details>

<details>

<summary>2016-05-18 14:42:47 - Bayesian Robust Quantile Regression</summary>

- *Mauro Bernardi, Marco Bottone, Lea Petrella*

- `1605.05602v1` - [abs](http://arxiv.org/abs/1605.05602v1) - [pdf](http://arxiv.org/pdf/1605.05602v1)

> Traditional Bayesian quantile regression relies on the Asymmetric Laplace distribution (ALD) mainly because of its satisfactory empirical and theoretical performances. However, the ALD displays medium tails and it is not suitable for data characterized by strong deviations from the Gaussian hypothesis. In this paper, we propose an extension of the ALD Bayesian quantile regression framework to account for fat-tails using the Skew Exponential Power (SEP) distribution. Beside having the $\tau$-level quantile as parameter, the SEP distribution has an additional key parameter governing the decay of the tails, making it attractive for robust modeling of conditional quantiles at different confidence levels. Linear and Generalized Additive Models (GAM) with penalized spline are considered to show the flexibility of the SEP in the Bayesian quantile regression context. Lasso priors are considered in both cases to account for shrinking parameters problem when the parameters space becomes wide. To implement the Bayesian inference we propose a new adaptive Metropolis--Hastings algorithm in the linear model and an adaptive Metropolis within Gibbs one in the GAM framework. Empirical evidence of the statistical properties of the proposed SEP Bayesian quantile regression method is provided through several example based on simulated and real dataset.

</details>

<details>

<summary>2016-05-18 15:16:28 - Variational Gaussian Copula Inference</summary>

- *Shaobo Han, Xuejun Liao, David B. Dunson, Lawrence Carin*

- `1506.05860v3` - [abs](http://arxiv.org/abs/1506.05860v3) - [pdf](http://arxiv.org/pdf/1506.05860v3)

> We utilize copulas to constitute a unified framework for constructing and optimizing variational proposals in hierarchical Bayesian models. For models with continuous and non-Gaussian hidden variables, we propose a semiparametric and automated variational Gaussian copula approach, in which the parametric Gaussian copula family is able to preserve multivariate posterior dependence, and the nonparametric transformations based on Bernstein polynomials provide ample flexibility in characterizing the univariate marginal posteriors.

</details>

<details>

<summary>2016-05-18 16:35:52 - Bayesian methods for event analysis of intracellular currents</summary>

- *Josh Merel, Ben Shababo, Alex Naka, Hillel Adesnik, Liam Paninski*

- `1603.06230v2` - [abs](http://arxiv.org/abs/1603.06230v2) - [pdf](http://arxiv.org/pdf/1603.06230v2)

> Investigation of neural circuit functioning often requires statistical interpretation of events in subthreshold electrophysiological recordings. This problem is non-trivial because recordings may have moderate levels of structured noise and events may have distinct kinetics. In addition, novel experimental designs that combine optical and electrophysiological methods will depend upon statistical tools that combine multimodal data. We present a Bayesian approach for inferring the timing, strength, and kinetics of postsynaptic currents (PSCs) from voltage-clamp recordings on a per event basis. The simple generative model for a single voltage-clamp recording flexibly extends to include network-level structure to enable experiments designed to probe synaptic connectivity. We validate the approach on simulated and real data. We also demonstrate that extensions of the basic PSC detection algorithm can handle recordings contaminated with optically evoked currents, and we simulate a scenario in which calcium imaging observations, available for a subset of neurons, can be fused with electrophysiological data to achieve higher temporal resolution. We apply this approach to simulated and real ground truth data to demonstrate its higher sensitivity in detecting small signal-to-noise events and its increased robustness to noise compared to standard methods for detecting PSCs. The new Bayesian event analysis approach for electrophysiological recordings should allow for better estimation of physiological parameters under more variable conditions and help support new experimental designs for circuit mapping.

</details>

<details>

<summary>2016-05-18 17:54:39 - Sub-optimality of some continuous shrinkage priors</summary>

- *Anirban Bhattacharya, David B. Dunson, Debdeep Pati, Natesh S. Pillai*

- `1605.05671v1` - [abs](http://arxiv.org/abs/1605.05671v1) - [pdf](http://arxiv.org/pdf/1605.05671v1)

> Two-component mixture priors provide a traditional way to induce sparsity in high-dimensional Bayes models. However, several aspects of such a prior, including computational complexities in high-dimensions, interpretation of exact zeros and non-sparse posterior summaries under standard loss functions, has motivated an amazing variety of continuous shrinkage priors, which can be expressed as global-local scale mixtures of Gaussians. Interestingly, we demonstrate that many commonly used shrinkage priors, including the Bayesian Lasso, do not have adequate posterior concentration in high-dimensional settings.

</details>

<details>

<summary>2016-05-19 16:06:09 - Bayesian Spatial Monotonic Multiple Regression</summary>

- *Christian Rohrbeck, Deborah Costain, Arnoldo Frigessi*

- `1605.06025v1` - [abs](http://arxiv.org/abs/1605.06025v1) - [pdf](http://arxiv.org/pdf/1605.06025v1)

> We consider monotonic, multiple regression for a set of contiguous regions (lattice data). The regression functions permissibly vary between regions and exhibit geographical structure. We develop new Bayesian non-parametric methodology which allows for both continuous and discontinuous functional shapes and which are estimated using marked point processes and reversible jump Markov Chain Monte Carlo techniques. Geographical dependency is incorporated by a flexible prior distribution; the parametrisation allows the dependency to vary with functional level. The approach is tuned using Bayesian global optimization and cross-validation. Estimates enable variable selection, threshold detection and prediction as well as the extrapolation of the regression function. Performance and flexibility of our approach is illustrated by simulation studies and an application to a Norwegian insurance data set.

</details>

<details>

<summary>2016-05-20 00:31:07 - Variational hybridization and transformation for large inaccurate noisy-or networks</summary>

- *Yusheng Xie, Nan Du, Wei Fan, Jing Zhai, Weicheng Zhu*

- `1605.06181v1` - [abs](http://arxiv.org/abs/1605.06181v1) - [pdf](http://arxiv.org/pdf/1605.06181v1)

> Variational inference provides approximations to the computationally intractable posterior distribution in Bayesian networks. A prominent medical application of noisy-or Bayesian network is to infer potential diseases given observed symptoms. Previous studies focus on approximating a handful of complicated pathological cases using variational transformation. Our goal is to use variational transformation as part of a novel hybridized inference for serving reliable and real time diagnosis at web scale. We propose a hybridized inference that allows variational parameters to be estimated without disease posteriors or priors, making the inference faster and much of its computation recyclable. In addition, we propose a transformation ranking algorithm that is very stable to large variances in network prior probabilities, a common issue that arises in medical applications of Bayesian networks. In experiments, we perform comparative study on a large real life medical network and scalability study on a much larger (36,000x) synthesized network.

</details>

<details>

<summary>2016-05-20 11:12:23 - Sequential design of experiments for estimating percentiles of black-box functions</summary>

- *T Labopin-Richard, V Picheny*

- `1605.05524v2` - [abs](http://arxiv.org/abs/1605.05524v2) - [pdf](http://arxiv.org/pdf/1605.05524v2)

> Estimating percentiles of black-box deterministic functions with random inputs is a challenging task when the number of function evaluations is severely restricted, which is typical for computer experiments. This article proposes two new sequential Bayesian methods for percentile estimation based on the Gaussian Process metamodel. Both rely on the Stepwise Uncertainty Reduction paradigm, hence aim at providing a sequence of function evaluations that reduces an uncertainty measure associated with the percentile estimator. The proposed strategies are tested on several numerical examples, showing that accurate estimators can be obtained using only a small number of functions evaluations.

</details>

<details>

<summary>2016-05-21 04:42:44 - Bayesian Analysis of Modified Weibull distribution under progressively censored competing risk model</summary>

- *Arabin Kumar Dey, Abhilash Jha, Sanku Dey*

- `1605.06585v1` - [abs](http://arxiv.org/abs/1605.06585v1) - [pdf](http://arxiv.org/pdf/1605.06585v1)

> In this paper we study bayesian analysis of Modified Weibull distribution under progressively censored competing risk model. This study is made for progressively censored data. We use deterministic scan Gibbs sampling combined with slice sampling to generate from the posterior distribution. Posterior distribution is formed by taking prior distribution as reference prior. A real life data analysis is shown for illustrative purpose.

</details>

<details>

<summary>2016-05-23 15:16:51 - Bayesian Model Selection of Stochastic Block Models</summary>

- *Xiaoran Yan*

- `1605.07057v1` - [abs](http://arxiv.org/abs/1605.07057v1) - [pdf](http://arxiv.org/pdf/1605.07057v1)

> A central problem in analyzing networks is partitioning them into modules or communities. One of the best tools for this is the stochastic block model, which clusters vertices into blocks with statistically homogeneous pattern of links. Despite its flexibility and popularity, there has been a lack of principled statistical model selection criteria for the stochastic block model. Here we propose a Bayesian framework for choosing the number of blocks as well as comparing it to the more elaborate degree- corrected block models, ultimately leading to a universal model selection framework capable of comparing multiple modeling combinations. We will also investigate its connection to the minimum description length principle.

</details>

<details>

<summary>2016-05-23 20:19:39 - Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models</summary>

- *Aki Vehtari, Tommi Mononen, Ville Tolvanen, Tuomas Sivula, Ole Winther*

- `1412.7461v3` - [abs](http://arxiv.org/abs/1412.7461v3) - [pdf](http://arxiv.org/pdf/1412.7461v3)

> The future predictive performance of a Bayesian model can be estimated using Bayesian cross-validation. In this article, we consider Gaussian latent variable models where the integration over the latent values is approximated using the Laplace method or expectation propagation (EP). We study the properties of several Bayesian leave-one-out (LOO) cross-validation approximations that in most cases can be computed with a small additional cost after forming the posterior approximation given the full data. Our main objective is to assess the accuracy of the approximative LOO cross-validation estimators. That is, for each method (Laplace and EP) we compare the approximate fast computation with the exact brute force LOO computation. Secondarily, we evaluate the accuracy of the Laplace and EP approximations themselves against a ground truth established through extensive Markov chain Monte Carlo simulation. Our empirical results show that the approach based upon a Gaussian approximation to the LOO marginal distribution (the so-called cavity distribution) gives the most accurate and reliable results among the fast methods.

</details>

<details>

<summary>2016-05-24 06:57:30 - Minimum Regret Search for Single- and Multi-Task Optimization</summary>

- *Jan Hendrik Metzen*

- `1602.01064v3` - [abs](http://arxiv.org/abs/1602.01064v3) - [pdf](http://arxiv.org/pdf/1602.01064v3)

> We propose minimum regret search (MRS), a novel acquisition function for Bayesian optimization. MRS bears similarities with information-theoretic approaches such as entropy search (ES). However, while ES aims in each query at maximizing the information gain with respect to the global maximum, MRS aims at minimizing the expected simple regret of its ultimate recommendation for the optimum. While empirically ES and MRS perform similar in most of the cases, MRS produces fewer outliers with high simple regret than ES. We provide empirical results both for a synthetic single-task optimization problem as well as for a simulated multi-task robotic control problem.

</details>

<details>

<summary>2016-05-24 07:11:44 - Posterior consistency and convergence rates for Bayesian inversion with hypoelliptic operators</summary>

- *Hanne Kekkonen, Matti Lassas, Samuli Siltanen*

- `1507.01772v2` - [abs](http://arxiv.org/abs/1507.01772v2) - [pdf](http://arxiv.org/pdf/1507.01772v2)

> Bayesian approach to inverse problems is studied in the case where the forward map is a linear hypoelliptic pseudodifferential operator and measurement error is additive white Gaussian noise. The measurement model for an unknown Gaussian random variable $U(x,\omega)$ is \begin{eqnarray*} M(y,\omega) = A(U(x,\omega) )+ \delta\hspace{.2mm}\mathcal{E}(y,\omega), \end{eqnarray*} where $A$ is a finitely many times smoothing linear hypoelliptic operator and $\delta>0$ is the noise magnitude. The covariance operator $C_U$ of $U$ is $2r$ times smoothing, self-adjoint, injective and elliptic pseudodifferential operator.   If $\mathcal{E}$ was taking values in $L^2$ then in Gaussian case solving the conditional mean (and maximum a posteriori) estimate is linked to solving the minimisation problem \begin{eqnarray*} T_\delta(M) = \text{argmin}_{u\in H^r}   \big\{\|A u-m\|_{L^2}^2+ \delta^2\|C_U^{-1/2}u\|_{L^2}^2 \big\}. \end{eqnarray*} However, Gaussian white noise does not take values in $L^2$ but in $H^{-s}$ where $s>0$ is big enough. A modification of the above approach to solve the inverse problem is presented, covering the case of white Gaussian measurement noise. Furthermore, the convergence of conditional mean estimate to the correct solution as $\delta\rightarrow 0$ is proven in appropriate function spaces using microlocal analysis. Also the contraction of the confidence regions is studied.

</details>

<details>

<summary>2016-05-24 12:48:19 - Gaussian process emulators for computer experiments with inequality constraints</summary>

- *Hassan Maatouk, Xavier Bay*

- `1606.01265v1` - [abs](http://arxiv.org/abs/1606.01265v1) - [pdf](http://arxiv.org/pdf/1606.01265v1)

> Physical phenomena are observed in many fields (sciences and engineering) and are often studied by time-consuming computer codes. These codes are analyzed with statistical models, often called emulators. In many situations, the physical system (computer model output) may be known to satisfy inequality constraints with respect to some or all input variables. Our aim is to build a model capable of incorporating both data interpolation and inequality constraints into a Gaussian process emulator. By using a functional decomposition, we propose to approximate the original Gaussian process by a finite-dimensional Gaussian process such that all conditional simulations satisfy the inequality constraints in the whole domain. The mean, mode (maximum a posteriori) and prediction intervals (uncertainty quantification) of the conditional Gaussian process are calculated. To investigate the performance of the proposed model, some conditional simulations with inequality constraints such as boundary, monotonicity or convexity conditions are given. 1. Introduction. In the engineering activity, runs of a computer code can be expensive and time-consuming. One solution is to use a statistical surrogate for conditioning computer model outputs at some input locations (design points). Gaussian process (GP) emulator is one of the most popular choices [23]. The reason comes from the property of the GP that uncertainty quantification can be calculated. Furthermore, it has several nice properties. For example, the conditional GP at observation data (linear equality constraints) is still a GP [5]. Additionally, some inequality constraints (such as monotonicity and convexity) of output computer responses are related to partial derivatives. In such cases, the partial derivatives of the GP are also GPs. Incorporating an infinite number of linear inequality constraints into a GP emulator, the problem becomes more difficult. The reason is that the resulting conditional process is not a GP. In the literature of interpolation with inequality constraints, we find two types of meth-ods. The first one is deterministic and based on splines, which have the advantage that the inequality constraints are satisfied in the whole input domain (see e.g. [16], [24] and [25]). The second one is based on the simulation of the conditional GP by using the subdivision of the input set (see e.g. [1], [6] and [11]). In that case, the inequality constraints are satisfied in a finite number of input locations. Notice that the advantage of such method is that un-certainty quantification can be calculated. In previous work, some methodologies have been based on the knowledge of the derivatives of the GP at some input locations ([11], [21] and [26]). For monotonicity constraints with noisy data, a Bayesian approach was developed in [21]. In [11] the problem is to build a GP emulator by using the prior monotonicity

</details>

<details>

<summary>2016-05-24 15:53:04 - An empirical Bayes approach to network recovery using external knowledge</summary>

- *Gino B. Kpogbezan, Aad W. van der Vaart, Wessel N. van Wieringen, GwenaÃ«l G. R. Leday, Mark A. van de Wiel*

- `1605.07514v1` - [abs](http://arxiv.org/abs/1605.07514v1) - [pdf](http://arxiv.org/pdf/1605.07514v1)

> Reconstruction of a high-dimensional network may benefit substantially from the inclusion of prior knowledge on the network topology. In the case of gene interaction networks such knowledge may come for instance from pathway repositories like KEGG, or be inferred from data of a pilot study. The Bayesian framework provides a natural means of including such prior knowledge. Based on a Bayesian Simultaneous Equation Model, we develop an appealing empirical Bayes procedure which automatically assesses the relevance of the used prior knowledge. We use variational Bayes method for posterior densities approximation and compare its accuracy with that of Gibbs sampling strategy. Our method is computationally fast, and can outperform known competitors. In a simulation study we show that accurate prior data can greatly improve the reconstruction of the network, but need not harm the reconstruction if wrong. We demonstrate the benefits of the method in an analysis of gene expression data from GEO. In particular, the edges of the recovered network have superior reproducibility (compared to that of competitors) over resampled versions of the data.

</details>

<details>

<summary>2016-05-24 16:12:11 - Bayesian Detection of Image Boundaries</summary>

- *Meng Li, Subhashis Ghosal*

- `1508.05847v3` - [abs](http://arxiv.org/abs/1508.05847v3) - [pdf](http://arxiv.org/pdf/1508.05847v3)

> Detecting boundary of an image based on noisy observations is a fundamental problem of image processing and image segmentation. For a $d$-dimensional image ($d = 2, 3, \ldots$), the boundary can often be described by a closed smooth $(d - 1)$-dimensional manifold. In this paper, we propose a nonparametric Bayesian approach based on priors indexed by $\mathbb{S}^{d - 1}$, the unit sphere in $\mathbb{R}^d$. We derive optimal posterior contraction rates using Gaussian processes or finite random series priors using basis functions such as trigonometric polynomials for 2-dimensional images and spherical harmonics for 3-dimensional images. For 2-dimensional images, we show a rescaled squared exponential Gaussian process on $\mathbb{S}^1$ achieves four goals of guaranteed geometric restriction, (nearly) minimax rate optimal and adaptive to the smoothness level, convenient for joint inference and computationally efficient. We conduct an extensive study of its reproducing kernel Hilbert space, which may be of interest by its own and can also be used in other contexts. Simulations confirm excellent performance of the proposed method and indicate its robustness under model misspecification at least under the simulated settings.

</details>

<details>

<summary>2016-05-25 03:18:09 - Learning Nonparametric Forest Graphical Models with Prior Information</summary>

- *Yuancheng Zhu, Zhe Liu, Siqi Sun*

- `1511.03796v2` - [abs](http://arxiv.org/abs/1511.03796v2) - [pdf](http://arxiv.org/pdf/1511.03796v2)

> We present a framework for incorporating prior information into nonparametric estimation of graphical models. To avoid distributional assumptions, we restrict the graph to be a forest and build on the work of forest density estimation (FDE). We reformulate the FDE approach from a Bayesian perspective, and introduce prior distributions on the graphs. As two concrete examples, we apply this framework to estimating scale-free graphs and learning multiple graphs with similar structures. The resulting algorithms are equivalent to finding a maximum spanning tree of a weighted graph with a penalty term on the connectivity pattern of the graph. We solve the optimization problem via a minorize-maximization procedure with Kruskal's algorithm. Simulations show that the proposed methods outperform competing parametric methods, and are robust to the true data distribution. They also lead to improvement in predictive power and interpretability in two real data sets.

</details>

<details>

<summary>2016-05-25 12:57:19 - One-Shot Generalization in Deep Generative Models</summary>

- *Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, Daan Wierstra*

- `1603.05106v2` - [abs](http://arxiv.org/abs/1603.05106v2) - [pdf](http://arxiv.org/pdf/1603.05106v2)

> Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples---having seen new examples just once---providing an important class of general-purpose models for one-shot machine learning.

</details>

<details>

<summary>2016-05-25 15:11:25 - Modelling and Bayesian analysis of the Abakaliki Smallpox Data</summary>

- *Jessica E. Stockdale, Theodore Kypraios, Philip D. O'Neill*

- `1605.07924v1` - [abs](http://arxiv.org/abs/1605.07924v1) - [pdf](http://arxiv.org/pdf/1605.07924v1)

> The celebrated Abakaliki smallpox data have appeared numerous times in the epidemic modelling literature, but in almost all cases only a specific subset of the data is considered. There is one previous analysis of the full data set, but this relies on approximation methods to derive a likelihood. The data themselves continue to be of interest due to concerns about the possible re-emergence of smallpox as a bioterrorism weapon. We present the first full Bayesian analysis using data-augmentation Markov chain Monte Carlo methods which avoid the need for likelihood approximations. Results include estimates of basic model parameters as well as reproduction numbers and the likely path of infection. Model assessment is carried out using simulation-based methods.

</details>

<details>

<summary>2016-05-25 17:49:12 - Bayesian Variable Selection and Estimation Based on Global-Local Shrinkage Priors</summary>

- *Xueying Tang, Xiaofan Xu, Malay Ghosh, Prasenjit Ghosh*

- `1605.07981v1` - [abs](http://arxiv.org/abs/1605.07981v1) - [pdf](http://arxiv.org/pdf/1605.07981v1)

> In this paper, we consider Bayesian variable selection problem of linear regression model with global-local shrinkage priors on the regression coefficients. We propose a variable selection procedure that select a variable if the ratio of the posterior mean to the ordinary least square estimate of the corresponding coefficient is greater than $1/2$. Under the assumption of orthogonal designs, we show that if the local parameters have polynomial-tailed priors, our proposed method enjoys the oracle property in the sense that it can achieve variable selection consistency and optimal estimation rate at the same time. However, if, instead, an exponential-tailed prior is used for the local parameters, the proposed method does not have the oracle property.

</details>

<details>

<summary>2016-05-25 18:33:10 - Toward a general, scaleable framework for Bayesian teaching with applications to topic models</summary>

- *Baxter S. Eaves Jr, Patrick Shafto*

- `1605.07999v1` - [abs](http://arxiv.org/abs/1605.07999v1) - [pdf](http://arxiv.org/pdf/1605.07999v1)

> Machines, not humans, are the world's dominant knowledge accumulators but humans remain the dominant decision makers. Interpreting and disseminating the knowledge accumulated by machines requires expertise, time, and is prone to failure. The problem of how best to convey accumulated knowledge from computers to humans is a critical bottleneck in the broader application of machine learning. We propose an approach based on human teaching where the problem is formalized as selecting a small subset of the data that will, with high probability, lead the human user to the correct inference. This approach, though successful for modeling human learning in simple laboratory experiments, has failed to achieve broader relevance due to challenges in formulating general and scalable algorithms. We propose general-purpose teaching via pseudo-marginal sampling and demonstrate the algorithm by teaching topic models. Simulation results show our sampling-based approach: effectively approximates the probability where ground-truth is possible via enumeration, results in data that are markedly different from those expected by random sampling, and speeds learning especially for small amounts of data. Application to movie synopsis data illustrates differences between teaching and random sampling for teaching distributions and specific topics, and demonstrates gains in scalability and applicability to real-world problems.

</details>

<details>

<summary>2016-05-25 18:46:56 - Dropout as a Bayesian Approximation: Appendix</summary>

- *Yarin Gal, Zoubin Ghahramani*

- `1506.02157v5` - [abs](http://arxiv.org/abs/1506.02157v5) - [pdf](http://arxiv.org/pdf/1506.02157v5)

> We show that a neural network with arbitrary depth and non-linearities, with dropout applied before every weight layer, is mathematically equivalent to an approximation to a well known Bayesian model. This interpretation might offer an explanation to some of dropout's key properties, such as its robustness to over-fitting. Our interpretation allows us to reason about uncertainty in deep learning, and allows the introduction of the Bayesian machinery into existing deep learning frameworks in a principled way.   This document is an appendix for the main paper "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning" by Gal and Ghahramani, 2015.

</details>

<details>

<summary>2016-05-26 20:01:38 - Approximate Bayesian estimation in large coloured graphical Gaussian models</summary>

- *Qiong Li, Xin Gao, Helene Massam*

- `1605.08441v1` - [abs](http://arxiv.org/abs/1605.08441v1) - [pdf](http://arxiv.org/pdf/1605.08441v1)

> Distributed estimation methods have recently been used to compute the maximum likelihood estimate of the precision matrix for large graphical Gaussian models. Our aim, in this paper, is to give a Bayesian estimate of the precision matrix for large graphical Gaussian models with, additionally, symmetry constraints imposed by an underlying graph which is coloured. We take the sample posterior mean of the precision matrix as our estimate. We study its asymptotic behaviour under the regular asymptotic regime when the number of variables p is fixed and under the double asymptotic regime when both p and n grow to infinity. We show in particular, that when the number of parameters of the local models is uniformly bounded, the standard convergence rate we obtain for the asymptotic consistency, in the Frobenius norm, of our estimate of the precision matrix compares well with the rates in the current literature for the maximum likelihood estimate.

</details>

<details>

<summary>2016-05-27 03:16:47 - Evaluation of Coded Aperture Radiation Detectors using a Bayesian Approach</summary>

- *K. Miller, P. Huggins, A. Dubrawski, S. Labov, K. Nelson*

- `1605.08499v1` - [abs](http://arxiv.org/abs/1605.08499v1) - [pdf](http://arxiv.org/pdf/1605.08499v1)

> We investigate the utility of coded aperture (CA) for roadside radiation threat detection applications. With coded aperture, information in the form of photon quantity is traded for directional information. Whether and in what scenarios this trade-off is beneficial is the focus of this study. We quantify the impact of a masking approach by comparing performance with an unmasked approach in terms of both detection and localization of a roadside nuclear threat. We measure performance over many instances of a drive-by scenario via Monte Carlo simulation based on empirical observations.

</details>

<details>

<summary>2016-05-27 11:15:55 - Mondrian Forests for Large-Scale Regression when Uncertainty Matters</summary>

- *Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh*

- `1506.03805v4` - [abs](http://arxiv.org/abs/1506.03805v4) - [pdf](http://arxiv.org/pdf/1506.03805v4)

> Many real-world regression problems demand a measure of the uncertainty associated with each prediction. Standard decision forests deliver efficient state-of-the-art predictive performance, but high-quality uncertainty estimates are lacking. Gaussian processes (GPs) deliver uncertainty estimates, but scaling GPs to large-scale data sets comes at the cost of approximating the uncertainty estimates. We extend Mondrian forests, first proposed by Lakshminarayanan et al. (2014) for classification problems, to the large-scale non-parametric regression setting. Using a novel hierarchical Gaussian prior that dovetails with the Mondrian forest framework, we obtain principled uncertainty estimates, while still retaining the computational advantages of decision forests. Through a combination of illustrative examples, real-world large-scale datasets, and Bayesian optimization benchmarks, we demonstrate that Mondrian forests outperform approximate GPs on large-scale regression tasks and deliver better-calibrated uncertainty assessments than decision-forest-based methods.

</details>

<details>

<summary>2016-05-27 13:00:31 - Variational Bayesian Inference for Hidden Markov Models With Multivariate Gaussian Output Distributions</summary>

- *Christian Gruhl, Bernhard Sick*

- `1605.08618v1` - [abs](http://arxiv.org/abs/1605.08618v1) - [pdf](http://arxiv.org/pdf/1605.08618v1)

> Hidden Markov Models (HMM) have been used for several years in many time series analysis or pattern recognitions tasks. HMM are often trained by means of the Baum-Welch algorithm which can be seen as a special variant of an expectation maximization (EM) algorithm. Second-order training techniques such as Variational Bayesian Inference (VI) for probabilistic models regard the parameters of the probabilistic models as random variables and define distributions over these distribution parameters, hence the name of this technique. VI can also bee regarded as a special case of an EM algorithm. In this article, we bring both together and train HMM with multivariate Gaussian output distributions with VI. The article defines the new training technique for HMM. An evaluation based on some case studies and a comparison to related approaches is part of our ongoing work.

</details>

<details>

<summary>2016-05-27 19:01:31 - EEF: Exponentially Embedded Families with Class-Specific Features for Classification</summary>

- *Bo Tang, Steven Kay, Haibo He, Paul M. Baggenstoss*

- `1605.03631v2` - [abs](http://arxiv.org/abs/1605.03631v2) - [pdf](http://arxiv.org/pdf/1605.03631v2)

> In this letter, we present a novel exponentially embedded families (EEF) based classification method, in which the probability density function (PDF) on raw data is estimated from the PDF on features. With the PDF construction, we show that class-specific features can be used in the proposed classification method, instead of a common feature subset for all classes as used in conventional approaches. We apply the proposed EEF classifier for text categorization as a case study and derive an optimal Bayesian classification rule with class-specific feature selection based on the Information Gain (IG) score. The promising performance on real-life data sets demonstrates the effectiveness of the proposed approach and indicates its wide potential applications.

</details>

<details>

<summary>2016-05-31 02:14:18 - Bayesian optimization under mixed constraints with a slack-variable augmented Lagrangian</summary>

- *Victor Picheny, Robert B. Gramacy, Stefan M. Wild, Sebastien Le Digabel*

- `1605.09466v1` - [abs](http://arxiv.org/abs/1605.09466v1) - [pdf](http://arxiv.org/pdf/1605.09466v1)

> An augmented Lagrangian (AL) can convert a constrained optimization problem into a sequence of simpler (e.g., unconstrained) problems, which are then usually solved with local solvers. Recently, surrogate-based Bayesian optimization (BO) sub-solvers have been successfully deployed in the AL framework for a more global search in the presence of inequality constraints; however, a drawback was that expected improvement (EI) evaluations relied on Monte Carlo. Here we introduce an alternative slack variable AL, and show that in this formulation the EI may be evaluated with library routines. The slack variables furthermore facilitate equality as well as inequality constraints, and mixtures thereof. We show how our new slack "ALBO" compares favorably to the original. Its superiority over conventional alternatives is reinforced on several mixed constraint examples.

</details>

<details>

<summary>2016-05-31 03:51:17 - A unified approach to mortality modelling using state-space framework: characterisation, identification, estimation and forecasting</summary>

- *Man Chung Fung, Gareth W. Peters, Pavel V. Shevchenko*

- `1605.09484v1` - [abs](http://arxiv.org/abs/1605.09484v1) - [pdf](http://arxiv.org/pdf/1605.09484v1)

> This paper explores and develops alternative statistical representations and estimation approaches for dynamic mortality models. The framework we adopt is to reinterpret popular mortality models such as the Lee-Carter class of models in a general state-space modelling methodology, which allows modelling, estimation and forecasting of mortality under a unified framework. Furthermore, we propose an alternative class of model identification constraints which is more suited to statistical inference in filtering and parameter estimation settings based on maximization of the marginalized likelihood or in Bayesian inference. We then develop a novel class of Bayesian state-space models which incorporate apriori beliefs about the mortality model characteristics as well as for more flexible and appropriate assumptions relating to heteroscedasticity that present in observed mortality data. We show that multiple period and cohort effect can be cast under a state-space structure. To study long term mortality dynamics, we introduce stochastic volatility to the period effect. The estimation of the resulting stochastic volatility model of mortality is performed using a recent class of Monte Carlo procedure specifically designed for state and parameter estimation in Bayesian state-space models, known as the class of particle Markov chain Monte Carlo methods. We illustrate the framework we have developed using Danish male mortality data, and show that incorporating heteroscedasticity and stochastic volatility markedly improves model fit despite an increase of model complexity. Forecasting properties of the enhanced models are examined with long term and short term calibration periods on the reconstruction of life tables.

</details>

<details>

<summary>2016-05-31 08:29:39 - A non-parametric Bayesian approach to decompounding from high frequency data</summary>

- *Shota Gugushvili, Frank van der Meulen, Peter Spreij*

- `1507.03263v3` - [abs](http://arxiv.org/abs/1507.03263v3) - [pdf](http://arxiv.org/pdf/1507.03263v3)

> Given a sample from a discretely observed compound Poisson process, we consider non-parametric estimation of the density $f_0$ of its jump sizes, as well as of its intensity $\lambda_0.$ We take a Bayesian approach to the problem and specify the prior on $f_0$ as the Dirichlet location mixture of normal densities. An independent prior for $\lambda_0$ is assumed to be compactly supported and to possess a positive density with respect to the Lebesgue measure. We show that under suitable assumptions the posterior contracts around the pair $(\lambda_0,f_0)$ at essentially (up to a logarithmic factor) the $\sqrt{n\Delta}$-rate, where $n$ is the number of observations and $\Delta$ is the mesh size at which the process is sampled. The emphasis is on high frequency data, $\Delta\to 0$, but the obtained results are also valid for fixed $\Delta$. In either case we assume that $n\Delta\rightarrow\infty$. Our main result implies existence of Bayesian point estimates converging (in the frequentist sense, in probability) to $(\lambda_0,f_0)$ at the same rate.   We also discuss a practical implementation of our approach. The computational problem is dealt with by inclusion of auxiliary variables and we develop a Markov Chain Monte Carlo algorithm that samples from the joint distribution of the unknown parameters in the mixture density and the introduced auxiliary variables. Numerical examples illustrate the feasibility of this approach.

</details>

<details>

<summary>2016-05-31 22:37:43 - Quantifying the probable approximation error of probabilistic inference programs</summary>

- *Marco F Cusumano-Towner, Vikash K Mansinghka*

- `1606.00068v1` - [abs](http://arxiv.org/abs/1606.00068v1) - [pdf](http://arxiv.org/pdf/1606.00068v1)

> This paper introduces a new technique for quantifying the approximation error of a broad class of probabilistic inference programs, including ones based on both variational and Monte Carlo approaches. The key idea is to derive a subjective bound on the symmetrized KL divergence between the distribution achieved by an approximate inference program and its true target distribution. The bound's validity (and subjectivity) rests on the accuracy of two auxiliary probabilistic programs: (i) a "reference" inference program that defines a gold standard of accuracy and (ii) a "meta-inference" program that answers the question "what internal random choices did the original approximate inference program probably make given that it produced a particular result?" The paper includes empirical results on inference problems drawn from linear regression, Dirichlet process mixture modeling, HMMs, and Bayesian networks. The experiments show that the technique is robust to the quality of the reference inference program and that it can detect implementation bugs that are not apparent from predictive performance.

</details>


## 2016-06

<details>

<summary>2016-06-01 09:39:56 - Cross-validatory extreme value threshold selection and uncertainty with application to ocean storm severity</summary>

- *Paul Northrop, Nicolas Attalides, Philip Jonathan*

- `1504.06653v3` - [abs](http://arxiv.org/abs/1504.06653v3) - [pdf](http://arxiv.org/pdf/1504.06653v3)

> Designs conditions for marine structures are typically informed by threshold-based extreme value analyses of oceanographic variables, in which excesses of a high threshold are modelled by a generalized Pareto (GP) distribution. Too low a threshold leads to bias from model mis-specification; raising the threshold increases the variance of estimators: a bias-variance trade-off. Many existing threshold selection methods do not address this trade-off directly, but rather aim to select the lowest threshold above which the GP model is judged to hold approximately. In this paper Bayesian cross-validation is used to address the trade-off by comparing thresholds based on predictive ability at extreme levels. Extremal inferences can be sensitive to the choice of a single threshold. We use Bayesian model-averaging to combine inferences from many thresholds, thereby reducing sensitivity to the choice of a single threshold. The methodology is applied to significant wave height datasets from the northern North Sea and the Gulf of Mexico.

</details>

<details>

<summary>2016-06-01 14:28:49 - Adaptive, delayed-acceptance MCMC for targets with expensive likelihoods</summary>

- *Chris Sherlock, Andrew Golightly, Daniel A. Henderson*

- `1509.00172v2` - [abs](http://arxiv.org/abs/1509.00172v2) - [pdf](http://arxiv.org/pdf/1509.00172v2)

> When conducting Bayesian inference, delayed acceptance (DA) Metropolis-Hastings (MH) algorithms and DA pseudo-marginal MH algorithms can be applied when it is computationally expensive to calculate the true posterior or an unbiased estimate thereof, but a computationally cheap approximation is available. A first accept-reject stage is applied, with the cheap approximation substituted for the true posterior in the MH acceptance ratio. Only for those proposals which pass through the first stage is the computationally expensive true posterior (or unbiased estimate thereof) evaluated, with a second accept-reject stage ensuring that detailed balance is satisfied with respect to the intended true posterior. In some scenarios there is no obvious computationally cheap approximation. A weighted average of previous evaluations of the computationally expensive posterior provides a generic approximation to the posterior. If only the $k$-nearest neighbours have non-zero weights then evaluation of the approximate posterior can be made computationally cheap provided that the points at which the posterior has been evaluated are stored in a multi-dimensional binary tree, known as a KD-tree. The contents of the KD-tree are potentially updated after every computationally intensive evaluation. The resulting adaptive, delayed-acceptance [pseudo-marginal] Metropolis-Hastings algorithm is justified both theoretically and empirically. Guidance on tuning parameters is provided and the methodology is applied to a discretely observed Markov jump process characterising predator-prey interactions and an ODE system describing the dynamics of an autoregulatory gene network.

</details>

<details>

<summary>2016-06-01 16:22:58 - Trace-class Monte Carlo Markov Chains for Bayesian Multivariate Linear Regression with Non-Gaussian Errors</summary>

- *Qian Qin, James P. Hobert*

- `1602.00136v2` - [abs](http://arxiv.org/abs/1602.00136v2) - [pdf](http://arxiv.org/pdf/1602.00136v2)

> Let $\pi$ denote the intractable posterior density that results when the likelihood from a multivariate linear regression model with errors from a scale mixture of normals is combined with the standard non-informative prior. There is a simple data augmentation algorithm (based on latent data from the mixing density) that can be used to explore $\pi$. Let $h(\cdot)$ and $d$ denote the mixing density and the dimension of the regression model, respectively. Hobert et al. (2016) [arXiv:1506.03113v2] have recently shown that, if $h$ converges to 0 at the origin at an appropriate rate, and $\int_0^\infty u^{\frac{d}{2}} \, h(u) \, du < \infty$, then the Markov chains underlying the DA algorithm and an alternative Haar PX-DA algorithm are both geometrically ergodic. In fact, something much stronger than geometric ergodicity often holds. Indeed, it is shown in this paper that, under simple conditions on $h$, the Markov operators defined by the DA and Haar PX-DA Markov chains are trace-class, i.e., compact with summable eigenvalues. Many of the mixing densities that satisfy Hobert et al.'s (2016) conditions also satisfy the new conditions developed in this paper. Thus, for this set of mixing densities, the new results provide a substantial strengthening of Hobert et al.'s (2016) conclusion without any additional assumptions. For example, Hobert et al. (2016) showed that the DA and Haar PX-DA Markov chains are geometrically ergodic whenever the mixing density is generalized inverse Gaussian, log-normal, Fr\'{e}chet (with shape parameter larger than $d/2$), or inverted gamma (with shape parameter larger than $d/2$). The results in this paper show that, in each of these cases, the DA and Haar PX-DA Markov operators are, in fact, trace-class.

</details>

<details>

<summary>2016-06-01 19:05:03 - Black-box $Î±$-divergence Minimization</summary>

- *JosÃ© Miguel HernÃ¡ndez-Lobato, Yingzhen Li, Mark Rowland, Daniel HernÃ¡ndez-Lobato, Thang Bui, Richard E. Turner*

- `1511.03243v3` - [abs](http://arxiv.org/abs/1511.03243v3) - [pdf](http://arxiv.org/pdf/1511.03243v3)

> Black-box alpha (BB-$\alpha$) is a new approximate inference method based on the minimization of $\alpha$-divergences. BB-$\alpha$ scales to large datasets because it can be implemented using stochastic gradient descent. BB-$\alpha$ can be applied to complex probabilistic models with little effort since it only requires as input the likelihood function and its gradients. These gradients can be easily obtained using automatic differentiation. By changing the divergence parameter $\alpha$, the method is able to interpolate between variational Bayes (VB) ($\alpha \rightarrow 0$) and an algorithm similar to expectation propagation (EP) ($\alpha = 1$). Experiments on probit regression and neural network regression and classification problems show that BB-$\alpha$ with non-standard settings of $\alpha$, such as $\alpha = 0.5$, usually produces better predictions than with $\alpha \rightarrow 0$ (VB) or $\alpha = 1$ (EP).

</details>

<details>

<summary>2016-06-01 20:01:24 - A Bayesian Hierarchical Model for Prediction of Latent Health States from Multiple Data Sources with Application to Active Surveillance of Prostate Cancer</summary>

- *R. Yates Coley, Aaron J. Fisher, Mufaddal Mamawala, H. Ballentine Carter, Kenneth J. Pienta, Scott L. Zeger*

- `1508.07511v4` - [abs](http://arxiv.org/abs/1508.07511v4) - [pdf](http://arxiv.org/pdf/1508.07511v4)

> In this article, we present a Bayesian hierarchical model for predicting a latent health state from longitudinal clinical measurements. Model development is motivated by the need to integrate multiple sources of data to improve clinical decisions about whether to remove or irradiate a patient's prostate cancer. Existing modeling approaches are extended to accommodate measurement error in cancer state determinations based on biopsied tissue, clinical measurements possibly not missing at random, and informative partial observation of the true state. The proposed model enables estimation of whether an individual's underlying prostate cancer is aggressive, requiring surgery and/or radiation, or indolent, permitting continued surveillance. These individualized predictions can then be communicated to clinicians and patients to inform decision-making. We demonstrate the model with data from a cohort of low risk prostate cancer patients at Johns Hopkins University and assess predictive accuracy among a subset for whom true cancer state is observed. Simulation studies confirm model performance and explore the impact of adjusting for informative missingness on true state predictions. R code and simulated data available at https://github.com/rycoley/prediction-prostate-surveillance.

</details>

<details>

<summary>2016-06-02 00:59:28 - Robust and Scalable Bayes via a Median of Subset Posterior Measures</summary>

- *Stanislav Minsker, Sanvesh Srivastava, Lizhen Lin, David B. Dunson*

- `1403.2660v3` - [abs](http://arxiv.org/abs/1403.2660v3) - [pdf](http://arxiv.org/pdf/1403.2660v3)

> We propose a novel approach to Bayesian analysis that is provably robust to outliers in the data and often has computational advantages over standard methods. Our technique is based on splitting the data into non-overlapping subgroups, evaluating the posterior distribution given each independent subgroup, and then combining the resulting measures. The main novelty of our approach is the proposed aggregation step, which is based on the evaluation of a median in the space of probability measures equipped with a suitable collection of distances that can be quickly and efficiently evaluated in practice. We present both theoretical and numerical evidence illustrating the improvements achieved by our method.

</details>

<details>

<summary>2016-06-02 06:46:28 - On collapsed representation of hierarchical Completely Random Measures</summary>

- *Gaurav Pandey, Ambedkar Dukkipati*

- `1509.01817v2` - [abs](http://arxiv.org/abs/1509.01817v2) - [pdf](http://arxiv.org/pdf/1509.01817v2)

> The aim of the paper is to provide an exact approach for generating a Poisson process sampled from a hierarchical CRM, without having to instantiate the infinitely many atoms of the random measures. We use completely random measures~(CRM) and hierarchical CRM to define a prior for Poisson processes. We derive the marginal distribution of the resultant point process, when the underlying CRM is marginalized out. Using well known properties unique to Poisson processes, we were able to derive an exact approach for instantiating a Poisson process with a hierarchical CRM prior. Furthermore, we derive Gibbs sampling strategies for hierarchical CRM models based on Chinese restaurant franchise sampling scheme. As an example, we present the sum of generalized gamma process (SGGP), and show its application in topic-modelling. We show that one can determine the power-law behaviour of the topics and words in a Bayesian fashion, by defining a prior on the parameters of SGGP.

</details>

<details>

<summary>2016-06-02 10:08:37 - Bayesian Learning of Kernel Embeddings</summary>

- *Seth Flaxman, Dino Sejdinovic, John P. Cunningham, Sarah Filippi*

- `1603.02160v2` - [abs](http://arxiv.org/abs/1603.02160v2) - [pdf](http://arxiv.org/pdf/1603.02160v2)

> Kernel methods are one of the mainstays of machine learning, but the problem of kernel learning remains challenging, with only a few heuristics and very little theory. This is of particular importance in methods based on estimation of kernel mean embeddings of probability measures. For characteristic kernels, which include most commonly used ones, the kernel mean embedding uniquely determines its probability measure, so it can be used to design a powerful statistical testing framework, which includes nonparametric two-sample and independence tests. In practice, however, the performance of these tests can be very sensitive to the choice of kernel and its lengthscale parameters. To address this central issue, we propose a new probabilistic model for kernel mean embeddings, the Bayesian Kernel Embedding model, combining a Gaussian process prior over the Reproducing Kernel Hilbert Space containing the mean embedding with a conjugate likelihood function, thus yielding a closed form posterior over the mean embedding. The posterior mean of our model is closely related to recently proposed shrinkage estimators for kernel mean embeddings, while the posterior uncertainty is a new, interesting feature with various possible applications. Critically for the purposes of kernel learning, our model gives a simple, closed form marginal pseudolikelihood of the observed data given the kernel hyperparameters. This marginal pseudolikelihood can either be optimized to inform the hyperparameter choice or fully Bayesian inference can be used.

</details>

<details>

<summary>2016-06-03 14:28:01 - Adapted Variational Bayes for Functional Data Registration, Smoothing, and Prediction</summary>

- *Cecilia Earls, Giles Hooker*

- `1502.00552v3` - [abs](http://arxiv.org/abs/1502.00552v3) - [pdf](http://arxiv.org/pdf/1502.00552v3)

> We propose a model for functional data registration that compares favorably to the best methods of functional data registration currently available. It also extends current inferential capabilities for unregistered data by providing a flexible probabilistic framework that 1) allows for functional prediction in the context of registration and 2) can be adapted to include smoothing and registration in one model. The proposed inferential framework is a Bayesian hierarchical model where the registered functions are modeled as Gaussian processes. To address the computational demands of inference in high-dimensional Bayesian models, we propose an adapted form of the variational Bayes algorithm for approximate inference that performs similarly to MCMC sampling methods for well-defined problems. The efficiency of the adapted variational Bayes (AVB) algorithm allows variability in a predicted registered, warping, and unregistered function to be depicted separately via bootstrapping. Temperature data related to the el-ni\~no phenomenon is used to demonstrate the unique inferential capabilities for prediction provided by this model.

</details>

<details>

<summary>2016-06-03 21:48:53 - Statistical Pattern Recognition for Driving Styles Based on Bayesian Probability and Kernel Density Estimation</summary>

- *Wenshuo Wang, Junqiang Xi, Xiaohan Li*

- `1606.01284v1` - [abs](http://arxiv.org/abs/1606.01284v1) - [pdf](http://arxiv.org/pdf/1606.01284v1)

> Driving styles have a great influence on vehicle fuel economy, active safety, and drivability. To recognize driving styles of path-tracking behaviors for different divers, a statistical pattern-recognition method is developed to deal with the uncertainty of driving styles or characteristics based on probability density estimation. First, to describe driver path-tracking styles, vehicle speed and throttle opening are selected as the discriminative parameters, and a conditional kernel density function of vehicle speed and throttle opening is built, respectively, to describe the uncertainty and probability of two representative driving styles, e.g., aggressive and normal. Meanwhile, a posterior probability of each element in feature vector is obtained using full Bayesian theory. Second, a Euclidean distance method is involved to decide to which class the driver should be subject instead of calculating the complex covariance between every two elements of feature vectors. By comparing the Euclidean distance between every elements in feature vector, driving styles are classified into seven levels ranging from low normal to high aggressive. Subsequently, to show benefits of the proposed pattern-recognition method, a cross-validated method is used, compared with a fuzzy logic-based pattern-recognition method. The experiment results show that the proposed statistical pattern-recognition method for driving styles based on kernel density estimation is more efficient and stable than the fuzzy logic-based method.

</details>

<details>

<summary>2016-06-03 22:08:29 - Bayesian Estimation Under Informative Sampling</summary>

- *Terrance D. Savitsky, Daniell Toth*

- `1507.07050v4` - [abs](http://arxiv.org/abs/1507.07050v4) - [pdf](http://arxiv.org/pdf/1507.07050v4)

> Bayesian analysis is increasingly popular for use in social science and other application areas where the data are observations from an informative sample. An informative sampling design leads to inclusion probabilities that are correlated with the response variable of interest. Model inference performed on the observed sample taken from the population will be biased for the population generative model under informative sampling since the balance of information in the sample data is different from that for the population. Typical approaches to account for an informative sampling design under Bayesian estimation are often difficult to implement because they require re-parameterization of the hypothesized generating model, or focus on design, rather than model-based, inference. We propose to construct a pseudo-posterior distribution that utilizes sampling weights based on the marginal inclusion probabilities to exponentiate the likelihood contribution of each sampled unit, which weights the information in the sample back to the population. Our approach provides a nearly automated estimation procedure applicable to any model specified by the data analyst for the population and retains the population model parameterization and posterior sampling geometry. We construct conditions on known marginal and pairwise inclusion probabilities that define a class of sampling designs where $L_{1}$ consistency of the pseudo posterior is guaranteed. We demonstrate our method on an application concerning the Bureau of Labor Statistics Job Openings and Labor Turnover Survey.

</details>

<details>

<summary>2016-06-04 04:08:51 - Bayesian index of superiority and the p-value of the conditional test for Poisson parameters</summary>

- *Masaaki Doi*

- `1606.01324v1` - [abs](http://arxiv.org/abs/1606.01324v1) - [pdf](http://arxiv.org/pdf/1606.01324v1)

> We consider the problem of comparing two Poisson parameters from the Bayesian perspective. Kawasaki and Miyaoka (2012b) proposed the Bayesian index $P(\lambda_1 < \lambda_2 | X_1,X_2)$ and expressed it using the hypergeometric series. In this paper, under some conditions, we give four other expressions of the Bayesian index in terms of the cumulative distribution functions of beta, $F$, binomial, and negative binomial distribution. Next, we investigate the relationship between the Bayesian index and the $p$-value of the conditional test with the null hypothesis $H_0: \lambda_1 \geq \lambda_2 $ versus an alternative hypothesis $H_1: \lambda_1<\lambda_2 $. Additionally, we investigate the generalized relationship between $P(\lambda_1/\lambda_2 <c | X_1, X_2)$ and the $p$-value of the conditional test with the null hypothesis $H_0: \lambda_1/\lambda_2 \geq c$ versus the alternative $H_1: \lambda_1/\lambda_2 < c$. We illustrate the utility of the Bayesian index using analyses of real data. Our finding suggests that the Bayesian index can potentially be useful in an epidemiology and in a clinical trial.

</details>

<details>

<summary>2016-06-05 19:47:36 - Nonparametric Bayes Modeling of Populations of Networks</summary>

- *Daniele Durante, David B. Dunson, Joshua T. Vogelstein*

- `1406.7851v3` - [abs](http://arxiv.org/abs/1406.7851v3) - [pdf](http://arxiv.org/pdf/1406.7851v3)

> Replicated network data are increasingly available in many research fields. In connectomic applications, inter-connections among brain regions are collected for each patient under study, motivating statistical models which can flexibly characterize the probabilistic generative mechanism underlying these network-valued data. Available models for a single network are not designed specifically for inference on the entire probability mass function of a network-valued random variable and therefore lack flexibility in characterizing the distribution of relevant topological structures. We propose a flexible Bayesian nonparametric approach for modeling the population distribution of network-valued data. The joint distribution of the edges is defined via a mixture model which reduces dimensionality and efficiently incorporates network information within each mixture component by leveraging latent space representations. The formulation leads to an efficient Gibbs sampler and provides simple and coherent strategies for inference and goodness-of-fit assessments. We provide theoretical results on the flexibility of our model and illustrate improved performance --- compared to state-of-the-art models --- in simulations and application to human brain networks.

</details>

<details>

<summary>2016-06-06 14:01:44 - A gamma approximation to the Bayesian posterior distribution of a discrete parameter of the Generalized Poisson model</summary>

- *T. F. Khang*

- `1606.01749v1` - [abs](http://arxiv.org/abs/1606.01749v1) - [pdf](http://arxiv.org/pdf/1606.01749v1)

> Let $X$ have a Generalized Poisson distribution with mean $kb$, where $b$ is a known constant in the unit interval and $k$ is a discrete, non-negative parameter. We show that if an uninformative uniform prior for $k$ is assumed, then the posterior distribution of $k$ can be approximated using the gamma distribution when $b$ is small.

</details>

<details>

<summary>2016-06-06 18:34:56 - Bayesian Poisson Tucker Decomposition for Learning the Structure of International Relations</summary>

- *Aaron Schein, Mingyuan Zhou, David M. Blei, Hanna Wallach*

- `1606.01855v1` - [abs](http://arxiv.org/abs/1606.01855v1) - [pdf](http://arxiv.org/pdf/1606.01855v1)

> We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling country--country interaction event data. These data consist of interaction events of the form "country $i$ took action $a$ toward country $j$ at time $t$." BPTD discovers overlapping country--community memberships, including the number of latent communities. In addition, it discovers directed community--community interaction networks that are specific to "topics" of action types and temporal "regimes." We show that BPTD yields an efficient MCMC inference algorithm and achieves better predictive performance than related models. We also demonstrate that it discovers interpretable latent structure that agrees with our knowledge of international relations.

</details>

<details>

<summary>2016-06-07 18:37:29 - Classification of weak multi-view signals by sharing factors in a mixture of Bayesian group factor analyzers</summary>

- *Sami Remes, Tommi Mononen, Samuel Kaski*

- `1512.05610v2` - [abs](http://arxiv.org/abs/1512.05610v2) - [pdf](http://arxiv.org/pdf/1512.05610v2)

> We propose a novel classification model for weak signal data, building upon a recent model for Bayesian multi-view learning, Group Factor Analysis (GFA). Instead of assuming all data to come from a single GFA model, we allow latent clusters, each having a different GFA model and producing a different class distribution. We show that sharing information across the clusters, by sharing factors, increases the classification accuracy considerably; the shared factors essentially form a flexible noise model that explains away the part of data not related to classification. Motivation for the setting comes from single-trial functional brain imaging data, having a very low signal-to-noise ratio and a natural multi-view setting, with the different sensors, measurement modalities (EEG, MEG, fMRI) and possible auxiliary information as views. We demonstrate our model on a MEG dataset.

</details>

<details>

<summary>2016-06-07 23:58:09 - Structure Learning in Graphical Modeling</summary>

- *Mathias Drton, Marloes H. Maathuis*

- `1606.02359v1` - [abs](http://arxiv.org/abs/1606.02359v1) - [pdf](http://arxiv.org/pdf/1606.02359v1)

> A graphical model is a statistical model that is associated to a graph whose nodes correspond to variables of interest. The edges of the graph reflect allowed conditional dependencies among the variables. Graphical models admit computationally convenient factorization properties and have long been a valuable tool for tractable modeling of multivariate distributions. More recently, applications such as reconstructing gene regulatory networks from gene expression data have driven major advances in structure learning, that is, estimating the graph underlying a model. We review some of these advances and discuss methods such as the graphical lasso and neighborhood selection for undirected graphical models (or Markov random fields), and the PC algorithm and score-based search methods for directed graphical models (or Bayesian networks). We further review extensions that account for effects of latent variables and heterogeneous data sources.

</details>

<details>

<summary>2016-06-08 07:42:53 - Nonparametric Modeling of Dynamic Functional Connectivity in fMRI Data</summary>

- *SÃ¸ren F. V. Nielsen, Kristoffer H. Madsen, Rasmus RÃ¸ge, Mikkel N. Schmidt, Morten MÃ¸rup*

- `1601.00496v2` - [abs](http://arxiv.org/abs/1601.00496v2) - [pdf](http://arxiv.org/pdf/1601.00496v2)

> Dynamic functional connectivity (FC) has in recent years become a topic of interest in the neuroimaging community. Several models and methods exist for both functional magnetic resonance imaging (fMRI) and electroencephalography (EEG), and the results point towards the conclusion that FC exhibits dynamic changes. The existing approaches modeling dynamic connectivity have primarily been based on time-windowing the data and k-means clustering. We propose a non-parametric generative model for dynamic FC in fMRI that does not rely on specifying window lengths and number of dynamic states. Rooted in Bayesian statistical modeling we use the predictive likelihood to investigate if the model can discriminate between a motor task and rest both within and across subjects. We further investigate what drives dynamic states using the model on the entire data collated across subjects and task/rest. We find that the number of states extracted are driven by subject variability and preprocessing differences while the individual states are almost purely defined by either task or rest. This questions how we in general interpret dynamic FC and points to the need for more research on what drives dynamic FC.

</details>

<details>

<summary>2016-06-08 12:56:42 - The forecasting of menstruation based on a state-space modeling of basal body temperature time series</summary>

- *Keiichi Fukaya, Ai Kawamori, Yutaka Osada, Masumi Kitazawa, Makio Ishiguro*

- `1606.02536v1` - [abs](http://arxiv.org/abs/1606.02536v1) - [pdf](http://arxiv.org/pdf/1606.02536v1)

> Women's basal body temperature (BBT) follows a periodic pattern that is associated with the events in their menstrual cycle. Although daily BBT time series contain potentially useful information for estimating the underlying menstrual phase and for predicting the length of current menstrual cycle, few models have been constructed for BBT time series. Here, we propose a state-space model that includes menstrual phase as a latent state variable to explain fluctuations in BBT and menstrual cycle length. Conditional distributions for the menstrual phase were obtained by using sequential Bayesian filtering techniques. A predictive distribution for the upcoming onset of menstruation was then derived based on the conditional distributions and the model, leading to a novel statistical framework that provided a sequentially updated prediction of the day of onset of menstruation. We applied this framework to a real dataset comprising women's self-reported BBT and days of menstruation, comparing the prediction accuracy of our proposed method with that of conventional calendar calculation. We found that our proposed method provided a better prediction of the day of onset of menstruation. Potential extensions of this framework may provide the basis of modeling and predicting other events that are associated with the menstrual cycle.

</details>

<details>

<summary>2016-06-08 14:15:58 - Clustering with a Reject Option: Interactive Clustering as Bayesian Prior Elicitation</summary>

- *Akash Srivastava, James Zou, Charles Sutton*

- `1602.06886v2` - [abs](http://arxiv.org/abs/1602.06886v2) - [pdf](http://arxiv.org/pdf/1602.06886v2)

> A good clustering can help a data analyst to explore and understand a data set, but what constitutes a good clustering may depend on domain-specific and application-specific criteria. These criteria can be difficult to formalize, even when it is easy for an analyst to know a good clustering when she sees one. We present a new approach to interactive clustering for data exploration, called \ciif, based on a particularly simple feedback mechanism, in which an analyst can choose to reject individual clusters and request new ones. The new clusters should be different from previously rejected clusters while still fitting the data well. We formalize this interaction in a novel Bayesian prior elicitation framework. In each iteration, the prior is adapted to account for all the previous feedback, and a new clustering is then produced from the posterior distribution. To achieve the computational efficiency necessary for an interactive setting, we propose an incremental optimization method over data minibatches using Lagrangian relaxation. Experiments demonstrate that \ciif can produce accurate and diverse clusterings.

</details>

<details>

<summary>2016-06-08 14:18:21 - A moment-matching Ferguson and Klass algorithm</summary>

- *Julyan Arbel, Igor PrÃ¼nster*

- `1606.02566v1` - [abs](http://arxiv.org/abs/1606.02566v1) - [pdf](http://arxiv.org/pdf/1606.02566v1)

> Completely random measures (CRM) represent the key building block of a wide variety of popular stochastic models and play a pivotal role in modern Bayesian Nonparametrics. A popular representation of CRMs as a random series with decreasing jumps is due to Ferguson and Klass (1972). This can immediately be turned into an algorithm for sampling realizations of CRMs or more elaborate models involving transformed CRMs. However, concrete implementation requires to truncate the random series at some threshold resulting in an approximation error. The goal of this paper is to quantify the quality of the approximation by a moment-matching criterion, which consists in evaluating a measure of discrepancy between actual moments and moments based on the simulation output. Seen as a function of the truncation level, the methodology can be used to determine the truncation level needed to reach a certain level of precision. The resulting moment-matching \FK algorithm is then implemented and illustrated on several popular Bayesian nonparametric models.

</details>

<details>

<summary>2016-06-08 17:39:42 - Robust Bayesian FDR Control using Bayes Factors, with Applications to Multi-tissue eQTL Discovery</summary>

- *Xiaoquan Wen*

- `1311.3981v2` - [abs](http://arxiv.org/abs/1311.3981v2) - [pdf](http://arxiv.org/pdf/1311.3981v2)

> Motivated by the genomic application of expression quantitative trait loci (eQTL) mapping, we propose a new procedure to perform simultaneous testing of multiple hypotheses using Bayes factors as input test statistics. One of the most significant features of this method is its robustness in controlling the targeted false discovery rate (FDR) even under misspecifications of parametric alternative models. Moreover, the proposed procedure is highly computationally efficient, which is ideal for treating both complex system and big data in genomic applications. We discuss the theoretical properties of the new procedure and demonstrate its power and computational efficiency in applications of single-tissue and multi-tissue eQTL mapping.

</details>

<details>

<summary>2016-06-09 00:00:10 - On the Theory and Practice of Privacy-Preserving Bayesian Data Analysis</summary>

- *James Foulds, Joseph Geumlek, Max Welling, Kamalika Chaudhuri*

- `1603.07294v2` - [abs](http://arxiv.org/abs/1603.07294v2) - [pdf](http://arxiv.org/pdf/1603.07294v2)

> Bayesian inference has great promise for the privacy-preserving analysis of sensitive data, as posterior sampling automatically preserves differential privacy, an algorithmic notion of data privacy, under certain conditions (Dimitrakakis et al., 2014; Wang et al., 2015). While this one posterior sample (OPS) approach elegantly provides privacy "for free," it is data inefficient in the sense of asymptotic relative efficiency (ARE). We show that a simple alternative based on the Laplace mechanism, the workhorse of differential privacy, is as asymptotically efficient as non-private posterior inference, under general assumptions. This technique also has practical advantages including efficient use of the privacy budget for MCMC. We demonstrate the practicality of our approach on a time-series analysis of sensitive military records from the Afghanistan and Iraq wars disclosed by the Wikileaks organization.

</details>

<details>

<summary>2016-06-09 03:35:53 - The space of ultrametric phylogenetic trees</summary>

- *Alex Gavryushkin, Alexei J. Drummond*

- `1410.3544v5` - [abs](http://arxiv.org/abs/1410.3544v5) - [pdf](http://arxiv.org/pdf/1410.3544v5)

> The reliability of a phylogenetic inference method from genomic sequence data is ensured by its statistical consistency. Bayesian inference methods produce a sample of phylogenetic trees from the posterior distribution given sequence data. Hence the question of statistical consistency of such methods is equivalent to the consistency of the summary of the sample. More generally, statistical consistency is ensured by the tree space used to analyse the sample.   In this paper, we consider two standard parameterisations of phylogenetic time-trees used in evolutionary models: inter-coalescent interval lengths and absolute times of divergence events. For each of these parameterisations we introduce a natural metric space on ultrametric phylogenetic trees. We compare the introduced spaces with existing models of tree space and formulate several formal requirements that a metric space on phylogenetic trees must possess in order to be a satisfactory space for statistical analysis, and justify them. We show that only a few known constructions of the space of phylogenetic trees satisfy these requirements. However, our results suggest that these basic requirements are not enough to distinguish between the two metric spaces we introduce and that the choice between metric spaces requires additional properties to be considered. Particularly, that the summary tree minimising the square distance to the trees from the sample might be different for different parameterisations. This suggests that further fundamental insight is needed into the problem of statistical consistency of phylogenetic inference methods.

</details>

<details>

<summary>2016-06-09 07:58:59 - Consensus Labeled Random Finite Set Filtering for Distributed Multi-Object Tracking</summary>

- *C. Fantacci, B. -N. Vo, B. -T. Vo, G. Battistelli, L. Chisci*

- `1501.01579v2` - [abs](http://arxiv.org/abs/1501.01579v2) - [pdf](http://arxiv.org/pdf/1501.01579v2)

> This paper addresses distributed multi-object tracking over a network of heterogeneous and geographically dispersed nodes with sensing, communication and processing capabilities. The main contribution is an approach to distributed multi-object estimation based on labeled Random Finite Sets (RFSs) and dynamic Bayesian inference, which enables the development of two novel consensus tracking filters, namely a Consensus Marginalized $\delta$-Generalized Labeled Multi-Bernoulli and Consensus Labeled Multi-Bernoulli tracking filter. The proposed algorithms provide fully distributed, scalable and computationally efficient solutions for multi-object tracking. Simulation experiments via Gaussian mixture implementations confirm the effectiveness of the proposed approach on challenging scenarios.

</details>

<details>

<summary>2016-06-10 13:39:45 - Gaussian Processes for Music Audio Modelling and Content Analysis</summary>

- *Pablo A. Alvarado, Dan Stowell*

- `1606.01039v2` - [abs](http://arxiv.org/abs/1606.01039v2) - [pdf](http://arxiv.org/pdf/1606.01039v2)

> Real music signals are highly variable, yet they have strong statistical structure. Prior information about the underlying physical mechanisms by which sounds are generated and rules by which complex sound structure is constructed (notes, chords, a complete musical score), can be naturally unified using Bayesian modelling techniques. Typically algorithms for Automatic Music Transcription independently carry out individual tasks such as multiple-F0 detection and beat tracking. The challenge remains to perform joint estimation of all parameters. We present a Bayesian approach for modelling music audio, and content analysis. The proposed methodology based on Gaussian processes seeks joint estimation of multiple music concepts by incorporating into the kernel prior information about non-stationary behaviour, dynamics, and rich spectral content present in the modelled music signal. We illustrate the benefits of this approach via two tasks: pitch estimation, and inferring missing segments in a polyphonic audio recording.

</details>

<details>

<summary>2016-06-11 20:50:53 - Drug response prediction by inferring pathway-response associations with Kernelized Bayesian Matrix Factorization</summary>

- *Muhammad Ammad-ud-din, Suleiman A. Khan, Disha Malani, Astrid MurumÃ¤gi, Olli Kallioniemi, Tero Aittokallio, Samuel Kaski*

- `1606.03623v1` - [abs](http://arxiv.org/abs/1606.03623v1) - [pdf](http://arxiv.org/pdf/1606.03623v1)

> A key goal of computational personalized medicine is to systematically utilize genomic and other molecular features of samples to predict drug responses for a previously unseen sample. Such predictions are valuable for developing hypotheses for selecting therapies tailored for individual patients. This is especially valuable in oncology, where molecular and genetic heterogeneity of the cells has a major impact on the response. However, the prediction task is extremely challenging, raising the need for methods that can effectively model and predict drug responses. In this study, we propose a novel formulation of multi-task matrix factorization that allows selective data integration for predicting drug responses. To solve the modeling task, we extend the state-of-the-art kernelized Bayesian matrix factorization (KBMF) method with component-wise multiple kernel learning. In addition, our approach exploits the known pathway information in a novel and biologically meaningful fashion to learn the drug response associations. Our method quantitatively outperforms the state of the art on predicting drug responses in two publicly available cancer data sets as well as on a synthetic data set. In addition, we validated our model predictions with lab experiments using an in-house cancer cell line panel. We finally show the practical applicability of the proposed method by utilizing prior knowledge to infer pathway-drug response associations, opening up the opportunity for elucidating drug action mechanisms. We demonstrate that pathway-response associations can be learned by the proposed model for the well known EGFR and MEK inhibitors.

</details>

<details>

<summary>2016-06-13 14:03:50 - Inferring Sparsity: Compressed Sensing using Generalized Restricted Boltzmann Machines</summary>

- *Eric W. Tramel, Andre Manoel, Francesco Caltagirone, Marylou GabriÃ©, Florent Krzakala*

- `1606.03956v1` - [abs](http://arxiv.org/abs/1606.03956v1) - [pdf](http://arxiv.org/pdf/1606.03956v1)

> In this work, we consider compressed sensing reconstruction from $M$ measurements of $K$-sparse structured signals which do not possess a writable correlation model. Assuming that a generative statistical model, such as a Boltzmann machine, can be trained in an unsupervised manner on example signals, we demonstrate how this signal model can be used within a Bayesian framework of signal reconstruction. By deriving a message-passing inference for general distribution restricted Boltzmann machines, we are able to integrate these inferred signal models into approximate message passing for compressed sensing reconstruction. Finally, we show for the MNIST dataset that this approach can be very effective, even for $M < K$.

</details>

<details>

<summary>2016-06-14 17:58:49 - Bayesian Inference on Matrix Manifolds for Linear Dimensionality Reduction</summary>

- *Andrew Holbrook, Alexander Vandenberg-Rodes, Babak Shahbaba*

- `1606.04478v1` - [abs](http://arxiv.org/abs/1606.04478v1) - [pdf](http://arxiv.org/pdf/1606.04478v1)

> We reframe linear dimensionality reduction as a problem of Bayesian inference on matrix manifolds. This natural paradigm extends the Bayesian framework to dimensionality reduction tasks in higher dimensions with simpler models at greater speeds. Here an orthogonal basis is treated as a single point on a manifold and is associated with a linear subspace on which observations vary maximally. Throughout this paper, we employ the Grassmann and Stiefel manifolds for various dimensionality reduction problems, explore the connection between the two manifolds, and use Hybrid Monte Carlo for posterior sampling on the Grassmannian for the first time. We delineate in which situations either manifold should be considered. Further, matrix manifold models are used to yield scientific insight in the context of cognitive neuroscience, and we conclude that our methods are suitable for basic inference as well as accurate prediction.

</details>

<details>

<summary>2016-06-14 20:52:42 - Non-Gaussian bivariate modelling with application to atmospheric trace-gas inversion</summary>

- *Andrew Zammit-Mangion, Noel Cressie, Anita L. Ganesan*

- `1606.04564v1` - [abs](http://arxiv.org/abs/1606.04564v1) - [pdf](http://arxiv.org/pdf/1606.04564v1)

> Atmospheric trace-gas inversion is the procedure by which the sources and sinks of a trace gas are identified from observations of its mole fraction at isolated locations in space and time. This is inherently a spatio-temporal bivariate inversion problem, since the mole-fraction field evolves in space and time and the flux is also spatio-temporally distributed. Further, the bivariate model is likely to be non-Gaussian since the flux field is rarely Gaussian. Here, we use conditioning to construct a non-Gaussian bivariate model, and we describe some of its properties through auto- and cross-cumulant functions. A bivariate non-Gaussian, specifically trans-Gaussian, model is then achieved through the use of Box--Cox transformations, and we facilitate Bayesian inference by approximating the likelihood in a hierarchical framework. Trace-gas inversion, especially at high spatial resolution, is frequently highly sensitive to prior specification. Therefore, unlike conventional approaches, we assimilate trace-gas inventory information with the observational data at the parameter layer, thus shifting prior sensitivity from the inventory itself to its spatial characteristics (e.g., its spatial length scale). We demonstrate the approach in controlled-experiment studies of methane inversion, using fluxes extracted from inventories of the UK and Ireland and of Northern Australia.

</details>

<details>

<summary>2016-06-15 07:02:04 - Spatially adaptive, Bayesian estimation for probabilistic temperature forecasts</summary>

- *Annette MÃ¶ller, Thordis L. Thorarinsdottir, Alex Lenkoski, Tilmann Gneiting*

- `1507.05066v3` - [abs](http://arxiv.org/abs/1507.05066v3) - [pdf](http://arxiv.org/pdf/1507.05066v3)

> Uncertainty in the prediction of future weather is commonly assessed through the use of forecast ensembles that employ a numerical weather prediction model in distinct variants. Statistical postprocessing can correct for biases in the numerical model and improves calibration. We propose a Bayesian version of the standard ensemble model output statistics (EMOS) postprocessing method, in which spatially varying bias coefficients are interpreted as realizations of Gaussian Markov random fields. Our Markovian EMOS (MEMOS) technique utilizes the recently developed stochastic partial differential equation (SPDE) and integrated nested Laplace approximation (INLA) methods for computationally efficient inference. The MEMOS approach shows good predictive performance in a comparative study of 24-hour ahead temperature forecasts over Germany based on the 50-member ensemble of the European Centre for Medium-Range Weather Forecasting (ECMWF).

</details>

<details>

<summary>2016-06-15 20:29:03 - Meta-analysis of two studies in the presence of heterogeneity with applications in rare diseases</summary>

- *Tim Friede, Christian RÃ¶ver, Simon Wandel, Beat Neuenschwander*

- `1606.04969v1` - [abs](http://arxiv.org/abs/1606.04969v1) - [pdf](http://arxiv.org/pdf/1606.04969v1)

> Random-effects meta-analyses are used to combine evidence of treatment effects from multiple studies. Since treatment effects may vary across trials due to differences in study characteristics, heterogeneity in treatment effects between studies must be accounted for to achieve valid inference. The standard model for random-effects meta-analysis assumes approximately normal effect estimates and a normal random-effects model. However, standard methods based on this model ignore the uncertainty in estimating the between-trial heterogeneity. In the special setting of only two studies and in the presence of heterogeneity we investigate here alternatives such as the Hartung-Knapp-Sidik-Jonkman method (HKSJ), the modified Knapp-Hartung method (mKH, a variation of the HKSJ method) and Bayesian random-effects meta-analyses with priors covering plausible heterogeneity values. The properties of these methods are assessed by applying them to five examples from various rare diseases and by a simulation study. Whereas the standard method based on normal quantiles has poor coverage, the HKSJ and mKH generally lead to very long, and therefore inconclusive, confidence intervals. The Bayesian intervals on the whole show satisfying properties and offer a reasonable compromise between these two extremes.

</details>

<details>

<summary>2016-06-16 07:00:49 - Mortality and life expectancy forecasting for a group of populations in developed countries: A multilevel functional data method</summary>

- *Han Lin Shang*

- `1606.05067v1` - [abs](http://arxiv.org/abs/1606.05067v1) - [pdf](http://arxiv.org/pdf/1606.05067v1)

> A multilevel functional data method is adapted for forecasting age-specific mortality for two or more populations in developed countries with high-quality vital registration systems. It uses multilevel functional principal component analysis of aggregate and population-specific data to extract the common trend and population-specific residual trend among populations. If the forecasts of population-specific residual trends do not show a long-term trend, then convergence in forecasts may be achieved. This method is first applied to age- and sex-specific data for the United Kingdom, and its forecast accuracy is then further compared with several existing methods, including independent functional data and product-ratio methods, through a multi-country comparison. The proposed method is also demonstrated by age-, sex- and state-specific data in Australia, where the convergence in forecasts can possibly be achieved by sex and state. For forecasting age-specific mortality, the multilevel functional data method is more accurate than the other coherent methods considered. For forecasting female life expectancy at birth, the multilevel functional data method is outperformed by the Bayesian method of \cite{RLG14}. For forecasting male life expectancy at birth, the multilevel functional data method performs better than the Bayesian methods in terms of point forecasts, but less well in terms of interval forecasts. Supplementary materials for this article are available online.

</details>

<details>

<summary>2016-06-16 14:22:10 - A Goldilocks principle for modeling radial velocity noise</summary>

- *Fabo Feng, M. Tuomi, H. R. A. Jones, R. P. Butler, S. Vogt*

- `1606.05196v1` - [abs](http://arxiv.org/abs/1606.05196v1) - [pdf](http://arxiv.org/pdf/1606.05196v1)

> The doppler measurements of stars are diluted and distorted by stellar activity noise. Different choices of noise models and statistical methods have led to much controversy in the confirmation of exoplanet candidates obtained through analysing radial velocity data. To quantify the limitation of various models and methods, we compare different noise models and signal detection criteria for various simulated and real data sets in the Bayesian framework. According to our analyses, the white noise model tend to interpret noise as signal, leading to false positives. On the other hand, the red noise models are likely to interprete signal as noise, resulting in false negatives. We find that the Bayesian information criterion combined with a Bayes factor threshold of 150 can efficiently rule out false positives and confirm true detections. We further propose a Goldilocks principle aimed at modeling radial velocity noise to avoid too many false positives and too many false negatives. We propose that the noise model with RHK-dependent jitter is used in combination with the moving average model to detect planetary signals for M dwarfs. Our work may also shed light on the noise modeling for hotter stars, and provide a valid approach for finding similar principles in other disciplines.

</details>

<details>

<summary>2016-06-16 17:19:30 - Stochastic blockmodels for exchangeable collections of networks</summary>

- *Perla Reyes, Abel Rodriguez*

- `1606.05277v1` - [abs](http://arxiv.org/abs/1606.05277v1) - [pdf](http://arxiv.org/pdf/1606.05277v1)

> We construct a novel class of stochastic blockmodels using Bayesian nonparametric mixtures. These model allows us to jointly estimate the structure of multiple networks and explicitly compare the community structures underlying them, while allowing us to capture realistic properties of the underlying networks. Inference is carried out using MCMC algorithms that incorporates sequentially allocated split-merge steps to improve mixing. The models are illustrated using a simulation study and a variety of real-life examples.

</details>

<details>

<summary>2016-06-16 20:52:21 - Exact Bayesian inference for off-line change-point detection in tree-structured graphical models</summary>

- *LoÃ¯c Schwaller, StÃ©phane Robin*

- `1603.07871v2` - [abs](http://arxiv.org/abs/1603.07871v2) - [pdf](http://arxiv.org/pdf/1603.07871v2)

> We consider the problem of change-point detection in multivariate time-series. The multivariate distribution of the observations is supposed to follow a graphical model, whose graph and parameters are affected by abrupt changes throughout time. We demonstrate that it is possible to perform exact Bayesian inference whenever one considers a simple class of undirected graphs called spanning trees as possible structures. We are then able to integrate on the graph and segmentation spaces at the same time by combining classical dynamic programming with algebraic results pertaining to spanning trees. In particular, we show that quantities such as posterior distributions for change-points or posterior edge probabilities over time can efficiently be obtained. We illustrate our results on both synthetic and experimental data arising from biology and neuroscience.

</details>

<details>

<summary>2016-06-16 21:18:32 - A Bayesian nonparametric chi-squared goodness-of-fit test</summary>

- *Reyhaneh Hosseini, Mahmoud Zarepour*

- `1602.00197v3` - [abs](http://arxiv.org/abs/1602.00197v3) - [pdf](http://arxiv.org/pdf/1602.00197v3)

> The Bayesian nonparametric inference and Dirichlet process are popular tools in statistical methodologies. In this paper, we employ the Dirichlet process in hypothesis testing to propose a Bayesian nonparametric chi-squared goodness-of-fit test. In our Bayesian nonparametric approach, we consider the Dirichlet process as the prior for the distribution of data and carry out the test based on the Kullback-Leibler distance between the updated Dirichlet process and the hypothesized distribution F0. We prove that this distance asymptotically converges to the same chi-squared distribution as the chi-squared test does. Similarly, a Bayesian nonparametric chi-squared test of independence for a contingency table is provided. Also, by computing the Kullback-Leibler distance between the Dirichlet process and the hypothesized distribution, a method to obtain an appropriate concentration parameter for the Dirichlet process is suggested.

</details>

<details>

<summary>2016-06-17 22:06:26 - Reducing MSE in estimation of heavy tails: a Bayesian approach</summary>

- *Gaonyalelwe Maribe, AndrÃ©hette Verster, Jan Beirlant*

- `1606.05687v1` - [abs](http://arxiv.org/abs/1606.05687v1) - [pdf](http://arxiv.org/pdf/1606.05687v1)

> Bias reduction in tail estimation has received considerable interest in extreme value analysis. Estimation methods that minimize the bias while keeping the mean squared error (MSE) under control, are especially useful when applying classical methods such as the Hill (1975) estimator. In Caeiro et al. (2005) minimum variance reduced bias estimators of the Pareto tail index were first proposed where the bias is reduced without increasing the variance with respect to the Hill estimator. This method is based on adequate external estimation of a pair of second-order parameters. Here we revisit this problem from a Bayesian point of view starting from the extended Pareto distribution (EPD) approximation to excesses over a high threshold, as developed in Beirlant et al. (2009) using maximum likelihood (ML) estimation. Using asymptotic considerations, we derive an appropriate choice of priors leading to a Bayes estimator for which the MSE curve is a weighted average of the Hill and EPD-ML MSE curves for a large range of thresholds, under the same conditions as in Beirlant et al.(2009). A similar result is obtained for tail probability estimation. Simulations show surprisingly good MSE performance with respect to the existing estimators.

</details>

<details>

<summary>2016-06-19 18:07:15 - Clustering with a Reject Option: Interactive Clustering as Bayesian Prior Elicitation</summary>

- *Akash Srivastava, James Zou, Ryan P. Adams, Charles Sutton*

- `1606.05896v1` - [abs](http://arxiv.org/abs/1606.05896v1) - [pdf](http://arxiv.org/pdf/1606.05896v1)

> A good clustering can help a data analyst to explore and understand a data set, but what constitutes a good clustering may depend on domain-specific and application-specific criteria. These criteria can be difficult to formalize, even when it is easy for an analyst to know a good clustering when they see one. We present a new approach to interactive clustering for data exploration called TINDER, based on a particularly simple feedback mechanism, in which an analyst can reject a given clustering and request a new one, which is chosen to be different from the previous clustering while fitting the data well. We formalize this interaction in a Bayesian framework as a method for prior elicitation, in which each different clustering is produced by a prior distribution that is modified to discourage previously rejected clusterings. We show that TINDER successfully produces a diverse set of clusterings, each of equivalent quality, that are much more diverse than would be obtained by randomized restarts.

</details>

<details>

<summary>2016-06-20 07:54:51 - Bayesian inference for age-structured population model of infectious disease with application to varicella in Poland</summary>

- *Piotr Gwiazda, BÅaÅ¼ej Miasojedow, Magdalena RosiÅska*

- `1602.08861v2` - [abs](http://arxiv.org/abs/1602.08861v2) - [pdf](http://arxiv.org/pdf/1602.08861v2)

> Dynamics of the infectious disease transmission is often best understood taking into account the structure of population with respect to specific features, in example age or immunity level. Practical utility of such models depends on the appropriate calibration with the observed data. Here, we discuss the Bayesian approach to data assimilation in case of two-state age-structured model. This kind of models are frequently used to describe the disease dynamics (i.e. force of infection) basing on prevalence data collected at several time points. We demonstrate that, in the case when the explicit solution to the model equation is known, accounting for the data collection process in the Bayesian framework allows to obtain an unbiased posterior distribution for the parameters determining the force of infection. We further show analytically and through numerical tests that the posterior distribution of these parameters is stable with respect to cohort approximation (Escalator Boxcar Train) to the solution. Finally, we apply the technique to calibrate the model based on observed sero-prevalence of varicella in Poland.

</details>

<details>

<summary>2016-06-20 12:17:48 - Identifying Mixtures of Mixtures Using Bayesian Estimation</summary>

- *Gertraud Malsiner-Walli, Sylvia FrÃ¼hwirth-Schnatter, Bettina GrÃ¼n*

- `1502.06449v3` - [abs](http://arxiv.org/abs/1502.06449v3) - [pdf](http://arxiv.org/pdf/1502.06449v3)

> The use of a finite mixture of normal distributions in model-based clustering allows to capture non-Gaussian data clusters. However, identifying the clusters from the normal components is challenging and in general either achieved by imposing constraints on the model or by using post-processing procedures. Within the Bayesian framework we propose a different approach based on sparse finite mixtures to achieve identifiability. We specify a hierarchical prior where the hyperparameters are carefully selected such that they are reflective of the cluster structure aimed at. In addition this prior allows to estimate the model using standard MCMC sampling methods. In combination with a post-processing approach which resolves the label switching issue and results in an identified model, our approach allows to simultaneously (1) determine the number of clusters, (2) flexibly approximate the cluster distributions in a semi-parametric way using finite mixtures of normals and (3) identify cluster-specific parameters and classify observations. The proposed approach is illustrated in two simulation studies and on benchmark data sets.

</details>

<details>

<summary>2016-06-20 18:59:34 - An Empirical Comparison of Sampling Quality Metrics: A Case Study for Bayesian Nonnegative Matrix Factorization</summary>

- *Arjumand Masood, Weiwei Pan, Finale Doshi-Velez*

- `1606.06250v1` - [abs](http://arxiv.org/abs/1606.06250v1) - [pdf](http://arxiv.org/pdf/1606.06250v1)

> In this work, we empirically explore the question: how can we assess the quality of samples from some target distribution? We assume that the samples are provided by some valid Monte Carlo procedure, so we are guaranteed that the collection of samples will asymptotically approximate the true distribution. Most current evaluation approaches focus on two questions: (1) Has the chain mixed, that is, is it sampling from the distribution? and (2) How independent are the samples (as MCMC procedures produce correlated samples)? Focusing on the case of Bayesian nonnegative matrix factorization, we empirically evaluate standard metrics of sampler quality as well as propose new metrics to capture aspects that these measures fail to expose. The aspect of sampling that is of particular interest to us is the ability (or inability) of sampling methods to move between multiple optima in NMF problems. As a proxy, we propose and study a number of metrics that might quantify the diversity of a set of NMF factorizations obtained by a sampler through quantifying the coverage of the posterior distribution. We compare the performance of a number of standard sampling methods for NMF in terms of these new metrics.

</details>

<details>

<summary>2016-06-21 07:34:36 - Generalisations of Fisher Matrices</summary>

- *Alan Heavens*

- `1606.06455v1` - [abs](http://arxiv.org/abs/1606.06455v1) - [pdf](http://arxiv.org/pdf/1606.06455v1)

> Fisher matrices play an important role in experimental design and in data analysis. Their primary role is to make predictions for the inference of model parameters - both their errors and covariances. In this short review, I outline a number of extensions to the simple Fisher matrix formalism, covering a number of recent developments in the field. These are: (a) situations where the data (in the form of (x,y) pairs) have errors in both x and y; (b) modifications to parameter inference in the presence of systematic errors, or through fixing the values of some model parameters; (c) Derivative Approximation for LIkelihoods (DALI) - higher-order expansions of the likelihood surface, going beyond the Gaussian shape approximation; (d) extensions of the Fisher-like formalism, to treat model selection problems with Bayesian evidence.

</details>

<details>

<summary>2016-06-21 11:07:39 - Gene-Proximity Models For Genome-Wide Association Studies</summary>

- *Ian Johnston, Timothy Hancock, Hiroshi Mamitsuka, Luis Carvalho*

- `1311.0431v2` - [abs](http://arxiv.org/abs/1311.0431v2) - [pdf](http://arxiv.org/pdf/1311.0431v2)

> Motivated by the important problem of detecting association between genetic markers and binary traits in genome-wide association studies, we present a novel Bayesian model that establishes a hierarchy between markers and genes by defining weights according to gene lengths and distances from genes to markers. The proposed hierarchical model uses these weights to define unique prior probabilities of association for markers based on their proximities to genes that are believed to be relevant to the trait of interest. We use an expectation-maximization algorithm in a filtering step to first reduce the dimensionality of the data and then sample from the posterior distribution of the model parameters to estimate posterior probabilities of association for the markers. We offer practical and meaningful guidelines for the selection of the model tuning parameters and propose a pipeline that exploits a singular value decomposition on the raw data to make our model run efficiently on large data sets. We demonstrate the performance of the model in a simulation study and conclude by discussing the results of a case study using a real-world dataset provided by the Wellcome Trust Case Control Consortium.

</details>

<details>

<summary>2016-06-21 17:00:41 - A fully Bayesian strategy for high-dimensional hierarchical modeling using massively parallel computing</summary>

- *Will Landau, Jarad Niemi*

- `1606.06659v1` - [abs](http://arxiv.org/abs/1606.06659v1) - [pdf](http://arxiv.org/pdf/1606.06659v1)

> Markov chain Monte Carlo (MCMC) is the predominant tool used in Bayesian parameter estimation for hierarchical models. When the model expands due to an increasing number of hierarchical levels, number of groups at a particular level, or number of observations in each group, a fully Bayesian analysis via MCMC can easily become computationally demanding, even intractable. We illustrate how the steps in an MCMC for hierarchical models are predominantly one of two types: conditionally independent draws or low-dimensional draws based on summary statistics of parameters at higher levels of the hierarchy. Parallel computing can increase efficiency by performing embarrassingly parallel computations for conditionally independent draws and calculating the summary statistics using parallel reductions. During the MCMC algorithm, we record running means and means of squared parameter values to allow convergence diagnosis and posterior inference while avoiding the costly memory transfer bottleneck. We demonstrate the effectiveness of the algorithm on a model motivated by next generation sequencing data, and we release our implementation in R packages fbseq and fbseqCUDA.

</details>

<details>

<summary>2016-06-22 06:59:15 - Model-based clustering based on sparse finite Gaussian mixtures</summary>

- *Gertraud Malsiner-Walli, Sylvia FrÃ¼hwirth-Schnatter, Bettina GrÃ¼n*

- `1606.06828v1` - [abs](http://arxiv.org/abs/1606.06828v1) - [pdf](http://arxiv.org/pdf/1606.06828v1)

> In the framework of Bayesian model-based clustering based on a finite mixture of Gaussian distributions, we present a joint approach to estimate the number of mixture components and identify cluster-relevant variables simultaneously as well as to obtain an identified model. Our approach consists in specifying sparse hierarchical priors on the mixture weights and component means. In a deliberately overfitting mixture model the sparse prior on the weights empties superfluous components during MCMC. A straightforward estimator for the true number of components is given by the most frequent number of non-empty components visited during MCMC sampling. Specifying a shrinkage prior, namely the normal gamma prior, on the component means leads to improved parameter estimates as well as identification of cluster-relevant variables. After estimating the mixture model using MCMC methods based on data augmentation and Gibbs sampling, an identified model is obtained by relabeling the MCMC output in the point process representation of the draws. This is performed using $K$-centroids cluster analysis based on the Mahalanobis distance. We evaluate our proposed strategy in a simulation setup with artificial data and by applying it to benchmark data sets.

</details>

<details>

<summary>2016-06-22 11:49:18 - Spatial Bayesian Latent Factor Regression Modeling of Coordinate-based Meta-analysis Data</summary>

- *Silvia Montagna, Tor Wager, Lisa Feldman-Barrett, Timothy D. Johnson, Thomas E. Nichols*

- `1606.06912v1` - [abs](http://arxiv.org/abs/1606.06912v1) - [pdf](http://arxiv.org/pdf/1606.06912v1)

> Now over 20 years old, functional MRI (fMRI) has a large and growing literature that is best synthesised with meta-analytic tools. As most authors do not share image data, only the peak activation coordinates (foci) reported in the paper are available for Coordinate-based Meta-analysis (CBMA). Neuroimaging meta-analysis is used to 1) identify areas of consistent activation; and 2) build a predictive model of task type or cognitive process for new studies (reverse inference). To simultaneously address these aims, we propose a Bayesian point process hierarchical model for CBMA. We model the foci from each study as a doubly stochastic Poisson process, where the study-specific log intensity function is characterised as a linear combination of a high-dimensional basis set. A sparse representation of the intensities is guaranteed through latent factor modeling of the basis coefficients. Within our framework, it is also possible to account for the effect of study-level covariates (meta-regression), significantly expanding the capabilities of the current neuroimaging meta-analysis methods available. We apply our methodology to synthetic data and a neuroimaging meta-analysis dataset.

</details>

<details>

<summary>2016-06-22 17:04:08 - Exact formulas for the normalizing constants of Wishart distributions for graphical models</summary>

- *Caroline Uhler, Alex Lenkoski, Donald Richards*

- `1406.4901v2` - [abs](http://arxiv.org/abs/1406.4901v2) - [pdf](http://arxiv.org/pdf/1406.4901v2)

> Gaussian graphical models have received considerable attention during the past four decades from the statistical and machine learning communities. In Bayesian treatments of this model, the G-Wishart distribution serves as the conjugate prior for inverse covariance matrices satisfying graphical constraints. While it is straightforward to posit the unnormalized densities, the normalizing constants of these distributions have been known only for graphs that are chordal, or decomposable. Up until now, it was unknown whether the normalizing constant for a general graph could be represented explicitly, and a considerable body of computational literature emerged that attempted to avoid this apparent intractability. We close this question by providing an explicit representation of the G-Wishart normalizing constant for general graphs.

</details>

<details>

<summary>2016-06-22 17:48:17 - Efficient Attack Graph Analysis through Approximate Inference</summary>

- *Luis MuÃ±oz-GonzÃ¡lez, Daniele Sgandurra, Andrea Paudice, Emil C. Lupu*

- `1606.07025v1` - [abs](http://arxiv.org/abs/1606.07025v1) - [pdf](http://arxiv.org/pdf/1606.07025v1)

> Attack graphs provide compact representations of the attack paths that an attacker can follow to compromise network resources by analysing network vulnerabilities and topology. These representations are a powerful tool for security risk assessment. Bayesian inference on attack graphs enables the estimation of the risk of compromise to the system's components given their vulnerabilities and interconnections, and accounts for multi-step attacks spreading through the system. Whilst static analysis considers the risk posture at rest, dynamic analysis also accounts for evidence of compromise, e.g. from SIEM software or forensic investigation. However, in this context, exact Bayesian inference techniques do not scale well. In this paper we show how Loopy Belief Propagation - an approximate inference technique - can be applied to attack graphs, and that it scales linearly in the number of nodes for both static and dynamic analysis, making such analyses viable for larger networks. We experiment with different topologies and network clustering on synthetic Bayesian attack graphs with thousands of nodes to show that the algorithm's accuracy is acceptable and converge to a stable solution. We compare sequential and parallel versions of Loopy Belief Propagation with exact inference techniques for both static and dynamic analysis, showing the advantages of approximate inference techniques to scale to larger attack graphs.

</details>

<details>

<summary>2016-06-23 01:19:17 - Fast robustness quantification with variational Bayes</summary>

- *Ryan Giordano, Tamara Broderick, Rachael Meager, Jonathan Huggins, Michael Jordan*

- `1606.07153v1` - [abs](http://arxiv.org/abs/1606.07153v1) - [pdf](http://arxiv.org/pdf/1606.07153v1)

> Bayesian hierarchical models are increasing popular in economics. When using hierarchical models, it is useful not only to calculate posterior expectations, but also to measure the robustness of these expectations to reasonable alternative prior choices. We use variational Bayes and linear response methods to provide fast, accurate posterior means and robustness measures with an application to measuring the effectiveness of microcredit in the developing world.

</details>

<details>

<summary>2016-06-23 08:21:20 - The combinatorial structure of beta negative binomial processes</summary>

- *Creighton Heaukulani, Daniel M. Roy*

- `1401.0062v4` - [abs](http://arxiv.org/abs/1401.0062v4) - [pdf](http://arxiv.org/pdf/1401.0062v4)

> We characterize the combinatorial structure of conditionally-i.i.d. sequences of negative binomial processes with a common beta process base measure. In Bayesian nonparametric applications, such processes have served as models for latent multisets of features underlying data. Analogously, random subsets arise from conditionally-i.i.d. sequences of Bernoulli processes with a common beta process base measure, in which case the combinatorial structure is described by the Indian buffet process. Our results give a count analogue of the Indian buffet process, which we call a negative binomial Indian buffet process. As an intermediate step toward this goal, we provide a construction for the beta negative binomial process that avoids a representation of the underlying beta process base measure. We describe the key Markov kernels needed to use a NB-IBP representation in a Markov Chain Monte Carlo algorithm targeting a posterior distribution.

</details>

<details>

<summary>2016-06-23 19:03:47 - Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors</summary>

- *Christos Louizos, Max Welling*

- `1603.04733v5` - [abs](http://arxiv.org/abs/1603.04733v5) - [pdf](http://arxiv.org/pdf/1603.04733v5)

> We introduce a variational Bayesian neural network where the parameters are governed via a probability distribution on random matrices. Specifically, we employ a matrix variate Gaussian \cite{gupta1999matrix} parameter posterior distribution where we explicitly model the covariance among the input and output dimensions of each layer. Furthermore, with approximate covariance matrices we can achieve a more efficient way to represent those correlations that is also cheaper than fully factorized parameter posteriors. We further show that with the "local reprarametrization trick" \cite{kingma2015variational} on this posterior distribution we arrive at a Gaussian Process \cite{rasmussen2006gaussian} interpretation of the hidden units in each layer and we, similarly with \cite{gal2015dropout}, provide connections with deep Gaussian processes. We continue in taking advantage of this duality and incorporate "pseudo-data" \cite{snelson2005sparse} in our model, which in turn allows for more efficient sampling while maintaining the properties of the original model. The validity of the proposed approach is verified through extensive experiments.

</details>

<details>

<summary>2016-06-24 04:46:21 - Bayesian Estimators in Uncertain Nested Error Regression Models</summary>

- *Shonosuke Sugasawa, Tatsuya Kubokawa*

- `1509.07942v2` - [abs](http://arxiv.org/abs/1509.07942v2) - [pdf](http://arxiv.org/pdf/1509.07942v2)

> Nested error regression models are useful tools for analysis of grouped data, especially in the case of small area estimation. This paper suggests a nested error regression model using uncertain random effects in which the random effect in each area is expressed as a mixture of a normal distribution and a positive mass at $0$. For estimation of the model parameters and prediction of the random effects, an objective Bayesian inference is proposed by setting non-informative prior distributions on the model parameters. Under mild sufficient conditions, it is shown that the posterior distribution is proper and the posterior variances are finite, confirming the validity of posterior inference. To generate samples from the posterior distribution, we provide the Gibbs sampling method with familiar forms for all the full conditional distributions. This paper also addresses the problem of predicting finite population means, and a sampling-based method is suggested to tackle this issue. Finally, the proposed model is compared with the conventional nested error regression model through simulation and empirical studies.

</details>

<details>

<summary>2016-06-24 12:50:11 - A Bayesian hierarchical model for monthly maxima of instantaneous flow</summary>

- *Egil Ferkingstad, Oli Pall Geirsson, Birgir Hrafnkelsson, Olafur Birgir Davidsson, Sigurdur Magnus Gardarsson*

- `1606.07667v1` - [abs](http://arxiv.org/abs/1606.07667v1) - [pdf](http://arxiv.org/pdf/1606.07667v1)

> We propose a comprehensive Bayesian hierarchical model for monthly maxima of instantaneous flow in river catchments. The Gumbel distribution is used as the probabilistic model for the observations, which are assumed to come from several catchments. Our suggested latent model is Gaussian and designed for monthly maxima, making better use of the data than the standard approach using annual maxima. At the latent level, linear mixed models are used for both the location and scale parameters of the Gumbel distribution, accounting for seasonal dependence and covariates from the catchments. The specification of prior distributions makes use of penalised complexity (PC) priors, to ensure robust inference for the latent parameters. The main idea behind the PC priors is to shrink toward a base model, thus avoiding overfitting. PC priors also provide a convenient framework for prior elicitation based on simple notions of scale. Prior distributions for regression coefficients are also elicited based on hydrological and meteorological knowledge. Posterior inference was done using the MCMC split sampler, an efficient Gibbs blocking scheme tailored to latent Gaussian models. The proposed model was applied to observed data from eight river catchments in Iceland. A cross-validation study demonstrates good predictive performance.

</details>

<details>

<summary>2016-06-24 15:17:10 - Multiple Change Point Analysis: Fast Implementation And Strong Consistency</summary>

- *Jie Ding, Yu Xiang, Lu Shen, Vahid Tarokh*

- `1605.00346v2` - [abs](http://arxiv.org/abs/1605.00346v2) - [pdf](http://arxiv.org/pdf/1605.00346v2)

> One of the main challenges in identifying structural changes in stochastic processes is to carry out analysis for time series with dependency structure in a computationally tractable way. Another challenge is that the number of true change points is usually unknown, requiring a suitable model selection criterion to arrive at informative conclusions. To address the first challenge, we model the data generating process as a segment-wise autoregression, which is composed of several segments (time epochs), each of which modeled by an autoregressive model. We propose a multi-window method that is both effective and efficient for discovering the structural changes. The proposed approach was motivated by transforming a segment-wise autoregression into a multivariate time series that is asymptotically segment-wise independent and identically distributed. To address the second challenge, we derive theoretical guarantees for (almost surely) selecting the true number of change points of segment-wise independent multivariate time series. Specifically, under mild assumptions, we show that a Bayesian Information Criterion (BIC)-like criterion gives a strongly consistent selection of the optimal number of change points, while an Akaike Information Criterion (AIC)-like criterion cannot. Finally, we demonstrate the theory and strength of the proposed algorithms by experiments on both synthetic and real-world data, including the Eastern US temperature data and the El Nino data from 1854 to 2015. The experiment leads to some interesting discoveries about temporal variability of the summer-time temperature over the Eastern US, and about the most dominant factor of ocean influence on climate.

</details>

<details>

<summary>2016-06-24 21:51:24 - Modeling Group Dynamics Using Probabilistic Tensor Decompositions</summary>

- *Lin Li, Ananthram Swami, Anna Scaglione*

- `1606.07840v1` - [abs](http://arxiv.org/abs/1606.07840v1) - [pdf](http://arxiv.org/pdf/1606.07840v1)

> We propose a probabilistic modeling framework for learning the dynamic patterns in the collective behaviors of social agents and developing profiles for different behavioral groups, using data collected from multiple information sources. The proposed model is based on a hierarchical Bayesian process, in which each observation is a finite mixture of an set of latent groups and the mixture proportions (i.e., group probabilities) are drawn randomly. Each group is associated with some distributions over a finite set of outcomes. Moreover, as time evolves, the structure of these groups also changes; we model the change in the group structure by a hidden Markov model (HMM) with a fixed transition probability. We present an efficient inference method based on tensor decompositions and the expectation-maximization (EM) algorithm for parameter estimation.

</details>

<details>

<summary>2016-06-24 22:15:19 - Robust and scalable Bayesian analysis of spatial neural tuning function data</summary>

- *Kamiar Rahnama Rad, Timothy A. Machado, Liam Paninski*

- `1606.07845v1` - [abs](http://arxiv.org/abs/1606.07845v1) - [pdf](http://arxiv.org/pdf/1606.07845v1)

> A common analytical problem in neuroscience is the interpretation of neural activity with respect to sensory input or behavioral output. This is typically achieved by regressing measured neural activity against known stimuli or behavioral variables to produce a "tuning function" for each neuron. Unfortunately, because this approach handles neurons individually, it cannot take advantage of simultaneous measurements from spatially adjacent neurons that often have similar tuning properties. On the other hand, sharing information between adjacent neurons can errantly degrade estimates of tuning functions across space if there are sharp discontinuities in tuning between nearby neurons. In this paper, we develop a computationally efficient block Gibbs sampler that effectively pools information between neurons to de-noise tuning function estimates while simultaneously preserving sharp discontinuities that might exist in the organization of tuning across space. This method is fully Bayesian and its computational cost per iteration scales sub-quadratically with total parameter dimensionality. We demonstrate the robustness and scalability of this approach by applying it to both real and synthetic datasets. In particular, an application to data from the spinal cord illustrates that the proposed methods can dramatically decrease the experimental time required to accurately estimate tuning functions.

</details>

<details>

<summary>2016-06-25 02:37:45 - Adaptive Image Denoising by Mixture Adaptation</summary>

- *Enming Luo, Stanley H. Chan, Truong Q. Nguyen*

- `1601.04770v3` - [abs](http://arxiv.org/abs/1601.04770v3) - [pdf](http://arxiv.org/pdf/1601.04770v3)

> We propose an adaptive learning procedure to learn patch-based image priors for image denoising. The new algorithm, called the Expectation-Maximization (EM) adaptation, takes a generic prior learned from a generic external database and adapts it to the noisy image to generate a specific prior. Different from existing methods that combine internal and external statistics in ad-hoc ways, the proposed algorithm is rigorously derived from a Bayesian hyper-prior perspective. There are two contributions of this paper: First, we provide full derivation of the EM adaptation algorithm and demonstrate methods to improve the computational complexity. Second, in the absence of the latent clean image, we show how EM adaptation can be modified based on pre-filtering. Experimental results show that the proposed adaptation algorithm yields consistently better denoising results than the one without adaptation and is superior to several state-of-the-art algorithms.

</details>

<details>

<summary>2016-06-26 01:23:28 - Efficient Bayesian Learning in Social Networks with Gaussian Estimators</summary>

- *Elchanan Mossel, Noah Olsman, Omer Tamuz*

- `1002.0747v3` - [abs](http://arxiv.org/abs/1002.0747v3) - [pdf](http://arxiv.org/pdf/1002.0747v3)

> We consider a group of Bayesian agents who try to estimate a state of the world $\theta$ through interaction on a social network. Each agent $v$ initially receives a private measurement of $\theta$: a number $S_v$ picked from a Gaussian distribution with mean $\theta$ and standard deviation one. Then, in each discrete time iteration, each reveals its estimate of $\theta$ to its neighbors, and, observing its neighbors' actions, updates its belief using Bayes' Law.   This process aggregates information efficiently, in the sense that all the agents converge to the belief that they would have, had they access to all the private measurements. We show that this process is computationally efficient, so that each agent's calculation can be easily carried out. We also show that on any graph the process converges after at most $2N \cdot D$ steps, where $N$ is the number of agents and $D$ is the diameter of the network. Finally, we show that on trees and on distance transitive-graphs the process converges after $D$ steps, and that it preserves privacy, so that agents learn very little about the private signal of most other agents, despite the efficient aggregation of information. Our results extend those in an unpublished manuscript of the first and last authors.

</details>

<details>

<summary>2016-06-26 12:59:35 - A Moreau-Yosida approximation scheme for a class of high-dimensional posterior distributions</summary>

- *Yves F. AtchadÃ©*

- `1505.07072v2` - [abs](http://arxiv.org/abs/1505.07072v2) - [pdf](http://arxiv.org/pdf/1505.07072v2)

> Exact-sparsity inducing prior distributions in Bayesian analysis typically lead to posterior distributions that are very challenging to handle by standard Markov Chain Monte Carlo (MCMC) methods, particular in high-dimensional models with large number of parameters. We propose a methodology to derive smooth approximations of such posterior distributions that are, in some cases, easier to handle by standard MCMC methods. The approximation is obtained from the forward-backward approximation of the Moreau-Yosida regularization of the negative log-density. We show that the derived approximation is within $O(\sqrt{\gamma})$ of the true posterior distribution in the $\beta$-metric, where $\gamma>0$ is a user-controlled parameter that defines the approximation. We illustrate the method with a high-dimensional linear regression model.

</details>

<details>

<summary>2016-06-26 18:59:11 - The Joint Projected and Skew Normal</summary>

- *Gianluca Mastrantonio*

- `1512.00487v2` - [abs](http://arxiv.org/abs/1512.00487v2) - [pdf](http://arxiv.org/pdf/1512.00487v2)

> We introduce a new multivariate circular linear distribution suitable for modeling direction and speed in (multiple) animal movement data. To properly account for specific data features, such as heterogeneity and time dependence, a hidden Markov model is used. Parameters are estimated under a Bayesian framework and we provide computational details to implement the Markov chain Monte Carlo algorithm.   The proposed model is applied to a dataset of six free-ranging Maremma Sheepdogs. Its predictive performance, as well as the interpretability of the results, are compared to those given by hidden Markov models built on all the combinations of von Mises (circular), wrapped Cauchy (circular), gamma (linear) and Weibull (linear) distributions

</details>

<details>

<summary>2016-06-27 08:20:46 - Geometric ergodicity of Rao and Teh's algorithm for Markov jump processes and CTBNs</summary>

- *BÅaÅ¼ej Miasojedow, Wojcieh Niemiro*

- `1606.08160v1` - [abs](http://arxiv.org/abs/1606.08160v1) - [pdf](http://arxiv.org/pdf/1606.08160v1)

> Rao and Teh (2012, 2013) introduced an efficient MCMC algorithm for sampling from the posterior distribution of a hidden Markov jump process. The algorithm is based on the idea of sampling virtual jumps. In the present paper we show that the Markov chain generated by Rao and Teh's algorithm is geometrically ergodic. To this end we establish a geometric drift condition towards a small set. A similar result is also proved for a special version of the algorithm, used for probabilistic inference in Continuous Time Bayesian Networks.

</details>

<details>

<summary>2016-06-27 14:40:23 - Bayesian forecasting and scalable multivariate volatility analysis using simultaneous graphical dynamic models</summary>

- *Lutz F. Gruber, Mike West*

- `1606.08291v1` - [abs](http://arxiv.org/abs/1606.08291v1) - [pdf](http://arxiv.org/pdf/1606.08291v1)

> The recently introduced class of simultaneous graphical dynamic linear models (SGDLMs) defines an ability to scale on-line Bayesian analysis and forecasting to higher-dimensional time series. This paper advances the methodology of SGDLMs, developing and embedding a novel, adaptive method of simultaneous predictor selection in forward filtering for on-line learning and forecasting. The advances include developments in Bayesian computation for scalability, and a case study in exploring the resulting potential for improved short-term forecasting of large-scale volatility matrices. A case study concerns financial forecasting and portfolio optimization with a 400-dimensional series of daily stock prices. Analysis shows that the SGDLM forecasts volatilities and co-volatilities well, making it ideally suited to contributing to quantitative investment strategies to improve portfolio returns. We also identify performance metrics linked to the sequential Bayesian filtering analysis that turn out to define a leading indicator of increased financial market stresses, comparable to but leading the standard St. Louis Fed Financial Stress Index (STLFSI) measure. Parallel computation using GPU implementations substantially advance the ability to fit and use these models.

</details>

<details>

<summary>2016-06-27 14:40:51 - Dynamics and sparsity in latent threshold factor models: A study in multivariate EEG signal processing</summary>

- *Jouchi Nakajima, Mike West*

- `1606.08292v1` - [abs](http://arxiv.org/abs/1606.08292v1) - [pdf](http://arxiv.org/pdf/1606.08292v1)

> We discuss Bayesian analysis of multivariate time series with dynamic factor models that exploit time-adaptive sparsity in model parametrizations via the latent threshold approach. One central focus is on the transfer responses of multiple interrelated series to underlying, dynamic latent factor processes. Structured priors on model hyper-parameters are key to the efficacy of dynamic latent thresholding, and MCMC-based computation enables model fitting and analysis. A detailed case study of electroencephalographic (EEG) data from experimental psychiatry highlights the use of latent threshold extensions of time-varying vector autoregressive and factor models. This study explores a class of dynamic transfer response factor models, extending prior Bayesian modeling of multiple EEG series and highlighting the practical utility of the latent thresholding concept in multivariate, non-stationary time series analysis.

</details>

<details>

<summary>2016-06-27 16:04:59 - Models of random sparse eigenmatrices matrices and Bayesian analysis of multivariate structure</summary>

- *Andrew J. Cron, Mike West*

- `1606.08337v1` - [abs](http://arxiv.org/abs/1606.08337v1) - [pdf](http://arxiv.org/pdf/1606.08337v1)

> We discuss probabilistic models of random covariance structures defined by distributions over sparse eigenmatrices. The decomposition of orthogonal matrices in terms of Givens rotations defines a natural, interpretable framework for defining distributions on sparsity structure of random eigenmatrices. We explore theoretical aspects and implications for conditional independence structures arising in multivariate Gaussian models, and discuss connections with sparse PCA, factor analysis and Gaussian graphical models. Methodology includes model-based exploratory data analysis and Bayesian analysis via reversible jump Markov chain Monte Carlo. A simulation study examines the ability to identify sparse multivariate structures compared to the benchmark graphical modelling approach. Extensions to multivariate normal mixture models with additional measurement errors move into the framework of latent structure analysis of broad practical interest. We explore the implications and utility of the new models with summaries of a detailed applied study of a 20-dimensional breast cancer genomics data set.

</details>

<details>

<summary>2016-06-27 16:05:42 - Dynamic dependence networks: Financial time series forecasting and portfolio decisions (with discussion)</summary>

- *Zoey Yi Zhao, Meng Xie, Mike West*

- `1606.08339v1` - [abs](http://arxiv.org/abs/1606.08339v1) - [pdf](http://arxiv.org/pdf/1606.08339v1)

> We discuss Bayesian forecasting of increasingly high-dimensional time series, a key area of application of stochastic dynamic models in the financial industry and allied areas of business. Novel state-space models characterizing sparse patterns of dependence among multiple time series extend existing multivariate volatility models to enable scaling to higher numbers of individual time series. The theory of these "dynamic dependence network" models shows how the individual series can be "decoupled" for sequential analysis, and then "recoupled" for applied forecasting and decision analysis. Decoupling allows fast, efficient analysis of each of the series in individual univariate models that are linked-- for later recoupling-- through a theoretical multivariate volatility structure defined by a sparse underlying graphical model. Computational advances are especially significant in connection with model uncertainty about the sparsity patterns among series that define this graphical model; Bayesian model averaging using discounting of historical information builds substantially on this computational advance. An extensive, detailed case study showcases the use of these models, and the improvements in forecasting and financial portfolio investment decisions that are achievable. Using a long series of daily international currency, stock indices and commodity prices, the case study includes evaluations of multi-day forecasts and Bayesian portfolio analysis with a variety of practical utility functions, as well as comparisons against commodity trading advisor benchmarks.

</details>

<details>

<summary>2016-06-27 20:01:08 - Anomaly detection in video with Bayesian nonparametrics</summary>

- *Olga Isupova, Danil Kuzin, Lyudmila Mihaylova*

- `1606.08455v1` - [abs](http://arxiv.org/abs/1606.08455v1) - [pdf](http://arxiv.org/pdf/1606.08455v1)

> A novel dynamic Bayesian nonparametric topic model for anomaly detection in video is proposed in this paper. Batch and online Gibbs samplers are developed for inference. The paper introduces a new abnormality measure for decision making. The proposed method is evaluated on both synthetic and real data. The comparison with a non-dynamic model shows the superiority of the proposed dynamic one in terms of the classification performance for anomaly detection.

</details>

<details>

<summary>2016-06-28 04:16:25 - Automatic Variational ABC</summary>

- *Alexander Moreno, Tameem Adel, Edward Meeds, James M. Rehg, Max Welling*

- `1606.08549v1` - [abs](http://arxiv.org/abs/1606.08549v1) - [pdf](http://arxiv.org/pdf/1606.08549v1)

> Approximate Bayesian Computation (ABC) is a framework for performing likelihood-free posterior inference for simulation models. Stochastic Variational inference (SVI) is an appealing alternative to the inefficient sampling approaches commonly used in ABC. However, SVI is highly sensitive to the variance of the gradient estimators, and this problem is exacerbated by approximating the likelihood. We draw upon recent advances in variance reduction for SV and likelihood-free inference using deterministic simulations to produce low variance gradient estimators of the variational lower-bound. By then exploiting automatic differentiation libraries we can avoid nearly all model-specific derivations. We demonstrate performance on three problems and compare to existing SVI algorithms. Our results demonstrate the correctness and efficiency of our algorithm.

</details>

<details>

<summary>2016-06-28 13:48:21 - Expectation propagation for continuous time stochastic processes</summary>

- *Botond Cseke, David Schnoerr, Manfred Opper, Guido Sanguinetti*

- `1512.06098v2` - [abs](http://arxiv.org/abs/1512.06098v2) - [pdf](http://arxiv.org/pdf/1512.06098v2)

> We consider the inverse problem of reconstructing the posterior measure over the trajec- tories of a diffusion process from discrete time observations and continuous time constraints. We cast the problem in a Bayesian framework and derive approximations to the posterior distributions of single time marginals using variational approximate inference. We then show how the approximation can be extended to a wide class of discrete-state Markov jump pro- cesses by making use of the chemical Langevin equation. Our empirical results show that the proposed method is computationally efficient and provides good approximations for these classes of inverse problems.

</details>

<details>

<summary>2016-06-28 15:40:17 - Bayesian analysis of immune response dynamics with sparse time series data</summary>

- *Fernando V. Bonassi, Cliburn Chan, Mike West*

- `1606.08759v1` - [abs](http://arxiv.org/abs/1606.08759v1) - [pdf](http://arxiv.org/pdf/1606.08759v1)

> In vaccine development, the temporal profiles of relative abundance of subtypes of immune cells (T-cells) is key to understanding vaccine efficacy. Complex and expensive experimental studies generate very sparse time series data on this immune response. Fitting multi-parameter dynamic models of the immune response dynamics-- central to evaluating mechanisms underlying vaccine efficacy-- is challenged by data sparsity. The research reported here addresses this challenge. For HIV/SIV vaccine studies in macaques, we: (a) introduce novel dynamic models of progression of cellular populations over time with relevant, time-delayed components reflecting the vaccine response; (b) define an effective Bayesian model fitting strategy that couples Markov chain Monte Carlo (MCMC) with Approximate Bayesian Computation (ABC)-- building on the complementary strengths of the two approaches, neither of which is effective alone; (c) explore questions of information content in the sparse time series for each of the model parameters, linking into experimental design and model simplification for future experiments; and (d) develop, apply and compare the analysis with samples from a recent HIV/SIV experiment, with novel insights and conclusions about the progressive response to the vaccine, and how this varies across subjects.

</details>

<details>

<summary>2016-06-28 18:06:58 - EigenPrism: Inference for High-Dimensional Signal-to-Noise Ratios</summary>

- *Lucas Janson, Rina Foygel Barber, Emmanuel CandÃ¨s*

- `1505.02097v2` - [abs](http://arxiv.org/abs/1505.02097v2) - [pdf](http://arxiv.org/pdf/1505.02097v2)

> Consider the following three important problems in statistical inference, namely, constructing confidence intervals for (1) the error of a high-dimensional ($p>n$) regression estimator, (2) the linear regression noise level, and (3) the genetic signal-to-noise ratio of a continuous-valued trait (related to the heritability). All three problems turn out to be closely related to the little-studied problem of performing inference on the $\ell_2$-norm of the signal in high-dimensional linear regression. We derive a novel procedure for this, which is asymptotically correct when the covariates are multivariate Gaussian and produces valid confidence intervals in finite samples as well. The procedure, called EigenPrism, is computationally fast and makes no assumptions on coefficient sparsity or knowledge of the noise level. We investigate the width of the EigenPrism confidence intervals, including a comparison with a Bayesian setting in which our interval is just 5% wider than the Bayes credible interval. We are then able to unify the three aforementioned problems by showing that the EigenPrism procedure with only minor modifications is able to make important contributions to all three. We also investigate the robustness of coverage and find that the method applies in practice and in finite samples much more widely than just the case of multivariate Gaussian covariates. Finally, we apply EigenPrism to a genetic dataset to estimate the genetic signal-to-noise ratio for a number of continuous phenotypes.

</details>

<details>

<summary>2016-06-29 09:27:40 - Consider avoiding the .05 significance level</summary>

- *David Navon, Yoav Cohen*

- `1606.09017v1` - [abs](http://arxiv.org/abs/1606.09017v1) - [pdf](http://arxiv.org/pdf/1606.09017v1)

> It is suggested that some shortcomings of Null Hypothesis Significance Testing (NHST), viewed from the perspective of Bayesian statistics, turn benign once the traditional threshold p value of .05 is substituted by a sufficiently smaller value. To illustrate, the posterior probability of H0 stating P=.5, given data that just render it rejected by NHST with a p value of .05 (and a uniform prior), is shown here to be not much smaller than .50 for most values of N below 100 (and even exceeds .50 for N>=100); in contrast, with a p value of .001 posterior probability does not exceed .06 for N<=100 (neither .25 for N<9000). Yet more interesting, posterior probability becomes quite independent of N with a p value of .0001, hence practically satisfying the alpha postulate - set by Cornfield (1966) as the condition for p value being a measure of evidence in itself. In view of the low prospect that most researchers will soon convert to use Bayesian statistics in any form, we thus suggest that researchers who elect the conservative option of resorting to NHST be encouraged to avoid as much as possible using a p value of .05 as a threshold for rejecting H0. The analysis presented here may be used to discuss afresh which level of threshold p value seems to be a reasonable, practical substitute.

</details>

<details>

<summary>2016-06-30 08:50:42 - An On-line Variational Bayesian Model for Multi-Person Tracking from Cluttered Scenes</summary>

- *Sileye Ba, Xavier Alameda-Pineda, Alessio Xompero, Radu Horaud*

- `1509.01520v3` - [abs](http://arxiv.org/abs/1509.01520v3) - [pdf](http://arxiv.org/pdf/1509.01520v3)

> Object tracking is an ubiquitous problem that appears in many applications such as remote sensing, audio processing, computer vision, human-machine interfaces, human-robot interaction, etc. Although thoroughly investigated in computer vision, tracking a time-varying number of persons remains a challenging open problem. In this paper, we propose an on-line variational Bayesian model for multi-person tracking from cluttered visual observations provided by person detectors. The contributions of this paper are the followings. First, we propose a variational Bayesian framework for tracking an unknown and varying number of persons. Second, our model results in a variational expectation-maximization (VEM) algorithm with closed-form expressions for the posterior distributions of the latent variables and for the estimation of the model parameters. Third, the proposed model exploits observations from multiple detectors, and it is therefore multimodal by nature. Finally, we propose to embed both object-birth and object-visibility processes in an effort to robustly handle person appearances and disappearances over time. Evaluated on classical multiple person tracking datasets, our method shows competitive results with respect to state-of-the-art multiple-object tracking models, such as the probability hypothesis density (PHD) filter among others.

</details>

<details>

<summary>2016-06-30 11:28:24 - Bayesian Design of Experiments using Approximate Coordinate Exchange</summary>

- *Antony Overstall, David Woods*

- `1501.00264v4` - [abs](http://arxiv.org/abs/1501.00264v4) - [pdf](http://arxiv.org/pdf/1501.00264v4)

> The construction of decision-theoretic Bayesian designs for realistically-complex nonlinear models is computationally challenging, as it requires the optimization of analytically intractable expected utility functions over high-dimensional design spaces. We provide the most general solution to date for this problem through a novel approximate coordinate exchange algorithm. This methodology uses a Gaussian process emulator to approximate the expected utility as a function of a single design coordinate in a series of conditional optimization steps. It has flexibility to address problems for any choice of utility function and for a wide range of statistical models with different numbers of variables, numbers of runs and randomization restrictions. In contrast to existing approaches to Bayesian design, the method can find multi-variable designs in large numbers of runs without resorting to asymptotic approximations to the posterior distribution or expected utility. The methodology is demonstrated on a variety of challenging examples of practical importance, including design for pharmacokinetic models and design for mixed models with discrete data. For many of these models, Bayesian designs are not currently available. Comparisons are made to results from the literature, and to designs obtained from asymptotic approximations.

</details>

<details>

<summary>2016-06-30 17:39:18 - Hierarchical animal movement models for population-level inference</summary>

- *Mevin B. Hooten, Frances E. Buderman, Brian M. Brost, Ephraim M. Hanks, Jacob S. Ivan*

- `1606.09585v1` - [abs](http://arxiv.org/abs/1606.09585v1) - [pdf](http://arxiv.org/pdf/1606.09585v1)

> New methods for modeling animal movement based on telemetry data are developed regularly. With advances in telemetry capabilities, animal movement models are becoming increasingly sophisticated. Despite a need for population-level inference, animal movement models are still predominantly developed for individual-level inference. Most efforts to upscale the inference to the population-level are either post hoc or complicated enough that only the developer can implement the model. Hierarchical Bayesian models provide an ideal platform for the development of population-level animal movement models but can be challenging to fit due to computational limitations or extensive tuning required. We propose a two-stage procedure for fitting hierarchical animal movement models to telemetry data. The two-stage approach is statistically rigorous and allows one to fit individual-level movement models separately, then resample them using a secondary MCMC algorithm. The primary advantages of the two-stage approach are that the first stage is easily parallelizable and the second stage is completely unsupervised, allowing for a completely automated fitting procedure in many cases. We demonstrate the two-stage procedure with two applications of animal movement models. The first application involves a spatial point process approach to modeling telemetry data and the second involves a more complicated continuous-time discrete-space animal movement model. We fit these models to simulated data and real telemetry data arising from a population of monitored Canada lynx in Colorado, USA.

</details>


## 2016-07

<details>

<summary>2016-07-01 01:10:35 - Reducing overfitting in challenge-based competitions</summary>

- *Elias Chaibub Neto, Bruce R Hoff, Chris Bare, Brian M Bot, Thomas Yu, Lara Magravite, Andrew D Trister, Thea Norman, Pablo Meyer, Julio Saez-Rodrigues, James C Costello, Justin Guinney, Gustavo Stolovitzky*

- `1607.00091v1` - [abs](http://arxiv.org/abs/1607.00091v1) - [pdf](http://arxiv.org/pdf/1607.00091v1)

> Over-fitting is a dreaded foe in challenge-based competitions. Because participants rely on public leaderboards to evaluate and refine their models, there is always the danger they might over-fit to the holdout data supporting the leaderboard. The recently published Ladder algorithm aims to address this problem by preventing the participants from exploiting willingly or inadvertently minor fluctuations in public leaderboard scores during model refinement. In this paper, we report a vulnerability of the Ladder that induces severe over-fitting of the leaderboard when the sample size is small. To circumvent this attack, we propose a variation of the Ladder that releases a bootstrapped estimate of the public leaderboard score instead of providing participants with a direct measure of performance. We also extend the scope of the Ladder to arbitrary performance metrics by relying on a more broadly applicable testing procedure based on the Bayesian bootstrap. Our method makes it possible to use a leaderboard, with the technical and social advantages that it provides, even in cases where data is scant.

</details>

<details>

<summary>2016-07-01 21:27:40 - Hybrid optimization and Bayesian inference techniques for a non-smooth radiation detection problem</summary>

- *Razvan Stefanescu, Kathleen Schmidt, Jason Hite, Ralph Smith, John Mattingly*

- `1607.00411v1` - [abs](http://arxiv.org/abs/1607.00411v1) - [pdf](http://arxiv.org/pdf/1607.00411v1)

> In this investigation, we propose several algorithms to recover the location and intensity of a radiation source located in a simulated 250 m x 180 m block in an urban center based on synthetic measurements. Radioactive decay and detection are Poisson random processes, so we employ likelihood functions based on this distribution. Due to the domain geometry and the proposed response model, the negative logarithm of the likelihood is only piecewise continuous differentiable, and it has multiple local minima. To address these difficulties, we investigate three hybrid algorithms comprised of mixed optimization techniques. For global optimization, we consider Simulated Annealing (SA), Particle Swarm (PS) and Genetic Algorithm (GA), which rely solely on objective function evaluations; i.e., they do not evaluate the gradient in the objective function. By employing early stopping criteria for the global optimization methods, a pseudo-optimum point is obtained. This is subsequently utilized as the initial value by the deterministic Implicit Filtering method (IF), which is able to find local extrema in non-smooth functions, to finish the search in a narrow domain. These new hybrid techniques combining global optimization and Implicit Filtering address difficulties associated with the non-smooth response, and their performances are shown to significantly decrease the computational time over the global optimization methods alone. To quantify uncertainties associated with the source location and intensity, we employ the Delayed Rejection Adaptive Metropolis (DRAM) and DiffeRential Evolution Adaptive Metropolis (DREAM) algorithms. Marginal densities of the source properties are obtained, and the means of the chains' compare accurately with the estimates produced by the hybrid algorithms.

</details>

<details>

<summary>2016-07-02 10:44:11 - Bayesian nonparametric inference for discovery probabilities: credible intervals and large sample asymptotics</summary>

- *Julyan Arbel, Stefano Favaro, Bernardo Nipoti, Yee Whye Teh*

- `1506.04915v2` - [abs](http://arxiv.org/abs/1506.04915v2) - [pdf](http://arxiv.org/pdf/1506.04915v2)

> Given a sample of size $n$ from a population of individuals belonging to different species with unknown proportions, a popular problem of practical interest consists in making inference on the probability $D_{n}(l)$ that the $(n+1)$-th draw coincides with a species with frequency $l$ in the sample, for any $l=0,1,\ldots,n$. This paper contributes to the methodology of Bayesian nonparametric inference for $D_{n}(l)$. Specifically, under the general framework of Gibbs-type priors we show how to derive credible intervals for a Bayesian nonparametric estimation of $D_{n}(l)$, and we investigate the large $n$ asymptotic behaviour of such an estimator. Of particular interest are special cases of our results obtained under the specification of the two parameter Poisson--Dirichlet prior and the normalized generalized Gamma prior, which are two of the most commonly used Gibbs-type priors. With respect to these two prior specifications, the proposed results are illustrated through a simulation study and a benchmark Expressed Sequence Tags dataset. To the best our knowledge, this illustration provides the first comparative study between the two parameter Poisson--Dirichlet prior and the normalized generalized Gamma prior in the context of Bayesian nonparemetric inference for $D_{n}(l)$.

</details>

<details>

<summary>2016-07-02 20:32:11 - Accelerating MCMC with active subspaces</summary>

- *Paul G. Constantine, Carson Kent, Tan Bui-Thanh*

- `1510.00024v2` - [abs](http://arxiv.org/abs/1510.00024v2) - [pdf](http://arxiv.org/pdf/1510.00024v2)

> The Markov chain Monte Carlo (MCMC) method is the computational workhorse for Bayesian inverse problems. However, MCMC struggles in high-dimensional parameter spaces, since its iterates must sequentially explore the high-dimensional space. This struggle is compounded in physical applications when the nonlinear forward model is computationally expensive. One approach to accelerate MCMC is to reduce the dimension of the state space. Active subspaces are part of an emerging set of tools for subspace-based dimension reduction. An active subspace in a given inverse problem indicates a separation between a low-dimensional subspace that is informed by the data and its orthogonal complement that is constrained by the prior. With this information, one can run the sequential MCMC on the active variables while sampling independently according to the prior on the inactive variables. However, this approach to increase efficiency may introduce bias. We provide a bound on the Hellinger distance between the true posterior and its active subspace- exploiting approximation. And we demonstrate the active subspace-accelerated MCMC on two computational examples: (i) a two-dimensional parameter space with a quadratic forward model and one-dimensional active subspace and (ii) a 100-dimensional parameter space with a PDE-based forward model and a two-dimensional active subspace.

</details>

<details>

<summary>2016-07-03 11:19:39 - Asymptotic Bayesian Theory of Quickest Change Detection for Hidden Markov Models</summary>

- *Chen-Der Fuh, Alexander G. Tartakovsky*

- `1607.00624v1` - [abs](http://arxiv.org/abs/1607.00624v1) - [pdf](http://arxiv.org/pdf/1607.00624v1)

> In the 1960s, Shiryaev developed a Bayesian theory of change-point detection in the i.i.d. case, which was generalized in the beginning of the 2000s by Tartakovsky and Veeravalli for general stochastic models assuming a certain stability of the log-likelihood ratio process. Hidden Markov models represent a wide class of stochastic processes that are very useful in a variety of applications. In this paper, we investigate the performance of the Bayesian Shiryaev change-point detection rule for hidden Markov models. We propose a set of regularity conditions under which the Shiryaev procedure is first-order asymptotically optimal in a Bayesian context, minimizing moments of the detection delay up to certain order asymptotically as the probability of false alarm goes to zero. The developed theory for hidden Markov models is based on Markov chain representation for the likelihood ratio and r-quick convergence for Markov random walks. In addition, applying Markov nonlinear renewal theory, we present a high-order asymptotic approximation for the expected delay to detection of the Shiryaev detection rule. Asymptotic properties of another popular change detection rule, the Shiryaev{Roberts rule, is studied as well. Some interesting examples are given for illustration.

</details>

<details>

<summary>2016-07-05 09:22:57 - Bayesian dimensionality reduction with PCA using penalized semi-integrated likelihood</summary>

- *Piotr Sobczyk, Malgorzata Bogdan, Julie Josse*

- `1606.05333v2` - [abs](http://arxiv.org/abs/1606.05333v2) - [pdf](http://arxiv.org/pdf/1606.05333v2)

> We discuss the problem of estimating the number of principal components in Principal Com- ponents Analysis (PCA). Despite of the importance of the problem and the multitude of solutions proposed in the literature, it comes as a surprise that there does not exist a coherent asymptotic framework which would justify different approaches depending on the actual size of the data set. In this paper we address this issue by presenting an approximate Bayesian approach based on Laplace approximation and introducing a general method for building the model selection criteria, called PEnalized SEmi-integrated Likelihood (PESEL). Our general framework encompasses a variety of existing approaches based on probabilistic models, like e.g. Bayesian Information Criterion for the Probabilistic PCA (PPCA), and allows for construction of new criteria, depending on the size of the data set at hand. Specifically, we define PESEL when the number of variables substantially exceeds the number of observations. We also report results of extensive simulation studies and real data analysis, which illustrate good properties of our proposed criteria as compared to the state-of- the-art methods and very recent proposals. Specifially, these simulations show that PESEL based criteria can be quite robust against deviations from the probabilistic model assumptions. Selected PESEL based criteria for the estimation of the number of principal components are implemented in R package varclust, which is available on github (https://github.com/psobczyk/varclust).

</details>

<details>

<summary>2016-07-05 13:36:33 - Fast Gibbs sampling for high-dimensional Bayesian inversion</summary>

- *Felix Lucka*

- `1602.08595v2` - [abs](http://arxiv.org/abs/1602.08595v2) - [pdf](http://arxiv.org/pdf/1602.08595v2)

> Solving ill-posed inverse problems by Bayesian inference has recently attracted considerable attention. Compared to deterministic approaches, the probabilistic representation of the solution by the posterior distribution can be exploited to explore and quantify its uncertainties. In applications where the inverse solution is subject to further analysis procedures, this can be a significant advantage. Alongside theoretical progress, various new computational techniques allow to sample very high dimensional posterior distributions: In [Lucka2012], a Markov chain Monte Carlo (MCMC) posterior sampler was developed for linear inverse problems with $\ell_1$-type priors. In this article, we extend this single component Gibbs-type sampler to a wide range of priors used in Bayesian inversion, such as general $\ell_p^q$ priors with additional hard constraints. Besides a fast computation of the conditional, single component densities in an explicit, parameterized form, a fast, robust and exact sampling from these one-dimensional densities is key to obtain an efficient algorithm. We demonstrate that a generalization of slice sampling can utilize their specific structure for this task and illustrate the performance of the resulting slice-within-Gibbs samplers by different computed examples. These new samplers allow us to perform sample-based Bayesian inference in high-dimensional scenarios with certain priors for the first time, including the inversion of computed tomography (CT) data with the popular isotropic total variation (TV) prior.

</details>

<details>

<summary>2016-07-06 02:02:51 - A hybrid adaptive MCMC algorithm in function spaces</summary>

- *Qingping Zhou, Zixi Hu, Zhewei Yao, Jinglai Li*

- `1607.01458v1` - [abs](http://arxiv.org/abs/1607.01458v1) - [pdf](http://arxiv.org/pdf/1607.01458v1)

> The preconditioned Crank-Nicolson (pCN) method is a Markov Chain Monte Carlo (MCMC) scheme, specifically designed to perform Bayesian inferences in function spaces. Unlike many standard MCMC algorithms, the pCN method can preserve the sampling efficiency under the mesh refinement, a property referred to as being dimension independent. In this work we consider an adaptive strategy to further improve the efficiency of pCN. In particular we develop a hybrid adaptive MCMC method: the algorithm performs an adaptive Metropolis scheme in a chosen finite dimensional subspace, and a standard pCN algorithm in the complement space of the chosen subspace. We show that the proposed algorithm satisfies certain important ergodicity conditions. Finally with numerical examples we demonstrate that the proposed method has competitive performance with existing adaptive algorithms.

</details>

<details>

<summary>2016-07-06 02:34:21 - An optimal learning method for developing personalized treatment regimes</summary>

- *Yingfei Wang, Warren Powell*

- `1607.01462v1` - [abs](http://arxiv.org/abs/1607.01462v1) - [pdf](http://arxiv.org/pdf/1607.01462v1)

> A treatment regime is a function that maps individual patient information to a recommended treatment, hence explicitly incorporating the heterogeneity in need for treatment across individuals. Patient responses are dichotomous and can be predicted through an unknown relationship that depends on the patient information and the selected treatment. The goal is to find the treatments that lead to the best patient responses on average. Each experiment is expensive, forcing us to learn the most from each experiment. We adopt a Bayesian approach both to incorporate possible prior information and to update our treatment regime continuously as information accrues, with the potential to allow smaller yet more informative trials and for patients to receive better treatment. By formulating the problem as contextual bandits, we introduce a knowledge gradient policy to guide the treatment assignment by maximizing the expected value of information, for which an approximation method is used to overcome computational challenges. We provide a detailed study on how to make sequential medical decisions under uncertainty to reduce health care costs on a real world knee replacement dataset. We use clustering and LASSO to deal with the intrinsic sparsity in health datasets. We show experimentally that even though the problem is sparse, through careful selection of physicians (versus picking them at random), we can significantly improve the success rates.

</details>

<details>

<summary>2016-07-06 14:21:41 - Bayesian emulation for optimization in multi-step portfolio decisions</summary>

- *Kaoru Irie, Mike West*

- `1607.01631v1` - [abs](http://arxiv.org/abs/1607.01631v1) - [pdf](http://arxiv.org/pdf/1607.01631v1)

> We discuss the Bayesian emulation approach to computational solution of multi-step portfolio studies in financial time series. "Bayesian emulation for decisions" involves mapping the technical structure of a decision analysis problem to that of Bayesian inference in a purely synthetic "emulating" statistical model. This provides access to standard posterior analytic, simulation and optimization methods that yield indirect solutions of the decision problem. We develop this in time series portfolio analysis using classes of economically and psychologically relevant multi-step ahead portfolio utility functions. Studies with multivariate currency, commodity and stock index time series illustrate the approach and show some of the practical utility and benefits of the Bayesian emulation methodology.

</details>

<details>

<summary>2016-07-07 11:00:24 - Extending approximate Bayesian computation methods to high dimensions via a Gaussian copula model</summary>

- *Jingjing Li, David J. Nott, Yanan Fan, Scott A. Sisson*

- `1504.04093v2` - [abs](http://arxiv.org/abs/1504.04093v2) - [pdf](http://arxiv.org/pdf/1504.04093v2)

> Approximate Bayesian computation (ABC) refers to a family of inference methods used in the Bayesian analysis of complex models where evaluation of the likelihood is difficult. Conventional ABC methods often suffer from the curse of dimensionality, and a marginal adjustment strategy was recently introduced in the literature to improve the performance of ABC algorithms in high-dimensional problems. The marginal adjustment approach is extended using a Gaussian copula approximation. The method first estimates the bivariate posterior for each pair of parameters separately using a 2-dimensional Gaussian copula, and then combines these estimates together to estimate the joint posterior. The approximation works well in large sample settings when the posterior is approximately normal, but also works well in many cases which are far from that situation due to the nonparametric estimation of the marginal posterior distributions. If each bivariate posterior distribution can be well estimated with a low-dimensional ABC analysis then this Gaussian copula method can extend ABC methods to problems of high dimension. The method also results in an analytic expression for the approximate posterior which is useful for many purposes such as approximation of the likelihood itself. This method is illustrated with several examples.

</details>

<details>

<summary>2016-07-07 13:55:05 - meta4diag: Bayesian Bivariate Meta-analysis of Diagnostic Test Studies for Routine Practice</summary>

- *Jingyi Guo, Andrea Riebler*

- `1512.06220v2` - [abs](http://arxiv.org/abs/1512.06220v2) - [pdf](http://arxiv.org/pdf/1512.06220v2)

> This paper introduces the \proglang{R} package \pkg{meta4diag} for implementing Bayesian bivariate meta-analyses of diagnostic test studies. Our package \pkg{meta4diag} is a purpose-built front end of the \proglang{R} package \pkg{INLA}. While \pkg{INLA} offers full Bayesian inference for the large set of latent Gaussian models using integrated nested Laplace approximations, \pkg{meta4diag} extracts the features needed for bivariate meta-analysis and presents them in an intuitive way. It allows the user a straightforward model-specification and offers user-specific prior distributions. Further, the newly proposed penalised complexity prior framework is supported, which builds on prior intuitions about the behaviours of the variance and correlation parameters. Accurate posterior marginal distributions for sensitivity and specificity as well as all hyperparameters, and covariates are directly obtained without Markov chain Monte Carlo sampling. Further, univariate estimates of interest, such as odds ratios, as well as the SROC curve and other common graphics are directly available for interpretation. An interactive graphical user interface provides the user with the full functionality of the package without requiring any \proglang{R} programming. The package is available through CRAN \url{https://cran.r-project.org/web/packages/meta4diag/} and its usage will be illustrated using three real data examples.

</details>

<details>

<summary>2016-07-07 23:06:39 - Estimating an Inverse Gamma distribution</summary>

- *A. Llera, C. F. Beckmann*

- `1605.01019v2` - [abs](http://arxiv.org/abs/1605.01019v2) - [pdf](http://arxiv.org/pdf/1605.01019v2)

> In this paper we introduce five different algorithms based on method of moments, maximum likelihood and full Bayesian estimation for learning the parameters of the Inverse Gamma distribution. We also provide an expression for the KL divergence for Inverse Gamma distributions which allows us to quantify the estimation accuracy of each of the algorithms. All the presented algorithms are novel. The most relevant novelties include the first conjugate prior for the Inverse Gamma shape parameter which allows analytical Bayesian inference, and two very fast algorithms, a maximum likelihood and a Bayesian one, both based on likelihood approximation. In order to compute expectations under the proposed distributions we use the Laplace approximation. The introduction of these novel Bayesian estimators opens the possibility of including Inverse Gamma distributions into more complex Bayesian structures, e.g. variational Bayesian mixture models. The algorithms introduced in this paper are computationally compared using synthetic data and interesting relationships between the maximum likelihood and the Bayesian approaches are derived.

</details>

<details>

<summary>2016-07-08 23:06:41 - Bayesian inference for a covariance matrix</summary>

- *Ignacio Alvarez, Jarad Niemi, Matt Simpson*

- `1408.4050v2` - [abs](http://arxiv.org/abs/1408.4050v2) - [pdf](http://arxiv.org/pdf/1408.4050v2)

> Covariance matrix estimation arises in multivariate problems including multivariate normal sampling models and regression models where random effects are jointly modeled, e.g. random-intercept, random-slope models. A Bayesian analysis of these problems requires a prior on the covariance matrix. Here we assess, through a simulation study and a real data set, the impact this prior choice has on posterior inference of the covariance matrix.   Inverse Wishart distribution is the natural choice for a covariance matrix prior because its conjugacy on normal model and simplicity, is usually available in Bayesian statistical software. However inverse Wishart distribution presents some undesirable properties from a modeling point of view. It can be too restrictive because assume the same amount of prior information about every variance parameters and, more important, it shows a prior relationship between the variances and correlations.   Some alternatives distributions has been proposed. The scaled inverse Wishart distribution, which give more flexibility on the variance priors conserving the conjugacy property but does not eliminate the prior relationship between variances and correlations. Secondly, it is possible to fit separate priors for individual correlations and standard deviations. This strategy eliminates any prior relationship within the covariance matrix parameters, but it is not conjugate and therefore computationally slow.

</details>

<details>

<summary>2016-07-09 20:11:20 - Scalable Bayesian modeling, monitoring and analysis of dynamic network flow data</summary>

- *Xi Chen, Kaoru Irie, David Banks, Robert Haslinger, Jewell Thomas, Mike West*

- `1607.02655v1` - [abs](http://arxiv.org/abs/1607.02655v1) - [pdf](http://arxiv.org/pdf/1607.02655v1)

> Traffic flow count data in networks arise in many applications, such as automobile or aviation transportation, certain directed social network contexts, and Internet studies. Using an example of Internet browser traffic flow through site-segments of an international news website, we present Bayesian analyses of two linked classes of models which, in tandem, allow fast, scalable and interpretable Bayesian inference. We first develop flexible state-space models for streaming count data, able to adaptively characterize and quantify network dynamics efficiently in real-time. We then use these models as emulators of more structured, time-varying gravity models that allow formal dissection of network dynamics. This yields interpretable inferences on traffic flow characteristics, and on dynamics in interactions among network nodes. Bayesian monitoring theory defines a strategy for sequential model assessment and adaptation in cases when network flow data deviates from model-based predictions. Exploratory and sequential monitoring analyses of evolving traffic on a network of web site-segments in e-commerce demonstrate the utility of this coupled Bayesian emulation approach to analysis of streaming network count data.

</details>

<details>

<summary>2016-07-09 22:29:32 - Sparse additive Gaussian process with soft interactions</summary>

- *Garret Vo, Debdeep Pati*

- `1607.02670v1` - [abs](http://arxiv.org/abs/1607.02670v1) - [pdf](http://arxiv.org/pdf/1607.02670v1)

> Additive nonparametric regression models provide an attractive tool for variable selection in high dimensions when the relationship between the response and predictors is complex. They offer greater flexibility compared to parametric non-linear regression models and better interpretability and scalability than the non-parametric regression models. However, achieving sparsity simultaneously in the number of nonparametric components as well as in the variables within each nonparametric component poses a stiff computational challenge. In this article, we develop a novel Bayesian additive regression model using a combination of hard and soft shrinkages to separately control the number of additive components and the variables within each component. An efficient algorithm is developed to select the importance variables and estimate the interaction network. Excellent performance is obtained in simulated and real data examples.

</details>

<details>

<summary>2016-07-10 01:20:57 - Bayesian quantile additive regression trees</summary>

- *Bereket P. Kindo, Hao Wang, Timothy Hanson, Edsel A. PeÃ±a*

- `1607.02676v1` - [abs](http://arxiv.org/abs/1607.02676v1) - [pdf](http://arxiv.org/pdf/1607.02676v1)

> Ensemble of regression trees have become popular statistical tools for the estimation of conditional mean given a set of predictors. However, quantile regression trees and their ensembles have not yet garnered much attention despite the increasing popularity of the linear quantile regression model. This work proposes a Bayesian quantile additive regression trees model that shows very good predictive performance illustrated using simulation studies and real data applications. Further extension to tackle binary classification problems is also considered.

</details>

<details>

<summary>2016-07-11 12:33:48 - Maximum-a-posteriori estimation with Bayesian confidence regions</summary>

- *Marcelo Pereyra*

- `1602.08590v3` - [abs](http://arxiv.org/abs/1602.08590v3) - [pdf](http://arxiv.org/pdf/1602.08590v3)

> Solutions to inverse problems that are ill-conditioned or ill-posed may have significant intrinsic uncertainty. Unfortunately, analysing and quantifying this uncertainty is very challenging, particularly in high-dimensional problems. As a result, while most modern mathematical imaging methods produce impressive point estimation results, they are generally unable to quantify the uncertainty in the solutions delivered. This paper presents a new general methodology for approximating Bayesian high-posterior-density credibility regions in inverse problems that are convex and potentially very high-dimensional. The approximations are derived by using recent concentration of measure results related to information theory for log-concave random vectors. A remarkable property of the approximations is that they can be computed very efficiently, even in large-scale problems, by using standard convex optimisation techniques. In particular, they are available as a by-product in problems solved by maximum-a-posteriori estimation. The approximations also have favourable theoretical properties, namely they outer-bound the true high-posterior-density credibility regions, and they are stable with respect to model dimension. The proposed methodology is illustrated on two high-dimensional imaging inverse problems related to tomographic reconstruction and sparse deconvolution, where the approximations are used to perform Bayesian hypothesis tests and explore the uncertainty about the solutions, and where proximal Markov chain Monte Carlo algorithms are used as benchmark to compute exact credible regions and measure the approximation error.

</details>

<details>

<summary>2016-07-11 12:50:56 - Marginalized Bayesian filtering with Gaussian priors and posteriors</summary>

- *John-Olof Nilsson*

- `1603.06462v2` - [abs](http://arxiv.org/abs/1603.06462v2) - [pdf](http://arxiv.org/pdf/1603.06462v2)

> Marginalization techniques are presented for the Bayesian filtering problem under the assumption of Gaussian priors and posteriors and a set of sequentially more constraining state space model assumptions. The techniques provide the capabilities to 1) reduce the filtering problem to essential marginal moment integrals, 2) combine model and numerical approximations to the moment integrals, and 3) decouple modelling and system composition. Closed-form expressions of the posterior means and covariances are developed as functions of subspace projection matrices, subsystem models, and the marginal moment integrals. Finally, we review prior work and how the results relate to Kalman and marginalized particle filtering techniques.

</details>

<details>

<summary>2016-07-11 15:11:27 - Bayesian variable selection in high dimensional problems without assumptions on prior model probabilities</summary>

- *James O. Berger, Gonzalo Garcia-Donato, Miguel A. Martinez-Beneito, Victor PeÃ±a*

- `1607.02993v1` - [abs](http://arxiv.org/abs/1607.02993v1) - [pdf](http://arxiv.org/pdf/1607.02993v1)

> We consider the problem of variable selection in linear models when $p$, the number of potential regressors, may exceed (and perhaps substantially) the sample size $n$ (which is possibly small).

</details>

<details>

<summary>2016-07-11 15:26:28 - Bayesian isochrone fitting and stellar ages</summary>

- *D. Valls-Gabaud*

- `1607.03000v1` - [abs](http://arxiv.org/abs/1607.03000v1) - [pdf](http://arxiv.org/pdf/1607.03000v1)

> Stellar evolution theory has been extraordinarily successful at explaining the different phases under which stars form, evolve and die. While the strongest constraints have traditionally come from binary stars, the advent of asteroseismology is bringing unique measures in well-characterised stars. For stellar populations in general, however, only photometric measures are usually available, and the comparison with the predictions of stellar evolution theory have mostly been qualitative. For instance, the geometrical shapes of isochrones have been used to infer ages of coeval populations, but without any proper statistical basis. In this chapter we provide a pedagogical review on a Bayesian formalism to make quantitative inferences on the properties of single, binary and small ensembles of stars, including unresolved populations. As an example, we show how stellar evolution theory can be used in a rigorous way as a prior information to measure the ages of stars between the ZAMS and the Helium flash, and their uncertainties, using photometric data only.

</details>

<details>

<summary>2016-07-11 16:04:53 - Bayesian estimation of discretely observed multi-dimensional diffusion processes using guided proposals</summary>

- *Frank van der Meulen, Moritz Schauer*

- `1406.4704v3` - [abs](http://arxiv.org/abs/1406.4704v3) - [pdf](http://arxiv.org/pdf/1406.4704v3)

> Estimation of parameters of a diffusion based on discrete time observations poses a difficult problem due to the lack of a closed form expression for the likelihood. From a Bayesian computational perspective it can be casted as a missing data problem where the diffusion bridges in between discrete-time observations are missing. The computational problem can then be dealt with using a Markov-chain Monte-Carlo method known as data-augmentation. If unknown parameters appear in the diffusion coefficient, direct implementation of data-augmentation results in a Markov chain that is reducible. Furthermore, data-augmentation requires efficient sampling of diffusion bridges, which can be difficult, especially in the multidimensional case.   We present a general framework to deal with with these problems that does not rely on discretisation. The construction generalises previous approaches and sheds light on the assumptions necessary to make these approaches work. We define a random-walk type Metropolis-Hastings sampler for updating diffusion bridges. Our methods are illustrated using guided proposals for sampling diffusion bridges. These are Markov processes obtained by adding a guiding term to the drift of the diffusion. We give general guidelines on the construction of these proposals and introduce a time change and scaling of the guided proposal that reduces discretisation error. Numerical examples demonstrate the performance of our methods.

</details>

<details>

<summary>2016-07-11 19:19:57 - Approximate Bayesian computation (ABC) coupled with Bayesian model averaging method for estimating mean and standard deviation</summary>

- *Deukwoo Kwon, Isildinha M. Reis*

- `1607.03080v1` - [abs](http://arxiv.org/abs/1607.03080v1) - [pdf](http://arxiv.org/pdf/1607.03080v1)

> Background: We proposed approximate Bayesian computation with single distribution selection (ABC-SD) for estimating mean and standard deviation from other reported summary statistics. The ABC-SD generates pseudo data from a single parametric distribution thought to be the true distribution of underlying study data. This single distribution is either an educated guess, or it is selected via model selection using posterior probability criterion for testing two or more candidate distributions. Further analysis indicated that when model selection is used, posterior model probabilities are sensitive to the prior distribution(s) for parameter(s) and dependable on the type of reported summary statistics. Method: We propose ABC with Bayesian model averaging (ABC-BMA) methodology to estimate mean and standard deviation based on various sets of other summary statistics reported in published studies. We conduct a Monte Carlo simulation study to compare the new proposed ABC-BMA method with our previous ABC-SD method. Results: In the estimation of standard deviation, ABC-BMA has smaller average relative errors (AREs) than that of ABC-SD for normal, lognormal, beta, and exponential distributions. For Weibull distribution, ARE of ABC-BMA is larger than that of ABC-SD but <0.05 in small sample sizes and moves toward zero as sample size increases. When underlying distribution is highly skewed and available summary statistics are only quartiles and sample size, ABC-BMA is recommended but it should be used with caution. Comparison of mean estimation between ABC-BMA and ABC-SD shows similar patterns of results as for standard deviation estimation. Conclusion: ABC-BMA is easy to implement and it performs even better than our previous ABC-SD method for estimation of mean and standard deviation.

</details>

<details>

<summary>2016-07-11 23:09:52 - Multi-Step Bayesian Optimization for One-Dimensional Feasibility Determination</summary>

- *J. Massey Cashore, Lemuel Kumarga, Peter I. Frazier*

- `1607.03195v1` - [abs](http://arxiv.org/abs/1607.03195v1) - [pdf](http://arxiv.org/pdf/1607.03195v1)

> Bayesian optimization methods allocate limited sampling budgets to maximize expensive-to-evaluate functions. One-step-lookahead policies are often used, but computing optimal multi-step-lookahead policies remains a challenge. We consider a specialized Bayesian optimization problem: finding the superlevel set of an expensive one-dimensional function, with a Markov process prior. We compute the Bayes-optimal sampling policy efficiently, and characterize the suboptimality of one-step lookahead. Our numerical experiments demonstrate that the one-step lookahead policy is close to optimal in this problem, performing within 98% of optimal in the experimental settings considered.

</details>

<details>

<summary>2016-07-12 10:38:55 - Bayesian estimators of the Gamma distribution</summary>

- *A. Llera, C. F. Beckmann*

- `1607.03302v1` - [abs](http://arxiv.org/abs/1607.03302v1) - [pdf](http://arxiv.org/pdf/1607.03302v1)

> In this paper we introduce two Bayesian estimators for learning the parameters of the Gamma distribution. The first algorithm uses a well known unnormalized conjugate prior for the Gamma shape and the second one uses a non-linear approximation to the likelihood and a prior on the shape that is conjugate to the approximated likelihood. In both cases use the Laplace approximation to compute the required expectations. We perform a theoretical comparison between maximum like- lihood and the presented Bayesian algorithms that allow us to provide non-informative parameter values for the priors hyper parameters. We also provide a numerical comparison using synthetic data. The introduction of these novel Bayesian estimators open the possibility of including Gamma distributions into more complex Bayesian structures, e.g. variational Bayesian mixture models.

</details>

<details>

<summary>2016-07-12 20:28:38 - Understanding the Impact of Microcredit Expansions: A Bayesian Hierarchical Analysis of 7 Randomised Experiments</summary>

- *Rachael Meager*

- `1506.06669v3` - [abs](http://arxiv.org/abs/1506.06669v3) - [pdf](http://arxiv.org/pdf/1506.06669v3)

> Bayesian hierarchical models are a methodology for aggregation and synthesis of data from heterogeneous settings, used widely in statistics and other disciplines. I apply this framework to the evidence from 7 randomized experiments of expanding access to microcredit to assess the general impact of the intervention on household outcomes and the heterogeneity in this impact across sites. The results suggest that the effect of microcredit is likely to be positive but small relative to control group average levels, and the possibility of a negative impact cannot be ruled out. By contrast, common meta-analytic methods that pool all the data without assessing the heterogeneity misleadingly produce "statistically significant" results in 2 of the 6 household outcomes. Standard pooling metrics for the studies indicate on average 60% pooling on the treatment effects, suggesting that the site-specific effects are reasonably externally valid, and thus informative for each other and for the general case. The cross-study heterogeneity is almost entirely generated by heterogeneous effects for the 27% households who previously operated businesses before microcredit expansion, although this group is likely to see much larger impacts overall. A Ridge regression procedure to assess the correlations between site-specific covariates and treatment effects indicates that the remaining heterogeneity is strongly correlated with differences in economic variables, but not with differences in study design protocols. The average interest rate and the average loan size have the strongest correlation with the treatment effects, and both are negative.

</details>

<details>

<summary>2016-07-12 22:37:01 - A Flexible Bayesian Model for Estimating Subnational Mortality</summary>

- *Monica Alexander, Emilio Zagheni, Magali Barbieri*

- `1607.03534v1` - [abs](http://arxiv.org/abs/1607.03534v1) - [pdf](http://arxiv.org/pdf/1607.03534v1)

> Reliable mortality estimates at the subnational level are essential in the study of health inequalities within a country. One of the difficulties in producing such estimates is the presence of small populations, where the stochastic variation in death counts is relatively high, and so the underlying mortality levels are unclear. We present a Bayesian hierarchical model to estimate mortality at the subnational level. The model builds on characteristic age patterns in mortality curves, which are constructed using principal components from a set of reference mortality curves. Information on mortality rates are pooled across geographic space and smoothed over time. Testing of the model shows reasonable estimates and uncertainty levels when the model is applied to both simulated data which mimic US counties, and real data for French departments. The estimates produced by the model have direct applications to the study of subregional health patterns and disparities.

</details>

<details>

<summary>2016-07-13 18:07:42 - Bayesian mixture modeling for multivariate conditional distributions</summary>

- *Maria DeYoreo, Jerome P. Reiter*

- `1606.04457v3` - [abs](http://arxiv.org/abs/1606.04457v3) - [pdf](http://arxiv.org/pdf/1606.04457v3)

> We present a Bayesian mixture model for estimating the joint distribution of mixed ordinal, nominal, and continuous data conditional on a set of fixed variables. The model uses multivariate normal and categorical mixture kernels for the random variables. It induces dependence between the random and fixed variables through the means of the multivariate normal mixture kernels and via a truncated local Dirichlet process. The latter encourages observations with similar values of the fixed variables to share mixture components. Using a simulation of data fusion, we illustrate that the model can estimate underlying relationships in the data and the distributions of the missing values more accurately than a mixture model applied to the random and fixed variables jointly. We use the model to analyze consumers' reading behaviors using a quota sample, i.e., a sample where the empirical distribution of some variables is fixed by design and so should not be modeled as random, conducted by the book publisher HarperCollins.

</details>

<details>

<summary>2016-07-13 21:01:52 - Optimally-Weighted Herding is Bayesian Quadrature</summary>

- *Ferenc Huszar, David Duvenaud*

- `1408.2049v2` - [abs](http://arxiv.org/abs/1408.2049v2) - [pdf](http://arxiv.org/pdf/1408.2049v2)

> Herding and kernel herding are deterministic methods of choosing samples which summarise a probability distribution. A related task is choosing samples for estimating integrals using Bayesian quadrature. We show that the criterion minimised when selecting samples in kernel herding is equivalent to the posterior variance in Bayesian quadrature. We then show that sequential Bayesian quadrature can be viewed as a weighted version of kernel herding which achieves performance superior to any other weighted herding method. We demonstrate empirically a rate of convergence faster than O(1/N). Our results also imply an upper bound on the empirical error of the Bayesian quadrature estimate.

</details>

<details>

<summary>2016-07-14 17:12:28 - Semi-Parametric Survival Estimation for pedigrees</summary>

- *Flora Alarcon, Gregory Nuel, Violaine Plante-Bordeneuve*

- `1607.04215v1` - [abs](http://arxiv.org/abs/1607.04215v1) - [pdf](http://arxiv.org/pdf/1607.04215v1)

> Mendelian diseases are determined by a single mutation in a given gene. However, in the case of diseases with late onset, the age at onset is variable; it can even be the case that the onset is not observed in a lifetime. Estimating the survival function of the mutation carriers and the effect of modifying factors such as the sex, mutation, origin, etc, is a task of importance, both for management of mutation carriers and for prevention. In this work, we present a semi-parametric method based on a proportional to estimate the survival function using pedigrees ascertained through affected individuals (probands). Not all members of the pedigree need to be genotyped. The ascertainment bias is corrected by using only the phenotypic information from the relatives of the proband, and not of the proband himself. The method manage ungenotyped individuals through belief propagation in Bayesian networks and uses an EM algorithm to compute a Kaplan-Meier estimator of the survival function. The method is illustrated on simulated data and on a samples of families with transthyretin-related hereditary amyloidosis, a rare autosomal dominant disease with highly variable age of onset.

</details>

<details>

<summary>2016-07-15 18:39:09 - Optimally-Weighted Herding is Bayesian Quadrature</summary>

- *Ferenc HuszÃ¡r, David Duvenaud*

- `1204.1664v3` - [abs](http://arxiv.org/abs/1204.1664v3) - [pdf](http://arxiv.org/pdf/1204.1664v3)

> Herding and kernel herding are deterministic methods of choosing samples which summarise a probability distribution. A related task is choosing samples for estimating integrals using Bayesian quadrature. We show that the criterion minimised when selecting samples in kernel herding is equivalent to the posterior variance in Bayesian quadrature. We then show that sequential Bayesian quadrature can be viewed as a weighted version of kernel herding which achieves performance superior to any other weighted herding method. We demonstrate empirically a rate of convergence faster than O(1/N). Our results also imply an upper bound on the empirical error of the Bayesian quadrature estimate.

</details>

<details>

<summary>2016-07-16 03:37:01 - Dynamic Sum Product Networks for Tractable Inference on Sequence Data (Extended Version)</summary>

- *Mazen Melibari, Pascal Poupart, Prashant Doshi, George Trimponias*

- `1511.04412v2` - [abs](http://arxiv.org/abs/1511.04412v2) - [pdf](http://arxiv.org/pdf/1511.04412v2)

> Sum-Product Networks (SPN) have recently emerged as a new class of tractable probabilistic graphical models. Unlike Bayesian networks and Markov networks where inference may be exponential in the size of the network, inference in SPNs is in time linear in the size of the network. Since SPNs represent distributions over a fixed set of variables only, we propose dynamic sum product networks (DSPNs) as a generalization of SPNs for sequence data of varying length. A DSPN consists of a template network that is repeated as many times as needed to model data sequences of any length. We present a local search technique to learn the structure of the template network. In contrast to dynamic Bayesian networks for which inference is generally exponential in the number of variables per time slice, DSPNs inherit the linear inference complexity of SPNs. We demonstrate the advantages of DSPNs over DBNs and other models on several datasets of sequence data.

</details>

<details>

<summary>2016-07-16 19:23:59 - Objective Bayesian modelling of insurance risks with the skewed Student-t distribution</summary>

- *Fabrizio Leisen, Juan Miguel Marin, Cristiano Villa*

- `1607.04796v1` - [abs](http://arxiv.org/abs/1607.04796v1) - [pdf](http://arxiv.org/pdf/1607.04796v1)

> Insurance risks data typically exhibit skewed behaviour. In this paper, we propose a Bayesian approach to capture the main features of these datasets. This work extends the methodology introduced in Villa and Walker (2014a) by considering an extra parameter which captures the skewness of the data. In particular, a skewed Student-t distribution is considered. Two datasets are analysed: the Danish fire losses and the US indemnity loss. The analysis is carried with an objective Bayesian approach. For the discrete parameter representing the number of the degrees of freedom, we adopt a novel prior recently introduced in Villa and Walker (2014b).

</details>

<details>

<summary>2016-07-18 19:27:28 - A Decision-Theoretic Comparison of Treatments to Resolve Air Leaks After Lung Surgery Based on Nonparametric Modeling</summary>

- *Yanxun Xu, Peter F. Thall, Peter Mueller, Mehran J. Reza*

- `1506.07687v2` - [abs](http://arxiv.org/abs/1506.07687v2) - [pdf](http://arxiv.org/pdf/1506.07687v2)

> We propose a Bayesian nonparametric utility-based group sequential design for a randomized clinical trial to compare a gel sealant to standard care for resolving air leaks after pulmonary resection. Clinically, resolving air leaks in the days soon after surgery is highly important, since longer resolution time produces undesirable complications that require extended hospitalization. The problem of comparing treatments is complicated by the fact that the resolution time distributions are skewed and multi-modal, so using means is misleading. We address these challenges by assuming Bayesian nonparametric probability models for the resolution time distributions and basing the comparative test on weighted means. The weights are elicited as clinical utilities of the resolution times. The proposed design uses posterior expected utilities as group sequential test criteria. The procedure's frequentist properties are studied by extensive simulations.

</details>

<details>

<summary>2016-07-18 21:01:56 - Hyperspectral Unmixing in Presence of Endmember Variability, Nonlinearity or Mismodelling Effects</summary>

- *Abderrahim Halimi, Paul Honeine, Jose Bioucas-Dias*

- `1511.05698v2` - [abs](http://arxiv.org/abs/1511.05698v2) - [pdf](http://arxiv.org/pdf/1511.05698v2)

> This paper presents three hyperspectral mixture models jointly with Bayesian algorithms for supervised hyperspectral unmixing. Based on the residual component analysis model, the proposed general formulation assumes the linear model to be corrupted by an additive term whose expression can be adapted to account for nonlinearities (NL), endmember variability (EV), or mismodelling effects (ME). The NL effect is introduced by considering a polynomial expression that is related to bilinear models. The proposed new formulation of EV accounts for shape and scale endmember changes while enforcing a smooth spectral/spatial variation. The ME formulation takes into account the effect of outliers and copes with some types of EV and NL. The known constraints on the parameter of each observation model are modeled via suitable priors. The posterior distribution associated with each Bayesian model is optimized using a coordinate descent algorithm which allows the computation of the maximum a posteriori estimator of the unknown model parameters. The proposed mixture and Bayesian models and their estimation algorithms are validated on both synthetic and real images showing competitive results regarding the quality of the inferences and the computational complexity when compared to the state-of-the-art algorithms.

</details>

<details>

<summary>2016-07-19 20:54:51 - Independent Resampling Sequential Monte Carlo Algorithms</summary>

- *Roland Lamberti, Yohan Petetin, FranÃ§ois Desbouvries, FranÃ§ois Septier*

- `1607.05758v1` - [abs](http://arxiv.org/abs/1607.05758v1) - [pdf](http://arxiv.org/pdf/1607.05758v1)

> Sequential Monte Carlo algorithms, or Particle Filters, are Bayesian filtering algorithms which propagate in time a discrete and random approximation of the a posteriori distribution of interest. Such algorithms are based on Importance Sampling with a bootstrap resampling step which aims at struggling against weights degeneracy. However, in some situations (informative measurements, high dimensional model), the resampling step can prove inefficient. In this paper, we revisit the fundamental resampling mechanism which leads us back to Rubin's static resampling mechanism. We propose an alternative rejuvenation scheme in which the resampled particles share the same marginal distribution as in the classical setup, but are now independent. This set of independent particles provides a new alternative to compute a moment of the target distribution and the resulting estimate is analyzed through a CLT. We next adapt our results to the dynamic case and propose a particle filtering algorithm based on independent resampling. This algorithm can be seen as a particular auxiliary particle filter algorithm with a relevant choice of the first-stage weights and instrumental distributions. Finally we validate our results via simulations which carefully take into account the computational budget.

</details>

<details>

<summary>2016-07-20 14:51:18 - Fitting logistic multilevel models with crossed random effects via Bayesian Integrated Nested Laplace Approximations: a simulation study</summary>

- *Leonardo Grilli, Francesco Innocenti*

- `1607.05981v1` - [abs](http://arxiv.org/abs/1607.05981v1) - [pdf](http://arxiv.org/pdf/1607.05981v1)

> Fitting cross-classified multilevel models with binary response is challenging. In this setting a promising method is Bayesian inference through Integrated Nested Laplace Approximations (INLA), which performs well in several latent variable models. Therefore we devise a systematic simulation study to assess the performance of INLA with cross-classified logistic data under different scenarios defined by the magnitude of the random effects variances, the number of observations, the number of clusters, and the degree of cross-classification. In the simulations INLA is systematically compared with the popular method of Maximum Likelihood via Laplace Approximation. By an application to the classical salamander mating data, we compare INLA with the best performing methods. Given the computational speed and the generally good performance, INLA turns out to be a valuable method for fitting the considered cross-classified models.

</details>

<details>

<summary>2016-07-20 16:47:46 - An Empirical Study of Customer Spillover Learning about Service Quality</summary>

- *AndrÃ©s Musalem, Yan Shang, Jing-Sheng Song*

- `1607.06020v1` - [abs](http://arxiv.org/abs/1607.06020v1) - [pdf](http://arxiv.org/pdf/1607.06020v1)

> "Spillover" learning is defined as customers' learning about the quality of a service (or product) from their previous experiences with similar yet not identical services. In this paper, we propose a novel, parsimonious and general Bayesian hierarchical learning framework for estimating customers' spillover learning. We apply our model to a one-year shipping/sales historical data provided by a world-leading third party logistics company and study how customers' experiences from shipping on a particular route affect their future decisions about shipping not only on that route, but also on other routes serviced by the same logistics company. Our empirical results are consistent with information spillovers driving customer choices. Customers also display an asymmetric response such that they are more sensitive to delays than early deliveries. In addition, we find that customers are risk averse being more sensitive to their uncertainty about the mean service quality than to the intrinsic variability of the service. Finally, we develop policy simulation studies to show the importance of accounting for customer learning when a firm considers service quality improvement decisions.

</details>

<details>

<summary>2016-07-21 04:31:38 - Exploiting Big Data in Logistics Risk Assessment via Bayesian Nonparametrics</summary>

- *Yan Shang, David B. Dunson, Jing-Sheng Song*

- `1501.05349v2` - [abs](http://arxiv.org/abs/1501.05349v2) - [pdf](http://arxiv.org/pdf/1501.05349v2)

> In cargo logistics, a key performance measure is transport risk, defined as the deviation of the actual arrival time from the planned arrival time. Neither earliness nor tardiness is desirable for customer and freight forwarders. In this paper, we investigate ways to assess and forecast transport risks using a half-year of air cargo data, provided by a leading forwarder on 1336 routes served by 20 airlines. Interestingly, our preliminary data analysis shows a strong multimodal feature in the transport risks, driven by unobserved events, such as cargo missing flights. To accommodate this feature, we introduce a Bayesian nonparametric model -- the probit stick-breaking process (PSBP) mixture model -- for flexible estimation of the conditional (i.e., state-dependent) density function of transport risk. We demonstrate that using simpler methods, such as OLS linear regression, can lead to misleading inferences. Our model provides a tool for the forwarder to offer customized price and service quotes. It can also generate baseline airline performance to enable fair supplier evaluation. Furthermore, the method allows us to separate recurrent risks from disruption risks. This is important, because hedging strategies for these two kinds of risks are often drastically different.

</details>

<details>

<summary>2016-07-21 17:52:08 - Small-Variance Nonparametric Clustering on the Hypersphere</summary>

- *Julian Straub, Trevor Campbell, Jonathan P. How, John W. Fisher III*

- `1607.06407v1` - [abs](http://arxiv.org/abs/1607.06407v1) - [pdf](http://arxiv.org/pdf/1607.06407v1)

> Structural regularities in man-made environments reflect in the distribution of their surface normals. Describing these surface normal distributions is important in many computer vision applications, such as scene understanding, plane segmentation, and regularization of 3D reconstructions. Based on the small-variance limit of Bayesian nonparametric von-Mises-Fisher (vMF) mixture distributions, we propose two new flexible and efficient k-means-like clustering algorithms for directional data such as surface normals. The first, DP-vMF-means, is a batch clustering algorithm derived from the Dirichlet process (DP) vMF mixture. Recognizing the sequential nature of data collection in many applications, we extend this algorithm to DDP-vMF-means, which infers temporally evolving cluster structure from streaming data. Both algorithms naturally respect the geometry of directional data, which lies on the unit sphere. We demonstrate their performance on synthetic directional data and real 3D surface normals from RGB-D sensors. While our experiments focus on 3D data, both algorithms generalize to high dimensional directional data such as protein backbone configurations and semantic word vectors.

</details>

<details>

<summary>2016-07-21 20:27:33 - Bayesian Mixture Models With Focused Clustering for Mixed Ordinal and Nominal Data</summary>

- *Maria DeYoreo, Jerome P. Reiter, D. Sunshine Hillygus*

- `1508.03758v2` - [abs](http://arxiv.org/abs/1508.03758v2) - [pdf](http://arxiv.org/pdf/1508.03758v2)

> In some contexts, mixture models can fit certain variables well at the expense of others in ways beyond the analyst's control. For example, when the data include some variables with non-trivial amounts of missing values, the mixture model may fit the marginal distributions of the nearly and fully complete variables at the expense of the variables with high fractions of missing data. Motivated by this setting, we present a mixture model for mixed ordinal and nominal data that splits variables into two groups, focus variables and remainder variables. The model allows the analyst to specify a rich sub-model for the focus variables and a simpler sub-model for remainder variables, yet still capture associations among the variables. Using simulations, we illustrate advantages and limitations of focused clustering compared to mixture models that do not distinguish variables. We apply the model to handle missing values in an analysis of the 2012 American National Election Study, estimating relationships among voting behavior, ideology, and political party affiliation.

</details>

<details>

<summary>2016-07-21 20:27:50 - Multimodal, high-dimensional, model-based, Bayesian inverse problems with applications in biomechanics</summary>

- *Isabell M. Franck, P. S. Koutsourelakis*

- `1512.04481v3` - [abs](http://arxiv.org/abs/1512.04481v3) - [pdf](http://arxiv.org/pdf/1512.04481v3)

> This paper is concerned with the numerical solution of model-based, Bayesian inverse problems. We are particularly interested in cases where the cost of each likelihood evaluation (forward-model call) is expensive and the number of un- known (latent) variables is high. This is the setting in many problems in com- putational physics where forward models with nonlinear PDEs are used and the parameters to be calibrated involve spatio-temporarily varying coefficients, which upon discretization give rise to a high-dimensional vector of unknowns. One of the consequences of the well-documented ill-posedness of inverse prob- lems is the possibility of multiple solutions. While such information is contained in the posterior density in Bayesian formulations, the discovery of a single mode, let alone multiple, is a formidable task. The goal of the present paper is two- fold. On one hand, we propose approximate, adaptive inference strategies using mixture densities to capture multi-modal posteriors, and on the other, to ex- tend our work in [1] with regards to effective dimensionality reduction techniques that reveal low-dimensional subspaces where the posterior variance is mostly concentrated. We validate the model proposed by employing Importance Sam- pling which confirms that the bias introduced is small and can be efficiently corrected if the analyst wishes to do so. We demonstrate the performance of the proposed strategy in nonlinear elastography where the identification of the mechanical properties of biological materials can inform non-invasive, medical di- agnosis. The discovery of multiple modes (solutions) in such problems is critical in achieving the diagnostic objectives.

</details>

<details>

<summary>2016-07-22 09:48:25 - Latent Variable Discovery Using Dependency Patterns</summary>

- *Xuhui Zhang, Kevin B. Korb, Ann E. Nicholson, Steven Mascaro*

- `1607.06617v1` - [abs](http://arxiv.org/abs/1607.06617v1) - [pdf](http://arxiv.org/pdf/1607.06617v1)

> The causal discovery of Bayesian networks is an active and important research area, and it is based upon searching the space of causal models for those which can best explain a pattern of probabilistic dependencies shown in the data. However, some of those dependencies are generated by causal structures involving variables which have not been measured, i.e., latent variables. Some such patterns of dependency "reveal" themselves, in that no model based solely upon the observed variables can explain them as well as a model using a latent variable. That is what latent variable discovery is based upon. Here we did a search for finding them systematically, so that they may be applied in latent variable discovery in a more rigorous fashion.

</details>

<details>

<summary>2016-07-22 21:51:22 - Reciprocal Graphical Models for Integrative Gene Regulatory Network Analysis</summary>

- *Yang Ni, Yuan Ji, Peter Mueller*

- `1607.06849v1` - [abs](http://arxiv.org/abs/1607.06849v1) - [pdf](http://arxiv.org/pdf/1607.06849v1)

> Constructing gene regulatory networks is a fundamental task in systems biology. We introduce a Gaussian reciprocal graphical model for inference about gene regulatory relationships by integrating mRNA gene expression and DNA level information including copy number and methylation. Data integration allows for inference on the directionality of certain regulatory relationships, which would be otherwise indistinguishable due to Markov equivalence. Efficient inference is developed based on simultaneous equation models. Bayesian model selection techniques are adopted to estimate the graph structure. We illustrate our approach by simulations and two applications in ZODIAC pairwise gene interaction analysis and colon adenocarcinoma pathway analysis.

</details>

<details>

<summary>2016-07-24 01:07:30 - Importance sampling squared for Bayesian inference in latent variable models</summary>

- *Minh-Ngoc Tran, Marcel Scharth, Michael K. Pitt, Robert Kohn*

- `1309.3339v4` - [abs](http://arxiv.org/abs/1309.3339v4) - [pdf](http://arxiv.org/pdf/1309.3339v4)

> We consider Bayesian inference by importance sampling when the likelihood is analytically intractable but can be unbiasedly estimated. We refer to this procedure as importance sampling squared (IS2), as we can often estimate the likelihood itself by importance sampling. We provide a formal justification for importance sampling when working with an estimate of the likelihood and study its convergence properties. We analyze the effect of estimating the likelihood on the resulting inference and provide guidelines on how to set up the precision of the likelihood estimate in order to obtain an optimal tradeoff? between computational cost and accuracy for posterior inference on the model parameters. We illustrate the procedure in empirical applications for a generalized multinomial logit model and a stochastic volatility model. The results show that the IS2 method can lead to fast and accurate posterior inference under the optimal implementation.

</details>

<details>

<summary>2016-07-25 11:06:12 - Laplace based approximate posterior inference for differential equation models</summary>

- *Sarat C. Dass, Jaeyong Lee, Kyoungjae Lee, Jonghun Park*

- `1607.07203v1` - [abs](http://arxiv.org/abs/1607.07203v1) - [pdf](http://arxiv.org/pdf/1607.07203v1)

> Ordinary differential equations are arguably the most popular and useful mathematical tool for describing physical and biological processes in the real world. Often, these physical and biological processes are observed with errors, in which case the most natural way to model such data is via regression where the mean function is defined by an ordinary differential equation believed to provide an understanding of the underlying process. These regression based dynamical models are called differential equation models. Parameter inference from differential equation models poses computational challenges mainly due to the fact that analytic solutions to most differential equations are not available. In this paper, we propose an approximation method for obtaining the posterior distribution of parameters in differential equation models. The approximation is done in two steps. In the first step, the solution of a differential equation is approximated by the general one-step method which is a class of numerical methods for ordinary differential equations including the Euler and the Runge-Kutta procedures; in the second step, nuisance parameters are marginalized using Laplace approximation. The proposed Laplace approximated posterior gives a computationally fast alternative to the full Bayesian computational scheme (such as Markov Chain Monte Carlo) and produces more accurate and stable estimators than the popular smoothing methods (called collocation methods) based on frequentist procedures. For a theoretical support of the proposed method, we prove that the Laplace approximated posterior converges to the actual posterior under certain conditions and analyze the relation between the order of numerical error and its Laplace approximation. The proposed method is tested on simulated data sets and compared with the other existing methods.

</details>

<details>

<summary>2016-07-26 07:47:02 - Variational Mixture Models with Gamma or inverse-Gamma components</summary>

- *A. Llera, D. Vidaurre, R. H. R. Pruim, C. F. Beckmann*

- `1607.07573v1` - [abs](http://arxiv.org/abs/1607.07573v1) - [pdf](http://arxiv.org/pdf/1607.07573v1)

> Mixture models with Gamma and or inverse-Gamma distributed mixture components are useful for medical image tissue segmentation or as post-hoc models for regression coefficients obtained from linear regression within a Generalised Linear Modeling framework (GLM), used in this case to separate stochastic (Gaussian) noise from some kind of positive or negative "activation" (modeled as Gamma or inverse-Gamma distributed). To date, the most common choice in this context it is Gaussian/Gamma mixture models learned through a maximum likelihood (ML) approach; we recently extended such algorithm for mixture models with inverse-Gamma components. Here, we introduce a fully analytical Variational Bayes (VB) learning framework for both Gamma and/or inverse-Gamma components. We use synthetic and resting state fMRI data to compare the performance of the ML and VB algorithms in terms of area under the curve and computational cost. We observed that the ML Gaussian/Gamma model is very expensive specially when considering high resolution images; furthermore, these solutions are highly variable and they occasionally can overestimate the activations severely. The Bayesian Gauss-Gamma is in general the fastest algorithm but provides too dense solutions. The maximum likelihood Gaussian/inverse-Gamma is also very fast but provides in general very sparse solutions. The variational Gaussian/inverse-Gamma mixture model is the most robust and its cost is acceptable even for high resolution images. Further, the presented methodology represents an essential building block that can be directly used in more complex inference tasks, specially designed to analyse MRI-fMRI data; such models include for example analytical variational mixture models with adaptive spatial regularization or better source models for new spatial blind source separation approaches.

</details>

<details>

<summary>2016-07-26 10:29:33 - A New PAC-Bayesian Perspective on Domain Adaptation</summary>

- *Pascal Germain, Amaury Habrard, FranÃ§ois Laviolette, Emilie Morvant*

- `1506.04573v4` - [abs](http://arxiv.org/abs/1506.04573v4) - [pdf](http://arxiv.org/pdf/1506.04573v4)

> We study the issue of PAC-Bayesian domain adaptation: We want to learn, from a source domain, a majority vote model dedicated to a target one. Our theoretical contribution brings a new perspective by deriving an upper-bound on the target risk where the distributions' divergence---expressed as a ratio---controls the trade-off between a source error measure and the target voters' disagreement. Our bound suggests that one has to focus on regions where the source data is informative.From this result, we derive a PAC-Bayesian generalization bound, and specialize it to linear classifiers. Then, we infer a learning algorithmand perform experiments on real data.

</details>

<details>

<summary>2016-07-26 12:21:25 - Bayesian spatial transformation models with applications in neuroimaging data</summary>

- *Michelle F. Miranda, Hongtu Zhu, Joseph G. Ibrahim*

- `1607.07664v1` - [abs](http://arxiv.org/abs/1607.07664v1) - [pdf](http://arxiv.org/pdf/1607.07664v1)

> The aim of this paper is to develop a class of spatial transformation models (STM) to spatially model the varying association between imaging measures in a three-dimensional (3D) volume (or 2D surface) and a set of covariates. Our STMs include a varying Box-Cox transformation model for dealing with the issue of non-Gaussian distributed imaging data and a Gaussian Markov Random Field model for incorporating spatial smoothness of the imaging data. Posterior computation proceeds via an efficient Markov chain Monte Carlo algorithm. Simulations and real data analysis demonstrate that the STM significantly outperforms the voxel-wise linear model with Gaussian noise in recovering meaningful geometric patterns. Our STM is able to reveal important brain regions with morphological changes in children with attention deficit hyperactivity disorder.

</details>

<details>

<summary>2016-07-27 15:17:52 - Bayesian Nonparametric Modelling of Joint Gap Time Distributions for Recurrent Event Data</summary>

- *Marta Tallarita, Maria De Iorio, Alessandra Guglielmi, James Malone-Lee*

- `1607.08141v1` - [abs](http://arxiv.org/abs/1607.08141v1) - [pdf](http://arxiv.org/pdf/1607.08141v1)

> We propose autoregressive Bayesian semi-parametric models for waiting times between recurrent events. The aim is two-fold: inference on the effect of possibly time-varying covariates on the gap times and clustering of individuals based on the time trajectory of the recurrent event. Time-dependency between gap times is taken into account through the specification of an autoregressive component for the random effects parameters influencing the response at different times. The order of the autoregression may be assumed unknown and object of inference and we consider two alternative approaches to perform model selection under this scenario. Covariates may be easily included in the regression framework and censoring and missing data are easily accounted for. As the proposed methodologies lies within the class of Dirichlet process mixtures, posterior inference can be performed through efficient MCMC algorithms. We illustrate the approach through simulations and medical applications involving recurrent hospitalizations of cancer patients and successive urinary tract infections.

</details>

<details>

<summary>2016-07-27 16:18:15 - Bayesian modelling for binary outcomes in the Regression Discontinuity Design</summary>

- *Sara Geneletti, Federico Ricciardi, Aidan O'Keeffe, Gianluca Baio*

- `1607.08169v1` - [abs](http://arxiv.org/abs/1607.08169v1) - [pdf](http://arxiv.org/pdf/1607.08169v1)

> The Regression Discontinuity (RD) design is a quasi-experimental design which emulates a randomised study by exploiting situations where treatment is assigned according to a continuous variable as is common in many drug treatment guidelines. The RD design literature focuses principally on continuous outcomes. In this paper we exploit the link between the RD design and instrumental variables to obtain a causal effect estimator, the risk ratio for the treated (RRT), for the RD design when the outcome is binary. Occasionally the RRT estimator can give negative lower confindence bounds. In the Bayesian framework we impose prior constraints that prevent this from happening. This is novel and cannot be easily reproduced in a frequentist framework. We compare our estimators to those based on estimating equation and generalized methods of moments methods. Based on extensive simulations our methods compare favourably with both methods. We apply our method on a real example to estimate the effect of statins on the probability of Low-density Lipoprotein (LDL) cholesterol levels reaching recommended levels.

</details>

<details>

<summary>2016-07-28 18:01:50 - Bayesian Multiple Testing Under Sparsity for Polynomial-Tailed Distributions</summary>

- *Xueying Tang, Ke Li, Malay Ghosh*

- `1509.08100v2` - [abs](http://arxiv.org/abs/1509.08100v2) - [pdf](http://arxiv.org/pdf/1509.08100v2)

> This paper considers Bayesian multiple testing under sparsity for polynomial-tailed distributions satisfying a monotone likelihood ratio property. Included in this class of distributions are the Student's t, the Pareto, and many other distributions. We prove some general asymptotic optimality results under fixed and random thresholding. As examples of these general results, we establish the Bayesian asymptotic optimality of several multiple testing procedures in the literature for appropriately chosen false discovery rate levels. We also show by simulation that the Benjamini-Hochberg procedure with a false discovery rate level different from the asymptotically optimal one can lead to high Bayes risk.

</details>

<details>

<summary>2016-07-28 20:57:45 - Strong data-processing inequalities for channels and Bayesian networks</summary>

- *Yury Polyanskiy, Yihong Wu*

- `1508.06025v4` - [abs](http://arxiv.org/abs/1508.06025v4) - [pdf](http://arxiv.org/pdf/1508.06025v4)

> The data-processing inequality, that is, $I(U;Y) \le I(U;X)$ for a Markov chain $U \to X \to Y$, has been the method of choice for proving impossibility (converse) results in information theory and many other disciplines. Various channel-dependent improvements (called strong data-processing inequalities, or SDPIs) of this inequality have been proposed both classically and more recently. In this note we first survey known results relating various notions of contraction for a single channel. Then we consider the basic extension: given SDPI for each constituent channel in a Bayesian network, how to produce an end-to-end SDPI?   Our approach is based on the (extract of the) Evans-Schulman method, which is demonstrated for three different kinds of SDPIs, namely, the usual Ahslwede-G\'acs type contraction coefficients (mutual information), Dobrushin's contraction coefficients (total variation), and finally the $F_I$-curve (the best possible non-linear SDPI for a given channel). Resulting bounds on the contraction coefficients are interpreted as probability of site percolation. As an example, we demonstrate how to obtain SDPI for an $n$-letter memoryless channel with feedback given an SDPI for $n=1$.   Finally, we discuss a simple observation on the equivalence of a linear SDPI and comparison to an erasure channel (in the sense of "less noisy" order). This leads to a simple proof of a curious inequality of Samorodnitsky (2015), and sheds light on how information spreads in the subsets of inputs of a memoryless channel.

</details>

<details>

<summary>2016-07-29 02:18:46 - ABC in Nuclear Imaging</summary>

- *Y. Fan, S. R. Meikle, G. Angelis, A. Sitek*

- `1607.08678v1` - [abs](http://arxiv.org/abs/1607.08678v1) - [pdf](http://arxiv.org/pdf/1607.08678v1)

> We consider the application of approximate Bayesian Computation (ABC) in the context of medical imaging data. We consider the parameter estimation of compartmental models in PET imaging analysis, and provide a simple ABC algorithm for its estimation. We demonstrate the utility of the proposed estimation methods on a neurotransmitter response model, and compare our approach to existing methods.

</details>


## 2016-08

<details>

<summary>2016-08-01 11:23:58 - Why the Decision Theoretic Perspective Misrepresents Frequentist Inference: 'Nuts and Bolts' vs. Learning from Data</summary>

- *Aris Spanos*

- `1211.0638v3` - [abs](http://arxiv.org/abs/1211.0638v3) - [pdf](http://arxiv.org/pdf/1211.0638v3)

> The primary objective of this paper is to revisit and make a case for the merits of R.A. Fisher's objections to the decision-theoretic framing of frequentist inference. It is argued that this framing is congruent with the Bayesian but incongruent with the frequentist inference. It provides the Bayesian approach with a theory of optimal inference, but it misrepresents the theory of optimal frequentist inference by framing inferences solely in terms of the universal quantifier `for all values of theta in the parameter space'. This framing is at odds with the primary objective of model-based frequentist inference, which is to learn from data about the true value of theta (unknown parameter(s)); the one that gave rise to the particular data. The frequentist approach relies on factual (estimation, prediction), as well as hypothetical (testing) reasoning whose primary aim is to learn from data about the true theta. The paper calls into question the appropriateness of admissibility and reassesses Stein's paradox as it relates to the capacity of frequentist estimators to pinpoint the true theta. The paper also compares and contrasts loss-based errors with traditional frequentist errors, such as coverage, type I and II; the former are attached to {\theta}, but the latter to the inference procedure itself.

</details>

<details>

<summary>2016-08-03 01:39:38 - Combining Random Walks and Nonparametric Bayesian Topic Model for Community Detection</summary>

- *Ruimin Zhu, Wenxin Jiang*

- `1607.05573v2` - [abs](http://arxiv.org/abs/1607.05573v2) - [pdf](http://arxiv.org/pdf/1607.05573v2)

> Community detection has been an active research area for decades. Among all probabilistic models, Stochastic Block Model has been the most popular one. This paper introduces a novel probabilistic model: RW-HDP, based on random walks and Hierarchical Dirichlet Process, for community extraction. In RW-HDP, random walks conducted in a social network are treated as documents; nodes are treated as words. By using Hierarchical Dirichlet Process, a nonparametric Bayesian model, we are not only able to cluster nodes into different communities, but also determine the number of communities automatically. We use Stochastic Variational Inference for our model inference, which makes our method time efficient and can be easily extended to an online learning algorithm.

</details>

<details>

<summary>2016-08-04 01:33:34 - Bayesian Kernel and Mutual $k$-Nearest Neighbor Regression</summary>

- *Hyun-Chul Kim*

- `1608.01410v1` - [abs](http://arxiv.org/abs/1608.01410v1) - [pdf](http://arxiv.org/pdf/1608.01410v1)

> We propose Bayesian extensions of two nonparametric regression methods which are kernel and mutual $k$-nearest neighbor regression methods. Derived based on Gaussian process models for regression, the extensions provide distributions for target value estimates and the framework to select the hyperparameters. It is shown that both the proposed methods asymptotically converge to kernel and mutual $k$-nearest neighbor regression methods, respectively. The simulation results show that the proposed methods can select proper hyperparameters and are better than or comparable to the former methods for an artificial data set and a real world data set.

</details>

<details>

<summary>2016-08-04 08:11:55 - Using Approximate Bayesian Computation by Subset Simulation for Efficient Posterior Assessment of Dynamic State-Space Model Classes</summary>

- *Majid K. Vakilzadeh, James L. Beck, Thomas Abrahamsson*

- `1608.01455v1` - [abs](http://arxiv.org/abs/1608.01455v1) - [pdf](http://arxiv.org/pdf/1608.01455v1)

> Approximate Bayesian Computation (ABC) methods have gained in their popularity over the last decade because they expand the horizon of Bayesian parameter inference methods to the range of models for which only forward simulation is available. The majority of the ABC methods rely on the choice of a set of summary statistics to reduce the dimension of the data. However, as has been noted in the ABC literature, the lack of convergence guarantees that is induced by the absence of a vector of sufficient summary statistics that assures inter-model sufficiency over the set of competing models, hinders the use of the usual ABC methods when applied to Bayesian model selection or assessment. In this paper, we present a novel ABC model selection procedure for dynamical systems based on a newly appeared multi-level Markov chain Monte Carlo method, self-regulating ABC-SubSim, and a hierarchical state-space formulation of dynamic models. We show that this formulation makes it possible to independently approximate the model evidence required for assessing the posterior probability of each of the competing models. We also show that ABC-SubSim not only provides an estimate of the model evidence as a simple by-product but also it gives the posterior probability of each model as a function of the tolerance level, which allows the ABC model choices made in previous studies to be understood. We illustrate the performance of the proposed framework for ABC model updating and model class selection by applying it to two problems in Bayesian system identification: a single degree-of-freedom bilinear hysteretic oscillator and a three-story shear building with Masing hysteresis, both of which are subject to a seismic excitation.

</details>

<details>

<summary>2016-08-04 12:30:59 - Variational Bayes with Intractable Likelihood</summary>

- *Minh-Ngoc Tran, David J. Nott, Robert Kohn*

- `1503.08621v2` - [abs](http://arxiv.org/abs/1503.08621v2) - [pdf](http://arxiv.org/pdf/1503.08621v2)

> Variational Bayes (VB) is rapidly becoming a popular tool for Bayesian inference in statistical modeling. However, the existing VB algorithms are restricted to cases where the likelihood is tractable, which precludes the use of VB in many interesting situations such as in state space models and in approximate Bayesian computation (ABC), where application of VB methods was previously impossible. This paper extends the scope of application of VB to cases where the likelihood is intractable, but can be estimated unbiasedly. The proposed VB method therefore makes it possible to carry out Bayesian inference in many statistical applications, including state space models and ABC. The method is generic in the sense that it can be applied to almost all statistical models without requiring too much model-based derivation, which is a drawback of many existing VB algorithms. We also show how the proposed method can be used to obtain highly accurate VB approximations of marginal posterior distributions.

</details>

<details>

<summary>2016-08-05 14:49:57 - Bridging the Gap between Stochastic Gradient MCMC and Stochastic Optimization</summary>

- *Changyou Chen, David Carlson, Zhe Gan, Chunyuan Li, Lawrence Carin*

- `1512.07962v3` - [abs](http://arxiv.org/abs/1512.07962v3) - [pdf](http://arxiv.org/pdf/1512.07962v3)

> Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesian analogs to popular stochastic optimization methods; however, this connection is not well studied. We explore this relationship by applying simulated annealing to an SGMCMC algorithm. Furthermore, we extend recent SG-MCMC methods with two key components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii) adaptive element-wise momentum weights. The zero-temperature limit gives a novel stochastic optimization method with adaptive element-wise momentum weights, while conventional optimization methods only have a shared, static momentum weight. Under certain assumptions, our theoretical analysis suggests the proposed simulated annealing approach converges close to the global optima. Experiments on several deep neural network models show state-of-the-art results compared to related stochastic optimization algorithms.

</details>

<details>

<summary>2016-08-05 20:43:21 - The Future of Data Analysis in the Neurosciences</summary>

- *Danilo Bzdok, B. T. Thomas Yeo*

- `1608.03465v1` - [abs](http://arxiv.org/abs/1608.03465v1) - [pdf](http://arxiv.org/pdf/1608.03465v1)

> Neuroscience is undergoing faster changes than ever before. Over 100 years our field qualitatively described and invasively manipulated single or few organisms to gain anatomical, physiological, and pharmacological insights. In the last 10 years neuroscience spawned quantitative big-sample datasets on microanatomy, synaptic connections, optogenetic brain-behavior assays, and high-level cognition. While growing data availability and information granularity have been amply discussed, we direct attention to a routinely neglected question: How will the unprecedented data richness shape data analysis practices? Statistical reasoning is becoming more central to distill neurobiological knowledge from healthy and pathological brain recordings. We believe that large-scale data analysis will use more models that are non-parametric, generative, mixing frequentist and Bayesian aspects, and grounded in different statistical inferences.

</details>

<details>

<summary>2016-08-08 06:51:29 - Prioritizing covariates in the planning of future studies in the meta-analytic framework</summary>

- *Juha Karvanen, Mikko J. SillanpÃ¤Ã¤*

- `1608.02333v1` - [abs](http://arxiv.org/abs/1608.02333v1) - [pdf](http://arxiv.org/pdf/1608.02333v1)

> Science can be seen as a sequential process where each new study augments evidence to the existing knowledge. To have the best prospects to make an impact in this process, a new study should be designed optimally taking into account the previous studies and other prior information. We propose a formal approach for the covariate prioritization, i.e., the decision about the covariates to be measured in a new study. The decision criteria can be based on conditional power, change of the p-value, change in lower confidence limit, Kullback-Leibler divergence, Bayes factors, Bayesian false discovery rate or difference between prior and posterior expectation. The criteria can be also used for decisions on the sample size. As an illustration, we consider covariate prioritization based on genome-wide association studies for C-reactive protein levels and make suggestions on the genes to be studied further.   keywords: design; evidence-based medicine; meta-analysis; power; scientific method

</details>

<details>

<summary>2016-08-09 08:34:04 - PAC-Bayesian Theorems for Domain Adaptation with Specialization to Linear Classifiers</summary>

- *Pascal Germain, Amaury Habrard, FranÃ§ois Laviolette, Emilie Morvant*

- `1503.06944v3` - [abs](http://arxiv.org/abs/1503.06944v3) - [pdf](http://arxiv.org/pdf/1503.06944v3)

> In this paper, we provide two main contributions in PAC-Bayesian theory for domain adaptation where the objective is to learn, from a source distribution, a well-performing majority vote on a different target distribution. On the one hand, we propose an improvement of the previous approach proposed by Germain et al. (2013), that relies on a novel distribution pseudodistance based on a disagreement averaging, allowing us to derive a new tighter PAC-Bayesian domain adaptation bound for the stochastic Gibbs classifier. We specialize it to linear classifiers, and design a learning algorithm which shows interesting results on a synthetic problem and on a popular sentiment annotation task. On the other hand, we generalize these results to multisource domain adaptation allowing us to take into account different source domains. This study opens the door to tackle domain adaptation tasks by making use of all the PAC-Bayesian tools.

</details>

<details>

<summary>2016-08-10 07:04:02 - A Moran coefficient-based mixed effects approach to investigate spatially varying relationships</summary>

- *Daisuke Murakami, Takahiro Yoshida, Hajime Seya, Daniel A. Griffith, Yoshiki Yamagata*

- `1606.06885v2` - [abs](http://arxiv.org/abs/1606.06885v2) - [pdf](http://arxiv.org/pdf/1606.06885v2)

> This study develops a spatially varying coefficient model by extending the random effects eigenvector spatial filtering model. The developed model has the following properties: its coefficients are interpretable in terms of the Moran coefficient; each of its coefficients can have a different degree of spatial smoothness; and it yields a variant of a Bayesian spatially varying coefficient model. Also, parameter estimation of the model can be executed with a relatively small computationally burden. Results of a Monte Carlo simulation reveal that our model outperforms a conventional eigenvector spatial filtering (ESF) model and geographically weighted regression (GWR) models in terms of the accuracy of the coefficient estimates and computational time. We empirically apply our model to the hedonic land price analysis of flood risk in Japan.

</details>

<details>

<summary>2016-08-10 07:55:02 - Variational Bayes with Synthetic Likelihood</summary>

- *Victor M-H. Ong, David J. Nott, Minh-Ngoc Tran, Scott A. Sisson, Christopher C. Drovandi*

- `1608.03069v1` - [abs](http://arxiv.org/abs/1608.03069v1) - [pdf](http://arxiv.org/pdf/1608.03069v1)

> Synthetic likelihood is an attractive approach to likelihood-free inference when an approximately Gaussian summary statistic for the data, informative for inference about the parameters, is available. The synthetic likelihood method derives an approximate likelihood function from a plug-in normal density estimate for the summary statistic, with plug-in mean and covariance matrix obtained by Monte Carlo simulation from the model. In this article, we develop alternatives to Markov chain Monte Carlo implementations of Bayesian synthetic likelihoods with reduced computational overheads. Our approach uses stochastic gradient variational inference methods for posterior approximation in the synthetic likelihood context, employing unbiased estimates of the log likelihood. We compare the new method with a related likelihood free variational inference technique in the literature, while at the same time improving the implementation of that approach in a number of ways. These new algorithms are feasible to implement in situations which are challenging for conventional approximate Bayesian computation (ABC) methods, in terms of the dimensionality of the parameter and summary statistic.

</details>

<details>

<summary>2016-08-10 08:59:43 - Prediction of tool-wear in turning of medical grade cobalt chromium molybdenum alloy (ASTM F75) using non-parametric Bayesian models</summary>

- *Damien McParland, Szymon Baron, Sarah O'Rourke, Denis Dowling, Eamonn Ahearne, Andrew Parnell*

- `1608.03091v1` - [abs](http://arxiv.org/abs/1608.03091v1) - [pdf](http://arxiv.org/pdf/1608.03091v1)

> We present a novel approach to estimating the effect of control parameters on tool wear rates and related changes in the three force components in turning of medical grade Co-Cr-Mo (ASTM F75) alloy. Co-Cr-Mo is known to be a difficult to cut material which, due to a combination of mechanical and physical properties, is used for the critical structural components of implantable medical prosthetics. We run a designed experiment which enables us to estimate tool wear from feed rate and cutting speed, and constrain them using a Bayesian hierarchical Gaussian Process model which enables prediction of tool wear rates for untried experimental settings. The predicted tool wear rates are non-linear and, using our models, we can identify experimental settings which optimise the life of the tool. This approach has potential in the future for realtime application of data analytics to machining processes.

</details>

<details>

<summary>2016-08-10 09:19:16 - Detecting non-binomial sex allocation when developmental mortality operates</summary>

- *Richard D. Wilkinson, Apostolos Kapranas, Ian C. W. Hardy*

- `1608.03102v1` - [abs](http://arxiv.org/abs/1608.03102v1) - [pdf](http://arxiv.org/pdf/1608.03102v1)

> Optimal sex allocation theory is one of the most intricately developed areas of evolutionary ecology. Under a range of conditions, particularly under population sub-division, selection favours sex being allocated to offspring non-randomly, generating non-binomial variances of offspring group sex ratios. Detecting non-binomial sex allocation is complicated by stochastic developmental mortality, as offspring sex can often only be identified on maturity with the sex of non-maturing offspring remaining unknown. We show that current approaches for detecting non-binomiality have limited ability to detect non-binomial sex allocation when developmental mortality has occurred. We present a new procedure using an explicit model of sex allocation and mortality and develop a Bayesian model selection approach (available as an R package). We use the double and multiplicative binomial distributions to model over- and under-dispersed sex allocation and show how to calculate Bayes factors for comparing these alternative models to the null hypothesis of binomial sex allocation. The ability to detect non-binomial sex allocation is greatly increased, particularly in cases where mortality is common. The use of Bayesian methods allows for the quantification of the evidence in favour of each hypothesis, and our modelling approach provides an improved descriptive capability over existing approaches. We use a simulation study to demonstrate substantial improvements in power for detecting non-binomial sex allocation in situations where current methods fail, and we illustrate the approach in real scenarios using empirically obtained datasets on the sexual composition of groups of gregarious parasitoid wasps.

</details>

<details>

<summary>2016-08-10 14:44:51 - Efficient model-based clustering with coalescents: Application to multiple outcomes using medical records data</summary>

- *Ricardo Henao, Joseph E. Lucas*

- `1608.03191v1` - [abs](http://arxiv.org/abs/1608.03191v1) - [pdf](http://arxiv.org/pdf/1608.03191v1)

> We present a sequential Monte Carlo sampler for coalescent based Bayesian hierarchical clustering. The model is appropriate for multivariate non-\iid data and our approach offers a substantial reduction in computational cost when compared to the original sampler. We also propose a quadratic complexity approximation that in practice shows almost no loss in performance compared to its counterpart. Our formulation leads to a greedy algorithm that exhibits performance improvement over other greedy algorithms, particularly in small data sets. We incorporate the Coalescent into a hierarchical regression model that allows joint modeling of multiple correlated outcomes. The approach does not require {\em a priori} knowledge of either the degree or structure of the correlation and, as a byproduct, generates additional models for a subset of the composite outcomes. We demonstrate the utility of the approach by predicting multiple different types of outcomes using medical records data from a cohort of diabetic patients.

</details>

<details>

<summary>2016-08-11 01:45:48 - An Accurate Gaussian Process-Based Early Warning System for Dengue Fever</summary>

- *Julio Albinati, Wagner Meira Jr, Gisele Lobo Pappa*

- `1608.03343v1` - [abs](http://arxiv.org/abs/1608.03343v1) - [pdf](http://arxiv.org/pdf/1608.03343v1)

> Dengue fever is a mosquito-borne disease present in all Brazilian territory. Brazilian government, however, lacks an accurate early warning system to quickly predict future dengue outbreaks. Such system would help health authorities to plan their actions and to reduce the impact of the disease in the country. However, most attempts to model dengue fever use parametric models which enforce a specific expected behaviour and fail to capture the inherent complexity of dengue dynamics. Therefore, we propose a new Bayesian non-parametric model based on Gaussian processes to design an accurate and flexible model that outperforms previous/standard techniques and can be incorporated into an early warning system, specially at cities from Southeast and Center-West regions. The model also helps understanding dengue dynamics in Brazil through the analysis of the covariance functions generated.

</details>

<details>

<summary>2016-08-11 09:30:40 - Approximate maximum likelihood estimation using data-cloning ABC</summary>

- *Umberto Picchini, Rachele Anderson*

- `1505.06318v4` - [abs](http://arxiv.org/abs/1505.06318v4) - [pdf](http://arxiv.org/pdf/1505.06318v4)

> A maximum likelihood methodology for a general class of models is presented, using an approximate Bayesian computation (ABC) approach. The typical target of ABC methods are models with intractable likelihoods, and we combine an ABC-MCMC sampler with so-called "data cloning" for maximum likelihood estimation. Accuracy of ABC methods relies on the use of a small threshold value for comparing simulations from the model and observed data. The proposed methodology shows how to use large threshold values, while the number of data-clones is increased to ease convergence towards an approximate maximum likelihood estimate. We show how to exploit the methodology to reduce the number of iterations of a standard ABC-MCMC algorithm and therefore reduce the computational effort, while obtaining reasonable point estimates. Simulation studies show the good performance of our approach on models with intractable likelihoods such as g-and-k distributions, stochastic differential equations and state-space models.

</details>

<details>

<summary>2016-08-11 14:48:58 - Dual Control for Approximate Bayesian Reinforcement Learning</summary>

- *Edgar D. Klenske, Philipp Hennig*

- `1510.03591v2` - [abs](http://arxiv.org/abs/1510.03591v2) - [pdf](http://arxiv.org/pdf/1510.03591v2)

> Control of non-episodic, finite-horizon dynamical systems with uncertain dynamics poses a tough and elementary case of the exploration-exploitation trade-off. Bayesian reinforcement learning, reasoning about the effect of actions and future observations, offers a principled solution, but is intractable. We review, then extend an old approximate approach from control theory---where the problem is known as dual control---in the context of modern regression methods, specifically generalized linear regression. Experiments on simulated systems show that this framework offers a useful approximation to the intractable aspects of Bayesian RL, producing structured exploration strategies that differ from standard RL approaches. We provide simple examples for the use of this framework in (approximate) Gaussian process regression and feedforward neural networks for the control of exploration.

</details>

<details>

<summary>2016-08-11 15:10:30 - Estimating standard errors for importance sampling estimators with multiple Markov chains</summary>

- *Vivekananda Roy, Aixin Tan, James M. Flegal*

- `1509.06310v2` - [abs](http://arxiv.org/abs/1509.06310v2) - [pdf](http://arxiv.org/pdf/1509.06310v2)

> The naive importance sampling estimator, based on samples from a single importance density, can be numerically unstable. Instead, we consider generalized importance sampling estimators where samples from more than one probability distribution are combined. We study this problem in the Markov chain Monte Carlo context, where independent samples are replaced with Markov chain samples. If the chains converge to their respective target distributions at a polynomial rate, then under two finite moment conditions, we show a central limit theorem holds for the generalized estimators. Further, we develop an easy to implement method to calculate valid asymptotic standard errors based on batch means. We also provide a batch means estimator for calculating asymptotically valid standard errors of Geyer(1994) reverse logistic estimator. We illustrate the method using a Bayesian variable selection procedure in linear regression. In particular, the generalized importance sampling estimator is used to perform empirical Bayes variable selection and the batch means estimator is used to obtain standard errors in a high-dimensional setting where current methods are not applicable.

</details>

<details>

<summary>2016-08-11 19:56:27 - Warm Starting Bayesian Optimization</summary>

- *Matthias Poloczek, Jialei Wang, Peter I. Frazier*

- `1608.03585v1` - [abs](http://arxiv.org/abs/1608.03585v1) - [pdf](http://arxiv.org/pdf/1608.03585v1)

> We develop a framework for warm-starting Bayesian optimization, that reduces the solution time required to solve an optimization problem that is one in a sequence of related problems. This is useful when optimizing the output of a stochastic simulator that fails to provide derivative information, for which Bayesian optimization methods are well-suited. Solving sequences of related optimization problems arises when making several business decisions using one optimization model and input data collected over different time periods or markets. While many gradient-based methods can be warm started by initiating optimization at the solution to the previous problem, this warm start approach does not apply to Bayesian optimization methods, which carry a full metamodel of the objective function from iteration to iteration. Our approach builds a joint statistical model of the entire collection of related objective functions, and uses a value of information calculation to recommend points to evaluate.

</details>

<details>

<summary>2016-08-14 06:01:21 - Bayesian Model Selection Methods for Mutual and Symmetric $k$-Nearest Neighbor Classification</summary>

- *Hyun-Chul Kim*

- `1608.04063v1` - [abs](http://arxiv.org/abs/1608.04063v1) - [pdf](http://arxiv.org/pdf/1608.04063v1)

> The $k$-nearest neighbor classification method ($k$-NNC) is one of the simplest nonparametric classification methods. The mutual $k$-NN classification method (M$k$NNC) is a variant of $k$-NNC based on mutual neighborship. We propose another variant of $k$-NNC, the symmetric $k$-NN classification method (S$k$NNC) based on both mutual neighborship and one-sided neighborship. The performance of M$k$NNC and S$k$NNC depends on the parameter $k$ as the one of $k$-NNC does. We propose the ways how M$k$NN and S$k$NN classification can be performed based on Bayesian mutual and symmetric $k$-NN regression methods with the selection schemes for the parameter $k$. Bayesian mutual and symmetric $k$-NN regression methods are based on Gaussian process models, and it turns out that they can do M$k$NN and S$k$NN classification with new encodings of target values (class labels). The simulation results show that the proposed methods are better than or comparable to $k$-NNC, M$k$NNC and S$k$NNC with the parameter $k$ selected by the leave-one-out cross validation method not only for an artificial data set but also for real world data sets.

</details>

<details>

<summary>2016-08-15 10:18:09 - Gaussian process hyper-parameter estimation using parallel asymptotically independent Markov sampling</summary>

- *A. Garbuno-Inigo, F. A. DiazDelaO, K. M. Zuev*

- `1506.08010v4` - [abs](http://arxiv.org/abs/1506.08010v4) - [pdf](http://arxiv.org/pdf/1506.08010v4)

> Gaussian process emulators of computationally expensive computer codes provide fast statistical approximations to model physical processes. The training of these surrogates depends on the set of design points chosen to run the simulator. Due to computational cost, such training set is bound to be limited and quantifying the resulting uncertainty in the hyper-parameters of the emulator by uni-modal distributions is likely to induce bias. In order to quantify this uncertainty, this paper proposes a computationally efficient sampler based on an extension of Asymptotically Independent Markov Sampling, a recently developed algorithm for Bayesian inference. Structural uncertainty of the emulator is obtained as a by-product of the Bayesian treatment of the hyper-parameters. Additionally, the user can choose to perform stochastic optimisation to sample from a neighbourhood of the Maximum a Posteriori estimate, even in the presence of multimodality. Model uncertainty is also acknowledged through numerical stabilisation measures by including a nugget term in the formulation of the probability model. The efficiency of the proposed sampler is illustrated in examples where multi-modal distributions are encountered. For the purpose of reproducibility, further development, and use in other applications the code used to generate the examples is freely available for download at https://github.com/agarbuno/paims_codes

</details>

<details>

<summary>2016-08-15 11:41:02 - Bayesian Community Detection</summary>

- *StÃ©phanie van der Pas, Aad van der Vaart*

- `1608.04242v1` - [abs](http://arxiv.org/abs/1608.04242v1) - [pdf](http://arxiv.org/pdf/1608.04242v1)

> We introduce a Bayesian estimator of the underlying class structure in the stochastic block model, when the number of classes is known. The estimator is the posterior mode corresponding to a Dirichlet prior on the class proportions, a generalized Bernoulli prior on the class labels, and a beta prior on the edge probabilities. We show that this estimator is strongly consistent when the expected degree is at least of order $\log^2{n}$, where $n$ is the number of nodes in the network.

</details>

<details>

<summary>2016-08-16 10:41:32 - The Bayesian Low-Rank Determinantal Point Process Mixture Model</summary>

- *Mike Gartrell, Ulrich Paquet, Noam Koenigstein*

- `1608.04245v2` - [abs](http://arxiv.org/abs/1608.04245v2) - [pdf](http://arxiv.org/pdf/1608.04245v2)

> Determinantal point processes (DPPs) are an elegant model for encoding probabilities over subsets, such as shopping baskets, of a ground set, such as an item catalog. They are useful for a number of machine learning tasks, including product recommendation. DPPs are parametrized by a positive semi-definite kernel matrix. Recent work has shown that using a low-rank factorization of this kernel provides remarkable scalability improvements that open the door to training on large-scale datasets and computing online recommendations, both of which are infeasible with standard DPP models that use a full-rank kernel. In this paper we present a low-rank DPP mixture model that allows us to represent the latent structure present in observed subsets as a mixture of a number of component low-rank DPPs, where each component DPP is responsible for representing a portion of the observed data. The mixture model allows us to effectively address the capacity constraints of the low-rank DPP model. We present an efficient and scalable Markov Chain Monte Carlo (MCMC) learning algorithm for our model that uses Gibbs sampling and stochastic gradient Hamiltonian Monte Carlo (SGHMC). Using an evaluation on several real-world product recommendation datasets, we show that our low-rank DPP mixture model provides substantially better predictive performance than is possible with a single low-rank or full-rank DPP, and significantly better performance than several other competing recommendation methods in many cases.

</details>

<details>

<summary>2016-08-16 11:26:25 - Fast Calculation of the Knowledge Gradient for Optimization of Deterministic Engineering Simulations</summary>

- *Joachim van der Herten, Ivo Couckuyt, Dirk Deschrijver, Tom Dhaene*

- `1608.04550v1` - [abs](http://arxiv.org/abs/1608.04550v1) - [pdf](http://arxiv.org/pdf/1608.04550v1)

> A novel efficient method for computing the Knowledge-Gradient policy for Continuous Parameters (KGCP) for deterministic optimization is derived. The differences with Expected Improvement (EI), a popular choice for Bayesian optimization of deterministic engineering simulations, are explored. Both policies and the Upper Confidence Bound (UCB) policy are compared on a number of benchmark functions including a problem from structural dynamics. It is empirically shown that KGCP has similar performance as the EI policy for many problems, but has better convergence properties for complex (multi-modal) optimization problems as it emphasizes more on exploration when the model is confident about the shape of optimal regions. In addition, the relationship between Maximum Likelihood Estimation (MLE) and slice sampling for estimation of the hyperparameters of the underlying models, and the complexity of the problem at hand, is studied.

</details>

<details>

<summary>2016-08-16 19:28:21 - Bayesian aggregation of two forecasts in the partial information framework</summary>

- *Philip Ernst, Robin Pemantle, Ville Satopaa, Lyle Ungar*

- `1608.04717v1` - [abs](http://arxiv.org/abs/1608.04717v1) - [pdf](http://arxiv.org/pdf/1608.04717v1)

> We generalize the results of \cite{SPU, SJPU} by showing how the Gaussian aggregator may be computed in a setting where parameter estimation is not required. We proceed to provide an explicit formula for a "one-shot" aggregation problem with two forecasters.

</details>

<details>

<summary>2016-08-17 11:12:02 - Bayesian Inference and Testing of Group Differences in Brain Networks</summary>

- *Daniele Durante, David B. Dunson*

- `1411.6506v5` - [abs](http://arxiv.org/abs/1411.6506v5) - [pdf](http://arxiv.org/pdf/1411.6506v5)

> Network data are increasingly collected along with other variables of interest. Our motivation is drawn from neurophysiology studies measuring brain connectivity networks for a sample of individuals along with their membership to a low or high creative reasoning group. It is of paramount importance to develop statistical methods for testing of global and local changes in the structural interconnections among brain regions across groups. We develop a general Bayesian procedure for inference and testing of group differences in the network structure, which relies on a nonparametric representation for the conditional probability mass function associated with a network-valued random variable. By leveraging a mixture of low-rank factorizations, we allow simple global and local hypothesis testing adjusting for multiplicity. An efficient Gibbs sampler is defined for posterior computation. We provide theoretical results on the flexibility of the model and assess testing performance in simulations. The approach is applied to provide novel insights on the relationships between human brain networks and creativity.

</details>

<details>

<summary>2016-08-17 23:30:04 - A Bayesian Network approach to County-Level Corn Yield Prediction using historical data and expert knowledge</summary>

- *Vikas Chawla, Hsiang Sing Naik, Adedotun Akintayo, Dermot Hayes, Patrick Schnable, Baskar Ganapathysubramanian, Soumik Sarkar*

- `1608.05127v1` - [abs](http://arxiv.org/abs/1608.05127v1) - [pdf](http://arxiv.org/pdf/1608.05127v1)

> Crop yield forecasting is the methodology of predicting crop yields prior to harvest. The availability of accurate yield prediction frameworks have enormous implications from multiple standpoints, including impact on the crop commodity futures markets, formulation of agricultural policy, as well as crop insurance rating. The focus of this work is to construct a corn yield predictor at the county scale. Corn yield (forecasting) depends on a complex, interconnected set of variables that include economic, agricultural, management and meteorological factors. Conventional forecasting is either knowledge-based computer programs (that simulate plant-weather-soil-management interactions) coupled with targeted surveys or statistical model based. The former is limited by the need for painstaking calibration, while the latter is limited to univariate analysis or similar simplifying assumptions that fail to capture the complex interdependencies affecting yield. In this paper, we propose a data-driven approach that is "gray box" i.e. that seamlessly utilizes expert knowledge in constructing a statistical network model for corn yield forecasting. Our multivariate gray box model is developed on Bayesian network analysis to build a Directed Acyclic Graph (DAG) between predictors and yield. Starting from a complete graph connecting various carefully chosen variables and yield, expert knowledge is used to prune or strengthen edges connecting variables. Subsequently the structure (connectivity and edge weights) of the DAG that maximizes the likelihood of observing the training data is identified via optimization. We curated an extensive set of historical data (1948-2012) for each of the 99 counties in Iowa as data to train the model.

</details>

<details>

<summary>2016-08-18 04:46:00 - Approximate Bayesian Computation via Sufficient Dimension Reduction</summary>

- *Xiaolong Zhong, Malay Ghosh*

- `1608.05173v1` - [abs](http://arxiv.org/abs/1608.05173v1) - [pdf](http://arxiv.org/pdf/1608.05173v1)

> Approximate Bayesian computation (ABC) has gained popularity in recent years owing to its easy implementation, nice interpretation and good performance. Its advantages are more visible when one encounters complex models where maximum likelihood estimation as well as Bayesian analysis via Markov chain Monte Carlo demand prohibitively large amount of time. This paper examines properties of ABC both from a theoretical as well as from a computational point of view.We consolidate the ABC theory by proving theorems related to its limiting behaviour. In particular, we consider partial posteriors, which serve as the first step towards approximating the full posteriors. Also, a new semi-automatic algorithm of ABC is proposed using sufficient dimension reduction (SDR) method. SDR has primarily surfaced in the frequentist literature. But we have demonstrated in this paper that it has connections with ABC as well.

</details>

<details>

<summary>2016-08-18 17:47:53 - Probabilistic Data Analysis with Probabilistic Programming</summary>

- *Feras Saad, Vikash Mansinghka*

- `1608.05347v1` - [abs](http://arxiv.org/abs/1608.05347v1) - [pdf](http://arxiv.org/pdf/1608.05347v1)

> Probabilistic techniques are central to data analysis, but different approaches can be difficult to apply, combine, and compare. This paper introduces composable generative population models (CGPMs), a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques. Examples include hierarchical Bayesian models, multivariate kernel methods, discriminative machine learning, clustering algorithms, dimensionality reduction, and arbitrary probabilistic programs. We also demonstrate the integration of CGPMs into BayesDB, a probabilistic programming platform that can express data analysis tasks using a modeling language and a structured query language. The practical value is illustrated in two ways. First, CGPMs are used in an analysis that identifies satellite data records which probably violate Kepler's Third Law, by composing causal probabilistic programs with non-parametric Bayes in under 50 lines of probabilistic code. Second, for several representative data analysis tasks, we report on lines of code and accuracy measurements of various CGPMs, plus comparisons with standard baseline solutions from Python and MATLAB libraries.

</details>

<details>

<summary>2016-08-19 08:25:17 - String and Membrane Gaussian Processes</summary>

- *Yves-Laurent Kom Samo, Stephen Roberts*

- `1507.06977v4` - [abs](http://arxiv.org/abs/1507.06977v4) - [pdf](http://arxiv.org/pdf/1507.06977v4)

> In this paper we introduce a novel framework for making exact nonparametric Bayesian inference on latent functions, that is particularly suitable for Big Data tasks. Firstly, we introduce a class of stochastic processes we refer to as string Gaussian processes (string GPs), which are not to be mistaken for Gaussian processes operating on text. We construct string GPs so that their finite-dimensional marginals exhibit suitable local conditional independence structures, which allow for scalable, distributed, and flexible nonparametric Bayesian inference, without resorting to approximations, and while ensuring some mild global regularity constraints. Furthermore, string GP priors naturally cope with heterogeneous input data, and the gradient of the learned latent function is readily available for explanatory analysis. Secondly, we provide some theoretical results relating our approach to the standard GP paradigm. In particular, we prove that some string GPs are Gaussian processes, which provides a complementary global perspective on our framework. Finally, we derive a scalable and distributed MCMC scheme for supervised learning tasks under string GP priors. The proposed MCMC scheme has computational time complexity $\mathcal{O}(N)$ and memory requirement $\mathcal{O}(dN)$, where $N$ is the data size and $d$ the dimension of the input space. We illustrate the efficacy of the proposed approach on several synthetic and real-world datasets, including a dataset with $6$ millions input points and $8$ attributes.

</details>

<details>

<summary>2016-08-22 12:23:40 - Object Depth Profile and Reflectivity Restoration from Sparse Single-Photon Data Acquired in Underwater Environments</summary>

- *Abderrahim Halimi, Aurora Maccarone, Aongus McCarthy, Steve McLaughlin, Gerald S. Buller*

- `1608.06143v1` - [abs](http://arxiv.org/abs/1608.06143v1) - [pdf](http://arxiv.org/pdf/1608.06143v1)

> This paper presents two new algorithms for the joint restoration of depth and reflectivity (DR) images constructed from time-correlated single-photon counting (TCSPC) measurements. Two extreme cases are considered: (i) a reduced acquisition time that leads to very low photon counts and (ii) a highly attenuating environment (such as a turbid medium) which makes the reflectivity estimation more difficult at increasing range. Adopting a Bayesian approach, the Poisson distributed observations are combined with prior distributions about the parameters of interest, to build the joint posterior distribution. More precisely, two Markov random field (MRF) priors enforcing spatial correlations are assigned to the DR images. Under some justified assumptions, the restoration problem (regularized likelihood) reduces to a convex formulation with respect to each of the parameters of interest. This problem is first solved using an adaptive Markov chain Monte Carlo (MCMC) algorithm that approximates the minimum mean square parameter estimators. This algorithm is fully automatic since it adjusts the parameters of the MRFs by maximum marginal likelihood estimation. However, the MCMC-based algorithm exhibits a relatively long computational time. The second algorithm deals with this issue and is based on a coordinate descent algorithm. Results on single-photon depth data from laboratory based underwater measurements demonstrate the benefit of the proposed strategy that improves the quality of the estimated DR images.

</details>

<details>

<summary>2016-08-23 05:15:25 - Softplus Regressions and Convex Polytopes</summary>

- *Mingyuan Zhou*

- `1608.06383v1` - [abs](http://arxiv.org/abs/1608.06383v1) - [pdf](http://arxiv.org/pdf/1608.06383v1)

> To construct flexible nonlinear predictive distributions, the paper introduces a family of softplus function based regression models that convolve, stack, or combine both operations by convolving countably infinite stacked gamma distributions, whose scales depend on the covariates. Generalizing logistic regression that uses a single hyperplane to partition the covariate space into two halves, softplus regressions employ multiple hyperplanes to construct a confined space, related to a single convex polytope defined by the intersection of multiple half-spaces or a union of multiple convex polytopes, to separate one class from the other. The gamma process is introduced to support the convolution of countably infinite (stacked) covariate-dependent gamma distributions. For Bayesian inference, Gibbs sampling derived via novel data augmentation and marginalization techniques is used to deconvolve and/or demix the highly complex nonlinear predictive distribution. Example results demonstrate that softplus regressions provide flexible nonlinear decision boundaries, achieving classification accuracies comparable to that of kernel support vector machine while requiring significant less computation for out-of-sample prediction.

</details>

<details>

<summary>2016-08-24 01:40:39 - Bayesian inference on dynamic linear models of day-to-day origin-destination flows in transportation networks</summary>

- *Anselmo Ramalho Pitombeira-Neto, Carlos Felipe Grangeiro Loureiro, Luis Eduardo Carvalho*

- `1608.06682v1` - [abs](http://arxiv.org/abs/1608.06682v1) - [pdf](http://arxiv.org/pdf/1608.06682v1)

> Estimation of origin-destination (OD) demand plays a key role in successful transportation studies. In this paper, we consider the estimation of time-varying day-to-day OD flows given data on traffic volumes in a transportation network for a sequence of days. We propose a dynamic linear model (DLM) in order to represent the stochastic evolution of OD flows over time. DLM's are Bayesian state-space models which can capture non-stationarity. We take into account the hierarchical relationships between the distribution of OD flows among routes and the assignment of traffic volumes on links. Route choice probabilities are obtained through a utility model based on past route costs. We propose a Markov chain Monte Carlo algorithm, which integrates Gibbs sampling and a forward filtering backward sampling technique, in order to approximate the joint posterior distribution of mean OD flows and parameters of the route choice model. Our approach can be applied to congested networks and in the case when data are available on only a subset of links. We illustrate the application of our approach through simulated experiments on a test network from the literature.

</details>

<details>

<summary>2016-08-24 16:10:33 - Bridging AIC and BIC: a new criterion for autoregression</summary>

- *Jie Ding, Vahid Tarokh, Yuhong Yang*

- `1508.02473v4` - [abs](http://arxiv.org/abs/1508.02473v4) - [pdf](http://arxiv.org/pdf/1508.02473v4)

> We introduce a new criterion to determine the order of an autoregressive model fitted to time series data. It has the benefits of the two well-known model selection techniques, the Akaike information criterion and the Bayesian information criterion. When the data is generated from a finite order autoregression, the Bayesian information criterion is known to be consistent, and so is the new criterion. When the true order is infinity or suitably high with respect to the sample size, the Akaike information criterion is known to be efficient in the sense that its prediction performance is asymptotically equivalent to the best offered by the candidate models; in this case, the new criterion behaves in a similar manner. Different from the two classical criteria, the proposed criterion adaptively achieves either consistency or efficiency depending on the underlying true model. In practice where the observed time series is given without any prior information about the model specification, the proposed order selection criterion is more flexible and robust compared with classical approaches. Numerical results are presented demonstrating the adaptivity of the proposed technique when applied to various datasets.

</details>

<details>

<summary>2016-08-26 11:25:28 - The cylindrical K-function and Poisson line cluster point processes</summary>

- *Jesper MÃ¸ller, Farzaneh Safavimanesh, Jakob G. Rasmussen*

- `1503.07423v5` - [abs](http://arxiv.org/abs/1503.07423v5) - [pdf](http://arxiv.org/pdf/1503.07423v5)

> Analyzing point patterns with linear structures has recently been of interest in e.g. neuroscience and geography. To detect anisotropy in such cases, we introduce a functional summary statistic, called the cylindrical $K$-function, since it is a directional $K$-function whose structuring element is a cylinder. Further we introduce a class of anisotropic Cox point processes, called Poisson line cluster point processes. The points of such a process are random displacements of Poisson point processes defined on the lines of a Poisson line process. Parameter estimation based on moment methods or Bayesian inference for this model is discussed when the underlying Poisson line process and the cluster memberships are treated as hidden processes. To illustrate the methodologies, we analyze a two and a three-dimensional point pattern data set. The 3D data set is of particular interest as it relates to the minicolumn hypothesis in neuroscience, claiming that pyramidal and other brain cells have a columnar arrangement perpendicular to the pial surface of the brain.

</details>

<details>

<summary>2016-08-26 19:23:55 - Approximate Bayesian Computation and Model Validation for Repulsive Spatial Point Processes</summary>

- *Shinichiro Shirota, Alan. E. Gelfand*

- `1604.07027v2` - [abs](http://arxiv.org/abs/1604.07027v2) - [pdf](http://arxiv.org/pdf/1604.07027v2)

> In many applications involving spatial point patterns, we find evidence of inhibition or repulsion. The most commonly used class of models for such settings are the Gibbs point processes. A recent alternative, at least to the statistical community, is the determinantal point process. Here, we examine model fitting and inference for both of these classes of processes in a Bayesian framework. While usual MCMC model fitting can be available, the algorithms are complex and are not always well behaved. We propose using approximate Bayesian computation (ABC) for such fitting. This approach becomes attractive because, though likelihoods are very challenging to work with for these processes, generation of realizations given parameter values is relatively straightforward. As a result, the ABC fitting approach is well-suited for these models. In addition, such simulation makes them well-suited for posterior predictive inference as well as for model assessment. We provide details for all of the above along with some simulation investigation and an illustrative analysis of a point pattern of tree data exhibiting repulsion. R-code and datasets are included in the supplementary material.

</details>

<details>

<summary>2016-08-29 06:18:05 - Bayesian Nonparametric Instrumental Variable Regression Approach to Quantile Inference</summary>

- *Genya Kobayashi, Kota Ogasawara*

- `1608.07921v1` - [abs](http://arxiv.org/abs/1608.07921v1) - [pdf](http://arxiv.org/pdf/1608.07921v1)

> This study extends the Bayesian nonparametric instrumental variable regression model to determine the structural effects of covariates on the conditional quantile of the response variable. The error distribution is nonparametrically modelled using a Dirichlet mixture of bivariate normal distributions. The mean functions include the smooth effects of the covariates represented using the spline functions in an additive manner. The conditional variance of the second-stage error is also modelled using the spline functions such that it varies smoothly with the covariates. Accordingly, the proposed model allows for considerable flexibility in the shape of the quantile function while correcting for an endogeneity effect. The posterior inference for the proposed model is based on the Markov chain Monte Carlo method that requires no Metropolis-Hastings update. The approach is demonstrated using simulated and real data on the death rate in Japan during the inter-war period.

</details>

<details>

<summary>2016-08-29 07:21:20 - Relevant based structure learning for feature selection</summary>

- *Hadi Zare, Mojtaba Niazi*

- `1608.07934v1` - [abs](http://arxiv.org/abs/1608.07934v1) - [pdf](http://arxiv.org/pdf/1608.07934v1)

> Feature selection is an important task in many problems occurring in pattern recognition, bioinformatics, machine learning and data mining applications. The feature selection approach enables us to reduce the computation burden and the falling accuracy effect of dealing with huge number of features in typical learning problems. There is a variety of techniques for feature selection in supervised learning problems based on different selection metrics. In this paper, we propose a novel unified framework for feature selection built on the graphical models and information theoretic tools. The proposed approach exploits the structure learning among features to select more relevant and less redundant features to the predictive modeling problem according to a primary novel likelihood based criterion. In line with the selection of the optimal subset of features through the proposed method, it provides us the Bayesian network classifier without the additional cost of model training on the selected subset of features. The optimal properties of our method are established through empirical studies and computational complexity analysis. Furthermore the proposed approach is evaluated on a bunch of benchmark datasets based on the well-known classification algorithms. Extensive experiments confirm the significant improvement of the proposed approach compared to the earlier works.

</details>

<details>

<summary>2016-08-29 14:08:54 - Bayesian nonparametric forecasting of monotonic functional time series</summary>

- *Antonio Canale, Matteo Ruggiero*

- `1608.08056v1` - [abs](http://arxiv.org/abs/1608.08056v1) - [pdf](http://arxiv.org/pdf/1608.08056v1)

> We propose a Bayesian nonparametric approach to modelling and predicting a class of functional time series with application to energy markets, based on fully observed, noise-free functional data. Traders in such contexts conceive profitable strategies if they can anticipate the impact of their bidding actions on the aggregate demand and supply curves, which in turn need to be predicted reliably. Here we propose a simple Bayesian nonparametric method for predicting such curves, which take the form of monotonic bounded step functions. We borrow ideas from population genetics by defining a class of interacting particle systems to model the functional trajectory, and develop an implementation strategy which uses ideas from Markov chain Monte Carlo and approximate Bayesian computation techniques and allows to circumvent the intractability of the likelihood. Our approach shows great adaptation to the degree of smoothness of the curves and the volatility of the functional series, proves to be robust to an increase of the forecast horizon and yields an uncertainty quantification for the functional forecasts. We illustrate the model and discuss its performance with simulated datasets and on real data relative to the Italian natural gas market.

</details>

<details>

<summary>2016-08-29 14:39:58 - Bayesian Sequentially Monitored Multi-arm Experiments with Multiple Comparison Adjustments</summary>

- *Andrew W. Correia*

- `1608.08076v1` - [abs](http://arxiv.org/abs/1608.08076v1) - [pdf](http://arxiv.org/pdf/1608.08076v1)

> Randomized experiments play a major role in data-driven decision making across many different fields and disciplines. In medicine, for example, randomized controlled trials (RCTs) are the backbone of clinical trial methodology for testing the efficacy of new drugs and therapies versus existing treatments or placebo. In business and marketing, randomized experiments are typically referred to as A/B tests when there are only two arms, or variants, in the experiment, and as multivariate A/B tests when there are more than two arms. Typical applications of A/B tests include comparing the effectiveness of different ad campaigns, evaluating how people respond to different website layouts, or comparing different customer subpopulations to each other.   This paper focuses on multivariate A/B testing from a digital marketing perspective, and presents a method for the sequential monitoring of such experiments while accounting for the issue of multiple comparisons. In adapting and combining the methods of two previous works, the method presented herein is straightforward to implement using standard statistical software and performs quite well in various simulation studies, exhibiting better power and smaller average sample sizes than comparable methods.

</details>

<details>

<summary>2016-08-30 07:29:03 - Bayesian Adaptive Lasso with Variational Bayes for Variable Selection in High-dimensional Generalized Linear Mixed Models</summary>

- *Dao Thanh Tung, Minh-Ngoc Tran, Tran Manh Cuong*

- `1608.08347v1` - [abs](http://arxiv.org/abs/1608.08347v1) - [pdf](http://arxiv.org/pdf/1608.08347v1)

> This article describes a full Bayesian treatment for simultaneous fixed-effect selection and parameter estimation in high-dimensional generalized linear mixed models. The approach consists of using a Bayesian adaptive Lasso penalty for signal-level adaptive shrinkage and a fast Variational Bayes scheme for estimating the posterior mode of the coefficients. The proposed approach offers several advantages over the existing methods, for example, the adaptive shrinkage parameters are automatically incorporated, no Laplace approximation step is required to integrate out the random effects. The performance of our approach is illustrated on several simulated and real data examples. The algorithm is implemented in the R package glmmvb and is made available online.

</details>

<details>

<summary>2016-08-30 13:02:08 - Conjugacy properties of time-evolving Dirichlet and gamma random measures</summary>

- *Omiros Papaspiliopoulos, Matteo Ruggiero, Dario SpanÃ²*

- `1607.02896v2` - [abs](http://arxiv.org/abs/1607.02896v2) - [pdf](http://arxiv.org/pdf/1607.02896v2)

> We extend classic characterisations of posterior distributions under Dirichlet process and gamma random measures priors to a dynamic framework. We consider the problem of learning, from indirect observations, two families of time-dependent processes of interest in Bayesian nonparametrics: the first is a dependent Dirichlet process driven by a Fleming-Viot model, and the data are random samples from the process state at discrete times; the second is a collection of dependent gamma random measures driven by a Dawson-Watanabe model, and the data are collected according to a Poisson point process with intensity given by the process state at discrete times. Both driving processes are diffusions taking values in the space of discrete measures whose support varies with time, and are stationary and reversible with respect to Dirichlet and gamma priors respectively. A common methodology is developed to obtain in closed form the time-marginal posteriors given past and present data. These are shown to belong to classes of finite mixtures of Dirichlet processes and gamma random measures for the two models respectively, yielding conjugacy of these classes to the type of data we consider. We provide explicit results on the parameters of the mixture components and on the mixing weights, which are time-varying and drive the mixtures towards the respective priors in absence of further data. Explicit algorithms are provided to recursively compute the parameters of the mixtures. Our results are based on the projective properties of the signals and on certain duality properties of their projections.

</details>

<details>

<summary>2016-08-30 13:49:27 - The discriminative Kalman filter for nonlinear and non-Gaussian sequential Bayesian filtering</summary>

- *Michael C. Burkhart, David M. Brandman, Carlos E. Vargas-Irwin, Matthew T. Harrison*

- `1608.06622v2` - [abs](http://arxiv.org/abs/1608.06622v2) - [pdf](http://arxiv.org/pdf/1608.06622v2)

> The Kalman filter (KF) is used in a variety of applications for computing the posterior distribution of latent states in a state space model. The model requires a linear relationship between states and observations. Extensions to the Kalman filter have been proposed that incorporate linear approximations to nonlinear models, such as the extended Kalman filter (EKF) and the unscented Kalman filter (UKF). However, we argue that in cases where the dimensionality of observed variables greatly exceeds the dimensionality of state variables, a model for $p(\text{state}|\text{observation})$ proves both easier to learn and more accurate for latent space estimation. We derive and validate what we call the discriminative Kalman filter (DKF): a closed-form discriminative version of Bayesian filtering that readily incorporates off-the-shelf discriminative learning techniques. Further, we demonstrate that given mild assumptions, highly non-linear models for $p(\text{state}|\text{observation})$ can be specified. We motivate and validate on synthetic datasets and in neural decoding from non-human primates, showing substantial increases in decoding performance versus the standard Kalman filter.

</details>

<details>

<summary>2016-08-30 15:00:51 - High Dimensional and Banded Vector Autoregressions</summary>

- *Shaojun Guo, Yazhen Wang, Qiwei Yao*

- `1502.07831v2` - [abs](http://arxiv.org/abs/1502.07831v2) - [pdf](http://arxiv.org/pdf/1502.07831v2)

> We consider a class of vector autoregressive models with banded coefficient matrices. The setting represents a type of sparse structure for high-dimensional time series, though the implied autocovariance matrices are not banded. The structure is also practically meaningful when the order of component time series is arranged appropriately. The convergence rates for the estimated banded autoregressive coefficient matrices are established. We also propose a Bayesian information criterion for determining the width of the bands in the coefficient matrices, which is proved to be consistent. By exploring some approximate banded structure for the auto-covariance functions of banded vector autoregressive processes, consistent estimators for the auto-covariance matrices are constructed.

</details>

<details>

<summary>2016-08-30 22:00:19 - Online state and parameter estimation in Dynamic Generalised Linear Models</summary>

- *Rui Vieira, Darren J. Wilkinson*

- `1608.08666v1` - [abs](http://arxiv.org/abs/1608.08666v1) - [pdf](http://arxiv.org/pdf/1608.08666v1)

> Inference for streaming time-series is tightly coupled with the problem of Bayesian on-line state and parameter inference. In this paper we will introduce Dynamic Generalised Linear Models, the class of models often chosen to model continuous and discrete time-series data. We will look at three different approaches which allow on-line estimation and analyse the results when applied to different real world datasets related to inference for streaming data. Sufficient statistics based methods delay known problems, such as particle impoverishment, especially when applied to long running time-series, while providing reasonable parameter estimations when compared to exact methods, such as Particle Marginal Metropolis-Hastings. State and observation forecasts will also be analysed as a performance metric. By benchmarking against a "gold standard" (off-line) method, we can better understand the performance of on-line methods in challenging real-world scenarios.

</details>

<details>

<summary>2016-08-31 17:01:10 - Penalised complexity priors for stationary autoregressive processes</summary>

- *Sigrunn Holbek SÃ¸rbye, HÃ¥vard Rue*

- `1608.08941v1` - [abs](http://arxiv.org/abs/1608.08941v1) - [pdf](http://arxiv.org/pdf/1608.08941v1)

> The autoregressive process of order $p$ (AR($p$)) is a central model in time series analysis. A Bayesian approach requires the user to define a prior distribution for the coefficients of the AR($p$) model. Although it is easy to write down some prior, it is not at all obvious how to understand and interpret the prior, to ensure that it behaves according to the users prior knowledge. In this paper, we approach this problem using the recently developed ideas of penalised complexity (PC) priors. These priors have important properties like robustness and invariance to reparameterisations, as well as a clear interpretation. A PC prior is computed based on specific principles, where model component complexity is penalised in terms of deviation from simple base model formulations. In the AR(1) case, we discuss two natural base model choices, corresponding to either independence in time or no change in time. The latter case is illustrated in a survival model with possible time-dependent frailty. For higher-order processes, we propose a sequential approach, where the base model for AR($p$) is the corresponding AR($p-1$) model expressed using the partial autocorrelations. The properties of the new prior are compared with the reference prior in a simulation study.

</details>

<details>

<summary>2016-08-31 20:58:31 - Variable selection via penalized credible regions with Dirichlet-Laplace global-local shrinkage priors</summary>

- *Yan Zhang, Howard D. Bondell*

- `1602.01160v2` - [abs](http://arxiv.org/abs/1602.01160v2) - [pdf](http://arxiv.org/pdf/1602.01160v2)

> The method of Bayesian variable selection via penalized credible regions separates model fitting and variable selection. The idea is to search for the sparsest solution within the joint posterior credible regions. Although the approach was successful, it depended on the use of conjugate normal priors. More recently, improvements in the use of global-local shrinkage priors have been made for high-dimensional Bayesian variable selection. In this paper, we incorporate global-local priors into the credible region selection framework. The Dirichlet-Laplace (DL) prior is adapted to linear regression. Posterior consistency for the normal and DL priors are shown, along with variable selection consistency. We further introduce a new method to tune hyperparameters in prior distributions for linear regression. We propose to choose the hyperparameters to minimize a discrepancy between the induced distribution on R-square and a prespecified target distribution. Prior elicitation on R-square is more natural, particularly when there are a large number of predictor variables in which elicitation on that scale is not feasible. For a normal prior, these hyperparameters are available in closed form to minimize the Kullback-Leibler divergence between the distributions.

</details>


## 2016-09

<details>

<summary>2016-09-01 07:55:12 - The Bayesian SLOPE</summary>

- *Amir Sepehri*

- `1608.08968v2` - [abs](http://arxiv.org/abs/1608.08968v2) - [pdf](http://arxiv.org/pdf/1608.08968v2)

> The SLOPE estimates regression coefficients by minimizing a regularized residual sum of squares using a sorted-$\ell_1$-norm penalty. The SLOPE combines testing and estimation in regression problems. It exhibits suitable variable selection and prediction properties, as well as minimax optimality. This paper introduces the Bayesian SLOPE procedure for linear regression. The classical SLOPE estimate is the posterior mode in the normal regression problem with an appropriate prior on the coefficients. The Bayesian SLOPE considers the full Bayesian model and has the advantage of offering credible sets and standard error estimates for the parameters. Moreover, the hierarchical Bayesian framework allows for full Bayesian and empirical Bayes treatment of the penalty coefficients; whereas it is not clear how to choose these coefficients when using the SLOPE on a general design matrix. A direct characterization of the posterior is provided which suggests a Gibbs sampler that does not involve latent variables. An efficient hybrid Gibbs sampler for the Bayesian SLOPE is introduced. Point estimation using the posterior mean is highlighted, which automatically facilitates the Bayesian prediction of future observations. These are demonstrated on real and synthetic data.

</details>

<details>

<summary>2016-09-02 14:58:25 - Composable Models for Online Bayesian Analysis of Streaming Data</summary>

- *Jonathan Law, Darren Wilkinson*

- `1609.00635v1` - [abs](http://arxiv.org/abs/1609.00635v1) - [pdf](http://arxiv.org/pdf/1609.00635v1)

> Data is rapidly increasing in volume and velocity and the Internet of Things (IoT) is one important source of this data. The IoT is a collection of connected devices (things) which are constantly recording data from their surroundings using on-board sensors. These devices can record and stream data to the cloud at a very high rate, leading to high storage and analysis costs. In order to ameliorate these costs, we can analyse the data as it arrives in a stream to learn about the underlying process, perform interpolation and smoothing and make forecasts and predictions.   Conventional tools of state space modelling assume data on a fixed regular time grid. However, many sensors change their sampling frequency, sometimes adaptively, or get interrupted and re-started out of sync with the previous sampling grid, or just generate event data at irregular times. It is therefore desirable to model the system as a partially and irregularly observed Markov process which evolves in continuous time. Both the process and the observation model are potentially non-linear. Particle filters therefore represent the simplest approach to online analysis. A functional Scala library of composable continuous time Markov process models has been developed in order to model the wide variety of data captured in the IoT.

</details>

<details>

<summary>2016-09-03 15:32:04 - Towards Bayesian Deep Learning: A Framework and Some Existing Methods</summary>

- *Hao Wang, Dit-Yan Yeung*

- `1608.06884v2` - [abs](http://arxiv.org/abs/1608.06884v2) - [pdf](http://arxiv.org/pdf/1608.06884v2)

> While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This paper proposes a general framework for Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this paper, we also discuss the relationship and differences between Bayesian deep learning and other related topics like Bayesian treatment of neural networks.

</details>

<details>

<summary>2016-09-04 09:14:39 - Some recent developments in statistics for spatial point patterns</summary>

- *Jesper MÃ¸ller, Rasmus Waagepetersen*

- `1609.00908v1` - [abs](http://arxiv.org/abs/1609.00908v1) - [pdf](http://arxiv.org/pdf/1609.00908v1)

> This paper reviews developments in statistics for spatial point processes obtained within roughly the last decade. These developments include new classes of spatial point process models such as determinantal point processes, models incorporating both regularity and aggregation, and models where points are randomly distributed around latent geometric structures. Regarding parametric inference the main focus is on various types of estimating functions derived from so-called innovation measures. Optimality of such estimating functions is discussed as well as computational issues. Maximum likelihood inference for determinantal point processes and Bayesian inference are briefly considered too. Concerning non-parametric inference, we consider extensions of functional summary statistics to the case of inhomogeneous point processes as well as new approaches to simulation based inference.

</details>

<details>

<summary>2016-09-04 20:01:28 - A General Framework for Constrained Bayesian Optimization using Information-based Search</summary>

- *JosÃ© Miguel HernÃ¡ndez-Lobato, Michael A. Gelbart, Ryan P. Adams, Matthew W. Hoffman, Zoubin Ghahramani*

- `1511.09422v2` - [abs](http://arxiv.org/abs/1511.09422v2) - [pdf](http://arxiv.org/pdf/1511.09422v2)

> We present an information-theoretic framework for solving global black-box optimization problems that also have black-box constraints. Of particular interest to us is to efficiently solve problems with decoupled constraints, in which subsets of the objective and constraint functions may be evaluated independently. For example, when the objective is evaluated on a CPU and the constraints are evaluated independently on a GPU. These problems require an acquisition function that can be separated into the contributions of the individual function evaluations. We develop one such acquisition function and call it Predictive Entropy Search with Constraints (PESC). PESC is an approximation to the expected information gain criterion and it compares favorably to alternative approaches based on improvement in several synthetic and real-world problems. In addition to this, we consider problems with a mix of functions that are fast and slow to evaluate. These problems require balancing the amount of time spent in the meta-computation of PESC and in the actual evaluation of the target objective. We take a bounded rationality approach and develop partial update for PESC which trades off accuracy against speed. We then propose a method for adaptively switching between the partial and full updates for PESC. This allows us to interpolate between versions of PESC that are efficient in terms of function evaluations and those that are efficient in terms of wall-clock time. Overall, we demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization.

</details>

<details>

<summary>2016-09-05 07:36:58 - Towards optimal nonlinearities for sparse recovery using higher-order statistics</summary>

- *Steffen Limmer, SÅawomir StaÅczak*

- `1605.08201v2` - [abs](http://arxiv.org/abs/1605.08201v2) - [pdf](http://arxiv.org/pdf/1605.08201v2)

> We consider machine learning techniques to develop low-latency approximate solutions to a class of inverse problems. More precisely, we use a probabilistic approach for the problem of recovering sparse stochastic signals that are members of the $\ell_p$-balls. In this context, we analyze the Bayesian mean-square-error (MSE) for two types of estimators: (i) a linear estimator and (ii) a structured estimator composed of a linear operator followed by a Cartesian product of univariate nonlinear mappings. By construction, the complexity of the proposed nonlinear estimator is comparable to that of its linear counterpart since the nonlinear mapping can be implemented efficiently in hardware by means of look-up tables (LUTs). The proposed structure lends itself to neural networks and iterative shrinkage/thresholding-type algorithms restricted to a single iterate (e.g. due to imposed hardware or latency constraints). By resorting to an alternating minimization technique, we obtain a sequence of optimized linear operators and nonlinear mappings that converge in the MSE objective. The result is attractive for real-time applications where general iterative and convex optimization methods are infeasible.

</details>

<details>

<summary>2016-09-06 13:39:42 - Bayesian subcohort selection for longitudinal covariate measurements in follow-up studies</summary>

- *Jaakko Reinikainen, Juha Karvanen*

- `1609.01547v1` - [abs](http://arxiv.org/abs/1609.01547v1) - [pdf](http://arxiv.org/pdf/1609.01547v1)

> We consider planning longitudinal covariate measurements in follow-up studies where covariates are time-varying. We assume that the entire cohort cannot be selected for longitudinal measurements due to financial limitations and study how a subset of the cohort should be selected optimally in order to obtain precise estimates of covariate effects in a survival model. In our approach, the study will be designed sequentially utilizing the data collected in previous measurements of the individuals as prior information. We propose using a Bayesian optimality criterion in the subcohort selections, which is compared with simple random sampling using simulated and real follow-up data. This study extends previous results where optimal subcohort selection was studied with only one re-measurement and one covariate, to more realistic cases where several covariates and measurement points are allowed. Our results support the conclusion that the precision of the estimates can be clearly improved by optimal design.

</details>

<details>

<summary>2016-09-06 17:41:50 - The Waves and the Sigmas (To Say Nothing of the 750 GeV Mirage)</summary>

- *Giulio D'Agostini*

- `1609.01668v1` - [abs](http://arxiv.org/abs/1609.01668v1) - [pdf](http://arxiv.org/pdf/1609.01668v1)

> This paper shows how p-values do not only create, as well known, wrong expectations in the case of flukes, but they might also dramatically diminish the `significance' of most likely genuine signals. As real life examples, the 2015 first detections of gravitational waves are discussed. The March 2016 statement of the American Statistical Association, warning scientists about interpretation and misuse of p-values, is also reminded and commented. (The paper is complemented with some remarks on past, recent and future claims of discoveries based on sigmas from Particles Physics.)

</details>

<details>

<summary>2016-09-07 00:30:16 - A General Method for Robust Bayesian Modeling</summary>

- *Chong Wang, David M. Blei*

- `1510.05078v3` - [abs](http://arxiv.org/abs/1510.05078v3) - [pdf](http://arxiv.org/pdf/1510.05078v3)

> Robust Bayesian models are appealing alternatives to standard models, providing protection from data that contains outliers or other departures from the model assumptions. Historically, robust models were mostly developed on a case-by-case basis; examples include robust linear regression, robust mixture models, and bursty topic models. In this paper we develop a general approach to robust Bayesian modeling. We show how to turn an existing Bayesian model into a robust model, and then develop a generic strategy for computing with it. We use our method to study robust variants of several models, including linear regression, Poisson regression, logistic regression, and probabilistic topic models. We discuss the connections between our methods and existing approaches, especially empirical Bayes and James-Stein estimation.

</details>

<details>

<summary>2016-09-07 20:46:00 - Bayes Factors via Savage-Dickey Supermodels</summary>

- *A. Mootoovaloo, Bruce A. Bassett, M. Kunz*

- `1609.02186v1` - [abs](http://arxiv.org/abs/1609.02186v1) - [pdf](http://arxiv.org/pdf/1609.02186v1)

> We outline a new method to compute the Bayes Factor for model selection which bypasses the Bayesian Evidence. Our method combines multiple models into a single, nested, Supermodel using one or more hyperparameters. Since the models are now nested the Bayes Factors between the models can be efficiently computed using the Savage-Dickey Density Ratio (SDDR). In this way model selection becomes a problem of parameter estimation. We consider two ways of constructing the supermodel in detail: one based on combined models, and a second based on combined likelihoods. We report on these two approaches for a Gaussian linear model for which the Bayesian evidence can be calculated analytically and a toy nonlinear problem. Unlike the combined model approach, where a standard Monte Carlo Markov Chain (MCMC) struggles, the combined-likelihood approach fares much better in providing a reliable estimate of the log-Bayes Factor. This scheme potentially opens the way to computationally efficient ways to compute Bayes Factors in high dimensions that exploit the good scaling properties of MCMC, as compared to methods such as nested sampling that fail for high dimensions.

</details>

<details>

<summary>2016-09-09 08:35:48 - Bayesian Quantile-Based Joint Modelling of Repeated Measurement and Time-to-Event data, with an Application to Lung Function Decline and Time to Infection in Patients with Cystic Fibrosis</summary>

- *Elisabeth Waldmann, David Taylor-Robinson*

- `1609.02696v1` - [abs](http://arxiv.org/abs/1609.02696v1) - [pdf](http://arxiv.org/pdf/1609.02696v1)

> Background: The most widely used approach to joint modelling of repeated measurement and time to event data is to combine a linear Gaussian random effects model for the repeated measurements with a log-Gaussian frailty model for the time-to-event outcome, linking the two through some form of correlation structure between the random effects and the log-frailty. In this approach, covariates are assumed to affect the mean response profile of the repeated measurement data. Objectives: Some applications raise substantive questions that cannot be captured by this structure. For example, an important question in cystic fibrosis (CF) research is to understand the impact of a patient's lung function trajectory on their risk of acquiring a variety of infections, and how this varies at different quantiles of the lung function distribution. Methods: Motivated by this question, we develop a joint quantile modelling framework in this paper with an associated Markov Chain Monte Carlo algorithm for Bayesian inference. Results: The translation from the common joint model towards quantile regression succeeds and is applied to CF data from the United Kingdom. The method helps detecting an overall difference in the relation between lung function decline and onset of infection in the different quantiles. Conclusions: Joint modelling without taking into account the special heteroscedastic structure is not sufficient in certain research question and the extensions towards models beyond the mean is necessary.

</details>

<details>

<summary>2016-09-09 08:42:12 - Efficient batch-sequential Bayesian optimization with moments of truncated Gaussian vectors</summary>

- *SÃ©bastien Marmin, ClÃ©ment Chevalier, David Ginsbourger*

- `1609.02700v1` - [abs](http://arxiv.org/abs/1609.02700v1) - [pdf](http://arxiv.org/pdf/1609.02700v1)

> We deal with the efficient parallelization of Bayesian global optimization algorithms, and more specifically of those based on the expected improvement criterion and its variants. A closed form formula relying on multivariate Gaussian cumulative distribution functions is established for a generalized version of the multipoint expected improvement criterion. In turn, the latter relies on intermediate results that could be of independent interest concerning moments of truncated Gaussian vectors. The obtained expansion of the criterion enables studying its differentiability with respect to point batches and calculating the corresponding gradient in closed form. Furthermore , we derive fast numerical approximations of this gradient and propose efficient batch optimization strategies. Numerical experiments illustrate that the proposed approaches enable computational savings of between one and two order of magnitudes, hence enabling derivative-based batch-sequential acquisition function maximization to become a practically implementable and efficient standard.

</details>

<details>

<summary>2016-09-09 20:44:04 - Hierarchical Bayesian Level Set Inversion</summary>

- *Matthew M. Dunlop, Marco A. Iglesias, Andrew M. Stuart*

- `1601.03605v2` - [abs](http://arxiv.org/abs/1601.03605v2) - [pdf](http://arxiv.org/pdf/1601.03605v2)

> The level set approach has proven widely successful in the study of inverse problems for interfaces, since its systematic development in the 1990s. Recently it has been employed in the context of Bayesian inversion, allowing for the quantification of uncertainty within the reconstruction of interfaces. However the Bayesian approach is very sensitive to the length and amplitude scales in the prior probabilistic model. This paper demonstrates how the scale-sensitivity can be circumvented by means of a hierarchical approach, using a single scalar parameter. Together with careful consideration of the development of algorithms which encode probability measure equivalences as the hierarchical parameter is varied, this leads to well-defined Gibbs based MCMC methods found by alternating Metropolis-Hastings updates of the level set function and the hierarchical parameter. These methods demonstrably outperform non-hierarchical Bayesian level set methods.

</details>

<details>

<summary>2016-09-09 21:36:43 - Bayesian Quantile Regression Using Random B-spline Series Prior</summary>

- *Priyam Das, Subhashis Ghoshal*

- `1609.02950v1` - [abs](http://arxiv.org/abs/1609.02950v1) - [pdf](http://arxiv.org/pdf/1609.02950v1)

> We consider a Bayesian method for simultaneous quantile regression on a real variable. By monotone transformation, we can make both the response variable and the predictor variable take values in the unit interval. A representation of quantile function is given by a convex combination of two monotone increasing functions $\xi_1$ and $\xi_2$ not depending on the prediction variables. In a Bayesian approach, a prior is put on quantile functions by putting prior distributions on $\xi_1$ and $\xi_2$. The monotonicity constraint on the curves $\xi_1$ and $\xi_2$ are obtained through a spline basis expansion with coefficients increasing and lying in the unit interval. We put a Dirichlet prior distribution on the spacings of the coefficient vector. A finite random series based on splines obeys the shape restrictions. We compare our approach with a Bayesian method using Gaussian process prior through an extensive simulation study and some other Bayesian approaches proposed in the literature. An application to a data on hurricane activities in the Atlantic region is given. We also apply our method on region-wise population data of USA for the period 1985--2010.

</details>

<details>

<summary>2016-09-12 12:32:25 - Using phase II data for the analysis of phase III studies: an application in rare diseases</summary>

- *Simon Wandel, Beat Neuenschwander, Tim Friede, Christian RÃ¶ver*

- `1609.03367v1` - [abs](http://arxiv.org/abs/1609.03367v1) - [pdf](http://arxiv.org/pdf/1609.03367v1)

> Clinical research and drug development in orphan diseases is challenging, since large-scale randomized studies are difficult to conduct. Formally synthesizing the evidence is therefore of great value, yet this is rarely done in the drug approval process. Phase III designs that make better use of phase II data can facilitate drug development in orphan diseases.   A Bayesian meta-analytic approach is used to inform the phase III study with phase II data. It is particularly attractive, since uncertainty of between-trial heterogeneity can be dealt with probabilistically, which is critical if the number of studies is small. Furthermore, it allows quantifying and discounting the phase II data through the predictive distribution relevant for phase III. A phase III design is proposed which uses the phase II data and considers approval based on a phase III interim analysis. The design is illustrated with a non-inferiority case study from an FDA approval in herpetic keratitis (an orphan disease). Design operating characteristics are compared to those of a traditional design, which ignores the phase II data.   An analysis of the phase II data reveals good but insufficient evidence for non-inferiority, highlighting the need for a phase III study. For the phase III study supported by phase II data, the interim analysis is based on half of the patients. For this design, the meta-analytic interim results are conclusive and would justify approval. In contrast, based on the phase III data only, interim results are inconclusive and would require further evidence.   To accelerate drug development for orphan diseases, innovative study designs and appropriate methodology are needed. Taking advantage of randomized phase II data when analyzing phase III studies looks promising because the evidence from phase II supports informed decision making. The implementation of the Bayesian design is straightforward.

</details>

<details>

<summary>2016-09-12 17:48:38 - Adaptive matching pursuit for sparse signal recovery</summary>

- *Tiep H. Vu, Hojjat S. Mousavi, Vishal Monga*

- `1610.08495v1` - [abs](http://arxiv.org/abs/1610.08495v1) - [pdf](http://arxiv.org/pdf/1610.08495v1)

> Spike and Slab priors have been of much recent interest in signal processing as a means of inducing sparsity in Bayesian inference. Applications domains that benefit from the use of these priors include sparse recovery, regression and classification. It is well-known that solving for the sparse coefficient vector to maximize these priors results in a hard non-convex and mixed integer programming problem. Most existing solutions to this optimization problem either involve simplifying assumptions/relaxations or are computationally expensive. We propose a new greedy and adaptive matching pursuit (AMP) algorithm to directly solve this hard problem. Essentially, in each step of the algorithm, the set of active elements would be updated by either adding or removing one index, whichever results in better improvement. In addition, the intermediate steps of the algorithm are calculated via an inexpensive Cholesky decomposition which makes the algorithm much faster. Results on simulated data sets as well as real-world image recovery challenges confirm the benefits of the proposed AMP, particularly in providing a superior cost-quality trade-off over existing alternatives.

</details>

<details>

<summary>2016-09-12 18:33:50 - Optimal Encoding and Decoding for Point Process Observations: an Approximate Closed-Form Filter</summary>

- *Yuval Harel, Ron Meir, Manfred Opper*

- `1609.03519v1` - [abs](http://arxiv.org/abs/1609.03519v1) - [pdf](http://arxiv.org/pdf/1609.03519v1)

> The process of dynamic state estimation (filtering) based on point process observations is in general intractable. Numerical sampling techniques are often practically useful, but lead to limited conceptual insight about optimal encoding/decoding strategies, which are of significant relevance to Computational Neuroscience. We develop an analytically tractable Bayesian approximation to optimal filtering based on point process observations, which allows us to introduce distributional assumptions about sensor properties, that greatly facilitate the analysis of optimal encoding in situations deviating from common assumptions of uniform coding. Numerical comparison with particle filtering demonstrate the quality of the approximation. The analytic framework leads to insights which are difficult to obtain from numerical algorithms, and is consistent with biological observations about the distribution of sensory cells' tuning curve centers.

</details>

<details>

<summary>2016-09-12 18:34:18 - Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC</summary>

- *Aki Vehtari, Andrew Gelman, Jonah Gabry*

- `1507.04544v5` - [abs](http://arxiv.org/abs/1507.04544v5) - [pdf](http://arxiv.org/pdf/1507.04544v5)

> Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called 'loo' and demonstrate using models fit with the Bayesian inference package Stan.

</details>

<details>

<summary>2016-09-13 17:06:05 - Relabelling in Bayesian mixture models by pivotal units</summary>

- *Leonardo Egidi, Roberta PappadÃ , Francesco Pauli, Nicola Torelli*

- `1501.05478v2` - [abs](http://arxiv.org/abs/1501.05478v2) - [pdf](http://arxiv.org/pdf/1501.05478v2)

> In this paper a simple procedure to deal with label switching when exploring complex posterior distributions by MCMC algorithms is proposed. Although it cannot be generalized to any situation, it may be handy in many applications because of its simplicity and very low computational burden. A possible area where it proves to be useful is when deriving a sample for the posterior distribution arising from finite mixture models when no simple or rational ordering between the components is available.

</details>

<details>

<summary>2016-09-14 14:27:32 - Gray-box inference for structured Gaussian process models</summary>

- *Pietro Galliani, Amir Dezfouli, Edwin V. Bonilla, Novi Quadrianto*

- `1609.04289v1` - [abs](http://arxiv.org/abs/1609.04289v1) - [pdf](http://arxiv.org/pdf/1609.04289v1)

> We develop an automated variational inference method for Bayesian structured prediction problems with Gaussian process (GP) priors and linear-chain likelihoods. Our approach does not need to know the details of the structured likelihood model and can scale up to a large number of observations. Furthermore, we show that the required expected likelihood term and its gradients in the variational objective (ELBO) can be estimated efficiently by using expectations over very low-dimensional Gaussian distributions. Optimization of the ELBO is fully parallelizable over sequences and amenable to stochastic optimization, which we use along with control variate techniques and state-of-the-art incremental optimization to make our framework useful in practice. Results on a set of natural language processing tasks show that our method can be as good as (and sometimes better than) hard-coded approaches including SVM-struct and CRFs, and overcomes the scalability limitations of previous inference algorithms based on sampling. Overall, this is a fundamental step to developing automated inference methods for Bayesian structured prediction.

</details>

<details>

<summary>2016-09-14 19:11:03 - Probabilistic Population Projections for Countries with Generalized HIV/AIDS Epidemics</summary>

- *David J. Sharrow, Jessica Godwin, Yanjun He, Samuel J. Clark, Adrian E. Raftery*

- `1609.04383v1` - [abs](http://arxiv.org/abs/1609.04383v1) - [pdf](http://arxiv.org/pdf/1609.04383v1)

> The United Nations (UN) issued official probabilistic population projections for all countries to 2100 in July 2015. This was done by simulating future levels of total fertility and life expectancy from Bayesian hierarchical models, and combining the results using a standard cohort-component projection method. The 40 countries with generalized HIV/AIDS epidemics were treated differently from others, in that the projections used the highly multistate Spectrum/EPP model, a complex 15-compartment model that was designed for short-term projections of quantities relevant to policy for the epidemic. Here we propose a simpler approach that is more compatible with the existing UN probabilistic projection methodology for other countries. Changes in life expectancy are projected probabilistically using a simple time series regression model on current life expectancy, HIV prevalence and ART coverage. These are then converted to age- and sex-specific mortality rates using a new family of model life tables designed for countries with HIV/AIDS epidemics that reproduces the characteristic hump in middle adult mortality. These are then input to the standard cohort-component method, as for other countries. The method performed well in an out-of-sample cross-validation experiment. It gives similar population projections to Spectrum/EPP in the short run, while being simpler and avoiding multistate modeling.

</details>

<details>

<summary>2016-09-14 20:34:26 - Bayesian Reinforcement Learning: A Survey</summary>

- *Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar*

- `1609.04436v1` - [abs](http://arxiv.org/abs/1609.04436v1) - [pdf](http://arxiv.org/pdf/1609.04436v1)

> Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.

</details>

<details>

<summary>2016-09-15 10:36:36 - Multilevel Monte Carlo for Scalable Bayesian Computations</summary>

- *Mike Giles, Tigran Nagapetyan, Lukasz Szpruch, Sebastian Vollmer, Konstantinos Zygalakis*

- `1609.06144v1` - [abs](http://arxiv.org/abs/1609.06144v1) - [pdf](http://arxiv.org/pdf/1609.06144v1)

> Markov chain Monte Carlo (MCMC) algorithms are ubiquitous in Bayesian computations. However, they need to access the full data set in order to evaluate the posterior density at every step of the algorithm. This results in a great computational burden in big data applications. In contrast to MCMC methods, Stochastic Gradient MCMC (SGMCMC) algorithms such as the Stochastic Gradient Langevin Dynamics (SGLD) only require access to a batch of the data set at every step. This drastically improves the computational performance and scales well to large data sets. However, the difficulty with SGMCMC algorithms comes from the sensitivity to its parameters which are notoriously difficult to tune. Moreover, the Root Mean Square Error (RMSE) scales as $\mathcal{O}(c^{-\frac{1}{3}})$ as opposed to standard MCMC $\mathcal{O}(c^{-\frac{1}{2}})$ where $c$ is the computational cost.   We introduce a new class of Multilevel Stochastic Gradient Markov chain Monte Carlo algorithms that are able to mitigate the problem of tuning the step size and more importantly of recovering the $\mathcal{O}(c^{-\frac{1}{2}})$ convergence of standard Markov Chain Monte Carlo methods without the need to introduce Metropolis-Hasting steps. A further advantage of this new class of algorithms is that it can easily be parallelised over a heterogeneous computer architecture. We illustrate our methodology using Bayesian logistic regression and provide numerical evidence that for a prescribed relative RMSE the computational cost is sublinear in the number of data items.

</details>

<details>

<summary>2016-09-15 19:37:03 - Sequential Bayesian Analysis of Multivariate Count Data</summary>

- *Tevfik Aktekin, Nicholas G. Polson, Refik Soyer*

- `1602.01445v2` - [abs](http://arxiv.org/abs/1602.01445v2) - [pdf](http://arxiv.org/pdf/1602.01445v2)

> We develop a new class of dynamic multivariate Poisson count models that allow for fast online updating and we refer to these models as multivariate Poisson-scaled beta (MPSB). The MPSB model allows for serial dependence in the counts as well as dependence across multiple series with a random common environment. Other notable features include analytic forms for state propagation and predictive likelihood densities. Sequential updating occurs through the updating of the sufficient statistics for static model parameters, leading to a fully adapted particle learning algorithm and a new class of predictive likelihoods and marginal distributions which we refer to as the (dynamic) multivariate confluent hyper-geometric negative binomial distribution (MCHG-NB) and the the dynamic multivariate negative binomial (DMNB) distribution. To illustrate our methodology, we use various simulation studies and count data on weekly non-durable goods consumer demand.

</details>

<details>

<summary>2016-09-15 21:19:51 - A Bayesian approach to denoising of single-photon binary images</summary>

- *Yoann Altmann, Reuben Aspden, Miles Padgett, Steve McLaughlin*

- `1609.04862v1` - [abs](http://arxiv.org/abs/1609.04862v1) - [pdf](http://arxiv.org/pdf/1609.04862v1)

> This paper discusses new methods for processing images in the photon-limited regime where the number of photons per pixel is binary. We present a new Bayesian denoising method for binary, single-photon images. Each pixel measurement is assumed to follow a Bernoulli distribution whose mean is related by a nonlinear function to the underlying intensity value to be recovered. Adopting a Bayesian approach, we assign the unknown intensity field a smoothness promoting spatial and potentially temporal prior while enforcing the positivity of the intensity. A stochastic simulation method is then used to sample the resulting joint posterior distribution and estimate the unknown intensity, as well as the regularization parameters. We show that this new unsupervised denoising method can also be used to analyze images corrupted by Poisson noise. The proposed algorithm is compared to state-of-the art denoising techniques dedicated to photon-limited images using synthetic and real single-photon measurements. The results presented illustrate the potential benefits of the proposed methodology for photon-limited imaging, in particular with non photonnumber resolving detectors.

</details>

<details>

<summary>2016-09-18 15:14:38 - A New Bayesian Test to test for the Intractability-Countering Hypothesis</summary>

- *Dalia Chakrabarty*

- `1304.5982v3` - [abs](http://arxiv.org/abs/1304.5982v3) - [pdf](http://arxiv.org/pdf/1304.5982v3)

> We present a new test of hypothesis in which we seek the probability of the null conditioned on the data, where the null is a simplification undertaken to counter the intractability of the more complex model, that the simpler null model is nested within. With the more complex model rendered intractable, the null model uses a simplifying assumption that capacitates the learning of an unknown parameter vector given the data. Bayes factors are shown to be known only up to a ratio of unknown data-dependent constants--a problem that cannot be cured using prescriptions similar to those suggested to solve the problem caused to Bayes factor computation, by non-informative priors. Thus, a new test is needed in which we can circumvent Bayes factor computation. In this test, we undertake generation of data from the model in which the null hypothesis is true and can achieve support in the measured data for the null by comparing the marginalised posterior of the model parameter given the measured data, to that given such generated data. However, such a ratio of marginalised posteriors can confound interpretation of comparison of support in one measured data for a null, with that in another data set for a different null. Given an application in which such comparison is undertaken, we alternatively define support in a measured data set for a null by identifying the model parameters that are less consistent with the measured data than is minimally possible given the generated data, and realising that the higher the number of such parameter values, less is the support in the measured data for the null. Then, the probability of the null conditional on the data is given within an MCMC-based scheme, by marginalising the posterior given the measured data, over parameter values that are as, or more consistent with the measured data, than with the generated data.

</details>

<details>

<summary>2016-09-19 13:41:35 - Bayesian Computing with INLA: A Review</summary>

- *HÃ¥vard Rue, Andrea Riebler, Sigrunn H. SÃ¸rbye, Janine B. Illian, Daniel P. Simpson, Finn K. Lindgren*

- `1604.00860v2` - [abs](http://arxiv.org/abs/1604.00860v2) - [pdf](http://arxiv.org/pdf/1604.00860v2)

> The key operation in Bayesian inference, is to compute high-dimensional integrals. An old approximate technique is the Laplace method or approximation, which dates back to Pierre- Simon Laplace (1774). This simple idea approximates the integrand with a second order Taylor expansion around the mode and computes the integral analytically. By developing a nested version of this classical idea, combined with modern numerical techniques for sparse matrices, we obtain the approach of Integrated Nested Laplace Approximations (INLA) to do approximate Bayesian inference for latent Gaussian models (LGMs). LGMs represent an important model-abstraction for Bayesian inference and include a large proportion of the statistical models used today. In this review, we will discuss the reasons for the success of the INLA-approach, the R-INLA package, why it is so accurate, why the approximations are very quick to compute and why LGMs make such a useful concept for Bayesian computing.

</details>

<details>

<summary>2016-09-19 16:29:37 - Airborne contaminant source estimation using a finite-volume forward solver coupled with a Bayesian inversion approach</summary>

- *Bamdad Hosseini, John M. Stockie*

- `1607.03518v2` - [abs](http://arxiv.org/abs/1607.03518v2) - [pdf](http://arxiv.org/pdf/1607.03518v2)

> We propose a numerical algorithm for solving the atmospheric dispersion problem with elevated point sources and ground-level deposition. The problem is modelled by the 3D advection-diffusion equation with delta-distribution source terms, as well as height-dependent advection speed and diffusion coefficients. We construct a finite volume scheme using a splitting approach in which the Clawpack software package is used as the advection solver and an implicit time discretization is proposed for the diffusion terms. The algorithm is then applied to an actual industrial scenario involving emissions of airborne particulates from a zinc smelter using actual wind measurements. We also address various practical considerations such as choosing appropriate methods for regularizing noisy wind data and quantifying sensitivity of the model to parameter uncertainty. Afterwards, we use the algorithm within a Bayesian framework for estimating emission rates of zinc from multiple sources over the industrial site. We compare our finite volume solver with a Gaussian plume solver within the Bayesian framework and demonstrate that the finite volume solver results in tighter uncertainty bounds on the estimated emission rates.

</details>

<details>

<summary>2016-09-19 19:44:06 - Online and Distributed learning of Gaussian mixture models by Bayesian Moment Matching</summary>

- *Priyank Jaini, Pascal Poupart*

- `1609.05881v1` - [abs](http://arxiv.org/abs/1609.05881v1) - [pdf](http://arxiv.org/pdf/1609.05881v1)

> The Gaussian mixture model is a classic technique for clustering and data modeling that is used in numerous applications. With the rise of big data, there is a need for parameter estimation techniques that can handle streaming data and distribute the computation over several processors. While online variants of the Expectation Maximization (EM) algorithm exist, their data efficiency is reduced by a stochastic approximation of the E-step and it is not clear how to distribute the computation over multiple processors. We propose a Bayesian learning technique that lends itself naturally to online and distributed computation. Since the Bayesian posterior is not tractable, we project it onto a family of tractable distributions after each observation by matching a set of sufficient moments. This Bayesian moment matching technique compares favorably to online EM in terms of time and accuracy on a set of data modeling benchmarks.

</details>

<details>

<summary>2016-09-19 22:30:36 - Conformalized Kernel Ridge Regression</summary>

- *Evgeny Burnaev, Ivan Nazarov*

- `1609.05959v1` - [abs](http://arxiv.org/abs/1609.05959v1) - [pdf](http://arxiv.org/pdf/1609.05959v1)

> General predictive models do not provide a measure of confidence in predictions without Bayesian assumptions. A way to circumvent potential restrictions is to use conformal methods for constructing non-parametric confidence regions, that offer guarantees regarding validity. In this paper we provide a detailed description of a computationally efficient conformal procedure for Kernel Ridge Regression (KRR), and conduct a comparative numerical study to see how well conformal regions perform against the Bayesian confidence sets. The results suggest that conformalized KRR can yield predictive confidence regions with specified coverage rate, which is essential in constructing anomaly detection systems based on predictive models.

</details>

<details>

<summary>2016-09-20 06:34:27 - Bayesian Variable Selection for Ultrahigh-dimensional Sparse Linear Models</summary>

- *Minerva Mukhopadhyay, Subhajit Dutta*

- `1609.06031v1` - [abs](http://arxiv.org/abs/1609.06031v1) - [pdf](http://arxiv.org/pdf/1609.06031v1)

> We propose a Bayesian variable selection procedure for ultrahigh-dimensional linear regression models. The number of regressors involved in regression, $p_n$, is allowed to grow exponentially with $n$. Assuming the true model to be sparse, in the sense that only a small number of regressors contribute to this model, we propose a set of priors suitable for this regime. The model selection procedure based on the proposed set of priors is shown to be variable selection consistent when all the $2^{p_n}$ models are considered. In the ultrahigh-dimensional setting, selection of the true model among all the $2^{p_n}$ possible ones involves prohibitive computation. To cope with this, we present a two-step model selection algorithm based on screening and Gibbs sampling. The first step of screening discards a large set of unimportant covariates, and retains a smaller set containing all the active covariates with probability tending to one. In the next step, we search for the best model among the covariates obtained in the screening step. This procedure is computationally quite fast, simple and intuitive. We demonstrate competitive performance of the proposed algorithm for a variety of simulated and real data sets when compared with several frequentist, as well as Bayesian methods.

</details>

<details>

<summary>2016-09-20 15:02:17 - Bayesian Nonparametric Modeling for Multivariate Ordinal Regression</summary>

- *Maria DeYoreo, Athanasios Kottas*

- `1408.1027v3` - [abs](http://arxiv.org/abs/1408.1027v3) - [pdf](http://arxiv.org/pdf/1408.1027v3)

> Univariate or multivariate ordinal responses are often assumed to arise from a latent continuous parametric distribution, with covariate effects which enter linearly. We introduce a Bayesian nonparametric modeling approach for univariate and multivariate ordinal regression, which is based on mixture modeling for the joint distribution of latent responses and covariates. The modeling framework enables highly flexible inference for ordinal regression relationships, avoiding assumptions of linearity or additivity in the covariate effects. In standard parametric ordinal regression models, computational challenges arise from identifiability constraints and estimation of parameters requiring nonstandard inferential techniques. A key feature of the nonparametric model is that it achieves inferential flexibility, while avoiding these difficulties. In particular, we establish full support of the nonparametric mixture model under fixed cut-off points that relate through discretization the latent continuous responses with the ordinal responses. The practical utility of the modeling approach is illustrated through application to two data sets from econometrics, an example involving regression relationships for ozone concentration, and a multirater agreement problem.

</details>

<details>

<summary>2016-09-20 16:15:21 - Bayesian Variable Selection for Globally Sparse Probabilistic PCA</summary>

- *Charles Bouveyron, Pierre Latouche, Pierre-Alexandre Mattei*

- `1605.05918v2` - [abs](http://arxiv.org/abs/1605.05918v2) - [pdf](http://arxiv.org/pdf/1605.05918v2)

> Sparse versions of principal component analysis (PCA) have imposed themselves as simple, yet powerful ways of selecting relevant features of high-dimensional data in an unsupervised manner. However, when several sparse principal components are computed, the interpretation of the selected variables is difficult since each axis has its own sparsity pattern and has to be interpreted separately. To overcome this drawback, we propose a Bayesian procedure called globally sparse probabilistic PCA (GSPPCA) that allows to obtain several sparse components with the same sparsity pattern. This allows the practitioner to identify the original variables which are relevant to describe the data. To this end, using Roweis' probabilistic interpretation of PCA and a Gaussian prior on the loading matrix, we provide the first exact computation of the marginal likelihood of a Bayesian PCA model. To avoid the drawbacks of discrete model selection, a simple relaxation of this framework is presented. It allows to find a path of models using a variational expectation-maximization algorithm. The exact marginal likelihood is then maximized over this path. This approach is illustrated on real and synthetic data sets. In particular, using unlabeled microarray data, GSPPCA infers much more relevant gene subsets than traditional sparse PCA algorithms.

</details>

<details>

<summary>2016-09-21 00:36:45 - Dynamic social networks based on movement</summary>

- *Henry R. Scharf, Mevin B. Hooten, Bailey K. Fosdick, Devin S. Johnson, Josh M. London, John W. Durban*

- `1512.07607v2` - [abs](http://arxiv.org/abs/1512.07607v2) - [pdf](http://arxiv.org/pdf/1512.07607v2)

> Network modeling techniques provide a means for quantifying social structure in populations of individuals. Data used to define social connectivity are often expensive to collect and based on case-specific, ad hoc criteria. Moreover, in applications involving animal social networks, collection of these data is often opportunistic and can be invasive. Frequently, the social network of interest for a given population is closely related to the way individuals move. Thus telemetry data, which are minimally-invasive and relatively inexpensive to collect, present an alternative source of information. We develop a framework for using telemetry data to infer social relationships among animals. To achieve this, we propose a Bayesian hierarchical model with an underlying dynamic social network controlling movement of individuals via two mechanisms: an attractive effect, and an aligning effect. We demonstrate the model and its ability to accurately identify complex social behavior in simulation, and apply our model to telemetry data arising from killer whales. Using auxiliary information about the study population, we investigate model validity and find the inferred dynamic social network is consistent with killer whale ecology and expert knowledge.

</details>

<details>

<summary>2016-09-21 02:04:40 - Global-Local Mixtures</summary>

- *Anindya Bhadra, Jyotishka Datta, Nicholas G. Polson, Brandon Willard*

- `1604.07487v2` - [abs](http://arxiv.org/abs/1604.07487v2) - [pdf](http://arxiv.org/pdf/1604.07487v2)

> Global-local mixtures are derived from the Cauchy-Schlomilch and Liouville integral transformation identities. We characterize well-known normal-scale mixture distributions including the Laplace or lasso, logit and quantile as well as new global-local mixtures. We also apply our methodology to convolutions that commonly arise in Bayesian inference. Finally, we conclude with a conjecture concerning bridge and uniform correlation mixtures.

</details>

<details>

<summary>2016-09-21 05:41:08 - Gaussian Process Pseudo-Likelihood Models for Sequence Labeling</summary>

- *P. K. Srijith, P. Balamurugan, Shirish Shevade*

- `1412.7868v2` - [abs](http://arxiv.org/abs/1412.7868v2) - [pdf](http://arxiv.org/pdf/1412.7868v2)

> Several machine learning problems arising in natural language processing can be modeled as a sequence labeling problem. We provide Gaussian process models based on pseudo-likelihood approximation to perform sequence labeling. Gaussian processes (GPs) provide a Bayesian approach to learning in a kernel based framework. The pseudo-likelihood model enables one to capture long range dependencies among the output components of the sequence without becoming computationally intractable. We use an efficient variational Gaussian approximation method to perform inference in the proposed model. We also provide an iterative algorithm which can effectively make use of the information from the neighboring labels to perform prediction. The ability to capture long range dependencies makes the proposed approach useful for a wide range of sequence labeling problems. Numerical experiments on some sequence labeling data sets demonstrate the usefulness of the proposed approach.

</details>

<details>

<summary>2016-09-22 00:10:16 - Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor Processes</summary>

- *Kar Wai Lim, Wray Buntine, Changyou Chen, Lan Du*

- `1609.06783v1` - [abs](http://arxiv.org/abs/1609.06783v1) - [pdf](http://arxiv.org/pdf/1609.06783v1)

> The Dirichlet process and its extension, the Pitman-Yor process, are stochastic processes that take probability distributions as a parameter. These processes can be stacked up to form a hierarchical nonparametric Bayesian model. In this article, we present efficient methods for the use of these processes in this hierarchical context, and apply them to latent variable models for text analytics. In particular, we propose a general framework for designing these Bayesian models, which are called topic models in the computer science community. We then propose a specific nonparametric Bayesian topic model for modelling text from social media. We focus on tweets (posts on Twitter) in this article due to their ease of access. We find that our nonparametric model performs better than existing parametric models in both goodness of fit and real world applications.

</details>

<details>

<summary>2016-09-22 13:34:47 - Some comments about "Penalising model component complexity" by Simpson et al. (2017)</summary>

- *Christian P. Robert, Judith Rousseau*

- `1609.06968v1` - [abs](http://arxiv.org/abs/1609.06968v1) - [pdf](http://arxiv.org/pdf/1609.06968v1)

> This note discusses the paper "Penalising model component complexity" by Simpson et al. (2017). While we acknowledge the highly novel approach to prior construction and commend the authors for setting new-encompassing principles that will Bayesian modelling, and while we perceive the potential connection with other branches of the literature, we remain uncertain as to what extent the principles exposed in the paper can be developed outside specific models, given their lack of precision. The very notions of model component, base model, overfitting prior are for instance conceptual rather than mathematical and we thus fear the concept of penalised complexity may not further than extending first-guess priors into larger families, thus failing to establish reference priors on a novel sound ground.

</details>

<details>

<summary>2016-09-22 16:46:18 - An equivalence between high dimensional Bayes optimal inference and M-estimation</summary>

- *Madhu Advani, Surya Ganguli*

- `1609.07060v1` - [abs](http://arxiv.org/abs/1609.07060v1) - [pdf](http://arxiv.org/pdf/1609.07060v1)

> When recovering an unknown signal from noisy measurements, the computational difficulty of performing optimal Bayesian MMSE (minimum mean squared error) inference often necessitates the use of maximum a posteriori (MAP) inference, a special case of regularized M-estimation, as a surrogate. However, MAP is suboptimal in high dimensions, when the number of unknown signal components is similar to the number of measurements. In this work we demonstrate, when the signal distribution and the likelihood function associated with the noise are both log-concave, that optimal MMSE performance is asymptotically achievable via another M-estimation procedure. This procedure involves minimizing convex loss and regularizer functions that are nonlinearly smoothed versions of the widely applied MAP optimization problem. Our findings provide a new heuristic derivation and interpretation for recent optimal M-estimators found in the setting of linear measurements and additive noise, and further extend these results to nonlinear measurements with non-additive noise. We numerically demonstrate superior performance of our optimal M-estimators relative to MAP. Overall, at the heart of our work is the revelation of a remarkable equivalence between two seemingly very different computational problems: namely that of high dimensional Bayesian integration underlying MMSE inference, and high dimensional convex optimization underlying M-estimation. In essence we show that the former difficult integral may be computed by solving the latter, simpler optimization problem.

</details>

<details>

<summary>2016-09-23 04:38:23 - Schwarz type model comparison for LAQ models</summary>

- *Shoichi Eguchi, Hiroki Masuda*

- `1606.01627v2` - [abs](http://arxiv.org/abs/1606.01627v2) - [pdf](http://arxiv.org/pdf/1606.01627v2)

> For model-specification purpose, we study asymptotic behavior of the marginal quasi-log likelihood associated with a family of locally asymptotically quadratic (LAQ) statistical experiments. Our result entails a far-reaching extension of applicable scope of the classical approximate Bayesian model comparison due to Schwarz, with frequentist-view theoretical foundation. In particular, the proposed statistics can deal with both ergodic and non-ergodic stochastic-process models, where the corresponding $M$-estimator is of multi-scaling type and the asymptotic quasi-information matrix is random. Focusing on the ergodic diffusion model, we also deduce the consistency of the multistage optimal-model selection where we may select an optimal sub-model structure step by step, so that computational cost can be much reduced. We illustrate the proposed method by the Gaussian quasi-likelihood for diffusion-type models in details, together with several numerical experiments.

</details>

<details>

<summary>2016-09-23 05:20:09 - Fully Bayesian Estimation and Variable Selection in Partially Linear Wavelet Models</summary>

- *Norbert Remenyi*

- `1609.07233v1` - [abs](http://arxiv.org/abs/1609.07233v1) - [pdf](http://arxiv.org/pdf/1609.07233v1)

> In this paper we propose a wavelet-based methodology for estimation and variable selection in partially linear models. The inference is conducted in the wavelet domain, which provides a sparse and localized decomposition appropriate for nonparametric components with various degrees of smoothness. A hierarchical Bayes model is formulated on the parameters of this representation, where the estimation and variable selection is performed by a Gibbs sampling procedure. For both the parametric and nonparametric part of the model we are using point-mass-at-zero contamination priors with a double exponential spread distribution. Only a few papers in the area of partially linear wavelet models exist, and we show that the proposed methodology is often superior to the existing methods with respect to the task of estimating model parameters. Moreover, the method is able to perform Bayesian variable selection by a stochastic search for the parametric part of the model.

</details>

<details>

<summary>2016-09-23 23:12:06 - A Tutorial on Distributed (Non-Bayesian) Learning: Problem, Algorithms and Results</summary>

- *Angelia NediÄ, Alex Olshevsky, CÃ©sar A. Uribe*

- `1609.07537v1` - [abs](http://arxiv.org/abs/1609.07537v1) - [pdf](http://arxiv.org/pdf/1609.07537v1)

> We overview some results on distributed learning with focus on a family of recently proposed algorithms known as non-Bayesian social learning. We consider different approaches to the distributed learning problem and its algorithmic solutions for the case of finitely many hypotheses. The original centralized problem is discussed at first, and then followed by a generalization to the distributed setting. The results on convergence and convergence rate are presented for both asymptotic and finite time regimes. Various extensions are discussed such as those dealing with directed time-varying networks, Nesterov's acceleration technique and a continuum sets of hypothesis.

</details>

<details>

<summary>2016-09-25 11:06:09 - Estimating the unobservable moose - converting index to population size using a Bayesian Hierarchical state space model</summary>

- *Jonas Wallin, Kjell Wallin*

- `1607.06307v2` - [abs](http://arxiv.org/abs/1607.06307v2) - [pdf](http://arxiv.org/pdf/1607.06307v2)

> Indirect information on population size, like pellet counts or volunteer counts, is the main source of information in most ecological studies and applied population management situations. Often, such observations are treaded as if they were actual measurements of population size. This assumption results in incorrect conclusions about a population's size and its dynamics. We propose a model with a temporal varying link, denoted countability, between indirect observations and actual population size. We show that, when indirect measurement has high precision (for instance many observation hours) the assumption of temporal varying countability can have a crucial effect on the estimated population dynamic. We apply the model on two local moose populations in Sweden. The estimated population dynamics is found to explain 30-50 percent of the total variability in the observation data; thus, countability accounts for most of the variation. This unreliability of the estimated dynamics has a substantial negative impact on the ability to manage populations; for example, reducing (increasing) the number of animals that needs to be harvested in order to sustain the population above (below) a fixed level. Finally, large difference in countability between two study areas implies a substantial spatial variation in the countability; this variation in itself is highly worthy of study.

</details>

<details>

<summary>2016-09-25 11:10:48 - Cooperative Parallel Particle Filters for online model selection and applications to Urban Mobility</summary>

- *Luca Martino, Jesse Read, Victor Elvira, Francisco Louzada*

- `1609.07731v1` - [abs](http://arxiv.org/abs/1609.07731v1) - [pdf](http://arxiv.org/pdf/1609.07731v1)

> We design a sequential Monte Carlo scheme for the dual purpose of Bayesian inference and model selection. We consider the application context of urban mobility, where several modalities of transport and different measurement devices can be employed. Therefore, we address the joint problem of online tracking and detection of the current modality. For this purpose, we use interacting parallel particle filters, each one addressing a different model. They cooperate for providing a global estimator of the variable of interest and, at the same time, an approximation of the posterior density of each model given the data. The interaction occurs by a parsimonious distribution of the computational effort, with online adaptation for the number of particles of each filter according to the posterior probability of the corresponding model. The resulting scheme is simple and flexible. We have tested the novel technique in different numerical experiments with artificial and real data, which confirm the robustness of the proposed scheme.

</details>

<details>

<summary>2016-09-25 11:37:28 - Orthogonal parallel MCMC methods for sampling and optimization</summary>

- *L. Martino, V. Elvira, D. Luengo, J. Corander, F. Louzada*

- `1507.08577v2` - [abs](http://arxiv.org/abs/1507.08577v2) - [pdf](http://arxiv.org/pdf/1507.08577v2)

> Monte Carlo (MC) methods are widely used for Bayesian inference and optimization in statistics, signal processing and machine learning. A well-known class of MC methods are Markov Chain Monte Carlo (MCMC) algorithms. In order to foster better exploration of the state space, specially in high-dimensional applications, several schemes employing multiple parallel MCMC chains have been recently introduced. In this work, we describe a novel parallel interacting MCMC scheme, called {\it orthogonal MCMC} (O-MCMC), where a set of "vertical" parallel MCMC chains share information using some "horizontal" MCMC techniques working on the entire population of current states. More specifically, the vertical chains are led by random-walk proposals, whereas the horizontal MCMC techniques employ independent proposals, thus allowing an efficient combination of global exploration and local approximation. The interaction is contained in these horizontal iterations. Within the analysis of different implementations of O-MCMC, novel schemes in order to reduce the overall computational cost of parallel multiple try Metropolis (MTM) chains are also presented. Furthermore, a modified version of O-MCMC for optimization is provided by considering parallel simulated annealing (SA) algorithms. Numerical results show the advantages of the proposed sampling scheme in terms of efficiency in the estimation, as well as robustness in terms of independence with respect to initial values and the choice of the parameters.

</details>

<details>

<summary>2016-09-26 00:03:20 - Mixtures of Bivariate von Mises Distributions with Applications to Modelling of Protein Dihedral Angles</summary>

- *Parthan Kasarapu*

- `1607.01312v2` - [abs](http://arxiv.org/abs/1607.01312v2) - [pdf](http://arxiv.org/pdf/1607.01312v2)

> The modelling of empirically observed data is commonly done using mixtures of probability distributions. In order to model angular data, directional probability distributions such as the bivariate von Mises (BVM) is typically used. The critical task involved in mixture modelling is to determine the optimal number of component probability distributions. We employ the Bayesian information-theoretic principle of minimum message length (MML) to distinguish mixture models by balancing the trade-off between the model's complexity and its goodness-of-fit to the data. We consider the problem of modelling angular data resulting from the spatial arrangement of protein structures using BVM distributions. The main contributions of the paper include the development of the mixture modelling apparatus along with the MML estimation of the parameters of the BVM distribution. We demonstrate that statistical inference using the MML framework supersedes the traditional methods and offers a mechanism to objectively determine models that are of practical significance.

</details>

<details>

<summary>2016-09-26 00:31:50 - Joint Models for Time-to-Event Data and Longitudinal Biomarkers of High Dimension</summary>

- *Molei Liu, Jiehuan Sun, Jose D. Herazo-Maya, Naftali Kaminski, Hongyu Zhao*

- `1609.02986v2` - [abs](http://arxiv.org/abs/1609.02986v2) - [pdf](http://arxiv.org/pdf/1609.02986v2)

> Joint models for longitudinal biomarkers and time-to-event data are widely used in longitudinal studies. Many joint modeling approaches have been proposed to deal with different types of longitudinal biomarkers and survival outcomes. However, most existing joint modeling methods cannot deal with a large number of longitudinal biomarkers simultaneously, such as the longitudinally collected gene expression profiles. In this article, we propose a new joint modeling method under the Bayesian framework, which is able to deal with longitudinal biomarkers of high dimension. Specifically, we assume that only a few unobserved latent variables are related to the survival outcome and the latent variables are inferred using a factor analysis model, which greatly reduces the dimensionality of the biomarkers and also accounts for the high correlations among the biomarkers. Through extensive simulation studies, we show that our proposed method has improved prediction accuracy over other joint modeling methods. We illustrate the usefulness of our method on a dataset of idiopathic pulmonary fibrosis patients in which we are interested in predicting the patients' time-to-death using their gene expression profiles.

</details>

<details>

<summary>2016-09-26 00:32:06 - A Bayesian Semiparametric Factor Analysis Model for Subtype Identification</summary>

- *Jiehuan Sun, Joshua L. Warren, Hongyu Zhao*

- `1609.02984v2` - [abs](http://arxiv.org/abs/1609.02984v2) - [pdf](http://arxiv.org/pdf/1609.02984v2)

> Disease subtype identification (clustering) is an important problem in biomedical research. Gene expression profiles are commonly utilized to infer disease subtypes, which often lead to biologically meaningful insights into disease. Despite many successes, existing clustering methods may not perform well when genes are highly correlated and many uninformative genes are included for clustering due to the high dimensionality. In this article, we introduce a novel subtype identification method in the Bayesian setting based on gene expression profiles. This method, called BCSub, adopts an innovative semiparametric Bayesian factor analysis model to reduce the dimension of the data to a few factor scores for clustering. Specifically, the factor scores are assumed to follow the Dirichlet process mixture model in order to induce clustering. Through extensive simulation studies, we show that BCSub has improved performance over commonly used clustering methods. When applied to two gene expression datasets, our model is able to identify subtypes that are clinically more relevant than those identified from the existing methods.

</details>

<details>

<summary>2016-09-26 00:32:20 - A Dirichlet Process Mixture Model for Clustering Longitudinal Gene Expression Data</summary>

- *Jiehuan Sun, Jose D. Herazo-Maya, Naftali Kaminski, Hongyu Zhao, Joshua L. Warren*

- `1609.02980v2` - [abs](http://arxiv.org/abs/1609.02980v2) - [pdf](http://arxiv.org/pdf/1609.02980v2)

> Subgroup identification (clustering) is an important problem in biomedical research. Gene expression profiles are commonly utilized to define subgroups. Longitudinal gene expression profiles might provide additional information on disease progression than what is captured by baseline profiles alone. Moreover, the longitudinal gene expression data allows for intra-individual variability to be accounted for when grouping patients. Therefore, subgroup identification could be more accurate and effective with the aid of longitudinal gene expression data. However, existing statistical methods are unable to fully utilize these data for patient clustering. In this article, we introduce a novel subgroup identification method in the Bayesian setting based on longitudinal gene expression profiles. This method, called BClustLonG, adopts a linear mixed-effects framework to model the trajectory of genes over time while clustering is jointly conducted based on the regression coefficients obtained from all genes. In order to account for the correlations among genes and alleviate the high dimensionality challenges, we adopt a factor analysis model for the regression coefficients. The Dirichlet process prior distribution is utilized for the means of the regression coefficients to induce clustering. Through extensive simulation studies, we show that BClustLonG has improved performance over other clustering methods. When applied to a dataset of severely injured (burn or trauma) patients, our model is able to distinguish burn patients from trauma patients and identify interesting subgroups in trauma patients.

</details>

<details>

<summary>2016-09-26 10:32:45 - A Bayesian model selection approach for identifying differentially expressed transcripts from RNA-Seq data</summary>

- *Panagiotis Papastamoulis, Magnus Rattray*

- `1412.3050v4` - [abs](http://arxiv.org/abs/1412.3050v4) - [pdf](http://arxiv.org/pdf/1412.3050v4)

> Recent advances in molecular biology allow the quantification of the transcriptome and scoring transcripts as differentially or equally expressed between two biological conditions. Although these two tasks are closely linked, the available inference methods treat them separately: a primary model is used to estimate expression and its output is post-processed using a differential expression model. In this paper, both issues are simultaneously addressed by proposing the joint estimation of expression levels and differential expression: the unknown relative abundance of each transcript can either be equal or not between two conditions. A hierarchical Bayesian model builds upon the BitSeq framework and the posterior distribution of transcript expression and differential expression is inferred using Markov Chain Monte Carlo (MCMC). It is shown that the proposed model enjoys conjugacy for fixed dimension variables, thus the full conditional distributions are analytically derived. Two samplers are constructed, a reversible jump MCMC sampler and a collapsed Gibbs sampler, and the latter is found to perform best. A cluster representation of the aligned reads to the transcriptome is introduced, allowing parallel estimation of the marginal posterior distribution of subsets of transcripts under reasonable computing time. The proposed algorithm is benchmarked against alternative methods using synthetic datasets and applied to real RNA-sequencing data. Source code is available online (https://github.com/mqbssppe/cjBitSeq).

</details>

<details>

<summary>2016-09-26 12:35:35 - Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints</summary>

- *Eduardo C. Garrido-MerchÃ¡n, Daniel HernÃ¡ndez-Lobato*

- `1609.01051v2` - [abs](http://arxiv.org/abs/1609.01051v2) - [pdf](http://arxiv.org/pdf/1609.01051v2)

> This work presents PESMOC, Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints, an information-based strategy for the simultaneous optimization of multiple expensive-to-evaluate black-box functions under the presence of several constraints. PESMOC can hence be used to solve a wide range of optimization problems. Iteratively, PESMOC chooses an input location on which to evaluate the objective functions and the constraints so as to maximally reduce the entropy of the Pareto set of the corresponding optimization problem. The constraints considered in PESMOC are assumed to have similar properties to those of the objective functions in typical Bayesian optimization problems. That is, they do not have a known expression (which prevents gradient computation), their evaluation is considered to be very expensive, and the resulting observations may be corrupted by noise. These constraints arise in a plethora of expensive black-box optimization problems. We carry out synthetic experiments to illustrate the effectiveness of PESMOC, where we sample both the objectives and the constraints from a Gaussian process prior. The results obtained show that PESMOC is able to provide better recommendations with a smaller number of evaluations than a strategy based on random search.

</details>

<details>

<summary>2016-09-26 13:56:31 - A Bayesian hidden Markov mixture model to detect overexpressed chromosome regions</summary>

- *VinÃ­cius Diniz Mayrink, FlÃ¡vio Bambirra GonÃ§alves*

- `1511.06730v3` - [abs](http://arxiv.org/abs/1511.06730v3) - [pdf](http://arxiv.org/pdf/1511.06730v3)

> In this study, we propose a hidden Markov mixture model for the analysis of gene expression measurements mapped to chromosome locations. These expression values represent preprocessed light intensities observed in each probe of Affymetrix oligonucleotide arrays. Here, the algorithm BLAT is used to align thousands of probe sequences to each chromosome. The main goal is to identify genome regions associated with high expression values which define clusters composed by consecutive observations. The proposed model assumes a mixture distribution in which one of the components (the one with the highest expected value) is supposed to accommodate the overexpressed clusters. The model takes advantage of the serial structure of the data and uses the distance information between neighbours to infer about the existence of a Markov dependence. This dependence is crucially important in the detection of overexpressed regions. We propose and discuss a Markov chain Monte Carlo algorithm to fit the model. Finally, the proposed methodology is used to analyse five data sets representing three types of cancer (breast, ovarian and brain).

</details>

<details>

<summary>2016-09-26 16:56:11 - Data Integration Model for Air Quality: A Hierarchical Approach to the Global Estimation of Exposures to Ambient Air Pollution</summary>

- *Gavin Shaddick, Matthew L. Thomas, Amelia Jobling, Michael Brauer, Aaron van Donkelaar, Rick Burnett, Howard Chang, Aaron Cohen, Rita Van Dingenen, Carlos Dora, Sophie Gumy, Yang Liu, Randall Martin, Lance A. Waller, Jason West, James V. Zidek, Annette PrÃ¼ss-UstÃ¼n*

- `1609.00141v2` - [abs](http://arxiv.org/abs/1609.00141v2) - [pdf](http://arxiv.org/pdf/1609.00141v2)

> Air pollution is a major risk factor for global health, with both ambient and household air pollution contributing substantial components of the overall global disease burden. One of the key drivers of adverse health effects is fine particulate matter ambient pollution (PM$_{2.5}$) to which an estimated 3 million deaths can be attributed annually. The primary source of information for estimating exposures has been measurements from ground monitoring networks but, although coverage is increasing, there remain regions in which monitoring is limited. Ground monitoring data therefore needs to be supplemented with information from other sources, such as satellite retrievals of aerosol optical depth and chemical transport models. A hierarchical modelling approach for integrating data from multiple sources is proposed allowing spatially-varying relationships between ground measurements and other factors that estimate air quality. Set within a Bayesian framework, the resulting Data Integration Model for Air Quality (DIMAQ) is used to estimate exposures, together with associated measures of uncertainty, on a high resolution grid covering the entire world. Bayesian analysis on this scale can be computationally challenging and here approximate Bayesian inference is performed using Integrated Nested Laplace Approximations. Model selection and assessment is performed by cross-validation with the final model offering substantial increases in predictive accuracy, particularly in regions where there is sparse ground monitoring, when compared to current approaches: root mean square error (RMSE) reduced from 17.1 to 10.7, and population weighted RMSE from 23.1 to 12.1 $\mu$gm$^{-3}$. Based on summaries of the posterior distributions for each grid cell, it is estimated that 92% of the world's population reside in areas exceeding the World Health Organization's Air Quality Guidelines.

</details>

<details>

<summary>2016-09-27 02:27:25 - Statistical Inference for Ergodic Point Processes and Application to Limit Order Book</summary>

- *Simon Clinet, Nakahiro Yoshida*

- `1512.01899v3` - [abs](http://arxiv.org/abs/1512.01899v3) - [pdf](http://arxiv.org/pdf/1512.01899v3)

> We construct a general procedure for the Quasi Likelihood Analysis applied to a multivariate point process on the real half line in an ergodic framework. More precisely, we assume that the stochastic intensity of the underlying model belongs to a family of processes indexed by a finite dimensional parameter. When a particular family of laws of large numbers applies to those processes, we establish the consistency, the asymptotic normality and the convergence of moments of both the Quasi Maximum Likelihood estimator and the Quasi Bayesian estimator. In addition, we illustrate our main results by showing how they can be applied to various Limit Order Book models existing in the literature. In particular, we address the fundamental cases of Markovian models and exponential Hawkes process-based models.

</details>

<details>

<summary>2016-09-27 17:14:34 - Bayesian Projection of Life Expectancy Accounting for the HIV/AIDS Epidemic</summary>

- *Jessica Godwin, Adrian E. Raftery*

- `1608.07330v2` - [abs](http://arxiv.org/abs/1608.07330v2) - [pdf](http://arxiv.org/pdf/1608.07330v2)

> While probabilistic projection methods for projecting life expectancy exist, few account for covariates related to life expectancy. Generalized HIV/AIDS epidemics have a large, immediate negative impact on the life expectancy in a country, but this impact can be mitigated by widespread use of antiretroviral therapy (ART). Thus projection methods for countries with generalized HIV/AIDS epidemics could be improved by accounting for HIV prevalence, the future course of the epidemic and coverage of ART. We propose a method for making probabilistic projections of life expectancy to 2100 for all countries in the world accounting for HIV prevalence, the future course of the epidemic and its uncertainty, and adult ART coverage. We extend the current Bayesian probabilistic life expectancy projection methods of Raftery et al. (2013) to account for HIV prevalence and adult ART coverage. We evaluate our method using out-of-sample validation. We find that the proposed method performs better than the method that does not account for HIV prevalence or ART coverage for projections of life expectancy in countries with a generalized epidemic, while projections for countries without an epidemic remain essentially unchanged.

</details>

<details>

<summary>2016-09-27 23:10:42 - Optimizing Neural Network Hyperparameters with Gaussian Processes for Dialog Act Classification</summary>

- *Franck Dernoncourt, Ji Young Lee*

- `1609.08703v1` - [abs](http://arxiv.org/abs/1609.08703v1) - [pdf](http://arxiv.org/pdf/1609.08703v1)

> Systems based on artificial neural networks (ANNs) have achieved state-of-the-art results in many natural language processing tasks. Although ANNs do not require manually engineered features, ANNs have many hyperparameters to be optimized. The choice of hyperparameters significantly impacts models' performances. However, the ANN hyperparameters are typically chosen by manual, grid, or random search, which either requires expert experiences or is computationally expensive. Recent approaches based on Bayesian optimization using Gaussian processes (GPs) is a more systematic way to automatically pinpoint optimal or near-optimal machine learning hyperparameters. Using a previously published ANN model yielding state-of-the-art results for dialog act classification, we demonstrate that optimizing hyperparameters using GP further improves the results, and reduces the computational time by a factor of 4 compared to a random search. Therefore it is a useful technique for tuning ANN models to yield the best performances for natural language processing tasks.

</details>

<details>

<summary>2016-09-28 02:28:17 - A Bayesian Interval Dose-Finding Design Addressing Ockham's Razor: mTPI-2</summary>

- *Wentian Guo, Sue-Jane Wang, Shengjie Yang, Suiheng Lin, Yuan Ji*

- `1609.08737v1` - [abs](http://arxiv.org/abs/1609.08737v1) - [pdf](http://arxiv.org/pdf/1609.08737v1)

> There has been an increasing interest in using interval-based Bayesian designs for dose finding, one of which is the modified toxicity probability interval (mTPI) method. We show that the decision rules in mTPI correspond to an optimal rule under a formal Bayesian decision theoretic framework. However, the probability models in mTPI are overly sharpened by the Ockham's razor, which, while in general helps with parsimonious statistical inference, leads to suboptimal decisions in small-sample inference such as dose finding. We propose a new framework that blunts the Ockham's razor, and demonstrate the superior performance of the new method, called mTPI-2. An online web tool is provided for users who can generate the design, conduct clinical trials, and examine operating characteristics of the designs through big data and crowd sourcing.

</details>

<details>

<summary>2016-09-28 09:27:20 - Predictive Coarse-Graining</summary>

- *Markus SchÃ¶berl, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis*

- `1605.08301v2` - [abs](http://arxiv.org/abs/1605.08301v2) - [pdf](http://arxiv.org/pdf/1605.08301v2)

> We propose a data-driven, coarse-graining formulation in the context of equilibrium statistical mechanics. In contrast to existing techniques which are based on a fine-to-coarse map, we adopt the opposite strategy by prescribing a probabilistic coarse-to-fine map. This corresponds to a directed probabilistic model where the coarse variables play the role of latent generators of the fine scale (all-atom) data. From an information-theoretic perspective, the framework proposed provides an improvement upon the relative entropy method and is capable of quantifying the uncertainty due to the information loss that unavoidably takes place during the CG process. Furthermore, it can be readily extended to a fully Bayesian model where various sources of uncertainties are reflected in the posterior of the model parameters. The latter can be used to produce not only point estimates of fine-scale reconstructions or macroscopic observables, but more importantly, predictive posterior distributions on these quantities. Predictive posterior distributions reflect the confidence of the model as a function of the amount of data and the level of coarse-graining. The issues of model complexity and model selection are seamlessly addressed by employing a hierarchical prior that favors the discovery of sparse solutions, revealing the most prominent features in the coarse-grained model. A flexible and parallelizable Monte Carlo - Expectation-Maximization (MC-EM) scheme is proposed for carrying out inference and learning tasks. A comparative assessment of the proposed methodology is presented for a lattice spin system and the SPC/E water model.

</details>

<details>

<summary>2016-09-28 15:56:15 - Variational Autoencoder for Deep Learning of Images, Labels and Captions</summary>

- *Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, Lawrence Carin*

- `1609.08976v1` - [abs](http://arxiv.org/abs/1609.08976v1) - [pdf](http://arxiv.org/pdf/1609.08976v1)

> A novel variational autoencoder is developed to model images, as well as associated labels or captions. The Deep Generative Deconvolutional Network (DGDN) is used as a decoder of the latent image features, and a deep Convolutional Neural Network (CNN) is used as an image encoder; the CNN is used to approximate a distribution for the latent DGDN features/code. The latent code is also linked to generative models for labels (Bayesian support vector machine) or captions (recurrent neural network). When predicting a label/caption for a new image at test, averaging is performed across the distribution of latent codes; this is computationally efficient as a consequence of the learned CNN-based encoder. Since the framework is capable of modeling the image in the presence/absence of associated labels/captions, a new semi-supervised setting is manifested for CNN learning with images; the framework even allows unsupervised CNN learning, based on images alone.

</details>

<details>

<summary>2016-09-29 02:02:09 - Improving Grid Based Bayesian Methods</summary>

- *Chaitanya Joshi, Paul T. Brown, Stephen Joe*

- `1609.09174v1` - [abs](http://arxiv.org/abs/1609.09174v1) - [pdf](http://arxiv.org/pdf/1609.09174v1)

> In some cases, computational benefit can be gained by exploring the hyper parameter space using a deterministic set of grid points instead of a Markov chain. We view this as a numerical integration problem and make three unique contributions. First, we explore the space using low discrepancy point sets instead of a grid. This allows for accurate estimation of marginals of any shape at a much lower computational cost than a grid based approach and thus makes it possible to extend the computational benefit to a hyper parameter space with higher dimensionality (10 or more). Second, we propose a new, quick and easy method to estimate the marginal using a least squares polynomial and prove the conditions under which this polynomial will converge to the true marginal. Our results are valid for a wide range of point sets including grids, random points and low discrepancy points. Third, we show that further accuracy and efficiency can be gained by taking into consideration the functional decomposition of the integrand and illustrate how this can be done using anchored f-ANOVA on weighted spaces.

</details>

<details>

<summary>2016-09-29 08:40:58 - Fast Bayesian whole-brain fMRI analysis with spatial 3D priors</summary>

- *Per SidÃ©n, Anders Eklund, David Bolin, Mattias Villani*

- `1606.00980v2` - [abs](http://arxiv.org/abs/1606.00980v2) - [pdf](http://arxiv.org/pdf/1606.00980v2)

> Spatial whole-brain Bayesian modeling of task-related functional magnetic resonance imaging (fMRI) is a great computational challenge. Most of the currently proposed methods therefore do inference in subregions of the brain separately or do approximate inference without comparison to the true posterior distribution. A popular such method, which is now the standard method for Bayesian single subject analysis in the SPM software, is introduced in Penny et al. (2005b). The method processes the data slice-by-slice and uses an approximate variational Bayes (VB) estimation algorithm that enforces posterior independence between activity coefficients in different voxels. We introduce a fast and practical Markov chain Monte Carlo (MCMC) scheme for exact inference in the same model, both slice-wise and for the whole brain using a 3D prior on activity coefficients. The algorithm exploits sparsity and uses modern techniques for efficient sampling from high-dimensional Gaussian distributions, leading to speed-ups without which MCMC would not be a practical option. Using MCMC, we are for the first time able to evaluate the approximate VB posterior against the exact MCMC posterior, and show that VB can lead to spurious activation. In addition, we develop an improved VB method that drops the assumption of independent voxels a posteriori. This algorithm is shown to be much faster than both MCMC and the original VB for large datasets, with negligible error compared to the MCMC posterior.

</details>

<details>

<summary>2016-09-29 11:13:00 - Online Optimization with Costly and Noisy Measurements using Random Fourier Expansions</summary>

- *Laurens Bliek, Hans R. G. W. Verstraete, Michel Verhaegen, Sander Wahls*

- `1603.09620v3` - [abs](http://arxiv.org/abs/1603.09620v3) - [pdf](http://arxiv.org/pdf/1603.09620v3)

> This paper analyzes DONE, an online optimization algorithm that iteratively minimizes an unknown function based on costly and noisy measurements. The algorithm maintains a surrogate of the unknown function in the form of a random Fourier expansion (RFE). The surrogate is updated whenever a new measurement is available, and then used to determine the next measurement point. The algorithm is comparable to Bayesian optimization algorithms, but its computational complexity per iteration does not depend on the number of measurements. We derive several theoretical results that provide insight on how the hyper-parameters of the algorithm should be chosen. The algorithm is compared to a Bayesian optimization algorithm for a benchmark problem and three applications, namely, optical coherence tomography, optical beam-forming network tuning, and robot arm control. It is found that the DONE algorithm is significantly faster than Bayesian optimization in the discussed problems, while achieving a similar or better performance.

</details>

<details>

<summary>2016-09-29 12:40:38 - Network structure, metadata and the prediction of missing nodes and annotations</summary>

- *Darko Hric, Tiago P. Peixoto, Santo Fortunato*

- `1604.00255v2` - [abs](http://arxiv.org/abs/1604.00255v2) - [pdf](http://arxiv.org/pdf/1604.00255v2)

> The empirical validation of community detection methods is often based on available annotations on the nodes that serve as putative indicators of the large-scale network structure. Most often, the suitability of the annotations as topological descriptors itself is not assessed, and without this it is not possible to ultimately distinguish between actual shortcomings of the community detection algorithms on one hand, and the incompleteness, inaccuracy or structured nature of the data annotations themselves on the other. In this work we present a principled method to access both aspects simultaneously. We construct a joint generative model for the data and metadata, and a nonparametric Bayesian framework to infer its parameters from annotated datasets. We assess the quality of the metadata not according to its direct alignment with the network communities, but rather in its capacity to predict the placement of edges in the network. We also show how this feature can be used to predict the connections to missing nodes when only the metadata is available, as well as missing metadata. By investigating a wide range of datasets, we show that while there are seldom exact agreements between metadata tokens and the inferred data groups, the metadata is often informative of the network structure nevertheless, and can improve the prediction of missing nodes. This shows that the method uncovers meaningful patterns in both the data and metadata, without requiring or expecting a perfect agreement between the two.

</details>

<details>

<summary>2016-09-29 17:31:44 - On the statistical properties of viral misinformation in online social media</summary>

- *Alessandro Bessi*

- `1609.09435v1` - [abs](http://arxiv.org/abs/1609.09435v1) - [pdf](http://arxiv.org/pdf/1609.09435v1)

> The massive diffusion of online social media allows for the rapid and uncontrolled spreading of conspiracy theories, hoaxes, unsubstantiated claims, and false news. Such an impressive amount of misinformation can influence policy preferences and encourage behaviors strongly divergent from recommended practices. In this paper, we study the statistical properties of viral misinformation in online social media. By means of methods belonging to Extreme Value Theory, we show that the number of extremely viral posts over time follows a homogeneous Poisson process, and that the interarrival times between such posts are independent and identically distributed, following an exponential distribution. Moreover, we characterize the uncertainty around the rate parameter of the Poisson process through Bayesian methods. Finally, we are able to derive the predictive posterior probability distribution of the number of posts exceeding a certain threshold of shares over a finite interval of time.

</details>

<details>

<summary>2016-09-30 07:56:07 - Convergence analysis of block Gibbs samplers for Bayesian linear mixed models with $p>N$</summary>

- *Tavis Abrahamsen, James P. Hobert*

- `1502.05460v3` - [abs](http://arxiv.org/abs/1502.05460v3) - [pdf](http://arxiv.org/pdf/1502.05460v3)

> Exploration of the intractable posterior distributions associated with Bayesian versions of the general linear mixed model is often performed using Markov chain Monte Carlo. In particular, if a conditionally conjugate prior is used, then there is a simple two-block Gibbs sampler available. Rom\'{a}n and Hobert [Linear Algebra Appl. 473 (2015) 54-77] showed that, when the priors are proper and the $X$ matrix has full column rank, the Markov chains underlying these Gibbs samplers are nearly always geometrically ergodic. In this paper, Rom\'{a}n and Hobert's (2015) result is extended by allowing improper priors on the variance components, and, more importantly, by removing all assumptions on the $X$ matrix. So, not only is $X$ allowed to be (column) rank deficient, which provides additional flexibility in parameterizing the fixed effects, it is also allowed to have more columns than rows, which is necessary in the increasingly important situation where $p>N$. The full rank assumption on $X$ is at the heart of Rom\'{a}n and Hobert's (2015) proof. Consequently, the extension to unrestricted $X$ requires a substantially different analysis.

</details>

<details>

<summary>2016-09-30 10:17:47 - On Identification of Sparse Multivariable ARX Model: A Sparse Bayesian Learning Approach</summary>

- *J. Jin, Y. Yuan, W. Pan, D. L. T. Pham, C. J. Tomlin, A. Webb, J. Goncalves*

- `1609.09660v1` - [abs](http://arxiv.org/abs/1609.09660v1) - [pdf](http://arxiv.org/pdf/1609.09660v1)

> This paper begins with considering the identification of sparse linear time-invariant networks described by multivariable ARX models. Such models possess relatively simple structure thus used as a benchmark to promote further research. With identifiability of the network guaranteed, this paper presents an identification method that infers both the Boolean structure of the network and the internal dynamics between nodes. Identification is performed directly from data without any prior knowledge of the system, including its order. The proposed method solves the identification problem using Maximum a posteriori estimation (MAP) but with inseparable penalties for complexity, both in terms of element (order of nonzero connections) and group sparsity (network topology). Such an approach is widely applied in Compressive Sensing (CS) and known as Sparse Bayesian Learning (SBL). We then propose a novel scheme that combines sparse Bayesian and group sparse Bayesian to efficiently solve the problem. The resulted algorithm has a similar form of the standard Sparse Group Lasso (SGL) while with known noise variance, it simplifies to exact re-weighted SGL. The method and the developed toolbox can be applied to infer networks from a wide range of fields, including systems biology applications such as signaling and genetic regulatory networks.

</details>

<details>

<summary>2016-09-30 16:37:28 - Beyond p values: practical methods for analyzing uncertainty in research</summary>

- *Michael Wood*

- `1609.09803v1` - [abs](http://arxiv.org/abs/1609.09803v1) - [pdf](http://arxiv.org/pdf/1609.09803v1)

> This article explains, and discusses the merits of, three approaches for analyzing the certainty with which statistical results can be extrapolated beyond the data gathered. Sometimes it may be possible to use more than one of these approaches. (1) If there is an exact null hypothesis which is credible and interesting (usually not the case), researchers should cite a p value (significance level), although jargon is best avoided. (2) If the research result is a numerical value, researchers should cite a confidence interval. (3) If there are one or more hypotheses of interest, it may be possible to adapt the methods used for confidence intervals to derive an "estimated probability" for each. Under certain circumstances these could be interpreted as Bayesian posterior probabilities. These estimated probabilities can easily be worked out from the p values and confidence intervals produced by packages such as SPSS. Estimating probabilities for hypotheses means researchers can give a direct answer to the question "How certain can we be that this hypothesis is right?".

</details>


## 2016-10

<details>

<summary>2016-10-01 20:57:13 - A Birth and Death Process for Bayesian Network Structure Inference</summary>

- *D. Jennings, J. N. Corcoran*

- `1610.00189v1` - [abs](http://arxiv.org/abs/1610.00189v1) - [pdf](http://arxiv.org/pdf/1610.00189v1)

> Bayesian networks (BNs) are graphical models that are useful for representing high-dimensional probability distributions. There has been a great deal of interest in recent years in the NP-hard problem of learning the structure of a BN from observed data. Typically, one assigns a score to various structures and the search becomes an optimization problem that can be approached with either deterministic or stochastic methods. In this paper, we walk through the space of graphs by modeling the appearance and disappearance of edges as a birth and death process and compare our novel approach to the popular Metropolis-Hastings search strategy. We give empirical evidence that the birth and death process has superior mixing properties.

</details>

<details>

<summary>2016-10-03 10:42:34 - Optimal scaling of the independence sampler: Theory and Practice</summary>

- *Peter Neal, Clement Lee*

- `1511.04334v2` - [abs](http://arxiv.org/abs/1511.04334v2) - [pdf](http://arxiv.org/pdf/1511.04334v2)

> The independence sampler is one of the most commonly used MCMC algorithms usually as a component of a Metropolis-within-Gibbs algorithm. The common focus for the independence sampler is on the choice of proposal distribution to obtain an as high as possible acceptance rate. In this paper we have a somewhat different focus concentrating on the use of the independence sampler for updating augmented data in a Bayesian framework where a natural proposal distribution for the independence sampler exists. Thus we concentrate on the proportion of the augmented data to update to optimise the independence sampler. Generic guidelines for optimising the independence sampler are obtained for independent and identically distributed product densities mirroring findings for the random walk Metropolis algorithm. The generic guidelines are shown to be informative beyond the narrow confines of idealised product densities in two epidemic examples.

</details>

<details>

<summary>2016-10-03 18:40:25 - Data Integration with High Dimensionality</summary>

- *Xin Gao, Raymond J. Carroll*

- `1610.00667v1` - [abs](http://arxiv.org/abs/1610.00667v1) - [pdf](http://arxiv.org/pdf/1610.00667v1)

> We consider a problem of data integration. Consider determining which genes affect a disease. The genes, which we call predictor objects, can be measured in different experiments on the same individual. We address the question of finding which genes are predictors of disease by any of the experiments. Our formulation is more general. In a given data set, there are a fixed number of responses for each individual, which may include a mix of discrete, binary and continuous variables. There is also a class of predictor objects, which may differ within a subject depending on how the predictor object is measured, i.e., depend on the experiment. The goal is to select which predictor objects affect any of the responses, where the number of such informative predictor objects or features tends to infinity as sample size increases. There are marginal likelihoods for each way the predictor object is measured, i.e., for each experiment. We specify a pseudolikelihood combining the marginal likelihoods, and propose a pseudolikelihood information criterion. Under regularity conditions, we establish selection consistency for the pseudolikelihood information criterion with unbounded true model size, which includes a Bayesian information criterion with appropriate penalty term as a special case. Simulations indicate that data integration improves upon, sometimes dramatically, using only one of the data sources.

</details>

<details>

<summary>2016-10-04 16:16:55 - Adaptive Bayesian Spectral Analysis of Nonstationary Biomedical Time Series</summary>

- *Scott A. Bruce, Martica H. Hall, Daniel J. Buysse, Robert T. Krafty*

- `1609.00696v2` - [abs](http://arxiv.org/abs/1609.00696v2) - [pdf](http://arxiv.org/pdf/1609.00696v2)

> Many studies of biomedical time series signals aim to measure the association between frequency-domain properties of time series and clinical and behavioral covariates. However, the time-varying dynamics of these associations are largely ignored due to a lack of methods that can assess the changing nature of the relationship through time. This article introduces a method for the simultaneous and automatic analysis of the association between the time-varying power spectrum and covariates. The procedure adaptively partitions the grid of time and covariate values into an unknown number of approximately stationary blocks and nonparametrically estimates local spectra within blocks through penalized splines. The approach is formulated in a fully Bayesian framework, in which the number and locations of partition points are random, and fit using reversible jump Markov chain Monte Carlo techniques. Estimation and inference averaged over the distribution of partitions allows for the accurate analysis of spectra with both smooth and abrupt changes. The proposed methodology is used to analyze the association between the time-varying spectrum of heart rate variability and self-reported sleep quality in a study of older adults serving as the primary caregiver for their ill spouse.

</details>

<details>

<summary>2016-10-04 16:50:26 - Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</summary>

- *Yarin Gal, Zoubin Ghahramani*

- `1506.02142v6` - [abs](http://arxiv.org/abs/1506.02142v6) - [pdf](http://arxiv.org/pdf/1506.02142v6)

> Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.

</details>

<details>

<summary>2016-10-05 15:09:30 - A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</summary>

- *Yarin Gal, Zoubin Ghahramani*

- `1512.05287v5` - [abs](http://arxiv.org/abs/1512.05287v5) - [pdf](http://arxiv.org/pdf/1512.05287v5)

> Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.

</details>

<details>

<summary>2016-10-06 14:43:22 - Bayesian nonparametric estimation for Quantum Homodyne Tomography</summary>

- *Zacharie Naulet, Eric Barat*

- `1610.01895v1` - [abs](http://arxiv.org/abs/1610.01895v1) - [pdf](http://arxiv.org/pdf/1610.01895v1)

> We estimate the quantum state of a light beam from results of quantum homodyne tomography noisy measurements performed on identically prepared quantum systems. We propose two Bayesian nonparametric approaches. The first approach is based on mixture models and is illustrated through simulation examples. The second approach is based on random basis expansions. We study the theoretical performance of the second approach by quantifying the rate of contraction of the posterior distribution around the true quantum state in the $L^2$ metric.

</details>

<details>

<summary>2016-10-06 21:44:22 - Bayesian mixture of Plackett-Luce models for partially ranked data</summary>

- *Cristina Mollica, Luca Tardella*

- `1501.03519v5` - [abs](http://arxiv.org/abs/1501.03519v5) - [pdf](http://arxiv.org/pdf/1501.03519v5)

> The elicitation of an ordinal judgment on multiple alternatives is often required in many psychological and behavioral experiments to investigate preference/choice orientation of a specific population. The Plackett-Luce model is one of the most popular and frequently applied parametric distributions to analyze rankings of a finite set of items. The present work introduces a Bayesian finite mixture of Plackett-Luce models to account for unobserved sample heterogeneity of partially ranked data. We describe an efficient way to incorporate the latent group structure in the data augmentation approach and the derivation of existing maximum likelihood procedures as special instances of the proposed Bayesian method. Inference can be conducted with the combination of the Expectation-Maximization algorithm for maximum \textit{a posteriori} estimation and the Gibbs sampling iterative procedure. We additionally investigate several Bayesian criteria for selecting the optimal mixture configuration and describe diagnostic tools for assessing the fitness of ranking distributions conditionally and unconditionally on the number of ranked items. The utility of the novel Bayesian parametric Plackett-Luce mixture for characterizing sample heterogeneity is illustrated with several applications to simulated and real preference ranked data. We compare our method with the frequentist approach and a Bayesian nonparametric mixture model both assuming the Plackett-Luce model as a mixture component. Our analysis on real datasets reveals the importance of an accurate diagnostic check for an appropriate in-depth understanding of the heterogenous nature of the partial ranking data.

</details>

<details>

<summary>2016-10-07 00:39:46 - On Oracle Property and Asymptotic Validity of Bayesian Generalized Method of Moments</summary>

- *Cheng Li, Wenxin Jiang*

- `1405.6693v2` - [abs](http://arxiv.org/abs/1405.6693v2) - [pdf](http://arxiv.org/pdf/1405.6693v2)

> Statistical inference based on moment conditions and estimating equations is of substantial interest when it is difficult to specify a full probabilistic model. We propose a Bayesian flavored model selection framework based on (quasi-)posterior probabilities from the Bayesian Generalized Method of Moments (BGMM), which allows us to incorporate two important advantages of a Bayesian approach: the expressiveness of posterior distributions and the convenient computational method of Markov Chain Monte Carlo (MCMC). Theoretically we show that BGMM can achieve the posterior consistency for selecting the unknown true model, and that it possesses a Bayesian version of the oracle property, i.e. the posterior distribution for the parameter of interest is asymptotically normal and is as informative as if the true model were known. In addition, we show that the proposed quasi-posterior is valid to be interpreted as an approximate posterior distribution given a data summary. Our applications include modeling of correlated data, quantile regression, and graphical models based on partial correlations. We demonstrate the implementation of the BGMM model selection through numerical examples.

</details>

<details>

<summary>2016-10-07 16:08:25 - Pseudo-Bayesian Robust PCA: Algorithms and Analyses</summary>

- *Tae-Hyun Oh, Yasuyuki Matsushita, In So Kweon, David Wipf*

- `1512.02188v2` - [abs](http://arxiv.org/abs/1512.02188v2) - [pdf](http://arxiv.org/pdf/1512.02188v2)

> Commonly used in computer vision and other applications, robust PCA represents an algorithmic attempt to reduce the sensitivity of classical PCA to outliers. The basic idea is to learn a decomposition of some data matrix of interest into low rank and sparse components, the latter representing unwanted outliers. Although the resulting optimization problem is typically NP-hard, convex relaxations provide a computationally-expedient alternative with theoretical support. However, in practical regimes performance guarantees break down and a variety of non-convex alternatives, including Bayesian-inspired models, have been proposed to boost estimation quality. Unfortunately though, without additional a priori knowledge none of these methods can significantly expand the critical operational range such that exact principal subspace recovery is possible. Into this mix we propose a novel pseudo-Bayesian algorithm that explicitly compensates for design weaknesses in many existing non-convex approaches leading to state-of-the-art performance with a sound analytical foundation. Surprisingly, our algorithm can even outperform convex matrix completion despite the fact that the latter is provided with perfect knowledge of which entries are not corrupted.

</details>

<details>

<summary>2016-10-08 09:12:08 - Some comments about A Bayesian criterion for singular models by M. Drton and M. Plummer</summary>

- *Christian P. Robert, Judith Rousseau*

- `1610.02503v1` - [abs](http://arxiv.org/abs/1610.02503v1) - [pdf](http://arxiv.org/pdf/1610.02503v1)

> These are written comments about the Read Paper A Bayesian criterion for singular models by M. Drton and M. Plummer, read to the Royal Statistical Society on October 5, 2016. The discussion was delivered by Judith Rousseau.

</details>

<details>

<summary>2016-10-09 01:50:25 - Learning Bayesian Networks with Incomplete Data by Augmentation</summary>

- *Tameem Adel, Cassio P. de Campos*

- `1608.07734v2` - [abs](http://arxiv.org/abs/1608.07734v2) - [pdf](http://arxiv.org/pdf/1608.07734v2)

> We present new algorithms for learning Bayesian networks from data with missing values using a data augmentation approach. An exact Bayesian network learning algorithm is obtained by recasting the problem into a standard Bayesian network learning problem without missing data. To the best of our knowledge, this is the first exact algorithm for this problem. As expected, the exact algorithm does not scale to large domains. We build on the exact method to create an approximate algorithm using a hill-climbing technique. This algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available. We perform a wide range of experiments to demonstrate the benefits of learning Bayesian networks with such new approach.

</details>

<details>

<summary>2016-10-10 06:08:03 - Pseudo-Bayesian Quantum Tomography with Rank-adaptation</summary>

- *The Tien Mai, Pierre Alquier*

- `1605.05933v2` - [abs](http://arxiv.org/abs/1605.05933v2) - [pdf](http://arxiv.org/pdf/1605.05933v2)

> Quantum state tomography, an important task in quantum information processing, aims at reconstructing a state from prepared measurement data. Bayesian methods are recognized to be one of the good and reliable choice in estimating quantum states~\cite{blume2010optimal}. Several numerical works showed that Bayesian estimations are comparable to, and even better than other methods in the problem of $1$-qubit state recovery. However, the problem of choosing prior distribution in the general case of $n$ qubits is not straightforward. More importantly, the statistical performance of Bayesian type estimators have not been studied from a theoretical perspective yet. In this paper, we propose a novel prior for quantum states (density matrices), and we define pseudo-Bayesian estimators of the density matrix. Then, using PAC-Bayesian theorems, we derive rates of convergence for the posterior mean. The numerical performance of these estimators are tested on simulated and real datasets.

</details>

<details>

<summary>2016-10-10 20:25:26 - On Metrizing Vague Convergence of Random Measures with Applications on Bayesian Nonparametric Models</summary>

- *Luai Al-Labadi*

- `1610.03083v1` - [abs](http://arxiv.org/abs/1610.03083v1) - [pdf](http://arxiv.org/pdf/1610.03083v1)

> This paper deals with studying vague convergence of random measures of the form $\mu_{n}=\sum_{i=1}^{n} p_{i,n} \delta_{\theta_i}$, where $(\theta_i)_{1\le i \le n}$ is a sequence of independent and identically distributed random variables with common distribution $\Pi$, $(p_{i,n})_{1 \le i \le n}$ are random variables chosen according to certain procedures and are independent of $(\theta_i)_{i \geq 1}$ and $\delta_{\theta_i}$ denotes the Dirac measure at $\theta_i$. We show that $\mu_{n}$ converges vaguely to $\mu=\sum_{i=1}^{\infty} p_{i} \delta_{\theta_i}$ if and only if $\mu^{(k)}_{n}=\sum_{i=1}^{k} p_{i,n} \delta_{\theta_i}$ converges vaguely to $\mu^{(k)}=\sum_{i=1}^{k} p_{i} \delta_{\theta_i}$ for all $k$ fixed. The limiting process $\mu$ plays a central role in many areas in statistics, including Bayesian nonparametric models. A finite approximation of the beta process is derived from the application of this result. A simulated example is incorporated, in which the proposed approach exhibits an excellent performance over several existing algorithms.

</details>

<details>

<summary>2016-10-11 15:06:11 - A Bayesian Approach to Modelling Fine-Scale Spatial Dynamics of Non-State Terrorism: World Study, 2002-2013</summary>

- *AndrÃ© Python, Janine Illian, Charlotte Jones-Todd, Marta Blangiardo*

- `1610.01215v3` - [abs](http://arxiv.org/abs/1610.01215v3) - [pdf](http://arxiv.org/pdf/1610.01215v3)

> To this day, terrorism persists as a worldwide threat, as exemplified by the ongoing lethal attacks perpetrated by ISIS in Iraq, Syria, Al Qaeda in Yemen, and Boko Haram in Nigeria. In response, states deploy various counterterrorism policies, the costs of which could be reduced through efficient preventive measures. Statistical models able to account for complex spatio-temporal dependencies have not yet been applied, despite their potential for providing guidance to explain and prevent terrorism. In an effort to address this shortcoming, we employ hierarchical models in a Bayesian context, where the spatial random field is represented by a stochastic partial differential equation. Our results confirm the contagious nature of the lethality of terrorism and the number of lethal terrorist attacks in both space and time. Moreover, the frequency of lethal attacks tends to be higher in richer areas, close to large cities, and within democratic countries. In contrast, attacks are more likely to be lethal far away from large cities, at higher altitudes, in poorer areas, and in locations with higher ethnic diversity. We argue that, on a local scale, the lethality of terrorism and the frequency of lethal attacks are driven by antagonistic mechanisms.

</details>

<details>

<summary>2016-10-12 09:19:07 - On a generalization of the preconditioned Crank-Nicolson Metropolis algorithm</summary>

- *Daniel Rudolf, BjÃ¶rn Sprungk*

- `1504.03461v2` - [abs](http://arxiv.org/abs/1504.03461v2) - [pdf](http://arxiv.org/pdf/1504.03461v2)

> Metropolis algorithms for approximate sampling of probability measures on infinite dimensional Hilbert spaces are considered and a generalization of the preconditioned Crank-Nicolson (pCN) proposal is introduced. The new proposal is able to incorporate information of the measure of interest. A numerical simulation of a Bayesian inverse problem indicates that a Metropolis algorithm with such a proposal performs independent of the state space dimension and the variance of the observational noise. Moreover, a qualitative convergence result is provided by a comparison argument for spectral gaps. In particular, it is shown that the generalization inherits geometric ergodicity from the Metropolis algorithm with pCN proposal.

</details>

<details>

<summary>2016-10-12 11:59:49 - Bayesian multi-tensor factorization</summary>

- *Suleiman A. Khan, Eemeli LeppÃ¤aho, Samuel Kaski*

- `1412.4679v5` - [abs](http://arxiv.org/abs/1412.4679v5) - [pdf](http://arxiv.org/pdf/1412.4679v5)

> We introduce Bayesian multi-tensor factorization, a model that is the first Bayesian formulation for joint factorization of multiple matrices and tensors. The research problem generalizes the joint matrix-tensor factorization problem to arbitrary sets of tensors of any depth, including matrices, can be interpreted as unsupervised multi-view learning from multiple data tensors, and can be generalized to relax the usual trilinear tensor factorization assumptions. The result is a factorization of the set of tensors into factors shared by any subsets of the tensors, and factors private to individual tensors. We demonstrate the performance against existing baselines in multiple tensor factorization tasks in structural toxicogenomics and functional neuroimaging.

</details>

<details>

<summary>2016-10-13 14:18:25 - Robust Bayesian target detection algorithm for depth imaging from sparse single-photon data</summary>

- *Yoann Altmann, Ximing Ren, Aongus McCarthy, Gerald S. Buller, Steve McLaughlin*

- `1601.06149v2` - [abs](http://arxiv.org/abs/1601.06149v2) - [pdf](http://arxiv.org/pdf/1601.06149v2)

> This paper presents a new Bayesian model and associated algorithm for depth and intensity profiling using full waveforms from time-correlated single-photon counting (TCSPC) measurements in the limit of very low photon counts (i.e., typically less than 20 photons per pixel). The model represents each Lidar waveform as an unknown constant background level, which is combined in the presence of a target, to a known impulse response weighted by the target intensity and finally corrupted by Poisson noise. The joint target detection and depth imaging problem is expressed as a pixel-wise model selection and estimation problem which is solved using Bayesian inference. Prior knowledge about the problem is embedded in a hierarchical model that describes the dependence structure between the model parameters while accounting for their constraints. In particular, Markov random fields (MRFs) are used to model the joint distribution of the background levels and of the target presence labels, which are both expected to exhibit significant spatial correlations. An adaptive Markov chain Monte Carlo algorithm including reversible-jump updates is then proposed to compute the Bayesian estimates of interest. This algorithm is equipped with a stochastic optimization adaptation mechanism that automatically adjusts the parameters of the MRFs by maximum marginal likelihood estimation. Finally, the benefits of the proposed methodology are demonstrated through a series of experiments using real data.

</details>

<details>

<summary>2016-10-13 14:56:08 - Scalable semiparametric inference for the means of heavy-tailed distributions</summary>

- *Matt Taddy, Hedibert Freitas Lopes, Matt Gardner*

- `1602.08066v3` - [abs](http://arxiv.org/abs/1602.08066v3) - [pdf](http://arxiv.org/pdf/1602.08066v3)

> Heavy tailed distributions present a tough setting for inference. They are also common in industrial applications, particularly with Internet transaction datasets, and machine learners often analyze such data without considering the biases and risks associated with the misuse of standard tools. This paper outlines a procedure for inference about the mean of a (possibly conditional) heavy tailed distribution that combines nonparametric analysis for the bulk of the support with Bayesian parametric modeling -- motivated from extreme value theory -- for the heavy tail. The procedure is fast and massively scalable. The resulting point estimators attain lowest-possible error rates and, unique among alternatives, we are able to provide accurate uncertainty quantification for these estimators. The work should find application in settings wherever correct inference is important and reward tails are heavy; we illustrate the framework in causal inference for A/B experiments involving hundreds of millions of users of eBay.com.

</details>

<details>

<summary>2016-10-15 13:48:45 - deBInfer: Bayesian inference for dynamical models of biological systems in R</summary>

- *Philipp H Boersch-Supan, Sadie J Ryan, Leah R Johnson*

- `1605.00021v3` - [abs](http://arxiv.org/abs/1605.00021v3) - [pdf](http://arxiv.org/pdf/1605.00021v3)

> 1. Understanding the mechanisms underlying biological systems, and ultimately, predicting their behaviours in a changing environment requires overcoming the gap between mathematical models and experimental or observational data. Differential equations (DEs) are commonly used to model the temporal evolution of biological systems, but statistical methods for comparing DE models to data and for parameter inference are relatively poorly developed. This is especially problematic in the context of biological systems where observations are often noisy and only a small number of time points may be available. 2. The Bayesian approach offers a coherent framework for parameter inference that can account for multiple sources of uncertainty, while making use of prior information. It offers a rigorous methodology for parameter inference, as well as modelling the link between unobservable model states and parameters, and observable quantities. 3. We present deBInfer, a package for the statistical computing environment R, implementing a Bayesian framework for parameter inference in DEs. deBInfer provides templates for the DE model, the observation model and data likelihood, and the model parameters and their prior distributions. A Markov chain Monte Carlo (MCMC) procedure processes these inputs to estimate the posterior distributions of the parameters and any derived quantities, including the model trajectories. Further functionality is provided to facilitate MCMC diagnostics, the visualisation of the posterior distributions of model parameters and trajectories, and the use of compiled DE models for improved computational performance. 4. The templating approach makes deBInfer applicable to a wide range of DE models. We demonstrate its application to ordinary and delay DE models for population ecology.

</details>

<details>

<summary>2016-10-17 15:25:56 - Spatio-temporal Gaussian processes modeling of dynamical systems in systems biology</summary>

- *Mu Niu, Zhenwen Dai, Neil Lawrence, Kolja Becker*

- `1610.05163v1` - [abs](http://arxiv.org/abs/1610.05163v1) - [pdf](http://arxiv.org/pdf/1610.05163v1)

> Quantitative modeling of post-transcriptional regulation process is a challenging problem in systems biology. A mechanical model of the regulatory process needs to be able to describe the available spatio-temporal protein concentration and mRNA expression data and recover the continuous spatio-temporal fields. Rigorous methods are required to identify model parameters. A promising approach to deal with these difficulties is proposed using Gaussian process as a prior distribution over the latent function of protein concentration and mRNA expression. In this study, we consider a partial differential equation mechanical model with differential operators and latent function. Since the operators at stake are linear, the information from the physical model can be encoded into the kernel function. Hybrid Monte Carlo methods are employed to carry out Bayesian inference of the partial differential equation parameters and Gaussian process kernel parameters. The spatio-temporal field of protein concentration and mRNA expression are reconstructed without explicitly solving the partial differential equation.

</details>

<details>

<summary>2016-10-17 18:46:27 - A Bayesian Group Sparse Multi-Task Regression Model for Imaging Genetics</summary>

- *Keelin Greenlaw, Elena Szefer, Jinko Graham, Mary Lesperance, Farouk S. Nathoo*

- `1605.02234v2` - [abs](http://arxiv.org/abs/1605.02234v2) - [pdf](http://arxiv.org/pdf/1605.02234v2)

> Motivation: Recent advances in technology for brain imaging and high-throughput genotyping have motivated studies examining the influence of genetic variation on brain structure. Wang et al. (Bioinformatics, 2012) have developed an approach for the analysis of imaging genomic studies using penalized multi-task regression with regularization based on a novel group $l_{2,1}$-norm penalty which encourages structured sparsity at both the gene level and SNP level. While incorporating a number of useful features, the proposed method only furnishes a point estimate of the regression coefficients; techniques for conducting statistical inference are not provided. A new Bayesian method is proposed here to overcome this limitation.   Results: We develop a Bayesian hierarchical modeling formulation where the posterior mode corresponds to the estimator proposed by Wang et al. (Bioinformatics, 2012), and an approach that allows for full posterior inference including the construction of interval estimates for the regression parameters. We show that the proposed hierarchical model can be expressed as a three-level Gaussian scale mixture and this representation facilitates the use of a Gibbs sampling algorithm for posterior simulation. Simulation studies demonstrate that the interval estimates obtained using our approach achieve adequate coverage probabilities that outperform those obtained from the nonparametric bootstrap. Our proposed methodology is applied to the analysis of neuroimaging and genetic data collected as part of the Alzheimer's Disease Neuroimaging Initiative (ADNI), and this analysis of the ADNI cohort demonstrates clearly the value added of incorporating interval estimation beyond only point estimation when relating SNPs to brain imaging endophenotypes.

</details>

<details>

<summary>2016-10-17 20:53:14 - Bayesian inference for the Brown-Resnick process, with an application to extreme low temperatures</summary>

- *Emeric Thibaud, Juha Aalto, Daniel S. Cooley, Anthony C. Davison, Juha Heikkinen*

- `1506.07836v2` - [abs](http://arxiv.org/abs/1506.07836v2) - [pdf](http://arxiv.org/pdf/1506.07836v2)

> The Brown-Resnick max-stable process has proven to be well-suited for modeling extremes of complex environmental processes, but in many applications its likelihood function is intractable and inference must be based on a composite likelihood, thereby preventing the use of classical Bayesian techniques. In this paper we exploit a case in which the full likelihood of a Brown-Resnick process can be calculated, using componentwise maxima and their partitions in terms of individual events, and we propose two new approaches to inference. The first estimates the partitions using declustering, while the second uses random partitions in a Markov chain Monte Carlo algorithm. We use these approaches to construct a Bayesian hierarchical model for extreme low temperatures in northern Fennoscandia.

</details>

<details>

<summary>2016-10-18 13:44:30 - Fast Sampling for Bayesian Max-Margin Models</summary>

- *Wenbo Hu, Jun Zhu, Bo Zhang*

- `1504.07107v5` - [abs](http://arxiv.org/abs/1504.07107v5) - [pdf](http://arxiv.org/pdf/1504.07107v5)

> Bayesian max-margin models have shown superiority in various practical applications, such as text categorization, collaborative prediction, social network link prediction and crowdsourcing, and they conjoin the flexibility of Bayesian modeling and predictive strengths of max-margin learning. However, Monte Carlo sampling for these models still remains challenging, especially for applications that involve large-scale datasets. In this paper, we present the stochastic subgradient Hamiltonian Monte Carlo (HMC) methods, which are easy to implement and computationally efficient. We show the approximate detailed balance property of subgradient HMC which reveals a natural and validated generalization of the ordinary HMC. Furthermore, we investigate the variants that use stochastic subsampling and thermostats for better scalability and mixing. Using stochastic subgradient Markov Chain Monte Carlo (MCMC), we efficiently solve the posterior inference task of various Bayesian max-margin models and extensive experimental results demonstrate the effectiveness of our approach.

</details>

<details>

<summary>2016-10-18 18:35:09 - Deep Amortized Inference for Probabilistic Programs</summary>

- *Daniel Ritchie, Paul Horsfall, Noah D. Goodman*

- `1610.05735v1` - [abs](http://arxiv.org/abs/1610.05735v1) - [pdf](http://arxiv.org/pdf/1610.05735v1)

> Probabilistic programming languages (PPLs) are a powerful modeling tool, able to represent any computable probability distribution. Unfortunately, probabilistic program inference is often intractable, and existing PPLs mostly rely on expensive, approximate sampling-based methods. To alleviate this problem, one could try to learn from past inferences, so that future inferences run faster. This strategy is known as amortized inference; it has recently been applied to Bayesian networks and deep generative models. This paper proposes a system for amortized inference in PPLs. In our system, amortization comes in the form of a parameterized guide program. Guide programs have similar structure to the original program, but can have richer data flow, including neural network components. These networks can be optimized so that the guide approximately samples from the posterior distribution defined by the original program. We present a flexible interface for defining guide programs and a stochastic gradient-based scheme for optimizing guide parameters, as well as some preliminary results on automatically deriving guide programs. We explore in detail the common machine learning pattern in which a 'local' model is specified by 'global' random values and used to generate independent observed data points; this gives rise to amortized local inference supporting global model learning.

</details>

<details>

<summary>2016-10-18 21:26:29 - Bayesian Variable Selection for Linear Regression with the $Îº$-$G$ Priors</summary>

- *Zichen Ma, Ernest FokouÃ©*

- `1503.06370v2` - [abs](http://arxiv.org/abs/1503.06370v2) - [pdf](http://arxiv.org/pdf/1503.06370v2)

> In this paper, we introduce a new methodology for Bayesian variable selection in linear regression that is independent of the traditional indicator method. A diagonal matrix $\mathbf{G}$ is introduced to the prior of the coefficient vector $\boldsymbol{\beta}$, with each of the $g_j$'s, bounded between $0$ and $1$, on the diagonal serves as a stabilizer of the corresponding $\beta_j$. Mathematically, a promising variable has a $g_j$ value that is close to $0$, whereas the value of $g_j$ corresponding to an unpromising variable is close to $1$. This property is proven in this paper under orthogonality together with other asymptotic properties. Computationally, the sample path of each $g_j$ is obtained through Metropolis-within-Gibbs sampling method. Also, in this paper we give two simulations to verify the capability of this methodology in variable selection.

</details>

<details>

<summary>2016-10-18 22:25:58 - phylodyn: an R package for phylodynamic simulation and inference</summary>

- *Michael D. Karcher, Julia A. Palacios, Shiwei Lan, Vladimir N. Minin*

- `1610.05817v1` - [abs](http://arxiv.org/abs/1610.05817v1) - [pdf](http://arxiv.org/pdf/1610.05817v1)

> We introduce phylodyn, an R package for phylodynamic analysis based on gene genealogies. The package main functionality is Bayesian nonparametric estimation of effective population size fluctuations over time. Our implementation includes several Markov chain Monte Carlo-based methods and an integrated nested Laplace approximation-based approach for phylodynamic inference that have been developed in recent years. Genealogical data describe the timed ancestral relationships of individuals sampled from a population of interest. Here, individuals are assumed to be sampled at the same point in time (isochronous sampling) or at different points in time (heterochronous sampling); in addition, sampling events can be modeled with preferential sampling, which means that the intensity of sampling events is allowed to depend on the effective population size trajectory. We assume the coalescent and the sequentially Markov coalescent processes as generative models of genealogies. We include several coalescent simulation functions that are useful for testing our phylodynamics methods via simulation studies. We compare the performance and outputs of various methods implemented in phylodyn and outline their strengths and weaknesses. R package phylodyn is available at https://github.com/mdkarcher/phylodyn.

</details>

<details>

<summary>2016-10-19 10:16:46 - A Bayesian Approach to Estimation of Speaker Normalization Parameters</summary>

- *Dhananjay Ram, Debasis Kundu, Rajesh M. Hegde*

- `1610.05948v1` - [abs](http://arxiv.org/abs/1610.05948v1) - [pdf](http://arxiv.org/pdf/1610.05948v1)

> In this work, a Bayesian approach to speaker normalization is proposed to compensate for the degradation in performance of a speaker independent speech recognition system. The speaker normalization method proposed herein uses the technique of vocal tract length normalization (VTLN). The VTLN parameters are estimated using a novel Bayesian approach which utilizes the Gibbs sampler, a special type of Markov Chain Monte Carlo method. Additionally the hyperparameters are estimated using maximum likelihood approach. This model is used assuming that human vocal tract can be modeled as a tube of uniform cross section. It captures the variation in length of the vocal tract of different speakers more effectively, than the linear model used in literature. The work has also investigated different methods like minimization of Mean Square Error (MSE) and Mean Absolute Error (MAE) for the estimation of VTLN parameters. Both single pass and two pass approaches are then used to build a VTLN based speech recognizer. Experimental results on recognition of vowels and Hindi phrases from a medium vocabulary indicate that the Bayesian method improves the performance by a considerable margin.

</details>

<details>

<summary>2016-10-21 00:21:30 - Sampling hyperparameters in hierarchical models: improving on Gibbs for high-dimensional latent fields and large data sets</summary>

- *Richard A. Norton, J. Andres Christen, Colin Fox*

- `1610.06632v1` - [abs](http://arxiv.org/abs/1610.06632v1) - [pdf](http://arxiv.org/pdf/1610.06632v1)

> We consider posterior sampling in the very common Bayesian hierarchical model in which observed data depends on high-dimensional latent variables that, in turn, depend on relatively few hyperparameters. When the full conditional over the latent variables has a known form, the marginal posterior distribution over hyperparameters is accessible and can be sampled using a Markov chain Monte Carlo (MCMC) method on a low-dimensional parameter space. This may improve computational efficiency over standard Gibbs sampling since computation is not over the high-dimensional space of latent variables and correlations between hyperparameters and latent variables become irrelevant. When the marginal posterior over hyperparameters depends on a fixed-dimensional sufficient statistic, precomputation of the sufficient statistic renders the cost of the low-dimensional MCMC independent of data size. Then, when the hyperparameters are the primary variables of interest, inference may be performed in big-data settings at modest cost. Moreover, since the form of the full conditional for the latent variables does not depend on the form of the hyperprior distribution, the method imposes no restriction on the hyperprior, unlike Gibbs sampling that typically requires conjugate distributions. We demonstrate these efficiency gains in four computed examples.

</details>

<details>

<summary>2016-10-21 04:18:11 - Stochastic Gradient MCMC with Stale Gradients</summary>

- *Changyou Chen, Nan Ding, Chunyuan Li, Yizhe Zhang, Lawrence Carin*

- `1610.06664v1` - [abs](http://arxiv.org/abs/1610.06664v1) - [pdf](http://arxiv.org/pdf/1610.06664v1)

> Stochastic gradient MCMC (SG-MCMC) has played an important role in large-scale Bayesian learning, with well-developed theoretical convergence properties. In such applications of SG-MCMC, it is becoming increasingly popular to employ distributed systems, where stochastic gradients are computed based on some outdated parameters, yielding what are termed stale gradients. While stale gradients could be directly used in SG-MCMC, their impact on convergence properties has not been well studied. In this paper we develop theory to show that while the bias and MSE of an SG-MCMC algorithm depend on the staleness of stochastic gradients, its estimation variance (relative to the expected estimate, based on a prescribed number of samples) is independent of it. In a simple Bayesian distributed system with SG-MCMC, where stale gradients are computed asynchronously by a set of workers, our theory indicates a linear speedup on the decrease of estimation variance w.r.t. the number of workers. Experiments on synthetic data and deep neural networks validate our theory, demonstrating the effectiveness and scalability of SG-MCMC with stale gradients.

</details>

<details>

<summary>2016-10-21 04:28:15 - On the Convergence of Stochastic Gradient MCMC Algorithms with High-Order Integrators</summary>

- *Changyou Chen, Nan Ding, Lawrence Carin*

- `1610.06665v1` - [abs](http://arxiv.org/abs/1610.06665v1) - [pdf](http://arxiv.org/pdf/1610.06665v1)

> Recent advances in Bayesian learning with large-scale data have witnessed emergence of stochastic gradient MCMC algorithms (SG-MCMC), such as stochastic gradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian MCMC (SGHMC), and the stochastic gradient thermostat. While finite-time convergence properties of the SGLD with a 1st-order Euler integrator have recently been studied, corresponding theory for general SG-MCMCs has not been explored. In this paper we consider general SG-MCMCs with high-order integrators, and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures. Our theoretical results show faster convergence rates and more accurate invariant measures for SG-MCMCs with higher-order integrators. For example, with the proposed efficient 2nd-order symmetric splitting integrator, the {\em mean square error} (MSE) of the posterior average for the SGHMC achieves an optimal convergence rate of $L^{-4/5}$ at $L$ iterations, compared to $L^{-2/3}$ for the SGHMC and SGLD with 1st-order Euler integrators. Furthermore, convergence results of decreasing-step-size SG-MCMCs are also developed, with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence. Experiments on both synthetic and real datasets verify our theory, and show advantages of the proposed method in two large-scale real applications.

</details>

<details>

<summary>2016-10-21 08:13:25 - Robust Bayesian Compressed sensing</summary>

- *Qian Wan, Huiping Duan, Jun Fang, Hongbin Li*

- `1610.02807v2` - [abs](http://arxiv.org/abs/1610.02807v2) - [pdf](http://arxiv.org/pdf/1610.02807v2)

> We consider the problem of robust compressed sensing whose objective is to recover a high-dimensional sparse signal from compressed measurements corrupted by outliers. A new sparse Bayesian learning method is developed for robust compressed sensing. The basic idea of the proposed method is to identify and remove the outliers from sparse signal recovery. To automatically identify the outliers, we employ a set of binary indicator hyperparameters to indicate which observations are outliers. These indicator hyperparameters are treated as random variables and assigned a beta process prior such that their values are confined to be binary. In addition, a Gaussian-inverse Gamma prior is imposed on the sparse signal to promote sparsity. Based on this hierarchical prior model, we develop a variational Bayesian method to estimate the indicator hyperparameters as well as the sparse signal. Simulation results show that the proposed method achieves a substantial performance improvement over existing robust compressed sensing techniques.

</details>

<details>

<summary>2016-10-21 12:07:47 - Comments on "Bayesian Solution Uncertainty Quantification for Differential Equations" by Chkrebtii, Campbell, Calderhead & Girolami</summary>

- *Francois-Xavier Briol, Jon Cockayne, Onur Teymur*

- `1610.06752v1` - [abs](http://arxiv.org/abs/1610.06752v1) - [pdf](http://arxiv.org/pdf/1610.06752v1)

> We commend the authors for an exciting paper which provides a strong contribution to the emerging field of probabilistic numerics (PN). Below, we discuss aspects of prior modelling which need to be considered thoroughly in future work.

</details>

<details>

<summary>2016-10-22 18:34:43 - Estimating model evidence using data assimilation</summary>

- *Alberto Carrassi, Marc Bocquet, Alexis Hannart, Michael Ghil*

- `1605.01526v2` - [abs](http://arxiv.org/abs/1605.01526v2) - [pdf](http://arxiv.org/pdf/1605.01526v2)

> We review the field of data assimilation (DA) from a Bayesian perspective and show that, in addition to its by now common application to state estimation, DA may be used for model selection. An important special case of the latter is the discrimination between a factual model --- which corresponds, to the best of the modeler's knowledge, to the situation in the actual world in which a sequence of events has occurred --- and a counterfactual model, in which a particular forcing or process might be absent or just quantitatively different from the actual world. Three different ensemble-DA methods are reviewed for this purpose: the ensemble Kalman filter (EnKF), the ensemble four-dimensional variational smoother (En-4D-Var), and the iterative ensemble Kalman smoother (IEnKS). An original contextual formulation of model evidence (CME) is introduced. It is shown how to apply these three methods to compute CME, using the approximated time-dependent probability distribution functions (pdfs) each of them provide in the process of state estimation. The theoretical formulae so derived are applied to two simplified nonlinear and chaotic models: (i) the Lorenz three-variable convection (L63) model, and (ii) the Lorenz 40-variable mid-latitude atmospheric dynamics model (L95). The numerical results of these three DA-based methods and those of an integration based on importance sampling are compared. It is found that better CME estimates are obtained by using DA, and the IEnKS method appears to be best among the DA methods. Differences among the performance of the three DA-based methods are discussed as a function of model properties. Finally, the methodology is implemented for parameter estimation and for event attribution.

</details>

<details>

<summary>2016-10-23 12:27:05 - Stochastic inference with spiking neurons in the high-conductance state</summary>

- *Mihai A. Petrovici, Johannes Bill, Ilja Bytschok, Johannes Schemmel, Karlheinz Meier*

- `1610.07161v1` - [abs](http://arxiv.org/abs/1610.07161v1) - [pdf](http://arxiv.org/pdf/1610.07161v1)

> The highly variable dynamics of neocortical circuits observed in vivo have been hypothesized to represent a signature of ongoing stochastic inference but stand in apparent contrast to the deterministic response of neurons measured in vitro. Based on a propagation of the membrane autocorrelation across spike bursts, we provide an analytical derivation of the neural activation function that holds for a large parameter space, including the high-conductance state. On this basis, we show how an ensemble of leaky integrate-and-fire neurons with conductance-based synapses embedded in a spiking environment can attain the correct firing statistics for sampling from a well-defined target distribution. For recurrent networks, we examine convergence toward stationarity in computer simulations and demonstrate sample-based Bayesian inference in a mixed graphical model. This points to a new computational role of high-conductance states and establishes a rigorous link between deterministic neuron models and functional stochastic dynamics on the network level.

</details>

<details>

<summary>2016-10-23 18:52:36 - Sets of Priors Reflecting Prior-Data Conflict and Agreement</summary>

- *Gero Walter, Frank P. A. Coolen*

- `1610.07217v1` - [abs](http://arxiv.org/abs/1610.07217v1) - [pdf](http://arxiv.org/pdf/1610.07217v1)

> In Bayesian statistics, the choice of prior distribution is often debatable, especially if prior knowledge is limited or data are scarce. In imprecise probability, sets of priors are used to accurately model and reflect prior knowledge. This has the advantage that prior-data conflict sensitivity can be modelled: Ranges of posterior inferences should be larger when prior and data are in conflict. We propose a new method for generating prior sets which, in addition to prior-data conflict sensitivity, allows to reflect strong prior-data agreement by decreased posterior imprecision.

</details>

<details>

<summary>2016-10-23 19:24:59 - Robust Bayesian Reliability for Complex Systems under Prior-Data Conflict</summary>

- *Gero Walter, Frank P. A. Coolen*

- `1610.07222v1` - [abs](http://arxiv.org/abs/1610.07222v1) - [pdf](http://arxiv.org/pdf/1610.07222v1)

> In reliability engineering, data about failure events is often scarce. To arrive at meaningful estimates for the reliability of a system, it is therefore often necessary to also include expert information in the analysis, which is straightforward in the Bayesian approach by using an informative prior distribution. A problem called prior-data conflict then can arise: observed data seem very surprising from the viewpoint of the prior, i.e., information from data is in conflict with prior assumptions. Models based on conjugate priors can be insensitive to prior-data conflict, in the sense that the spread of the posterior distribution does not increase in case of such a conflict, thus conveying a false sense of certainty. An approach to mitigate this issue is presented, by considering sets of prior distributions to model limited knowledge on Weibull distributed component lifetimes, treating systems with arbitrary layout using the survival signature. This approach can be seen as a robust Bayesian procedure or imprecise probability method that reflects surprisingly early or late component failures by wider system reliability bounds.

</details>

<details>

<summary>2016-10-23 21:47:37 - Bayesian Solution Uncertainty Quantification for Differential Equations</summary>

- *Oksana A. Chkrebtii, David A. Campbell, Ben Calderhead, Mark A. Girolami*

- `1306.2365v5` - [abs](http://arxiv.org/abs/1306.2365v5) - [pdf](http://arxiv.org/pdf/1306.2365v5)

> We explore probability modelling of discretization uncertainty for system states defined implicitly by ordinary or partial differential equations. Accounting for this uncertainty can avoid posterior under-coverage when likelihoods are constructed from a coarsely discretized approximation to system equations. A formalism is proposed for inferring a fixed but a priori unknown model trajectory through Bayesian updating of a prior process conditional on model information. A one-step-ahead sampling scheme for interrogating the model is described, its consistency and first order convergence properties are proved, and its computational complexity is shown to be proportional to that of numerical explicit one-step solvers. Examples illustrate the flexibility of this framework to deal with a wide variety of complex and large-scale systems. Within the calibration problem, discretization uncertainty defines a layer in the Bayesian hierarchy, and a Markov chain Monte Carlo algorithm that targets this posterior distribution is presented. This formalism is used for inference on the JAK-STAT delay differential equation model of protein dynamics from indirectly observed measurements. The discussion outlines implications for the new field of probabilistic numerics.

</details>

<details>

<summary>2016-10-24 12:18:00 - Truncated Variance Reduction: A Unified Approach to Bayesian Optimization and Level-Set Estimation</summary>

- *Ilija Bogunovic, Jonathan Scarlett, Andreas Krause, Volkan Cevher*

- `1610.07379v1` - [abs](http://arxiv.org/abs/1610.07379v1) - [pdf](http://arxiv.org/pdf/1610.07379v1)

> We present a new algorithm, truncated variance reduction (TruVaR), that treats Bayesian optimization (BO) and level-set estimation (LSE) with Gaussian processes in a unified fashion. The algorithm greedily shrinks a sum of truncated variances within a set of potential maximizers (BO) or unclassified points (LSE), which is updated based on confidence bounds. TruVaR is effective in several important settings that are typically non-trivial to incorporate into myopic algorithms, including pointwise costs and heteroscedastic noise. We provide a general theoretical guarantee for TruVaR covering these aspects, and use it to recover and strengthen existing results on BO and LSE. Moreover, we provide a new result for a setting where one can select from a number of noise levels having associated costs. We demonstrate the effectiveness of the algorithm on both synthetic and real-world data sets.

</details>

<details>

<summary>2016-10-24 19:12:23 - Some Relationships and Properties of the Hypergeometric Distribution</summary>

- *Peter Peskun*

- `1610.07554v1` - [abs](http://arxiv.org/abs/1610.07554v1) - [pdf](http://arxiv.org/pdf/1610.07554v1)

> The binomial and Poisson distributions have interesting relationships with the beta and gamma distributions, respectively, which involve their cumulative distribution functions and the use of conjugate priors in Bayesian statistics. We briefly discuss these relationships and some properties resulting from them which play an important role in the construction of exact nested two-sided confidence intervals and the computation of two-tailed P-values. The purpose of this article is to show that such relationships also exist between the hypergeometric distribution and a special case of the Polya (or beta-binomial) distribution, and to derive some properties of the hypergeometric distribution resulting from these relationships.   KEY WORDS: Beta, binomial, gamma, Poisson, and Polya (or beta-binomial) distributions; Conjugate prior distribution; Cumulative distribution function; Posterior distribution.

</details>

<details>

<summary>2016-10-24 23:07:16 - A Bayesian Ensemble for Unsupervised Anomaly Detection</summary>

- *Edward Yu, Parth Parekh*

- `1610.07677v1` - [abs](http://arxiv.org/abs/1610.07677v1) - [pdf](http://arxiv.org/pdf/1610.07677v1)

> Methods for unsupervised anomaly detection suffer from the fact that the data is unlabeled, making it difficult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classification and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classifier combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data.

</details>

<details>

<summary>2016-10-25 05:10:49 - Approximate cross-validation formula for Bayesian linear regression</summary>

- *Yoshiyuki Kabashima, Tomoyuki Obuchi, Makoto Uemura*

- `1610.07733v1` - [abs](http://arxiv.org/abs/1610.07733v1) - [pdf](http://arxiv.org/pdf/1610.07733v1)

> Cross-validation (CV) is a technique for evaluating the ability of statistical models/learning systems based on a given data set. Despite its wide applicability, the rather heavy computational cost can prevent its use as the system size grows. To resolve this difficulty in the case of Bayesian linear regression, we develop a formula for evaluating the leave-one-out CV error approximately without actually performing CV. The usefulness of the developed formula is tested by statistical mechanical analysis for a synthetic model. This is confirmed by application to a real-world supernova data set as well.

</details>

<details>

<summary>2016-10-25 08:26:56 - Bayesian Nonparametric Calibration and Combination of Predictive Distributions</summary>

- *Federico Bassetti, Roberto Casarin, Francesco Ravazzolo*

- `1502.07246v2` - [abs](http://arxiv.org/abs/1502.07246v2) - [pdf](http://arxiv.org/pdf/1502.07246v2)

> We introduce a Bayesian approach to predictive density calibration and combination that accounts for parameter uncertainty and model set incompleteness through the use of random calibration functionals and random combination weights. Building on the work of Ranjan, R. and Gneiting, T. (2010) and Gneiting, T. and Ranjan, R. (2013), we use infinite beta mixtures for the calibration. The proposed Bayesian nonparametric approach takes advantage of the flexibility of Dirichlet process mixtures to achieve any continuous deformation of linearly combined predictive distributions. The inference procedure is based on Gibbs sampling and allows accounting for uncertainty in the number of mixture components, mixture weights, and calibration parameters. The weak posterior consistency of the Bayesian nonparametric calibration is provided under suitable conditions for unknown true density. We study the methodology in simulation examples with fat tails and multimodal densities and apply it to density forecasts of daily S&P returns and daily maximum wind speed at the Frankfurt airport.

</details>

<details>

<summary>2016-10-25 11:48:39 - Comments on "Bayesian Solution Uncertainty Quantification for Differential Equations" by Chkrebtii, Campbell, Calderhead & Girolami</summary>

- *Jon Cockayne*

- `1610.08363v1` - [abs](http://arxiv.org/abs/1610.08363v1) - [pdf](http://arxiv.org/pdf/1610.08363v1)

> I would like to thank the authors for their interesting and very clearly presented paper discussing probabilistic solvers for ODEs and PDEs.

</details>

<details>

<summary>2016-10-26 00:10:44 - Fast Bayesian Non-Negative Matrix Factorisation and Tri-Factorisation</summary>

- *Thomas Brouwer, Jes Frellsen, Pietro Lio'*

- `1610.08127v1` - [abs](http://arxiv.org/abs/1610.08127v1) - [pdf](http://arxiv.org/pdf/1610.08127v1)

> We present a fast variational Bayesian algorithm for performing non-negative matrix factorisation and tri-factorisation. We show that our approach achieves faster convergence per iteration and timestep (wall-clock) than Gibbs sampling and non-probabilistic approaches, and do not require additional samples to estimate the posterior. We show that in particular for matrix tri-factorisation convergence is difficult, but our variational Bayesian approach offers a fast solution, allowing the tri-factorisation approach to be used more effectively.

</details>

<details>

<summary>2016-10-26 02:14:03 - Online Bayesian phylogenetic inference: theoretical foundations via Sequential Monte Carlo</summary>

- *Vu Dinh, Aaron E. Darling, Frederick A. Matsen IV*

- `1610.08148v1` - [abs](http://arxiv.org/abs/1610.08148v1) - [pdf](http://arxiv.org/pdf/1610.08148v1)

> Phylogenetics, the inference of evolutionary trees from molecular sequence data such as DNA, is an enterprise that yields valuable evolutionary understanding of many biological systems. Bayesian phylogenetic algorithms, which approximate a posterior distribution on trees, have become a popular if computationally expensive means of doing phylogenetics. Modern data collection technologies are quickly adding new sequences to already substantial databases. With all current techniques for Bayesian phylogenetics, computation must start anew each time a sequence becomes available, making it costly to maintain an up-to-date estimate of a phylogenetic posterior. These considerations highlight the need for an \emph{online} Bayesian phylogenetic method which can update an existing posterior with new sequences.   Here we provide theoretical results on the consistency and stability of methods for online Bayesian phylogenetic inference based on Sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). We first show a consistency result, demonstrating that the method samples from the correct distribution in the limit of a large number of particles. Next we derive the first reported set of bounds on how phylogenetic likelihood surfaces change when new sequences are added. These bounds enable us to characterize the theoretical performance of sampling algorithms by bounding the effective sample size (ESS) with a given number of particles from below. We show that the ESS is guaranteed to grow linearly as the number of particles in an SMC sampler grows. Surprisingly, this result holds even though the dimensions of the phylogenetic model grow with each new added sequence.

</details>

<details>

<summary>2016-10-26 16:09:01 - Kernel Bayesian Inference with Posterior Regularization</summary>

- *Yang Song, Jun Zhu, Yong Ren*

- `1607.02011v2` - [abs](http://arxiv.org/abs/1607.02011v2) - [pdf](http://arxiv.org/pdf/1607.02011v2)

> We propose a vector-valued regression problem whose solution is equivalent to the reproducing kernel Hilbert space (RKHS) embedding of the Bayesian posterior distribution. This equivalence provides a new understanding of kernel Bayesian inference. Moreover, the optimization problem induces a new regularization for the posterior embedding estimator, which is faster and has comparable performance to the squared regularization in kernel Bayes' rule. This regularization coincides with a former thresholding approach used in kernel POMDPs whose consistency remains to be established. Our theoretical work solves this open problem and provides consistency analysis in regression settings. Based on our optimizational formulation, we propose a flexible Bayesian posterior regularization framework which for the first time enables us to put regularization at the distribution level. We apply this method to nonparametric state-space filtering tasks with extremely nonlinear dynamics and show performance gains over all other baselines.

</details>

<details>

<summary>2016-10-26 16:19:47 - Bayesian design of experiments for generalised linear models and dimensional analysis with industrial and scientific application</summary>

- *David C. Woods, Antony M. Overstall, Maria Adamou, Timothy W. Waite*

- `1606.05892v3` - [abs](http://arxiv.org/abs/1606.05892v3) - [pdf](http://arxiv.org/pdf/1606.05892v3)

> The design of an experiment can be always be considered at least implicitly Bayesian, with prior knowledge used informally to aid decisions such as the variables to be studied and the choice of a plausible relationship between the explanatory variables and measured responses. Bayesian methods allow uncertainty in these decisions to be incorporated into design selection through prior distributions that encapsulate information available from scientific knowledge or previous experimentation. Further, a design may be explicitly tailored to the aim of the experiment through a decision-theoretic approach using an appropriate loss function. We review the area of decision-theoretic Bayesian design, with particular emphasis on recent advances in computational methods. For many problems arising in industry and science, experiments result in a discrete response that is well described by a member of the class of generalised linear models. We describe how Gaussian process emulation, commonly used in computer experiments, can play an important role in facilitating Bayesian design for realistic problems. A main focus is the combination of Gaussian process regression to approximate the expected loss with cyclic descent (coordinate exchange) optimisation algorithms to allow optimal designs to be found for previously infeasible problems. We also present the first optimal design results for statistical models formed from dimensional analysis, a methodology widely employed in the engineering and physical sciences to produce parsimonious and interpretable models. Using the famous paper helicopter experiment, we show the potential for the combination of Bayesian design, generalised linear models and dimensional analysis to produce small but informative experiments.

</details>

<details>

<summary>2016-10-26 16:37:56 - Model selection via Bayesian information capacity designs for generalised linear models</summary>

- *David C. Woods, James M. McGree, Susan M. Lewis*

- `1601.08088v3` - [abs](http://arxiv.org/abs/1601.08088v3) - [pdf](http://arxiv.org/pdf/1601.08088v3)

> The first investigation is made of designs for screening experiments where the response variable is approximated by a generalised linear model. A Bayesian information capacity criterion is defined for the selection of designs that are robust to the form of the linear predictor. For binomial data and logistic regression, the effectiveness of these designs for screening is assessed through simulation studies using all-subsets regression and model selection via maximum penalised likelihood and a generalised information criterion. For Poisson data and log-linear regression, similar assessments are made using maximum likelihood and the Akaike information criterion for minimally-supported designs that are constructed analytically. The results show that effective screening, that is, high power with moderate type I error rate and false discovery rate, can be achieved through suitable choices for the number of design support points and experiment size. Logistic regression is shown to present a more challenging problem than log-linear regression. Some areas for future work are also indicated.

</details>

<details>

<summary>2016-10-26 16:58:29 - Emulation of multivariate simulators using thin-plate splines with application to atmospheric dispersion</summary>

- *Veronica E. Bowman, David C. Woods*

- `1512.07451v3` - [abs](http://arxiv.org/abs/1512.07451v3) - [pdf](http://arxiv.org/pdf/1512.07451v3)

> It is often desirable to build a statistical emulator of a complex computer simulator in order to perform analysis which would otherwise be computationally infeasible. We propose methodology to model multivariate output from a computer simulator taking into account output structure in the responses. The utility of this approach is demonstrated by applying it to a chemical and biological hazard prediction model. Predicting the hazard area that results from an accidental or deliberate chemical or biological release is imperative in civil and military planning and also in emergency response. The hazard area resulting from such a release is highly structured in space and we therefore propose the use of a thin-plate spline to capture the spatial structure and fit a Gaussian process emulator to the coefficients of the resultant basis functions. We compare and contrast four different techniques for emulating multivariate output: dimension-reduction using (i) a fully Bayesian approach with a principal component basis, (ii) a fully Bayesian approach with a thin-plate spline basis, assuming that the basis coefficients are independent, and (iii) a "plug-in" Bayesian approach with a thin-plate spline basis and a separable covariance structure; and (iv) a functional data modeling approach using a tensor-product (separable) Gaussian process. We develop methodology for the two thin-plate spline emulators and demonstrate that these emulators significantly outperform the principal component emulator. Further, the separable thin-plate spline emulator, which accounts for the dependence between basis coefficients, provides substantially more realistic quantification of uncertainty, and is also computationally more tractable, allowing fast emulation. For high resolution output data, it also offers substantial predictive and computational advantages over the tensor-product Gaussian process emulator.

</details>

<details>

<summary>2016-10-26 18:27:27 - Body movement to sound interface with vector autoregressive hierarchical hidden Markov models</summary>

- *Dimitrije MarkoviÄ, Borjana ValÄiÄ, NebojÅ¡a MaleÅ¡eviÄ*

- `1610.08450v1` - [abs](http://arxiv.org/abs/1610.08450v1) - [pdf](http://arxiv.org/pdf/1610.08450v1)

> Interfacing a kinetic action of a person to an action of a machine system is an important research topic in many application areas. One of the key factors for intimate human-machine interaction is the ability of the control algorithm to detect and classify different user commands with shortest possible latency, thus making a highly correlated link between cause and effect. In our research, we focused on the task of mapping user kinematic actions into sound samples. The presented methodology relies on the wireless sensor nodes equipped with inertial measurement units and the real-time algorithm dedicated for early detection and classification of a variety of movements/gestures performed by a user. The core algorithm is based on the approximate Bayesian inference of Vector Autoregressive Hierarchical Hidden Markov Models (VAR-HHMM), where models database is derived from the set of motion gestures. The performance of the algorithm was compared with an online version of the K-nearest neighbours (KNN) algorithm, where we used offline expert based classification as the benchmark. In almost all of the evaluation metrics (e.g. confusion matrix, recall and precision scores) the VAR-HHMM algorithm outperformed KNN. Furthermore, the VAR-HHMM algorithm, in some cases, achieved faster movement onset detection compared with the offline standard. The proposed concept, although envisioned for movement-to-sound application, could be implemented in other human-machine interfaces.

</details>

<details>

<summary>2016-10-26 19:07:59 - Bayesian latent structure discovery from multi-neuron recordings</summary>

- *Scott W. Linderman, Ryan P. Adams, Jonathan W. Pillow*

- `1610.08465v1` - [abs](http://arxiv.org/abs/1610.08465v1) - [pdf](http://arxiv.org/pdf/1610.08465v1)

> Neural circuits contain heterogeneous groups of neurons that differ in type, location, connectivity, and basic response properties. However, traditional methods for dimensionality reduction and clustering are ill-suited to recovering the structure underlying the organization of neural circuits. In particular, they do not take advantage of the rich temporal dependencies in multi-neuron recordings and fail to account for the noise in neural spike trains. Here we describe new tools for inferring latent structure from simultaneously recorded spike train data using a hierarchical extension of a multi-neuron point process model commonly known as the generalized linear model (GLM). Our approach combines the GLM with flexible graph-theoretic priors governing the relationship between latent features and neural connectivity patterns. Fully Bayesian inference via P\'olya-gamma augmentation of the resulting model allows us to classify neurons and infer latent dimensions of circuit organization from correlated spike trains. We demonstrate the effectiveness of our method with applications to synthetic data and multi-neuron recordings in primate retina, revealing latent patterns of neural types and locations from spike trains alone.

</details>

<details>

<summary>2016-10-26 19:08:04 - Recurrent switching linear dynamical systems</summary>

- *Scott W. Linderman, Andrew C. Miller, Ryan P. Adams, David M. Blei, Liam Paninski, Matthew J. Johnson*

- `1610.08466v1` - [abs](http://arxiv.org/abs/1610.08466v1) - [pdf](http://arxiv.org/pdf/1610.08466v1)

> Many natural systems, such as neurons firing in the brain or basketball teams traversing a court, give rise to time series data with complex, nonlinear dynamics. We can gain insight into these systems by decomposing the data into segments that are each explained by simpler dynamic units. Building on switching linear dynamical systems (SLDS), we present a new model class that not only discovers these dynamical units, but also explains how their switching behavior depends on observations or continuous latent states. These "recurrent" switching linear dynamical systems provide further insight by discovering the conditions under which each unit is deployed, something that traditional SLDS models fail to do. We leverage recent algorithmic advances in approximate inference to make Bayesian inference in these models easy, fast, and scalable.

</details>

<details>

<summary>2016-10-27 12:13:37 - Things Bayes can't do</summary>

- *Daniil Ryabko*

- `1610.08239v2` - [abs](http://arxiv.org/abs/1610.08239v2) - [pdf](http://arxiv.org/pdf/1610.08239v2)

> The problem of forecasting conditional probabilities of the next event given the past is considered in a general probabilistic setting. Given an arbitrary (large, uncountable) set C of predictors, we would like to construct a single predictor that performs asymptotically as well as the best predictor in C, on any data. Here we show that there are sets C for which such predictors exist, but none of them is a Bayesian predictor with a prior concentrated on C. In other words, there is a predictor with sublinear regret, but every Bayesian predictor must have a linear regret. This negative finding is in sharp contrast with previous results that establish the opposite for the case when one of the predictors in $C$ achieves asymptotically vanishing error. In such a case, if there is a predictor that achieves asymptotically vanishing error for any measure in C, then there is a Bayesian predictor that also has this property, and whose prior is concentrated on (a countable subset of) C.

</details>

<details>

<summary>2016-10-27 13:52:28 - Prior Distributions for Ranking Problems</summary>

- *Toby Kenney, Hao He, Hong Gu*

- `1610.08779v1` - [abs](http://arxiv.org/abs/1610.08779v1) - [pdf](http://arxiv.org/pdf/1610.08779v1)

> The ranking problem is to order a collection of units by some unobserved parameter, based on observations from the associated distribution. This problem arises naturally in a number of contexts, such as business, where we may want to rank potential projects by profitability; or science, where we may want to rank variables potentially associated with some trait by the strength of the association. Most approaches to this problem are empirical Bayesian, where we use the data to estimate the hyperparameters of the prior distribution, then use that distribution to estimate the unobserved parameter values. There are a number of different approaches to this problem, based on different loss functions for mis-ranking units. However, little has been done on the choice of prior distribution. Typical approaches involve choosing a conjugate prior for convenience, and estimating the hyperparameters by MLE from the whole dataset. In this paper, we look in more detail at the effect of choice of prior distribution on Bayesian ranking. We focus on the use of posterior mean for ranking, but many of our conclusions should apply to other ranking criteria, and it is not too difficult to adapt our methods to other choices of prior distributions.

</details>

<details>

<summary>2016-10-27 18:07:27 - Rationalization and Identification of Binary Games with Correlated Types</summary>

- *Nianqing Liu, Quang Vuong, Haiqing Xu*

- `1610.08909v1` - [abs](http://arxiv.org/abs/1610.08909v1) - [pdf](http://arxiv.org/pdf/1610.08909v1)

> This paper studies the rationalization and identification of binary games where players have correlated private types. Allowing for correlation is crucial in global games and in models with social interactions as it represents correlated information and homophily, respectively. Our approach is fully nonparametric in the joint distribution of types and the strategic effects in the payoffs. First, under monotone pure Bayesian Nash Equilibrium strategy, we characterize all the restrictions if any on the distribution of players' choices imposed by the game-theoretic model as well as restrictions associated with two assumptions frequently made in the empirical analysis of discrete games. Namely, we consider exogeneity of payoff shifters relative to private information, and mutual independence of private information given payoff shifters. Second, we study the nonparametric identification of the payoff functions and types distribution. We show that the model with exogenous payoff shifters is fully identified up to a single location--scale normalization under some exclusion restrictions and rank conditions. Third, we discuss partial identification under weaker conditions and multiple equilibria. Lastly, we briefly point out the implications of our results for model testing and estimation.

</details>

<details>

<summary>2016-10-27 19:00:22 - Rapid Posterior Exploration in Bayesian Non-negative Matrix Factorization</summary>

- *M. Arjumand Masood, Finale Doshi-Velez*

- `1610.08928v1` - [abs](http://arxiv.org/abs/1610.08928v1) - [pdf](http://arxiv.org/pdf/1610.08928v1)

> Non-negative Matrix Factorization (NMF) is a popular tool for data exploration. Bayesian NMF promises to also characterize uncertainty in the factorization. Unfortunately, current inference approaches such as MCMC mix slowly and tend to get stuck on single modes. We introduce a novel approach using rapidly-exploring random trees (RRTs) to asymptotically cover regions of high posterior density. These are placed in a principled Bayesian framework via an online extension to nonparametric variational inference. On experiments on real and synthetic data, we obtain greater coverage of the posterior and higher ELBO values than standard NMF inference approaches.

</details>

<details>

<summary>2016-10-27 19:54:12 - On embedded hidden Markov models and particle Markov chain Monte Carlo methods</summary>

- *Axel Finke, Arnaud Doucet, Adam M. Johansen*

- `1610.08962v1` - [abs](http://arxiv.org/abs/1610.08962v1) - [pdf](http://arxiv.org/pdf/1610.08962v1)

> The embedded hidden Markov model (EHMM) sampling method is a Markov chain Monte Carlo (MCMC) technique for state inference in non-linear non-Gaussian state-space models which was proposed in Neal (2003); Neal et al. (2004) and extended in Shestopaloff and Neal (2016). An extension to Bayesian parameter inference was presented in Shestopaloff and Neal (2013). An alternative class of MCMC schemes addressing similar inference problems is provided by particle MCMC (PMCMC) methods (Andrieu et al. 2009; 2010). All these methods rely on the introduction of artificial extended target distributions for multiple state sequences which, by construction, are such that one randomly indexed sequence is distributed according to the posterior of interest. By adapting the Metropolis-Hastings algorithms developed in the framework of PMCMC methods to the EHMM framework, we obtain novel particle filter (PF)-type algorithms for state inference and novel MCMC schemes for parameter and state inference. In addition, we show that most of these algorithms can be viewed as particular cases of a general PF and PMCMC framework. We compare the empirical performance of the various algorithms on low- to high-dimensional state-space models. We demonstrate that a properly tuned conditional PF with "local" MCMC moves proposed in Shestopaloff and Neal (2016) can outperform the standard conditional PF significantly when applied to high-dimensional state-space models while the novel PF-type algorithm could prove to be an interesting alternative to standard PFs for likelihood estimation in some lower-dimensional scenarios.

</details>

<details>

<summary>2016-10-27 23:53:59 - Model Criticism for Bayesian Causal Inference</summary>

- *Dustin Tran, Francisco J. R. Ruiz, Susan Athey, David M. Blei*

- `1610.09037v1` - [abs](http://arxiv.org/abs/1610.09037v1) - [pdf](http://arxiv.org/pdf/1610.09037v1)

> The goal of causal inference is to understand the outcome of alternative courses of action. However, all causal inference requires assumptions. Such assumptions can be more influential than in typical tasks for probabilistic modeling, and testing those assumptions is important to assess the validity of causal inference. We develop model criticism for Bayesian causal inference, building on the idea of posterior predictive checks to assess model fit. Our approach involves decomposing the problem, separately criticizing the model of treatment assignments and the model of outcomes. Conditioned on the assumption of unconfoundedness---that the treatments are assigned independently of the potential outcomes---we show how to check any additional modeling assumption. Our approach provides a foundation for diagnosing model-based causal inferences.

</details>

<details>

<summary>2016-10-28 02:43:26 - Dirichlet Process Mixture Models for Modeling and Generating Synthetic Versions of Nested Categorical Data</summary>

- *Jingchen Hu, Jerome P. Reiter, Quanli Wang*

- `1412.2282v6` - [abs](http://arxiv.org/abs/1412.2282v6) - [pdf](http://arxiv.org/pdf/1412.2282v6)

> We present a Bayesian model for estimating the joint distribution of multivariate categorical data when units are nested within groups. Such data arise frequently in social science settings, for example, people living in households. The model assumes that (i) each group is a member of a group-level latent class, and (ii) each unit is a member of a unit-level latent class nested within its group-level latent class. This structure allows the model to capture dependence among units in the same group. It also facilitates simultaneous modeling of variables at both group and unit levels. We develop a version of the model that assigns zero probability to groups and units with physically impossible combinations of variables. We apply the model to estimate multivariate relationships in a subset of the American Community Survey. Using the estimated model, we generate synthetic household data that could be disseminated as redacted public use files with high analytic validity and low disclosure risks. Supplementary materials for this article are available online.

</details>

<details>

<summary>2016-10-28 18:16:29 - RÃ©nyi Divergence Variational Inference</summary>

- *Yingzhen Li, Richard E. Turner*

- `1602.02311v3` - [abs](http://arxiv.org/abs/1602.02311v3) - [pdf](http://arxiv.org/pdf/1602.02311v3)

> This paper introduces the variational R\'enyi bound (VR) that extends traditional variational inference to R\'enyi's alpha-divergences. This new family of variational methods unifies a number of existing approaches, and enables a smooth interpolation from the evidence lower-bound to the log (marginal) likelihood that is controlled by the value of alpha that parametrises the divergence. The reparameterization trick, Monte Carlo approximation and stochastic optimisation methods are deployed to obtain a tractable and unified framework for optimisation. We further consider negative alpha values and propose a novel variational inference method as a new special case in the proposed framework. Experiments on Bayesian neural networks and variational auto-encoders demonstrate the wide applicability of the VR bound.

</details>

<details>

<summary>2016-10-28 20:57:36 - Moment Matching Based Conjugacy Approximation for Bayesian Ranking and Selection</summary>

- *Qiong Zhang, Yongjia Song*

- `1610.09400v1` - [abs](http://arxiv.org/abs/1610.09400v1) - [pdf](http://arxiv.org/pdf/1610.09400v1)

> We study the conjugacy approximation method in the context of Bayesian ranking and selection with unknown correlations. Under the assumption of normal-inverse-Wishart prior distribution, the posterior distribution remains a normal-inverse-Wishart distribution thanks to the conjugacy property when all alternatives are sampled at each step. However, this conjugacy property no longer holds if only one alternative is sampled at a time, an appropriate setting when there is a limited budget on the number of samples. We propose two new conjugacy approximation methods based on the idea of moment matching. Both of them yield closed-form Bayesian prior updating formulas. This updating formula can then be combined with the knowledge gradient algorithm under the "value of information" framework. We conduct computational experiments to show the superiority of the proposed conjugacy approximation methods, including applications in wind farm placement and computer model calibration.

</details>

<details>

<summary>2016-10-29 10:51:14 - Crowdsourced science: sociotechnical epistemology in the e-research paradigm</summary>

- *David Watson, Luciano Floridi*

- `1610.09485v1` - [abs](http://arxiv.org/abs/1610.09485v1) - [pdf](http://arxiv.org/pdf/1610.09485v1)

> Recent years have seen a surge in online collaboration between experts and amateurs on scientific research. In this article, we analyse the epistemological implications of these crowdsourced projects, with a focus on Zooniverse, the world's largest citizen science web portal. We use quantitative methods to evaluate the platform's success in producing large volumes of observation statements and high impact scientific discoveries relative to more conventional means of data processing. Through empirical evidence, Bayesian reasoning, and conceptual analysis, we show how information and communication technologies enhance the reliability, scalability, and connectivity of crowdsourced e-research, giving online citizen science projects powerful epistemic advantages over more traditional modes of scientific investigation. These results highlight the essential role played by technologically mediated social interaction in contemporary knowledge production. We conclude by calling for an explicitly sociotechnical turn in the philosophy of science that combines insights from statistics and logic to analyse the latest developments in scientific research.

</details>

<details>

<summary>2016-10-31 00:53:08 - Sequential Bayesian Learning for Merton's Jump Model with Stochastic Volatility</summary>

- *Eric Jacquier, Nicholas Polson, Vadim Sokolov*

- `1610.09750v1` - [abs](http://arxiv.org/abs/1610.09750v1) - [pdf](http://arxiv.org/pdf/1610.09750v1)

> Jump stochastic volatility models are central to financial econometrics for volatility forecasting, portfolio risk management, and derivatives pricing. Markov Chain Monte Carlo (MCMC) algorithms are computationally unfeasible for the sequential learning of volatility state variables and parameters, whereby the investor must update all posterior and predictive densities as new information arrives. We develop a particle filtering and learning algorithm to sample posterior distribution in Merton's jump stochastic volatility. This allows to filter spot volatilities and jump times, together with sequentially updating (learning) of jump and volatility parameters. We illustrate our methodology on Google's stock return. We conclude with directions for future research.

</details>

<details>

<summary>2016-10-31 16:23:51 - A Note on the Posterior Inference for the Yule-Simon Distribution</summary>

- *Fabrizio Leisen, Luca Rossini, Cristiano Villa*

- `1604.07304v2` - [abs](http://arxiv.org/abs/1604.07304v2) - [pdf](http://arxiv.org/pdf/1604.07304v2)

> The Yule--Simon distribution has been out of the radar of the Bayesian community, so far. In this note, we propose an explicit Gibbs sampling scheme when a Gamma prior is chosen for the shape parameter. The performance of the algorithm is illustrated with simulation studies, including count data regression, and a real data application to text analysis. We compare our proposal to the frequentist counterparts showing better performance of our algorithm when a small sample size is considered.

</details>


## 2016-11

<details>

<summary>2016-11-01 13:01:03 - Universality of Bayesian mixture predictors</summary>

- *Daniil Ryabko*

- `1610.08249v2` - [abs](http://arxiv.org/abs/1610.08249v2) - [pdf](http://arxiv.org/pdf/1610.08249v2)

> The problem is that of sequential probability forecasting for finite-valued time series. The data is generated by an unknown probability distribution over the space of all one-way infinite sequences. It is known that this measure belongs to a given set C, but the latter is completely arbitrary (uncountably infinite, without any structure given). The performance is measured with asymptotic average log loss. In this work it is shown that the minimax asymptotic performance is always attainable, and it is attained by a convex combination of a countably many measures from the set C (a Bayesian mixture). This was previously only known for the case when the best achievable asymptotic error is 0. This also contrasts previous results that show that in the non-realizable case all Bayesian mixtures may be suboptimal, while there is a predictor that achieves the optimal performance.

</details>

<details>

<summary>2016-11-01 13:22:12 - Apocalypse Now? Reviving the Doomsday Argument</summary>

- *Fergus Simpson*

- `1611.03072v1` - [abs](http://arxiv.org/abs/1611.03072v1) - [pdf](http://arxiv.org/pdf/1611.03072v1)

> Whether the fate of our species can be forecast from its past has been the topic of considerable controversy. One refutation of the so-called Doomsday Argument is based on the premise that we are more likely to exist in a universe containing a greater number of observers. Here we present a Bayesian reformulation of the Doomsday Argument which is immune to this effect. By marginalising over the spatial configuration of observers, we find that any preference for a larger total number of observers has no impact on the inferred local number. Our results remain unchanged when we adopt either the Self-Indexing Assumption (SIA) or the Self-Sampling Assumption (SSA). Furthermore the median value of our posterior distribution is found to be in agreement with the frequentist forecast. Humanity's prognosis for the coming century is well approximated by a global catastrophic risk of 0.2% per year.

</details>

<details>

<summary>2016-11-02 01:15:31 - DNest4: Diffusive Nested Sampling in C++ and Python</summary>

- *Brendon J. Brewer, Daniel Foreman-Mackey*

- `1606.03757v4` - [abs](http://arxiv.org/abs/1606.03757v4) - [pdf](http://arxiv.org/pdf/1606.03757v4)

> In probabilistic (Bayesian) inferences, we typically want to compute properties of the posterior distribution, describing knowledge of unknown quantities in the context of a particular dataset and the assumed prior information. The marginal likelihood, also known as the "evidence", is a key quantity in Bayesian model selection. The Diffusive Nested Sampling algorithm, a variant of Nested Sampling, is a powerful tool for generating posterior samples and estimating marginal likelihoods. It is effective at solving complex problems including many where the posterior distribution is multimodal or has strong dependencies between variables. DNest4 is an open source (MIT licensed), multi-threaded implementation of this algorithm in C++11, along with associated utilities including: i) RJObject, a class template for finite mixture models, (ii) A Python package allowing basic use without C++ coding, and iii) Experimental support for models implemented in Julia. In this paper we demonstrate DNest4 usage through examples including simple Bayesian data analysis, finite mixture models, and Approximate Bayesian Computation.

</details>

<details>

<summary>2016-11-02 02:32:05 - Natural-Parameter Networks: A Class of Probabilistic Neural Networks</summary>

- *Hao Wang, Xingjian Shi, Dit-Yan Yeung*

- `1611.00448v1` - [abs](http://arxiv.org/abs/1611.00448v1) - [pdf](http://arxiv.org/pdf/1611.00448v1)

> Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance.

</details>

<details>

<summary>2016-11-02 02:49:44 - Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks</summary>

- *Hao Wang, Xingjian Shi, Dit-Yan Yeung*

- `1611.00454v1` - [abs](http://arxiv.org/abs/1611.00454v1) - [pdf](http://arxiv.org/pdf/1611.00454v1)

> Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.

</details>

<details>

<summary>2016-11-02 10:48:04 - A nonparametric HMM for genetic imputation and coalescent inference</summary>

- *Lloyd T. Elliott, Yee Whye Teh*

- `1611.00544v1` - [abs](http://arxiv.org/abs/1611.00544v1) - [pdf](http://arxiv.org/pdf/1611.00544v1)

> Genetic sequence data are well described by hidden Markov models (HMMs) in which latent states correspond to clusters of similar mutation patterns. Theory from statistical genetics suggests that these HMMs are nonhomogeneous (their transition probabilities vary along the chromosome) and have large support for self transitions. We develop a new nonparametric model of genetic sequence data, based on the hierarchical Dirichlet process, which supports these self transitions and nonhomogeneity. Our model provides a parameterization of the genetic process that is more parsimonious than other more general nonparametric models which have previously been applied to population genetics. We provide truncation-free MCMC inference for our model using a new auxiliary sampling scheme for Bayesian nonparametric HMMs. In a series of experiments on male X chromosome data from the Thousand Genomes Project and also on data simulated from a population bottleneck we show the benefits of our model over the popular finite model fastPHASE, which can itself be seen as a parametric truncation of our model. We find that the number of HMM states found by our model is correlated with the time to the most recent common ancestor in population bottlenecks. This work demonstrates the flexibility of Bayesian nonparametrics applied to large and complex genetic data.

</details>

<details>

<summary>2016-11-02 16:57:42 - Bayesian Heuristics for Group Decisions</summary>

- *M. Amin Rahimian, Ali Jadbabaie*

- `1611.01006v1` - [abs](http://arxiv.org/abs/1611.01006v1) - [pdf](http://arxiv.org/pdf/1611.01006v1)

> We propose a model of inference and heuristic decision-making in groups that is rooted in the Bayes rule but avoids the complexities of rational inference in partially observed environments with incomplete information, which are characteristic of group interactions. Our model is also consistent with a dual-process psychological theory of thinking: the group members behave rationally at the initiation of their interactions with each other (the slow and deliberative mode); however, in the ensuing decision epochs, they rely on a heuristic that replicates their experiences from the first stage (the fast automatic mode). We specialize this model to a group decision scenario where private observations are received at the beginning, and agents aim to take the best action given the aggregate observations of all group members. We study the implications of the information structure together with the properties of the probability distributions which determine the structure of the so-called "Bayesian heuristics" that the agents follow in our model. We also analyze the group decision outcomes in two classes of linear action updates and log-linear belief updates and show that many inefficiencies arise in group decisions as a result of repeated interactions between individuals, leading to overconfident beliefs as well as choice-shifts toward extremes. Nevertheless, balanced regular structures demonstrate a measure of efficiency in terms of aggregating the initial information of individuals. These results not only verify some well-known insights about group decision-making but also complement these insights by revealing additional mechanistic interpretations for the group declension-process, as well as psychological and cognitive intuitions about the group interaction model.

</details>

<details>

<summary>2016-11-02 18:30:49 - A Pareto scale-inflated outlier model and its Bayesian analysis</summary>

- *David P. M. Scollnik*

- `1611.00717v1` - [abs](http://arxiv.org/abs/1611.00717v1) - [pdf](http://arxiv.org/pdf/1611.00717v1)

> This paper develops a Pareto scale-inflated outlier model. This model is intended for use when data from some standard Pareto distribution of interest is suspected to have been contaminated with a relatively small number of outliers from a Pareto distribution with the same shape parameter but with an inflated scale parameter. The Bayesian analysis of this Pareto scale-inflated outlier model is considered and its implementation using the Gibbs sampler is discussed. The paper contains three worked illustrative examples, two of which feature actual insurance claims data.

</details>

<details>

<summary>2016-11-02 21:26:26 - Gaussian Processes for Survival Analysis</summary>

- *Tamara FernÃ¡ndez, NicolÃ¡s Rivera, Yee Whye Teh*

- `1611.00817v1` - [abs](http://arxiv.org/abs/1611.00817v1) - [pdf](http://arxiv.org/pdf/1611.00817v1)

> We introduce a semi-parametric Bayesian model for survival analysis. The model is centred on a parametric baseline hazard, and uses a Gaussian process to model variations away from it nonparametrically, as well as dependence on covariates. As opposed to many other methods in survival analysis, our framework does not impose unnecessary constraints in the hazard rate or in the survival function. Furthermore, our model handles left, right and interval censoring mechanisms common in survival analysis. We propose a MCMC algorithm to perform inference and an approximation scheme based on random Fourier features to make computations faster. We report experimental results on synthetic and real data, showing that our model performs better than competing models such as Cox proportional hazards, ANOVA-DDP and random survival forests.

</details>

<details>

<summary>2016-11-03 02:57:26 - Tensor Decomposition via Variational Auto-Encoder</summary>

- *Bin Liu, Zenglin Xu, Yingming Li*

- `1611.00866v1` - [abs](http://arxiv.org/abs/1611.00866v1) - [pdf](http://arxiv.org/pdf/1611.00866v1)

> Tensor decomposition is an important technique for capturing the high-order interactions among multiway data. Multi-linear tensor composition methods, such as the Tucker decomposition and the CANDECOMP/PARAFAC (CP), assume that the complex interactions among objects are multi-linear, and are thus insufficient to represent nonlinear relationships in data. Another assumption of these methods is that a predefined rank should be known. However, the rank of tensors is hard to estimate, especially for cases with missing values. To address these issues, we design a Bayesian generative model for tensor decomposition. Different from the traditional Bayesian methods, the high-order interactions of tensor entries are modeled with variational auto-encoder. The proposed model takes advantages of Neural Networks and nonparametric Bayesian models, by replacing the multi-linear product in traditional Bayesian tensor decomposition with a complex nonlinear function (via Neural Networks) whose parameters can be learned from data. Experimental results on synthetic data and real-world chemometrics tensor data have demonstrated that our new model can achieve significantly higher prediction performance than the state-of-the-art tensor decomposition approaches.

</details>

<details>

<summary>2016-11-03 15:45:06 - Maxima Units Search (MUS) algorithm: methodology and applications</summary>

- *Leonardo Egidi, Roberta PappadÃ , Francesco Pauli, Nicola Torelli*

- `1611.01069v1` - [abs](http://arxiv.org/abs/1611.01069v1) - [pdf](http://arxiv.org/pdf/1611.01069v1)

> An algorithm for extracting identity submatrices of small rank and pivotal units from large and sparse matrices is proposed. The procedure has already been satisfactorily applied for solving the label switching problem in Bayesian mixture models. Here we introduce it on its own and explore possible applications in different contexts.

</details>

<details>

<summary>2016-11-04 08:27:03 - Clustering dynamics in a class of normalised generalised gamma dependent priors</summary>

- *Matteo Ruggiero, Matteo Sordello*

- `1608.00733v2` - [abs](http://arxiv.org/abs/1608.00733v2) - [pdf](http://arxiv.org/pdf/1608.00733v2)

> Normalised generalised gamma processes are random probability measures that induce nonparametric prior distributions widely used in Bayesian statistics, particularly for mixture modelling. We construct a class of dependent normalised generalised gamma priors induced by a stationary population model of Moran type, which exploits a generalised P\'olya urn scheme associated with the prior. We study the asymptotic scaling for the dynamics of the number of clusters in the sample, which in turn provides a dynamic measure of diversity in the underlying population. The limit is formalised to be a positive nonstationary diffusion process which falls outside well known families, with unbounded drift and an entrance boundary at the origin. We also introduce a new class of stationary positive diffusions, whose invariant measures are explicit and have power law tails, which approximate weakly the scaling limit.

</details>

<details>

<summary>2016-11-04 14:11:42 - Bayesian inference for continuous time animal movement based on steps and turns</summary>

- *Alison Parton, Paul G. Blackwell, Anna Skarin*

- `1608.05583v2` - [abs](http://arxiv.org/abs/1608.05583v2) - [pdf](http://arxiv.org/pdf/1608.05583v2)

> Although animal locations gained via GPS, etc. are typically observed on a discrete time scale, movement models formulated in continuous time are preferable in order to avoid the struggles experienced in discrete time when faced with irregular observations or the prospect of comparing analyses on different time scales. A class of models able to emulate a range of movement ideas are defined by representing movement as a combination of stochastic processes describing both speed and bearing. A method for Bayesian inference for such models is described through the use of a Markov chain Monte Carlo approach. Such inference relies on an augmentation of the animal's locations in discrete time that have been observed with error, with a more detailed movement path gained via simulation techniques. Analysis on real data on an individual reindeer (Rangifer tarandus) illustrates the presented methods.

</details>

<details>

<summary>2016-11-04 16:16:24 - Kullback-Leibler Divergence for the Normal-Gamma Distribution</summary>

- *Joram Soch, Carsten Allefeld*

- `1611.01437v1` - [abs](http://arxiv.org/abs/1611.01437v1) - [pdf](http://arxiv.org/pdf/1611.01437v1)

> We derive the Kullback-Leibler divergence for the normal-gamma distribution and show that it is identical to the Bayesian complexity penalty for the univariate general linear model with conjugate priors. Based on this finding, we provide two applications of the KL divergence, one in simulated and one in empirical data.

</details>

<details>

<summary>2016-11-04 16:41:50 - Estimating the marginal likelihood with Integrated nested Laplace approximation (INLA)</summary>

- *Aliaksandr Hubin, Geir Storvik*

- `1611.01450v1` - [abs](http://arxiv.org/abs/1611.01450v1) - [pdf](http://arxiv.org/pdf/1611.01450v1)

> The marginal likelihood is a well established model selection criterion in Bayesian statistics. It also allows to efficiently calculate the marginal posterior model probabilities that can be used for Bayesian model averaging of quantities of interest. For many complex models, including latent modeling approaches, marginal likelihoods are however difficult to compute. One recent promising approach for approximating the marginal likelihood is Integrated Nested Laplace Approximation (INLA), design for models with latent Gaussian structures. In this study we compare the approximations obtained with INLA to some alternative approaches on a number of examples of different complexity. In particular we address a simple linear latent model, a Bayesian linear regression model, logistic Bayesian regression models with probit and logit links, and a Poisson longitudinal generalized linear mixed model.

</details>

<details>

<summary>2016-11-04 18:03:38 - Exact Inference Techniques for the Analysis of Bayesian Attack Graphs</summary>

- *Luis MuÃ±oz-GonzÃ¡lez, Daniele Sgandurra, MartÃ­n BarrÃ¨re, Emil Lupu*

- `1510.02427v2` - [abs](http://arxiv.org/abs/1510.02427v2) - [pdf](http://arxiv.org/pdf/1510.02427v2)

> Attack graphs are a powerful tool for security risk assessment by analysing network vulnerabilities and the paths attackers can use to compromise network resources. The uncertainty about the attacker's behaviour makes Bayesian networks suitable to model attack graphs to perform static and dynamic analysis. Previous approaches have focused on the formalization of attack graphs into a Bayesian model rather than proposing mechanisms for their analysis. In this paper we propose to use efficient algorithms to make exact inference in Bayesian attack graphs, enabling the static and dynamic network risk assessments. To support the validity of our approach we have performed an extensive experimental evaluation on synthetic Bayesian attack graphs with different topologies, showing the computational advantages in terms of time and memory use of the proposed techniques when compared to existing approaches.

</details>

<details>

<summary>2016-11-04 19:33:35 - Estimating Causal Direction and Confounding of Two Discrete Variables</summary>

- *Krzysztof Chalupka, Frederick Eberhardt, Pietro Perona*

- `1611.01504v1` - [abs](http://arxiv.org/abs/1611.01504v1) - [pdf](http://arxiv.org/pdf/1611.01504v1)

> We propose a method to classify the causal relationship between two discrete variables given only the joint distribution of the variables, acknowledging that the method is subject to an inherent baseline error. We assume that the causal system is acyclicity, but we do allow for hidden common causes. Our algorithm presupposes that the probability distributions $P(C)$ of a cause $C$ is independent from the probability distribution $P(E\mid C)$ of the cause-effect mechanism. While our classifier is trained with a Bayesian assumption of flat hyperpriors, we do not make this assumption about our test data. This work connects to recent developments on the identifiability of causal models over continuous variables under the assumption of "independent mechanisms". Carefully-commented Python notebooks that reproduce all our experiments are available online at http://vision.caltech.edu/~kchalupk/code.html.

</details>

<details>

<summary>2016-11-06 00:37:39 - Communication-Efficient Distributed Statistical Inference</summary>

- *Michael I. Jordan, Jason D. Lee, Yun Yang*

- `1605.07689v3` - [abs](http://arxiv.org/abs/1605.07689v3) - [pdf](http://arxiv.org/pdf/1605.07689v3)

> We present a Communication-efficient Surrogate Likelihood (CSL) framework for solving distributed statistical inference problems. CSL provides a communication-efficient surrogate to the global likelihood that can be used for low-dimensional estimation, high-dimensional regularized estimation and Bayesian inference. For low-dimensional estimation, CSL provably improves upon naive averaging schemes and facilitates the construction of confidence intervals. For high-dimensional regularized estimation, CSL leads to a minimax-optimal estimator with controlled communication cost. For Bayesian inference, CSL can be used to form a communication-efficient quasi-posterior distribution that converges to the true posterior. This quasi-posterior procedure significantly improves the computational efficiency of MCMC algorithms even in a non-distributed setting. We present both theoretical analysis and experiments to explore the properties of the CSL approximation.

</details>

<details>

<summary>2016-11-07 12:15:47 - Bayesian Learning without Recall</summary>

- *M. Amin Rahimian, Ali Jadbabaie*

- `1601.06103v2` - [abs](http://arxiv.org/abs/1601.06103v2) - [pdf](http://arxiv.org/pdf/1601.06103v2)

> We analyze a model of learning and belief formation in networks in which agents follow Bayes rule yet they do not recall their history of past observations and cannot reason about how other agents' beliefs are formed. They do so by making rational inferences about their observations which include a sequence of independent and identically distributed private signals as well as the actions of their neighboring agents at each time. Successive applications of Bayes rule to the entire history of past observations lead to forebodingly complex inferences: due to lack of knowledge about the global network structure, and unavailability of private observations, as well as third party interactions preceding every decision. Such difficulties make Bayesian updating of beliefs an implausible mechanism for social learning. To address these complexities, we consider a Bayesian without Recall model of inference. On the one hand, this model provides a tractable framework for analyzing the behavior of rational agents in social networks. On the other hand, this model also provides a behavioral foundation for the variety of non-Bayesian update rules in the literature. We present the implications of various choices for the structure of the action space and utility functions for such agents and investigate the properties of learning, convergence, and consensus in special cases.

</details>

<details>

<summary>2016-11-07 18:32:21 - Bayesian fractional posteriors</summary>

- *Anirban Bhattacharya, Debdeep Pati, Yun Yang*

- `1611.01125v2` - [abs](http://arxiv.org/abs/1611.01125v2) - [pdf](http://arxiv.org/pdf/1611.01125v2)

> We consider the fractional posterior distribution that is obtained by updating a prior distribution via Bayes theorem with a fractional likelihood function, a usual likelihood function raised to a fractional power. First, we analyze the contraction property of the fractional posterior in a general misspecified framework. Our contraction results only require a prior mass condition on certain Kullback-Leibler (KL) neighborhood of the true parameter (or the KL divergence minimizer in the misspecified case), and obviate constructions of test functions and sieves commonly used in the literature for analyzing the contraction property of a regular posterior. We show through a counterexample that some condition controlling the complexity of the parameter space is necessary for the regular posterior to contract, rendering additional flexibility on the choice of the prior for the fractional posterior. Second, we derive a novel Bayesian oracle inequality based on a PAC-Bayes inequality in misspecified models. Our derivation reveals several advantages of averaging based Bayesian procedures over optimization based frequentist procedures. As an application of the Bayesian oracle inequality, we derive a sharp oracle inequality in the convex regression problem under an arbitrary dimension. We also illustrate the theory in Gaussian process regression and density estimation problems.

</details>

<details>

<summary>2016-11-08 02:27:46 - Discussion of: "A Bayesian information criterion for singular models"</summary>

- *N. Friel, J. P. McKeone, C. J. Oates, A. N. Pettitt*

- `1611.02367v1` - [abs](http://arxiv.org/abs/1611.02367v1) - [pdf](http://arxiv.org/pdf/1611.02367v1)

> Contributed discussion to the paper of Drton and Plummer (2017), presented before the Royal Statistical Society on 5th October 2016.

</details>

<details>

<summary>2016-11-08 06:24:28 - Examining posterior propriety in the Bayesian analysis of capture-recapture models</summary>

- *Arjun M. Gopalaswamy, Mohan Delampady*

- `1611.02403v1` - [abs](http://arxiv.org/abs/1611.02403v1) - [pdf](http://arxiv.org/pdf/1611.02403v1)

> There lies a latent danger in utilizing some known mathematical results in ecology. Some results do not apply to the problem at hand. We identify one such trend. Based on a couple of theorems in mathematical statistics, Link (2013) cautions ecologists about the inappropriateness of using the discrete uniform prior in their analysis under certain conditions and instead recommends the routine use of the scale prior during analysis. This recommendation is been absorbed immediately and widely among ecologists. In this study, we consider the two fundamental capture-recapture models used widely in ecology, $M_0$ and $M_h$, and derive conditions for posterior propriety by examining the behavior of the right tail of the posterior distributions of animal population size $N$ in a Bayesian analysis. We demonstrate that both these likelihoods are far more efficient than the ones considered in Link (2013). We argue that no particularly prescriptive approach should be adopted by ecologists in regard to choosing priors of the fear of posterior impropriety. Instead, we recommend the efficient construction of likelihoods for the problem and data on hand, choosing priors based existing knowledge of a parameter of interest and encourage examining posterior propriety by asymptotic arguments as demonstrated in this study.

</details>

<details>

<summary>2016-11-09 05:09:29 - Hierarchical Stochastic Model in Bayesian Inference: Theoretical Implications and Efficient Approximation</summary>

- *Stephen Wu, Panagiotis Angelikopoulos, James L. Beck, Petros Koumoutsakos*

- `1611.02818v1` - [abs](http://arxiv.org/abs/1611.02818v1) - [pdf](http://arxiv.org/pdf/1611.02818v1)

> We classify two types of Hierarchical Bayesian Model found in the literature as Hierarchical Prior Model (HPM) and Hierarchical Stochastic Model (HSM). Then, we focus on studying the theoretical implications of the HSM. Using examples of polynomial functions, we show that the HSM is capable of separating different types of uncertainties in a system and quantifying uncertainty of reduced order models under the Bayesian model class selection framework. To tackle the huge computational cost for analyzing HSM, we propose an efficient approximation scheme based on Importance Sampling and Empirical Interpolation Method. We illustrate our method using two examples - a Molecular Dynamics simulation for Krypton and a pharmacokinetic/pharmacodynamic model for cancer drug.

</details>

<details>

<summary>2016-11-09 07:16:26 - Normalizing Flows on Riemannian Manifolds</summary>

- *Mevlana C. Gemici, Danilo Rezende, Shakir Mohamed*

- `1611.02304v2` - [abs](http://arxiv.org/abs/1611.02304v2) - [pdf](http://arxiv.org/pdf/1611.02304v2)

> We consider the problem of density estimation on Riemannian manifolds. Density estimation on manifolds has many applications in fluid-mechanics, optics and plasma physics and it appears often when dealing with angular variables (such as used in protein folding, robot limbs, gene-expression) and in general directional statistics. In spite of the multitude of algorithms available for density estimation in the Euclidean spaces $\mathbf{R}^n$ that scale to large n (e.g. normalizing flows, kernel methods and variational approximations), most of these methods are not immediately suitable for density estimation in more general Riemannian manifolds. We revisit techniques related to homeomorphisms from differential geometry for projecting densities to sub-manifolds and use it to generalize the idea of normalizing flows to more general Riemannian manifolds. The resulting algorithm is scalable, simple to implement and suitable for use with automatic differentiation. We demonstrate concrete examples of this method on the n-sphere $\mathbf{S}^n$.

</details>

<details>

<summary>2016-11-09 07:54:54 - Correlated Random Measures</summary>

- *Rajesh Ranganath, David Blei*

- `1507.00720v3` - [abs](http://arxiv.org/abs/1507.00720v3) - [pdf](http://arxiv.org/pdf/1507.00720v3)

> We develop correlated random measures, random measures where the atom weights can exhibit a flexible pattern of dependence, and use them to develop powerful hierarchical Bayesian nonparametric models. Hierarchical Bayesian nonparametric models are usually built from completely random measures, a Poisson-process based construction in which the atom weights are independent. Completely random measures imply strong independence assumptions in the corresponding hierarchical model, and these assumptions are often misplaced in real-world settings. Correlated random measures address this limitation. They model correlation within the measure by using a Gaussian process in concert with the Poisson process. With correlated random measures, for example, we can develop a latent feature model for which we can infer both the properties of the latent features and their dependency pattern. We develop several other examples as well. We study a correlated random measure model of pairwise count data. We derive an efficient variational inference algorithm and show improved predictive performance on large data sets of documents, web clicks, and electronic health records.

</details>

<details>

<summary>2016-11-10 09:08:41 - Feature Selection with the R Package MXM: Discovering Statistically-Equivalent Feature Subsets</summary>

- *Vincenzo Lagani, Giorgos Athineou, Alessio Farcomeni, Michail Tsagris, Ioannis Tsamardinos*

- `1611.03227v1` - [abs](http://arxiv.org/abs/1611.03227v1) - [pdf](http://arxiv.org/pdf/1611.03227v1)

> The statistically equivalent signature (SES) algorithm is a method for feature selection inspired by the principles of constrained-based learning of Bayesian Networks. Most of the currently available feature-selection methods return only a single subset of features, supposedly the one with the highest predictive power. We argue that in several domains multiple subsets can achieve close to maximal predictive accuracy, and that arbitrarily providing only one has several drawbacks. The SES method attempts to identify multiple, predictive feature subsets whose performances are statistically equivalent. Under that respect SES subsumes and extends previous feature selection algorithms, like the max-min parent children algorithm. SES is implemented in an homonym function included in the R package MXM, standing for mens ex machina, meaning 'mind from the machine' in Latin. The MXM implementation of SES handles several data-analysis tasks, namely classification, regression and survival analysis. In this paper we present the SES algorithm, its implementation, and provide examples of use of the SES function in R. Furthermore, we analyze three publicly available data sets to illustrate the equivalence of the signatures retrieved by SES and to contrast SES against the state-of-the-art feature selection method LASSO. Our results provide initial evidence that the two methods perform comparably well in terms of predictive accuracy and that multiple, equally predictive signatures are actually present in real world data.

</details>

<details>

<summary>2016-11-10 17:16:04 - Learning an Astronomical Catalog of the Visible Universe through Scalable Bayesian Inference</summary>

- *Jeffrey Regier, Kiran Pamnany, Ryan Giordano, Rollin Thomas, David Schlegel, Jon McAuliffe, Prabhat*

- `1611.03404v1` - [abs](http://arxiv.org/abs/1611.03404v1) - [pdf](http://arxiv.org/pdf/1611.03404v1)

> Celeste is a procedure for inferring astronomical catalogs that attains state-of-the-art scientific results. To date, Celeste has been scaled to at most hundreds of megabytes of astronomical images: Bayesian posterior inference is notoriously demanding computationally. In this paper, we report on a scalable, parallel version of Celeste, suitable for learning catalogs from modern large-scale astronomical datasets. Our algorithmic innovations include a fast numerical optimization routine for Bayesian posterior inference and a statistically efficient scheme for decomposing astronomical optimization problems into subproblems.   Our scalable implementation is written entirely in Julia, a new high-level dynamic programming language designed for scientific and numerical computing. We use Julia's high-level constructs for shared and distributed memory parallelism, and demonstrate effective load balancing and efficient scaling on up to 8192 Xeon cores on the NERSC Cori supercomputer.

</details>

<details>

<summary>2016-11-11 11:15:30 - A Bayesian Network Model for Interesting Itemsets</summary>

- *Jaroslav Fowkes, Charles Sutton*

- `1510.04130v2` - [abs](http://arxiv.org/abs/1510.04130v2) - [pdf](http://arxiv.org/pdf/1510.04130v2)

> Mining itemsets that are the most interesting under a statistical model of the underlying data is a commonly used and well-studied technique for exploratory data analysis, with the most recent interestingness models exhibiting state of the art performance. Continuing this highly promising line of work, we propose the first, to the best of our knowledge, generative model over itemsets, in the form of a Bayesian network, and an associated novel measure of interestingness. Our model is able to efficiently infer interesting itemsets directly from the transaction database using structural EM, in which the E-step employs the greedy approximation to weighted set cover. Our approach is theoretically simple, straightforward to implement, trivially parallelizable and retrieves itemsets whose quality is comparable to, if not better than, existing state of the art algorithms as we demonstrate on several real-world datasets.

</details>

<details>

<summary>2016-11-11 19:59:26 - A Bayesian adaptive ensemble Kalman filter for sequential state and parameter estimation</summary>

- *Jonathan R. Stroud, Matthias Katzfuss, Christopher K. Wikle*

- `1611.03835v1` - [abs](http://arxiv.org/abs/1611.03835v1) - [pdf](http://arxiv.org/pdf/1611.03835v1)

> This paper proposes new methodology for sequential state and parameter estimation within the ensemble Kalman filter. The method is fully Bayesian and propagates the joint posterior density of states and parameters over time. In order to implement the method we consider two representations of the marginal posterior distribution of the parameters: a grid-based approach and a Gaussian approximation. Contrary to existing algorithms, the new method explicitly accounts for parameter uncertainty and provides a formal way to combine information about the parameters from data at different time periods. The method is illustrated and compared to existing approaches using simulated and real data.

</details>

<details>

<summary>2016-11-11 22:20:41 - Low Latency Anomaly Detection and Bayesian Network Prediction of Anomaly Likelihood</summary>

- *Derek Farren, Thai Pham, Marco Alban-Hidalgo*

- `1611.03898v1` - [abs](http://arxiv.org/abs/1611.03898v1) - [pdf](http://arxiv.org/pdf/1611.03898v1)

> We develop a supervised machine learning model that detects anomalies in systems in real time. Our model processes unbounded streams of data into time series which then form the basis of a low-latency anomaly detection model. Moreover, we extend our preliminary goal of just anomaly detection to simultaneous anomaly prediction. We approach this very challenging problem by developing a Bayesian Network framework that captures the information about the parameters of the lagged regressors calibrated in the first part of our approach and use this structure to learn local conditional probability distributions.

</details>

<details>

<summary>2016-11-13 00:20:24 - Full Reconstruction of Non-Stationary Strand-Symmetric Models on Rooted Phylogenies</summary>

- *Benjamin D Kaehler*

- `1610.05057v2` - [abs](http://arxiv.org/abs/1610.05057v2) - [pdf](http://arxiv.org/pdf/1610.05057v2)

> Understanding the evolutionary relationship among species is of fundamental importance to the biological sciences. The location of the root in any phylogenetic tree is critical as it gives an order to evolutionary events. None of the popular models of nucleotide evolution used in likelihood or Bayesian methods are able to infer the location of the root without exogenous information. It is known that the most general Markov models of nucleotide substitution can also not identify the location of the root or be fitted to multiple sequence alignments with less than three sequences. We prove that the location of the root and the full model can be identified and statistically consistently estimated for a non-stationary, strand-symmetric substitution model given a multiple sequence alignment with two or more sequences. We also generalise earlier work to provide a practical means of overcoming the computationally intractable problem of labelling hidden states in a phylogenetic model.

</details>

<details>

<summary>2016-11-15 00:22:21 - Multi-Information Source Optimization</summary>

- *Matthias Poloczek, Jialei Wang, Peter I. Frazier*

- `1603.00389v2` - [abs](http://arxiv.org/abs/1603.00389v2) - [pdf](http://arxiv.org/pdf/1603.00389v2)

> We consider Bayesian optimization of an expensive-to-evaluate black-box objective function, where we also have access to cheaper approximations of the objective. In general, such approximations arise in applications such as reinforcement learning, engineering, and the natural sciences, and are subject to an inherent, unknown bias. This model discrepancy is caused by an inadequate internal model that deviates from reality and can vary over the domain, making the utilization of these approximations a non-trivial task.   We present a novel algorithm that provides a rigorous mathematical treatment of the uncertainties arising from model discrepancies and noisy observations. Its optimization decisions rely on a value of information analysis that extends the Knowledge Gradient factor to the setting of multiple information sources that vary in cost: each sampling decision maximizes the predicted benefit per unit cost.   We conduct an experimental evaluation that demonstrates that the method consistently outperforms other state-of-the-art techniques: it finds designs of considerably higher objective value and additionally inflicts less cost in the exploration process.

</details>

<details>

<summary>2016-11-15 06:06:42 - Recoverability of Joint Distribution from Missing Data</summary>

- *Jin Tian*

- `1611.04709v1` - [abs](http://arxiv.org/abs/1611.04709v1) - [pdf](http://arxiv.org/pdf/1611.04709v1)

> A probabilistic query may not be estimable from observed data corrupted by missing values if the data are not missing at random (MAR). It is therefore of theoretical interest and practical importance to determine in principle whether a probabilistic query is estimable from missing data or not when the data are not MAR. We present an algorithm that systematically determines whether the joint probability is estimable from observed data with missing values, assuming that the data-generation model is represented as a Bayesian network containing unobserved latent variables that not only encodes the dependencies among the variables but also explicitly portrays the mechanisms responsible for the missingness process. The result significantly advances the existing work.

</details>

<details>

<summary>2016-11-15 10:26:24 - Bayes linear kinematics in a dynamic Bayesian survival model</summary>

- *Kevin J. Wilson, Malcolm Farrow*

- `1411.2497v2` - [abs](http://arxiv.org/abs/1411.2497v2) - [pdf](http://arxiv.org/pdf/1411.2497v2)

> Bayes linear kinematics and Bayes linear Bayes graphical models provide an extension of Bayes linear methods so that full conditional updates may be combined with Bayes linear belief adjustment. In this paper we investigate the application of this approach to a more complicated problem: namely survival analysis with time-dependent covariate effects. We use a piecewise-constant hazard function with a prior in which covariate effects are correlated over time. The need for computationally intensive methods is avoided and the relatively simple structure facilitates interpretation. Our approach eliminates the problem of non-commutativity which was observed in earlier work by Gamerman. We apply the technique to data on survival times for leukemia patients.

</details>

<details>

<summary>2016-11-16 01:43:21 - Classifier comparison using precision</summary>

- *Lovedeep Gondara*

- `1609.09471v2` - [abs](http://arxiv.org/abs/1609.09471v2) - [pdf](http://arxiv.org/pdf/1609.09471v2)

> New proposed models are often compared to state-of-the-art using statistical significance testing. Literature is scarce for classifier comparison using metrics other than accuracy. We present a survey of statistical methods that can be used for classifier comparison using precision, accounting for inter-precision correlation arising from use of same dataset. Comparisons are made using per-class precision and methods presented to test global null hypothesis of an overall model comparison. Comparisons are extended to multiple multi-class classifiers and to models using cross validation or its variants. Partial Bayesian update to precision is introduced when population prevalence of a class is known. Applications to compare deep architectures are studied.

</details>

<details>

<summary>2016-11-16 18:48:18 - Correcting biased observation model error in data assimilation</summary>

- *John Harlim, Tyrus Berry*

- `1611.05405v1` - [abs](http://arxiv.org/abs/1611.05405v1) - [pdf](http://arxiv.org/pdf/1611.05405v1)

> While the formulation of most data assimilation schemes assumes an unbiased observation model error, in real applications, model error with nontrivial biases is unavoidable. A practical example is the error in the radiative transfer model (which is used to assimilate satellite measurements) in the presence of clouds. As a consequence, many (in fact 99\%) of the cloudy observed measurements are not being used although they may contain useful information. This paper presents a novel nonparametric Bayesian scheme which is able to learn the observation model error distribution and correct the bias in incoming observations. This scheme can be used in tandem with any data assimilation forecasting system. The proposed model error estimator uses nonparametric likelihood functions constructed with data-driven basis functions based on the theory of kernel embeddings of conditional distributions developed in the machine learning community. Numerically, we show positive results with two examples. The first example is designed to produce a bimodality in the observation model error (typical of "cloudy" observations) by introducing obstructions to the observations which occur randomly in space and time. The second example, which is physically more realistic, is to assimilate cloudy satellite brightness temperature-like quantities, generated from a stochastic cloud model for tropical convection and a simple radiative transfer model.

</details>

<details>

<summary>2016-11-16 21:49:21 - The Bayesian Formulation and Well-Posedness of Fractional Elliptic Inverse Problems</summary>

- *Nicolas Garcia Trillos, Daniel Sanz-Alonso*

- `1611.05475v1` - [abs](http://arxiv.org/abs/1611.05475v1) - [pdf](http://arxiv.org/pdf/1611.05475v1)

> We study the inverse problem of recovering the order and the diffusion coefficient of an elliptic fractional partial differential equation from a finite number of noisy observations of the solution. We work in a Bayesian framework and show conditions under which the posterior distribution is given by a change of measure from the prior. Moreover, we show well-posedness of the inverse problem, in the sense that small perturbations of the observed solution lead to small Hellinger perturbations of the associated posterior measures. We thus provide a mathematical foundation to the Bayesian learning of the order ---and other inputs--- of fractional models.

</details>

<details>

<summary>2016-11-17 17:13:32 - blavaan: Bayesian structural equation models via parameter expansion</summary>

- *Edgar C. Merkle, Yves Rosseel*

- `1511.05604v2` - [abs](http://arxiv.org/abs/1511.05604v2) - [pdf](http://arxiv.org/pdf/1511.05604v2)

> This article describes blavaan, an R package for estimating Bayesian structural equation models (SEMs) via JAGS and for summarizing the results. It also describes a novel parameter expansion approach for estimating specific types of models with residual covariances, which facilitates estimation of these models in JAGS. The methodology and software are intended to provide users with a general means of estimating Bayesian SEMs, both classical and novel, in a straightforward fashion. Users can estimate Bayesian versions of classical SEMs with lavaan syntax, they can obtain state-of-the-art Bayesian fit measures associated with the models, and they can export JAGS code to modify the SEMs as desired. These features and more are illustrated by example, and the parameter expansion approach is explained in detail.

</details>

<details>

<summary>2016-11-17 20:12:59 - Contributed Discussion to Bayesian Solution Uncertainty Quantification for Differential Equations</summary>

- *William Weimin Yoo*

- `1611.05843v1` - [abs](http://arxiv.org/abs/1611.05843v1) - [pdf](http://arxiv.org/pdf/1611.05843v1)

> We begin by introducing the main ideas of the paper under discussion, and we give a brief description of the method proposed. Next, we discuss an alternative approach based on B-spline expansion, and lastly we make some comments on the method's convergence rate.

</details>

<details>

<summary>2016-11-17 21:45:23 - Comparison of Bayesian and Frequentist Multiplicity Correction For Testing Mutually Exclusive Hypotheses Under Data Dependence</summary>

- *Sean Chang, James O. Berger*

- `1611.05909v1` - [abs](http://arxiv.org/abs/1611.05909v1) - [pdf](http://arxiv.org/pdf/1611.05909v1)

> The problem of testing mutually exclusive hypotheses with dependent test statistics is considered. Bayesian and frequentist approaches to multiplicity control are studied and compared to help gain understanding as to the effect of test statistic dependence on each approach. The Bayesian approach is shown to have excellent frequentist properties and is argued to be the most effective way of obtaining frequentist multiplicity control, without sacrificing power, when there is considerable test statistic dependence.

</details>

<details>

<summary>2016-11-18 14:00:48 - A Generalized Stochastic Variational Bayesian Hyperparameter Learning Framework for Sparse Spectrum Gaussian Process Regression</summary>

- *Quang Minh Hoang, Trong Nghia Hoang, Kian Hsiang Low*

- `1611.06080v1` - [abs](http://arxiv.org/abs/1611.06080v1) - [pdf](http://arxiv.org/pdf/1611.06080v1)

> While much research effort has been dedicated to scaling up sparse Gaussian process (GP) models based on inducing variables for big data, little attention is afforded to the other less explored class of low-rank GP approximations that exploit the sparse spectral representation of a GP kernel. This paper presents such an effort to advance the state of the art of sparse spectrum GP models to achieve competitive predictive performance for massive datasets. Our generalized framework of stochastic variational Bayesian sparse spectrum GP (sVBSSGP) models addresses their shortcomings by adopting a Bayesian treatment of the spectral frequencies to avoid overfitting, modeling these frequencies jointly in its variational distribution to enable their interaction a posteriori, and exploiting local data for boosting the predictive performance. However, such structural improvements result in a variational lower bound that is intractable to be optimized. To resolve this, we exploit a variational parameterization trick to make it amenable to stochastic optimization. Interestingly, the resulting stochastic gradient has a linearly decomposable structure that can be exploited to refine our stochastic optimization method to incur constant time per iteration while preserving its property of being an unbiased estimator of the exact gradient of the variational lower bound. Empirical evaluation on real-world datasets shows that sVBSSGP outperforms state-of-the-art stochastic implementations of sparse GP models.

</details>

<details>

<summary>2016-11-19 17:11:58 - Fractional Gaussian noise: Prior specification and model comparison</summary>

- *Sigrunn Holbek SÃ¸rbye, HÃ¥vard Rue*

- `1611.06399v1` - [abs](http://arxiv.org/abs/1611.06399v1) - [pdf](http://arxiv.org/pdf/1611.06399v1)

> Fractional Gaussian noise (fGn) is a self-similar stochastic process used to model anti-persistent or persistent dependency structures in observed time series. Properties of the autocovariance function of fGn are characterised by the Hurst exponent (H), which in Bayesian contexts typically has been assigned a uniform prior on the unit interval. This paper argues why a uniform prior is unreasonable and introduces the use of a penalised complexity (PC) prior for H. The PC prior is computed to penalise divergence from the special case of white noise, and is invariant to reparameterisations. An immediate advantage is that the exact same prior can be used for the autocorrelation coefficient of a first-order autoregressive process AR(1), as this model also reflects a flexible version of white noise. Within the general setting of latent Gaussian models, this allows us to compare an fGn model component with AR(1) using Bayes factors, avoiding confounding effects of prior choices for the hyperparameters. Among others, this is useful in climate regression models where inference for underlying linear or smooth trends depends heavily on the assumed noise model.

</details>

<details>

<summary>2016-11-20 05:45:25 - co-BPM: a Bayesian Model for Divergence Estimation</summary>

- *Kun Yang, Hao Su, Wing Hung Wong*

- `1410.0726v4` - [abs](http://arxiv.org/abs/1410.0726v4) - [pdf](http://arxiv.org/pdf/1410.0726v4)

> Divergence is not only an important mathematical concept in information theory, but also applied to machine learning problems such as low-dimensional embedding, manifold learning, clustering, classification, and anomaly detection. We proposed a bayesian model---co-BPM---to characterize the discrepancy of two sample sets, i.e., to estimate the divergence of their underlying distributions. In order to avoid the pitfalls of plug-in methods that estimate each density independently, our bayesian model attempts to learn a coupled binary partition of the sample space that best captures the landscapes of both distributions, then make direct inference on their divergences. The prior is constructed by leveraging the sequential buildup of the coupled binary partitions and the posterior is sampled via our specialized MCMC. Our model provides a unified way to estimate various types of divergences and enjoys convincing accuracy. We demonstrate its effectiveness through simulations, comparisons with the \emph{state-of-the-art} and a real data example.

</details>

<details>

<summary>2016-11-21 10:36:03 - Is Gun Violence Contagious?</summary>

- *Charles Loeffler, Seth Flaxman*

- `1611.06713v1` - [abs](http://arxiv.org/abs/1611.06713v1) - [pdf](http://arxiv.org/pdf/1611.06713v1)

> Existing theories of gun violence predict stable spatial concentrations and contagious diffusion of gun violence into surrounding areas. Recent empirical studies have reported confirmatory evidence of such spatiotemporal diffusion of gun violence. However, existing tests cannot readily distinguish spatiotemporal clustering from spatiotemporal diffusion. This leaves as an open question whether gun violence actually is contagious or merely clusters in space and time. Compounding this problem, gun violence is subject to considerable measurement error with many nonfatal shootings going unreported to police. Using point process data from an acoustical gunshot locator system and a combination of Bayesian spatiotemporal point process modeling and space/time interaction tests, this paper demonstrates that contemporary urban gun violence does diffuse, but only slightly, suggesting that a disease model for infectious spread of gun violence is a poor fit for the geographically stable and temporally stochastic process observed.

</details>

<details>

<summary>2016-11-21 11:37:17 - Variational Graph Auto-Encoders</summary>

- *Thomas N. Kipf, Max Welling*

- `1611.07308v1` - [abs](http://arxiv.org/abs/1611.07308v1) - [pdf](http://arxiv.org/pdf/1611.07308v1)

> We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.

</details>

<details>

<summary>2016-11-21 16:08:12 - Probabilistic structure discovery in time series data</summary>

- *David Janz, Brooks Paige, Tom Rainforth, Jan-Willem van de Meent, Frank Wood*

- `1611.06863v1` - [abs](http://arxiv.org/abs/1611.06863v1) - [pdf](http://arxiv.org/pdf/1611.06863v1)

> Existing methods for structure discovery in time series data construct interpretable, compositional kernels for Gaussian process regression models. While the learned Gaussian process model provides posterior mean and variance estimates, typically the structure is learned via a greedy optimization procedure. This restricts the space of possible solutions and leads to over-confident uncertainty estimates. We introduce a fully Bayesian approach, inferring a full posterior over structures, which more reliably captures the uncertainty of the model.

</details>

<details>

<summary>2016-11-21 16:13:51 - A fully objective Bayesian approach for the Behrens-Fisher problem using historical studies</summary>

- *Antoine Barbieri, Jean-Michel Marin, Karine Florin*

- `1611.06873v1` - [abs](http://arxiv.org/abs/1611.06873v1) - [pdf](http://arxiv.org/pdf/1611.06873v1)

> For in vivo research experiments with small sample sizes and available historical data, we propose a sequential Bayesian method for the Behrens-Fisher problem. We consider it as a model choice question with two models in competition: one for which the two expectations are equal and one for which they are different. The choice between the two models is performed through a Bayesian analysis, based on a robust choice of combined objective and subjective priors, set on the parameters space and on the models space. Three steps are necessary to evaluate the posterior probability of each model using two historical datasets similar to the one of interest. Starting from the Jeffreys prior, a posterior using a first historical dataset is deduced and allows to calibrate the Normal-Gamma informative priors for the second historical dataset analysis, in addition to a uniform prior on the model space. From this second step, a new posterior on the parameter space and the models space can be used as the objective informative prior for the last Bayesian analysis. Bayesian and frequentist methods have been compared on simulated and real data. In accordance with FDA recommendations, control of type I and type II error rates has been evaluated. The proposed method controls them even if the historical experiments are not completely similar to the one of interest.

</details>

<details>

<summary>2016-11-21 16:14:30 - Langevin Incremental Mixture Importance Sampling</summary>

- *Matteo Fasiolo, FlÃ¡vio Eler de Melo, Simon Maskell*

- `1611.06874v1` - [abs](http://arxiv.org/abs/1611.06874v1) - [pdf](http://arxiv.org/pdf/1611.06874v1)

> This work proposes a novel method through which local information about the target density can be used to construct an efficient importance sampler. The backbone of the proposed method is the Incremental Mixture Importance Sampling (IMIS) algorithm of Raftery and Bao (2010), which builds a mixture importance distribution incrementally, by positioning new mixture components where the importance density lacks mass, relative to the target. The key innovation proposed here is that the mixture components used by IMIS are local approximations to the target density. In particular, their mean vectors and covariance matrices are constructed by numerically solving certain differential equations, whose solution depends on the gradient field of the target log-density. The new sampler has a number of advantages: a) it provides an extremely parsimonious parametrization of the mixture importance density, whose configuration effectively depends only on the shape of the target and on a single free parameter representing pseudo-time; b) it scales well with the dimensionality of the target; c) it can deal with targets that are not log- concave. The performance of the proposed approach is demonstrated on a synthetic non-Gaussian multimodal density, defined on up to eighty dimensions, and on a Bayesian logistic regression model, using the Sonar data-set. The Julia code implementing the importance sampler proposed here can be found at https:/github.com/mfasiolo/LIMIS.

</details>

<details>

<summary>2016-11-21 20:19:50 - Neuron's Eye View: Inferring Features of Complex Stimuli from Neural Responses</summary>

- *Xin, Chen, Jeffrey M Beck, John M Pearson*

- `1512.01408v2` - [abs](http://arxiv.org/abs/1512.01408v2) - [pdf](http://arxiv.org/pdf/1512.01408v2)

> Experiments that study neural encoding of stimuli at the level of individual neurons typically choose a small set of features present in the world --- contrast and luminance for vision, pitch and intensity for sound --- and assemble a stimulus set that systematically varies along these dimensions. Subsequent analysis of neural responses to these stimuli typically focuses on regression models, with experimenter-controlled features as predictors and spike counts or firing rates as responses. Unfortunately, this approach requires knowledge in advance about the relevant features coded by a given population of neurons. For domains as complex as social interaction or natural movement, however, the relevant feature space is poorly understood, and an arbitrary \emph{a priori} choice of features may give rise to confirmation bias. Here, we present a Bayesian model for exploratory data analysis that is capable of automatically identifying the features present in unstructured stimuli based solely on neuronal responses. Our approach is unique within the class of latent state space models of neural activity in that it assumes that firing rates of neurons are sensitive to multiple discrete time-varying features tied to the \emph{stimulus}, each of which has Markov (or semi-Markov) dynamics. That is, we are modeling neural activity as driven by multiple simultaneous stimulus features rather than intrinsic neural dynamics. We derive a fast variational Bayesian inference algorithm and show that it correctly recovers hidden features in synthetic data, as well as ground-truth stimulus features in a prototypical neural dataset. To demonstrate the utility of the algorithm, we also apply it to cluster neural responses and demonstrate successful recovery of features corresponding to monkeys and faces in the image set.

</details>

<details>

<summary>2016-11-22 06:41:57 - Optimal Learning for Stochastic Optimization with Nonlinear Parametric Belief Models</summary>

- *Xinyu He, Warren B. Powell*

- `1611.07161v1` - [abs](http://arxiv.org/abs/1611.07161v1) - [pdf](http://arxiv.org/pdf/1611.07161v1)

> We consider the problem of estimating the expected value of information (the knowledge gradient) for Bayesian learning problems where the belief model is nonlinear in the parameters. Our goal is to maximize some metric, while simultaneously learning the unknown parameters of the nonlinear belief model, by guiding a sequential experimentation process which is expensive. We overcome the problem of computing the expected value of an experiment, which is computationally intractable, by using a sampled approximation, which helps to guide experiments but does not provide an accurate estimate of the unknown parameters. We then introduce a resampling process which allows the sampled model to adapt to new information, exploiting past experiments. We show theoretically that the method converges asymptotically to the true parameters, while simultaneously maximizing our metric. We show empirically that the process exhibits rapid convergence, yielding good results with a very small number of experiments.

</details>

<details>

<summary>2016-11-22 10:13:02 - Unimodal Thompson Sampling for Graph-Structured Arms</summary>

- *Stefano Paladino, Francesco TrovÃ², Marcello Restelli, Nicola Gatti*

- `1611.05724v2` - [abs](http://arxiv.org/abs/1611.05724v2) - [pdf](http://arxiv.org/pdf/1611.05724v2)

> We study, to the best of our knowledge, the first Bayesian algorithm for unimodal Multi-Armed Bandit (MAB) problems with graph structure. In this setting, each arm corresponds to a node of a graph and each edge provides a relationship, unknown to the learner, between two nodes in terms of expected reward. Furthermore, for any node of the graph there is a path leading to the unique node providing the maximum expected reward, along which the expected reward is monotonically increasing. Previous results on this setting describe the behavior of frequentist MAB algorithms. In our paper, we design a Thompson Sampling-based algorithm whose asymptotic pseudo-regret matches the lower bound for the considered setting. We show that -as it happens in a wide number of scenarios- Bayesian MAB algorithms dramatically outperform frequentist ones. In particular, we provide a thorough experimental evaluation of the performance of our and state-of-the-art algorithms as the properties of the graph vary.

</details>

<details>

<summary>2016-11-22 15:00:08 - Limbo: A Fast and Flexible Library for Bayesian Optimization</summary>

- *Antoine Cully, Konstantinos Chatzilygeroudis, Federico Allocati, Jean-Baptiste Mouret*

- `1611.07343v1` - [abs](http://arxiv.org/abs/1611.07343v1) - [pdf](http://arxiv.org/pdf/1611.07343v1)

> Limbo is an open-source C++11 library for Bayesian optimization which is designed to be both highly flexible and very fast. It can be used to optimize functions for which the gradient is unknown, evaluations are expensive, and runtime cost matters (e.g., on embedded systems or robots). Benchmarks on standard functions show that Limbo is about 2 times faster than BayesOpt (another C++ library) for a similar accuracy.

</details>

<details>

<summary>2016-11-22 15:16:45 - Statistical comparison of classifiers through Bayesian hierarchical modelling</summary>

- *Giorgio Corani, Alessio Benavoli, Janez DemÅ¡ar, Francesca Mangili, Marco Zaffalon*

- `1609.08905v3` - [abs](http://arxiv.org/abs/1609.08905v3) - [pdf](http://arxiv.org/pdf/1609.08905v3)

> Usually one compares the accuracy of two competing classifiers via null hypothesis significance tests (nhst). Yet the nhst tests suffer from important shortcomings, which can be overcome by switching to Bayesian hypothesis testing. We propose a Bayesian hierarchical model which jointly analyzes the cross-validation results obtained by two classifiers on multiple data sets. It returns the posterior probability of the accuracies of the two classifiers being practically equivalent or significantly different. A further strength of the hierarchical model is that, by jointly analyzing the results obtained on all data sets, it reduces the estimation error compared to the usual approach of averaging the cross-validation results obtained on a given data set.

</details>

<details>

<summary>2016-11-22 18:53:32 - Poisson Random Fields for Dynamic Feature Models</summary>

- *Valerio Perrone, Paul A. Jenkins, Dario Spano, Yee Whye Teh*

- `1611.07460v1` - [abs](http://arxiv.org/abs/1611.07460v1) - [pdf](http://arxiv.org/pdf/1611.07460v1)

> We present the Wright-Fisher Indian buffet process (WF-IBP), a probabilistic model for time-dependent data assumed to have been generated by an unknown number of latent features. This model is suitable as a prior in Bayesian nonparametric feature allocation models in which the features underlying the observed data exhibit a dependency structure over time. More specifically, we establish a new framework for generating dependent Indian buffet processes, where the Poisson random field model from population genetics is used as a way of constructing dependent beta processes. Inference in the model is complex, and we describe a sophisticated Markov Chain Monte Carlo algorithm for exact posterior simulation. We apply our construction to develop a nonparametric focused topic model for collections of time-stamped text documents and test it on the full corpus of NIPS papers published from 1987 to 2015.

</details>

<details>

<summary>2016-11-22 21:08:28 - Analyzing Genome-wide Association Study Data with the R Package genMOSS</summary>

- *Matthew Friedlander, Adrian Dobra, Helene Massam, Laurent Briollais*

- `1611.07537v1` - [abs](http://arxiv.org/abs/1611.07537v1) - [pdf](http://arxiv.org/pdf/1611.07537v1)

> The R package (R Core Team (2016)) genMOSS is specifically designed for the Bayesian analysis of genome-wide association study data. The package implements the mode oriented stochastic search (MOSS) procedure as well as a simple moving window approach to identify combinations of single nucleotide polymorphisms associated with a response. The prior used in Bayesian computations is the generalized hyper Dirichlet.

</details>

<details>

<summary>2016-11-23 00:18:34 - Comment on "A variational Bayesian approach for inverse problems with skew-t error distributions" (Guha et al., Journal of Computational Physics 301 (2015) 377-393)</summary>

- *Javier E. Contreras-Reyes, Freddy Omar LÃ³pez Quintero*

- `1605.02256v2` - [abs](http://arxiv.org/abs/1605.02256v2) - [pdf](http://arxiv.org/pdf/1605.02256v2)

> A brief comment on A variational Bayesian approach for inverse problems with skew-t error distributions (Guha et al., Journal of Computational Physics 301 (2015) 377-393) is given in this letter.

</details>

<details>

<summary>2016-11-23 16:42:29 - Piecewise Deterministic Markov Processes for Continuous-Time Monte Carlo</summary>

- *Paul Fearnhead, Joris Bierkens, Murray Pollock, Gareth O Roberts*

- `1611.07873v1` - [abs](http://arxiv.org/abs/1611.07873v1) - [pdf](http://arxiv.org/pdf/1611.07873v1)

> Recently there have been exciting developments in Monte Carlo methods, with the development of new MCMC and sequential Monte Carlo (SMC) algorithms which are based on continuous-time, rather than discrete-time, Markov processes. This has led to some fundamentally new Monte Carlo algorithms which can be used to sample from, say, a posterior distribution. Interestingly, continuous-time algorithms seem particularly well suited to Bayesian analysis in big-data settings as they need only access a small sub-set of data points at each iteration, and yet are still guaranteed to target the true posterior distribution. Whilst continuous-time MCMC and SMC methods have been developed independently we show here that they are related by the fact that both involve simulating a piecewise deterministic Markov process. Furthermore we show that the methods developed to date are just specific cases of a potentially much wider class of continuous-time Monte Carlo algorithms. We give an informal introduction to piecewise deterministic Markov processes, covering the aspects relevant to these new Monte Carlo algorithms, with a view to making the development of new continuous-time Monte Carlo more accessible. We focus on how and why sub-sampling ideas can be used with these algorithms, and aim to give insight into how these new algorithms can be implemented, and what are some of the issues that affect their efficiency.

</details>

<details>

<summary>2016-11-24 09:46:13 - BayesVarSel: Bayesian Testing, Variable Selection and model averaging in Linear Models using R</summary>

- *Gonzalo Garcia-Donato, Anabel Forte*

- `1611.08118v1` - [abs](http://arxiv.org/abs/1611.08118v1) - [pdf](http://arxiv.org/pdf/1611.08118v1)

> This paper introduces the R package BayesVarSel which implements objective Bayesian methodology for hypothesis testing and variable selection in linear models. The package computes posterior probabilities of the competing hypotheses/models and provides a suite of tools, specifically proposed in the literature, to properly summarize the results. Additionally, \ourpack\ is armed with functions to compute several types of model averaging estimations and predictions with weights given by the posterior probabilities. BayesVarSel contains exact algorithms to perform fast computations in problems of small to moderate size and heuristic sampling methods to solve large problems. The software is intended to appeal to a broad spectrum of users, so the interface has been carefully designed to be highly intuititive and is inspired by the well-known lm function. The issue of prior inputs is carefully addressed. In the default usage (fully automatic for the user)BayesVarSel implements the criteria-based priors proposed by Bayarri et al (2012), but the advanced user has the possibility of using several other popular priors in the literature. The package is available through the Comprehensive R Archive Network, CRAN. We illustrate the use of BayesVarSel with several data examples.

</details>

<details>

<summary>2016-11-25 15:16:45 - On the Exponentially Weighted Aggregate with the Laplace Prior</summary>

- *Arnak S. Dalalyan, Edwin Grappin, Quentin Paris*

- `1611.08483v1` - [abs](http://arxiv.org/abs/1611.08483v1) - [pdf](http://arxiv.org/pdf/1611.08483v1)

> In this paper, we study the statistical behaviour of the Exponentially Weighted Aggregate (EWA) in the problem of high-dimensional regression with fixed design. Under the assumption that the underlying regression vector is sparse, it is reasonable to use the Laplace distribution as a prior. The resulting estimator and, specifically, a particular instance of it referred to as the Bayesian lasso, was already used in the statistical literature because of its computational convenience, even though no thorough mathematical analysis of its statistical properties was carried out. The present work fills this gap by establishing sharp oracle inequalities for the EWA with the Laplace prior. These inequalities show that if the temperature parameter is small, the EWA with the Laplace prior satisfies the same type of oracle inequality as the lasso estimator does, as long as the quality of estimation is measured by the prediction loss. Extensions of the proposed methodology to the problem of prediction with low-rank matrices are considered.

</details>

<details>

<summary>2016-11-26 16:27:44 - Space and circular time log Gaussian Cox processes with application to crime event data</summary>

- *Shinichiro Shirota, Alan E. Gelfand*

- `1611.08719v1` - [abs](http://arxiv.org/abs/1611.08719v1) - [pdf](http://arxiv.org/pdf/1611.08719v1)

> We view the locations and times of a collection of crime events as a space-time point pattern. So, with either a nonhomogeneous Poisson process or with a more general Cox process, we need to specify a space-time intensity. For the latter, we need a \emph{random} intensity which we model as a realization of a spatio-temporal log Gaussian process. Importantly, we view time as circular not linear, necessitating valid separable and nonseparable covariance functions over a bounded spatial region crossed with circular time. In addition, crimes are classified by crime type. Furthermore, each crime event is recorded by day of the year which we convert to day of the week marks.   The contribution here is to develop models to accommodate such data. Our specifications take the form of hierarchical models which we fit within a Bayesian framework. In this regard, we consider model comparison between the nonhomogeneous Poisson process and the log Gaussian Cox process. We also compare separable vs. nonseparable covariance specifications.   Our motivating dataset is a collection of crime events for the city of San Francisco during the year 2012. We have location, hour, day of the year, and crime type for each event. We investigate models to enhance our understanding of the set of incidences.

</details>

<details>

<summary>2016-11-26 22:00:13 - Bayesian Analysis of AR(1) Model</summary>

- *Hossein Masoumi Karakani, Janet van Niekerk, Paul van Staden*

- `1611.08747v1` - [abs](http://arxiv.org/abs/1611.08747v1) - [pdf](http://arxiv.org/pdf/1611.08747v1)

> The first-order autoregressive process, AR (1), has been widely used and implemented in time series analysis. Different estimation methods have been employed in order to estimate the autoregressive parameter. This article focuses on subjective Bayesian estimation as opposed to objective Bayesian estimation and frequentist procedures. The truncated normal distribution is considered as a prior, to impose stationarity. The posterior distribution as well as the Bayes estimator are derived. A comparative study between the newly derived estimator and other existing estimation methods (frequentist) is employed in terms of simulation and real data. Furthermore, a posterior sensitivity analysis is performed based on four different priors; g prior, natural conjugate prior, Jeffreys' prior and truncated normal prior and the performance is compared in terms of Highest Posterior Density Region criterion.

</details>

<details>

<summary>2016-11-27 05:22:30 - Learning without recall in directed circles and rooted trees</summary>

- *M. Amin Rahimian, Ali Jadbabaie*

- `1611.08791v1` - [abs](http://arxiv.org/abs/1611.08791v1) - [pdf](http://arxiv.org/pdf/1611.08791v1)

> This work investigates the case of a network of agents that attempt to learn some unknown state of the world amongst the finitely many possibilities. At each time step, agents all receive random, independently distributed private signals whose distributions are dependent on the unknown state of the world. However, it may be the case that some or any of the agents cannot distinguish between two or more of the possible states based only on their private observations, as when several states result in the same distribution of the private signals. In our model, the agents form some initial belief (probability distribution) about the unknown state and then refine their beliefs in accordance with their private observations, as well as the beliefs of their neighbors. An agent learns the unknown state when her belief converges to a point mass that is concentrated at the true state. A rational agent would use the Bayes' rule to incorporate her neighbors' beliefs and own private signals over time. While such repeated applications of the Bayes' rule in networks can become computationally intractable, in this paper, we show that in the canonical cases of directed star, circle or path networks and their combinations, one can derive a class of memoryless update rules that replicate that of a single Bayesian agent but replace the self beliefs with the beliefs of the neighbors. This way, one can realize an exponentially fast rate of learning similar to the case of Bayesian (fully rational) agents. The proposed rules are a special case of the Learning without Recall.

</details>

<details>

<summary>2016-11-28 08:09:04 - Checking for prior-data conflict using prior to posterior divergences</summary>

- *David J. Nott, Xueou Wang, Michael Evans, Berthold-Georg Englert*

- `1611.00113v3` - [abs](http://arxiv.org/abs/1611.00113v3) - [pdf](http://arxiv.org/pdf/1611.00113v3)

> When using complex Bayesian models to combine information, the checking for consistency of the information being combined is good statistical practice. Here a new method is developed for detecting prior-data conflicts in Bayesian models based on comparing the observed value of a prior to posterior divergence to its distribution under the prior predictive distribution for the data. The divergence measure used in our model check is a measure of how much beliefs have changed from prior to posterior, and can be thought of as a measure of the overall size of a relative belief function. It is shown that the proposed method is intuitive, has desirable properties, can be extended to hierarchical settings, and is related asymptotically to Jeffreys' and reference prior distributions. In the case where calculations are difficult, the use of variational approximations as a way of relieving the computational burden is suggested. The methods are compared in a number of examples with an alternative but closely related approach in the literature based on the prior predictive distribution of a minimal sufficient statistic.

</details>

<details>

<summary>2016-11-28 15:59:31 - Statistical models for cores decomposition of an undirected random graph</summary>

- *Vishesh Karwa, Michael J. Pelsmajer, Sonja PetroviÄ, Despina Stasi, Dane Wilburne*

- `1410.7357v3` - [abs](http://arxiv.org/abs/1410.7357v3) - [pdf](http://arxiv.org/pdf/1410.7357v3)

> The $k$-core decomposition is a widely studied summary statistic that describes a graph's global connectivity structure. In this paper, we move beyond using $k$-core decomposition as a tool to summarize a graph and propose using $k$-core decomposition as a tool to model random graphs. We propose using the shell distribution vector, a way of summarizing the decomposition, as a sufficient statistic for a family of exponential random graph models. We study the properties and behavior of the model family, implement a Markov chain Monte Carlo algorithm for simulating graphs from the model, implement a direct sampler from the set of graphs with a given shell distribution, and explore the sampling distributions of some of the commonly used complementary statistics as good candidates for heuristic model fitting. These algorithms provide first fundamental steps necessary for solving the following problems: parameter estimation in this ERGM, extending the model to its Bayesian relative, and developing a rigorous methodology for testing goodness of fit of the model and model selection. The methods are applied to a synthetic network as well as the well-known Sampson monks dataset.

</details>

<details>

<summary>2016-11-28 16:28:41 - Robust Variational Inference</summary>

- *Michael Figurnov, Kirill Struminsky, Dmitry Vetrov*

- `1611.09226v1` - [abs](http://arxiv.org/abs/1611.09226v1) - [pdf](http://arxiv.org/pdf/1611.09226v1)

> Variational inference is a powerful tool for approximate inference. However, it mainly focuses on the evidence lower bound as variational objective and the development of other measures for variational inference is a promising area of research. This paper proposes a robust modification of evidence and a lower bound for the evidence, which is applicable when the majority of the training set samples are random noise objects. We provide experiments for variational autoencoders to show advantage of the objective over the evidence lower bound on synthetic datasets obtained by adding uninformative noise objects to MNIST and OMNIGLOT. Additionally, for the original MNIST and OMNIGLOT datasets we observe a small improvement over the non-robust evidence lower bound.

</details>

<details>

<summary>2016-11-29 19:01:06 - Paired-move multiple-try stochastic search for Bayesian variable selection</summary>

- *Xu Chen, Shaan Qamar, Surya T. Tokdar*

- `1611.09790v1` - [abs](http://arxiv.org/abs/1611.09790v1) - [pdf](http://arxiv.org/pdf/1611.09790v1)

> Variable selection is a key issue when analyzing high-dimensional data. The explosion of data with large sample sizes and dimensionality brings new challenges to this problem in both inference accuracy and computational complexity. To alleviate these problems, we propose a new scalable Markov chain Monte Carlo (MCMC) sampling algorithm for "large $p$ small $n$" scenarios by generalizing multiple-try Metropolis to discrete model spaces and further incorporating neighborhood-based stochastic search. The proof of reversibility of the proposed MCMC algorithm is provided. Extensive simulation studies are performed to examine the efficiency of the new algorithm compared with existing methods. A real data example is provided to illustrate the prediction performances of the new algorithm.

</details>

<details>

<summary>2016-11-29 21:25:54 - An Affine-Invariant Bayesian Cluster Process</summary>

- *Hsin-Hsiung Huang, Jie Yang*

- `1611.09890v1` - [abs](http://arxiv.org/abs/1611.09890v1) - [pdf](http://arxiv.org/pdf/1611.09890v1)

> In order to identify clusters of objects with features transformed by unknown affine transformations, we develop a Bayesian cluster process which is invariant with respect to certain linear transformations of the feature space and able to cluster data without knowing the number of clusters in advance. Specifically, our proposed method can identify clusters invariant to orthogonal transformations under model I, invariant to scaling-coordinate orthogonal transformations under model II, or invariant to arbitrary non-singular linear transformations under model III. The proposed split-merge algorithm leads to an irreducible and aperiodic Markov chain, which is also efficient at identifying clusters reasonably well for various applications. We illustrate the applications of our approach to both synthetic and real data such as leukemia gene expression data for model I; wine data and two half-moons benchmark data for model II; three-dimensional Denmark road geographic coordinate system data and an arbitrary non-singular transformed two half-moons data for model III. These examples show that the proposed method could be widely applied in many fields, especially for finding the number of clusters and identifying clusters of samples of interest in aerial photography and genomic data.

</details>

<details>

<summary>2016-11-30 15:37:58 - Non-Parametric Approximations for Anisotropy Estimation in Two-dimensional Differentiable Gaussian Random Fields</summary>

- *Manolis P. Petrakis, Dionissios T. Hristopulos*

- `1203.5010v2` - [abs](http://arxiv.org/abs/1203.5010v2) - [pdf](http://arxiv.org/pdf/1203.5010v2)

> Spatially referenced data often have autocovariance functions with elliptical isolevel contours, a property known as geometric anisotropy. The anisotropy parameters include the tilt of the ellipse (orientation angle) with respect to a reference axis and the aspect ratio of the principal correlation lengths. Since these parameters are unknown a priori, sample estimates are needed to define suitable spatial models for the interpolation of incomplete data. The distribution of the anisotropy statistics is determined by a non-Gaussian sampling joint probability density. By means of analytical calculations, we derive an explicit expression for the joint probability density function of the anisotropy statistics for Gaussian, stationary and differentiable random fields. Based on this expression, we obtain an approximate joint density which we use to formulate a statistical test for isotropy. The approximate joint density is independent of the autocovariance function and provides conservative probability and confidence regions for the anisotropy parameters. We validate the theoretical analysis by means of simulations using synthetic data, and we illustrate the detection of anisotropy changes with a case study involving background radiation exposure data. The approximate joint density provides (i) a stand-alone approximate estimate of the anisotropy statistics distribution (ii) informed initial values for maximum likelihood estimation, and (iii) a useful prior for Bayesian anisotropy inference.

</details>

<details>

<summary>2016-11-30 17:10:52 - Regularized maximum likelihood estimation of covariance matrices of elliptical distributions</summary>

- *Christophe Culan, Claude Adnet*

- `1611.10266v1` - [abs](http://arxiv.org/abs/1611.10266v1) - [pdf](http://arxiv.org/pdf/1611.10266v1)

> The maximum likelihood principle is widely used in statistics, and the associated estimators often display good properties. indeed maximum likelihood estimators are guaranteed to be asymptotically efficient under mild conditions. However in some settings, one has too few samples to get a good estimation. It then becomes desirable to take into account prior information about the distribution one wants to estimate. One possible approach is to extend the maximum likelihood principle in a bayesian context, which then becomes a maximum a posteriori estimate; however this requires a distribution model on the distribution parameters. We shall therefore concentrate on the alternative approach of regularized estimators in this paper; we will show how they can be naturally introduced in the framework of maximum likelihood estimation, and how they can be extended to form robust estimators which can reject outliers.

</details>


## 2016-12

<details>

<summary>2016-12-01 01:52:09 - Bayesian Non-parametric Simultaneous Quantile Regression for Complete and Grid Data</summary>

- *Priyam Das, Subhashis Ghosal*

- `1612.00111v1` - [abs](http://arxiv.org/abs/1612.00111v1) - [pdf](http://arxiv.org/pdf/1612.00111v1)

> In this paper, we consider Bayesian methods for non-parametric quantile regressions with multiple continuous predictors ranging values in the unit interval. In the first method, the quantile function is assumed to be smooth over the explanatory variable and is expanded in tensor product of B-spline basis functions. While in the second method, the distribution function is assumed to be smooth over the explanatory variable and is expanded in tensor product of B-spline basis functions. Unlike other existing methods of non-parametric quantile regressions, the proposed methods estimate the whole quantile function instead of estimating on a grid of quantiles. Priors on the B-spline coefficients are put in such a way that the monotonicity of the estimated quantile levels are maintained unlike local polynomial quantile regression methods. The proposed methods have also been modified for quantile grid data where only the percentile range of each response observations are known. Simulations studies have been provided for both complete and quantile grid data. The proposed method has been used to estimate the quantiles of US household income data and North Atlantic hurricane intensity data.

</details>

<details>

<summary>2016-12-01 10:24:09 - Instrumental variable approaches for estimating complier average causal effects on bivariate outcomes in randomised trials with non-compliance</summary>

- *Karla DiazOrdaz, Angelo Franchini, Richard Grieve*

- `1601.07127v2` - [abs](http://arxiv.org/abs/1601.07127v2) - [pdf](http://arxiv.org/pdf/1601.07127v2)

> In Randomised Controlled Trials (RCT) with treatment non-compliance, instrumental variable approaches are used to estimate complier average causal effects. We extend these approaches to cost-effectiveness analyses, where methods need to recognise the correlation between cost and health outcomes.   We propose a Bayesian full likelihood (BFL) approach, which jointly models the effects of random assignment on treatment received and the outcomes, and a three-stage least squares (3sls) method, which acknowledges the correlation between the endpoints, and the endogeneity of the treatment received.   This investigation is motivated by the REFLUX study, which exemplifies the setting where compliance differs between the RCT and routine practice. A simulation is used to compare the methods performance.   We find that failure to model the correlation between the outcomes and treatment received correctly can result in poor CI coverage and biased estimates. By contrast, BFL and 3sls methods provide unbiased estimates with good coverage.

</details>

<details>

<summary>2016-12-01 19:08:12 - Tuning the Scheduling of Distributed Stochastic Gradient Descent with Bayesian Optimization</summary>

- *Valentin Dalibard, Michael Schaarschmidt, Eiko Yoneki*

- `1612.00383v1` - [abs](http://arxiv.org/abs/1612.00383v1) - [pdf](http://arxiv.org/pdf/1612.00383v1)

> We present an optimizer which uses Bayesian optimization to tune the system parameters of distributed stochastic gradient descent (SGD). Given a specific context, our goal is to quickly find efficient configurations which appropriately balance the load between the available machines to minimize the average SGD iteration time. Our experiments consider setups with over thirty parameters. Traditional Bayesian optimization, which uses a Gaussian process as its model, is not well suited to such high dimensional domains. To reduce convergence time, we exploit the available structure. We design a probabilistic model which simulates the behavior of distributed SGD and use it within Bayesian optimization. Our model can exploit many runtime measurements for inference per evaluation of the objective function. Our experiments show that our resulting optimizer converges to efficient configurations within ten iterations, the optimized configurations outperform those found by generic optimizer in thirty iterations by up to 2X.

</details>

<details>

<summary>2016-12-01 19:41:50 - Hypervolume-based Multi-objective Bayesian Optimization with Student-t Processes</summary>

- *Joachim van der Herten, Ivo Couckuyt, Tom Dhaene*

- `1612.00393v1` - [abs](http://arxiv.org/abs/1612.00393v1) - [pdf](http://arxiv.org/pdf/1612.00393v1)

> Student-$t$ processes have recently been proposed as an appealing alternative non-parameteric function prior. They feature enhanced flexibility and predictive variance. In this work the use of Student-$t$ processes are explored for multi-objective Bayesian optimization. In particular, an analytical expression for the hypervolume-based probability of improvement is developed for independent Student-$t$ process priors of the objectives. Its effectiveness is shown on a multi-objective optimization problem which is known to be difficult with traditional Gaussian processes.

</details>

<details>

<summary>2016-12-01 22:50:33 - Multibrand geographic experiments</summary>

- *Art B. Owen, Tristan Launay*

- `1612.00503v1` - [abs](http://arxiv.org/abs/1612.00503v1) - [pdf](http://arxiv.org/pdf/1612.00503v1)

> In a geographic experiment to measure advertising effectiveness, some regions (hereafter GEOs) get increased advertising while others do not. This paper looks at running $B>1$ such experiments simultaneously on $B$ different brands in $G$ GEOs, and then using shrinkage methods to estimate returns to advertising. There are important practical gains from doing this. Data from any one brand helps to estimate the return of all other brands. We see this in both a frequentist and Bayesian formulation. As a result, each individual experiment could be made smaller and less expensive when they are analyzed together. We also provide an experimental design for multibrand experiments where half of the brands have increased spend in each GEO while half of the GEOs have increased spend for each brand. For $G>B$ the design is a two level factorial for each brand and simultaneously a supersaturated design for the GEOs. Multiple simultaneous experiments also allow one to identify GEOs in which advertising is generally more effective. That cannot be done in the single brand experiments we consider.

</details>

<details>

<summary>2016-12-02 04:35:42 - Bayesian Nonparametric Modeling of Heterogeneous Groups of Censored Data</summary>

- *Alexandre PichÃ©, Russell Steele, Ian Shrier, Stephanie Long*

- `1610.07262v2` - [abs](http://arxiv.org/abs/1610.07262v2) - [pdf](http://arxiv.org/pdf/1610.07262v2)

> Datasets containing large samples of time-to-event data arising from several small heterogeneous groups are commonly encountered in statistics. This presents problems as they cannot be pooled directly due to their heterogeneity or analyzed individually because of their small sample size. Bayesian nonparametric modelling approaches can be used to model such datasets given their ability to flexibly share information across groups. In this paper, we will compare three popular Bayesian nonparametric methods for modelling the survival functions of heterogeneous groups. Specifically, we will first compare the modelling accuracy of the Dirichlet process, the hierarchical Dirichlet process, and the nested Dirichlet process on simulated datasets of different sizes, where group survival curves differ in shape or in expectation. We, then, will compare the models on a real-world injury dataset.

</details>

<details>

<summary>2016-12-02 07:44:45 - Active Search for Sparse Signals with Region Sensing</summary>

- *Yifei Ma, Roman Garnett, Jeff Schneider*

- `1612.00583v1` - [abs](http://arxiv.org/abs/1612.00583v1) - [pdf](http://arxiv.org/pdf/1612.00583v1)

> Autonomous systems can be used to search for sparse signals in a large space; e.g., aerial robots can be deployed to localize threats, detect gas leaks, or respond to distress calls. Intuitively, search algorithms may increase efficiency by collecting aggregate measurements summarizing large contiguous regions. However, most existing search methods either ignore the possibility of such region observations (e.g., Bayesian optimization and multi-armed bandits) or make strong assumptions about the sensing mechanism that allow each measurement to arbitrarily encode all signals in the entire environment (e.g., compressive sensing). We propose an algorithm that actively collects data to search for sparse signals using only noisy measurements of the average values on rectangular regions (including single points), based on the greedy maximization of information gain. We analyze our algorithm in 1d and show that it requires $\tilde{O}(\frac{n}{\mu^2}+k^2)$ measurements to recover all of $k$ signal locations with small Bayes error, where $\mu$ and $n$ are the signal strength and the size of the search space, respectively. We also show that active designs can be fundamentally more efficient than passive designs with region sensing, contrasting with the results of Arias-Castro, Candes, and Davenport (2013). We demonstrate the empirical performance of our algorithm on a search problem using satellite image data and in high dimensions.

</details>

<details>

<summary>2016-12-02 20:13:50 - Variable Effects of Climate on Forest Growth in Relation to Climate Extremes, Disturbance, and Forest Stand Dynamics</summary>

- *Malcolm S. Itter, Andrew O. Finley, Anthony W. D'Amato, Jane R. Foster, John B. Bradford*

- `1602.07228v2` - [abs](http://arxiv.org/abs/1602.07228v2) - [pdf](http://arxiv.org/pdf/1602.07228v2)

> Changes in the frequency, duration, and severity of climate extremes are forecast to occur under global climate change. The impacts of climate extremes on forest productivity and health are complicated by potential interactions with disturbance events and stand dynamics. The effects of stand dynamics on forest responses to climate and disturbance are particularly important given forest characteristics driven by stand dynamics can be modified through forest management with the goal of increasing forest resistance and resilience to climate change. We develop a hierarchical Bayesian state-space model allowing climate effects on tree growth to vary over time and in relation to climate extremes, disturbance events, and stand dynamics. We apply the model to a dendrochronology dataset comprising measurements from forest stands of varying composition, structure, and development stage in northeastern Minnesota. Results indicate average forest growth was most sensitive to variables describing climatic water deficit. Forest growth responses to water deficit were partitioned into responses driven by climatic threshold exceedances and interactions with forest tent caterpillar defoliation. Forest growth was both resistant and resilient to climate extremes with the majority of forest growth responses occurring after multiple climatic threshold exceedances or insect defoliation events. Forest growth was most sensitive to water deficit during periods of high stem density following major regeneration events when average inter-tree competition was high. Results suggest that forest growth resistance and resilience to interactions between climate extremes and insect defoliation can be increased through management steps such as thinning to reduce competition during early stages of stand development and small-group selection harvests to maintain forest structures characteristic of older, mature stands.

</details>

<details>

<summary>2016-12-03 10:44:50 - On the Pitfalls of Nested Monte Carlo</summary>

- *Tom Rainforth, Robert Cornish, Hongseok Yang, Frank Wood*

- `1612.00951v1` - [abs](http://arxiv.org/abs/1612.00951v1) - [pdf](http://arxiv.org/pdf/1612.00951v1)

> There is an increasing interest in estimating expectations outside of the classical inference framework, such as for models expressed as probabilistic programs. Many of these contexts call for some form of nested inference to be applied. In this paper, we analyse the behaviour of nested Monte Carlo (NMC) schemes, for which classical convergence proofs are insufficient. We give conditions under which NMC will converge, establish a rate of convergence, and provide empirical data that suggests that this rate is observable in practice. Finally, we prove that general-purpose nested inference schemes are inherently biased. Our results serve to warn of the dangers associated with naive composition of inference and models.

</details>

<details>

<summary>2016-12-03 20:25:19 - Nonparametric Bayes Models of Fiber Curves Connecting Brain Regions</summary>

- *Zhengwu Zhang, Maxime Descoteaux, David B. Dunson*

- `1612.01014v1` - [abs](http://arxiv.org/abs/1612.01014v1) - [pdf](http://arxiv.org/pdf/1612.01014v1)

> In studying structural inter-connections in the human brain, it is common to first estimate fiber bundles connecting different regions of the brain relying on diffusion MRI. These fiber bundles act as highways for neural activity and communication, snaking through the brain and connecting different regions. Current statistical methods for analyzing these fibers reduce the rich information into an adjacency matrix, with the elements containing a count of the number of fibers or a mean diffusion feature (such as fractional anisotropy) along the fibers. The goal of this article is to avoid discarding the rich functional data on the shape, size and orientation of fibers, developing flexible models for characterizing the population distribution of fibers between brain regions of interest within and across different individuals. We start by decomposing each fiber in each individual's brain into a corresponding rotation matrix, shape and translation from a global reference curve. These components can be viewed as data lying on a product space composed of different Euclidean spaces and manifolds. To non-parametrically model the distribution within and across individuals, we rely on a hierarchical mixture of product kernels specific to the component spaces. Taking a Bayesian approach to inference, we develop an efficient method for posterior sampling. The approach automatically produces clusters of fibers within and across individuals, and yields interesting new insights into variation in fiber curves, while providing a useful starting point for more elaborate models relating fibers to covariates and neuropsychiatric traits.

</details>

<details>

<summary>2016-12-05 13:22:38 - Bayesian Semiparametric Multivariate Density Deconvolution</summary>

- *Abhra Sarkar, Debdeep Pati, Bani K. Mallick, Raymond J. Carroll*

- `1404.6462v5` - [abs](http://arxiv.org/abs/1404.6462v5) - [pdf](http://arxiv.org/pdf/1404.6462v5)

> We consider the problem of multivariate density deconvolution when the interest lies in estimating the distribution of a vector-valued random variable but precise measurements of the variable of interest are not available, observations being contaminated with additive measurement errors. The existing sparse literature on the problem assumes the density of the measurement errors to be completely known. We propose robust Bayesian semiparametric multivariate deconvolution approaches when the measurement error density is not known but replicated proxies are available for each unobserved value of the random vector. Additionally, we allow the variability of the measurement errors to depend on the associated unobserved value of the vector of interest through unknown relationships which also automatically includes the case of multivariate multiplicative measurement errors. Basic properties of finite mixture models, multivariate normal kernels and exchangeable priors are exploited in many novel ways to meet the modeling and computational challenges. Theoretical results that show the flexibility of the proposed methods are provided. We illustrate the efficiency of the proposed methods in recovering the true density of interest through simulation experiments. The methodology is applied to estimate the joint consumption pattern of different dietary components from contaminated 24 hour recalls.

</details>

<details>

<summary>2016-12-05 19:12:00 - A Nonparametric Latent Factor Model For Location-Aware Video Recommendations</summary>

- *Ehtsham Elahi*

- `1612.01481v1` - [abs](http://arxiv.org/abs/1612.01481v1) - [pdf](http://arxiv.org/pdf/1612.01481v1)

> We are interested in learning customers' video preferences from their historic viewing patterns and geographical location. We consider a Bayesian latent factor modeling approach for this task. In order to tune the complexity of the model to best represent the data, we make use of Bayesian nonparameteric techniques. We describe an inference technique that can scale to large real-world data sets. Finally we show results obtained by applying the model to a large internal Netflix data set, that illustrates that the model was able to capture interesting relationships between viewing patterns and geographical location.

</details>

<details>

<summary>2016-12-05 21:40:47 - Joint hierarchical models for sparsely sampled high-dimensional LiDAR and forest variables</summary>

- *Andrew O. Finley, Sudipto Banerjee, Yuzhen Zhou, Bruce D. Cook, Chad Babcock*

- `1603.07409v2` - [abs](http://arxiv.org/abs/1603.07409v2) - [pdf](http://arxiv.org/pdf/1603.07409v2)

> Recent advancements in remote sensing technology, specifically Light Detection and Ranging (LiDAR) sensors, provide the data needed to quantify forest characteristics at a fine spatial resolution over large geographic domains. From an inferential standpoint, there is interest in prediction and interpolation of the often sparsely sampled and spatially misaligned LiDAR signals and forest variables. We propose a fully process-based Bayesian hierarchical model for above ground biomass (AGB) and LiDAR signals. The process-based framework offers richness in inferential capabilities, e.g., inference on the entire underlying processes instead of estimates only at pre-specified points. Key challenges we obviate include misalignment between the AGB observations and LiDAR signals and the high-dimensionality in the model emerging from LiDAR signals in conjunction with the large number of spatial locations. We offer simulation experiments to evaluate our proposed models and also apply them to a challenging dataset comprising LiDAR and spatially coinciding forest inventory variables collected on the Penobscot Experimental Forest (PEF), Maine. Our key substantive contributions include AGB data products with associated measures of uncertainty for the PEF and, more broadly, a methodology that should find use in a variety of current and upcoming forest variable mapping efforts using sparsely sampled remotely sensed high-dimensional data.

</details>

<details>

<summary>2016-12-05 23:19:33 - Analyzing Ozone Concentration by Bayesian Spatio-temporal Quantile Regression</summary>

- *Priyam Das, Subhashis Ghosal*

- `1609.04843v2` - [abs](http://arxiv.org/abs/1609.04843v2) - [pdf](http://arxiv.org/pdf/1609.04843v2)

> Ground level Ozone is one of the six common air-pollutants on which the EPA has set national air quality standards. In order to capture the spatio-temporal trend of 1-hour and 8-hour average ozone concentration in the US, we develop a method for spatio-temporal simultaneous quantile regression. Unlike existing procedures, in the proposed method, smoothing across the sites is incorporated within modeling assumptions thus allowing borrowing of information across locations, an essential step when the number of samples in each location is low. The quantile function has been assumed to be linear in time and smooth over space and at any given site is given by a convex combination of two monotone increasing functions $\xi_1$ and $\xi_2$ not depending on time. A B-spline basis expansion with increasing coefficients varying smoothly over the space is used to put a prior and a Bayesian analysis is performed. We analyze the average daily 1-hour maximum and 8-hour maximum ozone concentration level data of US and California during 2006-2015 using the proposed method. It is noted that in the last ten years, there is an overall decreasing trend in both 1-hour maximum and 8-hour maximum ozone concentration level over the most parts of the US. In California, an overall a decreasing trend of 1-hour maximum ozone level is observed while no particular overall trend has been observed in the case of 8-hour maximum ozone level.

</details>

<details>

<summary>2016-12-06 00:03:31 - Rgbp: An R Package for Gaussian, Poisson, and Binomial Random Effects Models with Frequency Coverage Evaluations</summary>

- *Hyungsuk Tak, Joseph Kelly, Carl N. Morris*

- `1612.01595v1` - [abs](http://arxiv.org/abs/1612.01595v1) - [pdf](http://arxiv.org/pdf/1612.01595v1)

> Rgbp is an R package that provides estimates and verifiable confidence intervals for random effects in two-level conjugate hierarchical models for overdispersed Gaussian, Poisson, and Binomial data. Rgbp models aggregate data from k independent groups summarized by observed sufficient statistics for each random effect, such as sample means, possibly with covariates. Rgbp uses approximate Bayesian machinery with unique improper priors for the hyper-parameters, which leads to good repeated sampling coverage properties for random effects. A special feature of Rgbp is an option that generates synthetic data sets to check whether the interval estimates for random effects actually meet the nominal confidence levels. Additionally, Rgbp provides inference statistics for the hyper-parameters, e.g., regression coefficients.

</details>

<details>

<summary>2016-12-06 08:39:40 - Fast Measurements of Robustness to Changing Priors in Variational Bayes</summary>

- *Ryan Giordano, Tamara Broderick, Michael Jordan*

- `1611.07469v2` - [abs](http://arxiv.org/abs/1611.07469v2) - [pdf](http://arxiv.org/pdf/1611.07469v2)

> In Bayesian analysis, the posterior follows from the data and a choice of a prior and a likelihood. One hopes that the posterior is robust to reasonable variation in the choice of prior, since this choice is made by the modeler and is often somewhat subjective. A different, equally subjectively plausible choice of prior may result in a substantially different posterior, and so different conclusions drawn from the data. Were this to be the case, our conclusions would not be robust to the choice of prior. To determine whether our model is robust, we must quantify how sensitive our posterior is to perturbations of our prior. Despite the importance of the problem and a considerable body of literature, generic, easy-to-use methods to quantify Bayesian robustness are still lacking.   Abstract In this paper, we demonstrate that powerful measures of robustness can be easily calculated from Variational Bayes (VB) approximate posteriors. We begin with local robustness, which measures the effect of infinitesimal changes to the prior on a posterior mean of interest. In particular, we show that the influence function of Gustafson (2012) has a simple, easy-to-calculate closed form expression for VB approximations. We then demonstrate how local robustness measures can be inadequate for non-local prior changes, such as replacing one prior entirely with another. We propose a simple approximate non-local robustness measure and demonstrate its effectiveness on a simulated data set.

</details>

<details>

<summary>2016-12-06 15:56:17 - Fiducial, confidence and objective Bayesian posterior distributions for a multidimensional parameter</summary>

- *Piero Veronese, Eugenio Melilli*

- `1612.01882v1` - [abs](http://arxiv.org/abs/1612.01882v1) - [pdf](http://arxiv.org/pdf/1612.01882v1)

> We propose a way to construct fiducial distributions for a multidimensional parameter using a step-by-step conditional procedure related to the inferential importance of the components of the parameter. For discrete models, in which the non-uniqueness of the fiducial distribution is well known, we propose to use the geometric mean of the "extreme cases" and show its good behavior with respect to the more traditional arithmetic mean. Connections with the generalized fiducial inference approach developed by Hannig and with confidence distributions are also analyzed. The suggested procedure strongly simplifies when the statistical model belongs to a subclass of the natural exponential family, called conditionally reducible, which includes the multinomial and the negative-multinomial models. Furthermore, because fiducial inference and objective Bayesian analysis are both attempts to derive distributions for an unknown parameter without any prior information, it is natural to discuss their relationships. In particular, the reference posteriors, which also depend on the importance ordering of the parameters are the natural terms of comparison. We show that fiducial and reference posterior distributions coincide in the location-scale models, and we characterize the conditionally reducible natural exponential families for which this happens. The discussion of some classical examples closes the paper.

</details>

<details>

<summary>2016-12-07 03:39:59 - Distributed Gaussian Learning over Time-varying Directed Graphs</summary>

- *Angelia NediÄ, Alex Olshevsky, CÃ©sar A. Uribe*

- `1612.01600v2` - [abs](http://arxiv.org/abs/1612.01600v2) - [pdf](http://arxiv.org/pdf/1612.01600v2)

> We present a distributed (non-Bayesian) learning algorithm for the problem of parameter estimation with Gaussian noise. The algorithm is expressed as explicit updates on the parameters of the Gaussian beliefs (i.e. means and precision). We show a convergence rate of $O(1/k)$ with the constant term depending on the number of agents and the topology of the network. Moreover, we show almost sure convergence to the optimal solution of the estimation problem for the general case of time-varying directed graphs.

</details>

<details>

<summary>2016-12-07 12:17:27 - A Bayesian Approach to Predicting Disengaged Youth</summary>

- *David Kohn, Sally Cripps, Nick Glozier, Hugh Durrant-Whyte*

- `1612.00520v2` - [abs](http://arxiv.org/abs/1612.00520v2) - [pdf](http://arxiv.org/pdf/1612.00520v2)

> This article presents a Bayesian approach for predicting and identifying the factors which most influence an individual's propensity to fall into the category of Not in Employment Education or Training (NEET). The approach partitions the covariates into two groups: those which have the potential to be changed as a result of an intervention strategy and those which must be controlled for. This partition allows us to develop models and identify important factors conditional on the control covariates, which is useful for clinicians and policy makers who wish to identify potential intervention strategies. Using the data obtained by O'Dea (2014) we compare the results from this approach with the results from O'Dea (2014) and with the results obtained using the Bayesian variable selection procedure of Lamnisos (2009) when the covariates are not partitioned. We find that the relative importance of predictive factors varies greatly depending upon the control covariates. This has enormous implications when deciding on what interventions are most useful to prevent young people from being NEET.

</details>

<details>

<summary>2016-12-07 18:19:31 - Methods and Tools for Bayesian Variable Selection and Model Averaging in Univariate Linear Regression</summary>

- *Anabel Forte, Gonzalo Garcia-Donato, Mark Steel*

- `1612.02357v1` - [abs](http://arxiv.org/abs/1612.02357v1) - [pdf](http://arxiv.org/pdf/1612.02357v1)

> In this paper we briefly review the main methodological aspects concerned with the application of the Bayesian approach to model choice and model averaging in the context of variable selection in regression models. This includes prior elicitation, summaries of the posterior distribution and computational strategies. We then examine and compare various publicly available {\tt R}-packages for its practical implementation summarizing and explaining the differences between packages and giving recommendations for applied users. We find that all packages reviewed lead to very similar results, but there are potentially important differences in flexibility and efficiency of the packages.

</details>

<details>

<summary>2016-12-07 19:17:55 - A Model-Based Approach to Wildland Fire Reconstruction Using Sediment Charcoal Records</summary>

- *Malcolm S. Itter, Andrew O. Finley, Mevin B. Hooten, Philip E. Higuera, Jennifer R. Marlon, Ryan Kelly, Jason S. McLachlan*

- `1612.02382v1` - [abs](http://arxiv.org/abs/1612.02382v1) - [pdf](http://arxiv.org/pdf/1612.02382v1)

> Lake sediment charcoal records are used in paleoecological analyses to reconstruct fire history including the identification of past wildland fires. One challenge of applying sediment charcoal records to infer fire history is the separation of charcoal associated with local fire occurrence and charcoal originating from regional fire activity. Despite a variety of methods to identify local fires from sediment charcoal records, an integrated statistical framework for fire reconstruction is lacking. We develop a Bayesian point process model to estimate probability of fire associated with charcoal counts from individual-lake sediments and estimate mean fire return intervals. A multivariate extension of the model combines records from multiple lakes to reduce uncertainty in local fire identification and estimate a regional mean fire return interval. The univariate and multivariate models are applied to 13 lakes in the Yukon Flats region of Alaska. Both models resulted in similar mean fire return intervals (100-350 years) with reduced uncertainty under the multivariate model due to improved estimation of regional charcoal deposition. The point process model offers an integrated statistical framework for paleo-fire reconstruction and extends existing methods to infer regional fire history from multiple lake records with uncertainty following directly from posterior distributions.

</details>

<details>

<summary>2016-12-08 17:39:13 - A Poisson process reparameterisation for Bayesian inference for extremes</summary>

- *Paul Sharkey, Jonathan A. Tawn*

- `1605.03508v2` - [abs](http://arxiv.org/abs/1605.03508v2) - [pdf](http://arxiv.org/pdf/1605.03508v2)

> A common approach to modelling extreme values is to consider the excesses above a high threshold as realisations of a non-homogeneous Poisson process. While this method offers the advantage of modelling using threshold-invariant extreme value parameters, the dependence between these parameters makes estimation more difficult. We present a novel approach for Bayesian estimation of the Poisson process model parameters by reparameterising in terms of a tuning parameter $m$. This paper presents a method for choosing the optimal value of m that near-orthogonalises the parameters, which is achieved by minimising the correlation between the asymptotic posterior distribution of the parameters. This choice of m ensures more rapid convergence and efficient sampling from the joint posterior distribution using Markov Chain Monte Carlo methods. Samples from the parameterisation of interest are then obtained by a simple transform. Results are presented in the cases of identically and non-identically distributed models for extreme rainfall in Cumbria, UK.

</details>

<details>

<summary>2016-12-09 12:01:36 - Hyperpriors for MatÃ©rn fields with applications in Bayesian inversion</summary>

- *Lassi Roininen, Mark Girolami, Sari Lasanen, Markku Markkanen*

- `1612.02989v1` - [abs](http://arxiv.org/abs/1612.02989v1) - [pdf](http://arxiv.org/pdf/1612.02989v1)

> We introduce non-stationary Mat\'ern field priors with stochastic partial differential equations, and construct correlation length-scaling with hyperpriors. We model both the hyperprior and the Mat\'ern prior as continuous-parameter random fields. As hypermodels, we use Cauchy and Gaussian random fields, which we map suitably to a desired correlation length-scaling range. For computations, we discretise the models with finite difference methods. We consider the convergence of the discretised prior and posterior to the discretisation limit. We apply the developed methodology to certain interpolation and numerical differentiation problems, and show numerically that we can make Bayesian inversion which promotes competing constraints of smoothness and edge-preservation. For computing the conditional mean estimator of the posterior distribution, we use a combination of Gibbs and Metropolis-within-Gibbs sampling algorithms.

</details>

<details>

<summary>2016-12-09 18:20:00 - Advancing Bayesian Optimization: The Mixed-Global-Local (MGL) Kernel and Length-Scale Cool Down</summary>

- *Kim Peter Wabersich, Marc Toussaint*

- `1612.03117v1` - [abs](http://arxiv.org/abs/1612.03117v1) - [pdf](http://arxiv.org/pdf/1612.03117v1)

> Bayesian Optimization (BO) has become a core method for solving expensive black-box optimization problems. While much research focussed on the choice of the acquisition function, we focus on online length-scale adaption and the choice of kernel function. Instead of choosing hyperparameters in view of maximum likelihood on past data, we propose to use the acquisition function to decide on hyperparameter adaptation more robustly and in view of the future optimization progress. Further, we propose a particular kernel function that includes non-stationarity and local anisotropy and thereby implicitly integrates the efficiency of local convex optimization with global Bayesian optimization. Comparisons to state-of-the art BO methods underline the efficiency of these mechanisms on global optimization benchmarks.

</details>

<details>

<summary>2016-12-09 20:58:12 - Square Hellinger Subadditivity for Bayesian Networks and its Applications to Identity Testing</summary>

- *Constantinos Daskalakis, Qinxuan Pan*

- `1612.03164v1` - [abs](http://arxiv.org/abs/1612.03164v1) - [pdf](http://arxiv.org/pdf/1612.03164v1)

> We show that the square Hellinger distance between two Bayesian networks on the same directed graph, $G$, is subadditive with respect to the neighborhoods of $G$. Namely, if $P$ and $Q$ are the probability distributions defined by two Bayesian networks on the same DAG, our inequality states that the square Hellinger distance, $H^2(P,Q)$, between $P$ and $Q$ is upper bounded by the sum, $\sum_v H^2(P_{\{v\} \cup \Pi_v}, Q_{\{v\} \cup \Pi_v})$, of the square Hellinger distances between the marginals of $P$ and $Q$ on every node $v$ and its parents $\Pi_v$ in the DAG. Importantly, our bound does not involve the conditionals but the marginals of $P$ and $Q$. We derive a similar inequality for more general Markov Random Fields.   As an application of our inequality, we show that distinguishing whether two Bayesian networks $P$ and $Q$ on the same (but potentially unknown) DAG satisfy $P=Q$ vs $d_{\rm TV}(P,Q)>\epsilon$ can be performed from $\tilde{O}(|\Sigma|^{3/4(d+1)} \cdot n/\epsilon^2)$ samples, where $d$ is the maximum in-degree of the DAG and $\Sigma$ the domain of each variable of the Bayesian networks. If $P$ and $Q$ are defined on potentially different and potentially unknown trees, the sample complexity becomes $\tilde{O}(|\Sigma|^{4.5} n/\epsilon^2)$, whose dependence on $n, \epsilon$ is optimal up to logarithmic factors. Lastly, if $P$ and $Q$ are product distributions over $\{0,1\}^n$ and $Q$ is known, the sample complexity becomes $O(\sqrt{n}/\epsilon^2)$, which is optimal up to constant factors.

</details>

<details>

<summary>2016-12-10 00:52:01 - Bayesian Inference from Non-Ignorable Network Sampling Designs</summary>

- *Simon Lunagomez, Edoardo Airoldi*

- `1401.4718v2` - [abs](http://arxiv.org/abs/1401.4718v2) - [pdf](http://arxiv.org/pdf/1401.4718v2)

> Consider a population of individuals and a network that encodes social connections among them. We are interested in making inference on finite population and super-population estimands that are a function of both individuals' responses and of the network, from a sample. Neither the sampling frame nor the network are available. However, the sampling mechanism implicitly leverages the network to recruit individuals, thus partially revealing social interactions among the individuals in the sample, as well as their responses. This is a common setting that arises, for instance, in epidemiology and healthcare, where samples from hard-to-reach populations are collected using link-tracing mechanisms, including respondent-driven sampling. In this paper, we study statistical properties of popular network sampling mechanisms. We formulate the estimation problem in terms of Rubin's inferential framework to explicitly account for social network structure. We then identify key modeling elements that lead to inferences with good frequentist properties when dealing with data collected through non-ignorable network sampling mechanisms. We demonstrate these methods on a study of the incidence of HIV in Brazil

</details>

<details>

<summary>2016-12-10 14:47:40 - An Extension of Generalized Linear Models to Finite Mixture Outcome Distributions</summary>

- *Andrew M. Raim, Nagaraj K. Neerchal, Jorge G. Morel*

- `1612.03302v1` - [abs](http://arxiv.org/abs/1612.03302v1) - [pdf](http://arxiv.org/pdf/1612.03302v1)

> Finite mixture distributions arise in sampling a heterogeneous population. Data drawn from such a population will exhibit extra variability relative to any single subpopulation. Statistical models based on finite mixtures can assist in the analysis of categorical and count outcomes when standard generalized linear models (GLMs) cannot adequately account for variability observed in the data. We propose an extension of GLM where the response is assumed to follow a finite mixture distribution, while the regression of interest is linked to the mixture's mean. This approach may be preferred over a finite mixture of regressions when the population mean is the quantity of interest; here, only a single regression function must be specified and interpreted in the analysis. A technical challenge is that the mean of a finite mixture is a composite parameter which does not appear explicitly in the density. The proposed model is completely likelihood-based and maintains the link to the regression through a certain random effects structure. We consider typical GLM cases where means are either real-valued, constrained to be positive, or constrained to be on the unit interval. The resulting model is applied to two example datasets through a Bayesian analysis: one with success/failure outcomes and one with count outcomes. Supporting the extra variation is seen to improve residual plots and to appropriately widen prediction intervals.

</details>

<details>

<summary>2016-12-10 16:44:14 - A Bayesian Nonparametric Approach for Estimating Individualized Treatment-Response Curves</summary>

- *Yanbo Xu, Yanxun Xu, Suchi Saria*

- `1608.05182v2` - [abs](http://arxiv.org/abs/1608.05182v2) - [pdf](http://arxiv.org/pdf/1608.05182v2)

> We study the problem of estimating the continuous response over time to interventions using observational time series---a retrospective dataset where the policy by which the data are generated is unknown to the learner. We are motivated by applications where response varies by individuals and therefore, estimating responses at the individual-level is valuable for personalizing decision-making. We refer to this as the problem of estimating individualized treatment response (ITR) curves. In statistics, G-computation formula (Robins, 1986) has been commonly used for estimating treatment responses from observational data containing sequential treatment assignments. However, past studies have focused predominantly on obtaining point-in-time estimates at the population level. We leverage the G-computation formula and develop a novel Bayesian nonparametric (BNP) method that can flexibly model functional data and provide posterior inference over the treatment response curves at both the individual and population level. On a challenging dataset containing time series from patients admitted to a hospital, we estimate responses to treatments used in managing kidney function and show that the resulting fits are more accurate than alternative approaches. Accurate methods for obtaining ITRs from observational data can dramatically accelerate the pace at which personalized treatment plans become possible.

</details>

<details>

<summary>2016-12-12 07:40:50 - Global Estimation of Neonatal Mortality using a Bayesian Hierarchical Splines Regression Model</summary>

- *Monica Alexander, Leontine Alkema*

- `1612.03561v1` - [abs](http://arxiv.org/abs/1612.03561v1) - [pdf](http://arxiv.org/pdf/1612.03561v1)

> In recent years, much of the focus in monitoring child mortality has been on assessing changes in the under-five mortality rate (U5MR). However, as the U5MR decreases, the share of neonatal deaths (within the first month) tends to increase, warranting increased efforts in monitoring this indicator in addition to the U5MR. A Bayesian splines regression model is presented for estimating neonatal mortality rates (NMR) for all countries. In the model, the relationship between NMR and U5MR is assessed and used to inform estimates, and spline regression models are used to capture country-specific trends. As such, the resulting NMR estimates incorporate trends in overall child mortality while also capturing data-driven trends. The model is fitted to 195 countries using the database from the United Nations Interagency Group for Child Mortality Estimation, producing estimates from 1990, or earlier if data are available, until 2015. The results suggest that, above a U5MR of 34 deaths per 1000 live births, at the global level, a 1 per cent increase in the U5MR leads to a 0.6 per cent decrease in the ratio of NMR to U5MR. Below a U5MR of 34 deaths per 1000 live births, the proportion of deaths under-five that are neonatal is constant at around 54 per cent. However, the relationship between U5MR and NMR varies across countries. The model has now been adopted by the United Nations Inter-agency Group for Child Mortality Estimation.

</details>

<details>

<summary>2016-12-12 18:52:03 - Efficient Bayesian hierarchical functional data analysis with basis function approximations using Gaussian-Wishart processes</summary>

- *Jingjing Yang, Dennis D. Cox, Jong Soo Lee, Peng Ren, Taeryon Choi*

- `1512.07568v3` - [abs](http://arxiv.org/abs/1512.07568v3) - [pdf](http://arxiv.org/pdf/1512.07568v3)

> Functional data are defined as realizations of random functions (mostly smooth functions) varying over a continuum, which are usually collected with measurement errors on discretized grids. In order to accurately smooth noisy functional observations and deal with the issue of high-dimensional observation grids, we propose a novel Bayesian method based on the Bayesian hierarchical model with a Gaussian-Wishart process prior and basis function representations. We first derive an induced model for the basis-function coefficients of the functional data, and then use this model to conduct posterior inference through Markov chain Monte Carlo. Compared to the standard Bayesian inference that suffers serious computational burden and unstableness for analyzing high-dimensional functional data, our method greatly improves the computational scalability and stability, while inheriting the advantage of simultaneously smoothing raw observations and estimating the mean-covariance functions in a nonparametric way. In addition, our method can naturally handle functional data observed on random or uncommon grids. Simulation and real studies demonstrate that our method produces similar results as the standard Bayesian inference with low-dimensional common grids, while efficiently smoothing and estimating functional data with random and high-dimensional observation grids where the standard Bayesian inference fails. In conclusion, our method can efficiently smooth and estimate high-dimensional functional data, providing one way to resolve the curse of dimensionality for Bayesian functional data analysis with Gaussian-Wishart processes.

</details>

<details>

<summary>2016-12-12 19:24:39 - Frequency Coverage Properties of a Uniform Shrinkage Prior Distribution</summary>

- *Hyungsuk Tak*

- `1612.03858v1` - [abs](http://arxiv.org/abs/1612.03858v1) - [pdf](http://arxiv.org/pdf/1612.03858v1)

> A uniform shrinkage prior (USP) distribution on the unknown variance component of a random-effects model is known to produce good frequency properties. The USP has a parameter that determines the shape of its density function, but it has been neglected whether the USP can maintain such good frequency properties regardless of the choice for the shape parameter. We investigate which choice for the shape parameter of the USP produces Bayesian interval estimates of random effects that meet their nominal confidence levels better than several existent choices in the literature. Using univariate and multivariate Gaussian hierarchical models, we empirically show that the USP can achieve its best frequency properties when its shape parameter makes the USP behave similarly to an improper flat prior distribution on the unknown variance component.

</details>

<details>

<summary>2016-12-13 00:21:45 - Hybrid Repeat/Multi-point Sampling for Highly Volatile Objective Functions</summary>

- *Brett Israelsen, Nisar Ahmed*

- `1612.03981v1` - [abs](http://arxiv.org/abs/1612.03981v1) - [pdf](http://arxiv.org/pdf/1612.03981v1)

> A key drawback of the current generation of artificial decision-makers is that they do not adapt well to changes in unexpected situations. This paper addresses the situation in which an AI for aerial dog fighting, with tunable parameters that govern its behavior, will optimize behavior with respect to an objective function that must be evaluated and learned through simulations. Once this objective function has been modeled, the agent can then choose its desired behavior in different situations. Bayesian optimization with a Gaussian Process surrogate is used as the method for investigating the objective function. One key benefit is that during optimization the Gaussian Process learns a global estimate of the true objective function, with predicted outcomes and a statistical measure of confidence in areas that haven't been investigated yet. However, standard Bayesian optimization does not perform consistently or provide an accurate Gaussian Process surrogate function for highly volatile objective functions. We treat these problems by introducing a novel sampling technique called Hybrid Repeat/Multi-point Sampling. This technique gives the AI ability to learn optimum behaviors in a highly uncertain environment. More importantly, it not only improves the reliability of the optimization, but also creates a better model of the entire objective surface. With this improved model the agent is equipped to better adapt behaviors.

</details>

<details>

<summary>2016-12-13 08:22:32 - On the Log-Likelihood Ratio Evaluation of CWCU Linear and Widely Linear MMSE Data Estimators</summary>

- *Oliver Lang, Mario Huemer, Christian Hofbauer*

- `1607.02248v2` - [abs](http://arxiv.org/abs/1607.02248v2) - [pdf](http://arxiv.org/pdf/1607.02248v2)

> In soft decoding of data bits, the log-likelihood ratios are evaluated from the estimated data symbols. For proper constellation diagrams such as QPSK or 16-QAM, these data symbols are often estimated using the linear minimum mean square error (LMMSE) estimator. The LMMSE estimator only fulfills the weak Bayesian unbiasedness constraint. Recently, estimators fulfilling the more stringent component-wise conditionally unbiased (CWCU) constraints have been investigated, such as the CWCU LMMSE estimator. In this paper, we prove that the CWCU LMMSE estimates result in the very same log-likelihood ratios as the LMMSE estimates. For improper constellation diagrams such as 8-QAM, widely linear estimators are used. For this case, we show that the widely linear versions of the LMMSE estimator and the CWCU LMMSE estimator also yield identical log-likelihood ratios. Finally, we give a simulation example which illustrates a number of interesting properties of the discussed widely linear estimators.

</details>

<details>

<summary>2016-12-13 18:43:59 - Towards Adaptive Training of Agent-based Sparring Partners for Fighter Pilots</summary>

- *Brett W. Israelsen, Nisar Ahmed, Kenneth Center, Roderick Green, Winston Bennett Jr*

- `1612.04315v1` - [abs](http://arxiv.org/abs/1612.04315v1) - [pdf](http://arxiv.org/pdf/1612.04315v1)

> A key requirement for the current generation of artificial decision-makers is that they should adapt well to changes in unexpected situations. This paper addresses the situation in which an AI for aerial dog fighting, with tunable parameters that govern its behavior, must optimize behavior with respect to an objective function that is evaluated and learned through simulations. Bayesian optimization with a Gaussian Process surrogate is used as the method for investigating the objective function. One key benefit is that during optimization, the Gaussian Process learns a global estimate of the true objective function, with predicted outcomes and a statistical measure of confidence in areas that haven't been investigated yet. Having a model of the objective function is important for being able to understand possible outcomes in the decision space; for example this is crucial for training and providing feedback to human pilots. However, standard Bayesian optimization does not perform consistently or provide an accurate Gaussian Process surrogate function for highly volatile objective functions. We treat these problems by introducing a novel sampling technique called Hybrid Repeat/Multi-point Sampling. This technique gives the AI ability to learn optimum behaviors in a highly uncertain environment. More importantly, it not only improves the reliability of the optimization, but also creates a better model of the entire objective surface. With this improved model the agent is equipped to more accurately/efficiently predict performance in unexplored scenarios.

</details>

<details>

<summary>2016-12-14 04:13:49 - Fitting Bayesian item response models in Stata and Stan</summary>

- *Robert L. Grant, Daniel C. Furr, Bob Carpenter, Andrew Gelman*

- `1601.03443v2` - [abs](http://arxiv.org/abs/1601.03443v2) - [pdf](http://arxiv.org/pdf/1601.03443v2)

> Stata users have access to two easy-to-use implementations of Bayesian inference: Stata's native {\tt bayesmh} function and StataStan, which calls the general Bayesian engine Stan. We compare these on two models that are important for education research: the Rasch model and the hierarchical Rasch model. Stan (as called from Stata) fits a more general range of models than can be fit by {\tt bayesmh} and is also more scalable, in that it could easily fit models with at least ten times more parameters than could be fit using Stata's native Bayesian implementation. In addition, Stan runs between two and ten times faster than {\tt bayesmh} as measured in effective sample size per second: that is, compared to Stan, it takes Stata's built-in Bayesian engine twice to ten times as long to get inferences with equivalent precision. We attribute Stan's advantage in flexibility to its general modeling language, and its advantages in scalability and speed to an efficient sampling algorithm: Hamiltonian Monte Carlo using the no-U-turn sampler. In order to further investigate scalability, we also compared to the package Jags, which performed better than Stata's native Bayesian engine but not as well as StataStan.   Given its advantages in speed, generality, and scalability, and that Stan is open-source and can be run directly from Stata using StataStan, we recommend that Stata users adopt Stan as their Bayesian inference engine of choice.

</details>

<details>

<summary>2016-12-14 09:59:51 - Scalable Group Level Probabilistic Sparse Factor Analysis</summary>

- *Jesper L. Hinrich, SÃ¸ren F. V. Nielsen, Nicolai A. B. Riis, Casper T. Eriksen, Jacob FrÃ¸sig, Marco D. F. Kristensen, Mikkel N. Schmidt, Kristoffer H. Madsen, Morten MÃ¸rup*

- `1612.04555v1` - [abs](http://arxiv.org/abs/1612.04555v1) - [pdf](http://arxiv.org/pdf/1612.04555v1)

> Many data-driven approaches exist to extract neural representations of functional magnetic resonance imaging (fMRI) data, but most of them lack a proper probabilistic formulation. We propose a group level scalable probabilistic sparse factor analysis (psFA) allowing spatially sparse maps, component pruning using automatic relevance determination (ARD) and subject specific heteroscedastic spatial noise modeling. For task-based and resting state fMRI, we show that the sparsity constraint gives rise to components similar to those obtained by group independent component analysis. The noise modeling shows that noise is reduced in areas typically associated with activation by the experimental design. The psFA model identifies sparse components and the probabilistic setting provides a natural way to handle parameter uncertainties. The variational Bayesian framework easily extends to more complex noise models than the presently considered.

</details>

<details>

<summary>2016-12-15 02:40:35 - Bayesian Model Averaging for Ensemble-Based Estimates of Solvation Free Energies</summary>

- *Luke J. Gosink, Christopher C. Overall, Sarah M. Reehl, Paul D. Whitney, David L. Mobley, Nathan A. Baker*

- `1609.03257v2` - [abs](http://arxiv.org/abs/1609.03257v2) - [pdf](http://arxiv.org/pdf/1609.03257v2)

> This paper applies the Bayesian Model Averaging (BMA) statistical ensemble technique to estimate small molecule solvation free energies. There is a wide range of methods available for predicting solvation free energies, ranging from empirical statistical models to ab initio quantum mechanical approaches. Each of these methods is based on a set of conceptual assumptions that can affect predictive accuracy and transferability. Using an iterative statistical process, we have selected and combined solvation energy estimates using an ensemble of 17 diverse methods from the fourth Statistical Assessment of Modeling of Proteins and Ligands (SAMPL) blind prediction study to form a single, aggregated solvation energy estimate. The ensemble design process evaluates the statistical information in each individual method as well as the performance of the aggregate estimate obtained from the ensemble as a whole. Methods that possess minimal or redundant information are pruned from the ensemble and the evaluation process repeats until aggregate predictive performance can no longer be improved. We show that this process results in a final aggregate estimate that outperforms all individual methods by reducing estimate errors by as much as 91% to 1.2 kcal/mol accuracy. We also compare our iterative refinement approach to other statistical ensemble approaches and demonstrate that this iterative process reduces estimate errors by as much as 61%. This work provides a new approach for accurate solvation free energy prediction and lays the foundation for future work on aggregate models that can balance computational cost with prediction accuracy.

</details>

<details>

<summary>2016-12-15 11:43:21 - Simultaneous Estimation of Noise Variance and Number of Peaks in Bayesian Spectral Deconvolution</summary>

- *Satoru Tokuda, Kenji Nagata, Masato Okada*

- `1607.07590v2` - [abs](http://arxiv.org/abs/1607.07590v2) - [pdf](http://arxiv.org/pdf/1607.07590v2)

> The heuristic identification of peaks from noisy complex spectra often leads to misunderstanding of the physical and chemical properties of matter. In this paper, we propose a framework based on Bayesian inference, which enables us to separate multipeak spectra into single peaks statistically and consists of two steps. The first step is estimating both the noise variance and the number of peaks as hyperparameters based on Bayes free energy, which generally is not analytically tractable. The second step is fitting the parameters of each peak function to the given spectrum by calculating the posterior density, which has a problem of local minima and saddles since multipeak models are nonlinear and hierarchical. Our framework enables the escape from local minima or saddles by using the exchange Monte Carlo method and calculates Bayes free energy via the multiple histogram method. We discuss a simulation demonstrating how efficient our framework is and show that estimating both the noise variance and the number of peaks prevents overfitting, overpenalizing, and misunderstanding the precision of parameter estimation.

</details>

<details>

<summary>2016-12-15 12:49:40 - Bayesian estimation of incompletely observed diffusions</summary>

- *Frank van der Meulen, Moritz Schauer*

- `1606.04082v2` - [abs](http://arxiv.org/abs/1606.04082v2) - [pdf](http://arxiv.org/pdf/1606.04082v2)

> We present a general framework for Bayesian estimation of incompletely observed multivariate diffusion processes. Observations are assumed to be discrete in time, noisy and incomplete. We assume the drift and diffusion coefficient depend on an unknown parameter. A data-augmentation algorithm for drawing from the posterior distribution is presented which is based on simulating diffusion bridges conditional on a noisy incomplete observation at an intermediate time. The dynamics of such filtered bridges are derived and it is shown how these can be simulated using a generalised version of the guided proposals introduced in Schauer et al. (2016).

</details>

<details>

<summary>2016-12-15 12:59:33 - Adversarial Message Passing For Graphical Models</summary>

- *Theofanis Karaletsos*

- `1612.05048v1` - [abs](http://arxiv.org/abs/1612.05048v1) - [pdf](http://arxiv.org/pdf/1612.05048v1)

> Bayesian inference on structured models typically relies on the ability to infer posterior distributions of underlying hidden variables. However, inference in implicit models or complex posterior distributions is hard. A popular tool for learning implicit models are generative adversarial networks (GANs) which learn parameters of generators by fooling discriminators. Typically, GANs are considered to be models themselves and are not understood in the context of inference. Current techniques rely on inefficient global discrimination of joint distributions to perform learning, or only consider discriminating a single output variable. We overcome these limitations by treating GANs as a basis for likelihood-free inference in generative models and generalize them to Bayesian posterior inference over factor graphs. We propose local learning rules based on message passing minimizing a global divergence criterion involving cooperating local adversaries used to sidestep explicit likelihood evaluations. This allows us to compose models and yields a unified inference and learning framework for adversarial learning. Our framework treats model specification and inference separately and facilitates richly structured models within the family of Directed Acyclic Graphs, including components such as intractable likelihoods, non-differentiable models, simulators and generally cumbersome models. A key result of our treatment is the insight that Bayesian inference on structured models can be performed only with sampling and discrimination when using nonparametric variational families, without access to explicit distributions. As a side-result, we discuss the link to likelihood maximization. These approaches hold promise to be useful in the toolbox of probabilistic modelers and enrich the gamut of current probabilistic programming applications.

</details>

<details>

<summary>2016-12-15 13:16:46 - Expectation Propagation performs a smoothed gradient descent</summary>

- *Guillaume P. Dehaene*

- `1612.05053v1` - [abs](http://arxiv.org/abs/1612.05053v1) - [pdf](http://arxiv.org/pdf/1612.05053v1)

> Bayesian inference is a popular method to build learning algorithms but it is hampered by the fact that its key object, the posterior probability distribution, is often uncomputable. Expectation Propagation (EP) (Minka (2001)) is a popular algorithm that solves this issue by computing a parametric approximation (e.g: Gaussian) to the density of the posterior. However, while it is known empirically to quickly compute fine approximations, EP is extremely poorly understood which prevents it from being adopted by a larger fraction of the community.   The object of the present article is to shed intuitive light on EP, by relating it to other better understood methods. More precisely, we link it to using gradient descent to compute the Laplace approximation of a target probability distribution. We show that EP is exactly equivalent to performing gradient descent on a smoothed energy landscape: i.e: the original energy landscape convoluted with some smoothing kernel. This also relates EP to algorithms that compute the Gaussian approximation which minimizes the reverse KL divergence to the target distribution, a link that has been conjectured before but has not been proved rigorously yet. These results can help practitioners to get a better feel for how EP works, as well as lead to other new results on this important method.

</details>

<details>

<summary>2016-12-15 18:04:09 - Robust Particle Filter by Dynamic Averaging of Multiple Noise Models</summary>

- *Bin Liu*

- `1609.01336v2` - [abs](http://arxiv.org/abs/1609.01336v2) - [pdf](http://arxiv.org/pdf/1609.01336v2)

> State filtering is a key problem in many signal processing applications. From a series of noisy measurement, one would like to estimate the state of some dynamic system. Existing techniques usually adopt a Gaussian noise assumption which may result in a major degradation in performance when the measurements are with the presence of outliers. A robust algorithm immune to the presence of outliers is desirable. To this end, a robust particle filter (PF) algorithm is proposed, in which the heavier tailed Student's t distributions are employed together with the Gaussian distribution to model the measurement noise. The effect of each model is automatically and dynamically adjusted via a Bayesian model averaging mechanism. The validity of the proposed algorithm is evaluated by illustrative simulations.

</details>

<details>

<summary>2016-12-16 15:10:12 - The Bayesian analysis of contingency table data using the bayesloglin R package</summary>

- *Matthew Friedlander*

- `1612.05501v1` - [abs](http://arxiv.org/abs/1612.05501v1) - [pdf](http://arxiv.org/pdf/1612.05501v1)

> For log-linear analysis, the hyper Dirichlet conjugate prior is available to work in the Bayesian paradigm. With this prior, the MC3 algorithm allows for exploration of the space of models to try to find those with the highest posterior probability. Once top models have been identified, a block Gibbs sampler can be constructed to sample from the posterior distribution and to estimate parameters of interest. Our aim in this paper, is to introduce the bayesloglin R package \citep{R} which contains functions to carry out these tasks.

</details>

<details>

<summary>2016-12-16 16:22:19 - Priors on exchangeable directed graphs</summary>

- *Diana Cai, Nathanael Ackerman, Cameron Freer*

- `1510.08440v2` - [abs](http://arxiv.org/abs/1510.08440v2) - [pdf](http://arxiv.org/pdf/1510.08440v2)

> Directed graphs occur throughout statistical modeling of networks, and exchangeability is a natural assumption when the ordering of vertices does not matter. There is a deep structural theory for exchangeable undirected graphs, which extends to the directed case via measurable objects known as digraphons. Using digraphons, we first show how to construct models for exchangeable directed graphs, including special cases such as tournaments, linear orderings, directed acyclic graphs, and partial orderings. We then show how to construct priors on digraphons via the infinite relational digraphon model (di-IRM), a new Bayesian nonparametric block model for exchangeable directed graphs, and demonstrate inference on synthetic data.

</details>

<details>

<summary>2016-12-17 11:57:45 - Machine Learning, Linear and Bayesian Models for Logistic Regression in Failure Detection Problems</summary>

- *B. Pavlyshenko*

- `1612.05740v1` - [abs](http://arxiv.org/abs/1612.05740v1) - [pdf](http://arxiv.org/pdf/1612.05740v1)

> In this work, we study the use of logistic regression in manufacturing failures detection. As a data set for the analysis, we used the data from Kaggle competition Bosch Production Line Performance. We considered the use of machine learning, linear and Bayesian models. For machine learning approach, we analyzed XGBoost tree based classifier to obtain high scored classification. Using the generalized linear model for logistic regression makes it possible to analyze the influence of the factors under study. The Bayesian approach for logistic regression gives the statistical distribution for the parameters of the model. It can be useful in the probabilistic analysis, e.g. risk assessment.

</details>

<details>

<summary>2016-12-17 18:12:31 - Bayesian Distributed Lag Interaction Models to Identify Perinatal Windows of Vulnerability in Children's Health</summary>

- *Ander Wilson, Yueh-Hsiu Mathilda Chiu, Hsiao-Hsien Leon Hsu, Robert O. Wright, Rosalind J. Wright, Brent A. Coull*

- `1612.05800v1` - [abs](http://arxiv.org/abs/1612.05800v1) - [pdf](http://arxiv.org/pdf/1612.05800v1)

> Epidemiological research supports an association between maternal exposure to air pollution during pregnancy and adverse children's health outcomes. Advances in exposure assessment and statistics allow for estimation of both critical windows of vulnerability and exposure effect heterogeneity. Simultaneous estimation of windows of vulnerability and effect heterogeneity can be accomplished by fitting a distributed lag model (DLM) stratified by subgroup. However, this can provide an incomplete picture of how effects vary across subgroups because it does not allow for subgroups to have the same window but different within-window effects or to have different windows but the same within-window effect. Because the timing of some developmental processes are common across subpopulations of infants while for others the timing differs across subgroups, both scenarios are important to consider when evaluating health risks of prenatal exposures. We propose a new approach that partitions the DLM into a constrained functional predictor that estimates windows of vulnerability and a scalar effect representing the within-window effect directly. The proposed method allows for heterogeneity in only the window, only the within-window effect, or both. In a simulation study we show that a model assuming a shared component across groups results in lower bias and mean squared error for the estimated windows and effects when that component is in fact constant across groups. We apply the proposed method to estimate windows of vulnerability in the association between prenatal exposures to fine particulate matter and each of birth weight and asthma incidence, and estimate how these associations vary by sex and maternal obesity status, in a Boston-area prospective pre-birth cohort study.

</details>

<details>

<summary>2016-12-17 18:20:19 - Bayesian Semiparametric Mixed Effects Markov Chains</summary>

- *Abhra Sarkar, Jonathan Chabout, Joshua Jones Macopson, Erich D. Jarvis, David B. Dunson*

- `1611.05509v2` - [abs](http://arxiv.org/abs/1611.05509v2) - [pdf](http://arxiv.org/pdf/1611.05509v2)

> Studying the neurological, genetic and evolutionary basis of human vocal communication mechanisms using animal vocalization models is an important field of neuroscience. The data sets typically comprise structured sequences of syllables or `songs' produced by animals from different genotypes under different social contexts. We develop a novel Bayesian semiparametric framework for inference in such data sets. Our approach is built on a novel class of mixed effects Markov transition models for the songs that accommodates exogenous influences of genotype and context as well as animal-specific heterogeneity. We design efficient Markov chain Monte Carlo algorithms for posterior computation. Crucial advantages of the proposed approach include its ability to provide insights into key scientific queries related to global and local influences of the exogenous predictors on the transition dynamics via automated tests of hypotheses. The methodology is illustrated using simulation experiments and the aforementioned motivating application in neuroscience.

</details>

<details>

<summary>2016-12-19 10:55:16 - Parametric inference of hidden discrete-time diffusion processes by deconvolution</summary>

- *Salima El Kolei, Florian Pelgrin*

- `1512.08193v2` - [abs](http://arxiv.org/abs/1512.08193v2) - [pdf](http://arxiv.org/pdf/1512.08193v2)

> We study a new parametric approach for hidden discrete-time diffusion models. This method is based on contrast minimization and deconvolution and leads to estimate a large class of stochastic models with nonlinear drift and nonlinear diffusion. It can be applied, for example, for ecological and financial state space models. After proving consistency and asymptotic normality of the estimation, leading to asymptotic confidence intervals, we provide a thorough numerical study, which compares many classical methods used in practice (Non Linear Least Square estimator, Monte Carlo Expectation Maxi-mization Likelihood estimator and Bayesian estimators) to estimate stochastic volatility model. We prove that our estimator clearly outperforms the Maximum Likelihood Estimator in term of computing time, but also most of the other methods. We also show that this contrast method is the most stable and also does not need any tuning parameter.

</details>

<details>

<summary>2016-12-19 16:44:27 - Adaptive Bernstein-von Mises theorems in Gaussian white noise</summary>

- *Kolyan Ray*

- `1407.3397v3` - [abs](http://arxiv.org/abs/1407.3397v3) - [pdf](http://arxiv.org/pdf/1407.3397v3)

> We investigate Bernstein-von Mises theorems for adaptive nonparametric Bayesian procedures in the canonical Gaussian white noise model. We consider both a Hilbert space and multiscale setting with applications in $L^2$ and $L^\infty$ respectively. This provides a theoretical justification for plug-in procedures, for example the use of certain credible sets for sufficiently smooth linear functionals. We use this general approach to construct optimal frequentist confidence sets based on the posterior distribution. We also provide simulations to numerically illustrate our approach and obtain a visual representation of the geometries involved.

</details>

<details>

<summary>2016-12-20 01:01:25 - High-Dimensional Bayesian Regularised Regression with the BayesReg Package</summary>

- *Enes Makalic, Daniel F. Schmidt*

- `1611.06649v3` - [abs](http://arxiv.org/abs/1611.06649v3) - [pdf](http://arxiv.org/pdf/1611.06649v3)

> Bayesian penalized regression techniques, such as the Bayesian lasso and the Bayesian horseshoe estimator, have recently received a significant amount of attention in the statistics literature. However, software implementing state-of-the-art Bayesian penalized regression, outside of general purpose Markov chain Monte Carlo platforms such as STAN, is relatively rare. This paper introduces bayesreg, a new toolbox for fitting Bayesian penalized regression models with continuous shrinkage prior densities. The toolbox features Bayesian linear regression with Gaussian or heavy-tailed error models and Bayesian logistic regression with ridge, lasso, horseshoe and horseshoe$+$ estimators. The toolbox is free, open-source and available for use with the MATLAB and R numerical platforms.

</details>

<details>

<summary>2016-12-21 09:52:13 - Bayesian Non-Central Chi Regression For Neuroimaging</summary>

- *Bertil Wegmann, Anders Eklund, Mattias Villani*

- `1612.07034v1` - [abs](http://arxiv.org/abs/1612.07034v1) - [pdf](http://arxiv.org/pdf/1612.07034v1)

> We propose a regression model for non-central $\chi$ (NC-$\chi$) distributed functional magnetic resonance imaging (fMRI) and diffusion weighted imaging (DWI) data, with the heteroscedastic Rician regression model as a prominent special case. The model allows both parameters in the NC-$\chi$ distribution to be linked to explanatory variables, with the relevant covariates automatically chosen by Bayesian variable selection. A highly efficient Markov chain Monte Carlo (MCMC) algorithm is proposed for simulating from the joint Bayesian posterior distribution of all model parameters and the binary covariate selection indicators. Simulated fMRI data is used to demonstrate that the Rician model is able to localize brain activity much more accurately than the traditionally used Gaussian model at low signal-to-noise ratios. Using a diffusion dataset from the Human Connectome Project, it is also shown that the commonly used approximate Gaussian noise model underestimates the mean diffusivity (MD) and the fractional anisotropy (FA) in the single-diffusion tensor model compared to the theoretically correct Rician model.

</details>

<details>

<summary>2016-12-21 12:11:46 - Bayesian Inference for State Space Models using Block and Correlated Pseudo Marginal Methods</summary>

- *P. Choppala, D. Gunawan, J. Chen, M. -N. Tran, R. Kohn*

- `1612.07072v1` - [abs](http://arxiv.org/abs/1612.07072v1) - [pdf](http://arxiv.org/pdf/1612.07072v1)

> This article addresses the problem of efficient Bayesian inference in dynamic systems using particle methods and makes a number of contributions. First, we develop a correlated pseudo-marginal (CPM) approach for Bayesian inference in state space (SS) models that is based on filtering the disturbances, rather than the states. This approach is useful when the state transition density is intractable or inefficient to compute, and also when the dimension of the disturbance is lower than the dimension of the state. Second, we propose a block pseudo-marginal (BPM) method that uses as the estimate of the likelihood the average of G independent unbiased estimates of the likelihood. We associate a set of underlying uniform of standard normal random numbers used to construct each of the individual unbiased likelihood estimates and then use component-wise Markov Chain Monte Carlo to update the parameter vector jointly with one set of these random numbers at a time. This induces a correlation of approximately 1-1/G between the logs of the estimated likelihood at the proposed and current values of the model parameters. Third, we show for some non-stationary state space models that the BPM approach is much more efficient than the CPM approach, because it is difficult to translate the high correlation in the underlying random numbers to high correlation between the logs of the likelihood estimates. Although our focus has been on applying the BPM method to state space models, our results and approach can be used in a wide range of applications of the PM method, such as panel data models, subsampling problems and approximate Bayesian computation.

</details>

<details>

<summary>2016-12-21 16:24:27 - Bayesian Decision Process for Cost-Efficient Dynamic Ranking via Crowdsourcing</summary>

- *Xi Chen, Kevin Jiao, Qihang Lin*

- `1612.07222v1` - [abs](http://arxiv.org/abs/1612.07222v1) - [pdf](http://arxiv.org/pdf/1612.07222v1)

> Rank aggregation based on pairwise comparisons over a set of items has a wide range of applications. Although considerable research has been devoted to the development of rank aggregation algorithms, one basic question is how to efficiently collect a large amount of high-quality pairwise comparisons for the ranking purpose. Because of the advent of many crowdsourcing services, a crowd of workers are often hired to conduct pairwise comparisons with a small monetary reward for each pair they compare. Since different workers have different levels of reliability and different pairs have different levels of ambiguity, it is desirable to wisely allocate the limited budget for comparisons among the pairs of items and workers so that the global ranking can be accurately inferred from the comparison results. To this end, we model the active sampling problem in crowdsourced ranking as a Bayesian Markov decision process, which dynamically selects item pairs and workers to improve the ranking accuracy under a budget constraint. We further develop a computationally efficient sampling policy based on knowledge gradient as well as a moment matching technique for posterior approximation. Experimental evaluations on both synthetic and real data show that the proposed policy achieves high ranking accuracy with a lower labeling cost.

</details>

<details>

<summary>2016-12-22 07:31:59 - Efficient Bayesian computation by proximal Markov chain Monte Carlo: when Langevin meets Moreau</summary>

- *Alain Durmus, Eric Moulines, Marcelo Pereyra*

- `1612.07471v1` - [abs](http://arxiv.org/abs/1612.07471v1) - [pdf](http://arxiv.org/pdf/1612.07471v1)

> Modern imaging methods rely strongly on Bayesian inference techniques to solve challenging imaging problems. Currently, the predominant Bayesian computation approach is convex optimisation, which scales very efficiently to high dimensional image models and delivers accurate point estimation results. However, in order to perform more complex analyses, for example image uncertainty quantification or model selection, it is necessary to use more computationally intensive Bayesian computation techniques such as Markov chain Monte Carlo methods. This paper presents a new and highly efficient Markov chain Monte Carlo methodology to perform Bayesian computation for high dimensional models that are log-concave and non-smooth, a class of models that is central in imaging sciences. The methodology is based on a regularised unadjusted Langevin algorithm that exploits tools from convex analysis, namely Moreau-Yoshida envelopes and proximal operators, to construct Markov chains with favourable convergence properties. In addition to scaling efficiently to high dimensions, the method is straightforward to apply to models that are currently solved by using proximal optimisation algorithms. We provide a detailed theoretical analysis of the proposed methodology, including asymptotic and non-asymptotic convergence results with easily verifiable conditions, and explicit bounds on the convergence rates. The proposed methodology is demonstrated with four experiments related to image deconvolution and tomographic reconstruction with total-variation and $\ell_1$ priors, where we conduct a range of challenging Bayesian analyses related to uncertainty quantification, hypothesis testing, and model selection in the absence of ground truth.

</details>

<details>

<summary>2016-12-22 12:21:39 - Low-Cost Energy Meter Calibration Method for Measurement and Verification</summary>

- *Herman Carstens, Xiaohua Xia, Sarma Yadavalli*

- `1610.04196v2` - [abs](http://arxiv.org/abs/1610.04196v2) - [pdf](http://arxiv.org/pdf/1610.04196v2)

> Energy meters need to be calibrated for use in Measurement and Verification (M&V) projects. However, calibration can be prohibitively expensive and affect project feasibility negatively. This study presents a novel low-cost in-situ meter data calibration technique using a relatively low accuracy commercial energy meter as a calibrator. Calibration is achieved by combining two machine learning tools: the SIMulation EXtrapolation (SIMEX) Measurement Error Model, and Bayesian regression. The model is trained or calibrated on half-hourly building energy data for 24 hours. Measurements are then compared to the true values over the following months to verify the method. Results show that the hybrid method significantly improves parameter estimates and goodness of fit when compared to Ordinary Least Squares regression or standard SIMEX. This study also addresses the effect of mismeasurement in energy monitoring, and implements a powerful technique for mitigating the bias that arises because of it. Meters calibrated by the technique presented have satisfactory accuracy for most M&V applications, at a significantly lower cost.

</details>

<details>

<summary>2016-12-22 17:04:04 - Boosting Joint Models for Longitudinal and Time-to-Event Data</summary>

- *Elisabeth Waldmann, David Taylor-Robinson, Nadja Klein, Thomas Kneib, Tania Pressler, Matthias Schmid, Andreas Mayr*

- `1609.02686v2` - [abs](http://arxiv.org/abs/1609.02686v2) - [pdf](http://arxiv.org/pdf/1609.02686v2)

> Joint Models for longitudinal and time-to-event data have gained a lot of attention in the last few years as they are a helpful technique to approach common a data structure in clinical studies where longitudinal outcomes are recorded alongside event times. Those two processes are often linked and the two outcomes should thus be modeled jointly in order to prevent the potential bias introduced by independent modelling. Commonly, joint models are estimated in likelihood based expectation maximization or Bayesian approaches using frameworks where variable selection is problematic and which do not immediately work for high-dimensional data. In this paper, we propose a boosting algorithm tackling these challenges by being able to simultaneously estimate predictors for joint models and automatically select the most influential variables even in high-dimensional data situations. We analyse the performance of the new algorithm in a simulation study and apply it to the Danish cystic fibrosis registry which collects longitudinal lung function data on patients with cystic fibrosis together with data regarding the onset of pulmonary infections. This is the first approach to combine state-of-the art algorithms from the field of machine-learning with the model class of joint models, providing a fully data-driven mechanism to select variables and predictor effects in a unified framework of boosting joint models.

</details>

<details>

<summary>2016-12-22 17:44:29 - BaTFLED: Bayesian Tensor Factorization Linked to External Data</summary>

- *Nathan H Lazar, Mehmet GÃ¶nen, Kemal SÃ¶nmez*

- `1612.02965v2` - [abs](http://arxiv.org/abs/1612.02965v2) - [pdf](http://arxiv.org/pdf/1612.02965v2)

> The vast majority of current machine learning algorithms are designed to predict single responses or a vector of responses, yet many types of response are more naturally organized as matrices or higher-order tensor objects where characteristics are shared across modes. We present a new machine learning algorithm BaTFLED (Bayesian Tensor Factorization Linked to External Data) that predicts values in a three-dimensional response tensor using input features for each of the dimensions. BaTFLED uses a probabilistic Bayesian framework to learn projection matrices mapping input features for each mode into latent representations that multiply to form the response tensor. By utilizing a Tucker decomposition, the model can capture weights for interactions between latent factors for each mode in a small core tensor. Priors that encourage sparsity in the projection matrices and core tensor allow for feature selection and model regularization. This method is shown to far outperform elastic net and neural net models on 'cold start' tasks from data simulated in a three-mode structure. Additionally, we apply the model to predict dose-response curves in a panel of breast cancer cell lines treated with drug compounds that was used as a Dialogue for Reverse Engineering Assessments and Methods (DREAM) challenge.

</details>

<details>

<summary>2016-12-23 00:24:27 - Known Unknowns: Uncertainty Quality in Bayesian Neural Networks</summary>

- *Ramon Oliveira, Pedro Tabacof, Eduardo Valle*

- `1612.01251v2` - [abs](http://arxiv.org/abs/1612.01251v2) - [pdf](http://arxiv.org/pdf/1612.01251v2)

> We evaluate the uncertainty quality in neural networks using anomaly detection. We extract uncertainty measures (e.g. entropy) from the predictions of candidate models, use those measures as features for an anomaly detector, and gauge how well the detector differentiates known from unknown classes. We assign higher uncertainty quality to candidate models that lead to better detectors. We also propose a novel method for sampling a variational approximation of a Bayesian neural network, called One-Sample Bayesian Approximation (OSBA). We experiment on two datasets, MNIST and CIFAR10. We compare the following candidate neural network models: Maximum Likelihood, Bayesian Dropout, OSBA, and --- for MNIST --- the standard variational approximation. We show that Bayesian Dropout and OSBA provide better uncertainty information than Maximum Likelihood, and are essentially equivalent to the standard variational approximation, but much faster.

</details>

<details>

<summary>2016-12-23 01:55:17 - An Empirical Comparison of Multiple Imputation Methods for Categorical Data</summary>

- *Olanrewaju Akande, Fan Li, Jerome Reiter*

- `1508.05918v2` - [abs](http://arxiv.org/abs/1508.05918v2) - [pdf](http://arxiv.org/pdf/1508.05918v2)

> Multiple imputation is a common approach for dealing with missing values in statistical databases. The imputer fills in missing values with draws from predictive models estimated from the observed data, resulting in multiple, completed versions of the database. Researchers have developed a variety of default routines to implement multiple imputation; however, there has been limited research comparing the performance of these methods, particularly for categorical data. We use simulation studies to compare repeated sampling properties of three default multiple imputation methods for categorical data, including chained equations using generalized linear models, chained equations using classification and regression trees, and a fully Bayesian joint distribution based on Dirichlet Process mixture models. We base the simulations on categorical data from the American Community Survey. In the circumstances of this study, the results suggest that default chained equations approaches based on generalized linear models are dominated by the default regression tree and Bayesian mixture model approaches. They also suggest competing advantages for the regression tree and Bayesian mixture model approaches, making both reasonable default engines for multiple imputation of categorical data. A supplementary material for this article is available online.

</details>

<details>

<summary>2016-12-23 11:19:55 - Bayesian Nonparametric Survival Analysis using mixture of Burr XII distributions</summary>

- *S. B. Hajjar, S. Khazaei*

- `1612.07938v1` - [abs](http://arxiv.org/abs/1612.07938v1) - [pdf](http://arxiv.org/pdf/1612.07938v1)

> Recently, the Bayesian nonparametric approach in survival studies attracts much more attentions. Because of multi modality in survival data, the mixture models are very common in this field. One of the famous priors on Bayesian nonparametric models is Dirichlet process prior. In this paper we introduce a Bayesian nonparametric mixture model with Burr distribution(Burr type XII) as the kernel of mixture model. Since the Burr distribution shares good properties of common distributions on survival analysis, it has more flexibility than other distributions. By applying this model to simulated and real failure time data sets, we show the preference of this model and compare it with other Dirichlet process mixture models with different kernels. And also we show that this model can be applied for the right censored data. For calculating the posterior of the parameters for inference and modeling, we used the MCMC simulation methods, especially Gibbs sampling.

</details>

<details>

<summary>2016-12-23 12:28:36 - Bayesian Differential Privacy through Posterior Sampling</summary>

- *Christos Dimitrakakis, Blaine Nelson, and Zuhe Zhang, Aikaterini Mitrokotsa, Benjamin Rubinstein*

- `1306.1066v5` - [abs](http://arxiv.org/abs/1306.1066v5) - [pdf](http://arxiv.org/pdf/1306.1066v5)

> Differential privacy formalises privacy-preserving mechanisms that provide access to a database. We pose the question of whether Bayesian inference itself can be used directly to provide private access to data, with no modification. The answer is affirmative: under certain conditions on the prior, sampling from the posterior distribution can be used to achieve a desired level of privacy and utility. To do so, we generalise differential privacy to arbitrary dataset metrics, outcome spaces and distribution families. This allows us to also deal with non-i.i.d or non-tabular datasets. We prove bounds on the sensitivity of the posterior to the data, which gives a measure of robustness. We also show how to use posterior sampling to provide differentially private responses to queries, within a decision-theoretic framework. Finally, we provide bounds on the utility and on the distinguishability of datasets. The latter are complemented by a novel use of Le Cam's method to obtain lower bounds. All our general results hold for arbitrary database metrics, including those for the common definition of differential privacy. For specific choices of the metric, we give a number of examples satisfying our assumptions.

</details>

<details>

<summary>2016-12-24 02:24:41 - Simple, Scalable and Accurate Posterior Interval Estimation</summary>

- *Cheng Li, Sanvesh Srivastava, David B. Dunson*

- `1605.04029v2` - [abs](http://arxiv.org/abs/1605.04029v2) - [pdf](http://arxiv.org/pdf/1605.04029v2)

> There is a lack of simple and scalable algorithms for uncertainty quantification. Bayesian methods quantify uncertainty through posterior and predictive distributions, but it is difficult to rapidly estimate summaries of these distributions, such as quantiles and intervals. Variational Bayes approximations are widely used, but may badly underestimate posterior covariance. Typically, the focus of Bayesian inference is on point and interval estimates for one-dimensional functionals of interest. In small scale problems, Markov chain Monte Carlo algorithms remain the gold standard, but such algorithms face major problems in scaling up to big data. Various modifications have been proposed based on parallelization and approximations based on subsamples, but such approaches are either highly complex or lack theoretical support and/or good performance outside of narrow settings. We propose a very simple and general posterior interval estimation algorithm, which is based on running Markov chain Monte Carlo in parallel for subsets of the data and averaging quantiles estimated from each subset. We provide strong theoretical guarantees and illustrate performance in several applications.

</details>

<details>

<summary>2016-12-24 23:35:30 - Geodesic Lagrangian Monte Carlo over the space of positive definite matrices: with application to Bayesian spectral density estimation</summary>

- *Andrew Holbrook, Shiwei Lan, Alexander Vandenberg-Rodes, Babak Shahbaba*

- `1612.08224v1` - [abs](http://arxiv.org/abs/1612.08224v1) - [pdf](http://arxiv.org/pdf/1612.08224v1)

> We extend the application of Hamiltonian Monte Carlo to allow for sampling from probability distributions defined over symmetric or Hermitian positive definite matrices. To do so, we exploit the Riemannian structure induced by Cartan's century-old canonical metric. The geodesics that correspond to this metric are available in closed-form and---within the context of Lagrangian Monte Carlo---provide a principled way to travel around the space of positive definite matrices. Our method improves Bayesian inference on such matrices by allowing for a broad range of priors, so we are not limited to conjugate priors only. In the context of spectral density estimation, we use the (non-conjugate) complex reference prior as an example modeling option made available by the algorithm. Results based on simulated and real-world multivariate time series are presented in this context, and future directions are outlined.

</details>

<details>

<summary>2016-12-27 03:35:49 - Bayesian Semi-parametric Realized-CARE Models for Tail Risk Forecasting Incorporating Realized Measures</summary>

- *Richard Gerlach, Chao Wang*

- `1612.08488v1` - [abs](http://arxiv.org/abs/1612.08488v1) - [pdf](http://arxiv.org/pdf/1612.08488v1)

> A new model framework called Realized Conditional Autoregressive Expectile (Realized-CARE) is proposed, through incorporating a measurement equation into the conventional CARE model, in a manner analogous to the Realized-GARCH model. Competing realized measures (e.g. Realized Variance and Realized Range) are employed as the dependent variable in the measurement equation and to drive expectile dynamics. The measurement equation here models the contemporaneous dependence between the realized measure and the latent conditional expectile. We also propose employing the quantile loss function as the target criterion, instead of the conventional violation rate, during the expectile level grid search. For the proposed model, the usual search procedure and asymmetric least squares (ALS) optimization to estimate the expectile level and CARE parameters proves challenging and often fails to convergence. We incorporate a fast random walk Metropolis stochastic search method, combined with a more targeted grid search procedure, to allow reasonably fast and improved accuracy in estimation of this level and the associated model parameters. Given the convergence issue, Bayesian adaptive Markov Chain Monte Carlo methods are proposed for estimation, whilst their properties are assessed and compared with ALS via a simulation study. In a real forecasting study applied to 7 market indices and 2 individual asset returns, compared to the original CARE, the parametric GARCH and Realized-GARCH models, one-day-ahead Value-at-Risk and Expected Shortfall forecasting results favor the proposed Realized-CARE model, especially when incorporating the Realized Range and the sub-sampled Realized Range as the realized measure in the model.

</details>

<details>

<summary>2016-12-28 15:28:30 - Rising Above Chaotic Likelihoods</summary>

- *Hailiang Du, Leonard A. Smith*

- `1410.2568v2` - [abs](http://arxiv.org/abs/1410.2568v2) - [pdf](http://arxiv.org/pdf/1410.2568v2)

> Berliner (Likelihood and Bayesian prediction for chaotic systems, J. Am. Stat. Assoc. 1991) identified a number of difficulties in using the likelihood function within the Bayesian paradigm which arise both for state estimation and for parameter estimation of chaotic systems. Even when the equations of the system are given, he demonstrated "chaotic likelihood functions" both of initial conditions and of parameter values in the Logistic Map. Chaotic likelihood functions, while ultimately smooth, have such complicated small scale structure as to cast doubt on the possibility of identifying high likelihood states in practice. In this paper, the challenge of chaotic likelihoods is overcome by embedding the observations in a higher dimensional sequence-space; this allows good state estimation with finite computational power. An importance sampling approach is introduced, where Pseudo-orbit Data Assimilation is employed in the sequence-space, first to identify relevant pseudo-orbits and then relevant trajectories. Estimates are identified with likelihoods orders of magnitude higher than those previously identified in the examples given by Berliner. Pseudo-orbit Data Assimilation importance sampler exploits the information both from the model dynamics and from the observations. While sampling from the relevant prior (here, the natural measure) will, of course, eventually yield an accountable sample, given the realistic computational resource this traditional approach would provide no high likelihood points at all. While one of the challenges Berliner posed is overcome, his central conclusion is supported. "Chaotic likelihood functions" for parameter estimation still pose a challenge; this fact helps clarify why physical scientists maintain a strong distinction between the initial condition uncertainty and parameter uncertainty.

</details>

<details>

<summary>2016-12-28 16:12:28 - Bayesian Optimization with Shape Constraints</summary>

- *Michael Jauch, VÃ­ctor PeÃ±a*

- `1612.08915v1` - [abs](http://arxiv.org/abs/1612.08915v1) - [pdf](http://arxiv.org/pdf/1612.08915v1)

> In typical applications of Bayesian optimization, minimal assumptions are made about the objective function being optimized. This is true even when researchers have prior information about the shape of the function with respect to one or more argument. We make the case that shape constraints are often appropriate in at least two important application areas of Bayesian optimization: (1) hyperparameter tuning of machine learning algorithms and (2) decision analysis with utility functions. We describe a methodology for incorporating a variety of shape constraints within the usual Bayesian optimization framework and present positive results from simple applications which suggest that Bayesian optimization with shape constraints is a promising topic for further research.

</details>

<details>

<summary>2016-12-29 03:44:51 - A Divide and Conquer Strategy for High Dimensional Bayesian Factor Models</summary>

- *Gautam Sabnis, Debdeep Pati, Barbara Engelhardt, Natesh Pillai*

- `1612.02875v2` - [abs](http://arxiv.org/abs/1612.02875v2) - [pdf](http://arxiv.org/pdf/1612.02875v2)

> We propose a distributed computing framework, based on a divide and conquer strategy and hierarchical modeling, to accelerate posterior inference for high-dimensional Bayesian factor models. Our approach distributes the task of high-dimensional covariance matrix estimation to multiple cores, solves each subproblem separately via a latent factor model, and then combines these estimates to produce a global estimate of the covariance matrix. Existing divide and conquer methods focus exclusively on dividing the total number of observations $n$ into subsamples while keeping the dimension $p$ fixed. Our approach is novel in this regard: it includes all of the $n$ samples in each subproblem and, instead, splits the dimension $p$ into smaller subsets for each subproblem. The subproblems themselves can be challenging to solve when $p$ is large due to the dependencies across dimensions. To circumvent this issue, we specify a novel hierarchical structure on the latent factors that allows for flexible dependencies across dimensions, while still maintaining computational efficiency. Our approach is readily parallelizable and is shown to have computational efficiency of several orders of magnitude in comparison to fitting a full factor model. We report the performance of our method in synthetic examples and a genomics application.

</details>

<details>

<summary>2016-12-29 14:58:55 - High-dimensional Filtering using Nested Sequential Monte Carlo</summary>

- *Christian A. Naesseth, Fredrik Lindsten, Thomas B. SchÃ¶n*

- `1612.09162v1` - [abs](http://arxiv.org/abs/1612.09162v1) - [pdf](http://arxiv.org/pdf/1612.09162v1)

> Sequential Monte Carlo (SMC) methods comprise one of the most successful approaches to approximate Bayesian filtering. However, SMC without good proposal distributions struggle in high dimensions. We propose nested sequential Monte Carlo (NSMC), a methodology that generalises the SMC framework by requiring only approximate, properly weighted, samples from the SMC proposal distribution, while still resulting in a correct SMC algorithm. This way we can exactly approximate the locally optimal proposal, and extend the class of models for which we can perform efficient inference using SMC. We show improved accuracy over other state-of-the-art methods on several spatio-temporal state space models.

</details>

<details>

<summary>2016-12-29 21:44:49 - Bayesian inverse problems with $l_1$ priors: a Randomize-then-Optimize approach</summary>

- *Zheng Wang, Johnathan M. Bardsley, Antti Solonen, Tiangang Cui, Youssef M. Marzouk*

- `1607.01904v2` - [abs](http://arxiv.org/abs/1607.01904v2) - [pdf](http://arxiv.org/pdf/1607.01904v2)

> Prior distributions for Bayesian inference that rely on the $l_1$-norm of the parameters are of considerable interest, in part because they promote parameter fields with less regularity than Gaussian priors (e.g., discontinuities and blockiness). These $l_1$-type priors include the total variation (TV) prior and the Besov $B^s_{1,1}$ space prior, and in general yield non-Gaussian posterior distributions. Sampling from these posteriors is challenging, particularly in the inverse problem setting where the parameter space is high-dimensional and the forward problem may be nonlinear. This paper extends the randomize-then-optimize (RTO) method, an optimization-based sampling algorithm developed for Bayesian inverse problems with Gaussian priors, to inverse problems with $l_1$-type priors. We use a variable transformation to convert an $l_1$-type prior to a standard Gaussian prior, such that the posterior distribution of the transformed parameters is amenable to Metropolized sampling via RTO. We demonstrate this approach on several deconvolution problems and an elliptic PDE inverse problem, using TV or Besov $B^s_{1,1}$ space priors. Our results show that the transformed RTO algorithm characterizes the correct posterior distribution and can be more efficient than other sampling algorithms. The variable transformation can also be extended to other non-Gaussian priors.

</details>

<details>

<summary>2016-12-30 19:00:12 - Bayesian Transformed GARMA Models</summary>

- *Breno S. Andrade, Marinho G. Andrade, Ricardo S. Ehlers*

- `1612.09561v1` - [abs](http://arxiv.org/abs/1612.09561v1) - [pdf](http://arxiv.org/pdf/1612.09561v1)

> Transformed Generalized Autoregressive Moving Average (TGARMA) models were recently proposed to deal with non-additivity, non-normality and heteroscedasticity in real time series data. In this paper, a Bayesian approach is proposed for TGARMA models, thus extending the original model. We conducted a simulation study to investigate the performance of Bayesian estimation and Bayesian model selection criteria. In addition, a real dataset was analysed using the proposed approach.

</details>

<details>

<summary>2016-12-30 19:04:50 - Bayesian Learning of Dynamic Multilayer Networks</summary>

- *Daniele Durante, Nabanita Mukherjee, Rebecca C. Steorts*

- `1608.02209v2` - [abs](http://arxiv.org/abs/1608.02209v2) - [pdf](http://arxiv.org/pdf/1608.02209v2)

> A plethora of networks is being collected in a growing number of fields, including disease transmission, international relations, social interactions, and others. As data streams continue to grow, the complexity associated with these highly multidimensional connectivity data presents novel challenges. In this paper, we focus on the time-varying interconnections among a set of actors in multiple contexts, called layers. Current literature lacks flexible statistical models for dynamic multilayer networks, which can enhance quality in inference and prediction by efficiently borrowing information within each network, across time, and between layers. Motivated by this gap, we develop a Bayesian nonparametric model leveraging latent space representations. Our formulation characterizes the edge probabilities as a function of shared and layer-specific actors positions in a latent space, with these positions changing in time via Gaussian processes. This representation facilitates dimensionality reduction and incorporates different sources of information in the observed data. In addition, we obtain tractable procedures for posterior computation, inference, and prediction. We provide theoretical results on the flexibility of our model. Our methods are tested on simulations and infection studies monitoring dynamic face-to-face contacts among individuals in multiple days, where we perform better than current methods in inference and prediction.

</details>

<details>

<summary>2016-12-30 20:56:41 - Counterfactual Prediction with Deep Instrumental Variables Networks</summary>

- *Jason Hartford, Greg Lewis, Kevin Leyton-Brown, Matt Taddy*

- `1612.09596v1` - [abs](http://arxiv.org/abs/1612.09596v1) - [pdf](http://arxiv.org/pdf/1612.09596v1)

> We are in the middle of a remarkable rise in the use and capability of artificial intelligence. Much of this growth has been fueled by the success of deep learning architectures: models that map from observables to outputs via multiple layers of latent representations. These deep learning algorithms are effective tools for unstructured prediction, and they can be combined in AI systems to solve complex automated reasoning problems. This paper provides a recipe for combining ML algorithms to solve for causal effects in the presence of instrumental variables -- sources of treatment randomization that are conditionally independent from the response. We show that a flexible IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework imposes some specific structure on the stochastic gradient descent routine used for training, but it is general enough that we can take advantage of off-the-shelf ML capabilities and avoid extensive algorithm customization. We outline how to obtain out-of-sample causal validation in order to avoid over-fit. We also introduce schemes for both Bayesian and frequentist inference: the former via a novel adaptation of dropout training, and the latter via a data splitting routine.

</details>

