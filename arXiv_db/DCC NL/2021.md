# 2021

## TOC

- [2021-02](#2021-02)
- [2021-04](#2021-04)
- [2021-07](#2021-07)
- [2021-10](#2021-10)
- [2021-12](#2021-12)

## 2021-02

<details>

<summary>2021-02-09 07:40:50 - Associated Learning: Decomposing End-to-end Backpropagation based on Auto-encoders and Target Propagation</summary>

- *Yu-Wei Kao, Hung-Hsuan Chen*

- `1906.05560v4` - [abs](http://arxiv.org/abs/1906.05560v4) - [pdf](http://arxiv.org/pdf/1906.05560v4)

> Backpropagation (BP) is the cornerstone of today's deep learning algorithms, but it is inefficient partially because of backward locking, which means updating the weights of one layer locks the weight updates in the other layers. Consequently, it is challenging to apply parallel computing or a pipeline structure to update the weights in different layers simultaneously. In this paper, we introduce a novel learning structure called associated learning (AL), which modularizes the network into smaller components, each of which has a local objective. Because the objectives are mutually independent, AL can learn the parameters in different layers independently and simultaneously, so it is feasible to apply a pipeline structure to improve the training throughput. Specifically, this pipeline structure improves the complexity of the training time from O(nl), which is the time complexity when using BP and stochastic gradient descent (SGD) for training, to O(n + l), where n is the number of training instances and l is the number of hidden layers. Surprisingly, even though most of the parameters in AL do not directly interact with the target variable, training deep models by this method yields accuracies comparable to those from models trained using typical BP methods, in which all parameters are used to predict the target variable. Consequently, because of the scalability and the predictive power demonstrated in the experiments, AL deserves further study to determine the better hyperparameter settings, such as activation function selection, learning rate scheduling, and weight initialization, to accumulate experience, as we have done over the years with the typical BP method. Additionally, perhaps our design can also inspire new network designs for deep learning. Our implementation is available at https://github.com/SamYWK/Associated_Learning.

</details>


## 2021-04

<details>

<summary>2021-04-14 01:32:25 - Joint Negative and Positive Learning for Noisy Labels</summary>

- *Youngdong Kim, Juseung Yun, Hyounguk Shon, Junmo Kim*

- `2104.06574v1` - [abs](http://arxiv.org/abs/2104.06574v1) - [pdf](http://arxiv.org/pdf/2104.06574v1)

> Training of Convolutional Neural Networks (CNNs) with data with noisy labels is known to be a challenge. Based on the fact that directly providing the label to the data (Positive Learning; PL) has a risk of allowing CNNs to memorize the contaminated labels for the case of noisy data, the indirect learning approach that uses complementary labels (Negative Learning for Noisy Labels; NLNL) has proven to be highly effective in preventing overfitting to noisy data as it reduces the risk of providing faulty target. NLNL further employs a three-stage pipeline to improve convergence. As a result, filtering noisy data through the NLNL pipeline is cumbersome, increasing the training cost. In this study, we propose a novel improvement of NLNL, named Joint Negative and Positive Learning (JNPL), that unifies the filtering pipeline into a single stage. JNPL trains CNN via two losses, NL+ and PL+, which are improved upon NL and PL loss functions, respectively. We analyze the fundamental issue of NL loss function and develop new NL+ loss function producing gradient that enhances the convergence of noisy data. Furthermore, PL+ loss function is designed to enable faster convergence to expected-to-be-clean data. We show that the NL+ and PL+ train CNN simultaneously, significantly simplifying the pipeline, allowing greater ease of practical use compared to NLNL. With a simple semi-supervised training technique, our method achieves state-of-the-art accuracy for noisy data classification based on the superior filtering ability.

</details>

<details>

<summary>2021-04-17 19:17:11 - Mixed Gibbs Sampling Detector in High-Order Modulation Large-Scale MIMO Systems</summary>

- *Alex Mussi, Taufik AbrÃ£o*

- `2104.08626v1` - [abs](http://arxiv.org/abs/2104.08626v1) - [pdf](http://arxiv.org/pdf/2104.08626v1)

> A neighborhood restricted Mixed Gibbs Sampling (MGS) based approach is proposed for low-complexity high-order modulation large-scale Multiple-Input Multiple-Output (LS-MIMO) detection. The proposed LS-MIMO detector applies a neighborhood limitation (NL) on the noisy solution from the MGS at a distance d - thus, named d-simplified MGS (d-sMGS) - in order to mitigate its impact, which can be harmful when a high order modulation is considered. Numerical simulation results considering 64-QAM demonstrated that the proposed detection method can substantially improve the MGS algorithm convergence, whereas no extra computational complexity per iteration is required. The proposed d-sMGS-based detector suitable for high-order modulation LS-MIMO further exhibits improved performance vs. complexity tradeoff when the system loading is high, i.e., when K >= 0.75. N. Also, with increasing the number of dimensions, i.e., increasing the number of antennas and/or modulation order, a smaller restriction of 2-sMGS was shown to be a more interesting choice than 1-sMGS.

</details>


## 2021-07

<details>

<summary>2021-07-11 23:37:00 - Discovery of Bayes' Table at Tunbridge Wells</summary>

- *David C. Schneider, Roy Thompson*

- `2107.05145v1` - [abs](http://arxiv.org/abs/2107.05145v1) - [pdf](http://arxiv.org/pdf/2107.05145v1)

> In 1755 Thomas Bayes expressed an interest in the problem of combining repeated measurements of the location of a star. Bayes described a tandem set-up of a ball thrown on a table, followed by repeated throws of a second ball. Bayes' table has long been taken as a billiard table, for which there is no evidence. We report the discovery of Bayes' table, a bowling green located half a km uphill (SE) from the meeting house where Bayes served as minister for two decades. Bayes' drawing shows a rectangular space marked off in yards, which allows calculation of an interval measurement of uncertainty. The Bayes rule interval from 2.5% to 97.5% is from 0.56 - 0.42 = 0.12 perches equivalent to 0.61 m. The discovery of Bayes' table establishes the physical basis for Bayes' symmetrical probability model, a fixed parameter binomial ({\theta} = 0.5). The discovery establishes Bayes as the founder of statistical science, defined as the application of mathematics to scientific measurement.

</details>


## 2021-10

<details>

<summary>2021-10-07 16:18:16 - Ship Performance Monitoring using Machine-learning</summary>

- *Prateek Gupta, Adil Rasheed, Sverre Steen*

- `2110.03594v1` - [abs](http://arxiv.org/abs/2110.03594v1) - [pdf](http://arxiv.org/pdf/2110.03594v1)

> The hydrodynamic performance of a sea-going ship varies over its lifespan due to factors like marine fouling and the condition of the anti-fouling paint system. In order to accurately estimate the power demand and fuel consumption for a planned voyage, it is important to assess the hydrodynamic performance of the ship. The current work uses machine-learning (ML) methods to estimate the hydrodynamic performance of a ship using the onboard recorded in-service data. Three ML methods, NL-PCR, NL-PLSR and probabilistic ANN, are calibrated using the data from two sister ships. The calibrated models are used to extract the varying trend in ship's hydrodynamic performance over time and predict the change in performance through several propeller and hull cleaning events. The predicted change in performance is compared with the corresponding values estimated using the fouling friction coefficient ($\Delta C_F$). The ML methods are found to be performing well while modelling the hydrodynamic state variables of the ships with probabilistic ANN model performing the best, but the results from NL-PCR and NL-PLSR are not far behind, indicating that it may be possible to use simple methods to solve such problems with the help of domain knowledge.

</details>


## 2021-12

<details>

<summary>2021-12-22 19:28:41 - Tractable Profit Maximization over Multiple Attributes under Discrete Choice Models</summary>

- *Hongzhang Shao, Anton J. Kleywegt*

- `2007.09193v3` - [abs](http://arxiv.org/abs/2007.09193v3) - [pdf](http://arxiv.org/pdf/2007.09193v3)

> A fundamental problem in revenue management is to optimally choose the attributes of products, such that the total profit or revenue or market share is maximized. Usually, these attributes can affect both a product's market share (probability to be chosen) and its profit margin. For example, if a smart phone has a better battery, then it is more costly to be produced, but is more likely to be purchased by a customer. The decision maker then needs to choose an optimal vector of attributes for each product that balances this trade-off. In spite of the importance of such problems, there is not yet a method to solve it efficiently in general. Past literature in revenue management and discrete choice models focus on pricing problems, where price is the only attribute to be chosen for each product. Existing approaches to solve pricing problems tractably cannot be generalized to the optimization problem with multiple product attributes as decision variables. On the other hand, papers studying product line design with multiple attributes all result in intractable optimization problems. Then we found a way to reformulate the static multi-attribute optimization problem, as well as the multi-stage fluid optimization problem with both resource constraints and upper and lower bounds of attributes, as a tractable convex conic optimization problem. Our result applies to optimization problems under the multinomial logit (MNL) model, the Markov chain (MC) choice model, and with certain conditions, the nested logit (NL) model.

</details>

