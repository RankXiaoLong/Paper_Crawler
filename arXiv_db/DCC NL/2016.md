# 2016

## TOC

- [2016-03](#2016-03)
- [2016-07](#2016-07)
- [2016-09](#2016-09)
- [2016-11](#2016-11)
- [2016-12](#2016-12)

## 2016-03

<details>

<summary>2016-03-29 09:21:49 - On the stationarity of Dynamic Conditional Correlation models</summary>

- *Jean-David Fermanian, Hassan Malongo*

- `1405.6905v2` - [abs](http://arxiv.org/abs/1405.6905v2) - [pdf](http://arxiv.org/pdf/1405.6905v2)

> We provide conditions for the existence and the unicity of strictly stationary solutions of the usual Dynamic Conditional Correlation GARCH models (DCC-GARCH). The proof is based on Tweedie's (1988) criteria, after having rewritten DCC-GARCH models as nonlinear Markov chains. Moreover, we study the existence of their finite moments.

</details>


## 2016-07

<details>

<summary>2016-07-18 21:01:56 - Hyperspectral Unmixing in Presence of Endmember Variability, Nonlinearity or Mismodelling Effects</summary>

- *Abderrahim Halimi, Paul Honeine, Jose Bioucas-Dias*

- `1511.05698v2` - [abs](http://arxiv.org/abs/1511.05698v2) - [pdf](http://arxiv.org/pdf/1511.05698v2)

> This paper presents three hyperspectral mixture models jointly with Bayesian algorithms for supervised hyperspectral unmixing. Based on the residual component analysis model, the proposed general formulation assumes the linear model to be corrupted by an additive term whose expression can be adapted to account for nonlinearities (NL), endmember variability (EV), or mismodelling effects (ME). The NL effect is introduced by considering a polynomial expression that is related to bilinear models. The proposed new formulation of EV accounts for shape and scale endmember changes while enforcing a smooth spectral/spatial variation. The ME formulation takes into account the effect of outliers and copes with some types of EV and NL. The known constraints on the parameter of each observation model are modeled via suitable priors. The posterior distribution associated with each Bayesian model is optimized using a coordinate descent algorithm which allows the computation of the maximum a posteriori estimator of the unknown model parameters. The proposed mixture and Bayesian models and their estimation algorithms are validated on both synthetic and real images showing competitive results regarding the quality of the inferences and the computational complexity when compared to the state-of-the-art algorithms.

</details>

<details>

<summary>2016-07-18 21:39:00 - Fast Hyperspectral Unmixing in Presence of Nonlinearity or Mismodelling Effects</summary>

- *Abderrahim Halimi, Jose Bioucas-Dias, Nicolas Dobigeon, Gerald S. Buller, Steve McLaughlin*

- `1607.05336v1` - [abs](http://arxiv.org/abs/1607.05336v1) - [pdf](http://arxiv.org/pdf/1607.05336v1)

> This paper presents two novel hyperspectral mixture models and associated unmixing algorithms. The two models assume a linear mixing model corrupted by an additive term whose expression can be adapted to account for multiple scattering nonlinearities (NL), or mismodelling effects (ME). The NL model generalizes bilinear models by taking into account higher order interaction terms. The ME model accounts for different effects such as endmember variability or the presence of outliers. The abundance and residual parameters of these models are estimated by considering a convex formulation suitable for fast estimation algorithms. This formulation accounts for constraints such as the sum-to-one and non-negativity of the abundances, the non-negativity of the nonlinearity coefficients, the spectral smoothness of the ME terms and the spatial sparseness of the residuals. The resulting convex problem is solved using the alternating direction method of multipliers (ADMM) whose convergence is ensured theoretically. The proposed mixture models and their unmixing algorithms are validated on both synthetic and real images showing competitive results regarding the quality of the inference and the computational complexity when compared to the state-of-the-art algorithms.

</details>


## 2016-09

<details>

<summary>2016-09-28 23:31:45 - MPI-FAUN: An MPI-Based Framework for Alternating-Updating Nonnegative Matrix Factorization</summary>

- *Ramakrishnan Kannan, Grey Ballard, Haesun Park*

- `1609.09154v1` - [abs](http://arxiv.org/abs/1609.09154v1) - [pdf](http://arxiv.org/pdf/1609.09154v1)

> Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors $W$ and $H$, for the given input matrix $A$, such that $A \approx W H$. NMF is a useful tool for many applications in different domains such as topic modeling in text mining, background separation in video analysis, and community detection in social networks. Despite its popularity in the data mining community, there is a lack of efficient parallel algorithms to solve the problem for big data sets.   The main contribution of this work is a new, high-performance parallel computational framework for a broad class of NMF algorithms that iteratively solves alternating non-negative least squares (NLS) subproblems for $W$ and $H$. It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). The framework is flexible and able to leverage a variety of NMF and NLS algorithms, including Multiplicative Update, Hierarchical Alternating Least Squares, and Block Principal Pivoting. Our implementation allows us to benchmark and compare different algorithms on massive dense and sparse data matrices of size that spans for few hundreds of millions to billions. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements. The code and the datasets used for conducting the experiments are available online.

</details>


## 2016-11

<details>

<summary>2016-11-02 20:49:38 - Cross-validation based Nonlinear Shrinkage</summary>

- *Daniel Bartz*

- `1611.00798v1` - [abs](http://arxiv.org/abs/1611.00798v1) - [pdf](http://arxiv.org/pdf/1611.00798v1)

> Many machine learning algorithms require precise estimates of covariance matrices. The sample covariance matrix performs poorly in high-dimensional settings, which has stimulated the development of alternative methods, the majority based on factor models and shrinkage. Recent work of Ledoit and Wolf has extended the shrinkage framework to Nonlinear Shrinkage (NLS), a more powerful covariance estimator based on Random Matrix Theory.   Our contribution shows that, contrary to claims in the literature, cross-validation based covariance matrix estimation (CVC) yields comparable performance at strongly reduced complexity and runtime. On two real world data sets, we show that the CVC estimator yields superior results than competing shrinkage and factor based methods.

</details>


## 2016-12

<details>

<summary>2016-12-22 06:17:01 - How to Train Your Deep Neural Network with Dictionary Learning</summary>

- *Vanika Singhal, Shikha Singh, Angshul Majumdar*

- `1612.07454v1` - [abs](http://arxiv.org/abs/1612.07454v1) - [pdf](http://arxiv.org/pdf/1612.07454v1)

> Currently there are two predominant ways to train deep neural networks. The first one uses restricted Boltzmann machine (RBM) and the second one autoencoders. RBMs are stacked in layers to form deep belief network (DBN); the final representation layer is attached to the target to complete the deep neural network. Autoencoders are nested one inside the other to form stacked autoencoders; once the stcaked autoencoder is learnt the decoder portion is detached and the target attached to the deepest layer of the encoder to form the deep neural network. This work proposes a new approach to train deep neural networks using dictionary learning as the basic building block; the idea is to use the features from the shallower layer as inputs for training the next deeper layer. One can use any type of dictionary learning (unsupervised, supervised, discriminative etc.) as basic units till the pre-final layer. In the final layer one needs to use the label consistent dictionary learning formulation for classification. We compare our proposed framework with existing state-of-the-art deep learning techniques on benchmark problems; we are always within the top 10 results. In actual problems of age and gender classification, we are better than the best known techniques.

</details>

