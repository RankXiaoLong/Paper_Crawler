# 2019

## TOC

- [2019-02](#2019-02)
- [2019-03](#2019-03)
- [2019-04](#2019-04)
- [2019-05](#2019-05)
- [2019-08](#2019-08)
- [2019-09](#2019-09)
- [2019-10](#2019-10)
- [2019-11](#2019-11)

## 2019-02

<details>

<summary>2019-02-18 07:17:09 - Ain't Nobody Got Time For Coding: Structure-Aware Program Synthesis From Natural Language</summary>

- *Jakub Bednarek, Karol Piaskowski, Krzysztof Krawiec*

- `1810.09717v2` - [abs](http://arxiv.org/abs/1810.09717v2) - [pdf](http://arxiv.org/pdf/1810.09717v2)

> Program synthesis from natural language (NL) is practical for humans and, once technically feasible, would significantly facilitate software development and revolutionize end-user programming. We present SAPS, an end-to-end neural network capable of mapping relatively complex, multi-sentence NL specifications to snippets of executable code. The proposed architecture relies exclusively on neural components, and is trained on abstract syntax trees, combined with a pretrained word embedding and a bi-directional multi-layer LSTM for processing of word sequences. The decoder features a doubly-recurrent LSTM, for which we propose novel signal propagation schemes and soft attention mechanism. When applied to a large dataset of problems proposed in a previous study, SAPS performs on par with or better than the method proposed there, producing correct programs in over 92% of cases. In contrast to other methods, it does not require post-processing of the resulting programs, and uses a fixed-dimensional latent representation as the only interface between the NL analyzer and the source code generator.

</details>

<details>

<summary>2019-02-26 13:52:41 - A Fully-Automatic Framework for Parkinson's Disease Diagnosis by Multi-Modality Images</summary>

- *Jiahang Xu, Fangyang Jiao, Yechong Huang, Xinzhe Luo, Qian Xu, Ling Li, Xueling Liu, Chuantao Zuo, Ping Wu, Xiahai Zhuang*

- `1902.09934v1` - [abs](http://arxiv.org/abs/1902.09934v1) - [pdf](http://arxiv.org/pdf/1902.09934v1)

> Background: Parkinson's disease (PD) is a prevalent long-term neurodegenerative disease. Though the diagnostic criteria of PD are relatively well defined, the current medical imaging diagnostic procedures are expertise-demanding, and thus call for a higher-integrated AI-based diagnostic algorithm. Methods: In this paper, we proposed an automatic, end-to-end, multi-modality diagnosis framework, including segmentation, registration, feature generation and machine learning, to process the information of the striatum for the diagnosis of PD. Multiple modalities, including T1- weighted MRI and 11C-CFT PET, were used in the proposed framework. The reliability of this framework was then validated on a dataset from the PET center of Huashan Hospital, as the dataset contains paired T1-MRI and CFT-PET images of 18 Normal (NL) subjects and 49 PD subjects. Results: We obtained an accuracy of 100% for the PD/NL classification task, besides, we conducted several comparative experiments to validate the diagnosis ability of our framework. Conclusion: Through experiment we illustrate that (1) automatic segmentation has the same classification effect as the manual segmentation, (2) the multi-modality images generates a better prediction than single modality images, and (3) volume feature is shown to be irrelevant to PD diagnosis.

</details>


## 2019-03

<details>

<summary>2019-03-30 13:57:12 - Soil Texture Classification with 1D Convolutional Neural Networks based on Hyperspectral Data</summary>

- *Felix M. Riese, Sina Keller*

- `1901.04846v3` - [abs](http://arxiv.org/abs/1901.04846v3) - [pdf](http://arxiv.org/pdf/1901.04846v3)

> Soil texture is important for many environmental processes. In this paper, we study the classification of soil texture based on hyperspectral data. We develop and implement three 1-dimensional (1D) convolutional neural networks (CNN): the LucasCNN, the LucasResNet which contains an identity block as residual network, and the LucasCoordConv with an additional coordinates layer. Furthermore, we modify two existing 1D CNN approaches for the presented classification task. The code of all five CNN approaches is available on GitHub (Riese, 2019). We evaluate the performance of the CNN approaches and compare them to a random forest classifier. Thereby, we rely on the freely available LUCAS topsoil dataset. The CNN approach with the least depth turns out to be the best performing classifier. The LucasCoordConv achieves the best performance regarding the average accuracy. In future work, we can further enhance the introduced LucasCNN, LucasResNet and LucasCoordConv and include additional variables of the rich LUCAS dataset.

</details>


## 2019-04

<details>

<summary>2019-04-07 00:35:54 - Fast Supervised Discrete Hashing</summary>

- *Jie Gui, Tongliang Liu, Zhenan Sun, Dacheng Tao, Tieniu Tan*

- `1904.03556v1` - [abs](http://arxiv.org/abs/1904.03556v1) - [pdf](http://arxiv.org/pdf/1904.03556v1)

> Learning-based hashing algorithms are ``hot topics" because they can greatly increase the scale at which existing methods operate. In this paper, we propose a new learning-based hashing method called ``fast supervised discrete hashing" (FSDH) based on ``supervised discrete hashing" (SDH). Regressing the training examples (or hash code) to the corresponding class labels is widely used in ordinary least squares regression. Rather than adopting this method, FSDH uses a very simple yet effective regression of the class labels of training examples to the corresponding hash code to accelerate the algorithm. To the best of our knowledge, this strategy has not previously been used for hashing. Traditional SDH decomposes the optimization into three sub-problems, with the most critical sub-problem - discrete optimization for binary hash codes - solved using iterative discrete cyclic coordinate descent (DCC), which is time-consuming. However, FSDH has a closed-form solution and only requires a single rather than iterative hash code-solving step, which is highly efficient. Furthermore, FSDH is usually faster than SDH for solving the projection matrix for least squares regression, making FSDH generally faster than SDH. For example, our results show that FSDH is about 12-times faster than SDH when the number of hashing bits is 128 on the CIFAR-10 data base, and FSDH is about 151-times faster than FastHash when the number of hashing bits is 64 on the MNIST data-base. Our experimental results show that FSDH is not only fast, but also outperforms other comparative methods.

</details>


## 2019-05

<details>

<summary>2019-05-08 18:01:40 - Dependencies and systemic risk in the European insurance sector: Some new evidence based on copula-DCC-GARCH model and selected clustering methods</summary>

- *Anna Denkowska, StanisÅ‚aw Wanat*

- `1905.03273v1` - [abs](http://arxiv.org/abs/1905.03273v1) - [pdf](http://arxiv.org/pdf/1905.03273v1)

> The subject of the present article is the study of correlations between large insurance companies and their contribution to systemic risk in the insurance sector. Our main goal is to analyze the conditional structure of the correlation on the European insurance market and to compare systemic risk in different regimes of this market. These regimes are identified by monitoring the weekly rates of returns of eight of the largest insurers (five from Europe and the biggest insurers from the USA, Canada and China) during the period January 2005 to December 2018. To this aim we use statistical clustering methods for time units (weeks) to which we assigned the conditional variances obtained from the estimated copula-DCC-GARCH model. The advantage of such an approach is that there is no need to assume a priori a number of market regimes, since this number has been identified by means of clustering quality validation. In each of the identified market regimes we determined the commonly now used CoVaR systemic risk measure. From the performed analysis we conclude that all the considered insurance companies are positively correlated and this correlation is stronger in times of turbulences on global markets which shows an increased exposure of the European insurance sector to systemic risk during crisis. Moreover, in times of turbulences on global markets the value level of the CoVaR systemic risk index is much higher than in "normal conditions".

</details>


## 2019-08

<details>

<summary>2019-08-22 12:41:32 - An End-to-End Encrypted Neural Network for Gradient Updates Transmission in Federated Learning</summary>

- *Hongyu Li, Tianqi Han*

- `1908.08340v1` - [abs](http://arxiv.org/abs/1908.08340v1) - [pdf](http://arxiv.org/pdf/1908.08340v1)

> Federated learning is a distributed learning method to train a shared model by aggregating the locally-computed gradient updates. In federated learning, bandwidth and privacy are two main concerns of gradient updates transmission. This paper proposes an end-to-end encrypted neural network for gradient updates transmission. This network first encodes the input gradient updates to a lower-dimension space in each client, which significantly mitigates the pressure of data communication in federated learning. The encoded gradient updates are directly recovered as a whole, i.e. the aggregated gradient updates of the trained model, in the decoding layers of the network on the server. In this way, gradient updates encrypted in each client are not only prevented from interception during communication, but also unknown to the server. Based on the encrypted neural network, a novel federated learning framework is designed in real applications. Experimental results show that the proposed network can effectively achieve two goals, privacy protection and data compression, under a little sacrifice of the model accuracy in federated learning.

</details>

<details>

<summary>2019-08-22 13:02:20 - Automated Generation of Test Models from Semi-Structured Requirements</summary>

- *Jannik Fischbach, Maximilian Junker, Andreas Vogelsang, Dietmar Freudenstein*

- `1908.08810v1` - [abs](http://arxiv.org/abs/1908.08810v1) - [pdf](http://arxiv.org/pdf/1908.08810v1)

> [Context:] Model-based testing is an instrument for automated generation of test cases. It requires identifying requirements in documents, understanding them syntactically and semantically, and then translating them into a test model. One light-weight language for these test models are Cause-Effect-Graphs (CEG) that can be used to derive test cases. [Problem:] The creation of test models is laborious and we lack an automated solution that covers the entire process from requirement detection to test model creation. In addition, the majority of requirements is expressed in natural language (NL), which is hard to translate to test models automatically. [Principal Idea:] We build on the fact that not all NL requirements are equally unstructured. We found that 14 % of the lines in requirements documents of our industry partner contain "pseudo-code"-like descriptions of business rules. We apply Machine Learning to identify such semi-structured requirements descriptions and propose a rule-based approach for their translation into CEGs. [Contribution:] We make three contributions: (1) an algorithm for the automatic detection of semi-structured requirements descriptions in documents, (2) an algorithm for the automatic translation of the identified requirements into a CEG and (3) a study demonstrating that our proposed solution leads to 86 % time savings for test model creation without loss of quality.

</details>

<details>

<summary>2019-08-22 22:26:08 - Multitask Learning Deep Neural Networks to Combine Revealed and Stated Preference Data</summary>

- *Shenhao Wang, Qingyi Wang, Jinhua Zhao*

- `1901.00227v2` - [abs](http://arxiv.org/abs/1901.00227v2) - [pdf](http://arxiv.org/pdf/1901.00227v2)

> It is an enduring question how to combine revealed preference (RP) and stated preference (SP) data to analyze travel behavior. This study presents a framework of multitask learning deep neural networks (MTLDNNs) for this question, and demonstrates that MTLDNNs are more generic than the traditional nested logit (NL) method, due to its capacity of automatic feature learning and soft constraints. About 1,500 MTLDNN models are designed and applied to the survey data that was collected in Singapore and focused on the RP of four current travel modes and the SP with autonomous vehicles (AV) as the one new travel mode in addition to those in RP. We found that MTLDNNs consistently outperform six benchmark models and particularly the classical NL models by about 5% prediction accuracy in both RP and SP datasets. This performance improvement can be mainly attributed to the soft constraints specific to MTLDNNs, including its innovative architectural design and regularization methods, but not much to the generic capacity of automatic feature learning endowed by a standard feedforward DNN architecture. Besides prediction, MTLDNNs are also interpretable. The empirical results show that AV is mainly the substitute of driving and AV alternative-specific variables are more important than the socio-economic variables in determining AV adoption. Overall, this study introduces a new MTLDNN framework to combine RP and SP, and demonstrates its theoretical flexibility and empirical power for prediction and interpretation. Future studies can design new MTLDNN architectures to reflect the speciality of RP and SP and extend this work to other behavioral analysis.

</details>

<details>

<summary>2019-08-29 15:37:01 - Nearly-Linear uncertainty measures</summary>

- *Chiara Corsato, Renato Pelessoni, Paolo Vicig*

- `1908.11303v1` - [abs](http://arxiv.org/abs/1908.11303v1) - [pdf](http://arxiv.org/pdf/1908.11303v1)

> Several easy to understand and computationally tractable imprecise probability models, like the Pari-Mutuel model, are derived from a given probability measure P_0. In this paper we investigate a family of such models, called Nearly-Linear (NL). They generalise a number of well-known models, while preserving a simple mathematical structure. In fact, they are linear affine transformations of P_0 as long as the transformation returns a value in [0,1] . We study the properties of NL measures that are (at least) capacities, and show that they can be partitioned into three major subfamilies. We investigate their consistency, which ranges from 2-coherence, the minimal condition satisfied by all, to coherence, and the kind of beliefs they can represent. There is a variety of different situations that NL models can incorporate, from generalisations of the Pari-Mutuel model, the {\epsilon}-contamination model and other models to conflicting attitudes of an agent towards low/high P_0-probability events (both prudential and imprudent at the same time), or to symmetry judgments. The consistency properties vary with the beliefs represented, but not strictly: some conflicting and partly irrational moods may be compatible with coherence. In a final part, we compare NL models with their closest, but only partly overlapping, models, neo-additive capacities and probability intervals.

</details>


## 2019-09

<details>

<summary>2019-09-12 06:20:03 - Countering Noisy Labels By Learning From Auxiliary Clean Labels</summary>

- *Tsung Wei Tsai, Chongxuan Li, Jun Zhu*

- `1905.13305v2` - [abs](http://arxiv.org/abs/1905.13305v2) - [pdf](http://arxiv.org/pdf/1905.13305v2)

> We consider the learning from noisy labels (NL) problem which emerges in many real-world applications. In addition to the widely-studied synthetic noise in the NL literature, we also consider the pseudo labels in semi-supervised learning (Semi-SL) as a special case of NL. For both types of noise, we argue that the generalization performance of existing methods is highly coupled with the quality of noisy labels. Therefore, we counter the problem from a novel and unified perspective: learning from the auxiliary clean labels. Specifically, we propose the Rotational-Decoupling Consistency Regularization (RDCR) framework that integrates the consistency-based methods with the self-supervised rotation task to learn noise-tolerant representations. The experiments show that RDCR achieves comparable or superior performance than the state-of-the-art methods under small noise, while outperforms the existing methods significantly when there is large noise.

</details>


## 2019-10

<details>

<summary>2019-10-11 14:15:52 - NLS: an accurate and yet easy-to-interpret regression method</summary>

- *Victor Coscrato, Marco Henrique de Almeida InÃ¡cio, Tiago Botari, Rafael Izbicki*

- `1910.05206v1` - [abs](http://arxiv.org/abs/1910.05206v1) - [pdf](http://arxiv.org/pdf/1910.05206v1)

> An important feature of successful supervised machine learning applications is to be able to explain the predictions given by the regression or classification model being used. However, most state-of-the-art models that have good predictive power lead to predictions that are hard to interpret. Thus, several model-agnostic interpreters have been developed recently as a way of explaining black-box classifiers. In practice, using these methods is a slow process because a novel fitting is required for each new testing instance, and several non-trivial choices must be made. We develop NLS (neural local smoother), a method that is complex enough to give good predictions, and yet gives solutions that are easy to be interpreted without the need of using a separate interpreter. The key idea is to use a neural network that imposes a local linear shape to the output layer. We show that NLS leads to predictive power that is comparable to state-of-the-art machine learning models, and yet is easier to interpret.

</details>

<details>

<summary>2019-10-31 11:56:29 - Efficient Bayesian inference for nonlinear state space models with univariate autoregressive state equation</summary>

- *Alexander Kreuzer, Claudia Czado*

- `1902.10412v3` - [abs](http://arxiv.org/abs/1902.10412v3) - [pdf](http://arxiv.org/pdf/1902.10412v3)

> Latent autoregressive processes are a popular choice to model time varying parameters. These models can be formulated as nonlinear state space models for which inference is not straightforward due to the high number of parameters. Therefore maximum likelihood methods are often infeasible and researchers rely on alternative techniques, such as Gibbs sampling. But conventional Gibbs samplers are often tailored to specific situations and suffer from high autocorrelation among repeated draws. We present a Gibbs sampler for general nonlinear state space models with an univariate autoregressive state equation. For this we employ an interweaving strategy and elliptical slice sampling to exploit the dependence implied by the autoregressive process. Within a simulation study we demonstrate the efficiency of the proposed sampler for bivariate dynamic copula models. Further we are interested in modeling the volatility return relationship. Therefore we use the proposed sampler to estimate the parameters of stochastic volatility models with skew Student t errors and the parameters of a novel bivariate dynamic mixture copula model. This model allows for dynamic asymmetric tail dependence. Comparison to relevant benchmark models, such as the DCC-GARCH or a Student t copula model, with respect to predictive accuracy shows the superior performance of the proposed approach.

</details>


## 2019-11

<details>

<summary>2019-11-08 12:42:22 - Language Grounding through Social Interactions and Curiosity-Driven Multi-Goal Learning</summary>

- *Nicolas Lair, CÃ©dric Colas, RÃ©my Portelas, Jean-Michel Dussoux, Peter Ford Dominey, Pierre-Yves Oudeyer*

- `1911.03219v1` - [abs](http://arxiv.org/abs/1911.03219v1) - [pdf](http://arxiv.org/pdf/1911.03219v1)

> Autonomous reinforcement learning agents, like children, do not have access to predefined goals and reward functions. They must discover potential goals, learn their own reward functions and engage in their own learning trajectory. Children, however, benefit from exposure to language, helping to organize and mediate their thought. We propose LE2 (Language Enhanced Exploration), a learning algorithm leveraging intrinsic motivations and natural language (NL) interactions with a descriptive social partner (SP). Using NL descriptions from the SP, it can learn an NL-conditioned reward function to formulate goals for intrinsically motivated goal exploration and learn a goal-conditioned policy. By exploring, collecting descriptions from the SP and jointly learning the reward function and the policy, the agent grounds NL descriptions into real behavioral goals. From simple goals discovered early to more complex goals discovered by experimenting on simpler ones, our agent autonomously builds its own behavioral repertoire. This naturally occurring curriculum is supplemented by an active learning curriculum resulting from the agent's intrinsic motivations. Experiments are presented with a simulated robotic arm that interacts with several objects including tools.

</details>

