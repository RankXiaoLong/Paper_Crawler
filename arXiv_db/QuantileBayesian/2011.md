# 2011

## TOC

- [2011-01](#2011-01)
- [2011-02](#2011-02)
- [2011-03](#2011-03)
- [2011-04](#2011-04)
- [2011-05](#2011-05)
- [2011-06](#2011-06)
- [2011-07](#2011-07)
- [2011-08](#2011-08)
- [2011-09](#2011-09)
- [2011-10](#2011-10)
- [2011-11](#2011-11)
- [2011-12](#2011-12)

## 2011-01

<details>

<summary>2011-01-04 11:58:25 - Bayesian inference for a class of latent Markov models for categorical longitudinal data</summary>

- *Francesco Bartolucci, Silvia Pandolfi*

- `1101.0391v2` - [abs](http://arxiv.org/abs/1101.0391v2) - [pdf](http://arxiv.org/pdf/1101.0391v2)

> We propose a Bayesian inference approach for a class of latent Markov models. These models are widely used for the analysis of longitudinal categorical data, when the interest is in studying the evolution of an individual unobservable characteristic. We consider, in particular, the basic latent Markov, which does not account for individual covariates, and its version that includes such covariates in the measurement model. The proposed inferential approach is based on a system of priors formulated on a transformation of the initial and transition probabilities of the latent Markov chain. This system of priors is equivalent to one based on Dirichlet distributions. In order to draw samples from the joint posterior distribution of the parameters and the number of latent states, we implement a reversible jump algorithm which alternates moves of Metropolis-Hastings type with moves of split/combine and birth/death types. The proposed approach is illustrated through two applications based on longitudinal datasets.

</details>

<details>

<summary>2011-01-05 08:20:26 - On the Sample Information About Parameter and Prediction</summary>

- *Nader Ebrahimi, Ehsan S. Soofi, Refik Soyer*

- `1101.0899v1` - [abs](http://arxiv.org/abs/1101.0899v1) - [pdf](http://arxiv.org/pdf/1101.0899v1)

> The Bayesian measure of sample information about the parameter, known as Lindley's measure, is widely used in various problems such as developing prior distributions, models for the likelihood functions and optimal designs. The predictive information is defined similarly and used for model selection and optimal designs, though to a lesser extent. The parameter and predictive information measures are proper utility functions and have been also used in combination. Yet the relationship between the two measures and the effects of conditional dependence between the observable quantities on the Bayesian information measures remain unexplored. We address both issues. The relationship between the two information measures is explored through the information provided by the sample about the parameter and prediction jointly. The role of dependence is explored along with the interplay between the information measures, prior and sampling design. For the conditionally independent sequence of observable quantities, decompositions of the joint information characterize Lindley's measure as the sample information about the parameter and prediction jointly and the predictive information as part of it. For the conditionally dependent case, the joint information about parameter and prediction exceeds Lindley's measure by an amount due to the dependence. More specific results are shown for the normal linear models and a broad subfamily of the exponential family. Conditionally independent samples provide relatively little information for prediction, and the gap between the parameter and predictive information measures grows rapidly with the sample size.

</details>

<details>

<summary>2011-01-05 09:16:35 - A Conversation with George C. Tiao</summary>

- *Daniel Peña, Ruey S. Tsay*

- `1101.0912v1` - [abs](http://arxiv.org/abs/1101.0912v1) - [pdf](http://arxiv.org/pdf/1101.0912v1)

> George C. Tiao was born in London in 1933. After graduating with a B.A. in Economics from National Taiwan University in 1955 he went to the US to obtain an M.B.A from New York University in 1958 and a Ph.D. in Economics from the University of Wisconsin, Madison in 1962. From 1962 to 1982 he was Assistant, Associate, Professor and Bascom Professor of Statistics and Business at the University of Wisconsin, Madison, and in the period 1973--1975 was Chairman of the Department of Statistics. He moved to the Graduate School of Business at the University of Chicago in 1982 and is the W. Allen Wallis Professor of Econometrics and Statistics (emeritus). George Tiao has played a leading role in the development of Bayesian Statistics, Time Series Analysis and Environmental Statistics. He is co-author, with G.E.P. Box, of Bayesian Inference in Statistical Analysis and is the developer of a model-based approach to seasonal adjustment (with S. C. Hillmer), of outlier analysis in time series (with I. Chang), and of new ways of vector ARMA model building (with R. S. Tsay). He is the author/co-author/co-editor of 7 books and over 120 articles in refereed econometric, environmental and statistical journals and has been thesis advisor of over 25 students. He is a leading figure in the development of Statistics in Taiwan and China and is the Founding President of the International Chinese Statistical Association 1987--1988 and the Founding Chair Editor of the journal Statistica Sinica 1988--1993. He played a leading role (over the 20 year period 1979--1999) in the organization of the annual NBER/NSF Time Series Workshop and he was a founding member of the annual conference "Making Statistics More Effective in Schools of Business" 1986--2006.

</details>

<details>

<summary>2011-01-05 12:56:49 - Reuse, recycle, reweigh: Combating influenza through efficient sequential Bayesian computation for massive data</summary>

- *Jennifer A. Tom, Janet S. Sinsheimer, Marc A. Suchard*

- `1101.0959v1` - [abs](http://arxiv.org/abs/1101.0959v1) - [pdf](http://arxiv.org/pdf/1101.0959v1)

> Massive datasets in the gigabyte and terabyte range combined with the availability of increasingly sophisticated statistical tools yield analyses at the boundary of what is computationally feasible. Compromising in the face of this computational burden by partitioning the dataset into more tractable sizes results in stratified analyses, removed from the context that justified the initial data collection. In a Bayesian framework, these stratified analyses generate intermediate realizations, often compared using point estimates that fail to account for the variability within and correlation between the distributions these realizations approximate. However, although the initial concession to stratify generally precludes the more sensible analysis using a single joint hierarchical model, we can circumvent this outcome and capitalize on the intermediate realizations by extending the dynamic iterative reweighting MCMC algorithm. In doing so, we reuse the available realizations by reweighting them with importance weights, recycling them into a now tractable joint hierarchical model. We apply this technique to intermediate realizations generated from stratified analyses of 687 influenza A genomes spanning 13 years allowing us to revisit hypotheses regarding the evolutionary history of influenza within a hierarchical statistical framework.

</details>

<details>

<summary>2011-01-06 04:30:27 - Marginal Likelihood Computation via Arrogance Sampling</summary>

- *Benedict Escoto*

- `1101.1136v1` - [abs](http://arxiv.org/abs/1101.1136v1) - [pdf](http://arxiv.org/pdf/1101.1136v1)

> This paper describes a method for estimating the marginal likelihood or Bayes factors of Bayesian models using non-parametric importance sampling ("arrogance sampling"). This method can also be used to compute the normalizing constant of probability distributions. Because the required inputs are samples from the distribution to be normalized and the scaled density at those samples, this method may be a convenient replacement for the harmonic mean estimator. The method has been implemented in the open source R package margLikArrogance.

</details>

<details>

<summary>2011-01-06 17:40:36 - Bayesian Analysis of Loss Ratios Using the Reversible Jump Algorithm</summary>

- *Garfield Brown, Steve Brooks*

- `1101.1264v1` - [abs](http://arxiv.org/abs/1101.1264v1) - [pdf](http://arxiv.org/pdf/1101.1264v1)

> In this paper we consider the problem of model choice for a set of insurance loss ratios. We use a reversible jump algorithm for our model discrimination and show how the vanilla reversible jump algorithm can be improved on using recent methodological advances in reversible jump computation.

</details>

<details>

<summary>2011-01-07 08:29:37 - A Bayesian graphical modeling approach to microRNA regulatory network inference</summary>

- *Francesco C. Stingo, Yian A. Chen, Marina Vannucci, Marianne Barrier, Philip E. Mirkes*

- `1101.1377v1` - [abs](http://arxiv.org/abs/1101.1377v1) - [pdf](http://arxiv.org/pdf/1101.1377v1)

> It has been estimated that about 30% of the genes in the human genome are regulated by microRNAs (miRNAs). These are short RNA sequences that can down-regulate the levels of mRNAs or proteins in animals and plants. Genes regulated by miRNAs are called targets. Typically, methods for target prediction are based solely on sequence data and on the structure information. In this paper we propose a Bayesian graphical modeling approach that infers the miRNA regulatory network by integrating expression levels of miRNAs with their potential mRNA targets and, via the prior probability model, with their sequence/structure information. We use a directed graphical model with a particular structure adapted to our data based on biological considerations. We then achieve network inference using stochastic search methods for variable selection that allow us to explore the huge model space via MCMC. A time-dependent coefficients model is also implemented. We consider experimental data from a study on a very well-known developmental toxicant causing neural tube defects, hyperthermia. Some of the pairs of target gene and miRNA we identify seem very plausible and warrant future investigation. Our proposed method is general and can be easily applied to other types of network inference by integrating multiple data sources.

</details>

<details>

<summary>2011-01-07 10:35:33 - Model-robust regression and a Bayesian ``sandwich'' estimator</summary>

- *Adam A. Szpiro, Kenneth M. Rice, Thomas Lumley*

- `1101.1402v1` - [abs](http://arxiv.org/abs/1101.1402v1) - [pdf](http://arxiv.org/pdf/1101.1402v1)

> We present a new Bayesian approach to model-robust linear regression that leads to uncertainty estimates with the same robustness properties as the Huber--White sandwich estimator. The sandwich estimator is known to provide asymptotically correct frequentist inference, even when standard modeling assumptions such as linearity and homoscedasticity in the data-generating mechanism are violated. Our derivation provides a compelling Bayesian justification for using this simple and popular tool, and it also clarifies what is being estimated when the data-generating mechanism is not linear. We demonstrate the applicability of our approach using a simulation study and health care cost data from an evaluation of the Washington State Basic Health Plan.

</details>

<details>

<summary>2011-01-07 12:07:59 - Bayesian semiparametric inference for multivariate doubly-interval-censored data</summary>

- *Alejandro Jara, Emmanuel Lesaffre, Maria De Iorio, Fernando Quintana*

- `1101.1415v1` - [abs](http://arxiv.org/abs/1101.1415v1) - [pdf](http://arxiv.org/pdf/1101.1415v1)

> Based on a data set obtained in a dental longitudinal study, conducted in Flanders (Belgium), the joint time to caries distribution of permanent first molars was modeled as a function of covariates. This involves an analysis of multivariate continuous doubly-interval-censored data since: (i) the emergence time of a tooth and the time it experiences caries were recorded yearly, and (ii) events on teeth of the same child are dependent. To model the joint distribution of the emergence times and the times to caries, we propose a dependent Bayesian semiparametric model. A major feature of the proposed approach is that survival curves can be estimated without imposing assumptions such as proportional hazards, additive hazards, proportional odds or accelerated failure time.

</details>

<details>

<summary>2011-01-07 12:13:53 - Conjugate Projective Limits</summary>

- *Peter Orbanz*

- `1012.0363v2` - [abs](http://arxiv.org/abs/1012.0363v2) - [pdf](http://arxiv.org/pdf/1012.0363v2)

> We characterize conjugate nonparametric Bayesian models as projective limits of conjugate, finite-dimensional Bayesian models. In particular, we identify a large class of nonparametric models representable as infinite-dimensional analogues of exponential family distributions and their canonical conjugate priors. This class contains most models studied in the literature, including Dirichlet processes and Gaussian process regression models. To derive these results, we introduce a representation of infinite-dimensional Bayesian models by projective limits of regular conditional probabilities. We show under which conditions the nonparametric model itself, its sufficient statistics, and -- if they exist -- conjugate updates of the posterior are projective limits of their respective finite-dimensional counterparts. We illustrate our results both by application to existing nonparametric models and by construction of a model on infinite permutations.

</details>

<details>

<summary>2011-01-18 14:33:22 - Minimum mean square distance estimation of a subspace</summary>

- *Olivier Besson, Nicolas Dobigeon, Jean-Yves Tourneret*

- `1101.3462v1` - [abs](http://arxiv.org/abs/1101.3462v1) - [pdf](http://arxiv.org/pdf/1101.3462v1)

> We consider the problem of subspace estimation in a Bayesian setting. Since we are operating in the Grassmann manifold, the usual approach which consists of minimizing the mean square error (MSE) between the true subspace $U$ and its estimate $\hat{U}$ may not be adequate as the MSE is not the natural metric in the Grassmann manifold. As an alternative, we propose to carry out subspace estimation by minimizing the mean square distance (MSD) between $U$ and its estimate, where the considered distance is a natural metric in the Grassmann manifold, viz. the distance between the projection matrices. We show that the resulting estimator is no longer the posterior mean of $U$ but entails computing the principal eigenvectors of the posterior mean of $U U^{T}$. Derivation of the MMSD estimator is carried out in a few illustrative examples including a linear Gaussian model for the data and a Bingham or von Mises Fisher prior distribution for $U$. In all scenarios, posterior distributions are derived and the MMSD estimator is obtained either analytically or implemented via a Markov chain Monte Carlo simulation method. The method is shown to provide accurate estimates even when the number of samples is lower than the dimension of $U$. An application to hyperspectral imagery is finally investigated.

</details>

<details>

<summary>2011-01-19 11:45:22 - Bayesian ICA-based source separation of Cosmic Microwave Background by a discrete functional approximation</summary>

- *Simon P. Wilson, Jiwon Yoon*

- `1011.4018v2` - [abs](http://arxiv.org/abs/1011.4018v2) - [pdf](http://arxiv.org/pdf/1011.4018v2)

> A functional approximation to implement Bayesian source separation analysis is introduced and applied to separation of the Cosmic Microwave Background (CMB) using WMAP data. The approximation allows for tractable full-sky map reconstructions at the scale of both WMAP and Planck data and models the spatial smoothness of sources through a Gaussian Markov random field prior. It is orders of magnitude faster than the usual MCMC approaches. The performance and limitations of the approximation are also discussed.

</details>

<details>

<summary>2011-01-21 15:42:15 - Inference of global clusters from locally distributed data</summary>

- *XuanLong Nguyen*

- `1001.0597v2` - [abs](http://arxiv.org/abs/1001.0597v2) - [pdf](http://arxiv.org/pdf/1001.0597v2)

> We consider the problem of analyzing the heterogeneity of clustering distributions for multiple groups of observed data, each of which is indexed by a covariate value, and inferring global clusters arising from observations aggregated over the covariate domain. We propose a novel Bayesian nonparametric method reposing on the formalism of spatial modeling and a nested hierarchy of Dirichlet processes. We provide an analysis of the model properties, relating and contrasting the notions of local and global clusters. We also provide an efficient inference algorithm, and demonstrate the utility of our method in several data examples, including the problem of object tracking and a global clustering analysis of functional data where the functional identity information is not available.

</details>

<details>

<summary>2011-01-21 22:57:52 - Efficient Bayesian inference in stochastic chemical kinetic models using graphical processing units</summary>

- *Jarad Niemi, Matthew Wheeler*

- `1101.4242v1` - [abs](http://arxiv.org/abs/1101.4242v1) - [pdf](http://arxiv.org/pdf/1101.4242v1)

> A goal of systems biology is to understand the dynamics of intracellular systems. Stochastic chemical kinetic models are often utilized to accurately capture the stochastic nature of these systems due to low numbers of molecules. Collecting system data allows for estimation of stochastic chemical kinetic rate parameters. We describe a well-known, but typically impractical data augmentation Markov chain Monte Carlo algorithm for estimating these parameters. The impracticality is due to the use of rejection sampling for latent trajectories with fixed initial and final endpoints which can have diminutive acceptance probability. We show how graphical processing units can be efficiently utilized for parameter estimation in systems that hitherto were inestimable. For more complex systems, we show the efficiency gain over traditional CPU computing is on the order of 200. Finally, we show a Bayesian analysis of a system based on Michaelis-Menton kinetics.

</details>

<details>

<summary>2011-01-22 23:50:01 - Peak Reduction and Clipping Mitigation by Compressive Sensing</summary>

- *Ebrahim B. Al-Safadi, Tareq Y. Al-Naffouri*

- `1101.4335v1` - [abs](http://arxiv.org/abs/1101.4335v1) - [pdf](http://arxiv.org/pdf/1101.4335v1)

> This work establishes the design, analysis, and fine-tuning of a Peak-to-Average-Power-Ratio (PAPR) reducing system, based on compressed sensing at the receiver of a peak-reducing sparse clipper applied to an OFDM signal at the transmitter. By exploiting the sparsity of the OFDM signal in the time domain relative to a pre-defined clipping threshold, the method depends on partially observing the frequency content of extremely simple sparse clippers to recover the locations, magnitudes, and phases of the clipped coefficients of the peak-reduced signal. We claim that in the absence of optimization algorithms at the transmitter that confine the frequency support of clippers to a predefined set of reserved-tones, no other tone-reservation method can reliably recover the original OFDM signal with such low complexity.   Afterwards we focus on designing different clipping signals that can embed a priori information regarding the support and phase of the peak-reducing signal to the receiver, followed by modified compressive sensing techniques for enhanced recovery. This includes data-based weighted {\ell} 1 minimization for enhanced support recovery and phase-augmention for homogeneous clippers followed by Bayesian techniques.   We show that using such techniques for a typical OFDM signal of 256 subcarriers and 20% reserved tones, the PAPR can be reduced by approximately 4.5 dB with a significant increase in capacity compared to a system which uses all its tones for data transmission and clips to such levels. The design is hence appealing from both capacity and PAPR reduction aspects.

</details>

<details>

<summary>2011-01-23 13:32:31 - Inferences in Bayesian variable selection problems with large model spaces</summary>

- *Gonzalo Garcia-Donato, Miguel Angel Martinez-Beneito*

- `1101.4368v1` - [abs](http://arxiv.org/abs/1101.4368v1) - [pdf](http://arxiv.org/pdf/1101.4368v1)

> An important aspect of Bayesian model selection is how to deal with huge model spaces, since exhaustive enumeration of all the models entertained is unfeasible and inferences have to be based on the very small proportion of models visited. This is the case for the variable selection problem, with a moderate to large number of possible explanatory variables being considered in this paper. We review some of the strategies proposed in the literature and argue that inferences based on empirical frequencies via Markov Chain Monte Carlo sampling of the posterior distribution outperforms recently proposed searching methods. We give a plausible yet very simple explanation of this effect, showing that estimators based on frequencies are unbiased. The results obtained in two illustrative examples provide strong evidence in favor of our arguments.

</details>

<details>

<summary>2011-01-27 06:32:36 - Why approximate Bayesian computational (ABC) methods cannot handle model choice problems</summary>

- *Christian Robert, Jean-Michel Marin, Natesh S. Pillai*

- `1101.5091v2` - [abs](http://arxiv.org/abs/1101.5091v2) - [pdf](http://arxiv.org/pdf/1101.5091v2)

> Approximate Bayesian computation (ABC), also known as likelihood-free methods, have become a favourite tool for the analysis of complex stochastic models, primarily in population genetics but also in financial analyses. We advocated in Grelaud et al. (2009) the use of ABC for Bayesian model choice in the specific case of Gibbs random fields (GRF), relying on a sufficiency property mainly enjoyed by GRFs to show that the approach was legitimate. Despite having previously suggested the use of ABC for model choice in a wider range of models in the DIY ABC software (Cornuet et al., 2008), we present theoretical evidence that the general use of ABC for model choice is fraught with danger in the sense that no amount of computation, however large, can guarantee a proper approximation of the posterior probabilities of the models under comparison.

</details>


## 2011-02

<details>

<summary>2011-02-01 12:27:44 - Uncertainty quantification and weak approximation of an elliptic inverse problem</summary>

- *Masoumeh Dashti, Andrew M. Stuart*

- `1102.0143v1` - [abs](http://arxiv.org/abs/1102.0143v1) - [pdf](http://arxiv.org/pdf/1102.0143v1)

> We consider the inverse problem of determining the permeability from the pressure in a Darcy model of flow in a porous medium. Mathematically the problem is to find the diffusion coefficient for a linear uniformly elliptic partial differential equation in divergence form, in a bounded domain in dimension $d \le 3$, from measurements of the solution in the interior. We adopt a Bayesian approach to the problem. We place a prior random field measure on the log permeability, specified through the Karhunen-Lo\`eve expansion of its draws. We consider Gaussian measures constructed this way, and study the regularity of functions drawn from them. We also study the Lipschitz properties of the observation operator mapping the log permeability to the observations. Combining these regularity and continuity estimates, we show that the posterior measure is well-defined on a suitable Banach space. Furthermore the posterior measure is shown to be Lipschitz with respect to the data in the Hellinger metric, giving rise to a form of well-posedness of the inverse problem. Determining the posterior measure, given the data, solves the problem of uncertainty quantification for this inverse problem. In practice the posterior measure must be approximated in a finite dimensional space. We quantify the errors incurred by employing a truncated Karhunen-Lo\`eve expansion to represent this meausure. In particular we study weak convergence of a general class of locally Lipschitz functions of the log permeability, and apply this general theory to estimate errors in the posterior mean of the pressure and the pressure covariance, under refinement of the finite dimensional Karhunen-Lo\`eve truncation.

</details>

<details>

<summary>2011-02-04 01:05:25 - Chi-square Intervals for a Poisson Parameter - Bayes, Classical and Structural</summary>

- *E. A. Maxwell*

- `1102.0822v1` - [abs](http://arxiv.org/abs/1102.0822v1) - [pdf](http://arxiv.org/pdf/1102.0822v1)

> The 'standard' confidence interval for a Poisson parameter is only one of a number of estimation intervals based on the chi-square distribution that may be used in the estimation of the mean or mean rate for a Poisson model. Other chi-square intervals are available for experimenters using Bayesian or structural inference methods. Exploring these intervals also leads to other alternate approximate chi-square intervals. Although coverage probability may not always be of interest for Bayesian or structural intervals, coverage probabilities are useful for validating 'objective' priors. Coverage probabilities are explored for all of the intervals considered.

</details>

<details>

<summary>2011-02-04 19:07:57 - On statistical uncertainty in nested sampling</summary>

- *Charles R. Keeton*

- `1102.0996v1` - [abs](http://arxiv.org/abs/1102.0996v1) - [pdf](http://arxiv.org/pdf/1102.0996v1)

> Nested sampling has emerged as a valuable tool for Bayesian analysis, in particular for determining the Bayesian evidence. The method is based on a specific type of random sampling of the likelihood function and prior volume of the parameter space. I study the statistical uncertainty in the evidence computed with nested sampling. I examine the uncertainty estimator from Skilling (2004, 2006) and introduce a new estimator based on a detailed analysis of the statistical properties of nested sampling. Both perform well in test cases and make it possible to obtain the statistical uncertainty in the evidence with no additional computational cost.

</details>

<details>

<summary>2011-02-08 23:44:49 - Bayesian Nonparametric Covariance Regression</summary>

- *Emily Fox, David Dunson*

- `1101.2017v2` - [abs](http://arxiv.org/abs/1101.2017v2) - [pdf](http://arxiv.org/pdf/1101.2017v2)

> Although there is a rich literature on methods for allowing the variance in a univariate regression model to vary with predictors, time and other factors, relatively little has been done in the multivariate case. Our focus is on developing a class of nonparametric covariance regression models, which allow an unknown p x p covariance matrix to change flexibly with predictors. The proposed modeling framework induces a prior on a collection of covariance matrices indexed by predictors through priors for predictor-dependent loadings matrices in a factor model. In particular, the predictor-dependent loadings are characterized as a sparse combination of a collection of unknown dictionary functions (e.g, Gaussian process random functions). The induced covariance is then a regularized quadratic function of these dictionary elements. Our proposed framework leads to a highly-flexible, but computationally tractable formulation with simple conjugate posterior updates that can readily handle missing data. Theoretical properties are discussed and the methods are illustrated through simulations studies and an application to the Google Flu Trends data.

</details>

<details>

<summary>2011-02-10 12:46:51 - Estimating conditional quantiles with the help of the pinball loss</summary>

- *Ingo Steinwart, Andreas Christmann*

- `1102.2101v1` - [abs](http://arxiv.org/abs/1102.2101v1) - [pdf](http://arxiv.org/pdf/1102.2101v1)

> The so-called pinball loss for estimating conditional quantiles is a well-known tool in both statistics and machine learning. So far, however, only little work has been done to quantify the efficiency of this tool for nonparametric approaches. We fill this gap by establishing inequalities that describe how close approximate pinball risk minimizers are to the corresponding conditional quantile. These inequalities, which hold under mild assumptions on the data-generating distribution, are then used to establish so-called variance bounds, which recently turned out to play an important role in the statistical analysis of (regularized) empirical risk minimization approaches. Finally, we use both types of inequalities to establish an oracle inequality for support vector machines that use the pinball loss. The resulting learning rates are min--max optimal under some standard regularity assumptions on the conditional quantile.

</details>

<details>

<summary>2011-02-14 13:56:42 - A Hierarchical Model for Aggregated Functional Data</summary>

- *Ronaldo Dias, Nancy L. Garcia, Alexandra M. Schmidt*

- `1102.2773v1` - [abs](http://arxiv.org/abs/1102.2773v1) - [pdf](http://arxiv.org/pdf/1102.2773v1)

> In many areas of science one aims to estimate latent sub-population mean curves based only on observations of aggregated population curves. By aggregated curves we mean linear combination of functional data that cannot be observed individually. We assume that several aggregated curves with linear independent coefficients are available. More specifically, we assume each aggregated curve is an independent partial realization of a Gaussian process with mean modeled through a weighted linear combination of the disaggregated curves. We model the mean of the Gaussian processes as a smooth function approximated by a function belonging to a finite dimensional space ${\cal H}_K$ which is spanned by $K$ B-splines basis functions. We explore two different specifications of the covariance function of the Gaussian process: one that assumes a constant variance across the domain of the process, and a more general variance structure which is itself modelled as a smooth function, providing a nonstationary covariance function. Inference procedure is performed following the Bayesian paradigm allowing experts' opinion to be considered when estimating the disaggregated curves. Moreover, it naturally provides the uncertainty associated with the parameters estimates and fitted values. Our model is suitable for a wide range of applications. We concentrate on two different real examples: calibration problem for NIR spectroscopy data and an analysis of distribution of energy among different type of consumers.

</details>

<details>

<summary>2011-02-14 13:57:10 - Quantifying the Fraction of Missing Information for Hypothesis Testing in Statistical and Genetic Studies</summary>

- *Dan L. Nicolae, Xiao-Li Meng, Augustine Kong*

- `1102.2774v1` - [abs](http://arxiv.org/abs/1102.2774v1) - [pdf](http://arxiv.org/pdf/1102.2774v1)

> Many practical studies rely on hypothesis testing procedures applied to data sets with missing information. An important part of the analysis is to determine the impact of the missing data on the performance of the test, and this can be done by properly quantifying the relative (to complete data) amount of available information. The problem is directly motivated by applications to studies, such as linkage analyses and haplotype-based association projects, designed to identify genetic contributions to complex diseases. In the genetic studies the relative information measures are needed for the experimental design, technology comparison, interpretation of the data, and for understanding the behavior of some of the inference tools. The central difficulties in constructing such information measures arise from the multiple, and sometimes conflicting, aims in practice. For large samples, we show that a satisfactory, likelihood-based general solution exists by using appropriate forms of the relative Kullback--Leibler information, and that the proposed measures are computationally inexpensive given the maximized likelihoods with the observed data. Two measures are introduced, under the null and alternative hypothesis respectively. We exemplify the measures on data coming from mapping studies on the inflammatory bowel disease and diabetes. For small-sample problems, which appear rather frequently in practice and sometimes in disguised forms (e.g., measuring individual contributions to a large study), the robust Bayesian approach holds great promise, though the choice of a general-purpose "default prior" is a very challenging problem.

</details>

<details>

<summary>2011-02-15 07:11:29 - Compatibility of Prior Specifications Across Linear Models</summary>

- *Guido Consonni, Piero Veronese*

- `1102.2981v1` - [abs](http://arxiv.org/abs/1102.2981v1) - [pdf](http://arxiv.org/pdf/1102.2981v1)

> Bayesian model comparison requires the specification of a prior distribution on the parameter space of each candidate model. In this connection two concerns arise: on the one hand the elicitation task rapidly becomes prohibitive as the number of models increases; on the other hand numerous prior specifications can only exacerbate the well-known sensitivity to prior assignments, thus producing less dependable conclusions. Within the subjective framework, both difficulties can be counteracted by linking priors across models in order to achieve simplification and compatibility; we discuss links with related objective approaches. Given an encompassing, or full, model together with a prior on its parameter space, we review and summarize a few procedures for deriving priors under a submodel, namely marginalization, conditioning, and Kullback--Leibler projection. These techniques are illustrated and discussed with reference to variable selection in linear models adopting a conventional $g$-prior; comparisons with existing standard approaches are provided. Finally, the relative merits of each procedure are evaluated through simulated and real data sets.

</details>

<details>

<summary>2011-02-15 07:31:44 - Bayesian theory of systematic measurement deviations</summary>

- *Michael Krystek*

- `1009.0942v3` - [abs](http://arxiv.org/abs/1009.0942v3) - [pdf](http://arxiv.org/pdf/1009.0942v3)

> Concerning systematic effects, the recommendation given in the GUM is to correct for them, but unfortunately no detailed information is available, how to do this. This publication will show, how systematic measurement deviations can be handled correctly based on the Bayesian probability theory. After a short overview about useful methods and tools, like the product rule of probability theory, Bayes' theorem, the principle of maximum entropy, and the marginalisation equation, an outline of a method to handle systematic measurement deviations is introduced. Finally some simple examples of practical interest are given, in order to demonstrate the applicability of the suggested method.

</details>

<details>

<summary>2011-02-17 14:06:16 - The Banff Challenge: Statistical Detection of a Noisy Signal</summary>

- *A. C. Davison, N. Sartori*

- `0712.2708v2` - [abs](http://arxiv.org/abs/0712.2708v2) - [pdf](http://arxiv.org/pdf/0712.2708v2)

> Particle physics experiments such as those run in the Large Hadron Collider result in huge quantities of data, which are boiled down to a few numbers from which it is hoped that a signal will be detected. We discuss a simple probability model for this and derive frequentist and noninformative Bayesian procedures for inference about the signal. Both are highly accurate in realistic cases, with the frequentist procedure having the edge for interval estimation, and the Bayesian procedure yielding slightly better point estimates. We also argue that the significance, or $p$-value, function based on the modified likelihood root provides a comprehensive presentation of the information in the data and should be used for inference.

</details>

<details>

<summary>2011-02-18 08:37:33 - A Conversation with Myles Hollander</summary>

- *Francisco J. Samaniego*

- `1102.3780v1` - [abs](http://arxiv.org/abs/1102.3780v1) - [pdf](http://arxiv.org/pdf/1102.3780v1)

> Myles Hollander was born in Brooklyn, New York, on March 21, 1941. He graduated from Carnegie Mellon University in 1961 with a B.S. in mathematics. In the fall of 1961, he entered the Department of Statistics, Stanford University, earning his M.S. in statistics in 1962 and his Ph.D. in statistics in 1965. He joined the Department of Statistics, Florida State University in 1965 and retired on May 31, 2007, after 42 years of service. He was department chair for nine years 1978-1981, 1999-2005. He was named Professor Emeritus at Florida State upon retirement in 2007. Hollander served as Editor of the Journal of the American Statistical Association, Theory and Methods, 1994-1996, and was an Associate Editor for that journal from 1985 until he became Theory and Methods Editor-Elect in 1993. He also served on the editorial boards of the Journal of Nonparametric Statistics (1993-1997; 2003-2005) and Lifetime Data Analysis (1994-2007). Hollander has published over 100 papers on nonparametric statistics, survival analysis, reliability theory, biostatistics, probability theory, decision theory, Bayesian statistics and multivariate analysis. He is grateful for the generous research support he has received throughout his career, most notably from the Office of Naval Research, the U.S. Air Force Office of Scientific Research, and the National Institutes of Health.

</details>

<details>

<summary>2011-02-22 20:51:04 - Accounting for Calibration Uncertainties in X-ray Analysis: Effective Areas in Spectral Fitting</summary>

- *Hyunsook Lee, Vinay L. Kashyap, David A. van Dyk, Alanna Connors, Jeremy J. Drake, Rima Izem, Xiao-Li Meng, Shandong Min, Taeyoung Park, Pete Ratzlaff, Aneta Siemiginowska, Andreas Zezas*

- `1102.4610v1` - [abs](http://arxiv.org/abs/1102.4610v1) - [pdf](http://arxiv.org/pdf/1102.4610v1)

> While considerable advance has been made to account for statistical uncertainties in astronomical analyses, systematic instrumental uncertainties have been generally ignored. This can be crucial to a proper interpretation of analysis results because instrumental calibration uncertainty is a form of systematic uncertainty. Ignoring it can underestimate error bars and introduce bias into the fitted values of model parameters. Accounting for such uncertainties currently requires extensive case-specific simulations if using existing analysis packages. Here we present general statistical methods that incorporate calibration uncertainties into spectral analysis of high-energy data. We first present a method based on multiple imputation that can be applied with any fitting method, but is necessarily approximate. We then describe a more exact Bayesian approach that works in conjunction with a Markov chain Monte Carlo based fitting. We explore methods for improving computational efficiency, and in particular detail a method of summarizing calibration uncertainties with a principal component analysis of samples of plausible calibration files. This method is implemented using recently codified Chandra effective area uncertainties for low-resolution spectral analysis and is verified using both simulated and actual Chandra data. Our procedure for incorporating effective area uncertainty is easily generalized to other types of calibration uncertainties.

</details>

<details>

<summary>2011-02-23 06:12:55 - Bayesian Variable Selection for Probit Mixed Models Applied to Gene Selection</summary>

- *Meili Baragatti*

- `1101.4577v2` - [abs](http://arxiv.org/abs/1101.4577v2) - [pdf](http://arxiv.org/pdf/1101.4577v2)

> In computational biology, gene expression datasets are characterized by very few individual samples compared to a large number of measurements per sample. Thus, it is appealing to merge these datasets in order to increase the number of observations and diversify the data, allowing a more reliable selection of genes relevant to the biological problem. Besides, the increased size of a merged dataset facilitates its re-splitting into training and validation sets. This necessitates the introduction of the dataset as a random effect. In this context, extending a work of Lee et al. (2003), a method is proposed to select relevant variables among tens of thousands in a probit mixed regression model, considered as part of a larger hierarchical Bayesian model. Latent variables are used to identify subsets of selected variables and the grouping (or blocking) technique of Liu (1994) is combined with a Metropolis-within-Gibbs algorithm (Robert and Casella 2004). The method is applied to a merged dataset made of three individual gene expression datasets, in which tens of thousands of measurements are available for each of several hundred human breast cancer samples. Even for this large dataset comprised of around 20000 predictors, the method is shown to be efficient and feasible. As an illustration, it is used to select the most important genes that characterize the estrogen receptor status of patients with breast cancer.

</details>

<details>

<summary>2011-02-24 14:49:45 - Bayesian nonparametric estimation and consistency of mixed multinomial logit choice models</summary>

- *Pierpaolo De Blasi, Lancelot F. James, John W. Lau*

- `1102.5008v1` - [abs](http://arxiv.org/abs/1102.5008v1) - [pdf](http://arxiv.org/pdf/1102.5008v1)

> This paper develops nonparametric estimation for discrete choice models based on the mixed multinomial logit (MMNL) model. It has been shown that MMNL models encompass all discrete choice models derived under the assumption of random utility maximization, subject to the identification of an unknown distribution $G$. Noting the mixture model description of the MMNL, we employ a Bayesian nonparametric approach, using nonparametric priors on the unknown mixing distribution $G$, to estimate choice probabilities. We provide an important theoretical support for the use of the proposed methodology by investigating consistency of the posterior distribution for a general nonparametric prior on the mixing distribution. Consistency is defined according to an $L_1$-type distance on the space of choice probabilities and is achieved by extending to a regression model framework a recent approach to strong consistency based on the summability of square roots of prior probabilities. Moving to estimation, slightly different techniques for non-panel and panel data models are discussed. For practical implementation, we describe efficient and relatively easy-to-use blocked Gibbs sampling procedures. These procedures are based on approximations of the random probability measure by classes of finite stick-breaking processes. A simulation study is also performed to investigate the performance of the proposed methods.

</details>

<details>

<summary>2011-02-25 07:11:13 - Fast Inference of Interactions in Assemblies of Stochastic Integrate-and-Fire Neurons from Spike Recordings</summary>

- *Remi Monasson, Simona Cocco*

- `1102.5171v1` - [abs](http://arxiv.org/abs/1102.5171v1) - [pdf](http://arxiv.org/pdf/1102.5171v1)

> We present two Bayesian procedures to infer the interactions and external currents in an assembly of stochastic integrate-and-fire neurons from the recording of their spiking activity. The first procedure is based on the exact calculation of the most likely time courses of the neuron membrane potentials conditioned by the recorded spikes, and is exact for a vanishing noise variance and for an instantaneous synaptic integration. The second procedure takes into account the presence of fluctuations around the most likely time courses of the potentials, and can deal with moderate noise levels. The running time of both procedures is proportional to the number S of spikes multiplied by the squared number N of neurons. The algorithms are validated on synthetic data generated by networks with known couplings and currents. We also reanalyze previously published recordings of the activity of the salamander retina (including from 32 to 40 neurons, and from 65,000 to 170,000 spikes). We study the dependence of the inferred interactions on the membrane leaking time; the differences and similarities with the classical cross-correlation analysis are discussed.

</details>


## 2011-03

<details>

<summary>2011-03-02 08:19:07 - Regularization Strategies and Empirical Bayesian Learning for MKL</summary>

- *Ryota Tomioka, Taiji Suzuki*

- `1011.3090v2` - [abs](http://arxiv.org/abs/1011.3090v2) - [pdf](http://arxiv.org/pdf/1011.3090v2)

> Multiple kernel learning (MKL), structured sparsity, and multi-task learning have recently received considerable attention. In this paper, we show how different MKL algorithms can be understood as applications of either regularization on the kernel weights or block-norm-based regularization, which is more common in structured sparsity and multi-task learning. We show that these two regularization strategies can be systematically mapped to each other through a concave conjugate operation. When the kernel-weight-based regularizer is separable into components, we can naturally consider a generative probabilistic model behind MKL. Based on this model, we propose learning algorithms for the kernel weights through the maximization of marginal likelihood. We show through numerical experiments that $\ell_2$-norm MKL and Elastic-net MKL achieve comparable accuracy to uniform kernel combination. Although uniform kernel combination might be preferable from its simplicity, $\ell_2$-norm MKL and Elastic-net MKL can learn the usefulness of the information sources represented as kernels. In particular, Elastic-net MKL achieves sparsity in the kernel weights.

</details>

<details>

<summary>2011-03-05 11:51:27 - Bayesian design of synthetic biological systems</summary>

- *Chris Barnes, Daniel Silk, Xia Sheng, Michael P. H. Stumpf*

- `1103.1046v1` - [abs](http://arxiv.org/abs/1103.1046v1) - [pdf](http://arxiv.org/pdf/1103.1046v1)

> Here we introduce a new design framework for synthetic biology that exploits the advantages of Bayesian model selection. We will argue that the difference between inference and design is that in the former we try to reconstruct the system that has given rise to the data that we observe, while in the latter, we seek to construct the system that produces the data that we would like to observe, i.e. the desired behavior. Our approach allows us to exploit methods from Bayesian statistics, including efficient exploration of models spaces and high-dimensional parameter spaces, and the ability to rank models with respect to their ability to generate certain types of data. Bayesian model selection furthermore automatically strikes a balance between complexity and (predictive or explanatory) performance of mathematical models. In order to deal with the complexities of molecular systems we employ an approximate Bayesian computation scheme which only requires us to simulate from different competing models in order to arrive at rational criteria for choosing between them. We illustrate the advantages resulting from combining the design and modeling (or in-silico prototyping) stages currently seen as separate in synthetic biology by reference to deterministic and stochastic model systems exhibiting adaptive and switch-like behavior, as well as bacterial two-component signaling systems.

</details>

<details>

<summary>2011-03-08 13:28:31 - New efficient estimation and variable selection methods for semiparametric varying-coefficient partially linear models</summary>

- *Bo Kai, Runze Li, Hui Zou*

- `1103.1525v1` - [abs](http://arxiv.org/abs/1103.1525v1) - [pdf](http://arxiv.org/pdf/1103.1525v1)

> The complexity of semiparametric models poses new challenges to statistical inference and model selection that frequently arise from real applications. In this work, we propose new estimation and variable selection procedures for the semiparametric varying-coefficient partially linear model. We first study quantile regression estimates for the nonparametric varying-coefficient functions and the parametric regression coefficients. To achieve nice efficiency properties, we further develop a semiparametric composite quantile regression procedure. We establish the asymptotic normality of proposed estimators for both the parametric and nonparametric parts and show that the estimators achieve the best convergence rate. Moreover, we show that the proposed method is much more efficient than the least-squares-based method for many non-normal errors and that it only loses a small amount of efficiency for normal errors. In addition, it is shown that the loss in efficiency is at most 11.1% for estimating varying coefficient functions and is no greater than 13.6% for estimating parametric components. To achieve sparsity with high-dimensional covariates, we propose adaptive penalization methods for variable selection in the semiparametric varying-coefficient partially linear model and prove that the methods possess the oracle property. Extensive Monte Carlo simulation studies are conducted to examine the finite-sample performance of the proposed procedures. Finally, we apply the new methods to analyze the plasma beta-carotene level data.

</details>

<details>

<summary>2011-03-11 08:13:16 - Deconvolution under Poisson noise using exact data fidelity and synthesis or analysis sparsity priors</summary>

- *François-Xavier Dupé, Jalal Fadili, Jean-Luc Starck*

- `1103.2213v1` - [abs](http://arxiv.org/abs/1103.2213v1) - [pdf](http://arxiv.org/pdf/1103.2213v1)

> In this paper, we propose a Bayesian MAP estimator for solving the deconvolution problems when the observations are corrupted by Poisson noise. Towards this goal, a proper data fidelity term (log-likelihood) is introduced to reflect the Poisson statistics of the noise. On the other hand, as a prior, the images to restore are assumed to be positive and sparsely represented in a dictionary of waveforms such as wavelets or curvelets. Both analysis and synthesis-type sparsity priors are considered. Piecing together the data fidelity and the prior terms, the deconvolution problem boils down to the minimization of non-smooth convex functionals (for each prior). We establish the well-posedness of each optimization problem, characterize the corresponding minimizers, and solve them by means of proximal splitting algorithms originating from the realm of non-smooth convex optimization theory. Experimental results are conducted to demonstrate the potential applicability of the proposed algorithms to astronomical imaging datasets.

</details>

<details>

<summary>2011-03-14 10:09:33 - Type I error rate control for testing many hypotheses: a survey with proofs</summary>

- *Etienne Roquain*

- `1012.4078v2` - [abs](http://arxiv.org/abs/1012.4078v2) - [pdf](http://arxiv.org/pdf/1012.4078v2)

> This paper presents a survey on some recent advances for the type I error rate control in multiple testing methodology. We consider the problem of controlling the $k$-family-wise error rate (kFWER, probability to make $k$ false discoveries or more) and the false discovery proportion (FDP, proportion of false discoveries among the discoveries). The FDP is controlled either via its expectation, which is the so-called false discovery rate (FDR), or via its upper-tail distribution function. We aim at deriving general and unified results together with concise and simple mathematical proofs. Furthermore, while this paper is mainly meant to be a survey paper, some new contributions for controlling the kFWER and the upper-tail distribution function of the FDP are provided. In particular, we derive a new procedure based on the quantiles of the binomial distribution that controls the FDP under independence.

</details>

<details>

<summary>2011-03-14 14:53:23 - Pac-bayesian bounds for sparse regression estimation with exponential weights</summary>

- *Pierre Alquier, Karim Lounici*

- `1009.2707v2` - [abs](http://arxiv.org/abs/1009.2707v2) - [pdf](http://arxiv.org/pdf/1009.2707v2)

> We consider the sparse regression model where the number of parameters $p$ is larger than the sample size $n$. The difficulty when considering high-dimensional problems is to propose estimators achieving a good compromise between statistical and computational performances. The BIC estimator for instance performs well from the statistical point of view \cite{BTW07} but can only be computed for values of $p$ of at most a few tens. The Lasso estimator is solution of a convex minimization problem, hence computable for large value of $p$. However stringent conditions on the design are required to establish fast rates of convergence for this estimator. Dalalyan and Tsybakov \cite{arnak} propose a method achieving a good compromise between the statistical and computational aspects of the problem. Their estimator can be computed for reasonably large $p$ and satisfies nice statistical properties under weak assumptions on the design. However, \cite{arnak} proposes sparsity oracle inequalities in expectation for the empirical excess risk only. In this paper, we propose an aggregation procedure similar to that of \cite{arnak} but with improved statistical performances. Our main theoretical result is a sparsity oracle inequality in probability for the true excess risk for a version of exponential weight estimator. We also propose a MCMC method to compute our estimator for reasonably large values of $p$.

</details>

<details>

<summary>2011-03-14 19:42:53 - Bahadur Representation for U-Quantiles of Dependent Data</summary>

- *Martin Wendler*

- `1004.2581v3` - [abs](http://arxiv.org/abs/1004.2581v3) - [pdf](http://arxiv.org/pdf/1004.2581v3)

> U-quantiles are applied in robust statistics, like the Hodges-Lehmann estimator of location for example. They have been analyzed in the case of independent random variables with the help of a generalized Bahadur representation. Our main aim is to extend these results to U-quantiles of strongly mixing random variables and functionals of absolutely regular sequences. We obtain the central limit theorem and the law of the iterated logarithm for U-quantiles as straightforward corollaries. Furthermore, we improve the existing result for sample quantiles of mixing data.

</details>

<details>

<summary>2011-03-16 03:54:24 - Structural Properties of Bayesian Bandits with Exponential Family Distributions</summary>

- *Yaming Yu*

- `1103.3089v1` - [abs](http://arxiv.org/abs/1103.3089v1) - [pdf](http://arxiv.org/pdf/1103.3089v1)

> We study a bandit problem where observations from each arm have an exponential family distribution and different arms are assigned independent conjugate priors. At each of n stages, one arm is to be selected based on past observations. The goal is to find a strategy that maximizes the expected discounted sum of the $n$ observations. Two structural results hold in broad generality: (i) for a fixed prior weight, an arm becomes more desirable as its prior mean increases; (ii) for a fixed prior mean, an arm becomes more desirable as its prior weight decreases. These generalize and unify several results in the literature concerning specific problems including Bernoulli and normal bandits. The second result captures an aspect of the exploration-exploitation dilemma in precise terms: given the same immediate payoff, the less one knows about an arm, the more desirable it becomes because there remains more information to be gained when selecting that arm. For Bernoulli and normal bandits we also obtain extensions to nonconjugate priors.

</details>

<details>

<summary>2011-03-17 20:05:28 - Approximating Probability Densities by Iterated Laplace Approximations</summary>

- *Björn Bornkamp*

- `1103.3508v1` - [abs](http://arxiv.org/abs/1103.3508v1) - [pdf](http://arxiv.org/pdf/1103.3508v1)

> The Laplace approximation is an old, but frequently used method to approximate integrals for Bayesian calculations. In this paper we develop an extension of the Laplace approximation, by applying it iteratively to the residual, i.e., the difference between the current approximation and the true function. The final approximation is thus a linear combination of multivariate normal densities, where the coefficients are chosen to achieve a good fit to the target distribution. We illustrate on real and artificial examples that the proposed procedure is a computationally efficient alternative to current approaches for approximation of multivariate probability densities. The R-package iterLap implementing the methods described in this article is available from the CRAN servers.

</details>

<details>

<summary>2011-03-22 08:38:28 - Coupling optional Pólya trees and the two sample problem</summary>

- *Li Ma, Wing H. Wong*

- `1011.1253v4` - [abs](http://arxiv.org/abs/1011.1253v4) - [pdf](http://arxiv.org/pdf/1011.1253v4)

> Testing and characterizing the difference between two data samples is of fundamental interest in statistics. Existing methods such as Kolmogorov-Smirnov and Cramer-von-Mises tests do not scale well as the dimensionality increases and provides no easy way to characterize the difference should it exist. In this work, we propose a theoretical framework for inference that addresses these challenges in the form of a prior for Bayesian nonparametric analysis. The new prior is constructed based on a random-partition-and-assignment procedure similar to the one that defines the standard optional P\'olya tree distribution, but has the ability to generate multiple random distributions jointly. These random probability distributions are allowed to "couple", that is to have the same conditional distribution, on subsets of the sample space. We show that this "coupling optional P\'olya tree" prior provides a convenient and effective way for both the testing of two sample difference and the learning of the underlying structure of the difference. In addition, we discuss some practical issues in the computational implementation of this prior and provide several numerical examples to demonstrate its work.

</details>

<details>

<summary>2011-03-24 20:17:17 - A Generic Multivariate Distribution for Counting Data</summary>

- *Marcos Capistrán, J. Andrés Christen*

- `1103.4866v1` - [abs](http://arxiv.org/abs/1103.4866v1) - [pdf](http://arxiv.org/pdf/1103.4866v1)

> Motivated by the need, in some Bayesian likelihood free inference problems, of imputing a multivariate counting distribution based on its vector of means and variance-covariance matrix, we define a generic multivariate discrete distribution. Based on blending the Binomial, Poisson and Negative-Binomial distributions, and using a normal multivariate copula, the required distribution is defined. This distribution tends to the Multivariate Normal for large counts and has an approximate pmf version that is quite simple to evaluate.

</details>

<details>

<summary>2011-03-25 09:12:03 - Group Lasso for high dimensional sparse quantile regression models</summary>

- *Kengo Kato*

- `1103.1458v2` - [abs](http://arxiv.org/abs/1103.1458v2) - [pdf](http://arxiv.org/pdf/1103.1458v2)

> This paper studies the statistical properties of the group Lasso estimator for high dimensional sparse quantile regression models where the number of explanatory variables (or the number of groups of explanatory variables) is possibly much larger than the sample size while the number of variables in "active" groups is sufficiently small. We establish a non-asymptotic bound on the $\ell_{2}$-estimation error of the estimator. This bound explains situations under which the group Lasso estimator is potentially superior/inferior to the $\ell_{1}$-penalized quantile regression estimator in terms of the estimation error. We also propose a data-dependent choice of the tuning parameter to make the method more practical, by extending the original proposal of Belloni and Chernozhukov (2011) for the $\ell_{1}$-penalized quantile regression estimator. As an application, we analyze high dimensional additive quantile regression models. We show that under a set of suitable regularity conditions, the group Lasso estimator can attain the convergence rate arbitrarily close to the oracle rate. Finally, we conduct simulations experiments to examine our theoretical results.

</details>

<details>

<summary>2011-03-28 16:10:24 - Parameter Estimation for Hidden Markov Models with Intractable Likelihoods</summary>

- *Thomas A. Dean, Sumeetpal S. Singh, Ajay Jasra, Gareth W. Peters*

- `1103.5399v1` - [abs](http://arxiv.org/abs/1103.5399v1) - [pdf](http://arxiv.org/pdf/1103.5399v1)

> Approximate Bayesian computation (ABC) is a popular technique for approximating likelihoods and is often used in parameter estimation when the likelihood functions are analytically intractable. Although the use of ABC is widespread in many fields, there has been little investigation of the theoretical properties of the resulting estimators. In this paper we give a theoretical analysis of the asymptotic properties of ABC based maximum likelihood parameter estimation for hidden Markov models. In particular, we derive results analogous to those of consistency and asymptotic normality for standard maximum likelihood estimation. We also discuss how Sequential Monte Carlo methods provide a natural method for implementing likelihood based ABC procedures.

</details>

<details>

<summary>2011-03-29 00:17:16 - Unification of Maximum Entropy and Bayesian Inference via Plausible Reasoning</summary>

- *Alexis Akira Toda*

- `1103.2411v4` - [abs](http://arxiv.org/abs/1103.2411v4) - [pdf](http://arxiv.org/pdf/1103.2411v4)

> This paper modifies Jaynes's axioms of plausible reasoning and derives the minimum relative entropy principle, Bayes's rule, as well as maximum likelihood from first principles. The new axioms, which I call the Optimum Information Principle, is applicable whenever the decision maker is given the data and the relevant background information. These axioms provide an answer to the question "why maximize entropy when faced with incomplete information?"

</details>

<details>

<summary>2011-03-29 10:02:33 - Probability boxes on totally preordered spaces for multivariate modelling</summary>

- *Matthias C. M. Troffaes, Sebastien Destercke*

- `1103.1805v2` - [abs](http://arxiv.org/abs/1103.1805v2) - [pdf](http://arxiv.org/pdf/1103.1805v2)

> A pair of lower and upper cumulative distribution functions, also called probability box or p-box, is among the most popular models used in imprecise probability theory. They arise naturally in expert elicitation, for instance in cases where bounds are specified on the quantiles of a random variable, or when quantiles are specified only at a finite number of points. Many practical and formal results concerning p-boxes already exist in the literature. In this paper, we provide new efficient tools to construct multivariate p-boxes and develop algorithms to draw inferences from them. For this purpose, we formalise and extend the theory of p-boxes using Walley's behavioural theory of imprecise probabilities, and heavily rely on its notion of natural extension and existing results about independence modeling. In particular, we allow p-boxes to be defined on arbitrary totally preordered spaces, hence thereby also admitting multivariate p-boxes via probability bounds over any collection of nested sets. We focus on the cases of independence (using the factorization property), and of unknown dependence (using the Fr\'echet bounds), and we show that our approach extends the probabilistic arithmetic of Williamson and Downs. Two design problems---a damped oscillator, and a river dike---demonstrate the practical feasibility of our results.

</details>

<details>

<summary>2011-03-29 17:02:35 - Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments</summary>

- *Yi Sun, Faustino Gomez, Juergen Schmidhuber*

- `1103.5708v1` - [abs](http://arxiv.org/abs/1103.5708v1) - [pdf](http://arxiv.org/pdf/1103.5708v1)

> To maximize its success, an AGI typically needs to explore its initially unknown world. Is there an optimal way of doing so? Here we derive an affirmative answer for a broad class of environments.

</details>

<details>

<summary>2011-03-31 14:55:30 - Bias-reduced extreme quantiles estimators of Weibull-tail distributions</summary>

- *Jean Diebolt, Laurent Gardes, Stéphane Girard, Armelle Guillou*

- `1103.6204v1` - [abs](http://arxiv.org/abs/1103.6204v1) - [pdf](http://arxiv.org/pdf/1103.6204v1)

> In this paper, we consider the problem of estimating an extreme quantile of a Weibull tail-distribution. The new extreme quantile estimator has a reduced bias compared to the more classical ones proposed in the literature. It is based on an exponential regression model that was introduced in Diebolt et al. (2008). The asymptotic normality of the extreme quantile estimator is established. We also introduce an adaptive selection procedure to determine the number of upper order statistics to be used. A simulation study as well as an application to a real data set are provided in order to prove the efficiency of the above mentioned methods.

</details>

<details>

<summary>2011-03-31 15:36:20 - Quasi-conjugate Bayes estimates for GPD parameters and application to heavy tails modelling</summary>

- *Jean Diebolt, Mhamed El-Aroui, Myriam Garrido, Stéphane Girard*

- `1103.6216v1` - [abs](http://arxiv.org/abs/1103.6216v1) - [pdf](http://arxiv.org/pdf/1103.6216v1)

> We present a quasi-conjugate Bayes approach for estimating Generalized Pareto Distribution (GPD) parameters, distribution tails and extreme quantiles within the Peaks-Over-Threshold framework. Damsleth conjugate Bayes structure on Gamma distributions is transfered to GPD. Posterior estimates are then computed by Gibbs samplers with Hastings-Metropolis steps. Accurate Bayes credibility intervals are also defined, they provide assessment of the quality of the extreme events estimates. An empirical Bayesian method is used in this work, but the suggested approach could incorporate prior information. It is shown that the obtained quasi-conjugate Bayes estimators compare well with the GPD standard estimators when simulated and real data sets are studied.

</details>


## 2011-04

<details>

<summary>2011-04-01 13:27:57 - Functional nonparametric estimation of conditional extreme quantiles</summary>

- *L. Gardes, S. Girard, A. Lekina*

- `1104.0166v1` - [abs](http://arxiv.org/abs/1104.0166v1) - [pdf](http://arxiv.org/pdf/1104.0166v1)

> We address the estimation of quantiles from heavy-tailed distributions when functional covariate information is available and in the case where the order of the quantile converges to one as the sample size increases. Such "extreme" quantiles can be located in the range of the data or near and even beyond the boundary of the sample, depending on the convergence rate of their order to one. Nonparametric estimators of these functional extreme quantiles are introduced, their asymptotic distributions are established and their finite sample behavior is investigated.

</details>

<details>

<summary>2011-04-05 08:19:20 - Comparison of Weibull tail-coefficient estimators</summary>

- *Laurent Gardes, Stéphane Girard*

- `1104.0764v1` - [abs](http://arxiv.org/abs/1104.0764v1) - [pdf](http://arxiv.org/pdf/1104.0764v1)

> We address the problem of estimating the Weibull tail-coefficient which is the regular variation exponent of the inverse failure rate function. We propose a family of estimators of this coefficient and an associate extreme quantile estimator. Their asymptotic normality are established and their asymptotic mean-square errors are compared. The results are illustrated on some finite sample situations.

</details>

<details>

<summary>2011-04-07 09:08:19 - DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model</summary>

- *Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvarinen, Yoshinobu Kawahara, Takashi Washio, Patrik O. Hoyer, Kenneth Bollen*

- `1101.2489v3` - [abs](http://arxiv.org/abs/1101.2489v3) - [pdf](http://arxiv.org/pdf/1101.2489v3)

> Structural equation models and Bayesian networks have been widely used to analyze causal relations between continuous variables. In such frameworks, linear acyclic models are typically used to model the data-generating process of variables. Recently, it was shown that use of non-Gaussianity identifies the full structure of a linear acyclic model, i.e., a causal ordering of variables and their connection strengths, without using any prior knowledge on the network structure, which is not the case with conventional methods. However, existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a finite number of steps. In this paper, we propose a new direct method to estimate a causal ordering and connection strengths based on non-Gaussianity.   In contrast to the previous methods, our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small fixed number of steps if the data strictly follows the model.

</details>

<details>

<summary>2011-04-12 13:45:16 - From EM to Data Augmentation: The Emergence of MCMC Bayesian Computation in the 1980s</summary>

- *Martin A. Tanner, Wing H. Wong*

- `1104.2210v1` - [abs](http://arxiv.org/abs/1104.2210v1) - [pdf](http://arxiv.org/pdf/1104.2210v1)

> It was known from Metropolis et al. [J. Chem. Phys. 21 (1953) 1087--1092] that one can sample from a distribution by performing Monte Carlo simulation from a Markov chain whose equilibrium distribution is equal to the target distribution. However, it took several decades before the statistical community embraced Markov chain Monte Carlo (MCMC) as a general computational tool in Bayesian inference. The usual reasons that are advanced to explain why statisticians were slow to catch on to the method include lack of computing power and unfamiliarity with the early dynamic Monte Carlo papers in the statistical physics literature. We argue that there was a deeper reason, namely, that the structure of problems in the statistical mechanics and those in the standard statistical literature are different. To make the methods usable in standard Bayesian problems, one had to exploit the power that comes from the introduction of judiciously chosen auxiliary variables and collective moves. This paper examines the development in the critical period 1980--1990, when the ideas of Markov chain simulation from the statistical physics literature and the latent variable formulation in maximum likelihood computation (i.e., EM algorithm) came together to spark the widespread application of MCMC methods in Bayesian computation.

</details>

<details>

<summary>2011-04-13 09:25:15 - Appropriate Methodology of Statistical Tests According to Prior Probability and Required Objectivity</summary>

- *Tomokazu Konishi*

- `1104.2424v1` - [abs](http://arxiv.org/abs/1104.2424v1) - [pdf](http://arxiv.org/pdf/1104.2424v1)

> In contrast to its common definition and calculation, interpretation of p-values diverges among statisticians. Since p-value is the basis of various methodologies, this divergence has led to a variety of test methodologies and evaluations of test results. This chaotic situation has complicated the application of tests and decision processes. Here, the origin of the divergence is found in the prior probability of the test. Effects of difference in Pr(H0 = true) on the character of p-values are investigated by comparing real microarray data and its artificial imitations as subjects of Student's t-tests. Also, the importance of the prior probability is discussed in terms of the applicability of Bayesian approaches. Suitable methodology is found in accordance with the prior probability and purpose of the test.

</details>

<details>

<summary>2011-04-13 19:34:21 - Constructing Summary Statistics for Approximate Bayesian Computation: Semi-automatic ABC</summary>

- *Paul Fearnhead, Dennis Prangle*

- `1004.1112v2` - [abs](http://arxiv.org/abs/1004.1112v2) - [pdf](http://arxiv.org/pdf/1004.1112v2)

> Many modern statistical applications involve inference for complex stochastic models, where it is easy to simulate from the models, but impossible to calculate likelihoods. Approximate Bayesian computation (ABC) is a method of inference for such models. It replaces calculation of the likelihood by a step which involves simulating artificial data for different parameter values, and comparing summary statistics of the simulated data to summary statistics of the observed data. Here we show how to construct appropriate summary statistics for ABC in a semi-automatic manner. We aim for summary statistics which will enable inference about certain parameters of interest to be as accurate as possible. Theoretical results show that optimal summary statistics are the posterior means of the parameters. While these cannot be calculated analytically, we use an extra stage of simulation to estimate how the posterior means vary as a function of the data; and then use these estimates of our summary statistics within ABC. Empirical results show that our approach is a robust method for choosing summary statistics, that can result in substantially more accurate ABC analyses than the ad-hoc choices of summary statistics proposed in the literature. We also demonstrate advantages over two alternative methods of simulation-based inference.

</details>

<details>

<summary>2011-04-14 11:55:44 - Bayesian methods to overcome the winner's curse in genetic studies</summary>

- *Lizhen Xu, Radu V. Craiu, Lei Sun*

- `0907.2770v2` - [abs](http://arxiv.org/abs/0907.2770v2) - [pdf](http://arxiv.org/pdf/0907.2770v2)

> Parameter estimates for associated genetic variants, report ed in the initial discovery samples, are often grossly inflated compared to the values observed in the follow-up replication samples. This type of bias is a consequence of the sequential procedure in which the estimated effect of an associated genetic marker must first pass a stringent significance threshold. We propose a hierarchical Bayes method in which a spike-and-slab prior is used to account for the possibility that the significant test result may be due to chance. We examine the robustness of the method using different priors corresponding to different degrees of confidence in the testing results and propose a Bayesian model averaging procedure to combine estimates produced by different models. The Bayesian estimators yield smaller variance compared to the conditional likelihood estimator and outperform the latter in studies with low power. We investigate the performance of the method with simulations and applications to four real data examples.

</details>

<details>

<summary>2011-04-14 16:44:18 - A frequentist two-sample test based on Bayesian model selection</summary>

- *Pietro Berkes, Jozsef Fiser*

- `1104.2826v1` - [abs](http://arxiv.org/abs/1104.2826v1) - [pdf](http://arxiv.org/pdf/1104.2826v1)

> Despite their importance in supporting experimental conclusions, standard statistical tests are often inadequate for research areas, like the life sciences, where the typical sample size is small and the test assumptions difficult to verify. In such conditions, standard tests tend to be overly conservative, and fail thus to detect significant effects in the data. Here we define a novel statistical test for the two-sample problem. Several characteristics make it an attractive alternative to classical two-sample tests: 1) It is based on Bayesian model selection, and thus takes into account uncertainty about the model's parameters, mitigating the problem of small samples size; 2) The null hypothesis is compared with several alternative hypotheses, making the test suitable in different experimental scenarios; 3) The test is constructed as a frequentist test, and defines significance with the conventional bound on Type I errors. We analyze the power of the test and find that it is higher than the power of other standard options, like the t-test (up to 25% higher) for a wide range of sample and effect sizes, and is at most 1% lower when the assumptions of the t-test are perfectly matched. We discuss and evaluate two variants of the test, that define different prior distributions over the parameters of the hypotheses.

</details>

<details>

<summary>2011-04-15 06:03:52 - Bayesian inference for queueing networks and modeling of internet services</summary>

- *Charles Sutton, Michael I. Jordan*

- `1001.3355v3` - [abs](http://arxiv.org/abs/1001.3355v3) - [pdf](http://arxiv.org/pdf/1001.3355v3)

> Modern Internet services, such as those at Google, Yahoo!, and Amazon, handle billions of requests per day on clusters of thousands of computers. Because these services operate under strict performance requirements, a statistical understanding of their performance is of great practical interest. Such services are modeled by networks of queues, where each queue models one of the computers in the system. A key challenge is that the data are incomplete, because recording detailed information about every request to a heavily used system can require unacceptable overhead. In this paper we develop a Bayesian perspective on queueing models in which the arrival and departure times that are not observed are treated as latent variables. Underlying this viewpoint is the observation that a queueing model defines a deterministic transformation between the data and a set of independent variables called the service times. With this viewpoint in hand, we sample from the posterior distribution over missing data and model parameters using Markov chain Monte Carlo. We evaluate our framework on data from a benchmark Web application. We also present a simple technique for selection among nested queueing models. We are unaware of any previous work that considers inference in networks of queues in the presence of missing data.

</details>

<details>

<summary>2011-04-16 19:45:40 - Inferences from prior-based loss functions</summary>

- *Michael Evans, Gun Ho Jang*

- `1104.3258v1` - [abs](http://arxiv.org/abs/1104.3258v1) - [pdf](http://arxiv.org/pdf/1104.3258v1)

> Inferences that arise from loss functions determined by the prior are considered and it is shown that these lead to limiting Bayes rules that are closely connected with likelihood. The procedures obtained via these loss functions are invariant under reparameterizations and are Bayesian unbiased or limits of Bayesian unbiased inferences. These inferences serve as well-supported alternatives to MAP-based inferences.

</details>

<details>

<summary>2011-04-18 12:44:08 - A dynamic Bayesian nonlinear mixed-effects model of HIV response incorporating medication adherence, drug resistance and covariates</summary>

- *Yangxin Huang, Hulin Wu, Jeanne Holden-Wiltse, Edward P. Acosta*

- `1104.3464v1` - [abs](http://arxiv.org/abs/1104.3464v1) - [pdf](http://arxiv.org/pdf/1104.3464v1)

> HIV dynamic studies have contributed significantly to the understanding of HIV pathogenesis and antiviral treatment strategies for AIDS patients. Establishing the relationship of virologic responses with clinical factors and covariates during long-term antiretroviral (ARV) therapy is important to the development of effective treatments. Medication adherence is an important predictor of the effectiveness of ARV treatment, but an appropriate determinant of adherence rate based on medication event monitoring system (MEMS) data is critical to predict virologic outcomes. The primary objective of this paper is to investigate the effects of a number of summary determinants of MEMS adherence rates on virologic response measured repeatedly over time in HIV-infected patients. We developed a mechanism-based differential equation model with consideration of drug adherence, interacted by virus susceptibility to drug and baseline characteristics, to characterize the long-term virologic responses after initiation of therapy. This model fully integrates viral load, MEMS adherence, drug resistance and baseline covariates into the data analysis. In this study we employed the proposed model and associated Bayesian nonlinear mixed-effects modeling approach to assess how to efficiently use the MEMS adherence data for prediction of virologic response, and to evaluate the predicting power of each summary metric of the MEMS adherence rates.

</details>

<details>

<summary>2011-04-18 13:00:46 - Free Energy Methods for Bayesian Inference: Efficient Exploration of Univariate Gaussian Mixture Posteriors</summary>

- *Nicolas Chopin, Tony Lelievre, Gabriel Stoltz*

- `1003.0428v4` - [abs](http://arxiv.org/abs/1003.0428v4) - [pdf](http://arxiv.org/pdf/1003.0428v4)

> Because of their multimodality, mixture posterior distributions are difficult to sample with standard Markov chain Monte Carlo (MCMC) methods. We propose a strategy to enhance the sampling of MCMC in this context, using a biasing procedure which originates from computational Statistical Physics. The principle is first to choose a "reaction coordinate", that is, a "direction" in which the target distribution is multimodal. In a second step, the marginal log-density of the reaction coordinate with respect to the posterior distribution is estimated; minus this quantity is called "free energy" in the computational Statistical Physics literature. To this end, we use adaptive biasing Markov chain algorithms which adapt their targeted invariant distribution on the fly, in order to overcome sampling barriers along the chosen reaction coordinate. Finally, we perform an importance sampling step in order to remove the bias and recover the true posterior. The efficiency factor of the importance sampling step can easily be estimated \emph{a priori} once the bias is known, and appears to be rather large for the test cases we considered. A crucial point is the choice of the reaction coordinate. One standard choice (used for example in the classical Wang-Landau algorithm) is minus the log-posterior density. We discuss other choices. We show in particular that the hyper-parameter that determines the order of magnitude of the variance of each component is both a convenient and an efficient reaction coordinate. We also show how to adapt the method to compute the evidence (marginal likelihood) of a mixture model. We illustrate our approach by analyzing two real data sets.

</details>

<details>

<summary>2011-04-22 02:27:45 - Intent Inference and Syntactic Tracking with GMTI Measurements</summary>

- *Alex Wang, Vikram Krishnamurthy, Bhashyam Balaji*

- `1104.4376v1` - [abs](http://arxiv.org/abs/1104.4376v1) - [pdf](http://arxiv.org/pdf/1104.4376v1)

> In conventional target tracking systems, human operators use the estimated target tracks to make higher level inference of the target behaviour/intent. This paper develops syntactic filtering algorithms that assist human operators by extracting spatial patterns from target tracks to identify suspicious/anomalous spatial trajectories. The targets' spatial trajectories are modeled by a stochastic context free grammar (SCFG) and a switched mode state space model. Bayesian filtering algorithms for stochastic context free grammars are presented for extracting the syntactic structure and illustrated for a ground moving target indicator (GMTI) radar example. The performance of the algorithms is tested with the experimental data collected using DRDC Ottawa's X-band Wideband Experimental Airborne Radar (XWEAR).

</details>

<details>

<summary>2011-04-22 06:47:28 - Limit theorems for functions of marginal quantiles</summary>

- *G. Jogesh Babu, Zhidong Bai, Kwok Pui Choi, Vasudevan Mangalam*

- `1104.4396v1` - [abs](http://arxiv.org/abs/1104.4396v1) - [pdf](http://arxiv.org/pdf/1104.4396v1)

> Multivariate distributions are explored using the joint distributions of marginal sample quantiles. Limit theory for the mean of a function of order statistics is presented. The results include a multivariate central limit theorem and a strong law of large numbers. A result similar to Bahadur's representation of quantiles is established for the mean of a function of the marginal quantiles. In particular, it is shown that \[\sqrt{n}\Biggl(\frac{1}{n}\sum_{i=1}^n\phi\bigl(X_{n:i}^{(1)},...,X_{n:i}^{(d)}\bigr)-\bar{\gamma}\Biggr)=\frac{1}{\sqrt{n}}\sum_{i=1}^nZ_{n,i}+\mathrm{o}_P(1)\] as $n\rightarrow\infty$, where $\bar{\gamma}$ is a constant and $Z_{n,i}$ are i.i.d. random variables for each $n$. This leads to the central limit theorem. Weak convergence to a Gaussian process using equicontinuity of functions is indicated. The results are established under very general conditions. These conditions are shown to be satisfied in many commonly occurring situations.

</details>

<details>

<summary>2011-04-28 02:48:08 - Iterative Reweighted Algorithms for Sparse Signal Recovery with Temporally Correlated Source Vectors</summary>

- *Zhilin Zhang, Bhaskar D. Rao*

- `1104.5280v1` - [abs](http://arxiv.org/abs/1104.5280v1) - [pdf](http://arxiv.org/pdf/1104.5280v1)

> Iterative reweighted algorithms, as a class of algorithms for sparse signal recovery, have been found to have better performance than their non-reweighted counterparts. However, for solving the problem of multiple measurement vectors (MMVs), all the existing reweighted algorithms do not account for temporal correlation among source vectors and thus their performance degrades significantly in the presence of correlation. In this work we propose an iterative reweighted sparse Bayesian learning (SBL) algorithm exploiting the temporal correlation, and motivated by it, we propose a strategy to improve existing reweighted $\ell_2$ algorithms for the MMV problem, i.e. replacing their row norms with Mahalanobis distance measure. Simulations show that the proposed reweighted SBL algorithm has superior performance, and the proposed improvement strategy is effective for existing reweighted $\ell_2$ algorithms.

</details>


## 2011-05

<details>

<summary>2011-05-02 09:15:18 - Deviance Information Criteria for Model Selection in Approximate Bayesian Computation</summary>

- *Olivier Francois, Guillaume Laval*

- `1105.0269v1` - [abs](http://arxiv.org/abs/1105.0269v1) - [pdf](http://arxiv.org/pdf/1105.0269v1)

> Approximate Bayesian computation (ABC) is a class of algorithmic methods in Bayesian inference using statistical summaries and computer simulations. ABC has become popular in evolutionary genetics and in other branches of biology. However model selection under ABC algorithms has been a subject of intense debate during the recent years. Here we propose novel approaches to model selection based on posterior predictive distributions and approximations of the deviance. We argue that this framework can settle some contradictions between the computation of model probabilities and posterior predictive checks using ABC posterior distributions. A simulation study and an analysis of a resequencing data set of human DNA show that the deviance criteria lead to sensible results in a number of model choice problems of interest to population geneticists.

</details>

<details>

<summary>2011-05-03 10:55:55 - HIV dynamics and natural history studies: Joint modeling with doubly interval-censored event time and infrequent longitudinal data</summary>

- *Li Su, Joseph W. Hogan*

- `1105.0543v1` - [abs](http://arxiv.org/abs/1105.0543v1) - [pdf](http://arxiv.org/pdf/1105.0543v1)

> Hepatitis C virus (HCV) coinfection has become one of the most challenging clinical situations to manage in HIV-infected patients. Recently the effect of HCV coinfection on HIV dynamics following initiation of highly active antiretroviral therapy (HAART) has drawn considerable attention. Post-HAART HIV dynamics are commonly studied in short-term clinical trials with frequent data collection design. For example, the elimination process of plasma virus during treatment is closely monitored with daily assessments in viral dynamics studies of AIDS clinical trials. In this article instead we use infrequent cohort data from long-term natural history studies and develop a model for characterizing post-HAART HIV dynamics and their associations with HCV coinfection. Specifically, we propose a joint model for doubly interval-censored data for the time between HAART initiation and viral suppression, and the longitudinal CD4 count measurements relative to the viral suppression. Inference is accomplished using a fully Bayesian approach. Doubly interval-censored data are modeled semiparametrically by Dirichlet process priors and Bayesian penalized splines are used for modeling population-level and individual-level mean CD4 count profiles. We use the proposed methods and data from the HIV Epidemiology Research Study (HERS) to investigate the effect of HCV coinfection on the response to HAART.

</details>

<details>

<summary>2011-05-04 08:30:58 - Variational Bayes approach for model aggregation in unsupervised classification with Markovian dependency</summary>

- *Stevenn Volant, Marie-Laure Martin Magniette, Stéphane Robin*

- `1105.0760v1` - [abs](http://arxiv.org/abs/1105.0760v1) - [pdf](http://arxiv.org/pdf/1105.0760v1)

> We consider a binary unsupervised classification problem where each observation is associated with an unobserved label that we want to retrieve. More precisely, we assume that there are two groups of observation: normal and abnormal. The `normal' observations are coming from a known distribution whereas the distribution of the `abnormal' observations is unknown. Several models have been developed to fit this unknown distribution. In this paper, we propose an alternative based on a mixture of Gaussian distributions. The inference is done within a variational Bayesian framework and our aim is to infer the posterior probability of belonging to the class of interest. To this end, it makes no sense to estimate the mixture component number since each mixture model provides more or less relevant information to the posterior probability estimation. By computing a weighted average (named aggregated estimator) over the model collection, Bayesian Model Averaging (BMA) is one way of combining models in order to account for information provided by each model. The aim is then the estimation of the weights and the posterior probability for one specific model. In this work, we derive optimal approximations of these quantities from the variational theory and propose other approximations of the weights. To perform our method, we consider that the data are dependent (Markovian dependency) and hence we consider a Hidden Markov Model. A simulation study is carried out to evaluate the accuracy of the estimates in terms of classification. We also present an application to the analysis of public health surveillance systems.

</details>

<details>

<summary>2011-05-13 08:07:26 - Bayesian analysis of variable-order, reversible Markov chains</summary>

- *Sergio Bacallado*

- `1105.2640v1` - [abs](http://arxiv.org/abs/1105.2640v1) - [pdf](http://arxiv.org/pdf/1105.2640v1)

> We define a conjugate prior for the reversible Markov chain of order $r$. The prior arises from a partially exchangeable reinforced random walk, in the same way that the Beta distribution arises from the exchangeable Poly\'{a} urn. An extension to variable-order Markov chains is also derived. We show the utility of this prior in testing the order and estimating the parameters of a reversible Markov model.

</details>

<details>

<summary>2011-05-16 00:23:49 - Fast approximate inference with INLA: the past, the present and the future</summary>

- *Daniel Simpson, Finn Lindgren, Håvard Rue*

- `1105.2982v1` - [abs](http://arxiv.org/abs/1105.2982v1) - [pdf](http://arxiv.org/pdf/1105.2982v1)

> Latent Gaussian models are an extremely popular, flexible class of models. Bayesian inference for these models is, however, tricky and time consuming. Recently, Rue, Martino and Chopin introduced the Integrated Nested Laplace Approximation (INLA) method for deterministic fast approximate inference. In this paper, we outline the INLA approximation and its related R package. We will discuss the newer components of the r-INLA program as well as some possible extensions.

</details>

<details>

<summary>2011-05-16 08:44:09 - Kernel estimators of asymptotic variance for adaptive Markov chain Monte Carlo</summary>

- *Yves F. Atchadé*

- `0911.1164v2` - [abs](http://arxiv.org/abs/0911.1164v2) - [pdf](http://arxiv.org/pdf/0911.1164v2)

> We study the asymptotic behavior of kernel estimators of asymptotic variances (or long-run variances) for a class of adaptive Markov chains. The convergence is studied both in $L^p$ and almost surely. The results also apply to Markov chains and improve on the existing literature by imposing weaker conditions. We illustrate the results with applications to the $\operatorname {GARCH}(1,1)$ Markov model and to an adaptive MCMC algorithm for Bayesian logistic regression.

</details>

<details>

<summary>2011-05-18 07:39:58 - Delta method in large deviations and moderate deviations for estimators</summary>

- *Fuqing Gao, Xingqiu Zhao*

- `1105.3552v1` - [abs](http://arxiv.org/abs/1105.3552v1) - [pdf](http://arxiv.org/pdf/1105.3552v1)

> The delta method is a popular and elementary tool for deriving limiting distributions of transformed statistics, while applications of asymptotic distributions do not allow one to obtain desirable accuracy of approximation for tail probabilities. The large and moderate deviation theory can achieve this goal. Motivated by the delta method in weak convergence, a general delta method in large deviations is proposed. The new method can be widely applied to driving the moderate deviations of estimators and is illustrated by examples including the Wilcoxon statistic, the Kaplan--Meier estimator, the empirical quantile processes and the empirical copula function. We also improve the existing moderate deviations results for $M$-estimators and $L$-statistics by the new method. Some applications of moderate deviations to statistical hypothesis testing are provided.

</details>

<details>

<summary>2011-05-18 14:55:53 - Asymptotic Behaviour of Approximate Bayesian Estimators</summary>

- *Thomas A. Dean, Sumeetpal S. Singh*

- `1105.3655v1` - [abs](http://arxiv.org/abs/1105.3655v1) - [pdf](http://arxiv.org/pdf/1105.3655v1)

> Although approximate Bayesian computation (ABC) has become a popular technique for performing parameter estimation when the likelihood functions are analytically intractable there has not as yet been a complete investigation of the theoretical properties of the resulting estimators. In this paper we give a theoretical analysis of the asymptotic properties of ABC based parameter estimators for hidden Markov models and show that ABC based estimators satisfy asymptotically biased versions of the standard results in the statistical literature.

</details>

<details>

<summary>2011-05-19 17:04:35 - PAC-Bayesian Analysis of Martingales and Multiarmed Bandits</summary>

- *Yevgeny Seldin, François Laviolette, John Shawe-Taylor, Jan Peters, Peter Auer*

- `1105.2416v2` - [abs](http://arxiv.org/abs/1105.2416v2) - [pdf](http://arxiv.org/pdf/1105.2416v2)

> We present two alternative ways to apply PAC-Bayesian analysis to sequences of dependent random variables. The first is based on a new lemma that enables to bound expectations of convex functions of certain dependent random variables by expectations of the same functions of independent Bernoulli random variables. This lemma provides an alternative tool to Hoeffding-Azuma inequality to bound concentration of martingale values. Our second approach is based on integration of Hoeffding-Azuma inequality with PAC-Bayesian analysis. We also introduce a way to apply PAC-Bayesian analysis in situation of limited feedback. We combine the new tools to derive PAC-Bayesian generalization and regret bounds for the multiarmed bandit problem. Although our regret bound is not yet as tight as state-of-the-art regret bounds based on other well-established techniques, our results significantly expand the range of potential applications of PAC-Bayesian analysis and introduce a new analysis tool to reinforcement learning and many other fields, where martingales and limited feedback are encountered.

</details>

<details>

<summary>2011-05-23 10:47:25 - An empirical Bayes procedure for the selection of Gaussian graphical models</summary>

- *Sophie Donnet, Jean-Michel Marin*

- `1003.5851v2` - [abs](http://arxiv.org/abs/1003.5851v2) - [pdf](http://arxiv.org/pdf/1003.5851v2)

> A new methodology for model determination in decomposable graphical Gaussian models is developed. The Bayesian paradigm is used and, for each given graph, a hyper inverse Wishart prior distribution on the covariance matrix is considered. This prior distribution depends on hyper-parameters. It is well-known that the models's posterior distribution is sensitive to the specification of these hyper-parameters and no completely satisfactory method is registered. In order to avoid this problem, we suggest adopting an empirical Bayes strategy, that is a strategy for which the values of the hyper-parameters are determined using the data. Typically, the hyper-parameters are fixed to their maximum likelihood estimations. In order to calculate these maximum likelihood estimations, we suggest a Markov chain Monte Carlo version of the Stochastic Approximation EM algorithm. Moreover, we introduce a new sampling scheme in the space of graphs that improves the add and delete proposal of Armstrong et al. (2009). We illustrate the efficiency of this new scheme on simulated and real datasets.

</details>

<details>

<summary>2011-05-23 19:10:03 - PAC-Bayesian Analysis of the Exploration-Exploitation Trade-off</summary>

- *Yevgeny Seldin, Nicolò Cesa-Bianchi, François Laviolette, Peter Auer, John Shawe-Taylor, Jan Peters*

- `1105.4585v1` - [abs](http://arxiv.org/abs/1105.4585v1) - [pdf](http://arxiv.org/pdf/1105.4585v1)

> We develop a coherent framework for integrative simultaneous analysis of the exploration-exploitation and model order selection trade-offs. We improve over our preceding results on the same subject (Seldin et al., 2011) by combining PAC-Bayesian analysis with Bernstein-type inequality for martingales. Such a combination is also of independent interest for studies of multiple simultaneously evolving martingales.

</details>

<details>

<summary>2011-05-24 16:35:59 - Simulation in Statistics</summary>

- *Christian P. Robert*

- `1105.4823v1` - [abs](http://arxiv.org/abs/1105.4823v1) - [pdf](http://arxiv.org/pdf/1105.4823v1)

> Simulation has become a standard tool in statistics because it may be the only tool available for analysing some classes of probabilistic models. We review in this paper simulation tools that have been specifically derived to address statistical challenges and, in particular, recent advances in the areas of adaptive Markov chain Monte Carlo (MCMC) algorithms, and approximate Bayesian calculation (ABC) algorithms.

</details>

<details>

<summary>2011-05-26 01:50:19 - Posterior model probabilities computed from model-specific Gibbs output</summary>

- *Richard J. Barker, William A. Link*

- `1012.0073v2` - [abs](http://arxiv.org/abs/1012.0073v2) - [pdf](http://arxiv.org/pdf/1012.0073v2)

> Reversible jump Markov chain Monte Carlo (RJMCMC) extends ordinary MCMC methods for use in Bayesian multimodel inference. We show that RJMCMC can be implemented as Gibbs sampling with alternating updates of a model indicator and a vector-valued "palette" of parameters denoted $\bm \psi$. Like an artist uses the palette to mix dabs of color for specific needs, we create model-specific parameters from the set available in $\bm \psi$. This description not only removes some of the mystery of RJMCMC, but also provides a basis for fitting models one at a time using ordinary MCMC and computing model weights or Bayes factors by post-processing the Monte Carlo output. We illustrate our procedure using several examples.

</details>

<details>

<summary>2011-05-26 10:46:36 - spikeSlabGAM: Bayesian Variable Selection, Model Choice and Regularization for Generalized Additive Mixed Models in R</summary>

- *Fabian Scheipl*

- `1105.5253v1` - [abs](http://arxiv.org/abs/1105.5253v1) - [pdf](http://arxiv.org/pdf/1105.5253v1)

> The R package spikeSlabGAM implements Bayesian variable selection, model choice, and regularized estimation in (geo-)additive mixed models for Gaussian, binomial, and Poisson responses. Its purpose is to (1) choose an appropriate subset of potential covariates and their interactions, (2) to determine whether linear or more flexible functional forms are required to model the effects of the respective covariates, and (3) to estimate their shapes. Selection and regularization of the model terms is based on a novel spike-and-slab-type prior on coefficient groups associated with parametric and semi-parametric effects.

</details>

<details>

<summary>2011-05-27 12:56:57 - Approximate Bayesian Computational methods</summary>

- *Jean-Michel Marin, Pierre Pudlo, Christian P. Robert, Robin Ryder*

- `1101.0955v2` - [abs](http://arxiv.org/abs/1101.0955v2) - [pdf](http://arxiv.org/pdf/1101.0955v2)

> Also known as likelihood-free methods, approximate Bayesian computational (ABC) methods have appeared in the past ten years as the most satisfactory approach to untractable likelihood problems, first in genetics then in a broader spectrum of applications. However, these methods suffer to some degree from calibration difficulties that make them rather volatile in their implementation and thus render them suspicious to the users of more traditional Monte Carlo methods. In this survey, we study the various improvements and extensions made to the original ABC algorithm over the recent years.

</details>

<details>

<summary>2011-05-28 20:11:25 - Density Estimation and Classification via Bayesian Nonparametric Learning of Affine Subspaces</summary>

- *Abhishek Bhattacharya, Garritt Page, David Dunson*

- `1105.5737v1` - [abs](http://arxiv.org/abs/1105.5737v1) - [pdf](http://arxiv.org/pdf/1105.5737v1)

> It is now practically the norm for data to be very high dimensional in areas such as genetics, machine vision, image analysis and many others. When analyzing such data, parametric models are often too inflexible while nonparametric procedures tend to be non-robust because of insufficient data on these high dimensional spaces. It is often the case with high-dimensional data that most of the variability tends to be along a few directions, or more generally along a much smaller dimensional submanifold of the data space. In this article, we propose a class of models that flexibly learn about this submanifold and its dimension which simultaneously performs dimension reduction. As a result, density estimation is carried out efficiently. When performing classification with a large predictor space, our approach allows the category probabilities to vary nonparametrically with a few features expressed as linear combinations of the predictors. As opposed to many black-box methods for dimensionality reduction, the proposed model is appealing in having clearly interpretable and identifiable parameters. Gibbs sampling methods are developed for posterior computation, and the methods are illustrated in simulated and real data applications.

</details>

<details>

<summary>2011-05-30 07:31:01 - Efficient sampling of high-dimensional Gaussian fields: the non-stationary / non-sparse case</summary>

- *F. Orieux, O. Féron, J. -F. Giovannelli*

- `1105.5887v1` - [abs](http://arxiv.org/abs/1105.5887v1) - [pdf](http://arxiv.org/pdf/1105.5887v1)

> This paper is devoted to the problem of sampling Gaussian fields in high dimension. Solutions exist for two specific structures of inverse covariance : sparse and circulant. The proposed approach is valid in a more general case and especially as it emerges in inverse problems. It relies on a perturbation-optimization principle: adequate stochastic perturbation of a criterion and optimization of the perturbed criterion. It is shown that the criterion minimizer is a sample of the target density. The motivation in inverse problems is related to general (non-convolutive) linear observation models and their resolution in a Bayesian framework implemented through sampling algorithms when existing samplers are not feasible. It finds a direct application in myopic and/or unsupervised inversion as well as in some non-Gaussian inversion. An illustration focused on hyperparameter estimation for super-resolution problems assesses the effectiveness of the proposed approach.

</details>

<details>

<summary>2011-05-30 11:22:44 - Context models on sequences of covers</summary>

- *Christos Dimitrakakis*

- `1005.2263v2` - [abs](http://arxiv.org/abs/1005.2263v2) - [pdf](http://arxiv.org/pdf/1005.2263v2)

> We present a class of models that, via a simple construction, enables exact, incremental, non-parametric, polynomial-time, Bayesian inference of conditional measures. The approach relies upon creating a sequence of covers on the conditioning variable and maintaining a different model for each set within a cover. Inference remains tractable by specifying the probabilistic model in terms of a random walk within the sequence of covers. We demonstrate the approach on problems of conditional density estimation, which, to our knowledge is the first closed-form, non-parametric Bayesian approach to this problem.

</details>

<details>

<summary>2011-05-30 11:52:55 - On multivariate quantiles under partial orders</summary>

- *Alexandre Belloni, Robert L. Winkler*

- `0912.5489v3` - [abs](http://arxiv.org/abs/0912.5489v3) - [pdf](http://arxiv.org/pdf/0912.5489v3)

> This paper focuses on generalizing quantiles from the ordering point of view. We propose the concept of partial quantiles, which are based on a given partial order. We establish that partial quantiles are equivariant under order-preserving transformations of the data, robust to outliers, characterize the probability distribution if the partial order is sufficiently rich, generalize the concept of efficient frontier, and can measure dispersion from the partial order perspective. We also study several statistical aspects of partial quantiles. We provide estimators, associated rates of convergence, and asymptotic distributions that hold uniformly over a continuum of quantile indices. Furthermore, we provide procedures that can restore monotonicity properties that might have been disturbed by estimation error, establish computational complexity bounds, and point out a concentration of measure phenomenon (the latter under independence and the componentwise natural order). Finally, we illustrate the concepts by discussing several theoretical examples and simulations. Empirical applications to compare intake nutrients within diets, to evaluate the performance of investment funds, and to study the impact of policies on tobacco awareness are also presented to illustrate the concepts and their use.

</details>


## 2011-06

<details>

<summary>2011-06-01 21:06:50 - Bayesian Sparsity-Path-Analysis of Genetic Association Signal using Generalized t Priors</summary>

- *Anthony Lee, Francois Caron, Arnaud Doucet, Chris Holmes*

- `1106.0322v1` - [abs](http://arxiv.org/abs/1106.0322v1) - [pdf](http://arxiv.org/pdf/1106.0322v1)

> We explore the use of generalized t priors on regression coefficients to help understand the nature of association signal within "hit regions" of genome-wide association studies. The particular generalized t distribution we adopt is a Student distribution on the absolute value of its argument. For low degrees of freedom we show that the generalized t exhibits 'sparsity-prior' properties with some attractive features over other common forms of sparse priors and includes the well known double-exponential distribution as the degrees of freedom tends to infinity. We pay particular attention to graphical representations of posterior statistics obtained from sparsity-path-analysis (SPA) where we sweep over the setting of the scale (shrinkage / precision) parameter in the prior to explore the space of posterior models obtained over a range of complexities, from very sparse models with all coefficient distributions heavily concentrated around zero, to models with diffuse priors and coefficients distributed around their maximum likelihood estimates. The SPA plots are akin to LASSO plots of maximum a posteriori (MAP) estimates but they characterise the complete marginal posterior distributions of the coefficients plotted as a function of the precision of the prior. Generating posterior distributions over a range of prior precisions is computationally challenging but naturally amenable to sequential Monte Carlo (SMC) algorithms indexed on the scale parameter. We show how SMC simulation on graphic-processing-units (GPUs) provides very efficient inference for SPA. We also present a scale-mixture representation of the generalized t prior that leads to an EM algorithm to obtain MAP estimates should only these be required.

</details>

<details>

<summary>2011-06-03 12:47:57 - Approximate simulation-free Bayesian inference for multiple changepoint models with dependence within segments</summary>

- *Jason Wyse, Nial Friel, Håvard Rue*

- `1011.5038v2` - [abs](http://arxiv.org/abs/1011.5038v2) - [pdf](http://arxiv.org/pdf/1011.5038v2)

> This paper proposes approaches for the analysis of multiple changepoint models when dependency in the data is modelled through a hierarchical Gaussian Markov random field. Integrated nested Laplace approximations are used to approximate data quantities, and an approximate filtering recursions approach is proposed for savings in compuational cost when detecting changepoints. All of these methods are simulation free. Analysis of real data demonstrates the usefulness of the approach in general. The new models which allow for data dependence are compared with conventional models where data within segments is assumed independent.

</details>

<details>

<summary>2011-06-09 14:15:23 - Classification Loss Function for Parameter Ensembles in Bayesian Hierarchical Models</summary>

- *Cedric E. Ginestet, Nicky G. Best, Sylvia Richardson*

- `1105.6322v2` - [abs](http://arxiv.org/abs/1105.6322v2) - [pdf](http://arxiv.org/pdf/1105.6322v2)

> Parameter ensembles or sets of point estimates constitute one of the cornerstones of modern statistical practice. This is especially the case in Bayesian hierarchical models, where different decision-theoretic frameworks can be deployed to summarize such parameter ensembles. The estimation of these parameter ensembles may thus substantially vary depending on which inferential goals are prioritised by the modeller. In this note, we consider the problem of classifying the elements of a parameter ensemble above or below a given threshold. Two threshold classification losses (TCLs) --weighted and unweighted-- are formulated. The weighted TCL can be used to emphasize the estimation of false positives over false negatives or the converse. We prove that the weighted and unweighted TCLs are optimized by the ensembles of unit-specific posterior quantiles and posterior medians, respectively. In addition, we relate these classification loss functions on parameter ensembles to the concepts of posterior sensitivity and specificity. Finally, we find some relationships between the unweighted TCL and the absolute value loss, which explain why both functions are minimized by posterior medians.

</details>

<details>

<summary>2011-06-09 23:43:22 - Exploiting Correlation in Sparse Signal Recovery Problems: Multiple Measurement Vectors, Block Sparsity, and Time-Varying Sparsity</summary>

- *Zhilin Zhang, Bhaskar D. Rao*

- `1105.0725v2` - [abs](http://arxiv.org/abs/1105.0725v2) - [pdf](http://arxiv.org/pdf/1105.0725v2)

> A trend in compressed sensing (CS) is to exploit structure for improved reconstruction performance. In the basic CS model, exploiting the clustering structure among nonzero elements in the solution vector has drawn much attention, and many algorithms have been proposed. However, few algorithms explicitly consider correlation within a cluster. Meanwhile, in the multiple measurement vector (MMV) model correlation among multiple solution vectors is largely ignored. Although several recently developed algorithms consider the exploitation of the correlation, these algorithms need to know a priori the correlation structure, thus limiting their effectiveness in practical problems.   Recently, we developed a sparse Bayesian learning (SBL) algorithm, namely T-SBL, and its variants, which adaptively learn the correlation structure and exploit such correlation information to significantly improve reconstruction performance. Here we establish their connections to other popular algorithms, such as the group Lasso, iterative reweighted $\ell_1$ and $\ell_2$ algorithms, and algorithms for time-varying sparsity. We also provide strategies to improve these existing algorithms.

</details>

<details>

<summary>2011-06-13 02:17:24 - Bayesian Sequential Detection with Phase-Distributed Change Time and Nonlinear Penalty -- A POMDP Approach</summary>

- *Vikram Krishnamurthy*

- `1011.5298v4` - [abs](http://arxiv.org/abs/1011.5298v4) - [pdf](http://arxiv.org/pdf/1011.5298v4)

> We show that the optimal decision policy for several types of Bayesian sequential detection problems has a threshold switching curve structure on the space of posterior distributions. This is established by using lattice programming and stochastic orders in a partially observed Markov decision process (POMDP) framework. A stochastic gradient algorithm is presented to estimate the optimal linear approximation to this threshold curve. We illustrate these results by first considering quickest time detection with phase-type distributed change time and a variance stopping penalty. Then it is proved that the threshold switching curve also arises in several other Bayesian decision problems such as quickest transient detection, exponential delay (risk-sensitive) penalties, stopping time problems in social learning, and multi-agent scheduling in a changing world. Using Blackwell dominance, it is shown that for dynamic decision making problems, the optimal decision policy is lower bounded by a myopic policy. Finally, it is shown how the achievable cost of the optimal decision policy varies with change time distribution by imposing a partial order on transition matrices.

</details>

<details>

<summary>2011-06-14 19:23:07 - abc: an R package for Approximate Bayesian Computation (ABC)</summary>

- *Katalin Csilléry, Olivier François, Michael GB Blum*

- `1106.2793v1` - [abs](http://arxiv.org/abs/1106.2793v1) - [pdf](http://arxiv.org/pdf/1106.2793v1)

> Many recent statistical applications involve inference under complex models, where it is computationally prohibitive to calculate likelihoods but possible to simulate data. Approximate Bayesian Computation (ABC) is devoted to these complex models because it bypasses evaluations of the likelihood function using comparisons between observed and simulated summary statistics. We introduce the R abc package that implements several ABC algorithms for performing parameter estimation and model selection. In particular, the recently developed non-linear heteroscedastic regression methods for ABC are implemented. The abc package also includes a cross-validation tool for measuring the accuracy of ABC estimates, and to calculate the misclassification probabilities when performing model selection. The main functions are accompanied by appropriate summary and plotting tools. Considering an example of demographic inference with population genetics data, we show the potential of the R package.   R is already widely used in bioinformatics and several fields of biology. The R abc package will make the ABC algorithms available to the large number of R users. abc is a freely available R package under the GPL license, and it can be downloaded at http://cran.r-project.org/web/packages/abc/index.html.

</details>

<details>

<summary>2011-06-16 11:06:04 - Pitman-Yor Diffusion Trees</summary>

- *David A. Knowles, Zoubin Ghahramani*

- `1106.2494v2` - [abs](http://arxiv.org/abs/1106.2494v2) - [pdf](http://arxiv.org/pdf/1106.2494v2)

> We introduce the Pitman Yor Diffusion Tree (PYDT) for hierarchical clustering, a generalization of the Dirichlet Diffusion Tree (Neal, 2001) which removes the restriction to binary branching structure. The generative process is described and shown to result in an exchangeable distribution over data points. We prove some theoretical properties of the model and then present two inference methods: a collapsed MCMC sampler which allows us to model uncertainty over tree structures, and a computationally efficient greedy Bayesian EM search algorithm. Both algorithms use message passing on the tree structure. The utility of the model and algorithms is demonstrated on synthetic and real world data, both continuous and binary.

</details>

<details>

<summary>2011-06-16 11:39:36 - Estimation of covariance matrices based on hierarchical inverse-Wishart priors</summary>

- *Mathilde Bouriga, Olivier Féron*

- `1106.3203v1` - [abs](http://arxiv.org/abs/1106.3203v1) - [pdf](http://arxiv.org/pdf/1106.3203v1)

> This paper focuses on Bayesian shrinkage for covariance matrix estimation. We examine posterior properties and frequentist risks of Bayesian estimators based on new hierarchical inverse-Wishart priors. More precisely, we give the existence conditions of the posterior distributions. Advantages in terms of numerical simulations of posteriors are shown. A simulation study illustrates the performance of the estimation procedures under three loss functions for relevant sample sizes and various covariance structures.

</details>

<details>

<summary>2011-06-16 12:56:48 - Bayesian Statistical Pragmatism</summary>

- *Andrew Gelman*

- `1106.3220v1` - [abs](http://arxiv.org/abs/1106.3220v1) - [pdf](http://arxiv.org/pdf/1106.3220v1)

> Discussion of "Statistical Inference: The Big Picture" by R. E. Kass [arXiv:1106.2895]

</details>

<details>

<summary>2011-06-20 21:12:27 - Lack of confidence in ABC model choice</summary>

- *Christian P. Robert, Jean-Marie Cornuet, Jean-Michel Marin, Natesh Pillai*

- `1102.4432v4` - [abs](http://arxiv.org/abs/1102.4432v4) - [pdf](http://arxiv.org/pdf/1102.4432v4)

> Approximate Bayesian computation (ABC) have become a essential tool for the analysis of complex stochastic models. Earlier, Grelaud et al. (2009) advocated the use of ABC for Bayesian model choice in the specific case of Gibbs random fields, relying on a inter-model sufficiency property to show that the approximation was legitimate. Having implemented ABC-based model choice in a wide range of phylogenetic models in the DIY-ABC software (Cornuet et al., 2008), we now present theoretical background as to why a generic use of ABC for model choice is ungrounded, since it depends on an unknown amount of information loss induced by the use of insufficient summary statistics. The approximation error of the posterior probabilities of the models under comparison may thus be unrelated with the computational effort spent in running an ABC algorithm. We then conclude that additional empirical verifications of the performances of the ABC procedure as those available in DIYABC are necessary to conduct model choice.

</details>

<details>

<summary>2011-06-22 09:07:34 - Statistical Inference: The Big Picture</summary>

- *Robert E. Kass*

- `1106.2895v2` - [abs](http://arxiv.org/abs/1106.2895v2) - [pdf](http://arxiv.org/pdf/1106.2895v2)

> Statistics has moved beyond the frequentist-Bayesian controversies of the past. Where does this leave our ability to interpret results? I suggest that a philosophy compatible with statistical practice, labeled here statistical pragmatism, serves as a foundation for inference. Statistical pragmatism is inclusive and emphasizes the assumptions that connect statistical models with observed data. I argue that introductory courses often mischaracterize the process of statistical inference and I propose an alternative "big picture" depiction.

</details>

<details>

<summary>2011-06-23 09:14:36 - Sparse Linear Identifiable Multivariate Modeling</summary>

- *Ricardo Henao, Ole Winther*

- `1004.5265v3` - [abs](http://arxiv.org/abs/1004.5265v3) - [pdf](http://arxiv.org/pdf/1004.5265v3)

> In this paper we consider sparse and identifiable linear latent variable (factor) and linear Bayesian network models for parsimonious analysis of multivariate data. We propose a computationally efficient method for joint parameter and model inference, and model comparison. It consists of a fully Bayesian hierarchy for sparse models using slab and spike priors (two-component delta-function and continuous mixtures), non-Gaussian latent factors and a stochastic search over the ordering of the variables. The framework, which we call SLIM (Sparse Linear Identifiable Multivariate modeling), is validated and bench-marked on artificial and real biological data sets. SLIM is closest in spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in inference, Bayesian network structure learning and model comparison. Experimentally, SLIM performs equally well or better than LiNGAM with comparable computational complexity. We attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identifiability), which is an explicit part of the model. We propose two extensions to the basic i.i.d. linear framework: non-linear dependence on observed variables, called SNIM (Sparse Non-linear Identifiable Multivariate modeling) and allowing for correlations between latent variables, called CSLIM (Correlated SLIM), for the temporal and/or spatial data. The source code and scripts are available from http://cogsys.imm.dtu.dk/slim/.

</details>

<details>

<summary>2011-06-25 16:39:24 - The Empirical Edgeworth Expansion for a Studentized Trimmed Mean</summary>

- *Nadezhda Gribkova, Roelof Helmers*

- `1106.2219v2` - [abs](http://arxiv.org/abs/1106.2219v2) - [pdf](http://arxiv.org/pdf/1106.2219v2)

> We establish the validity of the empirical Edgeworth expansion (EE) for a studentized trimmed mean, under the sole condition that the underlying distribution function of the observations satisfies a local smoothness condition near the two quantiles where the trimming occurs. A simple explicit formula for the N^{-1/2} term (correcting for skewness and bias; N being the sample size) of the EE is given. In particular our result supplements previous work by P. Hall and A.R. Padmanabhan, On the bootstrap and the trimmed mean}, J. of Multivariate Analysis, v. 41 (1992), pp. 132-153. and H. Putter and W.R. van Zwet,   Empirical Edgeworth expansions for symmetric statistics, Ann. Statist., v. 26 (1998), pp. 1540-1569. The proof is based on a U-statistic type approximation and also uses a version of Bahadur's representation for sample quantiles.

</details>

<details>

<summary>2011-06-28 05:28:50 - Construction and evaluation of classifiers for forensic document analysis</summary>

- *Christopher P. Saunders, Linda J. Davis, Andrea C. Lamas, John J. Miller, Donald T. Gantz*

- `1004.0678v2` - [abs](http://arxiv.org/abs/1004.0678v2) - [pdf](http://arxiv.org/pdf/1004.0678v2)

> In this study we illustrate a statistical approach to questioned document examination. Specifically, we consider the construction of three classifiers that predict the writer of a sample document based on categorical data. To evaluate these classifiers, we use a data set with a large number of writers and a small number of writing samples per writer. Since the resulting classifiers were found to have near perfect accuracy using leave-one-out cross-validation, we propose a novel Bayesian-based cross-validation method for evaluating the classifiers.

</details>

<details>

<summary>2011-06-28 09:14:14 - Introduction to Graphical Modelling</summary>

- *Marco Scutari, Korbinian Strimmer*

- `1005.1036v3` - [abs](http://arxiv.org/abs/1005.1036v3) - [pdf](http://arxiv.org/pdf/1005.1036v3)

> The aim of this chapter is twofold. In the first part we will provide a brief overview of the mathematical and statistical foundations of graphical models, along with their fundamental properties, estimation and basic inference procedures. In particular we will develop Markov networks (also known as Markov random fields) and Bayesian networks, which comprise most past and current literature on graphical models. In the second part we will review some applications of graphical models in systems biology.

</details>

<details>

<summary>2011-06-28 09:31:24 - Level sets estimation and Vorob'ev expectation of random compact sets</summary>

- *Philippe Heinrich, Radu Stefan Stoica, Viet Chi Tran*

- `1006.5135v2` - [abs](http://arxiv.org/abs/1006.5135v2) - [pdf](http://arxiv.org/pdf/1006.5135v2)

> The issue of a "mean shape" of a random set $X$ often arises, in particular in image analysis and pattern detection. There is no canonical definition but one possible approach is the so-called Vorob'ev expectation $\E_V(X)$, which is closely linked to quantile sets. In this paper, we propose a consistent and ready to use estimator of $\E_V(X)$ built from independent copies of $X$ with spatial discretization. The control of discretization errors is handled with a mild regularity assumption on the boundary of $X$: a not too large 'box counting' dimension. Some examples are developed and an application to cosmological data is presented.

</details>

<details>

<summary>2011-06-28 19:55:22 - Efficient Gaussian Process Regression for Large Data Sets</summary>

- *Anjishnu Banerjee, David Dunson, Surya Tokdar*

- `1106.5779v1` - [abs](http://arxiv.org/abs/1106.5779v1) - [pdf](http://arxiv.org/pdf/1106.5779v1)

> Gaussian processes (GPs) are widely used in nonparametric regression, classification and spatio-temporal modeling, motivated in part by a rich literature on theoretical properties. However, a well known drawback of GPs that limits their use is the expensive computation, typically O($n^3$) in performing the necessary matrix inversions with $n$ denoting the number of data points. In large data sets, data storage and processing also lead to computational bottlenecks and numerical stability of the estimates and predicted values degrades with $n$. To address these problems, a rich variety of methods have been proposed, with recent options including predictive processes in spatial data analysis and subset of regressors in machine learning. The underlying idea in these approaches is to use a subset of the data, leading to questions of sensitivity to the subset and limitations in estimating fine scale structure in regions that are not well covered by the subset. Motivated by the literature on compressive sensing, we propose an alternative random projection of all the data points onto a lower-dimensional subspace. We demonstrate the superiority of this approach from a theoretical perspective and through the use of simulated and real data examples. Some Keywords: Bayesian; Compressive Sensing; Dimension Reduction; Gaussian Processes; Random Projections; Subset Selection

</details>

<details>

<summary>2011-06-28 20:22:50 - Philosophy and the practice of Bayesian statistics</summary>

- *Andrew Gelman, Cosma Rohilla Shalizi*

- `1006.3868v4` - [abs](http://arxiv.org/abs/1006.3868v4) - [pdf](http://arxiv.org/pdf/1006.3868v4)

> A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science.   Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.

</details>

<details>

<summary>2011-06-29 12:18:45 - Monte Carlo algorithms for model assessment via conflicting summaries</summary>

- *Oliver Ratmann, Pierre Pudlo, Sylvia Richardson, Christian Robert*

- `1106.5919v1` - [abs](http://arxiv.org/abs/1106.5919v1) - [pdf](http://arxiv.org/pdf/1106.5919v1)

> The development of statistical methods and numerical algorithms for model choice is vital to many real-world applications. In practice, the ABC approach can be instrumental for sequential model design; however, the theoretical basis of its use has been questioned. We present a measure-theoretic framework for using the ABC error towards model choice and describe how easily existing rejection, Metropolis-Hastings and sequential importance sampling ABC algorithms are extended for the purpose of model checking. Considering a panel of applications from evolutionary biology to dynamic systems, we discuss the choice of summaries which differs from standard ABC approaches. The methods and algorithms presented here may provide the workhorse machinery for an exploratory approach to ABC model choice, particularly as the application of standard Bayesian tools can prove impossible.

</details>

<details>

<summary>2011-06-29 14:06:42 - Preference elicitation and inverse reinforcement learning</summary>

- *Constantin Rothkopf, Christos Dimitrakakis*

- `1104.5687v2` - [abs](http://arxiv.org/abs/1104.5687v2) - [pdf](http://arxiv.org/pdf/1104.5687v2)

> We state the problem of inverse reinforcement learning in terms of preference elicitation, resulting in a principled (Bayesian) statistical formulation. This generalises previous work on Bayesian inverse reinforcement learning and allows us to obtain a posterior distribution on the agent's preferences, policy and optionally, the obtained reward sequence, from observations. We examine the relation of the resulting approach to other statistical methods for inverse reinforcement learning via analysis and experimental results. We show that preferences can be determined accurately, even if the observed agent's policy is sub-optimal with respect to its own preferences. In that case, significantly improved policies with respect to the agent's preferences are obtained, compared to both other methods and to the performance of the demonstrated policy.

</details>

<details>

<summary>2011-06-29 15:53:06 - Application of Bayesian model inadequacy criterion for multiple data sets to radial velocity models of exoplanet systems</summary>

- *Mikko Tuomi, David Pinfield, Hugh R. A. Jones*

- `1106.5981v1` - [abs](http://arxiv.org/abs/1106.5981v1) - [pdf](http://arxiv.org/pdf/1106.5981v1)

> We present a simple mathematical criterion for determining whether a given statistical model does not describe several independent sets of measurements, or data modes, adequately. We derive this criterion for two data sets and generalise it to several sets by using the Bayesian updating of the posterior probability density. To demonstrate the usage of the criterion, we apply it to observations of exoplanet host stars by re-analysing the radial velocities of HD 217107, Gliese 581, and \u{psion} Andromedae and show that the currently used models are not necessarily adequate in describing the properties of these measurements. We show that while the two data sets of Gliese 581 can be modelled reasonably well, the noise model of HD 217107 needs to be revised. We also reveal some biases in the radial velocities of \u{psion} Andromedae and report updated orbital parameters for the recently proposed 4-planet model. Because of the generality of our criterion, no assumptions are needed on the nature of the measurements, models, or model parameters. The method we propose can be applied to any astronomical problems, as well as outside the field of astronomy, because it is a simple consequence of the Bayes' rule of conditional probabilities.

</details>


## 2011-07

<details>

<summary>2011-07-01 08:25:21 - Considerate Approaches to Achieving Sufficiency for ABC model selection</summary>

- *Chris Barnes, Sarah Filippi, Michael P. H. Stumpf, Thomas Thorne*

- `1106.6281v2` - [abs](http://arxiv.org/abs/1106.6281v2) - [pdf](http://arxiv.org/pdf/1106.6281v2)

> For nearly any challenging scientific problem evaluation of the likelihood is problematic if not impossible. Approximate Bayesian computation (ABC) allows us to employ the whole Bayesian formalism to problems where we can use simulations from a model, but cannot evaluate the likelihood directly. When summary statistics of real and simulated data are compared --- rather than the data directly --- information is lost, unless the summary statistics are sufficient. Here we employ an information-theoretical framework that can be used to construct (approximately) sufficient statistics by combining different statistics until the loss of information is minimized. Such sufficient sets of statistics are constructed for both parameter estimation and model selection problems. We apply our approach to a range of illustrative and real-world model selection problems.

</details>

<details>

<summary>2011-07-04 06:19:55 - Expectiles for subordinated Gaussian processes with applications</summary>

- *Jean-François Coeurjolly, Hedi Kortas*

- `1107.0540v1` - [abs](http://arxiv.org/abs/1107.0540v1) - [pdf](http://arxiv.org/pdf/1107.0540v1)

> In this paper, we introduce a new class of estimators of the Hurst exponent of the fractional Brownian motion (fBm) process. These estimators are based on sample expectiles of discrete variations of a sample path of the fBm process. In order to derive the statistical properties of the proposed estimators, we establish asymptotic results for sample expectiles of subordinated stationary Gaussian processes with unit variance and correlation function satisfying $\rho(i)\sim \kappa|i|^{-\alpha}$ ($\kappa\in \RR$) with $\alpha>0$. Via a simulation study, we demonstrate the relevance of the expectile-based estimation method and show that the suggested estimators are more robust to data rounding than their sample quantile-based counterparts.

</details>

<details>

<summary>2011-07-04 15:37:19 - A Variational Bayes Approach to Decoding in a Phase-Uncertain Digital Receiver</summary>

- *Arijit Das, Anthony Quinn*

- `1107.0662v1` - [abs](http://arxiv.org/abs/1107.0662v1) - [pdf](http://arxiv.org/pdf/1107.0662v1)

> This paper presents a Bayesian approach to symbol and phase inference in a phase-unsynchronized digital receiver. It primarily extends [Quinn 2011] to the multi-symbol case, using the variational Bayes (VB) approximation to deal with the combinatorial complexity of the phase inference in this case. The work provides a fully Bayesian extension of the EM-based framework underlying current turbo-synchronization methods, since it induces a von Mises prior on the time-invariant phase parmeter. As a result, we achieve tractable iterative algorithms with improved robustness in low SNR regimes, compared to the current EM-based approaches. As a corollary to our analysis we also discover the importance of prior regularization in elegantly tackling the significant problem of phase ambiguity.

</details>

<details>

<summary>2011-07-05 17:26:52 - Application of Predictive Model Selection to Coupled Models</summary>

- *Gabriel Terejanu, Todd Oliver, Chris Simmons*

- `1107.0927v1` - [abs](http://arxiv.org/abs/1107.0927v1) - [pdf](http://arxiv.org/pdf/1107.0927v1)

> A predictive Bayesian model selection approach is presented to discriminate coupled models used to predict an unobserved quantity of interest (QoI). The need for accurate predictions arises in a variety of critical applications such as climate, aerospace and defense. A model problem is introduced to study the prediction yielded by the coupling of two physics/sub-components. For each single physics domain, a set of model classes and a set of sensor observations are available. A goal-oriented algorithm using a predictive approach to Bayesian model selection is then used to select the combination of single physics models that best predict the QoI. It is shown that the best coupled model for prediction is the one that provides the most robust predictive distribution for the QoI.

</details>

<details>

<summary>2011-07-06 20:41:05 - Bayesian Inference from Composite Likelihoods, with an Application to Spatial Extremes</summary>

- *Mathieu Ribatet, Daniel Cooley, Anthony C. Davison*

- `0911.5357v2` - [abs](http://arxiv.org/abs/0911.5357v2) - [pdf](http://arxiv.org/pdf/0911.5357v2)

> Composite likelihoods are increasingly used in applications where the full likelihood is analytically unknown or computationally prohibitive. Although the maximum composite likelihood estimator has frequentist properties akin to those of the usual maximum likelihood estimator, Bayesian inference based on composite likelihoods has yet to be explored. In this paper we investigate the use of the Metropolis--Hastings algorithm to compute a pseudo-posterior distribution based on the composite likelihood. Two methodologies for adjusting the algorithm are presented and their performance on approximating the true posterior distribution is investigated using simulated data sets and real data on spatial extremes of rainfall.

</details>

<details>

<summary>2011-07-07 16:46:43 - Bayesian experimental design for the active nitridation of graphite by atomic nitrogen</summary>

- *Gabriel Terejanu, Rochan R. Upadhyay, Kenji Miki*

- `1107.1445v1` - [abs](http://arxiv.org/abs/1107.1445v1) - [pdf](http://arxiv.org/pdf/1107.1445v1)

> The problem of optimal data collection to efficiently learn the model parameters of a graphite nitridation experiment is studied in the context of Bayesian analysis using both synthetic and real experimental data. The paper emphasizes that the optimal design can be obtained as a result of an information theoretic sensitivity analysis. Thus, the preferred design is where the statistical dependence between the model parameters and observables is the highest possible. In this paper, the statistical dependence between random variables is quantified by mutual information and estimated using a k-nearest neighbor based approximation. It is shown, that by monitoring the inference process via measures such as entropy or Kullback-Leibler divergence, one can determine when to stop the data collection process. The methodology is applied to select the most informative designs on both a simulated data set and on an experimental data set, previously published in the literature. It is also shown that the sequential Bayesian analysis used in the experimental design can also be useful in detecting conflicting information between measurements and model predictions.

</details>

<details>

<summary>2011-07-11 15:57:34 - Finding Consensus Bayesian Network Structures</summary>

- *Jose M. Peña*

- `1101.1715v4` - [abs](http://arxiv.org/abs/1101.1715v4) - [pdf](http://arxiv.org/pdf/1101.1715v4)

> Suppose that multiple experts (or learning algorithms) provide us with alternative Bayesian network (BN) structures over a domain, and that we are interested in combining them into a single consensus BN structure. Specifically, we are interested in that the consensus BN structure only represents independences all the given BN structures agree upon and that it has as few parameters associated as possible. In this paper, we prove that there may exist several non-equivalent consensus BN structures and that finding one of them is NP-hard. Thus, we decide to resort to heuristics to find an approximated consensus BN structure. In this paper, we consider the heuristic proposed in \citep{MatzkevichandAbramson1992,MatzkevichandAbramson1993a,MatzkevichandAbramson1993b}. This heuristic builds upon two algorithms, called Methods A and B, for efficiently deriving the minimal directed independence map of a BN structure relative to a given node ordering. Methods A and B are claimed to be correct although no proof is provided (a proof is just sketched). In this paper, we show that Methods A and B are not correct and propose a correction of them.

</details>

<details>

<summary>2011-07-12 13:13:38 - Asymptotic Bayes optimality under sparsity for generally distributed effect sizes under the alternative</summary>

- *Florian Frommlet, Arijit Chakrabarti, Magdalena Murawska, Malgorzata Bogdan*

- `1005.4753v2` - [abs](http://arxiv.org/abs/1005.4753v2) - [pdf](http://arxiv.org/pdf/1005.4753v2)

> Recent results concerning asymptotic Bayes-optimality under sparsity (ABOS) of multiple testing procedures are extended to fairly generally distributed effect sizes under the alternative. An asymptotic framework is considered where both the number of tests m and the sample size m go to infinity, while the fraction p of true alternatives converges to zero. It is shown that under mild restrictions on the loss function nontrivial asymptotic inference is possible only if n increases to infinity at least at the rate of log m. Based on this assumption precise conditions are given under which the Bonferroni correction with nominal Family Wise Error Rate (FWER) level alpha and the Benjamini- Hochberg procedure (BH) at FDR level alpha are asymptotically optimal. When n is proportional to log m then alpha can remain fixed, whereas when n increases to infinity at a quicker rate, then alpha has to converge to zero roughly like n^(-1/2). Under these conditions the Bonferroni correction is ABOS in case of extreme sparsity, while BH adapts well to the unknown level of sparsity.   In the second part of this article these optimality results are carried over to model selection in the context of multiple regression with orthogonal regressors. Several modifications of Bayesian Information Criterion are considered, controlling either FWER or FDR, and conditions are provided under which these selection criteria are ABOS. Finally the performance of these criteria is examined in a brief simulation study.

</details>

<details>

<summary>2011-07-12 17:17:52 - Blending Bayesian and frequentist methods according to the precision of prior information with an application to hypothesis testing</summary>

- *David R. Bickel*

- `1107.2353v1` - [abs](http://arxiv.org/abs/1107.2353v1) - [pdf](http://arxiv.org/pdf/1107.2353v1)

> The following zero-sum game between nature and a statistician blends Bayesian methods with frequentist methods such as p-values and confidence intervals. Nature chooses a posterior distribution consistent with a set of possible priors. At the same time, the statistician selects a parameter distribution for inference with the goal of maximizing the minimum Kullback-Leibler information gained over a confidence distribution or other benchmark distribution. An application to testing a simple null hypothesis leads the statistician to report a posterior probability of the hypothesis that is informed by both Bayesian and frequentist methodology, each weighted according how well the prior is known.   Since neither the Bayesian approach nor the frequentist approach is entirely satisfactory in situations involving partial knowledge of the prior distribution, the proposed procedure reduces to a Bayesian method given complete knowledge of the prior, to a frequentist method given complete ignorance about the prior, and to a blend between the two methods given partial knowledge of the prior. The blended approach resembles the Bayesian method rather than the frequentist method to the precise extent that the prior is known.   The problem of testing a point null hypothesis illustrates the proposed framework. The blended probability that the null hypothesis is true is equal to the p-value or a lower bound of an unknown Bayesian posterior probability, whichever is greater. Thus, given total ignorance represented by a lower bound of 0, the p-value is used instead of any Bayesian posterior probability. At the opposite extreme of a known prior, the p-value is ignored. In the intermediate case, the possible Bayesian posterior probability that is closest to the p-value is used for inference. Thus, both the Bayesian method and the frequentist method influence the inferences made.

</details>

<details>

<summary>2011-07-15 01:40:25 - A Bayesian Approach to Detection of Small Low Emission Sources</summary>

- *Xiaolei Xun, Bani Mallick, Raymond J. Carroll, Peter Kuchment*

- `1107.2980v1` - [abs](http://arxiv.org/abs/1107.2980v1) - [pdf](http://arxiv.org/pdf/1107.2980v1)

> The article addresses the problem of detecting presence and location of a small low emission source inside of an object, when the background noise dominates. This problem arises, for instance, in some homeland security applications. The goal is to reach the signal-to-noise ratio (SNR) levels on the order of $10^{-3}$. A Bayesian approach to this problem is implemented in 2D. The method allows inference not only about the existence of the source, but also about its location. We derive Bayes factors for model selection and estimation of location based on Markov Chain Monte Carlo (MCMC) simulation. A simulation study shows that with sufficiently high total emission level, our method can effectively locate the source.

</details>

<details>

<summary>2011-07-18 08:49:12 - Special section on statistics in neuroscience</summary>

- *Karen Kafadar*

- `1107.3382v1` - [abs](http://arxiv.org/abs/1107.3382v1) - [pdf](http://arxiv.org/pdf/1107.3382v1)

> This article provides a brief introduction to seven papers that are included in this special section on Statistics in Neuroscience: (1) Xiaoyan Shi, Joseph G. Ibrahim, Jeffrey Lieberman, Martin Styner, Yimei Li and Hongtu Zhu: Two-state empirical likelihood for longitudinal neuroimaging data (2) Vincent Q. Vu, Pradeep Ravikumar, Thomas Naselaris, Kendrick N. Kay, Jack L. Gallant and Bin Yu: Encoding and decoding V1 fMRI responses to natural images with sparse nonparametric models (3) Sourabh Bhattacharya and Ranjan Maitra: A nonstationary nonparametric Bayesian approach to dynamically modeling effective connectivity in functional magnetic resonance imaging experiments (4) Christopher J. Long, Patrick L. Purdon, Simona Temereanca, Neil U. Desai, Matti S. H\"{a}m\"{a}l\"{a}inen and Emery Neal Brown: State-space solutions to the dynamic magnetoencephalography inverse problem using high performance computing (5) Yuriy Mishchencko, Joshua T. Vogelstein and Liam Paninski: A Bayesian approach for inferring neuronal connectivity from calcium fluorescent imaging data (6) Robert E. Kass, Ryan C. Kelly and Wei-Liem Loh: Assessment of synchrony in multiple neural spike trains using loglinear point process models (7) Sofia Olhede and Brandon Whitcher: Nonparametric tests of structure for high angular resolution diffusion imaging in Q-space

</details>

<details>

<summary>2011-07-19 03:16:25 - Varying-coefficient modeling via regularized basis functions</summary>

- *Hidetoshi Matsui, Toshihiro Misumi, Shuichi Kawano*

- `1107.3618v1` - [abs](http://arxiv.org/abs/1107.3618v1) - [pdf](http://arxiv.org/pdf/1107.3618v1)

> We address the problem of constructing varying-coefficient models based on basis expansions along with the technique of regularization. A crucial point in our modeling procedure is the selection of smoothing parameters in the regularization method. In order to choose the parameters objectively, we derive model selection criteria from the viewpoints of information-theoretic and Bayesian approach. We demonstrate the effectiveness of proposed modeling strategy through Monte Carlo simulations and analyzing a real data set.

</details>

<details>

<summary>2011-07-20 17:47:43 - A Bayesian Surrogate Model for Rapid Time Series Analysis and Application to Exoplanet Observations</summary>

- *Eric B. Ford, Althea V. Moorhead, Dimitri Veras*

- `1107.4047v1` - [abs](http://arxiv.org/abs/1107.4047v1) - [pdf](http://arxiv.org/pdf/1107.4047v1)

> We present a Bayesian surrogate model for the analysis of periodic or quasi-periodic time series data. We describe a computationally efficient implementation that enables Bayesian model comparison. We apply this model to simulated and real exoplanet observations. We discuss the results and demonstrate some of the challenges for applying our surrogate model to realistic exoplanet data sets. In particular, we find that analyses of real world data should pay careful attention to the effects of uneven spacing of observations and the choice of prior for the "jitter" parameter.

</details>

<details>

<summary>2011-07-21 06:56:34 - A nonstationary nonparametric Bayesian approach to dynamically modeling effective connectivity in functional magnetic resonance imaging experiments</summary>

- *Sourabh Bhattacharya, Ranjan Maitra*

- `1107.4181v1` - [abs](http://arxiv.org/abs/1107.4181v1) - [pdf](http://arxiv.org/pdf/1107.4181v1)

> Effective connectivity analysis provides an understanding of the functional organization of the brain by studying how activated regions influence one other. We propose a nonparametric Bayesian approach to model effective connectivity assuming a dynamic nonstationary neuronal system. Our approach uses the Dirichlet process to specify an appropriate (most plausible according to our prior beliefs) dynamic model as the "expectation" of a set of plausible models upon which we assign a probability distribution. This addresses model uncertainty associated with dynamic effective connectivity. We derive a Gibbs sampling approach to sample from the joint (and marginal) posterior distributions of the unknowns. Results on simulation experiments demonstrate our model to be flexible and a better candidate in many situations. We also used our approach to analyzing functional Magnetic Resonance Imaging (fMRI) data on a Stroop task: our analysis provided new insight into the mechanism by which an individual brain distinguishes and learns about shapes of objects.

</details>

<details>

<summary>2011-07-21 10:33:59 - A Bayesian approach for inferring neuronal connectivity from calcium fluorescent imaging data</summary>

- *Yuriy Mishchencko, Joshua T. Vogelstein, Liam Paninski*

- `1107.4228v1` - [abs](http://arxiv.org/abs/1107.4228v1) - [pdf](http://arxiv.org/pdf/1107.4228v1)

> Deducing the structure of neural circuits is one of the central problems of modern neuroscience. Recently-introduced calcium fluorescent imaging methods permit experimentalists to observe network activity in large populations of neurons, but these techniques provide only indirect observations of neural spike trains, with limited time resolution and signal quality. In this work we present a Bayesian approach for inferring neural circuitry given this type of imaging data. We model the network activity in terms of a collection of coupled hidden Markov chains, with each chain corresponding to a single neuron in the network and the coupling between the chains reflecting the network's connectivity matrix. We derive a Monte Carlo Expectation--Maximization algorithm for fitting the model parameters; to obtain the sufficient statistics in a computationally-efficient manner, we introduce a specialized blockwise-Gibbs algorithm for sampling from the joint activity of all observed neurons given the observed fluorescence data. We perform large-scale simulations of randomly connected neuronal networks with biophysically realistic parameters and find that the proposed methods can accurately infer the connectivity in these networks given reasonable experimental and computational constraints. In addition, the estimation accuracy may be improved significantly by incorporating prior knowledge about the sparseness of connectivity in the network, via standard L$_1$ penalization methods.

</details>

<details>

<summary>2011-07-25 05:37:12 - False discovery rates in somatic mutation studies of cancer</summary>

- *Lorenzo Trippa, Giovanni Parmigiani*

- `1107.4843v1` - [abs](http://arxiv.org/abs/1107.4843v1) - [pdf](http://arxiv.org/pdf/1107.4843v1)

> The purpose of cancer genome sequencing studies is to determine the nature and types of alterations present in a typical cancer and to discover genes mutated at high frequencies. In this article we discuss statistical methods for the analysis of somatic mutation frequency data generated in these studies. We place special emphasis on a two-stage study design introduced by Sj\"{o}blom et al. [Science 314 (2006) 268--274]. In this context, we describe and compare statistical methods for constructing scores that can be used to prioritize candidate genes for further investigation and to assess the statistical significance of the candidates thus identified. Controversy has surrounded the reliability of the false discovery rates estimates provided by the approximations used in early cancer genome studies. To address these, we develop a semiparametric Bayesian model that provides an accurate fit to the data. We use this model to generate a large collection of realistic scenarios, and evaluate alternative approaches on this collection. Our assessment is impartial in that the model used for generating data is not used by any of the approaches compared. And is objective, in that the scenarios are generated by a model that fits data. Our results quantify the conservative control of the false discovery rate with the Benjamini and Hockberg method compared to the empirical Bayes approach and the multiple testing method proposed in Storey [J. R. Stat. Soc. Ser. B Stat. Methodol. 64 (2002) 479--498]. Simulation results also show a negligible departure from the target false discovery rate for the methodology used in Sj\"{o}blom et al. [Science 314 (2006) 268--274].

</details>

<details>

<summary>2011-07-25 07:36:41 - Causal inference in transportation safety studies: Comparison of potential outcomes and causal diagrams</summary>

- *Vishesh Karwa, Aleksandra B. Slavković, Eric T. Donnell*

- `1107.4855v1` - [abs](http://arxiv.org/abs/1107.4855v1) - [pdf](http://arxiv.org/pdf/1107.4855v1)

> The research questions that motivate transportation safety studies are causal in nature. Safety researchers typically use observational data to answer such questions, but often without appropriate causal inference methodology. The field of causal inference presents several modeling frameworks for probing empirical data to assess causal relations. This paper focuses on exploring the applicability of two such modeling frameworks---Causal Diagrams and Potential Outcomes---for a specific transportation safety problem. The causal effects of pavement marking retroreflectivity on safety of a road segment were estimated. More specifically, the results based on three different implementations of these frameworks on a real data set were compared: Inverse Propensity Score Weighting with regression adjustment and Propensity Score Matching with regression adjustment versus Causal Bayesian Network. The effect of increased pavement marking retroreflectivity was generally found to reduce the probability of target nighttime crashes. However, we found that the magnitude of the causal effects estimated are sensitive to the method used and to the assumptions being violated.

</details>

<details>

<summary>2011-07-25 08:00:13 - Semiparametric Bayesian Information Criterion for Model Selection in Ultra-high Dimensional Additive Models</summary>

- *Heng Lian*

- `1107.4861v1` - [abs](http://arxiv.org/abs/1107.4861v1) - [pdf](http://arxiv.org/pdf/1107.4861v1)

> For linear models with a diverging number of parameters, it has recently been shown that modified versions of Bayesian information criterion (BIC) can identify the true model consistently. However, in many cases there is little justification that the effects of the covariates are actually linear. Thus a semiparametric model such as the additive model studied here, is a viable alternative. We demonstrate that theoretical results on the consistency of BIC-type criterion can be extended to this more challenging situation, with dimension diverging exponentially fast with sample size. Besides, the noise assumptions are relaxed in our theoretical studies. These efforts significantly enlarge the applicability of the criterion to a more general class of models.

</details>

<details>

<summary>2011-07-25 11:42:58 - Bayesian hierarchical modeling for temperature reconstruction from geothermal data</summary>

- *Jenný Brynjarsdóttir, L. Mark Berliner*

- `1107.4905v1` - [abs](http://arxiv.org/abs/1107.4905v1) - [pdf](http://arxiv.org/pdf/1107.4905v1)

> We present a Bayesian hierarchical modeling approach to paleoclimate reconstruction using borehole temperature profiles. The approach relies on modeling heat conduction in solids via the heat equation with step function, surface boundary conditions. Our analysis includes model error and assumes that the boundary conditions are random processes. The formulation also enables separation of measurement error and model error. We apply the analysis to data from nine borehole temperature records from the San Rafael region in Utah. We produce ground surface temperature histories with uncertainty estimates for the past 400 years. We pay special attention to use of prior parameter models that illustrate borrowing strength in a combined analysis for all nine boreholes. In addition, we review selected sensitivity analyses.

</details>

<details>

<summary>2011-07-26 15:19:27 - Autoregressive Models for Variance Matrices: Stationary Inverse Wishart Processes</summary>

- *Emily B. Fox, Mike West*

- `1107.5239v1` - [abs](http://arxiv.org/abs/1107.5239v1) - [pdf](http://arxiv.org/pdf/1107.5239v1)

> We introduce and explore a new class of stationary time series models for variance matrices based on a constructive definition exploiting inverse Wishart distribution theory. The main class of models explored is a novel class of stationary, first-order autoregressive (AR) processes on the cone of positive semi-definite matrices. Aspects of the theory and structure of these new models for multivariate "volatility" processes are described in detail and exemplified. We then develop approaches to model fitting via Bayesian simulation-based computations, creating a custom filtering method that relies on an efficient innovations sampler. An example is then provided in analysis of a multivariate electroencephalogram (EEG) time series in neurological studies. We conclude by discussing potential further developments of higher-order AR models and a number of connections with prior approaches.

</details>

<details>

<summary>2011-07-27 08:18:43 - Bayesian nonparametric models for peak identification in MALDI-TOF mass spectroscopy</summary>

- *Leanna L. House, Merlise A. Clyde, Robert L. Wolpert*

- `1107.5407v1` - [abs](http://arxiv.org/abs/1107.5407v1) - [pdf](http://arxiv.org/pdf/1107.5407v1)

> We present a novel nonparametric Bayesian approach based on L\'{e}vy Adaptive Regression Kernels (LARK) to model spectral data arising from MALDI-TOF (Matrix Assisted Laser Desorption Ionization Time-of-Flight) mass spectrometry. This model-based approach provides identification and quantification of proteins through model parameters that are directly interpretable as the number of proteins, mass and abundance of proteins and peak resolution, while having the ability to adapt to unknown smoothness as in wavelet based methods. Informative prior distributions on resolution are key to distinguishing true peaks from background noise and resolving broad peaks into individual peaks for multiple protein species. Posterior distributions are obtained using a reversible jump Markov chain Monte Carlo algorithm and provide inference about the number of peaks (proteins), their masses and abundance. We show through simulation studies that the procedure has desirable true-positive and false-discovery rates. Finally, we illustrate the method on five example spectra: a blank spectrum, a spectrum with only the matrix of a low-molecular-weight substance used to embed target proteins, a spectrum with known proteins, and a single spectrum and average of ten spectra from an individual lung cancer patient.

</details>

<details>

<summary>2011-07-27 10:48:05 - Large Deviation Strategy for Inverse Problem</summary>

- *Izumi Ojima, Kazuya Okamura*

- `1101.3690v3` - [abs](http://arxiv.org/abs/1101.3690v3) - [pdf](http://arxiv.org/pdf/1101.3690v3)

> Taken traditionally as a no-go theorem against the theorization of inductive processes, Duhem-Quine thesis may interfere with the essence of statistical inference. This difficulty can be resolved by Micro-Macro duality \cite{Oj03, Oj05} which clarifies the importance of specifying the pertinent aspects and accuracy relevant to concrete contexts of scientific discussions and which ensures the matching between what to be described and what to describe in the form of the validity of duality relations. This consolidates the foundations of the inverse problem, induction method, and statistical inference crucial for the sound relations between theory and experiments. To achieve the purpose, we propose here Large Deviation Strategy (LDS for short) on the basis of Micro-Macro duality, quadrality scheme, and large deviation principle. According to the quadrality scheme emphasizing the basic roles played by the dynamics, algebra of observables together with its representations and universal notion of classifying space, LDS consists of four levels and we discuss its first and second levels in detail, aiming at establishing statistical inference concerning observables and states. By efficient use of the central measure, we will establish a quantum version of Sanov's theorem, the Bayesian escort predictive state and the widely applicable information criteria for quantum states in LDS second level. Finally, these results are reexamined in the context of quantum estimation theory, and organized as quantum model selection, i.e., a quantum version of model selection.

</details>

<details>

<summary>2011-07-27 15:30:56 - Proximity penalty priors for Bayesian mixture models</summary>

- *Matthew Sperrin*

- `1107.5508v1` - [abs](http://arxiv.org/abs/1107.5508v1) - [pdf](http://arxiv.org/pdf/1107.5508v1)

> When using mixture models it may be the case that the modeller has a-priori beliefs or desires about what the components of the mixture should represent. For example, if a mixture of normal densities is to be fitted to some data, it may be desirable for components to focus on capturing differences in location rather than scale. We introduce a framework called proximity penalty priors (PPPs) that allows this preference to be made explicit in the prior information. The approach is scale-free and imposes minimal restrictions on the posterior; in particular no arbitrary thresholds need to be set. We show the theoretical validity of the approach, and demonstrate the effects of using PPPs on posterior distributions with simulated and real data.

</details>

<details>

<summary>2011-07-28 12:10:46 - A hierarchical Bayesian approach to record linkage and population size problems</summary>

- *Andrea Tancredi, Brunero Liseo*

- `1011.2649v3` - [abs](http://arxiv.org/abs/1011.2649v3) - [pdf](http://arxiv.org/pdf/1011.2649v3)

> We propose and illustrate a hierarchical Bayesian approach for matching statistical records observed on different occasions. We show how this model can be profitably adopted both in record linkage problems and in capture--recapture setups, where the size of a finite population is the real object of interest. There are at least two important differences between the proposed model-based approach and the current practice in record linkage. First, the statistical model is built up on the actually observed categorical variables and no reduction (to 0--1 comparisons) of the available information takes place. Second, the hierarchical structure of the model allows a two-way propagation of the uncertainty between the parameter estimation step and the matching procedure so that no plug-in estimates are used and the correct uncertainty is accounted for both in estimating the population size and in performing the record linkage. We illustrate and motivate our proposal through a real data example and simulations.

</details>

<details>

<summary>2011-07-28 12:20:24 - Nonparametric Bayesian sparse factor models with application to gene expression modeling</summary>

- *David Knowles, Zoubin Ghahramani*

- `1011.6293v2` - [abs](http://arxiv.org/abs/1011.6293v2) - [pdf](http://arxiv.org/pdf/1011.6293v2)

> A nonparametric Bayesian extension of Factor Analysis (FA) is proposed where observed data $\mathbf{Y}$ is modeled as a linear superposition, $\mathbf{G}$, of a potentially infinite number of hidden factors, $\mathbf{X}$. The Indian Buffet Process (IBP) is used as a prior on $\mathbf{G}$ to incorporate sparsity and to allow the number of latent features to be inferred. The model's utility for modeling gene expression data is investigated using randomly generated data sets based on a known sparse connectivity matrix for E. Coli, and on three biological data sets of increasing complexity.

</details>

<details>

<summary>2011-07-28 13:26:29 - Degradation modeling applied to residual lifetime prediction using functional data analysis</summary>

- *Rensheng R. Zhou, Nicoleta Serban, Nagi Gebraeel*

- `1107.5712v1` - [abs](http://arxiv.org/abs/1107.5712v1) - [pdf](http://arxiv.org/pdf/1107.5712v1)

> Sensor-based degradation signals measure the accumulation of damage of an engineering system using sensor technology. Degradation signals can be used to estimate, for example, the distribution of the remaining life of partially degraded systems and/or their components. In this paper we present a nonparametric degradation modeling framework for making inference on the evolution of degradation signals that are observed sparsely or over short intervals of times. Furthermore, an empirical Bayes approach is used to update the stochastic parameters of the degradation model in real-time using training degradation signals for online monitoring of components operating in the field. The primary application of this Bayesian framework is updating the residual lifetime up to a degradation threshold of partially degraded components. We validate our degradation modeling approach using a real-world crack growth data set as well as a case study of simulated degradation signals.

</details>

<details>

<summary>2011-07-28 19:51:53 - Rotating Stars and Revolving Planets: Bayesian Exploration of the Pulsating Sky</summary>

- *Thomas J. Loredo*

- `1107.5805v1` - [abs](http://arxiv.org/abs/1107.5805v1) - [pdf](http://arxiv.org/pdf/1107.5805v1)

> I describe ongoing work on development of Bayesian methods for exploring periodically varying phenomena in astronomy, addressing two classes of sources: pulsars, and extrasolar planets (exoplanets). For pulsars, the methods aim to detect and measure periodically varying signals in data consisting of photon arrival times, modeled as non-homogeneous Poisson point processes. For exoplanets, the methods address detection and estimation of planetary orbits using observations of the reflex motion "wobble" of a host star, including adaptive scheduling of observations to optimize inferences.

</details>

<details>

<summary>2011-07-29 07:10:56 - Response-adaptive dose-finding under model uncertainty</summary>

- *Björn Bornkamp, Frank Bretz, Holger Dette, José Pinheiro*

- `1107.5883v1` - [abs](http://arxiv.org/abs/1107.5883v1) - [pdf](http://arxiv.org/pdf/1107.5883v1)

> Dose-finding studies are frequently conducted to evaluate the effect of different doses or concentration levels of a compound on a response of interest. Applications include the investigation of a new medicinal drug, a herbicide or fertilizer, a molecular entity, an environmental toxin, or an industrial chemical. In pharmaceutical drug development, dose-finding studies are of critical importance because of regulatory requirements that marketed doses are safe and provide clinically relevant efficacy. Motivated by a dose-finding study in moderate persistent asthma, we propose response-adaptive designs addressing two major challenges in dose-finding studies: uncertainty about the dose-response models and large variability in parameter estimates. To allocate new cohorts of patients in an ongoing study, we use optimal designs that are robust under model uncertainty. In addition, we use a Bayesian shrinkage approach to stabilize the parameter estimates over the successive interim analyses used in the adaptations. This approach allows us to calculate updated parameter estimates and model probabilities that can then be used to calculate the optimal design for subsequent cohorts. The resulting designs are hence robust with respect to model misspecification and additionally can efficiently adapt to the information accrued in an ongoing study. We focus on adaptive designs for estimating the minimum effective dose, although alternative optimality criteria or mixtures thereof could be used, enabling the design to address multiple objectives.

</details>

<details>

<summary>2011-07-29 08:56:00 - Hierarchical Bayesian estimation of inequality measures with nonrectangular censored survey data with an application to wealth distribution of French households</summary>

- *Eric Gautier*

- `1107.5899v1` - [abs](http://arxiv.org/abs/1107.5899v1) - [pdf](http://arxiv.org/pdf/1107.5899v1)

> We consider the estimation of wealth inequality measures with their confidence interval, based on survey data with interval censoring. We rely on a Bayesian hierarchical model. It consists of a model where, due to survey sampling and unit nonresponse, the summaries of the wealth distribution of households are observed with error; a mixture of multivariate models for the wealth components where groups correspond to portfolios of assets; and a prior on the parameters. A Gibbs sampler is used for numerical purposes to do the inference. We apply this strategy to the French 2004 Wealth Survey. In order to alleviate the nonresponse, the amounts were systematically collected in the form of brackets. Matched administrative data on the liability of the respondents for wealth tax and response to overview questions are used to better localize the wealth components. It implies nonrectangular multidimensional censoring. The variance of the error term in the model for the population inequality measures is obtained using linearization and taking into account the complex sampling design and the various weight adjustments.

</details>

<details>

<summary>2011-07-29 11:18:08 - Bayesian Synthesis: Combining subjective analyses, with an application to ozone data</summary>

- *Qingzhao Yu, Steven N. MacEachern, Mario Peruggia*

- `1107.5935v1` - [abs](http://arxiv.org/abs/1107.5935v1) - [pdf](http://arxiv.org/pdf/1107.5935v1)

> Bayesian model averaging enables one to combine the disparate predictions of a number of models in a coherent fashion, leading to superior predictive performance. The improvement in performance arises from averaging models that make different predictions. In this work, we tap into perhaps the biggest driver of different predictions---different analysts---in order to gain the full benefits of model averaging. In a standard implementation of our method, several data analysts work independently on portions of a data set, eliciting separate models which are eventually updated and combined through a specific weighting method. We call this modeling procedure Bayesian Synthesis. The methodology helps to alleviate concerns about the sizable gap between the foundational underpinnings of the Bayesian paradigm and the practice of Bayesian statistics. In experimental work we show that human modeling has predictive performance superior to that of many automatic modeling techniques, including AIC, BIC, Smoothing Splines, CART, Bagged CART, Bayes CART, BMA and LARS, and only slightly inferior to that of BART. We also show that Bayesian Synthesis further improves predictive performance. Additionally, we examine the predictive performance of a simple average across analysts, which we dub Convex Synthesis, and find that it also produces an improvement.

</details>


## 2011-08

<details>

<summary>2011-08-01 05:54:47 - Analysis of rolling group therapy data using conditionally autoregressive priors</summary>

- *Susan M. Paddock, Sarah B. Hunter, Katherine E. Watkins, Daniel F. McCaffrey*

- `1108.0240v1` - [abs](http://arxiv.org/abs/1108.0240v1) - [pdf](http://arxiv.org/pdf/1108.0240v1)

> Group therapy is a central treatment modality for behavioral health disorders such as alcohol and other drug use (AOD) and depression. Group therapy is often delivered under a rolling (or open) admissions policy, where new clients are continuously enrolled into a group as space permits. Rolling admissions policies result in a complex correlation structure among client outcomes. Despite the ubiquity of rolling admissions in practice, little guidance on the analysis of such data is available. We discuss the limitations of previously proposed approaches in the context of a study that delivered group cognitive behavioral therapy for depression to clients in residential substance abuse treatment. We improve upon previous rolling group analytic approaches by fully modeling the interrelatedness of client depressive symptom scores using a hierarchical Bayesian model that assumes a conditionally autoregressive prior for session-level random effects. We demonstrate improved performance using our method for estimating the variance of model parameters and the enhanced ability to learn about the complex correlation structure among participants in rolling therapy groups. Our approach broadly applies to any group therapy setting where groups have changing client composition. It will lead to more efficient analyses of client-level data and improve the group therapy research community's ability to understand how the dynamics of rolling groups lead to client outcomes.

</details>

<details>

<summary>2011-08-01 22:23:13 - Adaptive Gaussian Predictive Process Approximation</summary>

- *Surya T Tokdar*

- `1108.0445v1` - [abs](http://arxiv.org/abs/1108.0445v1) - [pdf](http://arxiv.org/pdf/1108.0445v1)

> We address the issue of knots selection for Gaussian predictive process methodology. Predictive process approximation provides an effective solution to the cubic order computational complexity of Gaussian process models. This approximation crucially depends on a set of points, called knots, at which the original process is retained, while the rest is approximated via a deterministic extrapolation. Knots should be few in number to keep the computational complexity low, but provide a good coverage of the process domain to limit approximation error. We present theoretical calculations to show that coverage must be judged by the canonical metric of the Gaussian process. This necessitates having in place a knots selection algorithm that automatically adapts to the changes in the canonical metric affected by changes in the parameter values controlling the Gaussian process covariance function. We present an algorithm toward this by employing an incomplete Cholesky factorization with pivoting and dynamic stopping. Although these concepts already exist in the literature, our contribution lies in unifying them into a fast algorithm and in using computable error bounds to finesse implementation of the predictive process approximation. The resulting adaptive predictive process offers a substantial automatization of Guassian process model fitting, especially for Bayesian applications where thousands of values of the covariance parameters are to be explored.

</details>

<details>

<summary>2011-08-02 09:18:02 - Priors for New Physics</summary>

- *Maurizio Pierini, Harrison B. Prosper, Sezen Sekmen, Maria Spiropulu*

- `1108.0523v1` - [abs](http://arxiv.org/abs/1108.0523v1) - [pdf](http://arxiv.org/pdf/1108.0523v1)

> The interpretation of data in terms of multi-parameter models of new physics, using the Bayesian approach, requires the construction of multi-parameter priors. We propose a construction that uses elements of Bayesian reference analysis. Our idea is to initiate the chain of inference with the reference prior for a likelihood function that depends on a single parameter of interest that is a function of the parameters of the physics model. The reference posterior density of the parameter of interest induces on the parameter space of the physics model a class of posterior densities. We propose to continue the chain of inference with a particular density from this class, namely, the one for which indistinguishable models are equiprobable and use it as the prior for subsequent analysis. We illustrate our method by applying it to the constrained minimal supersymmetric Standard Model and two non-universal variants of it.

</details>

<details>

<summary>2011-08-03 09:07:08 - Bayesian hierarchical modeling for signaling pathway inference from single cell interventional data</summary>

- *Ruiyan Luo, Hongyu Zhao*

- `1108.0793v1` - [abs](http://arxiv.org/abs/1108.0793v1) - [pdf](http://arxiv.org/pdf/1108.0793v1)

> Recent technological advances have made it possible to simultaneously measure multiple protein activities at the single cell level. With such data collected under different stimulatory or inhibitory conditions, it is possible to infer the causal relationships among proteins from single cell interventional data. In this article we propose a Bayesian hierarchical modeling framework to infer the signaling pathway based on the posterior distributions of parameters in the model. Under this framework, we consider network sparsity and model the existence of an association between two proteins both at the overall level across all experiments and at each individual experimental level. This allows us to infer the pairs of proteins that are associated with each other and their causal relationships. We also explicitly consider both intrinsic noise and measurement error. Markov chain Monte Carlo is implemented for statistical inference. We demonstrate that this hierarchical modeling can effectively pool information from different interventional experiments through simulation studies and real data analysis.

</details>

<details>

<summary>2011-08-03 19:31:22 - On polyhedral approximations of polytopes for learning Bayes nets</summary>

- *Milan Studeny, David Haws*

- `1107.4708v3` - [abs](http://arxiv.org/abs/1107.4708v3) - [pdf](http://arxiv.org/pdf/1107.4708v3)

> We review three vector encodings of Bayesian network structures. The first one has recently been applied by Jaakkola 2010, the other two use special integral vectors formerly introduced, called imsets [Studeny 2005, Studeny 2010]. The central topic is the comparison of outer polyhedral approximations of the corresponding polytopes. We show how to transform the inequalities suggested by Jaakkola et al. to the framework of imsets. The result of our comparison is the observation that the implicit polyhedral approximation of the standard imset polytope suggested in [Studeny 2011] gives a closer approximation than the (transformed) explicit polyhedral approximation from [Jaakkola 2010]. Finally, we confirm a conjecture from [Studeny 2011] that the above-mentioned implicit polyhedral approximation of the standard imset polytope is an LP relaxation of the polytope.

</details>

<details>

<summary>2011-08-04 04:04:45 - A Tutorial on Bayesian Nonparametric Models</summary>

- *Samuel J. Gershman, David M. Blei*

- `1106.2697v2` - [abs](http://arxiv.org/abs/1106.2697v2) - [pdf](http://arxiv.org/pdf/1106.2697v2)

> A key problem in statistical modeling is model selection, how to choose a model at an appropriate level of complexity. This problem appears in many settings, most prominently in choosing the number ofclusters in mixture models or the number of factors in factor analysis. In this tutorial we describe Bayesian nonparametric methods, a class of methods that side-steps this issue by allowing the data to determine the complexity of the model. This tutorial is a high-level introduction to Bayesian nonparametric methods and contains several examples of their application.

</details>

<details>

<summary>2011-08-04 21:33:53 - Incorporating Individual and Collective Ethics into Phase I Cancer Trial Designs</summary>

- *Jay Bartroff, Tze Leung Lai*

- `1108.1223v1` - [abs](http://arxiv.org/abs/1108.1223v1) - [pdf](http://arxiv.org/pdf/1108.1223v1)

> A general framework is proposed for Bayesian model-based designs of Phase I cancer trials, in which a general criterion for coherence (Cheung, 2005) of a design is also developed. This framework can incorporate both "individual" and "collective" ethics into the design of the trial. We propose a new design which minimizes a risk function composed of two terms, with one representing the individual risk of the current dose and the other representing the collective risk. The performance of this design, which is measured in terms of the accuracy of the estimated target dose at the end of the trial, the toxicity and overdose rates, and certain loss functions reflecting the individual and collective ethics, is studied and compared with existing Bayesian model-based designs and is shown to have better performance than existing designs.

</details>

<details>

<summary>2011-08-08 07:12:21 - Bayesian phase I/II adaptively randomized oncology trials with combined drugs</summary>

- *Ying Yuan, Guosheng Yin*

- `1108.1614v1` - [abs](http://arxiv.org/abs/1108.1614v1) - [pdf](http://arxiv.org/pdf/1108.1614v1)

> We propose a new integrated phase I/II trial design to identify the most efficacious dose combination that also satisfies certain safety requirements for drug-combination trials. We first take a Bayesian copula-type model for dose finding in phase I. After identifying a set of admissible doses, we immediately move the entire set forward to phase II. We propose a novel adaptive randomization scheme to favor assigning patients to more efficacious dose-combination arms. Our adaptive randomization scheme takes into account both the point estimate and variability of efficacy. By using a moving reference to compare the relative efficacy among treatment arms, our method achieves a high resolution to distinguish different arms. We also consider groupwise adaptive randomization when efficacy is late-onset. We conduct extensive simulation studies to examine the operating characteristics of the proposed design, and illustrate our method using a phase I/II melanoma clinical trial.

</details>

<details>

<summary>2011-08-08 11:16:09 - Copula Gaussian graphical models and their application to modeling functional disability data</summary>

- *Adrian Dobra, Alex Lenkoski*

- `1108.1680v1` - [abs](http://arxiv.org/abs/1108.1680v1) - [pdf](http://arxiv.org/pdf/1108.1680v1)

> We propose a comprehensive Bayesian approach for graphical model determination in observational studies that can accommodate binary, ordinal or continuous variables simultaneously. Our new models are called copula Gaussian graphical models (CGGMs) and embed graphical model selection inside a semiparametric Gaussian copula. The domain of applicability of our methods is very broad and encompasses many studies from social science and economics. We illustrate the use of the copula Gaussian graphical models in the analysis of a 16-dimensional functional disability contingency table.

</details>

<details>

<summary>2011-08-09 06:34:33 - Percentile rank scores are congruous indicators of relative performance, or aren't they?</summary>

- *Ronald Rousseau*

- `1108.1860v1` - [abs](http://arxiv.org/abs/1108.1860v1) - [pdf](http://arxiv.org/pdf/1108.1860v1)

> Percentile ranks and the I3 indicator were introduced by Bornmann, Leydesdorff, Mutz and Opthof. These two notions are based on the concept of percentiles (or quantiles) for discrete data. As several definitions for these notions exist we propose one that we think is suitable in this context. Next we show that if the notion of relative congruous indicators is carefully defined then percentile rank scores are congruous indicators of relative performance. The I3 indicator is a strictly congruous indicator of absolute performance.

</details>

<details>

<summary>2011-08-09 08:31:58 - Boundary detection in disease mapping studies</summary>

- *Duncan Lee, Richard Mitchell*

- `1108.1879v1` - [abs](http://arxiv.org/abs/1108.1879v1) - [pdf](http://arxiv.org/pdf/1108.1879v1)

> In disease mapping, the aim is to estimate the spatial pattern in disease risk over an extended geographical region, so that areas with elevated risks can be identified. A Bayesian hierarchical approach is typically used to produce such maps, which models the risk surface with a set of spatially smooth random effects. However, in complex urban settings there are likely to be boundaries in the risk surface, which separate populations that are geographically adjacent but have very different risk profiles. Therefore this paper proposes an approach for detecting such risk boundaries, and tests its effectiveness by simulation. Finally, the model is applied to lung cancer incidence data in Greater Glasgow, Scotland, between 2001 and 2005.

</details>

<details>

<summary>2011-08-09 13:02:55 - Calibrated Bayes, for Statistics in General, and Missing Data in Particular</summary>

- *Roderick Little*

- `1108.1917v1` - [abs](http://arxiv.org/abs/1108.1917v1) - [pdf](http://arxiv.org/pdf/1108.1917v1)

> It is argued that the Calibrated Bayesian (CB) approach to statistical inference capitalizes on the strength of Bayesian and frequentist approaches to statistical inference. In the CB approach, inferences under a particular model are Bayesian, but frequentist methods are useful for model development and model checking. In this article the CB approach is outlined. Bayesian methods for missing data are then reviewed from a CB perspective. The basic theory of the Bayesian approach, and the closely related technique of multiple imputation, is described. Then applications of the Bayesian approach to normal models are described, both for monotone and nonmonotone missing data patterns. Sequential Regression Multivariate Imputation and Penalized Spline of Propensity Models are presented as two useful approaches for relaxing distributional assumptions.

</details>

<details>

<summary>2011-08-09 21:30:47 - Distance Dependent Chinese Restaurant Processes</summary>

- *David M. Blei, Peter I. Frazier*

- `0910.1022v3` - [abs](http://arxiv.org/abs/0910.1022v3) - [pdf](http://arxiv.org/pdf/0910.1022v3)

> We develop the distance dependent Chinese restaurant process (CRP), a flexible class of distributions over partitions that allows for non-exchangeability. This class can be used to model many kinds of dependencies between data in infinite clustering models, including dependencies across time or space. We examine the properties of the distance dependent CRP, discuss its connections to Bayesian nonparametric mixture models, and derive a Gibbs sampler for both observed and mixture settings. We study its performance with three text corpora. We show that relaxing the assumption of exchangeability with distance dependent CRPs can provide a better fit to sequential data. We also show its alternative formulation of the traditional CRP leads to a faster-mixing Gibbs sampling algorithm than the one based on the original formulation.

</details>

<details>

<summary>2011-08-10 08:18:28 - Objective Priors: An Introduction for Frequentists</summary>

- *Malay Ghosh*

- `1108.2120v1` - [abs](http://arxiv.org/abs/1108.2120v1) - [pdf](http://arxiv.org/pdf/1108.2120v1)

> Bayesian methods are increasingly applied in these days in the theory and practice of statistics. Any Bayesian inference depends on a likelihood and a prior. Ideally one would like to elicit a prior from related sources of information or past data. However, in its absence, Bayesian methods need to rely on some "objective" or "default" priors, and the resulting posterior inference can still be quite valuable. Not surprisingly, over the years, the catalog of objective priors also has become prohibitively large, and one has to set some specific criteria for the selection of such priors. Our aim is to review some of these criteria, compare their performance, and illustrate them with some simple examples. While for very large sample sizes, it does not possibly matter what objective prior one uses, the selection of such a prior does influence inference for small or moderate samples. For regular models where asymptotic normality holds, Jeffreys' general rule prior, the positive square root of the determinant of the Fisher information matrix, enjoys many optimality properties in the absence of nuisance parameters. In the presence of nuisance parameters, however, there are many other priors which emerge as optimal depending on the criterion selected. One new feature in this article is that a prior different from Jeffreys' is shown to be optimal under the chi-square divergence criterion even in the absence of nuisance parameters. The latter is also invariant under one-to-one reparameterization.

</details>

<details>

<summary>2011-08-10 13:29:08 - Bayesian Models and Methods in Public Policy and Government Settings</summary>

- *Stephen E. Fienberg*

- `1108.2177v1` - [abs](http://arxiv.org/abs/1108.2177v1) - [pdf](http://arxiv.org/pdf/1108.2177v1)

> Starting with the neo-Bayesian revival of the 1950s, many statisticians argued that it was inappropriate to use Bayesian methods, and in particular subjective Bayesian methods in governmental and public policy settings because of their reliance upon prior distributions. But the Bayesian framework often provides the primary way to respond to questions raised in these settings and the numbers and diversity of Bayesian applications have grown dramatically in recent years. Through a series of examples, both historical and recent, we argue that Bayesian approaches with formal and informal assessments of priors AND likelihood functions are well accepted and should become the norm in public settings. Our examples include census-taking and small area estimation, US election night forecasting, studies reported to the US Food and Drug Administration, assessing global climate change, and measuring potential declines in disability among the elderly.

</details>

<details>

<summary>2011-08-11 09:25:33 - Impact of Frequentist and Bayesian Methods on Survey Sampling Practice: A Selective Appraisal</summary>

- *J. N. K. Rao*

- `1108.2356v1` - [abs](http://arxiv.org/abs/1108.2356v1) - [pdf](http://arxiv.org/pdf/1108.2356v1)

> According to Hansen, Madow and Tepping [J. Amer. Statist. Assoc. 78 (1983) 776--793], "Probability sampling designs and randomization inference are widely accepted as the standard approach in sample surveys." In this article, reasons are advanced for the wide use of this design-based approach, particularly by federal agencies and other survey organizations conducting complex large scale surveys on topics related to public policy. Impact of Bayesian methods in survey sampling is also discussed in two different directions: nonparametric calibrated Bayesian inferences from large samples and hierarchical Bayes methods for small area estimation based on parametric models.

</details>

<details>

<summary>2011-08-12 10:56:21 - A Kernel Approach to Tractable Bayesian Nonparametrics</summary>

- *Ferenc Huszár, Simon Lacoste-Julien*

- `1103.1761v3` - [abs](http://arxiv.org/abs/1103.1761v3) - [pdf](http://arxiv.org/pdf/1103.1761v3)

> Inference in popular nonparametric Bayesian models typically relies on sampling or other approximations. This paper presents a general methodology for constructing novel tractable nonparametric Bayesian methods by applying the kernel trick to inference in a parametric Bayesian model. For example, Gaussian process regression can be derived this way from Bayesian linear regression. Despite the success of the Gaussian process framework, the kernel trick is rarely explicitly considered in the Bayesian literature. In this paper, we aim to fill this gap and demonstrate the potential of applying the kernel trick to tractable Bayesian parametric models in a wider context than just regression. As an example, we present an intuitive Bayesian kernel machine for density estimation that is obtained by applying the kernel trick to a Gaussian generative model in feature space.

</details>

<details>

<summary>2011-08-12 15:48:26 - Challenging the empirical mean and empirical variance: a deviation study</summary>

- *Olivier Catoni*

- `1009.2048v2` - [abs](http://arxiv.org/abs/1009.2048v2) - [pdf](http://arxiv.org/pdf/1009.2048v2)

> We present new M-estimators of the mean and variance of real valued random variables, based on PAC-Bayes bounds. We analyze the non-asymptotic minimax properties of the deviations of those estimators for sample distributions having either a bounded variance or a bounded variance and a bounded kurtosis. Under those weak hypotheses, allowing for heavy-tailed distributions, we show that the worst case deviations of the empirical mean are suboptimal. We prove indeed that for any confidence level, there is some M-estimator whose deviations are of the same order as the deviations of the empirical mean of a Gaussian statistical sample, even when the statistical sample is instead heavy-tailed. Experiments reveal that these new estimators perform even better than predicted by our bounds, showing deviation quantile functions uniformly lower at all probability levels than the empirical mean for non Gaussian sample distributions as simple as the mixture of two Gaussian measures.

</details>

<details>

<summary>2011-08-12 21:21:05 - Bayes Variable Selection in Semiparametric Linear Models</summary>

- *Suprateek Kundu, David B. Dunson*

- `1108.2722v1` - [abs](http://arxiv.org/abs/1108.2722v1) - [pdf](http://arxiv.org/pdf/1108.2722v1)

> There is a rich literature proposing methods and establishing asymptotic properties of Bayesian variable selection methods for parametric models, with a particular focus on the normal linear regression model and an increasing emphasis on settings in which the number of candidate predictors ($p$) diverges with sample size ($n$). Our focus is on generalizing methods and asymptotic theory established for mixtures of $g$-priors to semiparametric linear regression models having unknown residual densities. Using a Dirichlet process location mixture for the residual density, we propose a semiparametric $g$-prior which incorporates an unknown matrix of cluster allocation indicators. For this class of priors, posterior computation can proceed via a straightforward stochastic search variable selection algorithm. In addition, Bayes factor and variable selection consistency is shown to result under various cases including proper and improper priors on $g$ and $p>n$, with the models under comparison restricted to have model dimensions diverging at a rate less than $n$.

</details>

<details>

<summary>2011-08-16 01:41:07 - Adaptive Markov Chain Monte Carlo Forward Simulation for Statistical Analysis in Epidemic Modelling of Human Papillomavirus</summary>

- *Igor A. Korostil, Gareth W. Peters, Julien Cornebise, David G. Regan*

- `1108.3137v1` - [abs](http://arxiv.org/abs/1108.3137v1) - [pdf](http://arxiv.org/pdf/1108.3137v1)

> We develop a Bayesian statistical model and estimation methodology based on Forward Projection Adaptive Markov chain Monte Carlo in order to perform the calibration of a high-dimensional non-linear system of Ordinary Differential Equations representing an epidemic model for Human Papillomavirus types 6 and 11 (HPV-6, HPV-11). The model is compartmental and involves stratification by age, gender and sexual activity-group. Developing this model and a means to calibrate it efficiently is relevant since HPV is a very multi-typed and common sexually transmitted infection with more than 100 types currently known. The two types studied in this paper, types 6 and 11, are causing about 90% of anogenital warts.   We extend the development of a sexual mixing matrix for the population, based on a formulation first suggested by Garnett and Anderson. In particular we consider a stochastic mixing matrix framework which allows us to jointly estimate unknown attributes and parameters of the mixing matrix along with the parameters involved in the calibration of the HPV epidemic model. This matrix describes the sexual interactions between members of the population under study and relies on several quantities which are a-priori unknown. The Bayesian model developed allows one to estimate jointly the HPV-6 and HPV-11 epidemic model parameters such as the probability of transmission, HPV incubation period, duration of infection, duration of genital warts treatment, duration of immunity, the probability of seroconversion, per gender, age-group and sexual activity-group, as well as unknown sexual mixing matrix parameters related to assortativity. We conclude with simulation studies on synthetic and actual data from studies undertaken recently in Australia.

</details>

<details>

<summary>2011-08-16 09:16:24 - A sticky HDP-HMM with application to speaker diarization</summary>

- *Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, Alan S. Willsky*

- `0905.2592v4` - [abs](http://arxiv.org/abs/0905.2592v4) - [pdf](http://arxiv.org/pdf/0905.2592v4)

> We consider the problem of speaker diarization, the problem of segmenting an audio recording of a meeting into temporal segments corresponding to individual speakers. The problem is rendered particularly difficult by the fact that we are not allowed to assume knowledge of the number of people participating in the meeting. To address this problem, we take a Bayesian nonparametric approach to speaker diarization that builds on the hierarchical Dirichlet process hidden Markov model (HDP-HMM) of Teh et al. [J. Amer. Statist. Assoc. 101 (2006) 1566--1581]. Although the basic HDP-HMM tends to over-segment the audio data---creating redundant states and rapidly switching among them---we describe an augmented HDP-HMM that provides effective control over the switching rate. We also show that this augmentation makes it possible to treat emission distributions nonparametrically. To scale the resulting architecture to realistic diarization problems, we develop a sampling algorithm that employs a truncated approximation of the Dirichlet process to jointly resample the full state sequence, greatly improving mixing rates. Working with a benchmark NIST data set, we show that our Bayesian nonparametric architecture yields state-of-the-art speaker diarization results.

</details>

<details>

<summary>2011-08-16 23:46:59 - Overlapping Mixtures of Gaussian Processes for the Data Association Problem</summary>

- *Miguel Lázaro-Gredilla, Steven Van Vaerenbergh, Neil Lawrence*

- `1108.3372v1` - [abs](http://arxiv.org/abs/1108.3372v1) - [pdf](http://arxiv.org/pdf/1108.3372v1)

> In this work we introduce a mixture of GPs to address the data association problem, i.e. to label a group of observations according to the sources that generated them. Unlike several previously proposed GP mixtures, the novel mixture has the distinct characteristic of using no gating function to determine the association of samples and mixture components. Instead, all the GPs in the mixture are global and samples are clustered following "trajectories" across input space. We use a non-standard variational Bayesian algorithm to efficiently recover sample labels and learn the hyperparameters. We show how multi-object tracking problems can be disambiguated and also explore the characteristics of the model in traditional regression settings.

</details>

<details>

<summary>2011-08-17 00:03:36 - Sparse Signal Recovery with Temporally Correlated Source Vectors Using Sparse Bayesian Learning</summary>

- *Zhilin Zhang, Bhaskar D. Rao*

- `1102.3949v2` - [abs](http://arxiv.org/abs/1102.3949v2) - [pdf](http://arxiv.org/pdf/1102.3949v2)

> We address the sparse signal recovery problem in the context of multiple measurement vectors (MMV) when elements in each nonzero row of the solution matrix are temporally correlated. Existing algorithms do not consider such temporal correlations and thus their performance degrades significantly with the correlations. In this work, we propose a block sparse Bayesian learning framework which models the temporal correlations. In this framework we derive two sparse Bayesian learning (SBL) algorithms, which have superior recovery performance compared to existing algorithms, especially in the presence of high temporal correlations. Furthermore, our algorithms are better at handling highly underdetermined problems and require less row-sparsity on the solution matrix. We also provide analysis of the global and local minima of their cost function, and show that the SBL cost function has the very desirable property that the global minimum is at the sparsest solution to the MMV problem. Extensive experiments also provide some interesting results that motivate future theoretical research on the MMV model.

</details>

<details>

<summary>2011-08-17 16:39:18 - Gaussian process single-index models as emulators for computer experiments</summary>

- *Robert B. Gramacy, Heng Lian*

- `1009.4241v3` - [abs](http://arxiv.org/abs/1009.4241v3) - [pdf](http://arxiv.org/pdf/1009.4241v3)

> A single-index model (SIM) provides for parsimonious multi-dimensional nonlinear regression by combining parametric (linear) projection with univariate nonparametric (non-linear) regression models. We show that a particular Gaussian process (GP) formulation is simple to work with and ideal as an emulator for some types of computer experiment as it can outperform the canonical separable GP regression model commonly used in this setting. Our contribution focuses on drastically simplifying, re-interpreting, and then generalizing a recently proposed fully Bayesian GP-SIM combination, and then illustrating its favorable performance on synthetic data and a real-data computer experiment. Two R packages, both released on CRAN, have been augmented to facilitate inference under our proposed model(s).

</details>

<details>

<summary>2011-08-18 06:33:32 - Discussion of "Bayesian Models and Methods in Public Policy and Government Settings" by S. E. Fienberg</summary>

- *David J. Hand*

- `1108.3657v1` - [abs](http://arxiv.org/abs/1108.3657v1) - [pdf](http://arxiv.org/pdf/1108.3657v1)

> Fienberg convincingly demonstrates that Bayesian models and methods represent a powerful approach to squeezing illumination from data in public policy settings. However, no school of inference is without its weaknesses, and, in the face of the ambiguities, uncertainties, and poorly posed questions of the real world, perhaps we should not expect to find a formally correct inferential strategy which can be universally applied, whatever the nature of the question: we should not expect to be able to identify a "norm" approach. An analogy is made between George Box's "no models are right, but some are useful," and inferential systems [arXiv:1108.2177].

</details>

<details>

<summary>2011-08-19 07:40:39 - Automated analysis of quantitative image data using isomorphic functional mixed models, with application to proteomics data</summary>

- *Jeffrey S. Morris, Veerabhadran Baladandayuthapani, Richard C. Herrick, Pietro Sanna, Howard Gutstein*

- `1108.3910v1` - [abs](http://arxiv.org/abs/1108.3910v1) - [pdf](http://arxiv.org/pdf/1108.3910v1)

> Image data are increasingly encountered and are of growing importance in many areas of science. Much of these data are quantitative image data, which are characterized by intensities that represent some measurement of interest in the scanned images. The data typically consist of multiple images on the same domain and the goal of the research is to combine the quantitative information across images to make inference about populations or interventions. In this paper we present a unified analysis framework for the analysis of quantitative image data using a Bayesian functional mixed model approach. This framework is flexible enough to handle complex, irregular images with many local features, and can model the simultaneous effects of multiple factors on the image intensities and account for the correlation between images induced by the design. We introduce a general isomorphic modeling approach to fitting the functional mixed model, of which the wavelet-based functional mixed model is one special case. With suitable modeling choices, this approach leads to efficient calculations and can result in flexible modeling and adaptive smoothing of the salient features in the data. The proposed method has the following advantages: it can be run automatically, it produces inferential plots indicating which regions of the image are associated with each factor, it simultaneously considers the practical and statistical significance of findings, and it controls the false discovery rate.

</details>

<details>

<summary>2011-08-19 08:09:29 - Discussion of "Bayesian Models and Methods in Public Policy and Government Settings" by S. E. Fienberg</summary>

- *Graham Kalton*

- `1108.3912v1` - [abs](http://arxiv.org/abs/1108.3912v1) - [pdf](http://arxiv.org/pdf/1108.3912v1)

> Discussion of "Bayesian Models and Methods in Public Policy and Government Settings" by S. E. Fienberg [arXiv:1108.2177]

</details>

<details>

<summary>2011-08-19 08:25:47 - Sampling from a Bayesian Menu</summary>

- *Alan M. Zaslavsky*

- `1108.3913v1` - [abs](http://arxiv.org/abs/1108.3913v1) - [pdf](http://arxiv.org/pdf/1108.3913v1)

> Discussion of "Bayesian Models and Methods in Public Policy and Government Settings" by S. E. Fienberg [arXiv:1108.2177]

</details>

<details>

<summary>2011-08-19 08:44:30 - Rejoinder</summary>

- *Stephen E. Fienberg*

- `1108.3914v1` - [abs](http://arxiv.org/abs/1108.3914v1) - [pdf](http://arxiv.org/pdf/1108.3914v1)

> Rejoinder of "Bayesian Models and Methods in Public Policy and Government Settings" by S. E. Fienberg [arXiv:1108.2177]

</details>

<details>

<summary>2011-08-19 09:02:05 - Discussion of "Impact of Frequentist and Bayesian Methods on Survey Sampling Practice: A Selective Appraisal" by J. N. K. Rao</summary>

- *Glen Meeden*

- `1108.3919v1` - [abs](http://arxiv.org/abs/1108.3919v1) - [pdf](http://arxiv.org/pdf/1108.3919v1)

> Discussion of "Impact of Frequentist and Bayesian Methods on Survey Sampling Practice: A Selective Appraisal" by J. N. K. Rao [arXiv:1108.2356]

</details>

<details>

<summary>2011-08-19 09:27:52 - Discussion of "Impact of Frequentist and Bayesian Methods on Survey Sampling Practice: A Selective Appraisal" by J. N. K. Rao</summary>

- *J. Sedransk*

- `1108.3931v1` - [abs](http://arxiv.org/abs/1108.3931v1) - [pdf](http://arxiv.org/pdf/1108.3931v1)

> This comment emphasizes the importance of model checking and model fitting when making inferences about finite population quantities. It also suggests the value of using unit level models when making inferences for small subpopulations, that is, "small area" analyses [arXiv:1108.2356].

</details>

<details>

<summary>2011-08-19 10:21:08 - Discussion of "Impact of Frequentist and Bayesian Methods on Survey Sampling Practice: A Selective Appraisal" by J. N. K. Rao</summary>

- *Eric Slud*

- `1108.3938v1` - [abs](http://arxiv.org/abs/1108.3938v1) - [pdf](http://arxiv.org/pdf/1108.3938v1)

> Discussion of "Impact of Frequentist and Bayesian Methods on Survey Sampling Practice: A Selective Appraisal" by J. N. K. Rao [arXiv:1108.2356]

</details>

<details>

<summary>2011-08-19 11:03:44 - Rejoinder</summary>

- *J. N. K. Rao*

- `1108.3940v1` - [abs](http://arxiv.org/abs/1108.3940v1) - [pdf](http://arxiv.org/pdf/1108.3940v1)

> Rejoinder of "Impact of Frequentist and Bayesian Methods on Survey Sampling Practice: A Selective Appraisal" by J. N. K. Rao [arXiv:1108.2356]

</details>

<details>

<summary>2011-08-24 18:53:33 - The Importance of Prior Choice in Model Selection: a Density Dependence Example</summary>

- *James D. Lawrence, Dr. Robert B. Gramacy, Dr. Len Thomas, Prof. Stephen T. Buckland*

- `1108.4912v1` - [abs](http://arxiv.org/abs/1108.4912v1) - [pdf](http://arxiv.org/pdf/1108.4912v1)

> We perform a Bayesian analysis on abundance data for ten species of North American duck, using the results to investigate the evidence in favour of biologically motivated hypotheses about the causes and mechanisms of density dependence in these species. We explore the capabilities of our methods to detect density dependent effects, both by simulation and through analyzes of real data. The effect of the prior choice on predictive accuracy is also examined. We conclude that our priors, which are motivated by considering the dynamics of the system of interest, offer clear advances over the priors used by previous authors for the duck data sets. We use this analysis as a motivating example to demonstrate the importance of careful parameter prior selection if we are to perform a balanced model selection procedure. We also present some simple guidelines that can be followed in a wide variety of modelling frameworks where vague parameter prior choice is not a viable option. These will produce parameter priors that not only greatly reduce bias in selecting certain models, but improve the predictive ability of the resulting model-averaged predictor.

</details>

<details>

<summary>2011-08-26 14:32:08 - On frequency estimation of periodic ergodic diffusion process</summary>

- *Reinhard Höpfner, Yury A Kutoyants*

- `1108.5314v1` - [abs](http://arxiv.org/abs/1108.5314v1) - [pdf](http://arxiv.org/pdf/1108.5314v1)

> We consider the problem of frequency estimation by observations of the periodic diffusion process possesing ergodic properties in two different situations. The first one corresponds to continuously differentiable with respect to parameter trend coefficient and the second - to discontinuous trend coefficient. It is shown that in the first case the maximum likelihood and bayesian estimators are asymptotically normal with rate $T^{3/2}$ and in the second case these estimators have different limit distributions with the rate $T^2$.

</details>

<details>

<summary>2011-08-30 06:11:26 - Sparse Partitioning: Nonlinear regression with binary or tertiary predictors, with application to association studies</summary>

- *Doug Speed, Simon Tavaré*

- `1101.0632v2` - [abs](http://arxiv.org/abs/1101.0632v2) - [pdf](http://arxiv.org/pdf/1101.0632v2)

> This paper presents Sparse Partitioning, a Bayesian method for identifying predictors that either individually or in combination with others affect a response variable. The method is designed for regression problems involving binary or tertiary predictors and allows the number of predictors to exceed the size of the sample, two properties which make it well suited for association studies. Sparse Partitioning differs from other regression methods by placing no restrictions on how the predictors may influence the response. To compensate for this generality, Sparse Partitioning implements a novel way of exploring the model space. It searches for high posterior probability partitions of the predictor set, where each partition defines groups of predictors that jointly influence the response. The result is a robust method that requires no prior knowledge of the true predictor--response relationship. Testing on simulated data suggests Sparse Partitioning will typically match the performance of an existing method on a data set which obeys the existing method's model assumptions. When these assumptions are violated, Sparse Partitioning will generally offer superior performance.

</details>

<details>

<summary>2011-08-31 03:33:10 - The Value of Feedback in Decentralized Detection</summary>

- *Wee Peng Tay*

- `1108.6121v1` - [abs](http://arxiv.org/abs/1108.6121v1) - [pdf](http://arxiv.org/pdf/1108.6121v1)

> We consider the decentralized binary hypothesis testing problem in networks with feedback, where some or all of the sensors have access to compressed summaries of other sensors' observations. We study certain two-message feedback architectures, in which every sensor sends two messages to a fusion center, with the second message based on full or partial knowledge of the first messages of the other sensors. We also study one-message feedback architectures, in which each sensor sends one message to a fusion center, with a group of sensors having full or partial knowledge of the messages from the sensors not in that group. Under either a Neyman-Pearson or a Bayesian formulation, we show that the asymptotically optimal (in the limit of a large number of sensors) detection performance (as quantified by error exponents) does not benefit from the feedback messages, if the fusion center remembers all sensor messages. However, feedback can improve the Bayesian detection performance in the one-message feedback architecture if the fusion center has limited memory; for that case, we determine the corresponding optimal error exponents.

</details>


## 2011-09

<details>

<summary>2011-09-01 22:51:34 - Bayesian nonparametric multivariate convex regression</summary>

- *Lauren A. Hannah, David B. Dunson*

- `1109.0322v1` - [abs](http://arxiv.org/abs/1109.0322v1) - [pdf](http://arxiv.org/pdf/1109.0322v1)

> In many applications, such as economics, operations research and reinforcement learning, one often needs to estimate a multivariate regression function f subject to a convexity constraint. For example, in sequential decision processes the value of a state under optimal subsequent decisions may be known to be convex or concave. We propose a new Bayesian nonparametric multivariate approach based on characterizing the unknown regression function as the max of a random collection of unknown hyperplanes. This specification induces a prior with large support in a Kullback-Leibler sense on the space of convex functions, while also leading to strong posterior consistency. Although we assume that f is defined over R^p, we show that this model has a convergence rate of log(n)^{-1} n^{-1/(d+2)} under the empirical L2 norm when f actually maps a d dimensional linear subspace to R. We design an efficient reversible jump MCMC algorithm for posterior computation and demonstrate the methods through application to value function approximation.

</details>

<details>

<summary>2011-09-05 02:04:00 - Efficient variational inference in large-scale Bayesian compressed sensing</summary>

- *George Papandreou, Alan Yuille*

- `1107.4637v2` - [abs](http://arxiv.org/abs/1107.4637v2) - [pdf](http://arxiv.org/pdf/1107.4637v2)

> We study linear models under heavy-tailed priors from a probabilistic viewpoint. Instead of computing a single sparse most probable (MAP) solution as in standard deterministic approaches, the focus in the Bayesian compressed sensing framework shifts towards capturing the full posterior distribution on the latent variables, which allows quantifying the estimation uncertainty and learning model parameters using maximum likelihood. The exact posterior distribution under the sparse linear model is intractable and we concentrate on variational Bayesian techniques to approximate it. Repeatedly computing Gaussian variances turns out to be a key requisite and constitutes the main computational bottleneck in applying variational techniques in large-scale problems. We leverage on the recently proposed Perturb-and-MAP algorithm for drawing exact samples from Gaussian Markov random fields (GMRF). The main technical contribution of our paper is to show that estimating Gaussian variances using a relatively small number of such efficiently drawn random samples is much more effective than alternative general-purpose variance estimation techniques. By reducing the problem of variance estimation to standard optimization primitives, the resulting variational algorithms are fully scalable and parallelizable, allowing Bayesian computations in extremely large-scale problems with the same memory and time complexity requirements as conventional point estimation techniques. We illustrate these ideas with experiments in image deblurring.

</details>

<details>

<summary>2011-09-07 09:48:15 - Reverse engineering gene regulatory networks using approximate Bayesian computation</summary>

- *Andrea Rau, Florence Jaffrézic, Jean-Louis Foulley, R. W. Doerge*

- `1109.1402v1` - [abs](http://arxiv.org/abs/1109.1402v1) - [pdf](http://arxiv.org/pdf/1109.1402v1)

> Gene regulatory networks are collections of genes that interact with one other and with other substances in the cell. By measuring gene expression over time using high-throughput technologies, it may be possible to reverse engineer, or infer, the structure of the gene network involved in a particular cellular process. These gene expression data typically have a high dimensionality and a limited number of biological replicates and time points. Due to these issues and the complexity of biological systems, the problem of reverse engineering networks from gene expression data demands a specialized suite of statistical tools and methodologies. We propose a non-standard adaptation of a simulation-based approach known as Approximate Bayesian Computing based on Markov chain Monte Carlo sampling. This approach is particularly well suited for the inference of gene regulatory networks from longitudinal data. The performance of this approach is investigated via simulations and using longitudinal expression data from a genetic repair system in Escherichia coli.

</details>

<details>

<summary>2011-09-09 19:06:10 - Sparse Bayesian Methods for Low-Rank Matrix Estimation</summary>

- *S. Derin Babacan, Martin Luessi, Rafael Molina, Aggelos K. Katsaggelos*

- `1102.5288v2` - [abs](http://arxiv.org/abs/1102.5288v2) - [pdf](http://arxiv.org/pdf/1102.5288v2)

> Recovery of low-rank matrices has recently seen significant activity in many areas of science and engineering, motivated by recent theoretical results for exact reconstruction guarantees and interesting practical applications. A number of methods have been developed for this recovery problem. However, a principled method for choosing the unknown target rank is generally not provided. In this paper, we present novel recovery algorithms for estimating low-rank matrices in matrix completion and robust principal component analysis based on sparse Bayesian learning (SBL) principles. Starting from a matrix factorization formulation and enforcing the low-rank constraint in the estimates as a sparsity constraint, we develop an approach that is very effective in determining the correct rank while providing high recovery performance. We provide connections with existing methods in other similar problems and empirical results and comparisons with current state-of-the-art methods that illustrate the effectiveness of this approach.

</details>

<details>

<summary>2011-09-12 08:41:38 - Linear regression through PAC-Bayesian truncation</summary>

- *Jean-Yves Audibert, Olivier Catoni*

- `1010.0072v2` - [abs](http://arxiv.org/abs/1010.0072v2) - [pdf](http://arxiv.org/pdf/1010.0072v2)

> We consider the problem of predicting as well as the best linear combination of d given functions in least squares regression under L^\infty constraints on the linear combination. When the input distribution is known, there already exists an algorithm having an expected excess risk of order d/n, where n is the size of the training data. Without this strong assumption, standard results often contain a multiplicative log(n) factor, complex constants involving the conditioning of the Gram matrix of the covariates, kurtosis coefficients or some geometric quantity characterizing the relation between L^2 and L^\infty-balls and require some additional assumptions like exponential moments of the output. This work provides a PAC-Bayesian shrinkage procedure with a simple excess risk bound of order d/n holding in expectation and in deviations, under various assumptions. The common surprising factor of these results is their simplicity and the absence of exponential moment condition on the output distribution while achieving exponential deviations. The risk bounds are obtained through a PAC-Bayesian analysis on truncated differences of losses. We also show that these results can be generalized to other strongly convex loss functions.

</details>

<details>

<summary>2011-09-13 21:34:48 - State-of-the-Art in Sequential Change-Point Detection</summary>

- *Aleksey S. Polunchenko, Alexander G. Tartakovsky*

- `1109.2938v1` - [abs](http://arxiv.org/abs/1109.2938v1) - [pdf](http://arxiv.org/pdf/1109.2938v1)

> We provide an overview of the state-of-the-art in the area of sequential change-point detection assuming discrete time and known pre- and post-change distributions. The overview spans over all major formulations of the underlying optimization problem, namely, Bayesian, generalized Bayesian, and minimax. We pay particular attention to the latest advances in each. Also, we link together the generalized Bayesian problem with multi-cyclic disorder detection in a stationary regime when the change occurs at a distant time horizon. We conclude with two case studies to illustrate the cutting edge of the field at work.

</details>

<details>

<summary>2011-09-15 03:51:07 - Adjusted Bayesian inference for selected parameters</summary>

- *Daniel Yekutieli*

- `0801.0499v7` - [abs](http://arxiv.org/abs/0801.0499v7) - [pdf](http://arxiv.org/pdf/0801.0499v7)

> We address the problem of providing inference from a Bayesian perspective for parameters selected after viewing the data. We present a Bayesian framework for providing inference for selected parameters, based on the observation that providing Bayesian inference for selected parameters is a truncated data problem. We show that if the prior for the parameter is non-informative, or if the parameter is a "fixed" unknown constant, then it is necessary to adjust the Bayesian inference for selection. Our second contribution is the introduction of Bayesian False Discovery Rate controlling methodology,which generalizes existing Bayesian FDR methods that are only defined in the two-group mixture model.We illustrate our results by applying them to simulated data and data froma microarray experiment.

</details>

<details>

<summary>2011-09-15 04:28:06 - Beta processes, stick-breaking, and power laws</summary>

- *Tamara Broderick, Michael I. Jordan, Jim Pitman*

- `1106.0539v2` - [abs](http://arxiv.org/abs/1106.0539v2) - [pdf](http://arxiv.org/pdf/1106.0539v2)

> The beta-Bernoulli process provides a Bayesian nonparametric prior for models involving collections of binary-valued features. A draw from the beta process yields an infinite collection of probabilities in the unit interval, and a draw from the Bernoulli process turns these into binary-valued features. Recent work has provided stick-breaking representations for the beta process analogous to the well-known stick-breaking representation for the Dirichlet process. We derive one such stick-breaking representation directly from the characterization of the beta process as a completely random measure. This approach motivates a three-parameter generalization of the beta process, and we study the power laws that can be obtained from this generalized beta process. We present a posterior inference algorithm for the beta-Bernoulli process that exploits the stick-breaking representation, and we present experimental results for a discrete factor-analysis model.

</details>

<details>

<summary>2011-09-19 15:51:11 - Latent Factor Models for Density Estimation</summary>

- *Suprateek Kundu, David B. Dunson*

- `1108.2720v2` - [abs](http://arxiv.org/abs/1108.2720v2) - [pdf](http://arxiv.org/pdf/1108.2720v2)

> Although discrete mixture modeling has formed the backbone of the literature on Bayesian density estimation, there are some well known disadvantages. We propose an alternative class of priors based on random nonlinear functions of a uniform latent variable with an additive residual. The induced prior for the density is shown to have desirable properties including ease of centering on an initial guess for the density, large support, posterior consistency and straightforward computation via Gibbs sampling. Some advantages over discrete mixtures, such as Dirichlet process mixtures of Gaussian kernels, are discussed and illustrated via simulations and an epidemiology application.

</details>

<details>

<summary>2011-09-19 22:04:04 - Default Bayesian analysis for multi-way tables: a data-augmentation approach</summary>

- *Nicholas G. Polson, James G. Scott*

- `1109.4180v1` - [abs](http://arxiv.org/abs/1109.4180v1) - [pdf](http://arxiv.org/pdf/1109.4180v1)

> This paper proposes a strategy for regularized estimation in multi-way contingency tables, which are common in meta-analyses and multi-center clinical trials. Our approach is based on data augmentation, and appeals heavily to a novel class of Polya-Gamma distributions. Our main contributions are to build up the relevant distributional theory and to demonstrate three useful features of this data-augmentation scheme. First, it leads to simple EM and Gibbs-sampling algorithms for posterior inference, circumventing the need for analytic approximations, numerical integration, Metropolis--Hastings, or variational methods. Second, it allows modelers much more flexibility when choosing priors, which have traditionally come from the Dirichlet or logistic-normal family. For example, our approach allows users to incorporate Bayesian analogues of classical penalized-likelihood techniques (e.g. the lasso or bridge) in computing regularized estimates for log-odds ratios. Finally, our data-augmentation scheme naturally suggests a default strategy for prior selection based on the logistic-Z model, which is strongly related to Jeffreys' prior for a binomial proportion. To illustrate the method we focus primarily on the particular case of a meta-analysis/multi-center study (or a JxKxN table). But the general approach encompasses many other common situations, of which we will provide examples.

</details>

<details>

<summary>2011-09-21 08:13:36 - Tree Exploration for Bayesian RL Exploration</summary>

- *Christos Dimitrakakis*

- `0902.0392v2` - [abs](http://arxiv.org/abs/0902.0392v2) - [pdf](http://arxiv.org/pdf/0902.0392v2)

> Research in reinforcement learning has produced algorithms for optimal decision making under uncertainty that fall within two main types. The first employs a Bayesian framework, where optimality improves with increased computational time. This is because the resulting planning task takes the form of a dynamic programming problem on a belief tree with an infinite number of states. The second type employs relatively simple algorithm which are shown to suffer small regret within a distribution-free framework. This paper presents a lower bound and a high probability upper bound on the optimal value function for the nodes in the Bayesian belief tree, which are analogous to similar bounds in POMDPs. The bounds are then used to create more efficient strategies for exploring the tree. The resulting algorithms are compared with the distribution-free algorithm UCB1, as well as a simpler baseline algorithm on multi-armed bandit problems.

</details>

<details>

<summary>2011-09-22 11:32:31 - Beta-Product Poisson-Dirichlet Processes</summary>

- *Federico Bassetti, Roberto Casarin, Fabrizio Leisen*

- `1109.4777v1` - [abs](http://arxiv.org/abs/1109.4777v1) - [pdf](http://arxiv.org/pdf/1109.4777v1)

> Time series data may exhibit clustering over time and, in a multiple time series context, the clustering behavior may differ across the series. This paper is motivated by the Bayesian non--parametric modeling of the dependence between the clustering structures and the distributions of different time series. We follow a Dirichlet process mixture approach and introduce a new class of multivariate dependent Dirichlet processes (DDP). The proposed DDP are represented in terms of vector of stick-breaking processes with dependent weights. The weights are beta random vectors that determine different and dependent clustering effects along the dimension of the DDP vector. We discuss some theoretical properties and provide an efficient Monte Carlo Markov Chain algorithm for posterior computation. The effectiveness of the method is illustrated with a simulation study and an application to the United States and the European Union industrial production indexes.

</details>

<details>

<summary>2011-09-24 15:50:45 - Controlling the degree of caution in statistical inference with the Bayesian and frequentist approaches as opposite extremes</summary>

- *David R. Bickel*

- `1109.5278v1` - [abs](http://arxiv.org/abs/1109.5278v1) - [pdf](http://arxiv.org/pdf/1109.5278v1)

> In statistical practice, whether a Bayesian or frequentist approach is used in inference depends not only on the availability of prior information but also on the attitude taken toward partial prior information, with frequentists tending to be more cautious than Bayesians. The proposed framework defines that attitude in terms of a specified amount of caution, thereby enabling data analysis at the level of caution desired and on the basis of any prior information. The caution parameter represents the attitude toward partial prior information in much the same way as a loss function represents the attitude toward risk. When there is very little prior information and nonzero caution, the resulting inferences correspond to those of the candidate confidence intervals and p-values that are most similar to the credible intervals and hypothesis probabilities of the specified Bayesian posterior. On the other hand, in the presence of a known physical distribution of the parameter, inferences are based only on the corresponding physical posterior. In those extremes of either negligible prior information or complete prior information, inferences do not depend on the degree of caution. Partial prior information between those two extremes leads to intermediate inferences that are more frequentistic to the extent that the caution is high and more Bayesian to the extent that the caution is low.

</details>

<details>

<summary>2011-09-25 02:50:17 - On the half-Cauchy prior for a global scale parameter</summary>

- *Nicholas G. Polson, James G. Scott*

- `1104.4937v2` - [abs](http://arxiv.org/abs/1104.4937v2) - [pdf](http://arxiv.org/pdf/1104.4937v2)

> This paper argues that the half-Cauchy distribution should replace the inverse-Gamma distribution as a default prior for a top-level scale parameter in Bayesian hierarchical models, at least for cases where a proper prior is necessary. Our arguments involve a blend of Bayesian and frequentist reasoning, and are intended to complement the original case made by Gelman (2006) in support of the folded-t family of priors. First, we generalize the half-Cauchy prior to the wider class of hypergeometric inverted-beta priors. We derive expressions for posterior moments and marginal densities when these priors are used for a top-level normal variance in a Bayesian hierarchical model. We go on to prove a proposition that, together with the results for moments and marginals, allows us to characterize the frequentist risk of the Bayes estimators under all global-shrinkage priors in the class. These theoretical results, in turn, allow us to study the frequentist properties of the half-Cauchy prior versus a wide class of alternatives. The half-Cauchy occupies a sensible 'middle ground' within this class: it performs very well near the origin, but does not lead to drastic compromises in other parts of the parameter space. This provides an alternative, classical justification for the repeated, routine use of this prior. We also consider situations where the underlying mean vector is sparse, where we argue that the usual conjugate choice of an inverse-gamma prior is particularly inappropriate, and can lead to highly distorted posterior inferences. Finally, we briefly summarize some open issues in the specification of default priors for scale terms in hierarchical models.

</details>

<details>

<summary>2011-09-25 21:53:44 - Towards Optimal Learning of Chain Graphs</summary>

- *Jose M. Peña*

- `1109.5404v1` - [abs](http://arxiv.org/abs/1109.5404v1) - [pdf](http://arxiv.org/pdf/1109.5404v1)

> In this paper, we extend Meek's conjecture (Meek 1997) from directed and acyclic graphs to chain graphs, and prove that the extended conjecture is true. Specifically, we prove that if a chain graph H is an independence map of the independence model induced by another chain graph G, then (i) G can be transformed into H by a sequence of directed and undirected edge additions and feasible splits and mergings, and (ii) after each operation in the sequence H remains an independence map of the independence model induced by G. Our result has the same important consequence for learning chain graphs from data as the proof of Meek's conjecture in (Chickering 2002) had for learning Bayesian networks from data: It makes it possible to develop efficient and asymptotically correct learning algorithms under mild assumptions.

</details>

<details>

<summary>2011-09-29 05:32:47 - Introduction</summary>

- *P. Lahiri, Eric Slud*

- `1109.6405v1` - [abs](http://arxiv.org/abs/1109.6405v1) - [pdf](http://arxiv.org/pdf/1109.6405v1)

> The Statistics Consortium at the University of Maryland, College Park, hosted a two-day workshop on Bayesian Methods that Frequentists Should Know during April 30--May 1, 2008. The event was co-sponsored by the Institute of Mathematical Statistics (IMS), Office of Research and Methodology, National Center for Health Statistics, Survey Research Methods Section (SRMS) of the American Statistical Association, and Washington Statistical Society. The workshop was intended to bring out the positive features of Bayesian statistics in solving real-life problems, including complex problems in sample surveys and production of high-quality official statistics.

</details>

<details>

<summary>2011-09-29 14:55:20 - Robust Estimators in Generalized Pareto Models</summary>

- *Peter Ruckdeschel, Nataliya Horbenko*

- `1005.1476v6` - [abs](http://arxiv.org/abs/1005.1476v6) - [pdf](http://arxiv.org/pdf/1005.1476v6)

> This paper deals with optimally-robust parameter estimation in generalized Pareto distributions (GPDs). These arise naturally in many situations where one is interested in the behavior of extreme events as motivated by the Pickands-Balkema-de Haan extreme value theorem (PBHT). The application we have in mind is calculation of the regulatory capital required by Basel II for a bank to cover operational risk. In this context the tail behavior of the underlying distribution is crucial. This is where extreme value theory enters, suggesting to estimate these high quantiles parameterically using, e.g. GPDs. Robust statistics in this context offers procedures bounding the influence of single observations, so provides reliable inference in the presence of moderate deviations from the distributional model assumptions, respectively from the mechanisms underlying the PBHT.

</details>

<details>

<summary>2011-09-30 22:38:47 - Bayes Multiple Decision Functions</summary>

- *Wensong Wu, Edsel A. Peña*

- `1110.0043v1` - [abs](http://arxiv.org/abs/1110.0043v1) - [pdf](http://arxiv.org/pdf/1110.0043v1)

> This paper deals with the problem of simultaneously making many (M) binary decisions based on one realization of a random data matrix X. M is typically large and X will usually have M rows associated with each of the M decisions to make, but for each row the data may be low dimensional. A Bayesian decision-theoretic approach for this problem is implemented with the overall loss function being a cost-weighted linear combination of Type I and Type II loss functions. The class of loss functions considered allows for the use of the false discovery rate (FDR), false nondiscovery rate (FNR), and missed discovery rate (MDR) in assessing the decision. Through this Bayesian paradigm, the Bayes multiple decision function (BMDF) is derived and an efficient algorithm to obtain the optimal Bayes action is described. In contrast to many works in the literature where the rows of the matrix X are assumed to be stochastically independent, we allow in this paper a dependent data structure with the associations obtained through a class of frailty-induced Archimedean copulas. In particular, non-Gaussian dependent data structure, which is the norm rather than the exception when dealing with failure-time data, can be entertained. The numerical implementation of the determination of the Bayes optimal action is facilitated through sequential Monte Carlo techniques. The main theory developed could also be extended to the problem of multiple hypotheses testing, multiple classification and prediction, and high-dimensional variable selection. The proposed procedure is illustrated for the simple versus simple and for the composite hypotheses setting via simulation studies. The procedure is also applied to a subset of a real microarray data set from a colon cancer study.

</details>


## 2011-10

<details>

<summary>2011-10-05 09:16:34 - Adaptive Bayesian Quantum Tomography</summary>

- *Ferenc Huszár, Neil M. T. Houlsby*

- `1107.0895v2` - [abs](http://arxiv.org/abs/1107.0895v2) - [pdf](http://arxiv.org/pdf/1107.0895v2)

> In this letter we revisit the problem of optimal design of quantum tomographic experiments. In contrast to previous approaches where an optimal set of measurements is decided in advance of the experiment, we allow for measurements to be adaptively and efficiently re-optimised depending on data collected so far. We develop an adaptive statistical framework based on Bayesian inference and Shannon's information, and demonstrate a ten-fold reduction in the total number of measurements required as compared to non-adaptive methods, including mutually unbiased bases.

</details>

<details>

<summary>2011-10-06 11:35:03 - Constrained probability distributions of correlation functions</summary>

- *David Keitel, Peter Schneider*

- `1105.3672v2` - [abs](http://arxiv.org/abs/1105.3672v2) - [pdf](http://arxiv.org/pdf/1105.3672v2)

> Context: Two-point correlation functions are used throughout cosmology as a measure for the statistics of random fields. When used in Bayesian parameter estimation, their likelihood function is usually replaced by a Gaussian approximation. However, this has been shown to be insufficient.   Aims: For the case of Gaussian random fields, we search for an exact probability distribution of correlation functions, which could improve the accuracy of future data analyses.   Methods: We use a fully analytic approach, first expanding the random field in its Fourier modes, and then calculating the characteristic function. Finally, we derive the probability distribution function using integration by residues. We use a numerical implementation of the full analytic formula to discuss the behaviour of this function.   Results: We derive the univariate and bivariate probability distribution function of the correlation functions of a Gaussian random field, and outline how higher joint distributions could be calculated. We give the results in the form of mode expansions, but in one special case we also find a closed-form expression. We calculate the moments of the distribution and, in the univariate case, we discuss the Edgeworth expansion approximation. We also comment on the difficulties in a fast and exact numerical implementation of our results, and on possible future applications.

</details>

<details>

<summary>2011-10-06 19:23:14 - Sparse single-index model</summary>

- *Pierre Alquier, Gérard Biau*

- `1101.3229v2` - [abs](http://arxiv.org/abs/1101.3229v2) - [pdf](http://arxiv.org/pdf/1101.3229v2)

> Let $(\bX, Y)$ be a random pair taking values in $\mathbb R^p \times \mathbb R$. In the so-called single-index model, one has $Y=f^{\star}(\theta^{\star T}\bX)+\bW$, where $f^{\star}$ is an unknown univariate measurable function, $\theta^{\star}$ is an unknown vector in $\mathbb R^d$, and $W$ denotes a random noise satisfying $\mathbb E[\bW|\bX]=0$. The single-index model is known to offer a flexible way to model a variety of high-dimensional real-world phenomena. However, despite its relative simplicity, this dimension reduction scheme is faced with severe complications as soon as the underlying dimension becomes larger than the number of observations ("$p$ larger than $n$" paradigm). To circumvent this difficulty, we consider the single-index model estimation problem from a sparsity perspective using a PAC-Bayesian approach. On the theoretical side, we offer a sharp oracle inequality, which is more powerful than the best known oracle inequalities for other common procedures of single-index recovery. The proposed method is implemented by means of the reversible jump Markov chain Monte Carlo technique and its performance is compared with that of standard procedures.

</details>

<details>

<summary>2011-10-08 04:19:47 - Shrinking the Quadratic Estimator</summary>

- *Ethan Anderes, Debashis Paul*

- `1110.1694v1` - [abs](http://arxiv.org/abs/1110.1694v1) - [pdf](http://arxiv.org/pdf/1110.1694v1)

> We study a regression characterization for the quadratic estimator of weak lensing, developed by Hu and Okamoto (2001,2002), for cosmic microwave background observations. This characterization motivates a modification of the quadratic estimator by an adaptive Wiener filter which uses the robust Bayesian techniques described in Strawderman (1971) and Berger (1980). This technique requires the user to propose a fiducial model for the spectral density of the unknown lensing potential but the resulting estimator is developed to be robust to misspecification of this model. The role of the fiducial spectral density is to give the estimator superior statistical performance in a "neighborhood of the fiducial model" while controlling the statistical errors when the fiducial spectral density is drastically wrong. Our estimate also highlights some advantages provided by a Bayesian analysis of the quadratic estimator.

</details>

<details>

<summary>2011-10-08 14:34:43 - A-Collapsibility of Distribution Dependence and Quantile Regression Coefficients</summary>

- *P. Vellaisamy*

- `0906.5546v4` - [abs](http://arxiv.org/abs/0906.5546v4) - [pdf](http://arxiv.org/pdf/0906.5546v4)

> The Yule-Simpson paradox notes that an association between random variables can be reversed when averaged over a background variable. Cox and Wermuth (2003) introduced the concept of distribution dependence between two random variables X and Y, and developed two dependence conditions, each of which guarantees that reversal cannot occur. Ma, Xie and Geng (2006) studied the collapsibility of distribution dependence over a background variable W, under a rather strong homogeneity condition. Collapsibility ensures the association remains the same for conditional and marginal models, so that Yule-Simpson reversal cannot occur. In this paper, we investigate a more general condition for avoiding effect reversal: A-collapsibility. The conditions of Cox and Wermuth imply A-collapsibility, without assuming homogeneity. In fact, we show that, when W is a binary variable, collapsibility is equivalent to A-collapsibility plus homogeneity, and A-collapsibility is equivalent to the conditions of Cox and Wermuth. Recently, Cox (2007) extended Cochran's result on regression coefficients of conditional and marginal models, to quantile regression coefficients. The conditions of Cox and Wermuth are sufficient for A-collapsibility of quantile regression coefficients. If the conditional distribution of W, given Y = y and X = x, belong to one-dimensional natural exponential family, they are also necessary. Some applications of A-collapsibility include the analysis of a contingency table, linear regression models and quantile regression models.

</details>

<details>

<summary>2011-10-09 20:00:33 - Asymptotically Independent Markov Sampling: a new MCMC scheme for Bayesian Inference</summary>

- *James L. Beck, Konstantin M. Zuev*

- `1110.1880v1` - [abs](http://arxiv.org/abs/1110.1880v1) - [pdf](http://arxiv.org/pdf/1110.1880v1)

> In Bayesian statistics, many problems can be expressed as the evaluation of the expectation of a quantity of interest with respect to the posterior distribution. Standard Monte Carlo method is often not applicable because the encountered posterior distributions cannot be sampled directly. In this case, the most popular strategies are the importance sampling method, Markov chain Monte Carlo, and annealing. In this paper, we introduce a new scheme for Bayesian inference, called Asymptotically Independent Markov Sampling (AIMS), which is based on the above methods. We derive important ergodic properties of AIMS. In particular, it is shown that, under certain conditions, the AIMS algorithm produces a uniformly ergodic Markov chain. The choice of the free parameters of the algorithm is discussed and recommendations are provided for this choice, both theoretically and heuristically based. The efficiency of AIMS is demonstrated with three numerical examples, which include both multi-modal and higher-dimensional target posterior distributions.

</details>

<details>

<summary>2011-10-11 04:20:03 - Instant Replay: Investigating statistical Analysis in Sports</summary>

- *Gagan Sidhu*

- `1102.5549v4` - [abs](http://arxiv.org/abs/1102.5549v4) - [pdf](http://arxiv.org/pdf/1102.5549v4)

> Technology has had an unquestionable impact on the way people watch sports. Along with this technological evolution has come a higher standard to ensure a good viewing experience for the casual sports fan. It can be argued that the pervasion of statistical analysis in sports serves to satiate the fan's desire for detailed sports statistics. The goal of statistical analysis in sports is a simple one: to eliminate subjective analysis. In this paper, we review previous work that attempts to analyze various aspects in sports by using ideas from Markov Chains, Bayesian Inference and Markov Chain Monte Carlo (MCMC) methods. The unifying goal of these works is to achieve an accurate representation of the player's ability, the sport, or the environmental effects on the player's performance. With the prevalence of cheap computation, it is possible that using techniques in Artificial Intelligence could improve the result of statistical analysis in sport. This is best illustrated when evaluating football using Neuro Dynamic Programming, a Control Theory paradigm heavily based on theory in Stochastic processes. The results from this method suggest that statistical analysis in sports may benefit from using ideas from the area of Control Theory or Machine Learning

</details>

<details>

<summary>2011-10-12 12:17:51 - Improving parameter learning of Bayesian nets from incomplete data</summary>

- *Giorgio Corani, Cassio P. De Campos*

- `1110.3239v1` - [abs](http://arxiv.org/abs/1110.3239v1) - [pdf](http://arxiv.org/pdf/1110.3239v1)

> This paper addresses the estimation of parameters of a Bayesian network from incomplete data. The task is usually tackled by running the Expectation-Maximization (EM) algorithm several times in order to obtain a high log-likelihood estimate. We argue that choosing the maximum log-likelihood estimate (as well as the maximum penalized log-likelihood and the maximum a posteriori estimate) has severe drawbacks, being affected both by overfitting and model uncertainty. Two ideas are discussed to overcome these issues: a maximum entropy approach and a Bayesian model averaging approach. Both ideas can be easily applied on top of EM, while the entropy idea can be also implemented in a more sophisticated way, through a dedicated non-linear solver. A vast set of experiments shows that these ideas produce significantly better estimates and inferences than the traditional and widely used maximum (penalized) log-likelihood and maximum a posteriori estimates. In particular, if EM is adopted as optimization engine, the model averaging approach is the best performing one; its performance is matched by the entropy approach when implemented using the non-linear solver. The results suggest that the applicability of these ideas is immediate (they are easy to implement and to integrate in currently available inference engines) and that they constitute a better way to learn Bayesian network parameters.

</details>

<details>

<summary>2011-10-14 13:26:09 - Bayesian Group Factor Analysis</summary>

- *Seppo Virtanen, Arto Klami, Suleiman A. Khan, Samuel Kaski*

- `1110.3204v1` - [abs](http://arxiv.org/abs/1110.3204v1) - [pdf](http://arxiv.org/pdf/1110.3204v1)

> We introduce a factor analysis model that summarizes the dependencies between observed variable groups, instead of dependencies between individual variables as standard factor analysis does. A group may correspond to one view of the same set of objects, one of many data sets tied by co-occurrence, or a set of alternative variables collected from statistics tables to measure one property of interest. We show that by assuming group-wise sparse factors, active in a subset of the sets, the variation can be decomposed into factors explaining relationships between the sets and factors explaining away set-specific variation. We formulate the assumptions in a Bayesian model which provides the factors, and apply the model to two data analysis tasks, in neuroimaging and chemical systems biology.

</details>

<details>

<summary>2011-10-14 15:01:11 - Optimal Reinforcement Learning for Gaussian Systems</summary>

- *Philipp Hennig*

- `1106.0800v3` - [abs](http://arxiv.org/abs/1106.0800v3) - [pdf](http://arxiv.org/pdf/1106.0800v3)

> The exploration-exploitation trade-off is among the central challenges of reinforcement learning. The optimal Bayesian solution is intractable in general. This paper studies to what extent analytic statements about optimal learning are possible if all beliefs are Gaussian processes. A first order approximation of learning of both loss and dynamics, for nonlinear, time-varying systems in continuous time and space, subject to a relatively weak restriction on the dynamics, is described by an infinite-dimensional partial differential equation. An approximate finite-dimensional projection gives an impression for how this result may be helpful.

</details>

<details>

<summary>2011-10-14 16:01:57 - Modelling the impact of human activity on nitrogen dioxide concentrations in Europe</summary>

- *Gavin Shaddick, Haojie Yan, Danielle Vienneau*

- `1110.3257v1` - [abs](http://arxiv.org/abs/1110.3257v1) - [pdf](http://arxiv.org/pdf/1110.3257v1)

> Ambient concentrations of many pollutants are associated with emissions due to human activity, such as road transport and other combustion sources. In this paper we consider air pollution as a multi--level phenomenon within a Bayesian hierarchical model. We examine different scales of variation in pollution concentrations ranging from large scale transboundary effects to more localised effects which are directly related to human activity. Specifically, in the first stage of the model, we isolate underlying patterns in pollution concentrations due to global factors such as underlying climate and topography, which are modelled together with spatial structure. At this stage measurements from monitoring sites located within rural areas are used which, as far as possible, are chosen to reflect background concentrations. Having isolated these global effects, in the second stage we assess the effects of human activity on pollution in urban areas. The proposed model was applied to concentrations of nitrogen dioxide measured throughout the EU for which significant increases are found to be associated with human activity in urban areas. The approach proposed here provides valuable information that could be used in performing health impact assessments and to inform policy.

</details>

<details>

<summary>2011-10-15 06:21:37 - Bayesian Post-Processor and other Enhancements of Subset Simulation for Estimating Failure Probabilities in High Dimensions</summary>

- *Konstantin M. Zuev, James L. Beck, Siu-Kui Au, Lambros S. Katafygiotis*

- `1110.3390v1` - [abs](http://arxiv.org/abs/1110.3390v1) - [pdf](http://arxiv.org/pdf/1110.3390v1)

> Estimation of small failure probabilities is one of the most important and challenging computational problems in reliability engineering. The failure probability is usually given by an integral over a high-dimensional uncertain parameter space that is difficult to evaluate numerically. This paper focuses on enhancements to Subset Simulation (SS), proposed by Au and Beck, which provides an efficient algorithm based on MCMC (Markov chain Monte Carlo) simulation for computing small failure probabilities for general high-dimensional reliability problems. First, we analyze the Modified Metropolis algorithm (MMA), an MCMC technique, which is used in SS for sampling from high-dimensional conditional distributions. We present some observations on the optimal scaling of MMA, and develop an optimal scaling strategy for this algorithm when it is employed within SS. Next, we provide a theoretical basis for the optimal value of the conditional failure probability $p_0$, an important parameter one has to choose when using SS. Finally, a Bayesian post-processor SS+ for the original SS method is developed where the uncertain failure probability that one is estimating is modeled as a stochastic variable whose possible values belong to the unit interval. Simulated samples from SS are viewed as informative data relevant to the system's reliability. Instead of a single real number as an estimate, SS+ produces the posterior PDF of the failure probability, which takes into account both prior information and the information in the sampled data. This PDF quantifies the uncertainty in the value of the failure probability and it may be further used in risk analyses to incorporate this uncertainty. The relationship between the original SS and SS+ is also discussed

</details>

<details>

<summary>2011-10-15 07:16:52 - Multi-Domain Sampling With Applications to Structural Inference of Bayesian Networks</summary>

- *Qing Zhou*

- `1110.3392v1` - [abs](http://arxiv.org/abs/1110.3392v1) - [pdf](http://arxiv.org/pdf/1110.3392v1)

> When a posterior distribution has multiple modes, unconditional expectations, such as the posterior mean, may not offer informative summaries of the distribution. Motivated by this problem, we propose to decompose the sample space of a multimodal distribution into domains of attraction of local modes. Domain-based representations are defined to summarize the probability masses of and conditional expectations on domains of attraction, which are much more informative than the mean and other unconditional expectations. A computational method, the multi-domain sampler, is developed to construct domain-based representations for an arbitrary multimodal distribution. The multi-domain sampler is applied to structural learning of protein-signaling networks from high-throughput single-cell data, where a signaling network is modeled as a causal Bayesian network. Not only does our method provide a detailed landscape of the posterior distribution but also improves the accuracy and the predictive power of estimated networks.

</details>

<details>

<summary>2011-10-15 19:32:21 - New Entropy Estimator with an Application to Test of Normality</summary>

- *Salim Bouzebda, Issam Elhattab, Amor Keziou, Tewfik Lounis*

- `1110.3436v1` - [abs](http://arxiv.org/abs/1110.3436v1) - [pdf](http://arxiv.org/pdf/1110.3436v1)

> In the present paper we propose a new estimator of entropy based on smooth estimators of quantile density. The consistency and asymptotic distribution of the proposed estimates are obtained. As a consequence, a new test of normality is proposed. A small power comparison is provided. A simulation study for the comparison, in terms of mean squared error, of all estimators under study is performed.

</details>

<details>

<summary>2011-10-16 11:08:58 - Bayesian time series analysis of terrestrial impact cratering</summary>

- *C. A. L. Bailer-Jones*

- `1105.4100v3` - [abs](http://arxiv.org/abs/1105.4100v3) - [pdf](http://arxiv.org/pdf/1105.4100v3)

> Giant impacts by comets and asteroids have probably had an important influence on terrestrial biological evolution. We know of around 180 high velocity impact craters on the Earth with ages up to 2400Myr and diameters up to 300km. Some studies have identified a periodicity in their age distribution, with periods ranging from 13 to 50Myr. It has further been claimed that such periods may be causally linked to a periodic motion of the solar system through the Galactic plane. However, many of these studies suffer from methodological problems, for example misinterpretation of p-values, overestimation of significance in the periodogram or a failure to consider plausible alternative models. Here I develop a Bayesian method for this problem in which impacts are treated as a stochastic phenomenon. Models for the time variation of the impact probability are defined and the evidence for them in the geological record is compared using Bayes factors. This probabilistic approach obviates the need for ad hoc statistics, and also makes explicit use of the age uncertainties. I find strong evidence for a monotonic decrease in the recorded impact rate going back in time over the past 250Myr for craters larger than 5km. The same is found for the past 150Myr when craters with upper age limits are included. This is consistent with a crater preservation/discovery bias modulating an otherwise constant impact rate. The set of craters larger than 35km (so less affected by erosion and infilling) and younger than 400Myr are best explained by a constant impact probability model. A periodic variation in the cratering rate is strongly disfavoured in all data sets. There is also no evidence for a periodicity superimposed on a constant rate or trend, although this more complex signal would be harder to distinguish.

</details>

<details>

<summary>2011-10-19 11:00:46 - Projective Limit Random Probabilities on Polish Spaces</summary>

- *Peter Orbanz*

- `1101.4657v3` - [abs](http://arxiv.org/abs/1101.4657v3) - [pdf](http://arxiv.org/pdf/1101.4657v3)

> A pivotal problem in Bayesian nonparametrics is the construction of prior distributions on the space M(V) of probability measures on a given domain V. In principle, such distributions on the infinite-dimensional space M(V) can be constructed from their finite-dimensional marginals---the most prominent example being the construction of the Dirichlet process from finite-dimensional Dirichlet distributions. This approach is both intuitive and applicable to the construction of arbitrary distributions on M(V), but also hamstrung by a number of technical difficulties. We show how these difficulties can be resolved if the domain V is a Polish topological space, and give a representation theorem directly applicable to the construction of any probability distribution on M(V) whose first moment measure is well-defined. The proof draws on a projective limit theorem of Bochner, and on properties of set functions on Polish spaces to establish countable additivity of the resulting random probabilities.

</details>

<details>

<summary>2011-10-19 13:52:34 - Implicit inequality constraints in a binary tree model</summary>

- *Piotr Zwiernik, Jim Q. Smith*

- `0904.1980v6` - [abs](http://arxiv.org/abs/0904.1980v6) - [pdf](http://arxiv.org/pdf/0904.1980v6)

> In this paper we investigate the geometry of a discrete Bayesian network whose graph is a tree all of whose variables are binary and the only observed variables are those labeling its leaves. We provide the full geometric description of these models which is given by a set of polynomial equations together with a set of complementary implied inequalities induced by the positivity of probabilities on hidden variables. The phylogenetic invariants given by the equations can be useful in the construction of simple diagnostic tests. However, in this paper we point out the importance of also incorporating the associated inequalities into any statistical analysis. The full characterization of these inequality constraints derived in this paper helps us determine how and why routine statistical methods can break down for this model class.

</details>

<details>

<summary>2011-10-19 21:03:02 - Functional Uniform Priors for Nonlinear Modelling</summary>

- *Björn Bornkamp*

- `1110.4400v1` - [abs](http://arxiv.org/abs/1110.4400v1) - [pdf](http://arxiv.org/pdf/1110.4400v1)

> This paper considers the topic of finding prior distributions when a major component of the statistical model depends on a nonlinear function. Using results on how to construct uniform distributions in general metric spaces, we propose a prior distribution that is uniform in the space of functional shapes of the underlying nonlinear function and then back-transform to obtain a prior distribution for the original model parameters. The primary application considered in this article is nonlinear regression, but the idea might be of interest beyond this case. For nonlinear regression the so constructed priors have the advantage that they are parametrization invariant and do not violate the likelihood principle, as opposed to uniform distributions on the parameters or the Jeffrey's prior, respectively. The utility of the proposed priors is demonstrated in the context of nonlinear regression modelling in clinical dose-finding trials, through a real data example and simulation. In addition the proposed priors are used for calculation of an optimal Bayesian design.

</details>

<details>

<summary>2011-10-19 22:18:03 - Gaussian Process Regression Networks</summary>

- *Andrew Gordon Wilson, David A. Knowles, Zoubin Ghahramani*

- `1110.4411v1` - [abs](http://arxiv.org/abs/1110.4411v1) - [pdf](http://arxiv.org/pdf/1110.4411v1)

> We introduce a new regression framework, Gaussian process regression networks (GPRN), which combines the structural properties of Bayesian neural networks with the non-parametric flexibility of Gaussian processes. This model accommodates input dependent signal and noise correlations between multiple response variables, input dependent length-scales and amplitudes, and heavy-tailed predictive distributions. We derive both efficient Markov chain Monte Carlo and variational Bayes inference procedures for this model. We apply GPRN as a multiple output regression and multivariate volatility model, demonstrating substantially improved performance over eight popular multiple output (multi-task) Gaussian process models and three multivariate volatility models on benchmark datasets, including a 1000 dimensional gene expression dataset.

</details>

<details>

<summary>2011-10-20 22:54:02 - Bayes factors and the geometry of discrete hierarchical loglinear models</summary>

- *Gerard Letac, Helene Massam*

- `1103.5381v3` - [abs](http://arxiv.org/abs/1103.5381v3) - [pdf](http://arxiv.org/pdf/1103.5381v3)

> A standard tool for model selection in a Bayesian framework is the Bayes factor which compares the marginal likelihood of the data under two given different models. In this paper, we consider the class of hierarchical loglinear models for discrete data given under the form of a contingency table with multinomial sampling. We assume that the Diaconis-Ylvisaker conjugate prior is the prior distribution on the loglinear parameters and the uniform is the prior distribution on the space of models. Under these conditions, the Bayes factor between two models is a function of their prior and posterior normalizing constants. These constants are functions of the hyperparameters $(m,\alpha)$ which can be interpreted respectively as marginal counts and the total count of a fictive contingency table.   We study the behaviour of the Bayes factor when $\alpha$ tends to zero. In this study two mathematical objects play a most important role. They are, first, the interior $C$ of the convex hull $\bar{C}$ of the support of the multinomial distribution for a given hierarchical loglinear model together with its faces and second, the characteristic function $\mathbb{J}_C$ of this convex set $C$.   We show that, when $\alpha$ tends to 0, if the data lies on a face $F_i$ of $\bar{C_i},i=1,2$ of dimension $k_i$, the Bayes factor behaves like $\alpha^{k_1-k_2}$. This implies in particular that when the data is in $C_1$ and in $C_2$, i.e. when $k_i$ equals the dimension of model $J_i$, the sparser model is favored, thus confirming the idea of Bayesian regularization.

</details>

<details>

<summary>2011-10-24 14:01:26 - Multiple Gaussian Process Models</summary>

- *Cedric Archambeau, Francis Bach*

- `1110.5238v1` - [abs](http://arxiv.org/abs/1110.5238v1) - [pdf](http://arxiv.org/pdf/1110.5238v1)

> We consider a Gaussian process formulation of the multiple kernel learning problem. The goal is to select the convex combination of kernel matrices that best explains the data and by doing so improve the generalisation on unseen data. Sparsity in the kernel weights is obtained by adopting a hierarchical Bayesian approach: Gaussian process priors are imposed over the latent functions and generalised inverse Gaussians on their associated weights. This construction is equivalent to imposing a product of heavy-tailed process priors over function space. A variational inference algorithm is derived for regression and binary classification.

</details>

<details>

<summary>2011-10-24 14:07:45 - Infinitely exchangeable random graphs generated from a Poisson point process on monotone sets and applications to cluster analysis for networks</summary>

- *Harry Crane*

- `1110.4088v2` - [abs](http://arxiv.org/abs/1110.4088v2) - [pdf](http://arxiv.org/pdf/1110.4088v2)

> We construct an infinitely exchangeable process on the set $\cate$ of subsets of the power set of the natural numbers $\mathbb{N}$ via a Poisson point process with mean measure $\Lambda$ on the power set of $\mathbb{N}$. Each $E\in\cate$ has a least monotone cover in $\catf$, the collection of monotone subsets of $\cate$, and every monotone subset maps to an undirected graph $G\in\catg$, the space of undirected graphs with vertex set $\mathbb{N}$. We show a natural mapping $\cate\rightarrow\catf\rightarrow\catg$ which induces an infinitely exchangeable measure on the projective system $\catg^{\rest}$ of graphs $\catg$ under permutation and restriction mappings given an infinitely exchangeable family of measures on the projective system $\cate^{\rest}$ of subsets with permutation and restriction maps. We show potential connections of this process to applications in cluster analysis, machine learning, classification and Bayesian inference.

</details>

<details>

<summary>2011-10-27 09:07:17 - Ordinal Risk-Group Classification</summary>

- *Yizhar Toren*

- `1012.5487v4` - [abs](http://arxiv.org/abs/1012.5487v4) - [pdf](http://arxiv.org/pdf/1012.5487v4)

> Most classification methods provide either a prediction of class membership or an assessment of class membership probability. In the case of two-group classification the predicted probability can be described as "risk" of belonging to a "special" class . When the required output is a set of ordinal-risk groups, a discretization of the continuous risk prediction is achieved by two common methods: by constructing a set of models that describe the conditional risk function at specific points (quantile regression) or by dividing the output of an "optimal" classification model into adjacent intervals that correspond to the desired risk groups. By defining a new error measure for the distribution of risk onto intervals we are able to identify lower bounds on the accuracy of these methods, showing sub-optimality both in their distribution of risk and in the efficiency of their resulting partition into intervals. By adding a new form of constraint to the existing maximum likelihood optimization framework and by introducing a penalty function to avoid degenerate solutions, we show how existing methods can be augmented to solve the ordinal risk-group classification problem. We implement our method for logistic regression (LR) and show a numeric example.

</details>

<details>

<summary>2011-10-27 09:15:15 - Bayesian variable selection regression for genome-wide association studies and other large-scale problems</summary>

- *Yongtao Guan, Matthew Stephens*

- `1110.6019v1` - [abs](http://arxiv.org/abs/1110.6019v1) - [pdf](http://arxiv.org/pdf/1110.6019v1)

> We consider applying Bayesian Variable Selection Regression, or BVSR, to genome-wide association studies and similar large-scale regression problems. Currently, typical genome-wide association studies measure hundreds of thousands, or millions, of genetic variants (SNPs), in thousands or tens of thousands of individuals, and attempt to identify regions harboring SNPs that affect some phenotype or outcome of interest. This goal can naturally be cast as a variable selection regression problem, with the SNPs as the covariates in the regression. Characteristic features of genome-wide association studies include the following: (i) a focus primarily on identifying relevant variables, rather than on prediction; and (ii) many relevant covariates may have tiny effects, making it effectively impossible to confidently identify the complete "correct" subset of variables. Taken together, these factors put a premium on having interpretable measures of confidence for individual covariates being included in the model, which we argue is a strength of BVSR compared with alternatives such as penalized regression methods. Here we focus primarily on analysis of quantitative phenotypes, and on appropriate prior specification for BVSR in this setting, emphasizing the idea of considering what the priors imply about the total proportion of variance in outcome explained by relevant covariates. We also emphasize the potential for BVSR to estimate this proportion of variance explained, and hence shed light on the issue of "missing heritability" in genome-wide association studies.

</details>

<details>

<summary>2011-10-27 20:00:05 - Parameter Estimation with BEAMS in the presence of biases and correlations</summary>

- *James Newling, Bruce. A. Bassett, Renée Hlozek, Martin Kunz, Mathew Smith, Melvin Varughese*

- `1110.6178v1` - [abs](http://arxiv.org/abs/1110.6178v1) - [pdf](http://arxiv.org/pdf/1110.6178v1)

> The original formulation of BEAMS - Bayesian Estimation Applied to Multiple Species - showed how to use a dataset contaminated by points of multiple underlying types to perform unbiased parameter estimation. An example is cosmological parameter estimation from a photometric supernova sample contaminated by unknown Type Ibc and II supernovae. Where other methods require data cuts to increase purity, BEAMS uses all of the data points in conjunction with their probabilities of being each type. Here we extend the BEAMS formalism to allow for correlations between the data and the type probabilities of the objects as can occur in realistic cases. We show with simple simulations that this extension can be crucial, providing a 50% reduction in parameter estimation variance when such correlations do exist. We then go on to perform tests to quantify the importance of the type probabilities, one of which illustrates the effect of biasing the probabilities in various ways. Finally, a general presentation of the selection bias problem is given, and discussed in the context of future photometric supernova surveys and BEAMS, which lead to specific recommendations for future supernova surveys.

</details>

<details>

<summary>2011-10-29 05:23:36 - Bayesian Optimization for Adaptive MCMC</summary>

- *Nimalan Mahendran, Ziyu Wang, Firas Hamze, Nando de Freitas*

- `1110.6497v1` - [abs](http://arxiv.org/abs/1110.6497v1) - [pdf](http://arxiv.org/pdf/1110.6497v1)

> This paper proposes a new randomized strategy for adaptive MCMC using Bayesian optimization. This approach applies to non-differentiable objective functions and trades off exploration and exploitation to reduce the number of potentially costly objective function evaluations. We demonstrate the strategy in the complex setting of sampling from constrained, discrete and densely connected probabilistic graphical models where, for each variation of the problem, one needs to adjust the parameters of the proposal mechanism automatically to ensure efficient mixing of the Markov chains.

</details>

<details>

<summary>2011-10-29 18:36:00 - Efficient Marginal Likelihood Computation for Gaussian Process Regression</summary>

- *Andrea Schirru, Simone Pampuri, Giuseppe De Nicolao, Sean McLoone*

- `1110.6546v1` - [abs](http://arxiv.org/abs/1110.6546v1) - [pdf](http://arxiv.org/pdf/1110.6546v1)

> In a Bayesian learning setting, the posterior distribution of a predictive model arises from a trade-off between its prior distribution and the conditional likelihood of observed data. Such distribution functions usually rely on additional hyperparameters which need to be tuned in order to achieve optimum predictive performance; this operation can be efficiently performed in an Empirical Bayes fashion by maximizing the posterior marginal likelihood of the observed data. Since the score function of this optimization problem is in general characterized by the presence of local optima, it is necessary to resort to global optimization strategies, which require a large number of function evaluations. Given that the evaluation is usually computationally intensive and badly scaled with respect to the dataset size, the maximum number of observations that can be treated simultaneously is quite limited. In this paper, we consider the case of hyperparameter tuning in Gaussian process regression. A straightforward implementation of the posterior log-likelihood for this model requires O(N^3) operations for every iteration of the optimization procedure, where N is the number of examples in the input dataset. We derive a novel set of identities that allow, after an initial overhead of O(N^3), the evaluation of the score function, as well as the Jacobian and Hessian matrices, in O(N) operations. We prove how the proposed identities, that follow from the eigendecomposition of the kernel matrix, yield a reduction of several orders of magnitude in the computation time for the hyperparameter optimization problem. Notably, the proposed solution provides computational advantages even with respect to state of the art approximations that rely on sparse kernel matrices.

</details>


## 2011-11

<details>

<summary>2011-11-02 19:28:51 - The partition problem: case studies in Bayesian screening for time-varying model structure</summary>

- *Zesong Liu, Jesse Windle, James G. Scott*

- `1111.0617v1` - [abs](http://arxiv.org/abs/1111.0617v1) - [pdf](http://arxiv.org/pdf/1111.0617v1)

> This paper presents two case studies of data sets where the main inferential goal is to characterize time-varying patterns in model structure. Both of these examples are seen to be general cases of the so-called "partition problem," where auxiliary information (in this case, time) defines a partition over sample space, and where different models hold for each element of the partition. In the first case study, we identify time-varying graphical structure in the covariance matrix of asset returns from major European equity indices from 2006--2010. This structure has important implications for quantifying the notion of financial contagion, a term often mentioned in the context of the European sovereign debt crisis of this period. In the second case study, we screen a large database of historical corporate performance in order to identify specific firms with impressively good (or bad) streaks of performance.

</details>

<details>

<summary>2011-11-06 16:42:19 - Inherent Difficulties of Non-Bayesian Likelihood-based Inference, as Revealed by an Examination of a Recent Book by Aitkin</summary>

- *Andrew Gelman, Christian P. Robert, Judith Rousseau*

- `1012.2184v2` - [abs](http://arxiv.org/abs/1012.2184v2) - [pdf](http://arxiv.org/pdf/1012.2184v2)

> For many decades, statisticians have made attempts to prepare the Bayesian omelette without breaking the Bayesian eggs; that is, to obtain probabilistic likelihood-based inferences without relying on informative prior distributions. A recent example is Murray Aitkin's recent book, {\em Statistical Inference}, which presents an approach to statistical hypothesis testing based on comparisons of posterior distributions of likelihoods under competing models. Aitkin develops and illustrates his method using some simple examples of inference from iid data and two-way tests of independence. We analyze in this note some consequences of the inferential paradigm adopted therein, discussing why the approach is incompatible with a Bayesian perspective and why we do not find it relevant for applied work.

</details>

<details>

<summary>2011-11-06 20:06:22 - Bayesian quickest detection problems for some diffusion processes</summary>

- *Pavel V. Gapeev, Albert N. Shiryaev*

- `1010.3430v2` - [abs](http://arxiv.org/abs/1010.3430v2) - [pdf](http://arxiv.org/pdf/1010.3430v2)

> We study the Bayesian problems of detecting a change in the drift rate of an observable diffusion process with linear and exponential penalty costs for a detection delay. The optimal times of alarms are found as the first times at which the weighted likelihood ratios hit stochastic boundaries depending on the current observations. The proof is based on the reduction of the initial problems into appropriate three-dimensional optimal stopping problems and the analysis of the associated parabolic-type free-boundary problems. We provide closed form estimates for the value functions and the boundaries, under certain nontrivial relations between the coefficients of the observable diffusion.

</details>

<details>

<summary>2011-11-07 09:04:51 - The potential for bias in principal causal effect estimation when treatment received depends on a key covariate</summary>

- *Corwin M. Zigler, Thomas R. Belin*

- `1111.1509v1` - [abs](http://arxiv.org/abs/1111.1509v1) - [pdf](http://arxiv.org/pdf/1111.1509v1)

> Motivated by a potential-outcomes perspective, the idea of principal stratification has been widely recognized for its relevance in settings susceptible to posttreatment selection bias such as randomized clinical trials where treatment received can differ from treatment assigned. In one such setting, we address subtleties involved in inference for causal effects when using a key covariate to predict membership in latent principal strata. We show that when treatment received can differ from treatment assigned in both study arms, incorporating a stratum-predictive covariate can make estimates of the "complier average causal effect" (CACE) derive from observations in the two treatment arms with different covariate distributions. Adopting a Bayesian perspective and using Markov chain Monte Carlo for computation, we develop posterior checks that characterize the extent to which incorporating the pretreatment covariate endangers estimation of the CACE. We apply the method to analyze a clinical trial comparing two treatments for jaw fractures in which the study protocol allowed surgeons to overrule both possible randomized treatment assignments based on their clinical judgment and the data contained a key covariate (injury severity) predictive of treatment received.

</details>

<details>

<summary>2011-11-08 16:28:05 - Estimating the evidence -- a review</summary>

- *Nial Friel, Jason Wyse*

- `1111.1957v1` - [abs](http://arxiv.org/abs/1111.1957v1) - [pdf](http://arxiv.org/pdf/1111.1957v1)

> The model evidence is a vital quantity in the comparison of statistical models under the Bayesian paradigm. This paper presents a review of commonly used methods. We outline some guidelines and offer some practical advice. The reviewed methods are compared for two examples; non-nested Gaussian linear regression and covariate subset selection in logistic regression.

</details>

<details>

<summary>2011-11-09 09:04:40 - Sequential Monte Carlo on large binary sampling spaces</summary>

- *Christian Schäfer, Nicolas Chopin*

- `1101.6037v4` - [abs](http://arxiv.org/abs/1101.6037v4) - [pdf](http://arxiv.org/pdf/1101.6037v4)

> A Monte Carlo algorithm is said to be adaptive if it automatically calibrates its current proposal distribution using past simulations. The choice of the parametric family that defines the set of proposal distributions is critical for good performance. In this paper, we present such a parametric family for adaptive sampling on high-dimensional binary spaces. A practical motivation for this problem is variable selection in a linear regression context. We want to sample from a Bayesian posterior distribution on the model space using an appropriate version of Sequential Monte Carlo. Raw versions of Sequential Monte Carlo are easily implemented using binary vectors with independent components. For high-dimensional problems, however, these simple proposals do not yield satisfactory results. The key to an efficient adaptive algorithm are binary parametric families which take correlations into account, analogously to the multivariate normal distribution on continuous spaces. We provide a review of models for binary data and make one of them work in the context of Sequential Monte Carlo sampling. Computational studies on real life data with about a hundred covariates suggest that, on difficult instances, our Sequential Monte Carlo approach clearly outperforms standard techniques based on Markov chain exploration.

</details>

<details>

<summary>2011-11-09 16:27:18 - Towards Uncertainty Quantification and Inference in the stochastic SIR Epidemic Model</summary>

- *Marcos A. Capistrán, J. Andrés Christen, Jorge X. Velasco-Hernández*

- `1111.2260v1` - [abs](http://arxiv.org/abs/1111.2260v1) - [pdf](http://arxiv.org/pdf/1111.2260v1)

> In this paper we introduce a novel method to conduct inference with models defined through a continuous-time Markov process, and we apply these results to a classical stochastic SIR model as a case study. Using the inverse-size expansion of van Kampen we obtain approximations for first and second moments for the state variables. These approximate moments are in turn matched to the moments of an inputed generic discrete distribution aimed at generating an approximate likelihood that is valid both for low count or high count data. We conduct a full Bayesian inference to estimate epidemic parameters using informative priors. Excellent estimations and predictions are obtained both in a synthetic data scenario and in two Dengue fever case studies.

</details>

<details>

<summary>2011-11-10 21:01:19 - Issues in designing hybrid algorithms</summary>

- *Jeong Lee, Kerrie Mengersen, Christian Robert, Ross McVinish*

- `1111.2609v1` - [abs](http://arxiv.org/abs/1111.2609v1) - [pdf](http://arxiv.org/pdf/1111.2609v1)

> In the Bayesian community, an ongoing imperative is to develop efficient algorithms. An appealing approach is to form a hybrid algorithm by combining ideas from competing existing techniques. This paper addresses issues in designing hybrid methods by considering selected case studies: the delayed rejection algorithm, the pinball sampler, the Metropolis adjusted Langevin algorithm, and the population Monte Carlo algorithm. We observe that even if each component of a hybrid algorithm has individual strengths, they may not contribute equally or even positively when they are combined. Moreover, even if the statistical efficiency is improved, from a practical perspective there are technical issues to be considered such as applicability and computational workload. In order to optimize performance of the algorithm in real time, these issues should be taken into account.

</details>

<details>

<summary>2011-11-11 14:14:12 - Robust Bayesian reinforcement learning through tight lower bounds</summary>

- *Christos Dimitrakakis*

- `1106.3651v2` - [abs](http://arxiv.org/abs/1106.3651v2) - [pdf](http://arxiv.org/pdf/1106.3651v2)

> In the Bayesian approach to sequential decision making, exact calculation of the (subjective) utility is intractable. This extends to most special cases of interest, such as reinforcement learning problems. While utility bounds are known to exist for this problem, so far none of them were particularly tight. In this paper, we show how to efficiently calculate a lower bound, which corresponds to the utility of a near-optimal memoryless policy for the decision problem, which is generally different from both the Bayes-optimal policy and the policy which is optimal for the expected MDP under the current belief. We then show how these can be applied to obtain robust exploration policies in a Bayesian reinforcement learning setting.

</details>

<details>

<summary>2011-11-14 07:43:03 - A study of variable selection using g-prior distribution with ridge parameter</summary>

- *Meili Baragatti, Denys Pommeret*

- `1102.0470v4` - [abs](http://arxiv.org/abs/1102.0470v4) - [pdf](http://arxiv.org/pdf/1102.0470v4)

> In the Bayesian stochastic search variable selection framework, a common prior distribution for the regression coefficients is the g-prior of Zellner (1986). However, there are two standard cases in which the associated covariance matrix does not exist, and the conventional prior of Zellner can not be used: if the number of observations is lower than the number of variables (large p and small n paradigm), or if some variables are linear combinations of others. In such situations a prior distribution derived from the prior of Zellner can be used, by introducing a ridge parameter. This prior introduced by Gupta and Ibrahim (2007) is a flexible and simple adaptation of the g-prior. In this paper we study the influence of the ridge parameter on the selection of variables. A simple way to choose the associated hyper-parameters is proposed. The method is valid for any generalized linear mixed model and we focus on the case of probit mixed models when some variables are linear combinations of others. The method is applied to both simulated and real datasets obtained from Affymetrix microarray experiments. Results are compared to those obtained with the Bayesian Lasso.

</details>

<details>

<summary>2011-11-15 08:37:57 - Regularization in regression: comparing Bayesian and frequentist methods in a poorly informative situation</summary>

- *Gilles Celeux, Mohammed El Anbari, Jean-Michel Marin, Christian P. Robert*

- `1010.0300v3` - [abs](http://arxiv.org/abs/1010.0300v3) - [pdf](http://arxiv.org/pdf/1010.0300v3)

> Using a collection of simulated an real benchmarks, we compare Bayesian and frequentist regularization approaches under a low informative constraint when the number of variables is almost equal to the number of observations on simulated and real datasets. This comparison includes new global noninformative approaches for Bayesian variable selection built on Zellner's g-priors that are similar to Liang et al. (2008). The interest of those calibration-free proposals is discussed. The numerical experiments we present highlight the appeal of Bayesian regularization methods, when compared with non-Bayesian alternatives. They dominate frequentist methods in the sense that they provide smaller prediction errors while selecting the most relevant variables in a parsimonious way.

</details>

<details>

<summary>2011-11-15 14:18:39 - Besov priors for Bayesian inverse problems</summary>

- *Masoumeh Dashti, Stephen Harris, Andrew Stuart*

- `1105.0889v2` - [abs](http://arxiv.org/abs/1105.0889v2) - [pdf](http://arxiv.org/pdf/1105.0889v2)

> We consider the inverse problem of estimating a function $u$ from noisy, possibly nonlinear, observations. We adopt a Bayesian approach to the problem. This approach has a long history for inversion, dating back to 1970, and has, over the last decade, gained importance as a practical tool. However most of the existing theory has been developed for Gaussian prior measures. Recently Lassas, Saksman and Siltanen (Inv. Prob. Imag. 2009) showed how to construct Besov prior measures, based on wavelet expansions with random coefficients, and used these prior measures to study linear inverse problems. In this paper we build on this development of Besov priors to include the case of nonlinear measurements. In doing so a key technical tool, established here, is a Fernique-like theorem for Besov measures. This theorem enables us to identify appropriate conditions on the forward solution operator which, when matched to properties of the prior Besov measure, imply the well-definedness and well-posedness of the posterior measure. We then consider the application of these results to the inverse problem of finding the diffusion coefficient of an elliptic partial differential equation, given noisy measurements of its solution.

</details>

<details>

<summary>2011-11-16 16:46:48 - Sequential search based on kriging: convergence analysis of some algorithms</summary>

- *Emmanuel Vazquez, Julien Bect*

- `1111.3866v1` - [abs](http://arxiv.org/abs/1111.3866v1) - [pdf](http://arxiv.org/pdf/1111.3866v1)

> Let $\FF$ be a set of real-valued functions on a set $\XX$ and let $S:\FF \to \GG$ be an arbitrary mapping. We consider the problem of making inference about $S(f)$, with $f\in\FF$ unknown, from a finite set of pointwise evaluations of $f$. We are mainly interested in the problems of approximation and optimization. In this article, we make a brief review of results concerning average error bounds of Bayesian search methods that use a random process prior about $f$.

</details>

<details>

<summary>2011-11-17 13:52:39 - Bayesian versus frequentist upper limits</summary>

- *Christian Röver, Chris Messenger, Reinhard Prix*

- `1103.2987v3` - [abs](http://arxiv.org/abs/1103.2987v3) - [pdf](http://arxiv.org/pdf/1103.2987v3)

> While gravitational waves have not yet been measured directly, data analysis from detection experiments commonly includes an upper limit statement. Such upper limits may be derived via a frequentist or Bayesian approach; the theoretical implications are very different, and on the technical side, one notable difference is that one case requires maximization of the likelihood function over parameter space, while the other requires integration. Using a simple example (detection of a sinusoidal signal in white Gaussian noise), we investigate the differences in performance and interpretation, and the effect of the "trials factor", or "look-elsewhere effect".

</details>

<details>

<summary>2011-11-17 15:16:11 - Bayesian multitask inverse reinforcement learning</summary>

- *Christos Dimitrakakis, Constantin Rothkopf*

- `1106.3655v2` - [abs](http://arxiv.org/abs/1106.3655v2) - [pdf](http://arxiv.org/pdf/1106.3655v2)

> We generalise the problem of inverse reinforcement learning to multiple tasks, from multiple demonstrations. Each one may represent one expert trying to solve a different task, or as different experts trying to solve the same task. Our main contribution is to formalise the problem as statistical preference elicitation, via a number of structured priors, whose form captures our biases about the relatedness of different tasks or expert policies. In doing so, we introduce a prior on policy optimality, which is more natural to specify. We show that our framework allows us not only to learn to efficiently from multiple experts but to also effectively differentiate between the goals of each. Possible applications include analysing the intrinsic motivations of subjects in behavioural experiments and learning from multiple teachers.

</details>

<details>

<summary>2011-11-17 16:58:39 - Adaptive Convergence Rates of a Dirichlet Process Mixture of Multivariate Normals</summary>

- *Surya T. Tokdar*

- `1111.4148v1` - [abs](http://arxiv.org/abs/1111.4148v1) - [pdf](http://arxiv.org/pdf/1111.4148v1)

> It is shown that a simple Dirichlet process mixture of multivariate normals offers Bayesian density estimation with adaptive posterior convergence rates. Toward this, a novel sieve for non-parametric mixture densities is explored, and its rate adaptability to various smoothness classes of densities in arbitrary dimension is demonstrated. This sieve construction is expected to offer a substantial technical advancement in studying Bayesian non-parametric mixture models based on stick-breaking priors.

</details>

<details>

<summary>2011-11-17 21:28:04 - Joint Modeling of Multiple Related Time Series via the Beta Process</summary>

- *Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, Alan S. Willsky*

- `1111.4226v1` - [abs](http://arxiv.org/abs/1111.4226v1) - [pdf](http://arxiv.org/pdf/1111.4226v1)

> We propose a Bayesian nonparametric approach to the problem of jointly modeling multiple related time series. Our approach is based on the discovery of a set of latent, shared dynamical behaviors. Using a beta process prior, the size of the set and the sharing pattern are both inferred from data. We develop efficient Markov chain Monte Carlo methods based on the Indian buffet process representation of the predictive distribution of the beta process, without relying on a truncated model. In particular, our approach uses the sum-product algorithm to efficiently compute Metropolis-Hastings acceptance probabilities, and explores new dynamical behaviors via birth and death proposals. We examine the benefits of our proposed feature-based model on several synthetic datasets, and also demonstrate promising results on unsupervised segmentation of visual motion capture data.

</details>

<details>

<summary>2011-11-18 20:23:12 - On the Pickands stochastic process</summary>

- *Gane Samb Lo, Adja Mbarka Fall*

- `1111.4469v1` - [abs](http://arxiv.org/abs/1111.4469v1) - [pdf](http://arxiv.org/pdf/1111.4469v1)

> We consider the Pickands process {equation*} P_{n}(s)=\log (1/s)^{-1}\log \frac{X_{n-k+1,n}-X_{n-[k/s]+1,n}}{% X_{n-[k/s]+1,n}-X_{n-[k/s^{2}]+1,n}}, {equation*} {equation*} (\frac{k}{n}\leq s^2 \leq 1), {equation*} which is a generalization of the classical Pickands estimate $P_{n}(1/2)$ of the extremal index. We undertake here a purely stochastic process view for the asymptotic theory of that process by using the Cs\"{o}rg\H{o}-Cs\"{o}rg\H{o}-Horv\'{a}th-Mason (1986) \cite{cchm} weighted approximation of the empirical and quantile processes to suitable Brownian bridges. This leads to the uniform convergence of the margins of this process to the extremal index and a complete theory of weak convergence of $P_n$ in $\ell^{\infty}([a,b])$ to some Gaussian process $$\{\mathbb{G},a\leq s \leq b\} $$ for all $[a,b] \subset]0,1[$. This frame greatly simplifies the former results and enable applications based on stochastic processes methods.

</details>

<details>

<summary>2011-11-19 22:29:05 - Empirical Quantile CLTs for Time Dependent Data</summary>

- *James Kuelbs, Joel Zinn*

- `1111.4591v1` - [abs](http://arxiv.org/abs/1111.4591v1) - [pdf](http://arxiv.org/pdf/1111.4591v1)

> We establish empirical quantile process CLTs based on $n$ independent copies of a stochastic process $\{X_t: t \in E\}$ that are uniform in $t \in E$ and quantile levels $\alpha \in I$, where $I$ is a closed sub-interval of $(0,1)$. Typically $E=[0,T]$, or a finite product of such intervals. Also included are CLT's for the empirical process based on $\{I_{X_t \le y} - \rm {Pr}(X_t \le y): t \in E, y \in R \}$ that are uniform in $t \in E, y \in R$. The process $\{X_t: t \in E\}$ may be chosen from a broad collection of Gaussian processes, compound Poisson processes, stationary independent increment stable processes, and martingales.

</details>

<details>

<summary>2011-11-21 09:47:51 - Bayesian optimization using sequential Monte Carlo</summary>

- *Romain Benassi, Julien Bect, Emmanuel Vazquez*

- `1111.4802v1` - [abs](http://arxiv.org/abs/1111.4802v1) - [pdf](http://arxiv.org/pdf/1111.4802v1)

> We consider the problem of optimizing a real-valued continuous function $f$ using a Bayesian approach, where the evaluations of $f$ are chosen sequentially by combining prior information about $f$, which is described by a random process model, and past evaluation results. The main difficulty with this approach is to be able to compute the posterior distributions of quantities of interest which are used to choose evaluation points. In this article, we decide to use a Sequential Monte Carlo (SMC) approach.

</details>

<details>

<summary>2011-11-21 17:24:21 - Two adaptive rejection sampling schemes for probability density functions log-convex tails</summary>

- *Luca Martino, Joaquín Míguez*

- `1111.4942v1` - [abs](http://arxiv.org/abs/1111.4942v1) - [pdf](http://arxiv.org/pdf/1111.4942v1)

> Monte Carlo methods are often necessary for the implementation of optimal Bayesian estimators. A fundamental technique that can be used to generate samples from virtually any target probability distribution is the so-called rejection sampling method, which generates candidate samples from a proposal distribution and then accepts them or not by testing the ratio of the target and proposal densities. The class of adaptive rejection sampling (ARS) algorithms is particularly interesting because they can achieve high acceptance rates. However, the standard ARS method can only be used with log-concave target densities. For this reason, many generalizations have been proposed.   In this work, we investigate two different adaptive schemes that can be used to draw exactly from a large family of univariate probability density functions (pdf's), not necessarily log-concave, possibly multimodal and with tails of arbitrary concavity. These techniques are adaptive in the sense that every time a candidate sample is rejected, the acceptance rate is improved. The two proposed algorithms can work properly when the target pdf is multimodal, with first and second derivatives analytically intractable, and when the tails are log-convex in a infinite domain. Therefore, they can be applied in a number of scenarios in which the other generalizations of the standard ARS fail. Two illustrative numerical examples are shown.

</details>

<details>

<summary>2011-11-23 07:12:46 - Incorporating biological information into linear models: A Bayesian approach to the selection of pathways and genes</summary>

- *Francesco C. Stingo, Yian A. Chen, Mahlet G. Tadesse, Marina Vannucci*

- `1111.5419v1` - [abs](http://arxiv.org/abs/1111.5419v1) - [pdf](http://arxiv.org/pdf/1111.5419v1)

> The vast amount of biological knowledge accumulated over the years has allowed researchers to identify various biochemical interactions and define different families of pathways. There is an increased interest in identifying pathways and pathway elements involved in particular biological processes. Drug discovery efforts, for example, are focused on identifying biomarkers as well as pathways related to a disease. We propose a Bayesian model that addresses this question by incorporating information on pathways and gene networks in the analysis of DNA microarray data. Such information is used to define pathway summaries, specify prior distributions, and structure the MCMC moves to fit the model. We illustrate the method with an application to gene expression data with censored survival outcomes. In addition to identifying markers that would have been missed otherwise and improving prediction accuracy, the integration of existing biological knowledge into the analysis provides a better understanding of underlying molecular processes.

</details>

<details>

<summary>2011-11-23 07:23:48 - Universality of sample covariance matrices: CLT of the smoothed empirical spectral distribution</summary>

- *Guangming Pan, Qi-Man Shao, Wang Zhou*

- `1111.5420v1` - [abs](http://arxiv.org/abs/1111.5420v1) - [pdf](http://arxiv.org/pdf/1111.5420v1)

> A central limit theorem (CLT) for the smoothed empirical spectral distribution of sample covariance matrices is established. Moreover, the CLTs for the smoothed quantiles of Marcenko and Pastur's law have been also developed.

</details>

<details>

<summary>2011-11-24 18:03:37 - Error and Inference: an outsider stand on a frequentist philosophy</summary>

- *Christian P. Robert*

- `1111.5827v1` - [abs](http://arxiv.org/abs/1111.5827v1) - [pdf](http://arxiv.org/pdf/1111.5827v1)

> This note is an extended review of the book Error and Inference, edited by Deborah Mayo and Aris Spanos, about their frequentist and philosophical perspective on testing of hypothesis and on the criticisms of alternatives like the Bayesian approach.

</details>

<details>

<summary>2011-11-25 13:00:28 - Block-based Bayesian epistasis association mapping with application to WTCCC type 1 diabetes data</summary>

- *Yu Zhang, Jing Zhang, Jun S. Liu*

- `1111.5972v1` - [abs](http://arxiv.org/abs/1111.5972v1) - [pdf](http://arxiv.org/pdf/1111.5972v1)

> Interactions among multiple genes across the genome may contribute to the risks of many complex human diseases. Whole-genome single nucleotide polymorphisms (SNPs) data collected for many thousands of SNP markers from thousands of individuals under the case--control design promise to shed light on our understanding of such interactions. However, nearby SNPs are highly correlated due to linkage disequilibrium (LD) and the number of possible interactions is too large for exhaustive evaluation. We propose a novel Bayesian method for simultaneously partitioning SNPs into LD-blocks and selecting SNPs within blocks that are associated with the disease, either individually or interactively with other SNPs. When applied to homogeneous population data, the method gives posterior probabilities for LD-block boundaries, which not only result in accurate block partitions of SNPs, but also provide measures of partition uncertainty. When applied to case--control data for association mapping, the method implicitly filters out SNP associations created merely by LD with disease loci within the same blocks. Simulation study showed that this approach is more powerful in detecting multi-locus associations than other methods we tested, including one of ours. When applied to the WTCCC type 1 diabetes data, the method identified many previously known T1D associated genes, including PTPN22, CTLA4, MHC, and IL2RA.

</details>

<details>

<summary>2011-11-25 23:20:54 - Self-Avoiding Random Dynamics on Integer Complex Systems</summary>

- *Firas Hamze, Ziyu Wang, Nando de Freitas*

- `1111.5379v2` - [abs](http://arxiv.org/abs/1111.5379v2) - [pdf](http://arxiv.org/pdf/1111.5379v2)

> This paper introduces a new specialized algorithm for equilibrium Monte Carlo sampling of binary-valued systems, which allows for large moves in the state space. This is achieved by constructing self-avoiding walks (SAWs) in the state space. As a consequence, many bits are flipped in a single MCMC step. We name the algorithm SARDONICS, an acronym for Self-Avoiding Random Dynamics on Integer Complex Systems. The algorithm has several free parameters, but we show that Bayesian optimization can be used to automatically tune them. SARDONICS performs remarkably well in a broad number of sampling tasks: toroidal ferromagnetic and frustrated Ising models, 3D Ising models, restricted Boltzmann machines and chimera graphs arising in the design of quantum computers.

</details>

<details>

<summary>2011-11-26 16:31:56 - Resolving conflicts between statistical methods by probability combination: Application to empirical Bayes analyses of genomic data</summary>

- *David R. Bickel*

- `1111.6174v1` - [abs](http://arxiv.org/abs/1111.6174v1) - [pdf](http://arxiv.org/pdf/1111.6174v1)

> In the typical analysis of a data set, a single method is selected for statistical reporting even when equally applicable methods yield very different results. Examples of equally applicable methods can correspond to those of different ancillary statistics in frequentist inference and of different prior distributions in Bayesian inference. More broadly, choices are made between parametric and nonparametric methods and between frequentist and Bayesian methods.   Rather than choosing a single method, it can be safer, in a game-theoretic sense, to combine those that are equally appropriate in light of the available information. Since methods of combining subjectively assessed probability distributions are not objective enough for that purpose, this paper introduces a method of distribution combination that does not require any assignment of distribution weights. It does so by formalizing a hedging strategy in terms of a game between three players: nature, a statistician combining distributions, and a statistician refusing to combine distributions. The optimal move of the first statistician reduces to the solution of a simpler problem of selecting an estimating distribution that minimizes the Kullback-Leibler loss maximized over the plausible distributions to be combined. The resulting combined distribution is a linear combination of the most extreme of the distributions to be combined that are scientifically plausible. The optimal weights are close enough to each other that no extreme distribution dominates the others.   The new methodology is illustrated by combining conflicting empirical Bayes methodologies in the context of gene expression data analysis.

</details>

<details>

<summary>2011-11-27 21:24:11 - Summarizing posterior distributions in signal decomposition problems when the number of components is unknown</summary>

- *Alireza Roodaki, Julien Bect, Gilles Fleury*

- `1111.6298v1` - [abs](http://arxiv.org/abs/1111.6298v1) - [pdf](http://arxiv.org/pdf/1111.6298v1)

> This paper addresses the problem of summarizing the posterior distributions that typically arise, in a Bayesian framework, when dealing with signal decomposition problems with unknown number of components. Such posterior distributions are defined over union of subspaces of differing dimensionality and can be sampled from using modern Monte Carlo techniques, for instance the increasingly popular RJ-MCMC method. No generic approach is available, however, to summarize the resulting variable-dimensional samples and extract from them component-specific parameters.   We propose a novel approach to this problem, which consists in approximating the complex posterior of interest by a "simple"---but still variable-dimensional---parametric distribution. The distance between the two distributions is measured using the Kullback-Leibler divergence, and a Stochastic EM-type algorithm, driven by the RJ-MCMC sampler, is proposed to estimate the parameters. The proposed algorithm is illustrated on the fundamental signal processing example of joint detection and estimation of sinusoids in white Gaussian noise.

</details>

<details>

<summary>2011-11-28 12:25:35 - Semiparametric efficiency bounds for seemingly unrelated conditional moment restrictions</summary>

- *Marian Hristache, Valentin Patilea*

- `1111.6428v1` - [abs](http://arxiv.org/abs/1111.6428v1) - [pdf](http://arxiv.org/pdf/1111.6428v1)

> This paper addresses the problem of semiparametric efficiency bounds for conditional moment restriction models with different conditioning variables. We characterize such an efficiency bound, that in general is not explicit, as a limit of explicit efficiency bounds for a decreasing sequence of unconditional (marginal) moment restriction models. An iterative procedure for approximating the efficient score when this is not explicit is provided. Our theoretical results complete and extend existing results in the literature, provide new insight for the theory of semiparametric efficiency bounds literature and open the door to new applications. In particular, we investigate a class of regression-like (mean regression, quantile regression,...) models with missing data.

</details>

<details>

<summary>2011-11-29 17:04:26 - Extended Generalised Pareto Models for Tail Estimation</summary>

- *Ioannis Papastathopoulos, Jonathan A. Tawn*

- `1111.6899v1` - [abs](http://arxiv.org/abs/1111.6899v1) - [pdf](http://arxiv.org/pdf/1111.6899v1)

> The most popular approach in extreme value statistics is the modelling of threshold exceedances using the asymptotically motivated generalised Pareto distribution. This approach involves the selection of a high threshold above which the model fits the data well. Sometimes, few observations of a measurement process might be recorded in applications and so selecting a high quantile of the sample as the threshold leads to almost no exceedances. In this paper we propose extensions of the generalised Pareto distribution that incorporate an additional shape parameter while keeping the tail behaviour unaffected. The inclusion of this parameter offers additional structure for the main body of the distribution, improves the stability of the modified scale, tail index and return level estimates to threshold choice and allows a lower threshold to be selected. We illustrate the benefits of the proposed models with a simulation study and two case studies.

</details>

<details>

<summary>2011-11-30 00:28:23 - A Bayesian Statistical Approach for Inference on Static Origin-Destination Matrices</summary>

- *Luis Carvalho*

- `1012.1047v2` - [abs](http://arxiv.org/abs/1012.1047v2) - [pdf](http://arxiv.org/pdf/1012.1047v2)

> We address the problem of static OD matrix estimation from a formal statistical viewpoint. We adopt a novel Bayesian framework to develop a class of models that explicitly cast trip configurations in the study region as random variables. As a consequence, classical solutions from growth factor, gravity, and maximum entropy models are identified to specific estimators under the proposed models. We show that each of these solutions usually account for only a small fraction of the posterior probability mass in the ensemble and we then contend that the uncertainty in the inference should be propagated to later analyses or next-stage models. We also propose alternative, more robust estimators and devise Markov chain Monte Carlo sampling schemes to obtain them and perform other types of inference. We present several examples showcasing the proposed models and approach, and highlight how other sources of data can be incorporated in the model and inference in a principled, non-heuristic way.

</details>

<details>

<summary>2011-11-30 01:12:16 - Bayesian Causal Induction</summary>

- *Pedro A. Ortega*

- `1111.0708v2` - [abs](http://arxiv.org/abs/1111.0708v2) - [pdf](http://arxiv.org/pdf/1111.0708v2)

> Discovering causal relationships is a hard task, often hindered by the need for intervention, and often requiring large amounts of data to resolve statistical uncertainty. However, humans quickly arrive at useful causal relationships. One possible reason is that humans extrapolate from past experience to new, unseen situations: that is, they encode beliefs over causal invariances, allowing for sound generalization from the observations they obtain from directly acting in the world.   Here we outline a Bayesian model of causal induction where beliefs over competing causal hypotheses are modeled using probability trees. Based on this model, we illustrate why, in the general case, we need interventions plus constraints on our causal hypotheses in order to extract causal information from our experience.

</details>

<details>

<summary>2011-11-30 04:55:29 - Estimating Bayesian networks for high-dimensional data with complex mean structure and random effects</summary>

- *Jessica Kasza, Gary Glonek, Patty Solomon*

- `1002.2168v2` - [abs](http://arxiv.org/abs/1002.2168v2) - [pdf](http://arxiv.org/pdf/1002.2168v2)

> The estimation of Bayesian networks given high-dimensional data, in particular gene expression data, has been the focus of much recent research. Whilst there are several methods available for the estimation of such networks, these typically assume that the data consist of independent and identically distributed samples. However, it is often the case that the available data have a more complex mean structure plus additional components of variance, which must then be accounted for in the estimation of a Bayesian network. In this paper, score metrics that take account of such complexities are proposed for use in conjunction with score-based methods for the estimation of Bayesian networks. We propose firstly, a fully Bayesian score metric, and secondly, a metric inspired by the notion of restricted maximum likelihood. We demonstrate the performance of these new metrics for the estimation of Bayesian networks using simulated data with known complex mean structures. We then present the analysis of expression levels of grape berry genes adjusting for exogenous variables believed to affect the expression levels of the genes. Demonstrable biological effects can be inferred from the estimated conditional independence relationships and correlations amongst the grape-berry genes.

</details>

<details>

<summary>2011-11-30 06:36:10 - Group Symmetry and Covariance Regularization</summary>

- *Parikshit Shah, Venkat Chandrasekaran*

- `1111.7061v1` - [abs](http://arxiv.org/abs/1111.7061v1) - [pdf](http://arxiv.org/pdf/1111.7061v1)

> Statistical models that possess symmetry arise in diverse settings such as random fields associated to geophysical phenomena, exchangeable processes in Bayesian statistics, and cyclostationary processes in engineering. We formalize the notion of a symmetric model via group invariance. We propose projection onto a group fixed point subspace as a fundamental way of regularizing covariance matrices in the high-dimensional regime. In terms of parameters associated to the group we derive precise rates of convergence of the regularized covariance matrix and demonstrate that significant statistical gains may be expected in terms of the sample complexity. We further explore the consequences of symmetry on related model-selection problems such as the learning of sparse covariance and inverse covariance matrices. We also verify our results with simulations.

</details>

<details>

<summary>2011-11-30 10:09:55 - On Bayesian "central clustering": Application to landscape classification of Western Ghats</summary>

- *Sabyasachi Mukhopadhyay, Sourabh Bhattacharya, Kajal Dihidar*

- `1111.7105v1` - [abs](http://arxiv.org/abs/1111.7105v1) - [pdf](http://arxiv.org/pdf/1111.7105v1)

> Landscape classification of the well-known biodiversity hotspot, Western Ghats (mountains), on the west coast of India, is an important part of a world-wide program of monitoring biodiversity. To this end, a massive vegetation data set, consisting of 51,834 4-variate observations has been clustered into different landscapes by Nagendra and Gadgil [Current Sci. 75 (1998) 264--271]. But a study of such importance may be affected by nonuniqueness of cluster analysis and the lack of methods for quantifying uncertainty of the clusterings obtained. Motivated by this applied problem of much scientific importance, we propose a new methodology for obtaining the global, as well as the local modes of the posterior distribution of clustering, along with the desired credible and "highest posterior density" regions in a nonparametric Bayesian framework. To meet the need of an appropriate metric for computing the distance between any two clusterings, we adopt and provide a much simpler, but accurate modification of the metric proposed in [In Felicitation Volume in Honour of Prof. B. K. Kale (2009) MacMillan]. A very fast and efficient Bayesian methodology, based on [Sankhy\={a} Ser. B 70 (2008) 133--155], has been utilized to solve the computational problems associated with the massive data and to obtain samples from the posterior distribution of clustering on which our proposed methods of summarization are illustrated.

</details>


## 2011-12

<details>

<summary>2011-12-01 05:10:37 - A comparison of score-based methods for estimating Bayesian networks using the Kullback-Leibler divergence</summary>

- *Jessica Kasza, Patty Solomon*

- `1009.1463v2` - [abs](http://arxiv.org/abs/1009.1463v2) - [pdf](http://arxiv.org/pdf/1009.1463v2)

> In this paper, we compare the performance of two methods for estimating Bayesian networks from data containing exogenous variables and random effects. The first method is fully Bayesian in which a prior distribution is placed on the exogenous variables, whereas the second method, which we call the residual approach, accounts for the effects of exogenous variables by using the notion of restricted maximum likelihood. We review the two score-based metrics, then study their performance by measuring the Kullback Leibler divergence, or distance, between the two resulting posterior density functions. The Kullback Leibler divergence provides a natural framework for comparing distributions. The residual approach is considerably simpler to apply in practice and we demonstrate its utility both theoretically and via simulations. In particular, in applications where the exogenous variables are not of primary interest, we show that the potential loss of information about parameters and induced components of correlation, is generally small.

</details>

<details>

<summary>2011-12-02 14:58:31 - Some Asymptotic Results of Gaussian Random Fields with Varying Mean Functions and the Associated Processes</summary>

- *Jingchen Liu, Gongjun Xu*

- `1104.1801v2` - [abs](http://arxiv.org/abs/1104.1801v2) - [pdf](http://arxiv.org/pdf/1104.1801v2)

> In this paper, we derive tail approximations of integrals of exponential functions of Gaussian random fields with varying mean functions and approximations of the associated point processes. This study is motivated naturally by multiple applications such as hypothesis testing for spatial models, study of the distribution of Bayesian marginal likelihood and Bayes factor, and financial applications.

</details>

<details>

<summary>2011-12-04 04:35:07 - Dimension adaptability of Gaussian process models with variable selection and projection</summary>

- *Surya T. Tokdar*

- `1112.0716v1` - [abs](http://arxiv.org/abs/1112.0716v1) - [pdf](http://arxiv.org/pdf/1112.0716v1)

> It is now known that an extended Gaussian process model equipped with rescaling can adapt to different smoothness levels of a function valued parameter in many nonparametric Bayesian analyses, offering a posterior convergence rate that is optimal (up to logarithmic factors) for the smoothness class the true function belongs to. This optimal rate also depends on the dimension of the function's domain and one could potentially obtain a faster rate of convergence by casting the analysis in a lower dimensional subspace that does not amount to any loss of information about the true function. In general such a subspace is not known a priori but can be explored by equipping the model with variable selection or linear projection. We demonstrate that for nonparametric regression, classification, density estimation and density regression, a rescaled Gaussian process model equipped with variable selection or linear projection offers a posterior convergence rate that is optimal (up to logarithmic factors) for the lowest dimension in which the analysis could be cast without any loss of information about the true function. Theoretical exploration of such dimension reduction features appears novel for Bayesian nonparametric models with or without Gaussian processes.

</details>

<details>

<summary>2011-12-05 01:30:15 - Asymptotically minimax Bayesian predictive densities for multinomial models</summary>

- *Fumiyasu Komaki*

- `1112.0818v1` - [abs](http://arxiv.org/abs/1112.0818v1) - [pdf](http://arxiv.org/pdf/1112.0818v1)

> One-step ahead prediction for the multinomial model is considered. The performance of a predictive density is evaluated by the average Kullback-Leibler divergence from the true density to the predictive density. Asymptotic approximations of risk functions of Bayesian predictive densities based on Dirichlet priors are obtained. It is shown that a Bayesian predictive density based on a specific Dirichlet prior is asymptotically minimax. The asymptotically minimax prior is different from known objective priors such as the Jeffreys prior or the uniform prior.

</details>

<details>

<summary>2011-12-05 12:36:36 - Posterior convergence for approximated unknowns in non-Gaussian statistical inverse problems</summary>

- *Sari Lasanen*

- `1112.0906v1` - [abs](http://arxiv.org/abs/1112.0906v1) - [pdf](http://arxiv.org/pdf/1112.0906v1)

> The statistical inverse problem of estimating the probability distribution of an infinite-dimensional unknown given its noisy indirect observation is studied in the Bayesian framework. In practice, one often considers only finite-dimensional unknowns and investigates numerically their probabilities. As many unknowns are function-valued, it is of interest to know whether the estimated probabilities converge when the finite-dimensional approximations of the unknown are refined. In this work, the generalized Bayes formula is shown to be a powerful tool in the convergence studies. With the help of the generalized Bayes formula, the question of convergence of the posterior distributions is returned to the convergence of the finite-dimensional (or any other) approximations of the unknown. The approach allows many prior distributions while the restrictions are mainly for the noise model and the direct theory. Three modes of convergence of posterior distributions are considered -- weak convergence, setwise convergence and convergence in variation. The convergence of conditional mean estimates is studied. Several examples of applicable infinite-dimensional non-Gaussian noise models are provided, including a generalization of the Cameron-Martin formula for certain non-Gaussian measures. Also, the well-posedness of Bayesian statistical inverse problems is studied.

</details>

<details>

<summary>2011-12-06 10:40:21 - Measures of Variability for Bayesian Network Graphical Structures</summary>

- *Marco Scutari*

- `1005.4214v2` - [abs](http://arxiv.org/abs/1005.4214v2) - [pdf](http://arxiv.org/pdf/1005.4214v2)

> The structure of a Bayesian network includes a great deal of information about the probability distribution of the data, which is uniquely identified given some general distributional assumptions. Therefore it's important to study its variability, which can be used to compare the performance of different learning algorithms and to measure the strength of any arbitrary subset of arcs.   In this paper we will introduce some descriptive statistics and the corresponding parametric and Monte Carlo tests on the undirected graph underlying the structure of a Bayesian network, modeled as a multivariate Bernoulli random variable. A simple numeric example and the comparison of the performance of some structure learning algorithm on small samples will then illustrate their use.

</details>

<details>

<summary>2011-12-07 11:31:25 - A Bayesian Joinpoint regression model with an unknown number of break-points</summary>

- *Miguel A. Martinez-Beneito, Gonzalo García-Donato, Diego Salmerón*

- `1112.1526v1` - [abs](http://arxiv.org/abs/1112.1526v1) - [pdf](http://arxiv.org/pdf/1112.1526v1)

> Joinpoint regression is used to determine the number of segments needed to adequately explain the relationship between two variables. This methodology can be widely applied to real problems, but we focus on epidemiological data, the main goal being to uncover changes in the mortality time trend of a specific disease under study. Traditionally, Joinpoint regression problems have paid little or no attention to the quantification of uncertainty in the estimation of the number of change-points. In this context, we found a satisfactory way to handle the problem in the Bayesian methodology. Nevertheless, this novel approach involves significant difficulties (both theoretical and practical) since it implicitly entails a model selection (or testing) problem. In this study we face these challenges through (i) a novel reparameterization of the model, (ii) a conscientious definition of the prior distributions used and (iii) an encompassing approach which allows the use of MCMC simulation-based techniques to derive the results. The resulting methodology is flexible enough to make it possible to consider mortality counts (for epidemiological applications) as Poisson variables. The methodology is applied to the study of annual breast cancer mortality during the period 1980--2007 in Castell\'{o}n, a province in Spain.

</details>

<details>

<summary>2011-12-07 11:33:23 - On the unmixing of MEx/OMEGA hyperspectral data</summary>

- *Konstantinos E. Themelis, Frédéric Schmidt, Olga Sykioti, Athanasios A. Rontogiannis, Konstantinos D. Koutroumbas, Ioannis A. Daglis*

- `1112.1527v1` - [abs](http://arxiv.org/abs/1112.1527v1) - [pdf](http://arxiv.org/pdf/1112.1527v1)

> This article presents a comparative study of three different types of estimators used for supervised linear unmixing of two MEx/OMEGA hyperspectral cubes. The algorithms take into account the constraints of the abundance fractions, in order to get physically interpretable results. Abundance maps show that the Bayesian maximum a posteriori probability (MAP) estimator proposed in Themelis and Rontogiannis (2008) outperforms the other two schemes, offering a compromise between complexity and estimation performance. Thus, the MAP estimator is a candidate algorithm to perform ice and minerals detection on large hyperspectral datasets.

</details>

<details>

<summary>2011-12-07 19:16:30 - Quantile Mechanics II: Changes of Variables in Monte Carlo methods and GPU-Optimized Normal Quantiles</summary>

- *William T. Shaw, Thomas Luu, Nick Brickman*

- `0901.0638v5` - [abs](http://arxiv.org/abs/0901.0638v5) - [pdf](http://arxiv.org/pdf/0901.0638v5)

> This article presents differential equations and solution methods for the functions of the form $Q(x) = F^{-1}(G(x))$, where $F$ and $G$ are cumulative distribution functions. Such functions allow the direct recycling of Monte Carlo samples from one distribution into samples from another. The method may be developed analytically for certain special cases, and illuminate the idea that it is a more precise form of the traditional Cornish-Fisher expansion. In this manner the model risk of distributional risk may be assessed free of the Monte Carlo noise associated with resampling. Examples are given of equations for converting normal samples to Student t, and converting exponential to hyperbolic, variance gamma and normal. In the case of the normal distribution, the change of variables employed allows the sampling to take place to good accuracy based on a single rational approximation over a very wide range of the sample space. The avoidance of any branching statement is of use in optimal GPU computations as it avoids the effect of {\it warp divergence}, and we give examples of branch-free normal quantiles that offer performance improvements in a GPU environment, while retaining the best precision characteristics of well-known methods. We also offer models based on a low-probability of warp divergence. Comparisons of new and old forms are made on the Nvidia Quadro 4000, GTX 285 and 480, and Tesla C2050 GPUs. We argue that in single-precision mode, the change-of-variables approach offers performance competitive with the fastest existing scheme while substantially improving precision, and that in double-precision mode, this approach offers the most GPU-optimal Gaussian quantile yet, and without compromise on precision for Monte Carlo applications, working twice as fast as the CUDA 4 library function with increased precision.

</details>

<details>

<summary>2011-12-13 00:13:16 - Approximate Bayesian Computing for Spatial Extremes</summary>

- *Robert J. Erhardt, Richard L. Smith*

- `1109.4166v4` - [abs](http://arxiv.org/abs/1109.4166v4) - [pdf](http://arxiv.org/pdf/1109.4166v4)

> Statistical analysis of max-stable processes used to model spatial extremes has been limited by the difficulty in calculating the joint likelihood function. This precludes all standard likelihood-based approaches, including Bayesian approaches. In this paper we present a Bayesian approach through the use of approximate Bayesian computing. This circumvents the need for a joint likelihood function by instead relying on simulations from the (unavailable) likelihood. This method is compared with an alternative approach based on the composite likelihood. We demonstrate that approximate Bayesian computing can result in a lower mean square error than the composite likelihood approach when estimating the spatial dependence of extremes, though at an appreciably higher computational cost. We also illustrate the performance of the method with an application to US temperature data to estimate the risk of crop loss due to an unlikely freeze event.

</details>

<details>

<summary>2011-12-13 12:17:33 - On the informativeness of dominant and co-dominant genetic markers for Bayesian supervised clustering</summary>

- *Gilles Guillot, Alexandra Carpentier-Skandalis*

- `1112.2868v1` - [abs](http://arxiv.org/abs/1112.2868v1) - [pdf](http://arxiv.org/pdf/1112.2868v1)

> We study the accuracy of Bayesian supervised method used to cluster individuals into genetically homogeneous groups on the basis of dominant or codominant molecular markers. We provide a formula relating an error criterion the number of loci used and the number of clusters. This formula is exact and holds for arbitrary number of clusters and markers. Our work suggests that dominant markers studies can achieve an accuracy similar to that of codominant markers studies if the number of markers used in the former is about 1.7 times larger than in the latter.

</details>

<details>

<summary>2011-12-13 22:17:47 - The Dirichlet Process with Large Concentration Parameter</summary>

- *Luai Al Labadi, Mahmoud Zarepour*

- `1109.5261v3` - [abs](http://arxiv.org/abs/1109.5261v3) - [pdf](http://arxiv.org/pdf/1109.5261v3)

> Ferguson's Dirichlet process plays an important role in nonparametric Bayesian inference. Let $P_a$ be the Dirichlet process in $\mathbb{R}$ with a base probability measure $H$ and a concentration parameter $a>0.$ In this paper, we show that $\sqrt {a} \big(P_a((-\infty,t]) -H((-\infty,t])\big)$ converges to a certain Brownian bridge as $a \to \infty.$ We also derive a certain Glivenko-Cantelli theorem for the Dirichlet process. Using the functional delta method, the weak convergence of the quantile process is also obtained. A large concentration parameter occurs when a statistician puts too much emphasize on his/her prior guess. This scenario also happens when the sample size is large and the posterior is used to make inference.

</details>

<details>

<summary>2011-12-14 22:07:09 - Quantile Based Variable Mining : Detection, FDR based Extraction and Interpretation</summary>

- *S. Mukhopadhyay, Emanuel Parzen, S. N. Lahiri*

- `1112.3373v1` - [abs](http://arxiv.org/abs/1112.3373v1) - [pdf](http://arxiv.org/pdf/1112.3373v1)

> This paper outlines a unified framework for high dimensional variable selection for classification problems. Traditional approaches to finding interesting variables mostly utilize only partial information through moments (like mean difference). On the contrary, in this paper we address the question of variable selection in full generality from a distributional point of view. If a variable is not important for classification, then it will have similar distributional aspect under different classes. This simple and straightforward observation motivates us to quantify `How and Why' the distribution of a variable changes over classes through CR-statistic. The second contribution of our paper is to develop and investigate the FDR based thresholding technology from a completely new point of view for adaptive thresholding, which leads to a elegant algorithm called CDfdr. This paper attempts to show how all of these problems of detection, extraction and interpretation for interesting variables can be treated in a unified way under one broad general theme - comparison analysis. It is proposed that a key to accomplishing this unification is to think in terms of the quantile function and the comparison density. We illustrate and demonstrate the power of our methodology using three real data sets.

</details>

<details>

<summary>2011-12-15 10:43:27 - Understanding better (some) astronomical data using Bayesian methods</summary>

- *S. Andreon*

- `1112.3652v1` - [abs](http://arxiv.org/abs/1112.3652v1) - [pdf](http://arxiv.org/pdf/1112.3652v1)

> Current analysis of astronomical data are confronted with the daunting task of modeling the awkward features of astronomical data, among which heteroscedastic (point-dependent) errors, intrinsic scatter, non-ignorable data collection (selection effects), data structure, non-uniform populations (often called Malmquist bias), non-Gaussian data, and upper/lower limits. This chapter shows, by examples, how modeling all these features using Bayesian methods. In short, one just need to formalize, using maths, the logical link between the involved quantities, how the data arise and what we already known on the quantities we want to study. The posterior probability distribution summarizes what we known on the studied quantities after the data, and we should not be afraid about their actual numerical computation, because it is left to (special) Monte Carlo programs such as JAGS. As examples, we show how to predict the mass of a new object disposing of a calibrating sample, how to constraint cosmological parameters from supernovae data and how to check if the fitted data are in tension with the adopted fitting model. Examples are given with their coding. These examples can be easily used as template for completely different analysis, on totally unrelated astronomical objects, requiring to model the same awkward data features.

</details>

<details>

<summary>2011-12-16 00:32:45 - Random construction of interpolating sets for high dimensional integration</summary>

- *Mark Huber, Sarah Schott*

- `1112.3692v1` - [abs](http://arxiv.org/abs/1112.3692v1) - [pdf](http://arxiv.org/pdf/1112.3692v1)

> Many high dimensional integrals can be reduced to the problem of finding the relative measures of two sets. Often one set will be exponentially larger than the other, making it difficult to compare the sizes. A standard method of dealing with this problem is to interpolate between the sets with a sequence of nested sets where neighboring sets have relative measures bounded above by a constant. Choosing such a well balanced sequence can be very difficult in practice. Here a new approach that automatically creates such sets is presented. These well balanced sets allow for faster approximation algorithms for integrals and sums, and better tempering and annealing Markov chains for generating random samples. Applications such as finding the partition function of the Ising model and normalizing constants for posterior distributions in Bayesian methods are discussed.

</details>

<details>

<summary>2011-12-16 07:55:22 - On two estimates related to the change-point problem</summary>

- *Farida Enikeeva*

- `1112.3729v1` - [abs](http://arxiv.org/abs/1112.3729v1) - [pdf](http://arxiv.org/pdf/1112.3729v1)

> We consider the problem of estimating a smooth functional of an unknown signal with discontinuity from Gaussian observations. The signal is a known function that depends on an unknown parameter. This problem is closely related to the famous change-point problem. We obtain an asymptotic likelihood ratio process for the noise level tending to 0. Bayesian and maximum likelihood estimates are constructed and their relative efficiency is studied. Some simulation results and conclusions on non-asymptotic behavior of these estimates are presented.

</details>

<details>

<summary>2011-12-18 06:15:19 - The Geometry of Hamiltonian Monte Carlo</summary>

- *Michael Betancourt, Leo C. Stein*

- `1112.4118v1` - [abs](http://arxiv.org/abs/1112.4118v1) - [pdf](http://arxiv.org/pdf/1112.4118v1)

> With its systematic exploration of probability distributions, Hamiltonian Monte Carlo is a potent Markov Chain Monte Carlo technique; it is an approach, however, ultimately contingent on the choice of a suitable Hamiltonian function. By examining both the symplectic geometry underlying Hamiltonian dynamics and the requirements of Markov Chain Monte Carlo, we construct the general form of admissible Hamiltonians and propose a particular choice with potential application in Bayesian inference.

</details>

<details>

<summary>2011-12-18 23:59:28 - Bayesian Approaches to Copula Modelling</summary>

- *Michael Stanley Smith*

- `1112.4204v1` - [abs](http://arxiv.org/abs/1112.4204v1) - [pdf](http://arxiv.org/pdf/1112.4204v1)

> Copula models have become one of the most widely used tools in the applied modelling of multivariate data. Similarly, Bayesian methods are increasingly used to obtain efficient likelihood-based inference. However, to date, there has been only limited use of Bayesian approaches in the formulation and estimation of copula models. This article aims to address this shortcoming in two ways. First, to introduce copula models and aspects of copula theory that are especially relevant for a Bayesian analysis. Second, to outline Bayesian approaches to formulating and estimating copula models, and their advantages over alternative methods. Copulas covered include Archimedean, copulas constructed by inversion, and vine copulas; along with their interpretation as transformations. A number of parameterisations of a correlation matrix of a Gaussian copula are considered, along with hierarchical priors that allow for Bayesian selection and model averaging for each parameterisation. Markov chain Monte Carlo sampling schemes for fitting Gaussian and D-vine copulas, with and without selection, are given in detail. The relationship between the prior for the parameters of a D-vine, and the prior for a correlation matrix of a Gaussian copula, is discussed. Last, it is shown how to compute Bayesian inference when the data are discrete-valued using data augmentation. This approach generalises popular Bayesian methods for the estimation of models for multivariate binary and other ordinal data to more general copula models. Bayesian data augmentation has substantial advantages over other methods of estimation for this class of models.

</details>

<details>

<summary>2011-12-19 15:35:28 - Estimating meteor rates using Bayesian inference</summary>

- *Geert Barentsen, Rainer Arlt, Hans-Erich Fröhlich*

- `1112.4372v1` - [abs](http://arxiv.org/abs/1112.4372v1) - [pdf](http://arxiv.org/pdf/1112.4372v1)

> A method for estimating the true meteor rate \lambda\ from a small number of observed meteors n is derived. We employ Bayesian inference with a Poissonian likelihood function. We discuss the choice of a suitable prior and propose the adoption of Jeffreys prior, P(\lambda)=\lambda^{-0.5}, which yields an expectation value E(\lambda) = n+0.5 for any n \geq 0. We update the ZHR meteor activity formula accordingly, and explain how 68%- and 95%-confidence intervals can be computed.

</details>

<details>

<summary>2011-12-19 16:22:09 - Additive Gaussian Processes</summary>

- *David Duvenaud, Hannes Nickisch, Carl Edward Rasmussen*

- `1112.4394v1` - [abs](http://arxiv.org/abs/1112.4394v1) - [pdf](http://arxiv.org/pdf/1112.4394v1)

> We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but tractable parameterization of the kernel function, which allows efficient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks.

</details>

<details>

<summary>2011-12-23 13:28:07 - Is Bayes Posterior just Quick and Dirty Confidence?</summary>

- *D. A. S. Fraser*

- `1112.5582v1` - [abs](http://arxiv.org/abs/1112.5582v1) - [pdf](http://arxiv.org/pdf/1112.5582v1)

> Bayes [Philos. Trans. R. Soc. Lond. 53 (1763) 370--418; 54 296--325] introduced the observed likelihood function to statistical inference and provided a weight function to calibrate the parameter; he also introduced a confidence distribution on the parameter space but did not provide present justifications. Of course the names likelihood and confidence did not appear until much later: Fisher [Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Eng. Sci. 222 (1922) 309--368] for likelihood and Neyman [Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Eng. Sci. 237 (1937) 333--380] for confidence. Lindley [J. Roy. Statist. Soc. Ser. B 20 (1958) 102--107] showed that the Bayes and the confidence results were different when the model was not location. This paper examines the occurrence of true statements from the Bayes approach and from the confidence approach, and shows that the proportion of true statements in the Bayes case depends critically on the presence of linearity in the model; and with departure from this linearity the Bayes approach can be a poor approximation and be seriously misleading. Bayesian integration of weighted likelihood thus provides a first-order linear approximation to confidence, but without linearity can give substantially incorrect results.

</details>

<details>

<summary>2011-12-23 18:54:37 - Bayesian model choice and information criteria in sparse generalized linear models</summary>

- *Rina Foygel, Mathias Drton*

- `1112.5635v1` - [abs](http://arxiv.org/abs/1112.5635v1) - [pdf](http://arxiv.org/pdf/1112.5635v1)

> We consider Bayesian model selection in generalized linear models that are high-dimensional, with the number of covariates p being large relative to the sample size n, but sparse in that the number of active covariates is small compared to p. Treating the covariates as random and adopting an asymptotic scenario in which p increases with n, we show that Bayesian model selection using certain priors on the set of models is asymptotically equivalent to selecting a model using an extended Bayesian information criterion. Moreover, we prove that the smallest true model is selected by either of these methods with probability tending to one. Having addressed random covariates, we are also able to give a consistency result for pseudo-likelihood approaches to high-dimensional sparse graphical modeling. Experiments on real data demonstrate good performance of the extended Bayesian information criterion for regression and for graphical models.

</details>

<details>

<summary>2011-12-24 17:53:19 - Bayesian Active Learning for Classification and Preference Learning</summary>

- *Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, Máté Lengyel*

- `1112.5745v1` - [abs](http://arxiv.org/abs/1112.5745v1) - [pdf](http://arxiv.org/pdf/1112.5745v1)

> Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractability. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning.

</details>

<details>

<summary>2011-12-26 13:04:36 - On Bayesian Estimation via Divergences</summary>

- *Mohamed Cherfi*

- `1112.5854v1` - [abs](http://arxiv.org/abs/1112.5854v1) - [pdf](http://arxiv.org/pdf/1112.5854v1)

> In this Note we introduce a new methodology for Bayesian inference through the use of $\phi$-divergences and the duality technique. The asymptotic laws of the estimates are established.

</details>

<details>

<summary>2011-12-30 02:24:12 - Bayesian Quantile Regression for Single-Index Models</summary>

- *Yuao Hua, Robert B. Gramacy, Heng Lian*

- `1110.0219v2` - [abs](http://arxiv.org/abs/1110.0219v2) - [pdf](http://arxiv.org/pdf/1110.0219v2)

> Using an asymmetric Laplace distribution, which provides a mechanism for Bayesian inference of quantile regression models, we develop a fully Bayesian approach to fitting single-index models in conditional quantile regression. In this work, we use a Gaussian process prior for the unknown nonparametric link function and a Laplace distribution on the index vector, with the latter motivated by the recent popularity of the Bayesian lasso idea. We design a Markov chain Monte Carlo algorithm for posterior inference. Careful consideration of the singularity of the kernel matrix, and tractability of some of the full conditional distributions leads to a partially collapsed approach where the nonparametric link function is integrated out in some of the sampling steps. Our simulations demonstrate the superior performance of the Bayesian method versus the frequentist approach. The method is further illustrated by an application to the hurricane data.

</details>

