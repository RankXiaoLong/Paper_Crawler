# 2009

## TOC

- [2009-01](#2009-01)
- [2009-02](#2009-02)
- [2009-03](#2009-03)
- [2009-04](#2009-04)
- [2009-05](#2009-05)
- [2009-06](#2009-06)
- [2009-07](#2009-07)
- [2009-08](#2009-08)
- [2009-09](#2009-09)
- [2009-10](#2009-10)
- [2009-11](#2009-11)
- [2009-12](#2009-12)

## 2009-01

<details>

<summary>2009-01-05 14:35:30 - Scaling factors for ab initio vibrational frequencies: comparison of uncertainty models for quantified prediction</summary>

- *Pascal Pernot*

- `0901.0489v1` - [abs](http://arxiv.org/abs/0901.0489v1) - [pdf](http://arxiv.org/pdf/0901.0489v1)

> Bayesian Model Calibration is used to revisit the problem of scaling factor calibration for semi-empirical correction of ab initio calculations. A particular attention is devoted to uncertainty evaluation for scaling factors, and to their effect on prediction of observables involving scaled properties. We argue that linear models used for calibration of scaling factors are generally not statistically valid, in the sense that they are not able to fit calibration data within their uncertainty limits. Uncertainty evaluation and uncertainty propagation by statistical methods from such invalid models are doomed to failure. To relieve this problem, a stochastic function is included in the model to account for model inadequacy, according to the Bayesian Model Calibration approach. In this framework, we demonstrate that standard calibration summary statistics, as optimal scaling factor and root mean square, can be safely used for uncertainty propagation only when large calibration sets of precise data are used. For small datasets containing a few dozens of data, a more accurate formula is provided which involves scaling factor calibration uncertainty. For measurement uncertainties larger than model inadequacy, the problem can be reduced to a weighted least squares analysis. For intermediate cases, no analytical estimators were found, and numerical Bayesian estimation of parameters has to be used.

</details>

<details>

<summary>2009-01-09 15:13:19 - PAC-Bayesian Bounds for Randomized Empirical Risk Minimizers</summary>

- *Pierre Alquier*

- `0712.1698v3` - [abs](http://arxiv.org/abs/0712.1698v3) - [pdf](http://arxiv.org/pdf/0712.1698v3)

> The aim of this paper is to generalize the PAC-Bayesian theorems proved by Catoni in the classification setting to more general problems of statistical inference. We show how to control the deviations of the risk of randomized estimators. A particular attention is paid to randomized estimators drawn in a small neighborhood of classical estimators, whose study leads to control the risk of the latter. These results allow to bound the risk of very general estimation procedures, as well as to perform model selection.

</details>

<details>

<summary>2009-01-14 03:45:46 - Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems</summary>

- *Tina Toni, David Welch, Natalja Strelkowa, Andreas Ipsen, Michael P. H. Stumpf*

- `0901.1925v1` - [abs](http://arxiv.org/abs/0901.1925v1) - [pdf](http://arxiv.org/pdf/0901.1925v1)

> Approximate Bayesian computation methods can be used to evaluate posterior distributions without having to calculate likelihoods. In this paper we discuss and apply an approximate Bayesian computation (ABC) method based on sequential Monte Carlo (SMC) to estimate parameters of dynamical models. We show that ABC SMC gives information about the inferability of parameters and model sensitivity to changes in parameters, and tends to perform better than other ABC approaches. The algorithm is applied to several well known biological systems, for which parameters and their credible intervals are inferred. Moreover, we develop ABC SMC as a tool for model selection; given a range of different mathematical descriptions, ABC SMC is able to choose the best model using the standard Bayesian model selection apparatus.

</details>

<details>

<summary>2009-01-15 07:04:48 - Decision Approach and Empirical Bayes FCR-Controlling Interval for Mixed Prior Model</summary>

- *Zhigen Zhao*

- `0901.2193v1` - [abs](http://arxiv.org/abs/0901.2193v1) - [pdf](http://arxiv.org/pdf/0901.2193v1)

> In this paper, I apply the decision theory and empirical Bayesian approach to construct confidence intervals for selected populations when true parameters follow a mixture prior distribution. A loss function with two tuning parameters $k_1$ and $k_2$ is coined to address the mixture prior. One specific choice of $k_2$ can lead to the procedure in Qiu and Hwang (2007); the other choice of $k_2$ provides an interval construction which controls the Bayes FCR. Both the analytical and extensive numerical simulation studies demonstrate that the new empirical Bayesian FCR controlling approach enjoys great length reduction. At the end, I apply different methods to a microarray data set. It turns out that the average length of the new approach is only 57% of that of Qiu and Hwang's procedure which controls the simultaneous non-coverage probability and 66% of that of Benjamini and Yekutieli (2005)'s procedure which controls the frequentist's FCR.

</details>

<details>

<summary>2009-01-15 11:15:27 - Bayesian Computation and Model Selection in Population Genetics</summary>

- *Christoph Leuenberger Daniel Wegmann Laurent Excoffier*

- `0901.2231v1` - [abs](http://arxiv.org/abs/0901.2231v1) - [pdf](http://arxiv.org/pdf/0901.2231v1)

> Until recently, the use of Bayesian inference in population genetics was limited to a few cases because for many realistic population genetic models the likelihood function cannot be calculated analytically . The situation changed with the advent of likelihood-free inference algorithms, often subsumed under the term Approximate Bayesian Computation (ABC). A key innovation was the use of a post-sampling regression adjustment, allowing larger tolerance values and as such shifting computation time to realistic orders of magnitude (see Beaumont et al., 2002). Here we propose a reformulation of the regression adjustment in terms of a General Linear Model (GLM). This allows the integration into the framework of Bayesian statistics and the use of its methods, including model selection via Bayes factors. We then apply the proposed methodology to the question of population subdivision among western chimpanzees Pan troglodytes verus.

</details>

<details>

<summary>2009-01-16 15:47:31 - Construction of Bayesian Deformable Models via Stochastic Approximation Algorithm: A Convergence Study</summary>

- *Stéphanie Allassonnière, Estelle Kuhn, Alain Trouvé*

- `0706.0787v2` - [abs](http://arxiv.org/abs/0706.0787v2) - [pdf](http://arxiv.org/pdf/0706.0787v2)

> The problem of the definition and the estimation of generative models based on deformable templates from raw data is of particular importance for modelling non aligned data affected by various types of geometrical variability. This is especially true in shape modelling in the computer vision community or in probabilistic atlas building for Computational Anatomy (CA). A first coherent statistical framework modelling the geometrical variability as hidden variables has been given by Allassonni\`ere, Amit and Trouv\'e (JRSS 2006). Setting the problem in a Bayesian context they proved the consistency of the MAP estimator and provided a simple iterative deterministic algorithm with an EM flavour leading to some reasonable approximations of the MAP estimator under low noise conditions. In this paper we present a stochastic algorithm for approximating the MAP estimator in the spirit of the SAEM algorithm. We prove its convergence to a critical point of the observed likelihood with an illustration on images of handwritten digits.

</details>

<details>

<summary>2009-01-16 15:48:38 - Stochastic Algorithm For Parameter Estimation For Dense Deformable Template Mixture Model</summary>

- *Stéphanie Allassonnière, Estelle Kuhn*

- `0802.1521v2` - [abs](http://arxiv.org/abs/0802.1521v2) - [pdf](http://arxiv.org/pdf/0802.1521v2)

> Estimating probabilistic deformable template models is a new approach in the fields of computer vision and probabilistic atlases in computational anatomy. A first coherent statistical framework modelling the variability as a hidden random variable has been given by Allassonni\`ere, Amit and Trouv\'e in [1] in simple and mixture of deformable template models. A consistent stochastic algorithm has been introduced in [2] to face the problem encountered in [1] for the convergence of the estimation algorithm for the one component model in the presence of noise. We propose here to go on in this direction of using some "SAEM-like" algorithm to approximate the MAP estimator in the general Bayesian setting of mixture of deformable template model. We also prove the convergence of this algorithm toward a critical point of the penalised likelihood of the observations and illustrate this with handwritten digit images.

</details>

<details>

<summary>2009-01-18 20:07:17 - Maximum Entropy Discrimination Markov Networks</summary>

- *Jun Zhu, Eric P. Xing*

- `0901.2730v1` - [abs](http://arxiv.org/abs/0901.2730v1) - [pdf](http://arxiv.org/pdf/0901.2730v1)

> In this paper, we present a novel and general framework called {\it Maximum Entropy Discrimination Markov Networks} (MaxEnDNet), which integrates the max-margin structured learning and Bayesian-style estimation and combines and extends their merits. Major innovations of this model include: 1) It generalizes the extant Markov network prediction rule based on a point estimator of weights to a Bayesian-style estimator that integrates over a learned distribution of the weights. 2) It extends the conventional max-entropy discrimination learning of classification rule to a new structural max-entropy discrimination paradigm of learning the distribution of Markov networks. 3) It subsumes the well-known and powerful Maximum Margin Markov network (M$^3$N) as a special case, and leads to a model similar to an $L_1$-regularized M$^3$N that is simultaneously primal and dual sparse, or other types of Markov network by plugging in different prior distributions of the weights. 4) It offers a simple inference algorithm that combines existing variational inference and convex-optimization based M$^3$N solvers as subroutines. 5) It offers a PAC-Bayesian style generalization bound. This work represents the first successful attempt to combine Bayesian-style learning (based on generative models) with structured maximum margin learning (based on a discriminative model), and outperforms a wide array of competing methods for structured input/output learning on both synthetic and real data sets.

</details>

<details>

<summary>2009-01-23 12:54:20 - Interpolating fields of carbon monoxide data using a hybrid statistical-physical model</summary>

- *Anders Malmberg, Avelino Arellano, David P. Edwards, Natasha Flyer, Doug Nychka, Christopher Wikle*

- `0901.3670v1` - [abs](http://arxiv.org/abs/0901.3670v1) - [pdf](http://arxiv.org/pdf/0901.3670v1)

> Atmospheric Carbon Monoxide (CO) provides a window on the chemistry of the atmosphere since it is one of few chemical constituents that can be remotely sensed, and it can be used to determine budgets of other greenhouse gases such as ozone and OH radicals. Remote sensing platforms in geostationary Earth orbit will soon provide regional observations of CO at several vertical layers with high spatial and temporal resolution. However, cloudy locations cannot be observed and estimates of the complete CO concentration fields have to be estimated based on the cloud-free observations. The current state-of-the-art solution of this interpolation problem is to combine cloud-free observations with prior information, computed by a deterministic physical model, which might introduce uncertainties that do not derive from data. While sharing features with the physical model, this paper suggests a Bayesian hierarchical model to estimate the complete CO concentration fields. The paper also provides a direct comparison to state-of-the-art methods. To our knowledge, such a model and comparison have not been considered before.

</details>

<details>

<summary>2009-01-26 13:11:47 - Reconstructing the energy landscape of a distribution from Monte Carlo samples</summary>

- *Qing Zhou, Wing Hung Wong*

- `0901.3999v1` - [abs](http://arxiv.org/abs/0901.3999v1) - [pdf](http://arxiv.org/pdf/0901.3999v1)

> Defining the energy function as the negative logarithm of the density, we explore the energy landscape of a distribution via the tree of sublevel sets of its energy. This tree represents the hierarchy among the connected components of the sublevel sets. We propose ways to annotate the tree so that it provides information on both topological and statistical aspects of the distribution, such as the local energy minima (local modes), their local domains and volumes, and the barriers between them. We develop a computational method to estimate the tree and reconstruct the energy landscape from Monte Carlo samples simulated at a wide energy range of a distribution. This method can be applied to any arbitrary distribution on a space with defined connectedness. We test the method on multimodal distributions and posterior distributions to show that our estimated trees are accurate compared to theoretical values. When used to perform Bayesian inference of DNA sequence segmentation, this approach reveals much more information than the standard approach based on marginal posterior distributions.

</details>

<details>

<summary>2009-01-26 14:54:49 - Unsupervised bayesian convex deconvolution based on a field with an explicit partition function</summary>

- *Jean-Francois Giovannelli*

- `0901.3326v2` - [abs](http://arxiv.org/abs/0901.3326v2) - [pdf](http://arxiv.org/pdf/0901.3326v2)

> This paper proposes a non-Gaussian Markov field with a special feature: an explicit partition function. To the best of our knowledge, this is an original contribution. Moreover, the explicit expression of the partition function enables the development of an unsupervised edge-preserving convex deconvolution method. The method is fully Bayesian, and produces an estimate in the sense of the posterior mean, numerically calculated by means of a Monte-Carlo Markov Chain technique. The approach is particularly effective and the computational practicability of the method is shown on a simple simulated example.

</details>

<details>

<summary>2009-01-26 14:57:23 - Modeling long-term longitudinal HIV dynamics with application to an AIDS clinical study</summary>

- *Yangxin Huang, Tao Lu*

- `0901.3806v1` - [abs](http://arxiv.org/abs/0901.3806v1) - [pdf](http://arxiv.org/pdf/0901.3806v1)

> A virologic marker, the number of HIV RNA copies or viral load, is currently used to evaluate antiretroviral (ARV) therapies in AIDS clinical trials. This marker can be used to assess the ARV potency of therapies, but is easily affected by drug exposures, drug resistance and other factors during the long-term treatment evaluation process. HIV dynamic studies have significantly contributed to the understanding of HIV pathogenesis and ARV treatment strategies. However, the models of these studies are used to quantify short-term HIV dynamics ($<$ 1 month), and are not applicable to describe long-term virological response to ARV treatment due to the difficulty of establishing a relationship of antiviral response with multiple treatment factors such as drug exposure and drug susceptibility during long-term treatment. Long-term therapy with ARV agents in HIV-infected patients often results in failure to suppress the viral load. Pharmacokinetics (PK), drug resistance and imperfect adherence to prescribed antiviral drugs are important factors explaining the resurgence of virus. To better understand the factors responsible for the virological failure, this paper develops the mechanism-based nonlinear differential equation models for characterizing long-term viral dynamics with ARV therapy. The models directly incorporate drug concentration, adherence and drug susceptibility into a function of treatment efficacy and, hence, fully integrate virologic, PK, drug adherence and resistance from an AIDS clinical trial into the analysis. A Bayesian nonlinear mixed-effects modeling approach in conjunction with the rescaled version of dynamic differential equations is investigated to estimate dynamic parameters and make inference. In addition, the correlations of baseline factors with estimated dynamic parameters are explored and some biologically meaningful correlation results are presented. Further, the estimated dynamic parameters in patients with virologic success were compared to those in patients with virologic failure and significantly important findings were summarized. These results suggest that viral dynamic parameters may play an important role in understanding HIV pathogenesis, designing new treatment strategies for long-term care of AIDS patients.

</details>

<details>

<summary>2009-01-26 15:36:14 - A Bayesian framework for estimating vaccine efficacy per infectious contact</summary>

- *Yang Yang, Peter Gilbert, Ira M. Longini, Jr., M. Elizabeth Halloran*

- `0901.4025v1` - [abs](http://arxiv.org/abs/0901.4025v1) - [pdf](http://arxiv.org/pdf/0901.4025v1)

> In vaccine studies for infectious diseases such as human immunodeficiency virus (HIV), the frequency and type of contacts between study participants and infectious sources are among the most informative risk factors, but are often not adequately adjusted for in standard analyses. Such adjustment can improve the assessment of vaccine efficacy as well as the assessment of risk factors. It can be attained by modeling transmission per contact with infectious sources. However, information about contacts that rely on self-reporting by study participants are subject to nontrivial measurement error in many studies. We develop a Bayesian hierarchical model fitted using Markov chain Monte Carlo (MCMC) sampling to estimate the vaccine efficacy controlled for exposure to infection, while adjusting for measurement error in contact-related factors. Our method is used to re-analyze two recent HIV vaccine studies, and the results are compared with the published primary analyses that used standard methods. The proposed method could also be used for other vaccines where contact information is collected, such as human papilloma virus vaccines.

</details>

<details>

<summary>2009-01-27 09:45:16 - Bayesian multinomial regression with class-specific predictor selection</summary>

- *Paul Gustafson, Geneviève Lefebvre*

- `0901.4208v1` - [abs](http://arxiv.org/abs/0901.4208v1) - [pdf](http://arxiv.org/pdf/0901.4208v1)

> Consider a multinomial regression model where the response, which indicates a unit's membership in one of several possible unordered classes, is associated with a set of predictor variables. Such models typically involve a matrix of regression coefficients, with the $(j,k)$ element of this matrix modulating the effect of the $k$th predictor on the propensity of the unit to belong to the $j$th class. Thus, a supposition that only a subset of the available predictors are associated with the response corresponds to some of the columns of the coefficient matrix being zero. Under the Bayesian paradigm, the subset of predictors which are associated with the response can be treated as an unknown parameter, leading to typical Bayesian model selection and model averaging procedures. As an alternative, we investigate model selection and averaging, whereby a subset of individual elements of the coefficient matrix are zero. That is, the subset of predictors associated with the propensity to belong to a class varies with the class. We refer to this as class-specific predictor selection. We argue that such a scheme can be attractive on both conceptual and computational grounds.

</details>

<details>

<summary>2009-01-27 11:11:03 - Discretization-invariant Bayesian inversion and Besov space priors</summary>

- *Matti Lassas. Eero Saksman, Samuli Siltanen*

- `0901.4220v1` - [abs](http://arxiv.org/abs/0901.4220v1) - [pdf](http://arxiv.org/pdf/0901.4220v1)

> Bayesian solution of an inverse problem for indirect measurement $M = AU + {\mathcal{E}}$ is considered, where $U$ is a function on a domain of $R^d$. Here $A$ is a smoothing linear operator and $ {\mathcal{E}}$ is Gaussian white noise. The data is a realization $m_k$ of the random variable $M_k = P_kA U+P_k {\mathcal{E}}$, where $P_k$ is a linear, finite dimensional operator related to measurement device. To allow computerized inversion, the unknown is discretized as $U_n=T_nU$, where $T_n$ is a finite dimensional projection, leading to the computational measurement model $M_{kn}=P_k A U_n + P_k {\mathcal{E}}$. Bayes formula gives then the posterior distribution $\pi_{kn}(u_n | m_{kn})\sim\pi_n(u_n) \exp(-{1/2}\|m_{kn} - P_kA u_n\|_2^2)$ in $R^d$, and the mean $U^{CM}_{kn}:=\int u_n \pi_{kn}(u_n | m_k) du_n$ is considered as the reconstruction of $U$. We discuss a systematic way of choosing prior distributions $\prior_n$ for all $n\geq n_0>0$ by achieving them as projections of a distribution in a infinite-dimensional limit case. Such choice of prior distributions is {\em discretization-invariant} in the sense that $\prior_n$ represent the same {\em a priori} information for all $n$ and that the mean $U^{CM}_{kn}$ converges to a limit estimate as $k,n\to\infty$. Gaussian smoothness priors and wavelet-based Besov space priors are shown to be discretization invariant. In particular, Bayesian inversion in dimension two with $B^1_{11}$ prior is related to penalizing the $\ell^1$ norm of the wavelet coefficients of $U$.

</details>

<details>

<summary>2009-01-27 12:56:56 - Controlled stratification for quantile estimation</summary>

- *Claire Cannamela, Josselin Garnier, Bertrand Iooss*

- `0802.2426v2` - [abs](http://arxiv.org/abs/0802.2426v2) - [pdf](http://arxiv.org/pdf/0802.2426v2)

> In this paper we propose and discuss variance reduction techniques for the estimation of quantiles of the output of a complex model with random input parameters. These techniques are based on the use of a reduced model, such as a metamodel or a response surface. The reduced model can be used as a control variate; or a rejection method can be implemented to sample the realizations of the input parameters in prescribed relevant strata; or the reduced model can be used to determine a good biased distribution of the input parameters for the implementation of an importance sampling strategy. The different strategies are analyzed and the asymptotic variances are computed, which shows the benefit of an adaptive controlled stratification method. This method is finally applied to a real example (computation of the peak cladding temperature during a large-break loss of coolant accident in a nuclear reactor).

</details>

<details>

<summary>2009-01-29 05:46:06 - Bayesian projection approaches to variable selection and exploring model uncertainty</summary>

- *David Nott, Chenlei Leng*

- `0901.4605v1` - [abs](http://arxiv.org/abs/0901.4605v1) - [pdf](http://arxiv.org/pdf/0901.4605v1)

> A Bayesian approach to variable selection which is based on the expected Kullback-Leibler divergence between the full model and its projection onto a submodel has recently been suggested in the literature. Here we extend this idea by considering projections onto subspaces defined via some form of $L_1$ constraint on the parameter in the full model. This leads to Bayesian model selection approaches related to the lasso. In the posterior distribution of the projection there is positive probability that some components are exactly zero and the posterior distribution on the model space induced by the projection allows exploration of model uncertainty. We also consider use of the approach in structured variable selection problems such as ANOVA models where it is desired to incorporate main effects in the presence of interactions. Here we make use of projections related to the non-negative garotte which are able to respect the hierarchical constraints. We also prove a consistency result concerning the posterior distribution on the model induced by the projection, and show that for some projections related to the adaptive lasso and non-negative garotte the posterior distribution concentrates on the true model asymptotically.

</details>

<details>

<summary>2009-01-29 10:20:42 - A Bernstein-Von Mises Theorem for discrete probability distributions</summary>

- *S. Boucheron, E. Gassiat*

- `0807.2096v2` - [abs](http://arxiv.org/abs/0807.2096v2) - [pdf](http://arxiv.org/pdf/0807.2096v2)

> We investigate the asymptotic normality of the posterior distribution in the discrete setting, when model dimension increases with sample size. We consider a probability mass function $\theta_0$ on $\mathbbm{N}\setminus \{0\}$ and a sequence of truncation levels $(k_n)_n$ satisfying $k_n^3\leq n\inf_{i\leq k_n}\theta_0(i).$ Let $\hat{\theta}$ denote the maximum likelihood estimate of $(\theta_0(i))_{i\leq k_n}$ and let $\Delta_n(\theta_0)$ denote the $k_n$-dimensional vector which $i$-th coordinate is defined by \sqrt{n} (\hat{\theta}_n(i)-\theta_0(i)) for $1\leq i\leq k_n.$ We check that under mild conditions on $\theta_0$ and on the sequence of prior probabilities on the $k_n$-dimensional simplices, after centering and rescaling, the variation distance between the posterior distribution recentered around $\hat{\theta}_n$ and rescaled by $\sqrt{n}$ and the $k_n$-dimensional Gaussian distribution $\mathcal{N}(\Delta_n(\theta_0),I^{-1}(\theta_0))$ converges in probability to $0.$ This theorem can be used to prove the asymptotic normality of Bayesian estimators of Shannon and R\'{e}nyi entropies. The proofs are based on concentration inequalities for centered and non-centered Chi-square (Pearson) statistics. The latter allow to establish posterior concentration rates with respect to Fisher distance rather than with respect to the Hellinger distance as it is commonplace in non-parametric Bayesian statistics.

</details>


## 2009-02

<details>

<summary>2009-02-06 22:49:23 - Does adjustment for measurement error induce positive bias if there is no true association?</summary>

- *Igor Burstyn*

- `0902.1193v1` - [abs](http://arxiv.org/abs/0902.1193v1) - [pdf](http://arxiv.org/pdf/0902.1193v1)

> This article is a response to an off-the-record discussion that I had at an international meeting of epidemiologists. It centered on a concern, perhaps widely spread, that measurement error adjustment methods can induce positive bias in results of epidemiological studies when there is no true association. I trace the possible history of this supposition and test it in a simulation study of both continuous and binary health outcomes under a classical multiplicative measurement error model. A Bayesian measurement adjustment method is used. The main conclusion is that adjustment for the presumed measurement error does not 'induce' positive associations, especially if the focus of the interpretation of the result is taken away from the point estimate. This is in line with properties of earlier measurement error adjustment methods introduced to epidemiologists in the 1990s. An heuristic argument is provided to support the generalizability of this observation in the Bayesian framework. I find that when there is no true association, positive bias can only be induced by indefensible manipulation of the priors, such that they dominate the data. The misconception about bias induced by measurement error adjustment should be more clearly explained during the training of epidemiologists to ensure the appropriate (and wider) use of measurement error correction procedures. The simple message that can be derived from this paper is: 'Do not focus on point estimates, but mind the gap between boundaries that reflect variability in the estimate'. And of course: 'Treat measurement error as a tractable problem that deserves much more attention than just a qualitative (throw-away) discussion'.

</details>

<details>

<summary>2009-02-09 03:09:20 - Hierarchical Bayesian Modeling of Hitting Performance in Baseball</summary>

- *Shane T. Jensen, Blake McShane, Abraham J. Wyner*

- `0902.1360v1` - [abs](http://arxiv.org/abs/0902.1360v1) - [pdf](http://arxiv.org/pdf/0902.1360v1)

> We have developed a sophisticated statistical model for predicting the hitting performance of Major League baseball players. The Bayesian paradigm provides a principled method for balancing past performance with crucial covariates, such as player age and position. We share information across time and across players by using mixture distributions to control shrinkage for improved accuracy. We compare the performance of our model to current sabermetric methods on a held-out season (2006), and discuss both successes and limitations.

</details>

<details>

<summary>2009-02-11 09:22:01 - Likelihood ratios and Bayesian inference for Poisson channels</summary>

- *Anthony Reveillac*

- `0709.1211v3` - [abs](http://arxiv.org/abs/0709.1211v3) - [pdf](http://arxiv.org/pdf/0709.1211v3)

> In recent years, infinite-dimensional methods have been introduced for the Gaussian channels estimation. The aim of this paper is to study the application of similar methods to Poisson channels. In particular we compute the Bayesian estimator of a Poisson channel using the likelihood ratio and the discrete Malliavin gradient. This algorithm is suitable for numerical implementation via the Monte-Carlo scheme. As an application we provide an new proof of the formula obtained recently by Guo, Shamai and Verdu\'u relating some derivatives of the input-output mutual information of a time-continuous Poisson channel and the conditional mean estimator of the input. These results are then extended to mixed Gaussian-Poisson channels.

</details>

<details>

<summary>2009-02-13 14:47:14 - Marginal Likelihood Integrals for Mixtures of Independence Models</summary>

- *Shaowei Lin, Bernd Sturmfels, Zhiqiang Xu*

- `0805.3602v2` - [abs](http://arxiv.org/abs/0805.3602v2) - [pdf](http://arxiv.org/pdf/0805.3602v2)

> Inference in Bayesian statistics involves the evaluation of marginal likelihood integrals. We present algebraic algorithms for computing such integrals exactly for discrete data of small sample size. Our methods apply to both uniform priors and Dirichlet priors. The underlying statistical models are mixtures of independent distributions, or, in geometric language, secant varieties of Segre-Veronese varieties.

</details>

<details>

<summary>2009-02-17 20:47:41 - Bayesian inference of a negative quantity from positive measurement results</summary>

- *D Calonico, F Levi, L Lorini, G Mana*

- `0902.2994v1` - [abs](http://arxiv.org/abs/0902.2994v1) - [pdf](http://arxiv.org/pdf/0902.2994v1)

> In this paper the Bayesian analysis is applied to assign a probability density to the value of a quantity having a definite sign. This analysis is logically consistent with the results, positive or negative, of repeated measurements. Results are used to estimate the atom density shift in a caesium fountain clock. The comparison with the classical statistical analysis is also reported and the advantages of the Bayesian approach for the realization of the time unit are discussed.

</details>

<details>

<summary>2009-02-23 09:51:42 - Non-linear regression models for Approximate Bayesian Computation</summary>

- *M. G. B. Blum, O. Francois*

- `0809.4178v2` - [abs](http://arxiv.org/abs/0809.4178v2) - [pdf](http://arxiv.org/pdf/0809.4178v2)

> Approximate Bayesian inference on the basis of summary statistics is well-suited to complex problems for which the likelihood is either mathematically or computationally intractable. However the methods that use rejection suffer from the curse of dimensionality when the number of summary statistics is increased. Here we propose a machine-learning approach to the estimation of the posterior density by introducing two innovations. The new method fits a nonlinear conditional heteroscedastic regression of the parameter on the summary statistics, and then adaptively improves estimation using importance sampling. The new algorithm is compared to the state-of-the-art approximate Bayesian methods, and achieves considerable reduction of the computational burden in two examples of inference in statistical genetics and in a queueing model.

</details>

<details>

<summary>2009-02-25 16:16:17 - Quantile pyramids for Bayesian nonparametrics</summary>

- *Nils Lid Hjort, Stephen G. Walker*

- `0902.4410v1` - [abs](http://arxiv.org/abs/0902.4410v1) - [pdf](http://arxiv.org/pdf/0902.4410v1)

> P\'{o}lya trees fix partitions and use random probabilities in order to construct random probability measures. With quantile pyramids we instead fix probabilities and use random partitions. For nonparametric Bayesian inference we use a prior which supports piecewise linear quantile functions, based on the need to work with a finite set of partitions, yet we show that the limiting version of the prior exists. We also discuss and investigate an alternative model based on the so-called substitute likelihood. Both approaches factorize in a convenient way leading to relatively straightforward analysis via MCMC, since analytic summaries of posterior distributions are too complicated. We give conditions securing the existence of an absolute continuous quantile process, and discuss consistency and approximate normality for the sequence of posterior distributions. Illustrations are included.

</details>


## 2009-03

<details>

<summary>2009-03-04 18:55:45 - Estimation of cosmological parameters using adaptive importance sampling</summary>

- *Darren Wraith, Martin Kilbinger, Karim Benabed, Olivier Cappé, Jean-François Cardoso, Gersende Fort, Simon Prunet, Christian P. Robert*

- `0903.0837v1` - [abs](http://arxiv.org/abs/0903.0837v1) - [pdf](http://arxiv.org/pdf/0903.0837v1)

> We present a Bayesian sampling algorithm called adaptive importance sampling or Population Monte Carlo (PMC), whose computational workload is easily parallelizable and thus has the potential to considerably reduce the wall-clock time required for sampling, along with providing other benefits. To assess the performance of the approach for cosmological problems, we use simulated and actual data consisting of CMB anisotropies, supernovae of type Ia, and weak cosmological lensing, and provide a comparison of results to those obtained using state-of-the-art Markov Chain Monte Carlo (MCMC). For both types of data sets, we find comparable parameter estimates for PMC and MCMC, with the advantage of a significantly lower computational time for PMC. In the case of WMAP5 data, for example, the wall-clock time reduces from several days for MCMC to a few hours using PMC on a cluster of processors. Other benefits of the PMC approach, along with potential difficulties in using the approach, are analysed and discussed.

</details>

<details>

<summary>2009-03-15 18:58:19 - Perfect simulation for Bayesian wavelet thresholding with correlated coefficients</summary>

- *Graeme K. Ambler, Bernard W. Silverman*

- `0903.2654v1` - [abs](http://arxiv.org/abs/0903.2654v1) - [pdf](http://arxiv.org/pdf/0903.2654v1)

> We introduce a new method of Bayesian wavelet shrinkage for reconstructing a signal when we observe a noisy version. Rather than making the common assumption that the wavelet coefficients of the signal are independent, we allow for the possibility that they are locally correlated in both location (time) and scale (frequency). This leads us to a prior structure which is analytically intractable, but it is possible to draw independent samples from a close approximation to the posterior distribution by an approach based on Coupling From The Past.

</details>

<details>

<summary>2009-03-17 15:46:13 - Bayesian treed Gaussian process models with an application to computer modeling</summary>

- *Robert B. Gramacy, Herbert K. H. Lee*

- `0710.4536v10` - [abs](http://arxiv.org/abs/0710.4536v10) - [pdf](http://arxiv.org/pdf/0710.4536v10)

> Motivated by a computer experiment for the design of a rocket booster, this paper explores nonstationary modeling methodologies that couple stationary Gaussian processes with treed partitioning. Partitioning is a simple but effective method for dealing with nonstationarity. The methodological developments and statistical computing details which make this approach efficient are described in detail. In addition to providing an analysis of the rocket booster simulator, our approach is demonstrated to be effective in other arenas.

</details>

<details>

<summary>2009-03-24 22:04:35 - Statistical methods for cosmological parameter selection and estimation</summary>

- *Andrew R Liddle*

- `0903.4210v1` - [abs](http://arxiv.org/abs/0903.4210v1) - [pdf](http://arxiv.org/pdf/0903.4210v1)

> The estimation of cosmological parameters from precision observables is an important industry with crucial ramifications for particle physics. This article discusses the statistical methods presently used in cosmological data analysis, highlighting the main assumptions and uncertainties. The topics covered are parameter estimation, model selection, multi-model inference, and experimental design, all primarily from a Bayesian perspective.

</details>

<details>

<summary>2009-03-25 16:27:46 - Expansions for Quantiles and Multivariate Moments of Extremes for Distributions of Pareto Type</summary>

- *Saralees Nadarajah, Christopher S. Withers*

- `0903.4391v1` - [abs](http://arxiv.org/abs/0903.4391v1) - [pdf](http://arxiv.org/pdf/0903.4391v1)

> Let $X_{nr}$ be the $r$th largest of a random sample of size $n$ from a distribution $F (x) = 1 - \sum_{i = 0}^\infty c_i x^{-\alpha - i \beta}$ for $\alpha > 0$ and $\beta > 0$. An inversion theorem is proved and used to derive an expansion for the quantile $F^{-1} (u)$ and powers of it. From this an expansion in powers of $(n^{-1}, n^{-\beta/\alpha})$ is given for the multivariate moments of the extremes $\{X_{n, n - s_i}, 1 \leq i \leq k \}/n^{1/\alpha}$ for fixed ${\bf s} = (s_1, ..., s_k)$, where $k \geq 1$. Examples include the Cauchy, Student $t$, $F$, second extreme distributions and stable laws of index $\alpha < 1$.

</details>

<details>

<summary>2009-03-26 15:33:09 - On Properties of Estimators in non Regular Situations for Poisson Processes</summary>

- *Yury A. Kutoyants*

- `0903.4613v1` - [abs](http://arxiv.org/abs/0903.4613v1) - [pdf](http://arxiv.org/pdf/0903.4613v1)

> We consider the problem of parameter estimation by observations of inhomogeneous Poisson process. It is well-known that if the regularity conditions are fulfilled then the maximum likelihood and Bayesian estimators are consistent, asymptotically normal and asymptotically efficient. These regularity conditions can be roughly presented as follows: a) the intensity function of observed process belongs to known parametric family of functions, b) the model is identifiable, c) the Fisher information is positive continuous function, d) the intensity function is sufficiently smooth with respect to the unknown parameter, e) this parameter is an interior point of the interval. We are interested in the properties of estimators when these regularity conditions are not fulfilled. More precisely, we preset a review of the results which correspond to the rejection of these conditions one by one and we show how the properties of the MLE and Bayesian estimators change. The proofs of these results are essentially based on some general results by Ibragimov and Khasminskii.

</details>

<details>

<summary>2009-03-28 14:03:51 - Adaptive approximate Bayesian computation</summary>

- *Mark A. Beaumont, Jean-Marie Cornuet, Jean-Michel Marin, Christian P. Robert*

- `0805.2256v9` - [abs](http://arxiv.org/abs/0805.2256v9) - [pdf](http://arxiv.org/pdf/0805.2256v9)

> Sequential techniques can enhance the efficiency of the approximate Bayesian computation algorithm, as in Sisson et al.'s (2007) partial rejection control version. While this method is based upon the theoretical works of Del Moral et al. (2006), the application to approximate Bayesian computation results in a bias in the approximation to the posterior. An alternative version based on genuine importance sampling arguments bypasses this difficulty, in connection with the population Monte Carlo method of Cappe et al. (2004), and it includes an automatic scaling of the forward kernel. When applied to a population genetics example, it compares favourably with two other versions of the approximate algorithm.

</details>

<details>

<summary>2009-03-30 23:24:08 - Exact Non-Parametric Bayesian Inference on Infinite Trees</summary>

- *Marcus Hutter*

- `0903.5342v1` - [abs](http://arxiv.org/abs/0903.5342v1) - [pdf](http://arxiv.org/pdf/0903.5342v1)

> Given i.i.d. data from an unknown distribution, we consider the problem of predicting future items. An adaptive way to estimate the probability density is to recursively subdivide the domain to an appropriate data-dependent granularity. A Bayesian would assign a data-independent prior probability to "subdivide", which leads to a prior over infinite(ly many) trees. We derive an exact, fast, and simple inference algorithm for such a prior, for the data evidence, the predictive distribution, the effective model dimension, moments, and other quantities. We prove asymptotic convergence and consistency results, and illustrate the behavior of our model on some prototypical functions.

</details>


## 2009-04

<details>

<summary>2009-04-01 07:25:51 - Inference for censored quantile regression models in longitudinal studies</summary>

- *Huixia Judy Wang, Mendel Fygenson*

- `0904.0080v1` - [abs](http://arxiv.org/abs/0904.0080v1) - [pdf](http://arxiv.org/pdf/0904.0080v1)

> We develop inference procedures for longitudinal data where some of the measurements are censored by fixed constants. We consider a semi-parametric quantile regression model that makes no distributional assumptions. Our research is motivated by the lack of proper inference procedures for data from biomedical studies where measurements are censored due to a fixed quantification limit. In such studies the focus is often on testing hypotheses about treatment equality. To this end, we propose a rank score test for large sample inference on a subset of the covariates. We demonstrate the importance of accounting for both censoring and intra-subject dependency and evaluate the performance of our proposed methodology in a simulation study. We then apply the proposed inference procedures to data from an AIDS-related clinical trial. We conclude that our framework and proposed methodology is very valuable for differentiating the influences of predictors at different locations in the conditional distribution of a response variable.

</details>

<details>

<summary>2009-04-01 13:28:04 - The formal definition of reference priors</summary>

- *James O. Berger, José M. Bernardo, Dongchu Sun*

- `0904.0156v1` - [abs](http://arxiv.org/abs/0904.0156v1) - [pdf](http://arxiv.org/pdf/0904.0156v1)

> Reference analysis produces objective Bayesian inference, in the sense that inferential statements depend only on the assumed model and the available data, and the prior distribution used to make an inference is least informative in a certain information-theoretic sense. Reference priors have been rigorously defined in specific contexts and heuristically defined in general, but a rigorous general definition has been lacking. We produce a rigorous general definition here and then show how an explicit expression for the reference prior can be obtained under very weak regularity conditions. The explicit expression can be used to derive new reference priors both analytically and numerically.

</details>

<details>

<summary>2009-04-03 09:40:08 - ABC likelihood-freee methods for model choice in Gibbs random fields</summary>

- *Aude Grelaud, Christian Robert, Jean-Michel Marin, Francois Rodolphe, Jean-Francois Taly*

- `0807.2767v3` - [abs](http://arxiv.org/abs/0807.2767v3) - [pdf](http://arxiv.org/pdf/0807.2767v3)

> Gibbs random fields (GRF) are polymorphous statistical models that can be used to analyse different types of dependence, in particular for spatially correlated data. However, when those models are faced with the challenge of selecting a dependence structure from many, the use of standard model choice methods is hampered by the unavailability of the normalising constant in the Gibbs likelihood. In particular, from a Bayesian perspective, the computation of the posterior probabilities of the models under competition requires special likelihood-free simulation techniques like the Approximate Bayesian Computation (ABC) algorithm that is intensively used in population genetics. We show in this paper how to implement an ABC algorithm geared towards model choice in the general setting of Gibbs random fields, demonstrating in particular that there exists a sufficient statistic across models. The accuracy of the approximation to the posterior probabilities can be further improved by importance sampling on the distribution of the models. The practical aspects of the method are detailed through two applications, the test of an iid Bernoulli model versus a first-order Markov chain, and the choice of a folding structure for two proteins.

</details>

<details>

<summary>2009-04-03 10:15:49 - Pointwise adaptive estimation for robust and quantile regression</summary>

- *Markus Reiss, Yves Rozenholc, Charles-Andre Cuenod*

- `0904.0543v1` - [abs](http://arxiv.org/abs/0904.0543v1) - [pdf](http://arxiv.org/pdf/0904.0543v1)

> A nonparametric procedure for robust regression estimation and for quantile regression is proposed which is completely data-driven and adapts locally to the regularity of the regression function. This is achieved by considering in each point M-estimators over different local neighbourhoods and by a local model selection procedure based on sequential testing. Non-asymptotic risk bounds are obtained, which yield rate-optimality for large sample asymptotics under weak conditions. Simulations for different univariate median regression models show good finite sample properties, also in comparison to traditional methods. The approach is extended to image denoising and applied to CT scans in cancer research.

</details>

<details>

<summary>2009-04-04 15:23:20 - A Bayesian approach to the analysis of time symmetry in light curves: Reconsidering Scorpius X-1 occultations</summary>

- *Alexander W. Blocker, Pavlos Protopapas, Charles R. Alcock*

- `0904.0645v1` - [abs](http://arxiv.org/abs/0904.0645v1) - [pdf](http://arxiv.org/pdf/0904.0645v1)

> We present a new approach to the analysis of time symmetry in light curves, such as those in the x-ray at the center of the Scorpius X-1 occultation debate. Our method uses a new parameterization for such events (the bilogistic event profile) and provides a clear, physically relevant characterization of each event's key features. We also demonstrate a Markov Chain Monte Carlo algorithm to carry out this analysis, including a novel independence chain configuration for the estimation of each event's location in the light curve. These tools are applied to the Scorpius X-1 light curves presented in Chang et al. (2007), providing additional evidence based on the time series that the events detected thus far are most likely not occultations by TNOs.

</details>

<details>

<summary>2009-04-06 17:51:33 - Bayesian MAP Model Selection of Chain Event Graphs</summary>

- *Guy Freeman, Jim Q. Smith*

- `0904.0977v1` - [abs](http://arxiv.org/abs/0904.0977v1) - [pdf](http://arxiv.org/pdf/0904.0977v1)

> The class of chain event graph models is a generalisation of the class of discrete Bayesian networks, retaining most of the structural advantages of the Bayesian network for model interrogation, propagation and learning, while more naturally encoding asymmetric state spaces and the order in which events happen. In this paper we demonstrate how with complete sampling, conjugate closed form model selection based on product Dirichlet priors is possible, and prove that suitable homogeneity assumptions characterise the product Dirichlet prior on this class of models. We demonstrate our techniques using two educational examples.

</details>

<details>

<summary>2009-04-08 10:07:23 - Generalized Rejection Sampling Schemes and Applications in Signal Processing</summary>

- *Luca Martino, Joaquin Miguez*

- `0904.1300v1` - [abs](http://arxiv.org/abs/0904.1300v1) - [pdf](http://arxiv.org/pdf/0904.1300v1)

> Bayesian methods and their implementations by means of sophisticated Monte Carlo techniques, such as Markov chain Monte Carlo (MCMC) and particle filters, have become very popular in signal processing over the last years. However, in many problems of practical interest these techniques demand procedures for sampling from probability distributions with non-standard forms, hence we are often brought back to the consideration of fundamental simulation algorithms, such as rejection sampling (RS). Unfortunately, the use of RS techniques demands the calculation of tight upper bounds for the ratio of the target probability density function (pdf) over the proposal density from which candidate samples are drawn. Except for the class of log-concave target pdf's, for which an efficient algorithm exists, there are no general methods to analytically determine this bound, which has to be derived from scratch for each specific case. In this paper, we introduce new schemes for (a) obtaining upper bounds for likelihood functions and (b) adaptively computing proposal densities that approximate the target pdf closely. The former class of methods provides the tools to easily sample from a posteriori probability distributions (that appear very often in signal processing problems) by drawing candidates from the prior distribution. However, they are even more useful when they are exploited to derive the generalized adaptive RS (GARS) algorithm introduced in the second part of the paper. The proposed GARS method yields a sequence of proposal densities that converge towards the target pdf and enable a very efficient sampling of a broad class of probability distributions, possibly with multiple modes and non-standard forms.

</details>

<details>

<summary>2009-04-10 09:23:10 - The Redundancy of a Computable Code on a Noncomputable Distribution</summary>

- *Łukasz Dębowski*

- `0901.2321v2` - [abs](http://arxiv.org/abs/0901.2321v2) - [pdf](http://arxiv.org/pdf/0901.2321v2)

> We introduce new definitions of universal and superuniversal computable codes, which are based on a code's ability to approximate Kolmogorov complexity within the prescribed margin for all individual sequences from a given set. Such sets of sequences may be singled out almost surely with respect to certain probability measures. Consider a measure parameterized with a real parameter and put an arbitrary prior on the parameter. The Bayesian measure is the expectation of the parameterized measure with respect to the prior. It appears that a modified Shannon-Fano code for any computable Bayesian measure, which we call the Bayesian code, is superuniversal on a set of parameterized measure-almost all sequences for prior-almost every parameter. According to this result, in the typical setting of mathematical statistics no computable code enjoys redundancy which is ultimately much less than that of the Bayesian code. Thus we introduce another characteristic of computable codes: The catch-up time is the length of data for which the code length drops below the Kolmogorov complexity plus the prescribed margin. Some codes may have smaller catch-up times than Bayesian codes.

</details>

<details>

<summary>2009-04-19 11:33:01 - Sparse Bayesian Hierarchical Modeling of High-dimensional Clustering Problems</summary>

- *Heng Lian*

- `0904.2906v1` - [abs](http://arxiv.org/abs/0904.2906v1) - [pdf](http://arxiv.org/pdf/0904.2906v1)

> Clustering is one of the most widely used procedures in the analysis of microarray data, for example with the goal of discovering cancer subtypes based on observed heterogeneity of genetic marks between different tissues. It is well-known that in such high-dimensional settings, the existence of many noise variables can overwhelm the few signals embedded in the high-dimensional space. We propose a novel Bayesian approach based on Dirichlet process with a sparsity prior that simultaneous performs variable selection and clustering, and also discover variables that only distinguish a subset of the cluster components. Unlike previous Bayesian formulations, we use Dirichlet process (DP) for both clustering of samples as well as for regularizing the high-dimensional mean/variance structure. To solve the computational challenge brought by this double usage of DP, we propose to make use of a sequential sampling scheme embedded within Markov chain Monte Carlo (MCMC) updates to improve the naive implementation of existing algorithms for DP mixture models. Our method is demonstrated on a simulation study and illustrated with the leukemia gene expression dataset.

</details>

<details>

<summary>2009-04-20 08:48:27 - Consistency of Bayesian procedures for variable selection</summary>

- *George Casella, F. Javier Girón, M. Lina Martínez, Elías Moreno*

- `0904.2978v1` - [abs](http://arxiv.org/abs/0904.2978v1) - [pdf](http://arxiv.org/pdf/0904.2978v1)

> It has long been known that for the comparison of pairwise nested models, a decision based on the Bayes factor produces a consistent model selector (in the frequentist sense). Here we go beyond the usual consistency for nested pairwise models, and show that for a wide class of prior distributions, including intrinsic priors, the corresponding Bayesian procedure for variable selection in normal regression is consistent in the entire class of normal linear models. We find that the asymptotics of the Bayes factors for intrinsic priors are equivalent to those of the Schwarz (BIC) criterion. Also, recall that the Jeffreys--Lindley paradox refers to the well-known fact that a point null hypothesis on the normal mean parameter is always accepted when the variance of the conjugate prior goes to infinity. This implies that some limiting forms of proper prior distributions are not necessarily suitable for testing problems. Intrinsic priors are limits of proper prior distributions, and for finite sample sizes they have been proved to behave extremely well for variable selection in regression; a consequence of our results is that for intrinsic priors Lindley's paradox does not arise.

</details>

<details>

<summary>2009-04-23 09:52:48 - Bayesian outlier detection in Capital Asset Pricing Model</summary>

- *Maria Elena De Giuli, Mario Alessandro Maggi, Claudia Tarantola*

- `0806.1631v2` - [abs](http://arxiv.org/abs/0806.1631v2) - [pdf](http://arxiv.org/pdf/0806.1631v2)

> We propose a novel Bayesian optimisation procedure for outlier detection in the Capital Asset Pricing Model. We use a parametric product partition model to robustly estimate the systematic risk of an asset. We assume that the returns follow independent normal distributions and we impose a partition structure on the parameters of interest. The partition structure imposed on the parameters induces a corresponding clustering of the returns. We identify via an optimisation procedure the partition that best separates standard observations from the atypical ones. The methodology is illustrated with reference to a real data set, for which we also provide a microeconomic interpretation of the detected outliers.

</details>

<details>

<summary>2009-04-27 09:58:19 - A Central Limit Theorem and its Applications to Multicolor Randomly Reinforced Urns</summary>

- *Patrizia Berti, Irene Crimaldi, Luca Pratelli, Pietro Rigo*

- `0904.0932v2` - [abs](http://arxiv.org/abs/0904.0932v2) - [pdf](http://arxiv.org/pdf/0904.0932v2)

> We give a central limit theorem, which has applications to Bayesian statistics and urn problems. The latter are investigated, by paying special attention to multicolor randomly reinforced generalized Polya urns.

</details>


## 2009-05

<details>

<summary>2009-05-02 15:08:44 - A probability for classification based on the mixture of Dirichlet process model</summary>

- *Ruth Fuentes-Garcia, Ramses H Mena, Stephen G Walker*

- `0905.0210v1` - [abs](http://arxiv.org/abs/0905.0210v1) - [pdf](http://arxiv.org/pdf/0905.0210v1)

> In this paper, we provide an explicit probability distribution for classification purposes. It is derived from the Bayesian nonparametric mixture of Dirichlet process model, but with suitable modifications which remove unsuitable aspects of the classification based on this model. The resulting approach then more closely resembles a classical hierarchical grouping rule in that it depends on sums of squares of neighboring values. The proposed probability model for classification relies on a simulation algorithm which will be based on a reversible MCMC algorithm for determining the probabilities, and we provide numerical illustrations comparing with alternative ideas for classification.

</details>

<details>

<summary>2009-05-04 06:28:37 - Iterative Maximum Likelihood on Networks</summary>

- *Elchanan Mossel, Omer Tamuz*

- `0904.4903v2` - [abs](http://arxiv.org/abs/0904.4903v2) - [pdf](http://arxiv.org/pdf/0904.4903v2)

> We consider n agents located on the vertices of a connected graph. Each agent v receives a signal X_v(0)~N(s, 1) where s is an unknown quantity. A natural iterative way of estimating s is to perform the following procedure. At iteration t + 1 let X_v(t + 1) be the average of X_v(t) and of X_w(t) among all the neighbors w of v.   In this paper we consider a variant of simple iterative averaging, which models "greedy" behavior of the agents. At iteration t, each agent v declares the value of its estimator X_v(t) to all of its neighbors. Then, it updates X_v(t + 1) by taking the maximum likelihood (or minimum variance) estimator of s, given X_v(t) and X_w(t) for all neighbors w of v, and the structure of the graph.   We give an explicit efficient procedure for calculating X_v(t), study the convergence of the process as t goes to infinity and show that if the limit exists then it is the same for all v and w. For graphs that are symmetric under actions of transitive groups, we show that the process is efficient. Finally, we show that the greedy process is in some cases more efficient than simple averaging, while in other cases the converse is true, so that, in this model, "greed" of the individual agents may or may not have an adverse affect on the outcome.   The model discussed here may be viewed as the Maximum-Likelihood version of models studied in Bayesian Economics. The ML variant is more accessible and allows in particular to show the significance of symmetry in the efficiency of estimators using networks of agents.

</details>

<details>

<summary>2009-05-12 22:58:13 - Extremal dependence analysis of network sessions</summary>

- *Luis Lopez-Oliveros, Sidney I. Resnick*

- `0905.1983v1` - [abs](http://arxiv.org/abs/0905.1983v1) - [pdf](http://arxiv.org/pdf/0905.1983v1)

> We refine a stimulating study by Sarvotham et al. [2005] which highlighted the influence of peak transmission rate on network burstiness. From TCP packet headers, we amalgamate packets into sessions where each session is characterized by a 5-tuple (S, D, R, Peak R, Initiation T)=(total payload, duration, average transmission rate, peak transmission rate, initiation time). After careful consideration, a new definition of peak rate is required. Unlike Sarvotham et al. [2005] who segmented sessions into two groups labelled alpha and beta, we segment into 10 sessions according to the empirical quantiles of the peak rate variable as a demonstration that the beta group is far from homogeneous. Our more refined segmentation reveals additional structure that is missed by segmentation into two groups. In each segment, we study the dependence structure of (S, D, R) and find that it varies across the groups. Furthermore, within each segment, session initiation times are well approximated by a Poisson process whereas this property does not hold for the data set taken as a whole. Therefore, we conclude that the peak rate level is important for understanding structure and for constructing accurate simulations of data in the wild. We outline a simple method of simulating network traffic based on our findings.

</details>

<details>

<summary>2009-05-13 23:54:51 - Non-Bayesian particle filters</summary>

- *Alexandre J. Chorin, Xuemin Tu*

- `0905.2181v1` - [abs](http://arxiv.org/abs/0905.2181v1) - [pdf](http://arxiv.org/pdf/0905.2181v1)

> Particle filters for data assimilation in nonlinear problems use "particles" (replicas of the underlying system) to generate a sequence of probability density functions (pdfs) through a Bayesian process. This can be expensive because a significant number of particles has to be used to maintain accuracy. We offer here an alternative, in which the relevant pdfs are sampled directly by an iteration. An example is discussed in detail.

</details>

<details>

<summary>2009-05-15 11:16:13 - CHASSIS - Inverse Modelling of Relaxed Dynamical Systems</summary>

- *Dalia Chakrabarty*

- `0905.2524v1` - [abs](http://arxiv.org/abs/0905.2524v1) - [pdf](http://arxiv.org/pdf/0905.2524v1)

> The state of a non-relativistic gravitational dynamical system is known at any time $t$ if the dynamical rule, i.e. Newton's equations of motion, can be solved; this requires specification of the gravitational potential. The evolution of a bunch of phase space coordinates ${\bf w}$ is deterministic, though generally non-linear. We discuss the novel Bayesian non-parametric algorithm CHASSIS that gives phase space $pdf$ $f({\bf w})$ and potential $\Phi({\bf x})$ of a relaxed gravitational system. CHASSIS is undemanding in terms of input requirements in that it is viable given incomplete, single-component velocity information of system members. Here ${\bf x}$ is the 3-D spatial coordinate and ${\bf w}={\bf x+v}$ where ${\bf v}$ is the 3-D velocity vector. CHASSIS works with a 2-integral $f=f(E, L)$ where energy $E=\Phi + v^2/2, \: v^2 = \sum_{i=1}^{3}{v_i^2}$ and the angular momentum is $L = |{\bf r}\times{\bf v}|$, where ${\bf r}$ is the spherical spatial vector. Also, we assume spherical symmetry. CHASSIS obtains the $f(\cdot)$ from which the kinematic data is most likely to have been drawn, in the best choice for $\Phi(\cdot)$, using an MCMC optimiser (Metropolis-Hastings). The likelihood function ${\cal{L}}$ is defined in terms of the projections of $f(\cdot)$ into the space of observables and the maximum in ${\cal{L}}$ is sought by the optimiser.

</details>

<details>

<summary>2009-05-15 13:08:35 - Bayesian Analysis of Value-at-Risk with Product Partition Models</summary>

- *Giacomo Bormetti, Maria Elena De Giuli, Danilo Delpini, Claudia Tarantola*

- `0809.0241v2` - [abs](http://arxiv.org/abs/0809.0241v2) - [pdf](http://arxiv.org/pdf/0809.0241v2)

> In this paper we propose a novel Bayesian methodology for Value-at-Risk computation based on parametric Product Partition Models. Value-at-Risk is a standard tool to measure and control the market risk of an asset or a portfolio, and it is also required for regulatory purposes. Its popularity is partly due to the fact that it is an easily understood measure of risk. The use of Product Partition Models allows us to remain in a Normal setting even in presence of outlying points, and to obtain a closed-form expression for Value-at-Risk computation. We present and compare two different scenarios: a product partition structure on the vector of means and a product partition structure on the vector of variances. We apply our methodology to an Italian stock market data set from Mib30. The numerical results clearly show that Product Partition Models can be successfully exploited in order to quantify market risk exposure. The obtained Value-at-Risk estimates are in full agreement with Maximum Likelihood approaches, but our methodology provides richer information about the clustering structure of the data and the presence of outlying points.

</details>

<details>

<summary>2009-05-15 13:53:02 - Statistical analysis of stellar evolution</summary>

- *David A. van Dyk, Steven DeGennaro, Nathan Stein, William H. Jefferys, Ted von Hippel*

- `0905.2547v1` - [abs](http://arxiv.org/abs/0905.2547v1) - [pdf](http://arxiv.org/pdf/0905.2547v1)

> Color-Magnitude Diagrams (CMDs) are plots that compare the magnitudes (luminosities) of stars in different wavelengths of light (colors). High nonlinear correlations among the mass, color, and surface temperature of newly formed stars induce a long narrow curved point cloud in a CMD known as the main sequence. Aging stars form new CMD groups of red giants and white dwarfs. The physical processes that govern this evolution can be described with mathematical models and explored using complex computer models. These calculations are designed to predict the plotted magnitudes as a function of parameters of scientific interest, such as stellar age, mass, and metallicity. Here, we describe how we use the computer models as a component of a complex likelihood function in a Bayesian analysis that requires sophisticated computing, corrects for contamination of the data by field stars, accounts for complications caused by unresolved binary-star systems, and aims to compare competing physics-based computer models of stellar evolution.

</details>

<details>

<summary>2009-05-18 11:24:05 - Designing a Bayesian Network for Preventive Maintenance from Expert Opinions in a Rapid and Reliable Way</summary>

- *Gilles Celeux, Franck Corset, A. Lannoy, Benoit Ricard*

- `0905.2864v1` - [abs](http://arxiv.org/abs/0905.2864v1) - [pdf](http://arxiv.org/pdf/0905.2864v1)

> In this study, a Bayesian Network (BN) is considered to represent a nuclear plant mechanical system degradation. It describes a causal representation of the phenomena involved in the degradation process. Inference from such a BN needs to specify a great number of marginal and conditional probabilities. As, in the present context, information is based essentially on expert knowledge, this task becomes very complex and rapidly impossible. We present a solution which consists of considering the BN as a log-linear model on which simplification constraints are assumed. This approach results in a considerable decrease in the number of probabilities to be given by experts. In addition, we give some simple rules to choose the most reliable probabilities. We show that making use of those rules allows to check the consistency of the derived probabilities. Moreover, we propose a feedback procedure to eliminate inconsistent probabilities. Finally, the derived probabilities that we propose to solve the equations involved in a realistic Bayesian network are expected to be reliable. The resulting methodology to design a significant and powerful BN is applied to a reactor coolant sub-component in EDF Nuclear plants in an illustrative purpose.

</details>

<details>

<summary>2009-05-22 07:30:29 - Bayesian model comparison and model averaging for small-area estimation</summary>

- *Murray Aitkin, Charles C. Liu, Tom Chadwick*

- `0905.3620v1` - [abs](http://arxiv.org/abs/0905.3620v1) - [pdf](http://arxiv.org/pdf/0905.3620v1)

> This paper considers small-area estimation with lung cancer mortality data, and discusses the choice of upper-level model for the variation over areas. Inference about the random effects for the areas may depend strongly on the choice of this model, but this choice is not a straightforward matter. We give a general methodology for both evaluating the data evidence for different models and averaging over plausible models to give robust area effect distributions. We reanalyze the data of Tsutakawa [Biometrics 41 (1985) 69--79] on lung cancer mortality rates in Missouri cities, and show the differences in conclusions about the city rates from this methodology.

</details>

<details>

<summary>2009-05-25 20:11:36 - Adaptive design and analysis of supercomputer experiments</summary>

- *Robert B. Gramacy, Herbert K. H. Lee*

- `0805.4359v4` - [abs](http://arxiv.org/abs/0805.4359v4) - [pdf](http://arxiv.org/pdf/0805.4359v4)

> Computer experiments are often performed to allow modeling of a response surface of a physical experiment that can be too costly or difficult to run except using a simulator. Running the experiment over a dense grid can be prohibitively expensive, yet running over a sparse design chosen in advance can result in obtaining insufficient information in parts of the space, particularly when the surface calls for a nonstationary model. We propose an approach that automatically explores the space while simultaneously fitting the response surface, using predictive uncertainty to guide subsequent experimental runs. The newly developed Bayesian treed Gaussian process is used as the surrogate model, and a fully Bayesian approach allows explicit measures of uncertainty. We develop an adaptive sequential design framework to cope with an asynchronous, random, agent--based supercomputing environment, by using a hybrid approach that melds optimal strategies from the statistics literature with flexible strategies from the active learning literature. The merits of this approach are borne out in several examples, including the motivating computational fluid dynamics simulation of a rocket booster.

</details>

<details>

<summary>2009-05-29 17:33:23 - Skellam shrinkage: Wavelet-based intensity estimation for inhomogeneous Poisson data</summary>

- *Keigo Hirakawa, Patrick J. Wolfe*

- `0905.3217v2` - [abs](http://arxiv.org/abs/0905.3217v2) - [pdf](http://arxiv.org/pdf/0905.3217v2)

> The ubiquity of integrating detectors in imaging and other applications implies that a variety of real-world data are well modeled as Poisson random variables whose means are in turn proportional to an underlying vector-valued signal of interest. In this article, we first show how the so-called Skellam distribution arises from the fact that Haar wavelet and filterbank transform coefficients corresponding to measurements of this type are distributed as sums and differences of Poisson counts. We then provide two main theorems on Skellam shrinkage, one showing the near-optimality of shrinkage in the Bayesian setting and the other providing for unbiased risk estimation in a frequentist context. These results serve to yield new estimators in the Haar transform domain, including an unbiased risk estimate for shrinkage of Haar-Fisz variance-stabilized data, along with accompanying low-complexity algorithms for inference. We conclude with a simulation study demonstrating the efficacy of our Skellam shrinkage estimators both for the standard univariate wavelet test functions as well as a variety of test images taken from the image processing literature, confirming that they offer substantial performance improvements over existing alternatives.

</details>

<details>

<summary>2009-05-29 18:23:31 - Inferring dynamic genetic networks with low order independencies</summary>

- *Sophie Lèbre*

- `0704.2551v6` - [abs](http://arxiv.org/abs/0704.2551v6) - [pdf](http://arxiv.org/pdf/0704.2551v6)

> In this paper, we propose a novel inference method for dynamic genetic networks which makes it possible to face with a number of time measurements n much smaller than the number of genes p. The approach is based on the concept of low order conditional dependence graph that we extend here in the case of Dynamic Bayesian Networks. Most of our results are based on the theory of graphical models associated with the Directed Acyclic Graphs (DAGs). In this way, we define a minimal DAG G which describes exactly the full order conditional dependencies given the past of the process. Then, to face with the large p and small n estimation case, we propose to approximate DAG G by considering low order conditional independencies. We introduce partial qth order conditional dependence DAGs G(q) and analyze their probabilistic properties. In general, DAGs G(q) differ from DAG G but still reflect relevant dependence facts for sparse networks such as genetic networks. By using this approximation, we set out a non-bayesian inference method and demonstrate the effectiveness of this approach on both simulated and real data analysis. The inference procedure is implemented in the R package 'G1DBN' freely available from the CRAN archive.

</details>


## 2009-06

<details>

<summary>2009-06-05 11:29:56 - Modeling substitution and indel processes for AFLP marker evolution and phylogenetic inference</summary>

- *Ruiyan Luo, Bret Larget*

- `0906.1094v1` - [abs](http://arxiv.org/abs/0906.1094v1) - [pdf](http://arxiv.org/pdf/0906.1094v1)

> The amplified fragment length polymorphism (AFLP) method produces anonymous genetic markers from throughout a genome. We extend the nucleotide substitution model of AFLP evolution to additionally include insertion and deletion processes. The new Sub-ID model relaxes the common assumption that markers are independent and homologous. We build a Markov chain Monte Carlo methodology tailored for the Sub-ID model to implement a Bayesian approach to infer AFLP marker evolution. The method allows us to infer both the phylogenies and the subset of markers that are possibly homologous. In addition, we can infer the genome-wide relative rate of indels versus substitutions. In a case study with AFLP markers from sedges, a grass-like plant common in North America, we find that accounting for insertion and deletion makes a difference in phylogenetic inference. The inference of topologies is not sensitive to the prior settings and the Jukes--Cantor assumption for nucleotide substitution. The model for insertion and deletion we introduce has potential value in other phylogenetic applications.

</details>

<details>

<summary>2009-06-06 11:33:53 - U-Quantile-Statistics</summary>

- *Michael Mayer*

- `0906.1266v1` - [abs](http://arxiv.org/abs/0906.1266v1) - [pdf](http://arxiv.org/pdf/0906.1266v1)

> In 1948, W. Hoeffding introduced a large class of unbiased estimators called U-statistics, defined as the average value of a real-valued m-variate function h calculated at all possible sets of m points from a random sample. In the present paper, we investigate the corresponding robust analogue which we call U-quantile-statistics. We are concerned with the asymptotic behavior of the sample p-quantile of such function h instead of its average. Alternatively, U-quantile-statistics can be viewed as quantile estimators for a certain class of dependent random variables. Examples are given by a slightly modified Hodges-Lehmann estimator of location and the median interpoint distance among random points in space.

</details>

<details>

<summary>2009-06-07 22:01:01 - Some notable properties of the standard oncology phase I design</summary>

- *Gregory J. Hather, Howard Mackey*

- `0808.0359v2` - [abs](http://arxiv.org/abs/0808.0359v2) - [pdf](http://arxiv.org/pdf/0808.0359v2)

> We identify three properties of the standard oncology phase I trial design or 3 + 3 design. We show that the standard design implicitly uses isotonic regression to estimate a maximum tolerated dose. We next illustrate the relationship between the standard design and a Bayesian design proposed by Ji et al. (2007). A slight modification to this Bayesian design, under a particular model specification, would assign treatments in a manner identical to the standard design. We finally present calculations revealing the behavior of the standard design in a worst case scenario and compare its behavior with other 3 + 3-like designs.

</details>

<details>

<summary>2009-06-16 07:40:23 - A Three-Parameter Binomial Approximation</summary>

- *Vydas Čekanavičius, Erol A. Peköz, Adrian Röllin, Michael Shwartz*

- `0906.2855v1` - [abs](http://arxiv.org/abs/0906.2855v1) - [pdf](http://arxiv.org/pdf/0906.2855v1)

> We approximate the distribution of the sum of independent but not necessarily identically distributed Bernoulli random variables using a shifted binomial distribution where the three parameters (the number of trials, the probability of success, and the shift amount) are chosen to match up the first three moments of the two distributions. We give a bound on the approximation error in terms of the total variation metric using Stein's method. A numerical study is discussed that shows shifted binomial approximations typically are more accurate than Poisson or standard binomial approximations. The application of the approximation to solving a problem arising in Bayesian hierarchical modeling is also discussed.

</details>


## 2009-07

<details>

<summary>2009-07-02 23:59:08 - Optimal sequential procedures with Bayes decision rules</summary>

- *Andrey Novikov*

- `0812.0159v2` - [abs](http://arxiv.org/abs/0812.0159v2) - [pdf](http://arxiv.org/pdf/0812.0159v2)

> In this article, a general problem of sequential statistical inference for general discrete-time stochastic processes is considered. The problem is to minimize an average sample number given that Bayesian risk due to incorrect decision does not exceed some given bound. We characterize the form of optimal sequential stopping rules in this problem. In particular, we have a characterization of the form of optimal sequential decision procedures when the Bayesian risk includes both the loss due to incorrect decision and the cost of observations.

</details>

<details>

<summary>2009-07-04 18:24:06 - Bayesian Agglomerative Clustering with Coalescents</summary>

- *Yee Whye Teh, Hal Daumé III, Daniel Roy*

- `0907.0781v1` - [abs](http://arxiv.org/abs/0907.0781v1) - [pdf](http://arxiv.org/pdf/0907.0781v1)

> We introduce a new Bayesian model for hierarchical clustering based on a prior over trees called Kingman's coalescent. We develop novel greedy and sequential Monte Carlo inferences which operate in a bottom-up agglomerative fashion. We show experimentally the superiority of our algorithms over others, and demonstrate our approach in document clustering and phylolinguistics.

</details>

<details>

<summary>2009-07-15 01:34:16 - Why we (usually) don't have to worry about multiple comparisons</summary>

- *Andrew Gelman, Jennifer Hill, Masanao Yajima*

- `0907.2478v1` - [abs](http://arxiv.org/abs/0907.2478v1) - [pdf](http://arxiv.org/pdf/0907.2478v1)

> Applied researchers often find themselves making statistical inferences in settings that would seem to require multiple comparisons adjustments. We challenge the Type I error paradigm that underlies these corrections. Moreover we posit that the problem of multiple comparisons can disappear entirely when viewed from a hierarchical Bayesian perspective. We propose building multilevel models in the settings where multiple comparisons arise.   Multilevel models perform partial pooling (shifting estimates toward each other), whereas classical procedures typically keep the centers of intervals stationary, adjusting for multiple comparisons by making the intervals wider (or, equivalently, adjusting the $p$-values corresponding to intervals of fixed width). Thus, multilevel models address the multiple comparisons problem and also yield more efficient estimates, especially in settings with low group-level variation, which is where multiple comparisons are a particular concern.

</details>

<details>

<summary>2009-07-28 07:09:05 - Bayesian estimate of the zero-density frequency of a Cs fountain</summary>

- *D Calonico, F Levi, L Lorini, G Mana*

- `0907.4849v1` - [abs](http://arxiv.org/abs/0907.4849v1) - [pdf](http://arxiv.org/pdf/0907.4849v1)

> Caesium fountain frequency-standards realize the second in the International System of Units with a relative uncertainty approaching 10^-16. Among the main contributions to the accuracy budget, cold collisions play an important role because of the atomic density shift of the reference atomic transition. This paper describes an application of the Bayesian analysis of the clock frequency to estimate the density shift and describes how the Bayes theorem allows the a priori knowledge of the sign of the collisional coefficient to be rigourously embedded into the analysis. As an application, data from the INRIM caesium fountain are used and the Bayesian and orthodox analyses are compared. The Bayes theorem allows the orthodox uncertainty to be reduced by 28% and demonstrates to be an important tool in primary frequency-metrology.

</details>

<details>

<summary>2009-07-28 13:37:55 - Nonasymptotic bounds on the estimation error for regenerative MCMC algorithms</summary>

- *Krzysztof Latuszynski, Blazej Miasojedow, Wojciech Niemiro*

- `0907.4915v1` - [abs](http://arxiv.org/abs/0907.4915v1) - [pdf](http://arxiv.org/pdf/0907.4915v1)

> MCMC methods are used in Bayesian statistics not only to sample from posterior distributions but also to estimate expectations. Underlying functions are most often defined on a continuous state space and can be unbounded. We consider a regenerative setting and Monte Carlo estimators based on i.i.d. blocks of a Markov chain trajectory. The main result is an inequality for the mean square error. We also consider confidence bounds. We first derive the results in terms of the asymptotic variance and then bound the asymptotic variance for both uniformly ergodic and geometrically ergodic Markov chains.

</details>

<details>

<summary>2009-07-29 13:33:51 - Computational methods for Bayesian model choice</summary>

- *Christian P. Robert, Darren Wraith*

- `0907.5123v1` - [abs](http://arxiv.org/abs/0907.5123v1) - [pdf](http://arxiv.org/pdf/0907.5123v1)

> In this note, we shortly survey some recent approaches on the approximation of the Bayes factor used in Bayesian hypothesis testing and in Bayesian model choice. In particular, we reassess importance sampling, harmonic mean sampling, and nested sampling from a unified perspective.

</details>

<details>

<summary>2009-07-31 13:38:37 - On infinite-dimensional hierarchical probability models in statistical inverse problems</summary>

- *Tapio Helin*

- `0907.5322v2` - [abs](http://arxiv.org/abs/0907.5322v2) - [pdf](http://arxiv.org/pdf/0907.5322v2)

> In this article, the solution of a statistical inverse problem $M = AU+\mathcal{E}$ by the Bayesian approach is studied where $U$ is a function on the unit circle $\mathbb{T}$, i.e., a periodic signal. The mapping $A$ is a smoothing linear operator and $\mathcal{E}$ a Gaussian noise. The connection to the solution of a finite-dimensional computational model $M_{kn} = A_k U_n + \mathcal{E}_k$ is discussed. Furthermore, a novel hierarchical prior model for obtaining edge-preserving conditional mean estimates is introduced. The convergence of the method with respect to finer discretization is studied and the posterior distribution is shown to converge weakly. Finally, theoretical findings are illustrated by a numerical example with simulated data.

</details>


## 2009-08

<details>

<summary>2009-08-02 17:28:59 - Zero-state Markov switching count-data models: an empirical assessment</summary>

- *Nataliya V. Malyshkina, Fred L. Mannering*

- `0811.3639v2` - [abs](http://arxiv.org/abs/0811.3639v2) - [pdf](http://arxiv.org/pdf/0811.3639v2)

> In this study, a two-state Markov switching count-data model is proposed as an alternative to zero-inflated models to account for the preponderance of zeros sometimes observed in transportation count data, such as the number of accidents occurring on a roadway segment over some period of time. For this accident-frequency case, zero-inflated models assume the existence of two states: one of the states is a zero-accident count state, in which accident probabilities are so low that they cannot be statistically distinguished from zero, and the other state is a normal count state, in which counts can be non-negative integers that are generated by some counting process, for example, a Poisson or negative binomial. In contrast to zero-inflated models, Markov switching models allow specific roadway segments to switch between the two states over time. An important advantage of this Markov switching approach is that it allows for the direct statistical estimation of the specific roadway-segment state (i.e., zero or count state) whereas traditional zero-inflated models do not. To demonstrate the applicability of this approach, a two-state Markov switching negative binomial model (estimated with Bayesian inference) and standard zero-inflated negative binomial models are estimated using five-year accident frequencies on Indiana interstate highway segments. It is shown that the Markov switching model is a viable alternative and results in a superior statistical fit relative to the zero-inflated models.

</details>

<details>

<summary>2009-08-05 01:10:09 - The Infinite Hierarchical Factor Regression Model</summary>

- *Piyush Rai, Hal Daumé III*

- `0908.0570v1` - [abs](http://arxiv.org/abs/0908.0570v1) - [pdf](http://arxiv.org/pdf/0908.0570v1)

> We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman's coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis.

</details>

<details>

<summary>2009-08-12 15:41:16 - Missing data in a stochastic Dollo model for cognate data, and its application to the dating of Proto-Indo-European</summary>

- *Robin J. Ryder, Geoff K. Nicholls*

- `0908.1735v1` - [abs](http://arxiv.org/abs/0908.1735v1) - [pdf](http://arxiv.org/pdf/0908.1735v1)

> Nicholls and Gray (2008) describe a phylogenetic model for trait data. They use their model to estimate branching times on Indo-European language trees from lexical data. Alekseyenko et al. (2008) extended the model and give applications in genetics. In this paper we extend the inference to handle data missing at random. When trait data are gathered, traits are thinned in a way that depends on both the trait and missing-data content. Nicholls and Gray (2008) treat missing records as absent traits. Hittite has 12% missing trait records. Its age is poorly predicted in their cross-validation. Our prediction is consistent with the historical record. Nicholls and Gray (2008) dropped seven languages with too much missing data. We fit all twenty four languages in the lexical data of Ringe (2002). In order to model spatial-temporal rate heterogeneity we add a catastrophe process to the model. When a language passes through a catastrophe, many traits change at the same time. We fit the full model in a Bayesian setting, via MCMC.   We validate our fit using Bayes factors to test known age constraints. We reject three of thirty historically attested constraints. Our main result is a unimodel posterior distribution for the age of Proto-Indo-European centered at 8400 years BP with 95% HPD equal 7100-9800 years BP.

</details>

<details>

<summary>2009-08-13 12:21:10 - Asymptotics for posterior hazards</summary>

- *Pierpaolo De Blasi, Giovanni Peccati, Igor Prünster*

- `0908.1882v1` - [abs](http://arxiv.org/abs/0908.1882v1) - [pdf](http://arxiv.org/pdf/0908.1882v1)

> An important issue in survival analysis is the investigation and the modeling of hazard rates. Within a Bayesian nonparametric framework, a natural and popular approach is to model hazard rates as kernel mixtures with respect to a completely random measure. In this paper we provide a comprehensive analysis of the asymptotic behavior of such models. We investigate consistency of the posterior distribution and derive fixed sample size central limit theorems for both linear and quadratic functionals of the posterior hazard rate. The general results are then specialized to various specific kernels and mixing measures yielding consistency under minimal conditions and neat central limit theorems for the distribution of functionals.

</details>

<details>

<summary>2009-08-14 07:46:45 - Bayesball: A Bayesian hierarchical model for evaluating fielding in major league baseball</summary>

- *Shane T. Jensen, Kenneth E. Shirley, Abraham J. Wyner*

- `0802.4317v3` - [abs](http://arxiv.org/abs/0802.4317v3) - [pdf](http://arxiv.org/pdf/0802.4317v3)

> The use of statistical modeling in baseball has received substantial attention recently in both the media and academic community. We focus on a relatively under-explored topic: the use of statistical models for the analysis of fielding based on high-resolution data consisting of on-field location of batted balls. We combine spatial modeling with a hierarchical Bayesian structure in order to evaluate the performance of individual fielders while sharing information between fielders at each position. We present results across four seasons of MLB data (2002--2005) and compare our approach to other fielding evaluation procedures.

</details>

<details>

<summary>2009-08-14 13:52:33 - Statistical inference for stochastic epidemic models with three levels of mixing</summary>

- *Tom Britton, Theodore Kypraios, Philip O'Neill*

- `0908.2066v1` - [abs](http://arxiv.org/abs/0908.2066v1) - [pdf](http://arxiv.org/pdf/0908.2066v1)

> A stochastic epidemic model is defined in which each individual belongs to a household, a secondary grouping (typically school or workplace) and also the community as a whole. Moreover, infectious contacts take place in these three settings according to potentially different rates. For this model we consider how different kinds of data can be used to estimate the infection rate parameters with a view to understanding what can and cannot be inferred, and with what precision. Among other things we find that temporal data can be of considerable inferential benefit compared to final size data, that the degree of heterogeneity in the data can have a considerable effect on inference for non-household transmission, and that inferences can be materially different from those obtained from a model with two levels of mixing.   Keywords: Basic reproduction number, Bayesian inference, Epidemic model, Infectious disease data, Markov chain Monte Carlo, Networks.

</details>

<details>

<summary>2009-08-14 17:01:15 - Rigorous confidence bounds for MCMC under a geometric drift condition</summary>

- *Krzysztof Latuszynski, Wojciech Niemiro*

- `0908.2098v1` - [abs](http://arxiv.org/abs/0908.2098v1) - [pdf](http://arxiv.org/pdf/0908.2098v1)

> We assume a drift condition towards a small set and bound the mean square error of estimators obtained by taking averages along a single trajectory of a Markov chain Monte Carlo algorithm. We use these bounds to construct fixed-width nonasymptotic confidence intervals. For a possibly unbounded function $f:\stany \to R,$ let $I=\int_{\stany} f(x) \pi(x) dx$ be the value of interest and $\hat{I}_{t,n}=(1/n)\sum_{i=t}^{t+n-1}f(X_i)$ its MCMC estimate. Precisely, we derive lower bounds for the length of the trajectory $n$ and burn-in time $t$ which ensure that $$P(|\hat{I}_{t,n}-I|\leq \varepsilon)\geq 1-\alpha.$$ The bounds depend only and explicitly on drift parameters, on the $V-$norm of $f,$ where $V$ is the drift function and on precision and confidence parameters $\varepsilon, \alpha.$ Next we analyse an MCMC estimator based on the median of multiple shorter runs that allows for sharper bounds for the required total simulation cost. In particular the methodology can be applied for computing Bayesian estimators in practically relevant models. We illustrate our bounds numerically in a simple example.

</details>

<details>

<summary>2009-08-17 09:19:18 - Bayesian variable selection using cost-adjusted BIC, with application to cost-effective measurement of quality of health care</summary>

- *D. Fouskakis, I. Ntzoufras, D. Draper*

- `0908.2313v1` - [abs](http://arxiv.org/abs/0908.2313v1) - [pdf](http://arxiv.org/pdf/0908.2313v1)

> In the field of quality of health care measurement, one approach to assessing patient sickness at admission involves a logistic regression of mortality within 30 days of admission on a fairly large number of sickness indicators (on the order of 100) to construct a sickness scale, employing classical variable selection methods to find an ``optimal'' subset of 10--20 indicators. Such ``benefit-only'' methods ignore the considerable differences among the sickness indicators in cost of data collection, an issue that is crucial when admission sickness is used to drive programs (now implemented or under consideration in several countries, including the U.S. and U.K.) that attempt to identify substandard hospitals by comparing observed and expected mortality rates (given admission sickness). When both data-collection cost and accuracy of prediction of 30-day mortality are considered, a large variable-selection problem arises in which costly variables that do not predict well enough should be omitted from the final scale. In this paper (a) we develop a method for solving this problem based on posterior model odds, arising from a prior distribution that (1) accounts for the cost of each variable and (2) results in a set of posterior model probabilities that corresponds to a generalized cost-adjusted version of the Bayesian information criterion (BIC), and (b) we compare this method with a decision-theoretic cost-benefit approach based on maximizing expected utility. We use reversible-jump Markov chain Monte Carlo (RJMCMC) methods to search the model space, and we check the stability of our findings with two variants of the MCMC model composition ($\mathit{MC}^3$) algorithm.

</details>

<details>

<summary>2009-08-18 19:20:34 - A bayesian approach to the estimation of maps between riemannian manifolds, II: examples</summary>

- *Leo T. Butler, Boris Levit*

- `0908.2612v1` - [abs](http://arxiv.org/abs/0908.2612v1) - [pdf](http://arxiv.org/pdf/0908.2612v1)

> Let M be a smooth compact oriented manifold without boundary, imbedded in a euclidean space E and let f be a smooth map of M into a Riemannian manifold N. An unknown state x in M is observed via X=x+su where s>0 is a small parameter and u is a white Gaussian noise. For a given smooth prior on M and smooth estimators g of the map f we have derived a second-order asymptotic expansion for the related Bayesian risk (see arXiv:0705.2540). In this paper, we apply this technique to a variety of examples.   The second part examines the first-order conditions for equality-constrained regression problems. The geometric tools that are utilised in our earlier paper are naturally applicable to these regression problems.

</details>

<details>

<summary>2009-08-20 06:26:47 - Statistical methods for automated drug susceptibility testing: Bayesian minimum inhibitory concentration prediction from growth curves</summary>

- *Xi Kathy Zhou, Merlise A. Clyde, James Garrett, Viridiana Lourdes, Michael O'Connell, Giovanni Parmigiani, David J. Turner, Tim Wiles*

- `0908.2858v1` - [abs](http://arxiv.org/abs/0908.2858v1) - [pdf](http://arxiv.org/pdf/0908.2858v1)

> Determination of the minimum inhibitory concentration (MIC) of a drug that prevents microbial growth is an important step for managing patients with infections. In this paper we present a novel probabilistic approach that accurately estimates MICs based on a panel of multiple curves reflecting features of bacterial growth. We develop a probabilistic model for determining whether a given dilution of an antimicrobial agent is the MIC given features of the growth curves over time. Because of the potentially large collection of features, we utilize Bayesian model selection to narrow the collection of predictors to the most important variables. In addition to point estimates of MICs, we are able to provide posterior probabilities that each dilution is the MIC based on the observed growth curves. The methods are easily automated and have been incorporated into the Becton--Dickinson PHOENIX automated susceptibility system that rapidly and accurately classifies the resistance of a large number of microorganisms in clinical samples. Over seventy-five studies to date have shown this new method provides improved estimation of MICs over existing approaches.

</details>

<details>

<summary>2009-08-20 06:58:36 - Sensitivity of inferences in forensic genetics to assumptions about founding genes</summary>

- *Peter J. Green, Julia Mortera*

- `0908.2862v1` - [abs](http://arxiv.org/abs/0908.2862v1) - [pdf](http://arxiv.org/pdf/0908.2862v1)

> Many forensic genetics problems can be handled using structured systems of discrete variables, for which Bayesian networks offer an appealing practical modeling framework, and allow inferences to be computed by probability propagation methods. However, when standard assumptions are violated--for example, when allele frequencies are unknown, there is identity by descent or the population is heterogeneous--dependence is generated among founding genes, that makes exact calculation of conditional probabilities by propagation methods less straightforward. Here we illustrate different methodologies for assessing sensitivity to assumptions about founders in forensic genetics problems. These include constrained steepest descent, linear fractional programming and representing dependence by structure. We illustrate these methods on several forensic genetics examples involving criminal identification, simple and complex disputed paternity and DNA mixtures.

</details>

<details>

<summary>2009-08-20 08:17:58 - A hierarchical Dirichlet process mixture model for haplotype reconstruction from multi-population data</summary>

- *Kyung-Ah Sohn, Eric P. Xing*

- `0812.4648v2` - [abs](http://arxiv.org/abs/0812.4648v2) - [pdf](http://arxiv.org/pdf/0812.4648v2)

> The perennial problem of "how many clusters?" remains an issue of substantial interest in data mining and machine learning communities, and becomes particularly salient in large data sets such as populational genomic data where the number of clusters needs to be relatively large and open-ended. This problem gets further complicated in a co-clustering scenario in which one needs to solve multiple clustering problems simultaneously because of the presence of common centroids (e.g., ancestors) shared by clusters (e.g., possible descents from a certain ancestor) from different multiple-cluster samples (e.g., different human subpopulations). In this paper we present a hierarchical nonparametric Bayesian model to address this problem in the context of multi-population haplotype inference. Uncovering the haplotypes of single nucleotide polymorphisms is essential for many biological and medical applications. While it is uncommon for the genotype data to be pooled from multiple ethnically distinct populations, few existing programs have explicitly leveraged the individual ethnic information for haplotype inference. In this paper we present a new haplotype inference program, Haploi, which makes use of such information and is readily applicable to genotype sequences with thousands of SNPs from heterogeneous populations, with competent and sometimes superior speed and accuracy comparing to the state-of-the-art programs. Underlying Haploi is a new haplotype distribution model based on a nonparametric Bayesian formalism known as the hierarchical Dirichlet process, which represents a tractable surrogate to the coalescent process. The proposed model is exchangeable, unbounded, and capable of coupling demographic information of different populations.

</details>

<details>

<summary>2009-08-20 17:30:26 - Regression in random design and Bayesian warped wavelets estimators</summary>

- *Thanh Mai Pham Ngoc*

- `0908.2965v1` - [abs](http://arxiv.org/abs/0908.2965v1) - [pdf](http://arxiv.org/pdf/0908.2965v1)

> In this paper we deal with the regression problem in a random design setting. We investigate asymptotic optimality under minimax point of view of various Bayesian rules based on warped wavelets and show that they nearly attain optimal minimax rates of convergence over the Besov smoothness class considered. Warped wavelets have been introduced recently, they offer very good computable and easy-to-implement properties while being well adapted to the statistical problem at hand. We particularly put emphasis on Bayesian rules leaning on small and large variance Gaussian priors and discuss their simulation performances comparing them with a hard thresholding procedure.

</details>

<details>

<summary>2009-08-21 05:25:19 - Estimating limits from Poisson counting data using Dempster--Shafer analysis</summary>

- *Paul T. Edlefsen, Chuanhai Liu, Arthur P. Dempster*

- `0812.1690v2` - [abs](http://arxiv.org/abs/0812.1690v2) - [pdf](http://arxiv.org/pdf/0812.1690v2)

> We present a Dempster--Shafer (DS) approach to estimating limits from Poisson counting data with nuisance parameters. Dempster--Shafer is a statistical framework that generalizes Bayesian statistics. DS calculus augments traditional probability by allowing mass to be distributed over power sets of the event space. This eliminates the Bayesian dependence on prior distributions while allowing the incorporation of prior information when it is available. We use the Poisson Dempster--Shafer model (DSM) to derive a posterior DSM for the ``Banff upper limits challenge'' three-Poisson model. The results compare favorably with other approaches, demonstrating the utility of the approach. We argue that the reduced dependence on priors afforded by the Dempster--Shafer framework is both practically and theoretically desirable.

</details>

<details>

<summary>2009-08-22 16:18:15 - Bayesian interpretation of periodograms</summary>

- *J. -F. Giovannelli, J. Idier*

- `0908.3262v1` - [abs](http://arxiv.org/abs/0908.3262v1) - [pdf](http://arxiv.org/pdf/0908.3262v1)

> The usual nonparametric approach to spectral analysis is revisited within the regularization framework. Both usual and windowed periodograms are obtained as the squared modulus of the minimizer of regularized least squares criteria. Then, particular attention is paid to their interpretation within the Bayesian statistical framework. Finally, the question of unsupervised hyperparameter and window selection is addressed. It is shown that maximum likelihood solution is both formally achievable and practically useful.

</details>

<details>

<summary>2009-08-24 09:41:46 - Asymptotic equivalence of empirical likelihood and Bayesian MAP</summary>

- *Marian Grendár, George Judge*

- `0908.3397v1` - [abs](http://arxiv.org/abs/0908.3397v1) - [pdf](http://arxiv.org/pdf/0908.3397v1)

> In this paper we are interested in empirical likelihood (EL) as a method of estimation, and we address the following two problems: (1) selecting among various empirical discrepancies in an EL framework and (2) demonstrating that EL has a well-defined probabilistic interpretation that would justify its use in a Bayesian context. Using the large deviations approach, a Bayesian law of large numbers is developed that implies that EL and the Bayesian maximum a posteriori probability (MAP) estimators are consistent under misspecification and that EL can be viewed as an asymptotic form of MAP. Estimators based on other empirical discrepancies are, in general, inconsistent under misspecification.

</details>

<details>

<summary>2009-08-24 11:37:49 - Bayesian frequentist hybrid inference</summary>

- *Ao Yuan*

- `0908.3413v1` - [abs](http://arxiv.org/abs/0908.3413v1) - [pdf](http://arxiv.org/pdf/0908.3413v1)

> Bayesian and frequentist methods differ in many aspects, but share some basic optimality properties. In practice, there are situations in which one of the methods is more preferred by some criteria. We consider the case of inference about a set of multiple parameters, which can be divided into two disjoint subsets. On one set, a frequentist method may be favored and on the other, the Bayesian. This motivates a joint estimation procedure in which some of the parameters are estimated Bayesian, and the rest by the maximum-likelihood estimator in the same parametric model, and thus keep the strengths of both the methods and avoid their weaknesses. Such a hybrid procedure gives us more flexibility in achieving overall inference advantages. We study the consistency and high-order asymptotic behavior of the proposed estimator, and illustrate its application. Also, the results imply a new method for constructing objective prior.

</details>

<details>

<summary>2009-08-25 07:48:05 - Improving SAMC using smoothing methods: Theory and applications to Bayesian model selection problems</summary>

- *Faming Liang*

- `0908.3553v1` - [abs](http://arxiv.org/abs/0908.3553v1) - [pdf](http://arxiv.org/pdf/0908.3553v1)

> Stochastic approximation Monte Carlo (SAMC) has recently been proposed by Liang, Liu and Carroll [J. Amer. Statist. Assoc. 102 (2007) 305--320] as a general simulation and optimization algorithm. In this paper, we propose to improve its convergence using smoothing methods and discuss the application of the new algorithm to Bayesian model selection problems. The new algorithm is tested through a change-point identification example. The numerical results indicate that the new algorithm can outperform SAMC and reversible jump MCMC significantly for the model selection problems. The new algorithm represents a general form of the stochastic approximation Markov chain Monte Carlo algorithm. It allows multiple samples to be generated at each iteration, and a bias term to be included in the parameter updating step. A rigorous proof for the convergence of the general algorithm is established under verifiable conditions. This paper also provides a framework on how to improve efficiency of Monte Carlo simulations by incorporating some nonparametric techniques.

</details>

<details>

<summary>2009-08-25 08:05:59 - Adaptive Bayesian estimation using a Gaussian random field with inverse Gamma bandwidth</summary>

- *A. W. van der Vaart, J. H. van Zanten*

- `0908.3556v1` - [abs](http://arxiv.org/abs/0908.3556v1) - [pdf](http://arxiv.org/pdf/0908.3556v1)

> We consider nonparametric Bayesian estimation inference using a rescaled smooth Gaussian field as a prior for a multidimensional function. The rescaling is achieved using a Gamma variable and the procedure can be viewed as choosing an inverse Gamma bandwidth. The procedure is studied from a frequentist perspective in three statistical settings involving replicated observations (density estimation, regression and classification). We prove that the resulting posterior distribution shrinks to the distribution that generates the data at a speed which is minimax-optimal up to a logarithmic factor, whatever the regularity level of the data-generating distribution. Thus the hierachical Bayesian procedure, with a fixed prior, is shown to be fully adaptive.

</details>

<details>

<summary>2009-08-25 09:22:03 - Local linear quantile estimation for nonstationary time series</summary>

- *Zhou Zhou, Wei Biao Wu*

- `0908.3576v1` - [abs](http://arxiv.org/abs/0908.3576v1) - [pdf](http://arxiv.org/pdf/0908.3576v1)

> We consider estimation of quantile curves for a general class of nonstationary processes. Consistency and central limit results are obtained for local linear quantile estimates under a mild short-range dependence condition. Our results are applied to environmental data sets. In particular, our results can be used to address the problem of whether climate variability has changed, an important problem raised by IPCC (Intergovernmental Panel on Climate Change) in 2001.

</details>

<details>

<summary>2009-08-27 16:32:49 - The nested Chinese restaurant process and Bayesian nonparametric inference of topic hierarchies</summary>

- *David M. Blei, Thomas L. Griffiths, Michael I. Jordan*

- `0710.0845v3` - [abs](http://arxiv.org/abs/0710.0845v3) - [pdf](http://arxiv.org/pdf/0710.0845v3)

> We present the nested Chinese restaurant process (nCRP), a stochastic process which assigns probability distributions to infinitely-deep, infinitely-branching trees. We show how this stochastic process can be used as a prior distribution in a Bayesian nonparametric model of document collections. Specifically, we present an application to information retrieval in which documents are modeled as paths down a random tree, and the preferential attachment dynamics of the nCRP leads to clustering of documents according to sharing of topics at multiple levels of abstraction. Given a corpus of documents, a posterior inference algorithm finds an approximation to a posterior distribution over trees, topics and allocations of words to levels of the tree. We demonstrate this algorithm on collections of scientific abstracts from several journals. This model exemplifies a recent trend in statistical machine learning--the use of Bayesian nonparametric methods to infer distributions on flexible data structures.

</details>

<details>

<summary>2009-08-28 20:56:06 - Entropic Priors and Bayesian Model Selection</summary>

- *Brendon J. Brewer, Matthew J. Francis*

- `0906.5609v2` - [abs](http://arxiv.org/abs/0906.5609v2) - [pdf](http://arxiv.org/pdf/0906.5609v2)

> We demonstrate that the principle of maximum relative entropy (ME), used judiciously, can ease the specification of priors in model selection problems. The resulting effect is that models that make sharp predictions are disfavoured, weakening the usual Bayesian "Occam's Razor". This is illustrated with a simple example involving what Jaynes called a "sure thing" hypothesis. Jaynes' resolution of the situation involved introducing a large number of alternative "sure thing" hypotheses that were possible before we observed the data. However, in more complex situations, it may not be possible to explicitly enumerate large numbers of alternatives. The entropic priors formalism produces the desired result without modifying the hypothesis space or requiring explicit enumeration of alternatives; all that is required is a good model for the prior predictive distribution for the data. This idea is illustrated with a simple rigged-lottery example, and we outline how this idea may help to resolve a recent debate amongst cosmologists: is dark energy a cosmological constant, or has it evolved with time in some way? And how shall we decide, when the data are in?

</details>

<details>

<summary>2009-08-29 12:08:00 - One and two side generalisations of the log-Normal distribution by means of a new product definition</summary>

- *Silvio M. Duarte Queiros*

- `0908.4334v1` - [abs](http://arxiv.org/abs/0908.4334v1) - [pdf](http://arxiv.org/pdf/0908.4334v1)

> In this manuscript we introduce a generalisation of the log-Normal distribution that is inspired by a modification of the Kaypten multiplicative process using the $q$-product of Borges [Physica A \textbf{340}, 95 (2004)]. Depending on the value of q the distribution increases the tail for small (when $q<1$) or large (when $q>1$) values of the variable upon analysis. The usual log-Normal distribution is retrieved when $q=1$. The main statistical features of this distribution are presented as well as a related random number generators and tables of quantiles of the Kolmogorov-Smirnov. Lastly, we illustrate the application of this distribution studying the adjustment of a set of variables of biological and financial origin.

</details>


## 2009-09

<details>

<summary>2009-09-02 09:02:37 - Grapham: Graphical Models with Adaptive Random Walk Metropolis Algorithms</summary>

- *Matti Vihola*

- `0811.4095v3` - [abs](http://arxiv.org/abs/0811.4095v3) - [pdf](http://arxiv.org/pdf/0811.4095v3)

> Recently developed adaptive Markov chain Monte Carlo (MCMC) methods have been applied successfully to many problems in Bayesian statistics. Grapham is a new open source implementation covering several such methods, with emphasis on graphical models for directed acyclic graphs. The implemented algorithms include the seminal Adaptive Metropolis algorithm adjusting the proposal covariance according to the history of the chain and a Metropolis algorithm adjusting the proposal scale based on the observed acceptance probability. Different variants of the algorithms allow one, for example, to use these two algorithms together, employ delayed rejection and adjust several parameters of the algorithms. The implemented Metropolis-within-Gibbs update allows arbitrary sampling blocks. The software is written in C and uses a simple extension language Lua in configuration.

</details>

<details>

<summary>2009-09-02 11:43:55 - A conjugate prior for discrete hierarchical log-linear models</summary>

- *Hélène Massam, Jinnan Liu, Adrian Dobra*

- `0711.1609v3` - [abs](http://arxiv.org/abs/0711.1609v3) - [pdf](http://arxiv.org/pdf/0711.1609v3)

> In Bayesian analysis of multi-way contingency tables, the selection of a prior distribution for either the log-linear parameters or the cell probabilities parameters is a major challenge. In this paper, we define a flexible family of conjugate priors for the wide class of discrete hierarchical log-linear models, which includes the class of graphical models. These priors are defined as the Diaconis--Ylvisaker conjugate priors on the log-linear parameters subject to "baseline constraints" under multinomial sampling. We also derive the induced prior on the cell probabilities and show that the induced prior is a generalization of the hyper Dirichlet prior. We show that this prior has several desirable properties and illustrate its usefulness by identifying the most probable decomposable, graphical and hierarchical log-linear models for a six-way contingency table.

</details>

<details>

<summary>2009-09-04 18:25:22 - Tuning parameter selection for penalized likelihood estimation of inverse covariance matrix</summary>

- *Xin Gao, Daniel Q. Pu, Yuehua Wu, Hong Xu*

- `0909.0934v1` - [abs](http://arxiv.org/abs/0909.0934v1) - [pdf](http://arxiv.org/pdf/0909.0934v1)

> In a Gaussian graphical model, the conditional independence between two variables are characterized by the corresponding zero entries in the inverse covariance matrix. Maximum likelihood method using the smoothly clipped absolute deviation (SCAD) penalty (Fan and Li, 2001) and the adaptive LASSO penalty (Zou, 2006) have been proposed in literature. In this article, we establish the result that using Bayesian information criterion (BIC) to select the tuning parameter in penalized likelihood estimation with both types of penalties can lead to consistent graphical model selection. We compare the empirical performance of BIC with cross validation method and demonstrate the advantageous performance of BIC criterion for tuning parameter selection through simulation studies.

</details>

<details>

<summary>2009-09-07 00:47:12 - Shrinkage Tuning Parameter Selection in Precision Matrices Estimation</summary>

- *Heng Lian*

- `0909.1123v1` - [abs](http://arxiv.org/abs/0909.1123v1) - [pdf](http://arxiv.org/pdf/0909.1123v1)

> Recent literature provides many computational and modeling approaches for covariance matrices estimation in a penalized Gaussian graphical models but relatively little study has been carried out on the choice of the tuning parameter. This paper tries to fill this gap by focusing on the problem of shrinkage parameter selection when estimating sparse precision matrices using the penalized likelihood approach. Previous approaches typically used K-fold cross-validation in this regard. In this paper, we first derived the generalized approximate cross-validation for tuning parameter selection which is not only a more computationally efficient alternative, but also achieves smaller error rate for model fitting compared to leave-one-out cross-validation. For consistency in the selection of nonzero entries in the precision matrix, we employ a Bayesian information criterion which provably can identify the nonzero conditional correlations in the Gaussian model. Our simulations demonstrate the general superiority of the two proposed selectors in comparison with leave-one-out cross-validation, ten-fold cross-validation and Akaike information criterion.

</details>

<details>

<summary>2009-09-14 20:18:32 - Hierarchical models in statistical inverse problems and the Mumford--Shah functional</summary>

- *Tapio Helin, Matti Lassas*

- `0908.3396v2` - [abs](http://arxiv.org/abs/0908.3396v2) - [pdf](http://arxiv.org/pdf/0908.3396v2)

> The Bayesian methods for linear inverse problems is studied using hierarchical Gaussian models. The problems are considered with different discretizations, and we analyze the phenomena which appear when the discretization becomes finer. A hierarchical solution method for signal restoration problems is introduced and studied with arbitrarily fine discretization. We show that the maximum a posteriori estimate converges to a minimizer of the Mumford--Shah functional, up to a subsequence. A new result regarding the existence of a minimizer of the Mumford--Shah functional is proved.   Moreover, we study the inverse problem under different assumptions on the asymptotic behavior of the noise as discretization becomes finer. We show that the maximum a posteriori and conditional mean estimates converge under different conditions.

</details>

<details>

<summary>2009-09-23 07:58:17 - Bayesian separation of spectral sources under non-negativity and full additivity constraints</summary>

- *Nicolas Dobigeon, Said Moussaoui, Jean-Yves Tourneret, Cedric Carteret*

- `0906.4754v4` - [abs](http://arxiv.org/abs/0906.4754v4) - [pdf](http://arxiv.org/pdf/0906.4754v4)

> This paper addresses the problem of separating spectral sources which are linearly mixed with unknown proportions. The main difficulty of the problem is to ensure the full additivity (sum-to-one) of the mixing coefficients and non-negativity of sources and mixing coefficients. A Bayesian estimation approach based on Gamma priors was recently proposed to handle the non-negativity constraints in a linear mixture model. However, incorporating the full additivity constraint requires further developments. This paper studies a new hierarchical Bayesian model appropriate to the non-negativity and sum-to-one constraints associated to the regressors and regression coefficients of linear mixtures. The estimation of the unknown parameters of this model is performed using samples generated using an appropriate Gibbs sampler. The performance of the proposed algorithm is evaluated through simulation results conducted on synthetic mixture models. The proposed approach is also applied to the processing of multicomponent chemical mixtures resulting from Raman spectroscopy.

</details>

<details>

<summary>2009-09-23 08:23:42 - Hierarchical Bayesian sparse image reconstruction with application to MRFM</summary>

- *Nicolas Dobigeon, Alfred O. Hero, Jean-Yves Tourneret*

- `0809.3650v2` - [abs](http://arxiv.org/abs/0809.3650v2) - [pdf](http://arxiv.org/pdf/0809.3650v2)

> This paper presents a hierarchical Bayesian model to reconstruct sparse images when the observations are obtained from linear transformations and corrupted by an additive white Gaussian noise. Our hierarchical Bayes model is well suited to such naturally sparse image applications as it seamlessly accounts for properties such as sparsity and positivity of the image via appropriate Bayes priors. We propose a prior that is based on a weighted mixture of a positive exponential distribution and a mass at zero. The prior has hyperparameters that are tuned automatically by marginalization over the hierarchical Bayesian model. To overcome the complexity of the posterior distribution, a Gibbs sampling strategy is proposed. The Gibbs samples can be used to estimate the image to be recovered, e.g. by maximizing the estimated posterior distribution. In our fully Bayesian approach the posteriors of all the parameters are available. Thus our algorithm provides more information than other previously proposed sparse reconstruction methods that only give a point estimate. The performance of our hierarchical Bayesian sparse reconstruction method is illustrated on synthetic and real data collected from a tobacco virus sample using a prototype MRFM instrument.

</details>

<details>

<summary>2009-09-24 22:43:16 - Soccer matches as experiments: how often does the 'best' team win?</summary>

- *G. K. Skinner, G. H. Freeman*

- `0909.4555v1` - [abs](http://arxiv.org/abs/0909.4555v1) - [pdf](http://arxiv.org/pdf/0909.4555v1)

> Models in which the number of goals scored by a team in a soccer match follow a Poisson distribution, or a closely related one, have been widely discussed. We here consider a soccer match as an experiment to assess which of two teams is superior and examine the probability that the outcome of the experiment (match) truly represents the relative abilities of the two teams. Given a final score, it is possible by using a Bayesian approach to quantify the probability that it was or was not the case that 'the best team won'. For typical scores, the probability of a misleading result is significant. Modifying the rules of the game to increase the typical number of goals scored would improve the situation, but a level of confidence that would normally be regarded as satisfactory could not be obtained unless the character of the game was radically changed.

</details>

<details>

<summary>2009-09-29 07:57:21 - Information field theory for cosmological perturbation reconstruction and non-linear signal analysis</summary>

- *Torsten A. Ensslin, Mona Frommert, Francisco S. Kitaura*

- `0806.3474v3` - [abs](http://arxiv.org/abs/0806.3474v3) - [pdf](http://arxiv.org/pdf/0806.3474v3)

> We develop information field theory (IFT) as a means of Bayesian inference on spatially distributed signals, the information fields. A didactical approach is attempted. Starting from general considerations on the nature of measurements, signals, noise, and their relation to a physical reality, we derive the information Hamiltonian, the source field, propagator, and interaction terms. Free IFT reproduces the well known Wiener-filter theory. Interacting IFT can be diagrammatically expanded, for which we provide the Feynman rules in position-, Fourier-, and spherical harmonics space, and the Boltzmann-Shannon information measure. The theory should be applicable in many fields. However, here, two cosmological signal recovery problems are discussed in their IFT-formulation. 1) Reconstruction of the cosmic large-scale structure matter distribution from discrete galaxy counts in incomplete galaxy surveys within a simple model of galaxy formation. We show that a Gaussian signal, which should resemble the initial density perturbations of the Universe, observed with a strongly non-linear, incomplete and Poissonian-noise affected response, as the processes of structure and galaxy formation and observations provide, can be reconstructed thanks to the virtue of a response-renormalization flow equation. 2) We design a filter to detect local non-linearities in the cosmic microwave background, which are predicted from some Early-Universe inflationary scenarios, and expected due to measurement imperfections. This filter is the optimal Bayes' estimator up to linear order in the non-linearity parameter and can be used even to construct sky maps of non-linearities in the data.

</details>

<details>

<summary>2009-09-29 16:16:19 - High confidence estimates of the mean of heavy-tailed real random variables</summary>

- *Olivier Catoni*

- `0909.5366v1` - [abs](http://arxiv.org/abs/0909.5366v1) - [pdf](http://arxiv.org/pdf/0909.5366v1)

> We present new estimators of the mean of a real valued random variable, based on PAC-Bayesian iterative truncation. We analyze the non-asymptotic minimax properties of the deviations of estimators for distributions having either a bounded variance or a bounded kurtosis. It turns out that these minimax deviations are of the same order as the deviations of the empirical mean estimator of a Gaussian distribution. Nevertheless, the empirical mean itself performs poorly at high confidence levels for the worst distribution with a given variance or kurtosis (which turns out to be heavy tailed). To obtain (nearly) minimax deviations in these broad class of distributions, it is necessary to use some more robust estimator, and we describe an iterated truncation scheme whose deviations are close to minimax. In order to calibrate the truncation and obtain explicit confidence intervals, it is necessary to dispose of a prior bound either on the variance or the kurtosis. When a prior bound on the kurtosis is available, we obtain as a by-product a new variance estimator with good large deviation properties. When no prior bound is available, it is still possible to use Lepski's approach to adapt to the unknown variance, although it is no more possible to obtain observable confidence intervals.

</details>

<details>

<summary>2009-09-29 20:26:50 - Bayesian changepoint analysis for atomic force microscopy and soft material indentation</summary>

- *Daniel Rudoy, Shelten G. Yuen, Robert D. Howe, Patrick J. Wolfe*

- `0909.5438v1` - [abs](http://arxiv.org/abs/0909.5438v1) - [pdf](http://arxiv.org/pdf/0909.5438v1)

> Material indentation studies, in which a probe is brought into controlled physical contact with an experimental sample, have long been a primary means by which scientists characterize the mechanical properties of materials. More recently, the advent of atomic force microscopy, which operates on the same fundamental principle, has in turn revolutionized the nanoscale analysis of soft biomaterials such as cells and tissues. This paper addresses the inferential problems associated with material indentation and atomic force microscopy, through a framework for the changepoint analysis of pre- and post-contact data that is applicable to experiments across a variety of physical scales. A hierarchical Bayesian model is proposed to account for experimentally observed changepoint smoothness constraints and measurement error variability, with efficient Monte Carlo methods developed and employed to realize inference via posterior sampling for parameters such as Young's modulus, a key quantifier of material stiffness. These results are the first to provide the materials science community with rigorous inference procedures and uncertainty quantification, via optimized and fully automated high-throughput algorithms, implemented as the publicly available software package BayesCP. To demonstrate the consistent accuracy and wide applicability of this approach, results are shown for a variety of data sets from both macro- and micro-materials experiments--including silicone, neurons, and red blood cells--conducted by the authors and others.

</details>


## 2009-10

<details>

<summary>2009-10-01 03:50:56 - Model choice versus model criticism</summary>

- *Christian P. Robert, Kerrie L. Mengersen, Carla Chen*

- `0909.5673v2` - [abs](http://arxiv.org/abs/0909.5673v2) - [pdf](http://arxiv.org/pdf/0909.5673v2)

> The new perspectives on ABC and Bayesian model criticisms presented in Ratmann et al.(2009) are challenging standard approaches to Bayesian model choice. We discuss here some issues arising from the authors' approach, including prior influence, model assessment and criticism, and the meaning of error in ABC.

</details>

<details>

<summary>2009-10-08 14:04:55 - Bayesian testing of many hypotheses $\times$ many genes: A study of sleep apnea</summary>

- *Shane T. Jensen, Ibrahim Erkan, Erna S. Arnardottir, Dylan S. Small*

- `0903.0201v2` - [abs](http://arxiv.org/abs/0903.0201v2) - [pdf](http://arxiv.org/pdf/0903.0201v2)

> Substantial statistical research has recently been devoted to the analysis of large-scale microarray experiments which provide a measure of the simultaneous expression of thousands of genes in a particular condition. A typical goal is the comparison of gene expression between two conditions (e.g., diseased vs. nondiseased) to detect genes which show differential expression. Classical hypothesis testing procedures have been applied to this problem and more recent work has employed sophisticated models that allow for the sharing of information across genes. However, many recent gene expression studies have an experimental design with several conditions that requires an even more involved hypothesis testing approach. In this paper, we use a hierarchical Bayesian model to address the situation where there are many hypotheses that must be simultaneously tested for each gene. In addition to having many hypotheses within each gene, our analysis also addresses the more typical multiple comparison issue of testing many genes simultaneously. We illustrate our approach with an application to a study of genes involved in obstructive sleep apnea in humans.

</details>

<details>

<summary>2009-10-09 07:17:26 - Maximum likelihood estimates under $\mathbf{k}$-allele models with selection can be numerically unstable</summary>

- *Erkan Ozge Buzbas, Paul Joyce*

- `0910.1664v1` - [abs](http://arxiv.org/abs/0910.1664v1) - [pdf](http://arxiv.org/pdf/0910.1664v1)

> The stationary distribution of allele frequencies under a variety of Wright--Fisher $k$-allele models with selection and parent independent mutation is well studied. However, the statistical properties of maximum likelihood estimates of parameters under these models are not well understood. Under each of these models there is a point in data space which carries the strongest possible signal for selection, yet, at this point, the likelihood is unbounded. This result remains valid even if all of the mutation parameters are assumed to be known. Therefore, standard simulation approaches used to approximate the sampling distribution of the maximum likelihood estimate produce numerically unstable results in the presence of substantial selection. We describe the Bayesian alternative where the posterior distribution tends to produce more accurate and reliable interval estimates for the selection intensity at a locus.

</details>

<details>

<summary>2009-10-13 14:48:38 - Importance sampling methods for Bayesian discrimination between embedded models</summary>

- *Jean-Michel Marin, Christian P. Robert*

- `0910.2325v1` - [abs](http://arxiv.org/abs/0910.2325v1) - [pdf](http://arxiv.org/pdf/0910.2325v1)

> This paper surveys some well-established approaches on the approximation of Bayes factors used in Bayesian model choice, mostly as covered in Chen et al. (2000). Our focus here is on methods that are based on importance sampling strategies rather than variable dimension techniques like reversible jump MCMC, including: crude Monte Carlo, maximum likelihood based importance sampling, bridge and harmonic mean sampling, as well as Chib's method based on the exploitation of a functional equality. We demonstrate in this survey how these different methods can be efficiently implemented for testing the significance of a predictive variable in a probit model. Finally, we compare their performances on a real dataset.

</details>

<details>

<summary>2009-10-16 12:22:40 - Efficient Bayesian analysis of multiple changepoint models with dependence across segments</summary>

- *Paul Fearnhead, Zhen Liu*

- `0910.3099v1` - [abs](http://arxiv.org/abs/0910.3099v1) - [pdf](http://arxiv.org/pdf/0910.3099v1)

> We consider Bayesian analysis of a class of multiple changepoint models. While there are a variety of efficient ways to analyse these models if the parameters associated with each segment are independent, there are few general approaches for models where the parameters are dependent. Under the assumption that the dependence is Markov, we propose an efficient online algorithm for sampling from an approximation to the posterior distribution of the number and position of the changepoints. In a simulation study, we show that the approximation introduced is negligible. We illustrate the power of our approach through fitting piecewise polynomial models to data, under a model which allows for either continuity or discontinuity of the underlying curve at each changepoint. This method is competitive with, or out-performs, other methods for inferring curves from noisy data; and uniquely it allows for inference of the locations of discontinuities in the underlying curve.

</details>

<details>

<summary>2009-10-26 18:52:32 - Bayesian Core: The Complete Solution Manual</summary>

- *Christian P. Robert, Jean-Michel Marin*

- `0910.4696v1` - [abs](http://arxiv.org/abs/0910.4696v1) - [pdf](http://arxiv.org/pdf/0910.4696v1)

> This solution manual contains the unabridged and original solutions to all the exercises proposed in Bayesian Core, along with R programs when necessary.

</details>

<details>

<summary>2009-10-28 19:44:52 - Coherent frequentism</summary>

- *David R. Bickel*

- `0907.0139v3` - [abs](http://arxiv.org/abs/0907.0139v3) - [pdf](http://arxiv.org/pdf/0907.0139v3)

> By representing the range of fair betting odds according to a pair of confidence set estimators, dual probability measures on parameter space called frequentist posteriors secure the coherence of subjective inference without any prior distribution. The closure of the set of expected losses corresponding to the dual frequentist posteriors constrains decisions without arbitrarily forcing optimization under all circumstances. This decision theory reduces to those that maximize expected utility when the pair of frequentist posteriors is induced by an exact or approximate confidence set estimator or when an automatic reduction rule is applied to the pair. In such cases, the resulting frequentist posterior is coherent in the sense that, as a probability distribution of the parameter of interest, it satisfies the axioms of the decision-theoretic and logic-theoretic systems typically cited in support of the Bayesian posterior. Unlike the p-value, the confidence level of an interval hypothesis derived from such a measure is suitable as an estimator of the indicator of hypothesis truth since it converges in sample-space probability to 1 if the hypothesis is true or to 0 otherwise under general conditions.

</details>


## 2009-11

<details>

<summary>2009-11-02 13:43:29 - Particle filtering within adaptive Metropolis Hastings sampling</summary>

- *Ralph Silva, Paolo Giordani, Robert Kohn, Mike Pitt*

- `0911.0230v1` - [abs](http://arxiv.org/abs/0911.0230v1) - [pdf](http://arxiv.org/pdf/0911.0230v1)

> We show that it is feasible to carry out exact Bayesian inference for non-Gaussian state space models using an adaptive Metropolis Hastings sampling scheme with the likelihood approximated by the particle filter. Furthermore, an adapyive independent Metropolis Hastings sampler based on a mixture of normals proposal is computationally much more efficient than an adaptive random walk proposal because the cost of constructing a good adaptive proposal is negligible compared to the cost of approximating the likelihood. Independent Metropolis Hastings proposals are also attractive because they are easy to run in parallel on multiple processors. We also show that when the particle filter is used, the marginal likelihood of any model is obtained in an efficient and unbiased manner, making model comparison straightforward.

</details>

<details>

<summary>2009-11-11 08:45:17 - On sequential Monte Carlo, partial rejection control and approximate Bayesian computation</summary>

- *G. W. Peters, Y. Fan, S. A. Sisson*

- `0808.3466v2` - [abs](http://arxiv.org/abs/0808.3466v2) - [pdf](http://arxiv.org/pdf/0808.3466v2)

> We present a sequential Monte Carlo sampler variant of the partial rejection control algorithm, and show that this variant can be considered as a sequential Monte Carlo sampler with a modified mutation kernel. We prove that the new sampler can reduce the variance of the incremental importance weights when compared with standard sequential Monte Carlo samplers. We provide a study of theoretical properties of the new algorithm, and make connections with some existing algorithms. Finally, the sampler is adapted for application under the challenging "likelihood free," approximate Bayesian computation modelling framework, where we demonstrate superior performance over existing likelihood-free samplers.

</details>

<details>

<summary>2009-11-11 16:25:32 - On Bayesian Curve Fitting Via Auxiliary Variables</summary>

- *Y. Fan, J. -L Dortet-Bernadet, S. A. Sisson*

- `0911.1894v2` - [abs](http://arxiv.org/abs/0911.1894v2) - [pdf](http://arxiv.org/pdf/0911.1894v2)

> In this article we revisit the auxiliary variable method introduced in Smith and kohn (1996) for the fitting of P-th order spline regression models with an unknown number of knot points. We introduce modifications which allow the location of knot points to be random, and we further consider an extension of the method to handle models with non-Gaussian errors. We provide a new algorithm for the MCMC sampling of such models. Simulated data examples are used to compare the performance of our method with existing ones. Finally, we make a connection with some change-point problems, and show how they can be re-parameterised to the variable selection setting.

</details>

<details>

<summary>2009-11-12 13:08:20 - Analytical Determination of Fractal Structure in Stochastic Time Series</summary>

- *Fermín Moscoso del Prado Martín*

- `0911.2381v1` - [abs](http://arxiv.org/abs/0911.2381v1) - [pdf](http://arxiv.org/pdf/0911.2381v1)

> Current methods for determining whether a time series exhibits fractal structure (FS) rely on subjective assessments on estimators of the Hurst exponent (H). Here, I introduce the Bayesian Assessment of Scaling, an analytical framework for drawing objective and accurate inferences on the FS of time series. The technique exploits the scaling property of the diffusion associated to a time series. The resulting criterion is simple to compute and represents an accurate characterization of the evidence supporting different hypotheses on the scaling regime of a time series. Additionally, a closed-form Maximum Likelihood estimator of H is derived from the criterion, and this estimator outperforms the best available estimators.

</details>

<details>

<summary>2009-11-13 16:54:35 - Dynamics of Bayesian Updating with Dependent Data and Misspecified Models</summary>

- *Cosma Rohilla Shalizi*

- `0901.1342v5` - [abs](http://arxiv.org/abs/0901.1342v5) - [pdf](http://arxiv.org/pdf/0901.1342v5)

> Much is now known about the consistency of Bayesian updating on infinite-dimensional parameter spaces with independent or Markovian data. Necessary conditions for consistency include the prior putting enough weight on the correct neighborhoods of the data-generating distribution; various sufficient conditions further restrict the prior in ways analogous to capacity control in frequentist nonparametrics. The asymptotics of Bayesian updating with mis-specified models or priors, or non-Markovian data, are far less well explored. Here I establish sufficient conditions for posterior convergence when all hypotheses are wrong, and the data have complex dependencies. The main dynamical assumption is the asymptotic equipartition (Shannon-McMillan-Breiman) property of information theory. This, along with Egorov's Theorem on uniform convergence, lets me build a sieve-like structure for the prior. The main statistical assumption, also a form of capacity control, concerns the compatibility of the prior and the data-generating process, controlling the fluctuations in the log-likelihood when averaged over the sieve-like sets. In addition to posterior convergence, I derive a kind of large deviations principle for the posterior measure, extending in some cases to rates of convergence, and discuss the advantages of predicting using a combination of models known to be wrong. An appendix sketches connections between these results and the replicator dynamics of evolutionary theory.

</details>

<details>

<summary>2009-11-13 22:13:17 - Non-parametric Deprojection of Surface Brightness Profiles of Galaxies in Generalised Geometries</summary>

- *Dalia Chakrabarty*

- `0911.2716v1` - [abs](http://arxiv.org/abs/0911.2716v1) - [pdf](http://arxiv.org/pdf/0911.2716v1)

> We present a new Bayesian non-parametric deprojection algorithm DOPING (Deprojection of Observed Photometry using and INverse Gambit), that is designed to extract 3-D luminosity density distributions $\rho$ from observed surface brightness maps $I$, in generalised geometries, while taking into account changes in intrinsic shape with radius, using a penalised likelihood approach and an MCMC optimiser. We provide the most likely solution to the integral equation that represents deprojection of the measured $I$ to $\rho$. In order to keep the solution modular, we choose to express $\rho$ as a function of the line-of-sight (LOS) coordinate $z$. We calculate the extent of the system along the ${\bf z}$-axis, for a given point on the image that lies within an identified isophotal annulus. The extent along the LOS is binned and density is held a constant over each such $z$-bin. The code begins with a seed density and at the beginning of an iterative step, the trial $\rho$ is updated. Comparison of the projection of the current choice of $\rho$ and the observed $I$ defines the likelihood function (which is supplemented by Laplacian regularisation), the maximal region of which is sought by the optimiser (Metropolis Hastings). The algorithm is successfully tested on a set of test galaxies, the morphology of which ranges from an elliptical galaxy with varying eccentricity to an infinitesimally thin disk galaxy marked by an abruptly varying eccentricity profile. Applications are made to faint dwarf elliptical galaxy Ic~3019 and another dwarf elliptical that is characterised by a central spheroidal nuclear component superimposed upon a more extended flattened component. The result of deprojection of the X-ray image of triaxial cluster A1413 is also presented.

</details>

<details>

<summary>2009-11-15 17:07:28 - A Hierarchical Bayesian Model for Frame Representation</summary>

- *L. Chaâri, J. -C. Pesquet, J. -Y. Tourneret, Ph. Ciuciu, A. Benazza-Benyahia*

- `0911.2888v1` - [abs](http://arxiv.org/abs/0911.2888v1) - [pdf](http://arxiv.org/pdf/0911.2888v1)

> In many signal processing problems, it may be fruitful to represent the signal under study in a frame. If a probabilistic approach is adopted, it becomes then necessary to estimate the hyper-parameters characterizing the probability distribution of the frame coefficients. This problem is difficult since in general the frame synthesis operator is not bijective. Consequently, the frame coefficients are not directly observable. This paper introduces a hierarchical Bayesian model for frame representation. The posterior distribution of the frame coefficients and model hyper-parameters is derived. Hybrid Markov Chain Monte Carlo algorithms are subsequently proposed to sample from this posterior distribution. The generated samples are then exploited to estimate the hyper-parameters and the frame coefficients of the target signal. Validation experiments show that the proposed algorithms provide an accurate estimation of the frame coefficients and hyper-parameters. Application to practical problems of image denoising show the impact of the resulting Bayesian estimation on the recovered signal quality.

</details>

<details>

<summary>2009-11-18 10:45:44 - Quantile regression in partially linear varying coefficient models</summary>

- *Huixia Judy Wang, Zhongyi Zhu, Jianhui Zhou*

- `0911.3501v1` - [abs](http://arxiv.org/abs/0911.3501v1) - [pdf](http://arxiv.org/pdf/0911.3501v1)

> Semiparametric models are often considered for analyzing longitudinal data for a good balance between flexibility and parsimony. In this paper, we study a class of marginal partially linear quantile models with possibly varying coefficients. The functional coefficients are estimated by basis function approximations. The estimation procedure is easy to implement, and it requires no specification of the error distributions. The asymptotic properties of the proposed estimators are established for the varying coefficients as well as for the constant coefficients. We develop rank score tests for hypotheses on the coefficients, including the hypotheses on the constancy of a subset of the varying coefficients. Hypothesis testing of this type is theoretically challenging, as the dimensions of the parameter spaces under both the null and the alternative hypotheses are growing with the sample size. We assess the finite sample performance of the proposed method by Monte Carlo simulation studies, and demonstrate its value by the analysis of an AIDS data set, where the modeling of quantiles provides more comprehensive information than the usual least squares approach.

</details>

<details>

<summary>2009-11-20 14:56:30 - Efficient estimation of copula-based semiparametric Markov models</summary>

- *Xiaohong Chen, Wei Biao Wu, Yanping Yi*

- `0901.0751v4` - [abs](http://arxiv.org/abs/0901.0751v4) - [pdf](http://arxiv.org/pdf/0901.0751v4)

> This paper considers the efficient estimation of copula-based semiparametric strictly stationary Markov models. These models are characterized by nonparametric invariant (one-dimensional marginal) distributions and parametric bivariate copula functions where the copulas capture temporal dependence and tail dependence of the processes. The Markov processes generated via tail dependent copulas may look highly persistent and are useful for financial and economic applications. We first show that Markov processes generated via Clayton, Gumbel and Student's $t$ copulas and their survival copulas are all geometrically ergodic. We then propose a sieve maximum likelihood estimation (MLE) for the copula parameter, the invariant distribution and the conditional quantiles. We show that the sieve MLEs of any smooth functional is root-$n$ consistent, asymptotically normal and efficient and that their sieve likelihood ratio statistics are asymptotically chi-square distributed. Monte Carlo studies indicate that, even for Markov models generated via tail dependent copulas and fat-tailed marginals, our sieve MLEs perform very well.

</details>

<details>

<summary>2009-11-23 22:25:58 - A Bayesian Variable Selection Approach to Major League Baseball Hitting Metrics</summary>

- *Blakeley B. McShane, Alexander Braunstein, James Piette, Shane T. Jensen*

- `0911.4503v1` - [abs](http://arxiv.org/abs/0911.4503v1) - [pdf](http://arxiv.org/pdf/0911.4503v1)

> Numerous statistics have been proposed for the measure of offensive ability in major league baseball. While some of these measures may offer moderate predictive power in certain situations, it is unclear which simple offensive metrics are the most reliable or consistent. We address this issue with a Bayesian hierarchical model for variable selection to capture which offensive metrics are most predictive within players across time. Our sophisticated methodology allows for full estimation of the posterior distributions for our parameters and automatically adjusts for multiple testing, providing a distinct advantage over alternative approaches. We implement our model on a set of 50 different offensive metrics and discuss our results in the context of comparison to other variable selection techniques. We find that 33/50 metrics demonstrate signal. However, these metrics are highly correlated with one another and related to traditional notions of performance (e.g., plate discipline, power, and ability to make contact).

</details>

<details>

<summary>2009-11-25 17:32:03 - Finite Sample Size Optimality of GLR Tests</summary>

- *George V. Moustakides*

- `0903.3795v2` - [abs](http://arxiv.org/abs/0903.3795v2) - [pdf](http://arxiv.org/pdf/0903.3795v2)

> In several interesting applications one is faced with the problem of simultaneous binary hypothesis testing and parameter estimation. Although such joint problems are not infrequent, there exist no systematic analysis in the literature that treats them effectively. Existing approaches consider the detection and the estimation subproblems separately, applying in each case the corresponding optimum strategy. As it turns out the overall scheme is not necessarily optimum since the criteria used for the two parts are usually incompatible. In this article we propose a mathematical setup that considers the two problems jointly. Specifically we propose a meaningful combination of the Neyman-Pearson and the Bayesian criterion and we provide the optimum solution for the joint problem. In the resulting optimum scheme the two parts interact with each other, producing detection/estimation structures that are completely novel. Notable side-product of our work is the proof that the well known GLR test is finite-sample-size optimum under this combined sense.

</details>


## 2009-12

<details>

<summary>2009-12-14 16:58:04 - Influence tests I: ideal composite hypothesis tests, and causal semimeasures</summary>

- *Bruno Bauwens*

- `0912.2688v1` - [abs](http://arxiv.org/abs/0912.2688v1) - [pdf](http://arxiv.org/pdf/0912.2688v1)

> Ratios of universal enumerable semimeasures corresponding to hypotheses are investigated as a solution for statistical composite hypotheses testing if an unbounded amount of computation time can be assumed.   Influence testing for discrete time series is defined using generalized structural equations. Several ideal tests are introduced, and it is argued that when Halting information is transmitted, in some cases, instantaneous cause and consequence can be inferred where this is not possible classically.   The approach is contrasted with Bayesian definitions of influence, where it is left open whether all Bayesian causal associations of universal semimeasures are equal within a constant. Finally the approach is also contrasted with existing engineering procedures for influence and theoretical definitions of causation.

</details>

<details>

<summary>2009-12-16 16:31:51 - Notes to Robert et al.: Model criticism informs model choice and model comparison</summary>

- *Oliver Ratmann, Christophe Andrieu, Carsten Wiuf, Sylvia Richardson*

- `0912.3182v1` - [abs](http://arxiv.org/abs/0912.3182v1) - [pdf](http://arxiv.org/pdf/0912.3182v1)

> In their letter to PNAS and a comprehensive set of notes on arXiv [arXiv:0909.5673v2], Christian Robert, Kerrie Mengersen and Carla Chen (RMC) represent our approach to model criticism in situations when the likelihood cannot be computed as a way to "contrast several models with each other". In addition, RMC argue that model assessment with Approximate Bayesian Computation under model uncertainty (ABCmu) is unduly challenging and question its Bayesian foundations. We disagree, and clarify that ABCmu is a probabilistically sound and powerful too for criticizing a model against aspects of the observed data, and discuss further the utility of ABCmu.

</details>

<details>

<summary>2009-12-16 17:51:39 - Multi-Way, Multi-View Learning</summary>

- *Ilkka Huopaniemi, Tommi Suvitaival, Janne Nikkilä, Matej Orešič, Samuel Kaski*

- `0912.3211v1` - [abs](http://arxiv.org/abs/0912.3211v1) - [pdf](http://arxiv.org/pdf/0912.3211v1)

> We extend multi-way, multivariate ANOVA-type analysis to cases where one covariate is the view, with features of each view coming from different, high-dimensional domains. The different views are assumed to be connected by having paired samples; this is a common setup in recent bioinformatics experiments, of which we analyze metabolite profiles in different conditions (disease vs. control and treatment vs. untreated) in different tissues (views). We introduce a multi-way latent variable model for this new task, by extending the generative model of Bayesian canonical correlation analysis (CCA) both to take multi-way covariate information into account as population priors, and by reducing the dimensionality by an integrated factor analysis that assumes the metabolites to come in correlated groups.

</details>

<details>

<summary>2009-12-22 12:20:53 - On Bayesian "testimation" and its application to wavelet thresholding</summary>

- *Felix Abramovich, Vadim Grinshtein, Athanasia Petsa, Theofanis Sapatinas*

- `0912.4386v1` - [abs](http://arxiv.org/abs/0912.4386v1) - [pdf](http://arxiv.org/pdf/0912.4386v1)

> We consider the problem of estimating the unknown response function in the Gaussian white noise model. We first utilize the recently developed Bayesian maximum a posteriori "testimation" procedure of Abramovich et al. (2007) for recovering an unknown high-dimensional Gaussian mean vector. The existing results for its upper error bounds over various sparse $l_p$-balls are extended to more general cases. We show that, for a properly chosen prior on the number of non-zero entries of the mean vector, the corresponding adaptive estimator is simultaneously asymptotically minimax in a wide range of sparse and dense $l_p$-balls.   The proposed procedure is then applied in a wavelet context to derive adaptive global and level-wise wavelet estimators of the unknown response function in the Gaussian white noise model. These estimators are then proven to be, respectively, asymptotically near-minimax and minimax in a wide range of Besov balls. These results are also extended to the estimation of derivatives of the response function.   Simulated examples are conducted to illustrate the performance of the proposed level-wise wavelet estimator in finite sample situations, and to compare it with several existing

</details>

<details>

<summary>2009-12-23 20:53:12 - Likelihood-free Bayesian inference for alpha-stable models</summary>

- *G. W. Peters, S. A. Sisson, Y. Fan*

- `0912.4729v1` - [abs](http://arxiv.org/abs/0912.4729v1) - [pdf](http://arxiv.org/pdf/0912.4729v1)

> $\alpha$-stable distributions are utilised as models for heavy-tailed noise in many areas of statistics, finance and signal processing engineering.   However, in general, neither univariate nor multivariate $\alpha$-stable models admit closed form densities which can be evaluated pointwise. This complicates the inferential procedure.   As a result, $\alpha$-stable models are practically limited to the univariate setting under the Bayesian paradigm, and to bivariate models under the classical framework.   In this article we develop a novel Bayesian approach to modelling univariate and multivariate $\alpha$-stable distributions based on recent advances in "likelihood-free" inference.   We present an evaluation of the performance of this procedure in 1, 2 and 3 dimensions, and provide an analysis of real daily currency exchange rate data. The proposed approach provides a feasible inferential methodology at a moderate computational cost.

</details>

<details>

<summary>2009-12-24 15:29:32 - On Finding Predictors for Arbitrary Families of Processes</summary>

- *Daniil Ryabko*

- `0912.4883v1` - [abs](http://arxiv.org/abs/0912.4883v1) - [pdf](http://arxiv.org/pdf/0912.4883v1)

> The problem is sequence prediction in the following setting. A sequence $x_1,...,x_n,...$ of discrete-valued observations is generated according to some unknown probabilistic law (measure) $\mu$. After observing each outcome, it is required to give the conditional probabilities of the next observation. The measure $\mu$ belongs to an arbitrary but known class $C$ of stochastic process measures. We are interested in predictors $\rho$ whose conditional probabilities converge (in some sense) to the "true" $\mu$-conditional probabilities if any $\mu\in C$ is chosen to generate the sequence. The contribution of this work is in characterizing the families $C$ for which such predictors exist, and in providing a specific and simple form in which to look for a solution. We show that if any predictor works, then there exists a Bayesian predictor, whose prior is discrete, and which works too. We also find several sufficient and necessary conditions for the existence of a predictor, in terms of topological characterizations of the family $C$, as well as in terms of local behaviour of the measures in $C$, which in some cases lead to procedures for constructing such predictors. It should be emphasized that the framework is completely general: the stochastic processes considered are not required to be i.i.d., stationary, or to belong to any parametric or countable family.

</details>

<details>

<summary>2009-12-24 18:46:38 - Nonparametric Bayesian Density Modeling with Gaussian Processes</summary>

- *Ryan Prescott Adams, Iain Murray, David J. C. MacKay*

- `0912.4896v1` - [abs](http://arxiv.org/abs/0912.4896v1) - [pdf](http://arxiv.org/pdf/0912.4896v1)

> We present the Gaussian process density sampler (GPDS), an exchangeable generative model for use in nonparametric Bayesian density estimation. Samples drawn from the GPDS are consistent with exact, independent samples from a distribution defined by a density that is a transformation of a function drawn from a Gaussian process prior. Our formulation allows us to infer an unknown density from data using Markov chain Monte Carlo, which gives samples from the posterior distribution over density functions and from the predictive distribution on data space. We describe two such MCMC methods. Both methods also allow inference of the hyperparameters of the Gaussian process.

</details>

<details>

<summary>2009-12-26 12:57:29 - Inference for Extremal Conditional Quantile Models, with an Application to Market and Birthweight Risks</summary>

- *Victor Chernozhukov, Ivan Fernandez-Val*

- `0912.5013v1` - [abs](http://arxiv.org/abs/0912.5013v1) - [pdf](http://arxiv.org/pdf/0912.5013v1)

> Quantile regression is an increasingly important empirical tool in economics and other sciences for analyzing the impact of a set of regressors on the conditional distribution of an outcome. Extremal quantile regression, or quantile regression applied to the tails, is of interest in many economic and financial applications, such as conditional value-at-risk, production efficiency, and adjustment bands in (S,s) models. In this paper we provide feasible inference tools for extremal conditional quantile models that rely upon extreme value approximations to the distribution of self-normalized quantile regression statistics. The methods are simple to implement and can be of independent interest even in the non-regression case. We illustrate the results with two empirical examples analyzing extreme fluctuations of a stock return and extremely low percentiles of live infants' birthweights in the range between 250 and 1500 grams.

</details>

