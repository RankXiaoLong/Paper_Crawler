# 2007

## TOC

- [2007-01](#2007-01)
- [2007-02](#2007-02)
- [2007-03](#2007-03)
- [2007-05](#2007-05)
- [2007-06](#2007-06)
- [2007-07](#2007-07)
- [2007-08](#2007-08)
- [2007-09](#2007-09)
- [2007-10](#2007-10)
- [2007-11](#2007-11)
- [2007-12](#2007-12)

## 2007-01

<details>

<summary>2007-01-03 10:23:01 - Design Issues for Generalized Linear Models: A Review</summary>

- *André I. Khuri, Bhramar Mukherjee, Bikas K. Sinha, Malay Ghosh*

- `0701088v1` - [abs](http://arxiv.org/abs/0701088v1) - [pdf](http://arxiv.org/pdf/math/0701088v1)

> Generalized linear models (GLMs) have been used quite effectively in the modeling of a mean response under nonstandard conditions, where discrete as well as continuous data distributions can be accommodated. The choice of design for a GLM is a very important task in the development and building of an adequate model. However, one major problem that handicaps the construction of a GLM design is its dependence on the unknown parameters of the fitted model. Several approaches have been proposed in the past 25 years to solve this problem. These approaches, however, have provided only partial solutions that apply in only some special cases, and the problem, in general, remains largely unresolved. The purpose of this article is to focus attention on the aforementioned dependence problem. We provide a survey of various existing techniques dealing with the dependence problem. This survey includes discussions concerning locally optimal designs, sequential designs, Bayesian designs and the quantile dispersion graph approach for comparing designs for GLMs.

</details>

<details>

<summary>2007-01-11 20:28:53 - On the non-arbitrary assignment of equi-probable priors</summary>

- *William M. Briggs*

- `0701331v1` - [abs](http://arxiv.org/abs/0701331v1) - [pdf](http://arxiv.org/pdf/math/0701331v1)

> How to form priors that do not seem artificial or arbitrary is a central question in Bayesian statistics. The case of forming a prior on the truth of a proposition for which there is no evidence, and the definte evidence that the event can happen in a finite set of ways, is detailed. The truth of a propostion of this kind is frequently assigned a prior of 0.5 via arguments of ignorance, randomness, the Principle of Indiffernce, the Principal Principal, or by other methods. These are all shown to be flawed. The statistical syllogism introduced by Williams in 1947 is shown to fix the problems that the other arguments have. An example in the context of model selection is given.

</details>

<details>

<summary>2007-01-12 18:59:18 - A Bayesian Hierarchical Modeling Approach to Dietary Assessment via Food Frequency</summary>

- *Andrew Lawson, Daniela Nitcheva*

- `0701356v1` - [abs](http://arxiv.org/abs/0701356v1) - [pdf](http://arxiv.org/pdf/math/0701356v1)

> Previous likelihood-based linear modeling of nutritional data has been limited by the availability of software that allows flexible error structures in the data. We demonstrate the use of a Bayesian modeling approach to the analysis of such data. Our goal is to model the relationship between the energy intake derived from Food Frequency Questionnaires (FFQs) and the energy expenditure estimated from the doubly labeled water method. We consider models with different distributions for the FFQ energy intake. The models include previously identified covariates describing social desirability, education and their possible interaction that are felt to impact the reported FFQ. The models also include random effects to account for subject specific random variation (frailty) and also to account for the complex patterns of measurement error inherent in these data. Issues arising within the work relate both to the selection of relevant linear and non-linear models, the use of random effects, and the relevance of goodness-of-fit criteria such as DIC and PPL in assessing the most appropriate model.

</details>

<details>

<summary>2007-01-19 14:37:54 - The penalized profile sampler</summary>

- *Guang Cheng, Michael R. Kosorok*

- `0701540v1` - [abs](http://arxiv.org/abs/0701540v1) - [pdf](http://arxiv.org/pdf/math/0701540v1)

> The penalized profile sampler for semiparametric inference is an extension of the profile sampler method (Lee, Kosorok and Fine, 2005) obtained by profiling a penalized log-likelihood. The idea is to base inference on the posterior distribution obtained by multiplying a profiled penalized log-likelihood by a prior for the parametric component, where the profiling and penalization are applied to the nuisance parameter. Because the prior is not applied to the full likelihood, the method is not strictly Bayesian. A benefit of this approximately Bayesian method is that it circumvents the need to put a prior on the possibly infinite-dimensional nuisance components of the model. We investigate the first and second order frequentist performance of the penalized profile sampler, and demonstrate that the accuracy of the procedure can be adjusted by the size of the assigned smoothing parameter. The theoretical validity of the procedure is illustrated for two examples: a partly linear model with normal error for current status data and a semiparametric logistic regression model. As far as we are aware, there are no other methods of inference in this context known to have second order frequentist validity.

</details>

<details>

<summary>2007-01-21 02:04:41 - Bayesian shrinkage prediction for the regression problem</summary>

- *Kei Kobayashi, Fumiyasu Komaki*

- `0701583v1` - [abs](http://arxiv.org/abs/0701583v1) - [pdf](http://arxiv.org/pdf/math/0701583v1)

> We consider Bayesian shrinkage predictions for the Normal regression problem under the frequentist Kullback-Leibler risk function.   Firstly, we consider the multivariate Normal model with an unknown mean and a known covariance. While the unknown mean is fixed, the covariance of future samples can be different from training samples. We show that the Bayesian predictive distribution based on the uniform prior is dominated by that based on a class of priors if the prior distributions for the covariance and future covariance matrices are rotation invariant.   Then, we consider a class of priors for the mean parameters depending on the future covariance matrix. With such a prior, we can construct a Bayesian predictive distribution dominating that based on the uniform prior.   Lastly, applying this result to the prediction of response variables in the Normal linear regression model, we show that there exists a Bayesian predictive distribution dominating that based on the uniform prior. Minimaxity of these Bayesian predictions follows from these results.

</details>

<details>

<summary>2007-01-22 10:55:25 - Resampling-based confidence regions and multiple tests for a correlated random vector</summary>

- *Sylvain Arlot, Gilles Blanchard, Etienne Roquain*

- `0701605v1` - [abs](http://arxiv.org/abs/0701605v1) - [pdf](http://arxiv.org/pdf/math/0701605v1)

> We derive non-asymptotic confidence regions for the mean of a random vector whose coordinates have an unknown dependence structure. The random vector is supposed to be either Gaussian or to have a symmetric bounded distribution, and we observe $n$ i.i.d copies of it. The confidence regions are built using a data-dependent threshold based on a weighted bootstrap procedure. We consider two approaches, the first based on a concentration approach and the second on a direct boostrapped quantile approach. The first one allows to deal with a very large class of resampling weights while our results for the second are restricted to Rademacher weights. However, the second method seems more accurate in practice. Our results are motivated by multiple testing problems, and we show on simulations that our procedures are better than the Bonferroni procedure (union bound) as soon as the observed vector has sufficiently correlated coordinates.

</details>


## 2007-02

<details>

<summary>2007-02-08 08:48:06 - Hurst exponent estimation of locally self-similar Gaussian processes using sample quantiles</summary>

- *Jean-François Coeurjolly*

- `0506290v2` - [abs](http://arxiv.org/abs/0506290v2) - [pdf](http://arxiv.org/pdf/math/0506290v2)

> This paper is devoted to the introduction of a new class of consistent estimators of the fractal dimension of locally self-similar Gaussian processes. These estimators are based on convex combinations of sample quantiles of discrete variations of a sample path over a discrete grid of the interval $[0,1]$. We derive the almost sure convergence and the asymptotic normality for these estimators. The key-ingredient is a Bahadur representation for sample quantiles of non-linear functions of Gaussians sequences with correlation function decreasing as $k^{-\alpha}L(k)$ for some $\alpha>0$ and some slowly varying function $L(\cdot)$.

</details>

<details>

<summary>2007-02-08 16:24:50 - Bayesian Inference for Linear Dynamic Models with Dirichlet Process Mixtures</summary>

- *François Caron, Manuel Davy, Arnaud Doucet, Emmanuel Duflos, Philippe Vanheeghe*

- `0702225v1` - [abs](http://arxiv.org/abs/0702225v1) - [pdf](http://arxiv.org/pdf/math/0702225v1)

> Using Kalman techniques, it is possible to perform optimal estimation in linear Gaussian state-space models. We address here the case where the noise probability density functions are of unknown functional form. A flexible Bayesian nonparametric noise model based on Dirichlet process mixtures is introduced. Efficient Markov chain Monte Carlo and Sequential Monte Carlo methods are then developed to perform optimal batch and sequential estimation in such contexts. The algorithms are applied to blind deconvolution and change point detection. Experimental results on synthetic and real data demonstrate the efficiency of this approach in various contexts.

</details>

<details>

<summary>2007-02-14 06:23:47 - Ranking the best instances</summary>

- *Stéphan Clémençon, Nicolas Vayatis*

- `0611133v2` - [abs](http://arxiv.org/abs/0611133v2) - [pdf](http://arxiv.org/pdf/math/0611133v2)

> We formulate the local ranking problem in the framework of bipartite ranking where the goal is to focus on the best instances. We propose a methodology based on the construction of real-valued scoring functions. We study empirical risk minimization of dedicated statistics which involve empirical quantiles of the scores. We first state the problem of finding the best instances which can be cast as a classification problem with mass constraint. Next, we develop special performance measures for the local ranking problem which extend the Area Under an ROC Curve (AUC/AROC) criterion and describe the optimal elements of these new criteria. We also highlight the fact that the goal of ranking the best instances cannot be achieved in a stage-wise manner where first, the best instances would be tentatively identified and then a standard AUC criterion could be applied. Eventually, we state preliminary statistical results for the local ranking problem.

</details>

<details>

<summary>2007-02-19 15:56:45 - QQ plots, Random sets and data from a heavy tailed distribution</summary>

- *Bikramjit Das, Sidney I. Resnick*

- `0702551v1` - [abs](http://arxiv.org/abs/0702551v1) - [pdf](http://arxiv.org/pdf/math/0702551v1)

> The QQ plot is a commonly used technique for informally deciding whether a univariate random sample of size n comes from a specified distribution F. The QQ plot graphs the sample quantiles against the theoretical quantiles of F and then a visual check is made to see whether or not the points are close to a straight line. For a location and scale family of distributions, the intercept and slope of the straight line provide estimates for the shift and scale parameters of the distribution respectively. Here we consider the set S_n of points forming the QQ plot as a random closed set in R^2. We show that under certain regularity conditions on the distribution F, S_n converges in probability to a closed, non-random set. In the heavy tailed case where 1-F is a regularly varying function, a similar result can be shown but a modification is necessary to provide a statistically sensible result since typically F is not completely known.

</details>

<details>

<summary>2007-02-20 20:02:24 - A Method for Avoiding Bias from Feature Selection with Application to Naive Bayes Classification Models</summary>

- *Longhai Li, Jianguo Zhang, Radford M. Neal*

- `0702591v1` - [abs](http://arxiv.org/abs/0702591v1) - [pdf](http://arxiv.org/pdf/math/0702591v1)

> For many classification and regression problems, a large number of features are available for possible use - this is typical of DNA microarray data on gene expression, for example. Often, for computational or other reasons, only a small subset of these features are selected for use in a model, based on some simple measure such as correlation with the response variable. This procedure may introduce an optimistic bias, however, in which the response variable appears to be more predictable than it actually is, because the high correlation of the selected features with the response may be partly or wholely due to chance. We show how this bias can be avoided when using a Bayesian model for the joint distribution of features and response. The crucial insight is that even if we forget the exact values of the unselected features, we should retain, and condition on, the knowledge that their correlation with the response was too small for them to be selected. In this paper we describe how this idea can be implemented for ``naive Bayes'' models of binary data. Experiments with simulated data confirm that this method avoids bias due to feature selection. We also apply the naive Bayes model to subsets of data relating gene expression to colon cancer, and find that correcting for bias from feature selection does improve predictive performance.

</details>

<details>

<summary>2007-02-22 10:51:53 - Discussion paper. Conditional growth charts</summary>

- *Ying Wei, Xuming He*

- `0702634v1` - [abs](http://arxiv.org/abs/0702634v1) - [pdf](http://arxiv.org/pdf/math/0702634v1)

> Growth charts are often more informative when they are customized per subject, taking into account prior measurements and possibly other covariates of the subject. We study a global semiparametric quantile regression model that has the ability to estimate conditional quantiles without the usual distributional assumptions. The model can be estimated from longitudinal reference data with irregular measurement times and with some level of robustness against outliers, and it is also flexible for including covariate information. We propose a rank score test for large sample inference on covariates, and develop a new model assessment tool for longitudinal growth data. Our research indicates that the global model has the potential to be a very useful tool in conditional growth chart analysis.

</details>

<details>

<summary>2007-02-22 13:22:31 - From $ε$-entropy to KL-entropy: Analysis of minimum information complexity density estimation</summary>

- *Tong Zhang*

- `0702653v1` - [abs](http://arxiv.org/abs/0702653v1) - [pdf](http://arxiv.org/pdf/math/0702653v1)

> We consider an extension of $\epsilon$-entropy to a KL-divergence based complexity measure for randomized density estimation methods. Based on this extension, we develop a general information-theoretical inequality that measures the statistical complexity of some deterministic and randomized density estimators. Consequences of the new inequality will be presented. In particular, we show that this technique can lead to improvements of some classical results concerning the convergence of minimum description length and Bayesian posterior distributions. Moreover, we are able to derive clean finite-sample convergence bounds that are not obtainable using previous approaches.

</details>

<details>

<summary>2007-02-23 10:55:04 - The shape of incomplete preferences</summary>

- *Robert Nau*

- `0702689v1` - [abs](http://arxiv.org/abs/0702689v1) - [pdf](http://arxiv.org/pdf/math/0702689v1)

> Incomplete preferences provide the epistemic foundation for models of imprecise subjective probabilities and utilities that are used in robust Bayesian analysis and in theories of bounded rationality. This paper presents a simple axiomatization of incomplete preferences and characterizes the shape of their representing sets of probabilities and utilities. Deletion of the completeness assumption from the axiom system of Anscombe and Aumann yields preferences represented by a convex set of state-dependent expected utilities, of which at least one must be a probability/utility pair. A strengthening of the state-independence axiom is needed to obtain a representation purely in terms of a set of probability/utility pairs.

</details>

<details>

<summary>2007-02-23 13:19:12 - Efficient prediction for linear and nonlinear autoregressive models</summary>

- *Ursula U. Müller, Anton Schick, Wolfgang Wefelmeyer*

- `0702701v1` - [abs](http://arxiv.org/abs/0702701v1) - [pdf](http://arxiv.org/pdf/math/0702701v1)

> Conditional expectations given past observations in stationary time series are usually estimated directly by kernel estimators, or by plugging in kernel estimators for transition densities. We show that, for linear and nonlinear autoregressive models driven by independent innovations, appropriate smoothed and weighted von Mises statistics of residuals estimate conditional expectations at better parametric rates and are asymptotically efficient. The proof is based on a uniform stochastic expansion for smoothed and weighted von Mises processes of residuals. We consider, in particular, estimation of conditional distribution functions and of conditional quantile functions.

</details>


## 2007-03

<details>

<summary>2007-03-02 09:16:43 - Quantile regression when the covariates are functions</summary>

- *Hervé Cardot, Christophe Crambes, Pascal Sarda*

- `0703056v1` - [abs](http://arxiv.org/abs/0703056v1) - [pdf](http://arxiv.org/pdf/math/0703056v1)

> This paper deals with a linear model of regression on quantiles when the explanatory variable takes values in some functional space and the response is scalar. We propose a spline estimator of the functional coefficient that minimizes a penalized L1 type criterion. Then, we study the asymptotic behavior of this estimator. The penalization is of primary importance to get existence and convergence.

</details>

<details>

<summary>2007-03-05 12:43:19 - Algebraic Bayesian analysis of contingency tables with possibly zero-probability cells</summary>

- *Guido Consonni, Giovanni Pistone*

- `0703123v1` - [abs](http://arxiv.org/abs/0703123v1) - [pdf](http://arxiv.org/pdf/math/0703123v1)

> In this paper we consider a Bayesian analysis of contingency tables allowing for the possibility that cells may have probability zero. In this sense we depart from standard log-linear modeling that implicitly assumes a positivity constraint. Our approach leads us to consider mixture models for contingency tables, where the components of the mixture, which we call model-instances, have distinct support. We rely on ideas from polynomial algebra in order to identify the various model instances. We also provide a method to assign prior probabilities to each instance of the model, as well as describing methods for constructing priors on the parameter space of each instance. We illustrate our methodology through a $5 \times 2$ table involving two structural zeros, as well as a zero count. The results we obtain show that our analysis may lead to conclusions that are substantively different from those that would obtain in a standard framework, wherein the possibility of zero-probability cells is not explicitly accounted for.

</details>

<details>

<summary>2007-03-24 02:59:06 - Inferring Markov Chains: Bayesian Estimation, Model Comparison, Entropy Rate, and Out-of-class Modeling</summary>

- *Christopher C. Strelioff, James P. Crutchfield, Alfred W. Hubler*

- `0703715v1` - [abs](http://arxiv.org/abs/0703715v1) - [pdf](http://arxiv.org/pdf/math/0703715v1)

> Markov chains are a natural and well understood tool for describing one-dimensional patterns in time or space. We show how to infer $k$-th order Markov chains, for arbitrary $k$, from finite data by applying Bayesian methods to both parameter estimation and model-order selection. Extending existing results for multinomial models of discrete data, we connect inference to statistical mechanics through information-theoretic (type theory) techniques. We establish a direct relationship between Bayesian evidence and the partition function which allows for straightforward calculation of the expectation and variance of the conditional relative entropy and the source entropy rate. Finally, we introduce a novel method that uses finite data-size scaling with model-order comparison to infer the structure of out-of-class processes.

</details>


## 2007-05

<details>

<summary>2007-05-17 11:29:08 - Finite Element Model Updating Using Bayesian Approach</summary>

- *Tshilidzi Marwala, Lungile Mdlazi, Sibusiso Sibisi*

- `0705.2515v1` - [abs](http://arxiv.org/abs/0705.2515v1) - [pdf](http://arxiv.org/pdf/0705.2515v1)

> This paper compares the Maximum-likelihood method and Bayesian method for finite element model updating. The Maximum-likelihood method was implemented using genetic algorithm while the Bayesian method was implemented using the Markov Chain Monte Carlo. These methods were tested on a simple beam and an unsymmetrical H-shaped structure. The results show that the Bayesian method gave updated finite element models that predicted more accurate modal properties than the updated finite element models obtained through the use of the Maximum-likelihood method. Furthermore, both these methods were found to require the same levels of computational loads.

</details>

<details>

<summary>2007-05-22 19:52:46 - Algebraic geometry of Gaussian Bayesian networks</summary>

- *Seth Sullivant*

- `0704.0918v2` - [abs](http://arxiv.org/abs/0704.0918v2) - [pdf](http://arxiv.org/pdf/0704.0918v2)

> Conditional independence models in the Gaussian case are algebraic varieties in the cone of positive definite covariance matrices. We study these varieties in the case of Bayesian networks, with a view towards generalizing the recursive factorization theorem to situations with hidden variables. In the case when the underlying graph is a tree, we show that the vanishing ideal of the model is generated by the conditional independence statements implied by graph. We also show that the ideal of any Bayesian network is homogeneous with respect to a multigrading induced by a collection of upstream random variables. This has a number of important consequences for hidden variable models. Finally, we relate the ideals of Bayesian networks to a number of classical constructions in algebraic geometry including toric degenerations of the Grassmannian, matrix Schubert varieties, and secant varieties.

</details>

<details>

<summary>2007-05-31 12:25:47 - Variable Selection Incorporating Prior Constraint Information into Lasso</summary>

- *Shurong Zheng, Guodong Song, Ning-Zhong Shi*

- `0705.4588v1` - [abs](http://arxiv.org/abs/0705.4588v1) - [pdf](http://arxiv.org/pdf/0705.4588v1)

> We propose the variable selection procedure incorporating prior constraint information into lasso. The proposed procedure combines the sample and prior information, and selects significant variables for responses in a narrower region where the true parameters lie. It increases the efficiency to choose the true model correctly. The proposed procedure can be executed by many constrained quadratic programming methods and the initial estimator can be found by least square or Monte Carlo method. The proposed procedure also enjoys good theoretical properties. Moreover, the proposed procedure is not only used for linear models but also can be used for generalized linear models({\sl GLM}), Cox models, quantile regression models and many others with the help of Wang and Leng (2007)'s LSA, which changes these models as the approximation of linear models. The idea of combining sample and prior constraint information can be also used for other modified lasso procedures. Some examples are used for illustration of the idea of incorporating prior constraint information in variable selection procedures.

</details>


## 2007-06

<details>

<summary>2007-06-01 06:05:44 - Modeling Hourly Ozone Concentration Fields</summary>

- *Yiping Dou, Nhu D Le, James V Zidek*

- `0706.0073v1` - [abs](http://arxiv.org/abs/0706.0073v1) - [pdf](http://arxiv.org/pdf/0706.0073v1)

> This paper presents a dynamic linear model for modeling hourly ozone concentrations over the eastern United States. That model, which is developed within an Bayesian hierarchical framework, inherits the important feature of such models that its coefficients, treated as states of the process, can change with time. Thus the model includes a time--varying site invariant mean field as well as time varying coefficients for 24 and 12 diurnal cycle components. This cost of this model's great flexibility comes at the cost of computational complexity, forcing us to use an MCMC approach and to restrict application of our model domain to a small number of monitoring sites. We critically assess this model and discover some of its weaknesses in this type of application.

</details>

<details>

<summary>2007-06-01 15:00:20 - Evaluating Throwing Ability in Baseball</summary>

- *Matthew Carruth, Shane T. Jensen*

- `0705.3257v2` - [abs](http://arxiv.org/abs/0705.3257v2) - [pdf](http://arxiv.org/pdf/0705.3257v2)

> We present a quantitative analysis of throwing ability for major league outfielders and catchers. We use detailed game event data to tabulate success and failure events in outfielder and catcher throwing opportunities. We attribute a run contribution to each success or failure which are tabulated for each player in each season. We use four seasons of data to estimate the overall throwing ability of each player using a Bayesian hierarchical model. This model allows us to shrink individual player estimates towards an overall population mean depending on the number of opportunities for each player. We use the posterior distribution of player abilities from this model to identify players with significant positive and negative throwing contributions.

</details>

<details>

<summary>2007-06-09 04:55:02 - Bayesian Covariance Matrix Estimation using a Mixture of Decomposable Graphical Models</summary>

- *Helen Armstrong, Christopher K. Carter, Kevin F. Wong, Robert Kohn*

- `0706.1287v1` - [abs](http://arxiv.org/abs/0706.1287v1) - [pdf](http://arxiv.org/pdf/0706.1287v1)

> A Bayesian approach is used to estimate the covariance matrix of Gaussian data. Ideas from Gaussian graphical models and model selection are used to construct a prior for the covariance matrix that is a mixture over all decomposable graphs. For this prior the probability of each graph size is specified by the user and graphs of equal size are assigned equal probability. Most previous approaches assume that all graphs are equally probable. We show empirically that the prior that assigns equal probability over graph sizes outperforms the prior that assigns equal probability over all graphs, both in identifying the correct decomposable graph and in more efficiently estimating the covariance matrix.

</details>

<details>

<summary>2007-06-15 11:46:37 - Consistent reasoning about a continuum of hypotheses on the basis of finite evidence</summary>

- *Jochen Rau*

- `0706.2274v1` - [abs](http://arxiv.org/abs/0706.2274v1) - [pdf](http://arxiv.org/pdf/0706.2274v1)

> In the modern Bayesian view classical probability theory is simply an extension of conventional logic, i.e., a quantitative tool that allows for consistent reasoning in the presence of uncertainty. Classical theory presupposes, however, that--at least in principle--the amount of evidence that an experimenter can accumulate always matches the size of the hypothesis space. I investigate how the framework for consistent reasoning must be modified in non-classical situations where hypotheses form a continuum, yet the maximum evidence accessible through experiment is not allowed to exceed some finite upper bound. Invoking basic consistency requirements pertaining to the preparation and composition of systems, as well as to the continuity of probabilities, I show that the modified theory must have an internal symmetry isomorphic to the unitary group. It thus appears that the only consistent algorithm for plausible reasoning about a continuum of hypotheses on the basis of finite evidence is furnished by quantum theory in complex Hilbert space.

</details>


## 2007-07

<details>

<summary>2007-07-02 05:12:37 - On semiparametric regression with O'Sullivan penalised splines</summary>

- *M. P. Wand, J. T. Ormerod*

- `0707.0143v1` - [abs](http://arxiv.org/abs/0707.0143v1) - [pdf](http://arxiv.org/pdf/0707.0143v1)

> This is an expos\'e on the use of O'Sullivan penalised splines in contemporary semiparametric regression, including mixed model and Bayesian formulations. O'Sullivan penalised splines are similar to P-splines, but have an advantage of being a direct generalisation of smoothing splines. Exact expressions for the O'Sullivan penalty matrix are obtained. Comparisons between the two reveals that O'Sullivan penalised splines more closely mimic the natural boundary behaviour of smoothing splines. Implementation in modern computing environments such as Matlab, R and BUGS is discussed.

</details>

<details>

<summary>2007-07-14 16:00:44 - Variable Selection and Model Averaging in Semiparametric Overdispersed Generalized Linear Models</summary>

- *Remy Cottet, Robert Kohn, David Nott*

- `0707.2158v1` - [abs](http://arxiv.org/abs/0707.2158v1) - [pdf](http://arxiv.org/pdf/0707.2158v1)

> We express the mean and variance terms in a double exponential regression model as additive functions of the predictors and use Bayesian variable selection to determine which predictors enter the model, and whether they enter linearly or flexibly. When the variance term is null we obtain a generalized additive model, which becomes a generalized linear model if the predictors enter the mean linearly. The model is estimated using Markov chain Monte Carlo simulation and the methodology is illustrated using real and simulated data sets.

</details>

<details>

<summary>2007-07-16 03:37:39 - A Bayes method for a Bathtub Failure Rate via two $\mathbf{S}$-paths</summary>

- *Man-Wai Ho*

- `0707.2257v1` - [abs](http://arxiv.org/abs/0707.2257v1) - [pdf](http://arxiv.org/pdf/0707.2257v1)

> A class of semi-parametric hazard/failure rates with a bathtub shape is of interest. It does not only provide a great deal of flexibility over existing parametric methods in the modeling aspect but also results in a closed and tractable Bayes estimator for the bathtub-shaped failure rate (BFR). Such an estimator is derived to be a finite sum over two $\mathbf{S}$-paths due to an explicit posterior analysis in terms of two (conditionally independent) $\mathbf{S}$-paths. These, newly discovered, explicit results can be proved to be a Rao-Blackwellization of counterpart results in terms of partitions that are readily available by a specialization of James (2005)'s work. We develop both iterative and non-iterative computational procedures based on existing efficient Monte Carlo methods for sampling one single $\mathbf{S}$-path. Nmerical simulations are given to demonstrate the practicality and the effectiveness of our methodology. Last but not least, two applications of the proposed method are discussed, of which one is about a Bayesian test for failure rates and the other is related to modeling with covariates.

</details>

<details>

<summary>2007-07-20 08:23:41 - Application of probabilistic PCR5 Fusion Rule for Multisensor Target Tracking</summary>

- *Alois Kirchner, Frederic Dambreville, Francis Celeste, Jean Dezert, Florentin Smarandache*

- `0707.3013v1` - [abs](http://arxiv.org/abs/0707.3013v1) - [pdf](http://arxiv.org/pdf/0707.3013v1)

> This paper defines and implements a non-Bayesian fusion rule for combining densities of probabilities estimated by local (non-linear) filters for tracking a moving target by passive sensors. This rule is the restriction to a strict probabilistic paradigm of the recent and efficient Proportional Conflict Redistribution rule no 5 (PCR5) developed in the DSmT framework for fusing basic belief assignments. A sampling method for probabilistic PCR5 (p-PCR5) is defined. It is shown that p-PCR5 is more robust to an erroneous modeling and allows to keep the modes of local densities and preserve as much as possible the whole information inherent to each densities to combine. In particular, p-PCR5 is able of maintaining multiple hypotheses/modes after fusion, when the hypotheses are too distant in regards to their deviations. This new p-PCR5 rule has been tested on a simple example of distributed non-linear filtering application to show the interest of such approach for future developments. The non-linear distributed filter is implemented through a basic particles filtering technique. The results obtained in our simulations show the ability of this p-PCR5-based filter to track the target even when the models are not well consistent in regards to the initialization and real cinematic.

</details>

<details>

<summary>2007-07-24 05:04:53 - A Bayesian Framework for Combining Valuation Estimates</summary>

- *Kenton K. Yee*

- `0707.3482v1` - [abs](http://arxiv.org/abs/0707.3482v1) - [pdf](http://arxiv.org/pdf/0707.3482v1)

> Obtaining more accurate equity value estimates is the starting point for stock selection, value-based indexing in a noisy market, and beating benchmark indices through tactical style rotation. Unfortunately, discounted cash flow, method of comparables, and fundamental analysis typically yield discrepant valuation estimates. Moreover, the valuation estimates typically disagree with market price. Can one form a superior valuation estimate by averaging over the individual estimates, including market price? This article suggests a Bayesian framework for combining two or more estimates into a superior valuation estimate. The framework justifies the common practice of averaging over several estimates to arrive at a final point estimate.

</details>

<details>

<summary>2007-07-26 08:41:39 - Intrinsic tests for the equality of two correlated proportions</summary>

- *Guido Consonni, Luca La Rocca*

- `0707.3877v1` - [abs](http://arxiv.org/abs/0707.3877v1) - [pdf](http://arxiv.org/pdf/0707.3877v1)

> Correlated proportions arise in longitudinal (panel) studies. A typical example is the ``opinion swing'' problem: ``Has the proportion of people favoring a politician changed after his recent speech to the nation on TV?''. Since the same group of individuals is interviewed before and after the speech, the two proportions are correlated. A natural null hypothesis to be tested is whether the corresponding population proportions are equal. A standard Bayesian approach to this problem has already been considered in the literature, based on a Dirichlet prior for the cell-probabilities of the underlying two-by-two table under the alternative hypothesis, together with an induced prior under the null. In lack of specific prior information, a diffuse (e.g. uniform) distribution may be used. We claim that this approach is not satisfactory, since in a testing problem one should make sure that the prior under the alternative be adequately centered around the region specified by the null, in order to obtain a fair comparison between the two hypotheses. Following an intrinsic prior methodology, we develop two strategies for the construction of a collection of objective priors increasingly peaked around the null. We provide a simple interpretation of their structure in terms of weighted imaginary sample scenarios. We illustrate our method by means of three examples, carrying out sensitivity analysis and providing comparison with existing results.

</details>

<details>

<summary>2007-07-31 16:57:51 - Smooth tail index estimation</summary>

- *Samuel Müller, Kaspar Rufibach*

- `0612140v2` - [abs](http://arxiv.org/abs/0612140v2) - [pdf](http://arxiv.org/pdf/math/0612140v2)

> Both parametric distribution functions appearing in extreme value theory - the generalized extreme value distribution and the generalized Pareto distribution - have log-concave densities if the extreme value index gamma is in [-1,0]. Replacing the order statistics in tail index estimators by their corresponding quantiles from the distribution function that is based on the estimated log-concave density leads to novel smooth quantile and tail index estimators. These new estimators aim at estimating the tail index especially in small samples. Acting as a smoother of the empirical distribution function, the log-concave distribution function estimator reduces estimation variability to a much greater extent than it introduces bias. As a consequence, Monte Carlo simulations demonstrate that the smoothed version of the estimators are well superior to their non-smoothed counterparts, in terms of mean squared error.

</details>


## 2007-08

<details>

<summary>2007-08-01 15:08:04 - Convergence rates for Bayesian density estimation of infinite-dimensional exponential families</summary>

- *Catia Scricciolo*

- `0708.0175v1` - [abs](http://arxiv.org/abs/0708.0175v1) - [pdf](http://arxiv.org/pdf/0708.0175v1)

> We study the rate of convergence of posterior distributions in density estimation problems for log-densities in periodic Sobolev classes characterized by a smoothness parameter p. The posterior expected density provides a nonparametric estimation procedure attaining the optimal minimax rate of convergence under Hellinger loss if the posterior distribution achieves the optimal rate over certain uniformity classes. A prior on the density class of interest is induced by a prior on the coefficients of the trigonometric series expansion of the log-density. We show that when p is known, the posterior distribution of a Gaussian prior achieves the optimal rate provided the prior variances die off sufficiently rapidly. For a mixture of normal distributions, the mixing weights on the dimension of the exponential family are assumed to be bounded below by an exponentially decreasing sequence. To avoid the use of infinite bases, we develop priors that cut off the series at a sample-size-dependent truncation point. When the degree of smoothness is unknown, a finite mixture of normal priors indexed by the smoothness parameter, which is also assigned a prior, produces the best rate. A rate-adaptive estimator is derived.

</details>

<details>

<summary>2007-08-02 11:00:00 - Comment: Monitoring Networked Applications With Incremental Quantile Estimation</summary>

- *Lorraine Denby, James M. Landwehr, Jean Meloche*

- `0708.0317v1` - [abs](http://arxiv.org/abs/0708.0317v1) - [pdf](http://arxiv.org/pdf/0708.0317v1)

> Comment: Monitoring Networked Applications With Incremental Quantile Estimation [arXiv:0708.0302]

</details>

<details>

<summary>2007-08-02 12:46:18 - Comment: Monitoring Networked Applications With Incremental Quantile Estimation</summary>

- *Earl Lawrence, George Michailidis, Vijayan N. Nair*

- `0708.0336v1` - [abs](http://arxiv.org/abs/0708.0336v1) - [pdf](http://arxiv.org/pdf/0708.0336v1)

> Our comments are in two parts. First, we make some observations regarding the methodology in Chambers et al. [arXiv:0708.0302]. Second, we briefly describe another interesting network monitoring problem that arises in the context of assessing quality of service, such as loss rates and delay distributions, in packet-switched networks.

</details>

<details>

<summary>2007-08-02 12:59:51 - Comment: Monitoring Networked Applications With Incremental Quantile Estimation</summary>

- *Bin Yu*

- `0708.0338v1` - [abs](http://arxiv.org/abs/0708.0338v1) - [pdf](http://arxiv.org/pdf/0708.0338v1)

> Comment: Monitoring Networked Applications With Incremental Quantile Estimation [arXiv:0708.0302]

</details>

<details>

<summary>2007-08-02 13:14:05 - Rejoinder: Monitoring Networked Applications With Incremental Quantile Estimation</summary>

- *John M. Chambers, David A. James, Diane Lambert, Scott Vander Wiel*

- `0708.0339v1` - [abs](http://arxiv.org/abs/0708.0339v1) - [pdf](http://arxiv.org/pdf/0708.0339v1)

> Rejoinder: Monitoring Networked Applications With Incremental Quantile Estimation [arXiv:0708.0302]

</details>

<details>

<summary>2007-08-02 13:23:09 - Monitoring Networked Applications With Incremental Quantile Estimation</summary>

- *John M. Chambers, David A. James, Diane Lambert, Scott Vander Wiel*

- `0708.0302v1` - [abs](http://arxiv.org/abs/0708.0302v1) - [pdf](http://arxiv.org/pdf/0708.0302v1)

> Networked applications have software components that reside on different computers. Email, for example, has database, processing, and user interface components that can be distributed across a network and shared by users in different locations or work groups. End-to-end performance and reliability metrics describe the software quality experienced by these groups of users, taking into account all the software components in the pipeline. Each user produces only some of the data needed to understand the quality of the application for the group, so group performance metrics are obtained by combining summary statistics that each end computer periodically (and automatically) sends to a central server. The group quality metrics usually focus on medians and tail quantiles rather than on averages. Distributed quantile estimation is challenging, though, especially when passing large amounts of data around the network solely to compute quality metrics is undesirable. This paper describes an Incremental Quantile (IQ) estimation method that is designed for performance monitoring at arbitrary levels of network aggregation and time resolution when only a limited amount of data can be transferred. Applications to both real and simulated data are provided.

</details>

<details>

<summary>2007-08-02 14:42:06 - Advances in Data Combination, Analysis and Collection for System Reliability Assessment</summary>

- *Alyson G. Wilson, Todd L. Graves, Michael S. Hamada, C. Shane Reese*

- `0708.0355v1` - [abs](http://arxiv.org/abs/0708.0355v1) - [pdf](http://arxiv.org/pdf/0708.0355v1)

> The systems that statisticians are asked to assess, such as nuclear weapons, infrastructure networks, supercomputer codes and munitions, have become increasingly complex. It is often costly to conduct full system tests. As such, we present a review of methodology that has been proposed for addressing system reliability with limited full system testing. The first approaches presented in this paper are concerned with the combination of multiple sources of information to assess the reliability of a single component. The second general set of methodology addresses the combination of multiple levels of data to determine system reliability. We then present developments for complex systems beyond traditional series/parallel representations through the use of Bayesian networks and flowgraph models. We also include methodological contributions to resource allocation considerations for system relability assessment. We illustrate each method with applications primarily encountered at Los Alamos National Laboratory.

</details>

<details>

<summary>2007-08-03 08:32:52 - Quantile regression with varying coefficients</summary>

- *Mi-Ok Kim*

- `0708.0471v1` - [abs](http://arxiv.org/abs/0708.0471v1) - [pdf](http://arxiv.org/pdf/0708.0471v1)

> Quantile regression provides a framework for modeling statistical quantities of interest other than the conditional mean. The regression methodology is well developed for linear models, but less so for nonparametric models. We consider conditional quantiles with varying coefficients and propose a methodology for their estimation and assessment using polynomial splines. The proposed estimators are easy to compute via standard quantile regression algorithms and a stepwise knot selection algorithm. The proposed Rao-score-type test that assesses the model against a linear model is also easy to implement. We provide asymptotic results on the convergence of the estimators and the null distribution of the test statistic. Empirical results are also provided, including an application of the methodology to forced expiratory volume (FEV) data.

</details>

<details>

<summary>2007-08-07 14:29:19 - A flexible Bayesian generalized linear model for dichotomous response data with an application to text categorization</summary>

- *Susana Eyheramendy, David Madigan*

- `0708.0959v1` - [abs](http://arxiv.org/abs/0708.0959v1) - [pdf](http://arxiv.org/pdf/0708.0959v1)

> We present a class of sparse generalized linear models that include probit and logistic regression as special cases and offer some extra flexibility. We provide an EM algorithm for learning the parameters of these models from data. We apply our method in text classification and in simulated data and show that our method outperforms the logistic and probit models and also the elastic net, in general by a substantial margin.

</details>

<details>

<summary>2007-08-08 09:31:47 - A comparison of the accuracy of saddlepoint conditional cumulative distribution function approximations</summary>

- *Juan Zhang, John E. Kolassa*

- `0708.1069v1` - [abs](http://arxiv.org/abs/0708.1069v1) - [pdf](http://arxiv.org/pdf/0708.1069v1)

> Consider a model parameterized by a scalar parameter of interest and a nuisance parameter vector. Inference about the parameter of interest may be based on the signed root of the likelihood ratio statistic R. The standard normal approximation to the conditional distribution of R typically has error of order O(n^{-1/2}), where n is the sample size. There are several modifications for R, which reduce the order of error in the approximations. In this paper, we mainly investigate Barndorff-Nielsen's modified directed likelihood ratio statistic, Severini's empirical adjustment, and DiCiccio and Martin's two modifications, involving the Bayesian approach and the conditional likelihood ratio statistic. For each modification, two formats were employed to approximate the conditional cumulative distribution function; these are Barndorff-Nielson formats and the Lugannani and Rice formats. All approximations were applied to inference on the ratio of means for two independent exponential random variables. We constructed one and two-sided hypotheses tests and used the actual sizes of the tests as the measurements of accuracy to compare those approximations.

</details>

<details>

<summary>2007-08-11 13:36:44 - Markov Chain Modelling for Reliability Estimation of Engineering Systems at Different Scales - Some Considerations</summary>

- *K. Balaji Rao*

- `0708.1566v1` - [abs](http://arxiv.org/abs/0708.1566v1) - [pdf](http://arxiv.org/pdf/0708.1566v1)

> The concepts of probability, statistics and stochastic theory are being successfully used in structural engineering. Markov Chain modelling is a simple stochastic process model that has found its application in both describing stochastic evolution of system and in system reliability estimation. The recent developments in Markov Chain Monte Carlo and the possible integration of Bayesian theory within Markov Chain theory have enhanced its application possibilities. However, the application possibility can be furthered to range over wider scales of application (perhaps from nano- to macro-) by considering the developments in Physics (in particular Quantum Physics). This paper tries to present the results of quantum physics that would help in interpretation of transition probability matrix. However, care has to be taken in the choice of densities in computing the transition probability matrix. The paper is based on available literature, and the aim is only to make an attempt to show how Markov Chain can be used to model systems at various scales.

</details>

<details>

<summary>2007-08-14 13:29:15 - Posterior convergence rates of Dirichlet mixtures at smooth densities</summary>

- *Subhashis Ghosal, Aad van der Vaart*

- `0708.1885v1` - [abs](http://arxiv.org/abs/0708.1885v1) - [pdf](http://arxiv.org/pdf/0708.1885v1)

> We study the rates of convergence of the posterior distribution for Bayesian density estimation with Dirichlet mixtures of normal distributions as the prior. The true density is assumed to be twice continuously differentiable. The bandwidth is given a sequence of priors which is obtained by scaling a single prior by an appropriate order. In order to handle this problem, we derive a new general rate theorem by considering a countable covering of the parameter space whose prior probabilities satisfy a summability condition together with certain individual bounds on the Hellinger metric entropy. We apply this new general theorem on posterior convergence rates by computing bounds for Hellinger (bracketing) entropy numbers for the involved class of densities, the error in the approximation of a smooth density by normal mixtures and the concentration rate of the prior. The best obtainable rate of convergence of the posterior turns out to be equivalent to the well-known frequentist rate for integrated mean squared error $n^{-2/5}$ up to a logarithmic factor.

</details>

<details>

<summary>2007-08-14 13:59:00 - On rates of convergence for posterior distributions in infinite-dimensional models</summary>

- *Stephen G. Walker, Antonio Lijoi, Igor Prünster*

- `0708.1892v1` - [abs](http://arxiv.org/abs/0708.1892v1) - [pdf](http://arxiv.org/pdf/0708.1892v1)

> This paper introduces a new approach to the study of rates of convergence for posterior distributions. It is a natural extension of a recent approach to the study of Bayesian consistency. In particular, we improve on current rates of convergence for models including the mixture of Dirichlet process model and the random Bernstein polynomial model.

</details>

<details>

<summary>2007-08-14 14:34:01 - On the number of support points of maximin and Bayesian optimal designs</summary>

- *Dietrich Braess, Holger Dette*

- `0708.1901v1` - [abs](http://arxiv.org/abs/0708.1901v1) - [pdf](http://arxiv.org/pdf/0708.1901v1)

> We consider maximin and Bayesian $D$-optimal designs for nonlinear regression models. The maximin criterion requires the specification of a region for the nonlinear parameters in the model, while the Bayesian optimality criterion assumes that a prior for these parameters is available. On interval parameter spaces, it was observed empirically by many authors that an increase of uncertainty in the prior information (i.e., a larger range for the parameter space in the maximin criterion or a larger variance of the prior in the Bayesian criterion) yields a larger number of support points of the corresponding optimal designs. In this paper, we present analytic tools which are used to prove this phenomenon in concrete situations. The proposed methodology can be used to explain many empirically observed results in the literature. Moreover, it explains why maximin $D$-optimal designs are usually supported at more points than Bayesian $D$-optimal designs.

</details>

<details>

<summary>2007-08-17 06:08:58 - New Dirichlet Mean Identities</summary>

- *Lancelot F. James*

- `0708.0614v2` - [abs](http://arxiv.org/abs/0708.0614v2) - [pdf](http://arxiv.org/pdf/0708.0614v2)

> An important line of research is the investigation of the laws of random variables known as Dirichlet means as discussed in Cifarelli and Regazzini(1990). However there is not much information on inter-relationships between different Dirichlet means. Here we introduce two distributional operations, which consist of multiplying a mean functional by an independent beta random variable and an operation involving an exponential change of measure. These operations identify relationships between different means and their densities. This allows one to use the often considerable analytic work to obtain results for one Dirichlet mean to obtain results for an entire family of otherwise seemingly unrelated Dirichlet means. Additionally, it allows one to obtain explicit densities for the related class of random variables that have generalized gamma convolution distributions, and the finite-dimensional distribution of their associated L\'evy processes. This has implications in, for instance, the explicit description of Bayesian nonparametric prior and posterior models, and more generally in a variety of applications in probability and statistics involving Levy processes.

</details>

<details>

<summary>2007-08-22 20:23:25 - Updating Probabilities with Data and Moments</summary>

- *Adom Giffin, Ariel Caticha*

- `0708.1593v2` - [abs](http://arxiv.org/abs/0708.1593v2) - [pdf](http://arxiv.org/pdf/0708.1593v2)

> We use the method of Maximum (relative) Entropy to process information in the form of observed data and moment constraints. The generic "canonical" form of the posterior distribution for the problem of simultaneous updating with data and moments is obtained. We discuss the general problem of non-commuting constraints, when they should be processed sequentially and when simultaneously. As an illustration, the multinomial example of die tosses is solved in detail for two superficially similar but actually very different problems.

</details>

<details>

<summary>2007-08-28 14:22:23 - Embedding Population Dynamics Models in Inference</summary>

- *Stephen T. Buckland, Ken B. Newman, Carmen Fernández, Len Thomas, John Harwood*

- `0708.3796v1` - [abs](http://arxiv.org/abs/0708.3796v1) - [pdf](http://arxiv.org/pdf/0708.3796v1)

> Increasing pressures on the environment are generating an ever-increasing need to manage animal and plant populations sustainably, and to protect and rebuild endangered populations. Effective management requires reliable mathematical models, so that the effects of management action can be predicted, and the uncertainty in these predictions quantified. These models must be able to predict the response of populations to anthropogenic change, while handling the major sources of uncertainty. We describe a simple ``building block'' approach to formulating discrete-time models. We show how to estimate the parameters of such models from time series of data, and how to quantify uncertainty in those estimates and in numbers of individuals of different types in populations, using computer-intensive Bayesian methods. We also discuss advantages and pitfalls of the approach, and give an example using the British grey seal population.

</details>

<details>

<summary>2007-08-31 13:22:31 - A statistical approach to simultaneous mapping and localization for mobile robots</summary>

- *Anita Araneda, Stephen E. Fienberg, Alvaro Soto*

- `0708.4337v1` - [abs](http://arxiv.org/abs/0708.4337v1) - [pdf](http://arxiv.org/pdf/0708.4337v1)

> Mobile robots require basic information to navigate through an environment: they need to know where they are (localization) and they need to know where they are going. For the latter, robots need a map of the environment. Using sensors of a variety of forms, robots gather information as they move through an environment in order to build a map. In this paper we present a novel sampling algorithm to solving the simultaneous mapping and localization (SLAM) problem in indoor environments. We approach the problem from a Bayesian statistics perspective. The data correspond to a set of range finder and odometer measurements, obtained at discrete time instants. We focus on the estimation of the posterior distribution over the space of possible maps given the data. By exploiting different factorizations of this distribution, we derive three sampling algorithms based on importance sampling. We illustrate the results of our approach by testing the algorithms with two real data sets obtained through robot navigation inside office buildings at Carnegie Mellon University and the Pontificia Universidad Catolica de Chile.

</details>


## 2007-09

<details>

<summary>2007-09-04 12:53:04 - Probabilistic projections of HIV prevalence using Bayesian melding</summary>

- *Leontine Alkema, Adrian E. Raftery, Samuel J. Clark*

- `0709.0421v1` - [abs](http://arxiv.org/abs/0709.0421v1) - [pdf](http://arxiv.org/pdf/0709.0421v1)

> The Joint United Nations Programme on HIV/AIDS (UNAIDS) has developed the Estimation and Projection Package (EPP) for making national estimates and short-term projections of HIV prevalence based on observed prevalence trends at antenatal clinics. Assessing the uncertainty about its estimates and projections is important for informed policy decision making, and we propose the use of Bayesian melding for this purpose. Prevalence data and other information about the EPP model's input parameters are used to derive a probabilistic HIV prevalence projection, namely a probability distribution over a set of future prevalence trajectories. We relate antenatal clinic prevalence to population prevalence and account for variability between clinics using a random effects model. Predictive intervals for clinic prevalence are derived for checking the model. We discuss predictions given by the EPP model and the results of the Bayesian melding procedure for Uganda, where prevalence peaked at around 28% in 1990; the 95% prediction interval for 2010 ranges from 2% to 7%.

</details>

<details>

<summary>2007-09-04 13:27:07 - A multivariate semiparametric Bayesian spatial modeling framework for hurricane surface wind fields</summary>

- *Brian J. Reich, Montserrat Fuentes*

- `0709.0427v1` - [abs](http://arxiv.org/abs/0709.0427v1) - [pdf](http://arxiv.org/pdf/0709.0427v1)

> Storm surge, the onshore rush of sea water caused by the high winds and low pressure associated with a hurricane, can compound the effects of inland flooding caused by rainfall, leading to loss of property and loss of life for residents of coastal areas. Numerical ocean models are essential for creating storm surge forecasts for coastal areas. These models are driven primarily by the surface wind forcings. Currently, the gridded wind fields used by ocean models are specified by deterministic formulas that are based on the central pressure and location of the storm center. While these equations incorporate important physical knowledge about the structure of hurricane surface wind fields, they cannot always capture the asymmetric and dynamic nature of a hurricane. A new Bayesian multivariate spatial statistical modeling framework is introduced combining data with physical knowledge about the wind fields to improve the estimation of the wind vectors. Many spatial models assume the data follow a Gaussian distribution. However, this may be overly-restrictive for wind fields data which often display erratic behavior, such as sudden changes in time or space. In this paper we develop a semiparametric multivariate spatial model for these data. Our model builds on the stick-breaking prior, which is frequently used in Bayesian modeling to capture uncertainty in the parametric form of an outcome. The stick-breaking prior is extended to the spatial setting by assigning each location a different, unknown distribution, and smoothing the distributions in space with a series of kernel functions. This semiparametric spatial model is shown to improve prediction compared to usual Bayesian Kriging methods for the wind field of Hurricane Ivan.

</details>

<details>

<summary>2007-09-11 01:39:20 - On Universal Prediction and Bayesian Confirmation</summary>

- *Marcus Hutter*

- `0709.1516v1` - [abs](http://arxiv.org/abs/0709.1516v1) - [pdf](http://arxiv.org/pdf/0709.1516v1)

> The Bayesian framework is a well-studied and successful framework for inductive reasoning, which includes hypothesis testing and confirmation, parameter estimation, sequence prediction, classification, and regression. But standard statistical guidelines for choosing the model class and prior are not always available or fail, in particular in complex situations. Solomonoff completed the Bayesian framework by providing a rigorous, unique, formal, and universal choice for the model class and the prior. We discuss in breadth how and in which sense universal (non-i.i.d.) sequence prediction solves various (philosophical) problems of traditional Bayesian sequence prediction. We show that Solomonoff's model possesses many desirable properties: Strong total and weak instantaneous bounds, and in contrast to most classical continuous prior densities has no zero p(oste)rior problem, i.e. can confirm universal hypotheses, is reparametrization and regrouping invariant, and avoids the old-evidence and updating problem. It even performs well (actually better) in non-computable environments.

</details>

<details>

<summary>2007-09-18 23:56:17 - Bayesian Classification and Regression with High Dimensional Features</summary>

- *Longhai Li*

- `0709.2936v1` - [abs](http://arxiv.org/abs/0709.2936v1) - [pdf](http://arxiv.org/pdf/0709.2936v1)

> This thesis responds to the challenges of using a large number, such as thousands, of features in regression and classification problems.   There are two situations where such high dimensional features arise. One is when high dimensional measurements are available, for example, gene expression data produced by microarray techniques. For computational or other reasons, people may select only a small subset of features when modelling such data, by looking at how relevant the features are to predicting the response, based on some measure such as correlation with the response in the training data. Although it is used very commonly, this procedure will make the response appear more predictable than it actually is. In Chapter 2, we propose a Bayesian method to avoid this selection bias, with application to naive Bayes models and mixture models.   High dimensional features also arise when we consider high-order interactions. The number of parameters will increase exponentially with the order considered. In Chapter 3, we propose a method for compressing a group of parameters into a single one, by exploiting the fact that many predictor variables derived from high-order interactions have the same values for all the training cases. The number of compressed parameters may have converged before considering the highest possible order. We apply this compression method to logistic sequence prediction models and logistic classification models.   We use both simulated data and real data to test our methods in both chapters.

</details>

<details>

<summary>2007-09-21 03:54:42 - On posterior distribution of Bayesian wavelet thresholding</summary>

- *Heng Lian*

- `0709.3339v1` - [abs](http://arxiv.org/abs/0709.3339v1) - [pdf](http://arxiv.org/pdf/0709.3339v1)

> We investigate the posterior rate of convergence for wavelet shrinkage using a Bayesian approach in general Besov spaces. Instead of studying the Bayesian estimator related to a particular loss function, we focus on the posterior distribution itself from a nonparametric Bayesian asymptotics point of view and study its rate of convergence. We obtain the same rate as in \citet{abramovich04} where the authors studied the convergence of several Bayesian estimators.

</details>

<details>

<summary>2007-09-21 09:18:32 - Algebraic causality: Bayes nets and beyond</summary>

- *Eva Riccomagno, Jim Q Smith*

- `0709.3377v1` - [abs](http://arxiv.org/abs/0709.3377v1) - [pdf](http://arxiv.org/pdf/0709.3377v1)

> The relationship between algebraic geometry and the inferential framework of the Bayesian Networks with hidden variables has now been fruitfully explored and exploited by a number of authors. More recently the algebraic formulation of Causal Bayesian Networks has also been investigated in this context. After reviewing these newer relationships, we proceed to demonstrate that many of the ideas embodied in the concept of a ``causal model'' can be more generally expressed directly in terms of a partial order and a family of polynomial maps. The more conventional graphical constructions, when available, remain a powerful tool.

</details>

<details>

<summary>2007-09-21 09:43:41 - The causal manipulation of chain event graphs</summary>

- *Eva Riccomagno, Jim Q. Smith*

- `0709.3380v1` - [abs](http://arxiv.org/abs/0709.3380v1) - [pdf](http://arxiv.org/pdf/0709.3380v1)

> Discrete Bayesian Networks have been very successful as a framework both for inference and for expressing certain causal hypotheses. In this paper we present a class of graphical models called the chain event graph (CEG) models, that generalises the class of discrete BN models. It provides a flexible and expressive framework for representing and analysing the implications of causal hypotheses, expressed in terms of the effects of a manipulation of the generating underlying system. We prove that, as for a BN, identifiability analyses of causal effects can be performed through examining the topology of the CEG graph, leading to theorems analogous to the back-door theorem for the BN.

</details>

<details>

<summary>2007-09-21 22:48:10 - Locally Adaptive Nonparametric Binary Regression</summary>

- *Sally Wood, Robert Kohn, Remy Cottet, Wenxin Jiang, Martin Tanner*

- `0709.3545v1` - [abs](http://arxiv.org/abs/0709.3545v1) - [pdf](http://arxiv.org/pdf/0709.3545v1)

> A nonparametric and locally adaptive Bayesian estimator is proposed for estimating a binary regression. Flexibility is obtained by modeling the binary regression as a mixture of probit regressions with the argument of each probit regression having a thin plate spline prior with its own smoothing parameter and with the mixture weights depending on the covariates. The estimator is compared to a single spline estimator and to a recently proposed locally adaptive estimator. The methodology is illustrated by applying it to both simulated and real examples.

</details>


## 2007-10

<details>

<summary>2007-10-04 18:36:25 - Information and Entropy</summary>

- *Ariel Caticha*

- `0710.1068v1` - [abs](http://arxiv.org/abs/0710.1068v1) - [pdf](http://arxiv.org/pdf/0710.1068v1)

> What is information? Is it physical? We argue that in a Bayesian theory the notion of information must be defined in terms of its effects on the beliefs of rational agents. Information is whatever constrains rational beliefs and therefore it is the force that induces us to change our minds. This problem of updating from a prior to a posterior probability distribution is tackled through an eliminative induction process that singles out the logarithmic relative entropy as the unique tool for inference. The resulting method of Maximum relative Entropy (ME), which is designed for updating from arbitrary priors given information in the form of arbitrary constraints, includes as special cases both MaxEnt (which allows arbitrary constraints) and Bayes' rule (which allows arbitrary priors). Thus, ME unifies the two themes of these workshops -- the Maximum Entropy and the Bayesian methods -- into a single general inference scheme that allows us to handle problems that lie beyond the reach of either of the two methods separately. I conclude with a couple of simple illustrative examples.

</details>

<details>

<summary>2007-10-11 13:46:14 - Size, power and false discovery rates</summary>

- *Bradley Efron*

- `0710.2245v1` - [abs](http://arxiv.org/abs/0710.2245v1) - [pdf](http://arxiv.org/pdf/0710.2245v1)

> Modern scientific technology has provided a new class of large-scale simultaneous inference problems, with thousands of hypothesis tests to consider at the same time. Microarrays epitomize this type of technology, but similar situations arise in proteomics, spectroscopy, imaging, and social science surveys. This paper uses false discovery rate methods to carry out both size and power calculations on large-scale problems. A simple empirical Bayes approach allows the false discovery rate (fdr) analysis to proceed with a minimum of frequentist or Bayesian modeling assumptions. Closed-form accuracy formulas are derived for estimated false discovery rates, and used to compare different methodologies: local or tail-area fdr's, theoretical, permutation, or empirical null hypothesis estimates. Two microarray data sets as well as simulations are used to evaluate the methodology, the power diagnostics showing why nonnull cases might easily fail to appear on a list of ``significant'' discoveries.

</details>

<details>

<summary>2007-10-18 08:48:04 - Bayesian variable selection for high dimensional generalized linear models: convergence rates of the fitted densities</summary>

- *Wenxin Jiang*

- `0710.3458v1` - [abs](http://arxiv.org/abs/0710.3458v1) - [pdf](http://arxiv.org/pdf/0710.3458v1)

> Bayesian variable selection has gained much empirical success recently in a variety of applications when the number $K$ of explanatory variables $(x_1,...,x_K)$ is possibly much larger than the sample size $n$. For generalized linear models, if most of the $x_j$'s have very small effects on the response $y$, we show that it is possible to use Bayesian variable selection to reduce overfitting caused by the curse of dimensionality $K\gg n$. In this approach a suitable prior can be used to choose a few out of the many $x_j$'s to model $y$, so that the posterior will propose probability densities $p$ that are ``often close'' to the true density $p^*$ in some sense. The closeness can be described by a Hellinger distance between $p$ and $p^*$ that scales at a power very close to $n^{-1/2}$, which is the ``finite-dimensional rate'' corresponding to a low-dimensional situation. These findings extend some recent work of Jiang [Technical Report 05-02 (2005) Dept. Statistics, Northwestern Univ.] on consistency of Bayesian variable selection for binary classification.

</details>

<details>

<summary>2007-10-19 10:10:52 - Regression for partially observed variables and nonparametric quantiles of conditional probabilities</summary>

- *Odile Pons*

- `0710.3666v1` - [abs](http://arxiv.org/abs/0710.3666v1) - [pdf](http://arxiv.org/pdf/0710.3666v1)

> Efficient estimation under bias sampling, censoring or truncation is a difficult question which has been partially answered and the usual estimators are not always consistent. Several biased designs are considered for models with variables $(X,Y)$ where $Y$ is an indicator and $X$ an explanatory variable, or for continuous variables $(X,Y)$. The identifiability of the models are discussed. New nonparametric estimators of the regression functions and conditional quantiles are proposed.

</details>

<details>

<summary>2007-10-19 11:33:56 - Bayesian inference with rescaled Gaussian process priors</summary>

- *Aad van der Vaart, Harry van Zanten*

- `0710.3679v1` - [abs](http://arxiv.org/abs/0710.3679v1) - [pdf](http://arxiv.org/pdf/0710.3679v1)

> We use rescaled Gaussian processes as prior models for functional parameters in nonparametric statistical models. We show how the rate of contraction of the posterior distributions depends on the scaling factor. In particular, we exhibit rescaled Gaussian process priors yielding posteriors that contract around the true parameter at optimal convergence rates. To derive our results we establish bounds on small deviation probabilities for smooth stationary Gaussian processes.

</details>

<details>

<summary>2007-10-19 17:18:30 - Bayesian Online Changepoint Detection</summary>

- *Ryan Prescott Adams, David J. C. MacKay*

- `0710.3742v1` - [abs](http://arxiv.org/abs/0710.3742v1) - [pdf](http://arxiv.org/pdf/0710.3742v1)

> Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.

</details>

<details>

<summary>2007-10-23 10:57:15 - Stability of the Gibbs Sampler for Bayesian Hierarchical Models</summary>

- *Omiros Papaspiliopoulos, Gareth Roberts*

- `0710.4234v1` - [abs](http://arxiv.org/abs/0710.4234v1) - [pdf](http://arxiv.org/pdf/0710.4234v1)

> We characterise the convergence of the Gibbs sampler which samples from the joint posterior distribution of parameters and missing data in hierarchical linear models with arbitrary symmetric error distributions. We show that the convergence can be uniform, geometric or sub-geometric depending on the relative tail behaviour of the error distributions, and on the parametrisation chosen. Our theory is applied to characterise the convergence of the Gibbs sampler on latent Gaussian process models. We indicate how the theoretical framework we introduce will be useful in analyzing more complex models.

</details>

<details>

<summary>2007-10-25 11:19:59 - A Bayesian Hierarchical Model for the Analysis of a Longitudinal Dynamic Contrast-Enhanced MRI Cancer Study</summary>

- *Volker J. Schmid, Brandon Whitcher, Anwar R. Padhani, N. Jane Taylor, Guang-Zhong Yang*

- `0710.4788v1` - [abs](http://arxiv.org/abs/0710.4788v1) - [pdf](http://arxiv.org/pdf/0710.4788v1)

> Imaging in clinical oncology trials provides a wealth of information that contributes to the drug development process, especially in early phase studies. This paper focuses on kinetic modeling in DCE-MRI, inspired by mixed-effects models that are frequently used in the analysis of clinical trials. Instead of summarizing each scanning session as a single kinetic parameter -- such as median $\ktrans$ across all voxels in the tumor ROI -- we propose to analyze all voxel time courses from all scans and across all subjects simultaneously in a single model. The kinetic parameters from the usual non-linear regression model are decomposed into unique components associated with factors from the longitudinal study; e.g., treatment, patient and voxel effects. A Bayesian hierarchical model provides the framework in order to construct a data model, a parameter model, as well as prior distributions. The posterior distribution of the kinetic parameters is estimated using Markov chain Monte Carlo (MCMC) methods. Hypothesis testing at the study level for an overall treatment effect is straightforward and the patient- and voxel-level parameters capture random effects that provide additional information at various levels of resolution to allow a thorough evaluation of the clinical trial. The proposed method is validated with a breast cancer study, where the subjects were imaged before and after two cycles of chemotherapy, demonstrating the clinical potential of this method to longitudinal oncology studies.

</details>

<details>

<summary>2007-10-25 14:38:18 - Bayesian sequential change diagnosis</summary>

- *Savas Dayanik, Christian Goulding, H. Vincent Poor*

- `0710.4847v1` - [abs](http://arxiv.org/abs/0710.4847v1) - [pdf](http://arxiv.org/pdf/0710.4847v1)

> Sequential change diagnosis is the joint problem of detection and identification of a sudden and unobservable change in the distribution of a random sequence. In this problem, the common probability law of a sequence of i.i.d. random variables suddenly changes at some disorder time to one of finitely many alternatives. This disorder time marks the start of a new regime, whose fingerprint is the new law of observations. Both the disorder time and the identity of the new regime are unknown and unobservable. The objective is to detect the regime-change as soon as possible, and, at the same time, to determine its identity as accurately as possible. Prompt and correct diagnosis is crucial for quick execution of the most appropriate measures in response to the new regime, as in fault detection and isolation in industrial processes, and target detection and identification in national defense. The problem is formulated in a Bayesian framework. An optimal sequential decision strategy is found, and an accurate numerical scheme is described for its implementation. Geometrical properties of the optimal strategy are illustrated via numerical examples. The traditional problems of Bayesian change-detection and Bayesian sequential multi-hypothesis testing are solved as special cases. In addition, a solution is obtained for the problem of detection and identification of component failure(s) in a system with suspended animation.

</details>

<details>

<summary>2007-10-26 08:20:36 - Struggles with Survey Weighting and Regression Modeling</summary>

- *Andrew Gelman*

- `0710.5005v1` - [abs](http://arxiv.org/abs/0710.5005v1) - [pdf](http://arxiv.org/pdf/0710.5005v1)

> The general principles of Bayesian data analysis imply that models for survey responses should be constructed conditional on all variables that affect the probability of inclusion and nonresponse, which are also the variables used in survey weighting and clustering. However, such models can quickly become very complicated, with potentially thousands of poststratification cells. It is then a challenge to develop general families of multilevel probability models that yield reasonable Bayesian inferences. We discuss in the context of several ongoing public health and social surveys. This work is currently open-ended, and we conclude with thoughts on how research could proceed to solve these problems.

</details>

<details>

<summary>2007-10-31 14:55:24 - Estimating exposure response functions using ambient pollution concentrations</summary>

- *Gavin Shaddick, Duncan Lee, James V. Zidek, Ruth Salway*

- `0710.5805v1` - [abs](http://arxiv.org/abs/0710.5805v1) - [pdf](http://arxiv.org/pdf/0710.5805v1)

> This paper presents an approach to estimating the health effects of an environmental hazard. The approach is general in nature, but is applied here to the case of air pollution. It uses a computer model involving ambient pollution and temperature inputs, to simulate the exposures experienced by individuals in an urban area, whilst incorporating the mechanisms that determine exposures. The output from the model comprises a set of daily exposures for a sample of individuals from the population of interest. These daily exposures are approximated by parametric distributions, so that the predictive exposure distribution of a randomly selected individual can be generated. These distributions are then incorporated into a hierarchical Bayesian framework (with inference using Markov Chain Monte Carlo simulation) in order to examine the relationship between short-term changes in exposures and health outcomes, whilst making allowance for long-term trends, seasonality, the effect of potential confounders and the possibility of ecological bias.   The paper applies this approach to particulate pollution (PM$_{10}$) and respiratory mortality counts for seniors in greater London ($\geq$65 years) during 1997. Within this substantive epidemiological study, the effects on health of ambient concentrations and (estimated) personal exposures are compared.

</details>


## 2007-11

<details>

<summary>2007-11-01 18:57:06 - Population-Based Reversible Jump Markov Chain Monte Carlo</summary>

- *Ajay Jasra, David A. Stephens, Chris C. Holmes*

- `0711.0186v1` - [abs](http://arxiv.org/abs/0711.0186v1) - [pdf](http://arxiv.org/pdf/0711.0186v1)

> In this paper we present an extension of population-based Markov chain Monte Carlo (MCMC) to the trans-dimensional case. One of the main challenges in MCMC-based inference is that of simulating from high and trans-dimensional target measures. In such cases, MCMC methods may not adequately traverse the support of the target; the simulation results will be unreliable. We develop population methods to deal with such problems, and give a result proving the uniform ergodicity of these population algorithms, under mild assumptions. This result is used to demonstrate the superiority, in terms of convergence rate, of a population transition kernel over a reversible jump sampler for a Bayesian variable selection problem. We also give an example of a population algorithm for a Bayesian multivariate mixture model with an unknown number of components. This is applied to gene expression data of 1000 data points in six dimensions and it is demonstrated that our algorithm out performs some competing Markov chain samplers.

</details>

<details>

<summary>2007-11-03 14:40:47 - Bayesian finite mixtures: a note on prior specification and posterior computation</summary>

- *Agostino Nobile*

- `0711.0458v1` - [abs](http://arxiv.org/abs/0711.0458v1) - [pdf](http://arxiv.org/pdf/0711.0458v1)

> A new method for the computation of the posterior distribution of the number k of components in a finite mixture is presented. Two aspects of prior specification are also studied: an argument is made for the use of a Poisson(1) distribution as the prior for k; and methods are given for the selection of hyperparameter values in the mixture of normals model, with natural conjugate priors on the components parameters.

</details>

<details>

<summary>2007-11-06 13:53:00 - Bayesian nonparametric estimation of the spectral density of a long memory Gaussian time series</summary>

- *Judith Rousseau, Brunero Liseo*

- `0711.0876v1` - [abs](http://arxiv.org/abs/0711.0876v1) - [pdf](http://arxiv.org/pdf/0711.0876v1)

> Let $\mathbf {X}=\{X_t, t=1,2,... \}$ be a stationary Gaussian random process, with mean $EX_t=\mu$ and covariance function $\gamma(\tau)=E(X_t-\mu)(X_{t+\tau}-\mu)$. Let $f(\lambda)$ be the corresponding spectral density; a stationary Gaussian process is said to be long-range dependent, if the spectral density $f(\lambda)$ can be written as the product of a slowly varying function $\tilde{f}(\lambda)$ and the quantity $\lambda ^{-2d}$. In this paper we propose a novel Bayesian nonparametric approach to the estimation of the spectral density of $\mathbf {X}$. We prove that, under some specific assumptions on the prior distribution, our approach assures posterior consistency both when $f(\cdot)$ and $d$ are the objects of interest. The rate of convergence of the posterior sequence depends in a significant way on the structure of the prior; we provide some general results and also consider the fractionally exponential (FEXP) family of priors (see below). Since it has not a well founded justification in the long memory set-up, we avoid using the Whittle approximation to the likelihood function and prefer to use the true Gaussian likelihood.

</details>

<details>

<summary>2007-11-12 21:58:17 - Dated ancestral trees from binary trait data and its application to the diversification of languages</summary>

- *Geoff K. Nicholls, Russell D. Gray*

- `0711.1874v1` - [abs](http://arxiv.org/abs/0711.1874v1) - [pdf](http://arxiv.org/pdf/0711.1874v1)

> Binary trait data record the presence or absence of distinguishing traits in individuals. We treat the problem of estimating ancestral trees with time depth from binary trait data. Simple analysis of such data is problematic. Each homology class of traits has a unique birth event on the tree, and the birth event of a trait visible at the leaves is biased towards the leaves. We propose a model-based analysis of such data, and present an MCMC algorithm that can sample from the resulting posterior distribution. Our model is based on using a birth-death process for the evolution of the elements of sets of traits. Our analysis correctly accounts for the removal of singleton traits, which are commonly discarded in real data sets. We illustrate Bayesian inference for two binary-trait data sets which arise in historical linguistics. The Bayesian approach allows for the incorporation of information from ancestral languages. The marginal prior distribution of the root time is uniform. We present a thorough analysis of the robustness of our results to model mispecification, through analysis of predictive distributions for external data, and fitting data simulated under alternative observation models. The reconstructed ages of tree nodes are relatively robust, whilst posterior probabilities for topology are not reliable.

</details>

<details>

<summary>2007-11-13 13:27:54 - Functional limit laws for the increments of the quantile process; with applications</summary>

- *Vivian Viallon*

- `0612260v3` - [abs](http://arxiv.org/abs/0612260v3) - [pdf](http://arxiv.org/pdf/math/0612260v3)

> We establish a functional limit law of the logarithm for the increments of the normed quantile process based upon a random sample of size $n\to\infty$. We extend a limit law obtained by Deheuvels and Mason (12), showing that their results hold uniformly over the bandwidth $h$, restricted to vary in $[h'_n,h''_n]$, where $\{h'_n\}_{n\geq1}$ and $\{h''_n\}_{n\geq 1}$ are appropriate non-random sequences. We treat the case where the sample observations follow possibly non-uniform distributions. As a consequence of our theorems, we provide uniform limit laws for nearest-neighbor density estimators, in the spirit of those given by Deheuvels and Mason (13) for kernel-type estimators.

</details>

<details>

<summary>2007-11-23 16:48:11 - MCMC Inference for a Model with Sampling Bias: An Illustration using SAGE data</summary>

- *Russell Zaretzki, Michael A. Gilchrist, William M. Briggs, Artin Armagan*

- `0711.3765v1` - [abs](http://arxiv.org/abs/0711.3765v1) - [pdf](http://arxiv.org/pdf/0711.3765v1)

> This paper explores Bayesian inference for a biased sampling model in situations where the population of interest cannot be sampled directly, but rather through an indirect and inherently biased method. Observations are viewed as being the result of a multinomial sampling process from a tagged population which is, in turn, a biased sample from the original population of interest. This paper presents several Gibbs Sampling techniques to estimate the joint posterior distribution of the original population based on the observed counts of the tagged population. These algorithms efficiently sample from the joint posterior distribution of a very large multinomial parameter vector. Samples from this method can be used to generate both joint and marginal posterior inferences. We also present an iterative optimization procedure based upon the conditional distributions of the Gibbs Sampler which directly computes the mode of the posterior distribution. To illustrate our approach, we apply it to a tagged population of messanger RNAs (mRNA) generated using a common high-throughput technique, Serial Analysis of Gene Expression (SAGE). Inferences for the mRNA expression levels in the yeast Saccharomyces cerevisiae are reported.

</details>

<details>

<summary>2007-11-27 10:21:54 - Conjunctive Bayesian networks</summary>

- *Niko Beerenwinkel, Nicholas Eriksson, Bernd Sturmfels*

- `0608417v4` - [abs](http://arxiv.org/abs/0608417v4) - [pdf](http://arxiv.org/pdf/math/0608417v4)

> Conjunctive Bayesian networks (CBNs) are graphical models that describe the accumulation of events which are constrained in the order of their occurrence. A CBN is given by a partial order on a (finite) set of events. CBNs generalize the oncogenetic tree models of Desper et al. by allowing the occurrence of an event to depend on more than one predecessor event. The present paper studies the statistical and algebraic properties of CBNs. We determine the maximum likelihood parameters and present a combinatorial solution to the model selection problem. Our method performs well on two datasets where the events are HIV mutations associated with drug resistance. Concluding with a study of the algebraic properties of CBNs, we show that CBNs are toric varieties after a coordinate transformation and that their ideals possess a quadratic Gr\"{o}bner basis.

</details>

<details>

<summary>2007-11-28 08:43:41 - Bahadur--Kiefer theory for sample quantiles of weakly dependent linear processes</summary>

- *Rafał Kulik*

- `0605282v2` - [abs](http://arxiv.org/abs/0605282v2) - [pdf](http://arxiv.org/pdf/math/0605282v2)

> In this paper, we establish the Bahadur--Kiefer representation for sample quantiles for a class of weakly dependent linear processes. The rate of approximation is the same as for i.i.d. sequences and is thus optimal.

</details>

<details>

<summary>2007-11-29 19:39:19 - Fast estimation of multivariate stochastic volatility</summary>

- *Kostas Triantafyllopoulos, Giovanni Montana*

- `0708.4376v2` - [abs](http://arxiv.org/abs/0708.4376v2) - [pdf](http://arxiv.org/pdf/0708.4376v2)

> In this paper we develop a Bayesian procedure for estimating multivariate stochastic volatility (MSV) using state space models. A multiplicative model based on inverted Wishart and multivariate singular beta distributions is proposed for the evolution of the volatility, and a flexible sequential volatility updating is employed. Being computationally fast, the resulting estimation procedure is particularly suitable for on-line forecasting. Three performance measures are discussed in the context of model selection: the log-likelihood criterion, the mean of standardized one-step forecast errors, and sequential Bayes factors. Finally, the proposed methods are applied to a data set comprising eight exchange rates vis-a-vis the US dollar.

</details>

<details>

<summary>2007-11-30 17:24:41 - A Method for Compressing Parameters in Bayesian Models with Application to Logistic Sequence Prediction Models</summary>

- *Longhai Li, Radford M. Neal*

- `0711.4983v1` - [abs](http://arxiv.org/abs/0711.4983v1) - [pdf](http://arxiv.org/pdf/0711.4983v1)

> Bayesian classification and regression with high order interactions is largely infeasible because Markov chain Monte Carlo (MCMC) would need to be applied with a great many parameters, whose number increases rapidly with the order. In this paper we show how to make it feasible by effectively reducing the number of parameters, exploiting the fact that many interactions have the same values for all training cases. Our method uses a single ``compressed'' parameter to represent the sum of all parameters associated with a set of patterns that have the same value for all training cases. Using symmetric stable distributions as the priors of the original parameters, we can easily find the priors of these compressed parameters. We therefore need to deal only with a much smaller number of compressed parameters when training the model with MCMC. The number of compressed parameters may have converged before considering the highest possible order. After training the model, we can split these compressed parameters into the original ones as needed to make predictions for test cases. We show in detail how to compress parameters for logistic sequence prediction models. Experiments on both simulated and real data demonstrate that a huge number of parameters can indeed be reduced by our compression method.

</details>


## 2007-12

<details>

<summary>2007-12-03 13:49:36 - Pac-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning</summary>

- *Olivier Catoni*

- `0712.0248v1` - [abs](http://arxiv.org/abs/0712.0248v1) - [pdf](http://arxiv.org/pdf/0712.0248v1)

> This monograph deals with adaptive supervised classification, using tools borrowed from statistical mechanics and information theory, stemming from the PACBayesian approach pioneered by David McAllester and applied to a conception of statistical learning theory forged by Vladimir Vapnik. Using convex analysis on the set of posterior probability measures, we show how to get local measures of the complexity of the classification model involving the relative entropy of posterior distributions with respect to Gibbs posterior measures. We then discuss relative bounds, comparing the generalization error of two classification rules, showing how the margin assumption of Mammen and Tsybakov can be replaced with some empirical measure of the covariance structure of the classification model.We show how to associate to any posterior distribution an effective temperature relating it to the Gibbs prior distribution with the same level of expected error rate, and how to estimate this effective temperature from data, resulting in an estimator whose expected error rate converges according to the best possible power of the sample size adaptively under any margin and parametric complexity assumptions. We describe and study an alternative selection scheme based on relative bounds between estimators, and present a two step localization technique which can handle the selection of a parametric model from a family of those. We show how to extend systematically all the results obtained in the inductive setting to transductive learning, and use this to improve Vapnik's generalization bounds, extending them to the case when the sample is made of independent non-identically distributed pairs of patterns and labels. Finally we review briefly the construction of Support Vector Machines and show how to derive generalization bounds for them, measuring the complexity either through the number of support vectors or through the value of the transductive or inductive margin.

</details>

<details>

<summary>2007-12-06 10:50:15 - On optimality of Bayesian testimation in the normal means problem</summary>

- *Felix Abramovich, Vadim Grinshtein, Marianna Pensky*

- `0712.0904v1` - [abs](http://arxiv.org/abs/0712.0904v1) - [pdf](http://arxiv.org/pdf/0712.0904v1)

> We consider a problem of recovering a high-dimensional vector $\mu$ observed in white noise, where the unknown vector $\mu$ is assumed to be sparse. The objective of the paper is to develop a Bayesian formalism which gives rise to a family of $l_0$-type penalties. The penalties are associated with various choices of the prior distributions $\pi_n(\cdot)$ on the number of nonzero entries of $\mu$ and, hence, are easy to interpret. The resulting Bayesian estimators lead to a general thresholding rule which accommodates many of the known thresholding and model selection procedures as particular cases corresponding to specific choices of $\pi_n(\cdot)$. Furthermore, they achieve optimality in a rather general setting under very mild conditions on the prior. We also specify the class of priors $\pi_n(\cdot)$ for which the resulting estimator is adaptively optimal (in the minimax sense) for a wide range of sparse sequences and consider several examples of such priors.

</details>

<details>

<summary>2007-12-13 10:38:24 - Describing disability through individual-level mixture models for multivariate binary data</summary>

- *Elena A. Erosheva, Stephen E. Fienberg, Cyrille Joutard*

- `0712.2124v1` - [abs](http://arxiv.org/abs/0712.2124v1) - [pdf](http://arxiv.org/pdf/0712.2124v1)

> Data on functional disability are of widespread policy interest in the United States, especially with respect to planning for Medicare and Social Security for a growing population of elderly adults. We consider an extract of functional disability data from the National Long Term Care Survey (NLTCS) and attempt to develop disability profiles using variations of the Grade of Membership (GoM) model. We first describe GoM as an individual-level mixture model that allows individuals to have partial membership in several mixture components simultaneously. We then prove the equivalence between individual-level and population-level mixture models, and use this property to develop a Markov Chain Monte Carlo algorithm for Bayesian estimation of the model. We use our approach to analyze functional disability data from the NLTCS.

</details>

<details>

<summary>2007-12-14 07:52:35 - Bayesian variable selection and data integration for biological regulatory networks</summary>

- *Shane T. Jensen, Guang Chen, Christian J. Stoeckert, Jr*

- `0610034v4` - [abs](http://arxiv.org/abs/0610034v4) - [pdf](http://arxiv.org/pdf/math/0610034v4)

> A substantial focus of research in molecular biology are gene regulatory networks: the set of transcription factors and target genes which control the involvement of different biological processes in living cells. Previous statistical approaches for identifying gene regulatory networks have used gene expression data, ChIP binding data or promoter sequence data, but each of these resources provides only partial information. We present a Bayesian hierarchical model that integrates all three data types in a principled variable selection framework. The gene expression data are modeled as a function of the unknown gene regulatory network which has an informed prior distribution based upon both ChIP binding and promoter sequence data. We also present a variable weighting methodology for the principled balancing of multiple sources of prior information. We apply our procedure to the discovery of gene regulatory relationships in Saccharomyces cerevisiae (Yeast) for which we can use several external sources of information to validate our results. Our inferred relationships show greater biological relevance on the external validation measures than previous data integration methods. Our model also estimates synergistic and antagonistic interactions between transcription factors, many of which are validated by previous studies. We also evaluate the results from our procedure for the weighting for multiple sources of prior information. Finally, we discuss our methodology in the context of previous approaches to data integration and Bayesian variable selection.

</details>

