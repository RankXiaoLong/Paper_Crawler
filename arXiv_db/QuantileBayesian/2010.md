# 2010

## TOC

- [2010-01](#2010-01)
- [2010-02](#2010-02)
- [2010-03](#2010-03)
- [2010-04](#2010-04)
- [2010-05](#2010-05)
- [2010-06](#2010-06)
- [2010-07](#2010-07)
- [2010-08](#2010-08)
- [2010-09](#2010-09)
- [2010-10](#2010-10)
- [2010-11](#2010-11)
- [2010-12](#2010-12)

## 2010-01

<details>

<summary>2010-01-01 16:35:29 - Bayesian nonparametric analysis for a species sampling model with finitely many types</summary>

- *Annalisa Cerquetti*

- `1001.0245v1` - [abs](http://arxiv.org/abs/1001.0245v1) - [pdf](http://arxiv.org/pdf/1001.0245v1)

> We derive explicit Bayesian nonparametric analysis for a species sampling model with finitely many types of Gibbs form of type $\alpha= -1$ recently introduced in Gnedin (2009). Our results complement existing analysis under Gibbs priors of type $\alpha \in [0, 1)$ proposed in Lijoi et al. (2008). Calculations rely on a groups sequential construction of Gibbs partitions introduced in Cerquetti (2008).

</details>

<details>

<summary>2010-01-04 17:46:54 - Bayesian orthogonal component analysis for sparse representation</summary>

- *Nicolas Dobigeon, Jean-Yves Tourneret*

- `0908.4489v3` - [abs](http://arxiv.org/abs/0908.4489v3) - [pdf](http://arxiv.org/pdf/0908.4489v3)

> This paper addresses the problem of identifying a lower dimensional space where observed data can be sparsely represented. This under-complete dictionary learning task can be formulated as a blind separation problem of sparse sources linearly mixed with an unknown orthogonal mixing matrix. This issue is formulated in a Bayesian framework. First, the unknown sparse sources are modeled as Bernoulli-Gaussian processes. To promote sparsity, a weighted mixture of an atom at zero and a Gaussian distribution is proposed as prior distribution for the unobserved sources. A non-informative prior distribution defined on an appropriate Stiefel manifold is elected for the mixing matrix. The Bayesian inference on the unknown parameters is conducted using a Markov chain Monte Carlo (MCMC) method. A partially collapsed Gibbs sampler is designed to generate samples asymptotically distributed according to the joint posterior distribution of the unknown model parameters and hyperparameters. These samples are then used to approximate the joint maximum a posteriori estimator of the sources and mixing matrix. Simulations conducted on synthetic data are reported to illustrate the performance of the method for recovering sparse representations. An application to sparse coding on under-complete dictionary is finally investigated.

</details>

<details>

<summary>2010-01-08 05:21:40 - Sparse Empirical Bayes Analysis (SEBA)</summary>

- *Natalia Bochkina, Ya'acov Ritov*

- `0911.5482v2` - [abs](http://arxiv.org/abs/0911.5482v2) - [pdf](http://arxiv.org/pdf/0911.5482v2)

> We consider a joint processing of $n$ independent sparse regression problems. Each is based on a sample $(y_{i1},x_{i1})...,(y_{im},x_{im})$ of $m$ \iid observations from $y_{i1}=x_{i1}\t\beta_i+\eps_{i1}$, $y_{i1}\in \R$, $x_{i 1}\in\R^p$, $i=1,...,n$, and $\eps_{i1}\dist N(0,\sig^2)$, say. $p$ is large enough so that the empirical risk minimizer is not consistent. We consider three possible extensions of the lasso estimator to deal with this problem, the lassoes, the group lasso and the RING lasso, each utilizing a different assumption how these problems are related. For each estimator we give a Bayesian interpretation, and we present both persistency analysis and non-asymptotic error bounds based on restricted eigenvalue - type assumptions.

</details>

<details>

<summary>2010-01-11 09:52:24 - Rates of convergence for the posterior distributions of mixtures of Betas and adaptive nonparametric estimation of the density</summary>

- *Judith Rousseau*

- `1001.1615v1` - [abs](http://arxiv.org/abs/1001.1615v1) - [pdf](http://arxiv.org/pdf/1001.1615v1)

> In this paper, we investigate the asymptotic properties of nonparametric Bayesian mixtures of Betas for estimating a smooth density on $[0,1]$. We consider a parametrization of Beta distributions in terms of mean and scale parameters and construct a mixture of these Betas in the mean parameter, while putting a prior on this scaling parameter. We prove that such Bayesian nonparametric models have good frequentist asymptotic properties. We determine the posterior rate of concentration around the true density and prove that it is the minimax rate of concentration when the true density belongs to a H\"{o}lder class with regularity $\beta$, for all positive $\beta$, leading to a minimax adaptive estimating procedure of the density. We also believe that the approximating results obtained on these mixtures of Beta densities can be of interest in a frequentist framework.

</details>

<details>

<summary>2010-01-11 10:31:49 - Some nonasymptotic results on resampling in high dimension, I: Confidence regions, II: Multiple tests</summary>

- *Sylvain Arlot, Gilles Blanchard, Etienne Roquain*

- `0712.0775v3` - [abs](http://arxiv.org/abs/0712.0775v3) - [pdf](http://arxiv.org/pdf/0712.0775v3)

> We study generalized bootstrap confidence regions for the mean of a random vector whose coordinates have an unknown dependency structure. The random vector is supposed to be either Gaussian or to have a symmetric and bounded distribution. The dimensionality of the vector can possibly be much larger than the number of observations and we focus on a nonasymptotic control of the confidence level, following ideas inspired by recent results in learning theory. We consider two approaches, the first based on a concentration principle (valid for a large class of resampling weights) and the second on a resampled quantile, specifically using Rademacher weights. Several intermediate results established in the approach based on concentration principles are of interest in their own right. We also discuss the question of accuracy when using Monte Carlo approximations of the resampled quantities.

</details>

<details>

<summary>2010-01-12 07:58:12 - Bayesian analysis in moment inequality models</summary>

- *Yuan Liao, Wenxin Jiang*

- `1001.1810v1` - [abs](http://arxiv.org/abs/1001.1810v1) - [pdf](http://arxiv.org/pdf/1001.1810v1)

> This paper presents a study of the large-sample behavior of the posterior distribution of a structural parameter which is partially identified by moment inequalities. The posterior density is derived based on the limited information likelihood. The posterior distribution converges to zero exponentially fast on any $\delta$-contraction outside the identified region. Inside, it is bounded below by a positive constant if the identified region is assumed to have a nonempty interior. Our simulation evidence indicates that the Bayesian approach has advantages over frequentist methods, in the sense that, with a proper choice of the prior, the posterior provides more information about the true parameter inside the identified region. We also address the problem of moment and model selection. Our optimality criterion is the maximum posterior procedure and we show that, asymptotically, it selects the true moment/model combination with the most moment inequalities and the simplest model.

</details>

<details>

<summary>2010-01-13 13:14:33 - Rate of convergence of predictive distributions for dependent data</summary>

- *Patrizia Berti, Irene Crimaldi, Luca Pratelli, Pietro Rigo*

- `1001.2152v1` - [abs](http://arxiv.org/abs/1001.2152v1) - [pdf](http://arxiv.org/pdf/1001.2152v1)

> This paper deals with empirical processes of the type \[C_n(B)=\sqrt{n}\{\mu_n(B)-P(X_{n+1}\in B\mid X_1,...,X_n)\},\] where $(X_n)$ is a sequence of random variables and $\mu_n=(1/n)\sum_{i=1}^n\delta_{X_i}$ the empirical measure. Conditions for $\sup_B|C_n(B)|$ to converge stably (in particular, in distribution) are given, where $B$ ranges over a suitable class of measurable sets. These conditions apply when $(X_n)$ is exchangeable or, more generally, conditionally identically distributed (in the sense of Berti et al. [Ann. Probab. 32 (2004) 2029--2052]). By such conditions, in some relevant situations, one obtains that $\sup_B|C_n(B)|\stackrel{P}{\to}0$ or even that $\sqrt{n}\sup_B|C_n(B)|$ converges a.s. Results of this type are useful in Bayesian statistics.

</details>

<details>

<summary>2010-01-18 01:10:17 - A Monte Carlo Algorithm for Universally Optimal Bayesian Sequence Prediction and Planning</summary>

- *Anthony Di Franco*

- `1001.2813v1` - [abs](http://arxiv.org/abs/1001.2813v1) - [pdf](http://arxiv.org/pdf/1001.2813v1)

> The aim of this work is to address the question of whether we can in principle design rational decision-making agents or artificial intelligences embedded in computable physics such that their decisions are optimal in reasonable mathematical senses. Recent developments in rare event probability estimation, recursive bayesian inference, neural networks, and probabilistic planning are sufficient to explicitly approximate reinforcement learners of the AIXI style with non-trivial model classes (here, the class of resource-bounded Turing machines). Consideration of the effects of resource limitations in a concrete implementation leads to insights about possible architectures for learning systems using optimal decision makers as components.

</details>

<details>

<summary>2010-01-18 14:41:24 - Harold Jeffreys's Theory of Probability Revisited</summary>

- *Christian P. Robert, Nicolas Chopin, Judith Rousseau*

- `0804.3173v7` - [abs](http://arxiv.org/abs/0804.3173v7) - [pdf](http://arxiv.org/pdf/0804.3173v7)

> Published exactly seventy years ago, Jeffreys's Theory of Probability (1939) has had a unique impact on the Bayesian community and is now considered to be one of the main classics in Bayesian Statistics as well as the initiator of the objective Bayes school. In particular, its advances on the derivation of noninformative priors as well as on the scaling of Bayes factors have had a lasting impact on the field. However, the book reflects the characteristics of the time, especially in terms of mathematical rigor. In this paper we point out the fundamental aspects of this reference work, especially the thorough coverage of testing problems and the construction of both estimation and testing noninformative priors based on functional divergences. Our major aim here is to help modern readers in navigating in this difficult text and in concentrating on passages that are still relevant today.

</details>

<details>

<summary>2010-01-19 10:30:29 - Bayesian Thought in Early Modern Detective Stories: Monsieur Lecoq, C. Auguste Dupin and Sherlock Holmes</summary>

- *Joseph B. Kadane*

- `1001.3253v1` - [abs](http://arxiv.org/abs/1001.3253v1) - [pdf](http://arxiv.org/pdf/1001.3253v1)

> This paper reviews the maxims used by three early modern fictional detectives: Monsieur Lecoq, C. Auguste Dupin and Sherlock Holmes. It find similarities between these maxims and Bayesian thought. Poe's Dupin uses ideas very similar to Bayesian game theory. Sherlock Holmes' statements also show thought patterns justifiable in Bayesian terms.

</details>

<details>

<summary>2010-01-20 15:30:10 - Simulation-based model selection for dynamical systems in systems and population biology</summary>

- *Tina Toni, Michael P. H. Stumpf*

- `0911.1705v3` - [abs](http://arxiv.org/abs/0911.1705v3) - [pdf](http://arxiv.org/pdf/0911.1705v3)

> Computer simulations have become an important tool across the biomedical sciences and beyond. For many important problems several different models or hypotheses exist and choosing which one best describes reality or observed data is not straightforward. We therefore require suitable statistical tools that allow us to choose rationally between different mechanistic models of e.g. signal transduction or gene regulation networks. This is particularly challenging in systems biology where only a small number of molecular species can be assayed at any given time and all measurements are subject to measurement uncertainty. Here we develop such a model selection framework based on approximate Bayesian computation and employing sequential Monte Carlo sampling. We show that our approach can be applied across a wide range of biological scenarios, and we illustrate its use on real data describing influenza dynamics and the JAK-STAT signalling pathway. Bayesian model selection strikes a balance between the complexity of the simulation models and their ability to describe observed data. The present approach enables us to employ the whole formal apparatus to any system that can be (efficiently) simulated, even when exact likelihoods are computationally intractable.

</details>

<details>

<summary>2010-01-21 23:04:50 - Gibbs Sampling for a Bayesian Hierarchical General Linear Model</summary>

- *Alicia A. Johnson, Galin L. Jones*

- `0712.3056v5` - [abs](http://arxiv.org/abs/0712.3056v5) - [pdf](http://arxiv.org/pdf/0712.3056v5)

> We consider a Bayesian hierarchical version of the normal theory general linear model which is practically relevant in the sense that it is general enough to have many applications and it is not straightforward to sample directly from the corresponding posterior distribution. Thus we study a block Gibbs sampler that has the posterior as its invariant distribution. In particular, we establish that the Gibbs sampler converges at a geometric rate. This allows us to establish conditions for a central limit theorem for the ergodic averages used to estimate features of the posterior. Geometric ergodicity is also a key component for using batch means methods to consistently estimate the variance of the asymptotic normal distribution. Together, our results give practitioners the tools to be as confident in inferences based on the observations from the Gibbs sampler as they would be with inferences based on random samples from the posterior. Our theoretical results are illustrated with an application to data on the cost of health plans issued by health maintenance organizations.

</details>

<details>

<summary>2010-01-22 21:26:00 - Grouping Priors and the Bayesian Elastic Net</summary>

- *Luke Bornn, Raphael Gottardo, Arnaud Doucet*

- `1001.4083v1` - [abs](http://arxiv.org/abs/1001.4083v1) - [pdf](http://arxiv.org/pdf/1001.4083v1)

> In the literature surrounding Bayesian penalized regression, the two primary choices of prior distribution on the regression coefficients are zero-mean Gaussian and Laplace. While both have been compared numerically and theoretically, there remains little guidance on which to use in real-life situations. We propose two viable solutions to this problem in the form of prior distributions which combine and compromise between Laplace and Gaussian priors, respectively. Through cross-validation the prior which optimizes prediction performance is automatically selected. We then demonstrate the improved performance of these new prior distributions relative to Laplace and Gaussian priors in both a simulated and experimental environment.

</details>

<details>

<summary>2010-01-23 02:48:04 - Scalable Bayesian reduced-order models for high-dimensional multiscale dynamical systems</summary>

- *P. S. Koutsourelakis, Elias Bilionis*

- `1001.2753v2` - [abs](http://arxiv.org/abs/1001.2753v2) - [pdf](http://arxiv.org/pdf/1001.2753v2)

> While existing mathematical descriptions can accurately account for phenomena at microscopic scales (e.g. molecular dynamics), these are often high-dimensional, stochastic and their applicability over macroscopic time scales of physical interest is computationally infeasible or impractical. In complex systems, with limited physical insight on the coherent behavior of their constituents, the only available information is data obtained from simulations of the trajectories of huge numbers of degrees of freedom over microscopic time scales. This paper discusses a Bayesian approach to deriving probabilistic coarse-grained models that simultaneously address the problems of identifying appropriate reduced coordinates and the effective dynamics in this lower-dimensional representation. At the core of the models proposed lie simple, low-dimensional dynamical systems which serve as the building blocks of the global model. These approximate the latent, generating sources and parameterize the reduced-order dynamics. We discuss parallelizable, online inference and learning algorithms that employ Sequential Monte Carlo samplers and scale linearly with the dimensionality of the observed dynamics. We propose a Bayesian adaptive time-integration scheme that utilizes probabilistic predictive estimates and enables rigorous concurrent s imulation over macroscopic time scales. The data-driven perspective advocated assimilates computational and experimental data and thus can materialize data-model fusion. It can deal with applications that lack a mathematical description and where only observational data is available. Furthermore, it makes non-intrusive use of existing computational models.

</details>

<details>

<summary>2010-01-25 13:45:34 - Robust quantile estimation and prediction for spatial processes</summary>

- *Sophie Dabo Niang, Baba Thiam*

- `1001.4425v1` - [abs](http://arxiv.org/abs/1001.4425v1) - [pdf](http://arxiv.org/pdf/1001.4425v1)

> In this paper, we present a statistical framework for modeling conditional quantiles of spatial processes assumed to be strongly mixing in space. We establish the $L_1$ consistency and the asymptotic normality of the kernel conditional quantile estimator in the case of random fields. We also define a nonparametric spatial predictor and illustrate the methodology used with some simulations.

</details>

<details>

<summary>2010-01-26 10:23:04 - The scaling relation between richness and mass of galaxy clusters: a Bayesian approach</summary>

- *S. Andreon, M. A. Hurn*

- `1001.4639v1` - [abs](http://arxiv.org/abs/1001.4639v1) - [pdf](http://arxiv.org/pdf/1001.4639v1)

> We use a sample of 53 galaxy clusters at 0.03 < z < 0.1 with available masses derived from the caustic technique and with velocity dispersions computed using 208 galaxies on average per cluster, in order to investigate the scaling between richness, mass and velocity dispersion. A tight scaling between richness and mass is found, with an intrinsic scatter of only 0.19 dex in mass and with a slope one, i.e. clusters which have twice as many galaxies are twice as massive. When richness is measured without any knowledge of the cluster mass or linked parameters (such as r200), it can predict mass with an uncertainty of 0.29+/-0.01 dex. As a mass proxy, richness competes favourably with both direct measurements of mass given by the caustic method, which has typically 0.14 dex errors (vs 0.29) and X-ray luminosity, which offers a similar 0.30 dex uncertainty. The similar performances of X-ray luminosity and richness in predicting cluster masses has been confirmed using cluster masses derived from velocity dispersion fixed by numerical simulations. These results suggest that cluster masses can be reliably estimated from simple galaxy counts, at least at the redshift and masses explored in this work. This has important applications in the estimation of cosmological parameters from optical cluster surveys, because in current surveys clusters detected in the optical range outnumber, by at least one order of magnitude, those detected in X-ray. Our analysis is robust from astrophysical and statistical perspectives. The data and code used for the stochastic computation is distributed with the paper. [Abridged]

</details>


## 2010-02

<details>

<summary>2010-02-02 16:49:06 - On some Bayesian nonparametric estimators for species richness under two-parameter Poisson-Dirichlet priors</summary>

- *Annalisa Cerquetti*

- `1002.0535v1` - [abs](http://arxiv.org/abs/1002.0535v1) - [pdf](http://arxiv.org/pdf/1002.0535v1)

> We present an alternative approach to the Bayesian nonparametric analysis of conditional species richness under two-parameter Poisson Dirichlet priors. We rely on a known characterization by deletion of classes property and on results for Beta-Binomial distributions. Besides leading to simplified and much more direct proofs, our proposal provides a new scale mixture representation of the conditional asymptotic law.

</details>

<details>

<summary>2010-02-03 07:56:25 - A New Approximation to the Normal Distribution Quantile Function</summary>

- *Paul M. Voutier*

- `1002.0567v2` - [abs](http://arxiv.org/abs/1002.0567v2) - [pdf](http://arxiv.org/pdf/1002.0567v2)

> We present a new approximation to the normal distribution quantile function. It has a similar form to the approximation of Beasley and Springer [3], providing a maximum absolute error of less than $2.5 \cdot 10^{-5}$. This is less accurate than [3], but still sufficient for many applications. However it is faster than [3]. This is its primary benefit, which can be crucial to many applications, including in financial markets.

</details>

<details>

<summary>2010-02-03 13:20:07 - Minimal Markov Models</summary>

- *Jesus E. Garcia Veronica A. Gonzalez-Lopez*

- `1002.0729v1` - [abs](http://arxiv.org/abs/1002.0729v1) - [pdf](http://arxiv.org/pdf/1002.0729v1)

> In this work we introduce a new and richer class of finite order Markov chain models and address the following model selection problem: find the Markov model with the minimal set of parameters (minimal Markov model) which is necessary to represent a source as a Markov chain of finite order. Let us call $M$ the order of the chain and $A$ the finite alphabet, to determine the minimal Markov model, we define an equivalence relation on the state space $A^{M}$, such that all the sequences of size $M$ with the same transition probabilities are put in the same category. In this way we have one set of $(|A|-1)$ transition probabilities for each category, obtaining a model with a minimal number of parameters. We show that the model can be selected consistently using the Bayesian information criterion.

</details>

<details>

<summary>2010-02-09 21:22:45 - On Bayesian Data Analysis</summary>

- *Christian P. Robert, Judith Rousseau*

- `1001.4656v2` - [abs](http://arxiv.org/abs/1001.4656v2) - [pdf](http://arxiv.org/pdf/1001.4656v2)

> This introduction to Bayesian statistics presents the main concepts as well as the principal reasons advocated in favour of a Bayesian modelling. We cover the various approaches to prior determination as well as the basis asymptotic arguments in favour of using Bayes estimators. The testing aspects of Bayesian inference are also examined in details.

</details>

<details>

<summary>2010-02-10 11:53:25 - Bayesian Inference</summary>

- *Christian P. Robert, Jean-Michel Marin, Judith Rousseau*

- `1002.2080v1` - [abs](http://arxiv.org/abs/1002.2080v1) - [pdf](http://arxiv.org/pdf/1002.2080v1)

> This chapter provides a overview of Bayesian inference, mostly emphasising that it is a universal method for summarising uncertainty and making estimates and predictions using probability statements conditional on observed data and an assumed model (Gelman 2008). The Bayesian perspective is thus applicable to all aspects of statistical inference, while being open to the incorporation of information items resulting from earlier experiments and from expert opinions. We provide here the basic elements of Bayesian analysis when considered for standard models, refering to Marin and Robert (2007) and to Robert (2007) for book-length entries.1 In the following, we refrain from embarking upon philosophical discussions about the nature of knowledge (see, e.g., Robert 2007, Chapter 10), opting instead for a mathematically sound presentation of an eminently practical statistical methodology. We indeed believe that the most convincing arguments for adopting a Bayesian version of data analyses are in the versatility of this tool and in the large range of existing applications, rather than in those polemical arguments.

</details>

<details>

<summary>2010-02-13 13:57:26 - Evolutionary Stochastic Search for Bayesian model exploration</summary>

- *Leonardo Bottolo, Sylvia Richardson*

- `1002.2706v1` - [abs](http://arxiv.org/abs/1002.2706v1) - [pdf](http://arxiv.org/pdf/1002.2706v1)

> Implementing Bayesian variable selection for linear Gaussian regression models for analysing high dimensional data sets is of current interest in many fields. In order to make such analysis operational, we propose a new sampling algorithm based upon Evolutionary Monte Carlo and designed to work under the "large p, small n" paradigm, thus making fully Bayesian multivariate analysis feasible, for example, in genetics/genomics experiments. Two real data examples in genomics are presented, demonstrating the performance of the algorithm in a space of up to 10,000 covariates. Finally the methodology is compared with a recently proposed search algorithms in an extensive simulation study.

</details>

<details>

<summary>2010-02-13 15:36:25 - Bayesian computational methods</summary>

- *Christian P. Robert*

- `1002.2702v1` - [abs](http://arxiv.org/abs/1002.2702v1) - [pdf](http://arxiv.org/pdf/1002.2702v1)

> In this chapter, we will first present the most standard computational challenges met in Bayesian Statistics, focussing primarily on mixture estimation and on model choice issues, and then relate these problems with computational solutions. Of course, this chapter is only a terse introduction to the problems and solutions related to Bayesian computations. For more complete references, see Robert and Casella (2004, 2009), or Marin and Robert (2007), among others. We also restrain from providing an introduction to Bayesian Statistics per se and for comprehensive coverage, address the reader to Robert (2007), (again) among others.

</details>

<details>

<summary>2010-02-16 07:28:58 - Sparse Regression Learning by Aggregation and Langevin Monte-Carlo</summary>

- *Arnak Dalalyan, Alexandre B. Tsybakov*

- `0903.1223v3` - [abs](http://arxiv.org/abs/0903.1223v3) - [pdf](http://arxiv.org/pdf/0903.1223v3)

> We consider the problem of regression learning for deterministic design and independent random errors. We start by proving a sharp PAC-Bayesian type bound for the exponentially weighted aggregate (EWA) under the expected squared empirical loss. For a broad class of noise distributions the presented bound is valid whenever the temperature parameter $\beta$ of the EWA is larger than or equal to $4\sigma^2$, where $\sigma^2$ is the noise variance. A remarkable feature of this result is that it is valid even for unbounded regression functions and the choice of the temperature parameter depends exclusively on the noise level. Next, we apply this general bound to the problem of aggregating the elements of a finite-dimensional linear space spanned by a dictionary of functions $\phi_1,...,\phi_M$. We allow $M$ to be much larger than the sample size $n$ but we assume that the true regression function can be well approximated by a sparse linear combination of functions $\phi_j$. Under this sparsity scenario, we propose an EWA with a heavy tailed prior and we show that it satisfies a sparsity oracle inequality with leading constant one. Finally, we propose several Langevin Monte-Carlo algorithms to approximately compute such an EWA when the number $M$ of aggregated functions can be large. We discuss in some detail the convergence of these algorithms and present numerical experiments that confirm our theoretical findings.

</details>

<details>

<summary>2010-02-19 17:08:54 - Bayesian predictive densities for linear regression models under alpha-divergence loss: some results and open problems</summary>

- *Yuzo Maruyama, William E. Strawderman*

- `1002.3786v1` - [abs](http://arxiv.org/abs/1002.3786v1) - [pdf](http://arxiv.org/pdf/1002.3786v1)

> This paper considers estimation of the predictive density for a normal linear model with unknown variance under alpha-divergence loss for -1 <= alpha <= 1. We first give a general canonical form for the problem, and then give general expressions for the generalized Bayes solution under the above loss for each alpha. For a particular class of hierarchical generalized priors studied in Maruyama and Strawderman (2005, 2006) for the problems of estimating the mean vector and the variance respectively, we give the generalized Bayes predictive density. Additionally, we show that, for a subclass of these priors, the resulting estimator dominates the generalized Bayes estimator with respect to the right invariant prior when alpha=1, i.e., the best (fully) equivariant minimax estimator.

</details>

<details>

<summary>2010-02-23 09:20:57 - On the posterior distribution of classes of random means</summary>

- *Lancelot F. James, Antonio Lijoi, Igor Prünster*

- `1002.4276v1` - [abs](http://arxiv.org/abs/1002.4276v1) - [pdf](http://arxiv.org/pdf/1002.4276v1)

> The study of properties of mean functionals of random probability measures is an important area of research in the theory of Bayesian nonparametric statistics. Many results are now known for random Dirichlet means, but little is known, especially in terms of posterior distributions, for classes of priors beyond the Dirichlet process. In this paper, we consider normalized random measures with independent increments (NRMI's) and mixtures of NRMI. In both cases, we are able to provide exact expressions for the posterior distribution of their means. These general results are then specialized, leading to distributional results for means of two important particular cases of NRMI's and also of the two-parameter Poisson--Dirichlet process.

</details>

<details>

<summary>2010-02-23 11:04:33 - Large deviations for stochastic flows of diffeomorphisms</summary>

- *Amarjit Budhiraja, Paul Dupuis, Vasileios Maroulas*

- `1002.4295v1` - [abs](http://arxiv.org/abs/1002.4295v1) - [pdf](http://arxiv.org/pdf/1002.4295v1)

> A large deviation principle is established for a general class of stochastic flows in the small noise limit. This result is then applied to a Bayesian formulation of an image matching problem, and an approximate maximum likelihood property is shown for the solution of an optimization problem involving the large deviations rate function.

</details>

<details>

<summary>2010-02-23 14:36:42 - The distribution and quantiles of functionals of weighted empirical distributions when observations have different distributions</summary>

- *C. S. Withers, S. Nadarajah*

- `1002.4338v1` - [abs](http://arxiv.org/abs/1002.4338v1) - [pdf](http://arxiv.org/pdf/1002.4338v1)

> This paper extends Edgeworth-Cornish-Fisher expansions for the distribution and quantiles of nonparametric estimates in two ways. Firstly it allows observations to have different distributions. Secondly it allows the observations to be weighted in a predetermined way. The use of weighted estimates has a long history including applications to regression, rank statistics and Bayes theory. However, asymptotic results have generally been only first order (the CLT and weak convergence). We give third order asymptotics for the distribution and percentiles of any smooth functional of a weighted empirical distribution, thus allowing a considerable increase in accuracy over earlier CLT results.   Consider independent non-identically distributed ({\it non-iid}) observations $X_{1n}, ..., X_{nn}$ in $R^s$. Let $\hat{F}(x)$ be their {\it weighted empirical distribution} with weights $w_{1n}, ..., w_{nn}$. We obtain cumulant expansions and hence Edgeworth-Cornish-Fisher expansions for $T(\hat{F})$ for any smooth functional $T(\cdot)$ by extending the concepts of von Mises derivatives to signed measures of total measure 1. As an example we give the cumulant coefficients needed for Edgeworth-Cornish-Fisher expansions to $O(n^{-3/2})$ for the sample variance when observations are non-iid.

</details>

<details>

<summary>2010-02-24 09:10:29 - Discussion of "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth"</summary>

- *Ying Wei*

- `1002.4494v1` - [abs](http://arxiv.org/abs/1002.4494v1) - [pdf](http://arxiv.org/pdf/1002.4494v1)

> Discussion of "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth" by M. Hallin, D. Paindaveine and M. Siman [arXiv:1002.4486]

</details>

<details>

<summary>2010-02-24 09:23:46 - Discussion of "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth"</summary>

- *Robert Serfling, Yijun Zuo*

- `1002.4496v1` - [abs](http://arxiv.org/abs/1002.4496v1) - [pdf](http://arxiv.org/pdf/1002.4496v1)

> Discussion of "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth" by M. Hallin, D. Paindaveine and M. Siman [arXiv:1002.4486]

</details>

<details>

<summary>2010-02-24 10:09:24 - Discussion of "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth"</summary>

- *Linglong Kong, Ivan Mizera*

- `1002.4509v1` - [abs](http://arxiv.org/abs/1002.4509v1) - [pdf](http://arxiv.org/pdf/1002.4509v1)

> Discussion of "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth" by M. Hallin, D. Paindaveine and M. Siman [arXiv:1002.4486]

</details>

<details>

<summary>2010-02-24 10:35:55 - Rejoinder to "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth"</summary>

- *Marc Hallin, Davy Paindaveine, Miroslav Šiman*

- `1002.4515v1` - [abs](http://arxiv.org/abs/1002.4515v1) - [pdf](http://arxiv.org/pdf/1002.4515v1)

> Rejoinder to "Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth" by M. Hallin, D. Paindaveine and M. Siman [arXiv:1002.4486]

</details>

<details>

<summary>2010-02-24 10:58:01 - Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth</summary>

- *Marc Hallin, Davy Paindaveine, Miroslav Šiman*

- `1002.4486v1` - [abs](http://arxiv.org/abs/1002.4486v1) - [pdf](http://arxiv.org/pdf/1002.4486v1)

> A new multivariate concept of quantile, based on a directional version of Koenker and Bassett's traditional regression quantiles, is introduced for multivariate location and multiple-output regression problems. In their empirical version, those quantiles can be computed efficiently via linear programming techniques. Consistency, Bahadur representation and asymptotic normality results are established. Most importantly, the contours generated by those quantiles are shown to coincide with the classical halfspace depth contours associated with the name of Tukey. This relation does not only allow for efficient depth contour computations by means of parametric linear programming, but also for transferring from the quantile to the depth universe such asymptotic results as Bahadur representations. Finally, linear programming duality opens the way to promising developments in depth-related multivariate rank-based inference.

</details>

<details>

<summary>2010-02-25 00:00:47 - Syntactic Topic Models</summary>

- *Jordan Boyd-Graber, David M. Blei*

- `1002.4665v1` - [abs](http://arxiv.org/abs/1002.4665v1) - [pdf](http://arxiv.org/pdf/1002.4665v1)

> The syntactic topic model (STM) is a Bayesian nonparametric model of language that discovers latent distributions of words (topics) that are both semantically and syntactically coherent. The STM models dependency parsed corpora where sentences are grouped into documents. It assumes that each word is drawn from a latent topic chosen by combining document-level features and the local syntactic context. Each document has a distribution over latent topics, as in topic models, which provides the semantic consistency. Each element in the dependency parse tree also has a distribution over the topics of its children, as in latent-state syntax models, which provides the syntactic consistency. These distributions are convolved so that the topic of each word is likely under both its document and syntactic context. We derive a fast posterior inference algorithm based on variational methods. We report qualitative and quantitative studies on both synthetic data and hand-parsed documents. We show that the STM is a more predictive model of language than current models based only on syntax or only on topics.

</details>

<details>

<summary>2010-02-25 07:12:50 - On computational tools for Bayesian data analysis</summary>

- *Christian P. Robert, Jean-Michel Marin*

- `1002.2684v2` - [abs](http://arxiv.org/abs/1002.2684v2) - [pdf](http://arxiv.org/pdf/1002.2684v2)

> While Robert and Rousseau (2010) addressed the foundational aspects of Bayesian analysis, the current chapter details its practical aspects through a review of the computational methods available for approximating Bayesian procedures. Recent innovations like Monte Carlo Markov chain, sequential Monte Carlo methods and more recently Approximate Bayesian Computation techniques have considerably increased the potential for Bayesian applications and they have also opened new avenues for Bayesian inference, first and foremost Bayesian model choice.

</details>

<details>

<summary>2010-02-25 12:47:50 - A copula based approach to adaptive sampling</summary>

- *Ralph Silva, Robert Kohn, Paolo Giordani, Xiuyan Mun*

- `1002.4775v1` - [abs](http://arxiv.org/abs/1002.4775v1) - [pdf](http://arxiv.org/pdf/1002.4775v1)

> Our article is concerned with adaptive sampling schemes for Bayesian inference that update the proposal densities using previous iterates. We introduce a copula based proposal density which is made more efficient by combining it with antithetic variable sampling. We compare the copula based proposal to an adaptive proposal density based on a multivariate mixture of normals and an adaptive random walk Metropolis proposal. We also introduce a refinement of the random walk proposal which performs better for multimodal target distributions. We compare the sampling schemes using challenging but realistic models and priors applied to real data examples. The results show that for the examples studied, the adaptive independent \MH{} proposals are much more efficient than the adaptive random walk proposals and that in general the copula based proposal has the best acceptance rates and lowest inefficiencies.

</details>

<details>

<summary>2010-02-26 09:34:28 - Quantile estimation with adaptive importance sampling</summary>

- *Daniel Egloff, Markus Leippold*

- `1002.4946v1` - [abs](http://arxiv.org/abs/1002.4946v1) - [pdf](http://arxiv.org/pdf/1002.4946v1)

> We introduce new quantile estimators with adaptive importance sampling. The adaptive estimators are based on weighted samples that are neither independent nor identically distributed. Using a new law of iterated logarithm for martingales, we prove the convergence of the adaptive quantile estimators for general distributions with nonunique quantiles thereby extending the work of Feldman and Tucker [Ann. Math. Statist. 37 (1996) 451--457]. We illustrate the algorithm with an example from credit portfolio risk analysis.

</details>

<details>

<summary>2010-02-27 10:32:03 - Shrinkage regression for multivariate inference with missing data, and an application to portfolio balancing</summary>

- *Robert B. Gramacy, Ester Pantaleo*

- `0907.2135v3` - [abs](http://arxiv.org/abs/0907.2135v3) - [pdf](http://arxiv.org/pdf/0907.2135v3)

> Portfolio balancing requires estimates of covariance between asset returns. Returns data have histories which greatly vary in length, since assets begin public trading at different times. This can lead to a huge amount of missing data--too much for the conventional imputation-based approach. Fortunately, a well-known factorization of the MVN likelihood under the prevailing historical missingness pattern leads to a simple algorithm of OLS regressions that is much more reliable. When there are more assets than returns, however, OLS becomes unstable. Gramacy, et al. (2008), showed how classical shrinkage regression may be used instead, thus extending the state of the art to much bigger asset collections, with further accuracy and interpretation advantages. In this paper, we detail a fully Bayesian hierarchical formulation that extends the framework further by allowing for heavy-tailed errors, relaxing the historical missingness assumption, and accounting for estimation risk. We illustrate how this approach compares favorably to the classical one using synthetic data and an investment exercise with real returns. An accompanying R package is on CRAN.

</details>


## 2010-03

<details>

<summary>2010-03-07 16:31:27 - Making and Evaluating Point Forecasts</summary>

- *Tilmann Gneiting*

- `0912.0902v2` - [abs](http://arxiv.org/abs/0912.0902v2) - [pdf](http://arxiv.org/pdf/0912.0902v2)

> Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, such as the absolute error or the squared error. The individual scores are then averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the (root) mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched.   Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive.   A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance.

</details>

<details>

<summary>2010-03-12 10:41:26 - Gaussian Process Structural Equation Models with Latent Variables</summary>

- *Ricardo Silva, Robert B. Gramacy*

- `1002.4802v2` - [abs](http://arxiv.org/abs/1002.4802v2) - [pdf](http://arxiv.org/pdf/1002.4802v2)

> In a variety of disciplines such as social sciences, psychology, medicine and economics, the recorded data are considered to be noisy measurements of latent variables connected by some causal structure. This corresponds to a family of graphical models known as the structural equation model with latent variables. While linear non-Gaussian variants have been well-studied, inference in nonparametric structural equation models is still underdeveloped. We introduce a sparse Gaussian process parameterization that defines a non-linear structure connecting latent variables, unlike common formulations of Gaussian process latent variable models. The sparse parameterization is given a full Bayesian treatment without compromising Markov chain Monte Carlo efficiency. We compare the stability of the sampling procedure and the predictive ability of the model against the current practice.

</details>

<details>

<summary>2010-03-15 14:59:38 - Significance Tests in Climate Science</summary>

- *Maarten H. P. Ambaum*

- `1003.2934v1` - [abs](http://arxiv.org/abs/1003.2934v1) - [pdf](http://arxiv.org/pdf/1003.2934v1)

> A large fraction of papers in the climate literature includes erroneous uses of significance tests. A Bayesian analysis is presented to highlight the meaning of significance tests and why typical misuse occurs. It is concluded that a significance test very rarely provides useful quantitative information. The significance statistic is not a quantitative measure of how confident we can be of the 'reality' of a given result.

</details>

<details>

<summary>2010-03-18 09:55:14 - On Identification of the Threshold Diffusion Processes</summary>

- *Yury A. Kutoyants*

- `1003.3539v1` - [abs](http://arxiv.org/abs/1003.3539v1) - [pdf](http://arxiv.org/pdf/1003.3539v1)

> We consider the problems of parameter estimation for several models of threshold ergodic diffusion processes in the asymptotics of large samples. These models are the direct continuous time analogues of the well-known in time series analysis threshold autoregressive (TAR) models. In such models the trend is switching when the observed process atteints some (unknown) values and the problem is to estimate it or to test some hypotheses concerning these values. The related statistical problems correspond to the singular estimation or testing, for example, the rate of convergence of estimators is $T$ and not $\sqrt{T}$ as in regular estimation problems. We study the asymptotic behavior of the maximum likelihood and bayesian estimators and discuss the possibility of the construction of the goodness of fit test for such models of observation.

</details>

<details>

<summary>2010-03-18 10:02:21 - On Parameter Estimation of Threshold Autoregressive Models</summary>

- *Ngai Hang Chan, Yury A. Kutoyants*

- `1003.3800v1` - [abs](http://arxiv.org/abs/1003.3800v1) - [pdf](http://arxiv.org/pdf/1003.3800v1)

> This paper studies the threshold estimation of a TAR model when the underlying threshold parameter is a random variable. It is shown that the Bayesian estimator is consistent and its limit distribution is expressed in terms of a limit likelihood ratio. Furthermore, convergence of moments of the estimators is also established. The limit distribution can be computed via explicit simulations from which testing and inference for the threshold parameter can be conducted. The obtained results are illustrated with numerical simulations.

</details>

<details>

<summary>2010-03-19 16:22:02 - Bayesian Nonparametric Inference of Switching Linear Dynamical Systems</summary>

- *Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, Alan S. Willsky*

- `1003.3829v1` - [abs](http://arxiv.org/abs/1003.3829v1) - [pdf](http://arxiv.org/pdf/1003.3829v1)

> Many complex dynamical phenomena can be effectively modeled by a system that switches among a set of conditionally linear dynamical modes. We consider two such models: the switching linear dynamical system (SLDS) and the switching vector autoregressive (VAR) process. Our Bayesian nonparametric approach utilizes a hierarchical Dirichlet process prior to learn an unknown number of persistent, smooth dynamical modes. We additionally employ automatic relevance determination to infer a sparse set of dynamic dependencies allowing us to learn SLDS with varying state dimension or switching VAR processes with varying autoregressive order. We develop a sampling algorithm that combines a truncated approximation to the Dirichlet process with efficient joint sampling of the mode and state sequences. The utility and flexibility of our model are demonstrated on synthetic data, sequences of dancing honey bees, the IBOVESPA stock index, and a maneuvering target tracking application.

</details>

<details>

<summary>2010-03-21 08:39:51 - On MMSE and MAP Denoising Under Sparse Representation Modeling Over a Unitary Dictionary</summary>

- *Javier Turek, Irad Yavneh, Matan Protter, Michael Elad*

- `1003.3984v1` - [abs](http://arxiv.org/abs/1003.3984v1) - [pdf](http://arxiv.org/pdf/1003.3984v1)

> Among the many ways to model signals, a recent approach that draws considerable attention is sparse representation modeling. In this model, the signal is assumed to be generated as a random linear combination of a few atoms from a pre-specified dictionary. In this work we analyze two Bayesian denoising algorithms -- the Maximum-Aposteriori Probability (MAP) and the Minimum-Mean-Squared-Error (MMSE) estimators, under the assumption that the dictionary is unitary. It is well known that both these estimators lead to a scalar shrinkage on the transformed coefficients, albeit with a different response curve. In this work we start by deriving closed-form expressions for these shrinkage curves and then analyze their performance. Upper bounds on the MAP and the MMSE estimation errors are derived. We tie these to the error obtained by a so-called oracle estimator, where the support is given, establishing a worst-case gain-factor between the MAP/MMSE estimation errors and the oracle's performance. These denoising algorithms are demonstrated on synthetic signals and on true data (images).

</details>

<details>

<summary>2010-03-22 12:55:08 - Computational Methods in Bayesian Statistics</summary>

- *Alan Tua, Kristian Zarb Adami*

- `1003.3357v2` - [abs](http://arxiv.org/abs/1003.3357v2) - [pdf](http://arxiv.org/pdf/1003.3357v2)

> This paper focuses on utilizing two different Bayesian methods to deal with a variety of toy problems which occur in data analysis. In particular we implement the Variational Bayesian and Nested Sampling methods to tackle the problems of polynomial selection and Gaussian Mixture Models, comparing the algorithms in terms of processing speed and accuracy. In the problems tackled here it is the Variational Bayesian algorithms which are the faster though both results give similar results.

</details>

<details>

<summary>2010-03-24 20:06:55 - Resolution and Scale Independent Function Matching Using a String Energy Penalized Spline Prior</summary>

- *David M. Rogers, Thomas L. Beck*

- `1003.4741v1` - [abs](http://arxiv.org/abs/1003.4741v1) - [pdf](http://arxiv.org/pdf/1003.4741v1)

> The extension of the classical Bayesian penalized spline method to inference on vector-valued functions is considered, with an emphasis on characterizing the suitability of the method for general application.We show that the standard quadratic penalty is exactly analogous to the energy of a stretched string, with the penalty parameter corresponding to its tension. This physical analogy motivates a discussion of resolution independence, which we define as the convergence of a computational function estimate to arbitrary accuracy with increasing resolution.The multidimensional context makes direct application of standard procedures for choosing the penalty parameter difficult, and a new method is proposed and compared to the established generalized cross-validation (GCV) and Akaike information criterion (AIC) functions.Our Bayesian method for choosing this parameter is derived by introducing a scal e independence criterion to ensure that simultaneously scaling the function samples and their variances does not significantly change the posterior parameter distribution. Due to the possibility of an exact polynomial fit, numerical issues prevent the use of this prior, and a solution is presented based on adding a st ring zero-point energy. This makes more complicated approaches recently propose d in the literature unnecessary, and eliminates the requirement for sensitivity analysis when the function deviates from the above mentioned polynomial. An important class of problems which can be analyzed by this method are stochastic numerical integrators, which are considered as an example problem. This work represents the first extension of penalized spline methods to inference on multidimensional numerical integrators reported in the literature. Several numerical calculations illustrate the above points and address practical application issues.

</details>

<details>

<summary>2010-03-25 10:45:01 - On the relevance of the Bayesian approach to Statistics</summary>

- *Christian P. Robert*

- `0909.5369v3` - [abs](http://arxiv.org/abs/0909.5369v3) - [pdf](http://arxiv.org/pdf/0909.5369v3)

> We argue here about the relevance and the ultimate unity of the Bayesian approach in a neutral and agnostic manner. Our main theme is that Bayesian data analysis is an effective tool for handling complex models, as proven by the increasing proportion of Bayesian studies in the applied sciences. We disregard in this essay the philosophical debates on the deeper meaning of probability and on the random nature of parameters as things of the past that do a disservice to the approach and are incomprehensible to most bystanders.

</details>

<details>

<summary>2010-03-25 13:57:50 - Implementing Bayesian predictive procedures: The K-prime and K-square distributions</summary>

- *Jacques Poitevineau, Bruno Lecoutre*

- `1003.4890v1` - [abs](http://arxiv.org/abs/1003.4890v1) - [pdf](http://arxiv.org/pdf/1003.4890v1)

> The implementation of Bayesian predictive procedures under standard normal models is considered. Two distributions are of particular interest, the K-prime and K-square distributions. They also give exact inferences for simple and multiple correlation coefficients. Their cumulative distribution functions can be expressed in terms of infinite series of multiples of incomplete beta function ratios, thus adequate for recursive calculations. Efficient algorithms are provided. To deal with special cases where possible underflows may prevent a recurrence to work properly, a simple solution is proposed which results in a procedure which is intermediate between two classes of algorithm. Some examples of applications are given.

</details>

<details>

<summary>2010-03-29 13:22:37 - Sparse bayesian step-filtering for high-throughput analysis of molecular machine dynamics</summary>

- *Max A. Little, Nick S. Jones*

- `1003.5535v1` - [abs](http://arxiv.org/abs/1003.5535v1) - [pdf](http://arxiv.org/pdf/1003.5535v1)

> Nature has evolved many molecular machines such as kinesin, myosin, and the rotary flagellar motor powered by an ion current from the mitochondria. Direct observation of the step-like motion of these machines with time series from novel experimental assays has recently become possible. These time series are corrupted by molecular and experimental noise that requires removal, but classical signal processing is of limited use for recovering such step-like dynamics. This paper reports simple, novel Bayesian filters that are robust to step-like dynamics in noise, and introduce an L1-regularized, global filter whose sparse solution can be rapidly obtained by standard convex optimization methods. We show these techniques outperforming classical filters on simulated time series in terms of their ability to accurately recover the underlying step dynamics. To show the techniques in action, we extract step-like speed transitions from Rhodobacter sphaeroides flagellar motor time series. Code implementing these algorithms available from http://www.eng.ox.ac.uk/samp/members/max/software/.

</details>

<details>

<summary>2010-03-31 05:19:40 - The Search for Certainty: a critical assessment</summary>

- *Christian P. Robert*

- `1001.5109v2` - [abs](http://arxiv.org/abs/1001.5109v2) - [pdf](http://arxiv.org/pdf/1001.5109v2)

> The Search for Certainty was published in 2009 by Krzysztof Burdzy. It examines the "philosophical duopoly" of von Mises and de Finetti at the foundation of probability and statistics and find this duopoly missing. This review exposes the weakness of the arguments presented in the book, it questions the relevance of introducing a new set of probability axioms from a methodological perspective, and it concludes at the lack of impact of this book on statistical foundations and practice.

</details>


## 2010-04

<details>

<summary>2010-04-01 08:51:32 - A stochastic model of human visual attention with a dynamic Bayesian network</summary>

- *Akisato kimura, Derek Pang, Tatsuto Takeuchi, Kouji Miyazato, Junji Yamato, Kunio Kashino*

- `1004.0085v1` - [abs](http://arxiv.org/abs/1004.0085v1) - [pdf](http://arxiv.org/pdf/1004.0085v1)

> Recent studies in the field of human vision science suggest that the human responses to the stimuli on a visual display are non-deterministic. People may attend to different locations on the same visual input at the same time. Based on this knowledge, we propose a new stochastic model of visual attention by introducing a dynamic Bayesian network to predict the likelihood of where humans typically focus on a video scene. The proposed model is composed of a dynamic Bayesian network with 4 layers. Our model provides a framework that simulates and combines the visual saliency response and the cognitive state of a person to estimate the most probable attended regions. Sample-based inference with Markov chain Monte-Carlo based particle filter and stream processing with multi-core processors enable us to estimate human visual attention in near real time. Experimental results have demonstrated that our model performs significantly better in predicting human visual attention compared to the previous deterministic models.

</details>

<details>

<summary>2010-04-04 20:46:32 - Quantiles Equivariance</summary>

- *Reza Hosseini*

- `1004.0533v1` - [abs](http://arxiv.org/abs/1004.0533v1) - [pdf](http://arxiv.org/pdf/1004.0533v1)

> It is widely claimed that the quantile function is equivariant under increasing transformations. We show by a counterexample that this is not true (even for strictly increasing transformations). However, we show that the quantile function is equivariant under left continuous increasing transformations. We also provide an equivariance relation for continuous decreasing transformations. In the case that the transformation is not continuous, we show that while the transformed quantile at p can be arbitrarily far from the quantile of the transformed at p (in terms of absolute difference), the probability mass between the two is zero. We also show by an example that weighted definition of the median is not equivariant under even strictly increasing continuous transformations.

</details>

<details>

<summary>2010-04-04 23:22:28 - Quantiles symmetry</summary>

- *Reza Hosseini*

- `1004.0540v1` - [abs](http://arxiv.org/abs/1004.0540v1) - [pdf](http://arxiv.org/pdf/1004.0540v1)

> This paper finds a symmetry relation (between quantiles of a random variable and its negative) that is intuitively appealing. We show this symmetry is quite useful in finding new relations for quantiles, in particular an equivariance property for quantiles under continuous decreasing transformations.

</details>

<details>

<summary>2010-04-15 04:48:39 - Chain ladder method: Bayesian bootstrap versus classical bootstrap</summary>

- *Gareth W. Peters, Mario V. Wüthrich, Pavel V. Shevchenko*

- `1004.2548v1` - [abs](http://arxiv.org/abs/1004.2548v1) - [pdf](http://arxiv.org/pdf/1004.2548v1)

> The intention of this paper is to estimate a Bayesian distribution-free chain ladder (DFCL) model using approximate Bayesian computation (ABC) methodology. We demonstrate how to estimate quantities of interest in claims reserving and compare the estimates to those obtained from classical and credibility approaches. In this context, a novel numerical procedure utilising Markov chain Monte Carlo (MCMC), ABC and a Bayesian bootstrap procedure was developed in a truly distribution-free setting. The ABC methodology arises because we work in a distribution-free setting in which we make no parametric assumptions, meaning we can not evaluate the likelihood point-wise or in this case simulate directly from the likelihood model. The use of a bootstrap procedure allows us to generate samples from the intractable likelihood without the requirement of distributional assumptions, this is crucial to the ABC framework. The developed methodology is used to obtain the empirical distribution of the DFCL model parameters and the predictive distribution of the outstanding loss liabilities conditional on the observed claims. We then estimate predictive Bayesian capital estimates, the Value at Risk (VaR) and the mean square error of prediction (MSEP). The latter is compared with the classical bootstrap and credibility methods.

</details>

<details>

<summary>2010-04-21 15:24:58 - Using a priori knowledge to construct copulas</summary>

- *Dominique Drouet Mari, Valerie Monbet*

- `1004.3726v1` - [abs](http://arxiv.org/abs/1004.3726v1) - [pdf](http://arxiv.org/pdf/1004.3726v1)

> Our purpose is to model the dependence between two random variables, taking into account a priori knowledge on these variables. For example, in many applications (oceanography, finance...), there exists an order relation between the two variables; when one takes high values, the other cannot take low values, but the contrary is possible. The dependence for the high values of the two variables is, therefore, not symmetric.   However a minimal dependence also exists: low values of one variable are associated with low values of the other variable. The dependence can also be extreme for the maxima or the minima of the two variables. In this paper, we construct step by step asymmetric copulas with asymptotic minimal dependence, and with or without asymptotic maximal dependence, using mixture variables to get at first asymmetric dependence and then minimal dependence. We fit these models to a real dataset of sea states and compare them using Likelihood Ratio Tests when they are nested, and BIC- criterion (Bayesian Information criterion) otherwise.

</details>

<details>

<summary>2010-04-22 02:26:17 - Model Selection and Adaptive Markov chain Monte Carlo for Bayesian Cointegrated VAR model</summary>

- *Gareth W. Peters, Balakrishnan Kannan, Ben Lasscock, Chris Mellen*

- `1004.3830v1` - [abs](http://arxiv.org/abs/1004.3830v1) - [pdf](http://arxiv.org/pdf/1004.3830v1)

> This paper develops a matrix-variate adaptive Markov chain Monte Carlo (MCMC) methodology for Bayesian Cointegrated Vector Auto Regressions (CVAR). We replace the popular approach to sampling Bayesian CVAR models, involving griddy Gibbs, with an automated efficient alternative, based on the Adaptive Metropolis algorithm of Roberts and Rosenthal, (2009). Developing the adaptive MCMC framework for Bayesian CVAR models allows for efficient estimation of posterior parameters in significantly higher dimensional CVAR series than previously possible with existing griddy Gibbs samplers. For a n-dimensional CVAR series, the matrix-variate posterior is in dimension $3n^2 + n$, with significant correlation present between the blocks of matrix random variables. We also treat the rank of the CVAR model as a random variable and perform joint inference on the rank and model parameters. This is achieved with a Bayesian posterior distribution defined over both the rank and the CVAR model parameters, and inference is made via Bayes Factor analysis of rank. Practically the adaptive sampler also aids in the development of automated Bayesian cointegration models for algorithmic trading systems considering instruments made up of several assets, such as currency baskets. Previously the literature on financial applications of CVAR trading models typically only considers pairs trading (n=2) due to the computational cost of the griddy Gibbs. We are able to extend under our adaptive framework to $n >> 2$ and demonstrate an example with n = 10, resulting in a posterior distribution with parameters up to dimension 310. By also considering the rank as a random quantity we can ensure our resulting trading models are able to adjust to potentially time varying market conditions in a coherent statistical framework.

</details>

<details>

<summary>2010-04-23 21:46:55 - Imputation Estimators Partially Correct for Model Misspecification</summary>

- *Vladimir N. Minin, John D. O'Brien, Arseni Seregin*

- `0911.0930v2` - [abs](http://arxiv.org/abs/0911.0930v2) - [pdf](http://arxiv.org/pdf/0911.0930v2)

> Inference problems with incomplete observations often aim at estimating population properties of unobserved quantities. One simple way to accomplish this estimation is to impute the unobserved quantities of interest at the individual level and then take an empirical average of the imputed values. We show that this simple imputation estimator can provide partial protection against model misspecification. We illustrate imputation estimators' robustness to model specification on three examples: mixture model-based clustering, estimation of genotype frequencies in population genetics, and estimation of Markovian evolutionary distances. In the final example, using a representative model misspecification, we demonstrate that in non-degenerate cases, the imputation estimator dominates the plug-in estimate asymptotically. We conclude by outlining a Bayesian implementation of the imputation-based estimation.

</details>

<details>

<summary>2010-04-25 12:25:14 - Exact posterior distributions over the segmentation space and model selection for multiple change-point detection problems</summary>

- *Guillem Rigaill, Emilie Lebarbier, Stéphane Robin*

- `1004.4347v1` - [abs](http://arxiv.org/abs/1004.4347v1) - [pdf](http://arxiv.org/pdf/1004.4347v1)

> In segmentation problems, inference on change-point position and model selection are two difficult issues due to the discrete nature of change-points. In a Bayesian context, we derive exact, non-asymptotic, explicit and tractable formulae for the posterior distribution of variables such as the number of change-points or their positions. We also derive a new selection criterion that accounts for the reliability of the results. All these results are based on an efficient strategy to explore the whole segmentation space, which is very large. We illustrate our methodology on both simulated data and a comparative genomic hybridisation profile.

</details>

<details>

<summary>2010-04-28 16:13:44 - Evidence and Evolution: A Review</summary>

- *Christian P. Robert*

- `1004.5074v1` - [abs](http://arxiv.org/abs/1004.5074v1) - [pdf](http://arxiv.org/pdf/1004.5074v1)

> "Evidence and Evolution: the Logic behind the Science" was published in 2008 by Elliott Sober. It examines the philosophical foundations of the statistical arguments used to evaluate hypotheses in evolutionary biology, based on simple examples and likelihood ratios. The difficulty with reading the book from a statistician's perspective is the reluctance of the author to engage into model building and even less into parameter estimation. The first chapter nonetheless constitutes a splendid coverage of the most common statistical approaches to testing and model comparison, even though the advocation of the Akaike information criterion against Bayesian alternatives is rather forceful. The book also covers an examination of the "intelligent design" arguments against the Darwinian evolution theory, predictably if unnecessarily resorting to Popperian arguments to correctly argue that the creationist perspective fails to predict anything. The following chapters cover the more relevant issues of assessing selection versus drift and of testing for the presence of a common ancestor. While remaining a philosophy treatise, Evidence and Evolution is written in a way that is accessible to laymen, if rather unusual from a statistician viewpoint, and the insight about testing issues gained from Evidence and Evolution makes it a worthwhile read.

</details>

<details>

<summary>2010-04-29 21:41:52 - Approximation for a Toy Defective Ising Model</summary>

- *Adom Giffin*

- `1004.5408v1` - [abs](http://arxiv.org/abs/1004.5408v1) - [pdf](http://arxiv.org/pdf/1004.5408v1)

> It has been previously shown that one can use the ME methodology (Caticha Giffin 2006) to reproduce a mean field solution for a simple fluid (Tseng 2004). One could easily use the case of a simple ferromagnetic material as well. The drawback to the mean field approach is that one must assume that all atoms must all act the same. The problem becomes more tractable when the agents are only allowed to interact with their nearest neighbors and can be in only two possible states. The easiest case being an Ising model. The purpose of this paper is to illustrate the use of the ME method as an approximation tool. The paper show a simple case to compare with the traditional mean field approach. Then we show two examples that lie outside of traditional methodologies. These cases explore a ferromagnetic material with defects. The main result is that regardless of the case, the ME method provides good approximations for each case which would not otherwise be possible or at least well justified.

</details>

<details>

<summary>2010-04-30 14:23:46 - Bayesian estimation of regularization and PSF parameters for Wiener-Hunt deconvolution</summary>

- *Francois Orieux, Jean-Francois Giovannelli, Thomas Rodet*

- `1004.5538v1` - [abs](http://arxiv.org/abs/1004.5538v1) - [pdf](http://arxiv.org/pdf/1004.5538v1)

> This paper tackles the problem of image deconvolution with joint estimation of PSF parameters and hyperparameters. Within a Bayesian framework, the solution is inferred via a global a posteriori law for unknown parameters and object. The estimate is chosen as the posterior mean, numerically calculated by means of a Monte-Carlo Markov chain algorithm. The estimates are efficiently computed in the Fourier domain and the effectiveness of the method is shown on simulated examples. Results show precise estimates for PSF parameters and hyperparameters as well as precise image estimates including restoration of high-frequencies and spatial details, within a global and coherent approach.

</details>


## 2010-05

<details>

<summary>2010-05-04 16:21:21 - Notes on Using Control Variates for Estimation with Reversible MCMC Samplers</summary>

- *Ioannis Kontoyiannis, Petros Dellaportas*

- `0907.4160v2` - [abs](http://arxiv.org/abs/0907.4160v2) - [pdf](http://arxiv.org/pdf/0907.4160v2)

> A general methodology is presented for the construction and effective use of control variates for reversible MCMC samplers. The values of the coefficients of the optimal linear combination of the control variates are computed, and adaptive, consistent MCMC estimators are derived for these optimal coefficients. All methodological and asymptotic arguments are rigorously justified. Numerous MCMC simulation examples from Bayesian inference applications demonstrate that the resulting variance reduction can be quite dramatic.

</details>

<details>

<summary>2010-05-07 22:57:27 - Efficient computation of the cdf of the maximal difference between Brownian bridge and its concave majorant</summary>

- *Fadoua Balabdaoui, Karim Filali*

- `1005.1307v1` - [abs](http://arxiv.org/abs/1005.1307v1) - [pdf](http://arxiv.org/pdf/1005.1307v1)

> In this paper, we describe two computational methods for calculating the cumulative distribution function and the upper quantiles of the maximal difference between a Brownian bridge and its concave majorant. The first method has two different variants that are both based on a Monte Carlo approach, whereas the second uses the Gaver-Stehfest (GS) algorithm for numerical inversion of Laplace transform. If the former method is straightforward to implement, it is very much outperformed by the GS algorithm, which provides a very accurate approximation of the cumulative distribution as well as its upper quantiles. Our numerical work has a direct application in statistics: the maximal difference between a Brownian bridge and its concave majorant arises in connection with a nonparametric test for monotonicity of a density or regression curve on [0, 1]. Our results can be used to construct very accurate rejection region for this test at a given asymptotic level.

</details>

<details>

<summary>2010-05-10 05:40:22 - An Adaptive Sequential Monte Carlo Sampler</summary>

- *Paul Fearnhead, Benjamin M. Taylor*

- `1005.1193v2` - [abs](http://arxiv.org/abs/1005.1193v2) - [pdf](http://arxiv.org/pdf/1005.1193v2)

> Sequential Monte Carlo (SMC) methods are not only a popular tool in the analysis of state space models, but offer an alternative to MCMC in situations where Bayesian inference must proceed via simulation. This paper introduces a new SMC method that uses adaptive MCMC kernels for particle dynamics. The proposed algorithm features an online stochastic optimization procedure to select the best MCMC kernel and simultaneously learn optimal tuning parameters. Theoretical results are presented that justify the approach and give guidance on how it should be implemented. Empirical results, based on analysing data from mixture models, show that the new adaptive SMC algorithm (ASMC) can both choose the best MCMC kernel, and learn an appropriate scaling for it. ASMC with a choice between kernels outperformed the adaptive MCMC algorithm of Haario et al. (1998) in 5 out of the 6 cases considered.

</details>

<details>

<summary>2010-05-13 01:28:10 - Ecological non-linear state space model selection via adaptive particle Markov chain Monte Carlo (AdPMCMC)</summary>

- *Gareth W. Peters, Geoff R. Hosack, Keith R. Hayes*

- `1005.2238v1` - [abs](http://arxiv.org/abs/1005.2238v1) - [pdf](http://arxiv.org/pdf/1005.2238v1)

> We develop a novel advanced Particle Markov chain Monte Carlo algorithm that is capable of sampling from the posterior distribution of non-linear state space models for both the unobserved latent states and the unknown model parameters. We apply this novel methodology to five population growth models, including models with strong and weak Allee effects, and test if it can efficiently sample from the complex likelihood surface that is often associated with these models. Utilising real and also synthetically generated data sets we examine the extent to which observation noise and process error may frustrate efforts to choose between these models. Our novel algorithm involves an Adaptive Metropolis proposal combined with an SIR Particle MCMC algorithm (AdPMCMC). We show that the AdPMCMC algorithm samples complex, high-dimensional spaces efficiently, and is therefore superior to standard Gibbs or Metropolis Hastings algorithms that are known to converge very slowly when applied to the non-linear state space ecological models considered in this paper. Additionally, we show how the AdPMCMC algorithm can be used to recursively estimate the Bayesian Cram\'er-Rao Lower Bound of Tichavsk\'y (1998). We derive expressions for these Cram\'er-Rao Bounds and estimate them for the models considered. Our results demonstrate a number of important features of common population growth models, most notably their multi-modal posterior surfaces and dependence between the static and dynamic parameters. We conclude by sampling from the posterior distribution of each of the models, and use Bayes factors to highlight how observation noise significantly diminishes our ability to select among some of the models, particularly those that are designed to reproduce an Allee effect.

</details>

<details>

<summary>2010-05-13 15:53:10 - On a Multiplicative Algorithm for Computing Bayesian D-optimal Designs</summary>

- *Yaming Yu*

- `1005.2355v1` - [abs](http://arxiv.org/abs/1005.2355v1) - [pdf](http://arxiv.org/pdf/1005.2355v1)

> We use the minorization-maximization principle (Lange, Hunter and Yang 2000) to establish the monotonicity of a multiplicative algorithm for computing Bayesian D-optimal designs. This proves a conjecture of Dette, Pepelyshev and Zhigljavsky (2008).

</details>

<details>

<summary>2010-05-16 22:39:23 - Divergence of sample quantiles</summary>

- *Reza Hosseini*

- `1005.2781v1` - [abs](http://arxiv.org/abs/1005.2781v1) - [pdf](http://arxiv.org/pdf/1005.2781v1)

> We show that the left (right) sample quantile tends to the left (right) distribution quantile at p in [0,1], if the left and right quantiles are identical at p. We show that the sample quantiles diverge almost surely otherwise. The latter can be considered as a generalization of the well-known result that the sum of a random sample of a fair coin with 1 denoting heads and -1 denoting tails is 0 infinitely often. In the case that the sample quantiles do not converge we show that the limsup is the right quantile and the liminf is the left quantile.

</details>

<details>

<summary>2010-05-16 23:13:11 - Structure Variability in Bayesian Networks</summary>

- *Marco Scutari*

- `0909.1685v4` - [abs](http://arxiv.org/abs/0909.1685v4) - [pdf](http://arxiv.org/pdf/0909.1685v4)

> The structure of a Bayesian network encodes most of the information about the probability distribution of the data, which is uniquely identified given some general distributional assumptions. Therefore it's important to study the variability of its network structure, which can be used to compare the performance of different learning algorithms and to measure the strength of any arbitrary subset of arcs.   In this paper we will introduce some descriptive statistics and the corresponding parametric and Monte Carlo tests on the undirected graph underlying the structure of a Bayesian network, modeled as a multivariate Bernoulli random variable.

</details>

<details>

<summary>2010-05-18 15:11:49 - Selection of a Model of Cerebral Activity for fMRI Group Data Analysis</summary>

- *Merlin Keller, Alexis Roche, Marc Lavielle*

- `1005.3225v1` - [abs](http://arxiv.org/abs/1005.3225v1) - [pdf](http://arxiv.org/pdf/1005.3225v1)

> This thesis is dedicated to the statistical analysis of multi-sub ject fMRI data, with the purpose of identifying bain structures involved in certain cognitive or sensori-motor tasks, in a reproducible way across sub jects. To overcome certain limitations of standard voxel-based testing methods, as implemented in the Statistical Parametric Mapping (SPM) software, we introduce a Bayesian model selection approach to this problem, meaning that the most probable model of cerebral activity given the data is selected from a pre-defined collection of possible models. Based on a parcellation of the brain volume into functionally homogeneous regions, each model corresponds to a partition of the regions into those involved in the task under study and those inactive. This allows to incorporate prior information, and avoids the dependence of the SPM-like approach on an arbitrary threshold, called the cluster- forming threshold, to define active regions. By controlling a Bayesian risk, our approach balances false positive and false negative risk control. Furthermore, it is based on a generative model that accounts for the spatial uncertainty on the localization of individual effects, due to spatial normalization errors. On both simulated and real fMRI datasets, we show that this new paradigm corrects several biases of the SPM-like approach, which either swells or misses the different active regions, depending on the choice of a cluster-forming threshold.

</details>

<details>

<summary>2010-05-19 23:57:22 - Profile Likelihood Intervals for Quantiles in Extreme Value Distributions</summary>

- *A. Bolívar, E. Díaz-Francés, J. Ortega, E. Vilchis*

- `1005.3573v1` - [abs](http://arxiv.org/abs/1005.3573v1) - [pdf](http://arxiv.org/pdf/1005.3573v1)

> Profile likelihood intervals of large quantiles in Extreme Value distributions provide a good way to estimate these parameters of interest since they take into account the asymmetry of the likelihood surface in the case of small and moderate sample sizes; however they are seldom used in practice. In contrast, maximum likelihood asymptotic (mla) intervals are commonly used without respect to sample size. It is shown here that profile likelihood intervals actually are a good alternative for the estimation of quantiles for sample sizes $25 \leq n\leq 100$ of block maxima, since they presented adequate coverage frequencies in contrast to the poor coverage frequencies of mla intervals for these sample sizes, which also tended to underestimate the quantile and therefore might be a dangerous statistical practice.   In addition, maximum likelihood estimation can present problems when Weibull models are considered for moderate or small sample sizes due to singularities of the corresponding density function when the shape parameter is smaller than one. These estimation problems can be traced to the commonly used continuous approximation to the likelihood function and could be avoided by using the exact or correct likelihood function, at least for the settings considered here. A rainfall data example is presented to exemplify the suggested inferential procedure based on the analyses of profile likelihoods.

</details>

<details>

<summary>2010-05-21 22:49:04 - Bayesian inference for general Gaussian graphical models with application to multivariate lattice data</summary>

- *Adrian Dobra, Alex Lenkoski, Abel Rodriguez*

- `1005.4094v1` - [abs](http://arxiv.org/abs/1005.4094v1) - [pdf](http://arxiv.org/pdf/1005.4094v1)

> We introduce efficient Markov chain Monte Carlo methods for inference and model determination in multivariate and matrix-variate Gaussian graphical models. Our framework is based on the G-Wishart prior for the precision matrix associated with graphs that can be decomposable or non-decomposable. We extend our sampling algorithms to a novel class of conditionally autoregressive models for sparse estimation in multivariate lattice data, with a special emphasis on the analysis of spatial data. These models embed a great deal of flexibility in estimating both the correlation structure across outcomes and the spatial correlation structure, thereby allowing for adaptive smoothing and spatial autocorrelation parameters. Our methods are illustrated using simulated and real-world examples, including an application to cancer mortality surveillance.

</details>

<details>

<summary>2010-05-28 02:03:34 - A note on target distribution ambiguity of likelihood-free samplers</summary>

- *S. A. Sisson, G. W. Peters, M. Briers, Y. Fan*

- `1005.5201v1` - [abs](http://arxiv.org/abs/1005.5201v1) - [pdf](http://arxiv.org/pdf/1005.5201v1)

> Methods for Bayesian simulation in the presence of computationally intractable likelihood functions are of growing interest. Termed likelihood-free samplers, standard simulation algorithms such as Markov chain Monte Carlo have been adapted for this setting. In this article, by presenting generalisations of existing algorithms, we demonstrate that likelihood-free samplers can be ambiguous over the form of the target distribution. We also consider the theoretical justification of these samplers. Distinguishing between the forms of the target distribution may have implications for the future development of likelihood-free samplers.

</details>

<details>

<summary>2010-05-28 17:25:05 - Using a Kernel Adatron for Object Classification with RCS Data</summary>

- *Marten F. Byl, James T. Demers, Edward A. Rietman*

- `1005.5337v1` - [abs](http://arxiv.org/abs/1005.5337v1) - [pdf](http://arxiv.org/pdf/1005.5337v1)

> Rapid identification of object from radar cross section (RCS) signals is important for many space and military applications. This identification is a problem in pattern recognition which either neural networks or support vector machines should prove to be high-speed. Bayesian networks would also provide value but require significant preprocessing of the signals. In this paper, we describe the use of a support vector machine for object identification from synthesized RCS data. Our best results are from data fusion of X-band and S-band signals, where we obtained 99.4%, 95.3%, 100% and 95.6% correct identification for cylinders, frusta, spheres, and polygons, respectively. We also compare our results with a Bayesian approach and show that the SVM is three orders of magnitude faster, as measured by the number of floating point operations.

</details>

<details>

<summary>2010-05-31 07:04:18 - Sequential Quantile Prediction of Time Series</summary>

- *Gérard Biau, Benoît Patra*

- `0908.2503v2` - [abs](http://arxiv.org/abs/0908.2503v2) - [pdf](http://arxiv.org/pdf/0908.2503v2)

> Motivated by a broad range of potential applications, we address the quantile prediction problem of real-valued time series. We present a sequential quantile forecasting model based on the combination of a set of elementary nearest neighbor-type predictors called "experts" and show its consistency under a minimum of conditions. Our approach builds on the methodology developed in recent years for prediction of individual sequences and exploits the quantile structure as a minimizer of the so-called pinball loss function. We perform an in-depth analysis of real-world data sets and show that this nonparametric strategy generally outperforms standard quantile prediction methods

</details>

<details>

<summary>2010-05-31 08:23:30 - HIV with contact-tracing: a case study in Approximate Bayesian Computation</summary>

- *Michael G. B. Blum, Viet Chi Tran*

- `0810.0896v5` - [abs](http://arxiv.org/abs/0810.0896v5) - [pdf](http://arxiv.org/pdf/0810.0896v5)

> Missing data is a recurrent issue in epidemiology where the infection process may be partially observed. Approximate Bayesian Computation, an alternative to data imputation methods such as Markov Chain Monte Carlo integration, is proposed for making inference in epidemiological models. It is a likelihood-free method that relies exclusively on numerical simulations. ABC consists in computing a distance between simulated and observed summary statistics and weighting the simulations according to this distance. We propose an original extension of ABC to path-valued summary statistics, corresponding to the cumulated number of detections as a function of time. For a standard compartmental model with Suceptible, Infectious and Recovered individuals (SIR), we show that the posterior distributions obtained with ABC and MCMC are similar. In a refined SIR model well-suited to the HIV contact-tracing data in Cuba, we perform a comparison between ABC with full and binned detection times. For the Cuban data, we evaluate the efficiency of the detection system and predict the evolution of the HIV-AIDS disease. In particular, the percentage of undetected infectious individuals is found to be of the order of 40%.

</details>

<details>

<summary>2010-05-31 09:01:09 - Approximate Bayesian Computation: a nonparametric perspective</summary>

- *Michael Blum*

- `0904.0635v6` - [abs](http://arxiv.org/abs/0904.0635v6) - [pdf](http://arxiv.org/pdf/0904.0635v6)

> Approximate Bayesian Computation is a family of likelihood-free inference techniques that are well-suited to models defined in terms of a stochastic generating mechanism. In a nutshell, Approximate Bayesian Computation proceeds by computing summary statistics s_obs from the data and simulating summary statistics for different values of the parameter theta. The posterior distribution is then approximated by an estimator of the conditional density g(theta|s_obs). In this paper, we derive the asymptotic bias and variance of the standard estimators of the posterior distribution which are based on rejection sampling and linear adjustment. Additionally, we introduce an original estimator of the posterior distribution based on quadratic adjustment and we show that its bias contains a fewer number of terms than the estimator with linear adjustment. Although we find that the estimators with adjustment are not universally superior to the estimator based on rejection sampling, we find that they can achieve better performance when there is a nearly homoscedastic relationship between the summary statistics and the parameter of interest. To make this relationship as homoscedastic as possible, we propose to use transformations of the summary statistics. In different examples borrowed from the population genetics and epidemiological literature, we show the potential of the methods with adjustment and of the transformations of the summary statistics. Supplemental materials containing the details of the proofs are available online.

</details>


## 2010-06

<details>

<summary>2010-06-01 03:57:48 - Minimax Robust Quickest Change Detection</summary>

- *Jayakrishnan Unnikrishnan, Venugopal V. Veeravalli, Sean Meyn*

- `0911.2551v2` - [abs](http://arxiv.org/abs/0911.2551v2) - [pdf](http://arxiv.org/pdf/0911.2551v2)

> The popular criteria of optimality for quickest change detection procedures are the Lorden criterion, the Shiryaev-Roberts-Pollak criterion, and the Bayesian criterion. In this paper a robust version of these quickest change detection problems is considered when the pre-change and post-change distributions are not known exactly but belong to known uncertainty classes of distributions. For uncertainty classes that satisfy a specific condition, it is shown that one can identify least favorable distributions (LFDs) from the uncertainty classes, such that the detection rule designed for the LFDs is optimal for the robust problem in a minimax sense. The condition is similar to that required for the identification of LFDs for the robust hypothesis testing problem originally studied by Huber. An upper bound on the delay incurred by the robust test is also obtained in the asymptotic setting under the Lorden criterion of optimality. This bound quantifies the delay penalty incurred to guarantee robustness. When the LFDs can be identified, the proposed test is easier to implement than the CUSUM test based on the Generalized Likelihood Ratio (GLR) statistic which is a popular approach for such robust change detection problems. The proposed test is also shown to give better performance than the GLR test in simulations for some parameter values.

</details>

<details>

<summary>2010-06-05 18:52:13 - Tree-Structured Stick Breaking Processes for Hierarchical Data</summary>

- *Ryan Prescott Adams, Zoubin Ghahramani, Michael I. Jordan*

- `1006.1062v1` - [abs](http://arxiv.org/abs/1006.1062v1) - [pdf](http://arxiv.org/pdf/1006.1062v1)

> Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a flexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are infinitely exchangeable. One can view our model as providing infinite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data.

</details>

<details>

<summary>2010-06-07 19:42:34 - On Bayesian learning from Bernoulli observations</summary>

- *Pier Giovanni Bissiri, Stephen G. Walker*

- `0902.2544v2` - [abs](http://arxiv.org/abs/0902.2544v2) - [pdf](http://arxiv.org/pdf/0902.2544v2)

> We provide a reason for Bayesian updating, in the Bernoulli case, even when it is assumed that observations are independent and identically distributed with a fixed but unknown parameter $\theta_0$. The motivation relies on the use of loss functions and asymptotics. Such a justification is important due to the recent interest and focus on Bayesian consistency which indeed assumes that the observations are independent and identically distributed rather than being conditionally independent with joint distribution depending on the choice of prior.

</details>

<details>

<summary>2010-06-09 18:17:41 - Copula Processes</summary>

- *Andrew Gordon Wilson, Zoubin Ghahramani*

- `1006.1350v2` - [abs](http://arxiv.org/abs/1006.1350v2) - [pdf](http://arxiv.org/pdf/1006.1350v2)

> We define a copula process which describes the dependencies between arbitrarily many random variables independently of their marginal distributions. As an example, we develop a stochastic volatility model, Gaussian Copula Process Volatility (GCPV), to predict the latent standard deviations of a sequence of random variables. To make predictions we use Bayesian inference, with the Laplace approximation, and with Markov chain Monte Carlo as an alternative. We find both methods comparable. We also find our model can outperform GARCH on simulated and financial data. And unlike GARCH, GCPV can easily handle missing data, incorporate covariates other than time, and model a rich class of covariance structures.

</details>

<details>

<summary>2010-06-09 22:07:29 - Auxiliary Particle filtering within adaptive Metropolis-Hastings Sampling</summary>

- *Michael Pitt, Ralph Silva, Paolo Giordani, Robert Kohn*

- `1006.1914v1` - [abs](http://arxiv.org/abs/1006.1914v1) - [pdf](http://arxiv.org/pdf/1006.1914v1)

> Our article deals with Bayesian inference for a general state space model with the simulated likelihood computed by the particle filter. We show empirically that the partially or fully adapted particle filters can be much more efficient than the standard particle, especially when the signal to noise ratio is high. This is especially important because using the particle filter within MCMC sampling is O(T^2), where T is the sample size. We also show that an adaptive independent proposal for the unknown parameters based on a mixture of normals can be much more efficient than the usual optimal random walk methods because the simulated likelihood is not continuous in the parameters and the cost of constructing a good adaptive proposal is negligible compared to the cost of evaluating the simulated likelihood. Independent \MH proposals are also attractive because they are easy to run in parallel on multiple processors. The article also shows that the proposed \aimh sampler converges to the posterior distribution. We also show that the marginal likelihood of any state space model can be obtained in an efficient and unbiased manner by using the \pf making model comparison straightforward. Obtaining the marginal likelihood is often difficult using other methods. Finally, we prove that the simulated likelihood obtained by the auxiliary particle filter is unbiased. This result is fundamental to using the particle for MCMC sampling and is first obtained in a more abstract and difficult setting by Del Moral (2004). However, our proof is direct and will make the result accessible to readers.

</details>

<details>

<summary>2010-06-15 14:38:26 - Free energy Sequential Monte Carlo, application to mixture modelling</summary>

- *Nicolas Chopin, Pierre Jacob*

- `1006.3002v1` - [abs](http://arxiv.org/abs/1006.3002v1) - [pdf](http://arxiv.org/pdf/1006.3002v1)

> We introduce a new class of Sequential Monte Carlo (SMC) methods, which we call free energy SMC. This class is inspired by free energy methods, which originate from Physics, and where one samples from a biased distribution such that a given function $\xi(\theta)$ of the state $\theta$ is forced to be uniformly distributed over a given interval. From an initial sequence of distributions $(\pi_t)$ of interest, and a particular choice of $\xi(\theta)$, a free energy SMC sampler computes sequentially a sequence of biased distributions $(\tilde{\pi}_{t})$ with the following properties: (a) the marginal distribution of $\xi(\theta)$ with respect to $\tilde{\pi}_{t}$ is approximatively uniform over a specified interval, and (b) $\tilde{\pi}_{t}$ and $\pi_{t}$ have the same conditional distribution with respect to $\xi$. We apply our methodology to mixture posterior distributions, which are highly multimodal. In the mixture context, forcing certain hyper-parameters to higher values greatly faciliates mode swapping, and makes it possible to recover a symetric output. We illustrate our approach with univariate and bivariate Gaussian mixtures and two real-world datasets.

</details>

<details>

<summary>2010-06-16 20:07:20 - Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</summary>

- *Han Liu, Kathryn Roeder, Larry Wasserman*

- `1006.3316v1` - [abs](http://arxiv.org/abs/1006.3316v1) - [pdf](http://arxiv.org/pdf/1006.3316v1)

> A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include $K$-fold cross-validation ($K$-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including $K$-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.

</details>

<details>

<summary>2010-06-18 18:08:42 - Using Integrated Nested Laplace Approximation for Modeling Spatial Healthcare Utilization</summary>

- *Erik A. Sauleau, Valentina Mameli, Monica Musio*

- `1006.3764v1` - [abs](http://arxiv.org/abs/1006.3764v1) - [pdf](http://arxiv.org/pdf/1006.3764v1)

> In recent years, spatial and spatio-temporal modeling have become an important area of research in many fields (epidemiology, environmental studies, disease mapping). In this work we propose different spatial models to study hospital recruitment, including some potentially explicative variables. Interest is on the distribution per geographical unit of the ratio between the number of patients living in this geographical unit and the population in the same unit. Models considered are within the framework of Bayesian Latent Gaussian models. Our response variable is assumed to follow a binomial distribution, with logit link, whose parameters are the population in the geographical unit and the corresponding relative risk. The structured additive predictor accounts for effects of various covariates in an additive way, including smoothing functions of the covariates (for example spatial effect), linear effect of covariates. To approximate posterior marginals, which not available in closed form, we use integrated nested Laplace approximations (INLA), recently proposed for approximate Bayesian inference in latent Gaussian models. INLA has the advantage of giving very accurate approximations and being faster than McMC methods when the number of parameters does not exceed 6 (as it is in our case). Model comparisons are assessed using DIC criterion.

</details>

<details>

<summary>2010-06-19 10:31:49 - About incoherent inference</summary>

- *Christian P. Robert*

- `1006.3854v1` - [abs](http://arxiv.org/abs/1006.3854v1) - [pdf](http://arxiv.org/pdf/1006.3854v1)

> In Templeton (2010), the Approximate Bayesian Computation (ABC) algorithm (see, e.g., Pritchard et al., 1999, Beaumont et al., 2002, Marjoram et al., 2003, Ratmann et al., 2009) is criticised on mathematical and logical grounds: "the [Bayesian] inference is mathematically incorrect and formally illogical". Since those criticisms turn out to be bearing on Bayesian foundations rather than on the computational methodology they are primarily directed at, we endeavour to point out in this note the statistical errors and inconsistencies in Templeton (2010), refering to Beaumont et al. (2010) for a reply that is broader in scope since it also covers the phylogenetic aspects of nested clade versus a model-based approach.

</details>

<details>

<summary>2010-06-20 13:18:50 - An alternative marginal likelihood estimator for phylogenetic models</summary>

- *Serena Arima, Luca Tardella*

- `1001.2136v2` - [abs](http://arxiv.org/abs/1001.2136v2) - [pdf](http://arxiv.org/pdf/1001.2136v2)

> Bayesian phylogenetic methods are generating noticeable enthusiasm in the field of molecular systematics. Many phylogenetic models are often at stake and different approaches are used to compare them within a Bayesian framework. The Bayes factor, defined as the ratio of the marginal likelihoods of two competing models, plays a key role in Bayesian model selection. We focus on an alternative estimator of the marginal likelihood whose computation is still a challenging problem. Several computational solutions have been proposed none of which can be considered outperforming the others simultaneously in terms of simplicity of implementation, computational burden and precision of the estimates. Practitioners and researchers, often led by available software, have privileged so far the simplicity of the harmonic mean estimator (HM) and the arithmetic mean estimator (AM). However it is known that the resulting estimates of the Bayesian evidence in favor of one model are biased and often inaccurate up to having an infinite variance so that the reliability of the corresponding conclusions is doubtful. Our new implementation of the generalized harmonic mean (GHM) idea recycles MCMC simulations from the posterior, shares the computational simplicity of the original HM estimator, but, unlike it, overcomes the infinite variance issue. The alternative estimator is applied to simulated phylogenetic data and produces fully satisfactory results outperforming those simple estimators currently provided by most of the publicly available software.

</details>

<details>

<summary>2010-06-22 04:12:33 - Computing p-values of LiNGAM outputs via Multiscale Bootstrap</summary>

- *Yusuke Komatsu, Shohei Shimizu, Hidetoshi Shimodaira*

- `0909.2904v2` - [abs](http://arxiv.org/abs/0909.2904v2) - [pdf](http://arxiv.org/pdf/0909.2904v2)

> Structural equation models and Bayesian networks have been widely used to study causal relationships between continuous variables. Recently, a non-Gaussian method called LiNGAM was proposed to discover such causal models and has been extended in various directions. An important problem with LiNGAM is that the results are affected by the random sampling of the data as with any statistical method. Thus, some analysis of the statistical reliability or confidence level should be conducted. A common method to evaluate a confidence level is a bootstrap method. However, a confidence level computed by ordinary bootstrap method is known to be biased as a probability-value ($p$-value) of hypothesis testing. In this paper, we propose a new procedure to apply an advanced bootstrap method called multiscale bootstrap to compute confidence levels, i.e., p-values, of LiNGAM outputs. The multiscale bootstrap method gives unbiased $p$-values with asymptotic much higher accuracy. Experiments on artificial data demonstrate the utility of our approach.

</details>

<details>

<summary>2010-06-22 20:16:46 - Computing the Bayesian Factor from a Markov chain Monte Carlo Simulation of the Posterior Distribution</summary>

- *Martin D. Weinberg*

- `0911.1777v2` - [abs](http://arxiv.org/abs/0911.1777v2) - [pdf](http://arxiv.org/pdf/0911.1777v2)

> Computation of the marginal likelihood from a simulated posterior distribution is central to Bayesian model selection but is computationally difficult. I argue that the marginal likelihood can be reliably computed from a posterior sample by careful attention to the numerics of the probability integral. Posing the expression for the marginal likelihood as a Lebesgue integral, we may convert the harmonic mean approximation from a sample statistic to a quadrature rule. As a quadrature, the harmonic mean approximation suffers from enormous truncation error as consequence . In addition, I demonstrate that the integral expression for the harmonic-mean approximation converges slowly at best for high-dimensional problems with uninformative prior distributions. These observations lead to two computationally-modest families of quadrature algorithms that use the full generality sample posterior but without the instability. The first algorithm automatically eliminates the part of the sample that contributes large truncation error. The second algorithm uses the posterior sample to assign probability to a partition of the sample space and performs the marginal likelihood integral directly. This eliminates convergence issues. The first algorithm is analogous to standard quadrature but can only be applied for convergent problems. The second is a hybrid of cubature: it uses the posterior to discover and tessellate the subset of that sample space was explored and uses quantiles to compute a representive field value. Neither algorithm makes strong assumptions about the shape of the posterior distribution and neither is sensitive outliers. [abridged]

</details>

<details>

<summary>2010-06-25 08:08:17 - Detecting epistasis via Markov bases</summary>

- *Anna-Sapfo Malaspinas, Caroline Uhler*

- `1006.4929v1` - [abs](http://arxiv.org/abs/1006.4929v1) - [pdf](http://arxiv.org/pdf/1006.4929v1)

> Rapid research progress in genotyping techniques have allowed large genome-wide association studies. Existing methods often focus on determining associations between single loci and a specific phenotype. However, a particular phenotype is usually the result of complex relationships between multiple loci and the environment. In this paper, we describe a two-stage method for detecting epistasis by combining the traditionally used single-locus search with a search for multiway interactions. Our method is based on an extended version of Fisher's exact test. To perform this test, a Markov chain is constructed on the space of multidimensional contingency tables using the elements of a Markov basis as moves. We test our method on simulated data and compare it to a two-stage logistic regression method and to a fully Bayesian method, showing that we are able to detect the interacting loci when other methods fail to do so. Finally, we apply our method to a genome-wide data set consisting of 685 dogs and identify epistasis associated with canine hair length for four pairs of SNPs.

</details>

<details>

<summary>2010-06-26 22:11:47 - Bayesian Gene Set Analysis</summary>

- *Babak Shahbaba, Robert Tibshirani, Catherine M. Shachaf, Sylvia K. Plevritis*

- `1006.5170v1` - [abs](http://arxiv.org/abs/1006.5170v1) - [pdf](http://arxiv.org/pdf/1006.5170v1)

> Gene expression microarray technologies provide the simultaneous measurements of a large number of genes. Typical analyses of such data focus on the individual genes, but recent work has demonstrated that evaluating changes in expression across predefined sets of genes often increases statistical power and produces more robust results. We introduce a new methodology for identifying gene sets that are differentially expressed under varying experimental conditions. Our approach uses a hierarchical Bayesian framework where a hyperparameter measures the significance of each gene set. Using simulated data, we compare our proposed method to alternative approaches, such as Gene Set Enrichment Analysis (GSEA) and Gene Set Analysis (GSA). Our approach provides the best overall performance. We also discuss the application of our method to experimental data based on p53 mutation status.

</details>

<details>

<summary>2010-06-30 15:23:56 - Penalized Composite Quasi-Likelihood for Ultrahigh-Dimensional Variable Selection</summary>

- *Jelena Bradic, Jianqing Fan, Weiwei Wang*

- `0912.5200v2` - [abs](http://arxiv.org/abs/0912.5200v2) - [pdf](http://arxiv.org/pdf/0912.5200v2)

> In high-dimensional model selection problems, penalized simple least-square approaches have been extensively used. This paper addresses the question of both robustness and efficiency of penalized model selection methods, and proposes a data-driven weighted linear combination of convex loss functions, together with weighted $L_1$-penalty. It is completely data-adaptive and does not require prior knowledge of the error distribution. The weighted $L_1$-penalty is used both to ensure the convexity of the penalty term and to ameliorate the bias caused by the $L_1$-penalty. In the setting with dimensionality much larger than the sample size, we establish a strong oracle property of the proposed method that possesses both the model selection consistency and estimation efficiency for the true non-zero coefficients. As specific examples, we introduce a robust method of composite L1-L2, and optimal composite quantile method and evaluate their performance in both simulated and real data examples.

</details>


## 2010-07

<details>

<summary>2010-07-04 19:50:40 - Risk bounds in linear regression through PAC-Bayesian truncation</summary>

- *Jean-Yves Audibert, Olivier Catoni*

- `0902.1733v2` - [abs](http://arxiv.org/abs/0902.1733v2) - [pdf](http://arxiv.org/pdf/0902.1733v2)

> We consider the problem of predicting as well as the best linear combination of d given functions in least squares regression, and variants of this problem including constraints on the parameters of the linear combination. When the input distribution is known, there already exists an algorithm having an expected excess risk of order d/n, where n is the size of the training data. Without this strong assumption, standard results often contain a multiplicative log n factor, and require some additional assumptions like uniform boundedness of the d-dimensional input representation and exponential moments of the output. This work provides new risk bounds for the ridge estimator and the ordinary least squares estimator, and their variants. It also provides shrinkage procedures with convergence rate d/n (i.e., without the logarithmic factor) in expectation and in deviations, under various assumptions. The key common surprising factor of these results is the absence of exponential moment condition on the output distribution while achieving exponential deviations. All risk bounds are obtained through a PAC-Bayesian analysis on truncated differences of losses. Finally, we show that some of these results are not particular to the least squares loss, but can be generalized to similar strongly convex loss functions.

</details>

<details>

<summary>2010-07-05 16:39:50 - Optimization Under Unknown Constraints</summary>

- *Robert B. Gramacy, Herbert K. H. Lee*

- `1004.4027v2` - [abs](http://arxiv.org/abs/1004.4027v2) - [pdf](http://arxiv.org/pdf/1004.4027v2)

> Optimization of complex functions, such as the output of computer simulators, is a difficult task that has received much attention in the literature. A less studied problem is that of optimization under unknown constraints, i.e., when the simulator must be invoked both to determine the typical real-valued response and to determine if a constraint has been violated, either for physical or policy reasons. We develop a statistical approach based on Gaussian processes and Bayesian learning to both approximate the unknown function and estimate the probability of meeting the constraints. A new integrated improvement criterion is proposed to recognize that responses from inputs that violate the constraint may still be informative about the function, and thus could potentially be useful in the optimization. The new criterion is illustrated on synthetic data, and on a motivating optimization problem from health care policy.

</details>

<details>

<summary>2010-07-06 22:53:53 - Reference priors for high energy physics</summary>

- *Luc Demortier, Supriya Jain, Harrison B. Prosper*

- `1002.1111v2` - [abs](http://arxiv.org/abs/1002.1111v2) - [pdf](http://arxiv.org/pdf/1002.1111v2)

> Bayesian inferences in high energy physics often use uniform prior distributions for parameters about which little or no information is available before data are collected. The resulting posterior distributions are therefore sensitive to the choice of parametrization for the problem and may even be improper if this choice is not carefully considered. Here we describe an extensively tested methodology, known as reference analysis, which allows one to construct parametrization-invariant priors that embody the notion of minimal informativeness in a mathematically well-defined sense. We apply this methodology to general cross section measurements and show that it yields sensible results. A recent measurement of the single top quark cross section illustrates the relevant techniques in a realistic situation.

</details>

<details>

<summary>2010-07-07 01:24:41 - Approximating quantiles in very large datasets</summary>

- *Reza Hosseini*

- `1007.1032v1` - [abs](http://arxiv.org/abs/1007.1032v1) - [pdf](http://arxiv.org/pdf/1007.1032v1)

> Very large datasets are often encountered in climatology, either from a multiplicity of observations over time and space or outputs from deterministic models (sometimes in petabytes= 1 million gigabytes). Loading a large data vector and sorting it, is impossible sometimes due to memory limitations or computing power. We show that a proposed algorithm to approximating the median, "the median of the median" performs poorly. Instead we develop an algorithm to approximate quantiles of very large datasets which works by partitioning the data or use existing partitions (possibly of non-equal size). We show the deterministic precision of this algorithm and how it can be adjusted to get customized precisions.

</details>

<details>

<summary>2010-07-10 22:11:40 - Learning Bayesian Networks with the bnlearn R Package</summary>

- *Marco Scutari*

- `0908.3817v2` - [abs](http://arxiv.org/abs/0908.3817v2) - [pdf](http://arxiv.org/pdf/0908.3817v2)

> bnlearn is an R package which includes several algorithms for learning the structure of Bayesian networks with either discrete or continuous variables. Both constraint-based and score-based algorithms are implemented, and can use the functionality provided by the snow package to improve their performance via parallel computing. Several network scores and conditional independence algorithms are available for both the learning algorithms and independent use. Advanced plotting options are provided by the Rgraphviz package.

</details>

<details>

<summary>2010-07-14 10:20:16 - An Algorithm for Learning the Essential Graph</summary>

- *John M. Noble*

- `1007.2656v1` - [abs](http://arxiv.org/abs/1007.2656v1) - [pdf](http://arxiv.org/pdf/1007.2656v1)

> This article presents an algorithm for learning the essential graph of a Bayesian network. The basis of the algorithm is the Maximum Minimum Parents and Children algorithm developed by previous authors, with three substantial modifications. The MMPC algorithm is the first stage of the Maximum Minimum Hill Climbing algorithm for learning the directed acyclic graph of a Bayesian network, introduced by previous authors. The MMHC algorithm runs in two phases; firstly, the MMPC algorithm to locate the skeleton and secondly an edge orientation phase. The computationally expensive part is the edge orientation phase.   The first modification introduced to the MMPC algorithm, which requires little additional computational cost, is to obtain the immoralities and hence the essential graph. This renders the edge orientation phase, the computationally expensive part, unnecessary, since the entire Markov structure that can be derived from data is present in the essential graph.   Secondly, the MMPC algorithm can accept independence statements that are logically inconsistent with those rejected, since with tests for independence, a `do not reject' conclusion for a particular independence statement is taken as `accept' independence. An example is given to illustrate this and a modification is suggested to ensure that the conditional independence statements are logically consistent.   Thirdly, the MMHC algorithm makes an assumption of faithfulness. An example of a data set is given that does not satisfy this assumption and a modification is suggested to deal with some situations where the assumption is not satisfied. The example in question also illustrates problems with the `faithfulness' assumption that cannot be tackled by this modification.

</details>

<details>

<summary>2010-07-15 18:27:01 - Dirichlet Process Mixtures of Generalized Linear Models</summary>

- *Lauren A. Hannah, David M. Blei, Warren B. Powell*

- `0909.5194v2` - [abs](http://arxiv.org/abs/0909.5194v2) - [pdf](http://arxiv.org/pdf/0909.5194v2)

> We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM), a new method of nonparametric regression that accommodates continuous and categorical inputs, and responses that can be modeled by a generalized linear model. We prove conditions for the asymptotic unbiasedness of the DP-GLM regression mean function estimate. We also give examples for when those conditions hold, including models for compactly supported continuous distributions and a model with continuous covariates and categorical response. We empirically analyze the properties of the DP-GLM and why it provides better results than existing Dirichlet process mixture regression models. We evaluate DP-GLM on several data sets, comparing it to modern methods of nonparametric regression like CART, Bayesian trees and Gaussian processes. Compared to existing techniques, the DP-GLM provides a single model (and corresponding inference algorithms) that performs well in many regression settings.

</details>

<details>

<summary>2010-07-23 07:57:04 - Support Vector Machines for Additive Models: Consistency and Robustness</summary>

- *Andreas Christmann, Robert Hable*

- `1007.4062v1` - [abs](http://arxiv.org/abs/1007.4062v1) - [pdf](http://arxiv.org/pdf/1007.4062v1)

> Support vector machines (SVMs) are special kernel based methods and belong to the most successful learning methods since more than a decade. SVMs can informally be described as a kind of regularized M-estimators for functions and have demonstrated their usefulness in many complicated real-life problems. During the last years a great part of the statistical research on SVMs has concentrated on the question how to design SVMs such that they are universally consistent and statistically robust for nonparametric classification or nonparametric regression purposes. In many applications, some qualitative prior knowledge of the distribution P or of the unknown function f to be estimated is present or the prediction function with a good interpretability is desired, such that a semiparametric model or an additive model is of interest.   In this paper we mainly address the question how to design SVMs by choosing the reproducing kernel Hilbert space (RKHS) or its corresponding kernel to obtain consistent and statistically robust estimators in additive models. We give an explicit construction of kernels - and thus of their RKHSs - which leads in combination with a Lipschitz continuous loss function to consistent and statistically robust SMVs for additive models. Examples are quantile regression based on the pinball loss function, regression based on the epsilon-insensitive loss function, and classification based on the hinge loss function.

</details>

<details>

<summary>2010-07-24 15:00:24 - Variational Bayesian Inference and Complexity Control for Stochastic Block Models</summary>

- *Pierre Latouche, Etienne Birmele, Christophe Ambroise*

- `0912.2873v2` - [abs](http://arxiv.org/abs/0912.2873v2) - [pdf](http://arxiv.org/pdf/0912.2873v2)

> It is now widely accepted that knowledge can be acquired from networks by clustering their vertices according to connection profiles. Many methods have been proposed and in this paper we concentrate on the Stochastic Block Model (SBM). The clustering of vertices and the estimation of SBM model parameters have been subject to previous work and numerous inference strategies such as variational Expectation Maximization (EM) and classification EM have been proposed. However, SBM still suffers from a lack of criteria to estimate the number of components in the mixture. To our knowledge, only one model based criterion, ICL, has been derived for SBM in the literature. It relies on an asymptotic approximation of the Integrated Complete-data Likelihood and recent studies have shown that it tends to be too conservative in the case of small networks. To tackle this issue, we propose a new criterion that we call ILvb, based on a non asymptotic approximation of the marginal likelihood. We describe how the criterion can be computed through a variational Bayes EM algorithm.

</details>

<details>

<summary>2010-07-26 17:46:47 - Bayesian Segmentation of Oceanic SAR Images: Application to Oil Spill Detection</summary>

- *Sónia Pelizzari, José M. Bioucas-Dias*

- `1007.4969v1` - [abs](http://arxiv.org/abs/1007.4969v1) - [pdf](http://arxiv.org/pdf/1007.4969v1)

> This paper introduces Bayesian supervised and unsupervised segmentation algorithms aimed at oceanic segmentation of SAR images. The data term, \emph{i.e}., the density of the observed backscattered signal given the region, is modeled by a finite mixture of Gamma densities with a given predefined number of components. To estimate the parameters of the class conditional densities, a new expectation maximization algorithm was developed. The prior is a multi-level logistic Markov random field enforcing local continuity in a statistical sense. The smoothness parameter controlling the degree of homogeneity imposed on the scene is automatically estimated, by computing the evidence with loopy belief propagation; the classical coding and least squares fit methods are also considered. The maximum a posteriori segmentation is computed efficiently by means of recent graph-cut techniques, namely the $\alpha$-Expansion algorithm that extends the methodology to an optional number of classes. The effectiveness of the proposed approaches is illustrated with simulated images and real ERS and Envisat scenes containing oil spills.

</details>

<details>

<summary>2010-07-27 01:41:21 - Bayesian Symbol Detection in Wireless Relay Networks via Likelihood-Free Inference</summary>

- *Gareth W. Peters, Ido Nevat, Scott A. Sisson, Yanan Fan, Jinhong Yuan*

- `1007.4603v1` - [abs](http://arxiv.org/abs/1007.4603v1) - [pdf](http://arxiv.org/pdf/1007.4603v1)

> This paper presents a general stochastic model developed for a class of cooperative wireless relay networks, in which imperfect knowledge of the channel state information at the destination node is assumed. The framework incorporates multiple relay nodes operating under general known non-linear processing functions. When a non-linear relay function is considered, the likelihood function is generally intractable resulting in the maximum likelihood and the maximum a posteriori detectors not admitting closed form solutions. We illustrate our methodology to overcome this intractability under the example of a popular optimal non-linear relay function choice and demonstrate how our algorithms are capable of solving the previously intractable detection problem. Overcoming this intractability involves development of specialised Bayesian models. We develop three novel algorithms to perform detection for this Bayesian model, these include a Markov chain Monte Carlo Approximate Bayesian Computation (MCMC-ABC) approach; an Auxiliary Variable MCMC (MCMC-AV) approach; and a Suboptimal Exhaustive Search Zero Forcing (SES-ZF) approach. Finally, numerical examples comparing the symbol error rate (SER) performance versus signal to noise ratio (SNR) of the three detection algorithms are studied in simulated examples.

</details>

<details>

<summary>2010-07-29 13:54:46 - New Results for the MAP Problem in Bayesian Networks</summary>

- *Cassio P. de Campos*

- `1007.3884v2` - [abs](http://arxiv.org/abs/1007.3884v2) - [pdf](http://arxiv.org/pdf/1007.3884v2)

> This paper presents new results for the (partial) maximum a posteriori (MAP) problem in Bayesian networks, which is the problem of querying the most probable state configuration of some of the network variables given evidence. First, it is demonstrated that the problem remains hard even in networks with very simple topology, such as binary polytrees and simple trees (including the Naive Bayes structure). Such proofs extend previous complexity results for the problem. Inapproximability results are also derived in the case of trees if the number of states per variable is not bounded. Although the problem is shown to be hard and inapproximable even in very simple scenarios, a new exact algorithm is described that is empirically fast in networks of bounded treewidth and bounded number of states per variable. The same algorithm is used as basis of a Fully Polynomial Time Approximation Scheme for MAP under such assumptions. Approximation schemes were generally thought to be impossible for this problem, but we show otherwise for classes of networks that are important in practice. The algorithms are extensively tested using some well-known networks as well as random generated cases to show their effectiveness.

</details>

<details>

<summary>2010-07-31 20:30:32 - Bayesian Model Selection for Beta Autoregressive Processes</summary>

- *R. Casarin, L. Dalla Valle, F. Leisen*

- `1008.0121v1` - [abs](http://arxiv.org/abs/1008.0121v1) - [pdf](http://arxiv.org/pdf/1008.0121v1)

> We deal with Bayesian inference for Beta autoregressive processes. We restrict our attention to the class of conditionally linear processes. These processes are particularly suitable for forecasting purposes, but are difficult to estimate due to the constraints on the parameter space. We provide a full Bayesian approach to the estimation and include the parameter restrictions in the inference problem by a suitable specification of the prior distributions. Moreover in a Bayesian framework parameter estimation and model choice can be solved simultaneously. In particular we suggest a Markov-Chain Monte Carlo (MCMC) procedure based on a Metropolis-Hastings within Gibbs algorithm and solve the model selection problem following a reversible jump MCMC approach.

</details>


## 2010-08

<details>

<summary>2010-08-01 06:29:32 - Bayesian Cointegrated Vector Autoregression models incorporating Alpha-stable noise for inter-day price movements via Approximate Bayesian Computation</summary>

- *Gareth W. Peters, Balakrishnan B. Kannan, Ben Lasscock, Chris Mellen, Simon Godsill*

- `1008.0149v1` - [abs](http://arxiv.org/abs/1008.0149v1) - [pdf](http://arxiv.org/pdf/1008.0149v1)

> We consider a statistical model for pairs of traded assets, based on a Cointegrated Vector Auto Regression (CVAR) Model. We extend standard CVAR models to incorporate estimation of model parameters in the presence of price series level shifts which are not accurately modeled in the standard Gaussian error correction model (ECM) framework. This involves developing a novel matrix variate Bayesian CVAR mixture model comprised of Gaussian errors intra-day and Alpha-stable errors inter-day in the ECM framework. To achieve this we derive a novel conjugate posterior model for the Scaled Mixtures of Normals (SMiN CVAR) representation of Alpha-stable inter-day innovations. These results are generalized to asymmetric models for the innovation noise at inter-day boundaries allowing for skewed Alpha-stable models.   Our proposed model and sampling methodology is general, incorporating the current literature on Gaussian models as a special subclass and also allowing for price series level shifts either at random estimated time points or known a priori time points. We focus analysis on regularly observed non-Gaussian level shifts that can have significant effect on estimation performance in statistical models failing to account for such level shifts, such as at the close and open of markets. We compare the estimation accuracy of our model and estimation approach to standard frequentist and Bayesian procedures for CVAR models when non-Gaussian price series level shifts are present in the individual series, such as inter-day boundaries. We fit a bi-variate Alpha-stable model to the inter-day jumps and model the effect of such jumps on estimation of matrix-variate CVAR model parameters using the likelihood based Johansen procedure and a Bayesian estimation. We illustrate our model and the corresponding estimation procedures we develop on both synthetic and actual data.

</details>

<details>

<summary>2010-08-07 16:58:09 - Control Variates for Reversible MCMC Samplers</summary>

- *Petros Dellaportas, Ioannis Kontoyiannis*

- `1008.1355v1` - [abs](http://arxiv.org/abs/1008.1355v1) - [pdf](http://arxiv.org/pdf/1008.1355v1)

> A general methodology is introduced for the construction and effective application of control variates to estimation problems involving data from reversible MCMC samplers. We propose the use of a specific class of functions as control variates, and we introduce a new, consistent estimator for the values of the coefficients of the optimal linear combination of these functions. The form and proposed construction of the control variates is derived from our solution of the Poisson equation associated with a specific MCMC scenario. The new estimator, which can be applied to the same MCMC sample, is derived from a novel, finite-dimensional, explicit representation for the optimal coefficients. The resulting variance-reduction methodology is primarily applicable when the simulated data are generated by a conjugate random-scan Gibbs sampler. MCMC examples of Bayesian inference problems demonstrate that the corresponding reduction in the estimation variance is significant, and that in some cases it can be quite dramatic. Extensions of this methodology in several directions are given, including certain families of Metropolis-Hastings samplers and hybrid Metropolis-within-Gibbs algorithms. Corresponding simulation examples are presented illustrating the utility of the proposed methods. All methodological and asymptotic arguments are rigorously justified under easily verifiable and essentially minimal conditions.

</details>

<details>

<summary>2010-08-09 17:11:17 - Hyper-g Priors for Generalized Linear Models</summary>

- *Daniel Sabanés Bové, Leonhard Held*

- `1008.1550v1` - [abs](http://arxiv.org/abs/1008.1550v1) - [pdf](http://arxiv.org/pdf/1008.1550v1)

> We develop an extension of the classical Zellner's g-prior to generalized linear models. The prior on the hyperparameter g is handled in a flexible way, so that any continuous proper hyperprior f(g) can be used, giving rise to a large class of hyper-g priors. Connections with the literature are described in detail. A fast and accurate integrated Laplace approximation of the marginal likelihood makes inference in large model spaces feasible. For posterior parameter estimation we propose an efficient and tuning-free Metropolis-Hastings sampler. The methodology is illustrated with variable selection and automatic covariate transformation in the Pima Indians diabetes data set.

</details>

<details>

<summary>2010-08-10 08:30:14 - Gaussian Process Models for Nonparametric Functional Regression with Functional Responses</summary>

- *Heng Lian*

- `1008.1647v1` - [abs](http://arxiv.org/abs/1008.1647v1) - [pdf](http://arxiv.org/pdf/1008.1647v1)

> Recently nonparametric functional model with functional responses has been proposed within the functional reproducing kernel Hilbert spaces (fRKHS) framework. Motivated by its superior performance and also its limitations, we propose a Gaussian process model whose posterior mode coincide with the fRKHS estimator. The Bayesian approach has several advantages compared to its predecessor. Firstly, the multiple unknown parameters can be inferred together with the regression function in a unified framework. Secondly, as a Bayesian method, the statistical inferences are straightforward through the posterior distributions. We also use the predictive process models adapted from the spatial statistics literature to overcome the computational limitations, thus extending the applicability of this popular technique to a new problem. Modifications of predictive process models are nevertheless critical in our context to obtain valid inferences. The numerical results presented demonstrate the effectiveness of the modifications.

</details>

<details>

<summary>2010-08-12 00:41:23 - Discovering shared and individual latent structure in multiple time series</summary>

- *Suchi Saria, Daphne Koller, Anna Penn*

- `1008.2028v1` - [abs](http://arxiv.org/abs/1008.2028v1) - [pdf](http://arxiv.org/pdf/1008.2028v1)

> This paper proposes a nonparametric Bayesian method for exploratory data analysis and feature construction in continuous time series. Our method focuses on understanding shared features in a set of time series that exhibit significant individual variability. Our method builds on the framework of latent Diricihlet allocation (LDA) and its extension to hierarchical Dirichlet processes, which allows us to characterize each series as switching between latent ``topics'', where each topic is characterized as a distribution over ``words'' that specify the series dynamics. However, unlike standard applications of LDA, we discover the words as we learn the model. We apply this model to the task of tracking the physiological signals of premature infants; our model obtains clinically significant insights as well as useful features for supervised learning tasks.

</details>

<details>

<summary>2010-08-12 17:31:48 - Separable covariance arrays via the Tucker product, with applications to multivariate relational data</summary>

- *Peter D. Hoff*

- `1008.2169v1` - [abs](http://arxiv.org/abs/1008.2169v1) - [pdf](http://arxiv.org/pdf/1008.2169v1)

> Modern datasets are often in the form of matrices or arrays,potentially having correlations along each set of data indices. For example, data involving repeated measurements of several variables over time may exhibit temporal correlation as well as correlation among the variables. A possible model for matrix-valued data is the class of matrix normal distributions, which is parametrized by two covariance matrices, one for each index set of the data. In this article we describe an extension of the matrix normal model to accommodate multidimensional data arrays, or tensors. We generate a class of array normal distributions by applying a group of multilinear transformations to an array of independent standard normal random variables. The covariance structures of the resulting class take the form of outer products of dimension-specific covariance matrices. We derive some properties of these covariance structures and the corresponding array normal distributions, discuss maximum likelihood and Bayesian estimation of covariance parameters and illustrate the model in an analysis of multivariate longitudinal network data.

</details>

<details>

<summary>2010-08-12 20:29:23 - Large Scale Variational Inference and Experimental Design for Sparse Generalized Linear Models</summary>

- *Matthias W. Seeger, Hannes Nickisch*

- `0810.0901v2` - [abs](http://arxiv.org/abs/0810.0901v2) - [pdf](http://arxiv.org/pdf/0810.0901v2)

> Many problems of low-level computer vision and image processing, such as denoising, deconvolution, tomographic reconstruction or super-resolution, can be addressed by maximizing the posterior distribution of a sparse linear model (SLM). We show how higher-order Bayesian decision-making problems, such as optimizing image acquisition in magnetic resonance scanners, can be addressed by querying the SLM posterior covariance, unrelated to the density's mode. We propose a scalable algorithmic framework, with which SLM posteriors over full, high-resolution images can be approximated for the first time, solving a variational optimization problem which is convex iff posterior mode finding is convex. These methods successfully drive the optimization of sampling trajectories for real-world magnetic resonance imaging through Bayesian experimental design, which has not been attempted before. Our methodology provides new insight into similarities and differences between sparse reconstruction and approximate Bayesian inference, and has important implications for compressive sensing of real-world images.

</details>

<details>

<summary>2010-08-14 15:20:25 - Inference and Optimal Design for Nearest-Neighbour Interaction Models</summary>

- *Andrei Iu. Bejan, Gavin J. Gibson, Stan Zachary*

- `1008.2453v1` - [abs](http://arxiv.org/abs/1008.2453v1) - [pdf](http://arxiv.org/pdf/1008.2453v1)

> We consider problems of Bayesian inference for a spatial epidemic on a graph, where the final state of the epidemic corresponds to bond percolation, and where only the set or number of finally infected sites is observed. We develop appropriate Markov chain Monte Carlo algorithms, demonstrating their effectiveness, and we study problems of optimal experimental design. In particular, we demonstrate that for lattice-based processes an experiment on a sparsified lattice can yield more information on model parameters than one conducted on a complete lattice. We also prove some probabilistic results about the behaviour of estimators associated with large infected clusters.

</details>

<details>

<summary>2010-08-15 12:26:24 - Epistemic irrelevance in credal nets: the case of imprecise Markov trees</summary>

- *Gert de Cooman, Filip Hermans, Alessandro Antonucci, Marco Zaffalon*

- `1008.2514v1` - [abs](http://arxiv.org/abs/1008.2514v1) - [pdf](http://arxiv.org/pdf/1008.2514v1)

> We focus on credal nets, which are graphical models that generalise Bayesian nets to imprecise probability. We replace the notion of strong independence commonly used in credal nets with the weaker notion of epistemic irrelevance, which is arguably more suited for a behavioural theory of probability. Focusing on directed trees, we show how to combine the given local uncertainty models in the nodes of the graph into a global model, and we use this to construct and justify an exact message-passing algorithm that computes updated beliefs for a variable in the tree. The algorithm, which is linear in the number of nodes, is formulated entirely in terms of coherent lower previsions, and is shown to satisfy a number of rationality requirements. We supply examples of the algorithm's operation, and report an application to on-line character recognition that illustrates the advantages of our approach for prediction. We comment on the perspectives, opened by the availability, for the first time, of a truly efficient algorithm based on epistemic irrelevance.

</details>

<details>

<summary>2010-08-19 19:56:20 - Learning the Structure of Deep Sparse Graphical Models</summary>

- *Ryan Prescott Adams, Hanna M. Wallach, Zoubin Ghahramani*

- `1001.0160v2` - [abs](http://arxiv.org/abs/1001.0160v2) - [pdf](http://arxiv.org/pdf/1001.0160v2)

> Deep belief networks are a powerful way to model complex probability distributions. However, learning the structure of a belief network, particularly one with hidden units, is difficult. The Indian buffet process has been used as a nonparametric Bayesian prior on the directed structure of a belief network with a single infinitely wide hidden layer. In this paper, we introduce the cascading Indian buffet process (CIBP), which provides a nonparametric prior on the structure of a layered, directed belief network that is unbounded in both depth and width, yet allows tractable inference. We use the CIBP prior with the nonlinear Gaussian belief network so each unit can additionally vary its behavior between discrete and continuous representations. We provide Markov chain Monte Carlo algorithms for inference in these belief networks and explore the structures learned on several image data sets.

</details>

<details>

<summary>2010-08-24 14:47:38 - The distribution of the square sum of Dirichlet random variables and a table with quantiles of Greenwood's statistic</summary>

- *Thomas Royen*

- `1008.4059v1` - [abs](http://arxiv.org/abs/1008.4059v1) - [pdf](http://arxiv.org/pdf/1008.4059v1)

> The exact distribution of the square sum of Dirichlet random variables is given by two different univariate integral representations. Alternatively, three representations by orthogonal series with Jacobi or Legendre polynomials are derived. As a special case the distribution of the square sum of spacings - also called Greenwood's statistic - is obtained. Nine quantiles of this statistic are tabulated with eight digits where the number of squares ranges from 10 to 100.

</details>

<details>

<summary>2010-08-31 08:05:28 - Rate estimation in partially observed Markov jump processes with measurement errors</summary>

- *Michael Amrein, Hans R. Kuensch*

- `1008.5246v1` - [abs](http://arxiv.org/abs/1008.5246v1) - [pdf](http://arxiv.org/pdf/1008.5246v1)

> We present a simulation methodology for Bayesian estimation of rate parameters in Markov jump processes arising for example in stochastic kinetic models. To handle the problem of missing components and measurement errors in observed data, we embed the Markov jump process into the framework of a general state space model. We do not use diffusion approximations. Markov chain Monte Carlo and particle filter type algorithms are introduced, which allow sampling from the posterior distribution of the rate parameters and the Markov jump process also in data-poor scenarios. The algorithms are illustrated by applying them to rate estimation in a model for prokaryotic auto-regulation and in the stochastic Oregonator, respectively.

</details>


## 2010-09

<details>

<summary>2010-09-02 12:17:02 - A Novel Chronic Disease Policy Model</summary>

- *Nathan Green, Duncan Smith, Matthew Sperrin, Iain Buchan*

- `1009.0405v1` - [abs](http://arxiv.org/abs/1009.0405v1) - [pdf](http://arxiv.org/pdf/1009.0405v1)

> We develop a simulation tool to support policy-decisions about healthcare for chronic diseases in defined populations. Incident disease-cases are generated in-silico from an age-sex characterised general population using standard epidemiological approaches. A novel disease-treatment model then simulates continuous life courses for each patient using discrete event simulation. Ideally, the discrete event simulation model would be inferred from complete longitudinal healthcare data via a likelihood or Bayesian approach. Such data is seldom available for relevant populations, therefore an innovative approach to evidence synthesis is required. We propose a novel entropy-based approach to fit survival densities. This method provides a fully flexible way to incorporate the available information, which can be derived from arbitrary sources. Discrete event simulation then takes place on the fitted model using a competing hazards framework. The output is then used to help evaluate the potential impacts of policy options for a given population.

</details>

<details>

<summary>2010-09-02 18:28:22 - A PAC-Bayesian Analysis of Graph Clustering and Pairwise Clustering</summary>

- *Yevgeny Seldin*

- `1009.0499v1` - [abs](http://arxiv.org/abs/1009.0499v1) - [pdf](http://arxiv.org/pdf/1009.0499v1)

> We formulate weighted graph clustering as a prediction problem: given a subset of edge weights we analyze the ability of graph clustering to predict the remaining edge weights. This formulation enables practical and theoretical comparison of different approaches to graph clustering as well as comparison of graph clustering with other possible ways to model the graph. We adapt the PAC-Bayesian analysis of co-clustering (Seldin and Tishby, 2008; Seldin, 2009) to derive a PAC-Bayesian generalization bound for graph clustering. The bound shows that graph clustering should optimize a trade-off between empirical data fit and the mutual information that clusters preserve on the graph nodes. A similar trade-off derived from information-theoretic considerations was already shown to produce state-of-the-art results in practice (Slonim et al., 2005; Yom-Tov and Slonim, 2009). This paper supports the empirical evidence by providing a better theoretical foundation, suggesting formal generalization guarantees, and offering a more accurate way to deal with finite sample issues. We derive a bound minimization algorithm and show that it provides good results in real-life problems and that the derived PAC-Bayesian bound is reasonably tight.

</details>

<details>

<summary>2010-09-09 10:39:36 - Extreme shock models: an alternative perspective</summary>

- *Pasquale Cirillo, Jürg Hüsler*

- `1009.1732v1` - [abs](http://arxiv.org/abs/1009.1732v1) - [pdf](http://arxiv.org/pdf/1009.1732v1)

> Extreme shock models have been introduced in Gut and H\"usler (1999) to study systems that at random times are subject to shock of random magnitude. These systems break down when some shock overcomes a given resistance level. In this paper we propose an alternative approach to extreme shock models using reinforced urn processes. As a consequence of this we are able to look at the same problem under a Bayesian nonparametric perspective, providing the predictive distribution of systems' defaults.

</details>

<details>

<summary>2010-09-13 06:02:37 - Bayesian Adaptive Lasso</summary>

- *Chenlei Leng, Minh Ngoc Tran, David Nott*

- `1009.2300v1` - [abs](http://arxiv.org/abs/1009.2300v1) - [pdf](http://arxiv.org/pdf/1009.2300v1)

> We propose the Bayesian adaptive Lasso (BaLasso) for variable selection and coefficient estimation in linear regression. The BaLasso is adaptive to the signal level by adopting different shrinkage for different coefficients. Furthermore, we provide a model selection machinery for the BaLasso by assessing the posterior conditional mode estimates, motivated by the hierarchical Bayesian interpretation of the Lasso. Our formulation also permits prediction using a model averaging strategy. We discuss other variants of this new approach and provide a unified framework for variable selection using flexible penalties. Empirical evidence of the attractiveness of the method is demonstrated via extensive simulation studies and data analysis.

</details>

<details>

<summary>2010-09-13 13:20:04 - MAP model selection in Gaussian regression</summary>

- *Felix Abramovich, Vadim Grinshtein*

- `0912.4387v3` - [abs](http://arxiv.org/abs/0912.4387v3) - [pdf](http://arxiv.org/pdf/0912.4387v3)

> We consider a Bayesian approach to model selection in Gaussian linear regression, where the number of predictors might be much larger than the number of observations. From a frequentist view, the proposed procedure results in the penalized least squares estimation with a complexity penalty associated with a prior on the model size. We investigate the optimality properties of the resulting estimator. We establish the oracle inequality and specify conditions on the prior that imply its asymptotic minimaxity within a wide range of sparse and dense settings for "nearly-orthogonal" and "multicollinear" designs.

</details>

<details>

<summary>2010-09-13 19:01:15 - Estimation of conditional laws given an extreme component</summary>

- *Anne-Laure Fougères, Philippe Soulier*

- `0806.2426v4` - [abs](http://arxiv.org/abs/0806.2426v4) - [pdf](http://arxiv.org/pdf/0806.2426v4)

> Let $(X,Y)$ be a bivariate random vector. The estimation of a probability of the form $P(Y\leq y \mid X >t) $ is challenging when $t$ is large, and a fruitful approach consists in studying, if it exists, the limiting conditional distribution of the random vector $(X,Y)$, suitably normalized, given that $X$ is large. There already exists a wide literature on bivariate models for which this limiting distribution exists. In this paper, a statistical analysis of this problem is done. Estimators of the limiting distribution (which is assumed to exist) and the normalizing functions are provided, as well as an estimator of the conditional quantile function when the conditioning event is extreme. Consistency of the estimators is proved and a functional central limit theorem for the estimator of the limiting distribution is obtained. The small sample behavior of the estimator of the conditional quantile function is illustrated through simulations.

</details>

<details>

<summary>2010-09-15 14:37:29 - Multiscale Modelling and Inverse Problems</summary>

- *J. Nolen, G. A. Pavliotis, A. M. Stuart*

- `1009.2943v1` - [abs](http://arxiv.org/abs/1009.2943v1) - [pdf](http://arxiv.org/pdf/1009.2943v1)

> The need to blend observational data and mathematical models arises in many applications and leads naturally to inverse problems. Parameters appearing in the model, such as constitutive tensors, initial conditions, boundary conditions, and forcing can be estimated on the basis of observed data. The resulting inverse problems are often ill-posed and some form of regularization is required. These notes discuss parameter estimation in situations where the unknown parameters vary across multiple scales. We illustrate the main ideas using a simple model for groundwater flow.   We will highlight various approaches to regularization for inverse problems, including Tikhonov and Bayesian methods. We illustrate three ideas that arise when considering inverse problems in the multiscale context. The first idea is that the choice of space or set in which to seek the solution to the inverse problem is intimately related to whether a homogenized or full multiscale solution is required. This is a choice of regularization. The second idea is that, if a homogenized solution to the inverse problem is what is desired, then this can be recovered from carefully designed observations of the full multiscale system. The third idea is that the theory of homogenization can be used to improve the estimation of homogenized coefficients from multiscale data.

</details>

<details>

<summary>2010-09-16 01:44:17 - Bayesian matching of unlabelled point sets using Procrustes and configuration models</summary>

- *Kim Kenobi, Ian L. Dryden*

- `1009.3072v1` - [abs](http://arxiv.org/abs/1009.3072v1) - [pdf](http://arxiv.org/pdf/1009.3072v1)

> The problem of matching unlabelled point sets using Bayesian inference is considered. Two recently proposed models for the likelihood are compared, based on the Procrustes size-and-shape and the full configuration. Bayesian inference is carried out for matching point sets using Markov chain Monte Carlo simulation. An improvement to the existing Procrustes algorithm is proposed which improves convergence rates, using occasional large jumps in the burn-in period. The Procrustes and configuration methods are compared in a simulation study and using real data, where it is of interest to estimate the strengths of matches between protein binding sites. The performance of both methods is generally quite similar, and a connection between the two models is made using a Laplace approximation.

</details>

<details>

<summary>2010-09-16 20:20:37 - A Hierarchical Bayesian Framework for Constructing Sparsity-inducing Priors</summary>

- *Anthony Lee, Francois Caron, Arnaud Doucet, Chris Holmes*

- `1009.1914v2` - [abs](http://arxiv.org/abs/1009.1914v2) - [pdf](http://arxiv.org/pdf/1009.1914v2)

> Variable selection techniques have become increasingly popular amongst statisticians due to an increased number of regression and classification applications involving high-dimensional data where we expect some predictors to be unimportant. In this context, Bayesian variable selection techniques involving Markov chain Monte Carlo exploration of the posterior distribution over models can be prohibitively computationally expensive and so there has been attention paid to quasi-Bayesian approaches such as maximum a posteriori (MAP) estimation using priors that induce sparsity in such estimates. We focus on this latter approach, expanding on the hierarchies proposed to date to provide a Bayesian interpretation and generalization of state-of-the-art penalized optimization approaches and providing simultaneously a natural way to include prior information about parameters within this framework. We give examples of how to use this hierarchy to compute MAP estimates for linear and logistic regression as well as sparse precision-matrix estimates in Gaussian graphical models. In addition, an adaptive group lasso method is derived using the framework.

</details>

<details>

<summary>2010-09-19 02:28:35 - Pair-Wise Cluster Analysis</summary>

- *David R. Hardoon, Kristiaan Pelcksman*

- `1009.3601v1` - [abs](http://arxiv.org/abs/1009.3601v1) - [pdf](http://arxiv.org/pdf/1009.3601v1)

> This paper studies the problem of learning clusters which are consistently present in different (continuously valued) representations of observed data. Our setup differs slightly from the standard approach of (co-) clustering as we use the fact that some form of `labeling' becomes available in this setup: a cluster is only interesting if it has a counterpart in the alternative representation. The contribution of this paper is twofold: (i) the problem setting is explored and an analysis in terms of the PAC-Bayesian theorem is presented, (ii) a practical kernel-based algorithm is derived exploiting the inherent relation to Canonical Correlation Analysis (CCA), as well as its extension to multiple views. A content based information retrieval (CBIR) case study is presented on the multi-lingual aligned Europal document dataset which supports the above findings.

</details>

<details>

<summary>2010-09-22 12:32:22 - On the Role of Decision Theory in Uncertainty Analysis</summary>

- *Merlin Keller, Eric Parent, Alberto Pasanisi*

- `1009.4342v1` - [abs](http://arxiv.org/abs/1009.4342v1) - [pdf](http://arxiv.org/pdf/1009.4342v1)

> Maximum likelihood estimation (MLE) and heuristic predictive estimation (HPE) are two widely used approaches in industrial uncertainty analysis. We review them from the point of view of decision theory, using Bayesian inference as a gold standard for comparison. The main drawback of MLE is that it may fail to properly account for the uncertainty on the physical process generating the data, especially when only a small amount of data are available. HPE offers an improvement in that it takes this uncertainty into account. However, we show that this approach is actually equivalent to Bayes estimation for a particular cost function that is not explicitly chosen by the decision maker. This may produce results that are suboptimal from a decisional perspective. These results plead for a systematic use of Bayes estimators based on carefully defined cost functions.

</details>

<details>

<summary>2010-09-25 00:41:33 - Bayesian Tracking of Emerging Epidemics Using Ensemble Optimal Statistical Interpolation (EnOSI)</summary>

- *Ashok Krishnamurthy, Loren Cobb, Jan Mandel, Jonathan Beezley*

- `1009.4959v1` - [abs](http://arxiv.org/abs/1009.4959v1) - [pdf](http://arxiv.org/pdf/1009.4959v1)

> We explore the use of the optimal statistical interpolation (OSI) data assimilation method for the statistical tracking of emerging epidemics and to study the spatial dynamics of a disease. The epidemic models that we used for this study are spatial variants of the common susceptible-infectious-removed (S-I-R) compartmental model of epidemiology. The spatial S-I-R epidemic model is illustrated by application to simulated spatial dynamic epidemic data from the historic "Black Death" plague of 14th century Europe. Bayesian statistical tracking of emerging epidemic diseases using the OSI as it unfolds is illustrated for a simulated epidemic wave originating in Santa Fe, New Mexico.

</details>

<details>

<summary>2010-09-25 18:10:24 - Efficient Bayesian Community Detection using Non-negative Matrix Factorisation</summary>

- *Ioannis Psorakis, Stephen Roberts, Ben Sheldon*

- `1009.2646v5` - [abs](http://arxiv.org/abs/1009.2646v5) - [pdf](http://arxiv.org/pdf/1009.2646v5)

> Identifying overlapping communities in networks is a challenging task. In this work we present a novel approach to community detection that utilises the Bayesian non-negative matrix factorisation (NMF) model to produce a probabilistic output for node memberships. The scheme has the advantage of computational efficiency, soft community membership and an intuitive foundation. We present the performance of the method against a variety of benchmark problems and compare and contrast it to several other algorithms for community detection. Our approach performs favourably compared to other methods at a fraction of the computational costs.

</details>

<details>

<summary>2010-09-26 08:26:15 - Bayesian Predictive Densities Based on Latent Information Priors</summary>

- *Fumiyasu Komaki*

- `1009.5072v1` - [abs](http://arxiv.org/abs/1009.5072v1) - [pdf](http://arxiv.org/pdf/1009.5072v1)

> Construction methods for prior densities are investigated from a predictive viewpoint. Predictive densities for future observables are constructed by using observed data. The simultaneous distribution of future observables and observed data is assumed to belong to a parametric submodel of a multinomial model. Future observables and data are possibly dependent. The discrepancy of a predictive density to the true conditional density of future observables given observed data is evaluated by the Kullback-Leibler divergence. It is proved that limits of Bayesian predictive densities form an essentially complete class. Latent information priors are defined as priors maximizing the conditional mutual information between the parameter and the future observables given the observed data. Minimax predictive densities are constructed as limits of Bayesian predictive densities based on prior sequences converging to the latent information priors.

</details>

<details>

<summary>2010-09-26 17:42:04 - Classification and categorical inputs with treed Gaussian process models</summary>

- *Tamara Broderick, Robert B. Gramacy*

- `0904.4891v3` - [abs](http://arxiv.org/abs/0904.4891v3) - [pdf](http://arxiv.org/pdf/0904.4891v3)

> Recognizing the successes of treed Gaussian process (TGP) models as an interpretable and thrifty model for nonparametric regression, we seek to extend the model to classification. Both treed models and Gaussian processes (GPs) have, separately, enjoyed great success in application to classification problems. An example of the former is Bayesian CART. In the latter, real-valued GP output may be utilized for classification via latent variables, which provide classification rules by means of a softmax function. We formulate a Bayesian model averaging scheme to combine these two models and describe a Monte Carlo method for sampling from the full posterior distribution with joint proposals for the tree topology and the GP parameters corresponding to latent variables at the leaves. We concentrate on efficient sampling of the latent variables, which is important to obtain good mixing in the expanded parameter space. The tree structure is particularly helpful for this task and also for developing an efficient scheme for handling categorical predictors, which commonly arise in classification problems. Our proposed classification TGP (CTGP) methodology is illustrated on a collection of synthetic and real data sets. We assess performance relative to existing methods and thereby show how CTGP is highly flexible, offers tractable inference, produces rules that are easy to interpret, and performs well out of sample.

</details>

<details>

<summary>2010-09-29 06:05:19 - Hierarchical mixture models for assessing fingerprint individuality</summary>

- *Sarat C. Dass, Mingfei Li*

- `1009.5778v1` - [abs](http://arxiv.org/abs/1009.5778v1) - [pdf](http://arxiv.org/pdf/1009.5778v1)

> The study of fingerprint individuality aims to determine to what extent a fingerprint uniquely identifies an individual. Recent court cases have highlighted the need for measures of fingerprint individuality when a person is identified based on fingerprint evidence. The main challenge in studies of fingerprint individuality is to adequately capture the variability of fingerprint features in a population. In this paper hierarchical mixture models are introduced to infer the extent of individualization. Hierarchical mixtures utilize complementary aspects of mixtures at different levels of the hierarchy. At the first (top) level, a mixture is used to represent homogeneous groups of fingerprints in the population, whereas at the second level, nested mixtures are used as flexible representations of distributions of features from each fingerprint. Inference for hierarchical mixtures is more challenging since the number of unknown mixture components arise in both the first and second levels of the hierarchy. A Bayesian approach based on reversible jump Markov chain Monte Carlo methodology is developed for the inference of all unknown parameters of hierarchical mixtures. The methodology is illustrated on fingerprint images from the NIST database and is used to make inference on fingerprint individuality estimates from this population.

</details>

<details>

<summary>2010-09-29 06:57:37 - Profiling time course expression of virus genes---an illustration of Bayesian inference under shape restrictions</summary>

- *Li-Chu Chien, I-Shou Chang, Shih Sheng Jiang, Pramod K. Gupta, Chi-Chung Wen, Yuh-Jenn Wu, Chao A. Hsiung*

- `1009.5785v1` - [abs](http://arxiv.org/abs/1009.5785v1) - [pdf](http://arxiv.org/pdf/1009.5785v1)

> There have been several studies of the genome-wide temporal transcriptional program of viruses, based on microarray experiments, which are generally useful in the construction of gene regulation network. It seems that biological interpretations in these studies are directly based on the normalized data and some crude statistics, which provide rough estimates of limited features of the profile and may incur biases. This paper introduces a hierarchical Bayesian shape restricted regression method for making inference on the time course expression of virus genes. Estimates of many salient features of the expression profile like onset time, inflection point, maximum value, time to maximum value, area under curve, etc. can be obtained immediately by this method. Applying this method to a baculovirus microarray time course expression data set, we indicate that many biological questions can be formulated quantitatively and we are able to offer insights into the baculovirus biology.

</details>

<details>

<summary>2010-09-29 11:58:22 - Bayesian inference for exponential random graph models</summary>

- *Alberto Caimo, Nial Friel*

- `1007.5192v2` - [abs](http://arxiv.org/abs/1007.5192v2) - [pdf](http://arxiv.org/pdf/1007.5192v2)

> Exponential random graph models are extremely difficult models to handle from a statistical viewpoint, since their normalising constant, which depends on model parameters, is available only in very trivial cases. We show how inference can be carried out in a Bayesian framework using a MCMC algorithm, which circumvents the need to calculate the normalising constants. We use a population MCMC approach which accelerates convergence and improves mixing of the Markov chain. This approach improves performance with respect to the Monte Carlo maximum likelihood method of Geyer and Thompson (1992).

</details>

<details>

<summary>2010-09-29 12:45:35 - Nonparametric Bayesian multiple testing for longitudinal performance stratification</summary>

- *James G. Scott*

- `1009.5869v1` - [abs](http://arxiv.org/abs/1009.5869v1) - [pdf](http://arxiv.org/pdf/1009.5869v1)

> This paper describes a framework for flexible multiple hypothesis testing of autoregressive time series. The modeling approach is Bayesian, though a blend of frequentist and Bayesian reasoning is used to evaluate procedures. Nonparametric characterizations of both the null and alternative hypotheses will be shown to be the key robustification step necessary to ensure reasonable Type-I error performance. The methodology is applied to part of a large database containing up to 50 years of corporate performance statistics on 24,157 publicly traded American companies, where the primary goal of the analysis is to flag companies whose historical performance is significantly different from that expected due to chance.

</details>


## 2010-10

<details>

<summary>2010-10-01 11:20:42 - A model selection approach to genome wide association studies</summary>

- *Florian Frommlet, Felix Ruhaltinger, Piotr Twarog, Malgorzata Bogdan*

- `1010.0124v1` - [abs](http://arxiv.org/abs/1010.0124v1) - [pdf](http://arxiv.org/pdf/1010.0124v1)

> For the vast majority of genome wide association studies (GWAS) published so far, statistical analysis was performed by testing markers individually. In this article we present some elementary statistical considerations which clearly show that in case of complex traits the approach based on multiple regression or generalized linear models is preferable to multiple testing. We introduce a model selection approach to GWAS based on modifications of Bayesian Information Criterion (BIC) and develop some simple search strategies to deal with the huge number of potential models. Comprehensive simulations based on real SNP data confirm that model selection has larger power than multiple testing to detect causal SNPs in complex models. On the other hand multiple testing has substantial problems with proper ranking of causal SNPs and tends to detect a certain number of false positive SNPs, which are not linked to any of the causal mutations. We show that this behavior is typical in GWAS for complex traits and can be explained by an aggregated influence of many small random sample correlations between genotypes of a SNP under investigation and other causal SNPs. We believe that our findings at least partially explain problems with low power and nonreplicability of results in many real data GWAS. Finally, we discuss the advantages of our model selection approach in the context of real data analysis, where we consider publicly available gene expression data as traits for individuals from the HapMap project.

</details>

<details>

<summary>2010-10-02 08:32:00 - Interval Estimation for Messy Observational Data</summary>

- *Paul Gustafson, Sander Greenland*

- `1010.0306v1` - [abs](http://arxiv.org/abs/1010.0306v1) - [pdf](http://arxiv.org/pdf/1010.0306v1)

> We review some aspects of Bayesian and frequentist interval estimation, focusing first on their relative strengths and weaknesses when used in "clean" or "textbook" contexts. We then turn attention to observational-data situations which are "messy," where modeling that acknowledges the limitations of study design and data collection leads to nonidentifiability. We argue, via a series of examples, that Bayesian interval estimation is an attractive way to proceed in this context even for frequentists, because it can be supplied with a diagnostic in the form of a calibration-sensitivity simulation analysis. We illustrate the basis for this approach in a series of theoretical considerations, simulations and an application to a study of silica exposure and lung cancer.

</details>

<details>

<summary>2010-10-04 07:42:46 - Optional Pólya tree and Bayesian inference</summary>

- *Wing H. Wong, Li Ma*

- `1010.0490v1` - [abs](http://arxiv.org/abs/1010.0490v1) - [pdf](http://arxiv.org/pdf/1010.0490v1)

> We introduce an extension of the P\'olya tree approach for constructing distributions on the space of probability measures. By using optional stopping and optional choice of splitting variables, the construction gives rise to random measures that are absolutely continuous with piecewise smooth densities on partitions that can adapt to fit the data. The resulting "optional P\'{o}lya tree" distribution has large support in total variation topology and yields posterior distributions that are also optional P\'{o}lya trees with computable parameter values.

</details>

<details>

<summary>2010-10-04 09:24:59 - Quantile calculus and censored regression</summary>

- *Yijian Huang*

- `1010.0514v1` - [abs](http://arxiv.org/abs/1010.0514v1) - [pdf](http://arxiv.org/pdf/1010.0514v1)

> Quantile regression has been advocated in survival analysis to assess evolving covariate effects. However, challenges arise when the censoring time is not always observed and may be covariate-dependent, particularly in the presence of continuously-distributed covariates. In spite of several recent advances, existing methods either involve algorithmic complications or impose a probability grid. The former leads to difficulties in the implementation and asymptotics, whereas the latter introduces undesirable grid dependence. To resolve these issues, we develop fundamental and general quantile calculus on cumulative probability scale in this article, upon recognizing that probability and time scales do not always have a one-to-one mapping given a survival distribution. These results give rise to a novel estimation procedure for censored quantile regression, based on estimating integral equations. A numerically reliable and efficient Progressive Localized Minimization (PLMIN) algorithm is proposed for the computation. This procedure reduces exactly to the Kaplan--Meier method in the $k$-sample problem, and to standard uncensored quantile regression in the absence of censoring. Under regularity conditions, the proposed quantile coefficient estimator is uniformly consistent and converges weakly to a Gaussian process. Simulations show good statistical and algorithmic performance. The proposal is illustrated in the application to a clinical study.

</details>

<details>

<summary>2010-10-04 13:31:44 - Approximation of conditional densities by smooth mixtures of regressions</summary>

- *Andriy Norets*

- `1010.0581v1` - [abs](http://arxiv.org/abs/1010.0581v1) - [pdf](http://arxiv.org/pdf/1010.0581v1)

> This paper shows that large nonparametric classes of conditional multivariate densities can be approximated in the Kullback--Leibler distance by different specifications of finite mixtures of normal regressions in which normal means and variances and mixing probabilities can depend on variables in the conditioning set (covariates). These models are a special case of models known as "mixtures of experts" in statistics and computer science literature. Flexible specifications include models in which only mixing probabilities, modeled by multinomial logit, depend on the covariates and, in the univariate case, models in which only means of the mixed normals depend flexibly on the covariates. Modeling the variance of the mixed normals by flexible functions of the covariates can weaken restrictions on the class of the approximable densities. Obtained results can be generalized to mixtures of general location scale densities. Rates of convergence and easy to interpret bounds are also obtained for different model specifications. These approximation results can be useful for proving consistency of Bayesian and maximum likelihood density estimators based on these models. The results also have interesting implications for applied researchers.

</details>

<details>

<summary>2010-10-06 02:24:13 - On the Convergence of Bayesian Regression Models</summary>

- *Yuao Hu*

- `1010.1049v1` - [abs](http://arxiv.org/abs/1010.1049v1) - [pdf](http://arxiv.org/pdf/1010.1049v1)

> We consider heteroscedastic nonparametric regression models, when both the mean function and variance function are unknown and to be estimated with nonparametric approaches. We derive convergence rates of posterior distributions for this model with different priors, including splines and Gaussian process priors. The results are based on the general ones on the rates of convergence of posterior distributions for independent, non-identically distributed observations, and are established for both of the cases with random covariates, and deterministic covariates. We also illustrate that the results can be achieved for all levels of regularity, which means they are adaptive.

</details>

<details>

<summary>2010-10-06 14:02:13 - An integrative analysis of cancer gene expression studies using Bayesian latent factor modeling</summary>

- *Daniel Merl, Julia Ling-Yu Chen, Jen-Tsan Chi, Mike West*

- `1010.1157v1` - [abs](http://arxiv.org/abs/1010.1157v1) - [pdf](http://arxiv.org/pdf/1010.1157v1)

> We present an applied study in cancer genomics for integrating data and inferences from laboratory experiments on cancer cell lines with observational data obtained from human breast cancer studies. The biological focus is on improving understanding of transcriptional responses of tumors to changes in the pH level of the cellular microenvironment. The statistical focus is on connecting experimentally defined biomarkers of such responses to clinical outcome in observational studies of breast cancer patients. Our analysis exemplifies a general strategy for accomplishing this kind of integration across contexts. The statistical methodologies employed here draw heavily on Bayesian sparse factor models for identifying, modularizing and correlating with clinical outcome these signatures of aggregate changes in gene expression. By projecting patterns of biological response linked to specific experimental interventions into observational studies where such responses may be evidenced via variation in gene expression across samples, we are able to define biomarkers of clinically relevant physiological states and outcomes that are rooted in the biology of the original experiment. Through this approach we identify microenvironment-related prognostic factors capable of predicting long term survival in two independent breast cancer datasets. These results suggest possible directions for future laboratory studies, as well as indicate the potential for therapeutic advances though targeted disruption of specific pathway components.

</details>

<details>

<summary>2010-10-07 09:21:37 - BART: Bayesian additive regression trees</summary>

- *Hugh A. Chipman, Edward I. George, Robert E. McCulloch*

- `0806.3286v2` - [abs](http://arxiv.org/abs/0806.3286v2) - [pdf](http://arxiv.org/pdf/0806.3286v2)

> We develop a Bayesian "sum-of-trees" model where each tree is constrained by a regularization prior to be a weak learner, and fitting and inference are accomplished via an iterative Bayesian backfitting MCMC algorithm that generates samples from a posterior. Effectively, BART is a nonparametric Bayesian regression approach which uses dimensionally adaptive random basis elements. Motivated by ensemble methods in general, and boosting algorithms in particular, BART is defined by a statistical model: a prior and a likelihood. This approach enables full posterior inference including point and interval estimates of the unknown regression function as well as the marginal effects of potential predictors. By keeping track of predictor inclusion frequencies, BART can also be used for model-free variable selection. BART's many features are illustrated with a bake-off against competing methods on 42 different data sets, with a simulation experiment and on a drug discovery classification problem.

</details>

<details>

<summary>2010-10-07 12:51:36 - Variable selection and updating in model-based discriminant analysis for high dimensional data with food authenticity applications</summary>

- *Thomas Brendan Murphy, Nema Dean, Adrian E. Raftery*

- `0910.2585v2` - [abs](http://arxiv.org/abs/0910.2585v2) - [pdf](http://arxiv.org/pdf/0910.2585v2)

> Food authenticity studies are concerned with determining if food samples have been correctly labeled or not. Discriminant analysis methods are an integral part of the methodology for food authentication. Motivated by food authenticity applications, a model-based discriminant analysis method that includes variable selection is presented. The discriminant analysis model is fitted in a semi-supervised manner using both labeled and unlabeled data. The method is shown to give excellent classification performance on several high-dimensional multiclass food authenticity data sets with more variables than observations. The variables selected by the proposed method provide information about which variables are meaningful for classification purposes. A headlong search strategy for variable selection is shown to be efficient in terms of computation and achieves excellent classification performance. In applications to several food authenticity data sets, our proposed method outperformed default implementations of Random Forests, AdaBoost, transductive SVMs and Bayesian Multinomial Regression by substantial margins.

</details>

<details>

<summary>2010-10-07 18:42:36 - Certain Relations between Mutual Information and Fidelity of Statistical Estimation</summary>

- *Sudhakar Prasad*

- `1010.1508v1` - [abs](http://arxiv.org/abs/1010.1508v1) - [pdf](http://arxiv.org/pdf/1010.1508v1)

> I present several new relations between mutual information (MI) and statistical estimation error for a system that can be regarded simultaneously as a communication channel and as an estimator of an input parameter. I first derive a second-order result between MI and Fisher information (FI) that is valid for sufficiently narrow priors, but arbitrary channels. A second relation furnishes a lower bound on the MI in terms of the minimum mean-squared error (MMSE) on the Bayesian estimation of the input parameter from the channel output, one that is valid for arbitrary channels and priors. The existence of such a lower bound, while extending previous work relating the MI to the FI that is valid only in the asymptotic and high-SNR limits, elucidates further the fundamental connection between information and estimation theoretic measures of fidelity. The remaining relations I present are inequalities and correspondences among MI, FI, and MMSE in the presence of nuisance parameters.

</details>

<details>

<summary>2010-10-08 13:39:32 - Latent diffusion models for survival analysis</summary>

- *Gareth O. Roberts, Laura M. Sangalli*

- `1010.1688v1` - [abs](http://arxiv.org/abs/1010.1688v1) - [pdf](http://arxiv.org/pdf/1010.1688v1)

> We consider Bayesian hierarchical models for survival analysis, where the survival times are modeled through an underlying diffusion process which determines the hazard rate. We show how these models can be efficiently treated by means of Markov chain Monte Carlo techniques.

</details>

<details>

<summary>2010-10-08 19:35:14 - Bayesian Post-Processing Methods for Jitter Mitigation in Sampling</summary>

- *Daniel S. Weller, Vivek K Goyal*

- `1007.5098v2` - [abs](http://arxiv.org/abs/1007.5098v2) - [pdf](http://arxiv.org/pdf/1007.5098v2)

> Minimum mean squared error (MMSE) estimators of signals from samples corrupted by jitter (timing noise) and additive noise are nonlinear, even when the signal prior and additive noise have normal distributions. This paper develops a stochastic algorithm based on Gibbs sampling and slice sampling to approximate the optimal MMSE estimator in this Bayesian formulation. Simulations demonstrate that this nonlinear algorithm can improve significantly upon the linear MMSE estimator, as well as the EM algorithm approximation to the maximum likelihood (ML) estimator used in classical estimation. Effective off-chip post-processing to mitigate jitter enables greater jitter to be tolerated, potentially reducing on-chip ADC power consumption.

</details>

<details>

<summary>2010-10-11 07:30:34 - Reference priors of nuisance parameters in Bayesian sequential population analysis</summary>

- *Nicolas Bousquet*

- `1007.5388v4` - [abs](http://arxiv.org/abs/1007.5388v4) - [pdf](http://arxiv.org/pdf/1007.5388v4)

> Prior distributions elicited for modelling the natural fluctuations or the uncertainty on parameters of Bayesian fishery population models, can be chosen among a vast range of statistical laws. Since the statistical framework is defined by observational processes, observational parameters enter into the estimation and must be considered random, similarly to parameters or states of interest like population levels or real catches. The former are thus perceived as nuisance parameters whose values are intrinsically linked to the considered experiment, which also require noninformative priors. In fishery research Jeffreys methodology has been presented by Millar (2002) as a practical way to elicit such priors. However they can present wrong properties in multiparameter contexts. Therefore we suggest to use the elicitation method proposed by Berger and Bernardo to avoid paradoxical results raised by Jeffreys priors. These benchmark priors are derived here in the framework of sequential population analysis.

</details>

<details>

<summary>2010-10-11 11:13:59 - Asymptotic minimax risk of predictive density estimation for non-parametric regression</summary>

- *Xinyi Xu, Feng Liang*

- `1010.2064v1` - [abs](http://arxiv.org/abs/1010.2064v1) - [pdf](http://arxiv.org/pdf/1010.2064v1)

> We consider the problem of estimating the predictive density of future observations from a non-parametric regression model. The density estimators are evaluated under Kullback--Leibler divergence and our focus is on establishing the exact asymptotics of minimax risk in the case of Gaussian errors. We derive the convergence rate and constant for minimax risk among Bayesian predictive densities under Gaussian priors and we show that this minimax risk is asymptotically equivalent to that among all density estimators.

</details>

<details>

<summary>2010-10-15 16:17:32 - An Alternative Prior Process for Nonparametric Bayesian Clustering</summary>

- *Hanna M. Wallach, Shane T. Jensen, Lee Dicker, Katherine A. Heller*

- `0801.0461v2` - [abs](http://arxiv.org/abs/0801.0461v2) - [pdf](http://arxiv.org/pdf/0801.0461v2)

> Prior distributions play a crucial role in Bayesian approaches to clustering. Two commonly-used prior distributions are the Dirichlet and Pitman-Yor processes. In this paper, we investigate the predictive probabilities that underlie these processes, and the implicit "rich-get-richer" characteristic of the resulting partitions. We explore an alternative prior for nonparametric Bayesian clustering -- the uniform process -- for applications where the "rich-get-richer" property is undesirable. We also explore the cost of this process: partitions are no longer exchangeable with respect to the ordering of variables. We present new asymptotic and simulation-based results for the clustering characteristics of the uniform process and compare these with known results for the Dirichlet and Pitman-Yor processes. We compare performance on a real document clustering task, demonstrating the practical advantage of the uniform process despite its lack of exchangeability over orderings.

</details>

<details>

<summary>2010-10-17 11:25:14 - An attempt at reading Keynes' Treatise on Probability</summary>

- *Christian P. Robert*

- `1003.5544v3` - [abs](http://arxiv.org/abs/1003.5544v3) - [pdf](http://arxiv.org/pdf/1003.5544v3)

> The book A Treatise on Probability was published by John Maynard Keynes in 1921. It contains a critical assessment of the foundations of probability and of the current statistical methodology. As a modern reader, we review here the aspects that are most related with statistics, avoiding a neophyte's perspective on the philosophical issues. In particular, the book is quite critical of the Bayesian approach and we examine the arguments provided by Keynes, as well as the alternative he proposes. This review does not subsume the scholarly study of Aldrich (2008a) relating Keynes with the statistics community of the time.

</details>

<details>

<summary>2010-10-19 07:53:57 - Consistency of objective Bayes factors as the model dimension grows</summary>

- *Elías Moreno, F. Javier Girón, George Casella*

- `1010.3821v1` - [abs](http://arxiv.org/abs/1010.3821v1) - [pdf](http://arxiv.org/pdf/1010.3821v1)

> In the class of normal regression models with a finite number of regressors, and for a wide class of prior distributions, a Bayesian model selection procedure based on the Bayes factor is consistent [Casella and Moreno J. Amer. Statist. Assoc. 104 (2009) 1261--1271]. However, in models where the number of parameters increases as the sample size increases, properties of the Bayes factor are not totally understood. Here we study consistency of the Bayes factors for nested normal linear models when the number of regressors increases with the sample size. We pay attention to two successful tools for model selection [Schwarz Ann. Statist. 6 (1978) 461--464] approximation to the Bayes factor, and the Bayes factor for intrinsic priors [Berger and Pericchi J. Amer. Statist. Assoc. 91 (1996) 109--122, Moreno, Bertolino and Racugno J. Amer. Statist. Assoc. 93 (1998) 1451--1460]. We find that the the Schwarz approximation and the Bayes factor for intrinsic priors are consistent when the rate of growth of the dimension of the bigger model is $O(n^b)$ for $b<1$. When $b=1$ the Schwarz approximation is always inconsistent under the alternative while the Bayes factor for intrinsic priors is consistent except for a small set of alternative models which is characterized.

</details>

<details>

<summary>2010-10-19 13:04:20 - Nonparametric inference of quantile curves for nonstationary time series</summary>

- *Zhou Zhou*

- `1010.3891v1` - [abs](http://arxiv.org/abs/1010.3891v1) - [pdf](http://arxiv.org/pdf/1010.3891v1)

> The paper considers nonparametric specification tests of quantile curves for a general class of nonstationary processes. Using Bahadur representation and Gaussian approximation results for nonstationary time series, simultaneous confidence bands and integrated squared difference tests are proposed to test various parametric forms of the quantile curves with asymptotically correct type I error rates. A wild bootstrap procedure is implemented to alleviate the problem of slow convergence of the asymptotic results. In particular, our results can be used to test the trends of extremes of climate variables, an important problem in understanding climate change. Our methodology is applied to the analysis of the maximum speed of tropical cyclone winds. It was found that an inhomogeneous upward trend for cyclone wind speeds is pronounced at high quantile values. However, there is no trend in the mean lifetime-maximum wind speed. This example shows the effectiveness of the quantile regression technique.

</details>

<details>

<summary>2010-10-21 16:59:47 - Elicitation of Weibull priors</summary>

- *Nicolas Bousquet*

- `1007.4740v3` - [abs](http://arxiv.org/abs/1007.4740v3) - [pdf](http://arxiv.org/pdf/1007.4740v3)

> Based on expert opinions, informative prior elicitation for the common Weibull lifetime distribution usually presents some difficulties since it requires to elicit a two-dimensional joint prior. We consider here a reliability framework where the available expert information states directly in terms of prior predictive values (lifetimes) and not parameter values, which are less intuitive. The novelty of our procedure is to weigh the expert information by the size m of a virtual sample yielding a similar information, the prior being seen as a reference posterior. Thus, the prior calibration by the Bayesian analyst, who has to moderate the subjective information with respect to the data information, is made simple. A main result is the full tractability of the prior under mild conditions, despite the conjugation issues encountered with the Weibull distribution. Besides, m is a practical focus point for discussion between analysts and experts, and a helpful parameter for leading sensitivity studies and reducing the potential imbalance in posterior selection between Bayesian Weibull models, which can be due to favoring arbitrarily a prior. The calibration of m is discussed and a real example is treated along the paper.

</details>

<details>

<summary>2010-10-22 11:00:18 - A Bayesian Method for Detecting and Characterizing Allelic Heterogeneity and Boosting Signals in Genome-Wide Association Studies</summary>

- *Zhan Su, Niall Cardin, the Wellcome Trust Case Control Consortium, Peter Donnelly, Jonathan Marchini*

- `1010.4670v1` - [abs](http://arxiv.org/abs/1010.4670v1) - [pdf](http://arxiv.org/pdf/1010.4670v1)

> The standard paradigm for the analysis of genome-wide association studies involves carrying out association tests at both typed and imputed SNPs. These methods will not be optimal for detecting the signal of association at SNPs that are not currently known or in regions where allelic heterogeneity occurs. We propose a novel association test, complementary to the SNP-based approaches, that attempts to extract further signals of association by explicitly modeling and estimating both unknown SNPs and allelic heterogeneity at a locus. At each site we estimate the genealogy of the case-control sample by taking advantage of the HapMap haplotypes across the genome. Allelic heterogeneity is modeled by allowing more than one mutation on the branches of the genealogy. Our use of Bayesian methods allows us to assess directly the evidence for a causative SNP not well correlated with known SNPs and for allelic heterogeneity at each locus. Using simulated data and real data from the WTCCC project, we show that our method (i) produces a significant boost in signal and accurately identifies the form of the allelic heterogeneity in regions where it is known to exist, (ii) can suggest new signals that are not found by testing typed or imputed SNPs and (iii) can provide more accurate estimates of effect sizes in regions of association.

</details>

<details>

<summary>2010-10-25 12:02:52 - Replication in Genome-Wide Association Studies</summary>

- *Peter Kraft, Eleftheria Zeggini, John P. A. Ioannidis*

- `1010.5095v1` - [abs](http://arxiv.org/abs/1010.5095v1) - [pdf](http://arxiv.org/pdf/1010.5095v1)

> Replication helps ensure that a genotype-phenotype association observed in a genome-wide association (GWA) study represents a credible association and is not a chance finding or an artifact due to uncontrolled biases. We discuss prerequisites for exact replication, issues of heterogeneity, advantages and disadvantages of different methods of data synthesis across multiple studies, frequentist vs. Bayesian inferences for replication, and challenges that arise from multi-team collaborations. While consistent replication can greatly improve the credibility of a genotype-phenotype association, it may not eliminate spurious associations due to biases shared by many studies. Conversely, lack of replication in well-powered follow-up studies usually invalidates the initially proposed association, although occasionally it may point to differences in linkage disequilibrium or effect modifiers across studies.

</details>

<details>

<summary>2010-10-25 19:03:49 - Benchmarking Historical Corporate Performance</summary>

- *James G. Scott*

- `0911.1768v2` - [abs](http://arxiv.org/abs/0911.1768v2) - [pdf](http://arxiv.org/pdf/0911.1768v2)

> This paper uses Bayesian tree models for statistical benchmarking in data sets with awkward marginals and complicated dependence structures. The method is applied to a very large database on corporate performance over the last four decades. The results of this study provide a formal basis for making cross-peer-group comparisons among companies in very different industries and operating environments. This is done by using models for Bayesian multiple hypothesis testing to determine which firms, if any, have systematically outperformed their peer groups over time. We conclude that systematic outperformance, while it seems to exist, is quite rare worldwide.

</details>

<details>

<summary>2010-10-25 21:09:59 - Parameter expansion in local-shrinkage models</summary>

- *James G. Scott*

- `1010.5265v1` - [abs](http://arxiv.org/abs/1010.5265v1) - [pdf](http://arxiv.org/pdf/1010.5265v1)

> This paper considers the problem of using MCMC to fit sparse Bayesian models based on normal scale-mixture priors. Examples of this framework include the Bayesian LASSO and the horseshoe prior. We study the usefulness of parameter expansion (PX) for improving convergence in such models, which is notoriously slow when the global variance component is near zero. Our conclusion is that parameter expansion does improve matters in LASSO-type models, but only modestly. In most cases this improvement, while noticeable, is less than what might be expected, especially compared to the improvements that PX makes possible for models very similar to those considered here. We give some examples, and we attempt to provide some intuition as to why this is so. We also describe how slice sampling may be used to update the global variance component. In practice, this approach seems to perform almost as well as parameter expansion. As a practical matter, however, it is perhaps best viewed not as a replacement for PX, but as a tool for expanding the class of models to which PX is applicable.

</details>

<details>

<summary>2010-10-28 20:47:52 - Slice sampling covariance hyperparameters of latent Gaussian models</summary>

- *Iain Murray, Ryan Prescott Adams*

- `1006.0868v2` - [abs](http://arxiv.org/abs/1006.0868v2) - [pdf](http://arxiv.org/pdf/1006.0868v2)

> The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be specified using unknown hyperparameters. Integrating over these hyperparameters considers different possible explanations for the data when making predictions. This integration is often performed using Markov chain Monte Carlo (MCMC) sampling. However, with non-Gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly. In this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes.

</details>

<details>

<summary>2010-10-29 00:18:55 - Exact Bayesian Analysis of Mixtures</summary>

- *Christian P. Robert, Kerrie L. Mengersen*

- `1010.6113v1` - [abs](http://arxiv.org/abs/1010.6113v1) - [pdf](http://arxiv.org/pdf/1010.6113v1)

> In this paper, we show how a complete and exact Bayesian analysis of a parametric mixture model is possible in some cases when components of the mixture are taken from exponential families and when conjugate priors are used. This restricted set-up allows us to show the relevance of the Bayesian approach as well as to exhibit the limitations of a complete analysis, namely that it is impossible to conduct this analysis when the sample size is too large, when the data are not from an exponential family, or when priors that are more complex than conjugate priors are used.

</details>


## 2010-11

<details>

<summary>2010-11-01 00:43:25 - Asymptotic Optimality Theory For Decentralized Sequential Multihypothesis Testing Problems</summary>

- *Yan Wang, Yajun Mei*

- `1011.0228v1` - [abs](http://arxiv.org/abs/1011.0228v1) - [pdf](http://arxiv.org/pdf/1011.0228v1)

> The Bayesian formulation of sequentially testing $M \ge 3$ hypotheses is studied in the context of a decentralized sensor network system. In such a system, local sensors observe raw observations and send quantized sensor messages to a fusion center which makes a final decision when stopping taking observations. Asymptotically optimal decentralized sequential tests are developed from a class of "two-stage" tests that allows the sensor network system to make a preliminary decision in the first stage and then optimize each local sensor quantizer accordingly in the second stage. It is shown that the optimal local quantizer at each local sensor in the second stage can be defined as a maximin quantizer which turns out to be a randomization of at most $M-1$ unambiguous likelihood quantizers (ULQ). We first present in detail our results for the system with a single sensor and binary sensor messages, and then extend to more general cases involving any finite alphabet sensor messages, multiple sensors, or composite hypotheses.

</details>

<details>

<summary>2010-11-02 12:46:20 - Bayesian inference and model choice in a hidden stochastic two-compartment model of hematopoietic stem cell fate decisions</summary>

- *Youyi Fong, Peter Guttorp, Janis Abkowitz*

- `1011.0601v1` - [abs](http://arxiv.org/abs/1011.0601v1) - [pdf](http://arxiv.org/pdf/1011.0601v1)

> Despite rapid advances in experimental cell biology, the in vivo behavior of hematopoietic stem cells (HSC) cannot be directly observed and measured. Previously we modeled feline hematopoiesis using a two-compartment hidden Markov process that had birth and emigration events in the first compartment. Here we perform Bayesian statistical inference on models which contain two additional events in the first compartment in order to determine if HSC fate decisions are linked to cell division or occur independently. Pareto Optimal Model Assessment approach is used to cross check the estimates from Bayesian inference. Our results show that HSC must divide symmetrically (i.e., produce two HSC daughter cells) in order to maintain hematopoiesis. We then demonstrate that the augmented model that adds asymmetric division events provides a better fit to the competitive transplantation data, and we thus provide evidence that HSC fate determination in vivo occurs both in association with cell division and at a separate point in time. Last we show that assuming each cat has a unique set of parameters leads to either a significant decrease or a nonsignificant increase in model fit, suggesting that the kinetic parameters for HSC are not unique attributes of individual animals, but shared within a species.

</details>

<details>

<summary>2010-11-02 14:26:36 - Semi-parametric dynamic time series modelling with applications to detecting neural dynamics</summary>

- *Fabio Rigat, Jim Q. Smith*

- `1011.0626v1` - [abs](http://arxiv.org/abs/1011.0626v1) - [pdf](http://arxiv.org/pdf/1011.0626v1)

> This paper illustrates novel methods for nonstationary time series modeling along with their applications to selected problems in neuroscience. These methods are semi-parametric in that inferences are derived by combining sequential Bayesian updating with a non-parametric change-point test. As a test statistic, we propose a Kullback--Leibler (KL) divergence between posterior distributions arising from different sets of data. A closed form expression of this statistic is derived for exponential family models, whereas standard Markov chain Monte Carlo output is used to approximate its value and its critical region for more general models. The behavior of one-step ahead predictive distributions under our semi-parametric framework is described analytically for a dynamic linear time series model. Conditions under which our approach reduces to fully parametric state-space modeling are also illustrated. We apply our methods to estimating the functional dynamics of a wide range of neural data, including multi-channel electroencephalogram recordings, longitudinal behavioral experiments and in-vivo multiple spike trains recordings. The estimated dynamics are related to the presentation of visual stimuli, to the evaluation of a learning performance and to changes in the functional connections between neurons over a sequence of experiments.

</details>

<details>

<summary>2010-11-02 19:58:44 - Entropic Inference</summary>

- *Ariel Caticha*

- `1011.0723v1` - [abs](http://arxiv.org/abs/1011.0723v1) - [pdf](http://arxiv.org/pdf/1011.0723v1)

> In this tutorial we review the essential arguments behing entropic inference. We focus on the epistemological notion of information and its relation to the Bayesian beliefs of rational agents. The problem of updating from a prior to a posterior probability distribution is tackled through an eliminative induction process that singles out the logarithmic relative entropy as the unique tool for inference. The resulting method of Maximum relative Entropy (ME), includes as special cases both MaxEnt and Bayes' rule, and therefore unifies the two themes of these workshops -- the Maximum Entropy and the Bayesian methods -- into a single general inference scheme.

</details>

<details>

<summary>2010-11-05 08:51:05 - Bayesian inference of stellar parameters and interstellar extinction using parallaxes and multiband photometry</summary>

- *C. A. L. Bailer-Jones*

- `1009.2766v2` - [abs](http://arxiv.org/abs/1009.2766v2) - [pdf](http://arxiv.org/pdf/1009.2766v2)

> Astrometric surveys provide the opportunity to measure the absolute magnitudes of large numbers of stars, but only if the individual line-of-sight extinctions are known. Unfortunately, extinction is highly degenerate with stellar effective temperature when estimated from broad band optical/infrared photometry. To address this problem, I introduce a Bayesian method for estimating the intrinsic parameters of a star and its line-of-sight extinction. It uses both photometry and parallaxes in a self-consistent manner in order to provide a non-parametric posterior probability distribution over the parameters. The method makes explicit use of domain knowledge by employing the Hertzsprung--Russell Diagram (HRD) to constrain solutions and to ensure that they respect stellar physics. I first demonstrate this method by using it to estimate effective temperature and extinction from BVJHK data for a set of artificially reddened Hipparcos stars, for which accurate effective temperatures have been estimated from high resolution spectroscopy. Using just the four colours, we see the expected strong degeneracy (positive correlation) between the temperature and extinction. Introducing the parallax, apparent magnitude and the HRD reduces this degeneracy and improves both the precision (reduces the error bars) and the accuracy of the parameter estimates, the latter by about 35%. The resulting accuracy is about 200K in temperature and 0.2mag in extinction. I then apply the method to estimate these parameters and absolute magnitudes for some 47000 F,G,K Hipparcos stars which have been cross-matched with 2MASS. The method can easily be extended to incorporate the estimation of other parameters, in particular metallicity and surface gravity, making it particularly suitable for the analysis of the 10^9 stars from Gaia.

</details>

<details>

<summary>2010-11-08 10:40:19 - Efficient Bayesian Inference for Generalized Bradley-Terry Models</summary>

- *Francois Caron, Arnaud Doucet*

- `1011.1761v1` - [abs](http://arxiv.org/abs/1011.1761v1) - [pdf](http://arxiv.org/pdf/1011.1761v1)

> The Bradley-Terry model is a popular approach to describe probabilities of the possible outcomes when elements of a set are repeatedly compared with one another in pairs. It has found many applications including animal behaviour, chess ranking and multiclass classification. Numerous extensions of the basic model have also been proposed in the literature including models with ties, multiple comparisons, group comparisons and random graphs. From a computational point of view, Hunter (2004) has proposed efficient iterative MM (minorization-maximization) algorithms to perform maximum likelihood estimation for these generalized Bradley-Terry models whereas Bayesian inference is typically performed using MCMC (Markov chain Monte Carlo) algorithms based on tailored Metropolis-Hastings (M-H) proposals. We show here that these MM\ algorithms can be reinterpreted as special instances of Expectation-Maximization (EM) algorithms associated to suitable sets of latent variables and propose some original extensions. These latent variables allow us to derive simple Gibbs samplers for Bayesian inference. We demonstrate experimentally the efficiency of these algorithms on a variety of applications.

</details>

<details>

<summary>2010-11-08 12:57:36 - Bayesian anomaly detection methods for social networks</summary>

- *Nicholas A. Heard, David J. Weston, Kiriaki Platanioti, David J. Hand*

- `1011.1788v1` - [abs](http://arxiv.org/abs/1011.1788v1) - [pdf](http://arxiv.org/pdf/1011.1788v1)

> Learning the network structure of a large graph is computationally demanding, and dynamically monitoring the network over time for any changes in structure threatens to be more challenging still. This paper presents a two-stage method for anomaly detection in dynamic graphs: the first stage uses simple, conjugate Bayesian models for discrete time counting processes to track the pairwise links of all nodes in the graph to assess normality of behavior; the second stage applies standard network inference tools on a greatly reduced subset of potentially anomalous nodes. The utility of the method is demonstrated on simulated and real data sets.

</details>

<details>

<summary>2010-11-09 07:00:47 - High-throughput data analysis in behavior genetics</summary>

- *Anat Sakov, Ilan Golani, Dina Lipkind, Yoav Benjamini*

- `1011.1987v1` - [abs](http://arxiv.org/abs/1011.1987v1) - [pdf](http://arxiv.org/pdf/1011.1987v1)

> In recent years, a growing need has arisen in different fields for the development of computational systems for automated analysis of large amounts of data (high-throughput). Dealing with nonstandard noise structure and outliers, that could have been detected and corrected in manual analysis, must now be built into the system with the aid of robust methods. We discuss such problems and present insights and solutions in the context of behavior genetics, where data consists of a time series of locations of a mouse in a circular arena. In order to estimate the location, velocity and acceleration of the mouse, and identify stops, we use a nonstandard mix of robust and resistant methods: LOWESS and repeated running median. In addition, we argue that protection against small deviations from experimental protocols can be handled automatically using statistical methods. In our case, it is of biological interest to measure a rodent's distance from the arena's wall, but this measure is corrupted if the arena is not a perfect circle, as required in the protocol. The problem is addressed by estimating robustly the actual boundary of the arena and its center using a nonparametric regression quantile of the behavioral data, with the aid of a fast algorithm developed for that purpose.

</details>

<details>

<summary>2010-11-09 08:14:31 - Changing approaches of prosecutors towards juvenile repeated sex-offenders: A Bayesian evaluation</summary>

- *Dipankar Bandyopadhyay, Debajyoti Sinha, Stuart Lipsitz, Elizabeth Letourneau*

- `1011.1999v1` - [abs](http://arxiv.org/abs/1011.1999v1) - [pdf](http://arxiv.org/pdf/1011.1999v1)

> Existing state-wide data bases on prosecutors' decisions about juvenile offenders are important, yet often un-explored resources for understanding changes in patterns of judicial decisions over time. We investigate the extent and nature of change in judicial behavior toward juveniles following the enactment of a new set of mandatory registration policies between 1992 and 1996 via analyzing the data on prosecutors' decisions of moving forward for youths repeatedly charged with sexual violence in South Carolina. To analyze this longitudinal binary data, we use a random effects logistic regression model via incorporating an unknown change-point year. For convenient physical interpretation, our models allow the proportional odds interpretation of effects of the explanatory variables and the change-point year with and without conditioning on the youth-specific random effects. As a consequence, the effects of the unknown change-point year and other factors can be interpreted as changes in both within youth and population averaged odds of moving forward. Using a Bayesian paradigm, we consider various prior opinions about the unknown year of the change in the pattern of prosecutors' decision. Based on the available data, we make posteriori conclusions about whether a change-point has occurred between 1992 and 1996 (inclusive), evaluate the degree of confidence about the year of change-point, estimate the magnitude of the effects of the change-point and other factors, and investigate other provocative questions about patterns of prosecutors' decisions over time.

</details>

<details>

<summary>2010-11-09 10:19:42 - Statistical inference of transmission fidelity of DNA methylation patterns over somatic cell divisions in mammals</summary>

- *Audrey Qiuyan Fu, Diane P. Genereux, Reinhard Stöger, Charles D. Laird, Matthew Stephens*

- `1011.2025v1` - [abs](http://arxiv.org/abs/1011.2025v1) - [pdf](http://arxiv.org/pdf/1011.2025v1)

> We develop Bayesian inference methods for a recently-emerging type of epigenetic data to study the transmission fidelity of DNA methylation patterns over cell divisions. The data consist of parent-daughter double-stranded DNA methylation patterns with each pattern coming from a single cell and represented as an unordered pair of binary strings. The data are technically difficult and time-consuming to collect, putting a premium on an efficient inference method. Our aim is to estimate rates for the maintenance and de novo methylation events that gave rise to the observed patterns, while accounting for measurement error. We model data at multiple sites jointly, thus using whole-strand information, and considerably reduce confounding between parameters. We also adopt a hierarchical structure that allows for variation in rates across sites without an explosion in the effective number of parameters. Our context-specific priors capture the expected stationarity, or near-stationarity, of the stochastic process that generated the data analyzed here. This expected stationarity is shown to greatly increase the precision of the estimation. Applying our model to a data set collected at the human FMR1 locus, we find that measurement errors, generally ignored in similar studies, occur at a nontrivial rate (inappropriate bisulfite conversion error: 1.6$%$ with 80$%$ CI: 0.9--2.3$%$). Accounting for these errors has a substantial impact on estimates of key biological parameters. The estimated average failure of maintenance rate and daughter de novo rate decline from 0.04 to 0.024 and from 0.14 to 0.07, respectively, when errors are accounted for. Our results also provide evidence that de novo events may occur on both parent and daughter strands: the median parent and daughter de novo rates are 0.08 (80$%$ CI: 0.04--0.13) and 0.07 (80$%$ CI: 0.04--0.11), respectively.

</details>

<details>

<summary>2010-11-09 14:47:11 - Bayesian meta-analysis for identifying periodically expressed genes in fission yeast cell cycle</summary>

- *Xiaodan Fan, Saumyadipta Pyne, Jun S. Liu*

- `1011.2104v1` - [abs](http://arxiv.org/abs/1011.2104v1) - [pdf](http://arxiv.org/pdf/1011.2104v1)

> The effort to identify genes with periodic expression during the cell cycle from genome-wide microarray time series data has been ongoing for a decade. However, the lack of rigorous modeling of periodic expression as well as the lack of a comprehensive model for integrating information across genes and experiments has impaired the effort for the accurate identification of periodically expressed genes. To address the problem, we introduce a Bayesian model to integrate multiple independent microarray data sets from three recent genome-wide cell cycle studies on fission yeast. A hierarchical model was used for data integration. In order to facilitate an efficient Monte Carlo sampling from the joint posterior distribution, we develop a novel Metropolis--Hastings group move. A surprising finding from our integrated analysis is that more than 40% of the genes in fission yeast are significantly periodically expressed, greatly enhancing the reported 10--15% of the genes in the current literature. It calls for a reconsideration of the periodically expressed gene detection problem.

</details>

<details>

<summary>2010-11-10 09:30:01 - Exact asymptotic distribution of change-point mle for change in the mean of Gaussian sequences</summary>

- *Stergios B. Fotopoulos, Venkata K. Jandhyala, Elena Khapalova*

- `1011.2322v1` - [abs](http://arxiv.org/abs/1011.2322v1) - [pdf](http://arxiv.org/pdf/1011.2322v1)

> We derive exact computable expressions for the asymptotic distribution of the change-point mle when a change in the mean occurred at an unknown point of a sequence of time-ordered independent Gaussian random variables. The derivation, which assumes that nuisance parameters such as the amount of change and variance are known, is based on ladder heights of Gaussian random walks hitting the half-line. We then show that the exact distribution easily extends to the distribution of the change-point mle when a change occurs in the mean vector of a multivariate Gaussian process. We perform simulations to examine the accuracy of the derived distribution when nuisance parameters have to be estimated as well as robustness of the derived distribution to deviations from Gaussianity. Through simulations, we also compare it with the well-known conditional distribution of the mle, which may be interpreted as a Bayesian solution to the change-point problem. Finally, we apply the derived methodology to monthly averages of water discharges of the Nacetinsky creek, Germany.

</details>

<details>

<summary>2010-11-10 10:27:54 - Bayes and empirical-Bayes multiplicity adjustment in the variable-selection problem</summary>

- *James G. Scott, James O. Berger*

- `1011.2333v1` - [abs](http://arxiv.org/abs/1011.2333v1) - [pdf](http://arxiv.org/pdf/1011.2333v1)

> This paper studies the multiplicity-correction effect of standard Bayesian variable-selection priors in linear regression. Our first goal is to clarify when, and how, multiplicity correction happens automatically in Bayesian analysis, and to distinguish this correction from the Bayesian Ockham's-razor effect. Our second goal is to contrast empirical-Bayes and fully Bayesian approaches to variable selection through examples, theoretical results and simulations. Considerable differences between the two approaches are found. In particular, we prove a theorem that characterizes a surprising aymptotic discrepancy between fully Bayes and empirical Bayes. This discrepancy arises from a different source than the failure to account for hyperparameter uncertainty in the empirical-Bayes estimate. Indeed, even at the extreme, when the empirical-Bayes estimate converges asymptotically to the true variable-inclusion probability, the potential for a serious difference remains.

</details>

<details>

<summary>2010-11-10 17:24:31 - Efficient Bayesian Inference for Switching State-Space Models using Discrete Particle Markov Chain Monte Carlo Methods</summary>

- *Nick Whiteley, Christophe Andrieu, Arnaud Doucet*

- `1011.2437v1` - [abs](http://arxiv.org/abs/1011.2437v1) - [pdf](http://arxiv.org/pdf/1011.2437v1)

> Switching state-space models (SSSM) are a very popular class of time series models that have found many applications in statistics, econometrics and advanced signal processing. Bayesian inference for these models typically relies on Markov chain Monte Carlo (MCMC) techniques. However, even sophisticated MCMC methods dedicated to SSSM can prove quite inefficient as they update potentially strongly correlated discrete-valued latent variables one-at-a-time (Carter and Kohn, 1996; Gerlach et al., 2000; Giordani and Kohn, 2008). Particle Markov chain Monte Carlo (PMCMC) methods are a recently developed class of MCMC algorithms which use particle filters to build efficient proposal distributions in high-dimensions (Andrieu et al., 2010). The existing PMCMC methods of Andrieu et al. (2010) are applicable to SSSM, but are restricted to employing standard particle filtering techniques. Yet, in the context of discrete-valued latent variables, specialised particle techniques have been developed which can outperform by up to an order of magnitude standard methods (Fearnhead, 1998; Fearnhead and Clifford, 2003; Fearnhead, 2004). In this paper we develop a novel class of PMCMC methods relying on these very efficient particle algorithms. We establish the theoretical validy of this new generic methodology referred to as discrete PMCMC and demonstrate it on a variety of examples including a multiple change-points model for well-log data and a model for U.S./U.K. exchange rate data. Discrete PMCMC algorithms are shown to outperform experimentally state-of-the-art MCMC techniques for a fixed computational complexity. Additionally they can be easily parallelized (Lee et al., 2010) which allows further substantial gains.

</details>

<details>

<summary>2010-11-12 09:07:34 - Age- and time-varying proportional hazards models for employment discrimination</summary>

- *George Woodworth, Joseph Kadane*

- `1011.2851v1` - [abs](http://arxiv.org/abs/1011.2851v1) - [pdf](http://arxiv.org/pdf/1011.2851v1)

> We use a discrete-time proportional hazards model of time to involuntary employment termination. This model enables us to examine both the continuous effect of the age of an employee and whether that effect has varied over time, generalizing earlier work [Kadane and Woodworth J. Bus. Econom. Statist. 22 (2004) 182--193]. We model the log hazard surface (over age and time) as a thin-plate spline, a Bayesian smoothness-prior implementation of penalized likelihood methods of surface-fitting [Wahba (1990) Spline Models for Observational Data. SIAM]. The nonlinear component of the surface has only two parameters, smoothness and anisotropy. The first, a scale parameter, governs the overall smoothness of the surface, and the second, anisotropy, controls the relative smoothness over time and over age. For any fixed value of the anisotropy parameter, the prior is equivalent to a Gaussian process with linear drift over the time--age plane with easily computed eigenvectors and eigenvalues that depend only on the configuration of data in the time--age plane and the anisotropy parameter. This model has application to legal cases in which a company is charged with disproportionately disadvantaging older workers when deciding whom to terminate. We illustrate the application of the modeling approach using data from an actual discrimination case.

</details>

<details>

<summary>2010-11-12 12:22:36 - Detection of radioactive material entering national ports: A Bayesian approach to radiation portal data</summary>

- *Siddhartha R. Dalal, Bing Han*

- `1011.2895v1` - [abs](http://arxiv.org/abs/1011.2895v1) - [pdf](http://arxiv.org/pdf/1011.2895v1)

> Given the potential for illicit nuclear material being used for terrorism, most ports now inspect a large number of goods entering national borders for radioactive cargo. The U.S. Department of Homeland Security is moving toward one hundred percent inspection of all containers entering the U.S. at various ports of entry for nuclear material. We propose a Bayesian classification approach for the real-time data collected by the inline Polyvinyl Toluene radiation portal monitors. We study the computational and asymptotic properties of the proposed method and demonstrate its efficacy in simulations. Given data available to the authorities, it should be feasible to implement this approach in practice.

</details>

<details>

<summary>2010-11-12 14:43:02 - Bayesian model search and multilevel inference for SNP association studies</summary>

- *Melanie A. Wilson, Edwin S. Iversen, Merlise A. Clyde, Scott C. Schmidler, Joellen M. Schildkraut*

- `0908.1144v3` - [abs](http://arxiv.org/abs/0908.1144v3) - [pdf](http://arxiv.org/pdf/0908.1144v3)

> Technological advances in genotyping have given rise to hypothesis-based association studies of increasing scope. As a result, the scientific hypotheses addressed by these studies have become more complex and more difficult to address using existing analytic methodologies. Obstacles to analysis include inference in the face of multiple comparisons, complications arising from correlations among the SNPs (single nucleotide polymorphisms), choice of their genetic parametrization and missing data. In this paper we present an efficient Bayesian model search strategy that searches over the space of genetic markers and their genetic parametrization. The resulting method for Multilevel Inference of SNP Associations, MISA, allows computation of multilevel posterior probabilities and Bayes factors at the global, gene and SNP level, with the prior distribution on SNP inclusion in the model providing an intrinsic multiplicity correction. We use simulated data sets to characterize MISA's statistical power, and show that MISA has higher power to detect association than standard procedures. Using data from the North Carolina Ovarian Cancer Study (NCOCS), MISA identifies variants that were not identified by standard methods and have been externally ``validated'' in independent studies. We examine sensitivity of the NCOCS results to prior choice and method for imputing missing data. MISA is available in an R package on CRAN.

</details>

<details>

<summary>2010-11-12 14:58:32 - Simulation-based Bayesian analysis for multiple changepoints</summary>

- *Jason Wyse, Nial Friel*

- `1011.2932v1` - [abs](http://arxiv.org/abs/1011.2932v1) - [pdf](http://arxiv.org/pdf/1011.2932v1)

> This paper presents a Markov chain Monte Carlo method to generate approximate posterior samples in retrospective multiple changepoint problems where the number of changes is not known in advance. The method uses conjugate models whereby the marginal likelihood for the data between consecutive changepoints is tractable. Inclusion of hyperpriors gives a near automatic algorithm providing a robust alternative to popular filtering recursions approaches in cases which may be sensitive to prior information. Three real examples are used to demonstrate the proposed approach.

</details>

<details>

<summary>2010-11-12 15:29:40 - Block clustering with collapsed latent block models</summary>

- *Jason Wyse, Nial Friel*

- `1011.2948v1` - [abs](http://arxiv.org/abs/1011.2948v1) - [pdf](http://arxiv.org/pdf/1011.2948v1)

> We introduce a Bayesian extension of the latent block model for model-based block clustering of data matrices. Our approach considers a block model where block parameters may be integrated out. The result is a posterior defined over the number of clusters in rows and columns and cluster memberships. The number of row and column clusters need not be known in advance as these are sampled along with cluster memberhips using Markov chain Monte Carlo. This differs from existing work on latent block models, where the number of clusters is assumed known or is chosen using some information criteria. We analyze both simulated and real data to validate the technique.

</details>

<details>

<summary>2010-11-15 10:29:24 - Optimal designs for random effect models with correlated errors with applications in population pharmacokinetics</summary>

- *Holger Dette, Andrey Pepelyshev, Tim Holland-Letz*

- `1011.3333v1` - [abs](http://arxiv.org/abs/1011.3333v1) - [pdf](http://arxiv.org/pdf/1011.3333v1)

> We consider the problem of constructing optimal designs for population pharmacokinetics which use random effect models. It is common practice in the design of experiments in such studies to assume uncorrelated errors for each subject. In the present paper a new approach is introduced to determine efficient designs for nonlinear least squares estimation which addresses the problem of correlation between observations corresponding to the same subject. We use asymptotic arguments to derive optimal design densities, and the designs for finite sample sizes are constructed from the quantiles of the corresponding optimal distribution function. It is demonstrated that compared to the optimal exact designs, whose determination is a hard numerical problem, these designs are very efficient. Alternatively, the designs derived from asymptotic theory could be used as starting designs for the numerical computation of exact optimal designs. Several examples of linear and nonlinear models are presented in order to illustrate the methodology. In particular, it is demonstrated that naively chosen equally spaced designs may lead to less accurate estimation.

</details>

<details>

<summary>2010-11-15 13:37:45 - Characterization of differentially expressed genes using high-dimensional co-expression networks</summary>

- *Gabriel C. G. de Abreu, Rodrigo Labouriau*

- `1011.3805v1` - [abs](http://arxiv.org/abs/1011.3805v1) - [pdf](http://arxiv.org/pdf/1011.3805v1)

> We present a technique to characterize differentially expressed genes in terms of their position in a high-dimensional co-expression network. The set-up of Gaussian graphical models is used to construct representations of the co-expression network in such a way that redundancy and the propagation of spurious information along the network are avoided. The proposed inference procedure is based on the minimization of the Bayesian Information Criterion (BIC) in the class of decomposable graphical models. This class of models can be used to represent complex relationships and has suitable properties that allow to make effective inference in problems with high degree of complexity (e.g. several thousands of genes) and small number of observations (e.g. 10-100) as typically occurs in high throughput gene expression studies. Taking advantage of the internal structure of decomposable graphical models, we construct a compact representation of the co-expression network that allows to identify the regions with high concentration of differentially expressed genes. It is argued that differentially expressed genes located in highly interconnected regions of the co-expression network are less informative than differentially expressed genes located in less interconnected regions. Based on that idea, a measure of uncertainty that resembles the notion of relative entropy is proposed. Our methods are illustrated with three publically available data sets on microarray experiments (the larger involving more than 50,000 genes and 64 patients) and a short simulation study.

</details>

<details>

<summary>2010-11-15 14:19:19 - PAC-Bayesian aggregation and multi-armed bandits</summary>

- *Jean-Yves Audibert*

- `1011.3396v1` - [abs](http://arxiv.org/abs/1011.3396v1) - [pdf](http://arxiv.org/pdf/1011.3396v1)

> This habilitation thesis presents several contributions to (1) the PAC-Bayesian analysis of statistical learning, (2) the three aggregation problems: given d functions, how to predict as well as (i) the best of these d functions (model selection type aggregation), (ii) the best convex combination of these d functions, (iii) the best linear combination of these d functions, (3) the multi-armed bandit problems.

</details>

<details>

<summary>2010-11-15 14:58:21 - Bayesian inference for double Pareto lognormal queues</summary>

- *Pepa Ramirez-Cobo, Rosa E. Lillo, Simon Wilson, Michael P. Wiper*

- `1011.3411v1` - [abs](http://arxiv.org/abs/1011.3411v1) - [pdf](http://arxiv.org/pdf/1011.3411v1)

> In this article we describe a method for carrying out Bayesian estimation for the double Pareto lognormal (dPlN) distribution which has been proposed as a model for heavy-tailed phenomena. We apply our approach to estimate the $\mathit{dPlN}/M/1$ and $M/\mathit{dPlN}/1$ queueing systems. These systems cannot be analyzed using standard techniques due to the fact that the dPlN distribution does not possess a Laplace transform in closed form. This difficulty is overcome using some recent approximations for the Laplace transform of the interarrival distribution for the $\mathit{Pareto}/M/1$ system. Our procedure is illustrated with applications in internet traffic analysis and risk theory.

</details>

<details>

<summary>2010-11-16 08:43:29 - Accounting for choice of measurement scale in extreme value modeling</summary>

- *J. L. Wadsworth, J. A. Tawn, P. Jonathan*

- `1011.3612v1` - [abs](http://arxiv.org/abs/1011.3612v1) - [pdf](http://arxiv.org/pdf/1011.3612v1)

> We investigate the effect that the choice of measurement scale has upon inference and extrapolation in extreme value analysis. Separate analyses of variables from a single process on scales which are linked by a nonlinear transformation may lead to discrepant conclusions concerning the tail behavior of the process. We propose the use of a Box--Cox power transformation incorporated as part of the inference procedure to account parametrically for the uncertainty surrounding the scale of extrapolation. This has the additional feature of increasing the rate of convergence of the distribution tails to an extreme value form in certain cases and thus reducing bias in the model estimation. Inference without reparameterization is practicably infeasible, so we explore a reparameterization which exploits the asymptotic theory of normalizing constants required for nondegenerate limit distributions. Inference is carried out in a Bayesian setting, an advantage of this being the availability of posterior predictive return levels. The methodology is illustrated on both simulated data and significant wave height data from the North Sea.

</details>

<details>

<summary>2010-11-16 15:17:28 - Predictor-dependent shrinkage for linear regression via partial factor modeling</summary>

- *P. Richard Hahn, Sayan Mukherjee, Carlos Carvalho*

- `1011.3725v1` - [abs](http://arxiv.org/abs/1011.3725v1) - [pdf](http://arxiv.org/pdf/1011.3725v1)

> In prediction problems with more predictors than observations, it can sometimes be helpful to use a joint probability model, $\pi(Y,X)$, rather than a purely conditional model, $\pi(Y \mid X)$, where $Y$ is a scalar response variable and $X$ is a vector of predictors. This approach is motivated by the fact that in many situations the marginal predictor distribution $\pi(X)$ can provide useful information about the parameter values governing the conditional regression. However, under very mild misspecification, this marginal distribution can also lead conditional inferences astray. Here, we explore these ideas in the context of linear factor models, to understand how they play out in a familiar setting. The resulting Bayesian model performs well across a wide range of covariance structures, on real and simulated data.

</details>

<details>

<summary>2010-11-19 08:32:36 - On Particle Learning</summary>

- *Nicolas Chopin, Alessandra Iacobucci, Jean-Michel Marin, Kerrie Mengersen, Christian P. Robert, Robin Ryder, Christian Schäfer*

- `1006.0554v3` - [abs](http://arxiv.org/abs/1006.0554v3) - [pdf](http://arxiv.org/pdf/1006.0554v3)

> This document is the aggregation of six discussions of Lopes et al. (2010) that we submitted to the proceedings of the Ninth Valencia Meeting, held in Benidorm, Spain, on June 3-8, 2010, in conjunction with Hedibert Lopes' talk at this meeting, and of a further discussion of the rejoinder by Lopes et al. (2010). The main point in those discussions is the potential for degeneracy in the particle learning methodology, related with the exponential forgetting of the past simulations. We illustrate in particular the resulting difficulties in the case of mixtures.

</details>

<details>

<summary>2010-11-23 13:45:37 - Modeling large scale species abundance with latent spatial processes</summary>

- *Avishek Chakraborty, Alan E. Gelfand, Adam M. Wilson, Andrew M. Latimer, John A. Silander Jr*

- `1011.3327v2` - [abs](http://arxiv.org/abs/1011.3327v2) - [pdf](http://arxiv.org/pdf/1011.3327v2)

> Modeling species abundance patterns using local environmental features is an important, current problem in ecology. The Cape Floristic Region (CFR) in South Africa is a global hot spot of diversity and endemism, and provides a rich class of species abundance data for such modeling. Here, we propose a multi-stage Bayesian hierarchical model for explaining species abundance over this region. Our model is specified at areal level, where the CFR is divided into roughly $37{,}000$ one minute grid cells; species abundance is observed at some locations within some cells. The abundance values are ordinally categorized. Environmental and soil-type factors, likely to influence the abundance pattern, are included in the model. We formulate the empirical abundance pattern as a degraded version of the potential pattern, with the degradation effect accomplished in two stages. First, we adjust for land use transformation and then we adjust for measurement error, hence misclassification error, to yield the observed abundance classifications. An important point in this analysis is that only $28%$ of the grid cells have been sampled and that, for sampled grid cells, the number of sampled locations ranges from one to more than one hundred. Still, we are able to develop potential and transformed abundance surfaces over the entire region. In the hierarchical framework, categorical abundance classifications are induced by continuous latent surfaces. The degradation model above is built on the latent scale. On this scale, an areal level spatial regression model was used for modeling the dependence of species abundance on the environmental factors.

</details>

<details>

<summary>2010-11-23 15:31:57 - Concentration inequalities of the cross-validation estimate for stable predictors</summary>

- *Matthieu Cornec*

- `1011.5133v1` - [abs](http://arxiv.org/abs/1011.5133v1) - [pdf](http://arxiv.org/pdf/1011.5133v1)

> In this article, we derive concentration inequalities for the cross-validation estimate of the generalization error for stable predictors in the context of risk assessment. The notion of stability has been first introduced by \cite{DEWA79} and extended by \cite{KEA95}, \cite{BE01} and \cite{KUNIY02} to characterize class of predictors with infinite VC dimension. In particular, this covers $k$-nearest neighbors rules, bayesian algorithm (\cite{KEA95}), boosting,... General loss functions and class of predictors are considered. We use the formalism introduced by \cite{DUD03} to cover a large variety of cross-validation procedures including leave-one-out cross-validation, $k$-fold cross-validation, hold-out cross-validation (or split sample), and the leave-$\upsilon$-out cross-validation.   In particular, we give a simple rule on how to choose the cross-validation, depending on the stability of the class of predictors. In the special case of uniform stability, an interesting consequence is that the number of elements in the test set is not required to grow to infinity for the consistency of the cross-validation procedure. In this special case, the particular interest of leave-one-out cross-validation is emphasized.

</details>

<details>

<summary>2010-11-26 06:22:20 - Prize insights in probability, and one goat of a recycled error: Jason Rosenhouse's The Monty Hall Problem</summary>

- *Anthony B. Morton*

- `1011.3400v2` - [abs](http://arxiv.org/abs/1011.3400v2) - [pdf](http://arxiv.org/pdf/1011.3400v2)

> The Monty Hall problem is the TV game scenario where you, the contestant, are presented with three doors, with a car hidden behind one and goats hidden behind the other two. After you select a door, the host (Monty Hall) opens a second door to reveal a goat. You are then invited to stay with your original choice of door, or to switch to the remaining unopened door, and claim whatever you find behind it. Assuming your objective is to win the car, is your best strategy to stay or switch, or does it not matter? Jason Rosenhouse has provided the definitive analysis of this game, along with several intriguing variations, and discusses some of its psychological and philosophical implications. This extended review examines several themes from the book in some detail from a Bayesian perspective, and points out one apparently inadvertent error.

</details>

<details>

<summary>2010-11-26 10:58:24 - Asymptotic distributions for a class of generalized $L$-statistics</summary>

- *Yuri V. Borovskikh, N. C. Weber*

- `1011.5757v1` - [abs](http://arxiv.org/abs/1011.5757v1) - [pdf](http://arxiv.org/pdf/1011.5757v1)

> We adapt the techniques in Stigler [Ann. Statist. 1 (1973) 472--477] to obtain a new, general asymptotic result for trimmed $U$-statistics via the generalized $L$-statistic representation introduced by Serfling [Ann. Statist. 12 (1984) 76--86]. Unlike existing results, we do not require continuity of an associated distribution at the truncation points. Our results are quite general and are expressed in terms of the quantile function associated with the distribution of the $U$-statistic summands. This approach leads to improved conditions for the asymptotic normality of these trimmed $U$-statistics.

</details>

<details>

<summary>2010-11-26 12:21:51 - A Bayesian approach to star-galaxy classification</summary>

- *Marc Henrion, Daniel J. Mortlock, David J. Hand, Axel Gandy*

- `1011.5770v1` - [abs](http://arxiv.org/abs/1011.5770v1) - [pdf](http://arxiv.org/pdf/1011.5770v1)

> Star-galaxy classification is one of the most fundamental data-processing tasks in survey astronomy, and a critical starting point for the scientific exploitation of survey data. For bright sources this classification can be done with almost complete reliability, but for the numerous sources close to a survey's detection limit each image encodes only limited morphological information. In this regime, from which many of the new scientific discoveries are likely to come, it is vital to utilise all the available information about a source, both from multiple measurements and also prior knowledge about the star and galaxy populations. It is also more useful and realistic to provide classification probabilities than decisive classifications. All these desiderata can be met by adopting a Bayesian approach to star-galaxy classification, and we develop a very general formalism for doing so. An immediate implication of applying Bayes's theorem to this problem is that it is formally impossible to combine morphological measurements in different bands without using colour information as well; however we develop several approximations that disregard colour information as much as possible. The resultant scheme is applied to data from the UKIRT Infrared Deep Sky Survey (UKIDSS), and tested by comparing the results to deep Sloan Digital Sky Survey (SDSS) Stripe 82 measurements of the same sources. The Bayesian classification probabilities obtained from the UKIDSS data agree well with the deep SDSS classifications both overall (a mismatch rate of 0.022, compared to 0.044 for the UKIDSS pipeline classifier) and close to the UKIDSS detection limit (a mismatch rate of 0.068 compared to 0.075 for the UKIDSS pipeline classifier). The Bayesian formalism developed here can be applied to improve the reliability of any star-galaxy classification schemes based on the measured values of morphology statistics alone.

</details>

<details>

<summary>2010-11-26 13:28:57 - Second order ancillary: A differential view from continuity</summary>

- *Ailana M. Fraser, D. A. S. Fraser, Ana-Maria Staicu*

- `1011.5779v1` - [abs](http://arxiv.org/abs/1011.5779v1) - [pdf](http://arxiv.org/pdf/1011.5779v1)

> Second order approximate ancillaries have evolved as the primary ingredient for recent likelihood development in statistical inference. This uses quantile functions rather than the equivalent distribution functions, and the intrinsic ancillary contour is given explicitly as the plug-in estimate of the vector quantile function. The derivation uses a Taylor expansion of the full quantile function, and the linear term gives a tangent to the observed ancillary contour. For the scalar parameter case, there is a vector field that integrates to give the ancillary contours, but for the vector case, there are multiple vector fields and the Frobenius conditions for mutual consistency may not hold. We demonstrate, however, that the conditions hold in a restricted way and that this verifies the second order ancillary contours in moderate deviations. The methodology can generate an appropriate exact ancillary when such exists or an approximate ancillary for the numerical or Monte Carlo calculation of $p$-values and confidence quantiles. Examples are given, including nonlinear regression and several enigmatic examples from the literature.

</details>

<details>

<summary>2010-11-30 08:31:06 - Dose Finding with Escalation with Overdose Control (EWOC) in Cancer Clinical Trials</summary>

- *Mourad Tighiouart, André Rogatko*

- `1011.6479v1` - [abs](http://arxiv.org/abs/1011.6479v1) - [pdf](http://arxiv.org/pdf/1011.6479v1)

> Traditionally, the major objective in phase I trials is to identify a working-dose for subsequent studies, whereas the major endpoint in phase II and III trials is treatment efficacy. The dose sought is typically referred to as the maximum tolerated dose (MTD). Several statistical methodologies have been proposed to select the MTD in cancer phase I trials. In this manuscript, we focus on a Bayesian adaptive design, known as escalation with overdose control (EWOC). Several aspects of this design are discussed, including large sample properties of the sequence of doses selected in the trial, choice of prior distributions, and use of covariates. The methodology is exemplified with real-life examples of cancer phase I trials. In particular, we show in the recently completed ABR-217620 (naptumomab estafenatox) trial that omitting an important predictor of toxicity when dose assignments to cancer patients are determined results in a high percent of patients experiencing severe side effects and a significant proportion treated at sub-optimal doses.

</details>

<details>

<summary>2010-11-30 09:25:14 - Bayesian Models and Decision Algorithms for Complex Early Phase Clinical Trials</summary>

- *Peter F. Thall*

- `1011.6494v1` - [abs](http://arxiv.org/abs/1011.6494v1) - [pdf](http://arxiv.org/pdf/1011.6494v1)

> An early phase clinical trial is the first step in evaluating the effects in humans of a potential new anti-disease agent or combination of agents. Usually called "phase I" or "phase I/II" trials, these experiments typically have the nominal scientific goal of determining an acceptable dose, most often based on adverse event probabilities. This arose from a tradition of phase I trials to evaluate cytotoxic agents for treating cancer, although some methods may be applied in other medical settings, such as treatment of stroke or immunological diseases. Most modern statistical designs for early phase trials include model-based, outcome-adaptive decision rules that choose doses for successive patient cohorts based on data from previous patients in the trial. Such designs have seen limited use in clinical practice, however, due to their complexity, the requirement of intensive, computer-based data monitoring, and the medical community's resistance to change. Still, many actual applications of model-based outcome-adaptive designs have been remarkably successful in terms of both patient benefit and scientific outcome. In this paper I will review several Bayesian early phase trial designs that were tailored to accommodate specific complexities of the treatment regime and patient outcomes in particular clinical settings.

</details>

<details>

<summary>2010-11-30 10:08:40 - Approximate Dynamic Programming and Its Applications to the Design of Phase I Cancer Trials</summary>

- *Jay Bartroff, Tze Leung Lai*

- `1011.6509v1` - [abs](http://arxiv.org/abs/1011.6509v1) - [pdf](http://arxiv.org/pdf/1011.6509v1)

> Optimal design of a Phase I cancer trial can be formulated as a stochastic optimization problem. By making use of recent advances in approximate dynamic programming to tackle the problem, we develop an approximation of the Bayesian optimal design. The resulting design is a convex combination of a "treatment" design, such as Babb et al.'s (1998) escalation with overdose control, and a "learning" design, such as Haines et al.'s (2003) $c$-optimal design, thus directly addressing the treatment versus experimentation dilemma inherent in Phase I trials and providing a simple and intuitive design for clinical use. Computational details are given and the proposed design is compared to existing designs in a simulation study. The design can also be readily modified to include a first stage that cautiously escalates doses similarly to traditional nonparametric step-up/down schemes, while validating the Bayesian parametric model for the efficient model-based design in the second stage.

</details>

<details>

<summary>2010-11-30 18:51:46 - Extended Bayesian Information Criteria for Gaussian Graphical Models</summary>

- *Rina Foygel, Mathias Drton*

- `1011.6640v1` - [abs](http://arxiv.org/abs/1011.6640v1) - [pdf](http://arxiv.org/pdf/1011.6640v1)

> Gaussian graphical models with sparsity in the inverse covariance matrix are of significant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n.

</details>

<details>

<summary>2010-11-30 19:45:50 - Dimension Reduction and Alleviation of Confounding for Spatial Generalized Linear Mixed Models</summary>

- *John Hughes, Murali Haran*

- `1011.6649v1` - [abs](http://arxiv.org/abs/1011.6649v1) - [pdf](http://arxiv.org/pdf/1011.6649v1)

> Non-gaussian spatial data are very common in many disciplines. For instance, count data are common in disease mapping, and binary data are common in ecology. When fitting spatial regressions for such data, one needs to account for dependence to ensure reliable inference for the regression coefficients. The spatial generalized linear mixed model (SGLMM) offers a very popular and flexible approach to modeling such data, but the SGLMM suffers from three major shortcomings: (1) uninterpretability of parameters due to spatial confounding, (2) variance inflation due to spatial confounding, and (3) high-dimensional spatial random effects that make fully Bayesian inference for such models computationally challenging. We propose a new parameterization of the SGLMM that alleviates spatial confounding and speeds computation by greatly reducing the dimension of the spatial random effects. We illustrate the application of our approach to simulated binary, count, and Gaussian spatial datasets, and to a large infant mortality dataset.

</details>


## 2010-12

<details>

<summary>2010-12-03 14:43:07 - An asymptotic approximation of the marginal likelihood for general Markov models</summary>

- *Piotr Zwiernik*

- `1012.0753v1` - [abs](http://arxiv.org/abs/1012.0753v1) - [pdf](http://arxiv.org/pdf/1012.0753v1)

> The standard Bayesian Information Criterion (BIC) is derived under regularity conditions which are not always satisfied by the graphical models with hidden variables. In this paper we derive the BIC score for Bayesian networks in the case of binary data and when the underlying graph is a rooted tree and all the inner nodes represent hidden variables. This provides a direct generalization of a similar formula given by Rusakov and Geiger for naive Bayes models. The main tool used in this paper is a connection between asymptotic approximation of Laplace integrals and the real log-canonical threshold.

</details>

<details>

<summary>2010-12-06 17:54:07 - Simultaneous Sequential Detection of Multiple Interacting Faults</summary>

- *Ram Rajagopal, XuanLong Nguyen, Sinem Coleri Ergen, Pravin Varaiya*

- `1012.1258v1` - [abs](http://arxiv.org/abs/1012.1258v1) - [pdf](http://arxiv.org/pdf/1012.1258v1)

> Single fault sequential change point problems have become important in modeling for various phenomena in large distributed systems, such as sensor networks. But such systems in many situations present multiple interacting faults. For example, individual sensors in a network may fail and detection is performed by comparing measurements between sensors, resulting in statistical dependency among faults. We present a new formulation for multiple interacting faults in a distributed system. The formulation includes specifications of how individual subsystems composing the large system may fail, the information that can be shared among these subsystems and the interaction pattern between faults. We then specify a new sequential algorithm for detecting these faults. The main feature of the algorithm is that it uses composite stopping rules for a subsystem that depend on the decision of other subsystems. We provide asymptotic false alarm and detection delay analysis for this algorithm in the Bayesian setting and show that under certain conditions the algorithm is optimal. The analysis methodology relies on novel detailed comparison techniques between stopping times. We validate the approach with some simulations.

</details>

<details>

<summary>2010-12-07 07:37:26 - Bayesian Statistics Then and Now</summary>

- *Andrew Gelman*

- `1012.1423v1` - [abs](http://arxiv.org/abs/1012.1423v1) - [pdf](http://arxiv.org/pdf/1012.1423v1)

> Discussion of "The Future of Indirect Evidence" by Bradley Efron [arXiv:1012.1161]

</details>

<details>

<summary>2010-12-08 08:33:48 - The Future of Indirect Evidence</summary>

- *Bradley Efron*

- `1012.1161v2` - [abs](http://arxiv.org/abs/1012.1161v2) - [pdf](http://arxiv.org/pdf/1012.1161v2)

> Familiar statistical tests and estimates are obtained by the direct observation of cases of interest: a clinical trial of a new drug, for instance, will compare the drug's effects on a relevant set of patients and controls. Sometimes, though, indirect evidence may be temptingly available, perhaps the results of previous trials on closely related drugs. Very roughly speaking, the difference between direct and indirect statistical evidence marks the boundary between frequentist and Bayesian thinking. Twentieth-century statistical practice focused heavily on direct evidence, on the grounds of superior objectivity. Now, however, new scientific devices such as microarrays routinely produce enormous data sets involving thousands of related situations, where indirect evidence seems too important to ignore. Empirical Bayes methodology offers an attractive direct/indirect compromise. There is already some evidence of a shift toward a less rigid standard of statistical objectivity that allows better use of indirect evidence. This article is basically the text of a recent talk featuring some examples from current practice, with a little bit of futuristic speculation.

</details>

<details>

<summary>2010-12-08 17:23:18 - New consistent and asymptotically normal estimators for random graph mixture models</summary>

- *Christophe Ambroise, Catherine Matias*

- `1003.5165v2` - [abs](http://arxiv.org/abs/1003.5165v2) - [pdf](http://arxiv.org/pdf/1003.5165v2)

> Random graph mixture models are now very popular for modeling real data networks. In these setups, parameter estimation procedures usually rely on variational approximations, either combined with the expectation-maximisation (\textsc{em}) algorithm or with Bayesian approaches. Despite good results on synthetic data, the validity of the variational approximation is however not established. Moreover, the behavior of the maximum likelihood or of the maximum a posteriori estimators approximated by these procedures is not known in these models, due to the dependency structure on the variables. In this work, we show that in many different affiliation contexts (for binary or weighted graphs), estimators based either on moment equations or on the maximization of some composite likelihood are strongly consistent and $\sqrt{n}$-convergent, where $n$ is the number of nodes. As a consequence, our result establishes that the overall structure of an affiliation model can be caught by the description of the network in terms of its number of triads (order 3 structures) and edges (order 2 structures). We illustrate the efficiency of our method on simulated data and compare its performances with other existing procedures. A data set of cross-citations among economics journals is also analyzed.

</details>

<details>

<summary>2010-12-08 19:29:52 - Mapping gravitational lensing of the CMB using local likelihoods</summary>

- *Ethan Anderes, Lloyd Knox, Alexander van Engelen*

- `1012.1833v1` - [abs](http://arxiv.org/abs/1012.1833v1) - [pdf](http://arxiv.org/pdf/1012.1833v1)

> We present a new estimation method for mapping the gravitational lensing potential from observed CMB intensity and polarization fields. Our method uses Bayesian techniques to estimate the average curvature of the potential over small local regions. These local curvatures are then used to construct an estimate of a low pass filter of the gravitational potential. By utilizing Bayesian/likelihood methods one can easily overcome problems with missing and/or non-uniform pixels and problems with partial sky observations (E and B mode mixing, for example). Moreover, our methods are local in nature which allow us to easily model spatially varying beams and are highly parallelizable. We note that our estimates do not rely on the typical Taylor approximation which is used to construct estimates of the gravitational potential by Fourier coupling. We present our methodology with a flat sky simulation under nearly ideal experimental conditions with a noise level of 1 $\mu K$-arcmin for the temperature field, $\sqrt{2}$ $\mu K$-arcmin for the polarization fields, with an instrumental beam full width at half maximum (FWHM) of 0.25 arcmin.

</details>

<details>

<summary>2010-12-08 21:41:02 - Multiple change-point Poisson model for threshold exceedances of air pollution concentrations</summary>

- *Janos Gyarmati-Szabo, Leonid V. Bogachev, Haibo Chen*

- `1012.1879v1` - [abs](http://arxiv.org/abs/1012.1879v1) - [pdf](http://arxiv.org/pdf/1012.1879v1)

> A Bayesian multiple change-point model is proposed to analyse violations of air quality standards by pollutants such as nitrogen oxides (NO2 and NO) and carbon monoxide (CO). The model is built on the assumption that the occurrence of threshold exceedances may be described by a non-homogeneous Poisson process with a step rate function. Unlike earlier approaches, our model is not restricted by a predetermined number of change-points, nor does it involve any covariates. Possible short-range correlations in the exceedance data (e.g., due to chemical and meteorological factors) are removed via declusterisation. The unknown rate function is estimated using a reversible jump MCMC sampling algorithm adapted from Green (1995), which allows for transitions between parameter subspaces of varying dimension. This technique is applied to the 17-year (1993-2009) daily NO2, NO and CO concentration data in the City of Leeds, UK. The results are validated by running the MCMC estimator on simulated data replicated via a posterior estimate of the rate function. The findings are interpreted and discussed in relation to some known traffic control actions. The proposed methodology may be useful in the air quality management context by providing quantitative objective means to measure the efficacy of pollution control programmes.

</details>

<details>

<summary>2010-12-15 18:01:25 - Translating biomarkers between multi-way time-series experiments</summary>

- *Ilkka Huopaniemi, Tommi Suvitaival, Matej Orešič, Samuel Kaski*

- `1012.3407v1` - [abs](http://arxiv.org/abs/1012.3407v1) - [pdf](http://arxiv.org/pdf/1012.3407v1)

> Translating potential disease biomarkers between multi-species 'omics' experiments is a new direction in biomedical research. The existing methods are limited to simple experimental setups such as basic healthy-diseased comparisons. Most of these methods also require an a priori matching of the variables (e.g., genes or metabolites) between the species. However, many experiments have a complicated multi-way experimental design often involving irregularly-sampled time-series measurements, and for instance metabolites do not always have known matchings between organisms. We introduce a Bayesian modelling framework for translating between multiple species the results from 'omics' experiments having a complex multi-way, time-series experimental design. The underlying assumption is that the unknown matching can be inferred from the response of the variables to multiple covariates including time.

</details>

<details>

<summary>2010-12-16 12:47:34 - Fast Convergent Algorithms for Expectation Propagation Approximate Bayesian Inference</summary>

- *Matthias W. Seeger, Hannes Nickisch*

- `1012.3584v1` - [abs](http://arxiv.org/abs/1012.3584v1) - [pdf](http://arxiv.org/pdf/1012.3584v1)

> We propose a novel algorithm to solve the expectation propagation relaxation of Bayesian inference for continuous-variable graphical models. In contrast to most previous algorithms, our method is provably convergent. By marrying convergent EP ideas from (Opper&Winther 05) with covariance decoupling techniques (Wipf&Nagarajan 08, Nickisch&Seeger 09), it runs at least an order of magnitude faster than the most commonly used EP solver.

</details>

<details>

<summary>2010-12-17 14:05:42 - Ultra-high Dimensional Multiple Output Learning With Simultaneous Orthogonal Matching Pursuit: A Sure Screening Approach</summary>

- *Mladen Kolar, Eric P. Xing*

- `1012.3880v1` - [abs](http://arxiv.org/abs/1012.3880v1) - [pdf](http://arxiv.org/pdf/1012.3880v1)

> We propose a novel application of the Simultaneous Orthogonal Matching Pursuit (S-OMP) procedure for sparsistant variable selection in ultra-high dimensional multi-task regression problems. Screening of variables, as introduced in \cite{fan08sis}, is an efficient and highly scalable way to remove many irrelevant variables from the set of all variables, while retaining all the relevant variables. S-OMP can be applied to problems with hundreds of thousands of variables and once the number of variables is reduced to a manageable size, a more computationally demanding procedure can be used to identify the relevant variables for each of the regression outputs. To our knowledge, this is the first attempt to utilize relatedness of multiple outputs to perform fast screening of relevant variables. As our main theoretical contribution, we prove that, asymptotically, S-OMP is guaranteed to reduce an ultra-high number of variables to below the sample size without losing true relevant variables. We also provide formal evidence that a modified Bayesian information criterion (BIC) can be used to efficiently determine the number of iterations in S-OMP. We further provide empirical evidence on the benefit of variable selection using multiple regression outputs jointly, as opposed to performing variable selection for each output separately. The finite sample performance of S-OMP is demonstrated on extensive simulation studies, and on a genetic association mapping problem. $Keywords$ Adaptive Lasso; Greedy forward regression; Orthogonal matching pursuit; Multi-output regression; Multi-task learning; Simultaneous orthogonal matching pursuit; Sure screening; Variable selection

</details>

<details>

<summary>2010-12-21 19:18:49 - Scalable Inference of Customer Similarities from Interactions Data using Dirichlet Processes</summary>

- *Michael Braun, André Bonfrer*

- `1012.4769v1` - [abs](http://arxiv.org/abs/1012.4769v1) - [pdf](http://arxiv.org/pdf/1012.4769v1)

> Under the sociological theory of homophily, people who are similar to one another are more likely to interact with one another. Marketers often have access to data on interactions among customers from which, with homophily as a guiding principle, inferences could be made about the underlying similarities. However, larger networks face a quadratic explosion in the number of potential interactions that need to be modeled. This scalability problem renders probability models of social interactions computationally infeasible for all but the smallest networks. In this paper we develop a probabilistic framework for modeling customer interactions that is both grounded in the theory of homophily, and is flexible enough to account for random variation in who interacts with whom. In particular, we present a novel Bayesian nonparametric approach, using Dirichlet processes, to moderate the scalability problems that marketing researchers encounter when working with networked data. We find that this framework is a powerful way to draw insights into latent similarities of customers, and we discuss how marketers can apply these insights to segmentation and targeting activities.

</details>

<details>

<summary>2010-12-29 20:47:55 - Large-scale interval and point estimates from an empirical Bayes extension of confidence posteriors</summary>

- *David R. Bickel*

- `1012.6033v1` - [abs](http://arxiv.org/abs/1012.6033v1) - [pdf](http://arxiv.org/pdf/1012.6033v1)

> The proposed approach extends the confidence posterior distribution to the semi-parametric empirical Bayes setting. Whereas the Bayesian posterior is defined in terms of a prior distribution conditional on the observed data, the confidence posterior is defined such that the probability that the parameter value lies in any fixed subset of parameter space, given the observed data, is equal to the coverage rate of the corresponding confidence interval. A confidence posterior that has correct frequentist coverage at each fixed parameter value is combined with the estimated local false discovery rate to yield a parameter distribution from which interval and point estimates are derived within the framework of minimizing expected loss. The point estimates exhibit suitable shrinkage toward the null hypothesis value, making them practical for automatically ranking features in order of priority. The corresponding confidence intervals are also shrunken and tend to be much shorter than their fixed-parameter counterparts, as illustrated with gene expression data. Further, simulations confirm a theoretical argument that the shrunken confidence intervals cover the parameter at a higher-than-nominal frequency.

</details>

