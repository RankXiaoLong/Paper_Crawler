# 2008

## TOC

- [2008-01](#2008-01)
- [2008-02](#2008-02)
- [2008-03](#2008-03)
- [2008-04](#2008-04)
- [2008-05](#2008-05)
- [2008-06](#2008-06)
- [2008-07](#2008-07)
- [2008-08](#2008-08)
- [2008-09](#2008-09)
- [2008-10](#2008-10)
- [2008-11](#2008-11)
- [2008-12](#2008-12)

## 2008-01

<details>

<summary>2008-01-07 08:30:01 - Estimating of $P(Y<X)$ in the Exponential case Based on Censored Samples</summary>

- *A. M. Abd Elfattah, O. Mohamed Marwa*

- `0801.0922v1` - [abs](http://arxiv.org/abs/0801.0922v1) - [pdf](http://arxiv.org/pdf/0801.0922v1)

> In this article, the estimation of reliability of a system is discussed $p(y<x)$ when strength, $X$, and stress, $Y$, are two independent exponential distribution with different scale parameters when the available data are type II Censored sample. Different methods for estimating the reliability are applied. The point estimators obtained are maximum likelihood estimator, uniformly minimum variance unbiased estimator, and Bayesian estimators based on conjugate and non informative prior distributions. A comparison of the estimates obtained is performed. Interval estimators of the reliability are also discussed.

</details>

<details>

<summary>2008-01-13 16:15:17 - Symmetry of models versus models of symmetry</summary>

- *Gert de Cooman, Enrique Miranda*

- `0801.1966v1` - [abs](http://arxiv.org/abs/0801.1966v1) - [pdf](http://arxiv.org/pdf/0801.1966v1)

> A model for a subject's beliefs about a phenomenon may exhibit symmetry, in the sense that it is invariant under certain transformations. On the other hand, such a belief model may be intended to represent that the subject believes or knows that the phenomenon under study exhibits symmetry. We defend the view that these are fundamentally different things, even though the difference cannot be captured by Bayesian belief models. In fact, the failure to distinguish between both situations leads to Laplace's so-called Principle of Insufficient Reason, which has been criticised extensively in the literature.   We show that there are belief models (imprecise probability models, coherent lower previsions) that generalise and include the Bayesian belief models, but where this fundamental difference can be captured. This leads to two notions of symmetry for such belief models: weak invariance (representing symmetry of beliefs) and strong invariance (modelling beliefs of symmetry). We discuss various mathematical as well as more philosophical aspects of these notions. We also discuss a few examples to show the relevance of our findings both to probabilistic modelling and to statistical inference, and to the notion of exchangeability in particular.

</details>

<details>

<summary>2008-01-15 18:03:40 - Variational inference for large-scale models of discrete choice</summary>

- *Michael Braun, Jon McAuliffe*

- `0712.2526v3` - [abs](http://arxiv.org/abs/0712.2526v3) - [pdf](http://arxiv.org/pdf/0712.2526v3)

> Discrete choice models are commonly used by applied statisticians in numerous fields, such as marketing, economics, finance, and operations research. When agents in discrete choice models are assumed to have differing preferences, exact inference is often intractable. Markov chain Monte Carlo techniques make approximate inference possible, but the computational cost is prohibitive on the large data sets now becoming routinely available. Variational methods provide a deterministic alternative for approximation of the posterior distribution. We derive variational procedures for empirical Bayes and fully Bayesian inference in the mixed multinomial logit model of discrete choice. The algorithms require only that we solve a sequence of unconstrained optimization problems, which are shown to be convex. Extensive simulations demonstrate that variational methods achieve accuracy competitive with Markov chain Monte Carlo, at a small fraction of the computational cost. Thus, variational methods permit inferences on data sets that otherwise could not be analyzed without bias-inducing modifications to the underlying model.

</details>

<details>

<summary>2008-01-16 19:34:50 - Penalized Clustering of Large Scale Functional Data with Multiple Covariates</summary>

- *Ping Ma, Wenxuan Zhong*

- `0801.2555v1` - [abs](http://arxiv.org/abs/0801.2555v1) - [pdf](http://arxiv.org/pdf/0801.2555v1)

> In this article, we propose a penalized clustering method for large scale data with multiple covariates through a functional data approach. In the proposed method, responses and covariates are linked together through nonparametric multivariate functions (fixed effects), which have great flexibility in modeling a variety of function features, such as jump points, branching, and periodicity. Functional ANOVA is employed to further decompose multivariate functions in a reproducing kernel Hilbert space and provide associated notions of main effect and interaction. Parsimonious random effects are used to capture various correlation structures. The mixed-effect models are nested under a general mixture model, in which the heterogeneity of functional data is characterized. We propose a penalized Henderson's likelihood approach for model-fitting and design a rejection-controlled EM algorithm for the estimation. Our method selects smoothing parameters through generalized cross-validation. Furthermore, the Bayesian confidence intervals are used to measure the clustering uncertainty. Simulation studies and real-data examples are presented to investigate the empirical performance of the proposed method. Open-source code is available in the R package MFDA.

</details>

<details>

<summary>2008-01-24 13:19:50 - Higher Accuracy for Bayesian and Frequentist Inference: Large Sample Theory for Small Sample Likelihood</summary>

- *M. Bédard, D. A. S. Fraser, A. Wong*

- `0801.3751v1` - [abs](http://arxiv.org/abs/0801.3751v1) - [pdf](http://arxiv.org/pdf/0801.3751v1)

> Recent likelihood theory produces $p$-values that have remarkable accuracy and wide applicability. The calculations use familiar tools such as maximum likelihood values (MLEs), observed information and parameter rescaling. The usual evaluation of such $p$-values is by simulations, and such simulations do verify that the global distribution of the $p$-values is uniform(0, 1), to high accuracy in repeated sampling. The derivation of the $p$-values, however, asserts a stronger statement, that they have a uniform(0, 1) distribution conditionally, given identified precision information provided by the data. We take a simple regression example that involves exact precision information and use large sample techniques to extract highly accurate information as to the statistical position of the data point with respect to the parameter: specifically, we examine various $p$-values and Bayesian posterior survivor $s$-values for validity. With observed data we numerically evaluate the various $p$-values and $s$-values, and we also record the related general formulas. We then assess the numerical values for accuracy using Markov chain Monte Carlo (McMC) methods. We also propose some third-order likelihood-based procedures for obtaining means and variances of Bayesian posterior distributions, again followed by McMC assessment. Finally we propose some adaptive McMC methods to improve the simulation acceptance rates. All these methods are based on asymptotic analysis that derives from the effect of additional data. And the methods use simple calculations based on familiar maximizing values and related informations. The example illustrates the general formulas and the ease of calculations, while the McMC assessments demonstrate the numerical validity of the $p$-values as percentage position of a data point. The example, however, is very simple and transparent, and thus gives little indication that in a wide generality of models the formulas do accurately separate information for almost any parameter of interest, and then do give accurate $p$-value determinations from that information. As illustration an enigmatic problem in the literature is discussed and simulations are recorded; various examples in the literature are cited.

</details>

<details>

<summary>2008-01-26 08:59:24 - A Semi-parametric Technique for the Quantitative Analysis of Dynamic Contrast-enhanced MR Images Based on Bayesian P-splines</summary>

- *Volker J. Schmid, Brandon Whitcher, Anwar R. Padhani, Guang-Zhong Yang*

- `0801.4065v1` - [abs](http://arxiv.org/abs/0801.4065v1) - [pdf](http://arxiv.org/pdf/0801.4065v1)

> Dynamic Contrast-enhanced Magnetic Resonance Imaging (DCE-MRI) is an important tool for detecting subtle kinetic changes in cancerous tissue. Quantitative analysis of DCE-MRI typically involves the convolution of an arterial input function (AIF) with a nonlinear pharmacokinetic model of the contrast agent concentration. Parameters of the kinetic model are biologically meaningful, but the optimization of the non-linear model has significant computational issues. In practice, convergence of the optimization algorithm is not guaranteed and the accuracy of the model fitting may be compromised. To overcome this problems, this paper proposes a semi-parametric penalized spline smoothing approach, with which the AIF is convolved with a set of B-splines to produce a design matrix using locally adaptive smoothing parameters based on Bayesian penalized spline models (P-splines). It has been shown that kinetic parameter estimation can be obtained from the resulting deconvolved response function, which also includes the onset of contrast enhancement. Detailed validation of the method, both with simulated and in vivo data, is provided.

</details>

<details>

<summary>2008-01-28 09:55:00 - Generalization of Jeffreys' divergence based priors for Bayesian hypothesis testing</summary>

- *M. J. Bayarri, G. García-Donato*

- `0801.4224v1` - [abs](http://arxiv.org/abs/0801.4224v1) - [pdf](http://arxiv.org/pdf/0801.4224v1)

> In this paper we introduce objective proper prior distributions for hypothesis testing and model selection based on measures of divergence between the competing models; we call them divergence based (DB) priors. DB priors have simple forms and desirable properties, like information (finite sample) consistency; often, they are similar to other existing proposals like the intrinsic priors; moreover, in normal linear models scenarios, they exactly reproduce Jeffreys-Zellner-Siow priors. Most importantly, in challenging scenarios such as irregular models and mixture models, the DB priors are well defined and very reasonable, while alternative proposals are not. We derive approximations to the DB priors as well as MCMC and asymptotic expressions for the associated Bayes factors.

</details>

<details>

<summary>2008-01-31 01:20:26 - A preferential attachment model with Poisson growth for scale-free networks</summary>

- *Paul Sheridan, Yuichi Yagahara, Hidetoshi Shimodaira*

- `0801.2800v2` - [abs](http://arxiv.org/abs/0801.2800v2) - [pdf](http://arxiv.org/pdf/0801.2800v2)

> We propose a scale-free network model with a tunable power-law exponent. The Poisson growth model, as we call it, is an offshoot of the celebrated model of Barab\'{a}si and Albert where a network is generated iteratively from a small seed network; at each step a node is added together with a number of incident edges preferentially attached to nodes already in the network. A key feature of our model is that the number of edges added at each step is a random variable with Poisson distribution, and, unlike the Barab\'{a}si-Albert model where this quantity is fixed, it can generate any network. Our model is motivated by an application in Bayesian inference implemented as Markov chain Monte Carlo to estimate a network; for this purpose, we also give a formula for the probability of a network under our model.

</details>


## 2008-02

<details>

<summary>2008-02-01 07:41:39 - Nonparametric Bayesian model selection and averaging</summary>

- *Subhashis Ghosal, Jüri Lember, Aad van der Vaart*

- `0802.0069v1` - [abs](http://arxiv.org/abs/0802.0069v1) - [pdf](http://arxiv.org/pdf/0802.0069v1)

> We consider nonparametric Bayesian estimation of a probability density $p$ based on a random sample of size $n$ from this density using a hierarchical prior. The prior consists, for instance, of prior weights on the regularity of the unknown density combined with priors that are appropriate given that the density has this regularity. More generally, the hierarchy consists of prior weights on an abstract model index and a prior on a density model for each model index. We present a general theorem on the rate of contraction of the resulting posterior distribution as $n\to \infty$, which gives conditions under which the rate of contraction is the one attached to the model that best approximates the true density of the observations. This shows that, for instance, the posterior distribution can adapt to the smoothness of the underlying density. We also study the posterior distribution of the model index, and find that under the same conditions the posterior distribution gives negligible weight to models that are bigger than the optimal one, and thus selects the optimal model or smaller models that also approximate the true density well. We apply these result to log spline density models, where we show that the prior weights on the regularity index interact with the priors on the models, making the exact rates depend in a complicated way on the priors, but also that the rate is fairly robust to specification of the prior weights.

</details>

<details>

<summary>2008-02-01 19:48:03 - Covariance estimation for multivariate conditionally Gaussian dynamic linear models</summary>

- *K. Triantafyllopoulos*

- `0802.0191v1` - [abs](http://arxiv.org/abs/0802.0191v1) - [pdf](http://arxiv.org/pdf/0802.0191v1)

> In multivariate time series, the estimation of the covariance matrix of the observation innovations plays an important role in forecasting as it enables the computation of the standardized forecast error vectors as well as it enables the computation of confidence bounds of the forecasts. We develop an on-line, non-iterative Bayesian algorithm for estimation and forecasting. It is empirically found that, for a range of simulated time series, the proposed covariance estimator has good performance converging to the true values of the unknown observation covariance matrix. Over a simulated time series, the new method approximates the correct estimates, produced by a non-sequential Monte Carlo simulation procedure, which is used here as the gold standard. The special, but important, vector autoregressive (VAR) and time-varying VAR models are illustrated by considering London metal exchange data consisting of spot prices of aluminium, copper, lead and zinc.

</details>

<details>

<summary>2008-02-01 22:18:34 - Posterior mean and variance approximation for regression and time series problems</summary>

- *K. Triantafyllopoulos, P. J. Harrison*

- `0802.0213v1` - [abs](http://arxiv.org/abs/0802.0213v1) - [pdf](http://arxiv.org/pdf/0802.0213v1)

> This paper develops a methodology for approximating the posterior first two moments of the posterior distribution in Bayesian inference. Partially specified probability models, which are defined only by specifying means and variances, are constructed based upon second-order conditional independence, in order to facilitate posterior updating and prediction of required distributional quantities. Such models are formulated particularly for multivariate regression and time series analysis with unknown observational variance-covariance components. The similarities and differences of these models with the Bayes linear approach are established. Several subclasses of important models, including regression and time series models with errors following multivariate $t$, inverted multivariate $t$ and Wishart distributions, are discussed in detail. Two numerical examples consisting of simulated data and of US investment and change in inventory data illustrate the proposed methodology.

</details>

<details>

<summary>2008-02-01 22:35:49 - Multivariate stochastic volatility with Bayesian dynamic linear models</summary>

- *K. Triantafyllopoulos*

- `0802.0214v1` - [abs](http://arxiv.org/abs/0802.0214v1) - [pdf](http://arxiv.org/pdf/0802.0214v1)

> This paper develops a Bayesian procedure for estimation and forecasting of the volatility of multivariate time series. The foundation of this work is the matrix-variate dynamic linear model, for the volatility of which we adopt a multiplicative stochastic evolution, using Wishart and singular multivariate beta distributions. A diagonal matrix of discount factors is employed in order to discount the variances element by element and therefore allowing a flexible and pragmatic variance modelling approach. Diagnostic tests and sequential model monitoring are discussed in some detail. The proposed estimation theory is applied to a four-dimensional time series, comprising spot prices of aluminium, copper, lead and zinc of the London metal exchange. The empirical findings suggest that the proposed Bayesian procedure can be effectively applied to financial data, overcoming many of the disadvantages of existing volatility models.

</details>

<details>

<summary>2008-02-01 22:46:05 - Multivariate control charts based on Bayesian state space models</summary>

- *K. Triantafyllopoulos*

- `0802.0218v1` - [abs](http://arxiv.org/abs/0802.0218v1) - [pdf](http://arxiv.org/pdf/0802.0218v1)

> This paper develops a new multivariate control charting method for vector autocorrelated and serially correlated processes. The main idea is to propose a Bayesian multivariate local level model, which is a generalization of the Shewhart-Deming model for autocorrelated processes, in order to provide the predictive error distribution of the process and then to apply a univariate modified EWMA control chart to the logarithm of the Bayes' factors of the predictive error density versus the target error density. The resulting chart is proposed as capable to deal with both the non-normality and the autocorrelation structure of the log Bayes' factors. The new control charting scheme is general in application and it has the advantage to control simultaneously not only the process mean vector and the dispersion covariance matrix, but also the entire target distribution of the process. Two examples of London metal exchange data and of production time series data illustrate the capabilities of the new control chart.

</details>

<details>

<summary>2008-02-01 22:52:41 - Dynamic generalized linear models for non-Gaussian time series forecasting</summary>

- *K. Triantafyllopoulos*

- `0802.0219v1` - [abs](http://arxiv.org/abs/0802.0219v1) - [pdf](http://arxiv.org/pdf/0802.0219v1)

> The purpose of this paper is to provide a discussion, with illustrating examples, on Bayesian forecasting for dynamic generalized linear models (DGLMs). Adopting approximate Bayesian analysis, based on conjugate forms and on Bayes linear estimation, we describe the theoretical framework and then we provide detailed examples of response distributions, including binomial, Poisson, negative binomial, geometric, normal, log-normal, gamma, exponential, Weibull, Pareto, beta, and inverse Gaussian. We give numerical illustrations for all distributions (except for the normal). Putting together all the above distributions, we give a unified Bayesian approach to non-Gaussian time series analysis, with applications from finance and medicine to biology and the behavioural sciences. Throughout the models we discuss Bayesian forecasting and, for each model, we derive the multi-step forecast mean. Finally, we describe model assessment using the likelihood function, and Bayesian model monitoring.

</details>

<details>

<summary>2008-02-01 23:34:43 - Multivariate stochastic volatility using state space models</summary>

- *K. Triantafyllopoulos*

- `0802.0223v1` - [abs](http://arxiv.org/abs/0802.0223v1) - [pdf](http://arxiv.org/pdf/0802.0223v1)

> A Bayesian procedure is developed for multivariate stochastic volatility, using state space models. An autoregressive model for the log-returns is employed. We generalize the inverted Wishart distribution to allow for different correlation structure between the observation and state innovation vectors and we extend the convolution between the Wishart and the multivariate singular beta distribution. A multiplicative model based on the generalized inverted Wishart and multivariate singular beta distributions is proposed for the evolution of the volatility and a flexible sequential volatility updating is employed. The proposed algorithm for the volatility is fast and computationally cheap and it can be used for on-line forecasting. The methods are illustrated with an example consisting of foreign exchange rates data of 8 currencies. The empirical results suggest that time-varying correlations can be estimated efficiently, even in situations of high dimensional data.

</details>

<details>

<summary>2008-02-04 15:17:34 - A regional Bayesian POT model for flood frequency analysis</summary>

- *Mathieu Ribatet, Eric Sauquet, Jean-Michel Grésillon, Taha B. M. J. Ouarda*

- `0802.0433v1` - [abs](http://arxiv.org/abs/0802.0433v1) - [pdf](http://arxiv.org/pdf/0802.0433v1)

> Flood frequency analysis is usually based on the fitting of an extreme value distribution to the local streamflow series. However, when the local data series is short, frequency analysis results become unreliable. Regional frequency analysis is a convenient way to reduce the estimation uncertainty. In this work, we propose a regional Bayesian model for short record length sites. This model is less restrictive than the index flood model while preserving the formalism of "homogeneous regions". The performance of the proposed model is assessed on a set of gauging stations in France. The accuracy of quantile estimates as a function of the degree of homogeneity of the pooling group is also analysed. The results indicate that the regional Bayesian model outperforms the index flood model and local estimators. Furthermore, it seems that working with relatively large and homogeneous regions may lead to more accurate results than working with smaller and highly homogeneous regions.

</details>

<details>

<summary>2008-02-04 15:21:26 - Modeling All Exceedances Above a Threshold Using an Extremal Dependence Structure: Inferences on Several Flood Characteristics</summary>

- *Mathieu Ribatet, Taha B. M. J. Ouarda, Eric Sauquet, Jean-Michel Grésillon*

- `0802.0436v1` - [abs](http://arxiv.org/abs/0802.0436v1) - [pdf](http://arxiv.org/pdf/0802.0436v1)

> Flood quantile estimation is of great importance for many engineering studies and policy decisions. However, practitioners must often deal with small data available. Thus, the information must be used optimally. In the last decades, to reduce the waste of data, inferential methodology has evolved from annual maxima modeling to peaks over a threshold one. To mitigate the lack of data, peaks over a threshold are sometimes combined with additional information - mostly regional and historical information. However, whatever the extra information is, the most precious information for the practitioner is found at the target site. In this study, a model that allows inferences on the whole time series is introduced. In particular, the proposed model takes into account the dependence between successive extreme observations using an appropriate extremal dependence structure. Results show that this model leads to more accurate flood peak quantile estimates than conventional estimators. In addition, as the time dependence is taken into account, inferences on other flood characteristics can be performed. An illustration is given on flood duration. Our analysis shows that the accuracy of the proposed models to estimate the flood duration is related to specific catchment characteristics. Some suggestions to increase the flood duration predictions are introduced.

</details>

<details>

<summary>2008-02-04 15:33:35 - Usefulness of the Reversible Jump Markov Chain Monte Carlo Model in Regional Flood Frequency Analysis</summary>

- *Mathieu Ribatet, Eric Sauquet, Jean-Michel Grésillon, Taha B. M. J. Ouarda*

- `0802.0444v1` - [abs](http://arxiv.org/abs/0802.0444v1) - [pdf](http://arxiv.org/pdf/0802.0444v1)

> Regional flood frequency analysis is a convenient way to reduce estimation uncertainty when few data are available at the gauging site. In this work, a model that allows a non-null probability to a regional fixed shape parameter is presented. This methodology is integrated within a Bayesian framework and uses reversible jump techniques. The performance on stochastic data of this new estimator is compared to two other models: a conventional Bayesian analysis and the index flood approach. Results show that the proposed estimator is absolutely suited to regional estimation when only a few data are available at the target site. Moreover, unlike the index flood estimator, target site index flood error estimation seems to have less impact on Bayesian estimators. Some suggestions about configurations of the pooling groups are also presented to increase the performance of each estimator.

</details>

<details>

<summary>2008-02-06 07:23:14 - Comment: Bayesian Checking of the Second Levels of Hierarchical Models</summary>

- *M. Evans*

- `0802.0746v1` - [abs](http://arxiv.org/abs/0802.0746v1) - [pdf](http://arxiv.org/pdf/0802.0746v1)

> We discuss the methods of Evans and Moshonov [Bayesian Analysis 1 (2006) 893--914, Bayesian Statistics and Its Applications (2007) 145--159] concerning checking for prior-data conflict and their relevance to the method proposed in this paper. [arXiv:0802.0743]

</details>

<details>

<summary>2008-02-06 07:31:54 - Comment: Bayesian Checking of the Second Levels of Hierarchical Models</summary>

- *Andrew Gelman*

- `0802.0747v1` - [abs](http://arxiv.org/abs/0802.0747v1) - [pdf](http://arxiv.org/pdf/0802.0747v1)

> Comment: Bayesian Checking of the Second Levels of Hierarchical Models [arXiv:0802.0743]

</details>

<details>

<summary>2008-02-06 07:38:22 - Comment: Bayesian Checking of the Second Levels of Hierarchical Models</summary>

- *Valen E. Johnson*

- `0802.0749v1` - [abs](http://arxiv.org/abs/0802.0749v1) - [pdf](http://arxiv.org/pdf/0802.0749v1)

> Comment: Bayesian Checking of the Second Levels of Hierarchical Models [arXiv:0802.0743]

</details>

<details>

<summary>2008-02-06 08:07:49 - Comment: Bayesian Checking of the Second Level of Hierarchical Models: Cross-Validated Posterior Predictive Checks Using Discrepancy Measures</summary>

- *Michael D. Larsen, Lu Lu*

- `0802.0752v1` - [abs](http://arxiv.org/abs/0802.0752v1) - [pdf](http://arxiv.org/pdf/0802.0752v1)

> Comment: Bayesian Checking of the Second Level of Hierarchical Models [arXiv:0802.0743]

</details>

<details>

<summary>2008-02-06 08:21:31 - Rejoinder: Bayesian Checking of the Second Levels of Hierarchical Models</summary>

- *M. J. Bayarri, M. E. Castellanos*

- `0802.0754v1` - [abs](http://arxiv.org/abs/0802.0754v1) - [pdf](http://arxiv.org/pdf/0802.0754v1)

> Rejoinder: Bayesian Checking of the Second Levels of Hierarchical Models [arXiv:0802.0743]

</details>

<details>

<summary>2008-02-06 08:31:37 - Bayesian Checking of the Second Levels of Hierarchical Models</summary>

- *M. J. Bayarri, M. E. Castellanos*

- `0802.0743v1` - [abs](http://arxiv.org/abs/0802.0743v1) - [pdf](http://arxiv.org/pdf/0802.0743v1)

> Hierarchical models are increasingly used in many applications. Along with this increased use comes a desire to investigate whether the model is compatible with the observed data. Bayesian methods are well suited to eliminate the many (nuisance) parameters in these complicated models; in this paper we investigate Bayesian methods for model checking. Since we contemplate model checking as a preliminary, exploratory analysis, we concentrate on objective Bayesian methods in which careful specification of an informative prior distribution is avoided. Numerous examples are given and different proposals are investigated and critically compared.

</details>

<details>

<summary>2008-02-07 17:31:58 - Reduction principles for quantile and Bahadur-Kiefer processes of long-range dependent linear sequences</summary>

- *Miklós Csörgő, Rafal Kulik*

- `0802.1025v1` - [abs](http://arxiv.org/abs/0802.1025v1) - [pdf](http://arxiv.org/pdf/0802.1025v1)

> In this paper we consider quantile and Bahadur-Kiefer processes for long range dependent linear sequences. These processes, unlike in previous studies, are considered on the whole interval $(0,1)$. As it is well-known, quantile processes can have very erratic behavior on the tails. We overcome this problem by considering these processes with appropriate weight functions. In this way we conclude strong approximations that yield some remarkable phenomena that are not shared with i.i.d. sequences, including weak convergence of the Bahadur-Kiefer processes, a different pointwise behavior of the general and uniform Bahadur-Kiefer processes, and a somewhat "strange" behavior of the general quantile process.

</details>

<details>

<summary>2008-02-10 23:28:34 - A Bayesian reassessment of nearest-neighbour classification</summary>

- *Lionel Cucala, Jean-Michel Marin, Christian Robert, Mike Titterington*

- `0802.1357v1` - [abs](http://arxiv.org/abs/0802.1357v1) - [pdf](http://arxiv.org/pdf/0802.1357v1)

> The k-nearest-neighbour procedure is a well-known deterministic method used in supervised classification. This paper proposes a reassessment of this approach as a statistical technique derived from a proper probabilistic model; in particular, we modify the assessment made in a previous analysis of this method undertaken by Holmes and Adams (2002,2003), and evaluated by Manocha and Girolami (2007), where the underlying probabilistic model is not completely well-defined. Once a clear probabilistic basis for the k-nearest-neighbour procedure is established, we derive computational tools for conducting Bayesian inference on the parameters of the corresponding model. In particular, we assess the difficulties inherent to pseudo-likelihood and to path sampling approximations of an intractable normalising constant, and propose a perfect sampling strategy to implement a correct MCMC sampler associated with our model. If perfect sampling is not available, we suggest using a Gibbs sampling approximation. Illustrations of the performance of the corresponding Bayesian classifier are provided for several benchmark datasets, demonstrating in particular the limitations of the pseudo-likelihood approximation in this set-up.

</details>

<details>

<summary>2008-02-17 12:00:10 - Forecasting with time-varying vector autoregressive models</summary>

- *K. Triantafyllopoulos*

- `0802.0220v2` - [abs](http://arxiv.org/abs/0802.0220v2) - [pdf](http://arxiv.org/pdf/0802.0220v2)

> The purpose of this paper is to propose a time-varying vector autoregressive model (TV-VAR) for forecasting multivariate time series. The model is casted into a state-space form that allows flexible description and analysis. The volatility covariance matrix of the time series is modelled via inverted Wishart and singular multivariate beta distributions allowing a fully conjugate Bayesian inference. Model performance and model comparison is done via the likelihood function, sequential Bayes factors, the mean of squared standardized forecast errors, the mean of absolute forecast errors (known also as mean absolute deviation), and the mean forecast error. Bayes factors are also used in order to choose the autoregressive order of the model. Multi-step forecasting is discussed in detail and a flexible formula is proposed to approximate the forecast function. Two examples, consisting of bivariate data of IBM shares and of foreign exchange (FX) rates for 8 currencies, illustrate the methods. For the IBM data we discuss model performance and multi-step forecasting in some detail. For the FX data we discuss sequential portfolio allocation; for both data sets our empirical findings suggest that the TV-VAR models outperform the widely used VAR models.

</details>

<details>

<summary>2008-02-22 14:36:35 - Consistent estimation of the architecture of multilayer perceptrons</summary>

- *Joseph Rynkiewicz*

- `0802.3327v1` - [abs](http://arxiv.org/abs/0802.3327v1) - [pdf](http://arxiv.org/pdf/0802.3327v1)

> We consider regression models involving multilayer perceptrons (MLP) with one hidden layer and a Gaussian noise. The estimation of the parameters of the MLP can be done by maximizing the likelihood of the model. In this framework, it is difficult to determine the true number of hidden units using an information criterion, like the Bayesian information criteria (BIC), because the information matrix of Fisher is not invertible if the number of hidden units is overestimated. Indeed, the classical theoretical justification of information criteria relies entirely on the invertibility of this matrix. However, using recent methodology introduced to deal with models with a loss of identifiability, we prove that suitable information criterion leads to consistent estimation of the true number of hidden units.

</details>

<details>

<summary>2008-02-23 00:06:12 - Full Bayesian analysis for a class of jump-diffusion models</summary>

- *Laura L. R. Rifo, Soledad Torres*

- `0708.4131v2` - [abs](http://arxiv.org/abs/0708.4131v2) - [pdf](http://arxiv.org/pdf/0708.4131v2)

> A new Bayesian significance test is adjusted for jump detection in a diffusion process. This is an advantageous procedure for temporal data having extreme valued outliers, like financial data, pluvial or tectonic forces records and others.

</details>

<details>

<summary>2008-02-28 12:35:02 - Bayesian Estimation of Inequalities with Non-Rectangular Censored Survey Data</summary>

- *Eric Gautier*

- `0802.4190v1` - [abs](http://arxiv.org/abs/0802.4190v1) - [pdf](http://arxiv.org/pdf/0802.4190v1)

> Synthetic indices are used in Economics to measure various aspects of monetary inequalities. These scalar indices take as input the distribution over a finite population, for example the population of a specific country. In this article we consider the case of the French 2004 Wealth survey. We have at hand a partial measurement on the distribution of interest consisting of bracketed and sometimes missing data, over a subsample of the population of interest. We present in this article the statistical methodology used to obtain point and interval estimates taking into account the various uncertainties. The inequality indices being nonlinear in the input distribution, we rely on a simulation based approach where the model for the wealth per household is multivariate. Using the survey data as well as matched auxiliary tax declarations data, we have at hand a quite intricate non-rectangle multidimensional censoring. For practical issues we use a Bayesian approach. Inference using Monte-Carlo approximations relies on a Monte-Carlo Markov chain algorithm namely the Gibbs sampler. The quantities interesting to the decision maker are taken to be the various inequality indices for the French population. Their distribution conditional on the data of the subsample are assumed to be normal centered on the design-based estimates with variance computed through linearization and taking into account the sample design and total nonresponse. Exogeneous selection of the subsample, in particular the nonresponse mechanism, is assumed and we condition on the adequate covariates.

</details>


## 2008-03

<details>

<summary>2008-03-04 19:31:23 - An EM algorithm for estimation in the Mixture Transition Distribution model</summary>

- *Sophie Lèbre, Pierre-Yves Bourguinon*

- `0803.0525v1` - [abs](http://arxiv.org/abs/0803.0525v1) - [pdf](http://arxiv.org/pdf/0803.0525v1)

> The Mixture Transition Distribution (MTD) model was introduced by Raftery to face the need for parsimony in the modeling of high-order Markov chains in discrete time. The particularity of this model comes from the fact that the effect of each lag upon the present is considered separately and additively, so that the number of parameters required is drastically reduced. However, the efficiency for the MTD parameter estimations proposed up to date still remains problematic on account of the large number of constraints on the parameters. In this paper, an iterative procedure, commonly known as Expectation-Maximization (EM) algorithm, is developed cooperating with the principle of Maximum Likelihood Estimation (MLE) to estimate the MTD parameters. Some applications of modeling MTD show the proposed EM algorithm is easier to be used than the algorithm developed by Berchtold. Moreover, the EM Estimations of parameters for high-order MTD models led on DNA sequences outperform the corresponding fully parametrized Markov chain in terms of Bayesian Information Criterion. A software implementation of our algorithm is available in the library seq++ at http://stat.genopole.cnrs.fr/seqpp

</details>

<details>

<summary>2008-03-07 06:47:24 - Multiple integral representation for functionals of Dirichlet processes</summary>

- *Giovanni Peccati*

- `0803.1029v1` - [abs](http://arxiv.org/abs/0803.1029v1) - [pdf](http://arxiv.org/pdf/0803.1029v1)

> We point out that a proper use of the Hoeffding--ANOVA decomposition for symmetric statistics of finite urn sequences, previously introduced by the author, yields a decomposition of the space of square-integrable functionals of a Dirichlet--Ferguson process, written $L^2(D)$, into orthogonal subspaces of multiple integrals of increasing order. This gives an isomorphism between $L^2(D)$ and an appropriate Fock space over a class of deterministic functions. By means of a well-known result due to Blackwell and MacQueen, we show that each element of the $n$th orthogonal space of multiple integrals can be represented as the $L^2$ limit of $U$-statistics with degenerate kernel of degree $n$. General formulae for the decomposition of a given functional are provided in terms of linear combinations of conditioned expectations whose coefficients are explicitly computed. We show that, in simple cases, multiple integrals have a natural representation in terms of Jacobi polynomials. Several connections are established, in particular with Bayesian decision problems, and with some classic formulae concerning the transition densities of multiallele diffusion models, due to Littler and Fackerell, and Griffiths. Our results may also be used to calculate the best approximation of elements of $L^2(D)$ by means of $U$-statistics of finite vectors of exchangeable observations.

</details>

<details>

<summary>2008-03-11 18:38:52 - Component models for large networks</summary>

- *Janne Sinkkonen, Janne Aukia, Samuel Kaski*

- `0803.1628v1` - [abs](http://arxiv.org/abs/0803.1628v1) - [pdf](http://arxiv.org/pdf/0803.1628v1)

> Being among the easiest ways to find meaningful structure from discrete data, Latent Dirichlet Allocation (LDA) and related component models have been applied widely. They are simple, computationally fast and scalable, interpretable, and admit nonparametric priors. In the currently popular field of network modeling, relatively little work has taken uncertainty of data seriously in the Bayesian sense, and component models have been introduced to the field only recently, by treating each node as a bag of out-going links. We introduce an alternative, interaction component model for communities (ICMc), where the whole network is a bag of links, stemming from different components. The former finds both disassortative and assortative structure, while the alternative assumes assortativity and finds community-like structures like the earlier methods motivated by physics. With Dirichlet Process priors and an efficient implementation the models are highly scalable, as demonstrated with a social network from the Last.fm web site, with 670,000 nodes and 1.89 million links.

</details>

<details>

<summary>2008-03-14 13:47:07 - Statistics of extremes under random censoring</summary>

- *John H. J. Einmahl, Amélie Fils-Villetard, Armelle Guillou*

- `0803.2162v1` - [abs](http://arxiv.org/abs/0803.2162v1) - [pdf](http://arxiv.org/pdf/0803.2162v1)

> We investigate the estimation of the extreme value index when the data are subject to random censorship. We prove, in a unified way, detailed asymptotic normality results for various estimators of the extreme value index and use these estimators as the main building block for estimators of extreme quantiles. We illustrate the quality of these methods by a small simulation study and apply the estimators to medical data.

</details>

<details>

<summary>2008-03-25 17:33:12 - A Bayesian approach to the estimation of maps between riemannian manifolds</summary>

- *Leo T. Butler, Boris Levit*

- `0705.2540v2` - [abs](http://arxiv.org/abs/0705.2540v2) - [pdf](http://arxiv.org/pdf/0705.2540v2)

> Let \Theta be a smooth compact oriented manifold without boundary, embedded in a euclidean space and let \gamma be a smooth map \Theta into a riemannian manifold \Lambda. An unknown state \theta \in \Theta is observed via X=\theta+\epsilon \xi where \epsilon>0 is a small parameter and \xi is a white Gaussian noise. For a given smooth prior on \Theta and smooth estimator g of the map \gamma we derive a second-order asymptotic expansion for the related Bayesian risk. The calculation involves the geometry of the underlying spaces \Theta and \Lambda, in particular, the integration-by-parts formula. Using this result, a second-order minimax estimator of \gamma is found based on the modern theory of harmonic maps and hypo-elliptic differential operators.

</details>

<details>

<summary>2008-03-27 07:01:08 - Simultaneous inference: When should hypothesis testing problems be combined?</summary>

- *Bradley Efron*

- `0803.3863v1` - [abs](http://arxiv.org/abs/0803.3863v1) - [pdf](http://arxiv.org/pdf/0803.3863v1)

> Modern statisticians are often presented with hundreds or thousands of hypothesis testing problems to evaluate at the same time, generated from new scientific technologies such as microarrays, medical and satellite imaging devices, or flow cytometry counters. The relevant statistical literature tends to begin with the tacit assumption that a single combined analysis, for instance, a False Discovery Rate assessment, should be applied to the entire set of problems at hand. This can be a dangerous assumption, as the examples in the paper show, leading to overly conservative or overly liberal conclusions within any particular subclass of the cases. A simple Bayesian theory yields a succinct description of the effects of separation or combination on false discovery rate analyses. The theory allows efficient testing within small subclasses, and has applications to ``enrichment,'' the detection of multi-case effects.

</details>

<details>

<summary>2008-03-31 14:27:41 - Distributions of linear functionals of two parameter Poisson--Dirichlet random measures</summary>

- *Lancelot F. James, Antonio Lijoi, Igor Prünster*

- `0609488v4` - [abs](http://arxiv.org/abs/0609488v4) - [pdf](http://arxiv.org/pdf/math/0609488v4)

> The present paper provides exact expressions for the probability distributions of linear functionals of the two-parameter Poisson--Dirichlet process $\operatorname {PD}(\alpha,\theta)$. We obtain distributional results yielding exact forms for density functions of these functionals. Moreover, several interesting integral identities are obtained by exploiting a correspondence between the mean of a Poisson--Dirichlet process and the mean of a suitable Dirichlet process. Finally, some distributional characterizations in terms of mixture representations are proved. The usefulness of the results contained in the paper is demonstrated by means of some illustrative examples. Indeed, our formulae are relevant to occupation time phenomena connected with Brownian motion and more general Bessel processes, as well as to models arising in Bayesian nonparametric statistics.

</details>


## 2008-04

<details>

<summary>2008-04-01 08:08:55 - Discussion of: Bayesian views of an archaeological find</summary>

- *Joseph B. Kadane*

- `0804.0102v1` - [abs](http://arxiv.org/abs/0804.0102v1) - [pdf](http://arxiv.org/pdf/0804.0102v1)

> Discussion of ``Statistical analysis of an archeological find'' by Andrey Feuerverger [arXiv:0804.0079]

</details>

<details>

<summary>2008-04-01 12:58:00 - Multisource Bayesian sequential change detection</summary>

- *Savas Dayanik, H. Vincent Poor, Semih O. Sezer*

- `0708.0224v3` - [abs](http://arxiv.org/abs/0708.0224v3) - [pdf](http://arxiv.org/pdf/0708.0224v3)

> Suppose that local characteristics of several independent compound Poisson and Wiener processes change suddenly and simultaneously at some unobservable disorder time. The problem is to detect the disorder time as quickly as possible after it happens and minimize the rate of false alarms at the same time. These problems arise, for example, from managing product quality in manufacturing systems and preventing the spread of infectious diseases. The promptness and accuracy of detection rules improve greatly if multiple independent information sources are available. Earlier work on sequential change detection in continuous time does not provide optimal rules for situations in which several marked count data and continuously changing signals are simultaneously observable. In this paper, optimal Bayesian sequential detection rules are developed for such problems when the marked count data is in the form of independent compound Poisson processes, and the continuously changing signals form a multi-dimensional Wiener process. An auxiliary optimal stopping problem for a jump-diffusion process is solved by transforming it first into a sequence of optimal stopping problems for a pure diffusion by means of a jump operator. This method is new and can be very useful in other applications as well, because it allows the use of the powerful optimal stopping theory for diffusions.

</details>

<details>

<summary>2008-04-03 07:31:13 - The False Dilemma: Bayesian vs. Frequentist</summary>

- *Jordi Vallverdú*

- `0804.0486v1` - [abs](http://arxiv.org/abs/0804.0486v1) - [pdf](http://arxiv.org/pdf/0804.0486v1)

> There are two main opposing schools of statistical reasoning, Frequentist and Bayesian approaches. Until recent days, the frequentist or classical approach has dominated the scientific research, but Bayesianism has reappeared with a strong impulse that is starting to change the situation. Recently the controversy about the primacy of one of the two approaches seems to be unfinished at a philosophical level, but scientific practices are giving an increasingly important position to the Bayesian approach. This paper eludes philosophical debate to focus on the pragmatic point of view of scientists' day-to-day practices, in which Bayesian methodology is very useful. Several facts and operational values are described as the core-set for understanding the change.

</details>

<details>

<summary>2008-04-04 15:28:45 - Bounds for Bayesian order identification with application to mixtures</summary>

- *Antoine Chambaz, Judith Rousseau*

- `0804.0768v1` - [abs](http://arxiv.org/abs/0804.0768v1) - [pdf](http://arxiv.org/pdf/0804.0768v1)

> The efficiency of two Bayesian order estimators is studied. By using nonparametric techniques, we prove new underestimation and overestimation bounds. The results apply to various models, including mixture models. In this case, the errors are shown to be $O(e^{-an})$ and $O((\log n)^b/\sqrt{n})$ ($a,b>0$), respectively.

</details>

<details>

<summary>2008-04-07 08:54:56 - Objective priors for the bivariate normal model</summary>

- *James O. Berger, Dongchu Sun*

- `0804.0987v1` - [abs](http://arxiv.org/abs/0804.0987v1) - [pdf](http://arxiv.org/pdf/0804.0987v1)

> Study of the bivariate normal distribution raises the full range of issues involving objective Bayesian inference, including the different types of objective priors (e.g., Jeffreys, invariant, reference, matching), the different modes of inference (e.g., Bayesian, frequentist, fiducial) and the criteria involved in deciding on optimal objective priors (e.g., ease of computation, frequentist performance, marginalization paradoxes). Summary recommendations as to optimal objective priors are made for a variety of inferences involving the bivariate normal distribution. In the course of the investigation, a variety of surprising results were found, including the availability of objective priors that yield exact frequentist inferences for many functions of the bivariate normal parameters, including the correlation coefficient.

</details>

<details>

<summary>2008-04-07 14:27:49 - A New Estimator for the Number of Species in a Population</summary>

- *L. Cecconi, A. Gandolfi, C. C. A. Sastri*

- `0804.1030v1` - [abs](http://arxiv.org/abs/0804.1030v1) - [pdf](http://arxiv.org/pdf/0804.1030v1)

> We consider the classic problem of estimating T, the total number of species in a population, from repeated counts in a simple random sample. We look first at the Chao-Lee estimator: we initially show that such estimator can be obtained by reconciling two estimators of the unobserved probability, and then develop a sequence of improvements culminating in a Dirichlet prior Bayesian reinterpretation of the estimation problem. By means of this, we obtain simultaneous estimates of T, of the normalized interspecies variance $\gamma^2$ and of the parameter $\lambda$ of the prior. Several simulations show that our estimation method is more flexible than several known methods we used as comparison; the only limitation, apparently shared by all other methods, seems to be that it cannot deal with the rare cases in which $\gamma^2 >1$

</details>

<details>

<summary>2008-04-08 16:58:11 - On the underestimation of model uncertainty by Bayesian K-nearest neighbors</summary>

- *Wanhua Su, Hugh Chipman, Mu Zhu*

- `0804.1325v1` - [abs](http://arxiv.org/abs/0804.1325v1) - [pdf](http://arxiv.org/pdf/0804.1325v1)

> When using the K-nearest neighbors method, one often ignores uncertainty in the choice of K. To account for such uncertainty, Holmes and Adams (2002) proposed a Bayesian framework for K-nearest neighbors (KNN). Their Bayesian KNN (BKNN) approach uses a pseudo-likelihood function, and standard Markov chain Monte Carlo (MCMC) techniques to draw posterior samples. Holmes and Adams (2002) focused on the performance of BKNN in terms of misclassification error but did not assess its ability to quantify uncertainty. We present some evidence to show that BKNN still significantly underestimates model uncertainty.

</details>

<details>

<summary>2008-04-15 15:39:46 - Approximating the marginal likelihood in mixture models</summary>

- *J. -M. Marin, Christian Robert*

- `0804.2414v1` - [abs](http://arxiv.org/abs/0804.2414v1) - [pdf](http://arxiv.org/pdf/0804.2414v1)

> In Chib (1995), a method for approximating marginal densities in a Bayesian setting is proposed, with one proeminent application being the estimation of the number of components in a normal mixture. As pointed out in Neal (1999) and Fruhwirth-Schnatter (2004), the approximation often fails short of providing a proper approximation to the true marginal densities because of the well-known label switching problem (Celeux et al., 2000). While there exist other alternatives to the derivation of approximate marginal densities, we reconsider the original proposal here and show as in Berkhof et al. (2003) and Lee et al. (2008) that it truly approximates the marginal densities once the label switching issue has been solved.

</details>

<details>

<summary>2008-04-15 15:45:03 - Bayesian Inference on Mixtures of Distributions</summary>

- *Kate Lee, Jean-Michel Marin, Kerrie Mengersen, Christian P. Robert*

- `0804.2413v1` - [abs](http://arxiv.org/abs/0804.2413v1) - [pdf](http://arxiv.org/pdf/0804.2413v1)

> This survey covers state-of-the-art Bayesian techniques for the estimation of mixtures. It complements the earlier Marin, Mengersen and Robert (2005) by studying new types of distributions, the multinomial, latent class and t distributions. It also exhibits closed form solutions for Bayesian inference in some discrete setups. Lastly, it sheds a new light on the computation of Bayes factors via the approximation of Chib (1995).

</details>

<details>

<summary>2008-04-16 20:37:59 - Bayesian Shrinkage Variable Selection</summary>

- *Artin Armagan, Russell L. Zaretzki*

- `0711.3657v4` - [abs](http://arxiv.org/abs/0711.3657v4) - [pdf](http://arxiv.org/pdf/0711.3657v4)

> Withdrawn due to extensions and submission as another paper.

</details>

<details>

<summary>2008-04-17 12:01:37 - Boosting Algorithms: Regularization, Prediction and Model Fitting</summary>

- *Peter Bühlmann, Torsten Hothorn*

- `0804.2752v1` - [abs](http://arxiv.org/abs/0804.2752v1) - [pdf](http://arxiv.org/pdf/0804.2752v1)

> We present a statistical perspective on boosting. Special emphasis is given to estimating potentially complex parametric or nonparametric models, including generalized linear and additive models as well as regression models for survival analysis. Concepts of degrees of freedom and corresponding Akaike or Bayesian information criteria, particularly useful for regularization and variable selection in high-dimensional covariate spaces, are discussed as well. The practical aspects of boosting procedures for fitting statistical models are illustrated by means of the dedicated open-source software package mboost. This package implements functions which can be used for model fitting, prediction and variable selection. It is flexible, allowing for the implementation of new boosting algorithms optimizing user-specified loss functions.

</details>

<details>

<summary>2008-04-21 11:07:23 - Bayesian computation for statistical models with intractable normalizing constants</summary>

- *Yves Atchade, Nicolas Lartillot, Christian P. Robert*

- `0804.3152v1` - [abs](http://arxiv.org/abs/0804.3152v1) - [pdf](http://arxiv.org/pdf/0804.3152v1)

> This paper deals with some computational aspects in the Bayesian analysis of statistical models with intractable normalizing constants. In the presence of intractable normalizing constants in the likelihood function, traditional MCMC methods cannot be applied. We propose an approach to sample from such posterior distributions. The method can be thought as a Bayesian version of the MCMC-MLE approach of Geyer and Thompson (1992). To the best of our knowledge, this is the first general and asymptotically consistent Monte Carlo method for such problems. We illustrate the method with examples from image segmentation and social network modeling. We study as well the asymptotic behavior of the algorithm and obtain a strong law of large numbers for empirical averages.

</details>

<details>

<summary>2008-04-24 13:44:28 - Maximum Probability and Relative Entropy Maximization. Bayesian Maximum Probability and Empirical Likelihood</summary>

- *M. Grendar*

- `0804.3926v1` - [abs](http://arxiv.org/abs/0804.3926v1) - [pdf](http://arxiv.org/pdf/0804.3926v1)

> Works, briefly surveyed here, are concerned with two basic methods: Maximum Probability and Bayesian Maximum Probability; as well as with their asymptotic instances: Relative Entropy Maximization and Maximum Non-parametric Likelihood. Parametric and empirical extensions of the latter methods - Empirical Maximum Maximum Entropy and Empirical Likelihood - are also mentioned. The methods are viewed as tools for solving certain ill-posed inverse problems, called Pi-problem, Phi-problem, respectively. Within the two classes of problems, probabilistic justification and interpretation of the respective methods are discussed.

</details>

<details>

<summary>2008-04-29 12:38:16 - Application of Girsanov Theorem to Particle Filtering of Discretely Observed Continuous-Time Non-Linear Systems</summary>

- *Simo Särkkä, Tommi Sottinen*

- `0705.1598v2` - [abs](http://arxiv.org/abs/0705.1598v2) - [pdf](http://arxiv.org/pdf/0705.1598v2)

> This article considers the application of particle filtering to continuous-discrete optimal filtering problems, where the system model is a stochastic differential equation, and noisy measurements of the system are obtained at discrete instances of time. It is shown how the Girsanov theorem can be used for evaluating the likelihood ratios needed in importance sampling. It is also shown how the methodology can be applied to a class of models, where the driving noise process is lower in the dimensionality than the state and thus the laws of state and noise are not absolutely continuous. Rao-Blackwellization of conditionally Gaussian models and unknown static parameter models is also considered.

</details>

<details>

<summary>2008-04-30 10:37:13 - Incorporating a contrast in the Bayesian formula: What consequences for the MAP estimator and the posterior distribution? Applications in spatial statistics</summary>

- *S. Soubeyrand, F. Carpentier, N. Desassis, J. Chadœuf*

- `0804.4780v1` - [abs](http://arxiv.org/abs/0804.4780v1) - [pdf](http://arxiv.org/pdf/0804.4780v1)

> In order to estimate model parameters and circumvent possible difficulties encountered with the likelihood function, we propose to replace the likelihood in the formula of the posterior distribution by a function depending on a contrast. The properties of the contrast-based (CB) posterior distribution and MAP estimator are studied to understand what the consequences of incorporating a contrast in the Bayesian formula are. We show that the proposed method can be used to make frequentist inference and allows the reduction of analytical calculations to get the limit variance matrix of the estimator. For specific contrasts, the CB--posterior distribution directly approximates the limit distribution of the estimator; the calculation of the limit variance matrix is then avoided. Moreover, for these contrasts, the CB--posterior distribution can also be used to make inference in the Bayesian way. The method is applied to three spatial data sets.

</details>


## 2008-05

<details>

<summary>2008-05-06 15:28:39 - On the Empirical Importance of the Conditional Skewness Assumption in Modelling the Relationship Between Risk and Return</summary>

- *Mateusz Pipien*

- `0712.4161v3` - [abs](http://arxiv.org/abs/0712.4161v3) - [pdf](http://arxiv.org/pdf/0712.4161v3)

> The main goal of this paper is an application of Bayesian inference in testing the relation between risk and return on the financial instruments. On the basis of the Intertemporal CAPM model we built a general sampling model suitable in analysing such a relationship. The most important feature of our assumptions is that the skewness of the conditional distribution of returns is used as an alternative source of relation between risk and return. This general specification relates to GARCH-In-Mean model. In order to make conditional distribution of financial returns skewed we considered a constructive approach based on the inverse probability integral transformation. In particular, we apply the hidden truncation mechanism, two equivalent approaches of the inverse scale factors, order statistics concept, Beta and Bernstein distribution transformations, and also the constructive method. Based on the daily excess returns on the Warsaw Stock Exchange Index we checked the empirical importance of the conditional skewness assumption on the relation between risk and return on the Warsaw Stock Market. We present posterior probabilities of all competing specifications as well as the posterior analysis of positive sign of the tested relationship.

</details>

<details>

<summary>2008-05-08 08:32:48 - Kullback Leibler property of kernel mixture priors in Bayesian density estimation</summary>

- *Yuefeng Wu, Subhashis Ghosal*

- `0710.2746v2` - [abs](http://arxiv.org/abs/0710.2746v2) - [pdf](http://arxiv.org/pdf/0710.2746v2)

> Positivity of the prior probability of Kullback-Leibler neighborhood around the true density, commonly known as the Kullback-Leibler property, plays a fundamental role in posterior consistency. A popular prior for Bayesian estimation is given by a Dirichlet mixture, where the kernels are chosen depending on the sample space and the class of densities to be estimated. The Kullback-Leibler property of the Dirichlet mixture prior has been shown for some special kernels like the normal density or Bernstein polynomial, under appropriate conditions. In this paper, we obtain easily verifiable sufficient conditions, under which a prior obtained by mixing a general kernel possesses the Kullback-Leibler property. We study a wide variety of kernel used in practice, including the normal, $t$, histogram, gamma, Weibull densities and so on, and show that the Kullback-Leibler property holds if some easily verifiable conditions are satisfied at the true density. This gives a catalog of conditions required for the Kullback-Leibler property, which can be readily used in applications.

</details>

<details>

<summary>2008-05-15 11:12:55 - A Bayesian test for excess zeros in a zero-inflated power series distribution</summary>

- *Archan Bhattacharya, Bertrand S. Clarke, Gauri S. Datta*

- `0805.2258v1` - [abs](http://arxiv.org/abs/0805.2258v1) - [pdf](http://arxiv.org/pdf/0805.2258v1)

> Power series distributions form a useful subclass of one-parameter discrete exponential families suitable for modeling count data. A zero-inflated power series distribution is a mixture of a power series distribution and a degenerate distribution at zero, with a mixing probability $p$ for the degenerate distribution. This distribution is useful for modeling count data that may have extra zeros. One question is whether the mixture model can be reduced to the power series portion, corresponding to $p=0$, or whether there are so many zeros in the data that zero inflation relative to the pure power series distribution must be included in the model i.e., $p\geq0$. The problem is difficult partially because $p=0$ is a boundary point. Here, we present a Bayesian test for this problem based on recognizing that the parameter space can be expanded to allow $p$ to be negative. Negative values of $p$ are inconsistent with the interpretation of $p$ as a mixing probability, however, they index distributions that are physically and probabilistically meaningful. We compare our Bayesian solution to two standard frequentist testing procedures and find that using a posterior probability as a test statistic has slightly higher power on the most important ranges of the sample size $n$ and parameter values than the score test and likelihood ratio test in simulations. Our method also performs well on three real data sets.

</details>

<details>

<summary>2008-05-15 11:32:52 - Posterior consistency of Dirichlet mixtures of beta densities in estimating positive false discovery rates</summary>

- *Subhashis Ghosal, Anindya Roy, Yongqiang Tang*

- `0805.2264v1` - [abs](http://arxiv.org/abs/0805.2264v1) - [pdf](http://arxiv.org/pdf/0805.2264v1)

> In recent years, multiple hypothesis testing has come to the forefront of statistical research, ostensibly in relation to applications in genomics and some other emerging fields. The false discovery rate (FDR) and its variants provide very important notions of errors in this context comparable to the role of error probabilities in classical testing problems. Accurate estimation of positive FDR (pFDR), a variant of the FDR, is essential in assessing and controlling this measure. In a recent paper, the authors proposed a model-based nonparametric Bayesian method of estimation of the pFDR function. In particular, the density of p-values was modeled as a mixture of decreasing beta densities and an appropriate Dirichlet process was considered as a prior on the mixing measure. The resulting procedure was shown to work well in simulations. In this paper, we provide some theoretical results in support of the beta mixture model for the density of p-values, and show that, under appropriate conditions, the resulting posterior is consistent as the number of hypotheses grows to infinity.

</details>

<details>

<summary>2008-05-16 09:06:03 - A comparison of the Benjamini-Hochberg procedure with some Bayesian rules for multiple testing</summary>

- *Małgorzata Bogdan, Jayanta K. Ghosh, Surya T. Tokdar*

- `0805.2479v1` - [abs](http://arxiv.org/abs/0805.2479v1) - [pdf](http://arxiv.org/pdf/0805.2479v1)

> In the spirit of modeling inference for microarrays as multiple testing for sparse mixtures, we present a similar approach to a simplified version of quantitative trait loci (QTL) mapping. Unlike in case of microarrays, where the number of tests usually reaches tens of thousands, the number of tests performed in scans for QTL usually does not exceed several hundreds. However, in typical cases, the sparsity $p$ of significant alternatives for QTL mapping is in the same range as for microarrays. For methodological interest, as well as some related applications, we also consider non-sparse mixtures. Using simulations as well as theoretical observations we study false discovery rate (FDR), power and misclassification probability for the Benjamini-Hochberg (BH) procedure and its modifications, as well as for various parametric and nonparametric Bayes and Parametric Empirical Bayes procedures. Our results confirm the observation of Genovese and Wasserman (2002) that for small p the misclassification error of BH is close to optimal in the sense of attaining the Bayes oracle. This property is shared by some of the considered Bayes testing rules, which in general perform better than BH for large or moderate $p$'s.

</details>

<details>

<summary>2008-05-20 12:36:52 - Objective Bayesian analysis under sequential experimentation</summary>

- *Dongchu Sun, James O. Berger*

- `0805.3064v1` - [abs](http://arxiv.org/abs/0805.3064v1) - [pdf](http://arxiv.org/pdf/0805.3064v1)

> Objective priors for sequential experiments are considered. Common priors, such as the Jeffreys prior and the reference prior, will typically depend on the stopping rule used for the sequential experiment. New expressions for reference priors are obtained in various contexts, and computational issues involving such priors are considered.

</details>

<details>

<summary>2008-05-20 12:40:44 - J. K. Ghosh's contribution to statistics: A brief outline</summary>

- *Bertrand Clarke, Subhashis Ghosal*

- `0805.3066v1` - [abs](http://arxiv.org/abs/0805.3066v1) - [pdf](http://arxiv.org/pdf/0805.3066v1)

> Professor Jayanta Kumar Ghosh has contributed massively to various areas of Statistics over the last five decades. Here, we survey some of his most important contributions. In roughly chronological order, we discuss his major results in the areas of sequential analysis, foundations, asymptotics, and Bayesian inference. It is seen that he progressed from thinking about data points, to thinking about data summarization, to the limiting cases of data summarization in as they relate to parameter estimation, and then to more general aspects of modeling including prior and model selection.

</details>

<details>

<summary>2008-05-20 13:12:22 - On predictive probability matching priors</summary>

- *Trevor J. Sweeting*

- `0805.3073v1` - [abs](http://arxiv.org/abs/0805.3073v1) - [pdf](http://arxiv.org/pdf/0805.3073v1)

> We revisit the question of priors that achieve approximate matching of Bayesian and frequentist predictive probabilities. Such priors may be thought of as providing frequentist calibration of Bayesian prediction or simply as devices for producing frequentist prediction regions. Here we analyse the $O(n^{-1})$ term in the expansion of the coverage probability of a Bayesian prediction region, as derived in [Ann. Statist. 28 (2000) 1414--1426]. Unlike the situation for parametric matching, asymptotic predictive matching priors may depend on the level $\alpha$. We investigate uniformly predictive matching priors (UPMPs); that is, priors for which this $O(n^{-1})$ term is zero for all $\alpha$. It was shown in [Ann. Statist. 28 (2000) 1414--1426] that, in the case of quantile matching and a scalar parameter, if such a prior exists then it must be Jeffreys' prior. In the present article we investigate UPMPs in the multiparameter case and present some general results about the form, and uniqueness or otherwise, of UPMPs for both quantile and highest predictive density matching.

</details>

<details>

<summary>2008-05-21 06:12:13 - Data-dependent probability matching priors for empirical and related likelihoods</summary>

- *Rahul Mukerjee*

- `0805.3203v1` - [abs](http://arxiv.org/abs/0805.3203v1) - [pdf](http://arxiv.org/pdf/0805.3203v1)

> We consider a general class of empirical-type likelihoods and develop higher order asymptotics with a view to characterizing members thereof that allow the existence of possibly data-dependent probability matching priors ensuring approximate frequentist validity of posterior quantiles. In particular, for the usual empirical likelihood, positive results are obtained. This is in contrast with what happens if only data-free priors are entertained.

</details>

<details>

<summary>2008-05-21 06:34:24 - Probability matching priors for some parameters of the bivariate normal distribution</summary>

- *Malay Ghosh, Upasana Santra, Dalho Kim*

- `0805.3204v1` - [abs](http://arxiv.org/abs/0805.3204v1) - [pdf](http://arxiv.org/pdf/0805.3204v1)

> This paper develops some objective priors for certain parameters of the bivariate normal distribution. The parameters considered are the regression coefficient, the generalized variance, and the ratio of the conditional variance of one variable given the other to the marginal variance of the other variable. The criterion used is the asymptotic matching of coverage probabilities of Bayesian credible intervals with the corresponding frequentist coverage probabilities. The paper uses various matching criteria, namely, quantile matching, matching of distribution functions, highest posterior density matching, and matching via inversion of test statistics. One particular prior is found which meets all the matching criteria individually for all the parameters of interest.

</details>

<details>

<summary>2008-05-21 07:05:33 - Fuzzy set representation of a prior distribution</summary>

- *Glen Meeden*

- `0805.3205v1` - [abs](http://arxiv.org/abs/0805.3205v1) - [pdf](http://arxiv.org/pdf/0805.3205v1)

> In the subjective Bayesian approach uncertainty is described by a prior distribution chosen by the statistician. Fuzzy set theory is another way of representing uncertainty. Here we give a decision theoretic approach which allows a Bayesian to convert their prior distribution into a fuzzy set membership function. This yields a formal relationship between these two different methods of expressing uncertainty.

</details>

<details>

<summary>2008-05-21 07:30:08 - Fuzzy sets in nonparametric Bayes regression</summary>

- *Jean-François Angers, Mohan Delampady*

- `0805.3209v1` - [abs](http://arxiv.org/abs/0805.3209v1) - [pdf](http://arxiv.org/pdf/0805.3209v1)

> A simple Bayesian approach to nonparametric regression is described using fuzzy sets and membership functions. Membership functions are interpreted as likelihood functions for the unknown regression function, so that with the help of a reference prior they can be transformed to prior density functions. The unknown regression function is decomposed into wavelets and a hierarchical Bayesian approach is employed for making inferences on the resulting wavelet coefficients.

</details>

<details>

<summary>2008-05-21 08:08:28 - Objective Bayes testing of Poisson versus inflated Poisson models</summary>

- *M. J. Bayarri, James O. Berger, Gauri S. Datta*

- `0805.3220v1` - [abs](http://arxiv.org/abs/0805.3220v1) - [pdf](http://arxiv.org/pdf/0805.3220v1)

> The Poisson distribution is often used as a standard model for count data. Quite often, however, such data sets are not well fit by a Poisson model because they have more zeros than are compatible with this model. For these situations, a zero-inflated Poisson (ZIP) distribution is often proposed. This article addresses testing a Poisson versus a ZIP model, using Bayesian methodology based on suitable objective priors. Specific choices of objective priors are justified and their properties investigated. The methodology is extended to include covariates in regression models. Several applications are given.

</details>

<details>

<summary>2008-05-21 10:12:05 - Risk and resampling under model uncertainty</summary>

- *Snigdhansu Chatterjee, Nitai D. Mukhopadhyay*

- `0805.3244v1` - [abs](http://arxiv.org/abs/0805.3244v1) - [pdf](http://arxiv.org/pdf/0805.3244v1)

> In statistical exercises where there are several candidate models, the traditional approach is to select one model using some data driven criterion and use that model for estimation, testing and other purposes, ignoring the variability of the model selection process. We discuss some problems associated with this approach. An alternative scheme is to use a model-averaged estimator, that is, a weighted average of estimators obtained under different models, as an estimator of a parameter. We show that the risk associated with a Bayesian model-averaged estimator is bounded as a function of the sample size, when parameter values are fixed. We establish conditions which ensure that a model-averaged estimator's distribution can be consistently approximated using the bootstrap. A new, data-adaptive, model averaging scheme is proposed that balances efficiency of estimation without compromising applicability of the bootstrap. This paper illustrates that certain desirable risk and resampling properties of model-averaged estimators are obtainable when parameters are fixed but unknown; this complements several studies on minimaxity and other properties of post-model-selected and model-averaged estimators, where parameters are allowed to vary.

</details>

<details>

<summary>2008-05-21 10:23:55 - Remarks on consistency of posterior distributions</summary>

- *Taeryon Choi, R. V. Ramamoorthi*

- `0805.3248v1` - [abs](http://arxiv.org/abs/0805.3248v1) - [pdf](http://arxiv.org/pdf/0805.3248v1)

> In recent years, the literature in the area of Bayesian asymptotics has been rapidly growing. It is increasingly important to understand the concept of posterior consistency and validate specific Bayesian methods, in terms of consistency of posterior distributions. In this paper, we build up some conceptual issues in consistency of posterior distributions, and discuss panoramic views of them by comparing various approaches to posterior consistency that have been investigated in the literature. In addition, we provide interesting results on posterior consistency that deal with non-exponential consistency, improper priors and non i.i.d. (independent but not identically distributed) observations. We describe a few examples for illustrative purposes.

</details>

<details>

<summary>2008-05-21 10:57:33 - Reproducing kernel Hilbert spaces of Gaussian priors</summary>

- *A. W. van der Vaart, J. H. van Zanten*

- `0805.3252v1` - [abs](http://arxiv.org/abs/0805.3252v1) - [pdf](http://arxiv.org/pdf/0805.3252v1)

> We review definitions and properties of reproducing kernel Hilbert spaces attached to Gaussian variables and processes, with a view to applications in nonparametric Bayesian statistics using Gaussian priors. The rate of contraction of posterior distributions based on Gaussian priors can be described through a concentration function that is expressed in the reproducing Hilbert space. Absolute continuity of Gaussian measures and concentration inequalities play an important role in understanding and deriving this result. Series expansions of Gaussian variables and transformations of their reproducing kernel Hilbert spaces under linear maps are useful tools to compute the concentration function.

</details>

<details>

<summary>2008-05-21 12:05:22 - A Bayesian semi-parametric model for small area estimation</summary>

- *Donald Malec, Peter Müller*

- `0805.3264v1` - [abs](http://arxiv.org/abs/0805.3264v1) - [pdf](http://arxiv.org/pdf/0805.3264v1)

> In public health management there is a need to produce subnational estimates of health outcomes. Often, however, funds are not available to collect samples large enough to produce traditional survey sample estimates for each subnational area. Although parametric hierarchical methods have been successfully used to derive estimates from small samples, there is a concern that the geographic diversity of the U.S. population may be oversimplified in these models. In this paper, a semi-parametric model is used to describe the geographic variability component of the model. Specifically, we assume Dirichlet process mixtures of normals for county-specific random effects. Results are compared to a parametric model based on the base measure of the Dirichlet process, using binary health outcomes related to mammogram usage.

</details>

<details>

<summary>2008-05-21 12:10:38 - Large sample asymptotics for the two-parameter Poisson--Dirichlet process</summary>

- *Lancelot F. James*

- `0708.4294v2` - [abs](http://arxiv.org/abs/0708.4294v2) - [pdf](http://arxiv.org/pdf/0708.4294v2)

> This paper explores large sample properties of the two-parameter $(\alpha,\theta)$ Poisson--Dirichlet Process in two contexts. In a Bayesian context of estimating an unknown probability measure, viewing this process as a natural extension of the Dirichlet process, we explore the consistency and weak convergence of the the two-parameter Poisson--Dirichlet posterior process. We also establish the weak convergence of properly centered two-parameter Poisson--Dirichlet processes for large $\theta+n\alpha.$ This latter result complements large $\theta$ results for the Dirichlet process and Poisson--Dirichlet sequences, and complements a recent result on large deviation principles for the two-parameter Poisson--Dirichlet process. A crucial component of our results is the use of distributional identities that may be useful in other contexts.

</details>

<details>

<summary>2008-05-21 13:00:00 - A hierarchical Bayesian approach for estimating the origin of a mixed population</summary>

- *Feng Guo, Dipak K. Dey, Kent E. Holsinger*

- `0805.3269v1` - [abs](http://arxiv.org/abs/0805.3269v1) - [pdf](http://arxiv.org/pdf/0805.3269v1)

> We propose a hierarchical Bayesian model to estimate the proportional contribution of source populations to a newly founded colony. Samples are derived from the first generation offspring in the colony, but mating may occur preferentially among migrants from the same source population. Genotypes of the newly founded colony and source populations are used to estimate the mixture proportions, and the mixture proportions are related to environmental and demographic factors that might affect the colonizing process. We estimate an assortative mating coefficient, mixture proportions, and regression relationships between environmental factors and the mixture proportions in a single hierarchical model. The first-stage likelihood for genotypes in the newly founded colony is a mixture multinomial distribution reflecting the colonizing process. The environmental and demographic data are incorporated into the model through a hierarchical prior structure. A simulation study is conducted to investigate the performance of the model by using different levels of population divergence and number of genetic markers included in the analysis. We use Markov chain Monte Carlo (MCMC) simulation to conduct inference for the posterior distributions of model parameters. We apply the model to a data set derived from grey seals in the Orkney Islands, Scotland. We compare our model with a similar model previously used to analyze these data. The results from both the simulation and application to real data indicate that our model provides better estimates for the covariate effects.

</details>

<details>

<summary>2008-05-21 13:32:46 - Orthogonalized smoothing for rescaled spike and slab models</summary>

- *Hemant Ishwaran, Ariadni Papana*

- `0805.3279v1` - [abs](http://arxiv.org/abs/0805.3279v1) - [pdf](http://arxiv.org/pdf/0805.3279v1)

> Rescaled spike and slab models are a new Bayesian variable selection method for linear regression models. In high dimensional orthogonal settings such models have been shown to possess optimal model selection properties. We review background theory and discuss applications of rescaled spike and slab models to prediction problems involving orthogonal polynomials. We first consider global smoothing and discuss potential weaknesses. Some of these deficiencies are remedied by using local regression. The local regression approach relies on an intimate connection between local weighted regression and weighted generalized ridge regression. An important implication is that one can trace the effective degrees of freedom of a curve as a way to visualize and classify curvature. Several motivating examples are presented.

</details>

<details>

<summary>2008-05-25 15:00:22 - Missing observation analysis for matrix-variate time series data</summary>

- *K. Triantafyllopoulos*

- `0805.3831v1` - [abs](http://arxiv.org/abs/0805.3831v1) - [pdf](http://arxiv.org/pdf/0805.3831v1)

> Bayesian inference is developed for matrix-variate dynamic linear models (MV-DLMs), in order to allow missing observation analysis, of any sub-vector or sub-matrix of the observation time series matrix. We propose modifications of the inverted Wishart and matrix $t$ distributions, replacing the scalar degrees of freedom by a diagonal matrix of degrees of freedom. The MV-DLM is then re-defined and modifications of the updating algorithm for missing observations are suggested.

</details>

<details>

<summary>2008-05-26 06:30:54 - Getting Your Eye In: A Bayesian Analysis of Early Dismissals in Cricket</summary>

- *Brendon J. Brewer*

- `0801.4408v2` - [abs](http://arxiv.org/abs/0801.4408v2) - [pdf](http://arxiv.org/pdf/0801.4408v2)

> A Bayesian Survival Analysis method is motivated and developed for analysing sequences of scores made by a batsman in test or first class cricket. In particular, we expect the presence of an effect whereby the distribution of scores has more probability near zero than a geometric distribution, due to the fact that batting is more difficult when the batsman is new at the crease. A Metropolis-Hastings algorithm is found to be efficient at estimating the proposed parameters, allowing us to quantify exactly how large this early-innings effect is, and how long a batsman needs to be at the crease in order to ``get their eye in''. Applying this model to several modern players shows that a batsman is typically only playing at about half of their potential ability when they first arrive at the crease, and gets their eye in surprisingly quickly. Additionally, some players are more ``robust'' (have a smaller early-innings effect) than others, which may have implications for selection policy.

</details>

<details>

<summary>2008-05-28 12:51:49 - Quantization of Prior Probabilities for Hypothesis Testing</summary>

- *Kush R. Varshney, Lav R. Varshney*

- `0805.4338v1` - [abs](http://arxiv.org/abs/0805.4338v1) - [pdf](http://arxiv.org/pdf/0805.4338v1)

> Bayesian hypothesis testing is investigated when the prior probabilities of the hypotheses, taken as a random vector, are quantized. Nearest neighbor and centroid conditions are derived using mean Bayes risk error as a distortion measure for quantization. A high-resolution approximation to the distortion-rate function is also obtained. Human decision making in segregated populations is studied assuming Bayesian hypothesis testing with quantized priors.

</details>

<details>

<summary>2008-05-28 15:20:18 - Adaptive Ridge Selector (ARiS)</summary>

- *Artin Armagan, Russell Zaretzki*

- `0803.2173v2` - [abs](http://arxiv.org/abs/0803.2173v2) - [pdf](http://arxiv.org/pdf/0803.2173v2)

> We introduce a new shrinkage variable selection operator for linear models which we term the \emph{adaptive ridge selector} (ARiS). This approach is inspired by the \emph{relevance vector machine} (RVM), which uses a Bayesian hierarchical linear setup to do variable selection and model estimation. Extending the RVM algorithm, we include a proper prior distribution for the precisions of the regression coefficients, $v_{j}^{-1} \sim f(v_{j}^{-1}|\eta)$, where $\eta$ is a scalar hyperparameter. A novel fitting approach which utilizes the full set of posterior conditional distributions is applied to maximize the joint posterior distribution $p(\boldsymbol\beta,\sigma^{2},\mathbf{v}^{-1}|\mathbf{y},\eta)$ given the value of the hyper-parameter $\eta$. An empirical Bayes method is proposed for choosing $\eta$. This approach is contrasted with other regularized least squares estimators including the lasso, its variants, nonnegative garrote and ordinary ridge regression. Performance differences are explored for various simulated data examples. Results indicate superior prediction and model selection accuracy under sparse setups and drastic improvement in accuracy of model choice with increasing sample size.

</details>


## 2008-06

<details>

<summary>2008-06-05 07:04:06 - On some difficulties with a posterior probability approximation technique</summary>

- *Christian Robert, Jean-Michel Marin*

- `0801.3513v5` - [abs](http://arxiv.org/abs/0801.3513v5) - [pdf](http://arxiv.org/pdf/0801.3513v5)

> In Scott (2002) and Congdon (2006), a new method is advanced to compute posterior probabilities of models under consideration. It is based solely on MCMC outputs restricted to single models, i.e., it is bypassing reversible jump and other model exploration techniques. While it is indeed possible to approximate posterior probabilities based solely on MCMC outputs from single models, as demonstrated by Gelfand and Dey (1994) and Bartolucci et al. (2006), we show that the proposals of Scott (2002) and Congdon (2006) are biased and advance several arguments towards this thesis, the primary one being the confusion between model-based posteriors and joint pseudo-posteriors. From a practical point of view, the bias in Scott's (2002) approximation appears to be much more severe than the one in Congdon's (2006), the later being often of the same magnitude as the posterior probability it approximates, although we also exhibit an example where the divergence from the true posterior probability is extreme.

</details>

<details>

<summary>2008-06-12 11:44:16 - A quantile-copula approach to conditional density estimation</summary>

- *Olivier P. Faugeras*

- `0709.3192v3` - [abs](http://arxiv.org/abs/0709.3192v3) - [pdf](http://arxiv.org/pdf/0709.3192v3)

> We present a new non-parametric estimator of the conditional density of the kernel type. It is based on an efficient transformation of the data by quantile transform. By use of the copula representation, it turns out to have a remarkable product form. We study its asymptotic properties and compare its bias and variance to competitors based on nonparametric regression.

</details>

<details>

<summary>2008-06-15 07:38:03 - Developing Bayesian Information Entropy-based Techniques for Spatially Explicit Model Assessment</summary>

- *Kostas Alexandridis, Bryan C. Pijanowski*

- `0806.2424v1` - [abs](http://arxiv.org/abs/0806.2424v1) - [pdf](http://arxiv.org/pdf/0806.2424v1)

> The aim of this paper is to explore and develop advanced spatial Bayesian assessment methods and techniques for land use modeling. The paper provides a comprehensive guide for assessing additional informational entropy value of model predictions at the spatially explicit domain of knowledge, and proposes a few alternative metrics and indicators for extracting higher-order information dynamics from simulation tournaments. A seven-county study area in South-Eastern Wisconsin (SEWI) has been used to simulate and assess the accuracy of historical land use changes (1963-1990) using artificial neural network simulations of the Land Transformation Model (LTM). The use of the analysis and the performance of the metrics helps: (a) understand and learn how well the model runs fits to different combinations of presence and absence of transitions in a landscape, not simply how well the model fits our given data; (b) derive (estimate) a theoretical accuracy that we would expect a model to assess under the presence of incomplete information and measurement; (c) understand the spatially explicit role and patterns of uncertainty in simulations and model estimations, by comparing results across simulation runs; (d) compare the significance or estimation contribution of transitional presence and absence (change versus no change) to model performance, and the contribution of the spatial drivers and variables to the explanatory value of our model; and (e) compare measurements of informational uncertainty at different scales of spatial resolution.

</details>

<details>

<summary>2008-06-15 16:38:17 - Chains of distributions, hierarchical Bayesian models and Benford's Law</summary>

- *Dennis Jang, Jung Uk Kang, Alex Kruckman, Jun Kudo, Steven J. Miller*

- `0805.4226v2` - [abs](http://arxiv.org/abs/0805.4226v2) - [pdf](http://arxiv.org/pdf/0805.4226v2)

> Kossovsky recently conjectured that the distribution of leading digits of a chain of probability distributions converges to Benford's law as the length of the chain grows. We prove his conjecture in many cases, and provide an interpretation in terms of products of independent random variables and a central limit theorem. An interesting consequence is that in hierarchical Bayesian models priors tend to satisfy Benford's Law as the number of levels of the hierarchy increases, which allows us to develop some simple tests (based on Benford's law) to test proposed models. We give explicit formulas for the error terms as sums of Mellin transforms, which converges extremely rapidly as the number of terms in the chain grows. We may interpret our results as showing that certain Markov chain Monte Carlo processes are rapidly mixing to Benford's law.

</details>

<details>

<summary>2008-06-17 18:23:15 - Decoding Beta-Decay Systematics: A Global Statistical Model for Beta^- Halflives</summary>

- *N. J. Costiris, E. Mavrommatis, K. A. Gernoth, J. W. Clark*

- `0806.2850v1` - [abs](http://arxiv.org/abs/0806.2850v1) - [pdf](http://arxiv.org/pdf/0806.2850v1)

> Statistical modeling of nuclear data provides a novel approach to nuclear systematics complementary to established theoretical and phenomenological approaches based on quantum theory. Continuing previous studies in which global statistical modeling is pursued within the general framework of machine learning theory, we implement advances in training algorithms designed to improved generalization, in application to the problem of reproducing and predicting the halflives of nuclear ground states that decay 100% by the beta^- mode. More specifically, fully-connected, multilayer feedforward artificial neural network models are developed using the Levenberg-Marquardt optimization algorithm together with Bayesian regularization and cross-validation. The predictive performance of models emerging from extensive computer experiments is compared with that of traditional microscopic and phenomenological models as well as with the performance of other learning systems, including earlier neural network models as well as the support vector machines recently applied to the same problem. In discussing the results, emphasis is placed on predictions for nuclei that are far from the stability line, and especially those involved in the r-process nucleosynthesis. It is found that the new statistical models can match or even surpass the predictive performance of conventional models for beta-decay systematics and accordingly should provide a valuable additional tool for exploring the expanding nuclear landscape.

</details>

<details>

<summary>2008-06-18 06:39:02 - Composite quantile regression and the oracle Model Selection Theory</summary>

- *Hui Zou, Ming Yuan*

- `0806.2905v1` - [abs](http://arxiv.org/abs/0806.2905v1) - [pdf](http://arxiv.org/pdf/0806.2905v1)

> Coefficient estimation and variable selection in multiple linear regression is routinely done in the (penalized) least squares (LS) framework. The concept of model selection oracle introduced by Fan and Li [J. Amer. Statist. Assoc. 96 (2001) 1348--1360] characterizes the optimal behavior of a model selection procedure. However, the least-squares oracle theory breaks down if the error variance is infinite. In the current paper we propose a new regression method called composite quantile regression (CQR). We show that the oracle model selection theory using the CQR oracle works beautifully even when the error variance is infinite. We develop a new oracular procedure to achieve the optimal properties of the CQR oracle. When the error variance is finite, CQR still enjoys great advantages in terms of estimation efficiency. We show that the relative efficiency of CQR compared to the least squares is greater than 70% regardless the error distribution. Moreover, CQR could be much more efficient and sometimes arbitrarily more efficient than the least squares. The same conclusions hold when comparing a CQR-oracular estimator with a LS-oracular estimator.

</details>

<details>

<summary>2008-06-23 20:01:42 - A Bayesian Approach to Network Modularity</summary>

- *Jake M. Hofman, Chris H. Wiggins*

- `0709.3512v3` - [abs](http://arxiv.org/abs/0709.3512v3) - [pdf](http://arxiv.org/pdf/0709.3512v3)

> We present an efficient, principled, and interpretable technique for inferring module assignments and for identifying the optimal number of modules in a given network. We show how several existing methods for finding modules can be described as variant, special, or limiting cases of our work, and how the method overcomes the resolution limit problem, accurately recovering the true number of modules. Our approach is based on Bayesian methods for model selection which have been used with success for almost a century, implemented using a variational technique developed only in the past decade. We apply the technique to synthetic and real networks and outline how the method naturally allows selection among competing models.

</details>

<details>

<summary>2008-06-26 05:59:26 - Online data processing: comparison of Bayesian regularized particle filters</summary>

- *Roberto Casarin, Jean-Michel Marin*

- `0806.4242v1` - [abs](http://arxiv.org/abs/0806.4242v1) - [pdf](http://arxiv.org/pdf/0806.4242v1)

> The aim of this paper is to compare three regularized particle filters in an online data processing context. We carry out the comparison in terms of hidden states filtering and parameters estimation, considering a Bayesian paradigm and a univariate stochastic volatility model. We discuss the use of an improper prior distribution in the initialization of the filtering procedure and show that the regularized Auxiliary Particle Filter (APF) outperforms the regularized Sequential Importance Sampling (SIS) and the regularized Sampling Importance Resampling (SIR).

</details>

<details>

<summary>2008-06-26 14:32:49 - Beyond Parametrics in Interdisciplinary Research: Festschrift in Honor of Professor Pranab K. Sen</summary>

- *N. Balakrishnan, Edsel A. Peña, Mervyn J. Silvapulle*

- `0806.4294v1` - [abs](http://arxiv.org/abs/0806.4294v1) - [pdf](http://arxiv.org/pdf/0806.4294v1)

> Pranab K. Sen has contributed extensively to many areas of Statistics including order statistics, nonparametrics, robust inference, sequential methods, asymptotics, biostatistics, clinical trials, bioenvironmental studies and bioinformatics. His long list of over 600 publications and 22 books and volumes along with numerous citations during the past 5 decades bear testimony to his work. All three of us have had the good fortune of being associated with him in different capacities. He has given professional and personal advice on many occasions to all of us, and we feel that our lives have certainly been enriched by our association with him. He has been over the years a friend, philosopher and a guide to us, and still continues to be one! While parametric statistical inference remains ever so popular, semi-parametric, Bayesian and nonparametric inferential methods have attracted great attention from numerous applied scientists because of their weaker assumptions, which make them naturally robust and so more appropriate in real-life applications. This clearly signals for ``beyond parametrics'' approaches which include nonparametrics, semi-parametrics, Bayes methods and many others. Motivated by this feature, and his drive in the ``beyond parametrics'' area, we thought that it will be only appropriate for a volume in honor of Pranab Kumar Sen to focus on this aspect of statistical inference and its applications. With this in mind, we have put together this volume in order to (i) review some of the recent developments in this direction, (ii) focus on some new methodologies and highlight their applications, and (iii) suggest some interesting open problems and possible new directions for further research.

</details>

<details>

<summary>2008-06-27 08:27:03 - Pushing the Limits of Contemporary Statistics: Contributions in Honor of Jayanta K. Ghosh</summary>

- *Bertrand Clarke, Subhashis Ghosal*

- `0806.4445v1` - [abs](http://arxiv.org/abs/0806.4445v1) - [pdf](http://arxiv.org/pdf/0806.4445v1)

> Jayanta Kumar Ghosh is one of the most extraordinary professors in the field of Statistics. His research in numerous areas, especially asymptotics, has been groundbreaking, influential throughout the world, and widely recognized through awards and other honors. His leadership in Statistics as Director of the Indian Statistical Institute and President of the International Statistical Institute, among other eminent positions, has been likewise outstanding. In recognition of Jayanta's enormous impact, this volume is an effort to honor him by drawing together contributions to the main areas in which he has worked and continues to work. The papers naturally fall into five categories. First, sequential estimation was Jayanta's starting point. Thus, beginning with that topic, there are two papers, one classical by Hall and Ding leading to a variant on p-values, and one Bayesian by Berger and Sun extending reference priors to stopping time problems. Second, there are five papers in the general area of prior specification. Much of Jayanta's earlier work involved group families as does Sweeting's paper here for instance. There are also two papers dwelling on the link between fuzzy sets and priors, by Meeden and by Delampady and Angers. Equally daring is the work by Mukerjee with data dependent priors and the pleasing confluence of several prior selection criteria found by Ghosh, Santra and Kim. Jayanta himself studied a variety of prior selection criteria including probability matching priors and reference priors.

</details>


## 2008-07

<details>

<summary>2008-07-01 01:33:17 - Frequentist and Bayesian measures of confidence via multiscale bootstrap for testing three regions</summary>

- *Hidetoshi Shimodaira*

- `0807.0053v1` - [abs](http://arxiv.org/abs/0807.0053v1) - [pdf](http://arxiv.org/pdf/0807.0053v1)

> A new computation method of frequentist $p$-values and Bayesian posterior probabilities based on the bootstrap probability is discussed for the multivariate normal model with unknown expectation parameter vector. The null hypothesis is represented as an arbitrary-shaped region. We introduce new parametric models for the scaling-law of bootstrap probability so that the multiscale bootstrap method, which was designed for one-sided test, can also computes confidence measures of two-sided test, extending applicability to a wider class of hypotheses. Parameter estimation is improved by the two-step multiscale bootstrap and also by including higher-order terms. Model selection is important not only as a motivating application of our method, but also as an essential ingredient in the method. A compromise between frequentist and Bayesian is attempted by showing that the Bayesian posterior probability with an noninformative prior is interpreted as a frequentist $p$-value of ``zero-sided'' test.

</details>

<details>

<summary>2008-07-07 12:39:53 - Bayesian Analysis of Marginal Log-Linear Graphical Models for Three Way Contingency Tables</summary>

- *Ioannis Ntzoufras, Claudia Tarantola*

- `0807.1001v1` - [abs](http://arxiv.org/abs/0807.1001v1) - [pdf](http://arxiv.org/pdf/0807.1001v1)

> This paper deals with the Bayesian analysis of graphical models of marginal independence for three way contingency tables. We use a marginal log-linear parametrization, under which the model is defined through suitable zero-constraints on the interaction parameters calculated within marginal distributions. We undertake a comprehensive Bayesian analysis of these models, involving suitable choices of prior distributions, estimation, model determination, as well as the allied computational issues. The methodology is illustrated with reference to two real data sets.

</details>

<details>

<summary>2008-07-07 12:57:23 - Catching Up Faster by Switching Sooner: A Prequential Solution to the AIC-BIC Dilemma</summary>

- *Tim van Erven, Peter Grunwald, Steven de Rooij*

- `0807.1005v1` - [abs](http://arxiv.org/abs/0807.1005v1) - [pdf](http://arxiv.org/pdf/0807.1005v1)

> Bayesian model averaging, model selection and its approximations such as BIC are generally statistically consistent, but sometimes achieve slower rates og convergence than other methods such as AIC and leave-one-out cross-validation. On the other hand, these other methods can br inconsistent. We identify the "catch-up phenomenon" as a novel explanation for the slow convergence of Bayesian methods. Based on this analysis we define the switch distribution, a modification of the Bayesian marginal distribution. We show that, under broad conditions,model selection and prediction based on the switch distribution is both consistent and achieves optimal convergence rates, thereby resolving the AIC-BIC dilemma. The method is practical; we give an efficient implementation. The switch distribution has a data compression interpretation, and can thus be viewed as a "prequential" or MDL method; yet it is different from the MDL methods that are usually considered in the literature. We compare the switch distribution to Bayes factor model selection and leave-one-out cross-validation.

</details>

<details>

<summary>2008-07-08 09:52:38 - Quantitative comparisons between finitary posterior distributions and Bayesian posterior distributions</summary>

- *Federico Bassetti*

- `0807.1201v1` - [abs](http://arxiv.org/abs/0807.1201v1) - [pdf](http://arxiv.org/pdf/0807.1201v1)

> The main object of Bayesian statistical inference is the determination of posterior distributions. Sometimes these laws are given for quantities devoid of empirical value. This serious drawback vanishes when one confines oneself to considering a finite horizon framework. However, assuming infinite exchangeability gives rise to fairly tractable {\it a posteriori} quantities, which is very attractive in applications. Hence, with a view to a reconciliation between these two aspects of the Bayesian way of reasoning, in this paper we provide quantitative comparisons between posterior distributions of finitary parameters and posterior distributions of allied parameters appearing in usual statistical models.

</details>

<details>

<summary>2008-07-13 09:19:09 - Gaussian Processes and Limiting Linear Models</summary>

- *Robert B. Gramacy, Herbert K. H. Lee*

- `0804.4685v4` - [abs](http://arxiv.org/abs/0804.4685v4) - [pdf](http://arxiv.org/pdf/0804.4685v4)

> Gaussian processes retain the linear model either as a special case, or in the limit. We show how this relationship can be exploited when the data are at least partially linear. However from the perspective of the Bayesian posterior, the Gaussian processes which encode the linear model either have probability of nearly zero or are otherwise unattainable without the explicit construction of a prior with the limiting linear model in mind. We develop such a prior, and show that its practical benefits extend well beyond the computational and conceptual simplicity of the linear model. For example, linearity can be extracted on a per-dimension basis, or can be combined with treed partition models to yield a highly efficient nonstationary model. Our approach is demonstrated on synthetic and real datasets of varying linearity and dimensionality.

</details>

<details>

<summary>2008-07-22 07:51:38 - Building Hyper Dirichlet Processes for Graphical Models</summary>

- *Daniel Heinz*

- `0807.3410v1` - [abs](http://arxiv.org/abs/0807.3410v1) - [pdf](http://arxiv.org/pdf/0807.3410v1)

> Graphical models are used to describe the conditional independence relations in multivariate data. They have been used for a variety of problems, including log-linear models (Liu and Massam, 2006), network analysis (Holland and Leinhardt, 1981; Strauss and Ikeda, 1990; Wasserman and Pattison, 1996; Pattison and Wasserman, 1999; Robins et al., 1999);, graphical Gaussian models (Roverato and Whittaker, 1998; Giudici and Green, 1999; Marrelec and Benali, 2006), and genetics (Dobra et al., 2004). A distribution that satisfies the conditional independence structure of a graph is Markov. A graphical model is a family of distributions that is restricted to be Markov with respect to a certain graph. In a Bayesian problem, one may specify a prior over the graphical model. Such a prior is called a hyper Markov law if the random marginals also satisfy the independence constraints. Previous work in this area includes (Dempster, 1972; Dawid and Lauritzen, 1993; Giudici and Green, 1999; Letac and Massam, 2007). We explore graphical models based on a non-parametric family of distributions, developed from Dirichlet processes.

</details>

<details>

<summary>2008-07-28 15:15:54 - Bayesian models to adjust for response bias in survey data for estimating rape and domestic violence rates from the NCVS</summary>

- *Qingzhao Yu, Elizabeth A. Stasny, Bin Li*

- `0801.3442v2` - [abs](http://arxiv.org/abs/0801.3442v2) - [pdf](http://arxiv.org/pdf/0801.3442v2)

> It is difficult to accurately estimate the rates of rape and domestic violence due to the sensitive nature of these crimes. There is evidence that bias in estimating the crime rates from survey data may arise because some women respondents are "gagged" in reporting some types of crimes by the use of a telephone rather than a personal interview, and by the presence of a spouse during the interview. On the other hand, as data on these crimes are collected every year, it would be more efficient in data analysis if we could identify and make use of information from previous data. In this paper we propose a model to adjust the estimates of the rates of rape and domestic violence to account for the response bias due to the "gag" factors. To estimate parameters in the model, we identify the information that is not sensitive to time and incorporate this into prior distributions. The strength of Bayesian estimators is their ability to combine information from long observational records in a sensible way. Within a Bayesian framework, we develop an Expectation-Maximization-Bayesian (EMB) algorithm for computation in analyzing contingency table and we apply the jackknife to estimate the accuracy of the estimates. Our approach is illustrated using the yearly crime data from the National Crime Victimization Survey. The illustration shows that compared with the classical method, our model leads to more efficient estimation but does not require more complicated computation.

</details>

<details>

<summary>2008-07-29 13:40:23 - Unsupervised empirical Bayesian multiple testing with external covariates</summary>

- *Egil Ferkingstad, Arnoldo Frigessi, Håvard Rue, Gudmar Thorleifsson, Augustine Kong*

- `0807.4658v1` - [abs](http://arxiv.org/abs/0807.4658v1) - [pdf](http://arxiv.org/pdf/0807.4658v1)

> In an empirical Bayesian setting, we provide a new multiple testing method, useful when an additional covariate is available, that influences the probability of each null hypothesis being true. We measure the posterior significance of each test conditionally on the covariate and the data, leading to greater power. Using covariate-based prior information in an unsupervised fashion, we produce a list of significant hypotheses which differs in length and order from the list obtained by methods not taking covariate-information into account. Covariate-modulated posterior probabilities of each null hypothesis are estimated using a fast approximate algorithm. The new method is applied to expression quantitative trait loci (eQTL) data.

</details>

<details>

<summary>2008-07-29 14:29:58 - Gamma shape mixtures for heavy-tailed distributions</summary>

- *Sergio Venturini, Francesca Dominici, Giovanni Parmigiani*

- `0807.4663v1` - [abs](http://arxiv.org/abs/0807.4663v1) - [pdf](http://arxiv.org/pdf/0807.4663v1)

> An important question in health services research is the estimation of the proportion of medical expenditures that exceed a given threshold. Typically, medical expenditures present highly skewed, heavy tailed distributions, for which (a) simple variable transformations are insufficient to achieve a tractable low-dimensional parametric form and (b) nonparametric methods are not efficient in estimating exceedance probabilities for large thresholds. Motivated by this context, in this paper we propose a general Bayesian approach for the estimation of tail probabilities of heavy-tailed distributions, based on a mixture of gamma distributions in which the mixing occurs over the shape parameter. This family provides a flexible and novel approach for modeling heavy-tailed distributions, it is computationally efficient, and it only requires to specify a prior distribution for a single parameter. By carrying out simulation studies, we compare our approach with commonly used methods, such as the log-normal model and nonparametric alternatives. We found that the mixture-gamma model significantly improves predictive performance in estimating tail probabilities, compared to these alternatives. We also applied our method to the Medical Current Beneficiary Survey (MCBS), for which we estimate the probability of exceeding a given hospitalization cost for smoking attributable diseases. We have implemented the method in the open source GSM package, available from the Comprehensive R Archive Network.

</details>

<details>

<summary>2008-07-31 21:11:32 - Lectures on Probability, Entropy, and Statistical Physics</summary>

- *Ariel Caticha*

- `0808.0012v1` - [abs](http://arxiv.org/abs/0808.0012v1) - [pdf](http://arxiv.org/pdf/0808.0012v1)

> These lectures deal with the problem of inductive inference, that is, the problem of reasoning under conditions of incomplete information. Is there a general method for handling uncertainty? Or, at least, are there rules that could in principle be followed by an ideally rational mind when discussing scientific matters? What makes one statement more plausible than another? How much more plausible? And then, when new information is acquired how do we change our minds? Or, to put it differently, are there rules for learning? Are there rules for processing information that are objective and consistent? Are they unique? And, come to think of it, what, after all, is information? It is clear that data contains or conveys information, but what does this precisely mean? Can information be conveyed in other ways? Is information physical? Can we measure amounts of information? Do we need to? Our goal is to develop the main tools for inductive inference--probability and entropy--from a thoroughly Bayesian point of view and to illustrate their use in physics with examples borrowed from the foundations of classical statistical physics.

</details>


## 2008-08

<details>

<summary>2008-08-05 10:04:45 - Microarrays, Empirical Bayes and the Two-Groups Model</summary>

- *Bradley Efron*

- `0808.0572v1` - [abs](http://arxiv.org/abs/0808.0572v1) - [pdf](http://arxiv.org/pdf/0808.0572v1)

> The classic frequentist theory of hypothesis testing developed by Neyman, Pearson and Fisher has a claim to being the twentieth century's most influential piece of applied mathematics. Something new is happening in the twenty-first century: high-throughput devices, such as microarrays, routinely require simultaneous hypothesis tests for thousands of individual cases, not at all what the classical theory had in mind. In these situations empirical Bayes information begins to force itself upon frequentists and Bayesians alike. The two-groups model is a simple Bayesian construction that facilitates empirical Bayes analysis. This article concerns the interplay of Bayesian and frequentist ideas in the two-groups setting, with particular attention focused on Benjamini and Hochberg's False Discovery Rate method. Topics include the choice and meaning of the null hypothesis in large-scale testing situations, power considerations, the limitations of permutation methods, significance testing for groups of cases (such as pathways in microarray studies), correlation effects, multiple confidence intervals and Bayesian competitors to the two-groups model.

</details>

<details>

<summary>2008-08-07 10:07:56 - Searching for a trail of evidence in a maze</summary>

- *Ery Arias-Castro, Emmanuel J. Candès, Hannes Helgason, Ofer Zeitouni*

- `0701668v2` - [abs](http://arxiv.org/abs/0701668v2) - [pdf](http://arxiv.org/pdf/math/0701668v2)

> Consider a graph with a set of vertices and oriented edges connecting pairs of vertices. Each vertex is associated with a random variable and these are assumed to be independent. In this setting, suppose we wish to solve the following hypothesis testing problem: under the null, the random variables have common distribution N(0,1) while under the alternative, there is an unknown path along which random variables have distribution $N(\mu,1)$, $\mu> 0$, and distribution N(0,1) away from it. For which values of the mean shift $\mu$ can one reliably detect and for which values is this impossible? Consider, for example, the usual regular lattice with vertices of the form \[\{(i,j):0\le i,-i\le j\le i and j has the parity of i\}\] and oriented edges $(i,j)\to (i+1,j+s)$, where $s=\pm1$. We show that for paths of length $m$ starting at the origin, the hypotheses become distinguishable (in a minimax sense) if $\mu_m\gg1/\sqrt{\log m}$, while they are not if $\mu_m\ll1/\log m$. We derive equivalent results in a Bayesian setting where one assumes that all paths are equally likely; there, the asymptotic threshold is $\mu_m\approx m^{-1/4}$. We obtain corresponding results for trees (where the threshold is of order 1 and independent of the size of the tree), for distributions other than the Gaussian and for other graphs. The concept of the predictability profile, first introduced by Benjamini, Pemantle and Peres, plays a crucial role in our analysis.

</details>

<details>

<summary>2008-08-07 19:13:19 - Changes in the Distribution of Income Volatility</summary>

- *Shane T. Jensen, Stephen H. Shore*

- `0808.1090v1` - [abs](http://arxiv.org/abs/0808.1090v1) - [pdf](http://arxiv.org/pdf/0808.1090v1)

> Recent research has documented a significant rise in the volatility (e.g., expected squared change) of individual incomes in the U.S. since the 1970s. Existing measures of this trend abstract from individual heterogeneity, effectively estimating an increase in average volatility. We decompose this increase in average volatility and find that it is far from representative of the experience of most people: there has been no systematic rise in volatility for the vast majority of individuals. The rise in average volatility has been driven almost entirely by a sharp rise in the income volatility of those expected to have the most volatile incomes, identified ex-ante by large income changes in the past. We document that the self-employed and those who self-identify as risk-tolerant are much more likely to have such volatile incomes; these groups have experienced much larger increases in income volatility than the population at large. These results color the policy implications one might draw from the rise in average volatility. While the basic results are apparent from PSID summary statistics, providing a complete characterization of the dynamics of the volatility distribution is a methodological challenge. We resolve these difficulties with a Markovian hierarchical Dirichlet process that builds on work from the non-parametric Bayesian statistics literature.

</details>

<details>

<summary>2008-08-20 07:07:10 - Posterior Convergence and Model Estimation in Bayesian Change-point Problems</summary>

- *Heng Lian*

- `0808.2700v1` - [abs](http://arxiv.org/abs/0808.2700v1) - [pdf](http://arxiv.org/pdf/0808.2700v1)

> We study the posterior distribution of the Bayesian multiple change-point regression problem when the number and the locations of the change-points are unknown. While it is relatively easy to apply the general theory to obtain the $O(1/\sqrt{n})$ rate up to some logarithmic factor, showing the exact parametric rate of convergence of the posterior distribution requires additional work and assumptions. Additionally, we demonstrate the asymptotic normality of the segment levels under these assumptions. For inferences on the number of change-points, we show that the Bayesian approach can produce a consistent posterior estimate. Finally, we argue that the point-wise posterior convergence property as demonstrated might have bad finite sample performance in that consistent posterior for model selection necessarily implies the maximal squared risk will be asymptotically larger than the optimal $O(1/\sqrt{n})$ rate. This is the Bayesian version of the same phenomenon that has been noted and studied by other authors.

</details>

<details>

<summary>2008-08-21 22:49:35 - Up-and-Down and the Percentile-Finding Problem</summary>

- *Assaf P. Oron*

- `0808.3004v1` - [abs](http://arxiv.org/abs/0808.3004v1) - [pdf](http://arxiv.org/pdf/0808.3004v1)

> Up-and-Down (U&D) is a popular sequential design for estimating threshold percentiles in binary experiments. However, U&D application practices have stagnated, and significant gaps in understanding its properties persist. The first part of my work aims to fill gaps in U&D theory. New results concerning stationary distribution properties are proven. A second focus of this study is nonparametric U&D estimation. An improvement to isotonic regression called "centered isotonic regression" (CIR), and a new averaging estimator called "auto-detect" are introduced and their properties studied. Bayesian percentile-finding designs, most notably the continual reassessment method (CRM) developed for Phase I clinical trials, are also studied. In general, CRM convergence depends upon random run-time conditions -- meaning that convergence is not always assured. Small-sample behavior is studied as well. It is shown that CRM is quite sensitive to outlier sub-sequences of thresholds, resulting in highly variable small-sample behavior between runs under identical conditions. Nonparametric CRM variants exhibit a similar sensitivity. Ideas to combine the advantages of U&D and Bayesian designs are examined. A new approach is developed, using a hybrid framework, that evaluates the evidence for overriding the U&D allocation with a Bayesian one.

</details>

<details>

<summary>2008-08-25 23:10:09 - Uncertainty quantification in complex systems using approximate solvers</summary>

- *Phaedon-Stelios Koutsourelakis*

- `0808.3416v1` - [abs](http://arxiv.org/abs/0808.3416v1) - [pdf](http://arxiv.org/pdf/0808.3416v1)

> This paper proposes a novel uncertainty quantification framework for computationally demanding systems characterized by a large vector of non-Gaussian uncertainties. It combines state-of-the-art techniques in advanced Monte Carlo sampling with Bayesian formulations. The key departure from existing works is the use of inexpensive, approximate computational models in a rigorous manner. Such models can readily be derived by coarsening the discretization size in the solution of the governing PDEs, increasing the time step when integration of ODEs is performed, using fewer iterations if a non-linear solver is employed or making use of lower order models. It is shown that even in cases where the inexact models provide very poor approximations of the exact response, statistics of the latter can be quantified accurately with significant reductions in the computational effort. Multiple approximate models can be used and rigorous confidence bounds of the estimates produced are provided at all stages.

</details>

<details>

<summary>2008-08-29 08:30:40 - Statistical models, likelihood, penalized likelihood and hierarchical likelihood</summary>

- *Daniel Commenges*

- `0808.4042v1` - [abs](http://arxiv.org/abs/0808.4042v1) - [pdf](http://arxiv.org/pdf/0808.4042v1)

> We give an overview of statistical models and likelihood, together with two of its variants: penalized and hierarchical likelihood. The Kullback-Leibler divergence is referred to repeatedly, for defining the misspecification risk of a model, for grounding the likelihood and the likelihood crossvalidation which can be used for choosing weights in penalized likelihood. Families of penalized likelihood and sieves estimators are shown to be equivalent. The similarity of these likelihood with a posteriori distributions in a Bayesian approach is considered.

</details>

<details>

<summary>2008-08-31 21:23:01 - Tracking Stopping Times Through Noisy Observations</summary>

- *Urs Niesen, Aslan Tchamkerten*

- `0701261v4` - [abs](http://arxiv.org/abs/0701261v4) - [pdf](http://arxiv.org/pdf/math/0701261v4)

> A novel quickest detection setting is proposed which is a generalization of the well-known Bayesian change-point detection model. Suppose \{(X_i,Y_i)\}_{i\geq 1} is a sequence of pairs of random variables, and that S is a stopping time with respect to \{X_i\}_{i\geq 1}. The problem is to find a stopping time T with respect to \{Y_i\}_{i\geq 1} that optimally tracks S, in the sense that T minimizes the expected reaction delay E(T-S)^+, while keeping the false-alarm probability P(T<S) below a given threshold \alpha \in [0,1]. This problem formulation applies in several areas, such as in communication, detection, forecasting, and quality control.   Our results relate to the situation where the X_i's and Y_i's take values in finite alphabets and where S is bounded by some positive integer \kappa. By using elementary methods based on the analysis of the tree structure of stopping times, we exhibit an algorithm that computes the optimal average reaction delays for all \alpha \in [0,1], and constructs the associated optimal stopping times T. Under certain conditions on \{(X_i,Y_i)\}_{i\geq 1} and S, the algorithm running time is polynomial in \kappa.

</details>


## 2008-09

<details>

<summary>2008-09-02 11:13:17 - A flexible Bayesian method for adaptive measurement in psychophysics</summary>

- *Simon Barthelmé, Pascal Mamassian*

- `0809.0387v1` - [abs](http://arxiv.org/abs/0809.0387v1) - [pdf](http://arxiv.org/pdf/0809.0387v1)

> In psychophysical experiments time and the limited goodwill of participants is usually a major constraint. This has been the main motivation behind the early development of adaptive methods for the measurements of psychometric thresholds. More recently methods have been developed to measure whole psychometric functions in an adaptive way. Here we describe a Bayesian method to measure adaptively any aspect of a psychophysical function, taking inspiration from Kontsevich and Tyler's optimal Bayesian measurement method. Our method is implemented in a complete and easy-to-use MATLAB package.

</details>

<details>

<summary>2008-09-05 01:09:56 - Applications of Bayesian Probability Theory in Astrophysics</summary>

- *Brendon J. Brewer*

- `0809.0939v1` - [abs](http://arxiv.org/abs/0809.0939v1) - [pdf](http://arxiv.org/pdf/0809.0939v1)

> Bayesian Inference is a powerful approach to data analysis that is based almost entirely on probability theory. In this approach, probabilities model {\it uncertainty} rather than randomness or variability. This thesis is composed of a series of papers that have been published in various astronomical journals during the years 2005-2008. The unifying thread running through the papers is the use of Bayesian Inference to solve underdetermined inverse problems in astrophysics. Firstly, a methodology is developed to solve a question in gravitational lens inversion - using the observed images of gravitational lens systems to reconstruct the undistorted source profile and the mass profile of the lensing galaxy. A similar technique is also applied to the task of inferring the number and frequency of modes of oscillation of a star from the time series observations that are used in the field of asteroseismology. For these complex problems, many of the required calculations cannot be done analytically, and so Markov Chain Monte Carlo algorithms have been used. Finally, probabilistic reasoning is applied to a controversial question in astrobiology: does the fact that life formed quite soon after the Earth constitute evidence that the formation of life is quite probable, given the right macroscopic conditions?

</details>

<details>

<summary>2008-09-17 06:35:28 - Case-deletion importance sampling estimators: Central limit theorems and related results</summary>

- *Ilenia Epifani, Steven N. MacEachern, Mario Peruggia*

- `0807.0725v2` - [abs](http://arxiv.org/abs/0807.0725v2) - [pdf](http://arxiv.org/pdf/0807.0725v2)

> Case-deleted analysis is a popular method for evaluating the influence of a subset of cases on inference. The use of Monte Carlo estimation strategies in complicated Bayesian settings leads naturally to the use of importance sampling techniques to assess the divergence between full-data and case-deleted posteriors and to provide estimates under the case-deleted posteriors. However, the dependability of the importance sampling estimators depends critically on the variability of the case-deleted weights. We provide theoretical results concerning the assessment of the dependability of case-deleted importance sampling estimators in several Bayesian models. In particular, these results allow us to establish whether or not the estimators satisfy a central limit theorem. Because the conditions we derive are of a simple analytical nature, the assessment of the dependability of the estimators can be verified routinely before estimation is performed. We illustrate the use of the results in several examples.

</details>

<details>

<summary>2008-09-23 09:59:48 - On adaptive Bayesian inference</summary>

- *Yang Xing*

- `0805.3584v2` - [abs](http://arxiv.org/abs/0805.3584v2) - [pdf](http://arxiv.org/pdf/0805.3584v2)

> We study the rate of Bayesian consistency for hierarchical priors consisting of prior weights on a model index set and a prior on a density model for each choice of model index. Ghosal, Lember and Van der Vaart [2] have obtained general in-probability theorems on the rate of convergence of the resulting posterior distributions. We extend their results to almost sure assertions. As an application we study log spline densities with a finite number of models and obtain that the Bayes procedure achieves the optimal minimax rate $n^{-\gamma/(2\gamma+1)}$ of convergence if the true density of the observations belongs to the H\"{o}lder space $C^{\gamma}[0,1]$. This strengthens a result in [1; 2]. We also study consistency of posterior distributions of the model index and give conditions ensuring that the posterior distributions concentrate their masses near the index of the best model.

</details>

<details>

<summary>2008-09-24 13:21:45 - Quantile Estimation of A general Single-Index Model</summary>

- *Efang Kong, Yingcun Xia*

- `0803.2474v4` - [abs](http://arxiv.org/abs/0803.2474v4) - [pdf](http://arxiv.org/pdf/0803.2474v4)

> The single-index model is one of the most popular semiparametric models in Econometrics. In this paper, we define a quantile regression single-index model, which includes the single-index structure for conditional mean and for conditional variance.

</details>

<details>

<summary>2008-09-24 18:01:43 - Nonlinear Digital Post-Processing to Mitigate Jitter in Sampling</summary>

- *Daniel S. Weller, Vivek K Goyal*

- `0809.4244v1` - [abs](http://arxiv.org/abs/0809.4244v1) - [pdf](http://arxiv.org/pdf/0809.4244v1)

> This paper describes several new algorithms for estimating the parameters of a periodic bandlimited signal from samples corrupted by jitter (timing noise) and additive noise. Both classical (non-random) and Bayesian formulations are considered: an Expectation-Maximization (EM) algorithm is developed to compute the maximum likelihood (ML) estimator for the classical estimation framework, and two Gibbs samplers are proposed to approximate the Bayes least squares (BLS) estimate for parameters independently distributed according to a uniform prior. Simulations are performed to demonstrate the significant performance improvement achievable using these algorithms as compared to linear estimators. The ML estimator is also compared to the Cramer-Rao lower bound to determine the range of jitter for which the estimator is approximately efficient. These simulations provide evidence that the nonlinear algorithms derived here can tolerate 1.4-2 times more jitter than linear estimators, reducing on-chip ADC power consumption by 50-75 percent.

</details>

<details>

<summary>2008-09-26 05:49:20 - Capital process and optimality properties of a Bayesian Skeptic in coin-tossing games</summary>

- *Masayuki Kumon, Akimichi Takemura, Kei Takeuchi*

- `0510662v2` - [abs](http://arxiv.org/abs/0510662v2) - [pdf](http://arxiv.org/pdf/math/0510662v2)

> We study capital process behavior in the fair-coin game and biased-coin games in the framework of the game-theoretic probability of Shafer and Vovk (2001). We show that if Skeptic uses a Bayesian strategy with a beta prior, the capital process is lucidly expressed in terms of the past average of Reality's moves.   From this it is proved that the Skeptic's Bayesian strategy weakly forces the strong law of large numbers (SLLN) with the convergence rate of O(\sqrt{\log n/n})$ and if Reality violates SLLN then the exponential growth rate of the capital process is very accurately described in terms of the Kullback divergence between the average of Reality's moves when she violates SLLN and the average when she observes SLLN. We also investigate optimality properties associated with Bayesian strategy.

</details>

<details>

<summary>2008-09-30 14:38:17 - Bahadur representation of sample quantiles for functional of Gaussian dependent sequences under a minimal assumption</summary>

- *Jean-François Coeurjolly*

- `0702440v2` - [abs](http://arxiv.org/abs/0702440v2) - [pdf](http://arxiv.org/pdf/math/0702440v2)

> We obtain a Bahadur representation for sample quantiles of nonlinear functional of Gaussian sequences with correlation function decreasing as $k^{-\alpha}$ for some $\alpha > 0$. This representation is derived under a mimimal assumption.

</details>


## 2008-10

<details>

<summary>2008-10-04 03:24:21 - A multi-resolution, non-parametric, Bayesian framework for identification of spatially-varying model parameters</summary>

- *P. S. Koutsourelakis*

- `0810.0744v1` - [abs](http://arxiv.org/abs/0810.0744v1) - [pdf](http://arxiv.org/pdf/0810.0744v1)

> This paper proposes a hierarchical, multi-resolution framework for the identification of model parameters and their spatially variability from noisy measurements of the response or output. Such parameters are frequently encountered in PDE-based models and correspond to quantities such as density or pressure fields, elasto-plastic moduli and internal variables in solid mechanics, conductivity fields in heat diffusion problems, permeability fields in fluid flow through porous media etc. The proposed model has all the advantages of traditional Bayesian formulations such as the ability to produce measures of confidence for the inferences made and providing not only predictive estimates but also quantitative measures of the predictive uncertainty. In contrast to existing approaches it utilizes a parsimonious, non-parametric formulation that favors sparse representations and whose complexity can be determined from the data. The proposed framework in non-intrusive and makes use of a sequence of forward solvers operating at various resolutions. As a result, inexpensive, coarse solvers are used to identify the most salient features of the unknown field(s) which are subsequently enriched by invoking solvers operating at finer resolutions. This leads to significant computational savings particularly in problems involving computationally demanding forward models but also improvements in accuracy. It is based on a novel, adaptive scheme based on Sequential Monte Carlo sampling which is embarrassingly parallelizable and circumvents issues with slow mixing encountered in Markov Chain Monte Carlo schemes.

</details>

<details>

<summary>2008-10-07 11:10:40 - A strong uniform convergence rate of a kernel conditional quantile estimator under random left-truncation and dependent data</summary>

- *Elias Ould-Saïd, Djabrane Yahia, Abdelhakim Necir*

- `0810.1156v1` - [abs](http://arxiv.org/abs/0810.1156v1) - [pdf](http://arxiv.org/pdf/0810.1156v1)

> In this paper we study some asymptotic properties of the kernel conditional quantile estimator with randomly left-truncated data which exhibit some kind of dependence. We extend the result obtained by Lemdani, Ould-Sa\"id and Poulin [16] in the iid case. The uniform strong convergence rate of the estimator under strong mixing hypothesis is obtained.

</details>

<details>

<summary>2008-10-07 11:28:00 - Generalised linear mixed model analysis via sequential Monte Carlo sampling</summary>

- *Y. Fan, D. S. Leslie, M. P. Wand*

- `0810.1163v1` - [abs](http://arxiv.org/abs/0810.1163v1) - [pdf](http://arxiv.org/pdf/0810.1163v1)

> We present a sequential Monte Carlo sampler algorithm for the Bayesian analysis of generalised linear mixed models (GLMMs). These models support a variety of interesting regression-type analyses, but performing inference is often extremely difficult, even when using the Bayesian approach combined with Markov chain Monte Carlo (MCMC). The Sequential Monte Carlo sampler (SMC) is a new and general method for producing samples from posterior distributions. In this article we demonstrate use of the SMC method for performing inference for GLMMs. We demonstrate the effectiveness of the method on both simulated and real data, and find that sequential Monte Carlo is a competitive alternative to the available MCMC techniques.

</details>

<details>

<summary>2008-10-15 09:36:23 - Bayesian evidence for finite element model updating</summary>

- *Linda Mthembu, Tshilidzi Marwala, Michael I. Friswell, Sondipon Adhikari*

- `0810.2643v1` - [abs](http://arxiv.org/abs/0810.2643v1) - [pdf](http://arxiv.org/pdf/0810.2643v1)

> This paper considers the problem of model selection within the context of finite element model updating. Given that a number of FEM updating models, with different updating parameters, can be designed, this paper proposes using the Bayesian evidence statistic to assess the probability of each updating model. This makes it possible then to evaluate the need for alternative updating parameters in the updating of the initial FE model. The model evidences are compared using the Bayes factor, which is the ratio of evidences. The Jeffrey scale is used to determine the differences in the models. The Bayesian evidence is calculated by integrating the likelihood of the data given the model and its parameters over the a priori model parameter space using the new nested sampling algorithm. The nested algorithm samples this likelihood distribution by using a hard likelihood-value constraint on the sampling region while providing the posterior samples of the updating model parameters as a by-product. This method is used to calculate the evidence of a number of plausible finite element models.

</details>

<details>

<summary>2008-10-23 09:04:56 - Smoothed weighted empirical likelihood ratio confidence intervals for quantiles</summary>

- *Jian-Jian Ren*

- `0810.4238v1` - [abs](http://arxiv.org/abs/0810.4238v1) - [pdf](http://arxiv.org/pdf/0810.4238v1)

> Thus far, likelihood-based interval estimates for quantiles have not been studied in the literature on interval censored case 2 data and partly interval censored data, and, in this context, the use of smoothing has not been considered for any type of censored data. This article constructs smoothed weighted empirical likelihood ratio confidence intervals (WELRCI) for quantiles in a unified framework for various types of censored data, including right censored data, doubly censored data, interval censored data and partly interval censored data. The fourth order expansion of the weighted empirical log-likelihood ratio is derived and the theoretical coverage accuracy equation for the proposed WELRCI is established, which generally guarantees at least `first order' accuracy. In particular, for right censored data, we show that the coverage accuracy is at least $O(n^{-1/2})$ and our simulation studies show that in comparison with empirical likelihood-based methods, the smoothing used in WELRCI generally provides a shorter confidence interval with comparable coverage accuracy. For interval censored data, it is interesting to find that with an adjusted rate $n^{-1/3}$, the weighted empirical log-likelihood ratio has an asymptotic distribution completely different from that obtained by the empirical likelihood approach and the resulting WELRCI perform favorably in the available comparison simulation studies.

</details>

<details>

<summary>2008-10-27 12:30:43 - Robust nonparametric estimation via wavelet median regression</summary>

- *Lawrence D. Brown, T. Tony Cai, Harrison H. Zhou*

- `0810.4802v1` - [abs](http://arxiv.org/abs/0810.4802v1) - [pdf](http://arxiv.org/pdf/0810.4802v1)

> In this paper we develop a nonparametric regression method that is simultaneously adaptive over a wide range of function classes for the regression function and robust over a large collection of error distributions, including those that are heavy-tailed, and may not even possess variances or means. Our approach is to first use local medians to turn the problem of nonparametric regression with unknown noise distribution into a standard Gaussian regression problem and then apply a wavelet block thresholding procedure to construct an estimator of the regression function. It is shown that the estimator simultaneously attains the optimal rate of convergence over a wide range of the Besov classes, without prior knowledge of the smoothness of the underlying functions or prior knowledge of the error distribution. The estimator also automatically adapts to the local smoothness of the underlying function, and attains the local adaptive minimax rate for estimating functions at a point. A key technical result in our development is a quantile coupling theorem which gives a tight bound for the quantile coupling between the sample medians and a normal variable. This median coupling inequality may be of independent interest.

</details>

<details>

<summary>2008-10-27 14:27:52 - Estimation of distributions, moments and quantiles in deconvolution problems</summary>

- *Peter Hall, Soumendra N. Lahiri*

- `0810.4821v1` - [abs](http://arxiv.org/abs/0810.4821v1) - [pdf](http://arxiv.org/pdf/0810.4821v1)

> When using the bootstrap in the presence of measurement error, we must first estimate the target distribution function; we cannot directly resample, since we do not have a sample from the target. These and other considerations motivate the development of estimators of distributions, and of related quantities such as moments and quantiles, in errors-in-variables settings. We show that such estimators have curious and unexpected properties. For example, if the distributions of the variable of interest, $W$, say, and of the observation error are both centered at zero, then the rate of convergence of an estimator of the distribution function of $W$ can be slower at the origin than away from the origin. This is an intrinsic characteristic of the problem, not a quirk of particular estimators; the property holds true for optimal estimators.

</details>

<details>

<summary>2008-10-30 12:05:12 - Approximating the marginal likelihood using copula</summary>

- *David J. Nott, Robert J. Kohn, Mark Fielding*

- `0810.5474v1` - [abs](http://arxiv.org/abs/0810.5474v1) - [pdf](http://arxiv.org/pdf/0810.5474v1)

> Model selection is an important activity in modern data analysis and the conventional Bayesian approach to this problem involves calculation of marginal likelihoods for different models, together with diagnostics which examine specific aspects of model fit. Calculating the marginal likelihood is a difficult computational problem. Our article proposes some extensions of the Laplace approximation for this task that are related to copula models and which are easy to apply. Variations which can be used both with and without simulation from the posterior distribution are considered, as well as use of the approximations with bridge sampling and in random effects models with a large number of latent variables. The use of a t-copula to obtain higher accuracy when multivariate dependence is not well captured by a Gaussian copula is also discussed.

</details>

<details>

<summary>2008-10-31 10:38:41 - Gibbs posterior for variable selection in high-dimensional classification and data mining</summary>

- *Wenxin Jiang, Martin A. Tanner*

- `0810.5655v1` - [abs](http://arxiv.org/abs/0810.5655v1) - [pdf](http://arxiv.org/pdf/0810.5655v1)

> In the popular approach of "Bayesian variable selection" (BVS), one uses prior and posterior distributions to select a subset of candidate variables to enter the model. A completely new direction will be considered here to study BVS with a Gibbs posterior originating in statistical mechanics. The Gibbs posterior is constructed from a risk function of practical interest (such as the classification error) and aims at minimizing a risk function without modeling the data probabilistically. This can improve the performance over the usual Bayesian approach, which depends on a probability model which may be misspecified. Conditions will be provided to achieve good risk performance, even in the presence of high dimensionality, when the number of candidate variables "$K$" can be much larger than the sample size "$n$." In addition, we develop a convenient Markov chain Monte Carlo algorithm to implement BVS with the Gibbs posterior.

</details>


## 2008-11

<details>

<summary>2008-11-08 22:19:44 - Optimal sequential multiple hypothesis tests</summary>

- *Andrey Novikov*

- `0811.1297v1` - [abs](http://arxiv.org/abs/0811.1297v1) - [pdf](http://arxiv.org/pdf/0811.1297v1)

> This work deals with a general problem of testing multiple hypotheses about the distribution of a discrete-time stochastic process. Both the Bayesian and the conditional settings are considered. The structure of optimal sequential tests is characterized.

</details>

<details>

<summary>2008-11-13 13:12:00 - On the frequentist coverage of Bayesian credible intervals for lower bounded means</summary>

- *Éric Marchand, William E. Strawderman, Keven Bosa, Aziz Lmoudden*

- `0809.1027v2` - [abs](http://arxiv.org/abs/0809.1027v2) - [pdf](http://arxiv.org/pdf/0809.1027v2)

> For estimating a lower bounded location or mean parameter for a symmetric and logconcave density, we investigate the frequentist performance of the $100(1-\alpha)%$ Bayesian HPD credible set associated with priors which are truncations of flat priors onto the restricted parameter space. Various new properties are obtained. Namely, we identify precisely where the minimum coverage is obtained and we show that this minimum coverage is bounded between $1-\frac{3\alpha}{2}$ and $1-\frac{3\alpha}{2}+\frac{\alpha^2}{1+\alpha}$; with the lower bound $1-\frac{3\alpha}{2}$ improving (for $\alpha \leq 1/3$) on the previously established ([9]; [8]) lower bound $\frac{1-\alpha}{1+\alpha}$. Several illustrative examples are given.

</details>

<details>

<summary>2008-11-13 20:09:36 - Markov switching negative binomial models: an application to vehicle accident frequencies</summary>

- *Nataliya V. Malyshkina, Fred L. Mannering, Andrew P. Tarko*

- `0811.1606v2` - [abs](http://arxiv.org/abs/0811.1606v2) - [pdf](http://arxiv.org/pdf/0811.1606v2)

> In this paper, two-state Markov switching models are proposed to study accident frequencies. These models assume that there are two unobserved states of roadway safety, and that roadway entities (roadway segments) can switch between these states over time. The states are distinct, in the sense that in the different states accident frequencies are generated by separate counting processes (by separate Poisson or negative binomial processes). To demonstrate the applicability of the approach presented herein, two-state Markov switching negative binomial models are estimated using five-year accident frequencies on Indiana interstate highway segments. Bayesian inference methods and Markov Chain Monte Carlo (MCMC) simulations are used for model estimation. The estimated Markov switching models result in a superior statistical fit relative to the standard (single-state) negative binomial model. It is found that the more frequent state is safer and it is correlated with better weather conditions. The less frequent state is found to be less safe and to be correlated with adverse weather conditions.

</details>

<details>

<summary>2008-11-17 07:12:34 - Estimation of bivariate excess probabilities for elliptical models</summary>

- *Belkacem Abdous, Anne-Laure Fougères, Kilani Ghoudi, Philippe Soulier*

- `0611914v4` - [abs](http://arxiv.org/abs/0611914v4) - [pdf](http://arxiv.org/pdf/math/0611914v4)

> Let $(X,Y)$ be a random vector whose conditional excess probability $\theta(x,y):=P(Y\leq y | X>x)$ is of interest. Estimating this kind of probability is a delicate problem as soon as $x$ tends to be large, since the conditioning event becomes an extreme set. Assume that $(X,Y)$ is elliptically distributed, with a rapidly varying radial component. In this paper, three statistical procedures are proposed to estimate $\theta(x,y)$ for fixed $x,y$, with $x$ large. They respectively make use of an approximation result of Abdous et al. (cf. Canad. J. Statist. 33 (2005) 317--334, Theorem 1), a new second order refinement of Abdous et al.'s Theorem 1, and a non-approximating method. The estimation of the conditional quantile function $\theta(x,\cdot)^{\leftarrow}$ for large fixed $x$ is also addressed and these methods are compared via simulations. An illustration in the financial context is also given.

</details>

<details>

<summary>2008-11-18 11:17:47 - Inference with Discriminative Posterior</summary>

- *Jarkko Salojärvi, Kai Puolamäki, Eerika Savia, Samuel Kaski*

- `0807.3470v2` - [abs](http://arxiv.org/abs/0807.3470v2) - [pdf](http://arxiv.org/pdf/0807.3470v2)

> We study Bayesian discriminative inference given a model family $p(c,\x, \theta)$ that is assumed to contain all our prior information but still known to be incorrect. This falls in between "standard" Bayesian generative modeling and Bayesian regression, where the margin $p(\x,\theta)$ is known to be uninformative about $p(c|\x,\theta)$. We give an axiomatic proof that discriminative posterior is consistent for conditional inference; using the discriminative posterior is standard practice in classical Bayesian regression, but we show that it is theoretically justified for model families of joint densities as well. A practical benefit compared to Bayesian regression is that the standard methods of handling missing values in generative modeling can be extended into discriminative inference, which is useful if the amount of data is small. Compared to standard generative modeling, discriminative posterior results in better conditional inference if the model family is incorrect. If the model family contains also the true model, the discriminative posterior gives the same result as standard Bayesian generative modeling. Practical computation is done with Markov chain Monte Carlo.

</details>

<details>

<summary>2008-11-21 22:05:53 - Markov switching multinomial logit model: an application to accident injury severities</summary>

- *Nataliya V. Malyshkina, Fred L. Mannering*

- `0811.3644v1` - [abs](http://arxiv.org/abs/0811.3644v1) - [pdf](http://arxiv.org/pdf/0811.3644v1)

> In this study, two-state Markov switching multinomial logit models are proposed for statistical modeling of accident injury severities. These models assume Markov switching in time between two unobserved states of roadway safety. The states are distinct, in the sense that in different states accident severity outcomes are generated by separate multinomial logit processes. To demonstrate the applicability of the approach presented herein, two-state Markov switching multinomial logit models are estimated for severity outcomes of accidents occurring on Indiana roads over a four-year time interval. Bayesian inference methods and Markov Chain Monte Carlo (MCMC) simulations are used for model estimation. The estimated Markov switching models result in a superior statistical fit relative to the standard (single-state) multinomial logit models. It is found that the more frequent state of roadway safety is correlated with better weather conditions. The less frequent state is found to be correlated with adverse weather conditions.

</details>

<details>

<summary>2008-11-26 22:11:11 - On optimal quantization rules for some problems in sequential decentralized detection</summary>

- *XuanLong Nguyen, Martin J. Wainwright, Michael I. Jordan*

- `0608556v2` - [abs](http://arxiv.org/abs/0608556v2) - [pdf](http://arxiv.org/pdf/math/0608556v2)

> We consider the design of systems for sequential decentralized detection, a problem that entails several interdependent choices: the choice of a stopping rule (specifying the sample size), a global decision function (a choice between two competing hypotheses), and a set of quantization rules (the local decisions on the basis of which the global decision is made). This paper addresses an open problem of whether in the Bayesian formulation of sequential decentralized detection, optimal local decision functions can be found within the class of stationary rules. We develop an asymptotic approximation to the optimal cost of stationary quantization rules and exploit this approximation to show that stationary quantizers are not optimal in a broad class of settings. We also consider the class of blockwise stationary quantizers, and show that asymptotically optimal quantizers are likelihood-based threshold rules.

</details>

<details>

<summary>2008-11-28 09:36:18 - Convergence rates of posterior distributions for observations without the iid structure</summary>

- *Yang Xing*

- `0811.4677v1` - [abs](http://arxiv.org/abs/0811.4677v1) - [pdf](http://arxiv.org/pdf/0811.4677v1)

> The classical condition on the existence of uniformly exponentially consistent tests for testing the true density against the complement of its arbitrary neighborhood has been widely adopted in study of asymptotics of Bayesian nonparametric procedures. Because we follow a Bayesian approach, it seems to be more natural to explore alternative and appropriate conditions which incorporate the prior distribution. In this paper we supply a new prior-dependent integration condition to establish general posterior convergence rate theorems for observations which may not be independent and identically distributed. The posterior convergence rates for such observations have recently studied by Ghosal and van der Vaart \cite{ghv1}. We moreover adopt the Hausdorff $\alpha$-entropy given by Xing and Ranneby \cite{xir1}\cite{xi1}, which is also prior-dependent and smaller than the widely used metric entropies. These lead to extensions of several existing theorems. In particular, we establish a posterior convergence rate theorem for general Markov processes and as its application we improve on the currently known posterior rate of convergence for a nonlinear autoregressive model.

</details>


## 2008-12

<details>

<summary>2008-12-05 19:33:17 - Both necessary and sufficient conditions for Bayesian exponential consistency</summary>

- *Yang Xing, Bo Ranneby*

- `0812.1084v1` - [abs](http://arxiv.org/abs/0812.1084v1) - [pdf](http://arxiv.org/pdf/0812.1084v1)

> The last decade has seen a remarkable development in the theory of asymptotics of Bayesian nonparametric procedures. Exponential consistency has played an important role in this area. It is known that the condition of $f_0$ being in the Kullback-Leibler support of the prior cannot ensure exponential consistency of posteriors. Many authors have obtained additional sufficient conditions for exponential consistency of posteriors, see, for instance, Schwartz (1965), Barron, Schervish and Wasserman (1999), Ghosal, Ghosh and Ramamoorthi (1999), Walker (2004), Xing and Ranneby (2008). However, given the Kullback-Leibler support condition, less is known about both necessary and sufficient conditions. In this paper we give one type of both necessary and sufficient conditions. As a consequence we derive a simple sufficient condition on Bayesian exponential consistency, which is weaker than the previous sufficient conditions.

</details>

<details>

<summary>2008-12-08 13:47:45 - Parallel hierarchical sampling: a practical multiple-chains sampler for Bayesian model selection</summary>

- *Fabio Rigat*

- `0812.1484v1` - [abs](http://arxiv.org/abs/0812.1484v1) - [pdf](http://arxiv.org/pdf/0812.1484v1)

> This paper introduces the parallel hierarchical sampler (PHS), a Markov chain Monte Carlo algorithm using several chains simultaneously. The connections between PHS and the parallel tempering (PT) algorithm are illustrated, convergence of PHS joint transition kernel is proved and and its practical advantages are emphasized. We illustrate the inferences obtained using PHS, parallel tempering and the Metropolis-Hastings algorithm for three Bayesian model selection problems, namely Gaussian clustering, the selection of covariates for a linear regression model and the selection of the structure of a treed survival model.

</details>

<details>

<summary>2008-12-08 23:35:30 - Markov switching models: an application to roadway safety</summary>

- *Nataliya V. Malyshkina*

- `0808.1448v2` - [abs](http://arxiv.org/abs/0808.1448v2) - [pdf](http://arxiv.org/pdf/0808.1448v2)

> In this research, two-state Markov switching models are proposed to study accident frequencies and severities. These models assume that there are two unobserved states of roadway safety, and that roadway entities (e.g., roadway segments) can switch between these states over time. The states are distinct, in the sense that in the different states accident frequencies or severities are generated by separate processes (e.g., Poisson, negative binomial, multinomial logit). Bayesian inference methods and Markov Chain Monte Carlo (MCMC) simulations are used for estimation of Markov switching models. To demonstrate the applicability of the approach, we conduct the following three studies. In the first study, two-state Markov switching count data models are considered as an alternative to zero-inflated models for annual accident frequencies, in order to account for preponderance of zeros typically observed in accident frequency data. In the second study, two-state Markov switching Poisson model and two-state Markov switching negative binomial model are estimated using weekly accident frequencies on selected Indiana interstate highway segments over a five-year time period. In the third study, two-state Markov switching multinomial logit models are estimated for severity outcomes of accidents occurring on Indiana roads over a four-year time period. One of the most important results found in each of the three studies, is that in each case the estimated Markov switching models are strongly favored by roadway safety data and result in a superior statistical fit, as compared to the corresponding standard (non-switching) models.

</details>

<details>

<summary>2008-12-16 12:04:40 - Strong Gaussian approximations of product-limit and Quantile Processes for Strong mixing and censored data</summary>

- *V. Fakoor, N. Nakhaee Rad*

- `0812.3038v1` - [abs](http://arxiv.org/abs/0812.3038v1) - [pdf](http://arxiv.org/pdf/0812.3038v1)

> In this paper, we consider the product-limit quantile estimator of an unknown quantile function under a censored dependent model. This is a parallel problem to the estimation of the unknown distribution function by the product-limit estimator under the same model. Simultaneous strong Gaussian approximations of the product-limit process and product-limit quantile process are constructed with rate $O((\log n)^{-\lambda})$ for some $\lambda>0,$. The strong Gaussian approximation of the product-limit process is then applied to derive the laws of the iterated logarithm for product-limit process.

</details>

<details>

<summary>2008-12-19 10:08:48 - Quickest Change Detection of a Markov Process Across a Sensor Array</summary>

- *Vasanthan Raghavan, Venugopal V. Veeravalli*

- `0812.3742v1` - [abs](http://arxiv.org/abs/0812.3742v1) - [pdf](http://arxiv.org/pdf/0812.3742v1)

> Recent attention in quickest change detection in the multi-sensor setting has been on the case where the densities of the observations change at the same instant at all the sensors due to the disruption. In this work, a more general scenario is considered where the change propagates across the sensors, and its propagation can be modeled as a Markov process. A centralized, Bayesian version of this problem, with a fusion center that has perfect information about the observations and a priori knowledge of the statistics of the change process, is considered. The problem of minimizing the average detection delay subject to false alarm constraints is formulated as a partially observable Markov decision process (POMDP). Insights into the structure of the optimal stopping rule are presented. In the limiting case of rare disruptions, we show that the structure of the optimal test reduces to thresholding the a posteriori probability of the hypothesis that no change has happened. We establish the asymptotic optimality (in the vanishing false alarm probability regime) of this threshold test under a certain condition on the Kullback-Leibler (K-L) divergence between the post- and the pre-change densities. In the special case of near-instantaneous change propagation across the sensors, this condition reduces to the mild condition that the K-L divergence be positive. Numerical studies show that this low complexity threshold test results in a substantial improvement in performance over naive tests such as a single-sensor test or a test that wrongly assumes that the change propagates instantaneously.

</details>

<details>

<summary>2008-12-22 15:23:06 - Optimal properties of some Bayesian inferences</summary>

- *M. Evans, M. Shakhatreh*

- `0710.1151v2` - [abs](http://arxiv.org/abs/0710.1151v2) - [pdf](http://arxiv.org/pdf/0710.1151v2)

> Relative surprise regions are shown to minimize, among Bayesian credible regions, the prior probability of covering a false value from the prior. Such regions are also shown to be unbiased in the sense that the prior probability of covering a false value is bounded above by the prior probability of covering the true value. Relative surprise regions are shown to maximize both the Bayes factor in favor of the region containing the true value and the relative belief ratio, among all credible regions with the same posterior content. Relative surprise regions emerge naturally when we consider equivalence classes of credible regions generated via reparameterizations.

</details>

<details>

<summary>2008-12-30 17:08:16 - Model-Based Clustering using multi-allelic loci data with loci selection</summary>

- *Wilson Toussile, Elisabeth Gassiat*

- `0812.1388v2` - [abs](http://arxiv.org/abs/0812.1388v2) - [pdf](http://arxiv.org/pdf/0812.1388v2)

> We propose a Model-Based Clustering (MBC) method combined with loci selection using multi-allelic loci genetic data. The loci selection problem is regarded as a model selection problem and models in competition are compared with the Bayesian Information Criterion (BIC). The resulting procedure selects the subset of clustering loci, the number of clusters, estimates the proportion of each cluster and the allelic frequencies within each cluster. We prove that the selected model converges in probability to the true model under a single realistic assumption as the size of the sample tends to infinity. The proposed method named MixMoGenD (Mixture Model using Genetic Data) was implemented using c++ programming language. Numerical experiments on simulated data sets was conducted to highlight the interest of the proposed loci selection procedure.

</details>

