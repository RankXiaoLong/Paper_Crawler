# 2012

## TOC

- [2012-01](#2012-01)
- [2012-02](#2012-02)
- [2012-03](#2012-03)
- [2012-04](#2012-04)
- [2012-05](#2012-05)
- [2012-06](#2012-06)
- [2012-07](#2012-07)
- [2012-08](#2012-08)
- [2012-09](#2012-09)
- [2012-10](#2012-10)
- [2012-11](#2012-11)
- [2012-12](#2012-12)

## 2012-01

<details>

<summary>2012-01-05 12:10:05 - A Problem in Particle Physics and Its Bayesian Analysis</summary>

- *Joshua Landon, Frank X. Lee, Nozer D. Singpurwalla*

- `1201.1141v1` - [abs](http://arxiv.org/abs/1201.1141v1) - [pdf](http://arxiv.org/pdf/1201.1141v1)

> There is a class of statistical problems that arises in several contexts, the Lattice QCD problem of particle physics being one that has attracted the most attention. In essence, the problem boils down to the estimation of an infinite number of parameters from a finite number of equations, each equation being an infinite sum of exponential functions. By introducing a latent parameter into the QCD system, we are able to identify a pattern which tantamounts to reducing the system to a telescopic series. A statistical model is then endowed on the series, and inference about the unknown parameters done via a Bayesian approach. A computationally intensive Markov Chain Monte Carlo (MCMC) algorithm is invoked to implement the approach. The algorithm shares some parallels with that used in the particle Kalman filter. The approach is validated against simulated as well as data generated by a physics code pertaining to the quark masses of protons. The value of our approach is that we are now able to answer questions that could not be readily answered using some standard approaches in particle physics. The structure of the Lattice QCD equations is not unique to physics. Such architectures also appear in mathematical biology, nuclear magnetic imaging, network analysis, ultracentrifuge, and a host of other relaxation and time decay phenomena. Thus, the methodology of this paper should have an appeal that transcends the Lattice QCD scenario which motivated us.

</details>

<details>

<summary>2012-01-05 22:02:07 - Some discussions of D. Fearnhead and D. Prangle's Read Paper "Constructing summary statistics for approximate Bayesian computation: semi-automatic approximate Bayesian computation"</summary>

- *Christophe Andrieu, Simon Barthelme, Nicolas Chopin, Julien Cornebise, Arnaud Doucet, Mark Girolami, Ioannis Kosmidis, Ajay Jasra, Anthony Lee, Jean-Michel Marin, Pierre Pudlo, Christian P. Robert, Mohammed Sedki., Sumeetpal S. Singh*

- `1201.1314v1` - [abs](http://arxiv.org/abs/1201.1314v1) - [pdf](http://arxiv.org/pdf/1201.1314v1)

> This report is a collection of comments on the Read Paper of Fearnhead and Prangle (2011), to appear in the Journal of the Royal Statistical Society Series B, along with a reply from the authors.

</details>

<details>

<summary>2012-01-08 20:28:42 - A Split-Merge MCMC Algorithm for the Hierarchical Dirichlet Process</summary>

- *Chong Wang, David M. Blei*

- `1201.1657v1` - [abs](http://arxiv.org/abs/1201.1657v1) - [pdf](http://arxiv.org/pdf/1201.1657v1)

> The hierarchical Dirichlet process (HDP) has become an important Bayesian nonparametric model for grouped data, such as document collections. The HDP is used to construct a flexible mixed-membership model where the number of components is determined by the data. As for most Bayesian nonparametric models, exact posterior inference is intractable---practitioners use Markov chain Monte Carlo (MCMC) or variational inference. Inspired by the split-merge MCMC algorithm for the Dirichlet process (DP) mixture model, we describe a novel split-merge MCMC sampling algorithm for posterior inference in the HDP. We study its properties on both synthetic data and text corpora. We find that split-merge MCMC for the HDP can provide significant improvements over traditional Gibbs sampling, and we give some understanding of the data properties that give rise to larger improvements.

</details>

<details>

<summary>2012-01-08 20:58:38 - Bayesian hierarchical modeling of simply connected 2D shapes</summary>

- *Kelvin Gu, Debdeep Pati, David B. Dunson*

- `1201.1658v1` - [abs](http://arxiv.org/abs/1201.1658v1) - [pdf](http://arxiv.org/pdf/1201.1658v1)

> Models for distributions of shapes contained within images can be widely used in biomedical applications ranging from tumor tracking for targeted radiation therapy to classifying cells in a blood sample. Our focus is on hierarchical probability models for the shape and size of simply connected 2D closed curves, avoiding the need to specify landmarks through modeling the entire curve while borrowing information across curves for related objects. Prevalent approaches follow a fundamentally different strategy in providing an initial point estimate of the curve and/or locations of landmarks, which are then fed into subsequent statistical analyses. Such two-stage methods ignore uncertainty in the first stage, and do not allow borrowing of information across objects in estimating object shapes and sizes. Our fully Bayesian hierarchical model is based on multiscale deformations within a linear combination of cyclic basis characterization, which facilitates automatic alignment of the different curves accounting for uncertainty. The characterization is shown to be highly flexible in representing 2D closed curves, leading to a nonparametric Bayesian prior with large support. Efficient Markov chain Monte Carlo methods are developed for simultaneous analysis of many objects. The methods are evaluated through simulation examples and applied to yeast cell imaging data.

</details>

<details>

<summary>2012-01-09 20:24:52 - Discussions on Fernhead and Prangle (2012)</summary>

- *D. J. Nott, Y. Fan, S. A. Sisson*

- `1201.1893v1` - [abs](http://arxiv.org/abs/1201.1893v1) - [pdf](http://arxiv.org/pdf/1201.1893v1)

> Two contributions to the discussion of Fearnhead P. and D. Prangle (2012). Constructing summary statistics for approximate Bayesian computation: Semi-automatic approx- imate Bayesian computation, J. Roy. Statist. Soc. B, 74 (3).

</details>

<details>

<summary>2012-01-10 19:10:14 - A Bias-reduced Estimator for the Mean of a Heavy-tailed Distribution with an Infinite Second Moment</summary>

- *Brahim Brahimi, Djamel Meraghni, Abdelhakim Necir, Djabrane Yahia*

- `1201.1578v2` - [abs](http://arxiv.org/abs/1201.1578v2) - [pdf](http://arxiv.org/pdf/1201.1578v2)

> We use bias-reduced estimators of high quantiles, of heavy-tailed distributions, to introduce a new estimator of the mean in the case of infinite second moment. The asymptotic normality of the proposed estimator is established and checked, in a simulation study, by four of the most popular goodness-of-fit tests for different sample sizes. Moreover, we compare, in terms of bias and mean squared error, our estimator with Peng's estimator (Peng, 2001) and we evaluate the accuracy of some resulting confidence intervals.

</details>

<details>

<summary>2012-01-17 06:41:45 - System Identification in Wireless Relay Networks via Gaussian Process</summary>

- *Gareth W. Peters, Ido Nevat, Jinhong Yuan, Ian B. Collings*

- `1106.3409v3` - [abs](http://arxiv.org/abs/1106.3409v3) - [pdf](http://arxiv.org/pdf/1106.3409v3)

> We present a flexible stochastic model for a class of cooperative wireless relay networks, in which the relay processing functionality is not known at the destination. In addressing this problem we develop efficient algorithms to perform relay identification in a wireless relay network. We first construct a statistical model based on a representation of the system using Gaussian Processes in a non-standard manner due to the way we treat the imperfect channel state information. We then formulate the estimation problem to perform system identification, taking into account complexity and computational efficiency. Next we develop a set of three algorithms to solve the identification problem each of decreasing complexity, trading-off the estimation bias for computational efficiency. The joint optimisation problem is tackled via a Bayesian framework using the Iterated Conditioning on the Modes methodology. We develop a lower bound and several sub-optimal computationally efficient solutions to the identification problem, for comparison. We illustrate the estimation performance of our methodology for a range of widely used relay functionalities. The relative total error attained by our algorithm when compared to the lower bound is found to be at worst 9% for low SNR values under all functions considered. The effect of the relay functional estimation error is also studied via BER simulations and is shown to be less than 2dB worse than the lower bound.

</details>

<details>

<summary>2012-01-17 15:18:25 - Path Following and Empirical Bayes Model Selection for Sparse Regression</summary>

- *Hua Zhou, Artin Armagan, David B. Dunson*

- `1201.3528v1` - [abs](http://arxiv.org/abs/1201.3528v1) - [pdf](http://arxiv.org/pdf/1201.3528v1)

> In recent years, a rich variety of regularization procedures have been proposed for high dimensional regression problems. However, tuning parameter choice and computational efficiency in ultra-high dimensional problems remain vexing issues. The routine use of $\ell_1$ regularization is largely attributable to the computational efficiency of the LARS algorithm, but similar efficiency for better behaved penalties has remained elusive. In this article, we propose a highly efficient path following procedure for combination of any convex loss function and a broad class of penalties. From a Bayesian perspective, this algorithm rapidly yields maximum a posteriori estimates at different hyper-parameter values. To bypass the inefficiency and potential instability of cross validation, we propose an empirical Bayes procedure for rapidly choosing the optimal model and corresponding hyper-parameter value. This approach applies to any penalty that corresponds to a proper prior distribution on the regression coefficients. While we mainly focus on sparse estimation of generalized linear models, the method extends to more general regularizations such as polynomial trend filtering after reparameterization. The proposed algorithm scales efficiently to large $p$ and/or $n$. Solution paths of 10,000 dimensional examples are computed within one minute on a laptop for various generalized linear models (GLM). Operating characteristics are assessed through simulation studies and the methods are applied to several real data sets.

</details>

<details>

<summary>2012-01-18 12:22:52 - Bayesian Parameter Inference for Partially Observed Stopped Processes</summary>

- *Ajay Jasra, Nikolas Kantas*

- `1201.3767v1` - [abs](http://arxiv.org/abs/1201.3767v1) - [pdf](http://arxiv.org/pdf/1201.3767v1)

> In this article we consider Bayesian parameter inference associated to partially-observed stochastic processes that start from a set B0 and are stopped or killed at the first hitting time of a known set A. Such processes occur naturally within the context of a wide variety of applications. The associated posterior distributions are highly complex and posterior parameter inference requires the use of advanced Markov chain Monte Carlo (MCMC) techniques. Our approach uses a recently introduced simulation methodology, particle Markov chain Monte Carlo (PMCMC) (Andrieu et. al. 2010 [1]), where sequential Monte Carlo (SMC) approximations (see Doucet et. al. 2001 [18] and Liu 2001 [27]) are embedded within MCMC. However, when the parameter of interest is fixed, standard SMC algorithms are not always appropriate for many stopped processes. In Chen et. al. [11] and Del Moral 2004 [15] the authors introduce SMC approximations of multi-level Feynman-Kac formulae, which can lead to more efficient algorithms. This is achieved by devising a sequence of nested sets from B0 to A and then perform the resampling step only when the samples of the process reach intermediate level sets in the sequence. Naturally, the choice of the intermediate level sets is critical to the performance of such a scheme. In this paper, we demonstrate that multi-level SMC algorithms can be used as a proposal in PMCMC. In addition, we propose a flexible strategy that adapts the level sets for different parameter proposals. Our methodology is illustrated on the coalescent model with migration.

</details>

<details>

<summary>2012-01-19 17:31:35 - Sines, steps and droplets: Semiparametric Bayesian modeling of arrival time series</summary>

- *Thomas J. Loredo*

- `1201.4114v1` - [abs](http://arxiv.org/abs/1201.4114v1) - [pdf](http://arxiv.org/pdf/1201.4114v1)

> I describe ongoing work developing Bayesian methods for flexible modeling of arrival time series data without binning, aiming to improve detection and measurement of X-ray and gamma-ray pulsars, and of pulses in gamma-ray bursts. The methods use parametric and semiparametric Poisson point process models for the event rate, and by design have close connections to conventional frequentist methods currently used in time-domain astronomy.

</details>

<details>

<summary>2012-01-20 21:44:14 - Locally Adaptive Bayes Nonparametric Regression via Nested Gaussian Processes</summary>

- *Bin Zhu, David B. Dunson*

- `1201.4403v1` - [abs](http://arxiv.org/abs/1201.4403v1) - [pdf](http://arxiv.org/pdf/1201.4403v1)

> We propose a nested Gaussian process (nGP) as a locally adaptive prior for Bayesian nonparametric regression. Specified through a set of stochastic differential equations (SDEs), the nGP imposes a Gaussian process prior for the function's $m$th-order derivative. The nesting comes in through including a local instantaneous mean function, which is drawn from another Gaussian process inducing adaptivity to locally-varying smoothness. We discuss the support of the nGP prior in terms of the closure of a reproducing kernel Hilbert space, and consider theoretical properties of the posterior. The posterior mean under the nGP prior is shown to be equivalent to the minimizer of a nested penalized sum-of-squares involving penalties for both the global and local roughness of the function. Using highly-efficient Markov chain Monte Carlo for posterior inference, the proposed method performs well in simulation studies compared to several alternatives, and is scalable to massive data, illustrated through a proteomics application.

</details>

<details>

<summary>2012-01-22 06:05:29 - Inference for a Class of Partially Observed Point Process Models</summary>

- *James S. Martin, Ajay Jasra, Emma McCoy*

- `1201.4529v1` - [abs](http://arxiv.org/abs/1201.4529v1) - [pdf](http://arxiv.org/pdf/1201.4529v1)

> This paper presents a simulation-based framework for sequential inference from partially and discretely observed point process (PP's) models with static parameters. Taking on a Bayesian perspective for the static parameters, we build upon sequential Monte Carlo (SMC) methods, investigating the problems of performing sequential filtering and smoothing in complex examples, where current methods often fail. We consider various approaches for approximating posterior distributions using SMC. Our approaches, with some theoretical discussion are illustrated on a doubly stochastic point process applied in the context of finance.

</details>

<details>

<summary>2012-01-23 14:16:44 - Tutorial on Exact Belief Propagation in Bayesian Networks: from Messages to Algorithms</summary>

- *G. Nuel*

- `1201.4724v1` - [abs](http://arxiv.org/abs/1201.4724v1) - [pdf](http://arxiv.org/pdf/1201.4724v1)

> In Bayesian networks, exact belief propagation is achieved through message passing algorithms. These algorithms (ex: inward and outward) provide only a recursive definition of the corresponding messages. In contrast, when working on hidden Markov models and variants, one classically first defines explicitly these messages (forward and backward quantities), and then derive all results and algorithms. In this paper, we generalize the hidden Markov model approach by introducing an explicit definition of the messages in Bayesian networks, from which we derive all the relevant properties and results including the recursive algorithms that allow to compute these messages. Two didactic examples (the precipitation hidden Markov model and the pedigree Bayesian network) are considered along the paper to illustrate the new formalism and standalone R source code is provided in the appendix.

</details>

<details>

<summary>2012-01-25 03:16:04 - On the Computational Complexity of MCMC-based Estimators in Large Samples</summary>

- *Alexandre Belloni, Victor Chernozhukov*

- `0704.2167v3` - [abs](http://arxiv.org/abs/0704.2167v3) - [pdf](http://arxiv.org/pdf/0704.2167v3)

> In this paper we examine the implications of the statistical large sample theory for the computational complexity of Bayesian and quasi-Bayesian estimation carried out using Metropolis random walks. Our analysis is motivated by the Laplace-Bernstein-Von Mises central limit theorem, which states that in large samples the posterior or quasi-posterior approaches a normal density. Using the conditions required for the central limit theorem to hold, we establish polynomial bounds on the computational complexity of general Metropolis random walks methods in large samples. Our analysis covers cases where the underlying log-likelihood or extremum criterion function is possibly non-concave, discontinuous, and with increasing parameter dimension. However, the central limit theorem restricts the deviations from continuity and log-concavity of the log-likelihood or extremum criterion function in a very specific manner.   Under minimal assumptions required for the central limit theorem to hold under the increasing parameter dimension, we show that the Metropolis algorithm is theoretically efficient even for the canonical Gaussian walk which is studied in detail. Specifically, we show that the running time of the algorithm in large samples is bounded in probability by a polynomial in the parameter dimension $d$, and, in particular, is of stochastic order $d^2$ in the leading cases after the burn-in period. We then give applications to exponential families, curved exponential families, and Z-estimation of increasing dimension.

</details>

<details>

<summary>2012-01-26 13:59:10 - Some uniform in bandwidth functional results for the tail uniform empirical and quantile processes</summary>

- *Davit Varron*

- `1201.5517v1` - [abs](http://arxiv.org/abs/1201.5517v1) - [pdf](http://arxiv.org/pdf/1201.5517v1)

> For fixed $t\in [0,1)$ and $h>0$, consider the local uniform empirical process $$\DD_{n,h,t}(s):=n^{-1/2}\coo\sliin 1_{[t,t+hs]}(U_i)-hs\cff,\;s\in [0,1],$$ where the $U_i$ are independent and uniformly distributed on $[0,1]$. We investigate the functional limit behaviour of $\DD_{n,h,t}$ uniformly in $\wth_n\le h\le h_n$ when $n\wth_n/\log\log n\rar \infty$ and $h_n\rar 0$.

</details>

<details>

<summary>2012-01-26 14:08:52 - Clustering rates and Chung type functional laws of the iterated logarithm for empirical and quantile processes</summary>

- *Davit Varron*

- `1201.5521v1` - [abs](http://arxiv.org/abs/1201.5521v1) - [pdf](http://arxiv.org/pdf/1201.5521v1)

> Following the works of Berthet (1997), we first obtain exact clustering rates in the functional law of the iterated logarithm for the uniform empirical and quantile processes and for their increments. In a second time, we obtain functional Chung-type limit laws for the local empirical process for a class of target functions on the border of the Strassen set.

</details>

<details>

<summary>2012-01-26 15:43:08 - Modelling the effects of air pollution on health using Bayesian Dynamic Generalised Linear Models</summary>

- *Duncan Lee, Gavin Shaddick*

- `0710.3473v2` - [abs](http://arxiv.org/abs/0710.3473v2) - [pdf](http://arxiv.org/pdf/0710.3473v2)

> The relationship between short-term exposure to air pollution and mortality or morbidity has been the subject of much recent research, in which the standard method of analysis uses Poisson linear or additive models. In this paper we use a Bayesian dynamic generalised linear model (DGLM) to estimate this relationship, which allows the standard linear or additive model to be extended in two ways: (i) the long-term trend and temporal correlation present in the health data can be modelled by an autoregressive process rather than a smooth function of calendar time; (ii) the effects of air pollution are allowed to evolve over time. The efficacy of these two extensions are investigated by applying a series of dynamic and non-dynamic models to air pollution and mortality data from Greater London. A Bayesian approach is taken throughout, and a Markov chain monte carlo simulation algorithm is presented for inference. An alternative likelihood based analysis is also presented, in order to allow a direct comparison with the only previous analysis of air pollution and health data using a DGLM.

</details>

<details>

<summary>2012-01-26 16:20:05 - Dynamic trees for streaming and massive data contexts</summary>

- *Christoforos Anagnostopoulos, Robert B. Gramacy*

- `1201.5568v1` - [abs](http://arxiv.org/abs/1201.5568v1) - [pdf](http://arxiv.org/pdf/1201.5568v1)

> Data collection at a massive scale is becoming ubiquitous in a wide variety of settings, from vast offline databases to streaming real-time information. Learning algorithms deployed in such contexts must rely on single-pass inference, where the data history is never revisited. In streaming contexts, learning must also be temporally adaptive to remain up-to-date against unforeseen changes in the data generating mechanism. Although rapidly growing, the online Bayesian inference literature remains challenged by massive data and transient, evolving data streams. Non-parametric modelling techniques can prove particularly ill-suited, as the complexity of the model is allowed to increase with the sample size. In this work, we take steps to overcome these challenges by porting standard streaming techniques, like data discarding and downweighting, into a fully Bayesian framework via the use of informative priors and active learning heuristics. We showcase our methods by augmenting a modern non-parametric modelling framework, dynamic trees, and illustrate its performance on a number of practical examples. The end product is a powerful streaming regression and classification tool, whose performance compares favourably to the state-of-the-art.

</details>

<details>

<summary>2012-01-27 18:55:03 - SMC^2: an efficient algorithm for sequential analysis of state-space models</summary>

- *Nicolas Chopin, Pierre E. Jacob, Omiros Papaspiliopoulos*

- `1101.1528v3` - [abs](http://arxiv.org/abs/1101.1528v3) - [pdf](http://arxiv.org/pdf/1101.1528v3)

> We consider the generic problem of performing sequential Bayesian inference in a state-space model with observation process y, state process x and fixed parameter theta. An idealized approach would be to apply the iterated batch importance sampling (IBIS) algorithm of Chopin (2002). This is a sequential Monte Carlo algorithm in the theta-dimension, that samples values of theta, reweights iteratively these values using the likelihood increments p(y_t|y_1:t-1, theta), and rejuvenates the theta-particles through a resampling step and a MCMC update step. In state-space models these likelihood increments are intractable in most cases, but they may be unbiasedly estimated by a particle filter in the x-dimension, for any fixed theta. This motivates the SMC^2 algorithm proposed in this article: a sequential Monte Carlo algorithm, defined in the theta-dimension, which propagates and resamples many particle filters in the x-dimension. The filters in the x-dimension are an example of the random weight particle filter as in Fearnhead et al. (2010). On the other hand, the particle Markov chain Monte Carlo (PMCMC) framework developed in Andrieu et al. (2010) allows us to design appropriate MCMC rejuvenation steps. Thus, the theta-particles target the correct posterior distribution at each iteration t, despite the intractability of the likelihood increments. We explore the applicability of our algorithm in both sequential and non-sequential applications and consider various degrees of freedom, as for example increasing dynamically the number of x-particles. We contrast our approach to various competing methods, both conceptually and empirically through a detailed simulation study, included here and in a supplement, and based on particularly challenging examples.

</details>

<details>

<summary>2012-01-30 10:38:22 - On the equivalence between standard and sequentially ordered hidden Markov models</summary>

- *Nicolas Chopin*

- `1201.6161v1` - [abs](http://arxiv.org/abs/1201.6161v1) - [pdf](http://arxiv.org/pdf/1201.6161v1)

> Chopin (2007) introduced a sequentially ordered hidden Markov model, for which states are ordered according to their order of appearance, and claimed that such a model is a re-parametrisation of a standard Markov model. This note gives a formal proof that this equivalence holds in Bayesian terms, as both formulations generate equivalent posterior distributions, but does not hold in Frequentist terms, as both formulations generate incompatible likelihood functions. Perhaps surprisingly, this shows that Bayesian re-parametrisation and Frequentist re-parametrisation are not identical concepts.

</details>

<details>

<summary>2012-01-31 03:35:43 - On principles of inductive inference</summary>

- *Ryszard Paweł Kostecki*

- `1109.3142v4` - [abs](http://arxiv.org/abs/1109.3142v4) - [pdf](http://arxiv.org/pdf/1109.3142v4)

> We propose an intersubjective epistemic approach to foundations of probability theory and statistical inference, based on relative entropy and category theory, and aimed to bypass the mathematical and conceptual problems of existing foundational approaches.

</details>


## 2012-02

<details>

<summary>2012-02-01 01:55:55 - A Class Coupler for Perfect Sampling from Continuous Distributions With and Without Atoms</summary>

- *Wenjin Mao, Jem Corcoran*

- `1202.0078v1` - [abs](http://arxiv.org/abs/1202.0078v1) - [pdf](http://arxiv.org/pdf/1202.0078v1)

> We consider the simulation of distributions that are a mixture of discrete and continuous components. We extend a Metropolis-Hastings-based perfect sampling algorithm of Corcoran and Tweedie to allow for a broader class of transition candidate densities. The resulting algorithm, know as a "class coupler", is fast to implement and is applicable to purely discrete or purely continuous densities as well. Our work is motivated by the study of a composite hypothesis test in a Bayesian setting via posterior simulation and we give simulation results for some problems in this area.

</details>

<details>

<summary>2012-02-02 09:34:19 - Parametric or nonparametric? A parametricness index for model selection</summary>

- *Wei Liu, Yuhong Yang*

- `1202.0391v1` - [abs](http://arxiv.org/abs/1202.0391v1) - [pdf](http://arxiv.org/pdf/1202.0391v1)

> In model selection literature, two classes of criteria perform well asymptotically in different situations: Bayesian information criterion (BIC) (as a representative) is consistent in selection when the true model is finite dimensional (parametric scenario); Akaike's information criterion (AIC) performs well in an asymptotic efficiency when the true model is infinite dimensional (nonparametric scenario). But there is little work that addresses if it is possible and how to detect the situation that a specific model selection problem is in. In this work, we differentiate the two scenarios theoretically under some conditions. We develop a measure, parametricness index (PI), to assess whether a model selected by a potentially consistent procedure can be practically treated as the true model, which also hints on AIC or BIC is better suited for the data for the goal of estimating the regression function. A consequence is that by switching between AIC and BIC based on the PI, the resulting regression estimator is simultaneously asymptotically efficient for both parametric and nonparametric scenarios. In addition, we systematically investigate the behaviors of PI in simulation and real data and show its usefulness.

</details>

<details>

<summary>2012-02-02 17:31:34 - Consistency of Bayesian Linear Model Selection With a Growing Number of Parameters</summary>

- *Zuofeng Shang, Murray K. Clayton*

- `1102.0826v2` - [abs](http://arxiv.org/abs/1102.0826v2) - [pdf](http://arxiv.org/pdf/1102.0826v2)

> Linear models with a growing number of parameters have been widely used in modern statistics. One important problem about this kind of model is the variable selection issue. Bayesian approaches, which provide a stochastic search of informative variables, have gained popularity. In this paper, we will study the asymptotic properties related to Bayesian model selection when the model dimension $p$ is growing with the sample size $n$. We consider $p\le n$ and provide sufficient conditions under which: (1) with large probability, the posterior probability of the true model (from which samples are drawn) uniformly dominates the posterior probability of any incorrect models; and (2) with large probability, the posterior probability of the true model converges to one. Both (1) and (2) guarantee that the true model will be selected under a Bayesian framework. We also demonstrate several situations when (1) holds but (2) fails, which illustrates the difference between these two properties. Simulated examples are provided to illustrate the main results.

</details>

<details>

<summary>2012-02-02 19:10:19 - An Application of Bayesian Variable Selection to Spatial Concurrent Linear Models</summary>

- *Zuofeng Shang, Murray K. Clayton*

- `1202.0517v1` - [abs](http://arxiv.org/abs/1202.0517v1) - [pdf](http://arxiv.org/pdf/1202.0517v1)

> Spatial concurrent linear models, in which the model coefficients are spatial processes varying at a local level, are flexible and useful tools for analyzing spatial data. One approach places stationary Gaussian process priors on the spatial processes, but in applications the data may display strong nonstationary patterns. In this article, we propose a Bayesian variable selection approach based on wavelet tools to address this problem. The proposed approach does not involve any stationarity assumptions on the priors, and instead we impose a mixture prior directly on each wavelet coefficient. We introduce an option to control the priors such that high resolution coefficients are more likely to be zero. Computationally efficient MCMC procedures are provided to address posterior sampling, and uncertainty in the estimation is assessed through posterior means and standard deviations. Examples based on simulated data demonstrate the estimation accuracy and advantages of the proposed method. We also illustrate the performance of the proposed method for real data obtained through remote sensing.

</details>

<details>

<summary>2012-02-03 09:16:16 - Frasian Inference</summary>

- *Larry Wasserman*

- `1202.0633v1` - [abs](http://arxiv.org/abs/1202.0633v1) - [pdf](http://arxiv.org/pdf/1202.0633v1)

> Don Fraser has given an interesting account of the agreements and disagreements between Bayesian posterior probabilities and confidence levels. In this comment I discuss some cases where the lack of such agreement is extreme. I then discuss a few cases where it is possible to have Bayes procedures with frequentist validity. Such frequentist-Bayesian---or Frasian---methods deserve more attention [arXiv:1112.5582].

</details>

<details>

<summary>2012-02-03 14:29:05 - Improving the Convergence Properties of the Data Augmentation Algorithm with an Application to Bayesian Mixture Modeling</summary>

- *James P. Hobert, Vivekananda Roy, Christian P. Robert*

- `0911.4546v3` - [abs](http://arxiv.org/abs/0911.4546v3) - [pdf](http://arxiv.org/pdf/0911.4546v3)

> The reversible Markov chains that drive the data augmentation (DA) and sandwich algorithms define self-adjoint operators whose spectra encode the convergence properties of the algorithms. When the target distribution has uncountable support, as is nearly always the case in practice, it is generally quite difficult to get a handle on these spectra. We show that, if the augmentation space is finite, then (under regularity conditions) the operators defined by the DA and sandwich chains are compact, and the spectra are finite subsets of $[0,1)$. Moreover, we prove that the spectrum of the sandwich operator dominates the spectrum of the DA operator in the sense that the ordered elements of the former are all less than or equal to the corresponding elements of the latter. As a concrete example, we study a widely used DA algorithm for the exploration of posterior densities associated with Bayesian mixture models [J. Roy. Statist. Soc. Ser. B 56 (1994) 363--375]. In particular, we compare this mixture DA algorithm with an alternative algorithm proposed by Fr\"{u}hwirth-Schnatter [J. Amer. Statist. Assoc. 96 (2001) 194--209] that is based on random label switching.

</details>

<details>

<summary>2012-02-04 16:43:41 - Beta-Negative Binomial Process and Poisson Factor Analysis</summary>

- *Mingyuan Zhou, Lauren Hannah, David Dunson, Lawrence Carin*

- `1112.3605v4` - [abs](http://arxiv.org/abs/1112.3605v4) - [pdf](http://arxiv.org/pdf/1112.3605v4)

> A beta-negative binomial (BNB) process is proposed, leading to a beta-gamma-Poisson process, which may be viewed as a "multi-scoop" generalization of the beta-Bernoulli process. The BNB process is augmented into a beta-gamma-gamma-Poisson hierarchical structure, and applied as a nonparametric Bayesian prior for an infinite Poisson factor analysis model. A finite approximation for the beta process Levy random measure is constructed for convenient implementation. Efficient MCMC computations are performed with data augmentation and marginalization techniques. Encouraging results are shown on document count matrix factorization.

</details>

<details>

<summary>2012-02-05 10:49:02 - Bayesian filtering for multi-object systems with independently generated observations</summary>

- *Daniel Edward Clark*

- `1202.0949v1` - [abs](http://arxiv.org/abs/1202.0949v1) - [pdf](http://arxiv.org/pdf/1202.0949v1)

> A general approach for Bayesian filtering of multi-object systems is studied, with particular emphasis on the model where each object generates observations independently of other objects. The approach is based on variational calculus applied to generating functionals, using the general version of Faa di Bruno's formula for Gateaux differentials. This result enables us to determine some general formulae for the updated generating functional after the application of a multi-object analogue of Bayes' rule.

</details>

<details>

<summary>2012-02-05 12:36:21 - Estimating a bivariate linear relationship</summary>

- *David Leonard*

- `1202.0957v1` - [abs](http://arxiv.org/abs/1202.0957v1) - [pdf](http://arxiv.org/pdf/1202.0957v1)

> Solutions of the bivariate, linear errors-in-variables estimation problem with unspecified errors are expected to be invariant under interchange and scaling of the coordinates. The appealing model of normally distributed true values and errors is unidentified without additional information. I propose a prior density that incorporates the fact that the slope and variance parameters together determine the covariance matrix of the unobserved true values but is otherwise diffuse. The marginal posterior density of the slope is invariant to interchange and scaling of the coordinates and depends on the data only through the sample correlation coefficient and ratio of standard deviations. It covers the interval between the two ordinary least squares estimates but diminishes rapidly outside of it. I introduce the R package leiv for computing the posterior density, and I apply it to examples in astronomy and method comparison.

</details>

<details>

<summary>2012-02-08 13:52:05 - Bayesian Inference of Whole-Brain Networks</summary>

- *M. Hinne, T. Heskes, M. A. J. van Gerven*

- `1202.1696v1` - [abs](http://arxiv.org/abs/1202.1696v1) - [pdf](http://arxiv.org/pdf/1202.1696v1)

> In structural brain networks the connections of interest consist of white-matter fibre bundles between spatially segregated brain regions. The presence, location and orientation of these white matter tracts can be derived using diffusion MRI in combination with probabilistic tractography. Unfortunately, as of yet no approaches have been suggested that provide an undisputed way of inferring brain networks from tractography. In this paper, we provide a computational framework which we refer to as Bayesian connectomics. Rather than applying an arbitrary threshold to obtain a single network, we consider the posterior distribution of networks that are supported by the data, combined with an exponential random graph (ERGM) prior that captures a priori knowledge concerning the graph-theoretical properties of whole-brain networks. We show that, on simulated probabilistic tractography data, our approach is able to reconstruct whole-brain networks. In addition, our approach directly supports multi-model data fusion and group-level network inference.

</details>

<details>

<summary>2012-02-09 14:58:47 - Detecting regime switches in the dependence structure of high dimensional financial data</summary>

- *Jakob Stoeber, Claudia Czado*

- `1202.2009v1` - [abs](http://arxiv.org/abs/1202.2009v1) - [pdf](http://arxiv.org/pdf/1202.2009v1)

> Misperceptions about extreme dependencies between different financial assets have been an im- portant element of the recent financial crisis. This paper studies inhomogeneity in dependence structures using Markov switching regular vine copulas. These account for asymmetric depen- dencies and tail dependencies in high dimensional data. We develop methods for fast maximum likelihood as well as Bayesian inference. Our algorithms are validated in simulations and applied to financial data. We find that regime switches are present in the dependence structure of various data sets and show that regime switching models could provide tools for the accurate description of inhomogeneity during times of crisis.

</details>

<details>

<summary>2012-02-09 22:31:01 - Active Bayesian Optimization: Minimizing Minimizer Entropy</summary>

- *Il Memming Park, Marcel Nassar, Mijung Park*

- `1202.2143v1` - [abs](http://arxiv.org/abs/1202.2143v1) - [pdf](http://arxiv.org/pdf/1202.2143v1)

> The ultimate goal of optimization is to find the minimizer of a target function.However, typical criteria for active optimization often ignore the uncertainty about the minimizer. We propose a novel criterion for global optimization and an associated sequential active learning strategy using Gaussian processes.Our criterion is the reduction of uncertainty in the posterior distribution of the function minimizer. It can also flexibly incorporate multiple global minimizers. We implement a tractable approximation of the criterion and demonstrate that it obtains the global minimizer accurately compared to conventional Bayesian optimization criteria.

</details>

<details>

<summary>2012-02-13 14:43:18 - An approximate Bayesian marginal likelihood approach for estimating finite mixtures</summary>

- *Ryan Martin*

- `1106.4432v4` - [abs](http://arxiv.org/abs/1106.4432v4) - [pdf](http://arxiv.org/pdf/1106.4432v4)

> Estimation of finite mixture models when the mixing distribution support is unknown is an important problem. This paper gives a new approach based on a marginal likelihood for the unknown support. Motivated by a Bayesian Dirichlet prior model, a computationally efficient stochastic approximation version of the marginal likelihood is proposed and large-sample theory is presented. By restricting the support to a finite grid, a simulated annealing method is employed to maximize the marginal likelihood and estimate the support. Real and simulated data examples show that this novel stochastic approximation--simulated annealing procedure compares favorably to existing methods.

</details>

<details>

<summary>2012-02-14 16:41:17 - Near-Optimal Target Learning With Stochastic Binary Signals</summary>

- *Mithun Chakraborty, Sanmay Das, Malik Magdon-Ismail*

- `1202.3704v1` - [abs](http://arxiv.org/abs/1202.3704v1) - [pdf](http://arxiv.org/pdf/1202.3704v1)

> We study learning in a noisy bisection model: specifically, Bayesian algorithms to learn a target value V given access only to noisy realizations of whether V is less than or greater than a threshold theta. At step t = 0, 1, 2, ..., the learner sets threshold theta t and observes a noisy realization of sign(V - theta t). After T steps, the goal is to output an estimate V^ which is within an eta-tolerance of V . This problem has been studied, predominantly in environments with a fixed error probability q < 1/2 for the noisy realization of sign(V - theta t). In practice, it is often the case that q can approach 1/2, especially as theta -> V, and there is little known when this happens. We give a pseudo-Bayesian algorithm which provably converges to V. When the true prior matches our algorithm's Gaussian prior, we show near-optimal expected performance. Our methods extend to the general multiple-threshold setting where the observation noisily indicates which of k >= 2 regions V belongs to.

</details>

<details>

<summary>2012-02-14 16:41:17 - PAC-Bayesian Policy Evaluation for Reinforcement Learning</summary>

- *Mahdi MIlani Fard, Joelle Pineau, Csaba Szepesvari*

- `1202.3717v1` - [abs](http://arxiv.org/abs/1202.3717v1) - [pdf](http://arxiv.org/pdf/1202.3717v1)

> Bayesian priors offer a compact yet general means of incorporating domain knowledge into many learning tasks. The correctness of the Bayesian analysis and inference, however, largely depends on accuracy and correctness of these priors. PAC-Bayesian methods overcome this problem by providing bounds that hold regardless of the correctness of the prior distribution. This paper introduces the first PAC-Bayesian bound for the batch reinforcement learning problem with function approximation. We show how this bound can be used to perform model-selection in a transfer learning scenario. Our empirical results confirm that PAC-Bayesian policy evaluation is able to leverage prior distributions when they are informative and, unlike standard Bayesian RL approaches, ignore them when they are misleading.

</details>

<details>

<summary>2012-02-14 16:41:17 - Partial Order MCMC for Structure Discovery in Bayesian Networks</summary>

- *Teppo Niinimaki, Pekka Parviainen, Mikko Koivisto*

- `1202.3753v1` - [abs](http://arxiv.org/abs/1202.3753v1) - [pdf](http://arxiv.org/pdf/1202.3753v1)

> We present a new Markov chain Monte Carlo method for estimating posterior probabilities of structural features in Bayesian networks. The method draws samples from the posterior distribution of partial orders on the nodes; for each sampled partial order, the conditional probabilities of interest are computed exactly. We give both analytical and empirical results that suggest the superiority of the new method compared to previous methods, which sample either directed acyclic graphs or linear orders on the nodes.

</details>

<details>

<summary>2012-02-14 16:41:17 - Fast MCMC sampling for Markov jump processes and continuous time Bayesian networks</summary>

- *Vinayak Rao, Yee Whye Teh*

- `1202.3760v1` - [abs](http://arxiv.org/abs/1202.3760v1) - [pdf](http://arxiv.org/pdf/1202.3760v1)

> Markov jump processes and continuous time Bayesian networks are important classes of continuous time dynamical systems. In this paper, we tackle the problem of inferring unobserved paths in these models by introducing a fast auxiliary variable Gibbs sampler. Our approach is based on the idea of uniformization, and sets up a Markov chain over paths by sampling a finite set of virtual jump times and then running a standard hidden Markov model forward filtering-backward sampling algorithm over states at the set of extant and virtual jump times. We demonstrate significant computational benefits over a state-of-the-art Gibbs sampler on a number of continuous time Bayesian networks.

</details>

<details>

<summary>2012-02-14 16:41:17 - Robust learning Bayesian networks for prior belief</summary>

- *Maomi Ueno*

- `1202.3766v1` - [abs](http://arxiv.org/abs/1202.3766v1) - [pdf](http://arxiv.org/pdf/1202.3766v1)

> Recent reports have described that learning Bayesian networks are highly sensitive to the chosen equivalent sample size (ESS) in the Bayesian Dirichlet equivalence uniform (BDeu). This sensitivity often engenders some unstable or undesirable results. This paper describes some asymptotic analyses of BDeu to explain the reasons for the sensitivity and its effects. Furthermore, this paper presents a proposal for a robust learning score for ESS by eliminating the sensitive factors from the approximation of log-BDeu.

</details>

<details>

<summary>2012-02-14 16:41:17 - Kernel-based Conditional Independence Test and Application in Causal Discovery</summary>

- *Kun Zhang, Jonas Peters, Dominik Janzing, Bernhard Schoelkopf*

- `1202.3775v1` - [abs](http://arxiv.org/abs/1202.3775v1) - [pdf](http://arxiv.org/pdf/1202.3775v1)

> Conditional independence testing is an important problem, especially in Bayesian network learning and causal discovery. Due to the curse of dimensionality, testing for conditional independence of continuous variables is particularly challenging. We propose a Kernel-based Conditional Independence test (KCI-test), by constructing an appropriate test statistic and deriving its asymptotic distribution under the null hypothesis of conditional independence. The proposed method is computationally efficient and easy to implement. Experimental results show that it outperforms other methods, especially when the conditioning set is large or the sample size is not very large, in which case other methods encounter difficulties.

</details>

<details>

<summary>2012-02-15 21:56:08 - A Bayesian View of the Poisson-Dirichlet Process</summary>

- *Wray Buntine, Marcus Hutter*

- `1007.0296v2` - [abs](http://arxiv.org/abs/1007.0296v2) - [pdf](http://arxiv.org/pdf/1007.0296v2)

> The two parameter Poisson-Dirichlet Process (PDP), a generalisation of the Dirichlet Process, is increasingly being used for probabilistic modelling in discrete areas such as language technology, bioinformatics, and image analysis. There is a rich literature about the PDP and its derivative distributions such as the Chinese Restaurant Process (CRP). This article reviews some of the basic theory and then the major results needed for Bayesian modelling of discrete problems including details of priors, posteriors and computation.   The PDP allows one to build distributions over countable partitions. The PDP has two other remarkable properties: first it is partially conjugate to itself, which allows one to build hierarchies of PDPs, and second using a marginalised relative the CRP, one gets fragmentation and clustering properties that lets one layer partitions to build trees. This article presents the basic theory for understanding the notion of partitions and distributions over them, the PDP and the CRP, and the important properties of conjugacy, fragmentation and clustering, as well as some key related properties such as consistency and convergence. This article also presents a Bayesian interpretation of the Poisson-Dirichlet process based on an improper and infinite dimensional Dirichlet distribution. This means we can understand the process as just another Dirichlet and thus all its sampling properties emerge naturally.   The theory of PDPs is usually presented for continuous distributions (more generally referred to as non-atomic distributions), however, when applied to discrete distributions its remarkable conjugacy property emerges. This context and basic results are also presented, as well as techniques for computing the second order Stirling numbers that occur in the posteriors for discrete distributions.

</details>

<details>

<summary>2012-02-17 16:38:02 - Multivariate probabilistic forecasting using Bayesian model averaging and copulas</summary>

- *Annette Möller, Alex Lenkoski, Thordis L. Thorarinsdottir*

- `1202.3956v1` - [abs](http://arxiv.org/abs/1202.3956v1) - [pdf](http://arxiv.org/pdf/1202.3956v1)

> We propose a method for post-processing an ensemble of multivariate forecasts in order to obtain a joint predictive distribution of weather. Our method utilizes existing univariate post-processing techniques, in this case ensemble Bayesian model averaging (BMA), to obtain estimated marginal distributions. However, implementing these methods individually offers no information regarding the joint distribution. To correct this, we propose the use of a Gaussian copula, which offers a simple procedure for recovering the dependence that is lost in the estimation of the ensemble BMA marginals. Our method is applied to 48-h forecasts of a set of five weather quantities using the 8-member University of Washington mesoscale ensemble. We show that our method recovers many well-understood dependencies between weather quantities and subsequently improves calibration and sharpness over both the raw ensemble and a method which does not incorporate joint distributional information.

</details>

<details>

<summary>2012-02-17 17:04:58 - BAMBI: blind accelerated multimodal Bayesian inference</summary>

- *Philip Graff, Farhan Feroz, Michael P. Hobson, Anthony Lasenby*

- `1110.2997v2` - [abs](http://arxiv.org/abs/1110.2997v2) - [pdf](http://arxiv.org/pdf/1110.2997v2)

> In this paper we present an algorithm for rapid Bayesian analysis that combines the benefits of nested sampling and artificial neural networks. The blind accelerated multimodal Bayesian inference (BAMBI) algorithm implements the MultiNest package for nested sampling as well as the training of an artificial neural network (NN) to learn the likelihood function. In the case of computationally expensive likelihoods, this allows the substitution of a much more rapid approximation in order to increase significantly the speed of the analysis. We begin by demonstrating, with a few toy examples, the ability of a NN to learn complicated likelihood surfaces. BAMBI's ability to decrease running time for Bayesian inference is then demonstrated in the context of estimating cosmological parameters from Wilkinson Microwave Anisotropy Probe and other observations. We show that valuable speed increases are achieved in addition to obtaining NNs trained on the likelihood functions for the different model and data combinations. These NNs can then be used for an even faster follow-up analysis using the same likelihood and different priors. This is a fully general algorithm that can be applied, without any pre-processing, to other problems with computationally expensive likelihood functions.

</details>

<details>

<summary>2012-02-18 12:33:27 - Bayesian inference through encompassing priors and importance sampling for a class of marginal models for categorical data</summary>

- *Francesco Bartolucci, Luisa Scaccia, Alessio Farcomeni*

- `1202.4074v1` - [abs](http://arxiv.org/abs/1202.4074v1) - [pdf](http://arxiv.org/pdf/1202.4074v1)

> We develop a Bayesian approach for selecting the model which is the most supported by the data within a class of marginal models for categorical variables formulated through equality and/or inequality constraints on generalised logits (local, global, continuation or reverse continuation), generalised log-odds ratios and similar higher-order interactions. For each constrained model, the prior distribution of the model parameters is formulated following the encompassing prior approach. Then, model selection is performed by using Bayes factors which are estimated by an importance sampling method. The approach is illustrated through three applications involving some datasets, which also include explanatory variables. In connection with one of these examples, a sensitivity analysis to the prior specification is also considered.

</details>

<details>

<summary>2012-02-18 19:01:56 - A Level-Set Hit-and-Run Sampler for Quasi-Concave Distributions</summary>

- *Dean Foster, Shane T. Jensen*

- `1202.4094v1` - [abs](http://arxiv.org/abs/1202.4094v1) - [pdf](http://arxiv.org/pdf/1202.4094v1)

> We develop a new sampling strategy that uses the hit-and-run algorithm within level sets of the target density. Our method can be applied to any quasi-concave density, which covers a broad class of models. Our sampler performs well in high-dimensional settings, which we illustrate with a comparison to Gibbs sampling on a spike-and-slab mixture model. We also extend our method to exponentially-tilted quasi-concave densities, which arise often in Bayesian models consisting of a log-concave likelihood and quasi-concave prior density. Within this class of models, our method is effective at sampling from posterior distributions with high dependence between parameters, which we illustrate with a simple multivariate normal example. We also implement our level-set sampler on a Cauchy-normal model where we demonstrate the ability of our level set sampler to handle multi-modal posterior distributions.

</details>

<details>

<summary>2012-02-22 09:09:13 - Bayesian semi-parametric estimation of the long-memory parameter under FEXP-priors</summary>

- *Willem Kruijer, Judith Rousseau*

- `1202.4863v1` - [abs](http://arxiv.org/abs/1202.4863v1) - [pdf](http://arxiv.org/pdf/1202.4863v1)

> For a Gaussian time series with long-memory behavior, we use the FEXP-model for semi-parametric estimation of the long-memory parameter $d$. The true spectral density $f_o$ is assumed to have long-memory parameter $d_o$ and a FEXP-expansion of Sobolev-regularity $\be > 1$. We prove that when $k$ follows a Poisson or geometric prior, or a sieve prior increasing at rate $n^{\frac{1}{1+2\be}}$, $d$ converges to $d_o$ at a suboptimal rate. When the sieve prior increases at rate $n^{\frac{1}{2\be}}$ however, the minimax rate is almost obtained. Our results can be seen as a Bayesian equivalent of the result which Moulines and Soulier obtained for some frequentist estimators.

</details>

<details>

<summary>2012-02-22 14:04:08 - Estimating Discrete Markov Models From Various Incomplete Data Schemes</summary>

- *Alberto Pasanisi, Shuai Fu, Nicolas Bousquet*

- `1009.1216v2` - [abs](http://arxiv.org/abs/1009.1216v2) - [pdf](http://arxiv.org/pdf/1009.1216v2)

> The parameters of a discrete stationary Markov model are transition probabilities between states. Traditionally, data consist in sequences of observed states for a given number of individuals over the whole observation period. In such a case, the estimation of transition probabilities is straightforwardly made by counting one-step moves from a given state to another. In many real-life problems, however, the inference is much more difficult as state sequences are not fully observed, namely the state of each individual is known only for some given values of the time variable. A review of the problem is given, focusing on Monte Carlo Markov Chain (MCMC) algorithms to perform Bayesian inference and evaluate posterior distributions of the transition probabilities in this missing-data framework. Leaning on the dependence between the rows of the transition matrix, an adaptive MCMC mechanism accelerating the classical Metropolis-Hastings algorithm is then proposed and empirically studied.

</details>

<details>

<summary>2012-02-23 07:55:39 - Robust linear least squares regression</summary>

- *Jean-Yves Audibert, Olivier Catoni*

- `1010.0074v3` - [abs](http://arxiv.org/abs/1010.0074v3) - [pdf](http://arxiv.org/pdf/1010.0074v3)

> We consider the problem of robustly predicting as well as the best linear combination of $d$ given functions in least squares regression, and variants of this problem including constraints on the parameters of the linear combination. For the ridge estimator and the ordinary least squares estimator, and their variants, we provide new risk bounds of order $d/n$ without logarithmic factor unlike some standard results, where $n$ is the size of the training data. We also provide a new estimator with better deviations in the presence of heavy-tailed noise. It is based on truncating differences of losses in a min--max framework and satisfies a $d/n$ risk bound both in expectation and in deviations. The key common surprising factor of these results is the absence of exponential moment condition on the output distribution while achieving exponential deviations. All risk bounds are obtained through a PAC-Bayesian analysis on truncated differences of losses. Experimental results strongly back up our truncated min--max estimator.

</details>

<details>

<summary>2012-02-23 12:31:58 - Computational approaches for empirical Bayes methods and Bayesian sensitivity analysis</summary>

- *Eugenia Buta, Hani Doss*

- `1202.5160v1` - [abs](http://arxiv.org/abs/1202.5160v1) - [pdf](http://arxiv.org/pdf/1202.5160v1)

> We consider situations in Bayesian analysis where we have a family of priors $\nu_h$ on the parameter $\theta$, where $h$ varies continuously over a space $\mathcal{H}$, and we deal with two related problems. The first involves sensitivity analysis and is stated as follows. Suppose we fix a function $f$ of $\theta$. How do we efficiently estimate the posterior expectation of $f(\theta)$ simultaneously for all $h$ in $\mathcal{H}$? The second problem is how do we identify subsets of $\mathcal{H}$ which give rise to reasonable choices of $\nu_h$? We assume that we are able to generate Markov chain samples from the posterior for a finite number of the priors, and we develop a methodology, based on a combination of importance sampling and the use of control variates, for dealing with these two problems. The methodology applies very generally, and we show how it applies in particular to a commonly used model for variable selection in Bayesian linear regression, and give an illustration on the US crime data of Vandaele.

</details>

<details>

<summary>2012-02-23 13:51:01 - Bayesian inverse problems with Gaussian priors</summary>

- *B. T. Knapik, A. W. van der Vaart, J. H. van Zanten*

- `1103.2692v2` - [abs](http://arxiv.org/abs/1103.2692v2) - [pdf](http://arxiv.org/pdf/1103.2692v2)

> The posterior distribution in a nonparametric inverse problem is shown to contract to the true parameter at a rate that depends on the smoothness of the parameter, and the smoothness and scale of the prior. Correct combinations of these characteristics lead to the minimax rate. The frequentist coverage of credible sets is shown to depend on the combination of prior and true parameter, with smoother priors leading to zero coverage and rougher priors to conservative coverage. In the latter case credible sets are of the correct order of magnitude. The results are numerically illustrated by the problem of recovering a function from observation of a noisy version of its primitive.

</details>

<details>

<summary>2012-02-23 15:22:28 - A spectral analytic comparison of trace-class data augmentation algorithms and their sandwich variants</summary>

- *Kshitij Khare, James P. Hobert*

- `1202.5205v1` - [abs](http://arxiv.org/abs/1202.5205v1) - [pdf](http://arxiv.org/pdf/1202.5205v1)

> The data augmentation (DA) algorithm is a widely used Markov chain Monte Carlo algorithm that is easy to implement but often suffers from slow convergence. The sandwich algorithm is an alternative that can converge much faster while requiring roughly the same computational effort per iteration. Theoretically, the sandwich algorithm always converges at least as fast as the corresponding DA algorithm in the sense that $\Vert {K^*}\Vert \le \Vert {K}\Vert$, where $K$ and $K^*$ are the Markov operators associated with the DA and sandwich algorithms, respectively, and $\Vert\cdot\Vert$ denotes operator norm. In this paper, a substantial refinement of this operator norm inequality is developed. In particular, under regularity conditions implying that $K$ is a trace-class operator, it is shown that $K^*$ is also a positive, trace-class operator, and that the spectrum of $K^*$ dominates that of $K$ in the sense that the ordered elements of the former are all less than or equal to the corresponding elements of the latter. Furthermore, if the sandwich algorithm is constructed using a group action, as described by Liu and Wu [J. Amer. Statist. Assoc. 94 (1999) 1264--1274] and Hobert and Marchev [Ann. Statist. 36 (2008) 532--554], then there is strict inequality between at least one pair of eigenvalues. These results are applied to a new DA algorithm for Bayesian quantile regression introduced by Kozumi and Kobayashi [J. Stat. Comput. Simul. 81 (2011) 1565--1578].

</details>

<details>

<summary>2012-02-27 08:47:32 - Two-Stage Bayesian Model Averaging in Endogenous Variable Models</summary>

- *A. Lenkoski, T. S. Eicher, A. E. Raftery*

- `1202.5858v1` - [abs](http://arxiv.org/abs/1202.5858v1) - [pdf](http://arxiv.org/pdf/1202.5858v1)

> Economic modeling in the presence of endogeneity is subject to model uncertainty at both the instrument and covariate level. We propose a Two-Stage Bayesian Model Averaging (2SBMA) methodology that extends the Two-Stage Least Squares (2SLS) estimator. By constructing a Two-Stage Unit Information Prior in the endogenous variable model, we are able to efficiently combine established methods for addressing model uncertainty in regression models with the classic technique of 2SLS. To assess the validity of instruments in the 2SBMA context, we develop Bayesian tests of the identification restriction that are based on model averaged posterior predictive p-values. A simulation study showed that 2SBMA has the ability to recover structure in both the instrument and covariate set, and substantially improves the sharpness of resulting coefficient estimates in comparison to 2SLS using the full specification in an automatic fashion. Due to the increased parsimony of the 2SBMA estimate, the Bayesian Sargan test had a power of 50 percent in detecting a violation of the exogeneity assumption, while the method based on 2SLS using the full specification had negligible power. We apply our approach to the problem of development accounting, and find support not only for institutions, but also for geography and integration as development determinants, once both model uncertainty and endogeneity have been jointly addressed.

</details>

<details>

<summary>2012-02-27 10:42:54 - On Bayesian quantile regression curve fitting via auxiliary variables</summary>

- *J. -L. Dortet-Bernadet, Y. Fan*

- `1202.5883v1` - [abs](http://arxiv.org/abs/1202.5883v1) - [pdf](http://arxiv.org/pdf/1202.5883v1)

> Quantile regression has received increased attention in the statistics community in recent years. This article adapts an auxiliary variable method, commonly used in Bayesian variable selection for mean regression models, to the fitting of quantile regression curves. We focus on the fitting of regression splines, with unknown number and location of knots. We provide an efficient algorithm with Metropolis-Hastings updates whose tuning is fully automated. The method is tested on simulated and real examples and its extension to additive models is described. Finally we propose a simple postprocessing procedure to deal with the problem of the crossing of multiple separately estimated quantile curves.

</details>

<details>

<summary>2012-02-27 11:50:31 - Modeling of the HIV infection epidemic in the Netherlands: A multi-parameter evidence synthesis approach</summary>

- *Stefano Conti, Anne M. Presanis, Maaike G. van Veen, Maria Xiridou, Martin C. Donoghoe, Annemarie Rinder Stengaard, Daniela De Angelis*

- `1202.5901v1` - [abs](http://arxiv.org/abs/1202.5901v1) - [pdf](http://arxiv.org/pdf/1202.5901v1)

> Multi-parameter evidence synthesis (MPES) is receiving growing attention from the epidemiological community as a coherent and flexible analytical framework to accommodate a disparate body of evidence available to inform disease incidence and prevalence estimation. MPES is the statistical methodology adopted by the Health Protection Agency in the UK for its annual national assessment of the HIV epidemic, and is acknowledged by the World Health Organization and UNAIDS as a valuable technique for the estimation of adult HIV prevalence from surveillance data. This paper describes the results of utilizing a Bayesian MPES approach to model HIV prevalence in the Netherlands at the end of 2007, using an array of field data from different study designs on various population risk subgroups and with a varying degree of regional coverage. Auxiliary data and expert opinion were additionally incorporated to resolve issues arising from biased, insufficient or inconsistent evidence. This case study offers a demonstration of the ability of MPES to naturally integrate and critically reconcile disparate and heterogeneous sources of evidence, while producing reliable estimates of HIV prevalence used to support public health decision-making.

</details>

<details>

<summary>2012-02-27 12:45:10 - Multivariate Bayesian semiparametric models for authentication of food and beverages</summary>

- *Luis Gutiérrez, Fernando A. Quintana*

- `1202.5914v1` - [abs](http://arxiv.org/abs/1202.5914v1) - [pdf](http://arxiv.org/pdf/1202.5914v1)

> Food and beverage authentication is the process by which foods or beverages are verified as complying with its label description, for example, verifying if the denomination of origin of an olive oil bottle is correct or if the variety of a certain bottle of wine matches its label description. The common way to deal with an authentication process is to measure a number of attributes on samples of food and then use these as input for a classification problem. Our motivation stems from data consisting of measurements of nine chemical compounds denominated Anthocyanins, obtained from samples of Chilean red wines of grape varieties Cabernet Sauvignon, Merlot and Carm\'{e}n\`{e}re. We consider a model-based approach to authentication through a semiparametric multivariate hierarchical linear mixed model for the mean responses, and covariance matrices that are specific to the classification categories. Specifically, we propose a model of the ANOVA-DDP type, which takes advantage of the fact that the available covariates are discrete in nature. The results suggest that the model performs well compared to other parametric alternatives. This is also corroborated by application to simulated data.

</details>

<details>

<summary>2012-02-29 07:47:45 - Bayesian interpretation of Generalized empirical likelihood by maximum entropy</summary>

- *Paul Rochet*

- `1202.6469v1` - [abs](http://arxiv.org/abs/1202.6469v1) - [pdf](http://arxiv.org/pdf/1202.6469v1)

> We study a parametric estimation problem related to moment condition models. As an alternative to the generalized empirical likelihood (GEL) and the generalized method of moments (GMM), a Bayesian approach to the problem can be adopted, extending the MEM procedure to parametric moment conditions. We show in particular that a large number of GEL estimators can be interpreted as a maximum entropy solution. Moreover, we provide a more general field of applications by proving the method to be robust to approximate moment conditions.

</details>


## 2012-03

<details>

<summary>2012-03-01 03:18:03 - Bayesian Nonparametric Variable Selection as an Exploratory Tool for Finding Genes that Matter</summary>

- *Babak Shahbaba*

- `1003.2390v3` - [abs](http://arxiv.org/abs/1003.2390v3) - [pdf](http://arxiv.org/pdf/1003.2390v3)

> High-throughput scientific studies involving no clear a'priori hypothesis are common. For example, a large-scale genomic study of a disease may examine thousands of genes without hypothesizing that any specific gene is responsible for the disease. In these studies, the objective is to explore a large number of possible factors (e.g. genes) in order to identify a small number that will be considered in follow-up studies that tend to be more thorough and on smaller scales. For large-scale studies, we propose a nonparametric Bayesian approach based on random partition models. Our model thus divides the set of candidate factors into several subgroups according to their degrees of relevance, or potential effect, in relation to the outcome of interest. The model allows for a latent rank to be assigned to each factor according to the overall potential importance of its corresponding group. The posterior expectation or mode of these ranks is used to set up a threshold for selecting potentially relevant factors. Using simulated data, we demonstrate that our approach could be quite effective in finding relevant genes compared to several alternative methods. We apply our model to two large-scale studies. The first study involves transcriptome analysis of infection by human cytomegalovirus (HCMV). The objective of the second study is to identify differentially expressed genes between two types of leukemia.

</details>

<details>

<summary>2012-03-01 06:26:42 - Bayesian matching of unlabeled marked point sets using random fields, with an application to molecular alignment</summary>

- *Irina Czogiel, Ian L. Dryden, Christopher J. Brignell*

- `1203.0098v1` - [abs](http://arxiv.org/abs/1203.0098v1) - [pdf](http://arxiv.org/pdf/1203.0098v1)

> Statistical methodology is proposed for comparing unlabeled marked point sets, with an application to aligning steroid molecules in chemoinformatics. Methods from statistical shape analysis are combined with techniques for predicting random fields in spatial statistics in order to define a suitable measure of similarity between two marked point sets. Bayesian modeling of the predicted field overlap between pairs of point sets is proposed, and posterior inference of the alignment is carried out using Markov chain Monte Carlo simulation. By representing the fields in reproducing kernel Hilbert spaces, the degree of overlap can be computed without expensive numerical integration. Superimposing entire fields rather than the configuration matrices of point coordinates thereby avoids the problem that there is usually no clear one-to-one correspondence between the points. In addition, mask parameters are introduced in the model, so that partial matching of the marked point sets can be carried out. We also propose an adaptation of the generalized Procrustes analysis algorithm for the simultaneous alignment of multiple point sets. The methodology is illustrated with a simulation study and then applied to a data set of 31 steroid molecules, where the relationship between shape and binding activity to the corticosteroid binding globulin receptor is explored.

</details>

<details>

<summary>2012-03-01 07:39:21 - Sparsity-Promoting Bayesian Dynamic Linear Models</summary>

- *François Caron, Luke Bornn, Arnaud Doucet*

- `1203.0106v1` - [abs](http://arxiv.org/abs/1203.0106v1) - [pdf](http://arxiv.org/pdf/1203.0106v1)

> Sparsity-promoting priors have become increasingly popular over recent years due to an increased number of regression and classification applications involving a large number of predictors. In time series applications where observations are collected over time, it is often unrealistic to assume that the underlying sparsity pattern is fixed. We propose here an original class of flexible Bayesian linear models for dynamic sparsity modelling. The proposed class of models expands upon the existing Bayesian literature on sparse regression using generalized multivariate hyperbolic distributions. The properties of the models are explored through both analytic results and simulation studies. We demonstrate the model on a financial application where it is shown that it accurately represents the patterns seen in the analysis of stock and derivative data, and is able to detect major events by filtering an artificial portfolio of assets.

</details>

<details>

<summary>2012-03-01 07:43:17 - Bernstein von Mises Theorems for Gaussian Regression with increasing number of regressors</summary>

- *Dominique Bontemps*

- `1009.1370v3` - [abs](http://arxiv.org/abs/1009.1370v3) - [pdf](http://arxiv.org/pdf/1009.1370v3)

> This paper brings a contribution to the Bayesian theory of nonparametric and semiparametric estimation. We are interested in the asymptotic normality of the posterior distribution in Gaussian linear regression models when the number of regressors increases with the sample size. Two kinds of Bernstein-von Mises Theorems are obtained in this framework: nonparametric theorems for the parameter itself, and semiparametric theorems for functionals of the parameter. We apply them to the Gaussian sequence model and to the regression of functions in Sobolev and $C^{\alpha}$ classes, in which we get the minimax convergence rates. Adaptivity is reached for the Bayesian estimators of functionals in our applications.

</details>

<details>

<summary>2012-03-01 10:23:07 - Covariance approximation for large multivariate spatial data sets with an application to multiple climate model errors</summary>

- *Huiyan Sang, Mikyoung Jun, Jianhua Z. Huang*

- `1203.0133v1` - [abs](http://arxiv.org/abs/1203.0133v1) - [pdf](http://arxiv.org/pdf/1203.0133v1)

> This paper investigates the cross-correlations across multiple climate model errors. We build a Bayesian hierarchical model that accounts for the spatial dependence of individual models as well as cross-covariances across different climate models. Our method allows for a nonseparable and nonstationary cross-covariance structure. We also present a covariance approximation approach to facilitate the computation in the modeling and analysis of very large multivariate spatial data sets. The covariance approximation consists of two parts: a reduced-rank part to capture the large-scale spatial dependence, and a sparse covariance matrix to correct the small-scale dependence error induced by the reduced rank approximation. We pay special attention to the case that the second part of the approximation has a block-diagonal structure. Simulation results of model fitting and prediction show substantial improvement of the proposed approximation over the predictive process approximation and the independent blocks analysis. We then apply our computational approach to the joint statistical modeling of multiple climate model errors.

</details>

<details>

<summary>2012-03-01 13:54:51 - Estimation of a sparse group of sparse vectors</summary>

- *Felix Abramovich, Vadim Grinshtein*

- `1104.1771v2` - [abs](http://arxiv.org/abs/1104.1771v2) - [pdf](http://arxiv.org/pdf/1104.1771v2)

> We consider a problem of estimating a sparse group of sparse normal mean vectors. The proposed approach is based on penalized likelihood estimation with complexity penalties on the number of nonzero mean vectors and the numbers of their "significant" components, and can be performed by a computationally fast algorithm. The resulting estimators are developed within Bayesian framework and can be viewed as MAP estimators. We establish their adaptive minimaxity over a wide range of sparse and dense settings. The presented short simulation study demonstrates the efficiency of the proposed approach that successfully competes with the recently developed sparse group lasso estimator.

</details>

<details>

<summary>2012-03-01 17:25:43 - Bayesian Posteriors Without Bayes' Theorem</summary>

- *Theodore P. Hill, Marco Dall'Aglio*

- `1203.0251v1` - [abs](http://arxiv.org/abs/1203.0251v1) - [pdf](http://arxiv.org/pdf/1203.0251v1)

> The classical Bayesian posterior arises naturally as the unique solution of several different optimization problems, without the necessity of interpreting data as conditional probabilities and then using Bayes' Theorem. For example, the classical Bayesian posterior is the unique posterior that minimizes the loss of Shannon information in combining the prior and the likelihood distributions. These results, direct corollaries of recent results about conflations of probability distributions, reinforce the use of Bayesian posteriors, and may help partially reconcile some of the differences between classical and Bayesian statistics.

</details>

<details>

<summary>2012-03-02 15:21:02 - Inference of Temporally Varying Bayesian Networks</summary>

- *Thomas Thorne, Michael P. H Stumpf*

- `1203.0489v1` - [abs](http://arxiv.org/abs/1203.0489v1) - [pdf](http://arxiv.org/pdf/1203.0489v1)

> When analysing gene expression time series data an often overlooked but crucial aspect of the model is that the regulatory network structure may change over time. Whilst some approaches have addressed this problem previously in the literature, many are not well suited to the sequential nature of the data. Here we present a method that allows us to infer regulatory network structures that may vary between time points, utilising a set of hidden states that describe the network structure at a given time point. To model the distribution of the hidden states we have applied the Hierarchical Dirichlet Process Hideen Markov Model, a nonparametric extension of the traditional Hidden Markov Model, that does not require us to fix the number of hidden states in advance. We apply our method to exisiting microarray expression data as well as demonstrating is efficacy on simulated test data.

</details>

<details>

<summary>2012-03-02 18:55:34 - Quickest Detection with Social Learning: Interaction of local and global decision makers</summary>

- *Vikram Krishnamurthy*

- `1007.0571v3` - [abs](http://arxiv.org/abs/1007.0571v3) - [pdf](http://arxiv.org/pdf/1007.0571v3)

> We consider how local and global decision policies interact in stopping time problems such as quickest time change detection. Individual agents make myopic local decisions via social learning, that is, each agent records a private observation of a noisy underlying state process, selfishly optimizes its local utility and then broadcasts its local decision. Given these local decisions, how can a global decision maker achieve quickest time change detection when the underlying state changes according to a phase-type distribution? The paper presents four results. First, using Blackwell dominance of measures, it is shown that the optimal cost incurred in social learning based quickest detection is always larger than that of classical quickest detection. Second, it is shown that in general the optimal decision policy for social learning based quickest detection is characterized by multiple thresholds within the space of Bayesian distributions. Third, using lattice programming and stochastic dominance, sufficient conditions are given for the optimal decision policy to consist of a single linear hyperplane, or, more generally, a threshold curve. Estimation of the optimal linear approximation to this threshold curve is formulated as a simulation-based stochastic optimization problem. Finally, the paper shows that in multi-agent sensor management with quickest detection, where each agent views the world according to its prior, the optimal policy has a similar structure to social learning.

</details>

<details>

<summary>2012-03-02 20:46:19 - On the relationship between ODEs and DBNs</summary>

- *Chris. J. Oates, Steven. M. Hill, Sach Mukherjee*

- `1201.3380v2` - [abs](http://arxiv.org/abs/1201.3380v2) - [pdf](http://arxiv.org/pdf/1201.3380v2)

> Recently, Li et al. (Bioinformatics 27(19), 2686-91, 2011) proposed a method, called Differential Equation-based Local Dynamic Bayesian Network (DELDBN), for reverse engineering gene regulatory networks from time-course data. We commend the authors for an interesting paper that draws attention to the close relationship between dynamic Bayesian networks (DBNs) and differential equations (DEs). Their central claim is that modifying a DBN to model Euler approximations to the gradient rather than expression levels themselves is beneficial for network inference. The empirical evidence provided is based on time-course data with equally-spaced observations. However, as we discuss below, in the particular case of equally-spaced observations, Euler approximations and conventional DBNs lead to equivalent statistical models that, absent artefacts due to the estimation procedure, yield networks with identical inter-gene edge sets. Here, we discuss further the relationship between DEs and conventional DBNs and present new empirical results on unequally spaced data which demonstrate that modelling Euler approximations in a DBN can lead to improved network reconstruction.

</details>

<details>

<summary>2012-03-05 11:29:10 - Robust detection of exotic infectious diseases in animal herds: A comparative study of three decision methodologies under severe uncertainty</summary>

- *Matthias C. M. Troffaes, John Paul Gosling*

- `1112.1868v2` - [abs](http://arxiv.org/abs/1112.1868v2) - [pdf](http://arxiv.org/pdf/1112.1868v2)

> When animals are transported and pass through customs, some of them may have dangerous infectious diseases. Typically, due to the cost of testing, not all animals are tested: a reasonable selection must be made. How to test effectively whilst avoiding costly disease outbreaks? First, we extend a model proposed in the literature for the detection of invasive species to suit our purpose. Secondly, we explore and compare three decision methodologies on the problem at hand, namely, Bayesian statistics, info-gap theory and imprecise probability theory, all of which are designed to handle severe uncertainty. We show that, under rather general conditions, every info-gap solution is maximal with respect to a suitably chosen imprecise probability model, and that therefore, perhaps surprisingly, the set of maximal options can be inferred at least partly---and sometimes entirely---from an info-gap analysis.

</details>

<details>

<summary>2012-03-05 13:35:07 - Identifying differentially expressed transcripts from RNA-seq data with biological variation</summary>

- *Peter Glaus, Antti Honkela, Magnus Rattray*

- `1109.0863v2` - [abs](http://arxiv.org/abs/1109.0863v2) - [pdf](http://arxiv.org/pdf/1109.0863v2)

> Motivation: High-throughput sequencing enables expression analysis at the level of individual transcripts. The analysis of transcriptome expression levels and differential expression estimation requires a probabilistic approach to properly account for ambiguity caused by shared exons and finite read sampling as well as the intrinsic biological variance of transcript expression.   Results: We present BitSeq (Bayesian Inference of Transcripts from Sequencing data), a Bayesian approach for estimation of transcript expression level from RNA-seq experiments. Inferred relative expression is represented by Markov chain Monte Carlo (MCMC) samples from the posterior probability distribution of a generative model of the read data. We propose a novel method for differential expression analysis across replicates which propagates uncertainty from the sample-level model while modelling biological variance using an expression-level-dependent prior. We demonstrate the advantages of our method using simulated data as well as an RNA-seq dataset with technical and biological replication for both studied conditions.   Availability: The implementation of the transcriptome expression estimation and differential expression analysis, BitSeq, has been written in C++.

</details>

<details>

<summary>2012-03-07 05:35:25 - Kepler Presearch Data Conditioning I - Architecture and Algorithms for Error Correction in Kepler Light Curves</summary>

- *Martin C. Stumpe, Jeffrey C. Smith, Jeffrey E. Van Cleve, Joseph D. Twicken, Thomas S. Barclay, Michael N. Fanelli, Forrest R. Girouard, Jon M. Jenkins, Jeffery J. Kolodziejczak, Sean D. McCauliff, Robert L. Morris*

- `1203.1382v1` - [abs](http://arxiv.org/abs/1203.1382v1) - [pdf](http://arxiv.org/pdf/1203.1382v1)

> Kepler provides light curves of 156,000 stars with unprecedented precision. However, the raw data as they come from the spacecraft contain significant systematic and stochastic errors. These errors, which include discontinuities, systematic trends, and outliers, obscure the astrophysical signals in the light curves. To correct these errors is the task of the Presearch Data Conditioning (PDC) module of the Kepler data analysis pipeline. The original version of PDC in Kepler did not meet the extremely high performance requirements for the detection of miniscule planet transits or highly accurate analysis of stellar activity and rotation. One particular deficiency was that astrophysical features were often removed as a side-effect to removal of errors. In this paper we introduce the completely new and significantly improved version of PDC which was implemented in Kepler SOC 8.0. This new PDC version, which utilizes a Bayesian approach for removal of systematics, reliably corrects errors in the light curves while at the same time preserving planet transits and other astrophysically interesting signals. We describe the architecture and the algorithms of this new PDC module, show typical errors encountered in Kepler data, and illustrate the corrections using real light curve examples.

</details>

<details>

<summary>2012-03-07 05:35:37 - Kepler Presearch Data Conditioning II - A Bayesian Approach to Systematic Error Correction</summary>

- *Jeffrey C. Smith, Martin C. Stumpe, Jeffrey E. Van Cleve, Jon M. Jenkins, Thomas S. Barclay, Michael N. Fanelli, Forrest R. Girouard, Jeffery J. Kolodziejczak, Sean D. McCauliff, Robert L. Morris, Joseph D. Twicken*

- `1203.1383v1` - [abs](http://arxiv.org/abs/1203.1383v1) - [pdf](http://arxiv.org/pdf/1203.1383v1)

> With the unprecedented photometric precision of the Kepler Spacecraft, significant systematic and stochastic errors on transit signal levels are observable in the Kepler photometric data. These errors, which include discontinuities, outliers, systematic trends and other instrumental signatures, obscure astrophysical signals. The Presearch Data Conditioning (PDC) module of the Kepler data analysis pipeline tries to remove these errors while preserving planet transits and other astrophysically interesting signals. The completely new noise and stellar variability regime observed in Kepler data poses a significant problem to standard cotrending methods such as SYSREM and TFA. Variable stars are often of particular astrophysical interest so the preservation of their signals is of significant importance to the astrophysical community. We present a Bayesian Maximum A Posteriori (MAP) approach where a subset of highly correlated and quiet stars is used to generate a cotrending basis vector set which is in turn used to establish a range of "reasonable" robust fit parameters. These robust fit parameters are then used to generate a Bayesian Prior and a Bayesian Posterior Probability Distribution Function (PDF) which when maximized finds the best fit that simultaneously removes systematic effects while reducing the signal distortion and noise injection which commonly afflicts simple least-squares (LS) fitting. A numerical and empirical approach is taken where the Bayesian Prior PDFs are generated from fits to the light curve distributions themselves.

</details>

<details>

<summary>2012-03-08 00:16:54 - Bayesian nonparametric estimation of Simpson's evenness index under $α-$Gibbs priors</summary>

- *Annalisa Cerquetti*

- `1203.1666v1` - [abs](http://arxiv.org/abs/1203.1666v1) - [pdf](http://arxiv.org/pdf/1203.1666v1)

> A Bayesian nonparametric approach to the study of species diversity based on choosing a random discrete distribution as a prior model for the unknown relative abundances of species has been recently introduced in Lijoi et al. (2007, 2008). Explicit posterior predictive estimation of {\it species richness} has been obtained under priors belonging to the $\alpha$-Gibbs class (Gnedin & Pitman, 2006). Here we focus on posterior estimation of {\it species evenness} which accounts for diversity in terms of the proximity to the situation of uniform distribution of the population into different species. We focus on Simpson's index and provide a Bayesian estimator under quadratic loss function, with its variance, under some specific $\alpha-$Gibbs priors.

</details>

<details>

<summary>2012-03-08 05:23:02 - Exact sampling for intractable probability distributions via a Bernoulli factory</summary>

- *James M. Flegal, Radu Herbei*

- `1012.3768v2` - [abs](http://arxiv.org/abs/1012.3768v2) - [pdf](http://arxiv.org/pdf/1012.3768v2)

> Many applications in the field of statistics require Markov chain Monte Carlo methods. Determining appropriate starting values and run lengths can be both analytically and empirically challenging. A desire to overcome these problems has led to the development of exact, or perfect, sampling algorithms which convert a Markov chain into an algorithm that produces i.i.d. samples from the stationary distribution. Unfortunately, very few of these algorithms have been developed for the distributions that arise in statistical applications, which typically have uncountable support. Here we study an exact sampling algorithm using a geometrically ergodic Markov chain on a general state space. Our work provides a significant reduction to the number of input draws necessary for the Bernoulli factory, which enables exact sampling via a rejection sampling approach. We illustrate the algorithm on a univariate Metropolis-Hastings sampler and a bivariate Gibbs sampler, which provide a proof of concept and insight into hyper-parameter selection. Finally, we illustrate the algorithm on a Bayesian version of the one-way random effects model with data from a styrene exposure study.

</details>

<details>

<summary>2012-03-09 13:54:20 - On image segmentation using information theoretic criteria</summary>

- *Alexander Aue, Thomas C. M. Lee*

- `1203.2087v1` - [abs](http://arxiv.org/abs/1203.2087v1) - [pdf](http://arxiv.org/pdf/1203.2087v1)

> Image segmentation is a long-studied and important problem in image processing. Different solutions have been proposed, many of which follow the information theoretic paradigm. While these information theoretic segmentation methods often produce excellent empirical results, their theoretical properties are still largely unknown. The main goal of this paper is to conduct a rigorous theoretical study into the statistical consistency properties of such methods. To be more specific, this paper investigates if these methods can accurately recover the true number of segments together with their true boundaries in the image as the number of pixels tends to infinity. Our theoretical results show that both the Bayesian information criterion (BIC) and the minimum description length (MDL) principle can be applied to derive statistically consistent segmentation methods, while the same is not true for the Akaike information criterion (AIC). Numerical experiments were conducted to illustrate and support our theoretical findings.

</details>

<details>

<summary>2012-03-10 00:49:34 - The quantile spectral density and comparison based tests for nonlinear time series</summary>

- *Junbum Lee, Suhasini Subba Rao*

- `1112.2759v2` - [abs](http://arxiv.org/abs/1112.2759v2) - [pdf](http://arxiv.org/pdf/1112.2759v2)

> In this paper we consider tests for nonlinear time series, which are motivated by the notion of serial dependence. The proposed tests are based on comparisons with the quantile spectral density, which can be considered as a quantile version of the usual spectral density function. The quantile spectral density 'measures' sequential dependence structure of a time series, and is well defined under relatively weak mixing conditions. We propose an estimator for the quantile spectral density and derive its asympototic sampling properties. We use the quantile spectral density to construct a goodness of fit test for time series and explain how this test can also be used for comparing the sequential dependence structure of two time series. The method is illustrated with simulations and some real data examples.

</details>

<details>

<summary>2012-03-11 21:22:11 - Maximum penalized likelihood estimation for skew-normal and skew-$t$ distributions</summary>

- *Adelchi Azzalini, Reinaldo B. Arellano-Valle*

- `1203.2376v1` - [abs](http://arxiv.org/abs/1203.2376v1) - [pdf](http://arxiv.org/pdf/1203.2376v1)

> The skew-normal and the skew-$t$ distributions are parametric families which are currently under intense investigation since they provide a more flexible formulation compared to the classical normal and $t$ distributions by introducing a parameter which regulates their skewness. While these families enjoy attractive formal properties from the probability viewpoint, a practical problem with their usage in applications is the possibility that the maximum likelihood estimate of the parameter which regulates skewness diverges. This situation has vanishing probability for increasing sample size, but for finite samples it occurs with non-negligible probability, and its occurrence has unpleasant effects on the inferential process. Methods for overcoming this problem have been put forward both in the classical and in the Bayesian formulation, but their applicability is restricted to simple situations. We formulate a proposal based on the idea of penalized likelihood, which has connections with some of the existing methods, but it applies more generally, including in the multivariate case.

</details>

<details>

<summary>2012-03-12 11:24:59 - Posterior consistency of nonparametric conditional moment restricted models</summary>

- *Yuan Liao, Wenxin Jiang*

- `1105.4847v2` - [abs](http://arxiv.org/abs/1105.4847v2) - [pdf](http://arxiv.org/pdf/1105.4847v2)

> This paper addresses the estimation of the nonparametric conditional moment restricted model that involves an infinite-dimensional parameter $g_0$. We estimate it in a quasi-Bayesian way, based on the limited information likelihood, and investigate the impact of three types of priors on the posterior consistency: (i) truncated prior (priors supported on a bounded set), (ii) thin-tail prior (a prior that has very thin tail outside a growing bounded set) and (iii) normal prior with nonshrinking variance. In addition, $g_0$ is allowed to be only partially identified in the frequentist sense, and the parameter space does not need to be compact. The posterior is regularized using a slowly growing sieve dimension, and it is shown that the posterior converges to any small neighborhood of the identified region. We then apply our results to the nonparametric instrumental regression model. Finally, the posterior consistency using a random sieve dimension parameter is studied.

</details>

<details>

<summary>2012-03-13 15:02:40 - Perfect Simulation for Mixtures with Known and Unknown Number of components</summary>

- *Sabyasachi Mukhopadhyay, Sourabh Bhattacharya*

- `1102.4152v2` - [abs](http://arxiv.org/abs/1102.4152v2) - [pdf](http://arxiv.org/pdf/1102.4152v2)

> We propose and develop a novel and effective perfect sampling methodology for simulating from posteriors corresponding to mixtures with either known (fixed) or unknown number of components. For the latter we consider the Dirichlet process-based mixture model developed by these authors, and show that our ideas are applicable to conjugate, and importantly, to non-conjugate cases. As to be expected, and, as we show, perfect sampling for mixtures with known number of components can be achieved with much less effort with a simplified version of our general methodology, whether or not conjugate or non-conjugate priors are used. While no special assumption is necessary in the conjugate set-up for our theory to work, we require the assumption of bounded parameter space in the non-conjugate set-up. However, we argue, with appropriate analytical, simulation, and real data studies as support, that such boundedness assumption is not unrealistic and is not an impediment in practice. Not only do we validate our ideas theoretically and with simulation studies, but we also consider application of our proposal to three real data sets used by several authors in the past in connection with mixture models. The results we achieved in each of our experiments with either simulation study or real data application, are quite encouraging.

</details>

<details>

<summary>2012-03-13 17:29:36 - On the use of backward simulation in particle Markov chain Monte Carlo methods</summary>

- *Fredrik Lindsten, Thomas B. Schön*

- `1110.2873v2` - [abs](http://arxiv.org/abs/1110.2873v2) - [pdf](http://arxiv.org/pdf/1110.2873v2)

> Recently, Andrieu, Doucet and Holenstein (2010) introduced a general framework for using particle filters (PFs) to construct proposal kernels for Markov chain Monte Carlo (MCMC) methods. This framework, termed Particle Markov chain Monte Carlo (PMCMC), was shown to provide powerful methods for joint Bayesian state and parameter inference in nonlinear/non-Gaussian state-space models. However, the mixing of the resulting MCMC kernels can be quite sensitive, both to the number of particles used in the underlying PF and to the number of observations in the data. In the discussion following (Andrieu et al., 2010), Whiteley suggested a modified version of one of the PMCMC samplers, namely the particle Gibbs (PG) sampler, and argued that this should improve its mixing. In this paper we explore the consequences of this modification and show that it leads to a method which is much more robust to a low number of particles as well as a large number of observations. Furthermore, we discuss how the modified PG sampler can be used as a basis for alternatives to all three PMCMC samplers derived in (Andrieu et al., 2010). We evaluate these methods on several challenging inference problems in a simulation study. One of these is the identification of an epidemiological model for predicting influenza epidemics, based on search engine query data.

</details>

<details>

<summary>2012-03-14 07:31:15 - Bayesian Parameter Estimation for Latent Markov Random Fields and Social Networks</summary>

- *Richard G. Everitt*

- `1203.3725v1` - [abs](http://arxiv.org/abs/1203.3725v1) - [pdf](http://arxiv.org/pdf/1203.3725v1)

> Undirected graphical models are widely used in statistics, physics and machine vision. However Bayesian parameter estimation for undirected models is extremely challenging, since evaluation of the posterior typically involves the calculation of an intractable normalising constant. This problem has received much attention, but very little of this has focussed on the important practical case where the data consists of noisy or incomplete observations of the underlying hidden structure. This paper specifically addresses this problem, comparing two alternative methodologies. In the first of these approaches particle Markov chain Monte Carlo (Andrieu et al., 2010) is used to efficiently explore the parameter space, combined with the exchange algorithm (Murray et al., 2006) for avoiding the calculation of the intractable normalising constant (a proof showing that this combination targets the correct distribution in found in a supplementary appendix online). This approach is compared with approximate Bayesian computation (Pritchard et al., 1999). Applications to estimating the parameters of Ising models and exponential random graphs from noisy data are presented. Each algorithm used in the paper targets an approximation to the true posterior due to the use of MCMC to simulate from the latent graphical model, in lieu of being able to do this exactly in general. The supplementary appendix also describes the nature of the resulting approximation.

</details>

<details>

<summary>2012-03-14 10:16:14 - A new Monte Carlo sampling in Bayesian probit regression</summary>

- *Yuzo Maruyama, William E. Strawderman*

- `1202.4339v3` - [abs](http://arxiv.org/abs/1202.4339v3) - [pdf](http://arxiv.org/pdf/1202.4339v3)

> We study probit regression from a Bayesian perspective and give an alternative form for the posterior distribution when the prior distribution for the regression parameters is the uniform distribution. This new form allows simple Monte Carlo simulation of the posterior as opposed to MCMC simulation studied in much of the literature and may therefore be more efficient computationally. We also provide alternative explicit expression for the first and second moments. Additionally we provide analogous results for Gaussian priors.

</details>

<details>

<summary>2012-03-15 11:17:56 - Bayesian Rose Trees</summary>

- *Charles Blundell, Yee Whye Teh, Katherine A. Heller*

- `1203.3468v1` - [abs](http://arxiv.org/abs/1203.3468v1) - [pdf](http://arxiv.org/pdf/1203.3468v1)

> Hierarchical structure is ubiquitous in data across many domains. There are many hierarchical clustering methods, frequently used by domain experts, which strive to discover this structure. However, most of these methods limit discoverable hierarchies to those with binary branching structure. This limitation, while computationally convenient, is often undesirable. In this paper we explore a Bayesian hierarchical clustering algorithm that can produce trees with arbitrary branching structure at each node, known as rose trees. We interpret these trees as mixtures over partitions of a data set, and use a computationally efficient, greedy agglomerative algorithm to find the rose trees which have high marginal likelihood given the data. Lastly, we perform experiments which demonstrate that rose trees are better models of data than the typical binary trees returned by other hierarchical clustering algorithms.

</details>

<details>

<summary>2012-03-15 11:17:56 - An Online Learning-based Framework for Tracking</summary>

- *Kamalika Chaudhuri, Yoav Freund, Daniel Hsu*

- `1203.3471v1` - [abs](http://arxiv.org/abs/1203.3471v1) - [pdf](http://arxiv.org/pdf/1203.3471v1)

> We study the tracking problem, namely, estimating the hidden state of an object over time, from unreliable and noisy measurements. The standard framework for the tracking problem is the generative framework, which is the basis of solutions such as the Bayesian algorithm and its approximation, the particle filters. However, these solutions can be very sensitive to model mismatches. In this paper, motivated by online learning, we introduce a new framework for tracking. We provide an efficient tracking algorithm for this framework. We provide experimental results comparing our algorithm to the Bayesian algorithm on simulated data. Our experiments show that when there are slight model mismatches, our algorithm outperforms the Bayesian algorithm.

</details>

<details>

<summary>2012-03-15 11:17:56 - Super-Samples from Kernel Herding</summary>

- *Yutian Chen, Max Welling, Alex Smola*

- `1203.3472v1` - [abs](http://arxiv.org/abs/1203.3472v1) - [pdf](http://arxiv.org/pdf/1203.3472v1)

> We extend the herding algorithm to continuous spaces by using the kernel trick. The resulting "kernel herding" algorithm is an infinite memory deterministic process that learns to approximate a PDF with a collection of samples. We show that kernel herding decreases the error of expectations of functions in the Hilbert space at a rate O(1/T) which is much faster than the usual O(1/pT) for iid random samples. We illustrate kernel herding by approximating Bayesian predictive distributions.

</details>

<details>

<summary>2012-03-15 11:17:56 - Inference-less Density Estimation using Copula Bayesian Networks</summary>

- *Gal Elidan*

- `1203.3476v1` - [abs](http://arxiv.org/abs/1203.3476v1) - [pdf](http://arxiv.org/pdf/1203.3476v1)

> We consider learning continuous probabilistic graphical models in the face of missing data. For non-Gaussian models, learning the parameters and structure of such models depends on our ability to perform efficient inference, and can be prohibitive even for relatively modest domains. Recently, we introduced the Copula Bayesian Network (CBN) density model - a flexible framework that captures complex high-dimensional dependency structures while offering direct control over the univariate marginals, leading to improved generalization. In this work we show that the CBN model also offers significant computational advantages when training data is partially observed. Concretely, we leverage on the specialized form of the model to derive a computationally amenable learning objective that is a lower bound on the log-likelihood function. Importantly, our energy-like bound circumvents the need for costly inference of an auxiliary distribution, thus facilitating practical learning of highdimensional densities. We demonstrate the effectiveness of our approach for learning the structure and parameters of a CBN model for two reallife continuous domains.

</details>

<details>

<summary>2012-03-15 11:17:56 - The Hierarchical Dirichlet Process Hidden Semi-Markov Model</summary>

- *Matthew J. Johnson, Alan Willsky*

- `1203.3485v1` - [abs](http://arxiv.org/abs/1203.3485v1) - [pdf](http://arxiv.org/pdf/1203.3485v1)

> There is much interest in the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) as a natural Bayesian nonparametric extension of the traditional HMM. However, in many settings the HDP-HMM's strict Markovian constraints are undesirable, particularly if we wish to learn or encode non-geometric state durations. We can extend the HDP-HMM to capture such structure by drawing upon explicit-duration semi-Markovianity, which has been developed in the parametric setting to allow construction of highly interpretable models that admit natural prior information on state durations. In this paper we introduce the explicitduration HDP-HSMM and develop posterior sampling algorithms for efficient inference in both the direct-assignment and weak-limit approximation settings. We demonstrate the utility of the model and our inference methods on synthetic data as well as experiments on a speaker diarization problem and an example of learning the patterns in Morse code.

</details>

<details>

<summary>2012-03-15 11:17:56 - Bayesian exponential family projections for coupled data sources</summary>

- *Arto Klami, Seppo Virtanen, Samuel Kaski*

- `1203.3489v1` - [abs](http://arxiv.org/abs/1203.3489v1) - [pdf](http://arxiv.org/pdf/1203.3489v1)

> Exponential family extensions of principal component analysis (EPCA) have received a considerable amount of attention in recent years, demonstrating the growing need for basic modeling tools that do not assume the squared loss or Gaussian distribution. We extend the EPCA model toolbox by presenting the first exponential family multi-view learning methods of the partial least squares and canonical correlation analysis, based on a unified representation of EPCA as matrix factorization of the natural parameters of exponential family. The models are based on a new family of priors that are generally usable for all such factorizations. We also introduce new inference strategies, and demonstrate how the methods outperform earlier ones when the Gaussianity assumption does not hold.

</details>

<details>

<summary>2012-03-15 11:17:56 - Algorithms and Complexity Results for Exact Bayesian Structure Learning</summary>

- *Sebastian Ordyniak, Stefan Szeider*

- `1203.3501v1` - [abs](http://arxiv.org/abs/1203.3501v1) - [pdf](http://arxiv.org/pdf/1203.3501v1)

> Bayesian structure learning is the NP-hard problem of discovering a Bayesian network that optimally represents a given set of training data. In this paper we study the computational worst-case complexity of exact Bayesian structure learning under graph theoretic restrictions on the super-structure. The super-structure (a concept introduced by Perrier, Imoto, and Miyano, JMLR 2008) is an undirected graph that contains as subgraphs the skeletons of solution networks. Our results apply to several variants of score-based Bayesian structure learning where the score of a network decomposes into local scores of its nodes. Results: We show that exact Bayesian structure learning can be carried out in non-uniform polynomial time if the super-structure has bounded treewidth and in linear time if in addition the super-structure has bounded maximum degree. We complement this with a number of hardness results. We show that both restrictions (treewidth and degree) are essential and cannot be dropped without loosing uniform polynomial time tractability (subject to a complexity-theoretic assumption). Furthermore, we show that the restrictions remain essential if we do not search for a globally optimal network but we aim to improve a given network by means of at most k arc additions, arc deletions, or arc reversals (k-neighborhood local search).

</details>

<details>

<summary>2012-03-15 11:17:56 - Irregular-Time Bayesian Networks</summary>

- *Michael Ramati, Yuval Shahar*

- `1203.3510v1` - [abs](http://arxiv.org/abs/1203.3510v1) - [pdf](http://arxiv.org/pdf/1203.3510v1)

> In many fields observations are performed irregularly along time, due to either measurement limitations or lack of a constant immanent rate. While discrete-time Markov models (as Dynamic Bayesian Networks) introduce either inefficient computation or an information loss to reasoning about such processes, continuous-time Markov models assume either a discrete state space (as Continuous-Time Bayesian Networks), or a flat continuous state space (as stochastic differential equations). To address these problems, we present a new modeling class called Irregular-Time Bayesian Networks (ITBNs), generalizing Dynamic Bayesian Networks, allowing substantially more compact representations, and increasing the expressivity of the temporal dynamics. In addition, a globally optimal solution is guaranteed when learning temporal systems, provided that they are fully observed at the same irregularly spaced time-points, and a semiparametric subclass of ITBNs is introduced to allow further adaptation to the irregular nature of the available data.

</details>

<details>

<summary>2012-03-15 11:17:56 - A Bayesian Matrix Factorization Model for Relational Data</summary>

- *Ajit P. Singh, Geoffrey Gordon*

- `1203.3517v1` - [abs](http://arxiv.org/abs/1203.3517v1) - [pdf](http://arxiv.org/pdf/1203.3517v1)

> Relational learning can be used to augment one data source with other correlated sources of information, to improve predictive accuracy. We frame a large class of relational learning problems as matrix factorization problems, and propose a hierarchical Bayesian model. Training our Bayesian model using random-walk Metropolis-Hastings is impractically slow, and so we develop a block Metropolis-Hastings sampler which uses the gradient and Hessian of the likelihood to dynamically tune the proposal. We demonstrate that a predictive model of brain response to stimuli can be improved by augmenting it with side information about the stimuli.

</details>

<details>

<summary>2012-03-15 11:17:56 - Variance-Based Rewards for Approximate Bayesian Reinforcement Learning</summary>

- *Jonathan Sorg, Satinder Singh, Richard L. Lewis*

- `1203.3518v1` - [abs](http://arxiv.org/abs/1203.3518v1) - [pdf](http://arxiv.org/pdf/1203.3518v1)

> The explore{exploit dilemma is one of the central challenges in Reinforcement Learning (RL). Bayesian RL solves the dilemma by providing the agent with information in the form of a prior distribution over environments; however, full Bayesian planning is intractable. Planning with the mean MDP is a common myopic approximation of Bayesian planning. We derive a novel reward bonus that is a function of the posterior distribution over environments, which, when added to the reward in planning with the mean MDP, results in an agent which explores efficiently and effectively. Although our method is similar to existing methods when given an uninformative or unstructured prior, unlike existing methods, our method can exploit structured priors. We prove that our method results in a polynomial sample complexity and empirically demonstrate its advantages in a structured exploration task.

</details>

<details>

<summary>2012-03-15 11:17:56 - Bayesian Inference in Monte-Carlo Tree Search</summary>

- *Gerald Tesauro, V T Rajan, Richard Segal*

- `1203.3519v1` - [abs](http://arxiv.org/abs/1203.3519v1) - [pdf](http://arxiv.org/pdf/1203.3519v1)

> Monte-Carlo Tree Search (MCTS) methods are drawing great interest after yielding breakthrough results in computer Go. This paper proposes a Bayesian approach to MCTS that is inspired by distributionfree approaches such as UCT [13], yet significantly differs in important respects. The Bayesian framework allows potentially much more accurate (Bayes-optimal) estimation of node values and node uncertainties from a limited number of simulation trials. We further propose propagating inference in the tree via fast analytic Gaussian approximation methods: this can make the overhead of Bayesian inference manageable in domains such as Go, while preserving high accuracy of expected-value estimates. We find substantial empirical outperformance of UCT in an idealized bandit-tree test environment, where we can obtain valuable insights by comparing with known ground truth. Additionally we rigorously prove on-policy and off-policy convergence of the proposed methods.

</details>

<details>

<summary>2012-03-15 11:17:56 - Bayesian Model Averaging Using the k-best Bayesian Network Structures</summary>

- *Jin Tian, Ru He, Lavanya Ram*

- `1203.3520v1` - [abs](http://arxiv.org/abs/1203.3520v1) - [pdf](http://arxiv.org/pdf/1203.3520v1)

> We study the problem of learning Bayesian network structures from data. We develop an algorithm for finding the k-best Bayesian network structures. We propose to compute the posterior probabilities of hypotheses of interest by Bayesian model averaging over the k-best Bayesian networks. We present empirical results on structural discovery over several real and synthetic data sets and show that the method outperforms the model selection method and the state of-the-art MCMC methods.

</details>

<details>

<summary>2012-03-15 11:17:56 - Learning networks determined by the ratio of prior and data</summary>

- *Maomi Ueno*

- `1203.3521v1` - [abs](http://arxiv.org/abs/1203.3521v1) - [pdf](http://arxiv.org/pdf/1203.3521v1)

> Recent reports have described that the equivalent sample size (ESS) in a Dirichlet prior plays an important role in learning Bayesian networks. This paper provides an asymptotic analysis of the marginal likelihood score for a Bayesian network. Results show that the ratio of the ESS and sample size determine the penalty of adding arcs in learning Bayesian networks. The number of arcs increases monotonically as the ESS increases; the number of arcs monotonically decreases as the ESS decreases. Furthermore, the marginal likelihood score provides a unified expression of various score metrics by changing prior knowledge.

</details>

<details>

<summary>2012-03-15 14:22:10 - Enhancing Bayesian risk prediction for epidemics using contact tracing</summary>

- *Chris Jewell, Gareth Roberts*

- `1203.3366v1` - [abs](http://arxiv.org/abs/1203.3366v1) - [pdf](http://arxiv.org/pdf/1203.3366v1)

> Contact tracing data collected from disease outbreaks has received relatively little attention in the epidemic modelling literature because it is thought to be unreliable: infection sources might be wrongly attributed, or data might be missing due to resource contraints in the questionnaire exercise. Nevertheless, these data might provide a rich source of information on disease transmission rate. This paper presents novel methodology for combining contact tracing data with rate-based contact network data to improve posterior precision, and therefore predictive accuracy. We present an advancement in Bayesian inference for epidemics that assimilates these data, and is robust to partial contact tracing. Using a simulation study based on the British poultry industry, we show how the presence of contact tracing data improves posterior predictive accuracy, and can directly inform a more effective control strategy.

</details>

<details>

<summary>2012-03-16 10:02:58 - Optimal pricing using online auction experiments: A Pólya tree approach</summary>

- *Edward I. George, Sam K. Hui*

- `1203.3653v1` - [abs](http://arxiv.org/abs/1203.3653v1) - [pdf](http://arxiv.org/pdf/1203.3653v1)

> We show how a retailer can estimate the optimal price of a new product using observed transaction prices from online second-price auction experiments. For this purpose we propose a Bayesian P\'olya tree approach which, given the limited nature of the data, requires a specially tailored implementation. Avoiding the need for a priori parametric assumptions, the P\'olya tree approach allows for flexible inference of the valuation distribution, leading to more robust estimation of optimal price than competing parametric approaches. In collaboration with an online jewelry retailer, we illustrate how our methodology can be combined with managerial prior knowledge to estimate the profit maximizing price of a new jewelry product.

</details>

<details>

<summary>2012-03-16 11:22:15 - An exact adaptive test with superior design sensitivity in an observational study of treatments for ovarian cancer</summary>

- *Paul R. Rosenbaum*

- `1203.3672v1` - [abs](http://arxiv.org/abs/1203.3672v1) - [pdf](http://arxiv.org/pdf/1203.3672v1)

> A sensitivity analysis in an observational study determines the magnitude of bias from nonrandom treatment assignment that would need to be present to alter the qualitative conclusions of a na\"{\i}ve analysis that presumes all biases were removed by matching or by other analytic adjustments. The power of a sensitivity analysis and the design sensitivity anticipate the outcome of a sensitivity analysis under an assumed model for the generation of the data. It is known that the power of a sensitivity analysis is affected by the choice of test statistic, and, in particular, that a statistic with good Pitman efficiency in a randomized experiment, such as Wilcoxon's signed rank statistic, may have low power in a sensitivity analysis and low design sensitivity when compared to other statistics. For instance, for an additive treatment effect and errors that are Normal or logistic or $t$-distributed with 3 degrees of freedom, Brown's combined quantile average test has Pitman efficiency close to that of Wilcoxon's test but has higher power in a sensitivity analysis, while a version of Noether's test has poor Pitman efficiency in a randomized experiment but much higher design sensitivity so it is vastly more powerful than Wilcoxon's statistic in a sensitivity analysis if the sample size is sufficiently large.

</details>

<details>

<summary>2012-03-19 09:09:26 - INLA or MCMC? A Tutorial and Comparative Evaluation for Spatial Prediction in log-Gaussian Cox Processes</summary>

- *Benjamin M. Taylor, Peter J. Diggle*

- `1202.1738v2` - [abs](http://arxiv.org/abs/1202.1738v2) - [pdf](http://arxiv.org/pdf/1202.1738v2)

> We investigate two options for performing Bayesian inference on spatial log-Gaussian Cox processes assuming a spatially continuous latent field: Markov chain Monte Carlo (MCMC) and the integrated nested Laplace approximation (INLA). We first describe the device of approximating a spatially continuous Gaussian field by a Gaussian Markov random field on a discrete lattice, and present a simulation study showing that, with careful choice of parameter values, small neighbourhood sizes can give excellent approximations. We then introduce the spatial log-Gaussian Cox process and describe MCMC and INLA methods for spatial prediction within this model class. We report the results of a simulation study in which we compare MALA and the technique of approximating the continuous latent field by a discrete one, followed by approximate Bayesian inference via INLA over a selection of 18 simulated scenarios. The results question the notion that the latter technique is both significantly faster and more robust than MCMC in this setting; 100,000 iterations of the MALA algorithm running in 20 minutes on a desktop PC delivered greater predictive accuracy than the default \verb=INLA= strategy, which ran in 4 minutes and gave comparative performance to the full Laplace approximation which ran in 39 minutes.

</details>

<details>

<summary>2012-03-19 09:26:34 - Instrumental Variable Bayesian Model Averaging via Conditional Bayes Factors</summary>

- *Anna Karl, Alex Lenkoski*

- `1202.5846v3` - [abs](http://arxiv.org/abs/1202.5846v3) - [pdf](http://arxiv.org/pdf/1202.5846v3)

> We develop a method to perform model averaging in two-stage linear regression systems subject to endogeneity. Our method extends an existing Gibbs sampler for instrumental variables to incorporate a component of model uncertainty. Direct evaluation of model probabilities is intractable in this setting. We show that by nesting model moves inside the Gibbs sampler, model comparison can be performed via conditional Bayes factors, leading to straightforward calculations. This new Gibbs sampler is only slightly more involved than the original algorithm and exhibits no evidence of mixing difficulties. We conclude with a study of two different modeling challenges: incorporating uncertainty into the determinants of macroeconomic growth, and estimating a demand function by instrumenting wholesale on retail prices.

</details>

<details>

<summary>2012-03-20 08:26:18 - Estimation and extrapolation of time trends in registry data---Borrowing strength from related populations</summary>

- *Andrea Riebler, Leonhard Held, Håvard Rue*

- `1108.0606v2` - [abs](http://arxiv.org/abs/1108.0606v2) - [pdf](http://arxiv.org/pdf/1108.0606v2)

> To analyze and project age-specific mortality or morbidity rates age-period-cohort (APC) models are very popular. Bayesian approaches facilitate estimation and improve predictions by assigning smoothing priors to age, period and cohort effects. Adjustments for overdispersion are straightforward using additional random effects. When rates are further stratified, for example, by countries, multivariate APC models can be used, where differences of stratum-specific effects are interpretable as log relative risks. Here, we incorporate correlated stratum-specific smoothing priors and correlated overdispersion parameters into the multivariate APC model, and use Markov chain Monte Carlo and integrated nested Laplace approximations for inference. Compared to a model without correlation, the new approach may lead to more precise relative risk estimates, as shown in an application to chronic obstructive pulmonary disease mortality in three regions of England and Wales. Furthermore, the imputation of missing data for one particular stratum may be improved, since the new approach takes advantage of the remaining strata if the corresponding observations are available there. This is shown in an application to female mortality in Denmark, Sweden and Norway from the 20th century, where we treat for each country in turn either the first or second half of the observations as missing and then impute the omitted data. The projections are compared to those obtained from a univariate APC model and an extended Lee--Carter demographic forecasting approach using the proper Dawid--Sebastiani scoring rule.

</details>

<details>

<summary>2012-03-20 08:51:50 - Robust Filtering and Smoothing with Gaussian Processes</summary>

- *Marc Peter Deisenroth, Ryan Turner, Marco F. Huber, Uwe D. Hanebeck, Carl Edward Rasmussen*

- `1203.4345v1` - [abs](http://arxiv.org/abs/1203.4345v1) - [pdf](http://arxiv.org/pdf/1203.4345v1)

> We propose a principled algorithm for robust Bayesian filtering and smoothing in nonlinear stochastic dynamic systems when both the transition function and the measurement function are described by non-parametric Gaussian process (GP) models. GPs are gaining increasing importance in signal processing, machine learning, robotics, and control for representing unknown system functions by posterior probability distributions. This modern way of "system identification" is more robust than finding point estimates of a parametric function representation. In this article, we present a principled algorithm for robust analytic smoothing in GP dynamic systems, which are increasingly used in robotics and control. Our numerical evaluations demonstrate the robustness of the proposed approach in situations where other state-of-the-art Gaussian filters and smoothers can fail.

</details>

<details>

<summary>2012-03-20 09:52:53 - Bayesian joint modeling of multiple gene networks and diverse genomic data to identify target genes of a transcription factor</summary>

- *Peng Wei, Wei Pan*

- `1203.4359v1` - [abs](http://arxiv.org/abs/1203.4359v1) - [pdf](http://arxiv.org/pdf/1203.4359v1)

> We consider integrative modeling of multiple gene networks and diverse genomic data, including protein-DNA binding, gene expression and DNA sequence data, to accurately identify the regulatory target genes of a transcription factor (TF). Rather than treating all the genes equally and independently a priori in existing joint modeling approaches, we incorporate the biological prior knowledge that neighboring genes on a gene network tend to be (or not to be) regulated together by a TF. A key contribution of our work is that, to maximize the use of all existing biological knowledge, we allow incorporation of multiple gene networks into joint modeling of genomic data by introducing a mixture model based on the use of multiple Markov random fields (MRFs). Another important contribution of our work is to allow different genomic data to be correlated and to examine the validity and effect of the independence assumption as adopted in existing methods. Due to a fully Bayesian approach, inference about model parameters can be carried out based on MCMC samples. Application to an E. coli data set, together with simulation studies, demonstrates the utility and statistical efficiency gains with the proposed joint model.

</details>

<details>

<summary>2012-03-20 12:27:04 - Bounding rare event probabilities in computer experiments</summary>

- *Yves Auffray, Pierre Barbillon, Jean-Michel Marin*

- `1105.0871v2` - [abs](http://arxiv.org/abs/1105.0871v2) - [pdf](http://arxiv.org/pdf/1105.0871v2)

> We are interested in bounding probabilities of rare events in the context of computer experiments. These rare events depend on the output of a physical model with random input variables. Since the model is only known through an expensive black box function, standard efficient Monte Carlo methods designed for rare events cannot be used. We then propose a strategy to deal with this difficulty based on importance sampling methods. This proposal relies on Kriging metamodeling and is able to achieve sharp upper confidence bounds on the rare event probabilities. The variability due to the Kriging metamodeling step is properly taken into account. The proposed methodology is applied to a toy example and compared to more standard Bayesian bounds. Finally, a challenging real case study is analyzed. It consists of finding an upper bound of the probability that the trajectory of an airborne load will collide with the aircraft that has released it.

</details>

<details>

<summary>2012-03-20 13:11:32 - Semi-Supervised Single- and Multi-Domain Regression with Multi-Domain Training</summary>

- *Tomer Michaeli, Yonina C. Eldar, Guillermo Sapiro*

- `1203.4422v1` - [abs](http://arxiv.org/abs/1203.4422v1) - [pdf](http://arxiv.org/pdf/1203.4422v1)

> We address the problems of multi-domain and single-domain regression based on distinct and unpaired labeled training sets for each of the domains and a large unlabeled training set from all domains. We formulate these problems as a Bayesian estimation with partial knowledge of statistical relations. We propose a worst-case design strategy and study the resulting estimators. Our analysis explicitly accounts for the cardinality of the labeled sets and includes the special cases in which one of the labeled sets is very large or, in the other extreme, completely missing. We demonstrate our estimators in the context of removing expressions from facial images and in the context of audio-visual word recognition, and provide comparisons to several recently proposed multi-modal learning algorithms.

</details>

<details>

<summary>2012-03-20 14:09:50 - A Bayesian measurement error model for two-channel cell-based RNAi data with replicates</summary>

- *Chung-Hsing Chen, Wen-Chi Su, Chih-Yu Chen, Jing-Ying Huang, Fang-Yu Tsai, Wen-Chang Wang, Chao A. Hsiung, King-Song Jeng, I-Shou Chang*

- `1203.4445v1` - [abs](http://arxiv.org/abs/1203.4445v1) - [pdf](http://arxiv.org/pdf/1203.4445v1)

> RNA interference (RNAi) is an endogenous cellular process in which small double-stranded RNAs lead to the destruction of mRNAs with complementary nucleoside sequence. With the production of RNAi libraries, large-scale RNAi screening in human cells can be conducted to identify unknown genes involved in a biological pathway. One challenge researchers face is how to deal with the multiple testing issue and the related false positive rate (FDR) and false negative rate (FNR). This paper proposes a Bayesian hierarchical measurement error model for the analysis of data from a two-channel RNAi high-throughput experiment with replicates, in which both the activity of a particular biological pathway and cell viability are monitored and the goal is to identify short hair-pin RNAs (shRNAs) that affect the pathway activity without affecting cell activity. Simulation studies demonstrate the flexibility and robustness of the Bayesian method and the benefits of having replicates in the experiment. This method is illustrated through analyzing the data from a RNAi high-throughput screening that searches for cellular factors affecting HCV replication without affecting cell viability; comparisons of the results from this HCV study and some of those reported in the literature are included.

</details>

<details>

<summary>2012-03-21 09:44:32 - Bayesian Nonparametric Shrinkage Applied to Cepheid Star Oscillations</summary>

- *James Berger, William H. Jefferys, Peter Müller*

- `1203.4690v1` - [abs](http://arxiv.org/abs/1203.4690v1) - [pdf](http://arxiv.org/pdf/1203.4690v1)

> Bayesian nonparametric regression with dependent wavelets has dual shrinkage properties: there is shrinkage through a dependent prior put on functional differences, and shrinkage through the setting of most of the wavelet coefficients to zero through Bayesian variable selection methods. The methodology can deal with unequally spaced data and is efficient because of the existence of fast moves in model space for the MCMC computation. The methodology is illustrated on the problem of modeling the oscillations of Cepheid variable stars; these are a class of pulsating variable stars with the useful property that their periods of variability are strongly correlated with their absolute luminosity. Once this relationship has been calibrated, knowledge of the period gives knowledge of the luminosity. This makes these stars useful as "standard candles" for estimating distances in the universe.

</details>

<details>

<summary>2012-03-21 12:38:34 - Semi-blind Sparse Image Reconstruction with Application to MRFM</summary>

- *Se Un Park, Nicolas Dobigeon, Alfred O. Hero*

- `1203.4723v1` - [abs](http://arxiv.org/abs/1203.4723v1) - [pdf](http://arxiv.org/pdf/1203.4723v1)

> We propose a solution to the image deconvolution problem where the convolution kernel or point spread function (PSF) is assumed to be only partially known. Small perturbations generated from the model are exploited to produce a few principal components explaining the PSF uncertainty in a high dimensional space. Unlike recent developments on blind deconvolution of natural images, we assume the image is sparse in the pixel basis, a natural sparsity arising in magnetic resonance force microscopy (MRFM). Our approach adopts a Bayesian Metropolis-within-Gibbs sampling framework. The performance of our Bayesian semi-blind algorithm for sparse images is superior to previously proposed semi-blind algorithms such as the alternating minimization (AM) algorithm and blind algorithms developed for natural images. We illustrate our myopic algorithm on real MRFM tobacco virus data.

</details>

<details>

<summary>2012-03-22 09:53:26 - Shrinkage Confidence Procedures</summary>

- *George Casella, J. T. Gene Hwang*

- `1203.4935v1` - [abs](http://arxiv.org/abs/1203.4935v1) - [pdf](http://arxiv.org/pdf/1203.4935v1)

> The possibility of improving on the usual multivariate normal confidence was first discussed in Stein (1962). Using the ideas of shrinkage, through Bayesian and empirical Bayesian arguments, domination results, both analytic and numerical, have been obtained. Here we trace some of the developments in confidence set estimation.

</details>

<details>

<summary>2012-03-24 20:58:48 - A Bayesian Model Committee Approach to Forecasting Global Solar Radiation</summary>

- *Philippe Lauret, Auline Rodler, Marc Muselli, Mathieu David, Hadja Diagne, Cyril Voyant*

- `1203.5446v1` - [abs](http://arxiv.org/abs/1203.5446v1) - [pdf](http://arxiv.org/pdf/1203.5446v1)

> This paper proposes to use a rather new modelling approach in the realm of solar radiation forecasting. In this work, two forecasting models: Autoregressive Moving Average (ARMA) and Neural Network (NN) models are combined to form a model committee. The Bayesian inference is used to affect a probability to each model in the committee. Hence, each model's predictions are weighted by their respective probability. The models are fitted to one year of hourly Global Horizontal Irradiance (GHI) measurements. Another year (the test set) is used for making genuine one hour ahead (h+1) out-of-sample forecast comparisons. The proposed approach is benchmarked against the persistence model. The very first results show an improvement brought by this approach.

</details>

<details>

<summary>2012-03-27 14:50:45 - Bayesian Network Enhanced with Structural Reliability Methods: Application</summary>

- *Daniel Straub, Armen Der Kiureghian*

- `1203.5985v1` - [abs](http://arxiv.org/abs/1203.5985v1) - [pdf](http://arxiv.org/pdf/1203.5985v1)

> The enhanced Bayesian network (eBN) methodology described in the companion paper facilitates the assessment of reliability and risk of engineering systems when information about the system evolves in time. We present the application of the eBN (a) to the assessment of the life-cycle reliability of a structural system, (b) to the optimization of a decision on performing measurements in that structural system, and (c) to the risk assessment of an infrastructure system subject to natural hazards and deterioration of constituent structures. In all applications, observations of system performances or the hazards are made at various points in time and the eBN efficiently includes these observations in the analysis to provide an updated probabilistic model of the system at all times.

</details>

<details>

<summary>2012-03-27 14:50:56 - Bayesian Network Enhanced with Structural Reliability Methods: Methodology</summary>

- *Daniel Straub, Armen Der Kiureghian*

- `1203.5986v1` - [abs](http://arxiv.org/abs/1203.5986v1) - [pdf](http://arxiv.org/pdf/1203.5986v1)

> We combine Bayesian networks (BNs) and structural reliability methods (SRMs) to create a new computational framework, termed enhanced Bayesian network (eBN), for reliability and risk analysis of engineering structures and infrastructure. BNs are efficient in representing and evaluating complex probabilistic dependence structures, as present in infrastructure and structural systems, and they facilitate Bayesian updating of the model when new information becomes available. On the other hand, SRMs enable accurate assessment of probabilities of rare events represented by computationally demanding, physically-based models. By combining the two methods, the eBN framework provides a unified and powerful tool for efficiently computing probabilities of rare events in complex structural and infrastructure systems in which information evolves in time. Strategies for modeling and efficiently analyzing the eBN are described by way of several conceptual examples. The companion paper applies the eBN methodology to example structural and infrastructure systems.

</details>

<details>

<summary>2012-03-28 12:46:41 - Reading Theorie Analytique des Probabilites</summary>

- *Christian P. Robert*

- `1203.6249v1` - [abs](http://arxiv.org/abs/1203.6249v1) - [pdf](http://arxiv.org/pdf/1203.6249v1)

> This note is an extended read of my read of Laplace's book Theorie Analytique des Probabilites, when considered from a Bayesian viewpoint but without historical nor comparative pretentions. A deeper analysis is provided in Dale (1999).

</details>

<details>

<summary>2012-03-28 15:34:57 - Statistical emulation of a tsunami model for sensitivity analysis and uncertainty quantification</summary>

- *A. Sarri, S. Guillas, F. Dias*

- `1203.6297v1` - [abs](http://arxiv.org/abs/1203.6297v1) - [pdf](http://arxiv.org/pdf/1203.6297v1)

> Due to the catastrophic consequences of tsunamis, early warnings need to be issued quickly in order to mitigate the hazard. Additionally, there is a need to represent the uncertainty in the predictions of tsunami characteristics corresponding to the uncertain trigger features (e.g. either position, shape and speed of a landslide, or sea floor deformation associated with an earthquake). Unfortunately, computer models are expensive to run. This leads to significant delays in predictions and makes the uncertainty quantification impractical. Statistical emulators run almost instantaneously and may represent well the outputs of the computer model. In this paper, we use the Outer Product Emulator to build a fast statistical surrogate of a landslide-generated tsunami computer model. This Bayesian framework enables us to build the emulator by combining prior knowledge of the computer model properties with a few carefully chosen model evaluations. The good performance of the emulator is validated using the Leave-One-Out method.

</details>

<details>

<summary>2012-03-30 09:05:35 - Adaptive Gaussian Mixture Filter Based on Statistical Linearization</summary>

- *Marco F. Huber*

- `1203.6750v1` - [abs](http://arxiv.org/abs/1203.6750v1) - [pdf](http://arxiv.org/pdf/1203.6750v1)

> Gaussian mixtures are a common density representation in nonlinear, non-Gaussian Bayesian state estimation. Selecting an appropriate number of Gaussian components, however, is difficult as one has to trade of computational complexity against estimation accuracy. In this paper, an adaptive Gaussian mixture filter based on statistical linearization is proposed. Depending on the nonlinearity of the considered estimation problem, this filter dynamically increases the number of components via splitting. For this purpose, a measure is introduced that allows for quantifying the locally induced linearization error at each Gaussian mixture component. The deviation between the nonlinear and the linearized state space model is evaluated for determining the splitting direction. The proposed approach is not restricted to a specific statistical linearization method. Simulations show the superior estimation performance compared to related approaches and common filtering algorithms.

</details>

<details>

<summary>2012-03-30 19:05:22 - Estimating the age of renal tumors</summary>

- *Allen B. Downey*

- `1203.6890v1` - [abs](http://arxiv.org/abs/1203.6890v1) - [pdf](http://arxiv.org/pdf/1203.6890v1)

> We present a Bayesian method for estimating the age of a renal tumor given its size. We use a model of tumor growth based on published data from observations of untreated tumors. We find, for example, that the median age of a 5 cm tumor is 20 years, with interquartile range 16-23 and 90% confidence interval 11-30 years.

</details>


## 2012-04

<details>

<summary>2012-04-02 22:51:25 - Bayesian sequential estimation of the reliability of a parallel-series system</summary>

- *Zohra Benkamra, Mekki Terbeche, Mounir Tlemcani*

- `1204.0549v1` - [abs](http://arxiv.org/abs/1204.0549v1) - [pdf](http://arxiv.org/pdf/1204.0549v1)

> We give a risk-averse solution to the problem of estimating the reliability of a parallel-series system. We adopt a beta-binomial model for components reliabilities, and assume that the total sample size for the experience is fixed. The allocation at subsystems or components level may be random. Based on the sampling schemes for parallel and series systems separately, we propose a hybrid sequential scheme for the parallel-series system. Asymptotic optimality of the Bayes risk associated with quadratic loss is proved with the help of martingale convergence properties.

</details>

<details>

<summary>2012-04-03 01:13:34 - On the Stationary Distribution of Iterative Imputations</summary>

- *Jingchen Liu, Andrew Gelman, Jennifer Hill, Yu-Sung Su*

- `1012.2902v2` - [abs](http://arxiv.org/abs/1012.2902v2) - [pdf](http://arxiv.org/pdf/1012.2902v2)

> Iterative imputation, in which variables are imputed one at a time each given a model predicting from all the others, is a popular technique that can be convenient and flexible, as it replaces a potentially difficult multivariate modeling problem with relatively simple univariate regressions.   In this paper, we begin to characterize the stationary distributions of iterative imputations and their statistical properties. More precisely, when the conditional models are compatible (defined in the text), we give a set of sufficient conditions under which the imputation distribution converges in total variation to the posterior distribution of a Bayesian model. When the conditional models are incompatible but are valid, we show that the combined imputation estimator is consistent.

</details>

<details>

<summary>2012-04-03 07:56:19 - Positive definite completion problems for directed acyclic graphs</summary>

- *Emanuel Ben-David, Bala Rajaratnam*

- `1201.0310v2` - [abs](http://arxiv.org/abs/1201.0310v2) - [pdf](http://arxiv.org/pdf/1201.0310v2)

> A positive definite completion problem pertains to determining whether the unspecified positions of a partial (or incomplete) matrix can be completed in a desired subclass of positive definite matrices. In this paper we study an important and new class of positive definite completion problems where the desired subclasses are the spaces of covariance and inverse-covariance matrices of probabilistic models corresponding to directed acyclic graph models (also known as Bayesian networks). We provide fast procedures that determine whether a partial matrix can be completed in either of these spaces and thereafter proceed to construct the completed matrices. We prove an analog of the positive definite completion result for undirected graphs in the context of directed acyclic graphs, and thus proceed to characterize the class of DAGs which can always be completed. We also proceed to give closed form expressions for the inverse and the determinant of a completed matrix as a function of only the elements of the corresponding partial matrix.

</details>

<details>

<summary>2012-04-03 11:12:52 - Application of Bayesian Hierarchical Prior Modeling to Sparse Channel Estimation</summary>

- *Niels Lovmand Pedersen, Carles Navarro Manchón, Dmitriy Shutin, Bernard Henri Fleury*

- `1204.0656v1` - [abs](http://arxiv.org/abs/1204.0656v1) - [pdf](http://arxiv.org/pdf/1204.0656v1)

> Existing methods for sparse channel estimation typically provide an estimate computed as the solution maximizing an objective function defined as the sum of the log-likelihood function and a penalization term proportional to the l1-norm of the parameter of interest. However, other penalization terms have proven to have strong sparsity-inducing properties. In this work, we design pilot-assisted channel estimators for OFDM wireless receivers within the framework of sparse Bayesian learning by defining hierarchical Bayesian prior models that lead to sparsity-inducing penalization terms. The estimators result as an application of the variational message-passing algorithm on the factor graph representing the signal model extended with the hierarchical prior models. Numerical results demonstrate the superior performance of our channel estimators as compared to traditional and state-of-the-art sparse methods.

</details>

<details>

<summary>2012-04-06 13:14:36 - Bayes and empirical Bayes: do they merge?</summary>

- *Sonia Petrone, Judith Rousseau, Catia Scricciolo*

- `1204.1470v1` - [abs](http://arxiv.org/abs/1204.1470v1) - [pdf](http://arxiv.org/pdf/1204.1470v1)

> Bayesian inference is attractive for its coherence and good frequentist properties. However, it is a common experience that eliciting a honest prior may be difficult and, in practice, people often take an {\em empirical Bayes} approach, plugging empirical estimates of the prior hyperparameters into the posterior distribution. Even if not rigorously justified, the underlying idea is that, when the sample size is large, empirical Bayes leads to "similar" inferential answers. Yet, precise mathematical results seem to be missing. In this work, we give a more rigorous justification in terms of merging of Bayes and empirical Bayes posterior distributions. We consider two notions of merging: Bayesian weak merging and frequentist merging in total variation. Since weak merging is related to consistency, we provide sufficient conditions for consistency of empirical Bayes posteriors. Also, we show that, under regularity conditions, the empirical Bayes procedure asymptotically selects the value of the hyperparameter for which the prior mostly favors the "truth". Examples include empirical Bayes density estimation with Dirichlet process mixtures.

</details>

<details>

<summary>2012-04-06 21:58:47 - Bayesian Centroid Estimation for Motif Discovery</summary>

- *Luis E. Carvalho*

- `1204.1571v1` - [abs](http://arxiv.org/abs/1204.1571v1) - [pdf](http://arxiv.org/pdf/1204.1571v1)

> Biological sequences may contain patterns that are signal important biomolecular functions; a classical example is regulation of gene expression by transcription factors that bind to specific patterns in genomic promoter regions. In motif discovery we are given a set of sequences that share a common motif and aim to identify not only the motif composition, but also the binding sites in each sequence of the set. We present a Bayesian model that is an extended version of the model adopted by the Gibbs motif sampler, and propose a new centroid estimator that arises from a refined and meaningful loss function for binding site inference. We discuss the main advantages of centroid estimation for motif discovery, including computational convenience, and how its principled derivation offers further insights about the posterior distribution of binding site configurations. We also illustrate, using simulated and real datasets, that the centroid estimator can differ from the maximum a posteriori estimator.

</details>

<details>

<summary>2012-04-07 21:09:48 - The threshold EM algorithm for parameter learning in bayesian network with incomplete data</summary>

- *Fradj Ben Lamine, Karim Kalti, Mohamed Ali Mahjoub*

- `1204.1681v1` - [abs](http://arxiv.org/abs/1204.1681v1) - [pdf](http://arxiv.org/pdf/1204.1681v1)

> Bayesian networks (BN) are used in a big range of applications but they have one issue concerning parameter learning. In real application, training data are always incomplete or some nodes are hidden. To deal with this problem many learning parameter algorithms are suggested foreground EM, Gibbs sampling and RBE algorithms. In order to limit the search space and escape from local maxima produced by executing EM algorithm, this paper presents a learning parameter algorithm that is a fusion of EM and RBE algorithms. This algorithm incorporates the range of a parameter into the EM algorithm. This range is calculated by the first step of RBE algorithm allowing a regularization of each parameter in bayesian network after the maximization step of the EM algorithm. The threshold EM algorithm is applied in brain tumor diagnosis and show some advantages and disadvantages over the EM algorithm.

</details>

<details>

<summary>2012-04-09 15:02:44 - Accounting for the Uncertainty in the Evaluation of Percentile Ranks</summary>

- *Loet Leydesdorff*

- `1204.1894v1` - [abs](http://arxiv.org/abs/1204.1894v1) - [pdf](http://arxiv.org/pdf/1204.1894v1)

> In a recent paper entitled "Inconsistencies of Recently Proposed Citation Impact Indicators and how to Avoid Them," Schreiber (2012, at arXiv:1202.3861) proposed (i) a method to assess tied ranks consistently and (ii) fractional attribution to percentile ranks in the case of relatively small samples (e.g., for n < 100). Schreiber's solution to the problem of how to handle tied ranks is convincing, in my opinion (cf. Pudovkin & Garfield, 2009). The fractional attribution, however, is computationally intensive and cannot be done manually for even moderately large batches of documents. Schreiber attributed scores fractionally to the six percentile rank classes used in the Science and Engineering Indicators of the U.S. National Science Board, and thus missed, in my opinion, the point that fractional attribution at the level of hundred percentiles-or equivalently quantiles as the continuous random variable-is only a linear, and therefore much less complex problem. Given the quantile-values, the non-linear attribution to the six classes or any other evaluation scheme is then a question of aggregation. A new routine based on these principles (including Schreiber's solution for tied ranks) is made available as software for the assessment of documents retrieved from the Web of Science (at http://www.leydesdorff.net/software/i3).

</details>

<details>

<summary>2012-04-11 18:53:58 - Concept Modeling with Superwords</summary>

- *Khalid El-Arini, Emily B. Fox, Carlos Guestrin*

- `1204.2523v1` - [abs](http://arxiv.org/abs/1204.2523v1) - [pdf](http://arxiv.org/pdf/1204.2523v1)

> In information retrieval, a fundamental goal is to transform a document into concepts that are representative of its content. The term "representative" is in itself challenging to define, and various tasks require different granularities of concepts. In this paper, we aim to model concepts that are sparse over the vocabulary, and that flexibly adapt their content based on other relevant semantic information such as textual structure or associated image features. We explore a Bayesian nonparametric model based on nested beta processes that allows for inferring an unknown number of strictly sparse concepts. The resulting model provides an inherently different representation of concepts than a standard LDA (or HDP) based topic model, and allows for direct incorporation of semantic features. We demonstrate the utility of this representation on multilingual blog data and the Congressional Record.

</details>

<details>

<summary>2012-04-11 22:58:46 - Probabilistic Latent Tensor Factorization Model for Link Pattern Prediction in Multi-relational Networks</summary>

- *Sheng Gao, Ludovic Denoyer, Patrick Gallinari*

- `1204.2588v1` - [abs](http://arxiv.org/abs/1204.2588v1) - [pdf](http://arxiv.org/pdf/1204.2588v1)

> This paper aims at the problem of link pattern prediction in collections of objects connected by multiple relation types, where each type may play a distinct role. While common link analysis models are limited to single-type link prediction, we attempt here to capture the correlations among different relation types and reveal the impact of various relation types on performance quality. For that, we define the overall relations between object pairs as a \textit{link pattern} which consists in interaction pattern and connection structure in the network, and then use tensor formalization to jointly model and predict the link patterns, which we refer to as \textit{Link Pattern Prediction} (LPP) problem. To address the issue, we propose a Probabilistic Latent Tensor Factorization (PLTF) model by introducing another latent factor for multiple relation types and furnish the Hierarchical Bayesian treatment of the proposed probabilistic model to avoid overfitting for solving the LPP problem. To learn the proposed model we develop an efficient Markov Chain Monte Carlo sampling method. Extensive experiments are conducted on several real world datasets and demonstrate significant improvements over several existing state-of-the-art methods.

</details>

<details>

<summary>2012-04-12 21:07:18 - Moving Taylor Bayesian Regression for nonparametric multidimensional function estimation with possibly correlated errors</summary>

- *Jobst Heitzig*

- `1204.2841v1` - [abs](http://arxiv.org/abs/1204.2841v1) - [pdf](http://arxiv.org/pdf/1204.2841v1)

> We present a nonparametric method for estimating the value and several derivatives of an unknown, sufficiently smooth real-valued function of real-valued arguments from a finite sample of points, where both the function arguments and the corresponding values are known only up to measurement errors having some assumed distribution and correlation structure. The method, Moving Taylor Bayesian Regression (MOTABAR), uses Bayesian updating to find the posterior mean of the coefficients of a Taylor polynomial of the function at a moving position of interest. When measurement errors are neglected, MOTABAR becomes a multivariate interpolation method. It contains several well-known regression and interpolation methods as special or limit cases. We demonstrate the performance of MOTABAR using the reconstruction of the Lorenz attractor from noisy observations as an example.

</details>

<details>

<summary>2012-04-14 02:29:41 - Selection of tuning parameters in bridge regression models via Bayesian information criterion</summary>

- *Shuichi Kawano*

- `1203.4326v3` - [abs](http://arxiv.org/abs/1203.4326v3) - [pdf](http://arxiv.org/pdf/1203.4326v3)

> We consider the bridge linear regression modeling, which can produce a sparse or non-sparse model. A crucial point in the model building process is the selection of adjusted parameters including a regularization parameter and a tuning parameter in bridge regression models. The choice of the adjusted parameters can be viewed as a model selection and evaluation problem. We propose a model selection criterion for evaluating bridge regression models in terms of Bayesian approach. This selection criterion enables us to select the adjusted parameters objectively. We investigate the effectiveness of our proposed modeling strategy through some numerical examples.

</details>

<details>

<summary>2012-04-16 17:40:40 - Kernels for Vector-Valued Functions: a Review</summary>

- *Mauricio A. Alvarez, Lorenzo Rosasco, Neil D. Lawrence*

- `1106.6251v2` - [abs](http://arxiv.org/abs/1106.6251v2) - [pdf](http://arxiv.org/pdf/1106.6251v2)

> Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.

</details>

<details>

<summary>2012-04-17 02:15:15 - The open-faced sandwich adjustment for MCMC using estimating functions</summary>

- *Benjamin A Shaby*

- `1204.3687v1` - [abs](http://arxiv.org/abs/1204.3687v1) - [pdf](http://arxiv.org/pdf/1204.3687v1)

> The situation frequently arises where working with the likelihood function is problematic. This can happen for several reasons---perhaps the likelihood is prohibitively computationally expensive, perhaps it lacks some robustness property, or perhaps it is simply not known for the model under consideration. In these cases, it is often possible to specify alternative functions of the parameters and the data that can be maximized to obtain asymptotically normal estimates. However, these scenarios present obvious problems if one is interested in applying Bayesian techniques. Here we describe open-faced sandwich adjustment, a way to incorporate a wide class of non-likelihood objective functions within Bayesian-like models to obtain asymptotically valid parameter estimates and inference via MCMC. Two simulation examples show that the method provides accurate frequentist uncertainty estimates. The open-faced sandwich adjustment is applied to a Poisson spatio-temporal model to analyze an ornithology dataset from the citizen science initiative eBird.

</details>

<details>

<summary>2012-04-18 07:03:04 - A hierarchical latent class model for predicting disability small area counts from survey data</summary>

- *Enrico Fabrizi, Giorgio E. Montanari, Maria Giovanna Ranalli*

- `1204.3993v1` - [abs](http://arxiv.org/abs/1204.3993v1) - [pdf](http://arxiv.org/pdf/1204.3993v1)

> This article considers the estimation of the number of severely disabled people using data from the Italian survey on Health Conditions and Appeal to Medicare. Disability is indirectly measured using a set of categorical items, which survey a set of functions concerning the ability of a person to accomplish everyday tasks. Latent Class Models can be employed to classify the population according to different levels of a latent variable connected with disability. The survey, however, is designed to provide reliable estimates at the level of Administrative Regions (NUTS2 level), while local authorities are interested in quantifying the amount of population that belongs to each latent class at a sub-regional level. Therefore, small area estimation techniques should be used. The challenge of the present application is that the variable of interest is not directly observed. Adopting a full Bayesian approach, we base small area estimation on a Latent Class model in which the probability of belonging to each latent class changes with covariates and the influence of age is learnt from the data using penalized splines. Deimmler-Reisch bases are shown to improve speed and mixing of MCMC chains used to simulate posteriors.

</details>

<details>

<summary>2012-04-19 02:59:03 - EP-GIG Priors and Applications in Bayesian Sparse Learning</summary>

- *Zhihua Zhang, Shusen Wang, Dehua Liu, Michael I. Jordan*

- `1204.4243v1` - [abs](http://arxiv.org/abs/1204.4243v1) - [pdf](http://arxiv.org/pdf/1204.4243v1)

> In this paper we propose a novel framework for the construction of sparsity-inducing priors. In particular, we define such priors as a mixture of exponential power distributions with a generalized inverse Gaussian density (EP-GIG). EP-GIG is a variant of generalized hyperbolic distributions, and the special cases include Gaussian scale mixtures and Laplace scale mixtures. Furthermore, Laplace scale mixtures can subserve a Bayesian framework for sparse learning with nonconvex penalization. The densities of EP-GIG can be explicitly expressed. Moreover, the corresponding posterior distribution also follows a generalized inverse Gaussian distribution. These properties lead us to EM algorithms for Bayesian sparse learning. We show that these algorithms bear an interesting resemblance to iteratively re-weighted $\ell_2$ or $\ell_1$ methods. In addition, we present two extensions for grouped variable selection and logistic regression.

</details>

<details>

<summary>2012-04-19 13:20:10 - The Discrete Infinite Logistic Normal Distribution</summary>

- *John Paisley, Chong Wang, David Blei*

- `1103.4789v3` - [abs](http://arxiv.org/abs/1103.4789v3) - [pdf](http://arxiv.org/pdf/1103.4789v3)

> We present the discrete infinite logistic normal distribution (DILN), a Bayesian nonparametric prior for mixed membership models. DILN is a generalization of the hierarchical Dirichlet process (HDP) that models correlation structure between the weights of the atoms at the group level. We derive a representation of DILN as a normalized collection of gamma-distributed random variables, and study its statistical properties. We consider applications to topic modeling and derive a variational inference algorithm for approximate posterior inference. We study the empirical performance of the DILN topic model on four corpora, comparing performance with the HDP and the correlated topic model (CTM). To deal with large-scale data sets, we also develop an online inference algorithm for DILN and compare with online HDP and online LDA on the Nature magazine, which contains approximately 350,000 articles.

</details>

<details>

<summary>2012-04-19 20:13:03 - On an Approach to Bayesian Sample Sizing in Clinical Trials</summary>

- *Robb J. Muirhead, Adina I. Soaita*

- `1204.4460v1` - [abs](http://arxiv.org/abs/1204.4460v1) - [pdf](http://arxiv.org/pdf/1204.4460v1)

> This paper explores an approach to Bayesian sample size determination in clinical trials. The approach falls into the category of what is often called "proper Bayesian", in that it does not mix frequentist concepts with Bayesian ones. A criterion for a "successful trial" is defined in terms of a posterior probability, its probability is assessed using the marginal distribution of the data, and this probability forms the basis for choosing sample sizes. We illustrate with a standard problem in clinical trials, that of establishing superiority of a new drug over a control.

</details>

<details>

<summary>2012-04-20 03:01:56 - A Privacy-Aware Bayesian Approach for Combining Classifier and Cluster Ensembles</summary>

- *Ayan Acharya, Eduardo R. Hruschka, Joydeep Ghosh*

- `1204.4521v1` - [abs](http://arxiv.org/abs/1204.4521v1) - [pdf](http://arxiv.org/pdf/1204.4521v1)

> This paper introduces a privacy-aware Bayesian approach that combines ensembles of classifiers and clusterers to perform semi-supervised and transductive learning. We consider scenarios where instances and their classification/clustering results are distributed across different data sites and have sharing restrictions. As a special case, the privacy aware computation of the model when instances of the target data are distributed across different data sites, is also discussed. Experimental results show that the proposed approach can provide good classification accuracies while adhering to the data/model sharing constraints.

</details>

<details>

<summary>2012-04-20 19:19:05 - Efficient hierarchical clustering for continuous data</summary>

- *Ricardo Henao, Joseph E. Lucas*

- `1204.4708v1` - [abs](http://arxiv.org/abs/1204.4708v1) - [pdf](http://arxiv.org/pdf/1204.4708v1)

> We present an new sequential Monte Carlo sampler for coalescent based Bayesian hierarchical clustering. Our model is appropriate for modeling non-i.i.d. data and offers a substantial reduction of computational cost when compared to the original sampler without resorting to approximations. We also propose a quadratic complexity approximation that in practice shows almost no loss in performance compared to its counterpart. We show that as a byproduct of our formulation, we obtain a greedy algorithm that exhibits performance improvement over other greedy algorithms, particularly in small data sets. In order to exploit the correlation structure of the data, we describe how to incorporate Gaussian process priors in the model as a flexible way to model non-i.i.d. data. Results on artificial and real data show significant improvements over closely related approaches.

</details>

<details>

<summary>2012-04-24 02:11:01 - Modeling, dependence, classification, united statistical science, many cultures</summary>

- *Emanuel Parzen, Subhadeep Mukhopadhyay*

- `1204.4699v3` - [abs](http://arxiv.org/abs/1204.4699v3) - [pdf](http://arxiv.org/pdf/1204.4699v3)

> Breiman (2001) proposed to statisticians awareness of two cultures: 1. Parametric modeling culture, pioneered by R.A.Fisher and Jerzy Neyman; 2. Algorithmic predictive culture, pioneered by machine learning research.   Parzen (2001), as a part of discussing Breiman (2001), proposed that researchers be aware of many cultures, including the focus of our research: 3. Nonparametric, quantile based, information theoretic modeling. We provide a unification of many statistical methods for traditional small data sets and emerging big data sets in terms of comparison density, copula density, measure of dependence, correlation, information, new measures (called LP score comoments) that apply to long tailed distributions with out finite second order moments. A very important goal is to unify methods for discrete and continuous random variables. Our research extends these methods to modern high dimensional data modeling.

</details>

<details>

<summary>2012-04-24 20:02:53 - Sequential design of computer experiments for the estimation of a probability of failure</summary>

- *Julien Bect, David Ginsbourger, Ling Li, Victor Picheny, Emmanuel Vazquez*

- `1009.5177v2` - [abs](http://arxiv.org/abs/1009.5177v2) - [pdf](http://arxiv.org/pdf/1009.5177v2)

> This paper deals with the problem of estimating the volume of the excursion set of a function $f:\mathbb{R}^d \to \mathbb{R}$ above a given threshold, under a probability measure on $\mathbb{R}^d$ that is assumed to be known. In the industrial world, this corresponds to the problem of estimating a probability of failure of a system. When only an expensive-to-simulate model of the system is available, the budget for simulations is usually severely limited and therefore classical Monte Carlo methods ought to be avoided. One of the main contributions of this article is to derive SUR (stepwise uncertainty reduction) strategies from a Bayesian-theoretic formulation of the problem of estimating a probability of failure. These sequential strategies use a Gaussian process model of $f$ and aim at performing evaluations of $f$ as efficiently as possible to infer the value of the probability of failure. We compare these strategies to other strategies also based on a Gaussian process model for estimating a probability of failure.

</details>

<details>

<summary>2012-04-29 19:12:26 - Marginally Specified Priors for Nonparametric Bayesian Estimation</summary>

- *David C. Kessler, Peter D. Hoff, David B. Dunson*

- `1204.6505v1` - [abs](http://arxiv.org/abs/1204.6505v1) - [pdf](http://arxiv.org/pdf/1204.6505v1)

> Prior specification for nonparametric Bayesian inference involves the difficult task of quantifying prior knowledge about a parameter of high, often infinite, dimension. Realistically, a statistician is unlikely to have informed opinions about all aspects of such a parameter, but may have real information about functionals of the parameter, such the population mean or variance. This article proposes a new framework for nonparametric Bayes inference in which the prior distribution for a possibly infinite-dimensional parameter is decomposed into two parts: an informative prior on a finite set of functionals, and a nonparametric conditional prior for the parameter given the functionals. Such priors can be easily constructed from standard nonparametric prior distributions in common use, and inherit the large support of the standard priors upon which they are based. Additionally, posterior approximations under these informative priors can generally be made via minor adjustments to existing Markov chain approximation algorithms for standard nonparametric prior distributions. We illustrate the use of such priors in the context of multivariate density estimation using Dirichlet process mixture models, and in the modeling of high-dimensional sparse contingency tables.

</details>

<details>

<summary>2012-04-30 06:54:58 - Likelihood-Free Parallel Tempering</summary>

- *Meili Baragatti, Agnès Grimaud, Denys Pommeret*

- `1108.3423v3` - [abs](http://arxiv.org/abs/1108.3423v3) - [pdf](http://arxiv.org/pdf/1108.3423v3)

> Approximate Bayesian Computational (ABC) methods (or likelihood-free methods) have appeared in the past fifteen years as useful methods to perform Bayesian analyses when the likelihood is analytically or computationally intractable. Several ABC methods have been proposed: Monte Carlo Markov Chains (MCMC) methods have been developped by Marjoramet al. (2003) and by Bortotet al. (2007) for instance, and sequential methods have been proposed among others by Sissonet al. (2007), Beaumont et al. (2009) and Del Moral et al. (2009). Until now, while ABC-MCMC methods remain the reference, sequential ABC methods have appeared to outperforms them (see for example McKinley et al. (2009) or Sisson et al. (2007)). In this paper a new algorithm combining population-based MCMC methods with ABC requirements is proposed, using an analogy with the Parallel Tempering algorithm (Geyer, 1991). Performances are compared with existing ABC algorithms on simulations and on a real example.

</details>


## 2012-05

<details>

<summary>2012-05-03 06:27:01 - Bayesian clustering in decomposable graphs</summary>

- *Luke Bornn, François Caron*

- `1005.5081v2` - [abs](http://arxiv.org/abs/1005.5081v2) - [pdf](http://arxiv.org/pdf/1005.5081v2)

> In this paper we propose a class of prior distributions on decomposable graphs, allowing for improved modeling flexibility. While existing methods solely penalize the number of edges, the proposed work empowers practitioners to control clustering, level of separation, and other features of the graph. Emphasis is placed on a particular prior distribution which derives its motivation from the class of product partition models; the properties of this prior relative to existing priors is examined through theory and simulation. We then demonstrate the use of graphical models in the field of agriculture, showing how the proposed prior distribution alleviates the inflexibility of previous approaches in properly modeling the interactions between the yield of different crop varieties.

</details>

<details>

<summary>2012-05-08 21:59:39 - Nonparametric Bayesian Approaches to Non-homogeneous Hidden Markov Models</summary>

- *Abhra Sarkar, Anindya Bhadra, Bani K. Mallick*

- `1205.1839v1` - [abs](http://arxiv.org/abs/1205.1839v1) - [pdf](http://arxiv.org/pdf/1205.1839v1)

> In this article a flexible Bayesian non-parametric model is proposed for non-homogeneous hidden Markov models. The model is developed through the amalgamation of the ideas of hidden Markov models and predictor dependent stick-breaking processes. Computation is carried out using auxiliary variable representation of the model which enable us to perform exact MCMC sampling from the posterior. Furthermore, the model is extended to the situation when the predictors can simultaneously in influence the transition dynamics of the hidden states as well as the emission distribution. Estimates of few steps ahead conditional predictive distributions of the response have been used as performance diagnostics for these models. The proposed methodology is illustrated through simulation experiments as well as analysis of a real data set concerned with the prediction of rainfall induced malaria epidemics.

</details>

<details>

<summary>2012-05-09 12:39:59 - A unified minimax result for restricted parameter spaces</summary>

- *Éric Marchand, William E. Strawderman*

- `1205.1964v1` - [abs](http://arxiv.org/abs/1205.1964v1) - [pdf](http://arxiv.org/pdf/1205.1964v1)

> We provide a development that unifies, simplifies and extends considerably a number of minimax results in the restricted parameter space literature. Various applications follow, such as that of estimating location or scale parameters under a lower (or upper) bound restriction, location parameter vectors restricted to a polyhedral cone, scale parameters subject to restricted ratios or products, linear combinations of restricted location parameters, location parameters bounded to an interval with unknown scale, quantiles for location-scale families with parametric restrictions and restricted covariance matrices.

</details>

<details>

<summary>2012-05-09 15:13:59 - Learning Continuous-Time Social Network Dynamics</summary>

- *Yu Fan, Christian R. Shelton*

- `1205.2648v1` - [abs](http://arxiv.org/abs/1205.2648v1) - [pdf](http://arxiv.org/pdf/1205.2648v1)

> We demonstrate that a number of sociology models for social network dynamics can be viewed as continuous time Bayesian networks (CTBNs). A sampling-based approximate inference method for CTBNs can be used as the basis of an expectation-maximization procedure that achieves better accuracy in estimating the parameters of the model than the standard method of moments algorithmfromthe sociology literature. We extend the existing social network models to allow for indirect and asynchronous observations of the links. A Markov chain Monte Carlo sampling algorithm for this new model permits estimation and inference. We provide results on both a synthetic network (for verification) and real social network data.

</details>

<details>

<summary>2012-05-09 15:30:07 - Bayesian Discovery of Linear Acyclic Causal Models</summary>

- *Patrik O. Hoyer, Antti Hyttinen*

- `1205.2641v1` - [abs](http://arxiv.org/abs/1205.2641v1) - [pdf](http://arxiv.org/pdf/1205.2641v1)

> Methods for automated discovery of causal relationships from non-interventional data have received much attention recently. A widely used and well understood model family is given by linear acyclic causal models (recursive structural equation models). For Gaussian data both constraint-based methods (Spirtes et al., 1993; Pearl, 2000) (which output a single equivalence class) and Bayesian score-based methods (Geiger and Heckerman, 1994) (which assign relative scores to the equivalence classes) are available. On the contrary, all current methods able to utilize non-Gaussianity in the data (Shimizu et al., 2006; Hoyer et al., 2008) always return only a single graph or a single equivalence class, and so are fundamentally unable to express the degree of certainty attached to that output. In this paper we develop a Bayesian score-based approach able to take advantage of non-Gaussianity when estimating linear acyclic causal models, and we empirically demonstrate that, at least on very modest size networks, its accuracy is as good as or better than existing methods. We provide a complete code package (in R) which implements all algorithms and performs all of the analysis provided in the paper, and hope that this will further the application of these methods to solving causal inference problems.

</details>

<details>

<summary>2012-05-09 17:24:52 - Virtual Vector Machine for Bayesian Online Classification</summary>

- *Thomas P. Minka, Rongjing Xiang, Yuan, Qi*

- `1205.2623v1` - [abs](http://arxiv.org/abs/1205.2623v1) - [pdf](http://arxiv.org/pdf/1205.2623v1)

> In a typical online learning scenario, a learner is required to process a large data stream using a small memory buffer. Such a requirement is usually in conflict with a learner's primary pursuit of prediction accuracy. To address this dilemma, we introduce a novel Bayesian online classi cation algorithm, called the Virtual Vector Machine. The virtual vector machine allows you to smoothly trade-off prediction accuracy with memory size. The virtual vector machine summarizes the information contained in the preceding data stream by a Gaussian distribution over the classi cation weights plus a constant number of virtual data points. The virtual data points are designed to add extra non-Gaussian information about the classi cation weights. To maintain the constant number of virtual points, the virtual vector machine adds the current real data point into the virtual point set, merges two most similar virtual points into a new virtual point or deletes a virtual point that is far from the decision boundary. The information lost in this process is absorbed into the Gaussian distribution. The extra information provided by the virtual points leads to improved predictive accuracy over previous online classification algorithms.

</details>

<details>

<summary>2012-05-09 18:25:09 - BPR: Bayesian Personalized Ranking from Implicit Feedback</summary>

- *Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme*

- `1205.2618v1` - [abs](http://arxiv.org/abs/1205.2618v1) - [pdf](http://arxiv.org/pdf/1205.2618v1)

> Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.

</details>

<details>

<summary>2012-05-09 18:33:52 - Computing Posterior Probabilities of Structural Features in Bayesian Networks</summary>

- *Jin Tian, Ru He*

- `1205.2612v1` - [abs](http://arxiv.org/abs/1205.2612v1) - [pdf](http://arxiv.org/pdf/1205.2612v1)

> We study the problem of learning Bayesian network structures from data. Koivisto and Sood (2004) and Koivisto (2006) presented algorithms that can compute the exact marginal posterior probability of a subnetwork, e.g., a single edge, in O(n2n) time and the posterior probabilities for all n(n-1) potential edges in O(n2n) total time, assuming that the number of parents per node or the indegree is bounded by a constant. One main drawback of their algorithms is the requirement of a special structure prior that is non uniform and does not respect Markov equivalence. In this paper, we develop an algorithm that can compute the exact posterior probability of a subnetwork in O(3n) time and the posterior probabilities for all n(n-1) potential edges in O(n3n) total time. Our algorithm also assumes a bounded indegree but allows general structure priors. We demonstrate the applicability of the algorithm on several data sets with up to 20 variables.

</details>

<details>

<summary>2012-05-09 18:42:32 - Statistical Methods for Astronomy</summary>

- *Eric D. Feigelson, G. Jogesh Babu*

- `1205.2064v1` - [abs](http://arxiv.org/abs/1205.2064v1) - [pdf](http://arxiv.org/pdf/1205.2064v1)

> This review outlines concepts of mathematical statistics, elements of probability theory, hypothesis tests and point estimation for use in the analysis of modern astronomical data. Least squares, maximum likelihood, and Bayesian approaches to statistical inference are treated. Resampling methods, particularly the bootstrap, provide valuable procedures when distributions functions of statistics are not known. Several approaches to model selection and good- ness of fit are considered. Applied statistics relevant to astronomical research are briefly discussed: nonparametric methods for use when little is known about the behavior of the astronomical populations or processes; data smoothing with kernel density estimation and nonparametric regression; unsupervised clustering and supervised classification procedures for multivariate problems; survival analysis for astronomical datasets with nondetections; time- and frequency-domain times series analysis for light curves; and spatial statistics to interpret the spatial distributions of points in low dimensions. Two types of resources are presented: about 40 recommended texts and monographs in various fields of statistics, and the public domain R software system for statistical analysis. Together with its \sim 3500 (and growing) add-on CRAN packages, R implements a vast range of statistical procedures in a coherent high-level language with advanced graphics.

</details>

<details>

<summary>2012-05-09 18:43:56 - The Infinite Latent Events Model</summary>

- *David Wingate, Noah Goodman, Daniel Roy, Joshua Tenenbaum*

- `1205.2604v1` - [abs](http://arxiv.org/abs/1205.2604v1) - [pdf](http://arxiv.org/pdf/1205.2604v1)

> We present the Infinite Latent Events Model, a nonparametric hierarchical Bayesian distribution over infinite dimensional Dynamic Bayesian Networks with binary state representations and noisy-OR-like transitions. The distribution can be used to learn structure in discrete timeseries data by simultaneously inferring a set of latent events, which events fired at each timestep, and how those events are causally linked. We illustrate the model on a sound factorization task, a network topology identification task, and a video game task.

</details>

<details>

<summary>2012-05-10 13:04:41 - The Pólya sum kernel and Bayes estimation</summary>

- *Mathias Rafler*

- `1202.4696v2` - [abs](http://arxiv.org/abs/1202.4696v2) - [pdf](http://arxiv.org/pdf/1202.4696v2)

> We consider a particular Cox process from a Bayesian viewpoint and show that the Bayes estimator of the intensity measure is the so-called P\'olya sum kernel, which occurred recently in the context of the construction of the so-called Papangelou processes. More precisely, if the prior, the directing measure of the Cox process, is a Poisson-Gamma random measure, then the posterior is again a Poisson-Gamma random measure and the Bayes estimator of the intensity is the P\'olya sum kernel. Moreover, we extend this result to doubly stochastic Poisson-Gamma priors and give conditions under which one can identify the Bayes estimator for the intensity.

</details>

<details>

<summary>2012-05-11 00:35:41 - Nonparametric Bayesian Inference on Bivariate Extremes</summary>

- *Simon Guillotte, Francois Perron, Johan Segers*

- `0911.3270v3` - [abs](http://arxiv.org/abs/0911.3270v3) - [pdf](http://arxiv.org/pdf/0911.3270v3)

> The tail of a bivariate distribution function in the domain of attraction of a bivariate extreme-value distribution may be approximated by the one of its extreme-value attractor. The extreme-value attractor has margins that belong to a three-parameter family and a dependence structure which is characterised by a spectral measure, that is a probability measure on the unit interval with mean equal to one half. As an alternative to parametric modelling of the spectral measure, we propose an infinite-dimensional model which is at the same time manageable and still dense within the class of spectral measures. Inference is done in a Bayesian framework, using the censored-likelihood approach. In particular, we construct a prior distribution on the class of spectral measures and develop a trans-dimensional Markov chain Monte Carlo algorithm for numerical computations. The method provides a bivariate predictive density which can be used for predicting the extreme outcomes of the bivariate distribution. In a practical perspective, this is useful for computing rare event probabilities and extreme conditional quantiles. The methodology is validated by simulations and applied to a data-set of Danish fire insurance claims.

</details>

<details>

<summary>2012-05-11 12:40:45 - Tobit Bayesian Model Averaging and the Determinants of Foreign Direct Investment</summary>

- *Alexander Jordan, Alex Lenkoski*

- `1205.2501v1` - [abs](http://arxiv.org/abs/1205.2501v1) - [pdf](http://arxiv.org/pdf/1205.2501v1)

> We develop a fully Bayesian, computationally efficient framework for incorporating model uncertainty into Type II Tobit models and apply this to the investigation of the determinants of Foreign Direct Investment (FDI). While direct evaluation of modelprobabilities is intractable in this setting, we show that by using conditional Bayes Factors, which nest model moves inside a Gibbs sampler, we are able to incorporate model uncertainty in a straight-forward fashion. We conclude with a study of global FDI flows between 1988-2000.

</details>

<details>

<summary>2012-05-12 07:50:17 - A Multivariate Graphical Stochastic Volatility Model</summary>

- *Yuan Cheng, Alex Lenkoski*

- `1205.2746v1` - [abs](http://arxiv.org/abs/1205.2746v1) - [pdf](http://arxiv.org/pdf/1205.2746v1)

> The Gaussian Graphical Model (GGM) is a popular tool for incorporating sparsity into joint multivariate distributions. The G-Wishart distribution, a conjugate prior for precision matrices satisfying general GGM constraints, has now been in existence for over a decade. However, due to the lack of a direct sampler, its use has been limited in hierarchical Bayesian contexts, relegating mixing over the class of GGMs mostly to situations involving standard Gaussian likelihoods. Recent work, however, has developed methods that couple model and parameter moves, first through reversible jump methods and later by direct evaluation of conditional Bayes factors and subsequent resampling. Further, methods for avoiding prior normalizing constant calculations--a serious bottleneck and source of numerical instability--have been proposed. We review and clarify these developments and then propose a new methodology for GGM comparison that blends many recent themes. Theoretical developments and computational timing experiments reveal an algorithm that has limited computational demands and dramatically improves on computing times of existing methods. We conclude by developing a parsimonious multivariate stochastic volatility model that embeds GGM uncertainty in a larger hierarchical framework. The method is shown to be capable of adapting to the extreme swings in market volatility experienced in 2008 after the collapse of Lehman Brothers, offering considerable improvement in posterior predictive distribution calibration.

</details>

<details>

<summary>2012-05-12 21:45:54 - Bayesian modeling of temporal dependence in large sparse contingency tables</summary>

- *Tsuyoshi Kunihama, David B. Dunson*

- `1205.2816v1` - [abs](http://arxiv.org/abs/1205.2816v1) - [pdf](http://arxiv.org/pdf/1205.2816v1)

> In many applications, it is of interest to study trends over time in relationships among categorical variables, such as age group, ethnicity, religious affiliation, political party and preference for particular policies. At each time point, a sample of individuals provide responses to a set of questions, with different individuals sampled at each time. In such settings, there tends to be abundant missing data and the variables being measured may change over time. At each time point, one obtains a large sparse contingency table, with the number of cells often much larger than the number of individuals being surveyed. To borrow information across time in modeling large sparse contingency tables, we propose a Bayesian autoregressive tensor factorization approach. The proposed model relies on a probabilistic Parafac factorization of the joint pmf characterizing the categorical data distribution at each time point, with autocorrelation included across times. Efficient computational methods are developed relying on MCMC. The methods are evaluated through simulation examples and applied to social survey data.

</details>

<details>

<summary>2012-05-15 18:36:56 - A Bayesian Model of NMR Spectra for the Deconvolution and Quantification of Metabolites in Complex Biological Mixtures</summary>

- *William Astle, Maria De Iorio, Sylvia Richardson, David Stephens, Timothy Ebbels*

- `1105.2204v3` - [abs](http://arxiv.org/abs/1105.2204v3) - [pdf](http://arxiv.org/pdf/1105.2204v3)

> Nuclear Magnetic Resonance (NMR) spectra are widely used in metabolomics to obtain profiles of metabolites dissolved in biofluids such as cell supernatants. Methods for estimating metabolite concentrations from these spectra are presently confined to manual peak fitting and to binning procedures for integrating resonance peaks. Extensive information on the patterns of spectral resonance generated by human metabolites is now available in online databases. By incorporating this information into a Bayesian model we can deconvolve resonance peaks from a spectrum and obtain explicit concentration estimates for the corresponding metabolites. Spectral resonances that cannot be deconvolved in this way may also be of scientific interest so we model them jointly using wavelets.   We describe a Markov chain Monte Carlo algorithm which allows us to sample from the joint posterior distribution of the model parameters, using specifically designed block updates to improve mixing. The strong prior on resonance patterns allows the algorithm to identify peaks corresponding to particular metabolites automatically, eliminating the need for manual peak assignment.   We assess our method for peak alignment and concentration estimation. Except in cases when the target resonance signal is very weak, alignment is unbiased and precise. We compare the Bayesian concentration estimates to those obtained from a conventional numerical integration method and find that our point estimates have sixfold lower mean squared error.   Finally, we apply our method to a spectral dataset taken from an investigation of the metabolic response of yeast to recombinant protein expression. We estimate the concentrations of 26 metabolites and compare to manual quantification by five expert spectroscopists. We discuss the reason for discrepancies and the robustness of our methods concentration estimates.

</details>

<details>

<summary>2012-05-16 11:52:37 - Locally adaptive spatial smoothing using conditional autoregressive models</summary>

- *Duncan Lee, Richard Mitchell*

- `1205.3641v1` - [abs](http://arxiv.org/abs/1205.3641v1) - [pdf](http://arxiv.org/pdf/1205.3641v1)

> Conditional autoregressive (CAR) models are commonly used to capture spatial correlation in areal unit data, and are typically specified as a prior distribution for a set of random effects, as part of a hierarchical Bayesian model. The spatial correlation structure induced by these models is determined by geographical adjacency, so that two areas have correlated random effects if they share a common border. However, this correlation structure is too simplistic for real data, which are instead likely to include sub-regions of strong correlation as well as locations at which the response exhibits a step-change. Therefore this paper proposes an extension to CAR priors, which can capture such localised spatial correlation. The proposed approach takes the form of an iterative algorithm, which sequentially updates the spatial correlation structure in the data as well as estimating the remaining model parameters. The efficacy of the approach is assessed by simulation, and its utility is illustrated in a disease mapping context, using data on respiratory disease risk in Greater Glasgow, Scotland.

</details>

<details>

<summary>2012-05-18 12:06:26 - Linear regression model selection using p-values when the model dimension grows</summary>

- *Piotr Pokarowski, Jan Mielniczuk, Paweł Teisseyre*

- `1205.4146v1` - [abs](http://arxiv.org/abs/1205.4146v1) - [pdf](http://arxiv.org/pdf/1205.4146v1)

> We consider a new criterion-based approach to model selection in linear regression. Properties of selection criteria based on p-values of a likelihood ratio statistic are studied for families of linear regression models. We prove that such procedures are consistent i.e. the minimal true model is chosen with probability tending to 1 even when the number of models under consideration slowly increases with a sample size. The simulation study indicates that introduced methods perform promisingly when compared with Akaike and Bayesian Information Criteria.

</details>

<details>

<summary>2012-05-20 23:56:17 - Sparse Signal Recovery in the Presence of Intra-Vector and Inter-Vector Correlation</summary>

- *Bhaskar D. Rao, Zhilin Zhang, Yuzhe Jin*

- `1205.4471v1` - [abs](http://arxiv.org/abs/1205.4471v1) - [pdf](http://arxiv.org/pdf/1205.4471v1)

> This work discusses the problem of sparse signal recovery when there is correlation among the values of non-zero entries. We examine intra-vector correlation in the context of the block sparse model and inter-vector correlation in the context of the multiple measurement vector model, as well as their combination. Algorithms based on the sparse Bayesian learning are presented and the benefits of incorporating correlation at the algorithm level are discussed. The impact of correlation on the limits of support recovery is also discussed highlighting the different impact intra-vector and inter-vector correlations have on such limits.

</details>

<details>

<summary>2012-05-21 07:32:50 - Parameter Exploration in Simulation Experiments: A Bayesian Framework</summary>

- *Jessica W. Leigh, David Bryant*

- `1205.4503v1` - [abs](http://arxiv.org/abs/1205.4503v1) - [pdf](http://arxiv.org/pdf/1205.4503v1)

> Simulations often involve the use of model parameters which are unknown or uncertain. For this reason, simulation experiments are often repeated for multiple combinations of parameter values, often iterating through parameter values lying on a fixed grid. However, the use of a discrete grid places limits on the dimension of the parameter space and creates the potential to miss important parameter combinations which fall in the gaps between grid points. Here we draw parallels with strategies for numerical integration and describe a Markov chain Monte-Carlo strategy for exploring parameter values. We illustrate the approach using examples from phylogenetics, archaeology, and epidemiology.

</details>

<details>

<summary>2012-05-21 19:40:51 - Bayes Factor Consistency for Unbalanced ANOVA Models</summary>

- *Min Wang, Xiaoqian Sun*

- `1204.1567v2` - [abs](http://arxiv.org/abs/1204.1567v2) - [pdf](http://arxiv.org/pdf/1204.1567v2)

> In practical situations, most experimental designs often yield unbalanced data which have different numbers of observations per unit because of cost constraints, or missing data, etc. In this paper, we consider the Bayesian approach to hypothesis testing or model selection under the one-way unbalanced fixed-effects ANOVA model. We adopt Zellner's g-prior with the beta-prime distribution for g, which results in an explicit closed-form expression of the Bayes factor without integral representation. Furthermore, we investigate the model selection consistency of the Bayes factor under three different asymptotic scenarios: either the number of units goes to infinity, the number of observations per unit goes to infinity, or both go to infinity. The results presented extend some existing ones of the Bayes factor for the balanced ANOVA models in the literature.

</details>

<details>

<summary>2012-05-22 05:36:49 - A Conversation with Eugenio Regazzini</summary>

- *Antonio Lijoi, Igor Prünster*

- `1205.4807v1` - [abs](http://arxiv.org/abs/1205.4807v1) - [pdf](http://arxiv.org/pdf/1205.4807v1)

> Eugenio Regazzini was born on August 12, 1946 in Cremona (Italy), and took his degree in 1969 at the University "L. Bocconi" of Milano. He has held positions at the universities of Torino, Bologna and Milano, and at the University "L. Bocconi" as assistant professor and lecturer from 1974 to 1980, and then professor since 1980. He is currently professor in probability and mathematical statistics at the University of Pavia. In the periods 1989-2001 and 2006-2009 he was head of the Institute for Applications of Mathematics and Computer Science of the Italian National Research Council (C.N.R.) in Milano and head of the Department of Mathematics at the University of Pavia, respectively. For twelve years between 1989 and 2006, he served as a member of the Scientific Board of the Italian Mathematical Union (U.M.I.). In 2007, he was elected Fellow of the IMS and, in 2001, Fellow of the "Istituto Lombardo---Accademia di Scienze e Lettere." His research activity in probability and statistics has covered a wide spectrum of topics, including finitely additive probabilities, foundations of the Bayesian paradigm, exchangeability and partial exchangeability, distribution of functionals of random probability measures, stochastic integration, history of probability and statistics. Overall, he has been one of the most authoritative developers of de Finetti's legacy. In the last five years, he has extended his scientific interests to probabilistic methods in mathematical physics; in particular, he has studied the asymptotic behavior of the solutions of equations, which are of interest for the kinetic theory of gases. The present interview was taken in occasion of his 65th birthday.

</details>

<details>

<summary>2012-05-22 15:59:54 - A Bayesian Mixture of Lasso Regressions with t-Errors</summary>

- *Alberto Cozzini, Ajay Jasra, Giovanni Montana*

- `1205.4955v1` - [abs](http://arxiv.org/abs/1205.4955v1) - [pdf](http://arxiv.org/pdf/1205.4955v1)

> Motivated by a challenging problem in financial trading we are presented with a mixture of regressions with variable selection problem. In this regard, one is faced with data which possess outliers, skewness and, simultaneously, due to the nature of financial trading, one would like to be able to construct clusters with specific predictors that are fairly sparse. We develop a Bayesian mixture of lasso regressions with $t-$errors to reflect these specific demands. The resulting model is necessarily complex and to fit the model to real data, we develop a state-of-the-art Particle Markov chain Monte Carlo (PMCMC) algorithm based upon sequential Monte Carlo (SMC) methods. The model and algorithm are investigated on both simulated and real data.

</details>

<details>

<summary>2012-05-23 01:19:37 - Bayesian Vertex Nomination</summary>

- *Dominic S. Lee, Carey E. Priebe*

- `1205.5082v1` - [abs](http://arxiv.org/abs/1205.5082v1) - [pdf](http://arxiv.org/pdf/1205.5082v1)

> Consider an attributed graph whose vertices are colored green or red, but only a few are observed to be red. The color of the other vertices is unobserved. Typically, the unknown total number of red vertices is small. The vertex nomination problem is to nominate one of the unobserved vertices as being red. The edge set of the graph is a subset of the set of unordered pairs of vertices. Suppose that each edge is also colored green or red and this is observed for all edges. The context statistic of a vertex is defined as the number of observed red vertices connected to it, and its content statistic is the number of red edges incident to it. Assuming that these statistics are independent between vertices and that red edges are more likely between red vertices, Coppersmith and Priebe (2012) proposed a likelihood model based on these statistics. Here, we formulate a Bayesian model using the proposed likelihood together with prior distributions chosen for the unknown parameters and unobserved vertex colors. From the resulting posterior distribution, the nominated vertex is the one with the highest posterior probability of being red. Inference is conducted using a Metropolis-within-Gibbs algorithm, and performance is illustrated by a simulation study. Results show that (i) the Bayesian model performs significantly better than chance; (ii) the probability of correct nomination increases with increasing posterior probability that the nominated vertex is red; and (iii) the Bayesian model either matches or performs better than the method in Coppersmith and Priebe. An application example is provided using the Enron email corpus, where vertices represent Enron employees and their associates, observed red vertices are known fraudsters, red edges represent email communications perceived as fraudulent, and we wish to identify one of the latent vertices as most likely to be a fraudster.

</details>

<details>

<summary>2012-05-24 14:38:52 - Transformed Gaussian Markov Random Fields and Spatial Modeling</summary>

- *Marcos O. Prates, Dipak K. Dey, Michael R. Willig, Jun Yan*

- `1205.5467v1` - [abs](http://arxiv.org/abs/1205.5467v1) - [pdf](http://arxiv.org/pdf/1205.5467v1)

> The Gaussian random field (GRF) and the Gaussian Markov random field (GMRF) have been widely used to accommodate spatial dependence under the generalized linear mixed model framework. These models have limitations rooted in the symmetry and thin tail of the Gaussian distribution. We introduce a new class of random fields, termed transformed GRF (TGRF), and a new class of Markov random fields, termed transformed GMRF (TGMRF). They are constructed by transforming the margins of GRFs and GMRFs, respectively, to desired marginal distributions to accommodate asymmetry and heavy tail as needed in practice. The Gaussian copula that characterizes the dependence structure facilitates inferences and applications in modeling spatial dependence. This construction leads to new models such as gamma or beta Markov fields with Gaussian copulas, which can be used to model Poisson intensity or Bernoulli rate in a spatial generalized linear mixed model. The method is naturally implemented in a Bayesian framework. We illustrate the utility of the methodology in an ecological application with spatial count data and spatial presence/absence data of some snail species, where the new models are shown to outperform the traditional spatial models. The validity of Bayesian inferences and model selection are assessed through simulation studies for both spatial Poisson regression and spatial Bernoulli regression.

</details>

<details>

<summary>2012-05-28 11:05:51 - Semi-supervised logistic discrimination for functional data</summary>

- *Shuichi Kawano, Sadanori Konishi*

- `1102.4399v3` - [abs](http://arxiv.org/abs/1102.4399v3) - [pdf](http://arxiv.org/pdf/1102.4399v3)

> Multi-class classification methods based on both labeled and unlabeled functional data sets are discussed. We present a semi-supervised logistic model for classification in the context of functional data analysis. Unknown parameters in our proposed model are estimated by regularization with the help of EM algorithm. A crucial point in the modeling procedure is the choice of a regularization parameter involved in the semi-supervised functional logistic model. In order to select the adjusted parameter, we introduce model selection criteria from information-theoretic and Bayesian viewpoints. Monte Carlo simulations and a real data analysis are given to examine the effectiveness of our proposed modeling strategy.

</details>

<details>

<summary>2012-05-29 08:41:12 - The semiparametric Bernstein-von Mises theorem</summary>

- *P. J. Bickel, B. J. K. Kleijn*

- `1007.0179v3` - [abs](http://arxiv.org/abs/1007.0179v3) - [pdf](http://arxiv.org/pdf/1007.0179v3)

> In a smooth semiparametric estimation problem, the marginal posterior for the parameter of interest is expected to be asymptotically normal and satisfy frequentist criteria of optimality if the model is endowed with a suitable prior. It is shown that, under certain straightforward and interpretable conditions, the assertion of Le Cam's acclaimed, but strictly parametric, Bernstein-von Mises theorem [Univ. California Publ. Statist. 1 (1953) 277-329] holds in the semiparametric situation as well. As a consequence, Bayesian point-estimators achieve efficiency, for example, in the sense of H\'{a}jek's convolution theorem [Z. Wahrsch. Verw. Gebiete 14 (1970) 323-330]. The model is required to satisfy differentiability and metric entropy conditions, while the nuisance prior must assign nonzero mass to certain Kullback-Leibler neighborhoods [Ghosal, Ghosh and van der Vaart Ann. Statist. 28 (2000) 500-531]. In addition, the marginal posterior is required to converge at parametric rate, which appears to be the most stringent condition in examples. The results are applied to estimation of the linear coefficient in partial linear regression, with a Gaussian prior on a smoothness class for the nuisance.

</details>

<details>

<summary>2012-05-29 12:30:09 - BATMAN-an R package for the automated quantification of metabolites from NMR spectra using a Bayesian Model</summary>

- *Jie Hao, William Astle, Maria De Iorio, Timothy Ebbels*

- `1112.5794v2` - [abs](http://arxiv.org/abs/1112.5794v2) - [pdf](http://arxiv.org/pdf/1112.5794v2)

> Motivation: NMR spectra are widely used in metabolomics to obtain metabolite profiles in complex biological mixtures. Common methods used to assign and estimate concentrations of metabolites involve either an expert manual peak fitting or extra pre-processing steps, such as peak alignment and binning. Peak fitting is very time consuming and is subject to human error. Conversely, alignment and binning can introduce artefacts and limit immediate biological interpretation of models. Results: We present the Bayesian AuTomated Metabolite Analyser for NMR spectra (BATMAN), an R package which deconvolutes peaks from 1-dimensional NMR spectra, automatically assigns them to specific metabolites from a target list and obtains concentration estimates. The Bayesian model incorporates information on charac-teristic peak patterns of metabolites and is able to account for shifts in the position of peaks commonly seen in NMR spectra of biological samples. It applies a Markov Chain Monte Carlo (MCMC) algorithm to sample from a joint posterior distribution of the model parameters and obtains concentration estimates with reduced error compared with conventional numerical integration and comparable to manual deconvolution by experienced spectroscopists. Availability: http://www1.imperial.ac.uk/medicine/people/t.ebbels/ Contact: t.ebbels@imperial.ac.uk

</details>


## 2012-06

<details>

<summary>2012-06-03 14:43:57 - Sampling decomposable graphs using a Markov chain on junction trees</summary>

- *Peter J. Green, Alun Thomas*

- `1104.4079v4` - [abs](http://arxiv.org/abs/1104.4079v4) - [pdf](http://arxiv.org/pdf/1104.4079v4)

> Full Bayesian computational inference for model determination in undirected graphical models is currently restricted to decomposable graphs, except for problems of very small scale. In this paper we develop new, more efficient methodology for such inference, by making two contributions to the computational geometry of decomposable graphs. The first of these provides sufficient conditions under which it is possible to completely connect two disconnected complete subsets of vertices, or perform the reverse procedure, yet maintain decomposability of the graph. The second is a new Markov chain Monte Carlo sampler for arbitrary positive distributions on decomposable graphs, taking a junction tree representing the graph as its state variable. The resulting methodology is illustrated with numerical experiments on three specific models.

</details>

<details>

<summary>2012-06-04 23:13:35 - Two stage design for estimating the product of means with cost in the case of the exponential family</summary>

- *Zohra Benkamra, Mekki Terbeche, Mounir Tlemcani*

- `1202.5325v2` - [abs](http://arxiv.org/abs/1202.5325v2) - [pdf](http://arxiv.org/pdf/1202.5325v2)

> We investigate the problem of estimating the product of means of independent populations from the one parameter exponential family in a Bayesian framework. We give a random design which allocates mi the number of observations from population Pi such that the Bayes risk associated with squared error loss and cost per unit observation is as small as possible. The design is shown to be asymptotically optimal.

</details>

<details>

<summary>2012-06-05 10:05:09 - Testing linear hypotheses in high-dimensional regressions</summary>

- *Z. Bai, D. Jiang, J. Yao, S. Zheng*

- `1206.0867v1` - [abs](http://arxiv.org/abs/1206.0867v1) - [pdf](http://arxiv.org/pdf/1206.0867v1)

> For a multivariate linear model, Wilk's likelihood ratio test (LRT) constitutes one of the cornerstone tools. However, the computation of its quantiles under the null or the alternative requires complex analytic approximations and more importantly, these distributional approximations are feasible only for moderate dimension of the dependent variable, say $p\le 20$. On the other hand, assuming that the data dimension $p$ as well as the number $q$ of regression variables are fixed while the sample size $n$ grows, several asymptotic approximations are proposed in the literature for Wilk's $\bLa$ including the widely used chi-square approximation. In this paper, we consider necessary modifications to Wilk's test in a high-dimensional context, specifically assuming a high data dimension $p$ and a large sample size $n$. Based on recent random matrix theory, the correction we propose to Wilk's test is asymptotically Gaussian under the null and simulations demonstrate that the corrected LRT has very satisfactory size and power, surely in the large $p$ and large $n$ context, but also for moderately large data dimensions like $p=30$ or $p=50$. As a byproduct, we give a reason explaining why the standard chi-square approximation fails for high-dimensional data. We also introduce a new procedure for the classical multiple sample significance test in MANOVA which is valid for high-dimensional data.

</details>

<details>

<summary>2012-06-07 17:54:06 - Multiple Kernel Learning: A Unifying Probabilistic Viewpoint</summary>

- *Hannes Nickisch, Matthias Seeger*

- `1103.0897v3` - [abs](http://arxiv.org/abs/1103.0897v3) - [pdf](http://arxiv.org/pdf/1103.0897v3)

> We present a probabilistic viewpoint to multiple kernel learning unifying well-known regularised risk approaches and recent advances in approximate Bayesian inference relaxations. The framework proposes a general objective function suitable for regression, robust regression and classification that is lower bound of the marginal likelihood and contains many regularised risk approaches as special cases. Furthermore, we derive an efficient and provably convergent optimisation algorithm.

</details>

<details>

<summary>2012-06-08 18:14:58 - Dynamic Bayesian Combination of Multiple Imperfect Classifiers</summary>

- *Edwin Simpson, Stephen Roberts, Ioannis Psorakis, Arfon Smith*

- `1206.1831v1` - [abs](http://arxiv.org/abs/1206.1831v1) - [pdf](http://arxiv.org/pdf/1206.1831v1)

> Classifier combination methods need to make best use of the outputs of multiple, imperfect classifiers to enable higher accuracy classifications. In many situations, such as when human decisions need to be combined, the base decisions can vary enormously in reliability. A Bayesian approach to such uncertain combination allows us to infer the differences in performance between individuals and to incorporate any available prior knowledge about their abilities when training data is sparse. In this paper we explore Bayesian classifier combination, using the computationally efficient framework of variational Bayesian inference. We apply the approach to real data from a large citizen science project, Galaxy Zoo Supernovae, and show that our method far outperforms other established approaches to imperfect decision combination. We go on to analyse the putative community structure of the decision makers, based on their inferred decision making strategies, and show that natural groupings are formed. Finally we present a dynamic Bayesian classifier combination approach and investigate the changes in base classifier performance over time.

</details>

<details>

<summary>2012-06-09 05:26:24 - "Not only defended but also applied": The perceived absurdity of Bayesian inference</summary>

- *Andrew Gelman, Christian P. Robert*

- `1006.5366v5` - [abs](http://arxiv.org/abs/1006.5366v5) - [pdf](http://arxiv.org/pdf/1006.5366v5)

> The missionary zeal of many Bayesians of old has been matched, in the other direction, by a view among some theoreticians that Bayesian methods are absurd-not merely misguided but obviously wrong in principle. We consider several examples, beginning with Feller's classic text on probability theory and continuing with more recent cases such as the perceived Bayesian nature of the so-called doomsday argument. We analyze in this note the intellectual background behind various misconceptions about Bayesian statistics, without aiming at a complete historical coverage of the reasons for this dismissal.

</details>

<details>

<summary>2012-06-11 13:57:44 - Gradient statistic: higher-order asymptotics and Bartlett-type correction</summary>

- *Tiago M. Vargas, Silvia L. P. Ferrari, Artur J. Lemonte*

- `1206.2206v1` - [abs](http://arxiv.org/abs/1206.2206v1) - [pdf](http://arxiv.org/pdf/1206.2206v1)

> We obtain an asymptotic expansion for the null distribution function of thegradient statistic for testing composite null hypotheses in the presence of nuisance parameters. The expansion is derived using a Bayesian route based on the shrinkage argument described in Ghosh and Mukerjee (1991). Using this expansion, we propose a Bartlett-type corrected gradient statistic with chi-square distribution up to an error of order o(n^{-1}) under the null hypothesis. Further, we also use the expansion to modify the percentage points of the large sample reference chi-square distribution. A small Monte Carlo experiment and various examples are presented and discussed.

</details>

<details>

<summary>2012-06-12 06:06:42 - Nonparametric quantile regression for twice censored data</summary>

- *Stanislav Volgushev, Holger Dette*

- `1007.3376v2` - [abs](http://arxiv.org/abs/1007.3376v2) - [pdf](http://arxiv.org/pdf/1007.3376v2)

> We consider the problem of nonparametric quantile regression for twice censored data. Two new estimates are presented, which are constructed by applying concepts of monotone rearrangements to estimates of the conditional distribution function. The proposed methods avoid the problem of crossing quantile curves. Weak uniform consistency and weak convergence is established for both estimates and their finite sample properties are investigated by means of a simulation study. As a by-product, we obtain a new result regarding the weak convergence of the Beran estimator for right censored data on the maximal possible domain, which is of its own interest.

</details>

<details>

<summary>2012-06-13 04:29:27 - A Graphical View of Bayesian Variable Selection</summary>

- *Zaili Fang, Inyoung Kim*

- `1206.2715v1` - [abs](http://arxiv.org/abs/1206.2715v1) - [pdf](http://arxiv.org/pdf/1206.2715v1)

> In recent years, Ising prior with the network information for the "in" or "out" binary random variable in Bayesian variable selections has received more and more attentions. In this paper, we discover that even without the informative prior a Bayesian variable selection problem itself can be considered as a complete graph and described by a Ising model with random interactions. There are many advantages of treating variable selection as a graphical model, such as it is easy to employ the single site updating as well as the cluster updating algorithm, suitable for problems with small sample size and larger variable number, easy to extend to nonparametric regression models and incorporate graphical prior information and so on. In a Bayesian variable selection Ising model the interactions are determined by the linear model coefficients, so we systematically study the performance of different scale normal mixture priors for the model coefficients by adopting the global-local shrinkage strategy. Our results prove that the best prior of the model coefficients in terms of variable selection should maintain substantial weight on small shrinkage instead of large shrinkage. We also discuss the connection between the tempering algorithms for Ising models and the global-local shrinkage approach, showing that the shrinkage parameter plays a tempering role. The methods are illustrated with simulated and real data.

</details>

<details>

<summary>2012-06-13 15:11:00 - Gibbs Sampling in Factorized Continuous-Time Markov Processes</summary>

- *Tal El-Hay, Nir Friedman, Raz Kupferman*

- `1206.3251v1` - [abs](http://arxiv.org/abs/1206.3251v1) - [pdf](http://arxiv.org/pdf/1206.3251v1)

> A central task in many applications is reasoning about processes that change over continuous time. Continuous-Time Bayesian Networks is a general compact representation language for multi-component continuous-time processes. However, exact inference in such processes is exponential in the number of components, and thus infeasible for most models of interest. Here we develop a novel Gibbs sampling procedure for multi-component processes. This procedure iteratively samples a trajectory for one of the components given the remaining ones. We show how to perform exact sampling that adapts to the natural time scale of the sampled process. Moreover, we show that this sampling procedure naturally exploits the structure of the network to reduce the computational cost of each step. This procedure is the first that can provide asymptotically unbiased approximation in such processes.

</details>

<details>

<summary>2012-06-13 15:11:36 - Convex Point Estimation using Undirected Bayesian Transfer Hierarchies</summary>

- *Gal Elidan, Ben Packer, Geremy Heitz, Daphne Koller*

- `1206.3252v1` - [abs](http://arxiv.org/abs/1206.3252v1) - [pdf](http://arxiv.org/pdf/1206.3252v1)

> When related learning tasks are naturally arranged in a hierarchy, an appealing approach for coping with scarcity of instances is that of transfer learning using a hierarchical Bayes framework. As fully Bayesian computations can be difficult and computationally demanding, it is often desirable to use posterior point estimates that facilitate (relatively) efficient prediction. However, the hierarchical Bayes framework does not always lend itself naturally to this maximum aposteriori goal. In this work we propose an undirected reformulation of hierarchical Bayes that relies on priors in the form of similarity measures. We introduce the notion of "degree of transfer" weights on components of these similarity measures, and show how they can be automatically learned within a joint probabilistic framework. Importantly, our reformulation results in a convex objective for many learning problems, thus facilitating optimal posterior point estimation using standard optimization techniques. In addition, we no longer require proper priors, allowing for flexible and straightforward specification of joint distributions over transfer hierarchies. We show that our framework is effective for learning models that are part of transfer hierarchies for two real-life tasks: object shape modeling using Gaussian density estimation and document classification.

</details>

<details>

<summary>2012-06-13 15:37:30 - Bayesian Out-Trees</summary>

- *Tony S. Jebara*

- `1206.3269v1` - [abs](http://arxiv.org/abs/1206.3269v1) - [pdf](http://arxiv.org/pdf/1206.3269v1)

> A Bayesian treatment of latent directed graph structure for non-iid data is provided where each child datum is sampled with a directed conditional dependence on a single unknown parent datum. The latent graph structure is assumed to lie in the family of directed out-tree graphs which leads to efficient Bayesian inference. The latent likelihood of the data and its gradients are computable in closed form via Tutte's directed matrix tree theorem using determinants and inverses of the out-Laplacian. This novel likelihood subsumes iid likelihood, is exchangeable and yields efficient unsupervised and semi-supervised learning algorithms. In addition to handling taxonomy and phylogenetic datasets the out-tree assumption performs surprisingly well as a semi-parametric density estimator on standard iid datasets. Experiments with unsupervised and semisupervised learning are shown on various UCI and taxonomy datasets.

</details>

<details>

<summary>2012-06-13 15:42:35 - The Phylogenetic Indian Buffet Process: A Non-Exchangeable Nonparametric Prior for Latent Features</summary>

- *Kurt T. Miller, Thomas Griffiths, Michael I. Jordan*

- `1206.3279v1` - [abs](http://arxiv.org/abs/1206.3279v1) - [pdf](http://arxiv.org/pdf/1206.3279v1)

> Nonparametric Bayesian models are often based on the assumption that the objects being modeled are exchangeable. While appropriate in some applications (e.g., bag-of-words models for documents), exchangeability is sometimes assumed simply for computational reasons; non-exchangeable models might be a better choice for applications based on subject matter. Drawing on ideas from graphical models and phylogenetics, we describe a non-exchangeable prior for a class of nonparametric latent feature models that is nearly as efficient computationally as its exchangeable counterpart. Our model is applicable to the general setting in which the dependencies between objects can be expressed using a tree, where edge lengths indicate the strength of relationships. We demonstrate an application to modeling probabilistic choice.

</details>

<details>

<summary>2012-06-13 15:45:39 - Learning the Bayesian Network Structure: Dirichlet Prior versus Data</summary>

- *Harald Steck*

- `1206.3287v1` - [abs](http://arxiv.org/abs/1206.3287v1) - [pdf](http://arxiv.org/pdf/1206.3287v1)

> In the Bayesian approach to structure learning of graphical models, the equivalent sample size (ESS) in the Dirichlet prior over the model parameters was recently shown to have an important effect on the maximum-a-posteriori estimate of the Bayesian network structure. In our first contribution, we theoretically analyze the case of large ESS-values, which complements previous work: among other results, we find that the presence of an edge in a Bayesian network is favoured over its absence even if both the Dirichlet prior and the data imply independence, as long as the conditional empirical distribution is notably different from uniform. In our second contribution, we focus on realistic ESS-values, and provide an analytical approximation to the "optimal" ESS-value in a predictive sense (its accuracy is also validated experimentally): this approximation provides an understanding as to which properties of the data have the main effect determining the "optimal" ESS-value.

</details>

<details>

<summary>2012-06-13 15:56:12 - Hybrid Variational/Gibbs Collapsed Inference in Topic Models</summary>

- *Max Welling, Yee Whye Teh, Hilbert Kappen*

- `1206.3297v1` - [abs](http://arxiv.org/abs/1206.3297v1) - [pdf](http://arxiv.org/pdf/1206.3297v1)

> Variational Bayesian inference and (collapsed) Gibbs sampling are the two important classes of inference algorithms for Bayesian networks. Both have their advantages and disadvantages: collapsed Gibbs sampling is unbiased but is also inefficient for large count values and requires averaging over many samples to reduce variance. On the other hand, variational Bayesian inference is efficient and accurate for large count values but suffers from bias for small counts. We propose a hybrid algorithm that combines the best of both worlds: it samples very small counts and applies variational updates to large counts. This hybridization is shown to significantly improve testset perplexity relative to variational inference at no computational cost.

</details>

<details>

<summary>2012-06-14 04:10:10 - An Adaptive Interacting Wang-Landau Algorithm for Automatic Density Exploration</summary>

- *Luke Bornn, Pierre Jacob, Pierre Del Moral, Arnaud Doucet*

- `1109.3829v3` - [abs](http://arxiv.org/abs/1109.3829v3) - [pdf](http://arxiv.org/pdf/1109.3829v3)

> While statisticians are well-accustomed to performing exploratory analysis in the modeling stage of an analysis, the notion of conducting preliminary general-purpose exploratory analysis in the Monte Carlo stage (or more generally, the model-fitting stage) of an analysis is an area which we feel deserves much further attention. Towards this aim, this paper proposes a general-purpose algorithm for automatic density exploration. The proposed exploration algorithm combines and expands upon components from various adaptive Markov chain Monte Carlo methods, with the Wang-Landau algorithm at its heart. Additionally, the algorithm is run on interacting parallel chains -- a feature which both decreases computational cost as well as stabilizes the algorithm, improving its ability to explore the density. Performance is studied in several applications. Through a Bayesian variable selection example, the authors demonstrate the convergence gains obtained with interacting chains. The ability of the algorithm's adaptive proposal to induce mode-jumping is illustrated through a trimodal density and a Bayesian mixture modeling application. Lastly, through a 2D Ising model, the authors demonstrate the ability of the algorithm to overcome the high correlations encountered in spatial models.

</details>

<details>

<summary>2012-06-14 14:51:29 - Significance testing in quantile regression</summary>

- *Stanislav Volgushev, Melanie Birke, Holger Dette, Natalie Neumeyer*

- `1206.3125v1` - [abs](http://arxiv.org/abs/1206.3125v1) - [pdf](http://arxiv.org/pdf/1206.3125v1)

> We consider the problem of testing significance of predictors in multivariate nonparametric quantile regression. A stochastic process is proposed, which is based on a comparison of the responses with a nonparametric quantile regression estimate under the null hypothesis. It is demonstrated that under the null hypothesis this process converges weakly to a centered Gaussian process and the asymptotic properties of the test under fixed and local alternatives are also discussed. In particular we show, that - in contrast to the nonparametric approach based on estimation of $L^2$-distances - the new test is able to detect local alternatives which converge to the null hypothesis with any rate $a_n \to 0$ such that $a_n \sqrt{n} \to \infty$ (here $n$ denotes the sample size). We also present a small simulation study illustrating the finite sample properties of a bootstrap version of the the corresponding Kolmogorov-Smirnov test.

</details>

<details>

<summary>2012-06-14 15:05:55 - Revisiting k-means: New Algorithms via Bayesian Nonparametrics</summary>

- *Brian Kulis, Michael I. Jordan*

- `1111.0352v2` - [abs](http://arxiv.org/abs/1111.0352v2) - [pdf](http://arxiv.org/pdf/1111.0352v2)

> Bayesian models offer great flexibility for clustering applications---Bayesian nonparametrics can be used for modeling infinite mixtures, and hierarchical Bayesian models can be utilized for sharing clusters across multiple data sets. For the most part, such flexibility is lacking in classical clustering methods such as k-means. In this paper, we revisit the k-means clustering algorithm from a Bayesian nonparametric viewpoint. Inspired by the asymptotic connection between k-means and mixtures of Gaussians, we show that a Gibbs sampling algorithm for the Dirichlet process mixture approaches a hard clustering algorithm in the limit, and further that the resulting algorithm monotonically minimizes an elegant underlying k-means-like clustering objective that includes a penalty for the number of clusters. We generalize this analysis to the case of clustering multiple data sets through a similar asymptotic argument with the hierarchical Dirichlet process. We also discuss further extensions that highlight the benefits of our analysis: i) a spectral relaxation involving thresholded eigenvectors, and ii) a normalized cut graph clustering algorithm that does not fix the number of clusters in the graph.

</details>

<details>

<summary>2012-06-15 18:44:40 - Guilt by Association: Finding Cosmic Ray Sources Using Hierarchical Bayesian Clustering</summary>

- *Kunlaya Soiaporn, David Chernoff, Thomas Loredo, David Ruppert, Ira Wasserman*

- `1206.3540v1` - [abs](http://arxiv.org/abs/1206.3540v1) - [pdf](http://arxiv.org/pdf/1206.3540v1)

> The Earth is continuously showered by charged cosmic ray particles, naturally produced atomic nuclei moving with velocity close to the speed of light. Among these are ultra high energy cosmic ray particles with energy exceeding 5x10^19 eV, which is ten million times more energetic than the most energetic particles produced at the Large Hadron Collider. Astrophysical questions include: what phenomenon accelerates particles to such high energies, and what sort of nuclei are energized? Also, the magnetic deflection of the trajectories of the cosmic rays makes them potential probes of galactic and intergalactic magnetic fields. We develop a Bayesian hierarchical model that can be used to compare different association models between the cosmic rays and source population, using Bayes factors. A measurement model with directional uncertainties and accounting for non-uniform sky exposure is incoporated into the model. The methodology allows us to learn about astrophysical parameters, such as those governing the source luminosity function and the cosmic magnetic field.

</details>

<details>

<summary>2012-06-18 14:40:38 - Bayesian Nonexhaustive Learning for Online Discovery and Modeling of Emerging Classes</summary>

- *Murat Dundar, Ferit Akova, Alan Qi, Bartek Rajwa*

- `1206.4600v1` - [abs](http://arxiv.org/abs/1206.4600v1) - [pdf](http://arxiv.org/pdf/1206.4600v1)

> We present a framework for online inference in the presence of a nonexhaustively defined set of classes that incorporates supervised classification with class discovery and modeling. A Dirichlet process prior (DPP) model defined over class distributions ensures that both known and unknown class distributions originate according to a common base distribution. In an attempt to automatically discover potentially interesting class formations, the prior model is coupled with a suitably chosen data model, and sequential Monte Carlo sampling is used to perform online inference. Our research is driven by a biodetection application, where a new class of pathogen may suddenly appear, and the rapid increase in the number of samples originating from this class indicates the onset of an outbreak.

</details>

<details>

<summary>2012-06-18 14:41:11 - Quasi-Newton Methods: A New Direction</summary>

- *Philipp Hennig, Martin Kiefel*

- `1206.4602v1` - [abs](http://arxiv.org/abs/1206.4602v1) - [pdf](http://arxiv.org/pdf/1206.4602v1)

> Four decades after their invention, quasi-Newton methods are still state of the art in unconstrained numerical optimization. Although not usually interpreted thus, these are learning algorithms that fit a local quadratic approximation to the objective function. We show that many, including the most popular, quasi-Newton methods can be interpreted as approximations of Bayesian linear regression under varying prior assumptions. This new notion elucidates some shortcomings of classical algorithms, and lights the way to a novel nonparametric quasi-Newton method, which is able to make more efficient use of available information at computational cost similar to its predecessors.

</details>

<details>

<summary>2012-06-18 14:43:42 - TrueLabel + Confusions: A Spectrum of Probabilistic Models in Analyzing Multiple Ratings</summary>

- *Chao Liu, Yi-Min Wang*

- `1206.4606v1` - [abs](http://arxiv.org/abs/1206.4606v1) - [pdf](http://arxiv.org/pdf/1206.4606v1)

> This paper revisits the problem of analyzing multiple ratings given by different judges. Different from previous work that focuses on distilling the true labels from noisy crowdsourcing ratings, we emphasize gaining diagnostic insights into our in-house well-trained judges. We generalize the well-known DawidSkene model (Dawid & Skene, 1979) to a spectrum of probabilistic models under the same "TrueLabel + Confusion" paradigm, and show that our proposed hierarchical Bayesian model, called HybridConfusion, consistently outperforms DawidSkene on both synthetic and real-world data sets.

</details>

<details>

<summary>2012-06-18 14:45:37 - Manifold Relevance Determination</summary>

- *Andreas Damianou, Carl Ek, Michalis Titsias, Neil Lawrence*

- `1206.4610v1` - [abs](http://arxiv.org/abs/1206.4610v1) - [pdf](http://arxiv.org/pdf/1206.4610v1)

> In this paper we present a fully Bayesian latent variable model which exploits conditional nonlinear(in)-dependence structures to learn an efficient latent representation. The latent space is factorized to represent shared and private information from multiple views of the data. In contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a "softly" shared latent space. Further, Bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces. The model is capable of capturing structure underlying extremely high dimensional spaces. This is illustrated by modelling unprocessed images with tenths of thousands of pixels. This also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces. We also demonstrate the model by prediction of human pose in an ambiguous setting. Our Bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data.

</details>

<details>

<summary>2012-06-18 15:00:40 - Near-Optimal BRL using Optimistic Local Transitions</summary>

- *Mauricio Araya, Olivier Buffet, Vincent Thomas*

- `1206.4613v1` - [abs](http://arxiv.org/abs/1206.4613v1) - [pdf](http://arxiv.org/pdf/1206.4613v1)

> Model-based Bayesian Reinforcement Learning (BRL) allows a found formalization of the problem of acting optimally while facing an unknown environment, i.e., avoiding the exploration-exploitation dilemma. However, algorithms explicitly addressing BRL suffer from such a combinatorial explosion that a large body of work relies on heuristic algorithms. This paper introduces BOLT, a simple and (almost) deterministic heuristic algorithm for BRL which is optimistic about the transition function. We analyze BOLT's sample complexity, and show that under certain parameters, the algorithm is near-optimal in the Bayesian sense with high probability. Then, experimental results highlight the key differences of this method compared to previous work.

</details>

<details>

<summary>2012-06-18 15:27:56 - Max-Margin Nonparametric Latent Feature Models for Link Prediction</summary>

- *Jun Zhu*

- `1206.4659v1` - [abs](http://arxiv.org/abs/1206.4659v1) - [pdf](http://arxiv.org/pdf/1206.4659v1)

> We present a max-margin nonparametric latent feature model, which unites the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction and automatically infer the unknown latent social dimension. By minimizing a hinge-loss using the linear expectation operator, we can perform posterior inference efficiently without dealing with a highly nonlinear link likelihood function; by using a fully-Bayesian formulation, we can avoid tuning regularization constants. Experimental results on real datasets appear to demonstrate the benefits inherited from max-margin learning and fully-Bayesian nonparametric inference.

</details>

<details>

<summary>2012-06-18 15:32:46 - A Bayesian Approach to Approximate Joint Diagonalization of Square Matrices</summary>

- *Mingjun Zhong, Mark Girolami*

- `1206.4666v1` - [abs](http://arxiv.org/abs/1206.4666v1) - [pdf](http://arxiv.org/pdf/1206.4666v1)

> We present a Bayesian scheme for the approximate diagonalisation of several square matrices which are not necessarily symmetric. A Gibbs sampler is derived to simulate samples of the common eigenvectors and the eigenvalues for these matrices. Several synthetic examples are used to illustrate the performance of the proposed Gibbs sampler and we then provide comparisons to several other joint diagonalization algorithms, which shows that the Gibbs sampler achieves the state-of-the-art performance on the examples considered. As a byproduct, the output of the Gibbs sampler could be used to estimate the log marginal likelihood, however we employ the approximation based on the Bayesian information criterion (BIC) which in the synthetic examples considered correctly located the number of common eigenvectors. We then succesfully applied the sampler to the source separation problem as well as the common principal component analysis and the common spatial pattern analysis problems.

</details>

<details>

<summary>2012-06-18 15:37:59 - Factorized Asymptotic Bayesian Hidden Markov Models</summary>

- *Ryohei Fujimaki, Kohei Hayashi*

- `1206.4679v1` - [abs](http://arxiv.org/abs/1206.4679v1) - [pdf](http://arxiv.org/pdf/1206.4679v1)

> This paper addresses the issue of model selection for hidden Markov models (HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which has been recently developed for model selection on independent hidden variables (i.e., mixture models), for time-dependent hidden variables. As with FAB in mixture models, FAB for HMMs is derived as an iterative lower bound maximization algorithm of a factorized information criterion (FIC). It inherits, from FAB for mixture models, several desirable properties for learning HMMs, such as asymptotic consistency of FIC with marginal log-likelihood, a shrinkage effect for hidden state selection, monotonic increase of the lower FIC bound through the iterative optimization. Further, it does not have a tunable hyper-parameter, and thus its model selection process can be fully automated. Experimental results shows that FAB outperforms states-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM in terms of model selection accuracy and computational efficiency.

</details>

<details>

<summary>2012-06-18 17:06:18 - On the relation between frequentist and Bayesian approaches for the case of Poisson statistics</summary>

- *Sergey Bitioukov, Nikolai Krasnikov*

- `1206.3991v1` - [abs](http://arxiv.org/abs/1206.3991v1) - [pdf](http://arxiv.org/pdf/1206.3991v1)

> We propose modified frequentist definition for the determination of confidence intervals for the case of Poisson statistics. Namely, we require that 1-\beta' \geq \sum_{n=o}^{n_{obs}+k} P(n|\lambda) \geq \alpha'. We show that this definition is equivalent to the Bayesian method with prior \pi(\lambda) \sim \lambda^{k}. We also propose modified frequentist definition for the case of nonzero background.

</details>

<details>

<summary>2012-06-18 18:04:07 - Exponentiated Weibull-Geometric Distribution and its Applications</summary>

- *Eisa Mahmoudi, Mitra Shiran*

- `1206.4008v1` - [abs](http://arxiv.org/abs/1206.4008v1) - [pdf](http://arxiv.org/pdf/1206.4008v1)

> In this paper a new lifetime distribution, which is called the exponentiated Weibull-geometric (EWG) distribution, is introduced. This new distribution obtained by compounding the exponentiated Weibull and geometric distributions. The EWG distribution includes as special cases the generalized exponential-geometric (GEG), complementary Weibull-geometric (CWG), complementary exponential-geometric (CEG), exponentiated Rayleigh-geometric (ERG) and Rayleigh-geometric (RG) distributions.   The hazard function of the EWG distribution can be decreasing, increasing, bathtub-shaped and unimodal among others. Several properties of the EWG distribution such as quantiles and moments, maximum likelihood estimation procedure via an EM-algorithm, R\'{e}nyi and Shannon entropies, moments of order statistics, residual life function and probability weighted moments are studied in this paper. In the end, we give two applications with real data sets to show the flexibility of the new distribution.

</details>

<details>

<summary>2012-06-18 19:41:44 - Rank-based model selection for multiple ions quantum tomography</summary>

- *Madalin Guta, Theodore Kypraios, Ian Dryden*

- `1206.4032v1` - [abs](http://arxiv.org/abs/1206.4032v1) - [pdf](http://arxiv.org/pdf/1206.4032v1)

> The statistical analysis of measurement data has become a key component of many quantum engineering experiments. As standard full state tomography becomes unfeasible for large dimensional quantum systems, one needs to exploit prior information and the "sparsity" properties of the experimental state in order to reduce the dimensionality of the estimation problem. In this paper we propose model selection as a general principle for finding the simplest, or most parsimonious explanation of the data, by fitting different models and choosing the estimator with the best trade-off between likelihood fit and model complexity. We apply two well established model selection methods -- the Akaike information criterion (AIC) and the Bayesian information criterion (BIC) -- to models consising of states of fixed rank and datasets such as are currently produced in multiple ions experiments. We test the performance of AIC and BIC on randomly chosen low rank states of 4 ions, and study the dependence of the selected rank with the number of measurement repetitions for one ion states. We then apply the methods to real data from a 4 ions experiment aimed at creating a Smolin state of rank 4. The two methods indicate that the optimal model for describing the data lies between ranks 6 and 9, and the Pearson $\chi^{2}$ test is applied to validate this conclusion. Additionally we find that the mean square error of the maximum likelihood estimator for pure states is close to that of the optimal over all possible measurements.

</details>

<details>

<summary>2012-06-19 13:50:41 - A Fast Non-Gaussian Bayesian Matching Pursuit Method for Sparse Reconstruction</summary>

- *Mudassir Masood, Tareq Al-Naffouri*

- `1206.4208v1` - [abs](http://arxiv.org/abs/1206.4208v1) - [pdf](http://arxiv.org/pdf/1206.4208v1)

> A fast matching pursuit method using a Bayesian approach is introduced for sparse signal recovery. This method, referred to as nGpFBMP, performs Bayesian estimates of sparse signals even when the signal prior is non-Gaussian or unknown. It is agnostic on signal statistics and utilizes a priori statistics of additive noise and the sparsity rate of the signal, which are shown to be easily estimated from data if not available. nGpFBMP utilizes a greedy approach and order-recursive updates of its metrics to find the most dominant sparse supports to determine the approximate minimum mean square error (MMSE) estimate of the sparse signal. Simulation results demonstrate the power and robustness of our proposed estimator.

</details>

<details>

<summary>2012-06-19 17:48:53 - Commentary on Bayesian coincidence assessment (cross-matching)</summary>

- *Thomas J. Loredo*

- `1206.4278v1` - [abs](http://arxiv.org/abs/1206.4278v1) - [pdf](http://arxiv.org/pdf/1206.4278v1)

> This paper is an invited commentary on Tamas Budavari's presentation, "On statistical cross-identification in astronomy," for the Statistical Challenges in Modern Astronomy V conference held at Pennsylvania State University in June 2011. I begin with a brief review of previous work on probabilistic (Bayesian) assessment of directional and spatio-temporal coincidences in astronomy (e.g., cross-matching or cross-identification of objects across multiple catalogs). Then I discuss an open issue in the recent innovative work of Budavari and his colleagues on large-scale probabilistic cross-identification: how to assign prior probabilities that play an important role in the analysis. With a simple toy problem, I show how Bayesian multilevel modeling (hierarchical Bayes) provides a principled framework that justifies and generalizes pragmatic rules of thumb that have been successfully used by Budavari's team to assign priors.

</details>

<details>

<summary>2012-06-20 14:54:06 - A new parameter Learning Method for Bayesian Networks with Qualitative Influences</summary>

- *Ad Feelders*

- `1206.5245v1` - [abs](http://arxiv.org/abs/1206.5245v1) - [pdf](http://arxiv.org/pdf/1206.5245v1)

> We propose a new method for parameter learning in Bayesian networks with qualitative influences. This method extends our previous work from networks of binary variables to networks of discrete variables with ordered values. The specified qualitative influences correspond to certain order restrictions on the parameters in the network. These parameters may therefore be estimated using constrained maximum likelihood estimation. We propose an alternative method, based on the isotonic regression. The constrained maximum likelihood estimates are fairly complicated to compute, whereas computation of the isotonic regression estimates only requires the repeated application of the Pool Adjacent Violators algorithm for linear orders. Therefore, the isotonic regression estimator is to be preferred from the viewpoint of computational complexity. Through experiments on simulated and real data, we show that the new learning method is competitive in performance to the constrained maximum likelihood estimator, and that both estimators improve on the standard estimator.

</details>

<details>

<summary>2012-06-20 14:54:43 - Bayesian structure learning using dynamic programming and MCMC</summary>

- *Daniel Eaton, Kevin Murphy*

- `1206.5247v1` - [abs](http://arxiv.org/abs/1206.5247v1) - [pdf](http://arxiv.org/pdf/1206.5247v1)

> MCMC methods for sampling from the space of DAGs can mix poorly due to the local nature of the proposals that are commonly used. It has been shown that sampling from the space of node orders yields better results [FK03, EW06]. Recently, Koivisto and Sood showed how one can analytically marginalize over orders using dynamic programming (DP) [KS04, Koi06]. Their method computes the exact marginal posterior edge probabilities, thus avoiding the need for MCMC. Unfortunately, there are four drawbacks to the DP technique: it can only use modular priors, it can only compute posteriors over modular features, it is difficult to compute a predictive density, and it takes exponential time and space. We show how to overcome the first three of these problems by using the DP algorithm as a proposal distribution for MCMC in DAG space. We show that this hybrid technique converges to the posterior faster than other methods, resulting in more accurate structure learning and higher predictive likelihoods on test data.

</details>

<details>

<summary>2012-06-20 14:56:02 - Probabilistic Models for Anomaly Detection in Remote Sensor Data Streams</summary>

- *Ethan W. Dereszynski, Thomas G. Dietterich*

- `1206.5250v1` - [abs](http://arxiv.org/abs/1206.5250v1) - [pdf](http://arxiv.org/pdf/1206.5250v1)

> Remote sensors are becoming the standard for observing and recording ecological data in the field. Such sensors can record data at fine temporal resolutions, and they can operate under extreme conditions prohibitive to human access. Unfortunately, sensor data streams exhibit many kinds of errors ranging from corrupt communications to partial or total sensor failures. This means that the raw data stream must be cleaned before it can be used by domain scientists. In our application environment|the H.J. Andrews Experimental Forest|this data cleaning is performed manually. This paper introduces a Dynamic Bayesian Network model for analyzing sensor observations and distinguishing sensor failures from valid data for the case of air temperature measured at 15 minute time resolution. The model combines an accurate distribution of long-term and short-term temperature variations with a single generalized fault model. Experiments with historical data show that the precision and recall of the method is comparable to that of the domain expert. The system is currently being deployed to perform real-time automated data cleaning.

</details>

<details>

<summary>2012-06-20 14:58:18 - Discovering Patterns in Biological Sequences by Optimal Segmentation</summary>

- *Joseph Bockhorst, Nebojsa Jojic*

- `1206.5256v1` - [abs](http://arxiv.org/abs/1206.5256v1) - [pdf](http://arxiv.org/pdf/1206.5256v1)

> Computational methods for discovering patterns of local correlations in sequences are important in computational biology. Here we show how to determine the optimal partitioning of aligned sequences into non-overlapping segments such that positions in the same segment are strongly correlated while positions in different segments are not. Our approach involves discovering the hidden variables of a Bayesian network that interact with observed sequences so as to form a set of independent mixture models. We introduce a dynamic program to efficiently discover the optimal segmentation, or equivalently the optimal set of hidden variables. We evaluate our approach on two computational biology tasks. One task is related to the design of vaccines against polymorphic pathogens and the other task involves analysis of single nucleotide polymorphisms (SNPs) in human DNA. We show how common tasks in these problems naturally correspond to inference procedures in the learned models. Error rates of our learned models for the prediction of missing SNPs are up to 1/3 less than the error rates of a state-of-the-art SNP prediction method. Source code is available at www.uwm.edu/~joebock/segmentation.

</details>

<details>

<summary>2012-06-20 15:01:43 - Reading Dependencies from Polytree-Like Bayesian Networks</summary>

- *Jose M. Pena*

- `1206.5263v1` - [abs](http://arxiv.org/abs/1206.5263v1) - [pdf](http://arxiv.org/pdf/1206.5263v1)

> We present a graphical criterion for reading dependencies from the minimal directed independence map G of a graphoid p when G is a polytree and p satisfies composition and weak transitivity. We prove that the criterion is sound and complete. We argue that assuming composition and weak transitivity is not too restrictive.

</details>

<details>

<summary>2012-06-20 15:04:30 - Determining the Number of Non-Spurious Arcs in a Learned DAG Model: Investigation of a Bayesian and a Frequentist Approach</summary>

- *Jennifer Listgarden, David Heckerman*

- `1206.5269v1` - [abs](http://arxiv.org/abs/1206.5269v1) - [pdf](http://arxiv.org/pdf/1206.5269v1)

> In many application domains, such as computational biology, the goal of graphical model structure learning is to uncover discrete relationships between entities. For example, in our problem of interest concerning HIV vaccine design, we want to infer which HIV peptides interact with which immune system molecules (HLA molecules). For problems of this nature, we are interested in determining the number of nonspurious arcs in a learned graphical model. We describe both a Bayesian and frequentist approach to this problem. In the Bayesian approach, we use the posterior distribution over model structures to compute the expected number of true arcs in a learned model. In the frequentist approach, we develop a method based on the concept of the False Discovery Rate. On synthetic data sets generated from models similar to the ones learned, we find that both the Bayesian and frequentist approaches yield accurate estimates of the number of non-spurious arcs. In addition, we speculate that the frequentist approach, which is non-parametric, may outperform the parametric Bayesian approach in situations where the models learned are less representative of the data. Finally, we apply the frequentist approach to our problem of HIV vaccine design.

</details>

<details>

<summary>2012-06-20 15:04:47 - Nonparametric Bayes Pachinko Allocation</summary>

- *Wei Li, David Blei, Andrew McCallum*

- `1206.5270v1` - [abs](http://arxiv.org/abs/1206.5270v1) - [pdf](http://arxiv.org/pdf/1206.5270v1)

> Recent advances in topic models have explored complicated structured distributions to represent topic correlation. For example, the pachinko allocation model (PAM) captures arbitrary, nested, and possibly sparse correlations between topics using a directed acyclic graph (DAG). While PAM provides more flexibility and greater expressive power than previous models like latent Dirichlet allocation (LDA), it is also more difficult to determine the appropriate topic structure for a specific dataset. In this paper, we propose a nonparametric Bayesian prior for PAM based on a variant of the hierarchical Dirichlet process (HDP). Although the HDP can capture topic correlations defined by nested data structure, it does not automatically discover such correlations from unstructured data. By assuming an HDP-based prior for PAM, we are able to learn both the number of topics and how the topics are correlated. We evaluate our model on synthetic and real-world text datasets, and show that nonparametric PAM achieves performance matching the best of PAM without manually tuning the number of topics.

</details>

<details>

<summary>2012-06-20 15:06:26 - Polynomial Constraints in Causal Bayesian Networks</summary>

- *Changsung Kang, Jin Tian*

- `1206.5275v1` - [abs](http://arxiv.org/abs/1206.5275v1) - [pdf](http://arxiv.org/pdf/1206.5275v1)

> We use the implicitization procedure to generate polynomial equality constraints on the set of distributions induced by local interventions on variables governed by a causal Bayesian network with hidden variables. We show how we may reduce the complexity of the implicitization problem and make the problem tractable in certain causal Bayesian networks. We also show some preliminary results on the algebraic structure of polynomial constraints. The results have applications in distinguishing between causal models and in testing causal models with combined observational and experimental data.

</details>

<details>

<summary>2012-06-20 15:12:35 - Learning Selectively Conditioned Forest Structures with Applications to DBNs and Classification</summary>

- *Brian D. Ziebart, Anind K. Dey, J Andrew Bagnell*

- `1206.5281v1` - [abs](http://arxiv.org/abs/1206.5281v1) - [pdf](http://arxiv.org/pdf/1206.5281v1)

> Dealing with uncertainty in Bayesian Network structures using maximum a posteriori (MAP) estimation or Bayesian Model Averaging (BMA) is often intractable due to the superexponential number of possible directed, acyclic graphs. When the prior is decomposable, two classes of graphs where efficient learning can take place are tree structures, and fixed-orderings with limited in-degree. We show how MAP estimates and BMA for selectively conditioned forests (SCF), a combination of these two classes, can be computed efficiently for ordered sets of variables. We apply SCFs to temporal data to learn Dynamic Bayesian Networks having an intra-timestep forest and inter-timestep limited in-degree structure, improving model accuracy over DBNs without the combination of structures. We also apply SCFs to Bayes Net classification to learn selective forest augmented Naive Bayes classifiers. We argue that the built-in feature selection of selective augmented Bayes classifiers makes them preferable to similar non-selective classifiers based on empirical evidence.

</details>

<details>

<summary>2012-06-20 15:14:55 - Bayesian Active Distance Metric Learning</summary>

- *Liu Yang, Rong Jin, Rahul Sukthankar*

- `1206.5283v1` - [abs](http://arxiv.org/abs/1206.5283v1) - [pdf](http://arxiv.org/pdf/1206.5283v1)

> Distance metric learning is an important component for many tasks, such as statistical classification and content-based image retrieval. Existing approaches for learning distance metrics from pairwise constraints typically suffer from two major problems. First, most algorithms only offer point estimation of the distance metric and can therefore be unreliable when the number of training examples is small. Second, since these algorithms generally select their training examples at random, they can be inefficient if labeling effort is limited. This paper presents a Bayesian framework for distance metric learning that estimates a posterior distribution for the distance metric from labeled pairwise constraints. We describe an efficient algorithm based on the variational method for the proposed Bayesian approach. Furthermore, we apply the proposed Bayesian framework to active distance metric learning by selecting those unlabeled example pairs with the greatest uncertainty in relative distance. Experiments in classification demonstrate that the proposed framework achieves higher classification accuracy and identifies more informative training examples than the non-Bayesian approach and state-of-the-art distance metric learning algorithms.

</details>

<details>

<summary>2012-06-20 15:15:41 - Importance Sampling via Variational Optimization</summary>

- *Ydo Wexler, Dan Geiger*

- `1206.5285v1` - [abs](http://arxiv.org/abs/1206.5285v1) - [pdf](http://arxiv.org/pdf/1206.5285v1)

> Computing the exact likelihood of data in large Bayesian networks consisting of thousands of vertices is often a difficult task. When these models contain many deterministic conditional probability tables and when the observed values are extremely unlikely even alternative algorithms such as variational methods and stochastic sampling often perform poorly. We present a new importance sampling algorithm for Bayesian networks which is based on variational techniques. We use the updates of the importance function to predict whether the stochastic sampling converged above or below the true likelihood, and change the proposal distribution accordingly. The validity of the method and its contribution to convergence is demonstrated on hard networks of large genetic linkage analysis tasks.

</details>

<details>

<summary>2012-06-20 15:18:02 - Imitation Learning with a Value-Based Prior</summary>

- *Umar Syed, Robert E. Schapire*

- `1206.5290v1` - [abs](http://arxiv.org/abs/1206.5290v1) - [pdf](http://arxiv.org/pdf/1206.5290v1)

> The goal of imitation learning is for an apprentice to learn how to behave in a stochastic environment by observing a mentor demonstrating the correct behavior. Accurate prior knowledge about the correct behavior can reduce the need for demonstrations from the mentor. We present a novel approach to encoding prior knowledge about the correct behavior, where we assume that this prior knowledge takes the form of a Markov Decision Process (MDP) that is used by the apprentice as a rough and imperfect model of the mentor's behavior. Specifically, taking a Bayesian approach, we treat the value of a policy in this modeling MDP as the log prior probability of the policy. In other words, we assume a priori that the mentor's behavior is likely to be a high value policy in the modeling MDP, though quite possibly different from the optimal policy. We describe an efficient algorithm that, given a modeling MDP and a set of demonstrations by a mentor, provably converges to a stationary point of the log posterior of the mentor's policy, where the posterior is computed with respect to the "value based" prior. We also present empirical evidence that this prior does in fact speed learning of the mentor's policy, and is an improvement in our experiments over similar previous methods.

</details>

<details>

<summary>2012-06-20 15:19:06 - On Sensitivity of the MAP Bayesian Network Structure to the Equivalent Sample Size Parameter</summary>

- *Tomi Silander, Petri Kontkanen, Petri Myllymaki*

- `1206.5293v1` - [abs](http://arxiv.org/abs/1206.5293v1) - [pdf](http://arxiv.org/pdf/1206.5293v1)

> BDeu marginal likelihood score is a popular model selection criterion for selecting a Bayesian network structure based on sample data. This non-informative scoring criterion assigns same score for network structures that encode same independence statements. However, before applying the BDeu score, one must determine a single parameter, the equivalent sample size alpha. Unfortunately no generally accepted rule for determining the alpha parameter has been suggested. This is disturbing, since in this paper we show through a series of concrete experiments that the solution of the network structure optimization problem is highly sensitive to the chosen alpha parameter value. Based on these results, we are able to give explanations for how and why this phenomenon happens, and discuss ideas for solving this problem.

</details>

<details>

<summary>2012-06-21 20:27:19 - On Bayesian Modelling of the Uncertainties in Palaeoclimate Reconstruction</summary>

- *Andrew C. Parnell, James Sweeney, Thinh K. Doan, Michael Salter-Townshend, Judy R. M. Allen, Brian Huntley, John Haslett*

- `1206.5009v1` - [abs](http://arxiv.org/abs/1206.5009v1) - [pdf](http://arxiv.org/pdf/1206.5009v1)

> We outline a model and algorithm to perform inference on the palaeoclimate and palaeoclimate volatility from pollen proxy data. We use a novel multivariate non-linear non-Gaussian state space model consisting of an observation equation linking climate to proxy data and an evolution equation driving climate change over time. The link from climate to proxy data is defined by a pre-calibrated forward model, as developed in Salter-Townshend and Haslett (2012) and Sweeney (2012). Climatic change is represented by a temporally-uncertain Normal-Inverse Gaussian Levy process, being able to capture large jumps in multivariate climate whilst remaining temporally consistent. The pre-calibrated nature of the forward model allows us to cut feedback between the observation and evolution equations and thus integrate out the state variable entirely whilst making minimal simplifying assumptions. A key part of this approach is the creation of mixtures of marginal data posteriors representing the information obtained about climate from each individual time point. Our approach allows for an extremely efficient MCMC algorithm, which we demonstrate with a pollen core from Sluggan Bog, County Antrim, Northern Ireland.

</details>

<details>

<summary>2012-06-21 20:41:48 - Quantitative model validation techniques: new insights</summary>

- *You Ling, Sankaran Mahadevan*

- `1206.5014v1` - [abs](http://arxiv.org/abs/1206.5014v1) - [pdf](http://arxiv.org/pdf/1206.5014v1)

> This paper develops new insights into quantitative methods for the validation of computational model prediction. Four types of methods are investigated, namely classical and Bayesian hypothesis testing, a reliability-based method, and an area metric-based method. Traditional Bayesian hypothesis testing is extended based on interval hypotheses on distribution parameters and equality hypotheses on probability distributions, in order to validate models with deterministic/stochastic output for given inputs. Two types of validation experiments are considered - fully characterized (all the model/experimental inputs are measured and reported as point values) and partially characterized (some of the model/experimental inputs are not measured or are reported as intervals). Bayesian hypothesis testing can minimize the risk in model selection by properly choosing the model acceptance threshold, and its results can be used in model averaging to avoid Type I/II errors. It is shown that Bayesian interval hypothesis testing, the reliability-based method, and the area metric-based method can account for the existence of directional bias, where the mean predictions of a numerical model may be consistently below or above the corresponding experimental observations. It is also found that under some specific conditions, the Bayes factor metric in Bayesian equality hypothesis testing and the reliability-based metric can both be mathematically related to the p-value metric in classical hypothesis testing. Numerical studies are conducted to apply the above validation methods to gas damping prediction for radio frequency (RF) microelectromechanical system (MEMS) switches. The model of interest is a general polynomial chaos (gPC) surrogate model constructed based on expensive runs of a physics-based simulation model, and validation data are collected from fully characterized experiments.

</details>

<details>

<summary>2012-06-21 20:43:48 - Calibration of multi-physics computational models using Bayesian networks</summary>

- *You Ling, Joshua Mullins, Sankaran Mahadevan*

- `1206.5015v1` - [abs](http://arxiv.org/abs/1206.5015v1) - [pdf](http://arxiv.org/pdf/1206.5015v1)

> This paper develops a Bayesian network-based method for the calibration of multi-physics models, integrating various sources of uncertainty with information from computational models and experimental data. We adopt the Kennedy and O'Hagan (KOH) framework for model calibration under uncertainty, and develop extensions to multi-physics models and various scenarios of available data. Both aleatoric uncertainty (due to natural variability) and epistemic uncertainty (due to lack of information, including data uncertainty and model uncertainty) are accounted for in the calibration process. Challenging aspects of Bayesian calibration for multi-physics models are investigated, including: (1) calibration with different forms of experimental data (e.g., interval data and time series data), (2) determination of the identifiability of model parameters when the analytical expression of model is known or unknown, (3) calibration of multiple physics models sharing common parameters, which enables efficient use of data especially when the experimental resources are limited. A first-order Taylor series expansion-based method is proposed to determine which model parameters are identifiable. Following the KOH framework, a probabilistic discrepancy function is estimated and added to the prediction of the calibrated model, attempting to account for model uncertainty. This discrepancy function is modeled as a Gaussian process when sufficient data are available for multiple model input combinations, and is modeled as a random variable when the available data are limited. The overall approach is illustrated using two application examples related to microelectromechanical system (MEMS) devices: (1) calibration of a dielectric charging model with time-series data, and (2) calibration of two physics models (pull-in voltage and creep) using measurements of different physical quantities in different devices.

</details>

<details>

<summary>2012-06-22 17:01:56 - Approximate Bayesian Computation for Smoothing</summary>

- *James S. Martin, Ajay Jasra, Sumeetpal S. Singh, Nick Whiteley, Emma McCoy*

- `1206.5208v1` - [abs](http://arxiv.org/abs/1206.5208v1) - [pdf](http://arxiv.org/pdf/1206.5208v1)

> We consider a method for approximate inference in hidden Markov models (HMMs). The method circumvents the need to evaluate conditional densities of observations given the hidden states. It may be considered an instance of Approximate Bayesian Computation (ABC) and it involves the introduction of auxiliary variables valued in the same space as the observations. The quality of the approximation may be controlled to arbitrary precision through a parameter \epsilon>0 . We provide theoretical results which quantify, in terms of \epsilon, the ABC error in approximation of expectations of additive functionals with respect to the smoothing distributions. Under regularity assumptions, this error is O(n\epsilon), where n is the number of time steps over which smoothing is performed. For numerical implementation we adopt the forward-only sequential Monte Carlo (SMC) scheme of [16] and quantify the combined error from the ABC and SMC approximations. This forms some of the first quantitative results for ABC methods which jointly treat the ABC and simulation errors, with a finite number of data and simulated samples. When the HMM has unknown static parameters, we consider particle Markov chain Monte Carlo [2] (PMCMC) methods for batch statistical inference.

</details>

<details>

<summary>2012-06-23 01:15:47 - Bayesian Structure Learning for Markov Random Fields with a Spike and Slab Prior</summary>

- *Yutian Chen, Max Welling*

- `1206.1088v2` - [abs](http://arxiv.org/abs/1206.1088v2) - [pdf](http://arxiv.org/pdf/1206.1088v2)

> In recent years a number of methods have been developed for automatically learning the (sparse) connectivity structure of Markov Random Fields. These methods are mostly based on L1-regularized optimization which has a number of disadvantages such as the inability to assess model uncertainty and expensive cross-validation to find the optimal regularization parameter. Moreover, the model's predictive performance may degrade dramatically with a suboptimal value of the regularization parameter (which is sometimes desirable to induce sparseness). We propose a fully Bayesian approach based on a "spike and slab" prior (similar to L0 regularization) that does not suffer from these shortcomings. We develop an approximate MCMC method combining Langevin dynamics and reversible jump MCMC to conduct inference in this model. Experiments show that the proposed model learns a good combination of the structure and parameter values without the need for separate hyper-parameter tuning. Moreover, the model's predictive performance is much more robust than L1-based methods with hyper-parameter settings that induce highly sparse model structures.

</details>

<details>

<summary>2012-06-24 18:00:21 - A Bayesian approach to comparing theoretic models to observational data: A case study from solar flare physics</summary>

- *S. Adamakis, C. L. Raftery, R. W. Walsh, P. T. Gallagher*

- `1102.0242v3` - [abs](http://arxiv.org/abs/1102.0242v3) - [pdf](http://arxiv.org/pdf/1102.0242v3)

> Solar flares are large-scale releases of energy in the solar atmosphere, which are characterised by rapid changes in the hydrodynamic properties of plasma from the photosphere to the corona. Solar physicists have typically attempted to understand these complex events using a combination of theoretical models and observational data. From a statistical perspective, there are many challenges associated with making accurate and statistically significant comparisons between theory and observations, due primarily to the large number of free parameters associated with physical models. This class of ill-posed statistical problem is ideally suited to Bayesian methods. In this paper, the solar flare studied by Raftery et al. (2009) is reanalysed using a Bayesian framework. This enables us to study the evolution of the flare's temperature, emission measure and energy loss in a statistically self-consistent manner. The Bayesian-based model selection techniques imply that no decision can be made regarding which of the conductive or non-thermal beam heating play the most important role in heating the flare plasma during the impulsive phase of this event.

</details>

<details>

<summary>2012-06-25 14:09:54 - Probabilities of exoplanet signals from posterior samplings</summary>

- *Mikko Tuomi, Hugh R. A. Jones*

- `1112.5969v4` - [abs](http://arxiv.org/abs/1112.5969v4) - [pdf](http://arxiv.org/pdf/1112.5969v4)

> Estimating the marginal likelihoods is an essential feature of model selection in the Bayesian context. It is especially crucial to have good estimates when assessing the number of planets orbiting stars when the models explain the noisy data with different numbers of Keplerian signals. We introduce a simple method for approximating the marginal likelihoods in practice when a statistically representative sample from the parameter posterior density is available.   We use our truncated posterior mixture estimate to receive accurate model probabilities for models with differing number of Keplerian signals in radial velocity data. We test this estimate in simple scenarios to assess its accuracy and rate of convergence in practice when the corresponding estimates calculated using deviance information criterion can be applied to receive trustworthy results for reliable comparison. As a test case, we determine the posterior probability of a planet orbiting HD 3651 given Lick and Keck radial velocity data.   The posterior mixture estimate appears to be a simple and an accurate way of calculating marginal integrals from posterior samples. We show, that it can be used to estimate the marginal integrals reliably in practice, given a suitable selection of parameter \lambda, that controls its accuracy and convergence rate. It is also more accurate than the one block Metropolis-Hastings estimate and can be used in any application because it is not based on assumptions on the nature of the posterior density nor the amount of data or parameters in the statistical model.

</details>

<details>

<summary>2012-06-26 10:55:25 - Zero Variance Markov Chain Monte Carlo for Bayesian Estimators</summary>

- *Antonietta Mira, Reza Solgi, Daniele Imparato*

- `1012.2983v2` - [abs](http://arxiv.org/abs/1012.2983v2) - [pdf](http://arxiv.org/pdf/1012.2983v2)

> Interest is in evaluating, by Markov chain Monte Carlo (MCMC) simulation, the expected value of a function with respect to a, possibly unnormalized, probability distribution. A general purpose variance reduction technique for the MCMC estimator, based on the zero-variance principle introduced in the physics literature, is proposed. Conditions for asymptotic unbiasedness of the zero-variance estimator are derived. A central limit theorem is also proved under regularity conditions. The potential of the idea is illustrated with real applications to probit, logit and GARCH Bayesian models. For all these models, a central limit theorem and unbiasedness for the zero-variance estimator are proved (see the supplementary material available on-line).

</details>

<details>

<summary>2012-06-27 12:11:03 - Multiscale Inference of Matter Fields and Baryon Acoustic Oscillations from the Ly-alpha Forest</summary>

- *Francisco-Shu Kitaura, Simona Gallerani, Andrea Ferrara*

- `1011.6233v3` - [abs](http://arxiv.org/abs/1011.6233v3) - [pdf](http://arxiv.org/pdf/1011.6233v3)

> We present a novel Bayesian method for the joint reconstruction of cosmological matter density fields, peculiar velocities and power-spectra in the quasi-nonlinear regime. We study its applicability to the Ly-alpha forest based on multiple quasar absorption spectra. Our approach to this problem includes a multiscale, nonlinear, two-step scheme since the statistics describing the matter distribution depends on scale, being strongly non-Gaussian on small scales (< 0.1 h^{-1} Mpc) and closely lognormal on scales >~10 h^{-1} Mpc. The first step consists on performing 1D highly resolved matter density reconstructions along the line-of-sight towards z~2-3 quasars based on an arbitrary non-Gaussian univariate model for matter statistics. The second step consists on Gibbs-sampling based on conditional PDFs. The matter density field is sampled in real space with Hamiltonian-sampling using the Poisson/Gamma-lognormal model, while redshift distortions are corrected with linear Lagrangian perturbation theory. The power-spectrum of the lognormal transformed variable which is Gaussian distributed (and thus close to the linear regime) can consistently be sampled with the inverse Gamma distribution function. We test our method through numerical N-body simulations with a computational volume large enough (> 1 h^{-3} Gpc^3) to show that the linear power-spectra are nicely recovered over scales larger than >~20 h^{-1} Mpc, i.e. the relevant range where features imprinted by the baryon-acoustics oscillations (BAOs) appear.

</details>

<details>

<summary>2012-06-27 16:15:14 - Advances in exact Bayesian structure discovery in Bayesian networks</summary>

- *Mikko Koivisto*

- `1206.6828v1` - [abs](http://arxiv.org/abs/1206.6828v1) - [pdf](http://arxiv.org/pdf/1206.6828v1)

> We consider a Bayesian method for learning the Bayesian network structure from complete data. Recently, Koivisto and Sood (2004) presented an algorithm that for any single edge computes its marginal posterior probability in O(n 2^n) time, where n is the number of attributes; the number of parents per attribute is bounded by a constant. In this paper we show that the posterior probabilities for all the n (n - 1) potential edges can be computed in O(n 2^n) total time. This result is achieved by a forward-backward technique and fast Moebius transform algorithms, which are of independent interest. The resulting speedup by a factor of about n^2 allows us to experimentally study the statistical power of learning moderate-size networks. We report results from a simulation study that covers data sets with 20 to 10,000 records over 5 to 25 discrete attributes

</details>

<details>

<summary>2012-06-27 16:15:29 - Inequality Constraints in Causal Models with Hidden Variables</summary>

- *Changsung Kang, Jin Tian*

- `1206.6829v1` - [abs](http://arxiv.org/abs/1206.6829v1) - [pdf](http://arxiv.org/pdf/1206.6829v1)

> We present a class of inequality constraints on the set of distributions induced by local interventions on variables governed by a causal Bayesian network, in which some of the variables remain unmeasured. We derive bounds on causal effects that are not directly measured in randomized experiments. We derive instrumental inequality type of constraints on nonexperimental distributions. The results have applications in testing causal models with observational or experimental data.

</details>

<details>

<summary>2012-06-27 16:15:42 - The AI&M Procedure for Learning from Incomplete Data</summary>

- *Manfred Jaeger*

- `1206.6830v1` - [abs](http://arxiv.org/abs/1206.6830v1) - [pdf](http://arxiv.org/pdf/1206.6830v1)

> We investigate methods for parameter learning from incomplete data that is not missing at random. Likelihood-based methods then require the optimization of a profile likelihood that takes all possible missingness mechanisms into account. Optimzing this profile likelihood poses two main difficulties: multiple (local) maxima, and its very high-dimensional parameter space. In this paper a new method is presented for optimizing the profile likelihood that addresses the second difficulty: in the proposed AI&M (adjusting imputation and mazimization) procedure the optimization is performed by operations in the space of data completions, rather than directly in the parameter space of the profile likelihood. We apply the AI&M method to learning parameters for Bayesian networks. The method is compared against conservative inference, which takes into account each possible data completion, and against EM. The results indicate that likelihood-based inference is still feasible in the case of unknown missingness mechanisms, and that conservative inference is unnecessarily weak. On the other hand, our results also provide evidence that the EM algorithm is still quite effective when the data is not missing at random.

</details>

<details>

<summary>2012-06-27 16:17:52 - Convex Structure Learning for Bayesian Networks: Polynomial Feature Selection and Approximate Ordering</summary>

- *Yuhong Guo, Dale Schuurmans*

- `1206.6832v1` - [abs](http://arxiv.org/abs/1206.6832v1) - [pdf](http://arxiv.org/pdf/1206.6832v1)

> We present a new approach to learning the structure and parameters of a Bayesian network based on regularized estimation in an exponential family representation. Here we show that, given a fixed variable order, the optimal structure and parameters can be learned efficiently, even without restricting the size of the parent sets. We then consider the problem of optimizing the variable order for a given set of features. This is still a computationally hard problem, but we present a convex relaxation that yields an optimal 'soft' ordering in polynomial time. One novel aspect of the approach is that we do not perform a discrete search over DAG structures, nor over variable orders, but instead solve a continuous relaxation that can then be rounded to obtain a valid network structure. We conduct an experimental comparison against standard structure search procedures over standard objectives, which cope with local minima, and evaluate the advantages of using convex relaxations that reduce the effects of local minima.

</details>

<details>

<summary>2012-06-27 16:20:30 - Chi-square Tests Driven Method for Learning the Structure of Factored MDPs</summary>

- *Thomas Degris, Olivier Sigaud, Pierre-Henri Wuillemin*

- `1206.6842v1` - [abs](http://arxiv.org/abs/1206.6842v1) - [pdf](http://arxiv.org/pdf/1206.6842v1)

> SDYNA is a general framework designed to address large stochastic reinforcement learning problems. Unlike previous model based methods in FMDPs, it incrementally learns the structure and the parameters of a RL problem using supervised learning techniques. Then, it integrates decision-theoric planning algorithms based on FMDPs to compute its policy. SPITI is an instanciation of SDYNA that exploits ITI, an incremental decision tree algorithm, to learn the reward function and the Dynamic Bayesian Networks with local structures representing the transition function of the problem. These representations are used by an incremental version of the Structured Value Iteration algorithm. In order to learn the structure, SPITI uses Chi-Square tests to detect the independence between two probability distributions. Thus, we study the relation between the threshold used in the Chi-Square test, the size of the model built and the relative error of the value function of the induced policy with respect to the optimal value. We show that, on stochastic problems, one can tune the threshold so as to generate both a compact model and an efficient policy. Then, we show that SPITI, while keeping its model compact, uses the generalization property of its learning method to perform better than a stochastic classical tabular algorithm in large RL problem with an unknown structure. We also introduce a new measure based on Chi-Square to qualify the accuracy of the model learned by SPITI. We qualitatively show that the generalization property in SPITI within the FMDP framework may prevent an exponential growth of the time required to learn the structure of large stochastic RL problems.

</details>

<details>

<summary>2012-06-27 16:21:35 - Gibbs Sampling for (Coupled) Infinite Mixture Models in the Stick Breaking Representation</summary>

- *Ian Porteous, Alexander T. Ihler, Padhraic Smyth, Max Welling*

- `1206.6845v1` - [abs](http://arxiv.org/abs/1206.6845v1) - [pdf](http://arxiv.org/pdf/1206.6845v1)

> Nonparametric Bayesian approaches to clustering, information retrieval, language modeling and object recognition have recently shown great promise as a new paradigm for unsupervised data analysis. Most contributions have focused on the Dirichlet process mixture models or extensions thereof for which efficient Gibbs samplers exist. In this paper we explore Gibbs samplers for infinite complexity mixture models in the stick breaking representation. The advantage of this representation is improved modeling flexibility. For instance, one can design the prior distribution over cluster sizes or couple multiple infinite mixture models (e.g. over time) at the level of their parameters (i.e. the dependent Dirichlet process model). However, Gibbs samplers for infinite mixture models (as recently introduced in the statistics literature) seem to mix poorly over cluster labels. Among others issues, this can have the adverse effect that labels for the same cluster in coupled mixture models are mixed up. We introduce additional moves in these samplers to improve mixing over cluster labels and to bring clusters into correspondence. An application to modeling of storm trajectories is used to illustrate these ideas.

</details>

<details>

<summary>2012-06-27 16:23:41 - Identifying the Relevant Nodes Without Learning the Model</summary>

- *Jose M. Pena, Roland Nilsson, Johan Björkegren, Jesper Tegnér*

- `1206.6847v1` - [abs](http://arxiv.org/abs/1206.6847v1) - [pdf](http://arxiv.org/pdf/1206.6847v1)

> We propose a method to identify all the nodes that are relevant to compute all the conditional probability distributions for a given set of nodes. Our method is simple, effcient, consistent, and does not require learning a Bayesian network first. Therefore, our method can be applied to high-dimensional databases, e.g. gene expression databases.

</details>

<details>

<summary>2012-06-27 16:24:57 - Structured Priors for Structure Learning</summary>

- *Vikash Mansinghka, Charles Kemp, Thomas Griffiths, Joshua Tenenbaum*

- `1206.6852v1` - [abs](http://arxiv.org/abs/1206.6852v1) - [pdf](http://arxiv.org/pdf/1206.6852v1)

> Traditional approaches to Bayes net structure learning typically assume little regularity in graph structure other than sparseness. However, in many cases, we expect more systematicity: variables in real-world systems often group into classes that predict the kinds of probabilistic dependencies they participate in. Here we capture this form of prior knowledge in a hierarchical Bayesian framework, and exploit it to enable structure learning and type discovery from small datasets. Specifically, we present a nonparametric generative model for directed acyclic graphs as a prior for Bayes net structure learning. Our model assumes that variables come in one or more classes and that the prior probability of an edge existing between two variables is a function only of their classes. We derive an MCMC algorithm for simultaneous inference of the number of classes, the class assignments of variables, and the Bayes net structure over variables. For several realistic, sparse datasets, we show that the bias towards systematicity of connections provided by our model yields more accurate learned networks than a traditional, uniform prior approach, and that the classes found by our model are appropriate.

</details>

<details>

<summary>2012-06-27 16:25:15 - A theoretical study of Y structures for causal discovery</summary>

- *Subramani Mani, Peter L. Spirtes, Gregory F. Cooper*

- `1206.6853v1` - [abs](http://arxiv.org/abs/1206.6853v1) - [pdf](http://arxiv.org/pdf/1206.6853v1)

> There are several existing algorithms that under appropriate assumptions can reliably identify a subset of the underlying causal relationships from observational data. This paper introduces the first computationally feasible score-based algorithm that can reliably identify causal relationships in the large sample limit for discrete models, while allowing for the possibility that there are unobserved common causes. In doing so, the algorithm does not ever need to assign scores to causal structures with unobserved common causes. The algorithm is based on the identification of so called Y substructures within Bayesian network structures that can be learned from observational data. An example of a Y substructure is A -> C, B -> C, C -> D. After providing background on causal discovery, the paper proves the conditions under which the algorithm is reliable in the large sample limit.

</details>

<details>

<summary>2012-06-27 16:27:25 - Predicting Conditional Quantiles via Reduction to Classification</summary>

- *John Langford, Roberto Oliveira, Bianca Zadrozny*

- `1206.6860v1` - [abs](http://arxiv.org/abs/1206.6860v1) - [pdf](http://arxiv.org/pdf/1206.6860v1)

> We show how to reduce the process of predicting general order statistics (and the median in particular) to solving classification. The accompanying theoretical statement shows that the regret of the classifier bounds the regret of the quantile regression under a quantile loss. We also test this reduction empirically against existing quantile regression methods on large real-world datasets and discover that it provides state-of-the-art performance.

</details>

<details>

<summary>2012-06-27 16:28:06 - On the Number of Samples Needed to Learn the Correct Structure of a Bayesian Network</summary>

- *Or Zuk, Shiri Margel, Eytan Domany*

- `1206.6862v1` - [abs](http://arxiv.org/abs/1206.6862v1) - [pdf](http://arxiv.org/pdf/1206.6862v1)

> Bayesian Networks (BNs) are useful tools giving a natural and compact representation of joint probability distributions. In many applications one needs to learn a Bayesian Network (BN) from data. In this context, it is important to understand the number of samples needed in order to guarantee a successful learning. Previous work have studied BNs sample complexity, yet it mainly focused on the requirement that the learned distribution will be close to the original distribution which generated the data. In this work, we study a different aspect of the learning, namely the number of samples needed in order to learn the correct structure of the network. We give both asymptotic results, valid in the large sample limit, and experimental results, demonstrating the learning behavior for feasible sample sizes. We show that structure learning is a more difficult task, compared to approximating the correct distribution, in the sense that it requires a much larger number of samples, regardless of the computational power available for the learner.

</details>

<details>

<summary>2012-06-27 16:28:18 - Bayesian Multicategory Support Vector Machines</summary>

- *Zhihua Zhang, Michael I. Jordan*

- `1206.6863v1` - [abs](http://arxiv.org/abs/1206.6863v1) - [pdf](http://arxiv.org/pdf/1206.6863v1)

> We show that the multi-class support vector machine (MSVM) proposed by Lee et. al. (2004), can be viewed as a MAP estimation procedure under an appropriate probabilistic interpretation of the classifier. We also show that this interpretation can be extended to a hierarchical Bayesian architecture and to a fully-Bayesian inference procedure for multi-class classification based on data augmentation. We present empirical results that show that the advantages of the Bayesian formalism are obtained without a loss in classification accuracy.

</details>

<details>

<summary>2012-06-27 16:28:41 - A Non-Parametric Bayesian Method for Inferring Hidden Causes</summary>

- *Frank Wood, Thomas Griffiths, Zoubin Ghahramani*

- `1206.6865v1` - [abs](http://arxiv.org/abs/1206.6865v1) - [pdf](http://arxiv.org/pdf/1206.6865v1)

> We present a non-parametric Bayesian approach to structure learning with hidden causes. Previous Bayesian treatments of this problem define a prior over the number of hidden causes and use algorithms such as reversible jump Markov chain Monte Carlo to move between solutions. In contrast, we assume that the number of hidden causes is unbounded, but only a finite number influence observable variables. This makes it possible to use a Gibbs sampler to approximate the distribution over causal structures. We evaluate the performance of both approaches in discovering hidden causes in simulated data, and use our non-parametric approach to discover hidden causes in a real medical dataset.

</details>

<details>

<summary>2012-06-27 16:29:18 - Bayesian Random Fields: The Bethe-Laplace Approximation</summary>

- *Max Welling, Sridevi Parise*

- `1206.6868v1` - [abs](http://arxiv.org/abs/1206.6868v1) - [pdf](http://arxiv.org/pdf/1206.6868v1)

> While learning the maximum likelihood value of parameters of an undirected graphical model is hard, modelling the posterior distribution over parameters given data is harder. Yet, undirected models are ubiquitous in computer vision and text modelling (e.g. conditional random fields). But where Bayesian approaches for directed models have been very successful, a proper Bayesian treatment of undirected models in still in its infant stages. We propose a new method for approximating the posterior of the parameters given data based on the Laplace approximation. This approximation requires the computation of the covariance matrix over features which we compute using the linear response approximation based in turn on loopy belief propagation. We develop the theory for conditional and 'unconditional' random fields with or without hidden variables. In the conditional setting we introduce a new variant of bagging suitable for structured domains. Here we run the loopy max-product algorithm on a 'super-graph' composed of graphs for individual models sampled from the posterior and connected by constraints. Experiments on real world data validate the proposed methods.

</details>

<details>

<summary>2012-06-27 16:29:52 - Ranking by Dependence - A Fair Criteria</summary>

- *Harald Steck*

- `1206.6871v1` - [abs](http://arxiv.org/abs/1206.6871v1) - [pdf](http://arxiv.org/pdf/1206.6871v1)

> Estimating the dependences between random variables, and ranking them accordingly, is a prevalent problem in machine learning. Pursuing frequentist and information-theoretic approaches, we first show that the p-value and the mutual information can fail even in simplistic situations. We then propose two conditions for regularizing an estimator of dependence, which leads to a simple yet effective new measure. We discuss its advantages and compare it to well-established model-selection criteria. Apart from that, we derive a simple constraint for regularizing parameter estimates in a graphical model. This results in an analytical approximation for the optimal value of the equivalent sample size, which agrees very well with the more involved Bayesian approach in our experiments.

</details>

<details>

<summary>2012-06-27 16:30:29 - Bayesian Inference for Gaussian Mixed Graph Models</summary>

- *Ricardo Silva, Zoubin Ghahramani*

- `1206.6874v1` - [abs](http://arxiv.org/abs/1206.6874v1) - [pdf](http://arxiv.org/pdf/1206.6874v1)

> We introduce priors and algorithms to perform Bayesian inference in Gaussian models defined by acyclic directed mixed graphs. Such a class of graphs, composed of directed and bi-directed edges, is a representation of conditional independencies that is closed under marginalization and arises naturally from causal models which allow for unmeasured confounding. Monte Carlo methods and a variational approximation for such models are presented. Our algorithms for Bayesian inference allow the evaluation of posterior distributions for several quantities of interest, including causal effects that are not identifiable from data alone but could otherwise be inferred where informative prior knowledge about confounding is available.

</details>

<details>

<summary>2012-06-27 16:31:08 - Inference in Hybrid Bayesian Networks Using Mixtures of Gaussians</summary>

- *Prakash P. Shenoy*

- `1206.6877v1` - [abs](http://arxiv.org/abs/1206.6877v1) - [pdf](http://arxiv.org/pdf/1206.6877v1)

> The main goal of this paper is to describe a method for exact inference in general hybrid Bayesian networks (BNs) (with a mixture of discrete and continuous chance variables). Our method consists of approximating general hybrid Bayesian networks by a mixture of Gaussians (MoG) BNs. There exists a fast algorithm by Lauritzen-Jensen (LJ) for making exact inferences in MoG Bayesian networks, and there exists a commercial implementation of this algorithm. However, this algorithm can only be used for MoG BNs. Some limitations of such networks are as follows. All continuous chance variables must have conditional linear Gaussian distributions, and discrete chance nodes cannot have continuous parents. The methods described in this paper will enable us to use the LJ algorithm for a bigger class of hybrid Bayesian networks. This includes networks with continuous chance nodes with non-Gaussian distributions, networks with no restrictions on the topology of discrete and continuous variables, networks with conditionally deterministic variables that are a nonlinear function of their continuous parents, and networks with continuous chance variables whose variances are functions of their parents.

</details>

<details>

<summary>2012-06-27 19:59:59 - Gaussian Process Quantile Regression using Expectation Propagation</summary>

- *Alexis Boukouvalas, Remi Barillec, Dan Cornford*

- `1206.6391v1` - [abs](http://arxiv.org/abs/1206.6391v1) - [pdf](http://arxiv.org/pdf/1206.6391v1)

> Direct quantile regression involves estimating a given quantile of a response variable as a function of input variables. We present a new framework for direct quantile regression where a Gaussian process model is learned, minimising the expected tilted loss function. The integration required in learning is not analytically tractable so to speed up the learning we employ the Expectation Propagation algorithm. We describe how this work relates to other quantile regression methods and apply the method on both synthetic and real data sets. The method is shown to be competitive with state of the art methods whilst allowing for the leverage of the full Gaussian process probabilistic framework.

</details>

<details>

<summary>2012-06-27 19:59:59 - Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring</summary>

- *Sungjin Ahn, Anoop Korattikara, Max Welling*

- `1206.6380v1` - [abs](http://arxiv.org/abs/1206.6380v1) - [pdf](http://arxiv.org/pdf/1206.6380v1)

> In this paper we address the following question: Can we approximately sample from a Bayesian posterior distribution if we are only allowed to touch a small mini-batch of data-items for every sample we generate?. An algorithm based on the Langevin equation with stochastic gradients (SGLD) was previously proposed to solve this, but its mixing rate was slow. By leveraging the Bayesian Central Limit Theorem, we extend the SGLD algorithm so that at high mixing rates it will sample from a normal approximation of the posterior, while for slow mixing rates it will mimic the behavior of SGLD with a pre-conditioner matrix. As a bonus, the proposed algorithm is reminiscent of Fisher scoring (with stochastic gradients) and as such an efficient optimizer during burn-in.

</details>

<details>

<summary>2012-06-27 19:59:59 - How To Grade a Test Without Knowing the Answers --- A Bayesian Graphical Model for Adaptive Crowdsourcing and Aptitude Testing</summary>

- *Yoram Bachrach, Thore Graepel, Tom Minka, John Guiver*

- `1206.6386v1` - [abs](http://arxiv.org/abs/1206.6386v1) - [pdf](http://arxiv.org/pdf/1206.6386v1)

> We propose a new probabilistic graphical model that jointly models the difficulties of questions, the abilities of participants and the correct answers to questions in aptitude testing and crowdsourcing settings. We devise an active learning/adaptive testing scheme based on a greedy minimization of expected model entropy, which allows a more efficient resource allocation by dynamically choosing the next question to be asked based on the previous responses. We present experimental results that confirm the ability of our model to infer the required parameters and demonstrate that the adaptive testing scheme requires fewer questions to obtain the same accuracy as a static test scenario.

</details>

<details>

<summary>2012-06-27 19:59:59 - Parallelizing Exploration-Exploitation Tradeoffs with Gaussian Process Bandit Optimization</summary>

- *Thomas Desautels, Andreas Krause, Joel Burdick*

- `1206.6402v1` - [abs](http://arxiv.org/abs/1206.6402v1) - [pdf](http://arxiv.org/pdf/1206.6402v1)

> Can one parallelize complex exploration exploitation tradeoffs? As an example, consider the problem of optimal high-throughput experimental design, where we wish to sequentially design batches of experiments in order to simultaneously learn a surrogate function mapping stimulus to response and identify the maximum of the function. We formalize the task as a multi-armed bandit problem, where the unknown payoff function is sampled from a Gaussian process (GP), and instead of a single arm, in each round we pull a batch of several arms in parallel. We develop GP-BUCB, a principled algorithm for choosing batches, based on the GP-UCB algorithm for sequential GP optimization. We prove a surprising result; as compared to the sequential approach, the cumulative regret of the parallel algorithm only increases by a constant factor independent of the batch size B. Our results provide rigorous theoretical support for exploiting parallelism in Bayesian global optimization. We demonstrate the effectiveness of our approach on two real-world applications.

</details>

<details>

<summary>2012-06-27 19:59:59 - Bayesian Optimal Active Search and Surveying</summary>

- *Roman Garnett, Yamuna Krishnamurthy, Xuehan Xiong, Jeff Schneider, Richard Mann*

- `1206.6406v1` - [abs](http://arxiv.org/abs/1206.6406v1) - [pdf](http://arxiv.org/pdf/1206.6406v1)

> We consider two active binary-classification problems with atypical objectives. In the first, active search, our goal is to actively uncover as many members of a given class as possible. In the second, active surveying, our goal is to actively query points to ultimately predict the proportion of a given class. Numerous real-world problems can be framed in these terms, and in either case typical model-based concerns such as generalization error are only of secondary importance.   We approach these problems via Bayesian decision theory; after choosing natural utility functions, we derive the optimal policies. We provide three contributions. In addition to introducing the active surveying problem, we extend previous work on active search in two ways. First, we prove a novel theoretical result, that less-myopic approximations to the optimal policy can outperform more-myopic approximations by any arbitrary degree. We then derive bounds that for certain models allow us to reduce (in practice dramatically) the exponential search space required by a naive implementation of the optimal policy, enabling further lookahead while still ensuring that optimal decisions are always made.

</details>

<details>

<summary>2012-06-27 19:59:59 - The Nonparametric Metadata Dependent Relational Model</summary>

- *Dae Il Kim, Michael Hughes, Erik Sudderth*

- `1206.6414v1` - [abs](http://arxiv.org/abs/1206.6414v1) - [pdf](http://arxiv.org/pdf/1206.6414v1)

> We introduce the nonparametric metadata dependent relational (NMDR) model, a Bayesian nonparametric stochastic block model for network data. The NMDR allows the entities associated with each node to have mixed membership in an unbounded collection of latent communities. Learned regression models allow these memberships to depend on, and be predicted from, arbitrary node metadata. We develop efficient MCMC algorithms for learning NMDR models from partially observed node relationships. Retrospective MCMC methods allow our sampler to work directly with the infinite stick-breaking representation of the NMDR, avoiding the need for finite truncations. Our results demonstrate recovery of useful latent communities from real-world social and ecological networks, and the usefulness of metadata in link prediction tasks.

</details>

<details>

<summary>2012-06-27 19:59:59 - An Infinite Latent Attribute Model for Network Data</summary>

- *Konstantina Palla, David Knowles, Zoubin Ghahramani*

- `1206.6416v1` - [abs](http://arxiv.org/abs/1206.6416v1) - [pdf](http://arxiv.org/pdf/1206.6416v1)

> Latent variable models for network data extract a summary of the relational structure underlying an observed network. The simplest possible models subdivide nodes of the network into clusters; the probability of a link between any two nodes then depends only on their cluster assignment. Currently available models can be classified by whether clusters are disjoint or are allowed to overlap. These models can explain a "flat" clustering structure. Hierarchical Bayesian models provide a natural approach to capture more complex dependencies. We propose a model in which objects are characterised by a latent feature vector. Each feature is itself partitioned into disjoint groups (subclusters), corresponding to a second layer of hierarchy. In experimental comparisons, the model achieves significantly improved predictive performance on social and biological link prediction tasks. The results indicate that models with a single layer hierarchy over-simplify real networks.

</details>

<details>

<summary>2012-06-27 19:59:59 - Sparse Stochastic Inference for Latent Dirichlet allocation</summary>

- *David Mimno, Matt Hoffman, David Blei*

- `1206.6425v1` - [abs](http://arxiv.org/abs/1206.6425v1) - [pdf](http://arxiv.org/pdf/1206.6425v1)

> We present a hybrid algorithm for Bayesian topic models that combines the efficiency of sparse Gibbs sampling with the scalability of online stochastic inference. We used our algorithm to analyze a corpus of 1.2 million books (33 billion words) with thousands of topics. Our approach reduces the bias of variational inference and generalizes to many Bayesian hidden-variable models.

</details>

<details>

<summary>2012-06-27 19:59:59 - Convergence of the EM Algorithm for Gaussian Mixtures with Unbalanced Mixing Coefficients</summary>

- *Iftekhar Naim, Daniel Gildea*

- `1206.6427v1` - [abs](http://arxiv.org/abs/1206.6427v1) - [pdf](http://arxiv.org/pdf/1206.6427v1)

> The speed of convergence of the Expectation Maximization (EM) algorithm for Gaussian mixture model fitting is known to be dependent on the amount of overlap among the mixture components. In this paper, we study the impact of mixing coefficients on the convergence of EM. We show that when the mixture components exhibit some overlap, the convergence of EM becomes slower as the dynamic range among the mixing coefficients increases. We propose a deterministic anti-annealing algorithm, that significantly improves the speed of convergence of EM for such mixtures with unbalanced mixing coefficients. The proposed algorithm is compared against other standard optimization techniques like BFGS, Conjugate Gradient, and the traditional EM algorithm. Finally, we propose a similar deterministic anti-annealing based algorithm for the Dirichlet process mixture model and demonstrate its advantages over the conventional variational Bayesian approach.

</details>

<details>

<summary>2012-06-27 19:59:59 - Variational Bayesian Inference with Stochastic Search</summary>

- *John Paisley, David Blei, Michael Jordan*

- `1206.6430v1` - [abs](http://arxiv.org/abs/1206.6430v1) - [pdf](http://arxiv.org/pdf/1206.6430v1)

> Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.

</details>

<details>

<summary>2012-06-27 19:59:59 - Exact Maximum Margin Structure Learning of Bayesian Networks</summary>

- *Robert Peharz, Franz Pernkopf*

- `1206.6431v1` - [abs](http://arxiv.org/abs/1206.6431v1) - [pdf](http://arxiv.org/pdf/1206.6431v1)

> Recently, there has been much interest in finding globally optimal Bayesian network structures. These techniques were developed for generative scores and can not be directly extended to discriminative scores, as desired for classification. In this paper, we propose an exact method for finding network structures maximizing the probabilistic soft margin, a successfully applied discriminative score. Our method is based on branch-and-bound techniques within a linear programming framework and maintains an any-time solution, together with worst-case sub-optimality bounds. We apply a set of order constraints for enforcing the network structure to be acyclic, which allows a compact problem representation and the use of general-purpose optimization techniques. In classification experiments, our methods clearly outperform generatively trained network structures and compete with support vector machines.

</details>

<details>

<summary>2012-06-27 19:59:59 - Copula Mixture Model for Dependency-seeking Clustering</summary>

- *Melanie Rey, Volker Roth*

- `1206.6433v1` - [abs](http://arxiv.org/abs/1206.6433v1) - [pdf](http://arxiv.org/pdf/1206.6433v1)

> We introduce a copula mixture model to perform dependency-seeking clustering when co-occurring samples from different data sources are available. The model takes advantage of the great flexibility offered by the copulas framework to extend mixtures of Canonical Correlation Analysis to multivariate data with arbitrary continuous marginal densities. We formulate our model as a non-parametric Bayesian mixture, while providing efficient MCMC inference. Experiments on synthetic and real data demonstrate that the increased flexibility of the copula mixture significantly improves the clustering and the interpretability of the results.

</details>

<details>

<summary>2012-06-27 19:59:59 - Large Scale Variational Bayesian Inference for Structured Scale Mixture Models</summary>

- *Young Jun Ko, Matthias Seeger*

- `1206.6437v1` - [abs](http://arxiv.org/abs/1206.6437v1) - [pdf](http://arxiv.org/pdf/1206.6437v1)

> Natural image statistics exhibit hierarchical dependencies across multiple scales. Representing such prior knowledge in non-factorial latent tree models can boost performance of image denoising, inpainting, deconvolution or reconstruction substantially, beyond standard factorial "sparse" methodology. We derive a large scale approximate Bayesian inference algorithm for linear models with non-factorial (latent tree-structured) scale mixture priors. Experimental results on a range of denoising and inpainting problems demonstrate substantially improved performance compared to MAP estimation or to inference with factorial priors.

</details>

<details>

<summary>2012-06-27 19:59:59 - Monte Carlo Bayesian Reinforcement Learning</summary>

- *Yi Wang, Kok Sung Won, David Hsu, Wee Sun Lee*

- `1206.6449v1` - [abs](http://arxiv.org/abs/1206.6449v1) - [pdf](http://arxiv.org/pdf/1206.6449v1)

> Bayesian reinforcement learning (BRL) encodes prior knowledge of the world in a model and represents uncertainty in model parameters by maintaining a probability distribution over them. This paper presents Monte Carlo BRL (MC-BRL), a simple and general approach to BRL. MC-BRL samples a priori a finite set of hypotheses for the model parameter values and forms a discrete partially observable Markov decision process (POMDP) whose state space is a cross product of the state space for the reinforcement learning task and the sampled model parameter space. The POMDP does not require conjugate distributions for belief representation, as earlier works do, and can be solved relatively easily with point-based approximation algorithms. MC-BRL naturally handles both fully and partially observable worlds. Theoretical and experimental results show that the discrete POMDP approximates the underlying BRL task well with guaranteed performance.

</details>

<details>

<summary>2012-06-27 19:59:59 - Smoothness and Structure Learning by Proxy</summary>

- *Benjamin Yackley, Terran Lane*

- `1206.6452v1` - [abs](http://arxiv.org/abs/1206.6452v1) - [pdf](http://arxiv.org/pdf/1206.6452v1)

> As data sets grow in size, the ability of learning methods to find structure in them is increasingly hampered by the time needed to search the large spaces of possibilities and generate a score for each that takes all of the observed data into account. For instance, Bayesian networks, the model chosen in this paper, have a super-exponentially large search space for a fixed number of variables. One possible method to alleviate this problem is to use a proxy, such as a Gaussian Process regressor, in place of the true scoring function, training it on a selection of sampled networks. We prove here that the use of such a proxy is well-founded, as we can bound the smoothness of a commonly-used scoring function for Bayesian network structure learning. We show here that, compared to an identical search strategy using the network?s exact scores, our proxy-based search is able to get equivalent or better scores on a number of data sets in a fraction of the time.

</details>

<details>

<summary>2012-06-27 19:59:59 - Lognormal and Gamma Mixed Negative Binomial Regression</summary>

- *Mingyuan Zhou, Lingbo Li, David Dunson, Lawrence Carin*

- `1206.6456v1` - [abs](http://arxiv.org/abs/1206.6456v1) - [pdf](http://arxiv.org/pdf/1206.6456v1)

> In regression analysis of counts, a lack of simple and efficient algorithms for posterior computation has made Bayesian approaches appear unattractive and thus underdeveloped. We propose a lognormal and gamma mixed negative binomial (NB) regression model for counts, and present efficient closed-form Bayesian inference; unlike conventional Poisson models, the proposed approach has two free parameters to include two different kinds of random effects, and allows the incorporation of prior information, such as sparsity in the regression coefficients. By placing a gamma distribution prior on the NB dispersion parameter r, and connecting a lognormal distribution prior with the logit of the NB probability parameter p, efficient Gibbs sampling and variational Bayes inference are both developed. The closed-form updates are obtained by exploiting conditional conjugacy via both a compound Poisson representation and a Polya-Gamma distribution based data augmentation approach. The proposed Bayesian inference can be implemented routinely, while being easily generalizable to more complex settings involving multivariate dependence structures. The algorithms are illustrated using real examples.

</details>

<details>

<summary>2012-06-27 19:59:59 - Bayesian Conditional Cointegration</summary>

- *Chris Bracegirdle, David Barber*

- `1206.6459v1` - [abs](http://arxiv.org/abs/1206.6459v1) - [pdf](http://arxiv.org/pdf/1206.6459v1)

> Cointegration is an important topic for time-series, and describes a relationship between two series in which a linear combination is stationary. Classically, the test for cointegration is based on a two stage process in which first the linear relation between the series is estimated by Ordinary Least Squares. Subsequently a unit root test is performed on the residuals. A well-known deficiency of this classical approach is that it can lead to erroneous conclusions about the presence of cointegration. As an alternative, we present a framework for estimating whether cointegration exists using Bayesian inference which is empirically superior to the classical approach. Finally, we apply our technique to model segmented cointegration in which cointegration may exist only for limited time. In contrast to previous approaches our model makes no restriction on the number of possible cointegration segments.

</details>

<details>

<summary>2012-06-27 19:59:59 - Bayesian Efficient Multiple Kernel Learning</summary>

- *Mehmet Gonen*

- `1206.6465v1` - [abs](http://arxiv.org/abs/1206.6465v1) - [pdf](http://arxiv.org/pdf/1206.6465v1)

> Multiple kernel learning algorithms are proposed to combine kernels in order to obtain a better similarity measure or to integrate feature representations coming from different data sources. Most of the previous research on such methods is focused on the computational efficiency issue. However, it is still not feasible to combine many kernels using existing Bayesian approaches due to their high time complexity. We propose a fully conjugate Bayesian formulation and derive a deterministic variational approximation, which allows us to combine hundreds or thousands of kernels very efficiently. We briefly explain how the proposed method can be extended for multiclass learning and semi-supervised learning. Experiments with large numbers of kernels on benchmark data sets show that our inference method is quite fast, requiring less than a minute. On one bioinformatics and three image recognition data sets, our method outperforms previously reported results with better generalization performance.

</details>

<details>

<summary>2012-06-27 19:59:59 - Variational Inference in Non-negative Factorial Hidden Markov Models for Efficient Audio Source Separation</summary>

- *Gautham Mysore, Maneesh Sahani*

- `1206.6468v1` - [abs](http://arxiv.org/abs/1206.6468v1) - [pdf](http://arxiv.org/pdf/1206.6468v1)

> The past decade has seen substantial work on the use of non-negative matrix factorization and its probabilistic counterparts for audio source separation. Although able to capture audio spectral structure well, these models neglect the non-stationarity and temporal dynamics that are important properties of audio. The recently proposed non-negative factorial hidden Markov model (N-FHMM) introduces a temporal dimension and improves source separation performance. However, the factorial nature of this model makes the complexity of inference exponential in the number of sound sources. Here, we present a Bayesian variant of the N-FHMM suited to an efficient variational inference algorithm, whose complexity is linear in the number of sound sources. Our algorithm performs comparably to exact inference in the original N-FHMM but is significantly faster. In typical configurations of the N-FHMM, our method achieves around a 30x increase in speed.

</details>

<details>

<summary>2012-06-27 19:59:59 - Flexible Modeling of Latent Task Structures in Multitask Learning</summary>

- *Alexandre Passos, Piyush Rai, Jacques Wainer, Hal Daume III*

- `1206.6486v1` - [abs](http://arxiv.org/abs/1206.6486v1) - [pdf](http://arxiv.org/pdf/1206.6486v1)

> Multitask learning algorithms are typically designed assuming some fixed, a priori known latent structure shared by all the tasks. However, it is usually unclear what type of latent task structure is the most appropriate for a given multitask learning problem. Ideally, the "right" latent task structure should be learned in a data-driven manner. We present a flexible, nonparametric Bayesian model that posits a mixture of factor analyzers structure on the tasks. The nonparametric aspect makes the model expressive enough to subsume many existing models of latent task structures (e.g, mean-regularized tasks, clustered tasks, low-rank or linear/non-linear subspace assumption on tasks, etc.). Moreover, it can also learn more general task structures, addressing the shortcomings of such models. We present a variational inference algorithm for our model. Experimental results on synthetic and real-world datasets, on both regression and classification problems, demonstrate the effectiveness of the proposed method.

</details>

<details>

<summary>2012-06-28 10:53:57 - Change-point model on nonhomogeneous Poisson processes with application in copy number profiling by next-generation DNA sequencing</summary>

- *Jeremy J. Shen, Nancy R. Zhang*

- `1206.6627v1` - [abs](http://arxiv.org/abs/1206.6627v1) - [pdf](http://arxiv.org/pdf/1206.6627v1)

> We propose a flexible change-point model for inhomogeneous Poisson Processes, which arise naturally from next-generation DNA sequencing, and derive score and generalized likelihood statistics for shifts in intensity functions. We construct a modified Bayesian information criterion (mBIC) to guide model selection, and point-wise approximate Bayesian confidence intervals for assessing the confidence in the segmentation. The model is applied to DNA Copy Number profiling with sequencing data and evaluated on simulated spike-in and real data sets.

</details>

<details>

<summary>2012-06-28 12:14:09 - Modeling dependent gene expression</summary>

- *Donatello Telesca, Peter Müller, Giovanni Parmigiani, Ralph S. Freedman*

- `1206.6650v1` - [abs](http://arxiv.org/abs/1206.6650v1) - [pdf](http://arxiv.org/pdf/1206.6650v1)

> In this paper we propose a Bayesian approach for inference about dependence of high throughput gene expression. Our goals are to use prior knowledge about pathways to anchor inference about dependence among genes; to account for this dependence while making inferences about differences in mean expression across phenotypes; and to explore differences in the dependence itself across phenotypes. Useful features of the proposed approach are a model-based parsimonious representation of expression as an ordinal outcome, a novel and flexible representation of prior information on the nature of dependencies, and the use of a coherent probability model over both the structure and strength of the dependencies of interest. We evaluate our approach through simulations and in the analysis of data on expression of genes in the Complement and Coagulation Cascade pathway in ovarian cancer.

</details>

<details>

<summary>2012-06-28 12:18:20 - Bayesian hierarchical rule modeling for predicting medical conditions</summary>

- *Tyler H. McCormick, Cynthia Rudin, David Madigan*

- `1206.6653v1` - [abs](http://arxiv.org/abs/1206.6653v1) - [pdf](http://arxiv.org/pdf/1206.6653v1)

> We propose a statistical modeling technique, called the Hierarchical Association Rule Model (HARM), that predicts a patient's possible future medical conditions given the patient's current and past history of reported conditions. The core of our technique is a Bayesian hierarchical model for selecting predictive association rules (such as "condition 1 and condition 2 $\rightarrow$ condition 3") from a large set of candidate rules. Because this method "borrows strength" using the conditions of many similar patients, it is able to provide predictions specialized to any given patient, even when little information about the patient's history of conditions is available.

</details>

<details>

<summary>2012-06-28 12:30:38 - On Some Asymptotic Properties and an Almost Sure Approximation of the Normalized Inverse-Gaussian Process</summary>

- *Luai Al Labadi, Mahmoud Zarepour*

- `1206.6658v1` - [abs](http://arxiv.org/abs/1206.6658v1) - [pdf](http://arxiv.org/pdf/1206.6658v1)

> In this paper, we present some asymptotic properties of the normalized inverse-Gaussian process. In particular, when the concentration parameter is large, we establish an analogue of the empirical functional central limit theorem, the strong law of large numbers and the Glivenko-Cantelli theorem for the normalized inverse-Gaussian process and its corresponding quantile process. We also derive a finite sum-representation that converges almost surely to the Ferguson and Klass representation of the normalized inverse-Gaussian process. This almost sure approximation can be used to simulate efficiently the normalized inverse-Gaussian process.

</details>

<details>

<summary>2012-06-28 12:44:42 - Bayesian modeling longitudinal dyadic data with nonignorable dropout, with application to a breast cancer study</summary>

- *Guangyu Zhang, Ying Yuan*

- `1206.6664v1` - [abs](http://arxiv.org/abs/1206.6664v1) - [pdf](http://arxiv.org/pdf/1206.6664v1)

> Dyadic data are common in the social and behavioral sciences, in which members of dyads are correlated due to the interdependence structure within dyads. The analysis of longitudinal dyadic data becomes complex when nonignorable dropouts occur. We propose a fully Bayesian selection-model-based approach to analyze longitudinal dyadic data with nonignorable dropouts. We model repeated measures on subjects by a transition model and account for within-dyad correlations by random effects. In the model, we allow subject's outcome to depend on his/her own characteristics and measure history, as well as those of the other member in the dyad. We further account for the nonignorable missing data mechanism using a selection model in which the probability of dropout depends on the missing outcome. We propose a Gibbs sampler algorithm to fit the model. Simulation studies show that the proposed method effectively addresses the problem of nonignorable dropouts. We illustrate our methodology using a longitudinal breast cancer study.

</details>

<details>

<summary>2012-06-28 13:10:38 - Meta-analysis of functional neuroimaging data using Bayesian nonparametric binary regression</summary>

- *Yu Ryan Yue, Martin A. Lindquist, Ji Meng Loh*

- `1206.6674v1` - [abs](http://arxiv.org/abs/1206.6674v1) - [pdf](http://arxiv.org/pdf/1206.6674v1)

> In this work we perform a meta-analysis of neuroimaging data, consisting of locations of peak activations identified in 162 separate studies on emotion. Neuroimaging meta-analyses are typically performed using kernel-based methods. However, these methods require the width of the kernel to be set a priori and to be constant across the brain. To address these issues, we propose a fully Bayesian nonparametric binary regression method to perform neuroimaging meta-analyses. In our method, each location (or voxel) has a probability of being a peak activation, and the corresponding probability function is based on a spatially adaptive Gaussian Markov random field (GMRF). We also include parameters in the model to robustify the procedure against miscoding of the voxel response. Posterior inference is implemented using efficient MCMC algorithms extended from those introduced in Holmes and Held [Bayesian Anal. 1 (2006) 145--168]. Our method allows the probability function to be locally adaptive with respect to the covariates, that is, to be smooth in one region of the covariate space and wiggly or even discontinuous in another. Posterior miscoding probabilities for each of the identified voxels can also be obtained, identifying voxels that may have been falsely classified as being activated. Simulation studies and application to the emotion neuroimaging data indicate that our method is superior to standard kernel-based methods.

</details>

<details>

<summary>2012-06-28 19:58:18 - Stirling's approximations for exchangeable Gibbs weights</summary>

- *Annalisa Cerquetti*

- `1206.6812v1` - [abs](http://arxiv.org/abs/1206.6812v1) - [pdf](http://arxiv.org/pdf/1206.6812v1)

> We obtain some approximation results for the weights appearing in the exchangeable partition probability function identifying Gibbs partition models of parameter $\alpha \in (0,1)$, as introduced in Gnedin and Pitman (2006). We rely on approximation results for central and non-central generalized Stirling numbers and on known results for conditional and unconditional $\alpha$ diversity. We provide an application to an approximate Bayesian nonparametric estimation of discovery probability in species sampling problems under normalized inverse Gaussian priors.

</details>

<details>

<summary>2012-06-29 06:42:21 - A Bayesian model averaging approach for observational gene expression studies</summary>

- *Xi Kathy Zhou, Fei Liu, Andrew J. Dannenberg*

- `1206.6952v1` - [abs](http://arxiv.org/abs/1206.6952v1) - [pdf](http://arxiv.org/pdf/1206.6952v1)

> Identifying differentially expressed (DE) genes associated with a sample characteristic is the primary objective of many microarray studies. As more and more studies are carried out with observational rather than well controlled experimental samples, it becomes important to evaluate and properly control the impact of sample heterogeneity on DE gene finding. Typical methods for identifying DE genes require ranking all the genes according to a preselected statistic based on a single model for two or more group comparisons, with or without adjustment for other covariates. Such single model approaches unavoidably result in model misspecification, which can lead to increased error due to bias for some genes and reduced efficiency for the others. We evaluated the impact of model misspecification from such approaches on detecting DE genes and identified parameters that affect the magnitude of impact. To properly control for sample heterogeneity and to provide a flexible and coherent framework for identifying simultaneously DE genes associated with a single or multiple sample characteristics and/or their interactions, we proposed a Bayesian model averaging approach which corrects the model misspecification by averaging over model space formed by all relevant covariates. An empirical approach is suggested for specifying prior model probabilities. We demonstrated through simulated microarray data that this approach resulted in improved performance in DE gene identification compared to the single model approaches. The flexibility of this approach is demonstrated through our analysis of data from two observational microarray studies.

</details>


## 2012-07

<details>

<summary>2012-07-01 10:46:44 - Sequential Design for Computer Experiments with a Flexible Bayesian Additive Model</summary>

- *Hugh Chipman, Pritam Ranjan, Weiwei Wang*

- `1203.1078v2` - [abs](http://arxiv.org/abs/1203.1078v2) - [pdf](http://arxiv.org/pdf/1203.1078v2)

> In computer experiments, a mathematical model implemented on a computer is used to represent complex physical phenomena. These models, known as computer simulators, enable experimental study of a virtual representation of the complex phenomena. Simulators can be thought of as complex functions that take many inputs and provide an output. Often these simulators are themselves expensive to compute, and may be approximated by "surrogate models" such as statistical regression models. In this paper we consider a new kind of surrogate model, a Bayesian ensemble of trees (Chipman et al. 2010), with the specific goal of learning enough about the simulator that a particular feature of the simulator can be estimated. We focus on identifying the simulator's global minimum. Utilizing the Bayesian version of the Expected Improvement criterion (Jones et al. 1998), we show that this ensemble is particularly effective when the simulator is ill-behaved, exhibiting nonstationarity or abrupt changes in the response. A number of illustrations of the approach are given, including a tidal power application.

</details>

<details>

<summary>2012-07-02 10:06:26 - Readouts for Echo-state Networks Built using Locally Regularized Orthogonal Forward Regression</summary>

- *Ján Dolinský, Kei Hirose, Sadanori Konishi*

- `1110.4304v3` - [abs](http://arxiv.org/abs/1110.4304v3) - [pdf](http://arxiv.org/pdf/1110.4304v3)

> Echo state network (ESN) is viewed as a temporal non-orthogonal expansion with pseudo-random parameters. Such expansions naturally give rise to regressors of various relevance to a teacher output. We illustrate that often only a certain amount of the generated echo-regressors effectively explain the variance of the teacher output and also that sole local regularization is not able to provide in-depth information concerning the importance of the generated regressors. The importance is therefore determined by a joint calculation of the individual variance contributions and Bayesian relevance using locally regularized orthogonal forward regression (LROFR) algorithm. This information can be advantageously used in a variety of ways for an in-depth analysis of an ESN structure and its state-space parameters in relation to the unknown dynamics of the underlying problem. We present locally regularized linear readout built using LROFR. The readout may have a different dimensionality than an ESN model itself, and besides improving robustness and accuracy of an ESN it relates the echo-regressors to different features of the training data and may determine what type of an additional readout is suitable for a task at hand. Moreover, as flexibility of the linear readout has limitations and might sometimes be insufficient for certain tasks, we also present a radial basis function (RBF) readout built using LROFR. It is a flexible and parsimonious readout with excellent generalization abilities and is a viable alternative to readouts based on a feed-forward neural network (FFNN) or an RBF net built using relevance vector machine (RVM).

</details>

<details>

<summary>2012-07-02 22:09:56 - Bayesian Analysis of Inertial Confinement Fusion Experiments at the National Ignition Facility</summary>

- *J. A. Gaffney, D. Clark, V. Sonnad, S. B. Libby*

- `1207.0532v1` - [abs](http://arxiv.org/abs/1207.0532v1) - [pdf](http://arxiv.org/pdf/1207.0532v1)

> We develop a Bayesian inference method that allows the efficient determination of several interesting parameters from complicated high-energy-density experiments performed on the National Ignition Facility (NIF). The model is based on an exploration of phase space using the hydrodynamic code HYDRA. A linear model is used to describe the effect of nuisance parameters on the analysis, allowing an analytic likelihood to be derived that can be determined from a small number of HYDRA runs and then used in existing advanced statistical analysis methods. This approach is applied to a recent experiment in order to determine the carbon opacity and X-ray drive; it is found that the inclusion of prior expert knowledge and fluctuations in capsule dimensions and chemical composition significantly improve the agreement between experiment and theoretical opacity calculations. A parameterisation of HYDRA results is used to test the application of both Markov chain Monte Carlo (MCMC) and genetic algorithm (GA) techniques to explore the posterior. These approaches have distinct advantages and we show that both can allow the efficient analysis of high energy density experiments.

</details>

<details>

<summary>2012-07-03 06:46:04 - Simultaneous SNP identification in association studies with missing data</summary>

- *Zhen Li, Vikneswaran Gopal, Xiaobo Li, John M. Davis, George Casella*

- `1207.0280v2` - [abs](http://arxiv.org/abs/1207.0280v2) - [pdf](http://arxiv.org/pdf/1207.0280v2)

> Association testing aims to discover the underlying relationship between genotypes (usually Single Nucleotide Polymorphisms, or SNPs) and phenotypes (attributes, or traits). The typically large data sets used in association testing often contain missing values. Standard statistical methods either impute the missing values using relatively simple assumptions, or delete them, or both, which can generate biased results. Here we describe the Bayesian hierarchical model BAMD (Bayesian Association with Missing Data). BAMD is a Gibbs sampler, in which missing values are multiply imputed based upon all of the available information in the data set. We estimate the parameters and prove that updating one SNP at each iteration preserves the ergodic property of the Markov chain, and at the same time improves computational speed. We also implement a model selection option in BAMD, which enables potential detection of SNP interactions. Simulations show that unbiased estimates of SNP effects are recovered with missing genotype data. Also, we validate associations between SNPs and a carbon isotope discrimination phenotype that were previously reported using a family based method, and discover an additional SNP associated with the trait. BAMD is available as an R-package from http://cran.r-project.org/package=BAMD

</details>

<details>

<summary>2012-07-03 12:53:41 - Bayesian estimation of a bivariate copula using the Jeffreys prior</summary>

- *Simon Guillotte, François Perron*

- `0908.2372v3` - [abs](http://arxiv.org/abs/0908.2372v3) - [pdf](http://arxiv.org/pdf/0908.2372v3)

> A bivariate distribution with continuous margins can be uniquely decomposed via a copula and its marginal distributions. We consider the problem of estimating the copula function and adopt a Bayesian approach. On the space of copula functions, we construct a finite-dimensional approximation subspace that is parametrized by a doubly stochastic matrix. A major problem here is the selection of a prior distribution on the space of doubly stochastic matrices also known as the Birkhoff polytope. The main contributions of this paper are the derivation of a simple formula for the Jeffreys prior and showing that it is proper. It is known in the literature that for a complex problem like the one treated here, the above results are difficult to obtain. The Bayes estimator resulting from the Jeffreys prior is then evaluated numerically via Markov chain Monte Carlo methodology. A rather extensive simulation experiment is carried out. In many cases, the results favour the Bayes estimator over frequentist estimators such as the standard kernel estimator and Deheuvels' estimator in terms of mean integrated squared error.

</details>

<details>

<summary>2012-07-04 10:46:14 - Flexible Mixture Modeling with the Polynomial Gaussian Cluster-Weighted Model</summary>

- *Antonio Punzo*

- `1207.0939v1` - [abs](http://arxiv.org/abs/1207.0939v1) - [pdf](http://arxiv.org/pdf/1207.0939v1)

> In the mixture modeling frame, this paper presents the polynomial Gaussian cluster-weighted model (CWM). It extends the linear Gaussian CWM, for bivariate data, in a twofold way. Firstly, it allows for possible nonlinear dependencies in the mixture components by considering a polynomial regression. Secondly, it is not restricted to be used for model-based clustering only being contextualized in the most general model-based classification framework. Maximum likelihood parameter estimates are derived using the EM algorithm and model selection is carried out using the Bayesian information criterion (BIC) and the integrated completed likelihood (ICL). The paper also investigates the conditions under which the posterior probabilities of component-membership from a polynomial Gaussian CWM coincide with those of other well-established mixture-models which are related to it. With respect to these models, the polynomial Gaussian CWM has shown to give excellent clustering and classification results when applied to the artificial and real data considered in the paper.

</details>

<details>

<summary>2012-07-04 15:09:05 - PAC-Bayesian Majority Vote for Late Classifier Fusion</summary>

- *Emilie Morvant, Amaury Habrard, Stéphane Ayache*

- `1207.1019v1` - [abs](http://arxiv.org/abs/1207.1019v1) - [pdf](http://arxiv.org/pdf/1207.1019v1)

> A lot of attention has been devoted to multimedia indexing over the past few years. In the literature, we often consider two kinds of fusion schemes: The early fusion and the late fusion. In this paper we focus on late classifier fusion, where one combines the scores of each modality at the decision level. To tackle this problem, we investigate a recent and elegant well-founded quadratic program named MinCq coming from the Machine Learning PAC-Bayes theory. MinCq looks for the weighted combination, over a set of real-valued functions seen as voters, leading to the lowest misclassification rate, while making use of the voters' diversity. We provide evidence that this method is naturally adapted to late fusion procedure. We propose an extension of MinCq by adding an order- preserving pairwise loss for ranking, helping to improve Mean Averaged Precision measure. We confirm the good behavior of the MinCq-based fusion approaches with experiments on a real image benchmark.

</details>

<details>

<summary>2012-07-04 16:03:10 - Learning from Sparse Data by Exploiting Monotonicity Constraints</summary>

- *Eric E. Altendorf, Angelo C. Restificar, Thomas G. Dietterich*

- `1207.1364v1` - [abs](http://arxiv.org/abs/1207.1364v1) - [pdf](http://arxiv.org/pdf/1207.1364v1)

> When training data is sparse, more domain knowledge must be incorporated into the learning algorithm in order to reduce the effective size of the hypothesis space. This paper builds on previous work in which knowledge about qualitative monotonicities was formally represented and incorporated into learning algorithms (e.g., Clark & Matwin's work with the CN2 rule learning algorithm). We show how to interpret knowledge of qualitative influences, and in particular of monotonicities, as constraints on probability distributions, and to incorporate this knowledge into Bayesian network learning algorithms. We show that this yields improved accuracy, particularly with very small training sets (e.g. less than 10 examples).

</details>

<details>

<summary>2012-07-04 16:03:31 - Learning Factor Graphs in Polynomial Time & Sample Complexity</summary>

- *Pieter Abbeel, Daphne Koller, Andrew Y. Ng*

- `1207.1366v1` - [abs](http://arxiv.org/abs/1207.1366v1) - [pdf](http://arxiv.org/pdf/1207.1366v1)

> We study computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded factor size and bounded connectivity can be learned in polynomial time and polynomial number of samples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Unlike maximum likelihood estimation, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks.

</details>

<details>

<summary>2012-07-04 16:05:05 - Belief Updating and Learning in Semi-Qualitative Probabilistic Networks</summary>

- *Cassio Polpo de Campos, Fabio Gagliardi Cozman*

- `1207.1367v1` - [abs](http://arxiv.org/abs/1207.1367v1) - [pdf](http://arxiv.org/pdf/1207.1367v1)

> This paper explores semi-qualitative probabilistic networks (SQPNs) that combine numeric and qualitative information. We first show that exact inferences with SQPNs are NPPP-Complete. We then show that existing qualitative relations in SQPNs (plus probabilistic logic and imprecise assessments) can be dealt effectively through multilinear programming. We then discuss learning: we consider a maximum likelihood method that generates point estimates given a SQPN and empirical data, and we describe a Bayesian-minded method that employs the Imprecise Dirichlet Model to generate set-valued estimates.

</details>

<details>

<summary>2012-07-04 16:10:18 - Bayes Blocks: An Implementation of the Variational Bayesian Building Blocks Framework</summary>

- *Markus Harva, Tapani Raiko, Antti Honkela, Harri Valpola, Juha Karhunen*

- `1207.1380v1` - [abs](http://arxiv.org/abs/1207.1380v1) - [pdf](http://arxiv.org/pdf/1207.1380v1)

> A software library for constructing and learning probabilistic models is presented. The library offers a set of building blocks from which a large variety of static and dynamic models can be built. These include hierarchical models for variances of other variables and many nonlinear models. The underlying variational Bayesian machinery, providing for fast and robust estimation but being mathematically rather involved, is almost completely hidden from the user thus making it very easy to use the library. The building blocks include Gaussian, rectified Gaussian and mixture-of-Gaussians variables and computational nodes which can be combined rather freely.

</details>

<details>

<summary>2012-07-04 16:12:02 - Maximum Margin Bayesian Networks</summary>

- *Yuhong Guo, Dana Wilkinson, Dale Schuurmans*

- `1207.1382v1` - [abs](http://arxiv.org/abs/1207.1382v1) - [pdf](http://arxiv.org/pdf/1207.1382v1)

> We consider the problem of learning Bayesian network classifiers that maximize the marginover a set of classification variables. We find that this problem is harder for Bayesian networks than for undirected graphical models like maximum margin Markov networks. The main difficulty is that the parameters in a Bayesian network must satisfy additional normalization constraints that an undirected graphical model need not respect. These additional constraints complicate the optimization task. Nevertheless, we derive an effective training algorithm that solves the maximum margin training problem for a range of Bayesian network topologies, and converges to an approximate solution for arbitrary network topologies. Experimental results show that the method can demonstrate improved generalization performance over Markov networks when the directed graphical structure encodes relevant knowledge. In practice, the training technique allows one to combine prior knowledge expressed as a directed (causal) model with state of the art discriminative learning methods.

</details>

<details>

<summary>2012-07-04 16:13:39 - Learning Bayesian Network Parameters with Prior Knowledge about Context-Specific Qualitative Influences</summary>

- *Ad Feelders, Linda C. van der Gaag*

- `1207.1387v1` - [abs](http://arxiv.org/abs/1207.1387v1) - [pdf](http://arxiv.org/pdf/1207.1387v1)

> We present a method for learning the parameters of a Bayesian network with prior knowledge about the signs of influences between variables. Our method accommodates not just the standard signs, but provides for context-specific signs as well. We show how the various signs translate into order constraints on the network parameters and how isotonic regression can be used to compute order-constrained estimates from the available data. Our experimental results show that taking prior knowledge about the signs of influences into account leads to an improved fit of the true distribution, especially when only a small sample of data is available. Moreover, the computed estimates are guaranteed to be consistent with the specified signs, thereby resulting in a network that is more likely to be accepted by experts in its domain of application.

</details>

<details>

<summary>2012-07-04 16:31:04 - Ordering-Based Search: A Simple and Effective Algorithm for Learning Bayesian Networks</summary>

- *Marc Teyssier, Daphne Koller*

- `1207.1429v1` - [abs](http://arxiv.org/abs/1207.1429v1) - [pdf](http://arxiv.org/pdf/1207.1429v1)

> One of the basic tasks for Bayesian networks (BNs) is that of learning a network structure from data. The BN-learning problem is NP-hard, so the standard solution is heuristic search. Many approaches have been proposed for this task, but only a very small number outperform the baseline of greedy hill-climbing with tabu lists; moreover, many of the proposed algorithms are quite complex and hard to implement. In this paper, we propose a very simple and easy-to-implement method for addressing this task. Our approach is based on the well-known fact that the best network (of bounded in-degree) consistent with a given node ordering can be found very efficiently. We therefore propose a search not over the space of structures, but over the space of orderings, selecting for each ordering the best network consistent with it. This search space is much smaller, makes more global search steps, has a lower branching factor, and avoids costly acyclicity checks. We present results for this algorithm on both synthetic and real data sets, evaluating both the score of the network found and in the running time. We show that ordering-based search outperforms the standard baseline, and is competitive with recent algorithms that are much harder to implement.

</details>

<details>

<summary>2012-07-05 11:07:27 - Robust Bayesian inference of networks using Dirichlet t-distributions</summary>

- *Michael Finegold, Mathias Drton*

- `1207.1221v1` - [abs](http://arxiv.org/abs/1207.1221v1) - [pdf](http://arxiv.org/pdf/1207.1221v1)

> Bayesian graphical modeling provides an appealing way to obtain uncertainty estimates when inferring network structures, and much recent progress has been made for Gaussian models. These models have been used extensively in applications to gene expression data, even in cases where there appears to be significant deviations from the Gaussian model. For more robust inferences, it is natural to consider extensions to t-distribution models. We argue that the classical multivariate t-distribution, defined using a single latent Gamma random variable to rescale a Gaussian random vector, is of little use in highly multivariate settings, and propose other, more flexible t-distributions. Using an independent Gamma-divisor for each component of the random vector defines what we term the alternative t-distribution. The associated model allows one to extract information from highly multivariate data even when most experiments contain outliers for some of their measurements. However, the use of this alternative model comes at increased computational cost and imposes constraints on the achievable correlation structures, raising the need for a compromise between the classical and alternative models. To this end we propose the use of Dirichlet processes for adaptive clustering of the latent Gamma-scalars, each of which may then divide a group of latent Gaussian variables. Dirichlet processes are commonly used to cluster independent observations; here they are used instead to cluster the dependent components of a single observation. The resulting Dirichlet t-distribution interpolates naturally between the two extreme cases of the classical and alternative t-distributions and combines more appealing modeling of the multivariate dependence structure with favorable computational properties.

</details>

<details>

<summary>2012-07-09 06:42:21 - Bayesian Subset Simulation: a kriging-based subset simulation algorithm for the estimation of small probabilities of failure</summary>

- *Ling Li, Julien Bect, Emmanuel Vazquez*

- `1207.1963v1` - [abs](http://arxiv.org/abs/1207.1963v1) - [pdf](http://arxiv.org/pdf/1207.1963v1)

> The estimation of small probabilities of failure from computer simulations is a classical problem in engineering, and the Subset Simulation algorithm proposed by Au & Beck (Prob. Eng. Mech., 2001) has become one of the most popular method to solve it. Subset simulation has been shown to provide significant savings in the number of simulations to achieve a given accuracy of estimation, with respect to many other Monte Carlo approaches. The number of simulations remains still quite high however, and this method can be impractical for applications where an expensive-to-evaluate computer model is involved. We propose a new algorithm, called Bayesian Subset Simulation, that takes the best from the Subset Simulation algorithm and from sequential Bayesian methods based on kriging (also known as Gaussian process modeling). The performance of this new algorithm is illustrated using a test case from the literature. We are able to report promising results. In addition, we provide a numerical study of the statistical properties of the estimator.

</details>

<details>

<summary>2012-07-10 17:38:18 - Dual-Space Analysis of the Sparse Linear Model</summary>

- *David Wipf, Yi Wu*

- `1207.2422v1` - [abs](http://arxiv.org/abs/1207.2422v1) - [pdf](http://arxiv.org/pdf/1207.2422v1)

> Sparse linear (or generalized linear) models combine a standard likelihood function with a sparse prior on the unknown coefficients. These priors can conveniently be expressed as a maximization over zero-mean Gaussians with different variance hyperparameters. Standard MAP estimation (Type I) involves maximizing over both the hyperparameters and coefficients, while an empirical Bayesian alternative (Type II) first marginalizes the coefficients and then maximizes over the hyperparameters, leading to a tractable posterior approximation. The underlying cost functions can be related via a dual-space framework from Wipf et al. (2011), which allows both the Type I or Type II objectives to be expressed in either coefficient or hyperparmeter space. This perspective is useful because some analyses or extensions are more conducive to development in one space or the other. Herein we consider the estimation of a trade-off parameter balancing sparsity and data fit. As this parameter is effectively a variance, natural estimators exist by assessing the problem in hyperparameter (variance) space, transitioning natural ideas from Type II to solve what is much less intuitive for Type I. In contrast, for analyses of update rules and sparsity properties of local and global solutions, as well as extensions to more general likelihood models, we can leverage coefficient-space techniques developed for Type I and apply them to Type II. For example, this allows us to prove that Type II-inspired techniques can be successful recovering sparse coefficients when unfavorable restricted isometry properties (RIP) lead to failure of popular L1 reconstructions. It also facilitates the analysis of Type II when non-Gaussian likelihood models lead to intractable integrations.

</details>

<details>

<summary>2012-07-10 18:41:04 - Non-Convex Rank Minimization via an Empirical Bayesian Approach</summary>

- *David Wipf*

- `1207.2440v1` - [abs](http://arxiv.org/abs/1207.2440v1) - [pdf](http://arxiv.org/pdf/1207.2440v1)

> In many applications that require matrix solutions of minimal rank, the underlying cost function is non-convex leading to an intractable, NP-hard optimization problem. Consequently, the convex nuclear norm is frequently used as a surrogate penalty term for matrix rank. The problem is that in many practical scenarios there is no longer any guarantee that we can correctly estimate generative low-rank matrices of interest, theoretical special cases notwithstanding. Consequently, this paper proposes an alternative empirical Bayesian procedure build upon a variational approximation that, unlike the nuclear norm, retains the same globally minimizing point estimate as the rank function under many useful constraints. However, locally minimizing solutions are largely smoothed away via marginalization, allowing the algorithm to succeed when standard convex relaxations completely fail. While the proposed methodology is generally applicable to a wide range of low-rank applications, we focus our attention on the robust principal component analysis problem (RPCA), which involves estimating an unknown low-rank matrix with unknown sparse corruptions. Theoretical and empirical evidence are presented to show that our method is potentially superior to related MAP-based approaches, for which the convex principle component pursuit (PCP) algorithm (Candes et al., 2011) can be viewed as a special case.

</details>

<details>

<summary>2012-07-11 14:42:26 - Algebraic Statistics in Model Selection</summary>

- *Luis David Garcia*

- `1207.4112v1` - [abs](http://arxiv.org/abs/1207.4112v1) - [pdf](http://arxiv.org/pdf/1207.4112v1)

> We develop the necessary theory in computational algebraic geometry to place Bayesian networks into the realm of algebraic statistics. We present an algebra{statistics dictionary focused on statistical modeling. In particular, we link the notion of effiective dimension of a Bayesian network with the notion of algebraic dimension of a variety. We also obtain the independence and non{independence constraints on the distributions over the observable variables implied by a Bayesian network with hidden variables, via a generating set of an ideal of polynomials associated to the network. These results extend previous work on the subject. Finally, the relevance of these results for model selection is discussed.

</details>

<details>

<summary>2012-07-11 14:44:26 - Iterative Conditional Fitting for Gaussian Ancestral Graph Models</summary>

- *Mathias Drton, Thomas S. Richardson*

- `1207.4118v1` - [abs](http://arxiv.org/abs/1207.4118v1) - [pdf](http://arxiv.org/pdf/1207.4118v1)

> Ancestral graph models, introduced by Richardson and Spirtes (2002), generalize both Markov random fields and Bayesian networks to a class of graphs with a global Markov property that is closed under conditioning and marginalization. By design, ancestral graphs encode precisely the conditional independence structures that can arise from Bayesian networks with selection and unobserved (hidden/latent) variables. Thus, ancestral graph models provide a potentially very useful framework for exploratory model selection when unobserved variables might be involved in the data-generating process but no particular hidden structure can be specified. In this paper, we present the Iterative Conditional Fitting (ICF) algorithm for maximum likelihood estimation in Gaussian ancestral graph models. The name reflects that in each step of the procedure a conditional distribution is estimated, subject to constraints, while a marginal distribution is held fixed. This approach is in duality to the well-known Iterative Proportional Fitting algorithm, in which marginal distributions are fitted while conditional distributions are held fixed.

</details>

<details>

<summary>2012-07-11 14:45:54 - Bayesian Biosurveillance of Disease Outbreaks</summary>

- *Gregory F. Cooper, Denver Dash, John Levander, Weng-Keen Wong, William Hogan, Michael Wagner*

- `1207.4122v1` - [abs](http://arxiv.org/abs/1207.4122v1) - [pdf](http://arxiv.org/pdf/1207.4122v1)

> Early, reliable detection of disease outbreaks is a critical problem today. This paper reports an investigation of the use of causal Bayesian networks to model spatio-temporal patterns of a non-contagious disease (respiratory anthrax infection) in a population of people. The number of parameters in such a network can become enormous, if not carefully managed. Also, inference needs to be performed in real time as population data stream in. We describe techniques we have applied to address both the modeling and inference challenges. A key contribution of this paper is the explication of assumptions and techniques that are sufficient to allow the scaling of Bayesian network modeling and inference to millions of nodes for real-time surveillance applications. The results reported here provide a proof-of-concept that Bayesian networks can serve as the foundation of a system that effectively performs Bayesian biosurveillance of disease outbreaks.

</details>

<details>

<summary>2012-07-11 14:51:23 - "Ideal Parent" Structure Learning for Continuous Variable Networks</summary>

- *Iftach Nachman, Gal Elidan, Nir Friedman*

- `1207.4133v1` - [abs](http://arxiv.org/abs/1207.4133v1) - [pdf](http://arxiv.org/pdf/1207.4133v1)

> In recent years, there is a growing interest in learning Bayesian networks with continuous variables. Learning the structure of such networks is a computationally expensive procedure, which limits most applications to parameter learning. This problem is even more acute when learning networks with hidden variables. We present a general method for significantly speeding the structure search algorithm for continuous variable networks with common parametric distributions. Importantly, our method facilitates the addition of new hidden variables into the network structure efficiently. We demonstrate the method on several data sets, both for learning structure on fully observable data, and for introducing new hidden variables during structure search.

</details>

<details>

<summary>2012-07-11 14:51:41 - Bayesian Learning in Undirected Graphical Models: Approximate MCMC algorithms</summary>

- *Iain Murray, Zoubin Ghahramani*

- `1207.4134v1` - [abs](http://arxiv.org/abs/1207.4134v1) - [pdf](http://arxiv.org/pdf/1207.4134v1)

> Bayesian learning in undirected graphical models|computing posterior distributions over parameters and predictive quantities is exceptionally difficult. We conjecture that for general undirected models, there are no tractable MCMC (Markov Chain Monte Carlo) schemes giving the correct equilibrium distribution over parameters. While this intractability, due to the partition function, is familiar to those performing parameter optimisation, Bayesian learning of posterior distributions over undirected model parameters has been unexplored and poses novel challenges. we propose several approximate MCMC schemes and test on fully observed binary models (Boltzmann machines) for a small coronary heart disease data set and larger artificial systems. While approximations must perform well on the model, their interaction with the sampling scheme is also important. Samplers based on variational mean- field approximations generally performed poorly, more advanced methods using loopy propagation, brief sampling and stochastic dynamics lead to acceptable parameter posteriors. Finally, we demonstrate these techniques on a Markov random field with hidden variables.

</details>

<details>

<summary>2012-07-11 14:54:55 - A Generative Bayesian Model for Aggregating Experts' Probabilities</summary>

- *Joseph Kahn*

- `1207.4144v1` - [abs](http://arxiv.org/abs/1207.4144v1) - [pdf](http://arxiv.org/pdf/1207.4144v1)

> In order to improve forecasts, a decisionmaker often combines probabilities given by various sources, such as human experts and machine learning classifiers. When few training data are available, aggregation can be improved by incorporating prior knowledge about the event being forecasted and about salient properties of the experts. To this end, we develop a generative Bayesian aggregation model for probabilistic classi cation. The model includes an event-specific prior, measures of individual experts' bias, calibration, accuracy, and a measure of dependence betweeen experts. Rather than require absolute measures, we show that aggregation may be expressed in terms of relative accuracy between experts. The model results in a weighted logarithmic opinion pool (LogOps) that satis es consistency criteria such as the external Bayesian property. We derive analytic solutions for independent and for exchangeable experts. Empirical tests demonstrate the model's use, comparing its accuracy with other aggregation methods.

</details>

<details>

<summary>2012-07-11 14:55:41 - A Bayesian Approach toward Active Learning for Collaborative Filtering</summary>

- *Rong Jin, Luo Si*

- `1207.4146v1` - [abs](http://arxiv.org/abs/1207.4146v1) - [pdf](http://arxiv.org/pdf/1207.4146v1)

> Collaborative filtering is a useful technique for exploiting the preference patterns of a group of users to predict the utility of items for the active user. In general, the performance of collaborative filtering depends on the number of rated examples given by the active user. The more the number of rated examples given by the active user, the more accurate the predicted ratings will be. Active learning provides an effective way to acquire the most informative rated examples from active users. Previous work on active learning for collaborative filtering only considers the expected loss function based on the estimated model, which can be misleading when the estimated model is inaccurate. This paper takes one step further by taking into account of the posterior distribution of the estimated model, which results in more robust active learning algorithm. Empirical studies with datasets of movie ratings show that when the number of ratings from the active user is restricted to be small, active learning methods only based on the estimated model don't perform well while the active learning method using the model distribution achieves substantially better performance.

</details>

<details>

<summary>2012-07-11 15:01:58 - Convergence and asymptotic normality of variational Bayesian approximations for exponential family models with missing values</summary>

- *Bo Wang, D. Titterington*

- `1207.4159v1` - [abs](http://arxiv.org/abs/1207.4159v1) - [pdf](http://arxiv.org/pdf/1207.4159v1)

> We study the properties of variational Bayes approximations for exponential family models with missing values. It is shown that the iterative algorithm for obtaining the variational Bayesian estimator converges locally to the true value with probability 1 as the sample size becomes inde nitely large. Moreover, the variational posterior distribution is proved to be asymptotically normal.

</details>

<details>

<summary>2012-07-12 19:46:25 - Dependent Dirichlet Priors and Optimal Linear Estimators for Belief Net Parameters</summary>

- *Peter Hooper*

- `1207.4178v1` - [abs](http://arxiv.org/abs/1207.4178v1) - [pdf](http://arxiv.org/pdf/1207.4178v1)

> A Bayesian belief network is a model of a joint distribution over a finite set of variables, with a DAG structure representing immediate dependencies among the variables. For each node, a table of parameters (CPtable) represents local conditional probabilities, with rows indexed by conditioning events (assignments to parents). CP-table rows are usually modeled as independent random vectors, each assigned a Dirichlet prior distribution. The assumption that rows are independent permits a relatively simple analysis but may not reflect actual prior opinion about the parameters. Rows representing similar conditioning events often have similar conditional probabilities. This paper introduces a more flexible family of "dependent Dirichlet" prior distributions, where rows are not necessarily independent. Simple methods are developed to approximate the Bayes estimators of CP-table parameters with optimal linear estimators; i.e., linear combinations of sample proportions and prior means. This approach yields more efficient estimators by sharing information among rows. Improvements in efficiency can be substantial when a CP-table has many rows and samples sizes are small.

</details>

<details>

<summary>2012-07-15 09:37:36 - MMANOVA: A general multilevel framework for multivariate analysis of variance</summary>

- *Steven Geinitz, Reinhard Furrer, Stephan R. Sain*

- `1207.2338v2` - [abs](http://arxiv.org/abs/1207.2338v2) - [pdf](http://arxiv.org/pdf/1207.2338v2)

> Classical analysis of variance requires that model terms be labeled as fixed or random and typically culminate by comparing variability from each batch (factor) to variability from errors; without a standard methodology to assess the magnitude of a batch's variability, to compare variability between batches, nor to consider the uncertainty in this assessment. In this paper we support recent work, placing ANOVA into a general multilevel framework, then refine this through batch level model specifications, and develop it further by extension to the multivariate case. Adopting a Bayesian multilevel model parametrization, with improper batch level prior densities, we derive a method that facilitates comparison across all sources of variability. Whereas classical multivariate ANOVA often utilizes a single covariance criterion, e.g. determinant for Wilks' lambda distribution, the method allows arbitrary covariance criteria to be employed. The proposed method also addresses computation. By introducing implicit batch level constraints, which yield improper priors, the full posterior is efficiently factored, thus alleviating computational demands. For a large class of models, the partitioning mitigates, or even obviates the need for methods such as MCMC. The method is illustrated with simulated examples and an application focusing on climate projections with global climate models.

</details>

<details>

<summary>2012-07-16 18:03:27 - When Do Phylogenetic Mixture Models Mimic Other Phylogenetic Models?</summary>

- *Elizabeth S. Allman, John A. Rhodes, Seth Sullivant*

- `1202.2396v2` - [abs](http://arxiv.org/abs/1202.2396v2) - [pdf](http://arxiv.org/pdf/1202.2396v2)

> Phylogenetic mixture models, in which the sites in sequences undergo different substitution processes along the same or different trees, allow the description of heterogeneous evolutionary processes. As data sets consisting of longer sequences become available, it is important to understand such models, for both theoretical insights and use in statistical analyses. Some recent articles have highlighted disturbing "mimicking" behavior in which a distribution from a mixture model is identical to one arising on a different tree or trees. Other works have indicated such problems are unlikely to occur in practice, as they require very special parameter choices.   After surveying some of these works on mixture models, we give several new results. In general, if the number of components in a generating mixture is not too large and we disallow zero or infinite branch lengths, then it cannot mimic the behavior of a non-mixture on a different tree. On the other hand, if the mixture model is locally over-parameterized, it is possible for a phylogenetic mixture model to mimic distributions of another tree model. Though theoretical questions remain, these sorts of results can serve as a guide to when the use of mixture models in either ML or Bayesian frameworks is likely to lead to statistically consistent inference, and when mimicking due to heterogeneity should be considered a realistic possibility.

</details>

<details>

<summary>2012-07-16 23:40:18 - Structure-Based Bayesian Sparse Reconstruction</summary>

- *Ahmed A. Quadeer, Tareq Y. Al-Naffouri*

- `1207.3847v1` - [abs](http://arxiv.org/abs/1207.3847v1) - [pdf](http://arxiv.org/pdf/1207.3847v1)

> Sparse signal reconstruction algorithms have attracted research attention due to their wide applications in various fields. In this paper, we present a simple Bayesian approach that utilizes the sparsity constraint and a priori statistical information (Gaussian or otherwise) to obtain near optimal estimates. In addition, we make use of the rich structure of the sensing matrix encountered in many signal processing applications to develop a fast sparse recovery algorithm. The computational complexity of the proposed algorithm is relatively low compared with the widely used convex relaxation methods as well as greedy matching pursuit techniques, especially at a low sparsity rate.

</details>

<details>

<summary>2012-07-18 08:47:33 - Expectation-Propagation for Likelihood-Free Inference</summary>

- *Simon Barthelmé, Nicolas Chopin*

- `1107.5959v2` - [abs](http://arxiv.org/abs/1107.5959v2) - [pdf](http://arxiv.org/pdf/1107.5959v2)

> Many models of interest in the natural and social sciences have no closed-form likelihood function, which means that they cannot be treated using the usual techniques of statistical inference. In the case where such models can be efficiently simulated, Bayesian inference is still possible thanks to the Approximate Bayesian Computation (ABC) algorithm. Although many refinements have been suggested, ABC inference is still far from routine. ABC is often excruciatingly slow due to very low acceptance rates. In addition, ABC requires introducing a vector of "summary statistics", the choice of which is relatively arbitrary, and often require some trial and error, making the whole process quite laborious for the user.   We introduce in this work the EP-ABC algorithm, which is an adaptation to the likelihood-free context of the variational approximation algorithm known as Expectation Propagation (Minka, 2001). The main advantage of EP-ABC is that it is faster by a few orders of magnitude than standard algorithms, while producing an overall approximation error which is typically negligible. A second advantage of EP-ABC is that it replaces the usual global ABC constraint on the vector of summary statistics computed on the whole dataset, by n local constraints of the form that apply separately to each data-point. As a consequence, it is often possible to do away with summary statistics entirely. In that case, EP-ABC approximates directly the evidence (marginal likelihood) of the model.   Comparisons are performed in three real-world applications which are typical of likelihood-free inference, including one application in neuroscience which is novel, and possibly too challenging for standard ABC techniques.

</details>

<details>

<summary>2012-07-19 06:56:28 - A Random Weighting Approach for Posterior Distributions</summary>

- *Zai-Ying Zhou*

- `1207.4566v1` - [abs](http://arxiv.org/abs/1207.4566v1) - [pdf](http://arxiv.org/pdf/1207.4566v1)

> In Bayesian theory, calculating a posterior probability distribution is highly important but usually difficult. Therefore, some methods have been put forward to deal with such problem, among which, the most popular one is the asymptotic expansions for posterior distributions. In this paper, we propose an alternative method, named random weighting method, for scaled posterior distributions, and give an ideal convergence speed, which serves as the theoretical guarantee for methods of numerical simulations.

</details>

<details>

<summary>2012-07-22 04:36:12 - Golden Ratio estimate of success probability based on one and only sample</summary>

- *Sun Ping*

- `1207.5198v1` - [abs](http://arxiv.org/abs/1207.5198v1) - [pdf](http://arxiv.org/pdf/1207.5198v1)

> This paper proposes iterative Bayesian method to estimate success probability based on unique sample. The procedure is replacing the distribution characteristic of prior with Bayes estimate on the every iteration until they coincide. Iterative Bayes estimate is generally independent of hyperparameters. For binomial, Poisson, exponential and normal model, iterative limit is shown to be MLE in case the expectation of conjugate prior is replaced respectively. Particularly, suppose success appears in one and only trial, while the mode of triangle prior is replaced iterative Bayesian method gives $1/\phi \approx 0.618$ ($\phi$ is Golden Ratio) as the estimate of success probability $p$, this result reveals the truth of Golden Ratio from the point of statistics. Furthermore, under triangle prior the unique sample $X$ from binomial model $B(n,p)$ is considered. Existence and uniqueness of iterative Bayes estimator $\hat{p}_{IB}$ for parameter $p$ is given.

</details>

<details>

<summary>2012-07-23 08:58:29 - Bayesian nonparametric estimation of the spectral density of a long or intermediate memory Gaussian process</summary>

- *Judith Rousseau, Nicolas Chopin, Brunero Liseo*

- `1007.3823v2` - [abs](http://arxiv.org/abs/1007.3823v2) - [pdf](http://arxiv.org/pdf/1007.3823v2)

> A stationary Gaussian process is said to be long-range dependent (resp., anti-persistent) if its spectral density $f(\lambda)$ can be written as $f(\lambda)=|\lambda|^{-2d}g(|\lambda|)$, where $0<d<1/2$ (resp., $-1/2<d<0$), and $g$ is continuous and positive. We propose a novel Bayesian nonparametric approach for the estimation of the spectral density of such processes. We prove posterior consistency for both $d$ and $g$, under appropriate conditions on the prior distribution. We establish the rate of convergence for a general class of priors and apply our results to the family of fractionally exponential priors. Our approach is based on the true likelihood and does not resort to Whittle's approximation.

</details>

<details>

<summary>2012-07-23 11:13:20 - Estimating the granularity coefficient of a Potts-Markov random field within an MCMC algorithm</summary>

- *Marcelo Pereyra, Nicolas Dobigeon, Hadj Batatia, Jean-Yves Tourneret*

- `1207.5355v1` - [abs](http://arxiv.org/abs/1207.5355v1) - [pdf](http://arxiv.org/pdf/1207.5355v1)

> This paper addresses the problem of estimating the Potts parameter B jointly with the unknown parameters of a Bayesian model within a Markov chain Monte Carlo (MCMC) algorithm. Standard MCMC methods cannot be applied to this problem because performing inference on B requires computing the intractable normalizing constant of the Potts model. In the proposed MCMC method the estimation of B is conducted using a likelihood-free Metropolis-Hastings algorithm. Experimental results obtained for synthetic data show that estimating B jointly with the other unknown parameters leads to estimation results that are as good as those obtained with the actual value of B. On the other hand, assuming that the value of B is known can degrade estimation performance significantly if this value is incorrect. To illustrate the interest of this method, the proposed algorithm is successfully applied to real bidimensional SAR and tridimensional ultrasound images.

</details>

<details>

<summary>2012-07-23 12:12:46 - Minimax and minimax adaptive estimation in multiplicative regression : locally bayesian approach</summary>

- *M. Chichignoud*

- `1006.0024v3` - [abs](http://arxiv.org/abs/1006.0024v3) - [pdf](http://arxiv.org/pdf/1006.0024v3)

> The paper deals with the non-parametric estimation in the regression with the multiplicative noise. Using the local polynomial fitting and the bayesian approach, we construct the minimax on isotropic H\"older class estimator. Next applying Lepski's method, we propose the estimator which is optimally adaptive over the collection of isotropic H\"older classes. To prove the optimality of the proposed procedure we establish, in particular, the exponential inequality for the deviation of locally bayesian estimator from the parameter to be estimated. These theoretical results are illustrated by simulation study.

</details>

<details>

<summary>2012-07-23 13:02:16 - Bayesian empirical likelihood for quantile regression</summary>

- *Yunwen Yang, Xuming He*

- `1207.5378v1` - [abs](http://arxiv.org/abs/1207.5378v1) - [pdf](http://arxiv.org/pdf/1207.5378v1)

> Bayesian inference provides a flexible way of combining data with prior information. However, quantile regression is not equipped with a parametric likelihood, and therefore, Bayesian inference for quantile regression demands careful investigation. This paper considers the Bayesian empirical likelihood approach to quantile regression. Taking the empirical likelihood into a Bayesian framework, we show that the resultant posterior from any fixed prior is asymptotically normal; its mean shrinks toward the true parameter values, and its variance approaches that of the maximum empirical likelihood estimator. A more interesting case can be made for the Bayesian empirical likelihood when informative priors are used to explore commonality across quantiles. Regression quantiles that are computed separately at each percentile level tend to be highly variable in the data sparse areas (e.g., high or low percentile levels). Through empirical likelihood, the proposed method enables us to explore various forms of commonality across quantiles for efficiency gains. By using an MCMC algorithm in the computation, we avoid the daunting task of directly maximizing empirical likelihood. The finite sample performance of the proposed method is investigated empirically, where substantial efficiency gains are demonstrated with informative priors on common features across several percentile levels. A theoretical framework of shrinking priors is used in the paper to better understand the power of the proposed method.

</details>

<details>

<summary>2012-07-23 16:51:10 - Nonlinear spectral unmixing of hyperspectral images using Gaussian processes</summary>

- *Yoann Altmann, Nicolas Dobigeon, Steve McLaughlin, Jean-Yves Tourneret*

- `1207.5451v1` - [abs](http://arxiv.org/abs/1207.5451v1) - [pdf](http://arxiv.org/pdf/1207.5451v1)

> This paper presents an unsupervised algorithm for nonlinear unmixing of hyperspectral images. The proposed model assumes that the pixel reflectances result from a nonlinear function of the abundance vectors associated with the pure spectral components. We assume that the spectral signatures of the pure components and the nonlinear function are unknown. The first step of the proposed method consists of the Bayesian estimation of the abundance vectors for all the image pixels and the nonlinear function relating the abundance vectors to the observations. The endmembers are subsequently estimated using Gaussian process regression. The performance of the unmixing strategy is evaluated with simulations conducted on synthetic and real data.

</details>

<details>

<summary>2012-07-24 10:39:32 - Joint Specification of Model Space and Parameter Space Prior Distributions</summary>

- *Petros Dellaportas, Jonathan J. Forster, Ioannis Ntzoufras*

- `1207.5651v1` - [abs](http://arxiv.org/abs/1207.5651v1) - [pdf](http://arxiv.org/pdf/1207.5651v1)

> We consider the specification of prior distributions for Bayesian model comparison, focusing on regression-type models. We propose a particular joint specification of the prior distribution across models so that sensitivity of posterior model probabilities to the dispersion of prior distributions for the parameters of individual models (Lindley's paradox) is diminished. We illustrate the behavior of inferential and predictive posterior quantities in linear and log-linear regressions under our proposed prior densities with a series of simulated and real data examples.

</details>

<details>

<summary>2012-07-24 18:02:25 - Bayesian inference for Gibbs random fields using composite likelihoods</summary>

- *Nial Friel*

- `1207.5758v1` - [abs](http://arxiv.org/abs/1207.5758v1) - [pdf](http://arxiv.org/pdf/1207.5758v1)

> Gibbs random fields play an important role in statistics, for example the autologistic model is commonly used to model the spatial distribution of binary variables defined on a lattice. However they are complicated to work with due to an intractability of the likelihood function. It is therefore natural to consider tractable approximations to the likelihood function. Composite likelihoods offer a principled approach to constructing such approximation. The contribution of this paper is to examine the performance of a collection of composite likelihood approximations in the context of Bayesian inference.

</details>

<details>

<summary>2012-07-26 10:27:31 - A class of measure-valued Markov chains and Bayesian nonparametrics</summary>

- *Stefano Favaro, Alessandra Guglielmi, Stephen G. Walker*

- `1207.6228v1` - [abs](http://arxiv.org/abs/1207.6228v1) - [pdf](http://arxiv.org/pdf/1207.6228v1)

> Measure-valued Markov chains have raised interest in Bayesian nonparametrics since the seminal paper by (Math. Proc. Cambridge Philos. Soc. 105 (1989) 579--585) where a Markov chain having the law of the Dirichlet process as unique invariant measure has been introduced. In the present paper, we propose and investigate a new class of measure-valued Markov chains defined via exchangeable sequences of random variables. Asymptotic properties for this new class are derived and applications related to Bayesian nonparametric mixture modeling, and to a generalization of the Markov chain proposed by (Math. Proc. Cambridge Philos. Soc. 105 (1989) 579--585), are discussed. These results and their applications highlight once again the interplay between Bayesian nonparametrics and the theory of measure-valued Markov chains.

</details>

<details>

<summary>2012-07-26 16:37:31 - Adaptive Bayesian Denoising for General Gaussian Distributed (GGD) Signals in Wavelet Domain</summary>

- *Masoud Hashemi, Soosan Beheshti*

- `1207.6323v1` - [abs](http://arxiv.org/abs/1207.6323v1) - [pdf](http://arxiv.org/pdf/1207.6323v1)

> Optimum Bayes estimator for General Gaussian Distributed (GGD) data in wavelet is provided. The GGD distribution describes a wide class of signals including natural images. A wavelet thresholding method for image denoising is proposed. Interestingly, we show that the Bayes estimator for this class of signals is well estimated by a thresholding approach. This result analytically confirms the importance of thresholding for noisy GGD signals. We provide the optimum soft thresholding value that mimics the behavior of the Bayes estimator and minimizes the resulting error.   The value of the threshold in BayesShrink, which is one of the most used and efficient soft thresholding methods, has been provided heuristically in the literature. Our proposed method, denoted by Rigorous BayesShrink (R-BayesShrink), explains the theory of BayesShrink threshold and proves its optimality for a subclass of GDD signals. R-BayesShrink improves and generalizes the existing BayesShrink for the class of GGD signals. While the BayesShrink threshold is independent from the wavelet coefficient distribution and is just a function of noise and noiseless signal variance, our method adapts to the distribution of wavelet coefficients of each scale. It is shown that BayesShrink is a special case of our method when shape parameter in GGD is one or signal follows Laplace distribution. Our simulation results confirm the optimality of R-BayesShrink in GGD denoising with regards to Peak Signal to Noise Ratio (PSNR) and Structural Similarity (SSIM) index.

</details>

<details>

<summary>2012-07-26 20:22:29 - A Hierarchical Bayesian Approach for Aerosol Retrieval Using MISR Data</summary>

- *Yueqing Wang, Xin Jiang, Bin Yu, Ming Jiang*

- `1107.3351v2` - [abs](http://arxiv.org/abs/1107.3351v2) - [pdf](http://arxiv.org/pdf/1107.3351v2)

> Atmospheric aerosols can cause serious damage to human health and life expectancy. Using the radiances observed by NASA's Multi-angle Imaging SpectroRadiometer (MISR), the current MISR operational algorithm retrieves Aerosol Optical Depth (AOD) at a spatial resolution of 17.6 km x 17.6 km. A systematic study of aerosols and their impact on public health, especially in highly-populated urban areas, requires a finer-resolution estimate of the spatial distribution of AOD values.   We embed MISR's operational weighted least squares criterion and its forward simulations for AOD retrieval in a likelihood framework and further expand it into a Bayesian hierarchical model to adapt to a finer spatial scale of 4.4 km x 4.4 km. To take advantage of AOD's spatial smoothness, our method borrows strength from data at neighboring pixels by postulating a Gaussian Markov Random Field prior for AOD. Our model considers both AOD and aerosol mixing vectors as continuous variables. The inference of AOD and mixing vectors is carried out using Metropolis-within-Gibbs sampling methods. Retrieval uncertainties are quantified by posterior variabilities. We also implement a parallel MCMC algorithm to reduce computational cost. We assess our retrievals performance using ground-based measurements from the AErosol RObotic NETwork (AERONET), a hand-held sunphotometer and satellite images from Google Earth.   Based on case studies in the greater Beijing area, China, we show that a 4.4 km resolution can improve the accuracy and coverage of remotely-sensed aerosol retrievals, as well as our understanding of the spatial and seasonal behaviors of aerosols. This improvement is particularly important during high-AOD events, which often indicate severe air pollution.

</details>

<details>

<summary>2012-07-30 14:02:53 - PAC-Bayesian Inequalities for Martingales</summary>

- *Yevgeny Seldin, François Laviolette, Nicolò Cesa-Bianchi, John Shawe-Taylor, Peter Auer*

- `1110.6886v3` - [abs](http://arxiv.org/abs/1110.6886v3) - [pdf](http://arxiv.org/pdf/1110.6886v3)

> We present a set of high-probability inequalities that control the concentration of weighted averages of multiple (possibly uncountably many) simultaneously evolving and interdependent martingales. Our results extend the PAC-Bayesian analysis in learning theory from the i.i.d. setting to martingales opening the way for its application to importance weighted sampling, reinforcement learning, and other interactive learning domains, as well as many other domains in probability theory and statistics, where martingales are encountered.   We also present a comparison inequality that bounds the expectation of a convex function of a martingale difference sequence shifted to the [0,1] interval by the expectation of the same function of independent Bernoulli variables. This inequality is applied to derive a tighter analog of Hoeffding-Azuma's inequality.

</details>


## 2012-08

<details>

<summary>2012-08-01 01:38:14 - An Improved Data Assimilation Scheme for High Dimensional Nonlinear Systems</summary>

- *Hatef Monajemi, Peter K. Kitanidis*

- `1208.0065v1` - [abs](http://arxiv.org/abs/1208.0065v1) - [pdf](http://arxiv.org/pdf/1208.0065v1)

> Nonlinear/non-Gaussian filtering has broad applications in many areas of life sciences where either the dynamic is nonlinear and/or the probability density function of uncertain state is non-Gaussian. In such problems, the accuracy of the estimated quantities depends highly upon how accurately their posterior pdf can be approximated. In low dimensional state spaces, methods based on Sequential Importance Sampling (SIS) can suitably approximate the posterior pdf. For higher dimensional problems, however, these techniques are usually inappropriate since the required number of particles to achieve satisfactory estimates grows exponentially with the dimension of state space. On the other hand, ensemble Kalman filter (EnKF) and its variants are more suitable for large-scale problems due to transformation of particles in the Bayesian update step. It has been shown that the latter class of methods may lead to suboptimal solutions for strongly nonlinear problems due to the Gaussian assumption in the update step. In this paper, we introduce a new technique based on the Gaussian sum expansion which captures the non-Gaussian features more accurately while the required computational effort remains within reason for high dimensional problems. We demonstrate the performance of the method for non-Gaussian processes through several examples including the strongly nonlinear Lorenz models. Results show a remarkable improvement in the mean square error compared to EnKF, and a desirable convergence behavior as the number of particles increases.

</details>

<details>

<summary>2012-08-02 19:27:58 - Bayesian Mode Regression</summary>

- *Keming Yu, Katerina Aristodemou*

- `1208.0579v1` - [abs](http://arxiv.org/abs/1208.0579v1) - [pdf](http://arxiv.org/pdf/1208.0579v1)

> Like mean, quantile and variance, mode is also an important measure of central tendency and data summary. Many practical questions often focus on "Which element (gene or file or signal) occurs most often or is the most typical among all elements in a network?". In such cases mode regression provides a convenient summary of how the regressors affect the conditional mode and is totally different from other regression models based on conditional mean or conditional quantile or conditional variance. Some inference methods have been used for mode regression but none of them from the Bayesian perspective. This paper introduces Bayesian mode regression by exploring three different approaches. We start from a parametric Bayesian model by employing a likelihood function that is based on a mode uniform distribution. It is shown that irrespective of the original distribution of the data, the use of this special uniform distribution is a very natural and effective way for Bayesian mode regression. Posterior estimates based on this parametric likelihood, even under misspecification, are consistent and asymptotically normal. We then develop a nonparametric Bayesian model by using Dirichlet process (DP) mixtures of mode uniform distributions and finally we explore Bayesian empirical likelihood mode regression by taking empirical likelihood into a Bayesian framework. The paper also demonstrates that a variety of improper priors for the unknown model parameters yield a proper joint posterior. The proposed approach is illustrated using simulated datasets and a real data set.

</details>

<details>

<summary>2012-08-03 07:42:47 - Evolutionary Inference for Function-valued Traits: Gaussian Process Regression on Phylogenies</summary>

- *Nick S. Jones, John Moriarty*

- `1004.4668v3` - [abs](http://arxiv.org/abs/1004.4668v3) - [pdf](http://arxiv.org/pdf/1004.4668v3)

> Biological data objects often have both of the following features: (i) they are functions rather than single numbers or vectors, and (ii) they are correlated due to phylogenetic relationships. In this paper we give a flexible statistical model for such data, by combining assumptions from phylogenetics with Gaussian processes. We describe its use as a nonparametric Bayesian prior distribution, both for prediction (placing posterior distributions on ancestral functions) and model selection (comparing rates of evolution across a phylogeny, or identifying the most likely phylogenies consistent with the observed data). Our work is integrative, extending the popular phylogenetic Brownian Motion and Ornstein-Uhlenbeck models to functional data and Bayesian inference, and extending Gaussian Process regression to phylogenies. We provide a brief illustration of the application of our method.

</details>

<details>

<summary>2012-08-04 16:36:14 - Massive parallelization of serial inference algorithms for a complex generalized linear model</summary>

- *Marc A. Suchard, Shawn E. Simpson, Ivan Zorych, Patrick Ryan, David Madigan*

- `1208.0945v1` - [abs](http://arxiv.org/abs/1208.0945v1) - [pdf](http://arxiv.org/pdf/1208.0945v1)

> Following a series of high-profile drug safety disasters in recent years, many countries are redoubling their efforts to ensure the safety of licensed medical products. Large-scale observational databases such as claims databases or electronic health record systems are attracting particular attention in this regard, but present significant methodological and computational concerns. In this paper we show how high-performance statistical computation, including graphics processing units, relatively inexpensive highly parallel computing devices, can enable complex methods in large databases. We focus on optimization and massive parallelization of cyclic coordinate descent approaches to fit a conditioned generalized linear model involving tens of millions of observations and thousands of predictors in a Bayesian context. We find orders-of-magnitude improvement in overall run-time. Coordinate descent approaches are ubiquitous in high-dimensional statistics and the algorithms we propose open up exciting new methodological possibilities with the potential to significantly improve drug safety.

</details>

<details>

<summary>2012-08-06 05:42:56 - An Interpretation of the Moore-Penrose Generalized Inverse of a Singular Fisher Information Matrix</summary>

- *Yen-Huan Li, Ping-Cheng Yeh*

- `1107.1944v4` - [abs](http://arxiv.org/abs/1107.1944v4) - [pdf](http://arxiv.org/pdf/1107.1944v4)

> It is proved that in a non-Bayesian parametric estimation problem, if the Fisher information matrix (FIM) is singular, unbiased estimators for the unknown parameter will not exist. Cramer-Rao bound (CRB), a popular tool to lower bound the variances of unbiased estimators, seems inapplicable in such situations. In this paper, we show that the Moore-Penrose generalized inverse of a singular FIM can be interpreted as the CRB corresponding to the minimum variance among all choices of minimum constraint functions. This result ensures the logical validity of applying the Moore-Penrose generalized inverse of an FIM as the covariance lower bound when the FIM is singular. Furthermore, the result can be applied as a performance bound on the joint design of constraint functions and unbiased estimators.

</details>

<details>

<summary>2012-08-06 13:47:02 - Studies in Astronomical Time Series Analysis. VI. Bayesian Block Representations</summary>

- *Jeffrey D. Scargle, Jay P. Norris, Brad Jackson, James Chiang*

- `1207.5578v3` - [abs](http://arxiv.org/abs/1207.5578v3) - [pdf](http://arxiv.org/pdf/1207.5578v3)

> This paper addresses the problem of detecting and characterizing local variability in time series and other forms of sequential data. The goal is to identify and characterize statistically significant variations, at the same time suppressing the inevitable corrupting observational errors. We present a simple nonparametric modeling technique and an algorithm implementing it - an improved and generalized version of Bayesian Blocks (Scargle 1998) - that finds the optimal segmentation of the data in the observation interval. The structure of the algorithm allows it to be used in either a real-time trigger mode, or a retrospective mode. Maximum likelihood or marginal posterior functions to measure model fitness are presented for events, binned counts, and measurements at arbitrary times with known error distributions. Problems addressed include those connected with data gaps, variable exposure, extension to piecewise linear and piecewise exponential representations, multi-variate time series data, analysis of variance, data on the circle, other data modes, and dispersed data. Simulations provide evidence that the detection efficiency for weak signals is close to a theoretical asymptotic limit derived by (Arias-Castro, Donoho and Huo 2003). In the spirit of Reproducible Research (Donoho et al. 2008) all of the code and data necessary to reproduce all of the figures in this paper are included as auxiliary material.

</details>

<details>

<summary>2012-08-08 16:41:23 - A Bayesian Analysis of the Correlations Among Sunspot Cycles</summary>

- *Yaming Yu, David A. van Dyk, Vinay L. Kashyap, C. Alex Young*

- `1208.1706v1` - [abs](http://arxiv.org/abs/1208.1706v1) - [pdf](http://arxiv.org/pdf/1208.1706v1)

> Sunspot numbers form a comprehensive, long-duration proxy of solar activity and have been used numerous times to empirically investigate the properties of the solar cycle. A number of correlations have been discovered over the 24 cycles for which observational records are available. Here we carry out a sophisticated statistical analysis of the sunspot record that reaffirms these correlations, and sets up an empirical predictive framework for future cycles. An advantage of our approach is that it allows for rigorous assessment of both the statistical significance of various cycle features and the uncertainty associated with predictions. We summarize the data into three sequential relations that estimate the amplitude, duration, and time of rise to maximum for any cycle, given the values from the previous cycle. We find that there is no indication of a persistence in predictive power beyond one cycle, and conclude that the dynamo does not retain memory beyond one cycle. Based on sunspot records up to October 2011, we obtain, for Cycle 24, an estimated maximum smoothed monthly sunspot number of 97 +- 15, to occur in January--February 2014 +- 6 months.

</details>

<details>

<summary>2012-08-08 19:40:53 - Prediction of quantiles by statistical learning and application to GDP forecasting</summary>

- *Pierre Alquier, Xiaoyin Li*

- `1202.4294v2` - [abs](http://arxiv.org/abs/1202.4294v2) - [pdf](http://arxiv.org/pdf/1202.4294v2)

> In this paper, we tackle the problem of prediction and confidence intervals for time series using a statistical learning approach and quantile loss functions. In a first time, we show that the Gibbs estimator (also known as Exponentially Weighted aggregate) is able to predict as well as the best predictor in a given family for a wide set of loss functions. In particular, using the quantile loss function of Koenker and Bassett (1978), this allows to build confidence intervals. We apply these results to the problem of prediction and confidence regions for the French Gross Domestic Product (GDP) growth, with promising results.

</details>

<details>

<summary>2012-08-09 00:51:45 - Marginals of multivariate Gibbs distributions with applications in Bayesian species sampling</summary>

- *Annalisa Cerquetti*

- `1207.5548v2` - [abs](http://arxiv.org/abs/1207.5548v2) - [pdf](http://arxiv.org/pdf/1207.5548v2)

> Gibbs partition models are the largest class of infinite exchangeable partitions of the positive integers generalizing the product form of the probability function of the two-parameter Poisson-Dirichlet family. Recently those models have been investigated in a Bayesian nonparametric approach to species sampling problems as alternatives to the Dirichlet and the Pitman-Yor process priors. Here we derive marginals of conditional and unconditional multivariate distributions arising from exchangeable Gibbs partitions to obtain explicit formulas for joint falling factorial moments of corresponding conditional and unconditional Gibbs sampling formulas. Our proofs rely on a known result on factorial moments of sum of non independent indicators. We provide an application to a Bayesian nonparametric estimation of the predictive probability to observe a species already observed a certain number of times.

</details>

<details>

<summary>2012-08-09 14:13:47 - Note on the computation of the Metropolis-Hastings ratio for Birth-or-Death moves in trans-dimensional MCMC algorithms for signal decomposition problems</summary>

- *Alireza Roodaki, Julien Bect, Gilles Fleury*

- `1111.6245v2` - [abs](http://arxiv.org/abs/1111.6245v2) - [pdf](http://arxiv.org/pdf/1111.6245v2)

> Reversible jump MCMC (RJ-MCMC) sampling techniques, which allow to jointly tackle model selection and parameter estimation problems in a coherent Bayesian framework, have become increasingly popular in the signal processing literature since the seminal paper of Andrieu and Doucet (IEEE Trans. Signal Process., 47(10), 1999). Crucial to the implementation of any RJ-MCMC sampler is the computation of the so-called Metropolis-Hastings-Green (MHG) ratio, which determines the acceptance probability for the proposed moves.   It turns out that the expression of the MHG ratio that was given in the paper of Andrieu and Doucet for "Birth-or-Death" moves---the simplest kind of trans-dimensional move, used in virtually all applications of RJ-MCMC to signal decomposition problems---was erroneous. Unfortunately, this mistake has been reproduced in many subsequent papers dealing with RJ-MCMC sampling in the signal processing literature.   This note discusses the computation of the MHG ratio, with a focus on the case where the proposal kernel can be decomposed as a mixture of simpler kernels, for which the MHG ratio is easy to compute. We provide sufficient conditions under which the MHG ratio of the mixture can be deduced from the MHG ratios of the elementary kernels of which it is composed. As an application, we consider the case of Birth-or-Death moves, and provide a corrected expression for the erroneous ratio in the paper of Andrieu and Doucet.

</details>

<details>

<summary>2012-08-09 15:19:26 - Generalized Direct Sampling for Hierarchical Bayesian Models</summary>

- *Michael Braun, Paul Damien*

- `1108.2245v3` - [abs](http://arxiv.org/abs/1108.2245v3) - [pdf](http://arxiv.org/pdf/1108.2245v3)

> We develop a new method to sample from posterior distributions in hierarchical models without using Markov chain Monte Carlo. This method, which is a variant of importance sampling ideas, is generally applicable to high-dimensional models involving large data sets. Samples are independent, so they can be collected in parallel, and we do not need to be concerned with issues like chain convergence and autocorrelation. Additionally, the method can be used to compute marginal likelihoods.

</details>

<details>

<summary>2012-08-13 21:48:56 - Prediction and Computer Model Calibration Using Outputs From Multi-fidelity Simulators</summary>

- *Joslin Goh, Derek Bingham, James Paul Holloway, Michael J. Grosskopf, Carolyn C. Kuranz, Erica Rutter*

- `1208.2716v1` - [abs](http://arxiv.org/abs/1208.2716v1) - [pdf](http://arxiv.org/pdf/1208.2716v1)

> Computer codes are widely used to describe physical processes in lieu of physical observations. In some cases, more than one computer simulator, each with different degrees of fidelity, can be used to explore the physical system. In this work, we combine field observations and model runs from deterministic multi-fidelity computer simulators to build a predictive model for the real process. The resulting model can be used to perform sensitivity analysis for the system, solve inverse problems and make predictions. Our approach is Bayesian and will be illustrated through a simple example, as well as a real application in predictive science at the Center for Radiative Shock Hydrodynamics at the University of Michigan.

</details>

<details>

<summary>2012-08-14 21:25:59 - Bayesian inference on dependence in multivariate longitudinal data</summary>

- *Hongxia Yang, Fan Li, Enrique F. Schisterman, Sunni L. Mumford, David Dunson*

- `1208.2977v1` - [abs](http://arxiv.org/abs/1208.2977v1) - [pdf](http://arxiv.org/pdf/1208.2977v1)

> In many applications, it is of interest to assess the dependence structure in multivariate longitudinal data. Discovering such dependence is challenging due to the dimensionality involved. By concatenating the random effects from component models for each response, dependence within and across longitudinal responses can be characterized through a large random effects covariance matrix. Motivated by the common problems in estimating this matrix, especially the off-diagonal elements, we propose a Bayesian approach that relies on shrinkage priors for parameters in a modified Cholesky decomposition. Without adjustment, such priors and previous related approaches are order-dependent and tend to shrink strongly toward an ARtype structure. We propose moment-matching (MM) priors to mitigate such problems. Efficient Gibbs samplers are developed for posterior computation. The methods are illustrated through simulated examples and are applied to a longitudinal epidemiologic study of hormones and oxidative stress.

</details>

<details>

<summary>2012-08-15 06:03:01 - On the future of astrostatistics: statistical foundations and statistical practice</summary>

- *Thomas J. Loredo*

- `1208.3035v1` - [abs](http://arxiv.org/abs/1208.3035v1) - [pdf](http://arxiv.org/pdf/1208.3035v1)

> This paper summarizes a presentation for a panel discussion on "The Future of Astrostatistics" held at the Statistical Challenges in Modern Astronomy V conference at Pennsylvania State University in June 2011. I argue that the emerging needs of astrostatistics may both motivate and benefit from fundamental developments in statistics. I highlight some recent work within statistics on fundamental topics relevant to astrostatistical practice, including the Bayesian/frequentist debate (and ideas for a synthesis), multilevel models, and multiple testing. As an important direction for future work in statistics, I emphasize that astronomers need a statistical framework that explicitly supports unfolding chains of discovery, with acquisition, cataloging, and modeling of data not seen as isolated tasks, but rather as parts of an ongoing, integrated sequence of analyses, with information and uncertainty propagating forward and backward through the chain. A prototypical example is surveying of astronomical populations, where source detection, demographic modeling, and the design of survey instruments and strategies all interact.

</details>

<details>

<summary>2012-08-17 04:15:40 - Bayesian and L1 Approaches to Sparse Unsupervised Learning</summary>

- *Shakir Mohamed, Katherine Heller, Zoubin Ghahramani*

- `1106.1157v3` - [abs](http://arxiv.org/abs/1106.1157v3) - [pdf](http://arxiv.org/pdf/1106.1157v3)

> The use of L1 regularisation for sparse learning has generated immense research interest, with successful application in such diverse areas as signal acquisition, image coding, genomics and collaborative filtering. While existing work highlights the many advantages of L1 methods, in this paper we find that L1 regularisation often dramatically underperforms in terms of predictive performance when compared with other methods for inferring sparsity. We focus on unsupervised latent variable models, and develop L1 minimising factor models, Bayesian variants of "L1", and Bayesian models with a stronger L0-like sparsity induced through spike-and-slab distributions. These spike-and-slab Bayesian factor models encourage sparsity while accounting for uncertainty in a principled manner and avoiding unnecessary shrinkage of non-zero values. We demonstrate on a number of data sets that in practice spike-and-slab Bayesian methods outperform L1 minimisation, even on a computational budget. We thus highlight the need to re-assess the wide use of L1 methods in sparsity-reliant applications, particularly when we care about generalising to previously unseen data, and provide an alternative that, over many varying conditions, provides improved generalisation performance.

</details>

<details>

<summary>2012-08-17 08:20:31 - The Dependence of Routine Bayesian Model Selection Methods on Irrelevant Alternatives</summary>

- *Piotr Zwiernik, Jim Q. Smith*

- `1208.3553v1` - [abs](http://arxiv.org/abs/1208.3553v1) - [pdf](http://arxiv.org/pdf/1208.3553v1)

> Bayesian methods - either based on Bayes Factors or BIC - are now widely used for model selection. One property that might reasonably be demanded of any model selection method is that if a model ${M}_{1}$ is preferred to a model ${M}_{0}$, when these two models are expressed as members of one model class $\mathbb{M}$, this preference is preserved when they are embedded in a different class $\mathbb{M}'$. However, we illustrate in this paper that with the usual implementation of these common Bayesian procedures this property does not hold true even approximately. We therefore contend that to use these methods it is first necessary for there to exist a "natural" embedding class. We argue that in any context like the one illustrated in our running example of Bayesian model selection of binary phylogenetic trees there is no such embedding.

</details>

<details>

<summary>2012-08-20 14:34:18 - Mixtures of g-Priors for Generalised Additive Model Selection with Penalised Splines</summary>

- *Daniel Sabanés Bové, Leonhard Held, Göran Kauermann*

- `1108.3520v2` - [abs](http://arxiv.org/abs/1108.3520v2) - [pdf](http://arxiv.org/pdf/1108.3520v2)

> We propose an objective Bayesian approach to the selection of covariates and their penalised splines transformations in generalised additive models. Specification of a reasonable default prior for the model parameters and combination with a multiplicity-correction prior for the models themselves is crucial for this task. Here we use well-studied and well-behaved continuous mixtures of g-priors as default priors. We introduce the methodology in the normal model and extend it to non-normal exponential families. A simulation study and an application from the literature illustrate the proposed approach. An efficient implementation is available in the R-package "hypergsplines".

</details>

<details>

<summary>2012-08-21 03:46:08 - Learning LiNGAM based on data with more variables than observations</summary>

- *Shohei Shimizu*

- `1208.4183v1` - [abs](http://arxiv.org/abs/1208.4183v1) - [pdf](http://arxiv.org/pdf/1208.4183v1)

> A very important topic in systems biology is developing statistical methods that automatically find causal relations in gene regulatory networks with no prior knowledge of causal connectivity. Many methods have been developed for time series data. However, discovery methods based on steady-state data are often necessary and preferable since obtaining time series data can be more expensive and/or infeasible for many biological systems. A conventional approach is causal Bayesian networks. However, estimation of Bayesian networks is ill-posed. In many cases it cannot uniquely identify the underlying causal network and only gives a large class of equivalent causal networks that cannot be distinguished between based on the data distribution. We propose a new discovery algorithm for uniquely identifying the underlying causal network of genes. To the best of our knowledge, the proposed method is the first algorithm for learning gene networks based on a fully identifiable causal model called LiNGAM. We here compare our algorithm with competing algorithms using artificially-generated data, although it is definitely better to test it based on real microarray gene expression data.

</details>

<details>

<summary>2012-08-24 00:47:07 - A Concise Resolution to the Two Envelope Paradox</summary>

- *Eric Bliss*

- `1202.4669v3` - [abs](http://arxiv.org/abs/1202.4669v3) - [pdf](http://arxiv.org/pdf/1202.4669v3)

> In this paper, I will demonstrate a new perspective on the Two Envelope Problem. I hope to show with convincing clarity how the paradox results from an inherent problem pertaining to the interpretation of Bayesian probability. Specifically, a subjective probability that is inconsistent with reality can mislead reasoning based on Bayesian decision theory.

</details>

<details>

<summary>2012-08-25 12:15:18 - Posterior Consistency via Precision Operators for Bayesian Nonparametric Drift Estimation in SDEs</summary>

- *Y. Pokern, A. M. Stuart, J. H. van Zanten*

- `1202.0976v3` - [abs](http://arxiv.org/abs/1202.0976v3) - [pdf](http://arxiv.org/pdf/1202.0976v3)

> We study a Bayesian approach to nonparametric estimation of the periodic drift function of a one-dimensional diffusion from continuous-time data. Rewriting the likelihood in terms of local time of the process, and specifying a Gaussian prior with precision operator of differential form, we show that the posterior is also Gaussian with precision operator also of differential form. The resulting expressions are explicit and lead to algorithms which are readily implementable. Using new functional limit theorems for the local time of diffusions on the circle, we bound the rate at which the posterior contracts around the true drift function.

</details>

<details>

<summary>2012-08-26 18:12:19 - Bayesian Network Structure Learning with Permutation Tests</summary>

- *Marco Scutari, Adriana Brogini*

- `1101.5184v3` - [abs](http://arxiv.org/abs/1101.5184v3) - [pdf](http://arxiv.org/pdf/1101.5184v3)

> In literature there are several studies on the performance of Bayesian network structure learning algorithms. The focus of these studies is almost always the heuristics the learning algorithms are based on, i.e. the maximisation algorithms (in score-based algorithms) or the techniques for learning the dependencies of each variable (in constraint-based algorithms). In this paper we investigate how the use of permutation tests instead of parametric ones affects the performance of Bayesian network structure learning from discrete data. Shrinkage tests are also covered to provide a broad overview of the techniques developed in current literature.

</details>

<details>

<summary>2012-08-27 07:35:30 - Comment on "Bayesian astrostatistics: a backward look to the future" by Tom Loredo, arXiv:1208.3036</summary>

- *S. Andreon*

- `1208.5313v1` - [abs](http://arxiv.org/abs/1208.5313v1) - [pdf](http://arxiv.org/pdf/1208.5313v1)

> This short note points out two of the incongruences that I find in the Loredo (2012) comments on Andreon (2012), i.e. on my chapter written for the book "Astrostatistical Challenges for the New Astronomy". First, I find illogic the Loredo decision of putting my chapter among those presenting simple models, because one of the models illustrated in my chapter is qualified by him as "impressing for his complexity". Second, Loredo criticizes my chapter at one location confusing it with another paper by another author, because my chapter do not touch the subject mentioned by Loredo (2012) critics, the comparison between Bayesian and frequentist fitting models.

</details>

<details>

<summary>2012-08-28 08:30:13 - Adaptive bridge regression modeling with model selection criteria</summary>

- *Shuichi Kawano*

- `1204.3130v2` - [abs](http://arxiv.org/abs/1204.3130v2) - [pdf](http://arxiv.org/pdf/1204.3130v2)

> We consider the problem of constructing an adaptive bridge regression modeling, which is a penalized procedure by imposing different weights to different coefficients in the bridge penalty term. A crucial issue in the modeling process is the choices of adjusted parameters included in the models. We treat the selection of the adjusted parameters as model selection and evaluation problems. In order to select the parameters, model selection criteria are derived from information-theoretic and Bayesian approach. We conduct some numerical studies to investigate the effectiveness of our proposed modeling strategy.

</details>

<details>

<summary>2012-08-29 06:36:23 - Practical Bayesian Optimization of Machine Learning Algorithms</summary>

- *Jasper Snoek, Hugo Larochelle, Ryan P. Adams*

- `1206.2944v2` - [abs](http://arxiv.org/abs/1206.2944v2) - [pdf](http://arxiv.org/pdf/1206.2944v2)

> Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.

</details>

<details>

<summary>2012-08-29 16:02:21 - Message passing with relaxed moment matching</summary>

- *Yuan Qi, Yandong Guo*

- `1204.4166v2` - [abs](http://arxiv.org/abs/1204.4166v2) - [pdf](http://arxiv.org/pdf/1204.4166v2)

> Bayesian learning is often hampered by large computational expense. As a powerful generalization of popular belief propagation, expectation propagation (EP) efficiently approximates the exact Bayesian computation. Nevertheless, EP can be sensitive to outliers and suffer from divergence for difficult cases. To address this issue, we propose a new approximate inference approach, relaxed expectation propagation (REP). It relaxes the moment matching requirement of expectation propagation by adding a relaxation factor into the KL minimization. We penalize this relaxation with a $l_1$ penalty. As a result, when two distributions in the relaxed KL divergence are similar, the relaxation factor will be penalized to zero and, therefore, we obtain the original moment matching; In the presence of outliers, these two distributions are significantly different and the relaxation factor will be used to reduce the contribution of the outlier. Based on this penalized KL minimization, REP is robust to outliers and can greatly improve the posterior approximation quality over EP. To examine the effectiveness of REP, we apply it to Gaussian process classification, a task known to be suitable to EP. Our classification results on synthetic and UCI benchmark datasets demonstrate significant improvement of REP over EP and Power EP--in terms of algorithmic stability, estimation accuracy and predictive performance.

</details>

<details>

<summary>2012-08-29 21:43:24 - Bayesian astrostatistics: a backward look to the future</summary>

- *Thomas J. Loredo*

- `1208.3036v2` - [abs](http://arxiv.org/abs/1208.3036v2) - [pdf](http://arxiv.org/pdf/1208.3036v2)

> This perspective chapter briefly surveys: (1) past growth in the use of Bayesian methods in astrophysics; (2) current misconceptions about both frequentist and Bayesian statistical inference that hinder wider adoption of Bayesian methods by astronomers; and (3) multilevel (hierarchical) Bayesian modeling as a major future direction for research in Bayesian astrostatistics, exemplified in part by presentations at the first ISI invited session on astrostatistics, commemorated in this volume. It closes with an intentionally provocative recommendation for astronomical survey data reporting, motivated by the multilevel Bayesian perspective on modeling cosmic populations: that astronomers cease producing catalogs of estimated fluxes and other source properties from surveys. Instead, summaries of likelihood functions (or marginal likelihood functions) for source properties should be reported (not posterior probability density functions), including nontrivial summaries (not simply upper limits) for candidate objects that do not pass traditional detection thresholds.

</details>

<details>

<summary>2012-08-30 06:22:03 - Bayesian Inference with Optimal Maps</summary>

- *Tarek A. El Moselhy, Youssef M. Marzouk*

- `1109.1516v3` - [abs](http://arxiv.org/abs/1109.1516v3) - [pdf](http://arxiv.org/pdf/1109.1516v3)

> We present a new approach to Bayesian inference that entirely avoids Markov chain simulation, by constructing a map that pushes forward the prior measure to the posterior measure. Existence and uniqueness of a suitable measure-preserving map is established by formulating the problem in the context of optimal transport theory. We discuss various means of explicitly parameterizing the map and computing it efficiently through solution of an optimization problem, exploiting gradient information from the forward model when possible. The resulting algorithm overcomes many of the computational bottlenecks associated with Markov chain Monte Carlo. Advantages of a map-based representation of the posterior include analytical expressions for posterior moments and the ability to generate arbitrary numbers of independent posterior samples without additional likelihood evaluations or forward solves. The optimization approach also provides clear convergence criteria for posterior approximation and facilitates model selection through automatic evaluation of the marginal likelihood. We demonstrate the accuracy and efficiency of the approach on nonlinear inverse problems of varying dimension, involving the inference of parameters appearing in ordinary and partial differential equations.

</details>

<details>

<summary>2012-08-30 16:30:40 - Mixture Models for Single Cell Assays with Applications to Vaccine Studies</summary>

- *Greg Finak, Andrew McDavid, Pratip Chattopadhyay, Maria Dominguez, Steve De Rosa, Mario Roederer, Raphael Gottardo*

- `1208.5809v2` - [abs](http://arxiv.org/abs/1208.5809v2) - [pdf](http://arxiv.org/pdf/1208.5809v2)

> In immunological studies, the characterization of small, functionally distinct cell subsets from blood and tissue is crucial to decipher system level biological changes. An increasing number of studies rely on assays that provide single-cell measurements of multiple genes and proteins from bulk cell samples. A common problem in the analysis of such data is to identify biomarkers (or combinations of thereof) that are differentially expressed between two biological conditions (e.g., before/after vaccination), where expression is defined as the proportion of cells expressing the biomarker or combination in the cell subset of interest.   Here, we present a Bayesian hierarchical framework based on a beta-binomial mixture model for testing for differential biomarker expression using single-cell assays. Our model allows inference to be subject specific, as is typically required when accessing vaccine responses, while borrowing strength across subjects through common prior distributions. We propose two approaches for parameter estimation: an empirical-Bayes approach using an Expectation-Maximization algorithm and a fully Bayesian one based on a Markov chain Monte Carlo algorithm. We compare our method against frequentist approaches for single-cell assays including Fisher's exact test, a likelihood ratio test, and basic log-fold changes. Using several experimental assays measuring proteins or genes at the single-cell level and simulated data, we show that our method has higher sensitivity and specificity than alternative methods. Additional simulations show that our framework is also robust to model misspecification. Finally, we also demonstrate how our approach can be extended to testing multivariate differential expression across multiple biomarker combinations using a Dirichlet-multinomial model and illustrate this multivariate approach using single-cell gene expression data and simulations.

</details>

<details>

<summary>2012-08-30 19:36:14 - Local Quantile Regression</summary>

- *Vladimir Spokoiny, Weining Wang, Wolfgang Karl Härdle*

- `1208.5384v2` - [abs](http://arxiv.org/abs/1208.5384v2) - [pdf](http://arxiv.org/pdf/1208.5384v2)

> Quantile regression is a technique to estimate conditional quantile curves. It provides a comprehensive picture of a response contingent on explanatory variables. In a flexible modeling framework, a specific form of the conditional quantile curve is not a priori fixed. % Indeed, the majority of applications do not per se require specific functional forms. This motivates a local parametric rather than a global fixed model fitting approach. A nonparametric smoothing estimator of the conditional quantile curve requires to balance between local curvature and stochastic variability. In this paper, we suggest a local model selection technique that provides an adaptive estimator of the conditional quantile regression curve at each design point. Theoretical results claim that the proposed adaptive procedure performs as good as an oracle which would minimize the local estimation risk for the problem at hand. We illustrate the performance of the procedure by an extensive simulation study and consider a couple of applications: to tail dependence analysis for the Hong Kong stock market and to analysis of the distributions of the risk factors of temperature dynamics.

</details>

<details>

<summary>2012-08-31 00:31:34 - A Widely Applicable Bayesian Information Criterion</summary>

- *Sumio Watanabe*

- `1208.6338v1` - [abs](http://arxiv.org/abs/1208.6338v1) - [pdf](http://arxiv.org/pdf/1208.6338v1)

> A statistical model or a learning machine is called regular if the map taking a parameter to a probability distribution is one-to-one and if its Fisher information matrix is always positive definite. If otherwise, it is called singular. In regular statistical models, the Bayes free energy, which is defined by the minus logarithm of Bayes marginal likelihood, can be asymptotically approximated by the Schwarz Bayes information criterion (BIC), whereas in singular models such approximation does not hold.   Recently, it was proved that the Bayes free energy of a singular model is asymptotically given by a generalized formula using a birational invariant, the real log canonical threshold (RLCT), instead of half the number of parameters in BIC. Theoretical values of RLCTs in several statistical models are now being discovered based on algebraic geometrical methodology. However, it has been difficult to estimate the Bayes free energy using only training samples, because an RLCT depends on an unknown true distribution.   In the present paper, we define a widely applicable Bayesian information criterion (WBIC) by the average log likelihood function over the posterior distribution with the inverse temperature $1/\log n$, where $n$ is the number of training samples. We mathematically prove that WBIC has the same asymptotic expansion as the Bayes free energy, even if a statistical model is singular for and unrealizable by a statistical model. Since WBIC can be numerically calculated without any information about a true distribution, it is a generalized version of BIC onto singular statistical models.

</details>

<details>

<summary>2012-08-31 21:32:30 - Simple techniques for likelihood analysis of univariate and multivariate stable distributions: with extensions to multivariate stochastic volatility and dynamic factor models</summary>

- *Efthymios G. Tsionas*

- `1209.0021v1` - [abs](http://arxiv.org/abs/1209.0021v1) - [pdf](http://arxiv.org/pdf/1209.0021v1)

> In this paper we consider a variety of procedures for numerical statistical inference in the family of univariate and multivariate stable distributions. In connection with univariate distributions (i) we provide approximations by finite location-scale mixtures and (ii) versions of approximate Bayesian computation (ABC) using the characteristic function and the asymptotic form of the likelihood function. In the context of multivariate stable distributions we propose several ways to perform statistical inference and obtain the spectral measure associated with the distributions, a quantity that has been a major impediment in using them in applied work. We extend the techniques to handle univariate and multivariate stochastic volatility models, static and dynamic factor models with disturbances and factors from general stable distributions, a novel way to model multivariate stochastic volatility through time-varying spectral measures and a novel way to multivariate stable distributions through copulae. The new techniques are applied to artificial as well as real data (ten major currencies, SP100 and individual returns). In connection with ABC special attention is paid to crafting well-performing proposal distributions for MCMC and extensive numerical experiments are conducted to provide critical values of the "closeness" parameter that can be useful for further applied econometric work.

</details>


## 2012-09

<details>

<summary>2012-09-03 13:39:44 - Dirichlet Posterior Sampling with Truncated Multinomial Likelihoods</summary>

- *Matthew James Johnson, Alan S. Willsky*

- `1208.6537v2` - [abs](http://arxiv.org/abs/1208.6537v2) - [pdf](http://arxiv.org/pdf/1208.6537v2)

> We consider the problem of drawing samples from posterior distributions formed under a Dirichlet prior and a truncated multinomial likelihood, by which we mean a Multinomial likelihood function where we condition on one or more counts being zero a priori. Sampling this posterior distribution is of interest in inference algorithms for hierarchical Bayesian models based on the Dirichlet distribution or the Dirichlet process, particularly Gibbs sampling algorithms for the Hierarchical Dirichlet Process Hidden Semi-Markov Model. We provide a data augmentation sampling algorithm that is easy to implement, fast both to mix and to execute, and easily scalable to many dimensions. We demonstrate the algorithm's advantages over a generic Metropolis-Hastings sampling algorithm in several numerical experiments.

</details>

<details>

<summary>2012-09-04 00:30:42 - Bayesian inference for nonlinear structural time series models</summary>

- *Jamie Hall, Michael K. Pitt, Robert Kohn*

- `1209.0253v2` - [abs](http://arxiv.org/abs/1209.0253v2) - [pdf](http://arxiv.org/pdf/1209.0253v2)

> This article discusses a partially adapted particle filter for estimating the likelihood of a nonlinear structural econometric state space models whose state transition density cannot be expressed in closed form. The filter generates the disturbances in the state transition equation and allows for multiple modes in the conditional disturbance distribution. The particle filter produces an unbiased estimate of the likelihood and so can be used to carry out Bayesian inference in a particle Markov chain Monte Carlo framework. We show empirically that when the signal to noise ratio is high, the new filter can be much more efficient than the standard particle filter, in the sense that it requires far fewer particles to give the same accuracy. The new filter is applied to several simulated and real examples and in particular to a dynamic stochastic general equilibrium model.

</details>

<details>

<summary>2012-09-04 06:59:33 - Enhancing hyperspectral image unmixing with spatial correlations</summary>

- *Olivier Eches, Nicolas Dobigeon, Jean-Yves Tourneret*

- `1002.1059v4` - [abs](http://arxiv.org/abs/1002.1059v4) - [pdf](http://arxiv.org/pdf/1002.1059v4)

> This paper describes a new algorithm for hyperspectral image unmixing. Most of the unmixing algorithms proposed in the literature do not take into account the possible spatial correlations between the pixels. In this work, a Bayesian model is introduced to exploit these correlations. The image to be unmixed is assumed to be partitioned into regions (or classes) where the statistical properties of the abundance coefficients are homogeneous. A Markov random field is then proposed to model the spatial dependency of the pixels within any class. Conditionally upon a given class, each pixel is modeled by using the classical linear mixing model with additive white Gaussian noise. This strategy is investigated the well known linear mixing model. For this model, the posterior distributions of the unknown parameters and hyperparameters allow ones to infer the parameters of interest. These parameters include the abundances for each pixel, the means and variances of the abundances for each class, as well as a classification map indicating the classes of all pixels in the image. To overcome the complexity of the posterior distribution of interest, we consider Markov chain Monte Carlo methods that generate samples distributed according to the posterior of interest. The generated samples are then used for parameter and hyperparameter estimation. The accuracy of the proposed algorithms is illustrated on synthetic and real data.

</details>

<details>

<summary>2012-09-04 08:40:42 - The enrichment history of the intracluster medium: a Bayesian approach</summary>

- *S. Andreon*

- `1209.0565v1` - [abs](http://arxiv.org/abs/1209.0565v1) - [pdf](http://arxiv.org/pdf/1209.0565v1)

> This work measures the evolution of the iron content in galaxy clusters by a rigorous analysis of the data of 130 clusters at 0.1<z<1.3. This task is made difficult by a) the low signal-to-noise ratio of abundance measurements and the upper limits, b) possible selection effects, c) boundaries in the parameter space, d) non-Gaussian errors, e) the intrinsic variety of the objects studied, and f) abundance systematics. We introduce a Bayesian model to address all these issues at the same time, thus allowing cross-talk (covariance). On simulated data, the Bayesian fit recovers the input enrichment history, unlike in standard analysis. After accounting for a possible dependence on X-ray temperature, for metal abundance systematics, and for the intrinsic variety of studied objects, we found that the present-day metal content is not reached either at high or at low redshifts, but gradually over time: iron abundance increases by a factor 1.5 in the 7 Gyr sampled by the data. Therefore, feedback in metal abundance does not end at high redshift. Evolution is established with a moderate amount of evidence, 19 to 1 odds against faster or slower metal enrichment histories. We quantify, for the first time, the intrinsic spread in metal abundance, 18+/-3 %, after correcting for the effect of evolution, X-ray temperature, and metal abundance systematics. Finally, we also present an analytic approximation of the X-ray temperature and metal abundance likelihood functions, which are useful for other regression fitting involving these parameters. The data for the 130 clusters and code used for the stochastic computation are provided with the paper.

</details>

<details>

<summary>2012-09-04 14:53:39 - Bayesian variable selection for spatially dependent generalized linear models</summary>

- *Kristian Lum*

- `1209.0661v1` - [abs](http://arxiv.org/abs/1209.0661v1) - [pdf](http://arxiv.org/pdf/1209.0661v1)

> Despite the abundance of methods for variable selection and accommodating spatial structure in regression models, there is little precedent for incorporating spatial dependence in covariate inclusion probabilities for regionally varying regression models. The lone existing approach is limited by difficult computation and the requirement that the spatial dependence be represented on a lattice, making this method inappropriate for areal models with irregular structures that often arise in ecology, epidemiology, and the social sciences. Here we present a novel method for spatial variable selection in areal generalized linear models that can accommodate arbitrary spatial structures and works with a broad subset of GLM likelihoods. The method uses a latent probit model with a spatial dependence structure where the binary response is taken as a covariate inclusion indicator for area-specific GLMs. The covariate inclusion indicators arise via thresholding of latent standard normals on which we place a conditionally autoregressive prior. We propose an efficient MCMC algorithm for computation that is entirely conjugate in any model with a conditionally Gaussian representation of the likelihood, thereby encompassing logistic, probit, multinomial probit and logit, Gaussian, and negative binomial regressions through the use of existing data augmentation methods. We demonstrate superior parameter recovery and prediction in simulation studies as well as in applications to geographic voting patterns and population estimation. Though the method is very broadly applicable, we note in particular that prior to this work, spatial population estimation/capture-recapture models allowing for varying list dependence structures has not been possible.

</details>

<details>

<summary>2012-09-04 17:50:18 - Isoelastic Agents and Wealth Updates in Machine Learning Markets</summary>

- *Amos Storkey, Jono Millin, Krzysztof Geras*

- `1206.6443v2` - [abs](http://arxiv.org/abs/1206.6443v2) - [pdf](http://arxiv.org/pdf/1206.6443v2)

> Recently, prediction markets have shown considerable promise for developing flexible mechanisms for machine learning. In this paper, agents with isoelastic utilities are considered. It is shown that the costs associated with homogeneous markets of agents with isoelastic utilities produce equilibrium prices corresponding to alpha-mixtures, with a particular form of mixing component relating to each agent's wealth. We also demonstrate that wealth accumulation for logarithmic and other isoelastic agents (through payoffs on prediction of training targets) can implement both Bayesian model updates and mixture weight updates by imposing different market payoff structures. An iterative algorithm is given for market equilibrium computation. We demonstrate that inhomogeneous markets of agents with isoelastic utilities outperform state of the art aggregate classifiers such as random forests, as well as single classifiers (neural networks, decision trees) on a number of machine learning benchmarks, and show that isoelastic combination methods are generally better than their logarithmic counterparts.

</details>

<details>

<summary>2012-09-05 09:23:29 - Spatial two tissue compartment model for DCE-MRI</summary>

- *Julia C. Sommer, Volker J. Schmid*

- `1209.0901v1` - [abs](http://arxiv.org/abs/1209.0901v1) - [pdf](http://arxiv.org/pdf/1209.0901v1)

> In the quantitative analysis of Dynamic Contrast-Enhanced Magnetic Resonance Imaging (DCE-MRI) compartment models allow to describe the uptake of contrast medium with biological meaningful kinetic parameters. As simple models often fail to adequately describe the observed uptake behavior, more complex compartment models have been proposed. However, the nonlinear regression problem arising from more complex compartment models often suffers from parameter redundancy. In this paper, we incorporate spatial smoothness on the kinetic parameters of a two tissue compartment model by imposing Gaussian Markov random field priors on them. We analyse to what extent this spatial regularisation helps to avoid parameter redundancy and to obtain stable parameter estimates. Choosing a full Bayesian approach, we obtain posteriors and point estimates running Markov Chain Monte Carlo simulations. The proposed approach is evaluated for simulated concentration time curves as well as for in vivo data from a breast cancer study.

</details>

<details>

<summary>2012-09-06 02:19:59 - Asymptotics for penalized spline estimators in quantile regression</summary>

- *Takuma Yoshida*

- `1209.1156v1` - [abs](http://arxiv.org/abs/1209.1156v1) - [pdf](http://arxiv.org/pdf/1209.1156v1)

> Quantile regression predicts the $\tau$-quantile of the conditional distribution of a response variable given the explanatory variable for $\tau\in(0,1)$. The aim of this paper is to establish the asymptotic distribution of the quantile estimator obtained by penalized spline method. A simulation and an exploration of real data are performed to validate our results.

</details>

<details>

<summary>2012-09-07 22:21:25 - Bayesian Nonparametric Hidden Semi-Markov Models</summary>

- *Matthew J. Johnson, Alan S. Willsky*

- `1203.1365v2` - [abs](http://arxiv.org/abs/1203.1365v2) - [pdf](http://arxiv.org/pdf/1203.1365v2)

> There is much interest in the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) as a natural Bayesian nonparametric extension of the ubiquitous Hidden Markov Model for learning from sequential and time-series data. However, in many settings the HDP-HMM's strict Markovian constraints are undesirable, particularly if we wish to learn or encode non-geometric state durations. We can extend the HDP-HMM to capture such structure by drawing upon explicit-duration semi-Markovianity, which has been developed mainly in the parametric frequentist setting, to allow construction of highly interpretable models that admit natural prior information on state durations.   In this paper we introduce the explicit-duration Hierarchical Dirichlet Process Hidden semi-Markov Model (HDP-HSMM) and develop sampling algorithms for efficient posterior inference. The methods we introduce also provide new methods for sampling inference in the finite Bayesian HSMM. Our modular Gibbs sampling methods can be embedded in samplers for larger hierarchical Bayesian models, adding semi-Markov chain modeling as another tool in the Bayesian inference toolbox. We demonstrate the utility of the HDP-HSMM and our inference methods on both synthetic and real experiments.

</details>

<details>

<summary>2012-09-08 11:31:49 - Default Bayesian Analysis for the Multivariate Ewens Distribution</summary>

- *Abel Rodriguez*

- `1209.1706v1` - [abs](http://arxiv.org/abs/1209.1706v1) - [pdf](http://arxiv.org/pdf/1209.1706v1)

> We derive the Jeffreys prior for the parameter of the Multivariate Ewens Distribution and study some of its properties. In particular, we show that this prior is proper and has no finite moments. We also investigate the impact of this default prior on the a priori distribution of the number of species and the a priori probability of discovery of a new species, which are usually employed in subjective prior elicitation. The effect of the Jeffreys prior for posterior inference is illustrated using examples arising in the context of inference for species sampling models and Dirichlet process mixture models.

</details>

<details>

<summary>2012-09-09 00:55:09 - Conditioned Likelihoods Using Bifurcation Continuation in Inverse Modeling of Dynamical Systems</summary>

- *Karleigh Cameron, Marissa Saladin*

- `1209.1764v1` - [abs](http://arxiv.org/abs/1209.1764v1) - [pdf](http://arxiv.org/pdf/1209.1764v1)

> The Morris-Lecar (ML) model has applications to neuroscience and cognition. A simple network consisting of a pair of synaptically coupled ML neurons can exhibit a wide variety of deterministic behaviors including asymmetric amplitude state (AAS), equal amplitude state (EAS), and steady state (SS). In addition, in the presence of noise this network can exhibit mixed-mode oscillations (MMO), which represent the system being stochastically driven between these behaviors. In this paper, we develop a method to specifically estimate the parameters representing the coupling strength (gsyn) and the applied current (Iapp) of two reciprocally coupled and biologically similar neurons. This method employs conditioning the likelihood on cumulative power and mean voltage. Conditioning has the potential to improve the identifiability of the estimation problem. Conditioning likelihoods are typically much simpler to model than the explicit joint distribution, which several studies have shown to be difficult or impossible to determine analytically. We adopt a rejection sampling procedure over a closed defined region determined by bifurcation continuation analyses. This rejection sampling procedure is easily embedded within the proposal distribution of a Bayesian Markov chain Monte Carlo (MCMC) scheme and we evaluate its performance. This is the first report of a Bayesian parameter estimation for two reciprocally coupled Morris-Lecar neurons, and we find a proposal utilizing rejection sampling reduces parameter estimate bias relative to naive sampling. Application to stochastically coupled ML neurons is a future goal.

</details>

<details>

<summary>2012-09-10 13:57:37 - A Bayesian Boosting Model</summary>

- *Alexander Lorbert, David M. Blei, Robert E. Schapire, Peter J. Ramadge*

- `1209.1996v1` - [abs](http://arxiv.org/abs/1209.1996v1) - [pdf](http://arxiv.org/pdf/1209.1996v1)

> We offer a novel view of AdaBoost in a statistical setting. We propose a Bayesian model for binary classification in which label noise is modeled hierarchically. Using variational inference to optimize a dynamic evidence lower bound, we derive a new boosting-like algorithm called VIBoost. We show its close connections to AdaBoost and give experimental results from four datasets.

</details>

<details>

<summary>2012-09-10 14:46:56 - Bayesian Adaptive Smoothing Spline using Stochastic Differential Equations</summary>

- *Yu Ryan Yue, Daniel Simpson, Finn Lindgren, Håvard Rue*

- `1209.2013v1` - [abs](http://arxiv.org/abs/1209.2013v1) - [pdf](http://arxiv.org/pdf/1209.2013v1)

> The smoothing spline is one of the most popular curve-fitting methods, partly because of empirical evidence supporting its effectiveness and partly because of its elegant mathematical formulation. However, there are two obstacles that restrict the use of smoothing spline in practical statistical work. Firstly, it becomes computationally prohibitive for large data sets because the number of basis functions roughly equals the sample size. Secondly, its global smoothing parameter can only provide constant amount of smoothing, which often results in poor performances when estimating inhomogeneous functions. In this work, we introduce a class of adaptive smoothing spline models that is derived by solving certain stochastic differential equations with finite element methods. The solution extends the smoothing parameter to a continuous data-driven function, which is able to capture the change of the smoothness of underlying process. The new model is Markovian, which makes Bayesian computation fast. A simulation study and real data example are presented to demonstrate the effectiveness of our method.

</details>

<details>

<summary>2012-09-10 18:19:22 - Distance Dependent Infinite Latent Feature Models</summary>

- *Samuel J. Gershman, Peter I. Frazier, David M. Blei*

- `1110.5454v2` - [abs](http://arxiv.org/abs/1110.5454v2) - [pdf](http://arxiv.org/pdf/1110.5454v2)

> Latent feature models are widely used to decompose data into a small number of components. Bayesian nonparametric variants of these models, which use the Indian buffet process (IBP) as a prior over latent features, allow the number of features to be determined from the data. We present a generalization of the IBP, the distance dependent Indian buffet process (dd-IBP), for modeling non-exchangeable data. It relies on distances defined between data points, biasing nearby data to share more features. The choice of distance measure allows for many kinds of dependencies, including temporal and spatial. Further, the original IBP is a special case of the dd-IBP. In this paper, we develop the dd-IBP and theoretically characterize its feature-sharing properties. We derive a Markov chain Monte Carlo sampler for a linear Gaussian model with a dd-IBP prior and study its performance on several non-exchangeable data sets.

</details>

<details>

<summary>2012-09-13 20:27:09 - Robust identification of local adaptation from allele frequencies</summary>

- *Torsten Günther, Graham Coop*

- `1209.3029v1` - [abs](http://arxiv.org/abs/1209.3029v1) - [pdf](http://arxiv.org/pdf/1209.3029v1)

> Comparing allele frequencies among populations that differ in environment has long been a tool for detecting loci involved in local adaptation. However, such analyses are complicated by an imperfect knowledge of population allele frequencies and neutral correlations of allele frequencies among populations due to shared population history and gene flow. Here we develop a set of methods to robustly test for unusual allele frequency patterns, and correlations between environmental variables and allele frequencies while accounting for these complications based on a Bayesian model previously implemented in the software Bayenv. Using this model, we calculate a set of `standardized allele frequencies' that allows investigators to apply tests of their choice to multiple populations, while accounting for sampling and covariance due to population history. We illustrate this first by showing that these standardized frequencies can be used to calculate powerful tests to detect non-parametric correlations with environmental variables, which are also less prone to spurious results due to outlier populations. We then demonstrate how these standardized allele frequencies can be used to construct a test to detect SNPs that deviate strongly from neutral population structure. This test is conceptually related to FST but should be more powerful as we account for population history. We also extend the model to next-generation sequencing of population pools, which is a cost-efficient way to estimate population allele frequencies, but it implies an additional level of sampling noise. The utility of these methods is demonstrated in simulations and by re-analyzing human SNP data from the HGDP populations. An implementation of our method will be available from http://gcbias.org.

</details>

<details>

<summary>2012-09-15 03:41:18 - Further Optimal Regret Bounds for Thompson Sampling</summary>

- *Shipra Agrawal, Navin Goyal*

- `1209.3353v1` - [abs](http://arxiv.org/abs/1209.3353v1) - [pdf](http://arxiv.org/pdf/1209.3353v1)

> Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. In this paper, we provide a novel regret analysis for Thompson Sampling that simultaneously proves both the optimal problem-dependent bound of $(1+\epsilon)\sum_i \frac{\ln T}{\Delta_i}+O(\frac{N}{\epsilon^2})$ and the first near-optimal problem-independent bound of $O(\sqrt{NT\ln T})$ on the expected regret of this algorithm. Our near-optimal problem-independent bound solves a COLT 2012 open problem of Chapelle and Li. The optimal problem-dependent regret bound for this problem was first proven recently by Kaufmann et al. [ALT 2012]. Our novel martingale-based analysis techniques are conceptually simple, easily extend to distributions other than the Beta distribution, and also extend to the more general contextual bandits setting [Manuscript, Agrawal and Goyal, 2012].

</details>

<details>

<summary>2012-09-17 00:06:06 - Variable Selection with Exponential Weights and $l_0$-Penalization</summary>

- *Ery Arias-Castro, Karim Lounici*

- `1208.2635v2` - [abs](http://arxiv.org/abs/1208.2635v2) - [pdf](http://arxiv.org/pdf/1208.2635v2)

> In the context of a linear model with a sparse coefficient vector, exponential weights methods have been shown to be achieve oracle inequalities for prediction. We show that such methods also succeed at variable selection and estimation under the necessary identifiability condition on the design matrix, instead of much stronger assumptions required by other methods such as the Lasso or the Dantzig Selector. The same analysis yields consistency results for Bayesian methods and BIC-type variable selection under similar conditions.

</details>

<details>

<summary>2012-09-17 11:46:47 - Off-grid Direction of Arrival Estimation Using Sparse Bayesian Inference</summary>

- *Zai Yang, Lihua Xie, Cishen Zhang*

- `1108.5838v4` - [abs](http://arxiv.org/abs/1108.5838v4) - [pdf](http://arxiv.org/pdf/1108.5838v4)

> Direction of arrival (DOA) estimation is a classical problem in signal processing with many practical applications. Its research has recently been advanced owing to the development of methods based on sparse signal reconstruction. While these methods have shown advantages over conventional ones, there are still difficulties in practical situations where true DOAs are not on the discretized sampling grid. To deal with such an off-grid DOA estimation problem, this paper studies an off-grid model that takes into account effects of the off-grid DOAs and has a smaller modeling error. An iterative algorithm is developed based on the off-grid model from a Bayesian perspective while joint sparsity among different snapshots is exploited by assuming a Laplace prior for signals at all snapshots. The new approach applies to both single snapshot and multi-snapshot cases. Numerical simulations show that the proposed algorithm has improved accuracy in terms of mean squared estimation error. The algorithm can maintain high estimation accuracy even under a very coarse sampling grid.

</details>

<details>

<summary>2012-09-20 02:52:08 - Repulsive Mixtures</summary>

- *Francesca Petralia, Vinayak Rao, David B. Dunson*

- `1204.5243v2` - [abs](http://arxiv.org/abs/1204.5243v2) - [pdf](http://arxiv.org/pdf/1204.5243v2)

> Discrete mixture models are routinely used for density estimation and clustering. While conducting inferences on the cluster-specific parameters, current frequentist and Bayesian methods often encounter problems when clusters are placed too close together to be scientifically meaningful. Current Bayesian practice generates component-specific parameters independently from a common prior, which tends to favor similar components and often leads to substantial probability assigned to redundant components that are not needed to fit the data. As an alternative, we propose to generate components from a repulsive process, which leads to fewer, better separated and more interpretable clusters. We characterize this repulsive prior theoretically and propose a Markov chain Monte Carlo sampling algorithm for posterior computation. The methods are illustrated using simulated data as well as real datasets.

</details>

<details>

<summary>2012-09-20 21:31:20 - Small-Sample Behavior of Novel Phase I Cancer Trial Designs</summary>

- *Assaf P. Oron, Peter D. Hoff*

- `1202.4962v2` - [abs](http://arxiv.org/abs/1202.4962v2) - [pdf](http://arxiv.org/pdf/1202.4962v2)

> Novel dose-finding designs, using estimation to assign the best estimated maximum- tolerated-dose (MTD) at each point in the experiment, most commonly via Bayesian techniques, have recently entered large-scale implementation in Phase I cancer clinical trials. We examine the small-sample behavior of these "Bayesian Phase I" (BP1) designs, and also of non-Bayesian designs sharing the same main "long-memory" traits (hereafter: LMP1s).   For all LMP1s examined, the number of cohorts treated at the true MTD (denoted here as n*) was highly variable between numerical runs drawn from the same toxicity-threshold distribution, especially when compared with "up-and-down" (U&D) short-memory designs. Further investigation using the same set of thresholds in permuted order, produced a nearly-identical magnitude of variability in n*. Therefore, this LMP1 behavior is driven by a strong sensitivity to the order in which toxicity thresholds appear in the experiment. We suggest that the sensitivity is related to LMP1's tendency to "settle" early on a specific dose level - a tendency caused by the repeated likelihood-based "winner-takes-all" dose assignment rule, which grants the early cohorts a disproportionately large influence upon experimental trajectories.   Presently, U&D designs offer a simpler and more stable alternative, with roughly equivalent MTD estimation performance. A promising direction for combining the two approaches is briefly discussed (note: the '3+3' protocol is not a U&D design).

</details>

<details>

<summary>2012-09-21 15:35:06 - Scaling Multidimensional Inference for Structured Gaussian Processes</summary>

- *Elad Gilboa, Yunus Saatçi, John P. Cunningham*

- `1209.4120v2` - [abs](http://arxiv.org/abs/1209.4120v2) - [pdf](http://arxiv.org/pdf/1209.4120v2)

> Exact Gaussian Process (GP) regression has O(N^3) runtime for data size N, making it intractable for large N. Many algorithms for improving GP scaling approximate the covariance with lower rank matrices. Other work has exploited structure inherent in particular covariance functions, including GPs with implied Markov structure, and equispaced inputs (both enable O(N) runtime). However, these GP advances have not been extended to the multidimensional input setting, despite the preponderance of multidimensional applications. This paper introduces and tests novel extensions of structured GPs to multidimensional inputs. We present new methods for additive GPs, showing a novel connection between the classic backfitting method and the Bayesian framework. To achieve optimal accuracy-complexity tradeoff, we extend this model with a novel variant of projection pursuit regression. Our primary result -- projection pursuit Gaussian Process Regression -- shows orders of magnitude speedup while preserving high accuracy. The natural second and third steps include non-Gaussian observations and higher dimensional equispaced grid methods. We introduce novel techniques to address both of these necessary directions. We thoroughly illustrate the power of these three advances on several datasets, achieving close performance to the naive Full GP at orders of magnitude less cost.

</details>

<details>

<summary>2012-09-22 21:01:06 - A Bayesian Nonparametric Approach to Image Super-resolution</summary>

- *Gungor Polatkan, Mingyuan Zhou, Lawrence Carin, David Blei, Ingrid Daubechies*

- `1209.5019v1` - [abs](http://arxiv.org/abs/1209.5019v1) - [pdf](http://arxiv.org/pdf/1209.5019v1)

> Super-resolution methods form high-resolution images from low-resolution images. In this paper, we develop a new Bayesian nonparametric model for super-resolution. Our method uses a beta-Bernoulli process to learn a set of recurring visual patterns, called dictionary elements, from the data. Because it is nonparametric, the number of elements found is also determined from the data. We test the results on both benchmark and natural images, comparing with several other models from the research literature. We perform large-scale human evaluation experiments to assess the visual quality of the results. In a first implementation, we use Gibbs sampling to approximate the posterior. However, this algorithm is not feasible for large-scale data. To circumvent this, we then develop an online variational Bayes (VB) algorithm. This algorithm finds high quality dictionaries in a fraction of the time needed by the Gibbs sampler.

</details>

<details>

<summary>2012-09-22 21:19:23 - Data augmentation for non-Gaussian regression models using variance-mean mixtures</summary>

- *Nicholas G. Polson, James G. Scott*

- `1103.5407v4` - [abs](http://arxiv.org/abs/1103.5407v4) - [pdf](http://arxiv.org/pdf/1103.5407v4)

> We use the theory of normal variance-mean mixtures to derive a data-augmentation scheme for a class of common regularization problems. This generalizes existing theory on normal variance mixtures for priors in regression and classification. It also allows variants of the expectation-maximization algorithm to be brought to bear on a wider range of models than previously appreciated. We demonstrate the method on several examples, including sparse quantile regression and binary logistic regression. We also show that quasi-Newton acceleration can substantially improve the speed of the algorithm without compromising its robustness.

</details>

<details>

<summary>2012-09-24 12:07:52 - Criteria for Bayesian model choice with application to variable selection</summary>

- *M. J. Bayarri, J. O. Berger, A. Forte, G. García-Donato*

- `1209.5240v1` - [abs](http://arxiv.org/abs/1209.5240v1) - [pdf](http://arxiv.org/pdf/1209.5240v1)

> In objective Bayesian model selection, no single criterion has emerged as dominant in defining objective prior distributions. Indeed, many criteria have been separately proposed and utilized to propose differing prior choices. We first formalize the most general and compelling of the various criteria that have been suggested, together with a new criterion. We then illustrate the potential of these criteria in determining objective model selection priors by considering their application to the problem of variable selection in normal linear models. This results in a new model selection objective prior with a number of compelling properties.

</details>

<details>

<summary>2012-09-24 13:59:01 - Fast Markov chain Monte Carlo sampling for sparse Bayesian inference in high-dimensional inverse problems using L1-type priors</summary>

- *Felix Lucka*

- `1206.0262v2` - [abs](http://arxiv.org/abs/1206.0262v2) - [pdf](http://arxiv.org/pdf/1206.0262v2)

> Sparsity has become a key concept for solving of high-dimensional inverse problems using variational regularization techniques. Recently, using similar sparsity-constraints in the Bayesian framework for inverse problems by encoding them in the prior distribution has attracted attention. Important questions about the relation between regularization theory and Bayesian inference still need to be addressed when using sparsity promoting inversion. A practical obstacle for these examinations is the lack of fast posterior sampling algorithms for sparse, high-dimensional Bayesian inversion: Accessing the full range of Bayesian inference methods requires being able to draw samples from the posterior probability distribution in a fast and efficient way. This is usually done using Markov chain Monte Carlo (MCMC) sampling algorithms. In this article, we develop and examine a new implementation of a single component Gibbs MCMC sampler for sparse priors relying on L1-norms. We demonstrate that the efficiency of our Gibbs sampler increases when the level of sparsity or the dimension of the unknowns is increased. This property is contrary to the properties of the most commonly applied Metropolis-Hastings (MH) sampling schemes: We demonstrate that the efficiency of MH schemes for L1-type priors dramatically decreases when the level of sparsity or the dimension of the unknowns is increased. Practically, Bayesian inversion for L1-type priors using MH samplers is not feasible at all. As this is commonly believed to be an intrinsic feature of MCMC sampling, the performance of our Gibbs sampler also challenges common beliefs about the applicability of sample based Bayesian inference.

</details>

<details>

<summary>2012-09-24 16:18:52 - Rao-Blackwellised Interacting Markov Chain Monte Carlo for Electromagnetic Scattering Inversion</summary>

- *François Giraud, Pierre Minvielle, Marc Sancandi, Pierre Del Moral*

- `1209.4006v2` - [abs](http://arxiv.org/abs/1209.4006v2) - [pdf](http://arxiv.org/pdf/1209.4006v2)

> The following electromagnetism (EM) inverse problem is addressed. It consists in estimating local radioelectric properties of materials recovering an object from the global EM scattering measurement, at various incidences and wave frequencies. This large scale ill-posed inverse problem is explored by an intensive exploitation of an efficient 2D Maxwell solver, distributed on High Performance Computing (HPC) machines. Applied to a large training data set, a statistical analysis reduces the problem to a simpler probabilistic metamodel, on which Bayesian inference can be performed. Considering the radioelectric properties as a dynamic stochastic process, evolving in function of the frequency, it is shown how advanced Markov Chain Monte Carlo methods, called Sequential Monte Carlo (SMC) or interacting particles, can provide estimations of the EM properties of each material, and their associated uncertainties.

</details>

<details>

<summary>2012-09-24 19:34:06 - Bayesian analysis of hierarchical multi-fidelity codes</summary>

- *Loic Le Gratiet*

- `1112.5389v2` - [abs](http://arxiv.org/abs/1112.5389v2) - [pdf](http://arxiv.org/pdf/1112.5389v2)

> This paper deals with the Gaussian process based approximation of a code which can be run at different levels of accuracy. This method, which is a particular case of co-kriging, allows us to improve a surrogate model of a complex computer code using fast approximations of it. In particular, we focus on the case of a large number of code levels on the one hand and on a Bayesian approach when we have two levels on the other hand. The main results of this paper are a new approach to estimate the model parameters which provides a closed form expression for an important parameter of the model (the scale factor), a reduction of the numerical complexity by simplifying the covariance matrix inversion, and a new Bayesian modelling that gives an explicit representation of the joint distribution of the parameters and that is not computationally expensive. A thermodynamic example is used to illustrate the comparison between 2-level and 3-level co-kriging.

</details>

<details>

<summary>2012-09-25 06:46:13 - Bayesian semi-parametric forecasting of ultrafine particle number concentration with penalised splines and autoregressive errors</summary>

- *Sam Clifford, Bjarke Mølgaard, Sama Low Choy, Jukka Corander, Kaarle Hämeri, Kerrie Mengersen, Tareq Hussein*

- `1207.0558v4` - [abs](http://arxiv.org/abs/1207.0558v4) - [pdf](http://arxiv.org/pdf/1207.0558v4)

> Observational time series data often exhibit both cyclic temporal trends and autocorrelation and may also depend on covariates. As such, there is a need for flexible regression models that are able to capture these trends and model any residual autocorrelation simultaneously. Modelling the autocorrelation in the residuals leads to more realistic forecasts than an assumption of independence. In this paper we propose a method which combines spline-based semi-parametric regression modelling with the modelling of auto-regressive errors.   The method is applied to a simulated data set in order to show its efficacy and to ultrafine particle number concentration in Helsinki, Finland, to show its use in real world problems.

</details>

<details>

<summary>2012-09-26 15:45:42 - Accounting for spatially varying directional effects in spatial covariance structures</summary>

- *Joaquim H. Vianna Neto, Alexandra Mello Schmidt, Peter Guttorp*

- `1209.5977v1` - [abs](http://arxiv.org/abs/1209.5977v1) - [pdf](http://arxiv.org/pdf/1209.5977v1)

> Wind direction plays an important role in the spread of pollutant levels over a geographical region. We discuss how to include wind directional information in the covariance function of spatial models. We follow the spatial convolution approach initially proposed by Higdon and co-authors, wherein a spatial process is described by a convolution between a smoothing kernel and a white noise process. We propose two different ways of accounting for wind direction in the kernel function. For comparison purposes, we also consider a more flexible kernel parametrization, that makes use of latent processes which vary smoothly across the region. Inference procedure follows the Bayesian paradigm, and uncertainty about parameter estimation is naturally accounted for when performing spatial interpolation. We analyze ozone levels observed at a monitoring network in the Northeast of the USA. Sam- ples from the posterior distribution under our proposed models are obtained much faster when compared to the kernel based on latent processes. Our models provide better results, in terms of model fitting and spatial interpolation, when compared to simple isotropic and geometrical anisotropic models. Despite the small number of parameters, our proposed models provide fits which are comparable to those obtained under the kernel based on latent processes.

</details>

<details>

<summary>2012-09-26 16:41:59 - Bayesian Mixture Models for Frequent Itemset Discovery</summary>

- *Ruefei He, Jonathan Shapiro*

- `1209.6001v1` - [abs](http://arxiv.org/abs/1209.6001v1) - [pdf](http://arxiv.org/pdf/1209.6001v1)

> In binary-transaction data-mining, traditional frequent itemset mining often produces results which are not straightforward to interpret. To overcome this problem, probability models are often used to produce more compact and conclusive results, albeit with some loss of accuracy. Bayesian statistics have been widely used in the development of probability models in machine learning in recent years and these methods have many advantages, including their abilities to avoid overfitting. In this paper, we develop two Bayesian mixture models with the Dirichlet distribution prior and the Dirichlet process (DP) prior to improve the previous non-Bayesian mixture model developed for transaction dataset mining. We implement the inference of both mixture models using two methods: a collapsed Gibbs sampling scheme and a variational approximation algorithm. Experiments in several benchmark problems have shown that both mixture models achieve better performance than a non-Bayesian mixture model. The variational algorithm is the faster of the two approaches while the Gibbs sampling method achieves a more accurate results. The Dirichlet process mixture model can automatically grow to a proper complexity for a better approximation. Once the model is built, it can be very fast to query and run analysis on (typically 10 times faster than Eclat, as we will show in the experiment section). However, these approaches also show that mixture models underestimate the probabilities of frequent itemsets. Consequently, these models have a higher sensitivity but a lower specificity.

</details>

<details>

<summary>2012-09-27 14:17:41 - Estimating Hidden Population Size using Respondent-Driven Sampling Data</summary>

- *Mark S. Handcock, Krista J. Gile, Corinne M. Mar*

- `1209.6241v1` - [abs](http://arxiv.org/abs/1209.6241v1) - [pdf](http://arxiv.org/pdf/1209.6241v1)

> Respondent-Driven Sampling (RDS) is an approach to sampling design and inference in hard-to-reach human populations. Typically, a sampling frame is not available, and population members are difficult to identify or recruit from broader sampling frames. Common examples include injecting drug users, men who have sex with men, and female sex workers. Most analysis of RDS data has focused on estimating aggregate characteristics, such as disease prevalence. However, RDS is often conducted in settings where the population size is unknown and of great independent interest. This paper presents an approach to estimating the size of a target population based on data collected through RDS.   The proposed approach uses a successive sampling approximation to RDS to leverage information in the ordered sequence of observed personal network sizes. The inference uses the Bayesian framework, allowing for the incorporation of prior knowledge. A flexible class of priors for the population size is proposed that aids elicitation. An extensive simulation study provides insight into the performance of the method for estimating population size under a broad range of conditions. A further study shows the approach also improves estimation of aggregate characteristics. A particular choice of the prior produces interval estimates with good frequentist properties. Finally, the method demonstrates sensible results when used to estimate the numbers of sub-populations most at risk for HIV in two cities in El Salvador.

</details>

<details>

<summary>2012-09-28 06:45:09 - Fibre-generated point processes and fields of orientations</summary>

- *Bryony J. Hill, Wilfrid S. Kendall, Elke Thönnes*

- `1109.0701v3` - [abs](http://arxiv.org/abs/1109.0701v3) - [pdf](http://arxiv.org/pdf/1109.0701v3)

> This paper introduces a new approach to analyzing spatial point data clustered along or around a system of curves or "fibres." Such data arise in catalogues of galaxy locations, recorded locations of earthquakes, aerial images of minefields and pore patterns on fingerprints. Finding the underlying curvilinear structure of these point-pattern data sets may not only facilitate a better understanding of how they arise but also aid reconstruction of missing data. We base the space of fibres on the set of integral lines of an orientation field. Using an empirical Bayes approach, we estimate the field of orientations from anisotropic features of the data. We then sample from the posterior distribution of fibres, exploring models with different numbers of clusters, fitting fibres to the clusters as we proceed. The Bayesian approach permits inference on various properties of the clusters and associated fibres, and the results perform well on a number of very different curvilinear structures.

</details>

<details>

<summary>2012-09-28 09:23:35 - Bayesian Stable Isotope Mixing Models</summary>

- *Andrew C. Parnell, Donald L. Phillips, Stuart Bearhop, Brice X. Semmens, Eric J. Ward, Jonathan W. Moore, Andrew L. Jackson, Richard Inger*

- `1209.6457v1` - [abs](http://arxiv.org/abs/1209.6457v1) - [pdf](http://arxiv.org/pdf/1209.6457v1)

> In this paper we review recent advances in Stable Isotope Mixing Models (SIMMs) and place them into an over-arching Bayesian statistical framework which allows for several useful extensions. SIMMs are used to quantify the proportional contributions of various sources to a mixture. The most widely used application is quantifying the diet of organisms based on the food sources they have been observed to consume. At the centre of the multivariate statistical model we propose is a compositional mixture of the food sources corrected for various metabolic factors. The compositional component of our model is based on the isometric log ratio (ilr) transform of Egozcue (2003). Through this transform we can apply a range of time series and non-parametric smoothing relationships. We illustrate our models with 3 case studies based on real animal dietary behaviour.

</details>

<details>

<summary>2012-09-28 11:39:04 - Quantile correlations and quantile autoregressive modeling</summary>

- *Guodong Li, Yang Li, Chih-Ling Tsai*

- `1209.6487v1` - [abs](http://arxiv.org/abs/1209.6487v1) - [pdf](http://arxiv.org/pdf/1209.6487v1)

> In this paper, we propose two important measures, quantile correlation (QCOR) and quantile partial correlation (QPCOR). We then apply them to quantile autoregressive (QAR) models, and introduce two valuable quantities, the quantile autocorrelation function (QACF) and the quantile partial autocorrelation function (QPACF). This allows us to extend the classical Box-Jenkins approach to quantile autoregressive models. Specifically, the QPACF of an observed time series can be employed to identify the autoregressive order, while the QACF of residuals obtained from the fitted model can be used to assess the model adequacy. We not only demonstrate the asymptotic properties of QCOR, QPCOR, QACF, and PQACF, but also show the large sample results of the QAR estimates and the quantile version of the Ljung-Box test. Simulation studies indicate that the proposed methods perform well in finite samples, and an empirical example is presented to illustrate usefulness.

</details>

<details>

<summary>2012-09-28 12:40:48 - Forecast verification for extreme value distributions with an application to probabilistic peak wind prediction</summary>

- *Petra Friederichs, Thordis L. Thorarinsdottir*

- `1204.1022v3` - [abs](http://arxiv.org/abs/1204.1022v3) - [pdf](http://arxiv.org/pdf/1204.1022v3)

> Predictions of the uncertainty associated with extreme events are a vital component of any prediction system for such events. Consequently, the prediction system ought to be probabilistic in nature, with the predictions taking the form of probability distributions. This paper concerns probabilistic prediction systems where the data is assumed to follow either a generalized extreme value distribution (GEV) or a generalized Pareto distribution (GPD). In this setting, the properties of proper scoring rules which facilitate the assessment of the prediction uncertainty are investigated and closed-from expressions for the continuous ranked probability score (CRPS) are provided. In an application to peak wind prediction, the predictive performance of a GEV model under maximum likelihood estimation, optimum score estimation with the CRPS, and a Bayesian framework are compared. The Bayesian inference yields the highest overall prediction skill and is shown to be a valuable tool for covariate selection, while the predictions obtained under optimum CRPS estimation are the sharpest and give the best performance for high thresholds and quantiles.

</details>

<details>

<summary>2012-09-29 13:08:06 - On convergence rates of Bayesian predictive densities and posterior distributions</summary>

- *Ryan Martin, Liang Hong*

- `1210.0103v1` - [abs](http://arxiv.org/abs/1210.0103v1) - [pdf](http://arxiv.org/pdf/1210.0103v1)

> Frequentist-style large-sample properties of Bayesian posterior distributions, such as consistency and convergence rates, are important considerations in nonparametric problems. In this paper we give an analysis of Bayesian asymptotics based primarily on predictive densities. Our analysis is unified in the sense that essentially the same approach can be taken to develop convergence rate results in iid, mis-specified iid, independent non-iid, and dependent data cases.

</details>


## 2012-10

<details>

<summary>2012-10-01 13:05:01 - Multivariate Bayesian Logistic Regression for Analysis of Clinical Study Safety Issues</summary>

- *William DuMouchel*

- `1210.0385v1` - [abs](http://arxiv.org/abs/1210.0385v1) - [pdf](http://arxiv.org/pdf/1210.0385v1)

> This paper describes a method for a model-based analysis of clinical safety data called multivariate Bayesian logistic regression (MBLR). Parallel logistic regression models are fit to a set of medically related issues, or response variables, and MBLR allows information from the different issues to "borrow strength" from each other. The method is especially suited to sparse response data, as often occurs when fine-grained adverse events are collected from subjects in studies sized more for efficacy than for safety investigations. A combined analysis of data from multiple studies can be performed and the method enables a search for vulnerable subgroups based on the covariates in the regression model. An example involving 10 medically related issues from a pool of 8 studies is presented, as well as simulations showing distributional properties of the method.

</details>

<details>

<summary>2012-10-02 05:37:11 - Discussion of "Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues" by W. DuMouchel</summary>

- *Bradley W. McEvoy, Ram C. Tiwari*

- `1210.0655v1` - [abs](http://arxiv.org/abs/1210.0655v1) - [pdf](http://arxiv.org/pdf/1210.0655v1)

> Discussion of "Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues" by W. DuMouchel [arXiv:1210.0385].

</details>

<details>

<summary>2012-10-02 06:06:15 - Discussion of "Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues" by W. DuMouchel</summary>

- *Don Berry*

- `1210.0658v1` - [abs](http://arxiv.org/abs/1210.0658v1) - [pdf](http://arxiv.org/pdf/1210.0658v1)

> Discussion of "Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues" by W. DuMouchel [arXiv:1210.0385].

</details>

<details>

<summary>2012-10-02 06:24:36 - An Answer to Multiple Problems with Analysis of Data on Harms?</summary>

- *Stephen Evans*

- `1210.0663v1` - [abs](http://arxiv.org/abs/1210.0663v1) - [pdf](http://arxiv.org/pdf/1210.0663v1)

> Discussion of "Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues" by W. DuMouchel [arXiv:1210.0385].

</details>

<details>

<summary>2012-10-02 06:38:04 - Rejoinder to "Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues"</summary>

- *William DuMouchel*

- `1210.0669v1` - [abs](http://arxiv.org/abs/1210.0669v1) - [pdf](http://arxiv.org/pdf/1210.0669v1)

> Rejoinder to "Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues" by W. DuMouchel [arXiv:1210.0385].

</details>

<details>

<summary>2012-10-02 08:56:26 - Regularization of Case-Specific Parameters for Robustness and Efficiency</summary>

- *Yoonkyung Lee, Steven N. MacEachern, Yoonsuh Jung*

- `1210.0701v1` - [abs](http://arxiv.org/abs/1210.0701v1) - [pdf](http://arxiv.org/pdf/1210.0701v1)

> Regularization methods allow one to handle a variety of inferential problems where there are more covariates than cases. This allows one to consider a potentially enormous number of covariates for a problem. We exploit the power of these techniques, supersaturating models by augmenting the "natural" covariates in the problem with an additional indicator for each case in the data set. We attach a penalty term for these case-specific indicators which is designed to produce a desired effect. For regression methods with squared error loss, an $\ell_1$ penalty produces a regression which is robust to outliers and high leverage cases; for quantile regression methods, an $\ell_2$ penalty decreases the variance of the fit enough to overcome an increase in bias. The paradigm thus allows us to robustify procedures which lack robustness and to increase the efficiency of procedures which are robust. We provide a general framework for the inclusion of case-specific parameters in regularization problems, describing the impact on the effective loss for a variety of regression and classification problems. We outline a computational strategy by which existing software can be modified to solve the augmented regularization problem, providing conditions under which such modification will converge to the optimum solution. We illustrate the benefits of including case-specific parameters in the context of mean regression and quantile regression through analysis of NHANES and linguistic data sets.

</details>

<details>

<summary>2012-10-03 09:40:36 - Predicting human preferences using the block structure of complex social networks</summary>

- *Roger Guimera, Alejandro Llorente, Esteban Moro, Marta Sales-Pardo*

- `1210.1048v1` - [abs](http://arxiv.org/abs/1210.1048v1) - [pdf](http://arxiv.org/pdf/1210.1048v1)

> With ever-increasing available data, predicting individuals' preferences and helping them locate the most relevant information has become a pressing need. Understanding and predicting preferences is also important from a fundamental point of view, as part of what has been called a "new" computational social science. Here, we propose a novel approach based on stochastic block models, which have been developed by sociologists as plausible models of complex networks of social interactions. Our model is in the spirit of predicting individuals' preferences based on the preferences of others but, rather than fitting a particular model, we rely on a Bayesian approach that samples over the ensemble of all possible models. We show that our approach is considerably more accurate than leading recommender algorithms, with major relative improvements between 38% and 99% over industry-level algorithms. Besides, our approach sheds light on decision-making processes by identifying groups of individuals that have consistently similar preferences, and enabling the analysis of the characteristics of those groups.

</details>

<details>

<summary>2012-10-03 12:43:33 - Nearly root-n approximation for regression quantile processes</summary>

- *Stephen Portnoy*

- `1210.1092v1` - [abs](http://arxiv.org/abs/1210.1092v1) - [pdf](http://arxiv.org/pdf/1210.1092v1)

> Traditionally, assessing the accuracy of inference based on regression quantiles has relied on the Bahadur representation. This provides an error of order $n^{-1/4}$ in normal approximations, and suggests that inference based on regression quantiles may not be as reliable as that based on other (smoother) approaches, whose errors are generally of order $n^{-1/2}$ (or better in special symmetric cases). Fortunately, extensive simulations and empirical applications show that inference for regression quantiles shares the smaller error rates of other procedures. In fact, the "Hungarian" construction of Koml\'{o}s, Major and Tusn\'{a}dy [Z. Wahrsch. Verw. Gebiete 32 (1975) 111-131, Z. Wahrsch. Verw. Gebiete 34 (1976) 33-58] provides an alternative expansion for the one-sample quantile process with nearly the root-$n$ error rate (specifically, to within a factor of $\log n$). Such an expansion is developed here to provide a theoretical foundation for more accurate approximations for inference in regression quantile models. One specific application of independent interest is a result establishing that for conditional inference, the error rate for coverage probabilities using the Hall and Sheather [J. R. Stat. Soc. Ser. B Stat. Methodol. 50 (1988) 381-391] method of sparsity estimation matches their one-sample rate.

</details>

<details>

<summary>2012-10-04 16:56:39 - Efficient Bayesian Multivariate Surface Regression</summary>

- *Feng Li, Mattias Villani*

- `1110.3689v2` - [abs](http://arxiv.org/abs/1110.3689v2) - [pdf](http://arxiv.org/pdf/1110.3689v2)

> Methods for choosing a fixed set of knot locations in additive spline models are fairly well established in the statistical literature. While most of these methods are in principle directly extendable to non-additive surface models, they are less likely to be successful in that setting because of the curse of dimensionality, especially when there are more than a couple of covariates. We propose a regression model for a multivariate Gaussian response that combines both additive splines and interactive splines, and a highly efficient MCMC algorithm that updates all the knot locations jointly. We use shrinkage priors to avoid overfitting with different estimated shrinkage factors for the additive and surface part of the model, and also different shrinkage parameters for the different response variables. This makes it possible for the model to adapt to varying degrees of nonlinearity in different parts of the data in a parsimonious way. Simulated data and an application to firm leverage data show that the approach is computationally efficient, and that allowing for freely estimated knot locations can offer a substantial improvement in out-of-sample predictive performance.

</details>

<details>

<summary>2012-10-05 11:39:35 - Automatic Relevance Determination in Nonnegative Matrix Factorization with the β-Divergence</summary>

- *Vincent Y. F. Tan, Cédric Févotte*

- `1111.6085v3` - [abs](http://arxiv.org/abs/1111.6085v3) - [pdf](http://arxiv.org/pdf/1111.6085v3)

> This paper addresses the estimation of the latent dimensionality in nonnegative matrix factorization (NMF) with the \beta-divergence. The \beta-divergence is a family of cost functions that includes the squared Euclidean distance, Kullback-Leibler and Itakura-Saito divergences as special cases. Learning the model order is important as it is necessary to strike the right balance between data fidelity and overfitting. We propose a Bayesian model based on automatic relevance determination in which the columns of the dictionary matrix and the rows of the activation matrix are tied together through a common scale parameter in their prior. A family of majorization-minimization algorithms is proposed for maximum a posteriori (MAP) estimation. A subset of scale parameters is driven to a small lower bound in the course of inference, with the effect of pruning the corresponding spurious components. We demonstrate the efficacy and robustness of our algorithms by performing extensive experiments on synthetic data, the swimmer dataset, a music decomposition example and a stock price prediction task.

</details>

<details>

<summary>2012-10-08 17:05:05 - Improved Adaptive Rejection Metropolis Sampling Algorithms</summary>

- *Luca Martino, Jesse Read, David Luengo*

- `1205.5494v4` - [abs](http://arxiv.org/abs/1205.5494v4) - [pdf](http://arxiv.org/pdf/1205.5494v4)

> Markov Chain Monte Carlo (MCMC) methods, such as the Metropolis-Hastings (MH) algorithm, are widely used for Bayesian inference. One of the most important issues for any MCMC method is the convergence of the Markov chain, which depends crucially on a suitable choice of the proposal density. Adaptive Rejection Metropolis Sampling (ARMS) is a well-known MH scheme that generates samples from one-dimensional target densities making use of adaptive piecewise proposals constructed using support points taken from rejected samples. In this work we pinpoint a crucial drawback in the adaptive procedure in ARMS: support points might never be added inside regions where the proposal is below the target. When this happens in many regions it leads to a poor performance of ARMS, with the proposal never converging to the target. In order to overcome this limitation we propose two improved adaptive schemes for constructing the proposal. The first one is a direct modification of the ARMS procedure that incorporates support points inside regions where the proposal is below the target, while satisfying the diminishing adaptation property, one of the required conditions to assure the convergence of the Markov chain. The second one is an adaptive independent MH algorithm with the ability to learn from all previous samples except for the current state of the chain, thus also guaranteeing the convergence to the invariant density. These two new schemes improve the adaptive strategy of ARMS, thus simplifying the complexity in the construction of the proposals. Numerical results show that the new techniques provide better performance w.r.t. the standard ARMS.

</details>

<details>

<summary>2012-10-11 16:46:39 - Optimizing Threshold - Schedules for Approximate Bayesian Computation Sequential Monte Carlo Samplers: Applications to Molecular Systems</summary>

- *Daniel Silk, Saran Filippi, Michael P. H. Stumpf*

- `1210.3296v1` - [abs](http://arxiv.org/abs/1210.3296v1) - [pdf](http://arxiv.org/pdf/1210.3296v1)

> The likelihood-free sequential Approximate Bayesian Computation (ABC) algorithms, are increasingly popular inference tools for complex biological models. Such algorithms proceed by constructing a succession of probability distributions over the parameter space conditional upon the simulated data lying in an $\epsilon$--ball around the observed data, for decreasing values of the threshold $\epsilon$. While in theory, the distributions (starting from a suitably defined prior) will converge towards the unknown posterior as $\epsilon$ tends to zero, the exact sequence of thresholds can impact upon the computational efficiency and success of a particular application. In particular, we show here that the current preferred method of choosing thresholds as a pre-determined quantile of the distances between simulated and observed data from the previous population, can lead to the inferred posterior distribution being very different to the true posterior. Threshold selection thus remains an important challenge. Here we propose an automated and adaptive method that allows us to balance the need to minimise the threshold with computational efficiency. Moreover, our method which centres around predicting the threshold - acceptance rate curve using the unscented transform, enables us to avoid local minima - a problem that has plagued previous threshold schemes.

</details>

<details>

<summary>2012-10-12 13:56:44 - Bayesian Estimation with Distance Bounds</summary>

- *Dave Zachariah, Isaac Skog, Magnus Jansson, Peter Händel*

- `1210.3516v1` - [abs](http://arxiv.org/abs/1210.3516v1) - [pdf](http://arxiv.org/pdf/1210.3516v1)

> We consider the problem of estimating a random state vector when there is information about the maximum distances between its subvectors. The estimation problem is posed in a Bayesian framework in which the minimum mean square error (MMSE) estimate of the state is given by the conditional mean. Since finding the conditional mean requires multidimensional integration, an approximate MMSE estimator is proposed. The performance of the proposed estimator is evaluated in a positioning problem. Finally, the application of the estimator in inequality constrained recursive filtering is illustrated by applying the estimator to a dead-reckoning problem. The MSE of the estimator is compared with two related posterior Cram\'er-Rao bounds.

</details>

<details>

<summary>2012-10-14 22:10:57 - Gaussian Process-Based Bayesian Nonparametric Inference of Population Trajectories from Gene Genealogies</summary>

- *Julia A. Palacios, Vladimir N. Minin*

- `1112.4138v2` - [abs](http://arxiv.org/abs/1112.4138v2) - [pdf](http://arxiv.org/pdf/1112.4138v2)

> Changes in population size influence genetic diversity of the population and, as a result, leave a signature of these changes in individual genomes in the population. We are interested in the inverse problem of reconstructing past population dynamics from genomic data. We start with a standard framework based on the coalescent, a stochastic process that generates genealogies connecting randomly sampled individuals from the population of interest. These genealogies serve as a glue between the population demographic history and genomic sequences. It turns out that only the times of genealogical lineage coalescences contain information about population size dynamics. Viewing these coalescent times as a point process, estimating population size trajectories is equivalent to estimating a conditional intensity of this point process. Therefore, our inverse problem is similar to estimating an inhomogeneous Poisson process intensity function. We demonstrate how recent advances in Gaussian process-based nonparametric inference for Poisson processes can be extended to Bayesian nonparametric estimation of population size dynamics under the coalescent. We compare our Gaussian process (GP) approach to one of the state of the art Gaussian Markov random field (GMRF) methods for estimating population trajectories. Using simulated data, we demonstrate that our method has better accuracy and precision. Next, we analyze two genealogies reconstructed from real sequences of hepatitis C and human Influenza A viruses. In both cases, we recover more believed aspects of the viral demographic histories than the GMRF approach. We also find that our GP method produces more reasonable uncertainty estimates than the GMRF method.

</details>

<details>

<summary>2012-10-15 14:44:56 - Expansions about the gamma for the distribution and quantiles of a standard estimate</summary>

- *C. S. Withers, S. Nadarajah*

- `1210.4052v1` - [abs](http://arxiv.org/abs/1210.4052v1) - [pdf](http://arxiv.org/pdf/1210.4052v1)

> We give expansions for the distribution, density, and quantiles of an estimate, building on results of Cornish, Fisher, Hill, Davis and the authors. The estimate is assumed to be non-lattice with the standard expansions for its cumulants. By expanding about a skew variable with matched skewness, one can drastically reduce the number of terms needed for a given level of accuracy. The building blocks generalize the Hermite polynomials. We demonstrate with expansions about the gamma.

</details>

<details>

<summary>2012-10-15 14:52:40 - On optimality of kernels for approximate Bayesian computation using sequential Monte Carlo</summary>

- *Sarah Filippi, Chris Barnes, Julien Cornebise, Michael P. H. Stumpf*

- `1106.6280v4` - [abs](http://arxiv.org/abs/1106.6280v4) - [pdf](http://arxiv.org/pdf/1106.6280v4)

> Approximate Bayesian computation (ABC) has gained popularity over the past few years for the analysis of complex models arising in population genetic, epidemiology and system biology. Sequential Monte Carlo (SMC) approaches have become work horses in ABC. Here we discuss how to construct the perturbation kernels that are required in ABC SMC approaches, in order to construct a set of distributions that start out from a suitably defined prior and converge towards the unknown posterior. We derive optimality criteria for different kernels, which are based on the Kullback-Leibler divergence between a distribution and the distribution of the perturbed particles. We will show that for many complicated posterior distributions, locally adapted kernels tend to show the best performance. In cases where it is possible to estimate the Fisher information we can construct particularly efficient perturbation kernels. We find that the added moderate cost of adapting kernel functions is easily regained in terms of the higher acceptance rate. We demonstrate the computational efficiency gains in a range of toy-examples which illustrate some of the challenges faced in real-world applications of ABC, before turning to two demanding parameter inference problem in molecular biology, which highlight the huge increases in efficiency that can be gained from choice of optimal models. We conclude with a general discussion of rational choice of perturbation kernels in ABC SMC settings.

</details>

<details>

<summary>2012-10-16 10:26:29 - Hilbert Space Embedding for Dirichlet Process Mixtures</summary>

- *Krikamol Muandet*

- `1210.4347v1` - [abs](http://arxiv.org/abs/1210.4347v1) - [pdf](http://arxiv.org/pdf/1210.4347v1)

> This paper proposes a Hilbert space embedding for Dirichlet Process mixture models via a stick-breaking construction of Sethuraman. Although Bayesian nonparametrics offers a powerful approach to construct a prior that avoids the need to specify the model size/complexity explicitly, an exact inference is often intractable. On the other hand, frequentist approaches such as kernel machines, which suffer from the model selection/comparison problems, often benefit from efficient learning algorithms. This paper discusses the possibility to combine the best of both worlds by using the Dirichlet Process mixture model as a case study.

</details>

<details>

<summary>2012-10-16 17:34:18 - Plackett-Luce regression: A new Bayesian model for polychotomous data</summary>

- *Cedric Archambeau, Francois Caron*

- `1210.4844v1` - [abs](http://arxiv.org/abs/1210.4844v1) - [pdf](http://arxiv.org/pdf/1210.4844v1)

> Multinomial logistic regression is one of the most popular models for modelling the effect of explanatory variables on a subject choice between a set of specified options. This model has found numerous applications in machine learning, psychology or economy. Bayesian inference in this model is non trivial and requires, either to resort to a MetropolisHastings algorithm, or rejection sampling within a Gibbs sampler. In this paper, we propose an alternative model to multinomial logistic regression. The model builds on the Plackett-Luce model, a popular model for multiple comparisons. We show that the introduction of a suitable set of auxiliary variables leads to an Expectation-Maximization algorithm to find Maximum A Posteriori estimates of the parameters. We further provide a full Bayesian treatment by deriving a Gibbs sampler, which only requires to sample from highly standard distributions. We also propose a variational approximate inference scheme. All are very simple to implement. One property of our Plackett-Luce regression model is that it learns a sparse set of feature weights. We compare our method to sparse Bayesian multinomial logistic regression and show that it is competitive, especially in presence of polychotomous data.

</details>

<details>

<summary>2012-10-16 17:39:28 - A Bayesian Approach to Constraint Based Causal Inference</summary>

- *Tom Claassen, Tom Heskes*

- `1210.4866v1` - [abs](http://arxiv.org/abs/1210.4866v1) - [pdf](http://arxiv.org/pdf/1210.4866v1)

> We target the problem of accuracy and robustness in causal inference from finite data sets. Some state-of-the-art algorithms produce clear output complete with solid theoretical guarantees but are susceptible to propagating erroneous decisions, while others are very adept at handling and representing uncertainty, but need to rely on undesirable assumptions. Our aim is to combine the inherent robustness of the Bayesian approach with the theoretical strength and clarity of constraint-based methods. We use a Bayesian score to obtain probability estimates on the input statements used in a constraint-based procedure. These are subsequently processed in decreasing order of reliability, letting more reliable decisions take precedence in case of con icts, until a single output model is obtained. Tests show that a basic implementation of the resulting Bayesian Constraint-based Causal Discovery (BCCD) algorithm already outperforms established procedures such as FCI and Conservative PC. It can also indicate which causal decisions in the output have high reliability and which do not.

</details>

<details>

<summary>2012-10-16 17:46:17 - Local Structure Discovery in Bayesian Networks</summary>

- *Teppo Niinimaki, Pekka Parviainen*

- `1210.4888v1` - [abs](http://arxiv.org/abs/1210.4888v1) - [pdf](http://arxiv.org/pdf/1210.4888v1)

> Learning a Bayesian network structure from data is an NP-hard problem and thus exact algorithms are feasible only for small data sets. Therefore, network structures for larger networks are usually learned with various heuristics. Another approach to scaling up the structure learning is local learning. In local learning, the modeler has one or more target variables that are of special interest; he wants to learn the structure near the target variables and is not interested in the rest of the variables. In this paper, we present a score-based local learning algorithm called SLL. We conjecture that our algorithm is theoretically sound in the sense that it is optimal in the limit of large sample size. Empirical results suggest that SLL is competitive when compared to the constraint-based HITON algorithm. We also study the prospects of constructing the network structure for the whole node set based on local results by presenting two algorithms and comparing them to several heuristics.

</details>

<details>

<summary>2012-10-16 17:47:18 - Unsupervised Joint Alignment and Clustering using Bayesian Nonparametrics</summary>

- *Marwan A. Mattar, Allen R. Hanson, Erik G. Learned-Miller*

- `1210.4892v1` - [abs](http://arxiv.org/abs/1210.4892v1) - [pdf](http://arxiv.org/pdf/1210.4892v1)

> Joint alignment of a collection of functions is the process of independently transforming the functions so that they appear more similar to each other. Typically, such unsupervised alignment algorithms fail when presented with complex data sets arising from multiple modalities or make restrictive assumptions about the form of the functions or transformations, limiting their generality. We present a transformed Bayesian infinite mixture model that can simultaneously align and cluster a data set. Our model and associated learning scheme offer two key advantages: the optimal number of clusters is determined in a data-driven fashion through the use of a Dirichlet process prior, and it can accommodate any transformation function parameterized by a continuous parameter vector. As a result, it is applicable to a wide range of data types, and transformation functions. We present positive results on synthetic two-dimensional data, on a set of one-dimensional curves, and on various image data sets, showing large improvements over previous work. We discuss several variations of the model and conclude with directions for future work.

</details>

<details>

<summary>2012-10-16 17:52:51 - Integrated Nested Laplace Approximation for Bayesian Nonparametric Phylodynamics</summary>

- *Julia A. Palacios, Vladimir N. Minin*

- `1210.4908v1` - [abs](http://arxiv.org/abs/1210.4908v1) - [pdf](http://arxiv.org/pdf/1210.4908v1)

> The goal of phylodynamics, an area on the intersection of phylogenetics and population genetics, is to reconstruct population size dynamics from genetic data. Recently, a series of nonparametric Bayesian methods have been proposed for such demographic reconstructions. These methods rely on prior specifications based on Gaussian processes and proceed by approximating the posterior distribution of population size trajectories via Markov chain Monte Carlo (MCMC) methods. In this paper, we adapt an integrated nested Laplace approximation (INLA), a recently proposed approximate Bayesian inference for latent Gaussian models, to the estimation of population size trajectories. We show that when a genealogy of sampled individuals can be reliably estimated from genetic data, INLA enjoys high accuracy and can replace MCMC entirely. We demonstrate significant computational efficiency over the state-of-the-art MCMC methods. We illustrate INLA-based population size inference using simulations and genealogies of hepatitis C and human influenza viruses.

</details>

<details>

<summary>2012-10-16 17:53:29 - New Advances and Theoretical Insights into EDML</summary>

- *Khaled S. Refaat, Arthur Choi, Adnan Darwiche*

- `1210.4910v1` - [abs](http://arxiv.org/abs/1210.4910v1) - [pdf](http://arxiv.org/pdf/1210.4910v1)

> EDML is a recently proposed algorithm for learning MAP parameters in Bayesian networks. In this paper, we present a number of new advances and insights on the EDML algorithm. First, we provide the multivalued extension of EDML, originally proposed for Bayesian networks over binary variables. Next, we identify a simplified characterization of EDML that further implies a simple fixed-point algorithm for the convex optimization problem that underlies it. This characterization further reveals a connection between EDML and EM: a fixed point of EDML is a fixed point of EM, and vice versa. We thus identify also a new characterization of EM fixed points, but in the semantics of EDML. Finally, we propose a hybrid EDML/EM algorithm that takes advantage of the improved empirical convergence behavior of EDML, while maintaining the monotonic improvement property of EM.

</details>

<details>

<summary>2012-10-16 17:55:57 - An Improved Admissible Heuristic for Learning Optimal Bayesian Networks</summary>

- *Changhe Yuan, Brandon Malone*

- `1210.4913v1` - [abs](http://arxiv.org/abs/1210.4913v1) - [pdf](http://arxiv.org/pdf/1210.4913v1)

> Recently two search algorithms, A* and breadth-first branch and bound (BFBnB), were developed based on a simple admissible heuristic for learning Bayesian network structures that optimize a scoring function. The heuristic represents a relaxation of the learning problem such that each variable chooses optimal parents independently. As a result, the heuristic may contain many directed cycles and result in a loose bound. This paper introduces an improved admissible heuristic that tries to avoid directed cycles within small groups of variables. A sparse representation is also introduced to store only the unique optimal parent choices. Empirical results show that the new techniques significantly improved the efficiency and scalability of A* and BFBnB on most of datasets tested in this paper.

</details>

<details>

<summary>2012-10-16 18:47:39 - Bayesian Estimation of Inverse Gaussian Distribution</summary>

- *B. N. Pandey, Pulastya Bandyopadhyay*

- `1210.4524v1` - [abs](http://arxiv.org/abs/1210.4524v1) - [pdf](http://arxiv.org/pdf/1210.4524v1)

> In this paper we consider Bayesian estimation for the parameters of inverse Gaussian distribution. Our emphasis is on Markov Chain Monte Carlo methods. We provide complete implementation of the Gibbs sampler algorithm. Assuming an informative prior, Bayes estimates are computed using the output of the Gibbs sampler and also from Lindley's approximation method. Maximum Likelihood and Uniformly Minimum Variance Unbiased estimates are obtained as well. We also compute Highest Posterior Density credible intervals, exact confidence intervals as well as "percentile" and "percentile-t" bootstrap approximations to the exact intervals. A simulation study was conducted to compare the long-run performance of the various point and interval estimation methods considered. One real data illustration has been provided which brings out some salient features of sampling-based approach to inference.

</details>

<details>

<summary>2012-10-17 09:57:20 - Static Parameter Estimation for ABC Approximations of Hidden Markov Models</summary>

- *Elena Ehrlich, Ajay Jasra, Nikolas Kantas*

- `1210.4683v1` - [abs](http://arxiv.org/abs/1210.4683v1) - [pdf](http://arxiv.org/pdf/1210.4683v1)

> In this article we focus on Maximum Likelihood estimation (MLE) for the static parameters of hidden Markov models (HMMs). We will consider the case where one cannot or does not want to compute the conditional likelihood density of the observation given the hidden state because of increased computational complexity or analytical intractability. Instead we will assume that one may obtain samples from this conditional likelihood and hence use approximate Bayesian computation (ABC) approximations of the original HMM. ABC approximations are biased, but the bias can be controlled to arbitrary precision via a parameter \epsilon>0; the bias typically goes to zero as \epsilon \searrow 0. We first establish that the bias in the log-likelihood and gradient of the log-likelihood of the ABC approximation, for a fixed batch of data, is no worse than \mathcal{O}(n\epsilon), n being the number of data; hence, for computational reasons, one might expect reasonable parameter estimates using such an ABC approximation. Turning to the computational problem of estimating $\theta$, we propose, using the ABC-sequential Monte Carlo (SMC) algorithm in Jasra et al. (2012), an approach based upon simultaneous perturbation stochastic approximation (SPSA). Our method is investigated on two numerical examples

</details>

<details>

<summary>2012-10-18 04:17:59 - Cramer Rao-Type Bounds for Sparse Bayesian Learning</summary>

- *Ranjitha Prasad, Chandra R. Murthy*

- `1202.1119v2` - [abs](http://arxiv.org/abs/1202.1119v2) - [pdf](http://arxiv.org/pdf/1202.1119v2)

> In this paper, we derive Hybrid, Bayesian and Marginalized Cram\'{e}r-Rao lower bounds (HCRB, BCRB and MCRB) for the single and multiple measurement vector Sparse Bayesian Learning (SBL) problem of estimating compressible vectors and their prior distribution parameters. We assume the unknown vector to be drawn from a compressible Student-t prior distribution. We derive CRBs that encompass the deterministic or random nature of the unknown parameters of the prior distribution and the regression noise variance. We extend the MCRB to the case where the compressible vector is distributed according to a general compressible prior distribution, of which the generalized Pareto distribution is a special case. We use the derived bounds to uncover the relationship between the compressibility and Mean Square Error (MSE) in the estimates. Further, we illustrate the tightness and utility of the bounds through simulations, by comparing them with the MSE performance of two popular SBL-based estimators. It is found that the MCRB is generally the tightest among the bounds derived and that the MSE performance of the Expectation-Maximization (EM) algorithm coincides with the MCRB for the compressible vector. Through simulations, we demonstrate the dependence of the MSE performance of SBL based estimators on the compressibility of the vector for several values of the number of observations and at different signal powers.

</details>

<details>

<summary>2012-10-18 11:20:05 - Application of Bayesian Methods for Age-dependent Reliability Analysis</summary>

- *Robertas Alzbutas, Tomas Iešmantas*

- `1210.5095v1` - [abs](http://arxiv.org/abs/1210.5095v1) - [pdf](http://arxiv.org/pdf/1210.5095v1)

> In this paper authors present a general methodology for age dependent reliability analysis of degrading or ageing systems, structures and components.The methodology is based on Bayesian methods and inference, its ability to incorporate prior information and on idea that ageing can be thought as age dependent change of believes about reliability parameters, when change of belief occurs not just due to new failure data or other information which becomes available in time, but also it continuously changes due to flow of time and beliefs evolution. The main objective of this paper is to present the clear way of how Bayesian methods can be applied by practitioners to deal with risk and reliability analysis considering ageing phenomena. The methodology describes step by step failure rate analysis of ageing systems: from the Bayesian model building to its verification and generalization with Bayesian model averaging which, as authors suggest in this paper, could serve as alternative for various goodness of fit assessment tools and as universal tool to cope with various sources of uncertainty.

</details>

<details>

<summary>2012-10-18 14:15:40 - LSBN: A Large-Scale Bayesian Structure Learning Framework for Model Averaging</summary>

- *Yang Lu, Mengying Wang, Menglu Li, Qili Zhu, Bo Yuan*

- `1210.5135v1` - [abs](http://arxiv.org/abs/1210.5135v1) - [pdf](http://arxiv.org/pdf/1210.5135v1)

> The motivation for this paper is to apply Bayesian structure learning using Model Averaging in large-scale networks. Currently, Bayesian model averaging algorithm is applicable to networks with only tens of variables, restrained by its super-exponential complexity. We present a novel framework, called LSBN(Large-Scale Bayesian Network), making it possible to handle networks with infinite size by following the principle of divide-and-conquer. The method of LSBN comprises three steps. In general, LSBN first performs the partition by using a second-order partition strategy, which achieves more robust results. LSBN conducts sampling and structure learning within each overlapping community after the community is isolated from other variables by Markov Blanket. Finally LSBN employs an efficient algorithm, to merge structures of overlapping communities into a whole. In comparison with other four state-of-art large-scale network structure learning algorithms such as ARACNE, PC, Greedy Search and MMHC, LSBN shows comparable results in five common benchmark datasets, evaluated by precision, recall and f-score. What's more, LSBN makes it possible to learn large-scale Bayesian structure by Model Averaging which used to be intractable. In summary, LSBN provides an scalable and parallel framework for the reconstruction of network structures. Besides, the complete information of overlapping communities serves as the byproduct, which could be used to mine meaningful clusters in biological networks, such as protein-protein-interaction network or gene regulatory network, as well as in social network.

</details>

<details>

<summary>2012-10-18 23:08:24 - Bayesian Conditional Monte Carlo Algorithms for Sequential Single and Multi-Object filtering</summary>

- *Yohan Petetin, François Desbouvries*

- `1210.5277v1` - [abs](http://arxiv.org/abs/1210.5277v1) - [pdf](http://arxiv.org/pdf/1210.5277v1)

> Bayesian filtering aims at tracking sequentially a hidden process from an observed one. In particular, sequential Monte Carlo (SMC) techniques propagate in time weighted trajectories which represent the posterior probability density function (pdf) of the hidden process given the available observations. On the other hand, Conditional Monte Carlo (CMC) is a variance reduction technique which replaces the estimator of a moment of interest by its conditional expectation given another variable. In this paper we show that up to some adaptations, one can make use of the time recursive nature of SMC algorithms in order to propose natural temporal CMC estimators of some point estimates of the hidden process, which outperform the associated crude Monte Carlo (MC) estimator whatever the number of samples. We next show that our Bayesian CMC estimators can be computed exactly, or approximated efficiently, in some hidden Markov chain (HMC) models; in some jump Markov state-space systems (JMSS); as well as in multitarget filtering. Finally our algorithms are validated via simulations.

</details>

<details>

<summary>2012-10-19 15:03:51 - Bayesian Hierarchical Mixtures of Experts</summary>

- *Christopher M. Bishop, Markus Svensen*

- `1212.2447v1` - [abs](http://arxiv.org/abs/1212.2447v1) - [pdf](http://arxiv.org/pdf/1212.2447v1)

> The Hierarchical Mixture of Experts (HME) is a well-known tree-based model for regression and classification, based on soft probabilistic splits. In its original formulation it was trained by maximum likelihood, and is therefore prone to over-fitting. Furthermore the maximum likelihood framework offers no natural metric for optimizing the complexity and structure of the tree. Previous attempts to provide a Bayesian treatment of the HME model have relied either on ad-hoc local Gaussian approximations or have dealt with related models representing the joint distribution of both input and output variables. In this paper we describe a fully Bayesian treatment of the HME model based on variational inference. By combining local and global variational methods we obtain a rigourous lower bound on the marginal probability of the data under the model. This bound is optimized during the training phase, and its resulting value can be used for model order selection. We present results using this approach for a data set describing robot arm kinematics.

</details>

<details>

<summary>2012-10-19 15:04:17 - Reasoning about Bayesian Network Classifiers</summary>

- *Hei Chan, Adnan Darwiche*

- `1212.2470v1` - [abs](http://arxiv.org/abs/1212.2470v1) - [pdf](http://arxiv.org/pdf/1212.2470v1)

> Bayesian network classifiers are used in many fields, and one common class of classifiers are naive Bayes classifiers. In this paper, we introduce an approach for reasoning about Bayesian network classifiers in which we explicitly convert them into Ordered Decision Diagrams (ODDs), which are then used to reason about the properties of these classifiers. Specifically, we present an algorithm for converting any naive Bayes classifier into an ODD, and we show theoretically and experimentally that this algorithm can give us an ODD that is tractable in size even given an intractable number of instances. Since ODDs are tractable representations of classifiers, our algorithm allows us to efficiently test the equivalence of two naive Bayes classifiers and characterize discrepancies between them. We also show a number of additional results including a count of distinct classifiers that can be induced by changing some CPT in a naive Bayes classifier, and the range of allowable changes to a CPT which keeps the current classifier unchanged.

</details>

<details>

<summary>2012-10-19 15:04:28 - Large-Sample Learning of Bayesian Networks is NP-Hard</summary>

- *David Maxwell Chickering, Christopher Meek, David Heckerman*

- `1212.2468v1` - [abs](http://arxiv.org/abs/1212.2468v1) - [pdf](http://arxiv.org/pdf/1212.2468v1)

> In this paper, we provide new complexity results for algorithms that learn discrete-variable Bayesian networks from data. Our results apply whenever the learning algorithm uses a scoring criterion that favors the simplest model able to represent the generative distribution exactly. Our results therefore hold whenever the learning algorithm uses a consistent scoring criterion and is applied to a sufficiently large dataset. We show that identifying high-scoring structures is hard, even when we are given an independence oracle, an inference oracle, and/or an information oracle. Our negative results also apply to the learning of discrete-variable Bayesian networks in which each node has at most k parents, for all k > 3.

</details>

<details>

<summary>2012-10-19 15:04:32 - Probabilistic models for joint clustering and time-warping of multidimensional curves</summary>

- *Darya Chudova, Scott Gaffney, Padhraic Smyth*

- `1212.2467v1` - [abs](http://arxiv.org/abs/1212.2467v1) - [pdf](http://arxiv.org/pdf/1212.2467v1)

> In this paper we present a family of algorithms that can simultaneously align and cluster sets of multidimensional curves measured on a discrete time grid. Our approach is based on a generative mixture model that allows non-linear time warping of the observed curves relative to the mean curves within the clusters. We also allow for arbitrary discrete-valued translation of the time axis, random real-valued offsets of the measured curves, and additive measurement noise. The resulting model can be viewed as a dynamic Bayesian network with a special transition structure that allows effective inference and learning. The Expectation-Maximization (EM) algorithm can be used to simultaneously recover both the curve models for each cluster, and the most likely time warping, translation, offset, and cluster membership for each curve. We demonstrate how Bayesian estimation methods improve the results for smaller sample sizes by enforcing smoothness in the cluster mean curves. We evaluate the methodology on two real-world data sets, and show that the DBN models provide systematic improvements in predictive power over competing approaches.

</details>

<details>

<summary>2012-10-19 15:04:44 - A Robust Independence Test for Constraint-Based Learning of Causal Structure</summary>

- *Denver Dash, Marek J. Druzdzel*

- `1212.2464v1` - [abs](http://arxiv.org/abs/1212.2464v1) - [pdf](http://arxiv.org/pdf/1212.2464v1)

> Constraint-based (CB) learning is a formalism for learning a causal network with a database D by performing a series of conditional-independence tests to infer structural information. This paper considers a new test of independence that combines ideas from Bayesian learning, Bayesian network inference, and classical hypothesis testing to produce a more reliable and robust test. The new test can be calculated in the same asymptotic time and space required for the standard tests such as the chi-squared test, but it allows the specification of a prior distribution over parameters and can be used when the database is incomplete. We prove that the test is correct, and we demonstrate empirically that, when used with a CB causal discovery algorithm with noninformative priors, it recovers structural features more reliably and it produces networks with smaller KL-Divergence, especially as the number of nodes increases or the number of records decreases. Another benefit is the dramatic reduction in the probability that a CB algorithm will stall during the search, providing a remedy for an annoying problem plaguing CB learning when the database is small.

</details>

<details>

<summary>2012-10-19 15:05:25 - A Distance-Based Branch and Bound Feature Selection Algorithm</summary>

- *Ari Frank, Dan Geiger, Zohar Yakhini*

- `1212.2488v1` - [abs](http://arxiv.org/abs/1212.2488v1) - [pdf](http://arxiv.org/pdf/1212.2488v1)

> There is no known efficient method for selecting k Gaussian features from n which achieve the lowest Bayesian classification error. We show an example of how greedy algorithms faced with this task are led to give results that are not optimal. This motivates us to propose a more robust approach. We present a Branch and Bound algorithm for finding a subset of k independent Gaussian features which minimizes the naive Bayesian classification error. Our algorithm uses additive monotonic distance measures to produce bounds for the Bayesian classification error in order to exclude many feature subsets from evaluation, while still returning an optimal solution. We test our method on synthetic data as well as data obtained from gene expression profiling.

</details>

<details>

<summary>2012-10-19 15:06:00 - Approximate Inference and Constrained Optimization</summary>

- *Tom Heskes, Kees Albers, Hilbert Kappen*

- `1212.2480v1` - [abs](http://arxiv.org/abs/1212.2480v1) - [pdf](http://arxiv.org/pdf/1212.2480v1)

> Loopy and generalized belief propagation are popular algorithms for approximate inference in Markov random fields and Bayesian networks. Fixed points of these algorithms correspond to extrema of the Bethe and Kikuchi free energy. However, belief propagation does not always converge, which explains the need for approaches that explicitly minimize the Kikuchi/Bethe free energy, such as CCCP and UPS. Here we describe a class of algorithms that solves this typically nonconvex constrained minimization of the Kikuchi free energy through a sequence of convex constrained minimizations of upper bounds on the Kikuchi free energy. Intuitively one would expect tighter bounds to lead to faster algorithms, which is indeed convincingly demonstrated in our simulations. Several ideas are applied to obtain tight convex bounds that yield dramatic speed-ups over CCCP.

</details>

<details>

<summary>2012-10-19 15:06:57 - Practically Perfect</summary>

- *Christopher Meek, David Maxwell Chickering*

- `1212.2503v1` - [abs](http://arxiv.org/abs/1212.2503v1) - [pdf](http://arxiv.org/pdf/1212.2503v1)

> The property of perfectness plays an important role in the theory of Bayesian networks. First, the existence of perfect distributions for arbitrary sets of variables and directed acyclic graphs implies that various methods for reading independence from the structure of the graph (e.g., Pearl, 1988; Lauritzen, Dawid, Larsen & Leimer, 1990) are complete. Second, the asymptotic reliability of various search methods is guaranteed under the assumption that the generating distribution is perfect (e.g., Spirtes, Glymour & Scheines, 2000; Chickering & Meek, 2002). We provide a lower-bound on the probability of sampling a non-perfect distribution when using a fixed number of bits to represent the parameters of the Bayesian network. This bound approaches zero exponentially fast as one increases the number of bits used to represent the parameters. This result implies that perfect distributions with fixed-length representations exist. We also provide a lower-bound on the number of bits needed to guarantee that a distribution sampled from a uniform Dirichlet distribution is perfect with probability greater than 1/2. This result is useful for constructing randomized reductions for hardness proofs.

</details>

<details>

<summary>2012-10-19 15:07:12 - On Local Optima in Learning Bayesian Networks</summary>

- *Jens D. Nielsen, Tomas Kocka, Jose M. Pena*

- `1212.2500v1` - [abs](http://arxiv.org/abs/1212.2500v1) - [pdf](http://arxiv.org/pdf/1212.2500v1)

> This paper proposes and evaluates the k-greedy equivalence search algorithm (KES) for learning Bayesian networks (BNs) from complete data. The main characteristic of KES is that it allows a trade-off between greediness and randomness, thus exploring different good local optima. When greediness is set at maximum, KES corresponds to the greedy equivalence search algorithm (GES). When greediness is kept at minimum, we prove that under mild assumptions KES asymptotically returns any inclusion optimal BN with nonzero probability. Experimental results for both synthetic and real data are reported showing that KES often finds a better local optima than GES. Moreover, we use KES to experimentally confirm that the number of different local optima is often huge.

</details>

<details>

<summary>2012-10-19 15:07:23 - Learning Continuous Time Bayesian Networks</summary>

- *Uri Nodelman, Christian R. Shelton, Daphne Koller*

- `1212.2498v1` - [abs](http://arxiv.org/abs/1212.2498v1) - [pdf](http://arxiv.org/pdf/1212.2498v1)

> Continuous time Bayesian networks (CTBNs) describe structured stochastic processes with finitely many states that evolve over continuous time. A CTBN is a directed (possibly cyclic) dependency graph over a set of variables, each of which represents a finite state continuous time Markov process whose transition model is a function of its parents. We address the problem of learning parameters and structure of a CTBN from fully observed data. We define a conjugate prior for CTBNs, and show how it can be used both for Bayesian parameter estimation and as the basis of a Bayesian score for structure learning. Because acyclicity is not a constraint in CTBNs, we can show that the structure learning problem is significantly easier, both in theory and in practice, than structure learning for dynamic Bayesian networks (DBNs). Furthermore, as CTBNs can tailor the parameters and dependency structure to the different time granularities of the evolution of different variables, they can provide a better fit to continuous-time processes than DBNs with a fixed time granularity.

</details>

<details>

<summary>2012-10-19 15:07:51 - Automated Analytic Asymptotic Evaluation of the Marginal Likelihood for Latent Models</summary>

- *Dmitry Rusakov, Dan Geiger*

- `1212.2491v1` - [abs](http://arxiv.org/abs/1212.2491v1) - [pdf](http://arxiv.org/pdf/1212.2491v1)

> We present and implement two algorithms for analytic asymptotic evaluation of the marginal likelihood of data given a Bayesian network with hidden nodes. As shown by previous work, this evaluation is particularly hard for latent Bayesian network models, namely networks that include hidden variables, where asymptotic approximation deviates from the standard BIC score. Our algorithms solve two central difficulties in asymptotic evaluation of marginal likelihood integrals, namely, evaluation of regular dimensionality drop for latent Bayesian network models and computation of non-standard approximation formulas for singular statistics for these models. The presented algorithms are implemented in Matlab and Maple and their usage is demonstrated for marginal likelihood approximations for Bayesian networks with hidden variables.

</details>

<details>

<summary>2012-10-19 15:08:06 - Learning Module Networks</summary>

- *Eran Segal, Dana Pe'er, Aviv Regev, Daphne Koller, Nir Friedman*

- `1212.2517v1` - [abs](http://arxiv.org/abs/1212.2517v1) - [pdf](http://arxiv.org/pdf/1212.2517v1)

> Methods for learning Bayesian network structure can discover dependency structure between observed variables, and have been shown to be useful in many applications. However, in domains that involve a large number of variables, the space of possible network structures is enormous, making it difficult, for both computational and statistical reasons, to identify a good model. In this paper, we consider a solution to this problem, suitable for domains where many variables have similar behavior. Our method is based on a new class of models, which we call module networks. A module network explicitly represents the notion of a module - a set of variables that have the same parents in the network and share the same conditional probability distribution. We define the semantics of module networks, and describe an algorithm that learns a module network from data. The algorithm learns both the partitioning of the variables into modules and the dependency structure between the variables. We evaluate our algorithm on synthetic data, and on real data in the domains of gene expression and the stock market. Our results show that module networks generalize better than Bayesian networks, and that the learned module network structure reveals regularities that are obscured in learned Bayesian networks.

</details>

<details>

<summary>2012-10-19 15:08:38 - Stochastic complexity of Bayesian networks</summary>

- *Keisuke Yamazaki, Sumio Watanbe*

- `1212.2511v1` - [abs](http://arxiv.org/abs/1212.2511v1) - [pdf](http://arxiv.org/pdf/1212.2511v1)

> Bayesian networks are now being used in enormous fields, for example, diagnosis of a system, data mining, clustering and so on. In spite of their wide range of applications, the statistical properties have not yet been clarified, because the models are nonidentifiable and non-regular. In a Bayesian network, the set of its parameter for a smaller model is an analytic set with singularities in the space of large ones. Because of these singularities, the Fisher information matrices are not positive definite. In other words, the mathematical foundation for learning was not constructed. In recent years, however, we have developed a method to analyze non-regular models using algebraic geometry. This method revealed the relation between the models singularities and its statistical properties. In this paper, applying this method to Bayesian networks with latent variables, we clarify the order of the stochastic complexities.Our result claims that the upper bound of those is smaller than the dimension of the parameter space. This means that the Bayesian generalization error is also far smaller than that of regular model, and that Schwarzs model selection criterion BIC needs to be improved for Bayesian networks.

</details>

<details>

<summary>2012-10-19 21:54:46 - Quickest Change Detection</summary>

- *Venugopal V. Veeravalli, Taposh Banerjee*

- `1210.5552v1` - [abs](http://arxiv.org/abs/1210.5552v1) - [pdf](http://arxiv.org/pdf/1210.5552v1)

> The problem of detecting changes in the statistical properties of a stochastic system and time series arises in various branches of science and engineering. It has a wide spectrum of important applications ranging from machine monitoring to biomedical signal processing. In all of these applications the observations being monitored undergo a change in distribution in response to a change or anomaly in the environment, and the goal is to detect the change as quickly as possibly, subject to false alarm constraints. In this chapter, two formulations of the quickest change detection problem, Bayesian and minimax, are introduced, and optimal or asymptotically optimal solutions to these formulations are discussed. Then some generalizations and extensions of the quickest change detection problem are described. The chapter is concluded with a discussion of applications and open issues.

</details>

<details>

<summary>2012-10-23 06:37:11 - A Bayesian method for the analysis of deterministic and stochastic time series</summary>

- *C. A. L. Bailer-Jones*

- `1209.3730v2` - [abs](http://arxiv.org/abs/1209.3730v2) - [pdf](http://arxiv.org/pdf/1209.3730v2)

> I introduce a general, Bayesian method for modelling univariate time series data assumed to be drawn from a continuous, stochastic process. The method accommodates arbitrary temporal sampling, and takes into account measurement uncertainties for arbitrary error models (not just Gaussian) on both the time and signal variables. Any model for the deterministic component of the variation of the signal with time is supported, as is any model of the stochastic component on the signal and time variables. Models illustrated here are constant and sinusoidal models for the signal mean combined with a Gaussian stochastic component, as well as a purely stochastic model, the Ornstein-Uhlenbeck process. The posterior probability distribution over model parameters is determined via Monte Carlo sampling. Models are compared using the "cross-validation likelihood", in which the posterior-averaged likelihood for different partitions of the data are combined. In principle this is more robust to changes in the prior than is the evidence (the prior-averaged likelihood). The method is demonstrated by applying it to the light curves of 11 ultra cool dwarf stars, claimed by a previous study to show statistically significant variability. This is reassessed here by calculating the cross-validation likelihood for various time series models, including a null hypothesis of no variability beyond the error bars. 10 of 11 light curves are confirmed as being significantly variable, and one of these seems to be periodic, with two plausible periods identified. Another object is best described by the Ornstein-Uhlenbeck process, a conclusion which is obviously limited to the set of models actually tested.

</details>

<details>

<summary>2012-10-23 13:41:39 - Measurement errors and scaling relations in astrophysics: a review</summary>

- *S. Andreon, M. A. Hurn*

- `1210.6232v1` - [abs](http://arxiv.org/abs/1210.6232v1) - [pdf](http://arxiv.org/pdf/1210.6232v1)

> This review article considers some of the most common methods used in astronomy for regressing one quantity against another in order to estimate the model parameters or to predict an observationally expensive quantity using trends between object values. These methods have to tackle some of the awkward features prevalent in astronomical data, namely heteroscedastic (point-dependent) errors, intrinsic scatter, non-ignorable data collection and selection effects, data structure and non-uniform population (often called Malmquist bias), non-Gaussian data, outliers and mixtures of regressions. We outline how least square fits, weighted least squares methods, Maximum Likelihood, survival analysis, and Bayesian methods have been applied in the astrophysics literature when one or more of these features is present. In particular we concentrate on errors-in-variables regression and we advocate Bayesian techniques.

</details>

<details>

<summary>2012-10-26 19:33:43 - The anti-Bayesian moment and its passing</summary>

- *Andrew Gelman, Christian P. Robert*

- `1210.7225v1` - [abs](http://arxiv.org/abs/1210.7225v1) - [pdf](http://arxiv.org/pdf/1210.7225v1)

> The present article is the reply to the discussion of our earlier "Not only defended but also applied" (arXiv:1006.5366, to appear in The American Statistician) that arose from our memory of a particularly intemperate anti-Bayesian statement in Feller's beautiful and classic book on probability theory. We felt that it was worth exploring the very extremeness of Feller's words, along with similar anti-Bayesian remarks by others, in order to better understand the background underlying controversies that still exist regarding the foundations of statistics. We thank the four discussants of our article for their contributions to our understanding of these controversies as they have existed in the past and persist today.

</details>

<details>

<summary>2012-10-27 21:01:16 - The Bayesian Bridge</summary>

- *Nicholas G. Polson, James G. Scott, Jesse Windle*

- `1109.2279v2` - [abs](http://arxiv.org/abs/1109.2279v2) - [pdf](http://arxiv.org/pdf/1109.2279v2)

> We propose the Bayesian bridge estimator for regularized regression and classification. Two key mixture representations for the Bayesian bridge model are developed: (1) a scale mixture of normals with respect to an alpha-stable random variable; and (2) a mixture of Bartlett--Fejer kernels (or triangle densities) with respect to a two-component mixture of gamma random variables. Both lead to MCMC methods for posterior simulation, and these methods turn out to have complementary domains of maximum efficiency. The first representation is a well known result due to West (1987), and is the better choice for collinear design matrices. The second representation is new, and is more efficient for orthogonal problems, largely because it avoids the need to deal with exponentially tilted stable random variables. It also provides insight into the multimodality of the joint posterior distribution, a feature of the bridge model that is notably absent under ridge or lasso-type priors. We prove a theorem that extends this representation to a wider class of densities representable as scale mixtures of betas, and provide an explicit inversion formula for the mixing distribution. The connections with slice sampling and scale mixtures of normals are explored. On the practical side, we find that the Bayesian bridge model outperforms its classical cousin in estimation and prediction across a variety of data sets, both simulated and real. We also show that the MCMC for fitting the bridge model exhibits excellent mixing properties, particularly for the global scale parameter. This makes for a favorable contrast with analogous MCMC algorithms for other sparse Bayesian models. All methods described in this paper are implemented in the R package BayesBridge. An extensive set of simulation results are provided in two supplemental files.

</details>

<details>

<summary>2012-10-29 17:42:43 - An introduction to particle integration methods: with applications to risk and insurance</summary>

- *P. Del Moral, G. W. Peters, Ch. Vergé*

- `1210.3851v2` - [abs](http://arxiv.org/abs/1210.3851v2) - [pdf](http://arxiv.org/pdf/1210.3851v2)

> Interacting particle methods are increasingly used to sample from complex and high-dimensional distributions. These stochastic particle integration techniques can be interpreted as an universal acceptance-rejection sequential particle sampler equipped with adaptive and interacting recycling mechanisms. Practically, the particles evolve randomly around the space independently and to each particle is associated a positive potential function. Periodically, particles with high potentials duplicate at the expense of low potential particle which die. This natural genetic type selection scheme appears in numerous applications in applied probability, physics, Bayesian statistics, signal processing, biology, and information engineering. It is the intention of this paper to introduce them to risk modeling. From a purely mathematical point of view, these stochastic samplers can be interpreted as Feynman-Kac particle integration methods. These functional models are natural mathematical extensions of the traditional change of probability measures, commonly used to design an importance sampling strategy. In this article, we provide a brief introduction to the stochastic modeling and the theoretical analysis of these particle algorithms. Then we conclude with an illustration of a subset of such methods to resolve important risk measure and capital estimation in risk and insurance modelling.

</details>

<details>

<summary>2012-10-29 18:20:39 - BEAMS: separating the wheat from the chaff in supernova analysis</summary>

- *Martin Kunz, Renée Hlozek, Bruce A. Bassett, Mathew Smith, James Newling, Melvin Varughese*

- `1210.7762v1` - [abs](http://arxiv.org/abs/1210.7762v1) - [pdf](http://arxiv.org/pdf/1210.7762v1)

> We introduce Bayesian Estimation Applied to Multiple Species (BEAMS), an algorithm designed to deal with parameter estimation when using contaminated data. We present the algorithm and demonstrate how it works with the help of a Gaussian simulation. We then apply it to supernova data from the Sloan Digital Sky Survey (SDSS), showing how the resulting confidence contours of the cosmological parameters shrink significantly.

</details>


## 2012-11

<details>

<summary>2012-11-01 04:22:03 - Bayesian sandwich posteriors for pseudo-true parameters</summary>

- *Peter Hoff, Jon Wakefield*

- `1211.0087v1` - [abs](http://arxiv.org/abs/1211.0087v1) - [pdf](http://arxiv.org/pdf/1211.0087v1)

> Under model misspecification, the MLE generally converges to the pseudo-true parameter, the parameter corresponding to the distribution within the model that is closest to the distribution from which the data are sampled. In many problems, the pseudo-true parameter corresponds to a population parameter of interest, and so a misspecified model can provide consistent estimation for this parameter. Furthermore, the well-known sandwich variance formula of Huber(1967) provides an asymptotically accurate sampling distribution for the MLE, even under model misspecification. However, confidence intervals based on a sandwich variance estimate may behave poorly for low sample sizes, partly due to the use of a plug-in estimate of the variance. From a Bayesian perspective, plug-in estimates of nuisance parameters generally underrepresent uncertainty in the unknown parameters, and averaging over such parameters is expected to give better performance. With this in mind, we present a Bayesian sandwich posterior distribution, whose likelihood is based on the sandwich sampling distribution of the MLE. This Bayesian approach allows for the incorporation of prior information about the parameter of interest, averages over uncertainty in the nuisance parameter and is asymptotically robust to model misspecification. In a small simulation study on estimating a regression parameter under heteroscedasticity, the addition of accurate prior information and the averaging over the nuisance parameter are both seen to improve the accuracy and calibration of confidence intervals for the parameter of interest.

</details>

<details>

<summary>2012-11-01 11:34:08 - The chain rule for functionals with applications to functions of moments</summary>

- *C. S. Withers, S. Nadarajah*

- `1211.0152v1` - [abs](http://arxiv.org/abs/1211.0152v1) - [pdf](http://arxiv.org/pdf/1211.0152v1)

> The chain rule for derivatives of a function of a function is extended to a function of a statistical functional, and applied to obtain approximations to the cumulants, distribution and quantiles of functions of sample moments, and so to obtain third order confidence intervals and estimates of reduced bias for functions of moments. As an example we give the distribution of the standardized skewness for a normal sample to magnitude $O(n^{-2})$, where $n$ is the sample size.

</details>

<details>

<summary>2012-11-01 12:18:40 - A Generalized Polynomial Chaos-Based Method for Efficient Bayesian Calibration of Uncertain Computational Models</summary>

- *Piyush Tagade, Han-Lim Choi*

- `1211.0158v1` - [abs](http://arxiv.org/abs/1211.0158v1) - [pdf](http://arxiv.org/pdf/1211.0158v1)

> This paper addresses the Bayesian calibration of dynamic models with parametric and structural uncertainties, in particular where the uncertain parameters are unknown/poorly known spatio-temporally varying subsystem models. Independent stationary Gaussian processes with uncertain hyper-parameters describe uncertainties of the model structure and parameters while Karhunnen-Loeve expansion is adopted to spectrally represent these Gaussian processes. The Karhunnen-Loeve expansion of a prior Gaussian process is projected on a generalized Polynomial Chaos basis, whereas intrusive Galerkin projection is utilized to calculate the associated coefficients of the simulator output. Bayesian inference is used to update the prior probability distribution of the generalized Polynomial Chaos basis, which along with the chaos expansion coefficients represent the posterior probability distribution. Parameters of the posterior distribution are identified that quantify credibility of the simulator model. The proposed method is demonstrated for calibration of a simulator of quasi-one-dimensional flow through a divergent nozzle.

</details>

<details>

<summary>2012-11-05 17:39:32 - A Framework for Evaluating Approximation Methods for Gaussian Process Regression</summary>

- *Krzysztof Chalupka, Christopher K. I. Williams, Iain Murray*

- `1205.6326v2` - [abs](http://arxiv.org/abs/1205.6326v2) - [pdf](http://arxiv.org/pdf/1205.6326v2)

> Gaussian process (GP) predictors are an important component of many Bayesian approaches to machine learning. However, even a straightforward implementation of Gaussian process regression (GPR) requires O(n^2) space and O(n^3) time for a dataset of n examples. Several approximation methods have been proposed, but there is a lack of understanding of the relative merits of the different approximations, and in what situations they are most useful. We recommend assessing the quality of the predictions obtained as a function of the compute time taken, and comparing to standard baselines (e.g., Subset of Data and FITC). We empirically investigate four different approximation algorithms on four different prediction problems, and make our code available to encourage future comparisons.

</details>

<details>

<summary>2012-11-06 12:18:35 - Needles and Straw in a Haystack: Posterior concentration for possibly sparse sequences</summary>

- *Ismaël Castillo, Aad van der Vaart*

- `1211.1197v1` - [abs](http://arxiv.org/abs/1211.1197v1) - [pdf](http://arxiv.org/pdf/1211.1197v1)

> We consider full Bayesian inference in the multivariate normal mean model in the situation that the mean vector is sparse. The prior distribution on the vector of means is constructed hierarchically by first choosing a collection of nonzero means and next a prior on the nonzero values. We consider the posterior distribution in the frequentist set-up that the observations are generated according to a fixed mean vector, and are interested in the posterior distribution of the number of nonzero components and the contraction of the posterior distribution to the true mean vector. We find various combinations of priors on the number of nonzero coefficients and on these coefficients that give desirable performance. We also find priors that give suboptimal convergence, for instance, Gaussian priors on the nonzero coefficients. We illustrate the results by simulations.

</details>

<details>

<summary>2012-11-06 13:01:48 - Generalized fiducial inference for normal linear mixed models</summary>

- *Jessi Cisewski, Jan Hannig*

- `1211.1208v1` - [abs](http://arxiv.org/abs/1211.1208v1) - [pdf](http://arxiv.org/pdf/1211.1208v1)

> While linear mixed modeling methods are foundational concepts introduced in any statistical education, adequate general methods for interval estimation involving models with more than a few variance components are lacking, especially in the unbalanced setting. Generalized fiducial inference provides a possible framework that accommodates this absence of methodology. Under the fabric of generalized fiducial inference along with sequential Monte Carlo methods, we present an approach for interval estimation for both balanced and unbalanced Gaussian linear mixed models. We compare the proposed method to classical and Bayesian results in the literature in a simulation study of two-fold nested models and two-factor crossed designs with an interaction term. The proposed method is found to be competitive or better when evaluated based on frequentist criteria of empirical coverage and average length of confidence intervals for small sample sizes. A MATLAB implementation of the proposed algorithm is available from the authors.

</details>

<details>

<summary>2012-11-06 21:35:00 - Bayesian Latent Variable Modeling of Longitudinal Family Data for Genetic Pleiotropy Studies</summary>

- *Lizhen Xu, Radu V. Craiu, Lei Sun*

- `1211.1405v1` - [abs](http://arxiv.org/abs/1211.1405v1) - [pdf](http://arxiv.org/pdf/1211.1405v1)

> Motivated by genetic association studies of pleiotropy, we propose here a Bayesian latent variable approach to jointly study multiple outcomes or phenotypes. The proposed method models both continuous and binary phenotypes, and it accounts for serial and familial correlations when longitudinal and pedigree data have been collected. We present a Bayesian estimation method for the model parameters, and we develop a novel MCMC algorithm that builds upon hierarchical centering and parameter expansion techniques to efficiently sample the posterior distribution. We discuss phenotype and model selection in the Bayesian setting, and we study the performance of two selection strategies based on Bayes factors and spike-and-slab priors. We evaluate the proposed method via extensive simulations and demonstrate its utility with an application to a genome-wide association study of various complication phenotypes related to type 1 diabetes.

</details>

<details>

<summary>2012-11-07 14:29:46 - Cosmic Structure and Dynamics of the Local Universe</summary>

- *Francisco-Shu Kitaura, Pirin Erdogdu, Sebastian E. Nuza, Arman Khalatyan, Raul E. Angulo, Yehuda Hoffman, Stefan Gottloeber*

- `1205.5560v2` - [abs](http://arxiv.org/abs/1205.5560v2) - [pdf](http://arxiv.org/pdf/1205.5560v2)

> We present a cosmography analysis of the Local Universe based on the recently released Two-Micron All-Sky Redshift Survey (2MRS). Our method is based on a Bayesian Networks Machine Learning algorithm (the Kigen-code) which self-consistently samples the initial density fluctuations compatible with the observed galaxy distribution and a structure formation model given by second order Lagrangian perturbation theory (2LPT). From the initial conditions we obtain an ensemble of reconstructed density and peculiar velocity fields which characterize the local cosmic structure with high accuracy unveiling nonlinear structures like filaments and voids in detail. Coherent redshift space distortions are consistently corrected within 2LPT. From the ensemble of cross-correlations between the reconstructions and the galaxy field and the variance of the recovered density fields we find that our method is extremely accurate up to k ~ 1 h Mpc^-1 and still yields reliable results down to scales of about 3-4 h^-1 Mpc. The motion of the local group we obtain within ~ 80 h^-1 Mpc (v_LG=522+-86 km s^-1, l_LG=291^o +- 16^o, b_LG=34^o+-8^o) is in good agreement with measurements derived from the CMB and from direct observations of peculiar motions and is consistent with the predictions of LambdaCDM.

</details>

<details>

<summary>2012-11-07 22:51:10 - Bayesian Learning and Predictability in a Stochastic Nonlinear Dynamical Model</summary>

- *John Parslow, Noel Cressie, Edward P. Campbell, Emlyn Jones, Lawrence Murray*

- `1211.1717v1` - [abs](http://arxiv.org/abs/1211.1717v1) - [pdf](http://arxiv.org/pdf/1211.1717v1)

> Bayesian inference methods are applied within a Bayesian hierarchical modelling framework to the problems of joint state and parameter estimation, and of state forecasting. We explore and demonstrate the ideas in the context of a simple nonlinear marine biogeochemical model. A novel approach is proposed to the formulation of the stochastic process model, in which ecophysiological properties of plankton communities are represented by autoregressive stochastic processes. This approach captures the effects of changes in plankton communities over time, and it allows the incorporation of literature metadata on individual species into prior distributions for process model parameters. The approach is applied to a case study at Ocean Station Papa, using Particle Markov chain Monte Carlo computational techniques. The results suggest that, by drawing on objective prior information, it is possible to extract useful information about model state and a subset of parameters, and even to make useful long-term forecasts, based on sparse and noisy observations.

</details>

<details>

<summary>2012-11-08 13:39:25 - Prediction of time series by statistical learning: general losses and fast rates</summary>

- *Pierre Alquier, Xiaoyin Li, Olivier Wintenberger*

- `1211.1847v1` - [abs](http://arxiv.org/abs/1211.1847v1) - [pdf](http://arxiv.org/pdf/1211.1847v1)

> We establish rates of convergences in time series forecasting using the statistical learning approach based on oracle inequalities. A series of papers extends the oracle inequalities obtained for iid observations to time series under weak dependence conditions. Given a family of predictors and $n$ observations, oracle inequalities state that a predictor forecasts the series as well as the best predictor in the family up to a remainder term $\Delta_n$. Using the PAC-Bayesian approach, we establish under weak dependence conditions oracle inequalities with optimal rates of convergence. We extend previous results for the absolute loss function to any Lipschitz loss function with rates $\Delta_n\sim\sqrt{c(\Theta)/ n}$ where $c(\Theta)$ measures the complexity of the model. We apply the method for quantile loss functions to forecast the french GDP. Under additional conditions on the loss functions (satisfied by the quadratic loss function) and on the time series, we refine the rates of convergence to $\Delta_n \sim c(\Theta)/n$. We achieve for the first time these fast rates for uniformly mixing processes. These rates are known to be optimal in the iid case and for individual sequences. In particular, we generalize the results of Dalalyan and Tsybakov on sparse regression estimation to the case of autoregression.

</details>

<details>

<summary>2012-11-08 13:55:50 - Clustering in networks with the collapsed Stochastic Block Model</summary>

- *Aaron F. McDaid, Thomas Brendan Murphy, Nial Friel, Neil J Hurley*

- `1203.3083v4` - [abs](http://arxiv.org/abs/1203.3083v4) - [pdf](http://arxiv.org/pdf/1203.3083v4)

> An efficient MCMC algorithm is presented to cluster the nodes of a network such that nodes with similar role in the network are clustered together. This is known as block-modelling or block-clustering. The model is the stochastic blockmodel (SBM) with block parameters integrated out. The resulting marginal distribution defines a posterior over the number of clusters and cluster memberships. Sampling from this posterior is simpler than from the original SBM as transdimensional MCMC can be avoided. The algorithm is based on the allocation sampler. It requires a prior to be placed on the number of clusters, thereby allowing the number of clusters to be directly estimated by the algorithm, rather than being given as an input parameter. Synthetic and real data are used to test the speed and accuracy of the model and algorithm, including the ability to estimate the number of clusters. The algorithm can scale to networks with up to ten thousand nodes and tens of millions of edges.

</details>

<details>

<summary>2012-11-09 13:25:15 - Adaptive nonparametric Bayesian inference using location-scale mixture priors</summary>

- *R. de Jonge, J. H. van Zanten*

- `1211.2121v1` - [abs](http://arxiv.org/abs/1211.2121v1) - [pdf](http://arxiv.org/pdf/1211.2121v1)

> We study location-scale mixture priors for nonparametric statistical problems, including multivariate regression, density estimation and classification. We show that a rate-adaptive procedure can be obtained if the prior is properly constructed. In particular, we show that adaptation is achieved if a kernel mixture prior on a regression function is constructed using a Gaussian kernel, an inverse gamma bandwidth, and Gaussian mixing weights.

</details>

<details>

<summary>2012-11-09 23:31:47 - Optimal Detection For Sparse Mixtures</summary>

- *T. Tony Cai, Yihong Wu*

- `1211.2265v1` - [abs](http://arxiv.org/abs/1211.2265v1) - [pdf](http://arxiv.org/pdf/1211.2265v1)

> Detection of sparse signals arises in a wide range of modern scientific studies. The focus so far has been mainly on Gaussian mixture models. In this paper, we consider the detection problem under a general sparse mixture model and obtain an explicit expression for the detection boundary. It is shown that the fundamental limits of detection is governed by the behavior of the log-likelihood ratio evaluated at an appropriate quantile of the null distribution. We also establish the adaptive optimality of the higher criticism procedure across all sparse mixtures satisfying certain mild regularity conditions. In particular, the general results obtained in this paper recover and extend in a unified manner the previously known results on sparse detection far beyond the conventional Gaussian model and other exponential families.

</details>

<details>

<summary>2012-11-10 07:37:44 - Probabilistic Combination of Classifier and Cluster Ensembles for Non-transductive Learning</summary>

- *Ayan Acharya, Eduardo R. Hruschka, Joydeep Ghosh, Badrul Sarwar, Jean-David Ruvini*

- `1211.2304v1` - [abs](http://arxiv.org/abs/1211.2304v1) - [pdf](http://arxiv.org/pdf/1211.2304v1)

> Unsupervised models can provide supplementary soft constraints to help classify new target data under the assumption that similar objects in the target set are more likely to share the same class label. Such models can also help detect possible differences between training and target distributions, which is useful in applications where concept drift may take place. This paper describes a Bayesian framework that takes as input class labels from existing classifiers (designed based on labeled data from the source domain), as well as cluster labels from a cluster ensemble operating solely on the target data to be classified, and yields a consensus labeling of the target data. This framework is particularly useful when the statistics of the target data drift or change from those of the training data. We also show that the proposed framework is privacy-aware and allows performing distributed learning when data/models have sharing restrictions. Experiments show that our framework can yield superior results to those provided by applying classifier ensembles only.

</details>

<details>

<summary>2012-11-10 18:09:17 - A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function</summary>

- *Pedro A. Ortega, Jordi Grau-Moya, Tim Genewein, David Balduzzi, Daniel A. Braun*

- `1206.1898v2` - [abs](http://arxiv.org/abs/1206.1898v2) - [pdf](http://arxiv.org/pdf/1206.1898v2)

> We propose a novel Bayesian approach to solve stochastic optimization problems that involve finding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of first, doing inference over the function space and second, finding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior based on a kernel regressor. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function. We illustrate the effectiveness of our model by optimizing a noisy, high-dimensional, non-convex objective function.

</details>

<details>

<summary>2012-11-11 23:09:02 - Random Utility Theory for Social Choice</summary>

- *Hossein Azari Soufiani, David C. Parkes, Lirong Xia*

- `1211.2476v1` - [abs](http://arxiv.org/abs/1211.2476v1) - [pdf](http://arxiv.org/pdf/1211.2476v1)

> Random utility theory models an agent's preferences on alternatives by drawing a real-valued score on each alternative (typically independently) from a parameterized distribution, and then ranking the alternatives according to scores. A special case that has received significant attention is the Plackett-Luce model, for which fast inference methods for maximum likelihood estimators are available. This paper develops conditions on general random utility models that enable fast inference within a Bayesian framework through MC-EM, providing concave loglikelihood functions and bounded sets of global maxima solutions. Results on both real-world and simulated data provide support for the scalability of the approach and capability for model selection among general random utility models including Plackett-Luce.

</details>

<details>

<summary>2012-11-12 11:53:55 - Sequentially interacting Markov chain Monte Carlo methods</summary>

- *Anthony Brockwell, Pierre Del Moral, Arnaud Doucet*

- `1211.2582v1` - [abs](http://arxiv.org/abs/1211.2582v1) - [pdf](http://arxiv.org/pdf/1211.2582v1)

> Sequential Monte Carlo (SMC) is a methodology for sampling approximately from a sequence of probability distributions of increasing dimension and estimating their normalizing constants. We propose here an alternative methodology named Sequentially Interacting Markov Chain Monte Carlo (SIMCMC). SIMCMC methods work by generating interacting non-Markovian sequences which behave asymptotically like independent Metropolis-Hastings (MH) Markov chains with the desired limiting distributions. Contrary to SMC, SIMCMC allows us to iteratively improve our estimates in an MCMC-like fashion. We establish convergence results under realistic verifiable assumptions and demonstrate its performance on several examples arising in Bayesian time series analysis.

</details>

<details>

<summary>2012-11-13 06:47:48 - From Agreement to Asymptotic Learning</summary>

- *Elchanan Mossel, Allan Sly, Omer Tamuz*

- `1105.4765v5` - [abs](http://arxiv.org/abs/1105.4765v5) - [pdf](http://arxiv.org/pdf/1105.4765v5)

> We consider a group of Bayesian agents who are each given an independent signal about an unknown state of the world, and proceed to communicate with each other. We study the question of asymptotic learning: do agents learn the state of the world with probability that approaches one as the number of agents tends to infinity?   We show that under general conditions asymptotic learning follows from agreement on posterior actions or posterior beliefs, regardless of the communication dynamics. In particular, we prove that asymptotic learning holds for the Gale-Kariv model on undirected networks and non-atomic private beliefs.

</details>

<details>

<summary>2012-11-13 14:54:47 - Time-series Scenario Forecasting</summary>

- *Sriharsha Veeramachaneni*

- `1211.3010v1` - [abs](http://arxiv.org/abs/1211.3010v1) - [pdf](http://arxiv.org/pdf/1211.3010v1)

> Many applications require the ability to judge uncertainty of time-series forecasts. Uncertainty is often specified as point-wise error bars around a mean or median forecast. Due to temporal dependencies, such a method obscures some information. We would ideally have a way to query the posterior probability of the entire time-series given the predictive variables, or at a minimum, be able to draw samples from this distribution. We use a Bayesian dictionary learning algorithm to statistically generate an ensemble of forecasts. We show that the algorithm performs as well as a physics-based ensemble method for temperature forecasts for Houston. We conclude that the method shows promise for scenario forecasting where physics-based methods are absent.

</details>

<details>

<summary>2012-11-13 15:55:39 - On the Prior and Posterior Distributions Used in Graphical Modelling</summary>

- *Marco Scutari*

- `1201.4058v2` - [abs](http://arxiv.org/abs/1201.4058v2) - [pdf](http://arxiv.org/pdf/1201.4058v2)

> Graphical model learning and inference are often performed using Bayesian techniques. In particular, learning is usually performed in two separate steps. First, the graph structure is learned from the data; then the parameters of the model are estimated conditional on that graph structure. While the probability distributions involved in this second step have been studied in depth, the ones used in the first step have not been explored in as much detail.   In this paper, we will study the prior and posterior distributions defined over the space of the graph structures for the purpose of learning the structure of a graphical model. In particular, we will provide a characterisation of the behaviour of those distributions as a function of the possible edges of the graph. We will then use the properties resulting from this characterisation to define measures of structural variability for both Bayesian and Markov networks, and we will point out some of their possible applications.

</details>

<details>

<summary>2012-11-13 20:43:20 - A Dynamic Bi-orthogonal Field Equation Approach for Efficient Bayesian Calibration of Large-Scale Systems</summary>

- *Piyush Tagade, Han-Lim Choi*

- `1211.0160v2` - [abs](http://arxiv.org/abs/1211.0160v2) - [pdf](http://arxiv.org/pdf/1211.0160v2)

> This paper proposes a novel computationally efficient dynamic bi-orthogonality based approach for calibration of a computer simulator with high dimensional parametric and model structure uncertainty. The proposed method is based on a decomposition of the solution into mean and a random field using a generic Karhunnen-Loeve expansion. The random field is represented as a convolution of separable Hilbert spaces in stochastic and spacial dimensions that are spectrally represented using respective orthogonal bases. In particular, the present paper investigates generalized polynomial chaos bases for stochastic dimension and eigenfunction bases for spacial dimension. Dynamic orthogonality is used to derive closed form equations for the time evolution of mean, spacial and the stochastic fields. The resultant system of equations consists of a partial differential equation (PDE) that define dynamic evolution of the mean, a set of PDEs to define the time evolution of eigenfunction bases, while a set of ordinary differential equations (ODEs) define dynamics of the stochastic field. This system of dynamic evolution equations efficiently propagates the prior parametric uncertainty to the system response. The resulting bi-orthogonal expansion of the system response is used to reformulate the Bayesian inference for efficient exploration of the posterior distribution. Efficacy of the proposed method is investigated for calibration of a 2D transient diffusion simulator with uncertain source location and diffusivity. Computational efficiency of the method is demonstrated against a Monte Carlo method and a generalized polynomial chaos approach.

</details>

<details>

<summary>2012-11-14 06:36:15 - Effectiveness of sparse Bayesian algorithm for MVAR coefficient estimation in MEG/EEG source-space causality analysis</summary>

- *Kensuke Sekihara, Hagai Attias, Julia P. Owen, Srikantan S. Nagarajan*

- `1211.3211v1` - [abs](http://arxiv.org/abs/1211.3211v1) - [pdf](http://arxiv.org/pdf/1211.3211v1)

> This paper examines the effectiveness of a sparse Bayesian algorithm to estimate multivariate autoregressive coefficients when a large amount of background interference exists. This paper employs computer experiments to compare two methods in the source-space causality analysis: the conventional least-squares method and a sparse Bayesian method. Results of our computer experiments show that the interference affects the least-squares method in a very severe manner. It produces large false-positive results, unless the signal-to-interference ratio is very high. On the other hand, the sparse Bayesian method is relatively insensitive to the existence of interference. However, this robustness of the sparse Bayesian method is attained on the scarifies of the detectability of true causal relationship. Our experiments also show that the surrogate data bootstrapping method tends to give a statistical threshold that are too low for the sparse method.   The permutation-test-based method gives a higher (more conservative) threshold and it should be used with the sparse Bayesian method whenever the control period is available.

</details>

<details>

<summary>2012-11-14 12:05:34 - Mixed Beta Regression: A Bayesian Perspective</summary>

- *Jorge I. Figueroa-Zuñiga, Reinaldo B. Arellano-Valle, Silvia L. P. Ferrari*

- `1201.2375v4` - [abs](http://arxiv.org/abs/1201.2375v4) - [pdf](http://arxiv.org/pdf/1201.2375v4)

> This paper builds on recent research that focuses on regression modeling of continuous bounded data, such as proportions measured on a continuous scale. Specifically, it deals with beta regression models with mixed effects from a Bayesian approach. We use a suitable parameterization of the beta law in terms of its mean and a precision parameter, and allow both parameters to be modeled through regression structures that may involve fixed and random effects. Specification of prior distributions is discussed, computational implementation via Gibbs sampling is provided, and illustrative examples are presented.

</details>

<details>

<summary>2012-11-14 16:03:49 - The relation between frequentist confidence intervals and Bayesian credible intervals</summary>

- *S. I. Bitioukov, N. V. Krasnikov*

- `1211.3343v1` - [abs](http://arxiv.org/abs/1211.3343v1) - [pdf](http://arxiv.org/pdf/1211.3343v1)

> We investigate the relation between frequentist and Bayesian approaches. Namely, we find the "frequentist" Bayes prior \pi_{f}(\lambda,x_{obs}) = -\frac{\int_{-\infty}^{x_{obs}}\frac{\partial f(x,\lambda)}{\partial \lambda}dx}{f(x_{obs},\lambda)} (here f(x,\lambda) is the probability density) for which the results of frequentist and Bayes approaches to the determination of confidence intervals coincide. In many cases (but not always) the "frequentist" prior which reproduces frequentist results coincides with the Jeffreys prior.

</details>

<details>

<summary>2012-11-14 22:30:27 - Polygenic Modeling with Bayesian Sparse Linear Mixed Models</summary>

- *Xiang Zhou, Peter Carbonetto, Matthew Stephens*

- `1209.1341v2` - [abs](http://arxiv.org/abs/1209.1341v2) - [pdf](http://arxiv.org/pdf/1209.1341v2)

> Both linear mixed models (LMMs) and sparse regression models are widely used in genetics applications, including, recently, polygenic modeling in genome-wide association studies. These two approaches make very different assumptions, so are expected to perform well in different situations. However, in practice, for a given data set one typically does not know which assumptions will be more accurate. Motivated by this, we consider a hybrid of the two, which we refer to as a "Bayesian sparse linear mixed model" (BSLMM) that includes both these models as special cases. We address several key computational and statistical issues that arise when applying BSLMM, including appropriate prior specification for the hyper-parameters, and a novel Markov chain Monte Carlo algorithm for posterior inference. We apply BSLMM and compare it with other methods for two polygenic modeling applications: estimating the proportion of variance in phenotypes explained (PVE) by available genotypes, and phenotype (or breeding value) prediction. For PVE estimation, we demonstrate that BSLMM combines the advantages of both standard LMMs and sparse regression modeling. For phenotype prediction it considerably outperforms either of the other two methods, as well as several other large-scale regression methods previously suggested for this problem. Software implementing our method is freely available from http://stephenslab.uchicago.edu/software.html

</details>

<details>

<summary>2012-11-15 13:37:04 - Estimating hyperparameters and instrument parameters in regularized inversion. Illustration for SPIRE/Herschel map making</summary>

- *F. Orieux, J. -F. Giovannelli, T. Rodet, A. Abergel*

- `1211.3603v1` - [abs](http://arxiv.org/abs/1211.3603v1) - [pdf](http://arxiv.org/pdf/1211.3603v1)

> We describe regularized methods for image reconstruction and focus on the question of hyperparameter and instrument parameter estimation, i.e. unsupervised and myopic problems. We developed a Bayesian framework that is based on the \post density for all unknown quantities, given the observations. This density is explored by a Markov Chain Monte-Carlo sampling technique based on a Gibbs loop and including a Metropolis-Hastings step. The numerical evaluation relies on the SPIRE instrument of the Herschel observatory. Using simulated and real observations, we show that the hyperparameters and instrument parameters are correctly estimated, which opens up many perspectives for imaging in astrophysics.

</details>

<details>

<summary>2012-11-15 20:42:21 - Data-Efficient Quickest Change Detection in Minimax Settings</summary>

- *Taposh Banerjee, Venugopal V. Veeravalli*

- `1211.3729v1` - [abs](http://arxiv.org/abs/1211.3729v1) - [pdf](http://arxiv.org/pdf/1211.3729v1)

> The classical problem of quickest change detection is studied with an additional constraint on the cost of observations used in the detection process. The change point is modeled as an unknown constant, and minimax formulations are proposed for the problem. The objective in these formulations is to find a stopping time and an on-off observation control policy for the observation sequence, to minimize a version of the worst possible average delay, subject to constraints on the false alarm rate and the fraction of time observations are taken before change. An algorithm called DE-CuSum is proposed and is shown to be asymptotically optimal for the proposed formulations, as the false alarm rate goes to zero. Numerical results are used to show that the DE-CuSum algorithm has good trade-off curves and performs significantly better than the approach of fractional sampling, in which the observations are skipped using the outcome of a sequence of coin tosses, independent of the observation process. This work is guided by the insights gained from an earlier study of a Bayesian version of this problem.

</details>

<details>

<summary>2012-11-19 07:40:51 - Bayesian nonparametric models for ranked data</summary>

- *Francois Caron, Yee Whye Teh*

- `1211.4321v1` - [abs](http://arxiv.org/abs/1211.4321v1) - [pdf](http://arxiv.org/pdf/1211.4321v1)

> We develop a Bayesian nonparametric extension of the popular Plackett-Luce choice model that can handle an infinite number of choice items. Our framework is based on the theory of random atomic measures, with the prior specified by a gamma process. We derive a posterior characterization and a simple and effective Gibbs sampler for posterior simulation. We develop a time-varying extension of our model, and apply it to the New York Times lists of weekly bestselling books.

</details>

<details>

<summary>2012-11-19 16:24:40 - Computational aspects of Bayesian spectral density estimation</summary>

- *Nicolas Chopin, Judith Rousseau, Brunero Liseo*

- `1211.4483v1` - [abs](http://arxiv.org/abs/1211.4483v1) - [pdf](http://arxiv.org/pdf/1211.4483v1)

> Gaussian time-series models are often specified through their spectral density. Such models present several computational challenges, in particular because of the non-sparse nature of the covariance matrix. We derive a fast approximation of the likelihood for such models. We propose to sample from the approximate posterior (that is, the prior times the approximate likelihood), and then to recover the exact posterior through importance sampling. We show that the variance of the importance sampling weights vanishes as the sample size goes to infinity. We explain why the approximate posterior may typically multi-modal, and we derive a Sequential Monte Carlo sampler based on an annealing sequence in order to sample from that target distribution. Performance of the overall approach is evaluated on simulated and real datasets. In addition, for one real world dataset, we provide some numerical evidence that a Bayesian approach to semi-parametric estimation of spectral density may provide more reasonable results than its Frequentist counter-parts.

</details>

<details>

<summary>2012-11-20 16:29:13 - A survey of non-exchangeable priors for Bayesian nonparametric models</summary>

- *Nicholas J. Foti, Sinead Williamson*

- `1211.4798v1` - [abs](http://arxiv.org/abs/1211.4798v1) - [pdf](http://arxiv.org/pdf/1211.4798v1)

> Dependent nonparametric processes extend distributions over measures, such as the Dirichlet process and the beta process, to give distributions over collections of measures, typically indexed by values in some covariate space. Such models are appropriate priors when exchangeability assumptions do not hold, and instead we want our model to vary fluidly with some set of covariates. Since the concept of dependent nonparametric processes was formalized by MacEachern [1], there have been a number of models proposed and used in the statistics and machine learning literatures. Many of these models exhibit underlying similarities, an understanding of which, we hope, will help in selecting an appropriate prior, developing new models, and leveraging inference techniques.

</details>

<details>

<summary>2012-11-20 16:52:23 - MCMC inference for Markov Jump Processes via the Linear Noise Approximation</summary>

- *Vassilios Stathopoulos, Mark A. Girolami*

- `1211.4801v1` - [abs](http://arxiv.org/abs/1211.4801v1) - [pdf](http://arxiv.org/pdf/1211.4801v1)

> Bayesian analysis for Markov jump processes is a non-trivial and challenging problem. Although exact inference is theoretically possible, it is computationally demanding thus its applicability is limited to a small class of problems. In this paper we describe the application of Riemann manifold MCMC methods using an approximation to the likelihood of the Markov jump process which is valid when the system modelled is near its thermodynamic limit. The proposed approach is both statistically and computationally efficient while the convergence rate and mixing of the chains allows for fast MCMC inference. The methodology is evaluated using numerical simulations on two problems from chemical kinetics and one from systems biology.

</details>

<details>

<summary>2012-11-20 21:50:22 - A Traveling Salesman Learns Bayesian Networks</summary>

- *Tuhin Sahai, Stefan Klus, Michael Dellnitz*

- `1211.4888v1` - [abs](http://arxiv.org/abs/1211.4888v1) - [pdf](http://arxiv.org/pdf/1211.4888v1)

> Structure learning of Bayesian networks is an important problem that arises in numerous machine learning applications. In this work, we present a novel approach for learning the structure of Bayesian networks using the solution of an appropriately constructed traveling salesman problem. In our approach, one computes an optimal ordering (partially ordered set) of random variables using methods for the traveling salesman problem. This ordering significantly reduces the search space for the subsequent greedy optimization that computes the final structure of the Bayesian network. We demonstrate our approach of learning Bayesian networks on real world census and weather datasets. In both cases, we demonstrate that the approach very accurately captures dependencies between random variables. We check the accuracy of the predictions based on independent studies in both application domains.

</details>

<details>

<summary>2012-11-21 09:51:29 - Asymptotic Bayes-optimality under sparsity of some multiple testing procedures</summary>

- *Małgorzata Bogdan, Arijit Chakrabarti, Florian Frommlet, Jayanta K. Ghosh*

- `1002.3501v2` - [abs](http://arxiv.org/abs/1002.3501v2) - [pdf](http://arxiv.org/pdf/1002.3501v2)

> Within a Bayesian decision theoretic framework we investigate some asymptotic optimality properties of a large class of multiple testing rules. A parametric setup is considered, in which observations come from a normal scale mixture model and the total loss is assumed to be the sum of losses for individual tests. Our model can be used for testing point null hypotheses, as well as to distinguish large signals from a multitude of very small effects. A rule is defined to be asymptotically Bayes optimal under sparsity (ABOS), if within our chosen asymptotic framework the ratio of its Bayes risk and that of the Bayes oracle (a rule which minimizes the Bayes risk) converges to one. Our main interest is in the asymptotic scheme where the proportion p of "true" alternatives converges to zero. We fully characterize the class of fixed threshold multiple testing rules which are ABOS, and hence derive conditions for the asymptotic optimality of rules controlling the Bayesian False Discovery Rate (BFDR). We finally provide conditions under which the popular Benjamini-Hochberg (BH) and Bonferroni procedures are ABOS and show that for a wide class of sparsity levels, the threshold of the former can be approximated by a nonrandom threshold.

</details>

<details>

<summary>2012-11-21 12:52:44 - Partition Tree Weighting</summary>

- *Joel Veness, Martha White, Michael Bowling, András György*

- `1211.0587v2` - [abs](http://arxiv.org/abs/1211.0587v2) - [pdf](http://arxiv.org/pdf/1211.0587v2)

> This paper introduces the Partition Tree Weighting technique, an efficient meta-algorithm for piecewise stationary sources. The technique works by performing Bayesian model averaging over a large class of possible partitions of the data into locally stationary segments. It uses a prior, closely related to the Context Tree Weighting technique of Willems, that is well suited to data compression applications. Our technique can be applied to any coding distribution at an additional time and space cost only logarithmic in the sequence length. We provide a competitive analysis of the redundancy of our method, and explore its application in a variety of settings. The order of the redundancy and the complexity of our algorithm matches those of the best competitors available in the literature, and the new algorithm exhibits a superior complexity-performance trade-off in our experiments.

</details>

<details>

<summary>2012-11-22 06:20:34 - A majorization-minimization approach to variable selection using spike and slab priors</summary>

- *Tso-Jung Yen*

- `1005.0891v2` - [abs](http://arxiv.org/abs/1005.0891v2) - [pdf](http://arxiv.org/pdf/1005.0891v2)

> We develop a method to carry out MAP estimation for a class of Bayesian regression models in which coefficients are assigned with Gaussian-based spike and slab priors. The objective function in the corresponding optimization problem has a Lagrangian form in that regression coefficients are regularized by a mixture of squared $l_2$ and $l_0$ norms. A tight approximation to the $l_0$ norm using majorization-minimization techniques is derived, and a coordinate descent algorithm in conjunction with a soft-thresholding scheme is used in searching for the optimizer of the approximate objective. Simulation studies show that the proposed method can lead to more accurate variable selection than other benchmark methods. Theoretical results show that under regular conditions, sign consistency can be established, even when the Irrepresentable Condition is violated. Results on posterior model consistency and estimation consistency, and an extension to parameter estimation in the generalized linear models are provided.

</details>

<details>

<summary>2012-11-23 07:24:05 - Asymptotics for a Bayesian nonparametric estimator of species variety</summary>

- *Stefano Favaro, Antonio Lijoi, Igor Prünster*

- `1211.5422v1` - [abs](http://arxiv.org/abs/1211.5422v1) - [pdf](http://arxiv.org/pdf/1211.5422v1)

> In Bayesian nonparametric inference, random discrete probability measures are commonly used as priors within hierarchical mixture models for density estimation and for inference on the clustering of the data. Recently, it has been shown that they can also be exploited in species sampling problems: indeed they are natural tools for modeling the random proportions of species within a population thus allowing for inference on various quantities of statistical interest. For applications that involve large samples, the exact evaluation of the corresponding estimators becomes impracticable and, therefore, asymptotic approximations are sought. In the present paper, we study the limiting behaviour of the number of new species to be observed from further sampling, conditional on observed data, assuming the observations are exchangeable and directed by a normalized generalized gamma process prior. Such an asymptotic study highlights a connection between the normalized generalized gamma process and the two-parameter Poisson-Dirichlet process that was previously known only in the unconditional case.

</details>

<details>

<summary>2012-11-23 22:28:54 - Pair-copula Bayesian networks</summary>

- *Alexander Bauer, Claudia Czado*

- `1211.5620v1` - [abs](http://arxiv.org/abs/1211.5620v1) - [pdf](http://arxiv.org/pdf/1211.5620v1)

> Pair-copula Bayesian networks (PCBNs) are a novel class of multivariate statistical models, which combine the distributional flexibility of pair-copula constructions (PCCs) with the parsimony of conditional independence models associated with directed acyclic graphs (DAG). We are first to provide generic algorithms for random sampling and likelihood inference in arbitrary PCBNs as well as for selecting orderings of the parents of the vertices in the underlying graphs. Model selection of the DAG is facilitated using a version of the well-known PC algorithm which is based on a novel test for conditional independence of random variables tailored to the PCC framework. A simulation study shows the PC algorithm's high aptitude for structure estimation in non-Gaussian PCBNs. The proposed methods are finally applied to modelling financial return data.

</details>

<details>

<summary>2012-11-24 21:20:12 - Data Augmentation for Hierarchical Capture-recapture Models</summary>

- *J. Andrew Royle, Sarah J. Converse, William A. Link*

- `1211.5706v1` - [abs](http://arxiv.org/abs/1211.5706v1) - [pdf](http://arxiv.org/pdf/1211.5706v1)

> Capture-recapture studies are widely used to obtain information about abundance (population size or density) of animal populations. A common design is that in which multiple distinct populations are sampled, and the research objective is modeling variation in population size $N_{s}; s=1,2,...,S$ among the populations such as estimating a treatment effect or some other source of variation related to landscape structure. The problem is naturally resolved using hierarchical models. We provide a Bayesian formulation of such models using data augmentation which preserves the individual encounter histories in the model and, as such, is amenable to modeling individual effects. We formulate the model by conditioning on the total population size among all populations. In this case, the abundance model can be formulated as a multinomial model that allocates individuals among sites. MCMC is easily carried out by the introduction of a categorical individual effect, $g_{i}$, which partitions the total population size. The prior distribution for the latent variable $g$ is derived from the model assumed for the population sizes $N_{s}$.

</details>

<details>

<summary>2012-11-26 09:55:27 - Bayesian learning of noisy Markov decision processes</summary>

- *Sumeetpal S. Singh, Nicolas Chopin, Nick Whiteley*

- `1211.5901v1` - [abs](http://arxiv.org/abs/1211.5901v1) - [pdf](http://arxiv.org/pdf/1211.5901v1)

> We consider the inverse reinforcement learning problem, that is, the problem of learning from, and then predicting or mimicking a controller based on state/action data. We propose a statistical model for such data, derived from the structure of a Markov decision process. Adopting a Bayesian approach to inference, we show how latent variables of the model can be estimated, and how predictions about actions can be made, in a unified framework. A new Markov chain Monte Carlo (MCMC) sampler is devised for simulation from the posterior distribution. This step includes a parameter expansion step, which is shown to be essential for good convergence properties of the MCMC sampler. As an illustration, the method is applied to learning a human controller.

</details>

<details>

<summary>2012-11-26 21:42:26 - Bayesian Model Robustness via Disparities</summary>

- *Giles Hooker, Anand Vidyashankar*

- `1112.4213v2` - [abs](http://arxiv.org/abs/1112.4213v2) - [pdf](http://arxiv.org/pdf/1112.4213v2)

> This paper develops a methodology for robust Bayesian inference through the use of disparities. Metrics such as Hellinger distance and negative exponential disparity have a long history in robust estimation in frequentist inference. We demonstrate that an equivalent robustification may be made in Bayesian inference by substituting an appropriately scaled disparity for the log likelihood to which standard Monte Carlo Markov Chain methods may be applied. A particularly appealing property of minimum-disparity methods is that while they yield robustness with a breakdown point of 1/2, the resulting parameter estimates are also efficient when the posited probabilistic model is correct. We demonstrate that a similar property holds for disparity-based Bayesian inference. We further show that in the Bayesian setting, it is also possible to extend these methods to robustify regression models, random effects distributions and other hierarchical models. The methods are demonstrated on real world data.

</details>

<details>

<summary>2012-11-27 21:11:41 - A LASSO-Penalized BIC for Mixture Model Selection</summary>

- *Sakyajit Bhattacharya, Paul D. McNicholas*

- `1211.6451v1` - [abs](http://arxiv.org/abs/1211.6451v1) - [pdf](http://arxiv.org/pdf/1211.6451v1)

> The efficacy of family-based approaches to mixture model-based clustering and classification depends on the selection of parsimonious models. Current wisdom suggests the Bayesian information criterion (BIC) for mixture model selection. However, the BIC has well-known limitations, including a tendency to overestimate the number of components as well as a proclivity for, often drastically, underestimating the number of components in higher dimensions. While the former problem might be soluble through merging components, the latter is impossible to mitigate in clustering and classification applications. In this paper, a LASSO-penalized BIC (LPBIC) is introduced to overcome this problem. This approach is illustrated based on applications of extensions of mixtures of factor analyzers, where the LPBIC is used to select both the number of components and the number of latent factors. The LPBIC is shown to match or outperform the BIC in several situations.

</details>

<details>

<summary>2012-11-28 11:26:35 - Optimal scaling and diffusion limits for the Langevin algorithm in high dimensions</summary>

- *Natesh S. Pillai, Andrew M. Stuart, Alexandre H. Thiéry*

- `1103.0542v3` - [abs](http://arxiv.org/abs/1103.0542v3) - [pdf](http://arxiv.org/pdf/1103.0542v3)

> The Metropolis-adjusted Langevin (MALA) algorithm is a sampling algorithm which makes local moves by incorporating information about the gradient of the logarithm of the target density. In this paper we study the efficiency of MALA on a natural class of target measures supported on an infinite dimensional Hilbert space. These natural measures have density with respect to a Gaussian random field measure and arise in many applications such as Bayesian nonparametric statistics and the theory of conditioned diffusions. We prove that, started in stationarity, a suitably interpolated and scaled version of the Markov chain corresponding to MALA converges to an infinite dimensional diffusion process. Our results imply that, in stationarity, the MALA algorithm applied to an N-dimensional approximation of the target will take $\mathcal{O}(N^{1/3})$ steps to explore the invariant measure, comparing favorably with the Random Walk Metropolis which was recently shown to require $\mathcal{O}(N)$ steps when applied to the same class of problems.

</details>

<details>

<summary>2012-11-28 16:50:23 - Nonparametric Bayesian Mixed-effect Model: a Sparse Gaussian Process Approach</summary>

- *Yuyang Wang, Roni Khardon*

- `1211.6653v1` - [abs](http://arxiv.org/abs/1211.6653v1) - [pdf](http://arxiv.org/pdf/1211.6653v1)

> Multi-task learning models using Gaussian processes (GP) have been developed and successfully applied in various applications. The main difficulty with this approach is the computational cost of inference using the union of examples from all tasks. Therefore sparse solutions, that avoid using the entire data directly and instead use a set of informative "representatives" are desirable. The paper investigates this problem for the grouped mixed-effect GP model where each individual response is given by a fixed-effect, taken from one of a set of unknown groups, plus a random individual effect function that captures variations among individuals. Such models have been widely used in previous work but no sparse solutions have been developed. The paper presents the first sparse solution for such problems, showing how the sparse approximation can be obtained by maximizing a variational lower bound on the marginal likelihood, generalizing ideas from single-task Gaussian processes to handle the mixed-effect model as well as grouping. Experiments using artificial and real data validate the approach showing that it can recover the performance of inference with the full sample, that it outperforms baseline methods, and that it outperforms state of the art sparse solutions for other multi-task GP formulations.

</details>

<details>

<summary>2012-11-30 23:34:15 - Simulation-based optimal Bayesian experimental design for nonlinear systems</summary>

- *Xun Huan, Youssef M. Marzouk*

- `1108.4146v3` - [abs](http://arxiv.org/abs/1108.4146v3) - [pdf](http://arxiv.org/pdf/1108.4146v3)

> The optimal selection of experimental conditions is essential to maximizing the value of data for inference and prediction, particularly in situations where experiments are time-consuming and expensive to conduct. We propose a general mathematical framework and an algorithmic approach for optimal experimental design with nonlinear simulation-based models; in particular, we focus on finding sets of experiments that provide the most information about targeted sets of parameters.   Our framework employs a Bayesian statistical setting, which provides a foundation for inference from noisy, indirect, and incomplete data, and a natural mechanism for incorporating heterogeneous sources of information. An objective function is constructed from information theoretic measures, reflecting expected information gain from proposed combinations of experiments. Polynomial chaos approximations and a two-stage Monte Carlo sampling method are used to evaluate the expected information gain. Stochastic approximation algorithms are then used to make optimization feasible in computationally intensive and high-dimensional settings. These algorithms are demonstrated on model problems and on nonlinear parameter estimation problems arising in detailed combustion kinetics.

</details>


## 2012-12

<details>

<summary>2012-12-02 03:13:59 - Stochastic Volatility Regression for Functional Data Dynamics</summary>

- *Bin Zhu, David B. Dunson*

- `1212.0181v1` - [abs](http://arxiv.org/abs/1212.0181v1) - [pdf](http://arxiv.org/pdf/1212.0181v1)

> Although there are many methods for functional data analysis (FDA), little emphasis is put on characterizing variability among volatilities of individual functions. In particular, certain individuals exhibit erratic swings in their trajectory while other individuals have more stable trajectories. There is evidence of such volatility heterogeneity in blood pressure trajectories during pregnancy, for example, and reason to suspect that volatility is a biologically important feature. Most FDA models implicitly assume similar or identical smoothness of the individual functions, and hence can lead to misleading inferences on volatility and an inadequate representation of the functions. We propose a novel class of FDA models characterized using hierarchical stochastic differential equations. We model the derivatives of a mean function and deviation functions using Gaussian processes, while also allowing covariate dependence including on the volatilities of the deviation functions. Following a Bayesian approach to inference, a Markov chain Monte Carlo algorithm is used for posterior computation. The methods are tested on simulated data and applied to blood pressure trajectories during pregnancy.

</details>

<details>

<summary>2012-12-03 17:40:41 - Regional Probabilistic Fertility Forecasting by Modeling Between-Country Correlations</summary>

- *Bailey K. Fosdick, Adrian E. Raftery*

- `1212.0462v1` - [abs](http://arxiv.org/abs/1212.0462v1) - [pdf](http://arxiv.org/pdf/1212.0462v1)

> The United Nations (UN) Population Division is considering producing probabilistic projections for the total fertility rate (TFR) using the Bayesian hierarchical model of Alkema et al. (2011), which produces predictive distributions of TFR for individual countries. The UN is interested in publishing probabilistic projections for aggregates of countries, such as regions and trading blocs. This requires joint probabilistic projections of future country-specific TFRs, taking account of the correlations between them. We propose an extension of the Bayesian hierarchical model that allows for probabilistic projection of TFR for any set of countries. We model the correlation between country forecast errors as a linear function of time invariant covariates, namely whether the countries are contiguous, whether they had a common colonizer after 1945, and whether they are in the same UN region. The resulting correlation model is incorporated into the Bayesian hierarchical model's error distribution. We produce predictive distributions of TFR for 1990-2010 for each of the UN's primary regions. We find that the proportions of the observed values that fall within the prediction intervals from our method are closer to their nominal levels than those produced by the current model. Our results suggest that a significant proportion of the correlation between forecast errors for TFR in different countries is due to countries' geographic proximity to one another, and that if this correlation is accounted for, the quality of probabilitistic projections of TFR for regions and other aggregates is improved.

</details>

<details>

<summary>2012-12-04 15:31:50 - Information Geometry and Sequential Monte Carlo</summary>

- *Aaron Sim, Sarah Filippi, Michael P. H. Stumpf*

- `1212.0764v1` - [abs](http://arxiv.org/abs/1212.0764v1) - [pdf](http://arxiv.org/pdf/1212.0764v1)

> This paper explores the application of methods from information geometry to the sequential Monte Carlo (SMC) sampler. In particular the Riemannian manifold Metropolis-adjusted Langevin algorithm (mMALA) is adapted for the transition kernels in SMC. Similar to its function in Markov chain Monte Carlo methods, the mMALA is a fully adaptable kernel which allows for efficient sampling of high-dimensional and highly correlated parameter spaces. We set up the theoretical framework for its use in SMC with a focus on the application to the problem of sequential Bayesian inference for dynamical systems as modelled by sets of ordinary differential equations. In addition, we argue that defining the sequence of distributions on geodesics optimises the effective sample sizes in the SMC run. We illustrate the application of the methodology by inferring the parameters of simulated Lotka-Volterra and Fitzhugh-Nagumo models. In particular we demonstrate that compared to employing a standard adaptive random walk kernel, the SMC sampler with an information geometric kernel design attains a higher level of statistical robustness in the inferred parameters of the dynamical systems.

</details>

<details>

<summary>2012-12-04 20:51:07 - Estimating the Static Parameters in Linear Gaussian Multiple Target Tracking Models</summary>

- *Sinan Yildirim, Lan Jiang, Sumeetpal S. Singh, Tom Dean*

- `1212.0849v1` - [abs](http://arxiv.org/abs/1212.0849v1) - [pdf](http://arxiv.org/pdf/1212.0849v1)

> We present both offline and online maximum likelihood estimation (MLE) techniques for inferring the static parameters of a multiple target tracking (MTT) model with linear Gaussian dynamics. We present the batch and online versions of the expectation-maximisation (EM) algorithm for short and long data sets respectively, and we show how Monte Carlo approximations of these methods can be implemented. Performance is assessed in numerical examples using simulated data for various scenarios and a comparison with a Bayesian estimation procedure is also provided.

</details>

<details>

<summary>2012-12-05 08:52:33 - Compiling Relational Database Schemata into Probabilistic Graphical Models</summary>

- *Sameer Singh, Thore Graepel*

- `1212.0967v1` - [abs](http://arxiv.org/abs/1212.0967v1) - [pdf](http://arxiv.org/pdf/1212.0967v1)

> Instead of requiring a domain expert to specify the probabilistic dependencies of the data, in this work we present an approach that uses the relational DB schema to automatically construct a Bayesian graphical model for a database. This resulting model contains customized distributions for columns, latent variables that cluster the data, and factors that reflect and represent the foreign key links. Experiments demonstrate the accuracy of the model and the scalability of inference on synthetic and real-world data.

</details>

<details>

<summary>2012-12-05 14:29:56 - A note on marginal posterior simulation via higher-order tail area approximations</summary>

- *Erlis Ruli, Nicola Sartori, Laura Ventura*

- `1212.1038v1` - [abs](http://arxiv.org/abs/1212.1038v1) - [pdf](http://arxiv.org/pdf/1212.1038v1)

> We explore the use of higher-order tail area approximations for Bayesian simulation. These approximations give rise to an alternative simulation scheme to MCMC for Bayesian computation of marginal posterior distributions for a scalar parameter of interest, in the presence of nuisance parameters. Its advantage over MCMC methods is that samples are drawn independently with lower computational time and the implementation requires only standard maximum likelihood routines. The method is illustrated by a genetic linkage model, a normal regression with censored data and a logistic regression model.

</details>

<details>

<summary>2012-12-05 16:06:21 - Functional kernel estimators of conditional extreme quantiles</summary>

- *L. Gardes, S. Girard*

- `1212.1076v1` - [abs](http://arxiv.org/abs/1212.1076v1) - [pdf](http://arxiv.org/pdf/1212.1076v1)

> We address the estimation of "extreme" conditional quantiles i.e. when their order converges to one as the sample size increases. Conditions on the rate of convergence of their order to one are provided to obtain asymptotically Gaussian distributed kernel estimators. A Weissman-type estimator and kernel estimators of the conditional tail-index are derived, permitting to estimate extreme conditional quantiles of arbitrary order.

</details>

<details>

<summary>2012-12-05 21:27:42 - Bayesian computation via empirical likelihood</summary>

- *K. L. Mengersen, P. Pudlo, C. P. Robert*

- `1205.5658v3` - [abs](http://arxiv.org/abs/1205.5658v3) - [pdf](http://arxiv.org/pdf/1205.5658v3)

> Approximate Bayesian computation (ABC) has become an essential tool for the analysis of complex stochastic models when the likelihood function is numerically unavailable. However, the well-established statistical method of empirical likelihood provides another route to such settings that bypasses simulations from the model and the choices of the ABC parameters (summary statistics, distance, tolerance), while being convergent in the number of observations. Furthermore, bypassing model simulations may lead to significant time savings in complex models, for instance those found in population genetics. The BCel algorithm we develop in this paper also provides an evaluation of its own performance through an associated effective sample size. The method is illustrated using several examples, including estimation of standard distributions, time series, and population genetics models.

</details>

<details>

<summary>2012-12-06 21:31:56 - Approximate Bayesian Computation via Regression Density Estimation</summary>

- *Y. Fan, D. J. Nott, S. A. Sisson*

- `1212.1479v1` - [abs](http://arxiv.org/abs/1212.1479v1) - [pdf](http://arxiv.org/pdf/1212.1479v1)

> Approximate Bayesian computation (ABC) methods, which are applicable when the likelihood is difficult or impossible to calculate, are an active topic of current research. Most current ABC algorithms directly approximate the posterior distribution, but an alternative, less common strategy is to approximate the likelihood function. This has several advantages. First, in some problems, it is easier to approximate the likelihood than to approximate the posterior. Second, an approximation to the likelihood allows reference analyses to be constructed based solely on the likelihood. Third, it is straightforward to perform sensitivity analyses for several different choices of prior once an approximation to the likelihood is constructed, which needs to be done only once. The contribution of the present paper is to consider regression density estimation techniques to approximate the likelihood in the ABC setting. Our likelihood approximations build on recently developed marginal adaptation density estimators by extending them for conditional density estimation. Our approach facilitates reference Bayesian inference, as well as frequentist inference. The method is demonstrated via a challenging problem of inference for stereological extremes, where we perform both frequentist and Bayesian inference.

</details>

<details>

<summary>2012-12-07 03:02:51 - Approximate Bayesian computation and Bayes linear analysis: Towards high-dimensional ABC</summary>

- *D. J. Nott, Y. Fan, L. Marshall, S. A. Sisson*

- `1112.4755v2` - [abs](http://arxiv.org/abs/1112.4755v2) - [pdf](http://arxiv.org/pdf/1112.4755v2)

> Bayes linear analysis and approximate Bayesian computation (ABC) are techniques commonly used in the Bayesian analysis of complex models. In this article we connect these ideas by demonstrating that regression-adjustment ABC algorithms produce samples for which first and second order moment summaries approximate adjusted expectation and variance for a Bayes linear analysis. This gives regression-adjustment methods a useful interpretation and role in exploratory analysis in high-dimensional problems. As a result, we propose a new method for combining high-dimensional, regression-adjustment ABC with lower-dimensional approaches (such as using MCMC for ABC). This method first obtains a rough estimate of the joint posterior via regression-adjustment ABC, and then estimates each univariate marginal posterior distribution separately in a lower-dimensional analysis. The marginal distributions of the initial estimate are then modified to equal the separately estimated marginals, thereby providing an improved estimate of the joint posterior. We illustrate this method with several examples. Supplementary materials for this article are available online.

</details>

<details>

<summary>2012-12-07 23:06:28 - Heavy tailed priors: an alternative to non-informative priors in the estimation of proportions on small areas</summary>

- *Jairo Fuquene, Brenda Betancourt*

- `1107.2724v2` - [abs](http://arxiv.org/abs/1107.2724v2) - [pdf](http://arxiv.org/pdf/1107.2724v2)

> We explore the Cauchy and a new heavy tailed (Fuquene, Perez and Pericchi (2011)) priors to estimate proportions on small areas. Hierarchical models and the Binomial likelihood in the exponential family form are used. We believe that the heavy tailed priors in survey sampling settings could be more effective than the choice of noninformative priors to eliminate antipathy towards methods that involve subjective elements or assumptions. To illustrate the robust Bayesian approach, we apply this methodology in a popular example: "the clement problem". Finally, we recommend to use the Cauchy prior in absence or presence of outliers within the small areas and the Fuquene et al. (2011) prior when the outlier is a particular small area.

</details>

<details>

<summary>2012-12-08 11:29:27 - Evaluation of Gaussian approximations for data assimilation in reservoir models</summary>

- *Marco A. Iglesias, Kody J. H. Law, Andrew M. Stuart*

- `1212.1779v1` - [abs](http://arxiv.org/abs/1212.1779v1) - [pdf](http://arxiv.org/pdf/1212.1779v1)

> In this paper we propose to numerically assess the performance of standard Gaussian approximations to probe the posterior distribution that arises from Bayesian data assimilation in petroleum reservoirs. In particular we assess the performance of (i) the linearization around the maximum a posterior estimate, (ii) the randomized maximum likelihood and (iii) standard ensemble Kalman filter-type methods. In order to fully resolve the posterior distribution we implement a state-of-the art MCMC method that scales well with respect to the dimension of the parameter space. Our implementation of the MCMC method provides the gold standard against which to assess the aforementioned Gaussian approximations. We present numerical synthetic experiments where we quantify the capability of each of the {\em ad hoc} Gaussian approximation in reproducing the mean and the variance of the posterior distribution (characterized via MCMC) associated to a data assimilation problem. The main objective of our controlled experiments is to exhibit the substantial discrepancies of the approximation properties of standard {\em ad hoc} Gaussian approximations. Numerical investigations of the type we present here will lead to greater understanding of the cost-efficient, but {\em ad hoc}, Bayesian techniques used for data assimilation in petroleum reservoirs, and hence ultimately to improved techniques with more accurate uncertainty quantification.

</details>

<details>

<summary>2012-12-09 16:36:55 - A Copula Based Bayesian Approach for Paid-Incurred Claims Models for Non-Life Insurance Reserving</summary>

- *Gareth W. Peters, Alice X. D. Dong, Robert Kohn*

- `1210.3849v4` - [abs](http://arxiv.org/abs/1210.3849v4) - [pdf](http://arxiv.org/pdf/1210.3849v4)

> Our article considers the class of recently developed stochastic models that combine claims payments and incurred losses information into a coherent reserving methodology. In particular, we develop a family of Heirarchical Bayesian Paid-Incurred-Claims models, combining the claims reserving models of Hertig et al. (1985) and Gogol et al. (1993). In the process we extend the independent log-normal model of Merz et al. (2010) by incorporating different dependence structures using a Data-Augmented mixture Copula Paid-Incurred claims model.   The utility and influence of incorporating both payment and incurred losses into estimating of the full predictive distribution of the outstanding loss liabilities and the resulting reserves is demonstrated in the following cases: (i) an independent payment (P) data model; (ii) the independent Payment-Incurred Claims (PIC) data model of Merz et al. (2010); (iii) a novel dependent lag-year telescoping block diagonal Gaussian Copula PIC data model incorporating conjugacy via transformation; (iv) a novel data-augmented mixture Archimedean copula dependent PIC data model.   Inference in such models is developed via a class of adaptive Markov chain Monte Carlo sampling algorithms. These incorporate a data-augmentation framework utilized to efficiently evaluate the likelihood for the copula based PIC model in the loss reserving triangles. The adaptation strategy is based on representing a positive definite covariance matrix by the exponential of a symmetric matrix as proposed by Leonard et al. (1992).

</details>

<details>

<summary>2012-12-11 09:03:17 - PAC-Bayesian Learning and Domain Adaptation</summary>

- *Pascal Germain, Amaury Habrard, François Laviolette, Emilie Morvant*

- `1212.2340v1` - [abs](http://arxiv.org/abs/1212.2340v1) - [pdf](http://arxiv.org/pdf/1212.2340v1)

> In machine learning, Domain Adaptation (DA) arises when the distribution gen- erating the test (target) data differs from the one generating the learning (source) data. It is well known that DA is an hard task even under strong assumptions, among which the covariate-shift where the source and target distributions diverge only in their marginals, i.e. they have the same labeling function. Another popular approach is to consider an hypothesis class that moves closer the two distributions while implying a low-error for both tasks. This is a VC-dim approach that restricts the complexity of an hypothesis class in order to get good generalization. Instead, we propose a PAC-Bayesian approach that seeks for suitable weights to be given to each hypothesis in order to build a majority vote. We prove a new DA bound in the PAC-Bayesian context. This leads us to design the first DA-PAC-Bayesian algorithm based on the minimization of the proposed bound. Doing so, we seek for a \rho-weighted majority vote that takes into account a trade-off between three quantities. The first two quantities being, as usual in the PAC-Bayesian approach, (a) the complexity of the majority vote (measured by a Kullback-Leibler divergence) and (b) its empirical risk (measured by the \rho-average errors on the source sample). The third quantity is (c) the capacity of the majority vote to distinguish some structural difference between the source and target samples.

</details>

<details>

<summary>2012-12-11 12:02:43 - Bayesian Estimation of a Gaussian source in Middleton's Class-A Impulsive Noise</summary>

- *Paolo Banelli*

- `1111.6828v2` - [abs](http://arxiv.org/abs/1111.6828v2) - [pdf](http://arxiv.org/pdf/1111.6828v2)

> The paper focuses on minimum mean square error (MMSE) Bayesian estimation for a Gaussian source impaired by additive Middleton's Class-A impulsive noise. In addition to the optimal Bayesian estimator, the paper considers also the soft-limiter and the blanker, which are two popular suboptimal estimators characterized by very low complexity. The MMSE-optimum thresholds for such suboptimal estimators are obtained by practical iterative algorithms with fast convergence. The paper derives also the optimal thresholds according to a maximum-SNR (MSNR) criterion, and establishes connections with the MMSE criterion. Furthermore, closed form analytic expressions are derived for the MSE and the SNR of all the suboptimal estimators, which perfectly match simulation results. Noteworthy, these results can be applied to characterize the receiving performance of any multicarrier system impaired by a Gaussian-mixture noise, such as asymmetric digital subscriber lines (ADSL) and power-line communications (PLC).

</details>

<details>

<summary>2012-12-12 06:27:55 - A Thermodynamical Approach for Probability Estimation</summary>

- *Takashi Isozaki*

- `1201.1384v2` - [abs](http://arxiv.org/abs/1201.1384v2) - [pdf](http://arxiv.org/pdf/1201.1384v2)

> The issue of discrete probability estimation for samples of small size is addressed in this study. The maximum likelihood method often suffers over-fitting when insufficient data is available. Although the Bayesian approach can avoid over-fitting by using prior distributions, it still has problems with objective analysis. In response to these drawbacks, a new theoretical framework based on thermodynamics, where energy and temperature are introduced, was developed. Entropy and likelihood are placed at the center of this method. The key principle of inference for probability mass functions is the minimum free energy, which is shown to unify the two principles of maximum likelihood and maximum entropy. Our method can robustly estimate probability functions from small size data.

</details>

<details>

<summary>2012-12-12 10:55:27 - Bayesian one-mode projection for dynamic bipartite graphs</summary>

- *Ioannis Psorakis, Iead Rezek, Zach Frankel, Stephen J. Roberts*

- `1212.2767v1` - [abs](http://arxiv.org/abs/1212.2767v1) - [pdf](http://arxiv.org/pdf/1212.2767v1)

> We propose a Bayesian methodology for one-mode projecting a bipartite network that is being observed across a series of discrete time steps. The resulting one mode network captures the uncertainty over the presence/absence of each link and provides a probability distribution over its possible weight values. Additionally, the incorporation of prior knowledge over previous states makes the resulting network less sensitive to noise and missing observations that usually take place during the data collection process. The methodology consists of computationally inexpensive update rules and is scalable to large problems, via an appropriate distributed implementation.

</details>

<details>

<summary>2012-12-12 15:55:05 - Learning Hierarchical Object Maps Of Non-Stationary Environments with mobile robots</summary>

- *Dragomir Anguelov, Rahul Biswas, Daphne Koller, Benson Limketkai, Sebastian Thrun*

- `1301.0551v1` - [abs](http://arxiv.org/abs/1301.0551v1) - [pdf](http://arxiv.org/pdf/1301.0551v1)

> Building models, or maps, of robot environments is a highly active research area; however, most existing techniques construct unstructured maps and assume static environments. In this paper, we present an algorithm for learning object models of non-stationary objects found in office-type environments. Our algorithm exploits the fact that many objects found in office environments look alike (e.g., chairs, recycling bins). It does so through a two-level hierarchical representation, which links individual objects with generic shape templates of object classes. We derive an approximate EM algorithm for learning shape parameters at both levels of the hierarchy, using local occupancy grid maps for representing shape. Additionally, we develop a Bayesian model selection algorithm that enables the robot to estimate the total number of objects and object templates in the environment. Experimental results using a real robot equipped with a laser range finder indicate that our approach performs well at learning object-based maps of simple office environments. The approach outperforms a previously developed non-hierarchical algorithm that models objects but lacks class templates.

</details>

<details>

<summary>2012-12-12 15:55:54 - Interpolating Conditional Density Trees</summary>

- *Scott Davies, Andrew Moore*

- `1301.0563v1` - [abs](http://arxiv.org/abs/1301.0563v1) - [pdf](http://arxiv.org/pdf/1301.0563v1)

> Joint distributions over many variables are frequently modeled by decomposing them into products of simpler, lower-dimensional conditional distributions, such as in sparsely connected Bayesian networks. However, automatically learning such models can be very computationally expensive when there are many datapoints and many continuous variables with complex nonlinear relationships, particularly when no good ways of decomposing the joint distribution are known a priori. In such situations, previous research has generally focused on the use of discretization techniques in which each continuous variable has a single discretization that is used throughout the entire network. \ In this paper, we present and compare a wide variety of tree-based algorithms for learning and evaluating conditional density estimates over continuous variables. These trees can be thought of as discretizations that vary according to the particular interactions being modeled; however, the density within a given leaf of the tree need not be assumed constant, and we show that such nonuniform leaf densities lead to more accurate density estimation. We have developed Bayesian network structure-learning algorithms that employ these tree-based conditional density representations, and we show that they can be used to practically learn complex joint probability models over dozens of continuous variables from thousands of datapoints. We focus on finding models that are simultaneously accurate, fast to learn, and fast to evaluate once they are learned.

</details>

<details>

<summary>2012-12-12 15:57:54 - Bayesian Network Classifiers in a High Dimensional Framework</summary>

- *Tatjana Pavlenko, Dietrich von Rosen*

- `1301.0593v1` - [abs](http://arxiv.org/abs/1301.0593v1) - [pdf](http://arxiv.org/pdf/1301.0593v1)

> We present a growing dimension asymptotic formalism. The perspective in this paper is classification theory and we show that it can accommodate probabilistic networks classifiers, including naive Bayes model and its augmented version. When represented as a Bayesian network these classifiers have an important advantage: The corresponding discriminant function turns out to be a specialized case of a generalized additive model, which makes it possible to get closed form expressions for the asymptotic misclassification probabilities used here as a measure of classification accuracy. Moreover, in this paper we propose a new quantity for assessing the discriminative power of a set of features which is then used to elaborate the augmented naive Bayes classifier. The result is a weighted form of the augmented naive Bayes that distributes weights among the sets of features according to their discriminative power. We derive the asymptotic distribution of the sample based discriminative power and show that it is seriously overestimated in a high dimensional case. We then apply this result to find the optimal, in a sense of minimum misclassification probability, type of weighting.

</details>

<details>

<summary>2012-12-12 15:58:38 - Discriminative Probabilistic Models for Relational Data</summary>

- *Ben Taskar, Pieter Abbeel, Daphne Koller*

- `1301.0604v1` - [abs](http://arxiv.org/abs/1301.0604v1) - [pdf](http://arxiv.org/pdf/1301.0604v1)

> In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies.

</details>

<details>

<summary>2012-12-12 15:59:15 - IPF for Discrete Chain Factor Graphs</summary>

- *Wim Wiegerinck, Tom Heskes*

- `1301.0613v1` - [abs](http://arxiv.org/abs/1301.0613v1) - [pdf](http://arxiv.org/pdf/1301.0613v1)

> Iterative Proportional Fitting (IPF), combined with EM, is commonly used as an algorithm for likelihood maximization in undirected graphical models. In this paper, we present two iterative algorithms that generalize upon IPF. The first one is for likelihood maximization in discrete chain factor graphs, which we define as a wide class of discrete variable models including undirected graphical models and Bayesian networks, but also chain graphs and sigmoid belief networks. The second one is for conditional likelihood maximization in standard undirected models and Bayesian networks. In both algorithms, the iteration steps are expressed in closed form. Numerical simulations show that the algorithms are competitive with state of the art methods.

</details>

<details>

<summary>2012-12-12 21:40:23 - Accelerating Inference: towards a full Language, Compiler and Hardware stack</summary>

- *Shawn Hershey, Jeff Bernstein, Bill Bradley, Andrew Schweitzer, Noah Stein, Theo Weber, Ben Vigoda*

- `1212.2991v1` - [abs](http://arxiv.org/abs/1212.2991v1) - [pdf](http://arxiv.org/pdf/1212.2991v1)

> We introduce Dimple, a fully open-source API for probabilistic modeling. Dimple allows the user to specify probabilistic models in the form of graphical models, Bayesian networks, or factor graphs, and performs inference (by automatically deriving an inference engine from a variety of algorithms) on the model. Dimple also serves as a compiler for GP5, a hardware accelerator for inference.

</details>

<details>

<summary>2012-12-14 14:08:58 - Normal Limits, Nonnormal Limits, and the Bootstrap for Quantiles of Dependent Data</summary>

- *O. Sh. Sharipov, M. Wendler*

- `1204.5633v3` - [abs](http://arxiv.org/abs/1204.5633v3) - [pdf](http://arxiv.org/pdf/1204.5633v3)

> We will show under very weak conditions on differentiability and dependence that the central limit theorem for quantiles holds and that the block bootstrap is weakly consistent. Under slightly stronger conditions, the bootstrap is strongly consistent. Without the differentiability condition, quantiles might have a non-normal asymptotic distribution and the bootstrap might fail.

</details>

<details>

<summary>2012-12-15 23:43:47 - A robust Bayesian formulation of the optimal phase measurement problem</summary>

- *K. R. W. Jones*

- `1212.3739v1` - [abs](http://arxiv.org/abs/1212.3739v1) - [pdf](http://arxiv.org/pdf/1212.3739v1)

> Optical phase measurement is a simple example of a quantum--limited measurement problem with important applications in metrology such as gravitational wave detection. The formulation of optimal strategies for such measurements is an important test-bed for the development of robust statistical methods for instrument evaluation. However, the class of possible distributions exhibits extreme pathologies not commonly encountered in conventional statistical analysis. To overcome these difficulties we reformulate the basic variational problem of optimal phase measurement within a Bayesian paradigm and employ the Shannon information as a robust figure of merit. Single-mode performance bounds are discussed, and we invoke a general theorem that reduces the problem of finding the multi-mode performance bounds to the bounding of a single integral, without need of the central limit theorem.

</details>

<details>

<summary>2012-12-17 14:39:41 - Decision-theoretic justifications for Bayesian hypothesis testing using credible sets</summary>

- *Måns Thulin*

- `1210.1066v3` - [abs](http://arxiv.org/abs/1210.1066v3) - [pdf](http://arxiv.org/pdf/1210.1066v3)

> In Bayesian statistics the precise point-null hypothesis $\theta=\theta_0$ can be tested by checking whether $\theta_0$ is contained in a credible set. This permits testing of $\theta=\theta_0$ without having to put prior probabilities on the hypotheses. While such inversions of credible sets have a long history in Bayesian inference, they have been criticised for lacking decision-theoretic justification.   We argue that these tests have many advantages over the standard Bayesian tests that use point-mass probabilities on the null hypothesis. We present a decision-theoretic justification for the inversion of central credible intervals, and in a special case HPD sets, by studying a three-decision problem with directional conclusions. Interpreting the loss function used in the justification, we discuss when test based on credible sets are applicable.   We then give some justifications for using credible sets when testing composite hypotheses, showing that tests based on credible sets coincide with standard tests in this setting.

</details>

<details>

<summary>2012-12-18 13:35:38 - Bayesian Group Nonnegative Matrix Factorization for EEG Analysis</summary>

- *Bonggun Shin, Alice Oh*

- `1212.4347v1` - [abs](http://arxiv.org/abs/1212.4347v1) - [pdf](http://arxiv.org/pdf/1212.4347v1)

> We propose a generative model of a group EEG analysis, based on appropriate kernel assumptions on EEG data. We derive the variational inference update rule using various approximation techniques. The proposed model outperforms the current state-of-the-art algorithms in terms of common pattern extraction. The validity of the proposed model is tested on the BCI competition dataset.

</details>

<details>

<summary>2012-12-18 17:17:41 - Consistency and efficiency of Bayesian estimators in generalised linear inverse problems</summary>

- *Natalia A. Bochkina, Peter J. Green*

- `1110.3015v3` - [abs](http://arxiv.org/abs/1110.3015v3) - [pdf](http://arxiv.org/pdf/1110.3015v3)

> Formulating a statistical inverse problem as one of inference in a Bayesian model has great appeal, notably for what this brings in terms of coherence, the interpretability of regularisation penalties, the integration of all uncertainties, and the principled way in which the set-up can be elaborated to encompass broader features of the context, such as measurement error, indirect observation, etc. The Bayesian formulation comes close to the way that most scientists intuitively regard the inferential task, and in principle allows the free use of subject knowledge in probabilistic model building. However, in some problems where the solution is not unique, for example in ill-posed inverse problems, it is important to understand the relationship between the chosen Bayesian model and the resulting solution.   Taking emission tomography as a canonical example for study, we present results about consistency of the posterior distribution of the reconstruction, and a general method to study convergence of posterior distributions. To study efficiency of Bayesian inference for ill-posed linear inverse problems with constraint, we prove a version of the Bernstein-von Mises theorem for nonregular Bayesian models.

</details>

<details>

<summary>2012-12-19 22:01:33 - On Bayesian credible sets in restricted parameter space problems and lower bounds for frequentist coverage</summary>

- *Eric Marchand, William E. Strawderman*

- `1208.0028v2` - [abs](http://arxiv.org/abs/1208.0028v2) - [pdf](http://arxiv.org/pdf/1208.0028v2)

> For estimating a lower bounded parametric function in the framework of Marchand and Strawderman (2006), we provide through a unified approach a class of Bayesian confidence intervals with credibility $1-\alpha$ and frequentist coverage probability bounded below by $\frac{1-\alpha}{1+\alpha}$. In cases where the underlying pivotal distribution is symmetric, the findings represent extensions with respect to the specification of the credible set achieved through the choice of a {\it spending function}, and include Marchand and Strawderman's HPD procedure result. For non-symmetric cases, the determination of a such a class of Bayesian credible sets fills a gap in the literature and includes an "equal-tails" modification of the HPD procedure. Several examples are presented demonstrating wide applicability.

</details>

<details>

<summary>2012-12-20 15:40:54 - Bayesian analysis of multivariate stochastic volatility with skew distribution</summary>

- *Jouchi Nakajima*

- `1212.5090v1` - [abs](http://arxiv.org/abs/1212.5090v1) - [pdf](http://arxiv.org/pdf/1212.5090v1)

> Multivariate stochastic volatility models with skew distributions are proposed. Exploiting Cholesky stochastic volatility modeling, univariate stochastic volatility processes with leverage effect and generalized hyperbolic skew t-distributions are embedded to multivariate analysis with time-varying correlations. Bayesian prior works allow this approach to provide parsimonious skew structure and to easily scale up for high-dimensional problem. Analyses of daily stock returns are illustrated. Empirical results show that the time-varying correlations and the sparse skew structure contribute to improved prediction performance and VaR forecasts.

</details>

<details>

<summary>2012-12-20 19:37:25 - An Experiment with Hierarchical Bayesian Record Linkage</summary>

- *Michael D. Larsen*

- `1212.5203v1` - [abs](http://arxiv.org/abs/1212.5203v1) - [pdf](http://arxiv.org/pdf/1212.5203v1)

> In record linkage (RL), or exact file matching, the goal is to identify the links between entities with information on two or more files. RL is an important activity in areas including counting the population, enhancing survey frames and data, and conducting epidemiological and follow-up studies. RL is challenging when files are very large, no accurate personal identification (ID) number is present on all files for all units, and some information is recorded with error. Without an unique ID number one must rely on comparisons of names, addresses, dates, and other information to find the links. Latent class models can be used to automatically score the value of information for determining match status. Data for fitting models come from comparisons made within groups of units that pass initial file blocking requirements. Data distributions can vary across blocks. This article examines the use of prior information and hierarchical latent class models in the context of RL.

</details>

<details>

<summary>2012-12-21 11:13:48 - Efficient Gibbs Sampling for Markov Switching GARCH Models</summary>

- *Monica Billio, Roberto Casarin, Anthony Osuntuyi*

- `1212.5397v1` - [abs](http://arxiv.org/abs/1212.5397v1) - [pdf](http://arxiv.org/pdf/1212.5397v1)

> We develop efficient simulation techniques for Bayesian inference on switching GARCH models. Our contribution to existing literature is manifold. First, we discuss different multi-move sampling techniques for Markov Switching (MS) state space models with particular attention to MS-GARCH models. Our multi-move sampling strategy is based on the Forward Filtering Backward Sampling (FFBS) applied to an approximation of MS-GARCH. Another important contribution is the use of multi-point samplers, such as the Multiple-Try Metropolis (MTM) and the Multiple trial Metropolize Independent Sampler, in combination with FFBS for the MS-GARCH process. In this sense we ex- tend to the MS state space models the work of So [2006] on efficient MTM sampler for continuous state space models. Finally, we suggest to further improve the sampler efficiency by introducing the antithetic sampling of Craiu and Meng [2005] and Craiu and Lemieux [2007] within the FFBS. Our simulation experiments on MS-GARCH model show that our multi-point and multi-move strategies allow the sampler to gain efficiency when compared with single-move Gibbs sampling.

</details>

<details>

<summary>2012-12-21 15:58:51 - Bayesian Nonstationary Spatial Modeling for Very Large Datasets</summary>

- *Matthias Katzfuss*

- `1204.2098v5` - [abs](http://arxiv.org/abs/1204.2098v5) - [pdf](http://arxiv.org/pdf/1204.2098v5)

> With the proliferation of modern high-resolution measuring instruments mounted on satellites, planes, ground-based vehicles and monitoring stations, a need has arisen for statistical methods suitable for the analysis of large spatial datasets observed on large spatial domains. Statistical analyses of such datasets provide two main challenges: First, traditional spatial-statistical techniques are often unable to handle large numbers of observations in a computationally feasible way. Second, for large and heterogeneous spatial domains, it is often not appropriate to assume that a process of interest is stationary over the entire domain.   We address the first challenge by using a model combining a low-rank component, which allows for flexible modeling of medium-to-long-range dependence via a set of spatial basis functions, with a tapered remainder component, which allows for modeling of local dependence using a compactly supported covariance function. Addressing the second challenge, we propose two extensions to this model that result in increased flexibility: First, the model is parameterized based on a nonstationary Matern covariance, where the parameters vary smoothly across space. Second, in our fully Bayesian model, all components and parameters are considered random, including the number, locations, and shapes of the basis functions used in the low-rank component.   Using simulated data and a real-world dataset of high-resolution soil measurements, we show that both extensions can result in substantial improvements over the current state-of-the-art.

</details>

<details>

<summary>2012-12-21 20:51:05 - Exponentiated Weibull-Poisson distribution: model, properties and applications</summary>

- *Eisa Mahmoudi, Afsaneh Sepahdar*

- `1212.5586v1` - [abs](http://arxiv.org/abs/1212.5586v1) - [pdf](http://arxiv.org/pdf/1212.5586v1)

> In this paper we propose a new four-parameters distribution with increasing, decreasing, bathtub-shaped and unimodal failure rate, called as the exponentiated Weibull-Poisson (EWP) distribution. The new distribution arises on a latent complementary risk problem base and is obtained by compounding exponentiated Weibull (EW) and Poisson distributions. This distribution contains several lifetime sub-models such as: generalized exponential-Poisson (GEP), complementary Weibull-Poisson (CWP), complementary exponential-Poisson (CEP), exponentiated Rayleigh-Poisson (ERP) and Rayleigh-Poisson (RP) distributions.   We obtain several properties of the new distribution such as its probability density function, its reliability and failure rate functions, quantiles and moments. The maximum likelihood estimation procedure via a EM-algorithm is presented in this paper. Sub-models of the EWP distribution are studied in details. In the end, Applications to two real data sets are given to show the flexibility and potentiality of the new distribution.

</details>

<details>

<summary>2012-12-21 21:21:48 - Exponentiated Weibull Power Series Distributions and its Applications</summary>

- *Eisa Mahmoudi, Mitra Shiran*

- `1212.5613v1` - [abs](http://arxiv.org/abs/1212.5613v1) - [pdf](http://arxiv.org/pdf/1212.5613v1)

> In this paper we introduce the exponentiated Weibull power series (EWPS) class of distributions which is obtained by compounding exponentiated Weibull and power series distributions, where the compounding procedure follows same way that was previously carried out by Roman et al. (2010) and Cancho et al. (2011) in introducing the complementary exponential-geometric (CEG) and the two-parameter Poisson-exponential (PE) lifetime distributions, respectively. This distribution contains several lifetime models such as: exponentiated weibull-geometric (EWG), exponentiated weibull-binomial (EWB), exponentiated weibull-poisson (EWP), exponentiated weibull-logarithmic (EWL) distributions as a special case.   The hazard rate function of the EWPS distribution can be increasing, decreasing, bathtub-shaped and unimodal failure rate among others. We obtain several properties of the EWPS distribution such as its probability density function, its reliability and failure rate functions, quantiles and moments. The maximum likelihood estimation procedure via a EM-algorithm is presented in this paper. Sub-models of the EWPS distribution are studied in details. In the end, Applications to two real data sets are given to show the flexibility and potentiality of the EWPS distribution.

</details>

<details>

<summary>2012-12-21 22:56:22 - Inference for best linear approximations to set identified functions</summary>

- *Arun Chandrasekhar, Victor Chernozhukov, Francesca Molinari, Paul Schrimpf*

- `1212.5627v1` - [abs](http://arxiv.org/abs/1212.5627v1) - [pdf](http://arxiv.org/pdf/1212.5627v1)

> This paper provides inference methods for best linear approximations to functions which are known to lie within a band. It extends the partial identification literature by allowing the upper and lower functions defining the band to be any functions, including ones carrying an index, which can be estimated parametrically or non-parametrically. The identification region of the parameters of the best linear approximation is characterized via its support function, and limit theory is developed for the latter. We prove that the support function approximately converges to a Gaussian process and establish validity of the Bayesian bootstrap. The paper nests as special cases the canonical examples in the literature: mean regression with interval valued outcome data and interval valued regressor data. Because the bounds may carry an index, the paper covers problems beyond mean regression; the framework is extremely versatile. Applications include quantile and distribution regression with interval valued data, sample selection problems, as well as mean, quantile, and distribution treatment effects. Moreover, the framework can account for the availability of instruments. An application is carried out, studying female labor force participation along the lines of Mulligan and Rubinstein (2008).

</details>

<details>

<summary>2012-12-25 22:02:47 - Bayesian shrinkage</summary>

- *Anirban Bhattacharya, Debdeep Pati, Natesh S. Pillai, David B. Dunson*

- `1212.6088v1` - [abs](http://arxiv.org/abs/1212.6088v1) - [pdf](http://arxiv.org/pdf/1212.6088v1)

> Penalized regression methods, such as $L_1$ regularization, are routinely used in high-dimensional applications, and there is a rich literature on optimality properties under sparsity assumptions. In the Bayesian paradigm, sparsity is routinely induced through two-component mixture priors having a probability mass at zero, but such priors encounter daunting computational problems in high dimensions. This has motivated an amazing variety of continuous shrinkage priors, which can be expressed as global-local scale mixtures of Gaussians, facilitating computation. In sharp contrast to the corresponding frequentist literature, very little is known about the properties of such priors. Focusing on a broad class of shrinkage priors, we provide precise results on prior and posterior concentration. Interestingly, we demonstrate that most commonly used shrinkage priors, including the Bayesian Lasso, are suboptimal in high-dimensional settings. A new class of Dirichlet Laplace (DL) priors are proposed, which are optimal and lead to efficient posterior computation exploiting results from normalized random measure theory. Finite sample performance of Dirichlet Laplace priors relative to alternatives is assessed in simulations.

</details>

