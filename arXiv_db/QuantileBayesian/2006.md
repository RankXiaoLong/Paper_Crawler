# 2006

## TOC

- [2006-02](#2006-02)
- [2006-03](#2006-03)
- [2006-05](#2006-05)
- [2006-06](#2006-06)
- [2006-07](#2006-07)
- [2006-08](#2006-08)
- [2006-09](#2006-09)
- [2006-10](#2006-10)
- [2006-11](#2006-11)

## 2006-02

<details>

<summary>2006-02-15 11:50:19 - Consistency of the jackknife-after-bootstrap variance estimator for the bootstrap quantiles of a studentized statistic</summary>

- *S. N. Lahiri*

- `0602328v1` - [abs](http://arxiv.org/abs/0602328v1) - [pdf](http://arxiv.org/pdf/math/0602328v1)

> Efron [J. Roy. Statist. Soc. Ser. B 54 (1992) 83--111] proposed a computationally efficient method, called the jackknife-after-bootstrap, for estimating the variance of a bootstrap estimator for independent data. For dependent data, a version of the jackknife-after-bootstrap method has been recently proposed by Lahiri [Econometric Theory 18 (2002) 79--98]. In this paper it is shown that the jackknife-after-bootstrap estimators of the variance of a bootstrap quantile are consistent for both dependent and independent data. Results from a simulation study are also presented.

</details>


## 2006-03

<details>

<summary>2006-03-06 10:19:07 - Multivariate Bayesian function estimation</summary>

- *Jean-François Angers, Peter T. Kim*

- `0603136v1` - [abs](http://arxiv.org/abs/0603136v1) - [pdf](http://arxiv.org/pdf/math/0603136v1)

> Bayesian methods are developed for the multivariate nonparametric regression problem where the domain is taken to be a compact Riemannian manifold. In terms of the latter, the underlying geometry of the manifold induces certain symmetries on the multivariate nonparametric regression function. The Bayesian approach then allows one to incorporate hierarchical Bayesian methods directly into the spectral structure, thus providing a symmetry-adaptive multivariate Bayesian function estimator. One can also diffuse away some prior information in which the limiting case is a smoothing spline on the manifold. This, together with the result that the smoothing spline solution obtains the minimax rate of convergence in the multivariate nonparametric regression problem, provides good frequentist properties for the Bayes estimators. An application to astronomy is included.

</details>

<details>

<summary>2006-03-09 22:59:21 - Nonparametric Bayesian Classification</summary>

- *Marc A. Coram*

- `0603228v1` - [abs](http://arxiv.org/abs/0603228v1) - [pdf](http://arxiv.org/pdf/math/0603228v1)

> A Bayesian approach to the classification problem is proposed in which random partitions play a central role. It is argued that the partitioning approach has the capacity to take advantage of a variety of large-scale spatial structures, if they are present in the unknown regression function $f_0$. An idealized one-dimensional problem is considered in detail. The proposed nonparametric prior uses random split points to partition the unit interval into a random number of pieces. This prior is found to provide a consistent estimate of the regression function in the $\L^p$ topology, for any $1 \leq p < \infty$, and for arbitrary measurable $f_0:[0,1] \to [0,1]$. A Markov chain Monte Carlo (MCMC) implementation is outlined and analyzed. Simulation experiments are conducted to show that the proposed estimate compares favorably with a variety of conventional estimators. A striking resemblance between the posterior mean estimate and the bagged CART estimate is noted and discussed. For higher dimensions, a generalized prior is introduced which employs a random Voronoi partition of the covariate-space. The resulting estimate displays promise on a two-dimensional problem, and extends with a minimum of additional computational effort to arbitrary metric spaces.

</details>


## 2006-05

<details>

<summary>2006-05-03 07:47:21 - On the Foundations of Universal Sequence Prediction</summary>

- *Marcus Hutter*

- `0605009v1` - [abs](http://arxiv.org/abs/0605009v1) - [pdf](http://arxiv.org/pdf/cs/0605009v1)

> Solomonoff completed the Bayesian framework by providing a rigorous, unique, formal, and universal choice for the model class and the prior. We discuss in breadth how and in which sense universal (non-i.i.d.) sequence prediction solves various (philosophical) problems of traditional Bayesian sequence prediction. We show that Solomonoff's model possesses many desirable properties: Fast convergence and strong bounds, and in contrast to most classical continuous prior densities has no zero p(oste)rior problem, i.e. can confirm universal hypotheses, is reparametrization and regrouping invariant, and avoids the old-evidence and updating problem. It even performs well (actually better) in non-computable environments.

</details>

<details>

<summary>2006-05-10 22:23:11 - Optimal rates in the Bahadur-Kiefer representation for GARCH sequences</summary>

- *Rafal Kulik*

- `0605283v1` - [abs](http://arxiv.org/abs/0605283v1) - [pdf](http://arxiv.org/pdf/math/0605283v1)

> In this paper we establish the Bahadur-Kiefer representation for sample quantiles of GARCH sequences with optimal rates.

</details>

<details>

<summary>2006-05-15 13:54:30 - Consistent estimation of the basic neighborhood of Markov random fields</summary>

- *Imre Csiszár, Zsolt Talata*

- `0605323v2` - [abs](http://arxiv.org/abs/0605323v2) - [pdf](http://arxiv.org/pdf/math/0605323v2)

> For Markov random fields on $\mathbb{Z}^d$ with finite state space, we address the statistical estimation of the basic neighborhood, the smallest region that determines the conditional distribution at a site on the condition that the values at all other sites are given. A modification of the Bayesian Information Criterion, replacing likelihood by pseudo-likelihood, is proved to provide strongly consistent estimation from observing a realization of the field on increasing finite regions: the estimated basic neighborhood equals the true one eventually almost surely, not assuming any prior bound on the size of the latter. Stationarity of the Markov field is not required, and phase transition does not affect the results.

</details>

<details>

<summary>2006-05-16 19:19:58 - Resampling from the past to improve on MCMC algorithms</summary>

- *Yves F. Atchade*

- `0605452v1` - [abs](http://arxiv.org/abs/0605452v1) - [pdf](http://arxiv.org/pdf/math/0605452v1)

> We introduce the idea that resampling from past observations in a Markov Chain Monte Carlo sampler can fasten convergence. We prove that proper resampling from the past does not disturb the limit distribution of the algorithm. We illustrate the method with two examples. The first on a Bayesian analysis of stochastic volatility models and the other on Bayesian phylogeny reconstruction.

</details>

<details>

<summary>2006-05-23 09:09:04 - Poisson calculus for spatial neutral to the right processes</summary>

- *Lancelot F. James*

- `0305053v3` - [abs](http://arxiv.org/abs/0305053v3) - [pdf](http://arxiv.org/pdf/math/0305053v3)

> Neutral to the right (NTR) processes were introduced by Doksum in 1974 as Bayesian priors on the class of distributions on the real line. Since that time there have been numerous applications to models that arise in survival analysis subject to possible right censoring. However, unlike the Dirichlet process, the larger class of NTR processes has not been used in a wider range of more complex statistical applications. Here, to circumvent some of these limitations, we describe a natural extension of NTR processes to arbitrary Polish spaces, which we call spatial neutral to the right processes. Our construction also leads to a new rich class of random probability measures, which we call NTR species sampling models. We show that this class contains the important two parameter extension of the Dirichlet process. We provide a posterior analysis, which yields tractable NTR analogues of the Blackwell--MacQueen distribution. Our analysis turns out to be closely related to the study of regenerative composition structures. A new computational scheme, which is an ordered variant of the general Chinese restaurant processes, is developed. This can be used to approximate complex posterior quantities. We also discuss some relationships to results that appear outside of Bayesian nonparametrics.

</details>

<details>

<summary>2006-05-23 09:43:49 - Nonsubjective priors via predictive relative entropy regret</summary>

- *Trevor J. Sweeting, Gauri S. Datta, Malay Ghosh*

- `0605609v1` - [abs](http://arxiv.org/abs/0605609v1) - [pdf](http://arxiv.org/pdf/math/0605609v1)

> We explore the construction of nonsubjective prior distributions in Bayesian statistics via a posterior predictive relative entropy regret criterion. We carry out a minimax analysis based on a derived asymptotic predictive loss function and show that this approach to prior construction has a number of attractive features. The approach here differs from previous work that uses either prior or posterior relative entropy regret in that we consider predictive performance in relation to alternative nondegenerate prior distributions. The theory is illustrated with an analysis of some specific examples.

</details>

<details>

<summary>2006-05-31 18:12:40 - PAC-Bayesian inductive and transductive learning</summary>

- *Olivier Catoni*

- `0605793v1` - [abs](http://arxiv.org/abs/0605793v1) - [pdf](http://arxiv.org/pdf/math/0605793v1)

> We present here a PAC-Bayesian point of view on adaptive supervised classification. Using convex analysis, we show how to get local measures of the complexity of the classification model involving the relative entropy of posterior distributions with respect to Gibbs posterior measures. We discuss relative bounds, comparing two classification rules, to show how the margin assumption of Mammen and Tsybakov can be replaced with some empirical measure of the covariance structure of the classification model. We also show how to associate to any posterior distribution an {\em effective temperature} relating it to the Gibbs prior distribution with the same level of expected error rate, and how to estimate this effective temperature from data, resulting in an estimator whose expected error rate adaptively converges according to the best possible power of the sample size. Then we introduce a PAC-Bayesian point of view on transductive learning and use it to improve on known Vapnik's generalization bounds, extending them to the case when the sample is independent but not identically distributed. Eventually we review briefly the construction of Support Vector Machines and show how to derive generalization bounds for them, measuring the complexity either through the number of support vectors or through transductive or inductive margin estimates.

</details>


## 2006-06

<details>

<summary>2006-06-03 16:57:32 - Inference in Perturbation Models, Finite Mixtures and Scan Statistics: The Volume-of-Tube Formula</summary>

- *Ramani S. Pilla, Catherine Loader*

- `0511503v2` - [abs](http://arxiv.org/abs/0511503v2) - [pdf](http://arxiv.org/pdf/math/0511503v2)

> This research creates a general class of "perturbation models" which are described by an underlying "null" model that accounts for most of the structure in data and a perturbation that accounts for possible small localized departures. The perturbation models encompass finite mixture models and spatial scan process. In this article, (1) we propose a new test statistic to detect the presence of perturbation, including the case where the null model contains a set of nuisance parameters, and show that it is equivalent to the likelihood ratio test; (2) we establish that the asymptotic distribution of the test statistic is equivalent to the supremum of a Gaussian random field over a high-dimensional manifold (e.g., curve, surface etc.) with boundaries and singularities; (3) we derive a technique for approximating the quantiles of the test statistic using the Hotelling-Weyl-Naiman "volume-of-tube formula"; and (4) we solve the long-pending problem of testing for the order of a mixture model; in particular, derive the asymptotic null distribution for a general family of mixture models including the multivariate mixtures. The inferential theory developed in this article is applicable for a class of non-regular statistical problems involving loss of identifiability or when some of the parameters are on the boundary of the parametric space.

</details>

<details>

<summary>2006-06-13 17:05:02 - Bayesian Regression of Piecewise Constant Functions</summary>

- *Marcus Hutter*

- `0606315v1` - [abs](http://arxiv.org/abs/0606315v1) - [pdf](http://arxiv.org/pdf/math/0606315v1)

> We derive an exact and efficient Bayesian regression algorithm for piecewise constant functions of unknown segment number, boundary location, and levels. It works for any noise and segment level prior, e.g. Cauchy which can handle outliers. We derive simple but good estimates for the in-segment variance. We also propose a Bayesian regression curve as a better way of smoothing data without blurring boundaries. The Bayesian approach also allows straightforward determination of the evidence, break probabilities and error estimates, useful for model selection and significance and robustness studies. We discuss the performance on synthetic and real-world examples. Many possible extensions will be discussed.

</details>

<details>

<summary>2006-06-20 11:04:40 - General Design Bayesian Generalized Linear Mixed Models</summary>

- *Y. Zhao, J. Staudenmayer, B. A. Coull, M. P. Wand*

- `0606491v1` - [abs](http://arxiv.org/abs/0606491v1) - [pdf](http://arxiv.org/pdf/math/0606491v1)

> Linear mixed models are able to handle an extraordinary range of complications in regression-type analyses. Their most common use is to account for within-subject correlation in longitudinal data analysis. They are also the standard vehicle for smoothing spatial count data. However, when treated in full generality, mixed models can also handle spline-type smoothing and closely approximate kriging. This allows for nonparametric regression models (e.g., additive models and varying coefficient models) to be handled within the mixed model framework. The key is to allow the random effects design matrix to have general structure; hence our label general design. For continuous response data, particularly when Gaussianity of the response is reasonably assumed, computation is now quite mature and supported by the R, SAS and S-PLUS packages. Such is not the case for binary and count responses, where generalized linear mixed models (GLMMs) are required, but are hindered by the presence of intractable multivariate integrals. Software known to us supports special cases of the GLMM (e.g., PROC NLMIXED in SAS or glmmML in R) or relies on the sometimes crude Laplace-type approximation of integrals (e.g., the SAS macro glimmix or glmmPQL in R). This paper describes the fitting of general design generalized linear mixed models. A Bayesian approach is taken and Markov chain Monte Carlo (MCMC) is used for estimation and inference. In this generalized setting, MCMC requires sampling from nonstandard distributions. In this article, we demonstrate that the MCMC package WinBUGS facilitates sound fitting of general design Bayesian generalized linear mixed models in practice.

</details>

<details>

<summary>2006-06-22 12:10:37 - Comment on "Sequential Monte Carlo for Bayesian Computation" (P. Del Moral, A. Doucet, A. Jasra)</summary>

- *David R. Bickel*

- `0606557v1` - [abs](http://arxiv.org/abs/0606557v1) - [pdf](http://arxiv.org/pdf/math/0606557v1)

> The main question concerns another recent advance in sequential Monte Carlo, the use of a mixture transition kernel that automatically adapts to the target distribution (Douc et al. 2006). Is there a class of static inference problems for which the backward-kernel approach is better suited, or is it too early to predict which method may have better performance in a particular situation?

</details>


## 2006-07

<details>

<summary>2006-07-01 12:20:29 - Frequentist optimality of Bayesian wavelet shrinkage rules for Gaussian and non-Gaussian noise</summary>

- *Marianna Pensky*

- `0607018v1` - [abs](http://arxiv.org/abs/0607018v1) - [pdf](http://arxiv.org/pdf/math/0607018v1)

> The present paper investigates theoretical performance of various Bayesian wavelet shrinkage rules in a nonparametric regression model with i.i.d. errors which are not necessarily normally distributed. The main purpose is comparison of various Bayesian models in terms of their frequentist asymptotic optimality in Sobolev and Besov spaces. We establish a relationship between hyperparameters, verify that the majority of Bayesian models studied so far achieve theoretical optimality, state which Bayesian models cannot achieve optimal convergence rate and explain why it happens.

</details>

<details>

<summary>2006-07-01 12:41:26 - Shrinkage priors for Bayesian prediction</summary>

- *Fumiyasu Komaki*

- `0607021v1` - [abs](http://arxiv.org/abs/0607021v1) - [pdf](http://arxiv.org/pdf/math/0607021v1)

> We investigate shrinkage priors for constructing Bayesian predictive distributions. It is shown that there exist shrinkage predictive distributions asymptotically dominating Bayesian predictive distributions based on the Jeffreys prior or other vague priors if the model manifold satisfies some differential geometric conditions. Kullback--Leibler divergence from the true distribution to a predictive distribution is adopted as a loss function. Conformal transformations of model manifolds corresponding to vague priors are introduced. We show several examples where shrinkage predictive distributions dominate Bayesian predictive distributions based on vague priors.

</details>

<details>

<summary>2006-07-01 13:16:49 - A Bayes method for a monotone hazard rate via S-paths</summary>

- *Man-Wai Ho*

- `0502432v5` - [abs](http://arxiv.org/abs/0502432v5) - [pdf](http://arxiv.org/pdf/math/0502432v5)

> A class of random hazard rates, which is defined as a mixture of an indicator kernel convolved with a completely random measure, is of interest. We provide an explicit characterization of the posterior distribution of this mixture hazard rate model via a finite mixture of S-paths. A closed and tractable Bayes estimator for the hazard rate is derived to be a finite sum over S-paths. The path characterization or the estimator is proved to be a Rao--Blackwellization of an existing partition characterization or partition-sum estimator. This accentuates the importance of S-paths in Bayesian modeling of monotone hazard rates. An efficient Markov chain Monte Carlo (MCMC) method is proposed to approximate this class of estimates. It is shown that S-path characterization also exists in modeling with covariates by a proportional hazard model, and the proposed algorithm again applies. Numerical results of the method are given to demonstrate its practicality and effectiveness.

</details>

<details>

<summary>2006-07-01 13:37:45 - Misspecification in infinite-dimensional Bayesian statistics</summary>

- *B. J. K. Kleijn, A. W. van der Vaart*

- `0607023v1` - [abs](http://arxiv.org/abs/0607023v1) - [pdf](http://arxiv.org/pdf/math/0607023v1)

> We consider the asymptotic behavior of posterior distributions if the model is misspecified. Given a prior distribution and a random sample from a distribution $P_0$, which may not be in the support of the prior, we show that the posterior concentrates its mass near the points in the support of the prior that minimize the Kullback--Leibler divergence with respect to $P_0$. An entropy condition and a prior-mass condition determine the rate of convergence. The method is applied to several examples, with special interest for infinite-dimensional models. These include Gaussian mixtures, nonparametric regression and parametric models.

</details>

<details>

<summary>2006-07-10 17:04:57 - Closed-Form Bayesian Inferences for the Logit Model via Polynomial Expansions</summary>

- *Steven J. Miller, Eric T. Bradlow, Kevin Dayaratna*

- `0512444v2` - [abs](http://arxiv.org/abs/0512444v2) - [pdf](http://arxiv.org/pdf/math/0512444v2)

> Articles in Marketing and choice literatures have demonstrated the need for incorporating person-level heterogeneity into behavioral models (e.g., logit models for multiple binary outcomes as studied here). However, the logit likelihood extended with a population distribution of heterogeneity doesn't yield closed-form inferences, and therefore numerical integration techniques are relied upon (e.g., MCMC methods).   We present here an alternative, closed-form Bayesian inferences for the logit model, which we obtain by approximating the logit likelihood via a polynomial expansion, and then positing a distribution of heterogeneity from a flexible family that is now conjugate and integrable. For problems where the response coefficients are independent, choosing the Gamma distribution leads to rapidly convergent closed-form expansions; if there are correlations among the coefficients one can still obtain rapidly convergent closed-form expansions by positing a distribution of heterogeneity from a Multivariate Gamma distribution. The solution then comes from the moment generating function of the Multivariate Gamma distribution or in general from the multivariate heterogeneity distribution assumed.   Closed-form Bayesian inferences, derivatives (useful for elasticity calculations), population distribution parameter estimates (useful for summarization) and starting values (useful for complicated algorithms) are hence directly available. Two simulation studies demonstrate the efficacy of our approach.

</details>

<details>

<summary>2006-07-31 14:33:25 - Consistency of Bayes estimators of a binary regression function</summary>

- *Marc Coram, Steven P. Lalley*

- `0412203v3` - [abs](http://arxiv.org/abs/0412203v3) - [pdf](http://arxiv.org/pdf/math/0412203v3)

> When do nonparametric Bayesian procedures ``overfit''? To shed light on this question, we consider a binary regression problem in detail and establish frequentist consistency for a certain class of Bayes procedures based on hierarchical priors, called uniform mixture priors. These are defined as follows: let $\nu$ be any probability distribution on the nonnegative integers. To sample a function $f$ from the prior $\pi^{\nu}$, first sample $m$ from $\nu$ and then sample $f$ uniformly from the set of step functions from $[0,1]$ into $[0,1]$ that have exactly $m$ jumps (i.e., sample all $m$ jump locations and $m+1$ function values independently and uniformly). The main result states that if a data-stream is generated according to any fixed, measurable binary-regression function $f_0\not\equiv1/2$, then frequentist consistency obtains: that is, for any $\nu$ with infinite support, the posterior of $\pi^{\nu}$ concentrates on any $L^1$ neighborhood of $f_0$. Solution of an associated large-deviations problem is central to the consistency proof.

</details>

<details>

<summary>2006-07-31 15:19:39 - Closed form expressions for Bayesian sample size</summary>

- *B. Clarke, Ao Yuan*

- `0607818v1` - [abs](http://arxiv.org/abs/0607818v1) - [pdf](http://arxiv.org/pdf/math/0607818v1)

> Sample size criteria are often expressed in terms of the concentration of the posterior density, as controlled by some sort of error bound. Since this is done pre-experimentally, one can regard the posterior density as a function of the data. Thus, when a sample size criterion is formalized in terms of a functional of the posterior, its value is a random variable. Generally, such functionals have means under the true distribution. We give asymptotic expressions for the expected value, under a fixed parameter, for certain types of functionals of the posterior density in a Bayesian analysis. The generality of our treatment permits us to choose functionals that encapsulate a variety of inference criteria and large ranges of error bounds. Consequently, we get simple inequalities which can be solved to give minimal sample sizes needed for various estimation goals. In several parametric examples, we verify that our asymptotic bounds give good approximations to the expected values of the functionals they approximate. Also, our numerical computations suggest our treatment gives reasonable results.

</details>


## 2006-08

<details>

<summary>2006-08-01 14:27:09 - Bayesian analysis for reversible Markov chains</summary>

- *Persi Diaconis, Silke W. W. Rolles*

- `0605582v2` - [abs](http://arxiv.org/abs/0605582v2) - [pdf](http://arxiv.org/pdf/math/0605582v2)

> We introduce a natural conjugate prior for the transition matrix of a reversible Markov chain. This allows estimation and testing. The prior arises from random walk with reinforcement in the same way the Dirichlet prior arises from P\'{o}lya's urn. We give closed form normalizing constants, a simple method of simulation from the posterior and a characterization along the lines of W. E. Johnson's characterization of the Dirichlet prior.

</details>


## 2006-09

<details>

<summary>2006-09-02 04:31:41 - Bayesian Nonparametric Estimation of a Unimodal Density via two $\mathbf{S}$-paths</summary>

- *Man-Wai Ho*

- `0609056v1` - [abs](http://arxiv.org/abs/0609056v1) - [pdf](http://arxiv.org/pdf/math/0609056v1)

> A Bayesian nonparametric method for unimodal densities on the real line is provided by considering a class of species sampling mixture models containing random densities that are unimodal and not necessarily symmetric. This class of densities generalize the model considered by Brunner (1992, Statist. Probab. Lett.), in which the Dirichlet process is replaced by a more general class of species sampling models. A novel and explicit characterization of the posterior distribution via a finite mixture of two dependent $\mathbf{S}$-paths is derived. This results in a closed-form and tractable Bayes estimator for any unimodal density in terms of a finite sum over two $\mathbf{S}$-paths. To approximate this class of estimates, we propose a sequential importance sampling algorithm that exploits the idea of the accelerated path sampler, an efficient path-sampling Markov chain Monte Carlo method. Numerical simulations are given to demonstrate the practicality and the effectiveness of our methodology.

</details>

<details>

<summary>2006-09-04 14:38:53 - The conjugate prior for discrete hierarchical log-linear models</summary>

- *Jinnan Liu, Helene Massam*

- `0609100v1` - [abs](http://arxiv.org/abs/0609100v1) - [pdf](http://arxiv.org/pdf/math/0609100v1)

> In the Bayesian analysis of contingency table data, the selection of a prior distribution for either the log-linear parameters or the cell probabilities parameter is a major challenge. Though the conjugate prior on cell probabilities has been defined by Dawid and Lauritzen (1993) for decomposable graphical models, it has not been identified for the larger class of graphical models Markov with respect to an arbitrary undirected graph or for the even wider class of hierarchical log-linear models. In this paper, working with the log-linear parameters used by GLIM, we first define the conjugate prior for these parameters and then derive the induced prior for the cell probabilities: this is done for the general class of hierarchical log-linear models. We show that the conjugate prior has all the properties that one expects from a prior: notational simplicity, ability to reflect either no prior knowledge or a priori expert knowledge, a moderate number of hyperparameters and mathematical convenience. It also has the strong hyper Markov property which allows for local updates within prime components for graphical models.

</details>

<details>

<summary>2006-09-16 20:19:45 - Asymptotic Optimality in Bayesian Change-Point Detection Problems Under Global False Alarm Probability Constraint</summary>

- *Alexander G. Tartakovsky*

- `0609467v1` - [abs](http://arxiv.org/abs/0609467v1) - [pdf](http://arxiv.org/pdf/math/0609467v1)

> In 1960s Shiryaev developed Bayesian theory of change detection in independent and identically distributed (i.i.d.) sequences. In Shiryaev's classical setting the goal is to minimize an average detection delay under the constraint imposed on the average probability of false alarm. Recently, Tartakovsky and Veeravalli (2005) developed a general Bayesian asymptotic change-point detection theory (in the classical setting) that is not limited to a restrictive i.i.d. assumption. It was proved that Shiryaev's detection procedure is asymptotically optimal under traditional average false alarm probability constraint, assuming that this probability is small. In the present paper, we consider a less conventional approach where the constraint is imposed on the global, supremum false alarm probability. An asymptotically optimal Bayesian change detection procedure is proposed and thoroughly evaluated for both i.i.d. and non-i.i.d. models when the global false alarm probability approaches zero.

</details>


## 2006-10

<details>

<summary>2006-10-06 21:00:20 - Bounds on the number of inference functions of a graphical model</summary>

- *Sergi Elizalde, Kevin Woods*

- `0610233v1` - [abs](http://arxiv.org/abs/0610233v1) - [pdf](http://arxiv.org/pdf/math/0610233v1)

> Directed and undirected graphical models, also called Bayesian networks and Markov random fields, respectively, are important statistical tools in a wide variety of fields, ranging from computational biology to probabilistic artificial intelligence. We give an upper bound on the number of inference functions of any graphical model. This bound is polynomial on the size of the model, for a fixed number of parameters, thus improving the exponential upper bound given by Pachter and Sturmfels. We also show that our bound is tight up to a constant factor, by constructing a family of hidden Markov models whose number of inference functions agrees asymptotically with the upper bound. Finally, we apply this bound to a model for sequence alignment that is used in computational biology.

</details>

<details>

<summary>2006-10-22 03:38:20 - Bayesian Clustering of Transcription Factor Binding Motifs</summary>

- *Shane T. Jensen, Jun S. Liu*

- `0610655v1` - [abs](http://arxiv.org/abs/0610655v1) - [pdf](http://arxiv.org/pdf/math/0610655v1)

> Genes are often regulated in living cells by proteins called transcription factors (TFs) that bind directly to short segments of DNA in close proximity to specific genes. These binding sites have a conserved nucleotide appearance, which is called a motif. Several recent studies of transcriptional regulation require the reduction of a large collection of motifs into clusters based on the similarity of their nucleotide composition. We present a principled approach to this clustering problem based upon a Bayesian hierarchical model that accounts for both within- and between-motif variability. We use a Dirichlet process prior distribution that allows the number of clusters to vary and we also present a novel generalization that allows the core width of each motif to vary. This clustering model is implemented, using a Gibbs sampling strategy, on several collections of transcription factor motif matrices. Our clusters provide a means by which to organize transcription factors based on binding motif similarities, which can be used to reduce motif redundancy within large databases such as JASPAR and TRANSFAC. Finally, our clustering procedure has been used in combination with discovery of evolutionarily-conserved motifs to predict co-regulated genes. An alternative to our Dirichlet process prior distribution is explored but shows no substantive difference in the clustering results for our datasets. Our Bayesian clustering model based on the Dirichlet process has several advantages over traditional clustering methods that could make our procedure appropriate and useful for many clustering applications.

</details>

<details>

<summary>2006-10-27 11:38:24 - Student's $t$-test for scale mixture errors</summary>

- *Gábor J. Székely*

- `0610838v1` - [abs](http://arxiv.org/abs/0610838v1) - [pdf](http://arxiv.org/pdf/math/0610838v1)

> Generalized t-tests are constructed under weaker than normal conditions. In the first part of this paper we assume only the symmetry (around zero) of the error distribution (i). In the second part we assume that the error distribution is a Gaussian scale mixture (ii). The optimal (smallest) critical values can be computed from generalizations of Student's cumulative distribution function (cdf), $t_n(x)$. The cdf's of the generalized $t$-test statistics are denoted by (i) $t_n^S(x)$ and (ii) $t_n^G(x)$, resp. As the sample size $n\to \infty$ we get the counterparts of the standard normal cdf $\Phi(x)$: (i) $\Phi^S(x):=\operatorname {lim}_{n\to \infty}t_n^S(x)$, and (ii) $\Phi^G(x):=\operatorname {lim}_{n\to \infty}t_n^G(x)$. Explicit formulae are given for the underlying new cdf's. For example $\Phi^G(x)=\Phi(x)$ iff $|x|\ge \sqrt{3}$. Thus the classical 95% confidence interval for the unknown expected value of Gaussian distributions covers the center of symmetry with at least 95% probability for Gaussian scale mixture distributions. On the other hand, the 90% quantile of $\Phi^G$ is $4\sqrt{3}/5=1.385... >\Phi^{-1}(0.9)=1.282...$.

</details>

<details>

<summary>2006-10-27 15:20:29 - Modeling inequality and spread in multiple regression</summary>

- *Rolf Aaberge, Steinar Bjerve, Kjell Doksum*

- `0610852v1` - [abs](http://arxiv.org/abs/0610852v1) - [pdf](http://arxiv.org/pdf/math/0610852v1)

> We consider concepts and models for measuring inequality in the distribution of resources with a focus on how inequality varies as a function of covariates. Lorenz introduced a device for measuring inequality in the distribution of income that indicates how much the incomes below the u$^{th}$ quantile fall short of the egalitarian situation where everyone has the same income. Gini introduced a summary measure of inequality that is the average over u of the difference between the Lorenz curve and its values in the egalitarian case. More generally, measures of inequality are useful for other response variables in addition to income, e.g. wealth, sales, dividends, taxes, market share and test scores. In this paper we show that a generalized van Zwet type dispersion ordering for distributions of positive random variables induces an ordering on the Lorenz curve, the Gini coefficient and other measures of inequality. We use this result and distributional orderings based on transformations of distributions to motivate parametric and semiparametric models whose regression coefficients measure effects of covariates on inequality. In particular, we extend a parametric Pareto regression model to a flexible semiparametric regression model and give partial likelihood estimates of the regression coefficients and a baseline distribution that can be used to construct estimates of the various conditional measures of inequality.

</details>


## 2006-11

<details>

<summary>2006-11-02 09:31:30 - Estimation of the Location of a 0-type or $\infty$-type Singularity by Poisson Observations</summary>

- *Serguei Dachian*

- `0611043v1` - [abs](http://arxiv.org/abs/0611043v1) - [pdf](http://arxiv.org/pdf/math/0611043v1)

> We consider an inhomogeneous Poisson process $X$ on $[0,T]$. The intensity function of $X$ is supposed to be strictly positive and smooth on $[0,T]$ except at the point $\theta$, in which it has either a 0-type singularity (tends to 0 like $\abs{x}^p$, $p\in(0,1)$), or an $\infty$-type singularity (tends to $\infty$ like $\abs{x}^p$, $p\in(-1,0)$). We suppose that we know the shape of the intensity function, but not the location of the singularity. We consider the problem of estimation of this location (shift) parameter $\theta$ based on $n$ observations of the process $X$. We study the Bayesian estimators and, in the case $p>0$, the maximum likelihood estimator. We show that these estimators are consistent, their rate of convergence is $n^{1/(p+1)}$, they have different limit distributions, and the Bayesian estimators are asymptotically efficient.

</details>

<details>

<summary>2006-11-07 07:52:08 - Bayesian transformation hazard models</summary>

- *Gousheng Yin, Joseph G. Ibrahim*

- `0611164v1` - [abs](http://arxiv.org/abs/0611164v1) - [pdf](http://arxiv.org/pdf/math/0611164v1)

> We propose a class of transformation hazard models for right-censored failure time data. It includes the proportional hazards model (Cox) and the additive hazards model (Lin and Ying) as special cases. Due to the requirement of a nonnegative hazard function, multidimensional parameter constraints must be imposed in the model formulation. In the Bayesian paradigm, the nonlinear parameter constraint introduces many new computational challenges. We propose a prior through a conditional-marginal specification, in which the conditional distribution is univariate, and absorbs all of the nonlinear parameter constraints. The marginal part of the prior specification is free of any constraints. This class of prior distributions allows us to easily compute the full conditionals needed for Gibbs sampling, and hence implement the Markov chain Monte Carlo algorithm in a relatively straightforward fashion. Model comparison is based on the conditional predictive ordinate and the deviance information criterion. This new class of models is illustrated with a simulation study and a real dataset from a melanoma clinical trial.

</details>

<details>

<summary>2006-11-08 14:26:11 - The Bernstein--von Mises theorem for the proportional hazard model</summary>

- *Yongdai Kim*

- `0611230v1` - [abs](http://arxiv.org/abs/0611230v1) - [pdf](http://arxiv.org/pdf/math/0611230v1)

> We study large sample properties of Bayesian analysis of the proportional hazard model with neutral to the right process priors on the baseline hazard function. We show that the posterior distribution of the baseline cumulative hazard function and regression coefficients centered at the maximum likelihood estimator is jointly asymptotically equivalent to the sampling distribution of the maximum likelihood estimator.

</details>

<details>

<summary>2006-11-09 14:51:23 - Confidence regions for high quantiles of a heavy tailed distribution</summary>

- *Liang Peng, Yongcheng Qi*

- `0611278v1` - [abs](http://arxiv.org/abs/0611278v1) - [pdf](http://arxiv.org/pdf/math/0611278v1)

> Estimating high quantiles plays an important role in the context of risk management. This involves extrapolation of an unknown distribution function. In this paper we propose three methods, namely, the normal approximation method, the likelihood ratio method and the data tilting method, to construct confidence regions for high quantiles of a heavy tailed distribution. A simulation study prefers the data tilting method.

</details>

<details>

<summary>2006-11-14 11:27:49 - Computing Bayesian predictive distributions: The K-square and K-prime distributions</summary>

- *Jacques Poitevineau, Bruno Lecoutre*

- `0611419v1` - [abs](http://arxiv.org/abs/0611419v1) - [pdf](http://arxiv.org/pdf/math/0611419v1)

> The computation of two Bayesian predictive distributions which are discrete mixtures of incomplete beta functions is considered. The number of iterations can easily become large for these distributions and thus, the accuracy of the result can be questionable. Therefore, existing algorithms for that class of mixtures are improved by introducing round-off error calculation into the stopping rule. A further simple modification is proposed to deal with possible underflows that may prevent recurrence to work properly.

</details>

<details>

<summary>2006-11-15 18:54:10 - An Auto-validating Rejection Sampler</summary>

- *Raazesh Sainudiin, Thomas L. York*

- `0611479v1` - [abs](http://arxiv.org/abs/0611479v1) - [pdf](http://arxiv.org/pdf/math/0611479v1)

> In Bayesian statistical inference and computationally intensive frequentist inference, one is interested in obtaining samples from a high dimensional, and possibly multi-modal target density. The challenge is to obtain samples from this target without any knowledge of the normalizing constant. Several approaches to this problem rely on Monte Carlo methods. One of the simplest such methods is the rejection sampler due to von Neumann. Here we introduce an auto-validating version of the rejection sampler via interval analysis. We show that our rejection sampler does provide us with independent samples from a large class of target densities in a guaranteed manner. We illustrate the efficiency of the sampler by theory and by examples in up to 10 dimensions. Our sampler is immune to the `pathologies' of some infamous densities including the witch's hat and can rigorously draw samples from piece-wise Euclidean spaces of small phylogenetic trees.

</details>

<details>

<summary>2006-11-21 19:49:16 - Linear and quadratic functionals of random hazard rates: an asymptotic analysis</summary>

- *Giovanni Peccati, Igor Prünster*

- `0611652v1` - [abs](http://arxiv.org/abs/0611652v1) - [pdf](http://arxiv.org/pdf/math/0611652v1)

> A popular Bayesian nonparametric approach to survival analysis consists in modeling hazard rates as kernel mixtures driven by a completely random measure. In this paper we derive asymptotic results for linear and quadratic functionals of such random hazard rates. In particular, we prove central limit theorems for the cumulative hazard function and for the path-second moment and path-variance of the hazard rate. Our techniques are based on recently established criteria for the weak convergence of single and double stochastic integrals with respect to Poisson random measures. We illustrate our results by considering specific models involving kernels and random measures commonly exploited in practice.

</details>

<details>

<summary>2006-11-22 10:06:45 - Group invariant inferred distributions via noncommutative probability</summary>

- *B. Heller, M. Wang*

- `0611675v1` - [abs](http://arxiv.org/abs/0611675v1) - [pdf](http://arxiv.org/pdf/math/0611675v1)

> One may consider three types of statistical inference: Bayesian, frequentist, and group invariance-based. The focus here is on the last method. We consider the Poisson and binomial distributions in detail to illustrate a group invariance method for constructing inferred distributions on parameter spaces given observed results. These inferred distributions are obtained without using Bayes' method and in particular without using a joint distribution of random variable and parameter. In the Poisson and binomial cases, the final formulas for inferred distributions coincide with the formulas for Bayes posteriors with uniform priors.

</details>

<details>

<summary>2006-11-22 11:09:40 - On the behavior of Bayesian credible intervals for some restricted parameter space problems</summary>

- *Éric Marchand, William E. Strawderman*

- `0611684v1` - [abs](http://arxiv.org/abs/0611684v1) - [pdf](http://arxiv.org/pdf/math/0611684v1)

> For estimating a positive normal mean, Zhang and Woodroofe (2003) as well as Roe and Woodroofe (2000) investigate 100($1-\alpha)%$ HPD credible sets associated with priors obtained as the truncation of noninformative priors onto the restricted parameter space. Namely, they establish the attractive lower bound of $\frac{1-\alpha}{1+\alpha}$ for the frequentist coverage probability of these procedures. In this work, we establish that the lower bound of $\frac{1-\alpha}{1+\alpha}$ is applicable for a substantially more general setting with underlying distributional symmetry, and obtain various other properties. The derivations are unified and are driven by the choice of a right Haar invariant prior. Investigations of non-symmetric models are carried out and similar results are obtained. Namely, (i) we show that the lower bound $\frac{1-\alpha}{1+\alpha}$ still applies for certain types of asymmetry (or skewness), and (ii) we extend results obtained by Zhang and Woodroofe (2002) for estimating the scale parameter of a Fisher distribution; which arises in estimating the ratio of variance components in a one-way balanced random effects ANOVA. Finally, various examples illustrating the wide scope of applications are expanded upon. Examples include estimating parameters in location models and location-scale models, estimating scale parameters in scale models, estimating linear combinations of location parameters such as differences, estimating ratios of scale parameters, and problems with non-independent observations.

</details>

<details>

<summary>2006-11-22 14:05:54 - On the false discovery rates of a frequentist: Asymptotic expansions</summary>

- *Anirban DasGupta, Tonglin Zhang*

- `0611671v1` - [abs](http://arxiv.org/abs/0611671v1) - [pdf](http://arxiv.org/pdf/math/0611671v1)

> Consider a testing problem for the null hypothesis $H_0:\theta\in\Theta_0$. The standard frequentist practice is to reject the null hypothesis when the p-value is smaller than a threshold value $\alpha$, usually 0.05. We ask the question how many of the null hypotheses a frequentist rejects are actually true. Precisely, we look at the Bayesian false discovery rate $\delta_n=P_g(\theta\in\Theta_0|p-value<\alpha)$ under a proper prior density $g(\theta)$. This depends on the prior $g$, the sample size $n$, the threshold value $\alpha$ as well as the choice of the test statistic. We show that the Benjamini--Hochberg FDR in fact converges to $\delta_n$ almost surely under $g$ for any fixed $n$. For one-sided null hypotheses, we derive a third order asymptotic expansion for $\delta_n$ in the continuous exponential family when the test statistic is the MLE and in the location family when the test statistic is the sample median. We also briefly mention the expansion in the uniform family when the test statistic is the MLE. The expansions are derived by putting together Edgeworth expansions for the CDF, Cornish--Fisher expansions for the quantile function and various Taylor expansions. Numerical results show that the expansions are very accurate even for a small value of $n$ (e.g., $n=10$). We make many useful conclusions from these expansions, and specifically that the frequentist is not prone to false discoveries except when the prior $g$ is too spiky. The results are illustrated by many examples.

</details>

