# 2015

## TOC

- [2015-01](#2015-01)
- [2015-02](#2015-02)
- [2015-03](#2015-03)
- [2015-04](#2015-04)
- [2015-05](#2015-05)
- [2015-06](#2015-06)
- [2015-07](#2015-07)
- [2015-08](#2015-08)
- [2015-09](#2015-09)
- [2015-10](#2015-10)
- [2015-11](#2015-11)
- [2015-12](#2015-12)

## 2015-01

<details>

<summary>2015-01-01 13:27:40 - Quantile estimation for Lévy measures</summary>

- *Mathias Trabs*

- `1405.6942v2` - [abs](http://arxiv.org/abs/1405.6942v2) - [pdf](http://arxiv.org/pdf/1405.6942v2)

> Generalizing the concept of quantiles to the jump measure of a L\'evy process, the generalized quantiles $q_{\tau}^{\pm}>0$, for $\tau>0$, are given by the smallest values such that a jump larger than $q_{\tau}^{+}$ or a negative jump smaller than $-q_{\tau}^{-}$, respectively, is expected only once in $1/\tau$ time units. Nonparametric estimators of the generalized quantiles are constructed using either discrete observations of the process or using option prices in an exponential L\'evy model of asset prices. In both models minimax convergence rates are shown. Applying Lepski's approach, we derive adaptive quantile estimators. The performance of the estimation method is illustrated in simulations and with real data.

</details>

<details>

<summary>2015-01-04 23:34:01 - Concave Penalized Estimation of Sparse Gaussian Bayesian Networks</summary>

- *Bryon Aragam, Qing Zhou*

- `1401.0852v2` - [abs](http://arxiv.org/abs/1401.0852v2) - [pdf](http://arxiv.org/pdf/1401.0852v2)

> We develop a penalized likelihood estimation framework to estimate the structure of Gaussian Bayesian networks from observational data. In contrast to recent methods which accelerate the learning problem by restricting the search space, our main contribution is a fast algorithm for score-based structure learning which does not restrict the search space in any way and works on high-dimensional datasets with thousands of variables. Our use of concave regularization, as opposed to the more popular $\ell_0$ (e.g. BIC) penalty, is new. Moreover, we provide theoretical guarantees which generalize existing asymptotic results when the underlying distribution is Gaussian. Most notably, our framework does not require the existence of a so-called faithful DAG representation, and as a result the theory must handle the inherent nonidentifiability of the estimation problem in a novel way. Finally, as a matter of independent interest, we provide a comprehensive comparison of our approach to several standard structure learning methods using open-source packages developed for the R language. Based on these experiments, we show that our algorithm is significantly faster than other competing methods while obtaining higher sensitivity with comparable false discovery rates for high-dimensional data. In particular, the total runtime for our method to generate a solution path of 20 estimates for DAGs with 8000 nodes is around one hour.

</details>

<details>

<summary>2015-01-05 12:20:09 - Inverse Renormalization Group Transformation in Bayesian Image Segmentations</summary>

- *Kazuyuki Tanaka, Shun Kataoka, Muneki Yasuda, Masayuki Ohzeki*

- `1501.00834v1` - [abs](http://arxiv.org/abs/1501.00834v1) - [pdf](http://arxiv.org/pdf/1501.00834v1)

> A new Bayesian image segmentation algorithm is proposed by combining a loopy belief propagation with an inverse real space renormalization group transformation to reduce the computational time. In results of our experiment, we observe that the proposed method can reduce the computational time to less than one-tenth of that taken by conventional Bayesian approaches.

</details>

<details>

<summary>2015-01-06 02:10:00 - Lasso Regression: Estimation and Shrinkage via Limit of Gibbs Sampling</summary>

- *Bala Rajaratnam, Steven Roberts, Doug Sparks, Onkar Dalal*

- `1401.2480v4` - [abs](http://arxiv.org/abs/1401.2480v4) - [pdf](http://arxiv.org/pdf/1401.2480v4)

> The application of the lasso is espoused in high-dimensional settings where only a small number of the regression coefficients are believed to be nonzero. Moreover, statistical properties of high-dimensional lasso estimators are often proved under the assumption that the correlation between the predictors is bounded. In this vein, coordinatewise methods, the most common means of computing the lasso solution, work well in the presence of low to moderate multicollinearity. The computational speed of coordinatewise algorithms degrades however as sparsity decreases and multicollinearity increases. Motivated by these limitations, we propose the novel "Deterministic Bayesian Lasso" algorithm for computing the lasso solution. This algorithm is developed by considering a limiting version of the Bayesian lasso. The performance of the Deterministic Bayesian Lasso improves as sparsity decreases and multicollinearity increases, and can offer substantial increases in computational speed. A rigorous theoretical analysis demonstrates that (1) the Deterministic Bayesian Lasso algorithm converges to the lasso solution, and (2) it leads to a representation of the lasso estimator which shows how it achieves both $\ell_1$ and $\ell_2$ types of shrinkage simultaneously. Connections to other algorithms are also provided. The benefits of the Deterministic Bayesian Lasso algorithm are then illustrated on simulated and real data.

</details>

<details>

<summary>2015-01-06 10:37:07 - Physiologically Informed Bayesian Analysis of ASL fMRI Data</summary>

- *Aina Frau-Pascual, Thomas Vincent, Jennifer Sloboda, Philippe CIUCIU, Florence Forbes*

- `1501.01133v1` - [abs](http://arxiv.org/abs/1501.01133v1) - [pdf](http://arxiv.org/pdf/1501.01133v1)

> Arterial Spin Labelling (ASL) functional Magnetic Resonance Imaging (fMRI) data provides a quantitative measure of blood perfusion, that can be correlated to neuronal activation. In contrast to BOLD measure, it is a direct measure of cerebral blood flow. However, ASL data has a lower SNR and resolution so that the recovery of the perfusion response of interest suffers from the contamination by a stronger hemodynamic component in the ASL signal. In this work we consider a model of both hemodynamic and perfusion components within the ASL signal. A physiological link between these two components is analyzed and used for a more accurate estimation of the perfusion response function in particular in the usual ASL low SNR conditions.

</details>

<details>

<summary>2015-01-06 16:53:33 - Causal inference for ordinal outcomes</summary>

- *Alexander Volfovsky, Edoardo M. Airoldi, Donald B. Rubin*

- `1501.01234v1` - [abs](http://arxiv.org/abs/1501.01234v1) - [pdf](http://arxiv.org/pdf/1501.01234v1)

> Many outcomes of interest in the social and health sciences, as well as in modern applications in computational social science and experimentation on social media platforms, are ordinal and do not have a meaningful scale. Causal analyses that leverage this type of data, termed ordinal non-numeric, require careful treatment, as much of the classical potential outcomes literature is concerned with estimation and hypothesis testing for outcomes whose relative magnitudes are well defined. Here, we propose a class of finite population causal estimands that depend on conditional distributions of the potential outcomes, and provide an interpretable summary of causal effects when no scale is available. We formulate a relaxation of the Fisherian sharp null hypothesis of constant effect that accommodates the scale-free nature of ordinal non-numeric data. We develop a Bayesian procedure to estimate the proposed causal estimands that leverages the rank likelihood. We illustrate these methods with an application to educational outcomes in the General Social Survey.

</details>

<details>

<summary>2015-01-10 18:49:32 - Robust Inference of Risks of Large Portfolios</summary>

- *Jianqing Fan, Fang Han, Han Liu, Byron Vickers*

- `1501.02382v1` - [abs](http://arxiv.org/abs/1501.02382v1) - [pdf](http://arxiv.org/pdf/1501.02382v1)

> We propose a bootstrap-based robust high-confidence level upper bound (Robust H-CLUB) for assessing the risks of large portfolios. The proposed approach exploits rank-based and quantile-based estimators, and can be viewed as a robust extension of the H-CLUB method (Fan et al., 2015). Such an extension allows us to handle possibly misspecified models and heavy-tailed data. Under mixing conditions, we analyze the proposed approach and demonstrate its advantage over the H-CLUB. We further provide thorough numerical results to back up the developed theory. We also apply the proposed method to analyze a stock market dataset.

</details>

<details>

<summary>2015-01-10 20:31:02 - A Potential Tale of Two by Two Tables from Completely Randomized Experiments</summary>

- *Peng Ding, Tirthankar Dasgupta*

- `1501.02389v1` - [abs](http://arxiv.org/abs/1501.02389v1) - [pdf](http://arxiv.org/pdf/1501.02389v1)

> Causal inference in completely randomized treatment-control studies with binary outcomes is discussed from Fisherian, Neymanian and Bayesian perspectives, using the potential outcomes framework. A randomization-based justification of Fisher's exact test is provided. Arguing that the crucial assumption of constant causal effect is often unrealistic, and holds only for extreme cases, some new asymptotic and Bayesian inferential procedures are proposed. The proposed procedures exploit the intrinsic non-additivity of unit-level causal effects, can be applied to linear and non-linear estimands, and dominate the existing methods, as verified theoretically and also through simulation studies.

</details>

<details>

<summary>2015-01-11 15:50:09 - Fast and optimal nonparametric sequential design for astronomical observations</summary>

- *Justin J. Yang, Xufei Wang, Pavlos Protopapas, Luke Bornn*

- `1501.02467v1` - [abs](http://arxiv.org/abs/1501.02467v1) - [pdf](http://arxiv.org/pdf/1501.02467v1)

> The spectral energy distribution (SED) is a relatively easy way for astronomers to distinguish between different astronomical objects such as galaxies, black holes, and stellar objects. By comparing the observations from a source at different frequencies with template models, astronomers are able to infer the type of this observed object. In this paper, we take a Bayesian model averaging perspective to learn astronomical objects, employing a Bayesian nonparametric approach to accommodate the deviation from convex combinations of known log-SEDs. To effectively use telescope time for observations, we then study Bayesian nonparametric sequential experimental design without conjugacy, in which we use sequential Monte Carlo as an efficient tool to maximize the volume of information stored in the posterior distribution of the parameters of interest. A new technique for performing inferences in log-Gaussian Cox processes called the Poisson log-normal approximation is also proposed. Simulations show the speed, accuracy, and usefulness of our method. While the strategy we propose in this paper is brand new in the astronomy literature, the inferential techniques developed apply to more general nonparametric sequential experimental design problems.

</details>

<details>

<summary>2015-01-11 23:17:34 - Predictive Modeling of Cholera Outbreaks in Bangladesh</summary>

- *Amanda A. Koepke, Ira M. Longini Jr., M. Elizabeth Halloran, Jon Wakefield, Vladimir N. Minin*

- `1402.0536v2` - [abs](http://arxiv.org/abs/1402.0536v2) - [pdf](http://arxiv.org/pdf/1402.0536v2)

> Despite seasonal cholera outbreaks in Bangladesh, little is known about the relationship between environmental conditions and cholera cases. We seek to develop a predictive model for cholera outbreaks in Bangladesh based on environmental predictors. To do this, we estimate the contribution of environmental variables, such as water depth and water temperature, to cholera outbreaks in the context of a disease transmission model. We implement a method which simultaneously accounts for disease dynamics and environmental variables in a Susceptible-Infected-Recovered-Susceptible (SIRS) model. The entire system is treated as a continuous-time hidden Markov model, where the hidden Markov states are the numbers of people who are susceptible, infected, or recovered at each time point, and the observed states are the numbers of cholera cases reported. We use a Bayesian framework to fit this hidden SIRS model, implementing particle Markov chain Monte Carlo methods to sample from the posterior distribution of the environmental and transmission parameters given the observed data. We test this method using both simulation and data from Mathbaria, Bangladesh. Parameter estimates are used to make short-term predictions that capture the formation and decline of epidemic peaks. We demonstrate that our model can successfully predict an increase in the number of infected individuals in the population weeks before the observed number of cholera cases increases, which could allow for early notification of an epidemic and timely allocation of resources.

</details>

<details>

<summary>2015-01-12 09:34:27 - Combined modeling of sparse and dense noise for improvement of Relevance Vector Machine</summary>

- *Martin Sundin, Saikat Chatterjee, Magnus Jansson*

- `1501.02579v1` - [abs](http://arxiv.org/abs/1501.02579v1) - [pdf](http://arxiv.org/pdf/1501.02579v1)

> Using a Bayesian approach, we consider the problem of recovering sparse signals under additive sparse and dense noise. Typically, sparse noise models outliers, impulse bursts or data loss. To handle sparse noise, existing methods simultaneously estimate the sparse signal of interest and the sparse noise of no interest. For estimating the sparse signal, without the need of estimating the sparse noise, we construct a robust Relevance Vector Machine (RVM). In the RVM, sparse noise and ever present dense noise are treated through a combined noise model. The precision of combined noise is modeled by a diagonal matrix. We show that the new RVM update equations correspond to a non-symmetric sparsity inducing cost function. Further, the combined modeling is found to be computationally more efficient. We also extend the method to block-sparse signals and noise with known and unknown block structures. Through simulations, we show the performance and computation efficiency of the new RVM in several applications: recovery of sparse and block sparse signals, housing price prediction and image denoising.

</details>

<details>

<summary>2015-01-12 14:30:28 - Bayesian Exponential Random Graph Models with Nodal Random Effects</summary>

- *Stephanie Thiemichen, Nial Friel, Alberto Caimo, Göran Kauermann*

- `1407.6895v2` - [abs](http://arxiv.org/abs/1407.6895v2) - [pdf](http://arxiv.org/pdf/1407.6895v2)

> We extend the well-known and widely used Exponential Random Graph Model (ERGM) by including nodal random effects to compensate for heterogeneity in the nodes of a network. The Bayesian framework for ERGMs proposed by Caimo and Friel (2011) yields the basis of our modelling algorithm. A central question in network models is the question of model selection and following the Bayesian paradigm we focus on estimating Bayes factors. To do so we develop an approximate but feasible calculation of the Bayes factor which allows one to pursue model selection. Two data examples and a small simulation study illustrate our mixed model approach and the corresponding model selection.

</details>

<details>

<summary>2015-01-13 09:26:58 - Multivariate quantiles and multivariate L-moments</summary>

- *Alexis Decurninge*

- `1409.6013v2` - [abs](http://arxiv.org/abs/1409.6013v2) - [pdf](http://arxiv.org/pdf/1409.6013v2)

> Univariate L-moments are expressed as projections of the quantile function onto an orthogonal basis of polynomials in $L_2([0;1],\mathbb{R})$. We present multivariate versions of L-moments expressed as collections of orthogonal projections of a multivariate quantile function on a basis of multivariate polynomials in $L_2([0;1]^d,\mathbb{R})$. We propose to consider quantile functions defined as transport from the uniform distribution on $[0;1]^d$ onto the distribution of interest. In particular, we present the quantiles defined by the transport of Rosenblatt and the optimal transport and the properties of the subsequent L-moments.

</details>

<details>

<summary>2015-01-13 13:50:58 - An Improvement to the Domain Adaptation Bound in a PAC-Bayesian context</summary>

- *Pascal Germain, Amaury Habrard, Francois Laviolette, Emilie Morvant*

- `1501.03002v1` - [abs](http://arxiv.org/abs/1501.03002v1) - [pdf](http://arxiv.org/pdf/1501.03002v1)

> This paper provides a theoretical analysis of domain adaptation based on the PAC-Bayesian theory. We propose an improvement of the previous domain adaptation bound obtained by Germain et al. in two ways. We first give another generalization bound tighter and easier to interpret. Moreover, we provide a new analysis of the constant term appearing in the bound that can be of high interest for developing new algorithmic solutions.

</details>

<details>

<summary>2015-01-13 14:14:49 - Automorphism groups of Gaussian chain graph models</summary>

- *Piotr Zwiernik, Jan Draisma*

- `1501.03013v1` - [abs](http://arxiv.org/abs/1501.03013v1) - [pdf](http://arxiv.org/pdf/1501.03013v1)

> In this paper we extend earlier work on groups acting on Gaussian graphical models to Gaussian Bayesian networks and more general Gaussian models defined by chain graphs. We discuss the maximal group which leaves a given model invariant and provide basic statistical applications of this result. This includes equivariant estimation, maximal invariants and robustness. The computation of the group requires finding the essential graph. However, by applying Studeny's theory of imsets we show that computations for DAGs can be performed efficiently without building the essential graph. In our proof we derive simple necessary and sufficient conditions on vanishing sub-minors of the concentration matrix in the model.

</details>

<details>

<summary>2015-01-13 16:04:23 - A Bernstein-type Inequality for Some Mixing Processes and Dynamical Systems with an Application to Learning</summary>

- *H. Hang, I. Steinwart*

- `1501.03059v1` - [abs](http://arxiv.org/abs/1501.03059v1) - [pdf](http://arxiv.org/pdf/1501.03059v1)

> We establish a Bernstein-type inequality for a class of stochastic processes that include the classical geometrically $\phi$-mixing processes, Rio's generalization of these processes, as well as many time-discrete dynamical systems. Modulo a logarithmic factor and some constants, our Bernstein-type inequality coincides with the classical Bernstein inequality for i.i.d.~data. We further use this new Bernstein-type inequality to derive an oracle inequality for generic regularized empirical risk minimization algorithms and data generated by such processes. Applying this oracle inequality to support vector machines using the Gaussian kernels for both least squares and quantile regression, it turns out that the resulting learning rates match, up to some arbitrarily small extra term in the exponent, the optimal rates for i.i.d.~processes.

</details>

<details>

<summary>2015-01-13 16:49:59 - Block Hyper-g Priors in Bayesian Regression</summary>

- *Agniva Som, Christopher M. Hans, Steven N. MacEachern*

- `1406.6419v2` - [abs](http://arxiv.org/abs/1406.6419v2) - [pdf](http://arxiv.org/pdf/1406.6419v2)

> The development of prior distributions for Bayesian regression has traditionally been driven by the goal of achieving sensible model selection and parameter estimation. The formalization of properties that characterize good performance has led to the development and popularization of thick tailed mixtures of g priors such as the Zellner--Siow and hyper-g priors. The properties of a particular prior are typically illuminated under limits on the likelihood or the prior. In this paper we introduce a new, conditional information asymptotic that is motivated by the common data analysis setting where at least one regression coefficient is much larger than others. We analyze existing mixtures of g priors under this limit and reveal two new behaviors, Essentially Least Squares (ELS) estimation and the Conditional Lindley's Paradox (CLP), and argue that these behaviors are, in general, undesirable. As the driver behind both of these behaviors is the use of a single, latent scale parameter that is common to all coefficients, we propose a block hyper-g prior, defined by first partitioning the covariates into groups and then placing independent hyper-g priors on the corresponding blocks of coefficients. We provide conditions under which ELS and the CLP are avoided by the new class of priors, and provide consistency results under traditional sample size asymptotics.

</details>

<details>

<summary>2015-01-14 19:05:45 - Evaluation of the Fourth Millennium Development Goal Realisation Using Robust and Nonparametric Tools Offered by Data Depth Concept</summary>

- *Ewa Kosiorowska, Daniel Kosiorowski, Zygmunt Zawadzki*

- `1409.3918v2` - [abs](http://arxiv.org/abs/1409.3918v2) - [pdf](http://arxiv.org/pdf/1409.3918v2)

> We briefly communicate results of a nonparametric and robust evaluation of effects of \emph{the Fourth Millennium Development Goal of United Nations}. Main aim of the goal was reducing by two thirds, between 1990--2015, the under five months child mortality. Our novel analysis was conducted by means of very powerful and user friendly tools offered by the \emph{Data Depth Concept} being a collection of multivariate techniques basing on multivariate generalizations of quantiles, ranges and order statistics. Results of our analysis are more convincing than results obtained using classical statistical tools.

</details>

<details>

<summary>2015-01-14 20:31:53 - Inference for Trans-dimensional Bayesian Models with Diffusive Nested Sampling</summary>

- *Brendon J. Brewer*

- `1411.3921v3` - [abs](http://arxiv.org/abs/1411.3921v3) - [pdf](http://arxiv.org/pdf/1411.3921v3)

> Many inference problems involve inferring the number $N$ of components in some region, along with their properties $\{\mathbf{x}_i\}_{i=1}^N$, from a dataset $\mathcal{D}$. A common statistical example is finite mixture modelling. In the Bayesian framework, these problems are typically solved using one of the following two methods: i) by executing a Monte Carlo algorithm (such as Nested Sampling) once for each possible value of $N$, and calculating the marginal likelihood or evidence as a function of $N$; or ii) by doing a single run that allows the model dimension $N$ to change (such as Markov Chain Monte Carlo with birth/death moves), and obtaining the posterior for $N$ directly. In this paper we present a general approach to this problem that uses trans-dimensional MCMC embedded within a Nested Sampling algorithm, allowing us to explore the posterior distribution and calculate the marginal likelihood (summed over $N$) even if the problem contains a phase transition or other difficult features such as multimodality. We present two example problems, finding sinusoidal signals in noisy data, and finding and measuring galaxies in a noisy astronomical image. Both of the examples demonstrate phase transitions in the relationship between the likelihood and the cumulative prior mass, highlighting the need for Nested Sampling.

</details>

<details>

<summary>2015-01-14 20:38:49 - Application of Kähler manifold to signal processing and Bayesian inference</summary>

- *Jaehyung Choi, Andrew P. Mullhaupt*

- `1409.4398v2` - [abs](http://arxiv.org/abs/1409.4398v2) - [pdf](http://arxiv.org/pdf/1409.4398v2)

> We review the information geometry of linear systems and its application to Bayesian inference, and the simplification available in the K\"ahler manifold case. We find conditions for the information geometry of linear systems to be K\"ahler, and the relation of the K\"ahler potential to information geometric quantities such as $\alpha $-divergence, information distance and the dual $\alpha $-connection structure. The K\"ahler structure simplifies the calculation of the metric tensor, connection, Ricci tensor and scalar curvature, and the $\alpha $-generalization of the geometric objects. The Laplace--Beltrami operator is also simplified in the K\"ahler geometry. One of the goals in information geometry is the construction of Bayesian priors outperforming the Jeffreys prior, which we use to demonstrate the utility of the K\"ahler structure.

</details>

<details>

<summary>2015-01-15 17:03:44 - Approximating Cross-validatory Predictive Evaluation in Bayesian Latent Variables Models with Integrated IS and WAIC</summary>

- *Longhai Li, Shi Qiu, Bei Zhang, Cindy X. Feng*

- `1404.2918v5` - [abs](http://arxiv.org/abs/1404.2918v5) - [pdf](http://arxiv.org/pdf/1404.2918v5)

> A natural method for approximating out-of-sample predictive evaluation is leave-one-out cross-validation (LOOCV) --- we alternately hold out each case from a full data set and then train a Bayesian model using Markov chain Monte Carlo (MCMC) without the held-out; at last we evaluate the posterior predictive distribution of all cases with their actual observations. However, actual LOOCV is time-consuming. This paper introduces two methods, namely iIS and iWAIC, for approximating LOOCV with only Markov chain samples simulated from a posterior based on a \textit{full} data set. iIS and iWAIC aim at improving the approximations given by importance sampling (IS) and WAIC in Bayesian models with possibly correlated latent variables. In iIS and iWAIC, we first integrate the predictive density over the distribution of the latent variables associated with the held-out without reference to its observation, then apply IS and WAIC approximations to the integrated predictive density. We compare iIS and iWAIC with other approximation methods in three real data examples that respectively use mixture models, models with correlated spatial effects, and a random effect logistic model. Our empirical results show that iIS and iWAIC give substantially better approximates than non-integrated IS and WAIC and other methods.

</details>

<details>

<summary>2015-01-16 01:59:34 - Bayesian Nonparametrics in Topic Modeling: A Brief Tutorial</summary>

- *Alexander Spangher*

- `1501.03861v1` - [abs](http://arxiv.org/abs/1501.03861v1) - [pdf](http://arxiv.org/pdf/1501.03861v1)

> Using nonparametric methods has been increasingly explored in Bayesian hierarchical modeling as a way to increase model flexibility. Although the field shows a lot of promise, inference in many models, including Hierachical Dirichlet Processes (HDP), remain prohibitively slow. One promising path forward is to exploit the submodularity inherent in Indian Buffet Process (IBP) to derive near-optimal solutions in polynomial time. In this work, I will present a brief tutorial on Bayesian nonparametric methods, especially as they are applied to topic modeling. I will show a comparison between different non-parametric models and the current state-of-the-art parametric model, Latent Dirichlet Allocation (LDA).

</details>

<details>

<summary>2015-01-16 13:15:39 - Bayesian protein structure alignment</summary>

- *Abel Rodriguez, Scott C. Schmidler*

- `1501.03971v1` - [abs](http://arxiv.org/abs/1501.03971v1) - [pdf](http://arxiv.org/pdf/1501.03971v1)

> The analysis of the three-dimensional structure of proteins is an important topic in molecular biochemistry. Structure plays a critical role in defining the function of proteins and is more strongly conserved than amino acid sequence over evolutionary timescales. A key challenge is the identification and evaluation of structural similarity between proteins; such analysis can aid in understanding the role of newly discovered proteins and help elucidate evolutionary relationships between organisms. Computational biologists have developed many clever algorithmic techniques for comparing protein structures, however, all are based on heuristic optimization criteria, making statistical interpretation somewhat difficult. Here we present a fully probabilistic framework for pairwise structural alignment of proteins. Our approach has several advantages, including the ability to capture alignment uncertainty and to estimate key "gap" parameters which critically affect the quality of the alignment. We show that several existing alignment methods arise as maximum a posteriori estimates under specific choices of prior distributions and error models. Our probabilistic framework is also easily extended to incorporate additional information, which we demonstrate by including primary sequence information to generate simultaneous sequence-structure alignments that can resolve ambiguities obtained using structure alone. This combined model also provides a natural approach for the difficult task of estimating evolutionary distance based on structural alignments. The model is illustrated by comparison with well-established methods on several challenging protein alignment examples.

</details>

<details>

<summary>2015-01-18 20:50:39 - Mathematical Language Processing: Automatic Grading and Feedback for Open Response Mathematical Questions</summary>

- *Andrew S. Lan, Divyanshu Vats, Andrew E. Waters, Richard G. Baraniuk*

- `1501.04346v1` - [abs](http://arxiv.org/abs/1501.04346v1) - [pdf](http://arxiv.org/pdf/1501.04346v1)

> While computer and communication technologies have provided effective means to scale up many aspects of education, the submission and grading of assessments such as homework assignments and tests remains a weak link. In this paper, we study the problem of automatically grading the kinds of open response mathematical questions that figure prominently in STEM (science, technology, engineering, and mathematics) courses. Our data-driven framework for mathematical language processing (MLP) leverages solution data from a large number of learners to evaluate the correctness of their solutions, assign partial-credit scores, and provide feedback to each learner on the likely locations of any errors. MLP takes inspiration from the success of natural language processing for text data and comprises three main steps. First, we convert each solution to an open response mathematical question into a series of numerical features. Second, we cluster the features from several solutions to uncover the structures of correct, partially correct, and incorrect solutions. We develop two different clustering approaches, one that leverages generic clustering algorithms and one based on Bayesian nonparametrics. Third, we automatically grade the remaining (potentially large number of) solutions based on their assigned cluster and one instructor-provided grade per cluster. As a bonus, we can track the cluster assignment of each step of a multistep solution and determine when it departs from a cluster of correct solutions, which enables us to indicate the likely locations of errors to learners. We test and validate MLP on real-world MOOC data to demonstrate how it can substantially reduce the human effort required in large-scale educational platforms.

</details>

<details>

<summary>2015-01-19 01:32:43 - Structure Learning in Bayesian Networks of Moderate Size by Efficient Sampling</summary>

- *Ru He, Jin Tian, Huaiqing Wu*

- `1501.04370v1` - [abs](http://arxiv.org/abs/1501.04370v1) - [pdf](http://arxiv.org/pdf/1501.04370v1)

> We study the Bayesian model averaging approach to learning Bayesian network structures (DAGs) from data. We develop new algorithms including the first algorithm that is able to efficiently sample DAGs according to the exact structure posterior. The DAG samples can then be used to construct estimators for the posterior of any feature. We theoretically prove good properties of our estimators and empirically show that our estimators considerably outperform the estimators from the previous state-of-the-art methods.

</details>

<details>

<summary>2015-01-19 06:29:41 - Global estimation of child mortality using a Bayesian B-spline Bias-reduction model</summary>

- *Leontine Alkema, Jin Rou New*

- `1309.1602v2` - [abs](http://arxiv.org/abs/1309.1602v2) - [pdf](http://arxiv.org/pdf/1309.1602v2)

> Estimates of the under-five mortality rate (U5MR) are used to track progress in reducing child mortality and to evaluate countries' performance related to Millennium Development Goal 4. However, for the great majority of developing countries without well-functioning vital registration systems, estimating the U5MR is challenging due to limited data availability and data quality issues. We describe a Bayesian penalized B-spline regression model for assessing levels and trends in the U5MR for all countries in the world, whereby biases in data series are estimated through the inclusion of a multilevel model to improve upon the limitations of current methods. B-spline smoothing parameters are also estimated through a multilevel model. Improved spline extrapolations are obtained through logarithmic pooling of the posterior predictive distribution of country-specific changes in spline coefficients with observed changes on the global level. The proposed model is able to flexibly capture changes in U5MR over time, gives point estimates and credible intervals reflecting potential biases in data series and performs reasonably well in out-of-sample validation exercises. It has been accepted by the United Nations Inter-agency Group for Child Mortality Estimation to generate estimates for all member countries.

</details>

<details>

<summary>2015-01-19 08:44:44 - Fully Bayesian binary Markov random field models: Prior specification and posterior simulation</summary>

- *Petter Arnesen, Håkon Tjelmeland*

- `1501.04419v1` - [abs](http://arxiv.org/abs/1501.04419v1) - [pdf](http://arxiv.org/pdf/1501.04419v1)

> We propose a flexible prior model for the parameters of binary Markov random fields (MRF) defined on rectangular lattices and with maximal cliques defined from a template maximal clique. The prior model allows higher-order interactions to be included. We also define a reversible jump Markov chain Monte Carlo (RJMCMC) algorithm to sample from the associated posterior distribution. The number of possible parameters for an MRF with for instance k x l maximal cliques becomes high even for small values of k and l. To get a flexible model which may adapt to the structure of a particular observed image we do not put any absolute restrictions on the parametrisation. Instead we define a parametric form for the MRF where the parameters have interpretation as potentials for the various clique configurations, and limit the effective number of parameters by assigning apriori discrete probabilities for events where groups of parameter values are equal. To run our RJMCMC algorithm we have to cope with the computationally intractable normalising constant of MRFs. For this we adopt a previously defined approximation for binary MRFs, but we also briefly discuss other alternatives. We demonstrate the flexibility of our prior formulation with simulated and real data examples.

</details>

<details>

<summary>2015-01-20 20:01:47 - Brittleness of Bayesian Inference Under Finite Information in a Continuous World</summary>

- *Houman Owhadi, Clint Scovel, Tim Sullivan*

- `1304.6772v3` - [abs](http://arxiv.org/abs/1304.6772v3) - [pdf](http://arxiv.org/pdf/1304.6772v3)

> We derive, in the classical framework of Bayesian sensitivity analysis, optimal lower and upper bounds on posterior values obtained from Bayesian models that exactly capture an arbitrarily large number of finite-dimensional marginals of the data-generating distribution and/or that are as close as desired to the data-generating distribution in the Prokhorov or total variation metrics; these bounds show that such models may still make the largest possible prediction error after conditioning on an arbitrarily large number of sample data measured at finite precision. These results are obtained through the development of a reduction calculus for optimization problems over measures on spaces of measures. We use this calculus to investigate the mechanisms that generate brittleness/robustness and, in particular, we observe that learning and robustness are antagonistic properties. It is now well understood that the numerical resolution of PDEs requires the satisfaction of specific stability conditions. Is there a missing stability condition for using Bayesian inference in a continuous world under finite information?

</details>

<details>

<summary>2015-01-21 01:37:19 - Non-Local Priors for High-Dimensional Estimation</summary>

- *David Rossell, Donatello Telesca*

- `1402.5107v3` - [abs](http://arxiv.org/abs/1402.5107v3) - [pdf](http://arxiv.org/pdf/1402.5107v3)

> Simultaneously achieving parsimony and good predictive power in high dimensions is a main challenge in statistics. Non-local priors (NLPs) possess appealing properties for high-dimensional model choice, but their use for estimation has not been studied in detail. We show that, for regular models, Bayesian model averaging (BMA) estimates based on NLPs shrink spurious parameters either at fast polynomial or quasi-exponential rates as the sample size $n$ increases (depending on the chosen prior density). Non-spurious parameter estimates only differ from the oracle MLE by a factor of $n^{-1}$. We extend some results to linear models with dimension $p$ growing with $n$.   Coupled with our theoretical investigations, we outline the constructive representation of NLPs as mixtures of truncated distributions. From a practitioners' perspective, our work enables simple posterior sampling and extending NLPs beyond previous proposals. Our results show notable high-dimensional estimation for linear models with $p>>n$ at reduced computational cost. NLPs provided lower estimation error than benchmark and hyper-g priors, SCAD and LASSO in simulations, and in gene expression data achieved higher cross-validated $R^2$ with an order of magnitude less predictors. Remarkably, these results were obtained without the need to pre-screen predictors. Our findings contribute to the debate of whether different priors should be used for estimation and model selection, showing that selection priors may actually be desirable for high-dimensional estimation.

</details>

<details>

<summary>2015-01-21 06:17:50 - Difficulties applying recent blind source separation techniques to EEG and MEG</summary>

- *Kevin H. Knuth*

- `1501.05068v1` - [abs](http://arxiv.org/abs/1501.05068v1) - [pdf](http://arxiv.org/pdf/1501.05068v1)

> High temporal resolution measurements of human brain activity can be performed by recording the electric potentials on the scalp surface (electroencephalography, EEG), or by recording the magnetic fields near the surface of the head (magnetoencephalography, MEG). The analysis of the data is problematic due to the fact that multiple neural generators may be simultaneously active and the potentials and magnetic fields from these sources are superimposed on the detectors. It is highly desirable to un-mix the data into signals representing the behaviors of the original individual generators. This general problem is called blind source separation and several recent techniques utilizing maximum entropy, minimum mutual information, and maximum likelihood estimation have been applied. These techniques have had much success in separating signals such as natural sounds or speech, but appear to be ineffective when applied to EEG or MEG signals. Many of these techniques implicitly assume that the source distributions have a large kurtosis, whereas an analysis of EEG/MEG signals reveals that the distributions are multimodal. This suggests that more effective separation techniques could be designed for EEG and MEG signals.

</details>

<details>

<summary>2015-01-21 06:18:25 - Convergent Bayesian formulations of blind source separation and electromagnetic source estimation</summary>

- *Kevin H. Knuth, Herbert G. Vaughan Jr*

- `1501.05069v1` - [abs](http://arxiv.org/abs/1501.05069v1) - [pdf](http://arxiv.org/pdf/1501.05069v1)

> We consider two areas of research that have been developing in parallel over the last decade: blind source separation (BSS) and electromagnetic source estimation (ESE). BSS deals with the recovery of source signals when only mixtures of signals can be obtained from an array of detectors and the only prior knowledge consists of some information about the nature of the source signals. On the other hand, ESE utilizes knowledge of the electromagnetic forward problem to assign source signals to their respective generators, while information about the signals themselves is typically ignored. We demonstrate that these two techniques can be derived from the same starting point using the Bayesian formalism. This suggests a means by which new algorithms can be developed that utilize as much relevant information as possible. We also briefly mention some preliminary work that supports the value of integrating information used by these two techniques and review the kinds of information that may be useful in addressing the ESE problem.

</details>

<details>

<summary>2015-01-21 13:37:40 - A Bayesian Approach for Noisy Matrix Completion: Optimal Rate under General Sampling Distribution</summary>

- *The Tien Mai, Pierre Alquier*

- `1408.5820v2` - [abs](http://arxiv.org/abs/1408.5820v2) - [pdf](http://arxiv.org/pdf/1408.5820v2)

> Bayesian methods for low-rank matrix completion with noise have been shown to be very efficient computationally. While the behaviour of penalized minimization methods is well understood both from the theoretical and computational points of view in this problem, the theoretical optimality of Bayesian estimators have not been explored yet. In this paper, we propose a Bayesian estimator for matrix completion under general sampling distribution. We also provide an oracle inequality for this estimator. This inequality proves that, whatever the rank of the matrix to be estimated, our estimator reaches the minimax-optimal rate of convergence (up to a logarithmic factor). We end the paper with a short simulation study.

</details>

<details>

<summary>2015-01-21 16:14:27 - A Separation Theorem for Chain Event Graphs</summary>

- *Peter A. Thwaites, Jim Q. Smith*

- `1501.05215v1` - [abs](http://arxiv.org/abs/1501.05215v1) - [pdf](http://arxiv.org/pdf/1501.05215v1)

> Bayesian Networks (BNs) are popular graphical models for the representation of statistical problems embodying dependence relationships between a number of variables. Much of this popularity is due to the d-separation theorem of Pearl and Lauritzen, which allows an analyst to identify the conditional independence statements that a model of the problem embodies using only the topology of the graph. However for many problems the complete model dependence structure cannot be depicted by a BN. The Chain Event Graph (CEG) was introduced for these types of problem. In this paper we introduce a separation theorem for CEGs, analogous to the d-separation theorem for BNs, which likewise allows an analyst to identify the conditional independence structure of their model from the topology of the graph.

</details>

<details>

<summary>2015-01-21 17:04:30 - Hierarchical modelling of species sensitivity distribution: development and application to the case of diatoms exposed to several herbicides</summary>

- *Guillaume Kon Kam King, Floriane Larras, Sandrine Charles, Marie Laure Delignette-Muller*

- `1501.05230v1` - [abs](http://arxiv.org/abs/1501.05230v1) - [pdf](http://arxiv.org/pdf/1501.05230v1)

> The Species Sensitivity Distribution (SSD) is a key tool to assess the ecotoxicological threat of contaminant to biodiversity. It predicts safe concentrations for a contaminant in a community. Widely used, this approach suffers from several drawbacks: i)summarizing the sensitivity of each species by a single value entails a loss of valuable information about the other parameters characterizing the concentration-effect curves; ii)it does not propagate the uncertainty on the critical effect concentration into the SSD; iii)the hazardous concentration estimated with SSD only indicates the threat to biodiversity, without any insight about a global response of the community related to the measured endpoint. We revisited the current SSD approach to account for all the sources of variability and uncertainty into the prediction and to assess a global response for the community. For this purpose, we built a global hierarchical model including the concentration-response model together with the distribution law for the SSD. Working within a Bayesian framework, we were able to compute an SSD taking into account all the uncertainty from the original raw data. From model simulations, it is also possible to extract a quantitative indicator of a global response of the community to the contaminant. We applied this methodology to study the toxicity of 6 herbicides to benthic diatoms from Lake Geneva, measured from biomass reduction.

</details>

<details>

<summary>2015-01-22 11:05:53 - Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets</summary>

- *Diederik P. Kingma, Max Welling*

- `1402.0480v5` - [abs](http://arxiv.org/abs/1402.0480v5) - [pdf](http://arxiv.org/pdf/1402.0480v5)

> Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the non-centered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.

</details>

<details>

<summary>2015-01-22 13:48:26 - Bayesian inverse problems with non-conjugate priors</summary>

- *Kolyan Ray*

- `1209.6156v3` - [abs](http://arxiv.org/abs/1209.6156v3) - [pdf](http://arxiv.org/pdf/1209.6156v3)

> We investigate the frequentist posterior contraction rate of nonparametric Bayesian procedures in linear inverse problems in both the mildly and severely ill-posed cases. A theorem is proved in a general Hilbert space setting under approximation-theoretic assumptions on the prior. The result is applied to non-conjugate priors, notably sieve and wavelet series priors, as well as in the conjugate setting. In the mildly ill-posed setting minimax optimal rates are obtained, with sieve priors being rate adaptive over Sobolev classes. In the severely ill-posed setting, oversmoothing the prior yields minimax rates. Previously established results in the conjugate setting are obtained using this method. Examples of applications include deconvolution, recovering the initial condition in the heat equation and the Radon transform.

</details>

<details>

<summary>2015-01-23 08:52:35 - Bayesian Learning for Low-Rank matrix reconstruction</summary>

- *Martin Sundin, Cristian R. Rojas, Magnus Jansson, Saikat Chatterjee*

- `1501.05740v1` - [abs](http://arxiv.org/abs/1501.05740v1) - [pdf](http://arxiv.org/pdf/1501.05740v1)

> We develop latent variable models for Bayesian learning based low-rank matrix completion and reconstruction from linear measurements. For under-determined systems, the developed methods are shown to reconstruct low-rank matrices when neither the rank nor the noise power is known a-priori. We derive relations between the latent variable models and several low-rank promoting penalty functions. The relations justify the use of Kronecker structured covariance matrices in a Gaussian based prior. In the methods, we use evidence approximation and expectation-maximization to learn the model parameters. The performance of the methods is evaluated through extensive numerical simulations.

</details>

<details>

<summary>2015-01-24 11:10:01 - Bayesian Hidden Markov Modelling Using Circular-Linear General Projected Normal Distribution</summary>

- *Gianluca Mastrantonio, Antonello Maruotti, Giovanna Jona Lasinio*

- `1408.4834v2` - [abs](http://arxiv.org/abs/1408.4834v2) - [pdf](http://arxiv.org/pdf/1408.4834v2)

> We introduce a multivariate hidden Markov model to jointly cluster time-series observations with different support, i.e. circular and linear. Relying on the general projected normal distribution, our approach allows for bimodal and/or skewed cluster-specific distributions for the circular variable. Furthermore, we relax the independence assumption between the circular and linear components observed at the same time. Such an assumption is generally used to alleviate the computational burden involved in the parameter estimation step, but it is hard to justify in empirical applications. We carry out a simulation study using different data-generation schemes to investigate model behavior, focusing on well recovering the hidden structure. Finally, the model is used to fit a real data example on a bivariate time series of wind speed and direction.

</details>

<details>

<summary>2015-01-25 04:09:22 - Expander Framework for Generating High-Dimensional GLM Gradient and Hessian from Low-Dimensional Base Distributions: R Package RegressionFactory</summary>

- *Alireza S. Mahani, Mansour T. A. Sharabiani*

- `1501.06111v1` - [abs](http://arxiv.org/abs/1501.06111v1) - [pdf](http://arxiv.org/pdf/1501.06111v1)

> The R package RegressionFactory provides expander functions for constructing the high-dimensional gradient vector and Hessian matrix of the log-likelihood function for generalized linear models (GLMs), from the lower-dimensional base-distribution derivatives. The software follows a modular implementation using the chain rule of derivatives. Such modularity offers a clear separation of case-specific components (base distribution functional form and link functions) from common steps (e.g., matrix algebra operations needed for expansion) in calculating log-likelihood derivatives. In doing so, RegressionFactory offers several advantages: 1) It provides a fast and convenient method for constructing log-likelihood and its derivatives by requiring only the low-dimensional, base-distribution derivatives, 2) The accompanying definiteness-invariance theorem allows researchers to reason about the negative-definiteness of the log-likelihood Hessian in the much lower-dimensional space of the base distributions, 3) The factorized, abstract view of regression suggests opportunities to generate novel regression models, and 4) Computational techniques for performance optimization can be developed generically in the abstract framework and be readily applicable across all the specific regression instances. We expect RegressionFactory to facilitate research and development on optimization and sampling techniques for GLM log-likelihoods as well as construction of composite models from GLM lego blocks, such as Hierarchical Bayesian models.

</details>

<details>

<summary>2015-01-26 09:29:17 - Fact Sheet Research on Bayesian Decision Theory</summary>

- *H. R. N. van Erp, R. O. Linger, P. H. A. J. M. van Gelder*

- `1409.8269v4` - [abs](http://arxiv.org/abs/1409.8269v4) - [pdf](http://arxiv.org/pdf/1409.8269v4)

> In this fact sheet we give some preliminary research results on the Bayesian Decision Theory. This theory has been under construction for the past two years. But what started as an intuitive enough idea, now seems to have the makings of something more fundamental.

</details>

<details>

<summary>2015-01-27 00:29:14 - Bayesian Approach to Handling Informative Sampling</summary>

- *Anna Sikov*

- `1501.06629v1` - [abs](http://arxiv.org/abs/1501.06629v1) - [pdf](http://arxiv.org/pdf/1501.06629v1)

> In the case of informative sampling the sampling scheme explicitly or implicitly depends on the response variable. As a result, the sample distribution of response variable can- not be used for making inference about the population. In this research I investigate the problem of informative sampling from the Bayesian perspective. Application of the Bayesian approach permits solving the problems, which arise due to complexity of the models, being used for handling informative sampling. The main objective of the re- search is to combine the elements of the classical sampling theory and Bayesian analysis, for identifying and estimating the population model, and the model describing the sam- pling mechanism. Utilizing the fact that inclusion probabilities are generally known, the population sum of squares of the models residuals can be estimated, implementing the techniques of the sampling theory. In this research I show, how these estimates can be incorporated in the Bayesian modeling and how the Full Bayesian Significance Test (FBST), which is based on the Bayesian measure of evidence for precise null hypothesis, can be utilized as a model identification tool. The results obtained by implementation of the proposed approach to estimation and identification of the sample selection model seem promising. At this point I am working on methods of estimation and identification of the population model. An interesting extension of my approach is incorporation of known population characteristics into the estimation process. Some other directions for continuation of my research are highlighted in the sections which describe the proposed methodology.

</details>

<details>

<summary>2015-01-27 10:35:01 - Fast Approximate Inference of Transcript Expression Levels from RNA-seq Data</summary>

- *James Hensman, Peter Glaus, Antti Honkela, Magnus Rattray*

- `1308.5953v2` - [abs](http://arxiv.org/abs/1308.5953v2) - [pdf](http://arxiv.org/pdf/1308.5953v2)

> Motivation: The mapping of RNA-seq reads to their transcripts of origin is a fundamental task in transcript expression estimation and differential expression scoring. Where ambiguities in mapping exist due to transcripts sharing sequence, e.g. alternative isoforms or alleles, the problem becomes an instance of non-trivial probabilistic inference. Bayesian inference in such a problem is intractable and approximate methods must be used such as Markov chain Monte Carlo (MCMC) and Variational Bayes. Standard implementations of these methods can be prohibitively slow for large datasets and complex gene models.   Results: We propose an approximate inference scheme based on Variational Bayes applied to an existing model of transcript expression inference from RNA-seq data. We apply recent advances in Variational Bayes algorithmics to improve the convergence of the algorithm beyond the standard variational expectation-maximisation approach. We apply our algorithm to simulated and biological datasets, demonstrating that the increase in speed requires only a small trade-off in accuracy of expression level estimation.   Availability: The methods were implemented in R and C++, and are available as part of the BitSeq project at https://code.google.com/p/bitseq/. The methods will be made available through the BitSeq Bioconductor package at the next stable release.

</details>

<details>

<summary>2015-01-27 19:37:09 - The Bayesian Echo Chamber: Modeling Social Influence via Linguistic Accommodation</summary>

- *Fangjian Guo, Charles Blundell, Hanna Wallach, Katherine Heller*

- `1411.2674v3` - [abs](http://arxiv.org/abs/1411.2674v3) - [pdf](http://arxiv.org/pdf/1411.2674v3)

> We present the Bayesian Echo Chamber, a new Bayesian generative model for social interaction data. By modeling the evolution of people's language usage over time, this model discovers latent influence relationships between them. Unlike previous work on inferring influence, which has primarily focused on simple temporal dynamics evidenced via turn-taking behavior, our model captures more nuanced influence relationships, evidenced via linguistic accommodation patterns in interaction content. The model, which is based on a discrete analog of the multivariate Hawkes process, permits a fully Bayesian inference algorithm. We validate our model's ability to discover latent influence patterns using transcripts of arguments heard by the US Supreme Court and the movie "12 Angry Men." We showcase our model's capabilities by using it to infer latent influence patterns from Federal Open Market Committee meeting transcripts, demonstrating state-of-the-art performance at uncovering social dynamics in group discussions.

</details>

<details>

<summary>2015-01-27 21:23:22 - A Probabilistic Least-Mean-Squares Filter</summary>

- *Jesus Fernandez-Bes, Víctor Elvira, Steven Van Vaerenbergh*

- `1501.06929v1` - [abs](http://arxiv.org/abs/1501.06929v1) - [pdf](http://arxiv.org/pdf/1501.06929v1)

> We introduce a probabilistic approach to the LMS filter. By means of an efficient approximation, this approach provides an adaptable step-size LMS algorithm together with a measure of uncertainty about the estimation. In addition, the proposed approximation preserves the linear complexity of the standard LMS. Numerical results show the improved performance of the algorithm with respect to standard LMS and state-of-the-art algorithms with similar complexity. The goal of this work, therefore, is to open the door to bring some more Bayesian machine learning techniques to adaptive filtering.

</details>

<details>

<summary>2015-01-27 22:54:14 - Fast Bayesian Inference for Exoplanet Discovery in Radial Velocity Data</summary>

- *Brendon J. Brewer, Courtney P. Donovan*

- `1501.06952v1` - [abs](http://arxiv.org/abs/1501.06952v1) - [pdf](http://arxiv.org/pdf/1501.06952v1)

> Inferring the number of planets $N$ in an exoplanetary system from radial velocity (RV) data is a challenging task. Recently, it has become clear that RV data can contain periodic signals due to stellar activity, which can be difficult to distinguish from planetary signals. However, even doing the inference under a given set of simplifying assumptions (e.g. no stellar activity) can be difficult. It is common for the posterior distribution for the planet parameters, such as orbital periods, to be multimodal and to have other awkward features. In addition, when $N$ is unknown, the marginal likelihood (or evidence) as a function of $N$ is required. Rather than doing separate runs with different trial values of $N$, we propose an alternative approach using a trans-dimensional Markov Chain Monte Carlo method within Nested Sampling. The posterior distribution for $N$ can be obtained with a single run. We apply the method to $\nu$ Oph and Gliese 581, finding moderate evidence for additional signals in $\nu$ Oph with periods of 36.11 $\pm$ 0.034 days, 75.58 $\pm$ 0.80 days, and 1709 $\pm$ 183 days; the posterior probability that at least one of these exists is 85%. The results also suggest Gliese 581 hosts many (7-15) "planets" (or other causes of other periodic signals), but only 4-6 have well determined periods. The analysis of both of these datasets shows phase transitions exist which are difficult to negotiate without Nested Sampling.

</details>

<details>

<summary>2015-01-28 08:19:07 - A hierarchical Bayesian setting for an inverse problem in linear parabolic PDEs with noisy boundary conditions</summary>

- *Fabrizio Ruggeri, Zaid Sawlan, Marco Scavino, Raul Tempone*

- `1501.04739v2` - [abs](http://arxiv.org/abs/1501.04739v2) - [pdf](http://arxiv.org/pdf/1501.04739v2)

> In this work we develop a Bayesian setting to infer unknown parameters in initial-boundary value problems related to linear parabolic partial differential equations. We realistically assume that the boundary data are noisy, for a given prescribed initial condition. We show how to derive the joint likelihood function for the forward problem, given some measurements of the solution field subject to Gaussian noise. Given Gaussian priors for the time-dependent Dirichlet boundary values, we analytically marginalize the joint likelihood using the linearity of the equation. Our hierarchical Bayesian approach is fully implemented in an example that involves the heat equation. In this example, the thermal diffusivity is the unknown parameter. We assume that the thermal diffusivity parameter can be modeled a priori through a lognormal random variable or by means of a space-dependent stationary lognormal random field. Synthetic data are used to test the inference. We exploit the behavior of the non-normalized log posterior distribution of the thermal diffusivity. Then, we use the Laplace method to obtain an approximated Gaussian posterior and therefore avoid costly Markov Chain Monte Carlo computations. Expected information gains and predictive posterior densities for observable quantities are numerically estimated using Laplace approximation for different experimental setups.

</details>

<details>

<summary>2015-01-28 15:10:21 - A Bayesian approach to the linking of key comparisons</summary>

- *Michael Krystek, Harald Bosse*

- `1501.07134v1` - [abs](http://arxiv.org/abs/1501.07134v1) - [pdf](http://arxiv.org/pdf/1501.07134v1)

> This contribution presents a Bayesian approach to the issue of linking of the results from key comparison measurements. A mathematical treatment based on Bayesian statistics for the analysis of the results from two comparisons with some joint participants is described. This robust statistical analysis provides expressions and standard uncertainties for the key comparison reference value (KCRV) and the degree of equivalence (DOE) as well as a conformity check without any assumption on a priority of one of the comparisons. In addition to the derivation of the mathematical formulae to be used for this type of "distributed linking", we also present one synthetic and one real linking example and discuss possible applications of this new linking procedure.

</details>

<details>

<summary>2015-01-28 17:10:27 - A nonparametric Bayesian test of dependence</summary>

- *Yimin Kao, Brian J Reich, Howard D Bondell*

- `1501.07198v1` - [abs](http://arxiv.org/abs/1501.07198v1) - [pdf](http://arxiv.org/pdf/1501.07198v1)

> In this article, we propose a new method for the fundamental task of testing for dependence between two groups of variables. The response densities under the null hypothesis of independence and the alternative hypothesis of dependence are specified by nonparametric Bayesian models. Under the null hypothesis, the joint distribution is modeled by the product of two independent Dirichlet Process Mixture (DPM) priors; under the alternative, the full joint density is modeled by a multivariate DPM prior. The test is then based on the posterior probability of favoring the alternative hypothesis. The proposed test not only has good performance for testing linear dependence among other popular nonparametric tests, but is also preferred to other methods in testing many of the nonlinear dependencies we explored. In the analysis of gene expression data, we compare different methods for testing pairwise dependence between genes. The results show that the proposed test identifies some dependence structures that are not detected by other tests.

</details>

<details>

<summary>2015-01-29 11:53:05 - D$^3$PO - Denoising, Deconvolving, and Decomposing Photon Observations</summary>

- *Marco Selig, Torsten Enßlin*

- `1311.1888v2` - [abs](http://arxiv.org/abs/1311.1888v2) - [pdf](http://arxiv.org/pdf/1311.1888v2)

> The analysis of astronomical images is a non-trivial task. The D3PO algorithm addresses the inference problem of denoising, deconvolving, and decomposing photon observations. Its primary goal is the simultaneous but individual reconstruction of the diffuse and point-like photon flux given a single photon count image, where the fluxes are superimposed. In order to discriminate between these morphologically different signal components, a probabilistic algorithm is derived in the language of information field theory based on a hierarchical Bayesian parameter model. The signal inference exploits prior information on the spatial correlation structure of the diffuse component and the brightness distribution of the spatially uncorrelated point-like sources. A maximum a posteriori solution and a solution minimizing the Gibbs free energy of the inference problem using variational Bayesian methods are discussed. Since the derivation of the solution is not dependent on the underlying position space, the implementation of the D3PO algorithm uses the NIFTY package to ensure applicability to various spatial grids and at any resolution. The fidelity of the algorithm is validated by the analysis of simulated data, including a realistic high energy photon count image showing a 32 x 32 arcmin^2 observation with a spatial resolution of 0.1 arcmin. In all tests the D3PO algorithm successfully denoised, deconvolved, and decomposed the data into a diffuse and a point-like signal estimate for the respective photon flux components.

</details>

<details>

<summary>2015-01-29 15:38:25 - Computer emulation with non-stationary Gaussian processes</summary>

- *Silvia Montagna, Surya T. Tokdar*

- `1308.4756v2` - [abs](http://arxiv.org/abs/1308.4756v2) - [pdf](http://arxiv.org/pdf/1308.4756v2)

> Gaussian process (GP) models are widely used to emulate propagation uncertainty in computer experiments. GP emulation sits comfortably within an analytically tractable Bayesian framework. Apart from propagating uncertainty of the input variables, a GP emulator trained on finitely many runs of the experiment also offers error bars for response surface estimates at unseen input values. This helps select future input values where the experiment should be run to minimize the uncertainty in the response surface estimation. However, traditional GP emulators use stationary covariance functions, which perform poorly and lead to sub-optimal selection of future input points when the response surface has sharp local features, such as a jump discontinuity or an isolated tall peak. We propose an easily implemented non-stationary GP emulator, based on two stationary GPs, one nested into the other, and demonstrate its superior ability in handling local features and selecting future input points from the boundaries of such features.

</details>

<details>

<summary>2015-01-30 13:48:19 - The Bayesian Analysis of Complex, High-Dimensional Models: Can It Be CODA?</summary>

- *Y. Ritov, P. J. Bickel, A. C. Gamst, B. J. K. Kleijn*

- `1203.5471v4` - [abs](http://arxiv.org/abs/1203.5471v4) - [pdf](http://arxiv.org/pdf/1203.5471v4)

> We consider the Bayesian analysis of a few complex, high-dimensional models and show that intuitive priors, which are not tailored to the fine details of the model and the estimated parameters, produce estimators which perform poorly in situations in which good, simple frequentist estimators exist. The models we consider are: stratified sampling, the partial linear model, linear and quadratic functionals of white noise and estimation with stopping times. We present a strong version of Doob's consistency theorem which demonstrates that the existence of a uniformly $\sqrt{n}$-consistent estimator ensures that the Bayes posterior is $\sqrt{n}$-consistent for values of the parameter in subsets of prior probability 1. We also demonstrate that it is, at least, in principle, possible to construct Bayes priors giving both global and local minimax rates, using a suitable combination of loss functions. We argue that there is no contradiction in these apparently conflicting findings.

</details>


## 2015-02

<details>

<summary>2015-02-01 01:02:57 - Adaptive Bayesian estimation in indirect Gaussian sequence space models</summary>

- *Jan Johannes, Anna Simoni, Rudolf Schenk*

- `1502.00184v1` - [abs](http://arxiv.org/abs/1502.00184v1) - [pdf](http://arxiv.org/pdf/1502.00184v1)

> In an indirect Gaussian sequence space model lower and upper bounds are derived for the concentration rate of the posterior distribution of the parameter of interest shrinking to the parameter value $\theta^\circ$ that generates the data. While this establishes posterior consistency, however, the concentration rate depends on both $\theta^\circ$ and a tuning parameter which enters the prior distribution. We first provide an oracle optimal choice of the tuning parameter, i.e., optimized for each $\theta^\circ$ separately. The optimal choice of the prior distribution allows us to derive an oracle optimal concentration rate of the associated posterior distribution. Moreover, for a given class of parameters and a suitable choice of the tuning parameter, we show that the resulting uniform concentration rate over the given class is optimal in a minimax sense. Finally, we construct a hierarchical prior that is adaptive. This means that, given a parameter $\theta^\circ$ or a class of parameters, respectively, the posterior distribution contracts at the oracle rate or at the minimax rate over the class. Notably, the hierarchical prior does not depend neither on $\theta^\circ$ nor on the given class. Moreover, convergence of the fully data-driven Bayes estimator at the oracle or at the minimax rate is established.

</details>

<details>

<summary>2015-02-02 05:23:21 - Bayesian Variable Selection on Model Spaces Constrained by Heredity Conditions</summary>

- *Daniel Taylor-Rodriguez, Andrew Womack, Nikolay Bliznyuk*

- `1312.6611v2` - [abs](http://arxiv.org/abs/1312.6611v2) - [pdf](http://arxiv.org/pdf/1312.6611v2)

> This paper investigates Bayesian variable selection when there is a hierarchical dependence structure on the inclusion of predictors in the model. In particular, we study the type of dependence found in polynomial response surfaces of orders two and higher, whose model spaces are required to satisfy weak or strong heredity conditions. These conditions restrict the inclusion of higher-order terms depending upon the inclusion of lower-order parent terms. We develop classes of priors on the model space, investigate their theoretical and finite sample properties, and provide a Metropolis-Hastings algorithm for searching the space of models. The tools proposed allow fast and thorough exploration of model spaces that account for hierarchical polynomial structure in the predictors and provide control of the inclusion of false positives in high posterior probability models.

</details>

<details>

<summary>2015-02-02 11:25:41 - A scaled gradient projection method for Bayesian learning in dynamical systems</summary>

- *Silvia Bonettini, Alessandro Chiuso, Marco Prato*

- `1406.6603v3` - [abs](http://arxiv.org/abs/1406.6603v3) - [pdf](http://arxiv.org/pdf/1406.6603v3)

> A crucial task in system identification problems is the selection of the most appropriate model class, and is classically addressed resorting to cross-validation or using asymptotic arguments. As recently suggested in the literature, this can be addressed in a Bayesian framework, where model complexity is regulated by few hyperparameters, which can be estimated via marginal likelihood maximization. It is thus of primary importance to design effective optimization methods to solve the corresponding optimization problem. If the unknown impulse response is modeled as a Gaussian process with a suitable kernel, the maximization of the marginal likelihood leads to a challenging nonconvex optimization problem, which requires a stable and effective solution strategy. In this paper we address this problem by means of a scaled gradient projection algorithm, in which the scaling matrix and the steplength parameter play a crucial role to provide a meaning solution in a computational time comparable with second order methods. In particular, we propose both a generalization of the split gradient approach to design the scaling matrix in the presence of box constraints, and an effective implementation of the gradient and objective function. The extensive numerical experiments carried out on several test problems show that our method is very effective in providing in few tenths of a second solutions of the problems with accuracy comparable with state-of-the-art approaches. Moreover, the flexibility of the proposed strategy makes it easily adaptable to a wider range of problems arising in different areas of machine learning, signal processing and system identification.

</details>

<details>

<summary>2015-02-02 14:07:50 - Is non-informative Bayesian analysis appropriate for wildlife management: survival of San Joaquin Kit Fox and declines in amphibian populations</summary>

- *Subhash R. Lele*

- `1502.00483v1` - [abs](http://arxiv.org/abs/1502.00483v1) - [pdf](http://arxiv.org/pdf/1502.00483v1)

> Computational convenience has led to widespread use of Bayesian inference with vague or flat priors to analyze state-space models in ecology. Vague priors are claimed to be objective and to let the data speak. Neither of these claims is valid. Statisticians have criticized the use of vague priors from philosophical to computational to pragmatic reasons. Ecologists, however, dismiss such criticisms as empty philosophical wonderings with no practical implications. We illustrate that use of vague priors in population viability analysis and occupancy models can have significant impact on the analysis and can lead to strikingly different managerial decisions. Given the wide spread applicability of the hierarchical models and uncritical use of non-informative Bayesian analysis in ecology, researchers should be cautious about using the vague priors as a default choice in practical situations.

</details>

<details>

<summary>2015-02-02 20:08:32 - Confidence Corridors for Multivariate Generalized Quantile Regression</summary>

- *Shih-Kang Chao, Katharina Proksch, Holger Dette, Wolfgang Härdle*

- `1406.4421v2` - [abs](http://arxiv.org/abs/1406.4421v2) - [pdf](http://arxiv.org/pdf/1406.4421v2)

> We focus on the construction of confidence corridors for multivariate nonparametric generalized quantile regression functions. This construction is based on asymptotic results for the maximal deviation between a suitable nonparametric estimator and the true function of interest which follow after a series of approximation steps including a Bahadur representation, a new strong approximation theorem and exponential tail inequalities for Gaussian random fields. As a byproduct we also obtain confidence corridors for the regression function in the classical mean regression. In order to deal with the problem of slowly decreasing error in coverage probability of the asymptotic confidence corridors, which results in meager coverage for small sample sizes, a simple bootstrap procedure is designed based on the leading term of the Bahadur representation. The finite sample properties of both procedures are investigated by means of a simulation study and it is demonstrated that the bootstrap procedure considerably outperforms the asymptotic bands in terms of coverage accuracy. Finally, the bootstrap confidence corridors are used to study the efficacy of the National Supported Work Demonstration, which is a randomized employment enhancement program launched in the 1970s. This article has supplementary materials.

</details>

<details>

<summary>2015-02-03 08:41:06 - Synthesising evidence to estimate pandemic (2009) A/H1N1 influenza severity in 2009-2011</summary>

- *Anne M. Presanis, Richard G. Pebody, Paul J. Birrell, Brian D. M. Tom, Helen K. Green, Hayley Durnall, Douglas Fleming, Daniela De Angelis*

- `1408.7025v2` - [abs](http://arxiv.org/abs/1408.7025v2) - [pdf](http://arxiv.org/pdf/1408.7025v2)

> Knowledge of the severity of an influenza outbreak is crucial for informing and monitoring appropriate public health responses, both during and after an epidemic. However, case-fatality, case-intensive care admission and case-hospitalisation risks are difficult to measure directly. Bayesian evidence synthesis methods have previously been employed to combine fragmented, under-ascertained and biased surveillance data coherently and consistently, to estimate case-severity risks in the first two waves of the 2009 A/H1N1 influenza pandemic experienced in England. We present in detail the complex probabilistic model underlying this evidence synthesis, and extend the analysis to also estimate severity in the third wave of the pandemic strain during the 2010/2011 influenza season. We adapt the model to account for changes in the surveillance data available over the three waves. We consider two approaches: (a) a two-stage approach using posterior distributions from the model for the first two waves to inform priors for the third wave model; and (b) a one-stage approach modelling all three waves simultaneously. Both approaches result in the same key conclusions: (1) that the age-distribution of the case-severity risks is "u"-shaped, with children and older adults having the highest severity; (2) that the age-distribution of the infection attack rate changes over waves, school-age children being most affected in the first two waves and the attack rate in adults over 25 increasing from the second to third waves; and (3) that when averaged over all age groups, case-severity appears to increase over the three waves. The extent to which the final conclusion is driven by the change in age-distribution of those infected over time is subject to discussion.

</details>

<details>

<summary>2015-02-03 10:47:56 - Detecting duplicates in a homicide registry using a Bayesian partitioning approach</summary>

- *Mauricio Sadinle*

- `1407.8219v2` - [abs](http://arxiv.org/abs/1407.8219v2) - [pdf](http://arxiv.org/pdf/1407.8219v2)

> Finding duplicates in homicide registries is an important step in keeping an accurate account of lethal violence. This task is not trivial when unique identifiers of the individuals are not available, and it is especially challenging when records are subject to errors and missing values. Traditional approaches to duplicate detection output independent decisions on the coreference status of each pair of records, which often leads to nontransitive decisions that have to be reconciled in some ad-hoc fashion. The task of finding duplicate records in a data file can be alternatively posed as partitioning the data file into groups of coreferent records. We present an approach that targets this partition of the file as the parameter of interest, thereby ensuring transitive decisions. Our Bayesian implementation allows us to incorporate prior information on the reliability of the fields in the data file, which is especially useful when no training data are available, and it also provides a proper account of the uncertainty in the duplicate detection decisions. We present a study to detect killings that were reported multiple times to the United Nations Truth Commission for El Salvador.

</details>

<details>

<summary>2015-02-03 12:47:54 - Probit models for capture-recapture data subject to imperfect detection, individual heterogeneity and misidentification</summary>

- *Brett T. McClintock, Larissa L. Bailey, Brian P. Dreher, William A. Link*

- `1401.3290v4` - [abs](http://arxiv.org/abs/1401.3290v4) - [pdf](http://arxiv.org/pdf/1401.3290v4)

> As noninvasive sampling techniques for animal populations have become more popular, there has been increasing interest in the development of capture-recapture models that can accommodate both imperfect detection and misidentification of individuals (e.g., due to genotyping error). However, current methods do not allow for individual variation in parameters, such as detection or survival probability. Here we develop misidentification models for capture-recapture data that can simultaneously account for temporal variation, behavioral effects and individual heterogeneity in parameters. To facilitate Bayesian inference using our approach, we extend standard probit regression techniques to latent multinomial models where the dimension and zeros of the response cannot be observed. We also present a novel Metropolis-Hastings within Gibbs algorithm for fitting these models using Markov chain Monte Carlo. Using closed population abundance models for illustration, we re-visit a DNA capture-recapture population study of black bears in Michigan, USA and find evidence of misidentification due to genotyping error, as well as temporal, behavioral and individual variation in detection probability. We also estimate a salamander population of known size from laboratory experiments evaluating the effectiveness of a marking technique commonly used for amphibians and fish. Our model was able to reliably estimate the size of this population and provided evidence of individual heterogeneity in misidentification probability that is attributable to variable mark quality. Our approach is more computationally demanding than previously proposed methods, but it provides the flexibility necessary for a much broader suite of models to be explored while properly accounting for uncertainty introduced by misidentification and imperfect detection. In the absence of misidentification, our probit formulation also provides a convenient and efficient Gibbs sampler for Bayesian analysis of traditional closed population capture-recapture data.

</details>

<details>

<summary>2015-02-03 16:10:35 - A Directional Multivariate Value at Risk</summary>

- *Raúl Torres, Rosa E. Lillo, Henry Laniado*

- `1502.00908v1` - [abs](http://arxiv.org/abs/1502.00908v1) - [pdf](http://arxiv.org/pdf/1502.00908v1)

> In economics, insurance and finance, value at risk (VaR) is a widely used measure of the risk of loss on a specific portfolio of financial assets. For a given portfolio, time horizon, and probability $\alpha$, the $100\alpha\%$ VaR is defined as a threshold loss value, such that the probability that the loss on the portfolio over the given time horizon exceeds this value is $\alpha$. That is to say, it is a quantile of the distribution of the losses, which has both good analytic properties and easy interpretation as a risk measure. However, its extension to the multivariate framework is not unique because a unique definition of multivariate quantile does not exist. In the current literature, the multivariate quantiles are related to a specific partial order considered in $\mathbb{R}^{n}$, or to a property of the univariate quantile that is desirable to be extended to $\mathbb{R}^{n}$. In this work, we introduce a multivariate value at risk as a vector-valued directional risk measure, based on a directional multivariate quantile, which has recently been introduced in the literature. The directional approach allows the manager to consider external information or risk preferences in her/his analysis. We have derived some properties of the risk measure and we have compared the univariate \textit{VaR} over the marginals with the components of the directional multivariate VaR. We have also analyzed the relationship between some families of copulas, for which it is possible to obtain closed forms of the multivariate VaR that we propose. Finally, comparisons with other alternative multivariate VaR given in the literature, are provided in terms of robustness.

</details>

<details>

<summary>2015-02-03 20:34:28 - Bayesian-based deconvolution fluorescence microscopy using dynamically updated nonparametric nonstationary expectation estimates</summary>

- *Alexander Wong, Xiao Yu Wang, Maud Gorbet*

- `1502.01002v1` - [abs](http://arxiv.org/abs/1502.01002v1) - [pdf](http://arxiv.org/pdf/1502.01002v1)

> Fluorescence microscopy is widely used for the study of biological specimens. Deconvolution can significantly improve the resolution and contrast of images produced using fluorescence microscopy; in particular, Bayesian-based methods have become very popular in deconvolution fluorescence microscopy. An ongoing challenge with Bayesian-based methods is in dealing with the presence of noise in low SNR imaging conditions. In this study, we present a Bayesian-based method for performing deconvolution using dynamically updated nonparametric nonstationary expectation estimates that can improve the fluorescence microscopy image quality in the presence of noise, without explicit use of spatial regularization.

</details>

<details>

<summary>2015-02-04 07:58:35 - Regression Adjustment for Noncrossing Bayesian Quantile Regression</summary>

- *Thais Rodrigues, Yanan Fan*

- `1502.01115v1` - [abs](http://arxiv.org/abs/1502.01115v1) - [pdf](http://arxiv.org/pdf/1502.01115v1)

> A two-stage approach is proposed to overcome the problem in quantile regression, where separately fitted curves for several quantiles may cross. The standard Bayesian quantile regression model is applied in the first stage, followed by a Gaussian process regression adjustment, which monotonizes the quantile function whilst borrowing strength from nearby quantiles. The two stage approach is computationally efficient, and more general than existing techniques. The method is shown to be competitive with alternative approaches via its performance in simulated examples.

</details>

<details>

<summary>2015-02-04 15:43:51 - Estimating the Attack Ratio of Dengue Epidemics under Time-varying Force of Infection using Aggregated Notification Data</summary>

- *Flavio Coelho, Luiz Max Carvalho*

- `1502.01236v1` - [abs](http://arxiv.org/abs/1502.01236v1) - [pdf](http://arxiv.org/pdf/1502.01236v1)

> Quantifying the attack ratio of disease is key to epidemiological inference and Public Health planning. For multi-serotype pathogens, however, different levels of serotype-specific immunity make it difficult to assess the population at risk. In this paper we propose a Bayesian method for estimation of the attack ratio of an epidemic and the initial fraction of susceptibles using aggregated incidence data. We derive the probability distribution of the effective reproductive number, R t , and use MCMC to obtain posterior distributions of the parameters of a single-strain SIR transmission model with time-varying force of infection. Our method is showcased in a data set consisting of 18 years of dengue incidence in the city of Rio de Janeiro, Brazil. We demonstrate that it is possible to learn about the initial fraction of susceptibles and the attack ratio even in the absence of serotype specific data. On the other hand, the information provided by this approach is limited, stressing the need for detailed serological surveys to characterise the distribution of serotype-specific immunity in the population.

</details>

<details>

<summary>2015-02-04 23:06:37 - Regression-based covariance functions for nonstationary spatial modeling</summary>

- *Mark D. Risser, Catherine A. Calder*

- `1410.1494v2` - [abs](http://arxiv.org/abs/1410.1494v2) - [pdf](http://arxiv.org/pdf/1410.1494v2)

> In many environmental applications involving spatially-referenced data, limitations on the number and locations of observations motivate the need for practical and efficient models for spatial interpolation, or kriging. A key component of models for continuously-indexed spatial data is the covariance function, which is traditionally assumed to belong to a parametric class of stationary models. However, stationarity is rarely a realistic assumption. Alternative methods which more appropriately model the nonstationarity present in environmental processes often involve high-dimensional parameter spaces, which lead to difficulties in model fitting and interpretability. To overcome this issue, we build on the growing literature of covariate-driven nonstationary spatial modeling. Using process convolution techniques, we propose a Bayesian model for continuously-indexed spatial data based on a flexible parametric covariance regression structure for a convolution-kernel covariance matrix. The resulting model is a parsimonious representation of the kernel process, and we explore properties of the implied model, including a description of the resulting nonstationary covariance function and the interpretational benefits in the kernel parameters. Furthermore, we demonstrate that our model provides a practical compromise between stationary and highly parameterized nonstationary spatial covariance functions that do not perform well in practice. We illustrate our approach through an analysis of annual precipitation data.

</details>

<details>

<summary>2015-02-05 13:10:43 - Some comments about A. Ronald Gallant's "Reflections on the Probability Space Induced by Moment Conditions with Implications for Bayesian Inference"</summary>

- *Christian P. Robert*

- `1502.01527v1` - [abs](http://arxiv.org/abs/1502.01527v1) - [pdf](http://arxiv.org/pdf/1502.01527v1)

> This note is commenting on Ronald Gallant's (2015) reflections on the construction of Bayesian prior distributions from moment conditions. The main conclusion is that the paper does not deliver a working principle that could justify inference based on such priors.

</details>

<details>

<summary>2015-02-06 02:57:05 - Sequential Channel State Tracking & SpatioTemporal Channel Prediction in Mobile Wireless Sensor Networks</summary>

- *Dionysios S. Kalogerias, Athina P. Petropulu*

- `1502.01780v1` - [abs](http://arxiv.org/abs/1502.01780v1) - [pdf](http://arxiv.org/pdf/1502.01780v1)

> We propose a nonlinear filtering framework for approaching the problems of channel state tracking and spatiotemporal channel gain prediction in mobile wireless sensor networks, in a Bayesian setting. We assume that the wireless channel constitutes an observable (by the sensors/network nodes), spatiotemporal, conditionally Gaussian stochastic process, which is statistically dependent on a set of hidden channel parameters, called the channel state. The channel state evolves in time according to a known, non stationary, nonlinear and/or non Gaussian Markov stochastic kernel. This formulation results in a partially observable system, with a temporally varying global state and spatiotemporally varying observations. Recognizing the intractability of general nonlinear state estimation, we advocate the use of grid based approximate filters as an effective and robust means for recursive tracking of the channel state. We also propose a sequential spatiotemporal predictor for tracking the channel gains at any point in time and space, providing real time sequential estimates for the respective channel gain map, for each sensor in the network. Additionally, we show that both estimators converge towards the true respective MMSE optimal estimators, in a common, relatively strong sense. Numerical simulations corroborate the practical effectiveness of the proposed approach.

</details>

<details>

<summary>2015-02-06 19:59:55 - Stochastic Newton Sampler: R Package sns</summary>

- *Alireza S. Mahani, Asad Hasan, Marshall Jiang, Mansour T. A. Sharabiani*

- `1502.02008v1` - [abs](http://arxiv.org/abs/1502.02008v1) - [pdf](http://arxiv.org/pdf/1502.02008v1)

> The R package sns implements Stochastic Newton Sampler (SNS), a Metropolis-Hastings Monte Carlo Markov Chain algorithm where the proposal density function is a multivariate Gaussian based on a local, second-order Taylor series expansion of log-density. The mean of the proposal function is the full Newton step in Newton-Raphson optimization algorithm. Taking advantage of the local, multivariate geometry captured in log-density Hessian allows SNS to be more efficient than univariate samplers, approaching independent sampling as the density function increasingly resembles a multivariate Gaussian. SNS requires the log-density Hessian to be negative-definite everywhere in order to construct a valid proposal function. This property holds, or can be easily checked, for many GLM-like models. When initial point is far from density peak, running SNS in non-stochastic mode by taking the Newton step, augmented with with line search, allows the MCMC chain to converge to high-density areas faster. For high-dimensional problems, partitioning of state space into lower-dimensional subsets, and applying SNS to the subsets within a Gibbs sampling framework can significantly improve the mixing of SNS chains. In addition to the above strategies for improving convergence and mixing, sns offers diagnostics and visualization capabilities, as well as a function for sample-based calculation of Bayesian predictive posterior distributions.

</details>

<details>

<summary>2015-02-07 05:51:25 - MCMC-free adaptive Bayesian procedures using random series prior</summary>

- *Weining Shen, Subhashis Ghosal*

- `1204.4238v2` - [abs](http://arxiv.org/abs/1204.4238v2) - [pdf](http://arxiv.org/pdf/1204.4238v2)

> We consider priors for several nonparametric Bayesian models which use finite random series with a random number of terms. The prior is constructed through distributions on the number of basis functions and the associated coefficients. We derive a general result on the construction of an appropriate sieve and obtain adaptive posterior contraction rates for all smoothness levels of the function in the true model. We apply this general result on several statistical problems such as signal processing, density estimation, nonparametric additive regression, classification, spectral density estimation, functional regression etc. The prior can be viewed as an alternative to commonly used Gaussian process prior, but can be analyzed by relatively simpler techniques and in many cases allows a simpler approach to computation without using Markov chain Monte-Carlo (MCMC) methods. A simulation study was conducted to show that the performance of the random series prior is comparable to that of a Gaussian process prior.

</details>

<details>

<summary>2015-02-07 06:21:26 - Adaptive Bayesian procedures using random series priors</summary>

- *Weining Shen, Subhashis Ghosal*

- `1403.0625v2` - [abs](http://arxiv.org/abs/1403.0625v2) - [pdf](http://arxiv.org/pdf/1403.0625v2)

> We consider a prior for nonparametric Bayesian estimation which uses finite random series with a random number of terms. The prior is constructed through distributions on the number of basis functions and the associated coefficients. We derive a general result on adaptive posterior convergence rates for all smoothness levels of the function in the true model by constructing an appropriate "sieve" and applying the general theory of posterior convergence rates. We apply this general result on several statistical problems such as signal processing, density estimation, various nonparametric regressions, classification, spectral density estimation, functional regression etc. The prior can be viewed as an alternative to the commonly used Gaussian process prior, but properties of the posterior distribution can be analyzed by relatively simpler techniques and in many cases allows a simpler approach to computation without using Markov chain Monte-Carlo (MCMC) methods. A simulation study is conducted to show that the accuracy of the Bayesian estimators based on the random series prior and the Gaussian process prior are comparable. We apply the method on two interesting data sets on functional regression.

</details>

<details>

<summary>2015-02-09 10:21:53 - Phase recovery from a Bayesian point of view: the variational approach</summary>

- *Angélique Drémeau, Florent Krzakala*

- `1410.1368v2` - [abs](http://arxiv.org/abs/1410.1368v2) - [pdf](http://arxiv.org/pdf/1410.1368v2)

> In this paper, we consider the phase recovery problem, where a complex signal vector has to be estimated from the knowledge of the modulus of its linear projections, from a naive variational Bayesian point of view. In particular, we derive an iterative algorithm following the minimization of the Kullback-Leibler divergence under the mean-field assumption, and show on synthetic data with random projections that this approach leads to an efficient and robust procedure, with a good computational cost.

</details>

<details>

<summary>2015-02-09 10:33:54 - Statistical methods for critical scenarios in aeronautics</summary>

- *Houssam Alrachid, Virginie Ehrlacher, Alexis Marceau, Karim Tekkal*

- `1409.1446v2` - [abs](http://arxiv.org/abs/1409.1446v2) - [pdf](http://arxiv.org/pdf/1409.1446v2)

> We present numerical results obtained on the CEMRACS project Predictive SMS proposed by Safety Line. The goal of this work was to elaborate a purely statistical method in order to reconstruct the deceleration profile of a plane during landing under normal operating conditions, from a database containing around $1500$ recordings. The aim of Safety Line is to use this model to detect malfunctions of the braking system of the plane from deviations of the measured deceleration profile of the plane to the one predicted by the model. This yields to a multivariate nonparametric regression problem, which we chose to tackle using a Bayesian approach based on the use of gaussian processes. We also compare this approach with other statistical methods.

</details>

<details>

<summary>2015-02-09 16:21:20 - Unbiased Bayes for Big Data: Paths of Partial Posteriors</summary>

- *Heiko Strathmann, Dino Sejdinovic, Mark Girolami*

- `1501.03326v2` - [abs](http://arxiv.org/abs/1501.03326v2) - [pdf](http://arxiv.org/pdf/1501.03326v2)

> A key quantity of interest in Bayesian inference are expectations of functions with respect to a posterior distribution. Markov Chain Monte Carlo is a fundamental tool to consistently compute these expectations via averaging samples drawn from an approximate posterior. However, its feasibility is being challenged in the era of so called Big Data as all data needs to be processed in every iteration. Realising that such simulation is an unnecessarily hard problem if the goal is estimation, we construct a computationally scalable methodology that allows unbiased estimation of the required expectations -- without explicit simulation from the full posterior. The scheme's variance is finite by construction and straightforward to control, leading to algorithms that are provably unbiased and naturally arrive at a desired error tolerance. This is achieved at an average computational complexity that is sub-linear in the size of the dataset and its free parameters are easy to tune. We demonstrate the utility and generality of the methodology on a range of common statistical models applied to large-scale benchmark and real-world datasets.

</details>

<details>

<summary>2015-02-09 19:20:16 - A Useful Algebraic System of Statistical Models</summary>

- *Ben Klemens*

- `1502.02614v1` - [abs](http://arxiv.org/abs/1502.02614v1) - [pdf](http://arxiv.org/pdf/1502.02614v1)

> This paper proposes a single form for statistical models that accommodates a broad range of models, from ordinary least squares to agent-based microsimulations. The definition makes it almost trivial to define morphisms to transform and combine existing models to produce new models. It offers a unified means of expressing and implementing methods that are typically given disparate treatment in the literature, including transformations via differentiable functions, Bayesian updating, multi-level and other types of composed models, Markov chain Monte Carlo, and several other common procedures. It especially offers benefit to simulation-type models, because of the value in being able to build complex models from simple parts, easily calculate robustness measures for simulation statistics and, where appropriate, test hypotheses. Running examples will be given using Apophenia, an open-source software library based on the model form and transformations described here.

</details>

<details>

<summary>2015-02-11 15:37:52 - A Bayesian Nonparametric IRT Model</summary>

- *George Karabatsos*

- `1502.03339v1` - [abs](http://arxiv.org/abs/1502.03339v1) - [pdf](http://arxiv.org/pdf/1502.03339v1)

> This paper introduces a flexible Bayesian nonparametric Item Response Theory (IRT) model, which applies to dichotomous or polytomous item responses, and which can apply to either unidimensional or multidimensional scaling. This is an infinite-mixture IRT model, with person ability and item difficulty parameters, and with a random intercept parameter that is assigned a mixing distribution, with mixing weights a probit function of other person and item parameters. As a result of its flexibility, the Bayesian nonparametric IRT model can provide outlier-robust estimation of the person ability parameters and the item difficulty parameters in the posterior distribution. The estimation of the posterior distribution of the model is undertaken by standard Markov chain Monte Carlo (MCMC) methods based on slice sampling. This mixture IRT model is illustrated through the analysis of real data obtained from a teacher preparation questionnaire, consisting of polytomous items, and consisting of other covariates that describe the examinees (teachers). For these data, the model obtains zero outliers and an R-squared of one. The paper concludes with a short discussion of how to apply the IRT model for the analysis of item response data, using menu-driven software that was developed by the author.

</details>

<details>

<summary>2015-02-11 18:28:17 - A Bayesian Nonparametric Causal Model for Regression Discontinuity Designs</summary>

- *George Karabatsos, Stephen G. Walker*

- `1311.4482v4` - [abs](http://arxiv.org/abs/1311.4482v4) - [pdf](http://arxiv.org/pdf/1311.4482v4)

> For non-randomized studies, the regression discontinuity design (RDD) can be used to identify and estimate causal effects from a "locally-randomized" subgroup of subjects, under relatively mild conditions. However, current models focus causal inferences on the impact of the treatment (versus non-treatment) variable on the mean of the dependent variable, via linear regression. For RDDs, we propose a flexible Bayesian nonparametric regression model that can provide accurate estimates of causal effects, in terms of the predictive mean, variance, quantile, probability density, distribution function, or any other chosen function of the outcome variable. We illustrate the model through the analysis of two real educational data sets, involving (resp.) a sharp RDD and a fuzzy RDD.

</details>

<details>

<summary>2015-02-11 19:58:46 - On The Sparse Bayesian Learning Of Linear Models</summary>

- *Yves Atchade, Chia Chye Yee*

- `1502.03416v1` - [abs](http://arxiv.org/abs/1502.03416v1) - [pdf](http://arxiv.org/pdf/1502.03416v1)

> This work is a re-examination of the sparse Bayesian learning (SBL) of linear regression models of Tipping (2001) in a high-dimensional setting. We propose a hard-thresholded version of the SBL estimator that achieves, for orthogonal design matrices, the non-asymptotic estimation error rate of $\sigma\sqrt{s\log p}/\sqrt{n}$, where $n$ is the sample size, $p$ the number of regressors, $\sigma$ is the regression model standard deviation, and $s$ the number of non-zero regression coefficients. We also establish that with high-probability the estimator identifies the non-zero regression coefficients. In our simulations we found that sparse Bayesian learning regression performs better than lasso (Tibshirani (1996)) when the signal to be recovered is strong.

</details>

<details>

<summary>2015-02-12 11:37:19 - Correcting for non-ignorable missingness in smoking trends</summary>

- *Juho Kopra, Tommi Härkänen, Hanna Tolonen, Juha Karvanen*

- `1502.03609v1` - [abs](http://arxiv.org/abs/1502.03609v1) - [pdf](http://arxiv.org/pdf/1502.03609v1)

> Data missing not at random (MNAR) is a major challenge in survey sampling. We propose an approach based on registry data to deal with non-ignorable missingness in health examination surveys. The approach relies on follow-up data available from administrative registers several years after the survey. For illustration we use data on smoking prevalence in Finnish National FINRISK study conducted in 1972-1997. The data consist of measured survey information including missingness indicators, register-based background information and register-based time-to-disease survival data. The parameters of missingness mechanism are estimable with these data although the original survey data are MNAR. The underlying data generation process is modelled by a Bayesian model. The results indicate that the estimated smoking prevalence rates in Finland may be significantly affected by missing data.

</details>

<details>

<summary>2015-02-13 11:26:26 - A simple Bayesian procedure for forecasting the outcomes of the UEFA Champions League matches</summary>

- *Jean-Louis Foulley*

- `1501.05831v2` - [abs](http://arxiv.org/abs/1501.05831v2) - [pdf](http://arxiv.org/pdf/1501.05831v2)

> This article presents a Bayesian implementation of a cumulative probit model to forecast the outcomes of the UEFA Champions League matches. The argument of the normal CDF involves a cut-off point, a home vs away playing effect and the difference in strength of the two competing teams. Team strength is assumed to follow a Gaussian distribution the expectation of which is expressed as a linear regression on an external rating of the team from eg the UEFA Club Ranking (UEFACR) or the Football Club World Ranking (FCWR). Priors on these parameters are updated at the beginning of each season from their posterior distributions obtained at the end of the previous one. This allows making predictions of match results for each phase of the competition: group stage and knock-out. An application is presented for the 2013-2014 season. Adjustment based on the FCWR performs better than on UEFACR.

</details>

<details>

<summary>2015-02-13 13:18:47 - Bayesian Hierarchical Modeling of Longitudinal Glaucomatous Visual Fields using a Two-Stage Approach</summary>

- *S. R. Bryan, P. H. C. Eilers, B. Li, D. Rizopoulos, K. A. Vermeer, H. G. Lemij, E. M. E. H. Lesaffre*

- `1502.03979v1` - [abs](http://arxiv.org/abs/1502.03979v1) - [pdf](http://arxiv.org/pdf/1502.03979v1)

> The Bayesian approach has become increasingly popular because it allows to model quite complex models via Markov chain Monte Carlo (MCMC) sampling. However, it is also recognized nowadays that MCMC sampling can become computationally prohibitive when a complex model needs to be fit to a large data set. To overcome this problem, we applied and extended a recently proposed two-stage approach to model a complex hierarchical data structure of glaucoma patients who participate in an ongoing Dutch study. Glaucoma is one of the leading causes of blindness in the world. In order to detect deterioration at an early stage, a model for predicting visual fields (VF) in time is needed. Hence, the true underlying VF progression can be determined, and treatment strategies can then be optimized to prevent further VF loss. Since we were unable to fit these data with the classical one-stage approach upon which the current popular Bayesian software is based, we made use of the two-stage Bayesian approach. The considered hierarchical longitudinal model involves estimating a large number of random effects and deals with censoring and high measurement variability. In addition, we extended the approach with tools for model evaluation

</details>

<details>

<summary>2015-02-13 13:54:20 - Inference on Dynamic Models for non-Gaussian Random Fields using INLA: A Homicide Rate Analysis of Brazilian Cities</summary>

- *Renan Xavier Cortes, Thiago Guerrera Martins, Marcos Oliveira Prates, Bráulio Figueiredo Alves da Silva*

- `1312.6896v2` - [abs](http://arxiv.org/abs/1312.6896v2) - [pdf](http://arxiv.org/pdf/1312.6896v2)

> Robust time series analysis is an important subject in statistical modeling. Models based on Gaussian distribution are sensitive to outliers, which may imply in a significant degradation in estimation performance as well as in prediction accuracy. State-space models, also referred as Dynamic Models, is a very useful way to describe the evolution of a time series variable through a structured latent evolution system. Integrated Nested Laplace Approximation (INLA) is a recent approach proposed to perform fast Bayesian inference in Latent Gaussian Models which naturally comprises Dynamic Models. We present how to perform fast and accurate non-Gaussian dynamic modeling with INLA and show how these models can provide a more robust time series analysis when compared with standard dynamic models based on Gaussian distributions. We formalize the framework used to fit complex non-Gaussian space-state models using the R package INLA and illustrate our approach in both a simulation study and on the brazilian homicide rate dataset.

</details>

<details>

<summary>2015-02-13 17:04:00 - Latent modeling of flow cytometry cell populations</summary>

- *Jonas Wallin, Kerstin Johnsson, Magnus Fontes*

- `1502.04058v1` - [abs](http://arxiv.org/abs/1502.04058v1) - [pdf](http://arxiv.org/pdf/1502.04058v1)

> Flow cytometry is a widespread single-cell measurement technology with a multitude of clinical and research applications. Interpretation of flow cytometry data is hard; the instrumentation is delicate and can not render absolute measurements, hence samples can only be interpreted in relation to each other while at the same time comparisons are confounded by inter-sample variation. Despite this, current automated flow cytometry data analysis methods either treat samples individually or ignore the variation by for example pooling the data. In this article we introduce a Bayesian hierarchical model for studying latent relations between cell populations in flow cytometry samples, thereby systematizing inter-sample variation. The model is applied to a data set containing replicated flow cytometry measurements of samples from healthy individuals, with informative priors capturing expert knowledge. It is shown that the technical variation in the inferred cell population sizes is small in comparison to the intrinsic biological variation. The large size of flow cytometry data, where a single sample can contain measurements on hundreds of thousands of cells, necessitates computationally efficient methods. To address this, we have implemented a parallel Markov Chain Monte Carlo scheme for sampling the posterior distribution.

</details>

<details>

<summary>2015-02-13 20:01:00 - Bayesian Models of Graphs, Arrays and Other Exchangeable Random Structures</summary>

- *Peter Orbanz, Daniel M. Roy*

- `1312.7857v2` - [abs](http://arxiv.org/abs/1312.7857v2) - [pdf](http://arxiv.org/pdf/1312.7857v2)

> The natural habitat of most Bayesian methods is data represented by exchangeable sequences of observations, for which de Finetti's theorem provides the theoretical foundation. Dirichlet process clustering, Gaussian process regression, and many other parametric and nonparametric Bayesian models fall within the remit of this framework; many problems arising in modern data analysis do not. This article provides an introduction to Bayesian models of graphs, matrices, and other data that can be modeled by random structures. We describe results in probability theory that generalize de Finetti's theorem to such data and discuss their relevance to nonparametric Bayesian modeling. With the basic ideas in place, we survey example models available in the literature; applications of such models include collaborative filtering, link prediction, and graph and network analysis. We also highlight connections to recent developments in graph theory and probability, and sketch the more general mathematical foundation of Bayesian methods for other types of data beyond sequences and arrays.

</details>

<details>

<summary>2015-02-16 16:48:30 - Particle Gibbs for Bayesian Additive Regression Trees</summary>

- *Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh*

- `1502.04622v1` - [abs](http://arxiv.org/abs/1502.04622v1) - [pdf](http://arxiv.org/pdf/1502.04622v1)

> Additive regression trees are flexible non-parametric models and popular off-the-shelf tools for real-world non-linear regression. In application domains, such as bioinformatics, where there is also demand for probabilistic predictions with measures of uncertainty, the Bayesian additive regression trees (BART) model, introduced by Chipman et al. (2010), is increasingly popular. As data sets have grown in size, however, the standard Metropolis-Hastings algorithms used to perform inference in BART are proving inadequate. In particular, these Markov chains make local changes to the trees and suffer from slow mixing when the data are high-dimensional or the best fitting trees are more than a few layers deep. We present a novel sampler for BART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and a top-down particle filtering algorithm for Bayesian decision trees (Lakshminarayanan et al., 2013). Rather than making local changes to individual trees, the PG sampler proposes a complete tree to fit the residual. Experiments show that the PG sampler outperforms existing samplers in many settings.

</details>

<details>

<summary>2015-02-16 21:17:52 - ICR: Iterative Convex Refinement for Sparse Signal Recovery Using Spike and Slab Priors</summary>

- *Hojjat S. Mousavi, Vishal Monga, Trac D. Tran*

- `1502.04726v1` - [abs](http://arxiv.org/abs/1502.04726v1) - [pdf](http://arxiv.org/pdf/1502.04726v1)

> In this letter, we address sparse signal recovery using spike and slab priors. In particular, we focus on a Bayesian framework where sparsity is enforced on reconstruction coefficients via probabilistic priors. The optimization resulting from spike and slab prior maximization is known to be a hard non-convex problem, and existing solutions involve simplifying assumptions and/or relaxations. We propose an approach called Iterative Convex Refinement (ICR) that aims to solve the aforementioned optimization problem directly allowing for greater generality in the sparse structure. Essentially, ICR solves a sequence of convex optimization problems such that sequence of solutions converges to a sub-optimal solution of the original hard optimization problem. We propose two versions of our algorithm: a.) an unconstrained version, and b.) with a non-negativity constraint on sparse coefficients, which may be required in some real-world problems. Experimental validation is performed on both synthetic data and for a real-world image recovery problem, which illustrates merits of ICR over state of the art alternatives.

</details>

<details>

<summary>2015-02-17 10:51:48 - Bayesian Estimation of Smooth Altimetric Parameters: Application to Conventional and Delay/Doppler Altimetry</summary>

- *Abderrahim Halimi, Corinne Mailhes, Jean-Yves Tourneret, Hichem Snoussi*

- `1502.04858v1` - [abs](http://arxiv.org/abs/1502.04858v1) - [pdf](http://arxiv.org/pdf/1502.04858v1)

> This paper proposes a new Bayesian strategy for the smooth estimation of altimetric parameters. The altimetric signal is assumed to be corrupted by a thermal and speckle noise distributed according to an independent and non identically Gaussian distribution. We introduce a prior enforcing a smooth temporal evolution of the altimetric parameters which improves their physical interpretation. The posterior distribution of the resulting model is optimized using a gradient descent algorithm which allows us to compute the maximum a posteriori estimator of the unknown model parameters. This algorithm presents a low computational cost which is suitable for real time applications. The proposed Bayesian strategy and the corresponding estimation algorithm are validated on both synthetic and real data associated with conventional and delay/Doppler altimetry. The analysis of real Jason-2 and Cryosat-2 waveforms shows an improvement in parameter estimation when compared to the state-of-the-art estimation algorithms.

</details>

<details>

<summary>2015-02-17 19:42:27 - Generalized beta convolution model of the true intensity for the Illumina BeadArrays</summary>

- *Rohmatul Fajriyah*

- `1312.5967v2` - [abs](http://arxiv.org/abs/1312.5967v2) - [pdf](http://arxiv.org/pdf/1312.5967v2)

> Microarray data come from many steps of production and have been known to contain noise. The pre-processing is implemented to reduce the noise, where the background is corrected. Prior to further analysis, many Illumina BeadArrays users had applied the convolution model, a model which had been adapted from when it was first developed on the Affymetrix platform, to adjust the intensity value: corrected background intensity value.   Several models based on different underlying distributions and or parameters estimation methods have been proposed and applied. For instance : the exponential-gamma, the normal-gamma and the exponential-normal convolutions with a maximum likelihood estimation, non-parametric, Bayesian and moment methods of the parameters estimation, including two recent exponential-lognormal and gamma-lognormal convolutions.   In this paper, we propose models and derive the corrected background intensity based on the generalized betas and the generalized beta-normal convolutions as a generalization of the existing models.

</details>

<details>

<summary>2015-02-19 03:17:18 - A note on an Adaptive Goodness-of-Fit test with Finite Sample Validity for Random Design Regression Models</summary>

- *Pierpaolo Brutti*

- `1502.05457v1` - [abs](http://arxiv.org/abs/1502.05457v1) - [pdf](http://arxiv.org/pdf/1502.05457v1)

> Given an i.i.d. sample $\{(X_i,Y_i)\}_{i \in \{1 \ldots n\}}$ from the random design regression model $Y = f(X) + \epsilon$ with $(X,Y) \in [0,1] \times [-M,M]$, in this paper we consider the problem of testing the (simple) null hypothesis $f = f_0$, against the alternative $f \neq f_0$ for a fixed $f_0 \in L^2([0,1],G_X)$, where $G_X(\cdot)$ denotes the marginal distribution of the design variable $X$. The procedure proposed is an adaptation to the regression setting of a multiple testing technique introduced by Fromont and Laurent (2005), and it amounts to consider a suitable collection of unbiased estimators of the $L^2$--distance $d_2(f,f_0) = \int {[f(x) - f_0 (x)]^2 d\,G_X (x)}$, rejecting the null hypothesis when at least one of them is greater than its $(1-u_\alpha)$ quantile, with $u_\alpha$ calibrated to obtain a level--$\alpha$ test. To build these estimators, we will use the warped wavelet basis introduced by Picard and Kerkyacharian (2004). We do not assume that the errors are normally distributed, and we do not assume that $X$ and $\epsilon$ are independent but, mainly for technical reasons, we will assume, as in most part of the current literature in learning theory, that $|f(x) - y|$ is uniformly bounded (almost everywhere). We show that our test is adaptive over a particular collection of approximation spaces linked to the classical Besov spaces.

</details>

<details>

<summary>2015-02-19 09:09:27 - Classification and Bayesian Optimization for Likelihood-Free Inference</summary>

- *Michael U. Gutmann, Jukka Corander, Ritabrata Dutta, Samuel Kaski*

- `1502.05503v1` - [abs](http://arxiv.org/abs/1502.05503v1) - [pdf](http://arxiv.org/pdf/1502.05503v1)

> Some statistical models are specified via a data generating process for which the likelihood function cannot be computed in closed form. Standard likelihood-based inference is then not feasible but the model parameters can be inferred by finding the values which yield simulated data that resemble the observed data. This approach faces at least two major difficulties: The first difficulty is the choice of the discrepancy measure which is used to judge whether the simulated data resemble the observed data. The second difficulty is the computationally efficient identification of regions in the parameter space where the discrepancy is low. We give here an introduction to our recent work where we tackle the two difficulties through classification and Bayesian optimization.

</details>

<details>

<summary>2015-02-19 10:47:58 - Estimation for models defined by conditions on their L-moments</summary>

- *Alexis Decurninge, Michel Broniatowski*

- `1409.5928v3` - [abs](http://arxiv.org/abs/1409.5928v3) - [pdf](http://arxiv.org/pdf/1409.5928v3)

> This paper extends the empirical minimum divergence approach for models which satisfy linear constraints with respect to the probability measure of the underlying variable (moment constraints) to the case where such constraints pertain to its quantile measure (called here semi parametric quantile models). The case when these constraints describe shape conditions as handled by the L-moments is considered and both the description of these models as well as the resulting non classical minimum divergence procedures are presented. These models describe neighborhoods of classical models used mainly for their tail behavior, for example neighborhoods of Pareto or Weibull distributions, with which they may share the same first L-moments. A parallel is drawn with similar problems held in elasticity theory and in optimal transport problems. The properties of the resulting estimators are illustrated by simulated examples comparing Maximum Likelihood estimators on Pareto and Weibull models to the minimum Chi-square empirical divergence approach on semi parametric quantile models, and others.

</details>

<details>

<summary>2015-02-20 10:03:07 - A lack-of-fit test for quantile regression models with high-dimensional covariates</summary>

- *Mercedes Conde-Amboage, César Sánchez-Sellero, Wenceslao González-Manteiga*

- `1502.05815v1` - [abs](http://arxiv.org/abs/1502.05815v1) - [pdf](http://arxiv.org/pdf/1502.05815v1)

> We propose a new lack-of-fit test for quantile regression models that is suitable even with high-dimensional covariates. The test is based on the cumulative sum of residuals with respect to unidimensional linear projections of the covariates. The test adapts concepts proposed by Escanciano (Econometric Theory, 22, 2006) to cope with many covariates to the test proposed by He and Zhu (Journal of the American Statistical Association, 98, 2003). To approximate the critical values of the test, a wild bootstrap mechanism is used, similar to that proposed by Feng et al. (Biometrika, 98, 2011). An extensive simulation study was undertaken that shows the good performance of the new test, particularly when the dimension of the covariate is high. The test can also be applied and performs well under heteroscedastic regression models. The test is illustrated with real data about the economic growth of 161 countries.

</details>

<details>

<summary>2015-02-21 00:30:56 - Tail dependence convergence rate for the bivariate skew normal under the equal-skewness condition</summary>

- *Thomas Fung, Eugene Seneta*

- `1502.06046v1` - [abs](http://arxiv.org/abs/1502.06046v1) - [pdf](http://arxiv.org/pdf/1502.06046v1)

> We derive the rate of decay of the tail dependence of the bivariate skew normal distribution under the equal-skewness condition {\theta}1 = {\theta}2,= {\theta}, say. The rate of convergence depends on whether {\theta} > 0 or {\theta} < 0. The latter case gives rate asymp- totically identical with the case {\theta} = 0. The asymptotic behaviour of the quantile function for the univariate skew normal is part of the theoretical development.

</details>

<details>

<summary>2015-02-22 15:53:22 - Mixture models with a prior on the number of components</summary>

- *Jeffrey W. Miller, Matthew T. Harrison*

- `1502.06241v1` - [abs](http://arxiv.org/abs/1502.06241v1) - [pdf](http://arxiv.org/pdf/1502.06241v1)

> A natural Bayesian approach for mixture models with an unknown number of components is to take the usual finite mixture model with Dirichlet weights, and put a prior on the number of components---that is, to use a mixture of finite mixtures (MFM). While inference in MFMs can be done with methods such as reversible jump Markov chain Monte Carlo, it is much more common to use Dirichlet process mixture (DPM) models because of the relative ease and generality with which DPM samplers can be applied. In this paper, we show that, in fact, many of the attractive mathematical properties of DPMs are also exhibited by MFMs---a simple exchangeable partition distribution, restaurant process, random measure representation, and in certain cases, a stick-breaking representation. Consequently, the powerful methods developed for inference in DPMs can be directly applied to MFMs as well. We illustrate with simulated and real data, including high-dimensional gene expression data.

</details>

<details>

<summary>2015-02-22 18:13:29 - Generative Deep Deconvolutional Learning</summary>

- *Yunchen Pu, Xin Yuan, Lawrence Carin*

- `1412.6039v3` - [abs](http://arxiv.org/abs/1412.6039v3) - [pdf](http://arxiv.org/pdf/1412.6039v3)

> A generative Bayesian model is developed for deep (multi-layer) convolutional dictionary learning. A novel probabilistic pooling operation is integrated into the deep model, yielding efficient bottom-up and top-down probabilistic learning. After learning the deep convolutional dictionary, testing is implemented via deconvolutional inference. To speed up this inference, a new statistical approach is proposed to project the top-layer dictionary elements to the data level. Following this, only one layer of deconvolution is required during testing. Experimental results demonstrate powerful capabilities of the model to learn multi-layer features from images. Excellent classification results are obtained on both the MNIST and Caltech 101 datasets.

</details>

<details>

<summary>2015-02-22 23:20:52 - Mixtures, envelopes, and hierarchical duality</summary>

- *Nicholas G. Polson, James G. Scott*

- `1406.0177v2` - [abs](http://arxiv.org/abs/1406.0177v2) - [pdf](http://arxiv.org/pdf/1406.0177v2)

> We develop a connection between mixture and envelope representations of objective functions that arise frequently in statistics. We refer to this connection using the term "hierarchical duality." Our results suggest an interesting and previously under-exploited relationship between marginalization and profiling, or equivalently between the Fenchel--Moreau theorem for convex functions and the Bernstein--Widder theorem for Laplace transforms. We give several different sets of conditions under which such a duality result obtains. We then extend existing work on envelope representations in several ways, including novel generalizations to variance-mean models and to multivariate Gaussian location models. This turns out to provide an elegant missing-data interpretation of the proximal gradient method, a widely used algorithm in machine learning. We show several statistical applications in which the proposed framework leads to easily implemented algorithms, including a robust version of the fused lasso, nonlinear quantile regression via trend filtering, and the binomial fused double Pareto model. Code for the examples is available on GitHub at https://github.com/jgscott/hierduals.

</details>

<details>

<summary>2015-02-23 04:09:52 - Differentially Private Bayesian Optimization</summary>

- *Matt J. Kusner, Jacob R. Gardner, Roman Garnett, Kilian Q. Weinberger*

- `1501.04080v2` - [abs](http://arxiv.org/abs/1501.04080v2) - [pdf](http://arxiv.org/pdf/1501.04080v2)

> Bayesian optimization is a powerful tool for fine-tuning the hyper-parameters of a wide variety of machine learning models. The success of machine learning has led practitioners in diverse real-world settings to learn classifiers for practical problems. As machine learning becomes commonplace, Bayesian optimization becomes an attractive method for practitioners to automate the process of classifier hyper-parameter tuning. A key observation is that the data used for tuning models in these settings is often sensitive. Certain data such as genetic predisposition, personal email statistics, and car accident history, if not properly private, may be at risk of being inferred from Bayesian optimization outputs. To address this, we introduce methods for releasing the best hyper-parameters and classifier accuracy privately. Leveraging the strong theoretical guarantees of differential privacy and known Bayesian optimization convergence bounds, we prove that under a GP assumption these private quantities are also near-optimal. Finally, even if this assumption is not satisfied, we can use different smoothness guarantees to protect privacy.

</details>

<details>

<summary>2015-02-23 15:36:14 - Bayesian Model Comparison in Genetic Association Analysis: Linear Mixed Modeling and SNP Set Testing</summary>

- *Xiaoquan Wen*

- `1404.7197v2` - [abs](http://arxiv.org/abs/1404.7197v2) - [pdf](http://arxiv.org/pdf/1404.7197v2)

> We consider the problems of hypothesis testing and model comparison under a flexible Bayesian linear regression model whose formulation is closely connected with the linear mixed effect model and the parametric models for SNP set analysis in genetic association studies. We derive a class of analytic approximate Bayes factors and illustrate their connections with a variety of frequentist test statistics, including the Wald statistic and the variance component score statistic. Taking advantage of Bayesian model averaging and hierarchical modeling, we demonstrate some distinct advantages and flexibilities in the approaches utilizing the derived Bayes factors in the context of genetic association studies. We demonstrate our proposed methods using real or simulated numerical examples in applications of single SNP association testing, multi-locus fine-mapping and SNP set association testing.

</details>

<details>

<summary>2015-02-24 15:02:07 - On a Poissonian Change-Point Model with Variable Jump Size</summary>

- *Serguei Dachian, Lin Yang*

- `1403.7866v2` - [abs](http://arxiv.org/abs/1403.7866v2) - [pdf](http://arxiv.org/pdf/1403.7866v2)

> A model of Poissonian observation having a jump (change-point) in the intensity function is considered. Two cases are studied. The first one corresponds to the situation when the jump size converges to a non-zero limit, while in the second one the limit is zero. The limiting likelihood ratios in these two cases are quite different. In the first case, like in the case of a fixed jump size, the normalized likelihood ratio converges to a log Poisson process. In the second case, the normalized likelihood ratio converges to a log Wiener process, and so, the statistical problems of parameter estimation and hypotheses testing are asymptotically equivalent in this case to the well known problems of change-point estimation and testing for the model of a signal in white Gaussian noise. The properties of the maximum likelihood and Bayesian estimators, as well as those of the general likelihood ratio, Wald's and Bayesian tests are deduced form the convergence of normalized likelihood ratios. The convergence of the moments of the estimators is also established. The obtained theoretical results are illustrated by numerical simulations.

</details>

<details>

<summary>2015-02-24 16:43:05 - Scalable Variational Inference in Log-supermodular Models</summary>

- *Josip Djolonga, Andreas Krause*

- `1502.06531v2` - [abs](http://arxiv.org/abs/1502.06531v2) - [pdf](http://arxiv.org/pdf/1502.06531v2)

> We consider the problem of approximate Bayesian inference in log-supermodular models. These models encompass regular pairwise MRFs with binary variables, but allow to capture high-order interactions, which are intractable for existing approximate inference techniques such as belief propagation, mean field, and variants. We show that a recently proposed variational approach to inference in log-supermodular models -L-FIELD- reduces to the widely-studied minimum norm problem for submodular minimization. This insight allows to leverage powerful existing tools, and hence to solve the variational problem orders of magnitude more efficiently than previously possible. We then provide another natural interpretation of L-FIELD, demonstrating that it exactly minimizes a specific type of R\'enyi divergence measure. This insight sheds light on the nature of the variational approximations produced by L-FIELD. Furthermore, we show how to perform parallel inference as message passing in a suitable factor graph at a linear convergence rate, without having to sum up over all the configurations of the factor. Finally, we apply our approach to a challenging image segmentation task. Our experiments confirm scalability of our approach, high quality of the marginals, and the benefit of incorporating higher-order potentials.

</details>

<details>

<summary>2015-02-24 19:55:05 - Approximate Bayesian Computation with composite score functions</summary>

- *Erlis Ruli, Nicola Sartori, Laura Ventura*

- `1311.7286v4` - [abs](http://arxiv.org/abs/1311.7286v4) - [pdf](http://arxiv.org/pdf/1311.7286v4)

> Both Approximate Bayesian Computation (ABC) and composite likelihood methods are useful for Bayesian and frequentist inference, respectively, when the likelihood function is intractable. We propose to use composite likelihood score functions as summary statistics in ABC in order to obtain accurate approximations to the posterior distribution. This is motivated by the use of the score function of the full likelihood, and extended to general unbiased estimating functions in complex models. Moreover, we show that if the composite score is suitably standardised, the resulting ABC procedure is invariant to reparameterisations and automatically adjusts the curvature of the composite likelihood, and of the corresponding posterior distribution. The method is illustrated through examples with simulated data, and an application to modelling of spatial extreme rainfall data is discussed.

</details>

<details>

<summary>2015-02-25 18:48:58 - Wishart Generator Distribution</summary>

- *A. Bekker, M. Arashi, J. van Niekerk*

- `1502.07300v1` - [abs](http://arxiv.org/abs/1502.07300v1) - [pdf](http://arxiv.org/pdf/1502.07300v1)

> The Wishart distribution and its generalizations are among the most prominent probability distributions in multivariate statistical analysis, arising naturally in applied research and as a basis for theoretical models. In this paper, we generalize the Wishart distribution utilizing a different approach that leads to the Wishart generator distribution with the Wishart distribution as a special case. It is not restricted, however some special cases are exhibited. Important statistical characteristics of the Wishart generator distribution are derived from the matrix theory viewpoint. Estimation is also touched upon as a guide for further research from the classical approach as well as from the Bayesian paradigm. The paper is concluded by giving applications of two special cases of this distribution in calculating the product of beta functions and astronomy.

</details>

<details>

<summary>2015-02-26 11:11:18 - A mixed effect model for bivariate meta-analysis of diagnostic test accuracy studies using a copula representation of the random effects distribution</summary>

- *Aristidis K. Nikoloulopoulos*

- `1502.07505v1` - [abs](http://arxiv.org/abs/1502.07505v1) - [pdf](http://arxiv.org/pdf/1502.07505v1)

> Diagnostic test accuracy studies typically report the number of true positives, false positives, true negatives and false negatives. There usually exists a negative association between the number of true positives and true negatives, because studies that adopt less stringent criterion for declaring a test positive invoke higher sensitivities and lower specificities. A generalized linear mixed model (GLMM) is currently recommended to synthesize diagnostic test accuracy studies. We propose a copula mixed model for bivariate meta-analysis of diagnostic test accuracy studies. Our general model includes the GLMM as a special case and can also operate on the original scale of sensitivity and specificity. Summary receiver operating characteristic curves are deduced for the proposed model through quantile regression techniques and different characterizations of the bivariate random effects distribution. Our general methodology is demonstrated with an extensive simulation study and illustrated by re-analysing the data of two published meta-analyses. Our study suggests that there can be an improvement on GLMM in fit to data and makes the argument for moving to copula random effects models. Our modelling framework is implemented in the package CopulaREMADA within the open source statistical environment R.

</details>

<details>

<summary>2015-02-26 17:14:48 - A discussion of "Bayesian model selection based on proper scoring rules" by A.P. Dawid and M. Musio</summary>

- *Clara Grazian, Ilaria Masiani, Christian P. Robert*

- `1502.07638v1` - [abs](http://arxiv.org/abs/1502.07638v1) - [pdf](http://arxiv.org/pdf/1502.07638v1)

> This note is a discussion of the article "Bayesian model selection based on proper scoring rules" by A.P. Dawid and M. Musio, to appear in Bayesian Analysis. While appreciating the concepts behind the use of proper scoring rules, including the inclusion of improper priors, we point out here some possible practical difficulties with the advocated approach.

</details>

<details>

<summary>2015-02-27 03:32:49 - Minimum message length estimation of mixtures of multivariate Gaussian and von Mises-Fisher distributions</summary>

- *Parthan Kasarapu, Lloyd Allison*

- `1502.07813v1` - [abs](http://arxiv.org/abs/1502.07813v1) - [pdf](http://arxiv.org/pdf/1502.07813v1)

> Mixture modelling involves explaining some observed evidence using a combination of probability distributions. The crux of the problem is the inference of an optimal number of mixture components and their corresponding parameters. This paper discusses unsupervised learning of mixture models using the Bayesian Minimum Message Length (MML) criterion. To demonstrate the effectiveness of search and inference of mixture parameters using the proposed approach, we select two key probability distributions, each handling fundamentally different types of data: the multivariate Gaussian distribution to address mixture modelling of data distributed in Euclidean space, and the multivariate von Mises-Fisher (vMF) distribution to address mixture modelling of directional data distributed on a unit hypersphere. The key contributions of this paper, in addition to the general search and inference methodology, include the derivation of MML expressions for encoding the data using multivariate Gaussian and von Mises-Fisher distributions, and the analytical derivation of the MML estimates of the parameters of the two distributions. Our approach is tested on simulated and real world data sets. For instance, we infer vMF mixtures that concisely explain experimentally determined three-dimensional protein conformations, providing an effective null model description of protein structures that is central to many inference problems in structural bioinformatics. The experimental results demonstrate that the performance of our proposed search and inference method along with the encoding schemes improve on the state of the art mixture modelling techniques.

</details>

<details>

<summary>2015-02-27 08:05:06 - Estimation in a change-point nonlinear quantile model</summary>

- *Gabriela Ciuperca*

- `1401.4883v3` - [abs](http://arxiv.org/abs/1401.4883v3) - [pdf](http://arxiv.org/pdf/1401.4883v3)

> This paper considers a nonlinear quantile model with change-points. The quantile estimation method, which as a particular case includes median model, is more robust with respect to other traditional methods when model errors contain outliers. Under relatively weak assumptions, the convergence rate and asymptotic distribution of change-point and of regression parameter estimators are obtained. Numerical study by Monte Carlo simulations shows the performance of the proposed method for nonlinear model with change-points.

</details>

<details>

<summary>2015-02-27 12:21:53 - Fast Bayesian Optimal Experimental Design for Seismic Source Inversion</summary>

- *Quan Long, Mohammad Motamed, Raul Tempone*

- `1502.07873v1` - [abs](http://arxiv.org/abs/1502.07873v1) - [pdf](http://arxiv.org/pdf/1502.07873v1)

> We develop a fast method for optimally designing experiments in the context of statistical seismic source inversion. In particular, we efficiently compute the optimal number and locations of the receivers or seismographs. The seismic source is modeled by a point moment tensor multiplied by a time-dependent function. The parameters include the source location, moment tensor components, and start time and frequency in the time function. The forward problem is modeled by elastodynamic wave equations. We show that the Hessian of the cost functional, which is usually defined as the square of the weighted L2 norm of the difference between the experimental data and the simulated data, is proportional to the measurement time and the number of receivers. Consequently, the posterior distribution of the parameters, in a Bayesian setting, concentrates around the "true" parameters, and we can employ Laplace approximation and speed up the estimation of the expected Kullback-Leibler divergence (expected information gain), the optimality criterion in the experimental design procedure. Since the source parameters span several magnitudes, we use a scaling matrix for efficient control of the condition number of the original Hessian matrix. We use a second-order accurate finite difference method to compute the Hessian matrix and either sparse quadrature or Monte Carlo sampling to carry out numerical integration. We demonstrate the efficiency, accuracy, and applicability of our method on a two-dimensional seismic source inversion problem.

</details>

<details>

<summary>2015-02-27 18:56:45 - Second-order Quantile Methods for Experts and Combinatorial Games</summary>

- *Wouter M. Koolen, Tim van Erven*

- `1502.08009v1` - [abs](http://arxiv.org/abs/1502.08009v1) - [pdf](http://arxiv.org/pdf/1502.08009v1)

> We aim to design strategies for sequential decision making that adjust to the difficulty of the learning problem. We study this question both in the setting of prediction with expert advice, and for more general combinatorial decision tasks. We are not satisfied with just guaranteeing minimax regret rates, but we want our algorithms to perform significantly better on easy data. Two popular ways to formalize such adaptivity are second-order regret bounds and quantile bounds. The underlying notions of 'easy data', which may be paraphrased as "the learning problem has small variance" and "multiple decisions are useful", are synergetic. But even though there are sophisticated algorithms that exploit one of the two, no existing algorithm is able to adapt to both.   In this paper we outline a new method for obtaining such adaptive algorithms, based on a potential function that aggregates a range of learning rates (which are essential tuning parameters). By choosing the right prior we construct efficient algorithms and show that they reap both benefits by proving the first bounds that are both second-order and incorporate quantiles.

</details>

<details>

<summary>2015-02-28 18:28:27 - Are Gibbs-type priors the most natural generalization of the Dirichlet process?</summary>

- *P. De Blasi, S. Favaro, A. Lijoi, R. H. Mena, I. Pruenster, M. Ruggiero*

- `1503.00163v1` - [abs](http://arxiv.org/abs/1503.00163v1) - [pdf](http://arxiv.org/pdf/1503.00163v1)

> Discrete random probability measures and the exchangeable random partitions they induce are key tools for addressing a variety of estimation and prediction problems in Bayesian inference. Indeed, many popular nonparametric priors, such as the Dirichlet and the Pitman-Yor process priors, select discrete probability distributions almost surely and, therefore, automatically induce exchangeable random partitions. Here we focus on the family of Gibbs-type priors, a recent and elegant generalization of the Dirichlet and the Pitman-Yor process priors. These random probability measures share properties that are appealing both from a theoretical and an applied point of view: (i) they admit an intuitive characterization in terms of their predictive structure justifying their use in terms of a precise assumption on the learning mechanism; (ii) they stand out in terms of mathematical tractability; (iii) they include several interesting special cases besides the Dirichlet and the Pitman-Yor processes. The goal of our paper is to provide a systematic and unified treatment of Gibbs-type priors and highlight their implications for Bayesian nonparametric inference. We will deal with their distributional properties, the resulting estimators, frequentist asymptotic validation and the construction of time-dependent versions. Applications, mainly concerning hierarchical mixture models and species sampling, will serve to convey the main ideas. The intuition inherent to this class of priors and the neat results that can be deduced for it lead one to wonder whether it actually represents the most natural generalization of the Dirichlet process.

</details>

<details>

<summary>2015-02-28 20:56:05 - Quantile Regression for Location-Scale Time Series Models with Conditional Heteroscedasticity</summary>

- *Jungsik Noh, Sangyeol Lee*

- `1401.0688v2` - [abs](http://arxiv.org/abs/1401.0688v2) - [pdf](http://arxiv.org/pdf/1401.0688v2)

> This paper considers quantile regression for a wide class of time series models including ARMA models with asymmetric GARCH (AGARCH) errors. The classical mean-variance models are reinterpreted as conditional location-scale models so that the quantile regression method can be naturally geared into the considered models. The consistency and asymptotic normality of the quantile regression estimator is established in location-scale time series models under mild conditions. In the application of this result to ARMA-AGARCH models, more primitive conditions are deduced to obtain the asymptotic properties. For illustration, a simulation study and a real data analysis are provided.

</details>


## 2015-03

<details>

<summary>2015-03-01 12:50:15 - Biased Online Parameter Inference for State-Space Models</summary>

- *Yan Zhou, Ajay Jasra*

- `1503.00266v1` - [abs](http://arxiv.org/abs/1503.00266v1) - [pdf](http://arxiv.org/pdf/1503.00266v1)

> We consider Bayesian online static parameter estimation for state-space models. This is a very important problem, but is very computationally challenging as the state- of-the art methods that are exact, often have a computational cost that grows with the time parameter; perhaps the most successful algorithm is that of SMC2 [9]. We present a version of the SMC2 algorithm which has computational cost that does not grow with the time parameter. In addition, under assumptions, the algorithm is shown to provide consistent estimates of expectations w.r.t. the posterior. However, the cost to achieve this consistency can be exponential in the dimension of the parameter space; if this exponential cost is avoided, typically the algorithm is biased. The bias is investigated from a theoretical perspective and, under assumptions, we find that the bias does not accumulate as the time parameter grows. The algorithm is implemented on several Bayesian statistical models.

</details>

<details>

<summary>2015-03-01 21:38:49 - Consistency of Importance Sampling estimates based on dependent sample sets and an application to models with factorizing likelihoods</summary>

- *Ingmar Schuster*

- `1503.00357v1` - [abs](http://arxiv.org/abs/1503.00357v1) - [pdf](http://arxiv.org/pdf/1503.00357v1)

> In this paper, I proof that Importance Sampling estimates based on dependent sample sets are consistent under certain conditions. This can be used to reduce variance in Bayesian Models with factorizing likelihoods, using sample sets that are much larger than the number of likelihood evaluations, a technique dubbed Sample Inflation. I evaluate Sample Inflation on a toy Gaussian problem and two Mixture Models.

</details>

<details>

<summary>2015-03-02 20:23:18 - Bayesian Optimization of Text Representations</summary>

- *Dani Yogatama, Noah A. Smith*

- `1503.00693v1` - [abs](http://arxiv.org/abs/1503.00693v1) - [pdf](http://arxiv.org/pdf/1503.00693v1)

> When applying machine learning to problems in NLP, there are many choices to make about how to represent input texts. These choices can have a big effect on performance, but they are often uninteresting to researchers or practitioners who simply need a module that performs well. We propose an approach to optimizing over this space of choices, formulating the problem as global optimization. We apply a sequential model-based optimization technique and show that our method makes standard linear models competitive with more sophisticated, expensive state-of-the-art methods based on latent variable models or neural networks on various topic classification and sentiment analysis problems. Our approach is a first step towards black-box NLP systems that work with raw text and do not require manual tuning.

</details>

<details>

<summary>2015-03-03 23:25:55 - The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification</summary>

- *Been Kim, Cynthia Rudin, Julie Shah*

- `1503.01161v1` - [abs](http://arxiv.org/abs/1503.01161v1) - [pdf](http://arxiv.org/pdf/1503.01161v1)

> We present the Bayesian Case Model (BCM), a general framework for Bayesian case-based reasoning (CBR) and prototype classification and clustering. BCM brings the intuitive power of CBR to a Bayesian generative framework. The BCM learns prototypes, the "quintessential" observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. Simultaneously, BCM pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. The prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy. Human subject experiments verify statistically significant improvements to participants' understanding when using explanations produced by BCM, compared to those given by prior art.

</details>

<details>

<summary>2015-03-04 08:35:43 - Reconstructing past temperatures from natural proxies and estimated climate forcings using short- and long-memory models</summary>

- *Luis Barboza, Bo Li, Martin P. Tingley, Frederi G. Viens*

- `1403.3260v2` - [abs](http://arxiv.org/abs/1403.3260v2) - [pdf](http://arxiv.org/pdf/1403.3260v2)

> We produce new reconstructions of Northern Hemisphere annually averaged temperature anomalies back to 1000 AD, and explore the effects of including external climate forcings within the reconstruction and of accounting for short-memory and long-memory features. Our reconstructions are based on two linear models, with the first linking the latent temperature series to three main external forcings (solar irradiance, greenhouse gas concentration and volcanism), and the second linking the observed temperature proxy data (tree rings, sediment record, ice cores, etc.) to the unobserved temperature series. Uncertainty is captured with additive noise, and a rigorous statistical investigation of the correlation structure in the regression errors is conducted through systematic comparisons between reconstructions that assume no memory, short-memory autoregressive models, and long-memory fractional Gaussian noise models. We use Bayesian estimation to fit the model parameters and to perform separate reconstructions of land-only and combined land-and-marine temperature anomalies. For model formulations that include forcings, both exploratory and Bayesian data analysis provide evidence against models with no memory. Model assessments indicate that models with no memory underestimate uncertainty. However, no single line of evidence is sufficient to favor short-memory models over long-memory ones, or to favor the opposite choice. When forcings are not included, the long-memory models appear to be necessary. While including external climate forcings substantially improves the reconstruction, accurate reconstructions that exclude these forcings are vital for testing the fidelity of climate models used for future projections.

</details>

<details>

<summary>2015-03-04 13:48:28 - Equivalence testing for functional data with an application to comparing pulmonary function devices</summary>

- *Colin B. Fogarty, Dylan S. Small*

- `1407.5079v2` - [abs](http://arxiv.org/abs/1407.5079v2) - [pdf](http://arxiv.org/pdf/1407.5079v2)

> Equivalence testing for scalar data has been well addressed in the literature, however, the same cannot be said for functional data. The resultant complexity from maintaining the functional structure of the data, rather than using a scalar transformation to reduce dimensionality, renders the existing literature on equivalence testing inadequate for the desired inference. We propose a framework for equivalence testing for functional data within both the frequentist and Bayesian paradigms. This framework combines extensions of scalar methodologies with new methodology for functional data. Our frequentist hypothesis test extends the Two One-Sided Testing (TOST) procedure for equivalence testing to the functional regime. We conduct this TOST procedure through the use of the nonparametric bootstrap. Our Bayesian methodology employs a functional analysis of variance model, and uses a flexible class of Gaussian Processes for both modeling our data and as prior distributions. Through our analysis, we introduce a model for heteroscedastic variances within a Gaussian Process by modeling variance curves via Log-Gaussian Process priors. We stress the importance of choosing prior distributions that are commensurate with the prior state of knowledge and evidence regarding practical equivalence. We illustrate these testing methods through data from an ongoing method comparison study between two devices for pulmonary function testing. In so doing, we provide not only concrete motivation for equivalence testing for functional data, but also a blueprint for researchers who hope to conduct similar inference.

</details>

<details>

<summary>2015-03-04 18:03:16 - Quantifying Uncertainty in Stochastic Models with Parametric Variability</summary>

- *Kyle S. Hickmann, James M. Hyman, Sara Y. Del Valle*

- `1503.01401v1` - [abs](http://arxiv.org/abs/1503.01401v1) - [pdf](http://arxiv.org/pdf/1503.01401v1)

> We present a method to quantify uncertainty in the predictions made by simulations of mathematical models that can be applied to a broad class of stochastic, discrete, and differential equation models. Quantifying uncertainty is crucial for determining how accurate the model predictions are and identifying which input parameters affect the outputs of interest. Most of the existing methods for uncertainty quantification require many samples to generate accurate results, are unable to differentiate where the uncertainty is coming from (e.g., parameters or model assumptions), or require a lot of computational resources. Our approach addresses these challenges and opportunities by allowing different types of uncertainty, that is, uncertainty in input parameters as well as uncertainty created through stochastic model components. This is done by combining the Karhunen-Loeve decomposition, polynomial chaos expansion, and Bayesian Gaussian process regression to create a statistical surrogate for the stochastic model. The surrogate separates the analysis of variation arising through stochastic simulation and variation arising through uncertainty in the model parameterization. We illustrate our approach by quantifying the uncertainty in a stochastic ordinary differential equation epidemic model. Specifically, we estimate four quantities of interest for the epidemic model and show agreement between the surrogate and the actual model results.

</details>

<details>

<summary>2015-03-04 20:03:23 - Heteroscedastic Treed Bayesian Optimisation</summary>

- *John-Alexander M. Assael, Ziyu Wang, Bobak Shahriari, Nando de Freitas*

- `1410.7172v2` - [abs](http://arxiv.org/abs/1410.7172v2) - [pdf](http://arxiv.org/pdf/1410.7172v2)

> Optimising black-box functions is important in many disciplines, such as tuning machine learning models, robotics, finance and mining exploration. Bayesian optimisation is a state-of-the-art technique for the global optimisation of black-box functions which are expensive to evaluate. At the core of this approach is a Gaussian process prior that captures our belief about the distribution over functions. However, in many cases a single Gaussian process is not flexible enough to capture non-stationarity in the objective function. Consequently, heteroscedasticity negatively affects performance of traditional Bayesian methods. In this paper, we propose a novel prior model with hierarchical parameter learning that tackles the problem of non-stationarity in Bayesian optimisation. Our results demonstrate substantial improvements in a wide range of applications, including automatic machine learning and mining exploration.

</details>

<details>

<summary>2015-03-04 21:25:31 - An Entropy Search Portfolio for Bayesian Optimization</summary>

- *Bobak Shahriari, Ziyu Wang, Matthew W. Hoffman, Alexandre Bouchard-Côté, Nando de Freitas*

- `1406.4625v4` - [abs](http://arxiv.org/abs/1406.4625v4) - [pdf](http://arxiv.org/pdf/1406.4625v4)

> Bayesian optimization is a sample-efficient method for black-box global optimization. How- ever, the performance of a Bayesian optimization method very much depends on its exploration strategy, i.e. the choice of acquisition function, and it is not clear a priori which choice will result in superior performance. While portfolio methods provide an effective, principled way of combining a collection of acquisition functions, they are often based on measures of past performance which can be misleading. To address this issue, we introduce the Entropy Search Portfolio (ESP): a novel approach to portfolio construction which is motivated by information theoretic considerations. We show that ESP outperforms existing portfolio methods on several real and synthetic problems, including geostatistical datasets and simulated control tasks. We not only show that ESP is able to offer performance as good as the best, but unknown, acquisition function, but surprisingly it often gives better performance. Finally, over a wide range of conditions we find that ESP is robust to the inclusion of poor acquisition functions.

</details>

<details>

<summary>2015-03-05 00:44:04 - Optimal Bayesian estimation in random covariate design with a rescaled Gaussian process prior</summary>

- *Debdeep Pati, Anirban Bhattacharya, Guang Cheng*

- `1411.7420v2` - [abs](http://arxiv.org/abs/1411.7420v2) - [pdf](http://arxiv.org/pdf/1411.7420v2)

> In Bayesian nonparametric models, Gaussian processes provide a popular prior choice for regression function estimation. Existing literature on the theoretical investigation of the resulting posterior distribution almost exclusively assume a fixed design for covariates. The only random design result we are aware of (van der Vaart & van Zanten, 2011) assumes the assigned Gaussian process to be supported on the smoothness class specified by the true function with probability one. This is a fairly restrictive assumption as it essentially rules out the Gaussian process prior with a squared exponential kernel when modeling rougher functions. In this article, we show that an appropriate rescaling of the above Gaussian process leads to a rate-optimal posterior distribution even when the covariates are independently realized from a known density on a compact set. The proofs are based on deriving sharp concentration inequalities for frequentist kernel estimators; the results might be of independent interest.

</details>

<details>

<summary>2015-03-05 22:21:50 - High-dimensional Ising model selection with Bayesian information criteria</summary>

- *Rina Foygel Barber, Mathias Drton*

- `1403.3374v2` - [abs](http://arxiv.org/abs/1403.3374v2) - [pdf](http://arxiv.org/pdf/1403.3374v2)

> We consider the use of Bayesian information criteria for selection of the graph underlying an Ising model. In an Ising model, the full conditional distributions of each variable form logistic regression models, and variable selection techniques for regression allow one to identify the neighborhood of each node and, thus, the entire graph. We prove high-dimensional consistency results for this pseudo-likelihood approach to graph selection when using Bayesian information criteria for the variable selection problems in the logistic regressions. The results pertain to scenarios of sparsity and following related prior work the information criteria we consider incorporate an explicit prior that encourages sparsity.

</details>

<details>

<summary>2015-03-06 01:32:13 - High dimensional Bayesian inference for Gaussian directed acyclic graph models</summary>

- *Emanuel Ben-David, Tianxi Li, Helene Massam, Bala Rajaratnam*

- `1109.4371v5` - [abs](http://arxiv.org/abs/1109.4371v5) - [pdf](http://arxiv.org/pdf/1109.4371v5)

> In this paper, we consider Gaussian models Markov with respect to an arbitrary DAG. We first construct a family of conjugate priors for the Cholesky parametrization of the covariance matrix of such models. This family has as many shape parameters as the DAG has vertices, and naturally extends the work of Geiger and Heckerman [8]. From these distributions, we derive prior distributions for the covariance and precision parameters of the Gaussian DAG Markov models. Our works thus extends the work of Dawid and Lauritzen [5] and Letac and Massam [16] for Gaussian models Markov with respect to a decomposable graph to arbitrary DAGs. For this reason, we call our distributions DAG-Wishart distributions. An advantage of these distributions is that they possess strong hyper Markov properties and thus allow for explicit estimation of the covariance and precision parameters, regardless of the dimension of the problem. They also allow us to develop methodology for model selection and covariance estimation in the space of DAG-Markov models. We demonstrate via several numerical examples that the proposed method scales well to high-dimensions.

</details>

<details>

<summary>2015-03-06 10:17:28 - Joint Detection and Super-Resolution Estimation of Multipath Signal Parameters Using Incremental Automatic Relevance Determination</summary>

- *Dmitriy Shutin, Nicolas Schneckenburger*

- `1503.01898v1` - [abs](http://arxiv.org/abs/1503.01898v1) - [pdf](http://arxiv.org/pdf/1503.01898v1)

> The presented work investigates a sparse Bayesian incremental automatic relevance determination (IARD) algorithm in the context of multipath parameter estimation in a super-resolution regime. The corresponding estimation problem is highly nonlinear and, in general, requires an estimation of the number of multipath components. In the IARD approach individual multipath components are processed sequentially, which permits a tractable convergence analysis of the corresponding inference expressions. This leads to a simple condition, termed here a pruning condition, that determines if a multipath component is "sparsified" or retained in the model, thus permitting a fast and adaptive realization of the estimation algorithm. Yet previous experiments demonstrated that IARD fails to select the correct number of components when the parameters entering nonlinearly the multipath model are also estimated. To understand this effect, an analysis of the statistical structure of the pruning condition is proposed. It is shown that the corresponding test statistic in the pruning condition follows an extreme value distribution. As a result, the standard IARD algorithm implements a statistical test with a very high probability of false alarm. This leads to insertion of estimation artifacts and underestimation of signal sparsity. Moreover, the probability of false alarm worsens as the number of measured signal samples grows. Based on the developed statistical interpretation of the IARD, an optimal adjustment of the pruning condition is proposed. This permits a reliable and efficient removal of estimation artifacts and joint estimation of signal parameters, as well as optimal model order selection within a sparse Bayesian learning framework.

</details>

<details>

<summary>2015-03-06 10:22:12 - A Bayesian Model of node interaction in networks</summary>

- *Ingmar Schuster*

- `1402.4279v2` - [abs](http://arxiv.org/abs/1402.4279v2) - [pdf](http://arxiv.org/pdf/1402.4279v2)

> We are concerned with modeling the strength of links in networks by taking into account how often those links are used. Link usage is a strong indicator of how closely two nodes are related, but existing network models in Bayesian Statistics and Machine Learning are able to predict only wether a link exists at all. As priors for latent attributes of network nodes we explore the Chinese Restaurant Process (CRP) and a multivariate Gaussian with fixed dimensionality. The model is applied to a social network dataset and a word coocurrence dataset.

</details>

<details>

<summary>2015-03-06 11:16:58 - Hamiltonian ABC</summary>

- *Edward Meeds, Robert Leenders, Max Welling*

- `1503.01916v1` - [abs](http://arxiv.org/abs/1503.01916v1) - [pdf](http://arxiv.org/pdf/1503.01916v1)

> Approximate Bayesian computation (ABC) is a powerful and elegant framework for performing inference in simulation-based models. However, due to the difficulty in scaling likelihood estimates, ABC remains useful for relatively low-dimensional problems. We introduce Hamiltonian ABC (HABC), a set of likelihood-free algorithms that apply recent advances in scaling Bayesian learning using Hamiltonian Monte Carlo (HMC) and stochastic gradients. We find that a small number forward simulations can effectively approximate the ABC gradient, allowing Hamiltonian dynamics to efficiently traverse parameter spaces. We also describe a new simple yet general approach of incorporating random seeds into the state of the Markov chain, further reducing the random walk behavior of HABC. We demonstrate HABC on several typical ABC problems, and show that HABC samples comparably to regular Bayesian inference using true gradients on a high-dimensional problem from machine learning.

</details>

<details>

<summary>2015-03-06 14:09:57 - Approximate Models and Robust Decisions</summary>

- *James Watson, Chris Holmes*

- `1402.6118v3` - [abs](http://arxiv.org/abs/1402.6118v3) - [pdf](http://arxiv.org/pdf/1402.6118v3)

> Decisions based partly or solely on predictions from probabilistic models may be sensitive to model misspecification. Statisticians are taught from an early stage that "all models are wrong", but little formal guidance exists on how to assess the impact of model approximation on decision making, or how to proceed when optimal actions appear sensitive to model fidelity. This article presents an overview of recent developments across different disciplines to address this. We review diagnostic techniques, including graphical approaches and summary statistics, to help highlight decisions made through minimised expected loss that are sensitive to model misspecification. We then consider formal methods for decision making under model misspecification by quantifying stability of optimal actions to perturbations to the model within a neighbourhood of model space. This neighbourhood is defined in either one of two ways. Firstly, in a strong sense via an information (Kullback-Leibler) divergence around the approximating model. Or using a nonparametric model extension, again centred at the approximating model, in order to `average out' over possible misspecifications. This is presented in the context of recent work in the robust control, macroeconomics and financial mathematics literature. We adopt a Bayesian approach throughout although the methods are agnostic to this position.

</details>

<details>

<summary>2015-03-06 21:55:10 - Variations of Q-Q Plots -- The Power of our Eyes!</summary>

- *Adam Loy, Lendie Follett, Heike Hofmann*

- `1503.02098v1` - [abs](http://arxiv.org/abs/1503.02098v1) - [pdf](http://arxiv.org/pdf/1503.02098v1)

> In statistical modeling we strive to specify models that resemble data collected in studies or observed from processes. Consequently, distributional specification and parameter estimation are central to parametric models. Graphical procedures, such as the quantile-quantile (Q-Q) plot, are arguably the most widely used method of distributional assessment, though critics find their interpretation to be overly subjective. Formal goodness-of-fit tests are available and are quite powerful, but only indicate whether there is a lack of fit, not why there is lack of fit. In this paper we explore the use of the lineup protocol to inject rigor to graphical distributional assessment and compare its power to that of formal distributional tests. We find that lineups of standard Q-Q plots are more powerful than lineups of de-trended Q-Q plots and that lineup tests are more powerful than traditional tests of normality. While, we focus on diagnosing non-normality, our approach is general and can be directly extended to the assessment of other distributions.

</details>

<details>

<summary>2015-03-06 23:20:39 - A Bayesian partial identification approach to inferring the prevalence of accounting misconduct</summary>

- *P. Richard Hahn, Jared S. Murray, Ioanna Manolopoulou*

- `1407.8430v3` - [abs](http://arxiv.org/abs/1407.8430v3) - [pdf](http://arxiv.org/pdf/1407.8430v3)

> This paper describes the use of flexible Bayesian regression models for estimating a partially identified probability function. Our approach permits efficient sensitivity analysis concerning the posterior impact of priors on the partially identified component of the regression model. The new methodology is illustrated on an important problem where only partially observed data is available - inferring the prevalence of accounting misconduct among publicly traded U.S. businesses.

</details>

<details>

<summary>2015-03-07 14:53:39 - Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data</summary>

- *Yarin Gal, Yutian Chen, Zoubin Ghahramani*

- `1503.02182v1` - [abs](http://arxiv.org/abs/1503.02182v1) - [pdf](http://arxiv.org/pdf/1503.02182v1)

> Multivariate categorical data occur in many applications of machine learning. One of the main difficulties with these vectors of categorical variables is sparsity. The number of possible observations grows exponentially with vector length, but dataset diversity might be poor in comparison. Recent models have gained significant improvement in supervised tasks with this data. These models embed observations in a continuous space to capture similarities between them. Building on these ideas we propose a Bayesian model for the unsupervised task of distribution estimation of multivariate categorical data. We model vectors of categorical variables as generated from a non-linear transformation of a continuous latent space. Non-linearity captures multi-modality in the distribution. The continuous representation addresses sparsity. Our model ties together many existing models, linking the linear categorical latent Gaussian model, the Gaussian process latent variable model, and Gaussian process classification. We derive inference for our model based on recent developments in sampling based variational inference. We show empirically that the model outperforms its linear and discrete counterparts in imputation tasks of sparse data.

</details>

<details>

<summary>2015-03-07 19:50:59 - The Informed Sampler: A Discriminative Approach to Bayesian Inference in Generative Computer Vision Models</summary>

- *Varun Jampani, Sebastian Nowozin, Matthew Loper, Peter V. Gehler*

- `1402.0859v3` - [abs](http://arxiv.org/abs/1402.0859v3) - [pdf](http://arxiv.org/pdf/1402.0859v3)

> Computer vision is hard because of a large variability in lighting, shape, and texture; in addition the image signal is non-additive due to occlusion. Generative models promised to account for this variability by accurately modelling the image formation process as a function of latent variables with prior beliefs. Bayesian posterior inference could then, in principle, explain the observation. While intuitively appealing, generative models for computer vision have largely failed to deliver on that promise due to the difficulty of posterior inference. As a result the community has favoured efficient discriminative approaches. We still believe in the usefulness of generative models in computer vision, but argue that we need to leverage existing discriminative or even heuristic computer vision methods. We implement this idea in a principled way with an "informed sampler" and in careful experiments demonstrate it on challenging generative models which contain renderer programs as their components. We concentrate on the problem of inverting an existing graphics rendering engine, an approach that can be understood as "Inverse Graphics". The informed sampler, using simple discriminative proposals based on existing computer vision technology, achieves significant improvements of inference.

</details>

<details>

<summary>2015-03-07 22:35:13 - Hit and Run ARMS: Adaptive Rejection Metropolis Sampling with Hit and Run Random Direction</summary>

- *Huaiye Zhang, Yuefeng Wu, Lulu Cheng, Inyoung Kim*

- `1503.02222v1` - [abs](http://arxiv.org/abs/1503.02222v1) - [pdf](http://arxiv.org/pdf/1503.02222v1)

> An algorithm for sampling from non-log-concave multivariate distributions is proposed, which improves the adaptive rejection Metropolis sampling (ARMS) algorithm by incorporating the hit and run sampling. It is not rare that the ARMS is trapped away from some subspace with significant probability in the support of the multivariate distribution. While the ARMS updates samples only in the directions that are parallel to dimensions, our proposed method, the hit and run ARMS (HARARMS), updates samples in arbitrary directions determined by the hit and run algorithm, which makes it almost not possible to be trapped in any isolated subspaces. The HARARMS performs the same as ARMS in a single dimension while more reliable in multidimensional spaces. Its performance is illustrated by a Bayesian free-knot spline regression example. We showed that it overcomes the well-known `lethargy' property and decisively find the global optimal number and locations of the knots of the spline function.

</details>

<details>

<summary>2015-03-08 12:51:48 - label.switching: An R Package for Dealing with the Label Switching Problem in MCMC Outputs</summary>

- *Panagiotis Papastamoulis*

- `1503.02271v1` - [abs](http://arxiv.org/abs/1503.02271v1) - [pdf](http://arxiv.org/pdf/1503.02271v1)

> Label switching is a well-known and fundamental problem in Bayesian estimation of mixture or hidden Markov models. In case that the prior distribution of the model parameters is the same for all states, then both the likelihood and posterior distribution are invariant to permutations of the parameters. This property makes Markov chain Monte Carlo (MCMC) samples simulated from the posterior distribution non-identifiable. In this paper, the \pkg{label.switching} package is introduced. It contains one probabilistic and seven deterministic relabelling algorithms in order to post-process a given MCMC sample, provided by the user. Each method returns a set of permutations that can be used to reorder the MCMC output. Then, any parametric function of interest can be inferred using the reordered MCMC sample. A set of user-defined permutations is also accepted, allowing the researcher to benchmark new relabelling methods against the available ones

</details>

<details>

<summary>2015-03-09 08:13:24 - Information criteria for multistep ahead predictions</summary>

- *Keisuke Yano, Fumiyasu Komaki*

- `1503.02390v1` - [abs](http://arxiv.org/abs/1503.02390v1) - [pdf](http://arxiv.org/pdf/1503.02390v1)

> We propose an information criterion for multistep ahead predictions. It is also used for extrapolations. For the derivation, we consider multistep ahead predictions under local misspecification. In the prediction, we show that Bayesian predictive distributions asymptotically have smaller Kullback--Leibler risks than plug-in predictive distributions. From the results, we construct an information criterion for multistep ahead predictions by using an asymptotically unbiased estimator of the Kullback--Leibler risk of Bayesian predictive distributions. We show the effectiveness of the proposed information criterion throughout the numerical experiments.

</details>

<details>

<summary>2015-03-09 17:09:54 - Sublinear-Time Approximate MCMC Transitions for Probabilistic Programs</summary>

- *Yutian Chen, Vikash Mansinghka, Zoubin Ghahramani*

- `1411.1690v2` - [abs](http://arxiv.org/abs/1411.1690v2) - [pdf](http://arxiv.org/pdf/1411.1690v2)

> Probabilistic programming languages can simplify the development of machine learning techniques, but only if inference is sufficiently scalable. Unfortunately, Bayesian parameter estimation for highly coupled models such as regressions and state-space models still scales poorly; each MCMC transition takes linear time in the number of observations. This paper describes a sublinear-time algorithm for making Metropolis-Hastings (MH) updates to latent variables in probabilistic programs. The approach generalizes recently introduced approximate MH techniques: instead of subsampling data items assumed to be independent, it subsamples edges in a dynamically constructed graphical model. It thus applies to a broader class of problems and interoperates with other general-purpose inference techniques. Empirical results, including confirmation of sublinear per-transition scaling, are presented for Bayesian logistic regression, nonlinear classification via joint Dirichlet process mixtures, and parameter estimation for stochastic volatility models (with state estimation via particle MCMC). All three applications use the same implementation, and each requires under 20 lines of probabilistic code.

</details>

<details>

<summary>2015-03-10 02:28:41 - Large-Scale Distributed Bayesian Matrix Factorization using Stochastic Gradient MCMC</summary>

- *Sungjin Ahn, Anoop Korattikara, Nathan Liu, Suju Rajan, Max Welling*

- `1503.01596v2` - [abs](http://arxiv.org/abs/1503.01596v2) - [pdf](http://arxiv.org/pdf/1503.01596v2)

> Despite having various attractive qualities such as high prediction accuracy and the ability to quantify uncertainty and avoid over-fitting, Bayesian Matrix Factorization has not been widely adopted because of the prohibitive cost of inference. In this paper, we propose a scalable distributed Bayesian matrix factorization algorithm using stochastic gradient MCMC. Our algorithm, based on Distributed Stochastic Gradient Langevin Dynamics, can not only match the prediction accuracy of standard MCMC methods like Gibbs sampling, but at the same time is as fast and simple as stochastic gradient descent. In our experiments, we show that our algorithm can achieve the same level of prediction accuracy as Gibbs sampling an order of magnitude faster. We also show that our method reduces the prediction error as fast as distributed stochastic gradient descent, achieving a 4.1% improvement in RMSE for the Netflix dataset and an 1.8% for the Yahoo music dataset.

</details>

<details>

<summary>2015-03-11 16:52:24 - Bayesian model comparison in cosmology</summary>

- *Daniel J. Mortlock*

- `1503.03414v1` - [abs](http://arxiv.org/abs/1503.03414v1) - [pdf](http://arxiv.org/pdf/1503.03414v1)

> The standard Bayesian model formalism comparison cannot be applied to most cosmological models as they lack well-motivated parameter priors. However, if the data-set being used is separable then it is possible to use some of the data to obtain the necessary parameter distributions, the rest of the data being retained for model comparison. While such methods are not fully prescriptive, they provide a route to applying Bayesian model comparison in cosmological situations where it could not otherwise be used.

</details>

<details>

<summary>2015-03-11 22:05:50 - Switching to Learn</summary>

- *Shahin Shahrampour, Mohammad Amin Rahimian, Ali Jadbabaie*

- `1503.03517v1` - [abs](http://arxiv.org/abs/1503.03517v1) - [pdf](http://arxiv.org/pdf/1503.03517v1)

> A network of agents attempt to learn some unknown state of the world drawn by nature from a finite set. Agents observe private signals conditioned on the true state, and form beliefs about the unknown state accordingly. Each agent may face an identification problem in the sense that she cannot distinguish the truth in isolation. However, by communicating with each other, agents are able to benefit from side observations to learn the truth collectively. Unlike many distributed algorithms which rely on all-time communication protocols, we propose an efficient method by switching between Bayesian and non-Bayesian regimes. In this model, agents exchange information only when their private signals are not informative enough; thence, by switching between the two regimes, agents efficiently learn the truth using only a few rounds of communications. The proposed algorithm preserves learnability while incurring a lower communication cost. We also verify our theoretical findings by simulation examples.

</details>

<details>

<summary>2015-03-13 15:17:25 - On the Mathematics of the Jeffreys-Lindley Paradox</summary>

- *Cristiano Villa, Stephen Walker*

- `1503.04098v1` - [abs](http://arxiv.org/abs/1503.04098v1) - [pdf](http://arxiv.org/pdf/1503.04098v1)

> This paper is concerned with the well known Jeffreys-Lindley paradox. In a Bayesian set up, the so-called paradox arises when a point null hypothesis is tested and an objective prior is sought for the alternative hypothesis. In particular, the posterior for the null hypothesis tends to one when the uncertainty, i.e. the variance, for the parameter value goes to infinity. We argue that the appropriate way to deal with the paradox is to use simple mathematics, and that any philosophical argument is to be regarded as irrelevant.

</details>

<details>

<summary>2015-03-14 13:26:29 - Laplace's rule of succession in information geometry</summary>

- *Yann Ollivier*

- `1503.04304v1` - [abs](http://arxiv.org/abs/1503.04304v1) - [pdf](http://arxiv.org/pdf/1503.04304v1)

> Laplace's "add-one" rule of succession modifies the observed frequencies in a sequence of heads and tails by adding one to the observed counts. This improves prediction by avoiding zero probabilities and corresponds to a uniform Bayesian prior on the parameter. The canonical Jeffreys prior corresponds to the "add-one-half" rule. We prove that, for exponential families of distributions, such Bayesian predictors can be approximated by taking the average of the maximum likelihood predictor and the \emph{sequential normalized maximum likelihood} predictor from information theory. Thus in this case it is possible to approximate Bayesian predictors without the cost of integrating or sampling in parameter space.

</details>

<details>

<summary>2015-03-16 00:28:25 - Semiparametric Bernstein-von Mises Theorem: Second Order Studies</summary>

- *Yun Yang, Guang Cheng, David B. Dunson*

- `1503.04493v1` - [abs](http://arxiv.org/abs/1503.04493v1) - [pdf](http://arxiv.org/pdf/1503.04493v1)

> The major goal of this paper is to study the second order frequentist properties of the marginal posterior distribution of the parametric component in semiparametric Bayesian models, in particular, a second order semiparametric Bernstein-von Mises (BvM) Theorem. Our first contribution is to discover an interesting interference phenomenon between Bayesian estimation and frequentist inferential accuracy: more accurate Bayesian estimation on the nuisance function leads to higher frequentist inferential accuracy on the parametric component. As the second contribution, we propose a new class of dependent priors under which Bayesian inference procedures for the parametric component are not only efficient but also adaptive (w.r.t. the smoothness of nonparametric component) up to the second order frequentist validity. However, commonly used independent priors may even fail to produce a desirable root-n contraction rate for the parametric component in this adaptive case unless some stringent assumption is imposed. Three important classes of semiparametric models are examined, and extensive simulations are also provided.

</details>

<details>

<summary>2015-03-16 14:19:21 - Bayesian Essentials with R: The Complete Solution Manual</summary>

- *Christian P. Robert, Jean-Michel Marin*

- `1503.04662v1` - [abs](http://arxiv.org/abs/1503.04662v1) - [pdf](http://arxiv.org/pdf/1503.04662v1)

> This is the collection of solutions for all the exercises proposed in Bayesian Essentials with R (2014).

</details>

<details>

<summary>2015-03-17 15:57:23 - Geometric shrinkage priors for Kählerian signal filters</summary>

- *Jaehyung Choi, Andrew P. Mullhaupt*

- `1408.6800v3` - [abs](http://arxiv.org/abs/1408.6800v3) - [pdf](http://arxiv.org/pdf/1408.6800v3)

> We construct geometric shrinkage priors for K\"ahlerian signal filters. Based on the characteristics of K\"ahler manifolds, an efficient and robust algorithm for finding superharmonic priors which outperform the Jeffreys prior is introduced. Several ans\"atze for the Bayesian predictive priors are also suggested. In particular, the ans\"atze related to K\"ahler potential are geometrically intrinsic priors to the information manifold of which the geometry is derived from the potential. The implication of the algorithm to time series models is also provided.

</details>

<details>

<summary>2015-03-17 20:47:30 - Age-Specific Mortality and Fertility Rates for Probabilistic Population Projections</summary>

- *Hana Ševčíková, Nan Li, Vladimíra Kantorová, Patrick Gerland, Adrian E. Raftery*

- `1503.05215v1` - [abs](http://arxiv.org/abs/1503.05215v1) - [pdf](http://arxiv.org/pdf/1503.05215v1)

> The United Nations released official probabilistic population projections (PPP) for all countries for the first time in July 2014. These were obtained by projecting the period total fertility rate (TFR) and life expectancy at birth ($e_0$) using Bayesian hierarchical models, yielding a large set of future trajectories of TFR and $e_0$ for all countries and future time periods to 2100, sampled from their joint predictive distribution. Each trajectory was then converted to age-specific mortality and fertility rates, and population was projected using the cohort-component method. This yielded a large set of trajectories of future age- and sex-specific population counts and vital rates for all countries. In this paper we describe the methodology used for deriving the age-specific mortality and fertility rates in the 2014 PPP, we identify limitations of these methods, and we propose several methodological improvements to overcome them. The methods presented in this paper are implemented in the publicly available bayesPop R package.

</details>

<details>

<summary>2015-03-18 03:37:40 - A General Framework for Robust Testing and Confidence Regions in High-Dimensional Quantile Regression</summary>

- *Tianqi Zhao, Mladen Kolar, Han Liu*

- `1412.8724v2` - [abs](http://arxiv.org/abs/1412.8724v2) - [pdf](http://arxiv.org/pdf/1412.8724v2)

> We propose a robust inferential procedure for assessing uncertainties of parameter estimation in high-dimensional linear models, where the dimension $p$ can grow exponentially fast with the sample size $n$. Our method combines the de-biasing technique with the composite quantile function to construct an estimator that is asymptotically normal. Hence it can be used to construct valid confidence intervals and conduct hypothesis tests. Our estimator is robust and does not require the existence of first or second moment of the noise distribution. It also preserves efficiency in the sense that the worst case efficiency loss is less than 30\% compared to the square-loss-based de-biased Lasso estimator. In many cases our estimator is close to or better than the latter, especially when the noise is heavy-tailed. Our de-biasing procedure does not require solving the $L_1$-penalized composite quantile regression. Instead, it allows for any first-stage estimator with desired convergence rate and empirical sparsity. The paper also provides new proof techniques for developing theoretical guarantees of inferential procedures with non-smooth loss functions. To establish the main results, we exploit the local curvature of the conditional expectation of composite quantile loss and apply empirical process theories to control the difference between empirical quantities and their conditional expectations. Our results are established under weaker assumptions compared to existing work on inference for high-dimensional quantile regression. Furthermore, we consider a high-dimensional simultaneous test for the regression parameters by applying the Gaussian approximation and multiplier bootstrap theories. We also study distributed learning and exploit the divide-and-conquer estimator to reduce computation complexity when the sample size is massive. Finally, we provide empirical results to verify the theory.

</details>

<details>

<summary>2015-03-18 07:47:20 - A warped kernel improving robustness in Bayesian optimization via random embeddings</summary>

- *Mickaël Binois, David Ginsbourger, Olivier Roustant*

- `1411.3685v3` - [abs](http://arxiv.org/abs/1411.3685v3) - [pdf](http://arxiv.org/pdf/1411.3685v3)

> This works extends the Random Embedding Bayesian Optimization approach by integrating a warping of the high dimensional subspace within the covariance kernel. The proposed warping, that relies on elementary geometric considerations, allows mitigating the drawbacks of the high extrinsic dimensionality while avoiding the algorithm to evaluate points giving redundant information. It also alleviates constraints on bound selection for the embedded domain, thus improving the robustness, as illustrated with a test case with 25 variables and intrinsic dimension 6.

</details>

<details>

<summary>2015-03-18 20:01:22 - The Knowledge Gradient Policy Using A Sparse Additive Belief Model</summary>

- *Yan Li, Han Liu, Warren Powell*

- `1503.05567v1` - [abs](http://arxiv.org/abs/1503.05567v1) - [pdf](http://arxiv.org/pdf/1503.05567v1)

> We propose a sequential learning policy for noisy discrete global optimization and ranking and selection (R\&S) problems with high dimensional sparse belief functions, where there are hundreds or even thousands of features, but only a small portion of these features contain explanatory power. We aim to identify the sparsity pattern and select the best alternative before the finite budget is exhausted. We derive a knowledge gradient policy for sparse linear models (KGSpLin) with group Lasso penalty. This policy is a unique and novel hybrid of Bayesian R\&S with frequentist learning. Particularly, our method naturally combines B-spline basis expansion and generalizes to the nonparametric additive model (KGSpAM) and functional ANOVA model. Theoretically, we provide the estimation error bounds of the posterior mean estimate and the functional estimate. Controlled experiments show that the algorithm efficiently learns the correct set of nonzero parameters even when the model is imbedded with hundreds of dummy parameters. Also it outperforms the knowledge gradient for a linear model.

</details>

<details>

<summary>2015-03-19 09:53:34 - Non-parametric Bayesian Models of Response Function in Dynamic Image Sequences</summary>

- *Ondřej Tichý, Václav Šmídl*

- `1503.05684v1` - [abs](http://arxiv.org/abs/1503.05684v1) - [pdf](http://arxiv.org/pdf/1503.05684v1)

> Estimation of response functions is an important task in dynamic medical imaging. This task arises for example in dynamic renal scintigraphy, where impulse response or retention functions are estimated, or in functional magnetic resonance imaging where hemodynamic response functions are required. These functions can not be observed directly and their estimation is complicated because the recorded images are subject to superposition of underlying signals. Therefore, the response functions are estimated via blind source separation and deconvolution. Performance of this algorithm heavily depends on the used models of the response functions. Response functions in real image sequences are rather complicated and finding a suitable parametric form is problematic. In this paper, we study estimation of the response functions using non-parametric Bayesian priors. These priors were designed to favor desirable properties of the functions, such as sparsity or smoothness. These assumptions are used within hierarchical priors of the blind source separation and deconvolution algorithm. Comparison of the resulting algorithms with these priors is performed on synthetic dataset as well as on real datasets from dynamic renal scintigraphy. It is shown that flexible non-parametric priors improve estimation of response functions in both cases. MATLAB implementation of the resulting algorithms is freely available for download.

</details>

<details>

<summary>2015-03-19 18:38:54 - Objective Bayes, conditional inference and the signed root likelihood ratio statistic</summary>

- *Thomas J. DiCiccio, Todd A. Kuffner, G. Alastair Young*

- `1503.05876v1` - [abs](http://arxiv.org/abs/1503.05876v1) - [pdf](http://arxiv.org/pdf/1503.05876v1)

> Bayesian properties of the signed root likelihood ratio statistic are analysed. Conditions for first-order probability matching are derived by the examination of the Bayesian posterior and frequentist means of this statistic. Second-order matching conditions are shown to arise from matching of the Bayesian posterior and frequentist variances of a mean-adjusted version of the signed root statistic. Conditions for conditional probability matching in ancillary statistic models are derived and discussed.

</details>

<details>

<summary>2015-03-20 05:17:02 - Sieve Wald and QLR Inferences on Semi/nonparametric Conditional Moment Models</summary>

- *Xiaohong Chen, Demian Pouzo*

- `1411.1144v2` - [abs](http://arxiv.org/abs/1411.1144v2) - [pdf](http://arxiv.org/pdf/1411.1144v2)

> This paper considers inference on functionals of semi/nonparametric conditional moment restrictions with possibly nonsmooth generalized residuals, which include all of the (nonlinear) nonparametric instrumental variables (IV) as special cases. These models are often ill-posed and hence it is difficult to verify whether a (possibly nonlinear) functional is root-$n$ estimable or not. We provide computationally simple, unified inference procedures that are asymptotically valid regardless of whether a functional is root-$n$ estimable or not. We establish the following new useful results: (1) the asymptotic normality of a plug-in penalized sieve minimum distance (PSMD) estimator of a (possibly nonlinear) functional; (2) the consistency of simple sieve variance estimators for the plug-in PSMD estimator, and hence the asymptotic chi-square distribution of the sieve Wald statistic; (3) the asymptotic chi-square distribution of an optimally weighted sieve quasi likelihood ratio (QLR) test under the null hypothesis; (4) the asymptotic tight distribution of a non-optimally weighted sieve QLR statistic under the null; (5) the consistency of generalized residual bootstrap sieve Wald and QLR tests; (6) local power properties of sieve Wald and QLR tests and of their bootstrap versions; (7) asymptotic properties of sieve Wald and SQLR for functionals of increasing dimension. Simulation studies and an empirical illustration of a nonparametric quantile IV regression are presented.

</details>

<details>

<summary>2015-03-21 05:45:16 - Hierarchical sparse Bayesian learning: theory and application for inferring structural damage from incomplete modal data</summary>

- *Yong Huang, James L. Beck*

- `1503.06267v1` - [abs](http://arxiv.org/abs/1503.06267v1) - [pdf](http://arxiv.org/pdf/1503.06267v1)

> Structural damage due to excessive loading or environmental degradation typically occurs in localized areas in the absence of collapse. This prior information about the spatial sparseness of structural damage is exploited here by a hierarchical sparse Bayesian learning framework with the goal of reducing the source of ill-conditioning in the stiffness loss inversion problem for damage detection. Sparse Bayesian learning methodologies automatically prune away irrelevant or inactive features from a set of potential candidates, and so they are effective probabilistic tools for producing sparse explanatory subsets. We have previously proposed such an approach to establish the probability of localized stiffness reductions that serve as a proxy for damage by using noisy incomplete modal data from before and after possible damage. The core idea centers on a specific hierarchical Bayesian model that promotes spatial sparseness in the inferred stiffness reductions in a way that is consistent with the Bayesian Ockham razor. In this paper, we improve the theory of our previously proposed sparse Bayesian learning approach by eliminating an approximation and, more importantly, incorporating a constraint on stiffness increases. Our approach has many appealing features that are summarized at the end of the paper. We validate the approach by applying it to the Phase II simulated and experimental benchmark studies sponsored by the IASC-ASCE Task Group on Structural Health Monitoring. The results show that it can reliably detect, locate and assess damage by inferring substructure stiffness losses from the identified modal parameters. The occurrence of missed and false damage alerts is effectively suppressed.

</details>

<details>

<summary>2015-03-22 21:25:02 - Variable Selection Using Shrinkage Priors</summary>

- *Hanning Li, Debdeep Pati*

- `1503.04303v2` - [abs](http://arxiv.org/abs/1503.04303v2) - [pdf](http://arxiv.org/pdf/1503.04303v2)

> Variable selection has received widespread attention over the last decade as we routinely encounter high-throughput datasets in complex biological and environment research. Most Bayesian variable selection methods are restricted to mixture priors having separate components for characterizing the signal and the noise. However, such priors encounter computational issues in high dimensions. This has motivated continuous shrinkage priors, resembling the two-component priors facilitating computation and interpretability. While such priors are widely used for estimating high-dimensional sparse vectors, selecting a subset of variables remains a daunting task. In this article, we propose a general approach for variable selection with shrinkage priors. The presence of very few tuning parameters makes our method attractive in comparison to adhoc thresholding approaches. The applicability of the approach is not limited to continuous shrinkage priors, but can be used along with any shrinkage prior. Theoretical properties for near-collinear design matrices are investigated and the method is shown to have good performance in a wide range of synthetic data examples.

</details>

<details>

<summary>2015-03-24 18:31:42 - Bayesian Inference for partially observed SDEs Driven by Fractional Brownian Motion</summary>

- *Alexandros Beskos, Joseph Dureau, Konstantinos Kalogeropoulos*

- `1307.0238v5` - [abs](http://arxiv.org/abs/1307.0238v5) - [pdf](http://arxiv.org/pdf/1307.0238v5)

> We consider continuous-time diffusion models driven by fractional Brownian motion. Observations are assumed to possess a non-trivial likelihood given the latent path. Due to the non-Markovianity and high-dimensionality of the latent paths, estimating posterior expectations is a computationally challenging undertaking. We present a reparameterization framework based on the Davies and Harte method for sampling stationary Gaussian processes and use this framework to construct a Markov chain Monte Carlo algorithm that allows computationally efficient Bayesian inference. The Markov chain Monte Carlo algorithm is based on a version of hybrid Monte Carlo that delivers increased efficiency when applied on the high-dimensional latent variables arising in this context. We specify the methodology on a stochastic volatility model allowing for memory in the volatility increments through a fractional specification. The methodology is illustrated on simulated data and on the S&P500/VIX time series and is shown to be effective. Contrary to a long range dependence attribute of such models often assumed in the literature, with Hurst parameter larger than 1/2, the posterior distribution favours values smaller than 1/2, pointing towards medium range dependence.

</details>

<details>

<summary>2015-03-24 18:36:51 - Nonparametric Bayes inference on conditional independence</summary>

- *Tsuyoshi Kunihama, David B. Dunson*

- `1404.1429v3` - [abs](http://arxiv.org/abs/1404.1429v3) - [pdf](http://arxiv.org/pdf/1404.1429v3)

> In broad applications, it is routinely of interest to assess whether there is evidence in the data to refute the assumption of conditional independence of $Y$ and $X$ conditionally on $Z$. Such tests are well developed in parametric models but are not straightforward in the nonparametric case. We propose a general Bayesian approach, which relies on an encompassing nonparametric Bayes model for the joint distribution of $Y$, $X$ and $Z$. The framework allows $Y$, $X$ and $Z$ to be random variables on arbitrary spaces, and can accommodate different dimensional vectors having a mixture of discrete and continuous measurement scales. Using conditional mutual information as a scalar summary of the strength of the conditional dependence relationship, we construct null and alternative hypotheses. We provide conditions under which the correct hypothesis will be consistently selected. Computational methods are developed, which can be incorporated within MCMC algorithms for the encompassing model. The methods are applied to variable selection and assessed through simulations and criminology applications.

</details>

<details>

<summary>2015-03-25 01:14:19 - Accuracy of Latent-Variable Estimation in Bayesian Semi-Supervised Learning</summary>

- *Keisuke Yamazaki*

- `1308.2029v3` - [abs](http://arxiv.org/abs/1308.2029v3) - [pdf](http://arxiv.org/pdf/1308.2029v3)

> Hierarchical probabilistic models, such as Gaussian mixture models, are widely used for unsupervised learning tasks. These models consist of observable and latent variables, which represent the observable data and the underlying data-generation process, respectively. Unsupervised learning tasks, such as cluster analysis, are regarded as estimations of latent variables based on the observable ones. The estimation of latent variables in semi-supervised learning, where some labels are observed, will be more precise than that in unsupervised, and one of the concerns is to clarify the effect of the labeled data. However, there has not been sufficient theoretical analysis of the accuracy of the estimation of latent variables. In a previous study, a distribution-based error function was formulated, and its asymptotic form was calculated for unsupervised learning with generative models. It has been shown that, for the estimation of latent variables, the Bayes method is more accurate than the maximum-likelihood method. The present paper reveals the asymptotic forms of the error function in Bayesian semi-supervised learning for both discriminative and generative models. The results show that the generative model, which uses all of the given data, performs better when the model is well specified.

</details>

<details>

<summary>2015-03-25 01:44:13 - Multilevel Sequential Monte Carlo Samplers</summary>

- *Alexandros Beskos, Ajay Jasra, Kody Law, Raul Tempone, Yan Zhou*

- `1503.07259v1` - [abs](http://arxiv.org/abs/1503.07259v1) - [pdf](http://arxiv.org/pdf/1503.07259v1)

> In this article we consider the approximation of expectations w.r.t. probability distributions associated to the solution of partial differential equations (PDEs); this scenario appears routinely in Bayesian inverse problems. In practice, one often has to solve the associated PDE numerically, using, for instance finite element methods and leading to a discretisation bias, with the step-size level $h_L$. In addition, the expectation cannot be computed analytically and one often resorts to Monte Carlo methods. In the context of this problem, it is known that the introduction of the multilevel Monte Carlo (MLMC) method can reduce the amount of computational effort to estimate expectations, for a given level of error. This is achieved via a telescoping identity associated to a Monte Carlo approximation of a sequence of probability distributions with discretisation levels $\infty>h_0>h_1\cdots>h_L$. In many practical problems of interest, one cannot achieve an i.i.d. sampling of the associated sequence of probability distributions. A sequential Monte Carlo (SMC) version of the MLMC method is introduced to deal with this problem. It is shown that under appropriate assumptions, the attractive property of a reduction of the amount of computational effort to estimate expectations, for a given level of error, can be maintained within the SMC context.

</details>

<details>

<summary>2015-03-25 17:25:47 - Kählerian information geometry for signal processing</summary>

- *Jaehyung Choi, Andrew P. Mullhaupt*

- `1404.2006v3` - [abs](http://arxiv.org/abs/1404.2006v3) - [pdf](http://arxiv.org/pdf/1404.2006v3)

> We prove the correspondence between the information geometry of a signal filter and a K\"ahler manifold. The information geometry of a minimum-phase linear system with a finite complex cepstrum norm is a K\"ahler manifold. The square of the complex cepstrum norm of the signal filter corresponds to the K\"ahler potential. The Hermitian structure of the K\"ahler manifold is explicitly emergent if and only if the impulse response function of the highest degree in $z$ is constant in model parameters. The K\"ahlerian information geometry takes advantage of more efficient calculation steps for the metric tensor and the Ricci tensor. Moreover, $\alpha$-generalization on the geometric tensors is linear in $\alpha$. It is also robust to find Bayesian predictive priors, such as superharmonic priors, because Laplace-Beltrami operators on K\"ahler manifolds are in much simpler forms than those of the non-K\"ahler manifolds. Several time series models are studied in the K\"ahlerian information geometry.

</details>

<details>

<summary>2015-03-26 04:36:38 - Bayesian Reconstruction of Missing Observations</summary>

- *Shun Kataoka, Muneki Yasuda, Kazuyuki Tanaka*

- `1404.5793v2` - [abs](http://arxiv.org/abs/1404.5793v2) - [pdf](http://arxiv.org/pdf/1404.5793v2)

> We focus on an interpolation method referred to Bayesian reconstruction in this paper. Whereas in standard interpolation methods missing data are interpolated deterministically, in Bayesian reconstruction, missing data are interpolated probabilistically using a Bayesian treatment. In this paper, we address the framework of Bayesian reconstruction and its application to the traffic data reconstruction problem in the field of traffic engineering. In the latter part of this paper, we describe the evaluation of the statistical performance of our Bayesian traffic reconstruction model using a statistical mechanical approach and clarify its statistical behavior.

</details>

<details>

<summary>2015-03-26 08:07:41 - Bayesian Model Choice in Cumulative Link Ordinal Regression Models</summary>

- *Trevelyan J. McKinley, Michelle Morters, James L. N. Wood*

- `1503.07642v1` - [abs](http://arxiv.org/abs/1503.07642v1) - [pdf](http://arxiv.org/pdf/1503.07642v1)

> The use of the proportional odds (PO) model for ordinal regression is ubiquitous in the literature. If the assumption of parallel lines does not hold for the data, then an alternative is to specify a non-proportional odds (NPO) model, where the regression parameters are allowed to vary depending on the level of the response. However, it is often difficult to fit these models, and challenges regarding model choice and fitting are further compounded if there are a large number of explanatory variables. We make two contributions towards tackling these issues: firstly, we develop a Bayesian method for fitting these models, that ensures the stochastic ordering conditions hold for an arbitrary finite range of the explanatory variables, allowing NPO models to be fitted to any observed data set. Secondly, we use reversible-jump Markov chain Monte Carlo to allow the model to choose between PO and NPO structures for each explanatory variable, and show how variable selection can be incorporated. These methods can be adapted for any monotonic increasing link functions. We illustrate the utility of these approaches on novel data from a longitudinal study of individual-level risk factors affecting body condition score in a dog population in Zenzele, South Africa.

</details>

<details>

<summary>2015-03-26 08:24:02 - Asymptotic Properties of Bayesian Predictive Densities When the Distributions of Data and Target Variables are Different</summary>

- *Fumiyasu Komaki*

- `1503.07643v1` - [abs](http://arxiv.org/abs/1503.07643v1) - [pdf](http://arxiv.org/pdf/1503.07643v1)

> Bayesian predictive densities when the observed data $x$ and the target variable $y$ to be predicted have different distributions are investigated by using the framework of information geometry. The performance of predictive densities is evaluated by the Kullback--Leibler divergence. The parametric models are formulated as Riemannian manifolds. In the conventional setting in which $x$ and $y$ have the same distribution, the Fisher--Rao metric and the Jeffreys prior play essential roles. In the present setting in which $x$ and $y$ have different distributions, a new metric, which we call the predictive metric, constructed by using the Fisher information matrices of $x$ and $y$, and the volume element based on the predictive metric play the corresponding roles. It is shown that Bayesian predictive densities based on priors constructed by using non-constant positive superharmonic functions with respect to the predictive metric asymptotically dominate those based on the volume element prior of the predictive metric.

</details>

<details>

<summary>2015-03-26 17:12:57 - Sequential Monte Carlo with Adaptive Weights for Approximate Bayesian Computation</summary>

- *Fernando V. Bonassi, Mike West*

- `1503.07791v1` - [abs](http://arxiv.org/abs/1503.07791v1) - [pdf](http://arxiv.org/pdf/1503.07791v1)

> Methods of approximate Bayesian computation (ABC) are increasingly used for analysis of complex models. A major challenge for ABC is over-coming the often inherent problem of high rejection rates in the accept/reject methods based on prior:predictive sampling. A number of recent developments aim to address this with extensions based on sequential Monte Carlo (SMC) strategies. We build on this here, introducing an ABC SMC method that uses data-based adaptive weights. This easily implemented and computationally trivial extension of ABC SMC can very substantially improve acceptance rates, as is demonstrated in a series of examples with simulated and real data sets, including a currently topical example from dynamic modelling in systems biology applications.

</details>

<details>

<summary>2015-03-27 06:21:06 - Bayesian Cross Validation and WAIC for Predictive Prior Design in Regular Asymptotic Theory</summary>

- *Sumio Watanabe*

- `1503.07970v1` - [abs](http://arxiv.org/abs/1503.07970v1) - [pdf](http://arxiv.org/pdf/1503.07970v1)

> Prior design is one of the most important problems in both statistics and machine learning. The cross validation (CV) and the widely applicable information criterion (WAIC) are predictive measures of the Bayesian estimation, however, it has been difficult to apply them to find the optimal prior because their mathematical properties in prior evaluation have been unknown and the region of the hyperparameters is too wide to be examined. In this paper, we derive a new formula by which the theoretical relation among CV, WAIC, and the generalization loss is clarified and the optimal hyperparameter can be directly found.   By the formula, three facts are clarified about predictive prior design. Firstly, CV and WAIC have the same second order asymptotic expansion, hence they are asymptotically equivalent to each other as the optimizer of the hyperparameter. Secondly, the hyperparameter which minimizes CV or WAIC makes the average generalization loss to be minimized asymptotically but does not the random generalization loss. And lastly, by using the mathematical relation between priors, the variances of the optimized hyperparameters by CV and WAIC are made smaller with small computational costs. Also we show that the optimized hyperparameter by DIC or the marginal likelihood does not minimize the average or random generalization loss in general.

</details>

<details>

<summary>2015-03-28 06:14:45 - Robust Bayesian compressive sensing with data loss recovery for structural health monitoring signals</summary>

- *Yong Huang, James L. Beck, Stephen Wu, Hui Li*

- `1503.08272v1` - [abs](http://arxiv.org/abs/1503.08272v1) - [pdf](http://arxiv.org/pdf/1503.08272v1)

> The application of compressive sensing (CS) to structural health monitoring is an emerging research topic. The basic idea in CS is to use a specially-designed wireless sensor to sample signals that are sparse in some basis (e.g. wavelet basis) directly in a compressed form, and then to reconstruct (decompress) these signals accurately using some inversion algorithm after transmission to a central processing unit. However, most signals in structural health monitoring are only approximately sparse, i.e. only a relatively small number of the signal coefficients in some basis are significant, but the other coefficients are usually not exactly zero. In this case, perfect reconstruction from compressed measurements is not expected. A new Bayesian CS algorithm is proposed in which robust treatment of the uncertain parameters is explored, including integration over the prediction-error precision parameter to remove it as a "nuisance" parameter. The performance of the new CS algorithm is investigated using compressed data from accelerometers installed on a space-frame structure and on a cable-stayed bridge. Compared with other state-of-the-art CS methods including our previously-published Bayesian method which uses MAP (maximum a posteriori) estimation of the prediction-error precision parameter, the new algorithm shows superior performance in reconstruction robustness and posterior uncertainty quantification. Furthermore, our method can be utilized for recovery of lost data during wireless transmission, regardless of the level of sparseness in the signal.

</details>

<details>

<summary>2015-03-28 07:41:16 - Modeling population structure under hierarchical Dirichlet processes</summary>

- *M. De Iorio, L. T. Elliott, S. Favaro, K. Adhikari, Y. W. Teh*

- `1503.08278v1` - [abs](http://arxiv.org/abs/1503.08278v1) - [pdf](http://arxiv.org/pdf/1503.08278v1)

> We propose a Bayesian nonparametric model to infer population admixture, extending the Hierarchical Dirichlet Process to allow for correlation between loci due to Linkage Disequilibrium. Given multilocus genotype data from a sample of individuals, the model allows inferring classifying individuals as unadmixed or admixed, inferring the number of subpopulations ancestral to an admixed population and the population of origin of chromosomal regions. Our model does not assume any specific mutation process and can be applied to most of the commonly used genetic markers. We present a MCMC algorithm to perform posterior inference from the model and discuss methods to summarise the MCMC output for the analysis of population admixture. We demonstrate the performance of the proposed model in simulations and in a real application, using genetic data from the EDAR gene, which is considered to be ancestry-informative due to well-known variations in allele frequency as well as phenotypic effects across ancestry. The structure analysis of this dataset leads to the identification of a rare haplotype in Europeans.

</details>

<details>

<summary>2015-03-28 19:10:47 - Laplace Approximation in High-dimensional Bayesian Regression</summary>

- *Rina Foygel Barber, Mathias Drton, Kean Ming Tan*

- `1503.08337v1` - [abs](http://arxiv.org/abs/1503.08337v1) - [pdf](http://arxiv.org/pdf/1503.08337v1)

> We consider Bayesian variable selection in sparse high-dimensional regression, where the number of covariates $p$ may be large relative to the samples size $n$, but at most a moderate number $q$ of covariates are active. Specifically, we treat generalized linear models. For a single fixed sparse model with well-behaved prior distribution, classical theory proves that the Laplace approximation to the marginal likelihood of the model is accurate for sufficiently large sample size $n$. We extend this theory by giving results on uniform accuracy of the Laplace approximation across all models in a high-dimensional scenario in which $p$ and $q$, and thus also the number of considered models, may increase with $n$. Moreover, we show how this connection between marginal likelihood and Laplace approximation can be used to obtain consistency results for Bayesian approaches to variable selection in high-dimensional regression.

</details>

<details>

<summary>2015-03-28 22:08:53 - Spatial Process Gradients and Their Use in Sensitivity Analysis for Environmental Processes</summary>

- *Maria A. Terres, Alan E. Gelfand*

- `1503.08357v1` - [abs](http://arxiv.org/abs/1503.08357v1) - [pdf](http://arxiv.org/pdf/1503.08357v1)

> This paper develops methodology for local sensitivity analysis based on directional derivatives associated with spatial processes. Formal gradient analysis for spatial processes was elaborated in previous papers, focusing on distribution theory for directional derivatives associated with a response variable assumed to follow a Gaussian process model. In the current work, these ideas are extended to additionally accommodate a continuous covariate whose directional derivatives are also of interest and to relate the behavior of the directional derivatives of the response surface to those of the covariate surface. It is of interest to assess whether, in some sense, the gradients of the response follow those of the explanatory variable. The joint Gaussian structure of all variables, including the directional derivatives, allows for explicit distribution theory and, hence, kriging across the spatial region using multivariate normal theory. Working within a Bayesian hierarchical modeling framework, posterior samples enable all gradient analysis to occur post model fitting. As a proof of concept, we show how our methodology can be applied to a standard geostatistical modeling setting using a simulation example. For a real data illustration, we work with point pattern data, deferring our gradient analysis to the intensity surface, adopting a log-Gaussian Cox process model. In particular, we relate elevation data to point patterns associated with several tree species in Duke Forest.

</details>

<details>

<summary>2015-03-29 15:14:34 - Posterior predictive p-values and the convex order</summary>

- *Patrick Rubin-Delanchy, Daniel John Lawson*

- `1412.3442v3` - [abs](http://arxiv.org/abs/1412.3442v3) - [pdf](http://arxiv.org/pdf/1412.3442v3)

> Posterior predictive p-values are a common approach to Bayesian model-checking. This article analyses their frequency behaviour, that is, their distribution when the parameters and the data are drawn from the prior and the model respectively. We show that the family of possible distributions is exactly described as the distributions that are less variable than uniform on [0,1], in the convex order. In general, p-values with such a property are not conservative, and we illustrate how the theoretical worst-case error rate for false rejection can occur in practice. We describe how to correct the p-values to recover conservatism in several common scenarios, for example, when interpreting a single p-value or when combining multiple p-values into an overall score of significance. We also handle the case where the p-value is estimated from posterior samples obtained from techniques such as Markov Chain or Sequential Monte Carlo. Our results place posterior predictive p-values in a much clearer theoretical framework, allowing them to be used with more assurance.

</details>

<details>

<summary>2015-03-30 09:27:36 - Exploiting Multi-Core Architectures for Reduced-Variance Estimation with Intractable Likelihoods</summary>

- *Nial Friel, Antonietta Mira, Chris. J. Oates*

- `1408.4663v2` - [abs](http://arxiv.org/abs/1408.4663v2) - [pdf](http://arxiv.org/pdf/1408.4663v2)

> Many popular statistical models for complex phenomena are intractable, in the sense that the likelihood function cannot easily be evaluated. Bayesian estimation in this setting remains challenging, with a lack of computational methodology to fully exploit modern processing capabilities. In this paper we introduce novel control variates for intractable likelihoods that can dramatically reduce the Monte Carlo variance of Bayesian estimators. We prove that our control variates are well-defined and provide a positive variance reduction. Furthermore we show how to optimise these control variates for variance reduction. The methodology is highly parallel and offers a route to exploit multi-core processing architectures that complements recent research in this direction. Indeed, our work shows that it may not be necessary to parallelise the sampling process itself in order to harness the potential of massively multi-core architectures. Simulation results presented on the Ising model, exponential random graph models and non-linear stochastic differential equation models support our theoretical findings.

</details>

<details>

<summary>2015-03-30 12:27:28 - Exponentiated Extended Weibull-Power Series Class of Distributions</summary>

- *Saeid Tahmasebi, Ali Akbar Jafari*

- `1503.08653v1` - [abs](http://arxiv.org/abs/1503.08653v1) - [pdf](http://arxiv.org/pdf/1503.08653v1)

> In this paper, we introduce a new class of distributions by compounding the exponentiated extended Weibull family and power series family. This distribution contains several lifetime models such as the complementary extended Weibull-power series, generalized exponential-power series, generalized linear failure rate-power series, exponentiated Weibull-power series, generalized modified Weibull-power series, generalized Gompertz-power series and exponentiated extended Weibull distributions as special cases. We obtain several properties of this new class of distributions such as Shannon entropy, mean residual life, hazard rate function, quantiles and moments. The maximum likelihood estimation procedure via a EM-algorithm is presented.

</details>

<details>

<summary>2015-03-30 16:10:23 - A Parzen-based distance between probability measures as an alternative of summary statistics in Approximate Bayesian Computation</summary>

- *Carlos D. Zuluaga, Edgar A. Valencia, Mauricio A. Álvarez*

- `1503.08727v1` - [abs](http://arxiv.org/abs/1503.08727v1) - [pdf](http://arxiv.org/pdf/1503.08727v1)

> Approximate Bayesian Computation (ABC) are likelihood-free Monte Carlo methods. ABC methods use a comparison between simulated data, using different parameters drew from a prior distribution, and observed data. This comparison process is based on computing a distance between the summary statistics from the simulated data and the observed data. For complex models, it is usually difficult to define a methodology for choosing or constructing the summary statistics. Recently, a nonparametric ABC has been proposed, that uses a dissimilarity measure between discrete distributions based on empirical kernel embeddings as an alternative for summary statistics. The nonparametric ABC outperforms other methods including ABC, kernel ABC or synthetic likelihood ABC. However, it assumes that the probability distributions are discrete, and it is not robust when dealing with few observations. In this paper, we propose to apply kernel embeddings using an smoother density estimator or Parzen estimator for comparing the empirical data distributions, and computing the ABC posterior. Synthetic data and real data were used to test the Bayesian inference of our method. We compare our method with respect to state-of-the-art methods, and demonstrate that our method is a robust estimator of the posterior distribution in terms of the number of observations.

</details>

<details>

<summary>2015-03-30 20:22:24 - New Fréchet features for random distributions and associated sensitivity indices</summary>

- *Jean-Claude Fort, Thierry Klein*

- `1503.08844v1` - [abs](http://arxiv.org/abs/1503.08844v1) - [pdf](http://arxiv.org/pdf/1503.08844v1)

> In this article we define new Fr\`Echet features for random cumulative distribution functions using contrast. These contrasts allow to construct Wasserstein costs and our new features minimize the average costs as the Fr\`Echet mean minimizes the mean square Wasserstein$_2$ distance. An example of new features is the median, and more generally the quantiles. From these definitions, we are able to define sensitivity indices when the random distribution is the output of a stochastic code. Associated to the Fr\`Echet mean we extend the Sobol indices, and in general the indices associated to a contrast that we previously proposed.

</details>

<details>

<summary>2015-03-31 02:22:27 - A Bayesian Change Point Model for Detecting Land Cover Changes in MODIS Time Series</summary>

- *Hunter Glanz, Xiaoman Huang, Minhui Zheng, Luis E. Carvalho*

- `1503.08886v1` - [abs](http://arxiv.org/abs/1503.08886v1) - [pdf](http://arxiv.org/pdf/1503.08886v1)

> As both a central task in Remote Sensing and a common problem in many other situations involving time series data, change point detection boasts a thorough and well-documented history of study. However, the treatment of missing data and proper exploitation of the structure in multivariate time series during change point detection remains lacking. Multispectral, high temporal resolution time series data from NASA's Moderate Resolution Imaging Spectroradiometer (MODIS) instruments provide an attractive and challenging context to contribute to the change point detection literature. In an effort to better monitor change in land cover using MODIS data, we present a novel approach to identifying periods of time in which regions experience some conversion-type of land cover change. That is, we propose a method for parameter estimation and change point detection in the presence of missing data which capitalizes on the high dimensionality of MODIS data. We test the quality of our method in a simulation study alongside a contemporary change point method and apply it in a case study at the Xingu River Basin in the Amazon. Not only does our method maintain a high accuracy, but can provide insight into the types of changes occurring via land cover conversion probabilities. In this way we can better characterize the amount and types of forest disturbance in our study area in comparison to traditional change point methods.

</details>

<details>

<summary>2015-03-31 17:36:49 - A Bayesian Level Set Method for Geometric Inverse Problems</summary>

- *Marco A. Iglesias, Yulong Lu, Andrew M. Stuart*

- `1504.00313v1` - [abs](http://arxiv.org/abs/1504.00313v1) - [pdf](http://arxiv.org/pdf/1504.00313v1)

> We introduce a level set based approach to Bayesian geometric inverse problems. In these problems the interface between different domains is the key unknown, and is realized as the level set of a function. This function itself becomes the object of the inference. Whilst the level set methodology has been widely used for the solution of geometric inverse problems, the Bayesian formulation that we develop here contains two significant advances: firstly it leads to a well-posed inverse problem in which the posterior distribution is Lipschitz with respect to the observed data; and secondly it leads to computationally expedient algorithms in which the level set itself is updated implicitly via the MCMC methodology applied to the level set function- no explicit velocity field is required for the level set interface. Applications are numerous and include medical imaging, modelling of subsurface formations and the inverse source problem; our theory is illustrated with computational results involving the last two applications.

</details>


## 2015-04

<details>

<summary>2015-04-01 09:32:11 - A new distribution function with bounded support: the reflected Generalized Topp-Leone Power Series distribution</summary>

- *Francesca Condino, Filippo Domma*

- `1504.00160v1` - [abs](http://arxiv.org/abs/1504.00160v1) - [pdf](http://arxiv.org/pdf/1504.00160v1)

> In this paper we introduce a new flexible class of distributions with bounded support, called reflected Generalized Topp-Leone Power Series (rGTL-PS), obtained by compounding the reflected Generalized Topp-Leone (van Drop and Kotz, 2006) and the family of Power Series distributions. The proposed class includes, as special cases, some new distributions with limited support such as the rGTL-Logarithmic, the rGTL-Geometric, the rGTL-Poisson and rGTL-Binomial. This work is an attempt to partially fill a gap regarding the presence, in the literature, of continuous distributions with bounded support, which instead appear to be very useful in many real contexts, included the reliability. Some properties of the class, including moments, hazard rate and quantile are investigated. Moreover, the maximum likelihood estimators of the parameters are examined and the observed Fisher information matrix provided. Finally, in order to show the usefulness of the new class, some applications to real data are reported.

</details>

<details>

<summary>2015-04-01 12:06:13 - Minimax-optimal nonparametric regression in high dimensions</summary>

- *Yun Yang, Surya T. Tokdar*

- `1401.7278v3` - [abs](http://arxiv.org/abs/1401.7278v3) - [pdf](http://arxiv.org/pdf/1401.7278v3)

> Minimax $L_2$ risks for high-dimensional nonparametric regression are derived under two sparsity assumptions: (1) the true regression surface is a sparse function that depends only on $d=O(\log n)$ important predictors among a list of $p$ predictors, with $\log p=o(n)$; (2) the true regression surface depends on $O(n)$ predictors but is an additive function where each additive component is sparse but may contain two or more interacting predictors and may have a smoothness level different from other components. For either modeling assumption, a practicable extension of the widely used Bayesian Gaussian process regression method is shown to adaptively attain the optimal minimax rate (up to $\log n$ terms) asymptotically as both $n,p\to\infty$ with $\log p=o(n)$.

</details>

<details>

<summary>2015-04-01 20:35:33 - Bayesian Clustering of Shapes of Curves</summary>

- *Zhengwu Zhang, Debdeep Pati, Anuj Srivastava*

- `1504.00377v1` - [abs](http://arxiv.org/abs/1504.00377v1) - [pdf](http://arxiv.org/pdf/1504.00377v1)

> Unsupervised clustering of curves according to their shapes is an important problem with broad scientific applications. The existing model-based clustering techniques either rely on simple probability models (e.g., Gaussian) that are not generally valid for shape analysis or assume the number of clusters. We develop an efficient Bayesian method to cluster curve data using an elastic shape metric that is based on joint registration and comparison of shapes of curves. The elastic-inner product matrix obtained from the data is modeled using a Wishart distribution whose parameters are assigned carefully chosen prior distributions to allow for automatic inference on the number of clusters. Posterior is sampled through an efficient Markov chain Monte Carlo procedure based on the Chinese restaurant process to infer (1) the posterior distribution on the number of clusters, and (2) clustering configuration of shapes. This method is demonstrated on a variety of synthetic data and real data examples on protein structure analysis, cell shape analysis in microscopy images, and clustering of shaped from MPEG7 database.

</details>

<details>

<summary>2015-04-02 11:36:04 - Strong oracle optimality of folded concave penalized estimation</summary>

- *Jianqing Fan, Lingzhou Xue, Hui Zou*

- `1210.5992v4` - [abs](http://arxiv.org/abs/1210.5992v4) - [pdf](http://arxiv.org/pdf/1210.5992v4)

> Folded concave penalization methods have been shown to enjoy the strong oracle property for high-dimensional sparse estimation. However, a folded concave penalization problem usually has multiple local solutions and the oracle property is established only for one of the unknown local solutions. A challenging fundamental issue still remains that it is not clear whether the local optimum computed by a given optimization algorithm possesses those nice theoretical properties. To close this important theoretical gap in over a decade, we provide a unified theory to show explicitly how to obtain the oracle solution via the local linear approximation algorithm. For a folded concave penalized estimation problem, we show that as long as the problem is localizable and the oracle estimator is well behaved, we can obtain the oracle estimator by using the one-step local linear approximation. In addition, once the oracle estimator is obtained, the local linear approximation algorithm converges, namely it produces the same estimator in the next iteration. The general theory is demonstrated by using four classical sparse estimation problems, that is, sparse linear regression, sparse logistic regression, sparse precision matrix estimation and sparse quantile regression.

</details>

<details>

<summary>2015-04-02 16:05:12 - A Novel Sparsity-Based Approach to Recursive Estimation of Dynamic Parameter Sets</summary>

- *Ashkan Panahi, Mats Viberg*

- `1504.00600v1` - [abs](http://arxiv.org/abs/1504.00600v1) - [pdf](http://arxiv.org/pdf/1504.00600v1)

> We consider the problem of estimating a variable number of parameters with a dynamic nature. A familiar example is finding the position of moving targets using sensor array observations. The problem is challenging in cases where either the observations are not reliable or the parameters evolve rapidly. Inspired by the sparsity based techniques, we introduce a novel Bayesian model for the problems of interest and study its associated recursive Bayesian filter. We propose an algorithm approximating the Bayesian filter, maintaining a reasonable amount of calculations. We compare by numerical evaluation the resulting technique to state-of-the-art algorithms in different scenarios. In a scenario with a low SNR, the proposed method outperforms other complex techniques.

</details>

<details>

<summary>2015-04-02 16:32:34 - Simulation-based Estimation of Mean and Standard Deviation for Meta-analysis via Approximate Bayesian Computation (ABC)</summary>

- *Deukwoo Kwon, Isildinha M. Reis*

- `1504.00608v1` - [abs](http://arxiv.org/abs/1504.00608v1) - [pdf](http://arxiv.org/pdf/1504.00608v1)

> Background: When conducting a meta-analysis of a continuous outcome, estimated means and standard deviations from the selected studies are required in order to obtain an overall estimate of the mean effect and its confidence interval. If these quantities are not directly reported in the publications, they need to must be estimated from other reported summary statistics, such as the median, the minimum, the maximum, and quartiles. Methods: We propose a simulation-based estimation approach using the Approximate Bayesian Computation (ABC) technique for estimating mean and standard deviation based on various sets of summary statistics found in published studies. We conduct a simulation study to compare the proposed ABC method with the existing methods of Hozo et al. (2005), Bland (2015), and Wan et al. (2014). Results: In the estimation of the standard deviation, our ABC method performs best in skewed or heavy-tailed distributions. The average relative error (ARE) approaches zero as sample size increases. In the normal distribution, our ABC performs well. However, the Wan et al. method is best since it is based on the normal distribution assumption. When the distribution is skewed or heavy-tailed, the ARE of Wan et al. moves away from zero even as sample size increases. In the estimation of the mean, our ABC method is best since the AREs converge to zero. Conclusion: ABC is a flexible method for estimating the study-specific mean and standard deviation for meta-analysis, especially with underlying skewed or heavy-tailed distributions. The ABC method can be applied using other reported summary statistics such as the posterior mean and 95% credible interval when Bayesian analysis has been employed.

</details>

<details>

<summary>2015-04-03 11:50:36 - Proximal Markov chain Monte Carlo algorithms</summary>

- *Marcelo Pereyra*

- `1306.0187v4` - [abs](http://arxiv.org/abs/1306.0187v4) - [pdf](http://arxiv.org/pdf/1306.0187v4)

> This paper presents a new Metropolis-adjusted Langevin algorithm (MALA) that uses convex analysis to simulate efficiently from high-dimensional densities that are log-concave, a class of probability distributions that is widely used in modern high-dimensional statistics and data analysis. The method is based on a new first-order approximation for Langevin diffusions that exploits log-concavity to construct Markov chains with favourable convergence properties. This approximation is closely related to Moreau-Yoshida regularisations for convex functions and uses proximity mappings instead of gradient mappings to approximate the continuous-time process. The proposed method complements existing MALA methods in two ways. First, the method is shown to have very robust stability properties and to converge geometrically for many target densities for which other MALA are not geometric, or only if the step size is sufficiently small. Second, the method can be applied to high-dimensional target densities that are not continuously differentiable, a class of distributions that is increasingly used in image processing and machine learning and that is beyond the scope of existing MALA and HMC algorithms. To use this method it is necessary to compute or to approximate efficiently the proximity mappings of the logarithm of the target density. For several popular models, including many Bayesian models used in modern signal and image processing and machine learning, this can be achieved with convex optimisation algorithms and with approximations based on proximal splitting techniques, which can be implemented in parallel. The proposed method is demonstrated on two challenging high-dimensional and non-differentiable models related to image resolution enhancement and low-rank matrix estimation that are not well addressed by existing MCMC methodology.

</details>

<details>

<summary>2015-04-03 12:17:06 - Looking-backward probabilities for Gibbs-type exchangeable random partitions</summary>

- *Sergio Bacallado, Stefano Favaro, Lorenzo Trippa*

- `1504.00828v1` - [abs](http://arxiv.org/abs/1504.00828v1) - [pdf](http://arxiv.org/pdf/1504.00828v1)

> Gibbs-type random probability measures and the exchangeable random partitions they induce represent the subject of a rich and active literature. They provide a probabilistic framework for a wide range of theoretical and applied problems that are typically referred to as species sampling problems. In this paper, we consider the class of looking-backward species sampling problems introduced in Lijoi et al. (Ann. Appl. Probab. 18 (2008) 1519-1547) in Bayesian nonparametrics. Specifically, given some information on the random partition induced by an initial sample from a Gibbs-type random probability measure, we study the conditional distributions of statistics related to the old species, namely those species detected in the initial sample and possibly re-observed in an additional sample. The proposed results contribute to the analysis of conditional properties of Gibbs-type exchangeable random partitions, so far focused mainly on statistics related to those species generated by the additional sample and not already detected in the initial sample.

</details>

<details>

<summary>2015-04-03 12:36:37 - Rate-optimal posterior contraction for sparse PCA</summary>

- *Chao Gao, Harrison H. Zhou*

- `1312.0142v5` - [abs](http://arxiv.org/abs/1312.0142v5) - [pdf](http://arxiv.org/pdf/1312.0142v5)

> Principal component analysis (PCA) is possibly one of the most widely used statistical tools to recover a low-rank structure of the data. In the high-dimensional settings, the leading eigenvector of the sample covariance can be nearly orthogonal to the true eigenvector. A sparse structure is then commonly assumed along with a low rank structure. Recently, minimax estimation rates of sparse PCA were established under various interesting settings. On the other side, Bayesian methods are becoming more and more popular in high-dimensional estimation, but there is little work to connect frequentist properties and Bayesian methodologies for high-dimensional data analysis. In this paper, we propose a prior for the sparse PCA problem and analyze its theoretical properties. The prior adapts to both sparsity and rank. The posterior distribution is shown to contract to the truth at optimal minimax rates. In addition, a computationally efficient strategy for the rank-one case is discussed.

</details>

<details>

<summary>2015-04-03 22:10:33 - Bayesian inference for Matérn repulsive processes</summary>

- *Vinayak Rao, Ryan P. Adams, David B. Dunson*

- `1308.1136v2` - [abs](http://arxiv.org/abs/1308.1136v2) - [pdf](http://arxiv.org/pdf/1308.1136v2)

> In many applications involving point pattern data, the Poisson process assumption is unrealistic, with the data exhibiting a more regular spread. Such a repulsion between events is exhibited by trees for example, because of competition for light and nutrients. Other examples include the locations of biological cells and cities, and the times of neuronal spikes. Given the many applications of repulsive point processes, there is a surprisingly limited literature developing flexible, realistic and interpretable models, as well as efficient inferential methods. We address this gap by developing a modelling framework around the Mat\'ern type-III repulsive process. We consider a number of extensions of the original Mat\'ern type-III process for both the homogeneous and inhomogeneous cases. We also derive the probability density of this generalized Mat\'ern process. This allows us to characterize the posterior distribution of the various latent variables, and leads to a novel and efficient Markov chain Monte Carlo algorithm. We apply our ideas to datasets involving the spatial locations of trees, nerve fiber cells and Greyhound bus stations.

</details>

<details>

<summary>2015-04-04 22:01:27 - Minimum Risk Equivariant Estimation of the Parameters of the General Half-Normal Distribution by Means of a Monte Carlo Method to Approximate Conditional Expectations</summary>

- *A. G. Nogales, P. Pérez, P. Monfort*

- `1504.01061v1` - [abs](http://arxiv.org/abs/1504.01061v1) - [pdf](http://arxiv.org/pdf/1504.01061v1)

> This work addresses the problem of estimating the parameters of the general half-normal distribution. Namely, the problem of determining the minimum risk equi\-va\-riant (MRE) estimators of the parameters is explored. Simulation studies are realized to compare the behavior of these estimators with maximum likelihood and unbiased estimators. A natural Monte Carlo method to compute conditional expectations is used to approximate the MRE estimation of the location parameter because its expression involves two conditional expectations not easily computables. The used Monte Carlo method is justified by a theorem of Besicovitch on differentiation of measures, and has been slightly modified to solve a sort of "curse of dimensionality" problem appearing in the estimation of this parameter. This method has been implicitly used in the last years in the context of ABC (approximate Bayesian computation) methods.

</details>

<details>

<summary>2015-04-06 14:31:38 - A New Class of Gamma distribution</summary>

- *Cícero Carlos Ramos de Brito, Leandro Chaves Rêgo, Wilson Rosa de Oliveira*

- `1504.01291v1` - [abs](http://arxiv.org/abs/1504.01291v1) - [pdf](http://arxiv.org/pdf/1504.01291v1)

> This paper presents a new class of probability distributions generated from the gamma distribution. For the new class proposed, we present several statistical properties, such as the risk function, the density expansions, Moment-generating function, characteristic function, the moments of order m, central moments of order m, the log likelihood and its partial derivatives and also entropy, kurtosis, symmetry and variance. These same properties are determined for a particular distribution within this new class that is used to illustrate the capability of the proposed new class through an application to a real data set. The database presented in Choulakian and Stephens (2001) was used. Six models are compared and for the selection of these models were used the Akaike Information Criterion (AIC), the Akaike Information Criterion corrected (AICc), Bayesian Information Criterion (BIC), Hannan Quinn Information Criterion (HQIC) and tests of Cramer-Von Mises and Anderson-Darling to assess the models fit. Finally, we present the conclusions from the analysis and comparison of the results obtained and the directions for future work.

</details>

<details>

<summary>2015-04-06 18:19:45 - Early Stopping is Nonparametric Variational Inference</summary>

- *Dougal Maclaurin, David Duvenaud, Ryan P. Adams*

- `1504.01344v1` - [abs](http://arxiv.org/abs/1504.01344v1) - [pdf](http://arxiv.org/pdf/1504.01344v1)

> We show that unconverged stochastic gradient descent can be interpreted as a procedure that samples from a nonparametric variational approximate posterior distribution. This distribution is implicitly defined as the transformation of an initial distribution by a sequence of optimization updates. By tracking the change in entropy over this sequence of transformations during optimization, we form a scalable, unbiased estimate of the variational lower bound on the log marginal likelihood. We can use this bound to optimize hyperparameters instead of using cross-validation. This Bayesian interpretation of SGD suggests improved, overfitting-resistant optimization procedures, and gives a theoretical foundation for popular tricks such as early stopping and ensembling. We investigate the properties of this marginal likelihood estimator on neural network models.

</details>

<details>

<summary>2015-04-07 09:23:23 - Bayesian Detection of Changepoints in Finite-State Markov Chains for Multiple Sequences</summary>

- *Petter Arnesen, Tracy Holsclaw, Padhraic Smyth*

- `1504.00860v2` - [abs](http://arxiv.org/abs/1504.00860v2) - [pdf](http://arxiv.org/pdf/1504.00860v2)

> We consider the analysis of sets of categorical sequences consisting of piecewise homogeneous Markov segments. The sequences are assumed to be governed by a common underlying process with segments occurring in the same order for each sequence. Segments are defined by a set of unobserved changepoints where the positions and number of changepoints can vary from sequence to sequence. We propose a Bayesian framework for analyzing such data, placing priors on the locations of the changepoints and on the transition matrices and using Markov chain Monte Carlo (MCMC) techniques to obtain posterior samples given the data. Experimental results using simulated data illustrates how the methodology can be used for inference of posterior distributions for parameters and changepoints, as well as the ability to handle considerable variability in the locations of the changepoints across different sequences. We also investigate the application of the approach to sequential data from two applications involving monsoonal rainfall patterns and branching patterns in trees.

</details>

<details>

<summary>2015-04-08 01:13:57 - Bayesian Inference for Duplication-Mutation with Complementarity Network Models</summary>

- *Ajay Jasra, Adam Persing, Alexandros Beskos, Kari Heine, Maria De Iorio*

- `1504.01794v1` - [abs](http://arxiv.org/abs/1504.01794v1) - [pdf](http://arxiv.org/pdf/1504.01794v1)

> We observe an undirected graph $G$ without multiple edges and self-loops, which is to represent a protein-protein interaction (PPI) network. We assume that $G$ evolved under the duplication-mutation with complementarity (DMC) model from a seed graph, $G_0$, and we also observe the binary forest $\Gamma$ that represents the duplication history of $G$. A posterior density for the DMC model parameters is established, and we outline a sampling strategy by which one can perform Bayesian inference; that sampling strategy employs a particle marginal Metropolis-Hastings (PMMH) algorithm. We test our methodology on numerical examples to demonstrate a high accuracy and precision in the inference of the DMC model's mutation and homodimerization parameters.

</details>

<details>

<summary>2015-04-09 00:04:14 - Hierarchical Modeling of Abundance in Closed Population Capture-Recapture Models Under Heterogeneity</summary>

- *Matthew R. Schofield, Richard J. Barker*

- `1009.3507v2` - [abs](http://arxiv.org/abs/1009.3507v2) - [pdf](http://arxiv.org/pdf/1009.3507v2)

> Hierarchical modeling of abundance in space or time using closed-population mark-recapture under heterogeneity (model M$_{h}$) presents two challenges: (i) finding a flexible likelihood in which abundance appears as an explicit parameter and (ii) fitting the hierarchical model for abundance. The first challenge arises because abundance not only indexes the population size, it also determines the dimension of the capture probabilities in heterogeneity models. A common approach is to use data augmentation to include these capture probabilities directly into the likelihood and fit the model using Bayesian inference via Markov chain Monte Carlo (MCMC). Two such examples of this approach are (i) explicit trans-dimensional MCMC, and (ii) superpopulation data augmentation. The superpopulation approach has the advantage of simple specification that is easily implemented in BUGS and related software. However, it reparameterizes the model so that abundance is no longer included, except as a derived quantity. This is a drawback when hierarchical models for abundance, or related parameters, are desired. Here, we analytically compare the two approaches and show that they are more closely related than might appear superficially. We exploit this relationship to specify the model in a way that allows us to include abundance as a parameter and that facilitates hierarchical modeling using readily available software such as BUGS. We use this approach to model trends in grizzly bear abundance in Yellowstone National Park from 1986-1998.

</details>

<details>

<summary>2015-04-09 09:13:28 - Bayesian estimation of the multifractality parameter for image texture using a Whittle approximation</summary>

- *Sébastien Combrexelle, Herwig Wendt, Nicolas Dobigeon, Jean-Yves Tourneret, Steve McLaughlin, Patrice Abry*

- `1410.4871v2` - [abs](http://arxiv.org/abs/1410.4871v2) - [pdf](http://arxiv.org/pdf/1410.4871v2)

> Texture characterization is a central element in many image processing applications. Multifractal analysis is a useful signal and image processing tool, yet, the accurate estimation of multifractal parameters for image texture remains a challenge. This is due in the main to the fact that current estimation procedures consist of performing linear regressions across frequency scales of the two-dimensional (2D) dyadic wavelet transform, for which only a few such scales are computable for images. The strongly non-Gaussian nature of multifractal processes, combined with their complicated dependence structure, makes it difficult to develop suitable models for parameter estimation. Here, we propose a Bayesian procedure that addresses the difficulties in the estimation of the multifractality parameter. The originality of the procedure is threefold: The construction of a generic semi-parametric statistical model for the logarithm of wavelet leaders; the formulation of Bayesian estimators that are associated with this model and the set of parameter values admitted by multifractal theory; the exploitation of a suitable Whittle approximation within the Bayesian model which enables the otherwise infeasible evaluation of the posterior distribution associated with the model. Performance is assessed numerically for several 2D multifractal processes, for several image sizes and a large range of process parameters. The procedure yields significant benefits over current benchmark estimators in terms of estimation performance and ability to discriminate between the two most commonly used classes of multifractal process models. The gains in performance are particularly pronounced for small image sizes, notably enabling for the first time the analysis of image patches as small as 64x64 pixels.

</details>

<details>

<summary>2015-04-10 02:57:46 - Connecting the latent multinomial</summary>

- *Matthew R. Schofield, Simon J. Bonner*

- `1504.04566v1` - [abs](http://arxiv.org/abs/1504.04566v1) - [pdf](http://arxiv.org/pdf/1504.04566v1)

> Link et al. (2010) define a general framework for analyzing capture-recapture data with potential misidentifications. In this framework, the observed vector of counts, $y$, is considered as a linear function of a vector of latent counts, $x$, such that $y = A x$, with $x$ assumed to follow a multinomial distribution conditional on the model parameters, $\theta$. Bayesian methods are then applied by sampling from the joint posterior distribution of both $x$ and $\theta$. In particular, Link et al. (2010) propose a Metropolis-Hastings algorithm to sample from the full conditional distribution of $x$, where new proposals are generated by sequentially adding elements from a basis of the null space (kernel) of $A$. We consider this algorithm and show that using elements from a simple basis for the kernel of $A$ may not produce an irreducible Markov chain. Instead, we require a Markov basis, as defined by Diaconis and Sturmfels (1998). We illustrate the importance of Markov bases with three capture-recapture examples. We prove that a specific lattice basis is a Markov basis for a class of models including the original model considered by Link et al. (2010) and confirm that the specific basis used by Link et al. (2010) for their example with two sampling occasions is a Markov basis. The constructive nature of our proof provides an immediate method to obtain a Markov basis for any model in this class.

</details>

<details>

<summary>2015-04-10 14:15:02 - Overall Objective Priors</summary>

- *James O. Berger, Jose M. Bernardo, Dongchu Sun*

- `1504.02689v1` - [abs](http://arxiv.org/abs/1504.02689v1) - [pdf](http://arxiv.org/pdf/1504.02689v1)

> In multi-parameter models, reference priors typically depend on the parameter or quantity of interest, and it is well known that this is necessary to produce objective posterior distributions with optimal properties. There are, however, many situations where one is simultaneously interested in all the parameters of the model or, more realistically, in functions of them that include aspects such as prediction, and it would then be useful to have a single objective prior that could safely be used to produce reasonable posterior inferences for all the quantities of interest. In this paper, we consider three methods for selecting a single objective prior and study, in a variety of problems including the multinomial problem, whether or not the resulting prior is a reasonable overall prior.

</details>

<details>

<summary>2015-04-10 17:28:06 - On the Brittleness of Bayesian Inference</summary>

- *Houman Owhadi, Clint Scovel, Tim Sullivan*

- `1308.6306v3` - [abs](http://arxiv.org/abs/1308.6306v3) - [pdf](http://arxiv.org/pdf/1308.6306v3)

> With the advent of high-performance computing, Bayesian methods are increasingly popular tools for the quantification of uncertainty throughout science and industry. Since these methods impact the making of sometimes critical decisions in increasingly complicated contexts, the sensitivity of their posterior conclusions with respect to the underlying models and prior beliefs is a pressing question for which there currently exist positive and negative results. We report new results suggesting that, although Bayesian methods are robust when the number of possible outcomes is finite or when only a finite number of marginals of the data-generating distribution are unknown, they could be generically brittle when applied to continuous systems (and their discretizations) with finite information on the data-generating distribution. If closeness is defined in terms of the total variation metric or the matching of a finite system of generalized moments, then (1) two practitioners who use arbitrarily close models and observe the same (possibly arbitrarily large amount of) data may reach opposite conclusions; and (2) any given prior and model can be slightly perturbed to achieve any desired posterior conclusions. The mechanism causing brittlenss/robustness suggests that learning and robustness are antagonistic requirements and raises the question of a missing stability condition for using Bayesian Inference in a continuous world under finite information.

</details>

<details>

<summary>2015-04-12 02:53:05 - Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo</summary>

- *Yu-Xiang Wang, Stephen E. Fienberg, Alex Smola*

- `1502.07645v2` - [abs](http://arxiv.org/abs/1502.07645v2) - [pdf](http://arxiv.org/pdf/1502.07645v2)

> We consider the problem of Bayesian learning on sensitive datasets and present two simple but somewhat surprising results that connect Bayesian learning to "differential privacy:, a cryptographic approach to protect individual-level privacy while permiting database-level utility. Specifically, we show that that under standard assumptions, getting one single sample from a posterior distribution is differentially private "for free". We will see that estimator is statistically consistent, near optimal and computationally tractable whenever the Bayesian model of interest is consistent, optimal and tractable. Similarly but separately, we show that a recent line of works that use stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preserve differentially privacy with minor or no modifications of the algorithmic procedure at all, these observations lead to an "anytime" algorithm for Bayesian learning under privacy constraint. We demonstrate that it performs much better than the state-of-the-art differential private methods on synthetic and real datasets.

</details>

<details>

<summary>2015-04-12 05:49:29 - Optimal Multiple Testing Under a Gaussian Prior on the Effect Sizes</summary>

- *Edgar Dobriban, Kristen Fortney, Stuart K. Kim, Art B. Owen*

- `1504.02935v1` - [abs](http://arxiv.org/abs/1504.02935v1) - [pdf](http://arxiv.org/pdf/1504.02935v1)

> We develop a new method for frequentist multiple testing with Bayesian prior information. Our procedure finds a new set of optimal p-value weights called the Bayes weights. Prior information is relevant to many multiple testing problems. Existing methods assume fixed, known effect sizes available from previous studies. However, the case of uncertain information is usually the norm. For a Gaussian prior on effect sizes, we show that finding the optimal weights is a non-convex problem. Despite the non-convexity, we give an efficient algorithm that solves this problem nearly exactly. We show that our method can discover new loci in genome-wide association studies. On several data sets it compares favorably to other methods. Open source code is available.

</details>

<details>

<summary>2015-04-13 12:40:55 - Bayesian computational algorithms for social network analysis</summary>

- *Alberto Caimo, Isabella Gollini*

- `1504.03152v1` - [abs](http://arxiv.org/abs/1504.03152v1) - [pdf](http://arxiv.org/pdf/1504.03152v1)

> In this chapter we review some of the most recent computational advances in the rapidly expanding field of statistical social network analysis using the R open-source software. In particular we will focus on Bayesian estimation for two important families of models: exponential random graph models (ERGMs) and latent space models (LSMs).

</details>

<details>

<summary>2015-04-16 12:45:35 - Quantile forecast discrimination ability and value</summary>

- *Zied Ben Bouallegue, Pierre Pinson, Petra Friederichs*

- `1504.04211v1` - [abs](http://arxiv.org/abs/1504.04211v1) - [pdf](http://arxiv.org/pdf/1504.04211v1)

> While probabilistic forecast verification for categorical forecasts is well established, some of the existing concepts and methods have not found their equivalent for the case of continuous variables. New tools dedicated to the assessment of forecast discrimination ability and forecast value are introduced here, based on quantile forecasts being the base product for the continuous case (hence in a nonparametric framework). The relative user characteristic (RUC) curve and the quantile value plot allow analysing the performance of a forecast for a specific user in a decision-making framework. The RUC curve is designed as a user-based discrimination tool and the quantile value plot translates forecast discrimination ability in terms of economic value. The relationship between the overall value of a quantile forecast and the respective quantile skill score is also discussed. The application of these new verification approaches and tools is illustrated based on synthetic datasets, as well as for the case of global radiation forecasts from the high resolution ensemble COSMO-DE-EPS of the German Weather Service.

</details>

<details>

<summary>2015-04-17 06:59:26 - Asymptotic Accuracy of Bayesian Estimation for a Single Latent Variable</summary>

- *Keisuke Yamazaki*

- `1408.5661v3` - [abs](http://arxiv.org/abs/1408.5661v3) - [pdf](http://arxiv.org/pdf/1408.5661v3)

> In data science and machine learning, hierarchical parametric models, such as mixture models, are often used. They contain two kinds of variables: observable variables, which represent the parts of the data that can be directly measured, and latent variables, which represent the underlying processes that generate the data. Although there has been an increase in research on the estimation accuracy for observable variables, the theoretical analysis of estimating latent variables has not been thoroughly investigated. In a previous study, we determined the accuracy of a Bayes estimation for the joint probability of the latent variables in a dataset, and we proved that the Bayes method is asymptotically more accurate than the maximum-likelihood method. However, the accuracy of the Bayes estimation for a single latent variable remains unknown. In the present paper, we derive the asymptotic expansions of the error functions, which are defined by the Kullback-Leibler divergence, for two types of single-variable estimations when the statistical regularity is satisfied. Our results indicate that the accuracies of the Bayes and maximum-likelihood methods are asymptotically equivalent and clarify that the Bayes method is only advantageous for multivariable estimations.

</details>

<details>

<summary>2015-04-17 09:53:02 - On econometric inference and multiple use of the same data</summary>

- *Benjamin Holcblat, Steffen Grønneberg*

- `1504.04472v1` - [abs](http://arxiv.org/abs/1504.04472v1) - [pdf](http://arxiv.org/pdf/1504.04472v1)

> In fields that are mainly nonexperimental, such as economics and finance, it is inescapable to compute test statistics and confidence regions that are not probabilistically independent from previously examined data. The Bayesian and Neyman-Pearson inference theories are known to be inadequate for such a practice. We show that these inadequacies also hold m.a.e. (modulo approximation error). We develop a general econometric theory, called the neoclassical inference theory, that is immune to this inadequacy m.a.e. The neoclassical inference theory appears to nest model calibration, and most econometric practices, whether they are labelled Bayesian or \`a la Neyman-Pearson. We derive a general, but simple adjustment to make standard errors account for the approximation error.

</details>

<details>

<summary>2015-04-17 14:08:40 - Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet Representations, and Forecast Rankings</summary>

- *Werner Ehm, Tilmann Gneiting, Alexander Jordan, Fabian Krüger*

- `1503.08195v2` - [abs](http://arxiv.org/abs/1503.08195v2) - [pdf](http://arxiv.org/pdf/1503.08195v2)

> In the practice of point prediction, it is desirable that forecasters receive a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. When evaluating and comparing competing forecasts, it is then critical that the scoring function used for these purposes be consistent for the functional at hand, in the sense that the expected score is minimized when following the directive.   We show that any scoring function that is consistent for a quantile or an expectile functional, respectively, can be represented as a mixture of extremal scoring functions that form a linearly parameterized family. Scoring functions for the mean value and probability forecasts of binary events constitute important examples. The quantile and expectile functionals along with the respective extremal scoring functions admit appealing economic interpretations in terms of thresholds in decision making.   The Choquet type mixture representations give rise to simple checks of whether a forecast dominates another in the sense that it is preferable under any consistent scoring function. In empirical settings it suffices to compare the average scores for only a finite number of extremal elements. Plots of the average scores with respect to the extremal scoring functions, which we call Murphy diagrams, permit detailed comparisons of the relative merits of competing forecasts.

</details>

<details>

<summary>2015-04-17 15:07:11 - Hyperspectral pansharpening: a review</summary>

- *Laetitia Loncan, Luis B. Almeida, José M. Bioucas-Dias, Xavier Briottet, Jocelyn Chanussot, Nicolas Dobigeon, Sophie Fabre, Wenzhi Liao, Giorgio A. Licciardi, Miguel Simões, Jean-Yves Tourneret, Miguel A. Veganzones, Gemine Vivone, Qi Wei, Naoto Yokoya*

- `1504.04531v1` - [abs](http://arxiv.org/abs/1504.04531v1) - [pdf](http://arxiv.org/pdf/1504.04531v1)

> Pansharpening aims at fusing a panchromatic image with a multispectral one, to generate an image with the high spatial resolution of the former and the high spectral resolution of the latter. In the last decade, many algorithms have been presented in the literature for pansharpening using multispectral data. With the increasing availability of hyperspectral systems, these methods are now being adapted to hyperspectral images. In this work, we compare new pansharpening techniques designed for hyperspectral data with some of the state of the art methods for multispectral pansharpening, which have been adapted for hyperspectral data. Eleven methods from different classes (component substitution, multiresolution analysis, hybrid, Bayesian and matrix factorization) are analyzed. These methods are applied to three datasets and their effectiveness and robustness are evaluated with widely used performance indicators. In addition, all the pansharpening techniques considered in this paper have been implemented in a MATLAB toolbox that is made available to the community.

</details>

<details>

<summary>2015-04-18 08:32:28 - On Bayesian based adaptive confidence sets for linear functionals</summary>

- *Botond Szabó*

- `1412.0459v2` - [abs](http://arxiv.org/abs/1412.0459v2) - [pdf](http://arxiv.org/pdf/1412.0459v2)

> We consider the problem of constructing Bayesian based confidence sets for linear functionals in the inverse Gaussian white noise model. We work with a scale of Gaussian priors indexed by a regularity hyper-parameter and apply the data-driven (slightly modified) marginal likelihood empirical Bayes method for the choice of this hyper-parameter. We show by theory and simulations that the credible sets constructed by this method have sub-optimal behaviour in general. However, by assuming "self-similarity" the credible sets have rate-adaptive size and optimal coverage. As an application of these results we construct $L_{\infty}$-credible bands for the true functional parameter with adaptive size and optimal coverage under self-similarity constraint.

</details>

<details>

<summary>2015-04-18 14:26:42 - On Robustness of the Shiryaev-Roberts Procedure for Quickest Change-Point Detection under Parameter Misspecification in the Post-Change Distribution</summary>

- *Wenyu Du, Aleksey S. Polunchenko, Grigory Sokolov*

- `1504.04722v1` - [abs](http://arxiv.org/abs/1504.04722v1) - [pdf](http://arxiv.org/pdf/1504.04722v1)

> The gist of the quickest change-point detection problem is to detect the presence of a change in the statistical behavior of a series of sequentially made observations, and do so in an optimal detection-speed-vs.-"false-positive"-risk manner. When optimality is understood either in the generalized Bayesian sense or as defined in Shiryaev's multi-cyclic setup, the so-called Shiryaev-Roberts (SR) detection procedure is known to be the "best one can do", provided, however, that the observations' pre- and post-change distributions are both fully specified. We consider a more realistic setup, viz. one where the post-change distribution is assumed known only up to a parameter, so that the latter may be "misspecified". The question of interest is the sensitivity (or robustness) of the otherwise "best" SR procedure with respect to a possible misspecification of the post-change distribution parameter. To answer this question, we provide a case study where, in a specific Gaussian scenario, we allow the SR procedure to be "out of tune" in the way of the post-change distribution parameter, and numerically assess the effect of the "mistuning" on Shiryaev's (multi-cyclic) Stationary Average Detection Delay delivered by the SR procedure. The comprehensive quantitative robustness characterization of the SR procedure obtained in the study can be used to develop the respective theory as well as to provide a rational for practical design of the SR procedure. The overall qualitative conclusion of the study is an expected one: the SR procedure is less (more) robust for less (more) contrast changes and for lower (higher) levels of the false alarm risk.

</details>

<details>

<summary>2015-04-20 03:27:33 - Spatial Capture-recapture with Partial Identity</summary>

- *J. Andrew Royle*

- `1503.06873v2` - [abs](http://arxiv.org/abs/1503.06873v2) - [pdf](http://arxiv.org/pdf/1503.06873v2)

> We develop an inference framework for spatial capture-recapture data when two methods are used in which individuality cannot generally be reconciled between the two methods. A special case occurs in camera trapping when left-side (method 1) and right-side (method 2) photos are obtained but not simultaneously. We specify a spatially explicit capture-recapture model for the latent "perfect" data set which is conditioned on known identity of individuals between methods. We regard the identity variable which associates individuals of the two data sets as an unknown in the model and we propose a Bayesian analysis strategy for the model in which the identity variable is updated using a Metropolis component algorithm. The work extends previous efforts to deal with incomplete data by recognizing that there is information about individuality in the spatial juxtaposition of captures. Thus, individual records obtained by both sampling methods that are in close proximity are more likely to be the same individual than individuals that are not in close proximity. The model proposed here formalizes this trade-off between spatial proximity and probabilistic determination of individuality using spatially explicit capture-recapture models.

</details>

<details>

<summary>2015-04-21 05:36:13 - Streaming Variational Inference for Bayesian Nonparametric Mixture Models</summary>

- *Alex Tank, Nicholas J. Foti, Emily B. Fox*

- `1412.0694v3` - [abs](http://arxiv.org/abs/1412.0694v3) - [pdf](http://arxiv.org/pdf/1412.0694v3)

> In theory, Bayesian nonparametric (BNP) models are well suited to streaming data scenarios due to their ability to adapt model complexity with the observed data. Unfortunately, such benefits have not been fully realized in practice; existing inference algorithms are either not applicable to streaming applications or not extensible to BNP models. For the special case of Dirichlet processes, streaming inference has been considered. However, there is growing interest in more flexible BNP models building on the class of normalized random measures (NRMs). We work within this general framework and present a streaming variational inference algorithm for NRM mixture models. Our algorithm is based on assumed density filtering (ADF), leading straightforwardly to expectation propagation (EP) for large-scale batch inference as well. We demonstrate the efficacy of the algorithm on clustering documents in large, streaming text corpora.

</details>

<details>

<summary>2015-04-21 13:09:25 - Bayesian Polynomial Regression Models to Fit Multiple Genetic Models for Quantitative Traits</summary>

- *Harold Bae, Thomas Perls, Martin Steinberg, Paola Sebastiani*

- `1504.05415v1` - [abs](http://arxiv.org/abs/1504.05415v1) - [pdf](http://arxiv.org/pdf/1504.05415v1)

> We present a coherent Bayesian framework for selection of the most likely model from the five genetic models (genotypic, additive, dominant, co-dominant, and recessive) commonly used in genetic association studies. The approach uses a polynomial parameterization of genetic data to simultaneously fit the five models and save computations. We provide a closed-form expression of the marginal likelihood for normally distributed data, and evaluate the performance of the proposed method and existing method through simulated and real genome-wide data sets.

</details>

<details>

<summary>2015-04-22 06:27:19 - Rebuilding Factorized Information Criterion: Asymptotically Accurate Marginal Likelihood</summary>

- *Kohei Hayashi, Shin-ichi Maeda, Ryohei Fujimaki*

- `1504.05665v1` - [abs](http://arxiv.org/abs/1504.05665v1) - [pdf](http://arxiv.org/pdf/1504.05665v1)

> Factorized information criterion (FIC) is a recently developed approximation technique for the marginal log-likelihood, which provides an automatic model selection framework for a few latent variable models (LVMs) with tractable inference algorithms. This paper reconsiders FIC and fills theoretical gaps of previous FIC studies. First, we reveal the core idea of FIC that allows generalization for a broader class of LVMs, including continuous LVMs, in contrast to previous FICs, which are applicable only to binary LVMs. Second, we investigate the model selection mechanism of the generalized FIC. Our analysis provides a formal justification of FIC as a model selection criterion for LVMs and also a systematic procedure for pruning redundant latent variables that have been removed heuristically in previous studies. Third, we provide an interpretation of FIC as a variational free energy and uncover a few previously-unknown their relationships. A demonstrative study on Bayesian principal component analysis is provided and numerical experiments support our theoretical results.

</details>

<details>

<summary>2015-04-22 10:47:01 - Noise Robust Online Inference for Linear Dynamic Systems</summary>

- *Saikat Saha*

- `1504.05723v1` - [abs](http://arxiv.org/abs/1504.05723v1) - [pdf](http://arxiv.org/pdf/1504.05723v1)

> We revisit the Bayesian online inference problems for the linear dynamic systems (LDS) under non- Gaussian environment. The noises can naturally be non-Gaussian (skewed and/or heavy tailed) or to accommodate spurious observations, noises can be modeled as heavy tailed. However, at the cost of such noise robustness, the performance may degrade when such spurious observations are absent. Therefore, any inference engine should not only be robust to noise outlier, but also be adaptive to potentially unknown and time varying noise parameters; yet it should be scalable and easy to implement.   To address them, we envisage here a new noise adaptive Rao-Blackwellized particle filter (RBPF), by leveraging a hierarchically Gaussian model as a proxy for any non-Gaussian (process or measurement) noise density. This leads to a conditionally linear Gaussian model (CLGM), that is tractable. However, this framework requires a valid transition kernel for the intractable state, targeted by the particle filter (PF). This is typically unknown. We outline how such kernel can be constructed provably, at least for certain classes encompassing many commonly occurring non-Gaussian noises, using auxiliary latent variable approach. The efficacy of this RBPF algorithm is demonstrated through numerical studies.

</details>

<details>

<summary>2015-04-22 12:24:21 - Efficient Sequential Monte-Carlo Samplers for Bayesian Inference</summary>

- *Thi Le Thu Nguyen, Francois Septier, Gareth W. Peters, Yves Delignon*

- `1504.05753v1` - [abs](http://arxiv.org/abs/1504.05753v1) - [pdf](http://arxiv.org/pdf/1504.05753v1)

> In many problems, complex non-Gaussian and/or nonlinear models are required to accurately describe a physical system of interest. In such cases, Monte Carlo algorithms are remarkably flexible and extremely powerful approaches to solve such inference problems. However, in the presence of a high-dimensional and/or multimodal posterior distribution, it is widely documented that standard Monte-Carlo techniques could lead to poor performance. In this paper, the study is focused on a Sequential Monte-Carlo (SMC) sampler framework, a more robust and efficient Monte Carlo algorithm. Although this approach presents many advantages over traditional Monte-Carlo methods, the potential of this emergent technique is however largely underexploited in signal processing. In this work, we aim at proposing some novel strategies that will improve the efficiency and facilitate practical implementation of the SMC sampler specifically for signal processing applications. Firstly, we propose an automatic and adaptive strategy that selects the sequence of distributions within the SMC sampler that minimizes the asymptotic variance of the estimator of the posterior normalization constant. This is critical for performing model selection in modelling applications in Bayesian signal processing. The second original contribution we present improves the global efficiency of the SMC sampler by introducing a novel correction mechanism that allows the use of the particles generated through all the iterations of the algorithm (instead of only particles from the last iteration). This is a significant contribution as it removes the need to discard a large portion of the samples obtained, as is standard in standard SMC methods. This will improve estimation performance in practical settings where computational budget is important to consider.

</details>

<details>

<summary>2015-04-23 08:21:22 - Estimation of a non-stationary model for annual precipitation in southern Norway using replicates of the spatial field</summary>

- *Rikke Ingebrigtsen, Finn Lindgren, Ingelin Steinsland, Sara Martino*

- `1412.2798v2` - [abs](http://arxiv.org/abs/1412.2798v2) - [pdf](http://arxiv.org/pdf/1412.2798v2)

> Estimation of stationary dependence structure parameters using only a single realisation of the spatial process, typically leads to inaccurate estimates and poorly identified parameters. A common way to handle this is to fix some of the parameters, or within the Bayesian framework, impose prior knowledge. In many applied settings, stationary models are not flexible enough to model the process of interest, thus non-stationary spatial models are used. However, more flexible models usually means more parameters, and the identifiability problem becomes even more challenging. We investigate aspects of estimation of a Bayesian non-stationary spatial model for annual precipitation using observations from multiple years. The model contains replicates of the spatial field, which increases precision of the estimates and makes them less prior sensitive. Using R-INLA, we analyse precipitation data from southern Norway, and investigate statistical properties of the replicate model in a simulation study. The non-stationary spatial model we explore belongs to a recently introduced class of stochastic partial differential equation (SPDE) based spatial models. This model class allows for non-stationary models with explanatory variables in the dependence structure. We derive conditions to facilitate prior specification for these types of non-stationary spatial models.

</details>

<details>

<summary>2015-04-23 17:24:57 - Bayesian inference of CMB gravitational lensing</summary>

- *Ethan Anderes, Benjamin Wandelt, Guilhem Lavaux*

- `1412.4079v2` - [abs](http://arxiv.org/abs/1412.4079v2) - [pdf](http://arxiv.org/pdf/1412.4079v2)

> The Planck satellite, along with several ground based telescopes, have mapped the cosmic microwave background (CMB) at sufficient resolution and signal-to-noise so as to allow a detection of the subtle distortions due to the gravitational influence of the intervening matter distribution. A natural modeling approach is to write a Bayesian hierarchical model for the lensed CMB in terms of the unlensed CMB and the lensing potential. So far there has been no feasible algorithm for inferring the posterior distribution of the lensing potential from the lensed CMB map. We propose a solution that allows efficient Markov Chain Monte Carlo sampling from the joint posterior of the lensing potential and the unlensed CMB map using the Hamiltonian Monte Carlo technique. The main conceptual step in the solution is a re-parameterization of CMB lensing in terms of the lensed CMB and the "inverse lensing" potential. We demonstrate a fast implementation on simulated data including noise and a sky cut, that uses a further acceleration based on a very mild approximation of the inverse lensing potential. We find that the resulting Markov Chain has short correlation lengths and excellent convergence properties, making it promising for application to high resolution CMB data sets of the future.

</details>

<details>

<summary>2015-04-23 23:01:04 - BayesSummaryStatLM: An R package for Bayesian Linear Models for Big Data and Data Science</summary>

- *Alexey Miroshnikov, Evgeny Savel'ev, Erin M. Conlon*

- `1503.00635v2` - [abs](http://arxiv.org/abs/1503.00635v2) - [pdf](http://arxiv.org/pdf/1503.00635v2)

> Recent developments in data science and big data research have produced an abundance of large data sets that are too big to be analyzed in their entirety, due to limits on either computer memory or storage capacity. Here, we introduce our R package 'BayesSummaryStatLM' for Bayesian linear regression models with Markov chain Monte Carlo implementation that overcomes these limitations. Our Bayesian models use only summary statistics of data as input; these summary statistics can be calculated from subsets of big data and combined over subsets. Thus, complete data sets do not need to be read into memory in full, which removes any physical memory limitations of a user. Our package incorporates the R package 'ff' and its functions for reading in big data sets in chunks while simultaneously calculating summary statistics. We describe our Bayesian linear regression models, including several choices of prior distributions for unknown model parameters, and illustrate capabilities and features of our R package using both simulated and real data sets.

</details>

<details>

<summary>2015-04-24 09:36:45 - Multilevel Monte Carlo simulation of a diffusion with non-smooth drift</summary>

- *Azzouz Dermoune Daoud Ounaissi Nadji Rahmania*

- `1504.06441v1` - [abs](http://arxiv.org/abs/1504.06441v1) - [pdf](http://arxiv.org/pdf/1504.06441v1)

> We show that Lasso and Bayesian Lasso are very close when the sparsity is large and the noise is small. Then we propose to solve Bayesian Lasso using multivalued stochastic differential equation. We obtain three discretizations algorithms, and propose a method for calculating the cost of Monte-Carlo (MC), multilevel Monte Carlo (MLMC) and MCMC algorithms.

</details>

<details>

<summary>2015-04-24 14:27:47 - Objective Bayesian Inference for Bilateral Data</summary>

- *Cyr Emile M'lan, Ming-Hui Chen*

- `1504.06523v1` - [abs](http://arxiv.org/abs/1504.06523v1) - [pdf](http://arxiv.org/pdf/1504.06523v1)

> This paper presents three objective Bayesian methods for analyzing bilateral data under Dallal's model and the saturated model. Three parameters are of interest, namely, the risk difference, the risk ratio, and the odds ratio. We derive Jeffreys' prior and Bernardo's reference prior associated with the three parameters that characterize Dallal's model. We derive the functional forms of the posterior distributions of the risk difference and the risk ratio and discuss how to sample from their posterior distributions. We demonstrate the use of the proposed methodology with two real data examples. We also investigate small, moderate, and large sample properties of the proposed methodology and the frequentist counterpart via simulations.

</details>

<details>

<summary>2015-04-24 16:08:30 - A Bayesian approach for structure learning in oscillating regulatory networks</summary>

- *D Trejo, AJ Millar, G Sanguinetti*

- `1504.06553v1` - [abs](http://arxiv.org/abs/1504.06553v1) - [pdf](http://arxiv.org/pdf/1504.06553v1)

> Oscillations lie at the core of many biological processes, from the cell cycle, to circadian oscillations and developmental processes. Time-keeping mechanisms are essential to enable organisms to adapt to varying conditions in environmental cycles, from day/night to seasonal. Transcriptional regulatory networks are one of the mechanisms behind these biological oscillations. However, while identifying cyclically expressed genes from time series measurements is relatively easy, determining the structure of the interaction network underpinning the oscillation is a far more challenging problem. Here, we explicitly leverage the oscillatory nature of the transcriptional signals and present a method for reconstructing network interactions tailored to this special but important class of genetic circuits. Our method is based on projecting the signal onto a set of oscillatory basis functions using a Discrete Fourier Transform. We build a Bayesian Hierarchical model within a frequency domain linear model in order to enforce sparsity and incorporate prior knowledge about the network structure. Experiments on real and simulated data show that the method can lead to substantial improvements over competing approaches if the oscillatory assumption is met, and remains competitive also in cases it is not.

</details>

<details>

<summary>2015-04-24 19:59:32 - Subjectivity, Bayesianism, and Causality</summary>

- *Pedro A. Ortega*

- `1407.4139v4` - [abs](http://arxiv.org/abs/1407.4139v4) - [pdf](http://arxiv.org/pdf/1407.4139v4)

> Bayesian probability theory is one of the most successful frameworks to model reasoning under uncertainty. Its defining property is the interpretation of probabilities as degrees of belief in propositions about the state of the world relative to an inquiring subject. This essay examines the notion of subjectivity by drawing parallels between Lacanian theory and Bayesian probability theory, and concludes that the latter must be enriched with causal interventions to model agency. The central contribution of this work is an abstract model of the subject that accommodates causal interventions in a measure-theoretic formalisation. This formalisation is obtained through a game-theoretic Ansatz based on modelling the inside and outside of the subject as an extensive-form game with imperfect information between two players. Finally, I illustrate the expressiveness of this model with an example of causal induction.

</details>

<details>

<summary>2015-04-25 08:35:30 - A Prior Distribution over Directed Acyclic Graphs for Sparse Bayesian Networks</summary>

- *Felix L. Rios, John M. Noble, Timo J. T. Koski*

- `1504.06701v1` - [abs](http://arxiv.org/abs/1504.06701v1) - [pdf](http://arxiv.org/pdf/1504.06701v1)

> The main contribution of this article is a new prior distribution over directed acyclic graphs, which gives larger weight to sparse graphs. This distribution is intended for structured Bayesian networks, where the structure is given by an ordered block model. That is, the nodes of the graph are objects which fall into categories (or blocks); the blocks have a natural ordering. The presence of a relationship between two objects is denoted by an arrow, from the object of lower category to the object of higher category. The models considered here were introduced in Kemp et al. (2004) for relational data and extended to multivariate data in Mansinghka et al. (2006). The prior over graph structures presented here has an explicit formula. The number of nodes in each layer of the graph follow a Hoppe Ewens urn model.   We consider the situation where the nodes of the graph represent random variables, whose joint probability distribution factorises along the DAG. We describe Monte Carlo schemes for finding the optimal aposteriori structure given a data matrix and compare the performance with Mansinghka et al. (2006) and also with the uniform prior.

</details>

<details>

<summary>2015-04-26 17:23:06 - Maximum a Posteriori Estimation by Search in Probabilistic Programs</summary>

- *David Tolpin, Frank Wood*

- `1504.06848v1` - [abs](http://arxiv.org/abs/1504.06848v1) - [pdf](http://arxiv.org/pdf/1504.06848v1)

> We introduce an approximate search algorithm for fast maximum a posteriori probability estimation in probabilistic programs, which we call Bayesian ascent Monte Carlo (BaMC). Probabilistic programs represent probabilistic models with varying number of mutually dependent finite, countable, and continuous random variables. BaMC is an anytime MAP search algorithm applicable to any combination of random variables and dependencies. We compare BaMC to other MAP estimation algorithms and show that BaMC is faster and more robust on a range of probabilistic models.

</details>

<details>

<summary>2015-04-26 20:08:51 - Bayesian kernel-based system identification with quantized output data</summary>

- *Giulio Bottegal, Gianluigi Pillonetto, Håkan Hjalmarsson*

- `1504.06877v1` - [abs](http://arxiv.org/abs/1504.06877v1) - [pdf](http://arxiv.org/pdf/1504.06877v1)

> In this paper we introduce a novel method for linear system identification with quantized output data. We model the impulse response as a zero-mean Gaussian process whose covariance (kernel) is given by the recently proposed stable spline kernel, which encodes information on regularity and exponential stability. This serves as a starting point to cast our system identification problem into a Bayesian framework. We employ Markov Chain Monte Carlo (MCMC) methods to provide an estimate of the system. In particular, we show how to design a Gibbs sampler which quickly converges to the target distribution. Numerical simulations show a substantial improvement in the accuracy of the estimates over state-of-the-art kernel-based methods when employed in identification of systems with quantized data.

</details>

<details>

<summary>2015-04-27 00:54:11 - Optimal Robustness Results for Some Bayesian Procedures and the Relationship to Prior-Data Conflict</summary>

- *Luai Al-Labadi, Michael Evans*

- `1504.06898v1` - [abs](http://arxiv.org/abs/1504.06898v1) - [pdf](http://arxiv.org/pdf/1504.06898v1)

> The robustness to the prior of Bayesian inference procedures based on a measure of statistical evidence are considered. These inferences are shown to have optimal properties with respect to robustness. Furthermore, a connection between robustness and prior-data conflict is established. In particular, the inferences are shown to be effectively robust when the choice of prior does not lead to prior-data conflict. When there is prior-data conflict, however, robustness may fail to hold.

</details>

<details>

<summary>2015-04-27 12:18:13 - Comment on Article by Berger, Bernardo, and Sun</summary>

- *Siva Sivaganesan*

- `1504.07046v1` - [abs](http://arxiv.org/abs/1504.07046v1) - [pdf](http://arxiv.org/pdf/1504.07046v1)

> Discussion of Overall Objective Priors by James O. Berger, Jose M. Bernardo, Dongchu Sun [arXiv:1504.02689].

</details>

<details>

<summary>2015-04-27 13:11:46 - Comment on Article by Berger, Bernardo, and Sun</summary>

- *Judith Rousseau*

- `1504.07072v1` - [abs](http://arxiv.org/abs/1504.07072v1) - [pdf](http://arxiv.org/pdf/1504.07072v1)

> Discussion of Overall Objective Priors by James O. Berger, Jose M. Bernardo, Dongchu Sun [arXiv:1504.02689].

</details>

<details>

<summary>2015-04-27 13:25:53 - Comment on Article by Berger, Bernardo, and Sun</summary>

- *Gauri Sankar Datta, Brunero Liseo*

- `1504.07078v1` - [abs](http://arxiv.org/abs/1504.07078v1) - [pdf](http://arxiv.org/pdf/1504.07078v1)

> Discussion of Overall Objective Priors by James O. Berger, Jose M. Bernardo, Dongchu Sun [arXiv:1504.02689].

</details>

<details>

<summary>2015-04-27 13:35:18 - Rejoinder to Article by Berger, Bernardo, and Sun</summary>

- *James O. Berger, Jose M. Bernardo, Dongchu Sun*

- `1504.07081v1` - [abs](http://arxiv.org/abs/1504.07081v1) - [pdf](http://arxiv.org/pdf/1504.07081v1)

> Rejoinder to Overall Objective Priors by James O. Berger, Jose M. Bernardo, Dongchu Sun [arXiv:1504.02689]

</details>

<details>

<summary>2015-04-27 16:08:02 - Entity Resolution with Empirically Motivated Priors</summary>

- *Rebecca C. Steorts*

- `1409.0643v2` - [abs](http://arxiv.org/abs/1409.0643v2) - [pdf](http://arxiv.org/pdf/1409.0643v2)

> Databases often contain corrupted, degraded, and noisy data with duplicate entries across and within each database. Such problems arise in citations, medical databases, genetics, human rights databases, and a variety of other applied settings. The target of statistical inference can be viewed as an unsupervised problem of determining the edges of a bipartite graph that links the observed records to unobserved latent entities. Bayesian approaches provide attractive benefits, naturally providing uncertainty quantification via posterior probabilities. We propose a novel record linkage approach based on empirical Bayesian principles. Specifically, the empirical Bayesian--type step consists of taking the empirical distribution function of the data as the prior for the latent entities. This approach improves on the earlier HB approach not only by avoiding the prior specification problem but also by allowing both categorical and string-valued variables. Our extension to string-valued variables also involves the proposal of a new probabilistic mechanism by which observed record values for string fields can deviate from the values of their associated latent entities. Categorical fields that deviate from their corresponding true value are simply drawn from the empirical distribution function. We apply our proposed methodology to a simulated data set of German names and an Italian household survey, showing our method performs favorably compared to several standard methods in the literature. We also consider the robustness of our methods to changes in the hyper-parameters.

</details>

<details>

<summary>2015-04-28 08:18:55 - Comment on Article by Berger, Bernardo, and Sun</summary>

- *Manuel Mendoza, Eduardo Gutiérrez-Peña*

- `1504.07320v1` - [abs](http://arxiv.org/abs/1504.07320v1) - [pdf](http://arxiv.org/pdf/1504.07320v1)

> Discussion of Overall Objective Priors by James O. Berger, Jose M. Bernardo, Dongchu Sun [arXiv:1504.02689].

</details>

<details>

<summary>2015-04-28 20:21:31 - Approximate Bayesian Solution for Estimating Population Size from Dual-record System</summary>

- *Kiranmoy Chatterjee, Diganta Mukherjee*

- `1311.3812v3` - [abs](http://arxiv.org/abs/1311.3812v3) - [pdf](http://arxiv.org/pdf/1311.3812v3)

> For Dual-record system, in the context of human population, the popular Chandrasekar-Deming model incorporates only the time variation effect on capture probabilities. How-ever, in practice population may undergo behavioral change after being captured first time. In this paper we focus on the Dual-record system model (equivalent to capture- recapture model with two sampling occasions) with both the time as well as behavioral response variation. The relevant model suffers from identifiability problem. Two approaches are proposed from which approximate Bayes estimates can be obtained using very simple Gibbs sampling strategies. We explore the features of our two proposed methods and their usages depending on the availability (or non-availability) of the information on the nature of behavioral response effect. Extensive simulation studies are carried out to evaluate their performances and compare with few available approaches. Finally, a real data application is provided to the model and the methods.

</details>

<details>

<summary>2015-04-29 19:21:10 - Adaptive Bayesian credible sets in regression with a Gaussian process prior</summary>

- *Suzanne Sniekers, Aad van der Vaart*

- `1504.07972v1` - [abs](http://arxiv.org/abs/1504.07972v1) - [pdf](http://arxiv.org/pdf/1504.07972v1)

> We investigate two empirical Bayes methods and a hierarchical Bayes method for adapting the scale of a Gaussian process prior in a nonparametric regression model. We show that all methods lead to a posterior contraction rate that adapts to the smoothness of the true regression function. Furthermore, we show that the corresponding credible sets cover the true regression function whenever this function satisfies a certain extrapolation condition. This condition depends on the specific method, but is implied by a condition of self-similarity. The latter condition is shown to be satisfied with probability one under the prior distribution.

</details>

<details>

<summary>2015-04-29 23:36:41 - Thresholded Multiscale Gaussian Processes with Application to Bayesian Feature Selection for Massive Neuroimaging Data</summary>

- *Ran Shi, Jian Kang*

- `1504.06074v2` - [abs](http://arxiv.org/abs/1504.06074v2) - [pdf](http://arxiv.org/pdf/1504.06074v2)

> Motivated by the needs of selecting important features for massive neuroimaging data, we propose a spatially varying coefficient model (SVCMs) with sparsity and piecewise smoothness imposed on the coefficient functions. A new class of nonparametric priors is developed based on thresholded multiresolution Gaussian processes (TMGP). We show that the TMGP has a large support on a space of sparse and piecewise smooth functions, leading to posterior consistency in coefficient function estimation and feature selection. Also, we develop a method for prior specifications of thresholding parameters in TMGPs and discuss their theoretical properties. Efficient posterior computation algorithms are developed by adopting a kernel convolution approach, where a modified square exponential kernel is chosen taking the advantage that the analytical form of the eigen decomposition is available. Based on simulation studies, we demonstrate that our methods can achieve better performance in estimating the spatially varying coefficient. Also, the proposed model has been applied to an analysis of resting state functional magnetic resonance imaging (Rs-fMRI) data from the Autism Brain Imaging Data Exchange (ABIDE) study, it provides biologically meaningful results.

</details>

<details>

<summary>2015-04-30 16:46:50 - Bayesian variable selection for latent class analysis using a collapsed Gibbs sampler</summary>

- *Arthur White, Jason Wyse, Thomas Brendan Murphy*

- `1402.6928v2` - [abs](http://arxiv.org/abs/1402.6928v2) - [pdf](http://arxiv.org/pdf/1402.6928v2)

> Latent class analysis is used to perform model based clustering for multivariate categorical responses. Selection of the variables most relevant for clustering is an important task which can affect the quality of clustering considerably. This work considers a Bayesian approach for selecting the number of clusters and the best clustering variables. The main idea is to reformulate the problem of group and variable selection as a probabilistically driven search over a large discrete space using Markov chain Monte Carlo (MCMC) methods. This approach results in estimates of degree of relevance of each variable for clustering along with posterior probability for the number of clusters. Bayes factors can then be easily calculated, and a suitable model chosen in a principled manner. Both selection tasks are carried out simultaneously using an MCMC approach based on a collapsed Gibbs sampling method, whereby several model parameters are integrated from the model, substantially improving computational performance. Approaches for estimating posterior marginal probabilities of class membership, variable inclusion and number of groups are proposed, and post-hoc procedures for parameter and uncertainty estimation are outlined. The approach is tested on simulated and real data.

</details>

<details>

<summary>2015-04-30 18:15:12 - On the Relationship between Sum-Product Networks and Bayesian Networks</summary>

- *Han Zhao, Mazen Melibari, Pascal Poupart*

- `1501.01239v2` - [abs](http://arxiv.org/abs/1501.01239v2) - [pdf](http://arxiv.org/pdf/1501.01239v2)

> In this paper, we establish some theoretical connections between Sum-Product Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be converted into a BN in linear time and space in terms of the network size. The key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent the local conditional probability distributions at each node in the resulting BN by exploiting context-specific independence (CSI). The generated BN has a simple directed bipartite graphical structure. We show that by applying the Variable Elimination algorithm (VE) to the generated BN with ADD representations, we can recover the original SPN where the SPN can be viewed as a history record or caching of the VE inference process. To help state the proof clearly, we introduce the notion of {\em normal} SPN and present a theoretical analysis of the consistency and decomposability properties. We conclude the paper with some discussion of the implications of the proof and establish a connection between the depth of an SPN and a lower bound of the tree-width of its corresponding BN.

</details>

<details>

<summary>2015-04-30 19:17:38 - Hidden population size estimation from respondent-driven sampling: a network approach</summary>

- *Forrest W. Crawford, Jiacheng Wu, Robert Heimer*

- `1504.08349v1` - [abs](http://arxiv.org/abs/1504.08349v1) - [pdf](http://arxiv.org/pdf/1504.08349v1)

> Estimating the size of stigmatized, hidden, or hard-to-reach populations is a major problem in epidemiology, demography, and public health research. Capture-recapture and multiplier methods have become standard tools for inference of hidden population sizes, but they require independent random sampling of target population members, which is rarely possible. Respondent-driven sampling (RDS) is a survey method for hidden populations that relies on social link tracing. The RDS recruitment process is designed to spread through the social network connecting members of the target population. In this paper, we show how to use network data revealed by RDS to estimate hidden population size. The key insight is that the recruitment chain, timing of recruitments, and network degrees of recruited subjects provide information about the number of individuals belonging to the target population who are not yet in the sample. We use a computationally efficient Bayesian method to integrate over the missing edges in the subgraph of recruited individuals. We validate the method using simulated data and apply the technique to estimate the number of people who inject drugs in St. Petersburg, Russia.

</details>


## 2015-05

<details>

<summary>2015-05-02 01:41:19 - Bayesian nonparametric tests via sliced inverse modeling</summary>

- *Bo Jiang, Chao Ye, Jun S. Liu*

- `1411.3070v4` - [abs](http://arxiv.org/abs/1411.3070v4) - [pdf](http://arxiv.org/pdf/1411.3070v4)

> We study the problem of independence and conditional independence tests between categorical covariates and a continuous response variable, which has an immediate application in genetics. Instead of estimating the conditional distribution of the response given values of covariates, we model the conditional distribution of covariates given the discretized response (aka "slices"). By assigning a prior probability to each possible discretization scheme, we can compute efficiently a Bayes factor (BF)-statistic for the independence (or conditional independence) test using a dynamic programming algorithm. Asymptotic and finite-sample properties such as power and null distribution of the BF statistic are studied, and a stepwise variable selection method based on the BF statistic is further developed. We compare the BF statistic with some existing classical methods and demonstrate its statistical power through extensive simulation studies. We apply the proposed method to a mouse genetics data set aiming to detect quantitative trait loci (QTLs) and obtain promising results.

</details>

<details>

<summary>2015-05-04 16:45:05 - Bayesian inference for Gaussian graphical models beyond decomposable graphs</summary>

- *Kshitij Khare, Bala Rajaratnam, Abhishek Saha*

- `1505.00703v1` - [abs](http://arxiv.org/abs/1505.00703v1) - [pdf](http://arxiv.org/pdf/1505.00703v1)

> Bayesian inference for graphical models has received much attention in the literature in recent years. It is well known that when the graph G is decomposable, Bayesian inference is significantly more tractable than in the general non-decomposable setting. Penalized likelihood inference on the other hand has made tremendous gains in the past few years in terms of scalability and tractability. Bayesian inference, however, has not had the same level of success, though a scalable Bayesian approach has its respective strengths, especially in terms of quantifying uncertainty. To address this gap, we propose a scalable and flexible novel Bayesian approach for estimation and model selection in Gaussian undirected graphical models. We first develop a class of generalized G-Wishart distributions with multiple shape parameters for an arbitrary underlying graph. This class contains the G-Wishart distribution as a special case. We then introduce the class of Generalized Bartlett (GB) graphs, and derive an efficient Gibbs sampling algorithm to obtain posterior draws from generalized G-Wishart distributions corresponding to a GB graph. The class of Generalized Bartlett graphs contains the class of decomposable graphs as a special case, but is substantially larger than the class of decomposable graphs. We proceed to derive theoretical properties of the proposed Gibbs sampler. We then demonstrate that the proposed Gibbs sampler is scalable to significantly higher dimensional problems as compared to using an accept-reject or a Metropolis-Hasting algorithm. Finally, we show the efficacy of the proposed approach on simulated and real data.

</details>

<details>

<summary>2015-05-05 20:01:08 - Achieving a Hyperlocal Housing Price Index: Overcoming Data Sparsity by Bayesian Dynamical Modeling of Multiple Data Streams</summary>

- *You Ren, Emily B. Fox, Andrew Bruce*

- `1505.01164v1` - [abs](http://arxiv.org/abs/1505.01164v1) - [pdf](http://arxiv.org/pdf/1505.01164v1)

> Understanding how housing values evolve over time is important to policy makers, consumers and real estate professionals. Existing methods for constructing housing indices are computed at a coarse spatial granularity, such as metropolitan regions, which can mask or distort price dynamics apparent in local markets, such as neighborhoods and census tracts. A challenge in moving to estimates at, for example, the census tract level is the sparsity of spatiotemporally localized house sales observations. Our work aims at addressing this challenge by leveraging observations from multiple census tracts discovered to have correlated valuation dynamics. Our proposed Bayesian nonparametric approach builds on the framework of latent factor models to enable a flexible, data-driven method for inferring the clustering of correlated census tracts. We explore methods for scalability and parallelizability of computations, yielding a housing valuation index at the level of census tract rather than zip code, and on a monthly basis rather than quarterly. Our analysis is provided on a large Seattle metropolitan housing dataset.

</details>

<details>

<summary>2015-05-06 09:50:12 - Cats & Co: Categorical Time Series Coclustering</summary>

- *Dominique Gay, Romain Guigourès, Marc Boullé, Fabrice Clérot*

- `1505.01300v1` - [abs](http://arxiv.org/abs/1505.01300v1) - [pdf](http://arxiv.org/pdf/1505.01300v1)

> We suggest a novel method of clustering and exploratory analysis of temporal event sequences data (also known as categorical time series) based on three-dimensional data grid models. A data set of temporal event sequences can be represented as a data set of three-dimensional points, each point is defined by three variables: a sequence identifier, a time value and an event value. Instantiating data grid models to the 3D-points turns the problem into 3D-coclustering.   The sequences are partitioned into clusters, the time variable is discretized into intervals and the events are partitioned into clusters. The cross-product of the univariate partitions forms a multivariate partition of the representation space, i.e., a grid of cells and it also represents a nonparametric estimator of the joint distribution of the sequences, time and events dimensions. Thus, the sequences are grouped together because they have similar joint distribution of time and events, i.e., similar distribution of events along the time dimension. The best data grid is computed using a parameter-free Bayesian model selection approach. We also suggest several criteria for exploiting the resulting grid through agglomerative hierarchies, for interpreting the clusters of sequences and characterizing their components through insightful visualizations. Extensive experiments on both synthetic and real-world data sets demonstrate that data grid models are efficient, effective and discover meaningful underlying patterns of categorical time series data.

</details>

<details>

<summary>2015-05-06 12:59:27 - The McDonald Gompertz Distribution: Properties and Applications</summary>

- *Rasool Roozegar, Saeid Tahmasebi, Ali Akbar Jafari*

- `1505.01351v1` - [abs](http://arxiv.org/abs/1505.01351v1) - [pdf](http://arxiv.org/pdf/1505.01351v1)

> This paper introduces a five-parameter lifetime model with increasing, decreasing, upside -down bathtub and bathtub shaped failure rate called as the McDonald Gompertz (McG) distribution. This new distribution extend the Gompertz, generalized Gompertz, generalized exponential, beta Gompertz and Kumaraswamy Gompertz distributions, among several other models. We obtain several properties of the McG distribution including moments, entropies, quantile and generating functions. We provide the density function of the order statistics and their moments. The parameter estimation is based on the usual maximum likelihood approach. We also provide the observed information matrix and discuss inferences issues. In the end, the flexibility and usefulness of the new distribution is illustrated by means of application to two real data sets.

</details>

<details>

<summary>2015-05-06 17:03:30 - Particle Gibbs algorithms for Markov jump processes</summary>

- *Blazej Miasojedow, Wojciech Niemiro*

- `1505.01434v1` - [abs](http://arxiv.org/abs/1505.01434v1) - [pdf](http://arxiv.org/pdf/1505.01434v1)

> In the present paper we propose a new MCMC algorithm for sampling from the posterior distribution of hidden trajectory of a Markov jump process. Our algorithm is based on the idea of exploiting virtual jumps, introduced by Rao and Teh (2013). The main novelty is that our algorithm uses particle Gibbs with ancestor sampling to update the skeleton, while Rao and Teh use forward filtering backward sampling (FFBS). In contrast to previous methods our algorithm can be implemented even if the state space is infinite. In addition, the cost of a single step of the proposed algorithm does not depend on the size of the state space. The computational cost of our methood is of order $\mathcal{O}(N\mathbb{E}(n))$, where $N$ is the number of particles used in the PGAS algorithm and $\mathbb{E}(n)$ is the expected number of jumps (together with virtual ones). The cost of the algorithm of Rao and Teh is of order $\mathcal{O}(|\mathcal{X}|^2\mathbb{E}(n))$, where $|\mathcal{X}|$ is the size of the state space. Simulation results show that our algorithm with PGAS converges slightly slower than the algorithm with FFBS, if the size of the state space is not big. However, if the size of the state space increases, the proposed method outperforms existing ones. We give special attention to a hierarchical version of our algorithm which can be applied to continuous time Bayesian networks (CTBNs).

</details>

<details>

<summary>2015-05-07 08:50:18 - Bayesian Optimization for Synthetic Gene Design</summary>

- *Javier González, Joseph Longworth, David C. James, Neil D. Lawrence*

- `1505.01627v1` - [abs](http://arxiv.org/abs/1505.01627v1) - [pdf](http://arxiv.org/pdf/1505.01627v1)

> We address the problem of synthetic gene design using Bayesian optimization. The main issue when designing a gene is that the design space is defined in terms of long strings of characters of different lengths, which renders the optimization intractable. We propose a three-step approach to deal with this issue. First, we use a Gaussian process model to emulate the behavior of the cell. As inputs of the model, we use a set of biologically meaningful gene features, which allows us to define optimal gene designs rules. Based on the model outputs we define a multi-task acquisition function to optimize simultaneously severals aspects of interest. Finally, we define an evaluation function, which allow us to rank sets of candidate gene sequences that are coherent with the optimal design strategy. We illustrate the performance of this approach in a real gene design experiment with mammalian cells.

</details>

<details>

<summary>2015-05-07 11:14:49 - Dirichlet Process Hidden Markov Multiple Change-point Model</summary>

- *Stanley I. M. Ko, Terence T. L. Chong, Pulak Ghosh*

- `1505.01665v1` - [abs](http://arxiv.org/abs/1505.01665v1) - [pdf](http://arxiv.org/pdf/1505.01665v1)

> This paper proposes a new Bayesian multiple change-point model which is based on the hidden Markov approach. The Dirichlet process hidden Markov model does not require the specification of the number of change-points a priori. Hence our model is robust to model specification in contrast to the fully parametric Bayesian model. We propose a general Markov chain Monte Carlo algorithm which only needs to sample the states around change-points. Simulations for a normal mean-shift model with known and unknown variance demonstrate advantages of our approach. Two applications, namely the coal-mining disaster data and the real United States Gross Domestic Product growth, are provided. We detect a single change-point for both the disaster data and US GDP growth. All the change-point locations and posterior inferences of the two applications are in line with existing methods.

</details>

<details>

<summary>2015-05-07 12:10:51 - Two-sample Bayesian nonparametric goodness-of-fit test</summary>

- *Luai Al Labadi, Emad Masuadi, Mahmoud Zarepour*

- `1411.3427v2` - [abs](http://arxiv.org/abs/1411.3427v2) - [pdf](http://arxiv.org/pdf/1411.3427v2)

> In recent years, Bayesian nonparametric statistics has gathered extraordinary attention. Nonetheless, a relatively little amount of work has been expended on Bayesian nonparametric hypothesis testing. In this paper, a novel Bayesian nonparametric approach to the two-sample problem is established. Precisely, given two samples $\mathbf{X}=X_1,\ldots,X_{m_1}$ $\overset {i.i.d.} \sim F$ and $\mathbf{Y}=Y_1,\ldots,Y_{m_2} \overset {i.i.d.} \sim G$, with $F$ and $G$ being unknown continuous cumulative distribution functions, we wish to test the null hypothesis $\mathcal{H}_0:~F=G$. The method is based on the Kolmogorov distance and approximate samples from the Dirichlet process centered at the standard normal distribution and a concentration parameter 1. It is demonstrated that the proposed test is robust with respect to any prior specification of the Dirichlet process. A power comparison with several well-known tests is incorporated. In particular, the proposed test dominates the standard Kolmogorov-Smirnov test in all the cases examined in the paper.

</details>

<details>

<summary>2015-05-07 12:53:40 - Scaling It Up: Stochastic Search Structure Learning in Graphical Models</summary>

- *Hao Wang*

- `1505.01687v1` - [abs](http://arxiv.org/abs/1505.01687v1) - [pdf](http://arxiv.org/pdf/1505.01687v1)

> Gaussian concentration graph models and covariance graph models are two classes of graphical models that are useful for uncovering latent dependence structures among multivariate variables. In the Bayesian literature, graphs are often determined through the use of priors over the space of positive definite matrices with fixed zeros, but these methods present daunting computational burdens in large problems. Motivated by the superior computational efficiency of continuous shrinkage priors for regression analysis, we propose a new framework for structure learning that is based on continuous spike and slab priors and uses latent variables to identify graphs. We discuss model specification, computation, and inference for both concentration and covariance graph models. The new approach produces reliable estimates of graphs and efficiently handles problems with hundreds of variables.

</details>

<details>

<summary>2015-05-07 13:09:44 - Adaptive approximate Bayesian computation for complex models</summary>

- *Maxime Lenormand, Franck Jabot, Guillaume Deffuant*

- `1111.1308v4` - [abs](http://arxiv.org/abs/1111.1308v4) - [pdf](http://arxiv.org/pdf/1111.1308v4)

> Approximate Bayesian computation (ABC) is a family of computational techniques in Bayesian statistics. These techniques allow to fi t a model to data without relying on the computation of the model likelihood. They instead require to simulate a large number of times the model to be fi tted. A number of re finements to the original rejection-based ABC scheme have been proposed, including the sequential improvement of posterior distributions. This technique allows to de- crease the number of model simulations required, but it still presents several shortcomings which are particu- larly problematic for costly to simulate complex models. We here provide a new algorithm to perform adaptive approximate Bayesian computation, which is shown to perform better on both a toy example and a complex social model.

</details>

<details>

<summary>2015-05-07 15:24:54 - Adaptive Monotone Shrinkage for Regression</summary>

- *Zhuang Ma, Dean Foster, Robert Stine*

- `1505.01743v1` - [abs](http://arxiv.org/abs/1505.01743v1) - [pdf](http://arxiv.org/pdf/1505.01743v1)

> We develop an adaptive monotone shrinkage estimator for regression models with the following characteristics: i) dense coefficients with small but important effects; ii) a priori ordering that indicates the probable predictive importance of the features. We capture both properties with an empirical Bayes estimator that shrinks coefficients monotonically with respect to their anticipated importance. This estimator can be rapidly computed using a version of Pool-Adjacent-Violators algorithm. We show that the proposed monotone shrinkage approach is competitive with the class of all Bayesian estimators that share the prior information. We further observe that the estimator also minimizes Stein's unbiased risk estimate. Along with our key result that the estimator mimics the oracle Bayes rule under an order assumption, we also prove that the estimator is robust. Even without the order assumption, our estimator mimics the best performance of a large family of estimators that includes the least squares estimator, constant-$\lambda$ ridge estimator, James-Stein estimator, etc. All the theoretical results are non-asymptotic. Simulation results and data analysis from a model for text processing are provided to support the theory.

</details>

<details>

<summary>2015-05-07 17:02:25 - Deriving the number of jobs in proximity services from the number of inhabitants in French rural municipalities</summary>

- *Maxime Lenormand, Sylvie Huet, Guillaume Deffuant*

- `1109.6760v3` - [abs](http://arxiv.org/abs/1109.6760v3) - [pdf](http://arxiv.org/pdf/1109.6760v3)

> We use a minimum requirement approach to derive the number of jobs in proximity services per inhabitant in French rural municipalities. We first classify the municipalities according to their time distance to the municipality where the inhabitants go the most frequently to get services (called MFM). For each set corresponding to a range of time distance to MFM, we perform a quantile regression estimating the minimum number of service jobs per inhabitant, that we interpret as an estimation of the number of proximity jobs per inhabitant. We observe that the minimum number of service jobs per inhabitant is smaller in small municipalities. Moreover, for municipalities of similar sizes, when the distance to the MFM increases, we find that the number of jobs of proximity services per inhabitant increases.

</details>

<details>

<summary>2015-05-08 08:04:07 - Bayesian Model Selection Based on Proper Scoring Rules</summary>

- *A. Philip Dawid, Monica Musio*

- `1409.5291v2` - [abs](http://arxiv.org/abs/1409.5291v2) - [pdf](http://arxiv.org/pdf/1409.5291v2)

> Bayesian model selection with improper priors is not well-defined because of the dependence of the marginal likelihood on the arbitrary scaling constants of the within-model prior densities. We show how this problem can be evaded by replacing marginal log-likelihood by a homogeneous proper scoring rule, which is insensitive to the scaling constants. Suitably applied, this will typically enable consistent selection of the true model.

</details>

<details>

<summary>2015-05-09 08:17:58 - Bayesian computation: a perspective on the current state, and sampling backwards and forwards</summary>

- *Peter J. Green, Krzysztof Łatuszyński, Marcelo Pereyra, Christian P. Robert*

- `1502.01148v3` - [abs](http://arxiv.org/abs/1502.01148v3) - [pdf](http://arxiv.org/pdf/1502.01148v3)

> The past decades have seen enormous improvements in computational inference based on statistical models, with continual enhancement in a wide range of computational tools, in competition. In Bayesian inference, first and foremost, MCMC techniques continue to evolve, moving from random walk proposals to Langevin drift, to Hamiltonian Monte Carlo, and so on, with both theoretical and algorithmic inputs opening wider access to practitioners. However, this impressive evolution in capacity is confronted by an even steeper increase in the complexity of the models and datasets to be addressed. The difficulties of modelling and then handling ever more complex datasets most likely call for a new type of tool for computational inference that dramatically reduce the dimension and size of the raw data while capturing its essential aspects. Approximate models and algorithms may thus be at the core of the next computational revolution.

</details>

<details>

<summary>2015-05-09 21:40:26 - Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes</summary>

- *Yves-Laurent Kom Samo, Stephen Roberts*

- `1410.6834v2` - [abs](http://arxiv.org/abs/1410.6834v2) - [pdf](http://arxiv.org/pdf/1410.6834v2)

> In this paper we propose the first non-parametric Bayesian model using Gaussian Processes to make inference on Poisson Point Processes without resorting to gridding the domain or to introducing latent thinning points. Unlike competing models that scale cubically and have a squared memory requirement in the number of data points, our model has a linear complexity and memory requirement. We propose an MCMC sampler and show that our model is faster, more accurate and generates less correlated samples than competing models on both synthetic and real-life data. Finally, we show that our model easily handles data sizes not considered thus far by alternate approaches.

</details>

<details>

<summary>2015-05-09 23:05:34 - Bayesian Numerical Homogenization</summary>

- *Houman Owhadi*

- `1406.6668v2` - [abs](http://arxiv.org/abs/1406.6668v2) - [pdf](http://arxiv.org/pdf/1406.6668v2)

> Numerical homogenization, i.e. the finite-dimensional approximation of solution spaces of PDEs with arbitrary rough coefficients, requires the identification of accurate basis elements. These basis elements are oftentimes found after a laborious process of scientific investigation and plain guesswork. Can this identification problem be facilitated? Is there a general recipe/decision framework for guiding the design of basis elements? We suggest that the answer to the above questions could be positive based on the reformulation of numerical homogenization as a Bayesian Inference problem in which a given PDE with rough coefficients (or multi-scale operator) is excited with noise (random right hand side/source term) and one tries to estimate the value of the solution at a given point based on a finite number of observations. We apply this reformulation to the identification of bases for the numerical homogenization of arbitrary integro-differential equations and show that these bases have optimal recovery properties. In particular we show how Rough Polyharmonic Splines can be re-discovered as the optimal solution of a Gaussian filtering problem.

</details>

<details>

<summary>2015-05-10 05:17:34 - Bayesian Sparse Tucker Models for Dimension Reduction and Tensor Completion</summary>

- *Qibin Zhao, Liqing Zhang, Andrzej Cichocki*

- `1505.02343v1` - [abs](http://arxiv.org/abs/1505.02343v1) - [pdf](http://arxiv.org/pdf/1505.02343v1)

> Tucker decomposition is the cornerstone of modern machine learning on tensorial data analysis, which have attracted considerable attention for multiway feature extraction, compressive sensing, and tensor completion. The most challenging problem is related to determination of model complexity (i.e., multilinear rank), especially when noise and missing data are present. In addition, existing methods cannot take into account uncertainty information of latent factors, resulting in low generalization performance. To address these issues, we present a class of probabilistic generative Tucker models for tensor decomposition and completion with structural sparsity over multilinear latent space. To exploit structural sparse modeling, we introduce two group sparsity inducing priors by hierarchial representation of Laplace and Student-t distributions, which facilitates fully posterior inference. For model learning, we derived variational Bayesian inferences over all model (hyper)parameters, and developed efficient and scalable algorithms based on multilinear operations. Our methods can automatically adapt model complexity and infer an optimal multilinear rank by the principle of maximum lower bound of model evidence. Experimental results and comparisons on synthetic, chemometrics and neuroimaging data demonstrate remarkable performance of our models for recovering ground-truth of multilinear rank and missing entries.

</details>

<details>

<summary>2015-05-11 10:36:00 - Extending Bayesian analysis of circular data to comparison of multiple groups</summary>

- *Kees Tim Mulder, Irene Klugkist*

- `1505.02556v1` - [abs](http://arxiv.org/abs/1505.02556v1) - [pdf](http://arxiv.org/pdf/1505.02556v1)

> Circular data are data measured in angles and occur in a variety of scientific disciplines. Bayesian methods promise to allow for flexible analysis of circular data. Three existing MCMC methods (Gibbs, Metropolis-Hastings, and Rejection) for a single group of circular data were extended to be used in a between-subjects design, providing a novel procedure to compare groups of circular data. Investigating the performance of the methods by simulation study, all methods were found to overestimate the concentration parameter of the posterior, while coverage was reasonable. The rejection sampler performed best. In future research, the MCMC method may be extended to include covariates, or a within-subjects design.

</details>

<details>

<summary>2015-05-11 12:47:08 - Predictions Based on the Clustering of Heterogeneous Functions via Shape and Subject-Specific Covariates</summary>

- *Garritt L. Page, Fernando A. Quintana*

- `1505.02589v1` - [abs](http://arxiv.org/abs/1505.02589v1) - [pdf](http://arxiv.org/pdf/1505.02589v1)

> We consider a study of players employed by teams who are members of the National Basketball Association where units of observation are functional curves that are realizations of production measurements taken through the course of one's career. The observed functional output displays large amounts of between player heterogeneity in the sense that some individuals produce curves that are fairly smooth while others are (much) more erratic. We argue that this variability in curve shape is a feature that can be exploited to guide decision making, learn about processes under study and improve prediction. In this paper we develop a methodology that takes advantage of this feature when clustering functional curves. Individual curves are flexibly modeled using Bayesian penalized B-splines while a hierarchical structure allows the clustering to be guided by the smoothness of individual curves. In a sense, the hierarchical structure balances the desire to fit individual curves well while still producing meaningful clusters that are used to guide prediction. We seamlessly incorporate available covariate information to guide the clustering of curves non-parametrically through the use of a product partition model prior for a random partition of individuals. Clustering based on curve smoothness and subject-specific covariate information is particularly important in carrying out the two types of predictions that are of interest, those that complete a partially observed curve from an active player, and those that predict the entire career curve for a player yet to play in the National Basketball Association.

</details>

<details>

<summary>2015-05-11 12:48:06 - Bayesian binomial mixture models for estimating abundance in ecological monitoring studies</summary>

- *Guohui Wu, Scott H. Holan, Charles H. Nilon, Christopher K. Wikle*

- `1505.02590v1` - [abs](http://arxiv.org/abs/1505.02590v1) - [pdf](http://arxiv.org/pdf/1505.02590v1)

> Investigation of species abundance has become a vital component of many ecological monitoring studies. The primary objective of these studies is to understand how specific species are distributed across the study domain, as well as quantification of the sampling efficiency for detecting these species. To achieve these goals, preselected locations are sampled during scheduled visits, in which the number of species observed at each location is recorded. This results in spatially referenced replicated count data that are often unbalanced in structure and exhibit overdispersion. Motivated by the Baltimore Ecosystem Study, we propose Bayesian hierarchical binomial mixture models, including Binomial Conway-Maxwell Poisson (Bin-CMP) mixture models, that formally account for varying levels of spatial dispersion. Our proposed models also allow for variable selection of model covariates and grouping of dispersion parameters through the implementation of reversible jump Markov chain Monte Carlo methodology. Finally, using demographic covariates from the American Community Survey, we demonstrate the effectiveness of our approach through estimation of abundance for the American Robin (Turdus migratorius) in the Baltimore Ecosystem Study.

</details>

<details>

<summary>2015-05-11 12:57:37 - Two-sample Bayesian Nonparametric Hypothesis Testing</summary>

- *Chris C. Holmes, François Caron, Jim E. Griffin, David A. Stephens*

- `0910.5060v3` - [abs](http://arxiv.org/abs/0910.5060v3) - [pdf](http://arxiv.org/pdf/0910.5060v3)

> In this article we describe Bayesian nonparametric procedures for two-sample hypothesis testing. Namely, given two sets of samples $\mathbf{y}^{\scriptscriptstyle(1)}\;$\stackrel{\scriptscriptstyle{iid}}{\s im}$\;F^{\scriptscriptstyle(1)}$ and $\mathbf{y}^{\scriptscriptstyle(2 )}\;$\stackrel{\scriptscriptstyle{iid}}{\sim}$\;F^{\scriptscriptstyle( 2)}$, with $F^{\scriptscriptstyle(1)},F^{\scriptscriptstyle(2)}$ unknown, we wish to evaluate the evidence for the null hypothesis $H_0:F^{\scriptscriptstyle(1)}\equiv F^{\scriptscriptstyle(2)}$ versus the alternative $H_1:F^{\scriptscriptstyle(1)}\neq F^{\scriptscriptstyle(2)}$. Our method is based upon a nonparametric P\'{o}lya tree prior centered either subjectively or using an empirical procedure. We show that the P\'{o}lya tree prior leads to an analytic expression for the marginal likelihood under the two hypotheses and hence an explicit measure of the probability of the null $\mathrm{Pr}(H_0|\{\mathbf {y}^{\scriptscriptstyle(1)},\mathbf{y}^{\scriptscriptstyle(2)}\}\mathbf{)}$.

</details>

<details>

<summary>2015-05-11 13:30:58 - Comment on Article by Dawid and Musio</summary>

- *Matthias Katzfuss, Anirban Bhattacharya*

- `1505.02605v1` - [abs](http://arxiv.org/abs/1505.02605v1) - [pdf](http://arxiv.org/pdf/1505.02605v1)

> Discussion of "Bayesian Model Selection Based on Proper Scoring Rules" by Dawid and Musio [arXiv:1409.5291].

</details>

<details>

<summary>2015-05-11 13:36:30 - Comment on Article by Dawid and Musio</summary>

- *Christopher M. Hans, Mario Peruggia*

- `1505.02607v1` - [abs](http://arxiv.org/abs/1505.02607v1) - [pdf](http://arxiv.org/pdf/1505.02607v1)

> Discussion of "Bayesian Model Selection Based on Proper Scoring Rules" by Dawid and Musio [arXiv:1409.5291].

</details>

<details>

<summary>2015-05-11 13:41:33 - Rejoinder to "Bayesian Model Selection Based on Proper Scoring Rules"</summary>

- *A. Philip Dawid, Monica Musio*

- `1505.02611v1` - [abs](http://arxiv.org/abs/1505.02611v1) - [pdf](http://arxiv.org/pdf/1505.02611v1)

> We are deeply appreciative of the initiative of the editor, Marina Vanucci, in commissioning a discussion of our paper, and extremely grateful to all the discussants for their insightful and thought-provoking comments. We respond to the discussions in alphabetical order [arXiv:1409.5291].

</details>

<details>

<summary>2015-05-11 20:34:15 - Modeling and Predicting Power Consumption of High Performance Computing Jobs</summary>

- *Curtis Storlie, Joe Sexton, Scott Pakin, Michael Lang, Brian Reich, William Rust*

- `1412.5247v2` - [abs](http://arxiv.org/abs/1412.5247v2) - [pdf](http://arxiv.org/pdf/1412.5247v2)

> Power is becoming an increasingly important concern for large supercomputing centers. Due to cost concerns, data centers are becoming increasingly limited in their ability to enhance their power infrastructure to support increased compute power on the machine-room floor. At Los Alamos National Laboratory it is projected that future-generation supercomputers will be power-limited rather than budget-limited. That is, it will be less costly to acquire a large number of nodes than it will be to upgrade an existing data-center and machine-room power infrastructure to run that large number of nodes at full power. In the power-limited systems of the future, machines will in principle be capable of drawing more power than they have available. Thus, power capping at the node/job level must be used to ensure the total system power draw remains below the available level. In this paper, we present a statistically grounded framework with which to predict (with uncertainty) how much power a given job will need and use these predictions to provide an optimal node-level power capping strategy. We model the power drawn by a given job (and subsequently by the entire machine) using hierarchical Bayesian modeling with hidden Markov and Dirichlet process models. We then demonstrate how this model can be used inside of a power-management scheme to minimize the affect of power capping on user jobs.

</details>

<details>

<summary>2015-05-11 22:51:02 - On Markov chain Monte Carlo methods for tall data</summary>

- *Rémi Bardenet, Arnaud Doucet, Chris Holmes*

- `1505.02827v1` - [abs](http://arxiv.org/abs/1505.02827v1) - [pdf](http://arxiv.org/pdf/1505.02827v1)

> Markov chain Monte Carlo methods are often deemed too computationally intensive to be of any practical use for big data applications, and in particular for inference on datasets containing a large number $n$ of individual data points, also known as tall datasets. In scenarios where data are assumed independent, various approaches to scale up the Metropolis-Hastings algorithm in a Bayesian inference context have been recently proposed in machine learning and computational statistics. These approaches can be grouped into two categories: divide-and-conquer approaches and, subsampling-based algorithms. The aims of this article are as follows. First, we present a comprehensive review of the existing literature, commenting on the underlying assumptions and theoretical guarantees of each method. Second, by leveraging our understanding of these limitations, we propose an original subsampling-based approach which samples from a distribution provably close to the posterior distribution of interest, yet can require less than $O(n)$ data point likelihood evaluations at each iteration for certain statistical models in favourable scenarios. Finally, we have only been able so far to propose subsampling-based methods which display good performance in scenarios where the Bernstein-von Mises approximation of the target posterior distribution is excellent. It remains an open challenge to develop such methods in scenarios where the Bernstein-von Mises approximation is poor.

</details>

<details>

<summary>2015-05-12 04:31:08 - Incorporating Type II Error Probabilities from Independence Tests into Score-Based Learning of Bayesian Network Structure</summary>

- *Eliot Brenner, David Sontag*

- `1505.02870v1` - [abs](http://arxiv.org/abs/1505.02870v1) - [pdf](http://arxiv.org/pdf/1505.02870v1)

> We give a new consistent scoring function for structure learning of Bayesian networks. In contrast to traditional approaches to score-based structure learning, such as BDeu or MDL, the complexity penalty that we propose is data-dependent and is given by the probability that a conditional independence test correctly shows that an edge cannot exist. What really distinguishes this new scoring function from earlier work is that it has the property of becoming computationally easier to maximize as the amount of data increases. We prove a polynomial sample complexity result, showing that maximizing this score is guaranteed to correctly learn a structure with no false edges and a distribution close to the generating distribution, whenever there exists a Bayesian network which is a perfect map for the data generating distribution. Although the new score can be used with any search algorithm, in our related UAI 2013 paper [BS13], we have given empirical results showing that it is particularly effective when used together with a linear programming relaxation approach to Bayesian network structure learning. The present paper contains all details of the proofs of the finite-sample complexity results in [BS13] as well as detailed explanation of the computation of the certain error probabilities called beta-values, whose precomputation and tabulation is necessary for the implementation of the algorithm in [BS13].

</details>

<details>

<summary>2015-05-12 06:54:38 - Modeling county level breast cancer survival data using a covariate-adjusted frailty proportional hazards model</summary>

- *Haiming Zhou, Timothy Hanson, Alejandro Jara, Jiajia Zhang*

- `1505.02886v1` - [abs](http://arxiv.org/abs/1505.02886v1) - [pdf](http://arxiv.org/pdf/1505.02886v1)

> Understanding the factors that explain differences in survival times is an important issue for establishing policies to improve national health systems. Motivated by breast cancer data arising from the Surveillance Epidemiology and End Results program, we propose a covariate-adjusted proportional hazards frailty model for the analysis of clustered right-censored data. Rather than incorporating exchangeable frailties in the linear predictor of commonly-used survival models, we allow the frailty distribution to flexibly change with both continuous and categorical cluster-level covariates and model them using a dependent Bayesian nonparametric model. The resulting process is flexible and easy to fit using an existing R package. The application of the model to our motivating example showed that, contrary to intuition, those diagnosed during a period of time in the 1990s in more rural and less affluent Iowan counties survived breast cancer better. Additional analyses showed the opposite trend for earlier time windows. We conjecture that this anomaly has to be due to increased hormone replacement therapy treatments prescribed to more urban and affluent subpopulations.

</details>

<details>

<summary>2015-05-13 12:11:11 - Approximate Bayesian Computation by Modelling Summary Statistics in a Quasi-likelihood Framework</summary>

- *Stefano Cabras, Maria Eugenia Castellanos Nueda, Erlis Ruli*

- `1505.03350v1` - [abs](http://arxiv.org/abs/1505.03350v1) - [pdf](http://arxiv.org/pdf/1505.03350v1)

> Approximate Bayesian Computation (ABC) is a useful class of methods for Bayesian inference when the likelihood function is computationally intractable. In practice, the basic ABC algorithm may be inefficient in the presence of discrepancy between prior and posterior. Therefore, more elaborate methods, such as ABC with the Markov chain Monte Carlo algorithm (ABC-MCMC), should be used. However, the elaboration of a proposal density for MCMC is a sensitive issue and very difficult in the ABC setting, where the likelihood is intractable. We discuss an automatic proposal distribution useful for ABC-MCMC algorithms. This proposal is inspired by the theory of quasi-likelihood (QL) functions and is obtained by modelling the distribution of the summary statistics as a function of the parameters. Essentially, given a real-valued vector of summary statistics, we reparametrize the model by means of a regression function of the statistics on parameters, obtained by sampling from the original model in a pilot-run simulation study. The QL theory is well established for a scalar parameter, and it is shown that when the conditional variance of the summary statistic is assumed constant, the QL has a closed-form normal density. This idea of constructing proposal distributions is extended to non constant variance and to real-valued parameter vectors. The method is illustrated by several examples and by an application to a real problem in population genetics.

</details>

<details>

<summary>2015-05-13 13:15:54 - Bayesian Indirect Inference Using a Parametric Auxiliary Model</summary>

- *Christopher C. Drovandi, Anthony N. Pettitt, Anthony Lee*

- `1505.03372v1` - [abs](http://arxiv.org/abs/1505.03372v1) - [pdf](http://arxiv.org/pdf/1505.03372v1)

> Indirect inference (II) is a methodology for estimating the parameters of an intractable (generative) model on the basis of an alternative parametric (auxiliary) model that is both analytically and computationally easier to deal with. Such an approach has been well explored in the classical literature but has received substantially less attention in the Bayesian paradigm. The purpose of this paper is to compare and contrast a collection of what we call parametric Bayesian indirect inference (pBII) methods. One class of pBII methods uses approximate Bayesian computation (referred to here as ABC II) where the summary statistic is formed on the basis of the auxiliary model, using ideas from II. Another approach proposed in the literature, referred to here as parametric Bayesian indirect likelihood (pBIL), uses the auxiliary likelihood as a replacement to the intractable likelihood. We show that pBIL is a fundamentally different approach to ABC II. We devise new theoretical results for pBIL to give extra insights into its behaviour and also its differences with ABC II. Furthermore, we examine in more detail the assumptions required to use each pBII method. The results, insights and comparisons developed in this paper are illustrated on simple examples and two other substantive applications. The first of the substantive examples involves performing inference for complex quantile distributions based on simulated data while the second is for estimating the parameters of a trivariate stochastic process describing the evolution of macroparasites within a host based on real data. We create a novel framework called Bayesian indirect likelihood (BIL) that encompasses pBII as well as general ABC methods so that the connections between the methods can be established.

</details>

<details>

<summary>2015-05-13 19:57:10 - Accelerated Gibbs sampling of normal distributions using matrix splittings and polynomials</summary>

- *Colin Fox, Albert Parker*

- `1505.03512v1` - [abs](http://arxiv.org/abs/1505.03512v1) - [pdf](http://arxiv.org/pdf/1505.03512v1)

> Standard Gibbs sampling applied to a multivariate normal distribution with a specified precision matrix is equivalent in fundamental ways to the Gauss-Seidel iterative solution of linear equations in the precision matrix. Specifically, the iteration operators, the conditions under which convergence occurs, and geometric convergence factors (and rates) are identical. These results hold for arbitrary matrix splittings from classical iterative methods in numerical linear algebra giving easy access to mature results in that field, including existing convergence results for antithetic-variable Gibbs sampling, REGS sampling, and generalizations. Hence, efficient deterministic stationary relaxation schemes lead to efficient generalizations of Gibbs sampling. The technique of polynomial acceleration that significantly improves the convergence rate of an iterative solver derived from a \emph{symmetric} matrix splitting may be applied to accelerate the equivalent generalized Gibbs sampler. Identicality of error polynomials guarantees convergence of the inhomogeneous Markov chain, while equality of convergence factors ensures that the optimal solver leads to the optimal sampler. Numerical examples are presented, including a Chebyshev accelerated SSOR Gibbs sampler applied to a stylized demonstration of low-level Bayesian image reconstruction in a large 3-dimensional linear inverse problem.

</details>

<details>

<summary>2015-05-14 03:37:58 - A Robust Approach for Identifying Gene-Environment Interactions for Prognosis</summary>

- *Hao Chai, Qingzhao Zhang, Yu Jiang, Guohua Wang, Sanguo Zhang, Shuangge Ma*

- `1505.03609v1` - [abs](http://arxiv.org/abs/1505.03609v1) - [pdf](http://arxiv.org/pdf/1505.03609v1)

> For many complex diseases, prognosis is of essential importance. It has been shown that, beyond the main effects of genetic (G) and environmental (E) risk factors, the gene-environment (G$\times$E) interactions also play a critical role. In practice, the prognosis outcome data can be contaminated, and most of the existing methods are not robust to data contamination. In the literature, it has been shown that even a single contaminated observation can lead to severely biased model estimation. In this study, we describe prognosis using an accelerated failure time (AFT) model. An exponential squared loss is proposed to accommodate possible data contamination. A penalization approach is adopted for regularized estimation and marker selection. The proposed method is realized using an effective coordinate descent (CD) and minorization maximization (MM) algorithm. Simulation shows that without contamination, the proposed method has performance comparable to or better than the unrobust alternative. With contamination, it outperforms the unrobust alternative and, under certain scenarios, can be superior to the robust method based on quantile regression. The proposed method is applied to the analysis of TCGA (The Cancer Genome Atlas) lung cancer data. It identifies interactions different from those using the alternatives. The identified marker have important implications and satisfactory stability.

</details>

<details>

<summary>2015-05-15 06:01:46 - Compound Poisson Processes, Latent Shrinkage Priors and Bayesian Nonconvex Penalization</summary>

- *Zhihua Zhang, Jin Li*

- `1308.6069v3` - [abs](http://arxiv.org/abs/1308.6069v3) - [pdf](http://arxiv.org/pdf/1308.6069v3)

> In this paper we discuss Bayesian nonconvex penalization for sparse learning problems. We explore a nonparametric formulation for latent shrinkage parameters using subordinators which are one-dimensional L\'{e}vy processes. We particularly study a family of continuous compound Poisson subordinators and a family of discrete compound Poisson subordinators. We exemplify four specific subordinators: Gamma, Poisson, negative binomial and squared Bessel subordinators. The Laplace exponents of the subordinators are Bernstein functions, so they can be used as sparsity-inducing nonconvex penalty functions. We exploit these subordinators in regression problems, yielding a hierarchical model with multiple regularization parameters. We devise ECME (Expectation/Conditional Maximization Either) algorithms to simultaneously estimate regression coefficients and regularization parameters. The empirical evaluation of simulated data shows that our approach is feasible and effective in high-dimensional data analysis.

</details>

<details>

<summary>2015-05-15 22:47:30 - Bayesian and empirical Bayesian forests</summary>

- *Matt Taddy, Chun-Sheng Chen, Jun Yu, Mitch Wyle*

- `1502.02312v2` - [abs](http://arxiv.org/abs/1502.02312v2) - [pdf](http://arxiv.org/pdf/1502.02312v2)

> We derive ensembles of decision trees through a nonparametric Bayesian model, allowing us to view random forests as samples from a posterior distribution. This insight provides large gains in interpretability, and motivates a class of Bayesian forest (BF) algorithms that yield small but reliable performance gains. Based on the BF framework, we are able to show that high-level tree hierarchy is stable in large samples. This leads to an empirical Bayesian forest (EBF) algorithm for building approximate BFs on massive distributed datasets and we show that EBFs outperform sub-sampling based alternatives by a large margin.

</details>

<details>

<summary>2015-05-16 04:21:08 - Bayesian inference for higher order ordinary differential equation models</summary>

- *Prithwish Bhaumik, Subhashis Ghosal*

- `1505.04242v1` - [abs](http://arxiv.org/abs/1505.04242v1) - [pdf](http://arxiv.org/pdf/1505.04242v1)

> Often the regression function appearing in fields like economics, engineering, biomedical sciences obeys a system of higher order ordinary differential equations (ODEs). The equations are usually not analytically solvable. We are interested in inferring on the unknown parameters appearing in the equations. Significant amount of work has been done on parameter estimation in first order ODE models. Bhaumik and Ghosal (2014a) considered a two-step Bayesian approach by putting a finite random series prior on the regression function using B-spline basis. The posterior distribution of the parameter vector is induced from that of the regression function. Although this approach is computationally fast, the Bayes estimator is not asymptotically efficient. Bhaumik and Ghosal (2014b) remedied this by directly considering the distance between the function in the nonparametric model and a Runge-Kutta (RK$4$) approximate solution of the ODE while inducing the posterior distribution on the parameter. They also studied the direct Bayesian method obtained from the approximate likelihood obtained by the RK4 method. In this paper we extend these ideas for the higher order ODE model and establish Bernstein-von Mises theorems for the posterior distribution of the parameter vector for each method with $n^{-1/2}$ contraction rate.

</details>

<details>

<summary>2015-05-16 20:11:52 - Sequential Bayesian inference for implicit hidden Markov models and current limitations</summary>

- *Pierre E. Jacob*

- `1505.04321v1` - [abs](http://arxiv.org/abs/1505.04321v1) - [pdf](http://arxiv.org/pdf/1505.04321v1)

> Hidden Markov models can describe time series arising in various fields of science, by treating the data as noisy measurements of an arbitrarily complex Markov process. Sequential Monte Carlo (SMC) methods have become standard tools to estimate the hidden Markov process given the observations and a fixed parameter value. We review some of the recent developments allowing the inclusion of parameter uncertainty as well as model uncertainty. The shortcomings of the currently available methodology are emphasised from an algorithmic complexity perspective. The statistical objects of interest for time series analysis are illustrated on a toy "Lotka-Volterra" model used in population ecology. Some open challenges are discussed regarding the scalability of the reviewed methodology to longer time series, higher-dimensional state spaces and more flexible models.

</details>

<details>

<summary>2015-05-16 23:09:53 - Fast Learning from Sparse Data</summary>

- *David Maxwell Chickering, David Heckerman*

- `1301.6685v2` - [abs](http://arxiv.org/abs/1301.6685v2) - [pdf](http://arxiv.org/pdf/1301.6685v2)

> We describe two techniques that significantly improve the running time of several standard machine-learning algorithms when data is sparse. The first technique is an algorithm that effeciently extracts one-way and two-way counts--either real or expected-- from discrete data. Extracting such counts is a fundamental step in learning algorithms for constructing a variety of models including decision trees, decision graphs, Bayesian networks, and naive-Bayes clustering models. The second technique is an algorithm that efficiently performs the E-step of the EM algorithm (i.e. inference) when applied to a naive-Bayes clustering model. Using real-world data sets, we demonstrate a dramatic decrease in running time for algorithms that incorporate these techniques.

</details>

<details>

<summary>2015-05-16 23:29:15 - A Bayesian Approach to Learning Bayesian Networks with Local Structure</summary>

- *David Maxwell Chickering, David Heckerman, Christopher Meek*

- `1302.1528v2` - [abs](http://arxiv.org/abs/1302.1528v2) - [pdf](http://arxiv.org/pdf/1302.1528v2)

> Recently several researchers have investigated techniques for using data to learn Bayesian networks containing compact representations for the conditional probability distributions (CPDs) stored at each node. The majority of this work has concentrated on using decision-tree representations for the CPDs. In addition, researchers typically apply non-Bayesian (or asymptotically Bayesian) scoring functions such as MDL to evaluate the goodness-of-fit of networks to the data. In this paper we investigate a Bayesian approach to learning Bayesian networks that contain the more general decision-graph representations of the CPDs. First, we describe how to evaluate the posterior probability that is, the Bayesian score of such a network, given a database of observed cases. Second, we describe various search spaces that can be used, in conjunction with a scoring function and a search procedure, to identify one or more high-scoring networks. Finally, we present an experimental evaluation of the search spaces, using a greedy algorithm and a Bayesian scoring function.

</details>

<details>

<summary>2015-05-16 23:35:58 - Asymptotic Model Selection for Directed Networks with Hidden Variables</summary>

- *Dan Geiger, David Heckerman, Christopher Meek*

- `1302.3580v2` - [abs](http://arxiv.org/abs/1302.3580v2) - [pdf](http://arxiv.org/pdf/1302.3580v2)

> We extend the Bayesian Information Criterion (BIC), an asymptotic approximation for the marginal likelihood, to Bayesian networks with hidden variables. This approximation can be used to select models given large samples of data. The standard BIC as well as our extension punishes the complexity of a model according to the dimension of its parameters. We argue that the dimension of a Bayesian network with hidden variables is the rank of the Jacobian matrix of the transformation between the parameters of the network and the parameters of the observable variables. We compute the dimensions of several networks including the naive Bayes model with a hidden root node.

</details>

<details>

<summary>2015-05-17 00:07:34 - Efficient Approximations for the Marginal Likelihood of Incomplete Data Given a Bayesian Network</summary>

- *David Maxwell Chickering, David Heckerman*

- `1302.3567v2` - [abs](http://arxiv.org/abs/1302.3567v2) - [pdf](http://arxiv.org/pdf/1302.3567v2)

> We discuss Bayesian methods for learning Bayesian networks when data sets are incomplete. In particular, we examine asymptotic approximations for the marginal likelihood of incomplete data given a Bayesian network. We consider the Laplace approximation and the less accurate but more efficient BIC/MDL approximation. We also consider approximations proposed by Draper (1993) and Cheeseman and Stutz (1995). These approximations are as efficient as BIC/MDL, but their accuracy has not been studied in any depth. We compare the accuracy of these approximations under the assumption that the Laplace approximation is the most accurate. In experiments using synthetic data generated from discrete naive-Bayes models having a hidden root node, we find that the CS measure is the most accurate.

</details>

<details>

<summary>2015-05-19 13:12:41 - Risk and Regret of Hierarchical Bayesian Learners</summary>

- *Jonathan H. Huggins, Joshua B. Tenenbaum*

- `1505.04984v1` - [abs](http://arxiv.org/abs/1505.04984v1) - [pdf](http://arxiv.org/pdf/1505.04984v1)

> Common statistical practice has shown that the full power of Bayesian methods is not realized until hierarchical priors are used, as these allow for greater "robustness" and the ability to "share statistical strength." Yet it is an ongoing challenge to provide a learning-theoretically sound formalism of such notions that: offers practical guidance concerning when and how best to utilize hierarchical models; provides insights into what makes for a good hierarchical prior; and, when the form of the prior has been chosen, can guide the choice of hyperparameter settings. We present a set of analytical tools for understanding hierarchical priors in both the online and batch learning settings. We provide regret bounds under log-loss, which show how certain hierarchical models compare, in retrospect, to the best single model in the model class. We also show how to convert a Bayesian log-loss regret bound into a Bayesian risk bound for any bounded loss, a result which may be of independent interest. Risk and regret bounds for Student's $t$ and hierarchical Gaussian priors allow us to formalize the concepts of "robustness" and "sharing statistical strength." Priors for feature selection are investigated as well. Our results suggest that the learning-theoretic benefits of using hierarchical priors can often come at little cost on practical problems.

</details>

<details>

<summary>2015-05-19 13:53:13 - Markov Chain Monte Carlo and Variational Inference: Bridging the Gap</summary>

- *Tim Salimans, Diederik P. Kingma, Max Welling*

- `1410.6460v4` - [abs](http://arxiv.org/abs/1410.6460v4) - [pdf](http://arxiv.org/pdf/1410.6460v4)

> Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.

</details>

<details>

<summary>2015-05-19 19:24:46 - Non-Gaussian Discriminative Factor Models via the Max-Margin Rank-Likelihood</summary>

- *Xin Yuan, Ricardo Henao, Ephraim L. Tsalik, Raymond J. Langley, Lawrence Carin*

- `1504.07468v3` - [abs](http://arxiv.org/abs/1504.07468v3) - [pdf](http://arxiv.org/pdf/1504.07468v3)

> We consider the problem of discriminative factor analysis for data that are in general non-Gaussian. A Bayesian model based on the ranks of the data is proposed. We first introduce a new {\em max-margin} version of the rank-likelihood. A discriminative factor model is then developed, integrating the max-margin rank-likelihood and (linear) Bayesian support vector machines, which are also built on the max-margin principle. The discriminative factor model is further extended to the {\em nonlinear} case through mixtures of local linear classifiers, via Dirichlet processes. Fully local conjugacy of the model yields efficient inference with both Markov Chain Monte Carlo and variational Bayes approaches. Extensive experiments on benchmark and real data demonstrate superior performance of the proposed model and its potential for applications in computational biology.

</details>

<details>

<summary>2015-05-19 20:02:07 - Posterior Contraction Rates of the Phylogenetic Indian Buffet Processes</summary>

- *Mengjie Chen, Chao Gao, Hongyu Zhao*

- `1307.8229v2` - [abs](http://arxiv.org/abs/1307.8229v2) - [pdf](http://arxiv.org/pdf/1307.8229v2)

> By expressing prior distributions as general stochastic processes, nonparametric Bayesian methods provide a flexible way to incorporate prior knowledge and constrain the latent structure in statistical inference. The Indian buffet process (IBP) is such an example that can be used to define a prior distribution on infinite binary features, where the exchangeability among subjects is assumed. The phylogenetic Indian buffet process (pIBP), a derivative of IBP, enables the modeling of non-exchangeability among subjects through a stochastic process on a rooted tree, which is similar to that used in phylogenetics, to describe relationships among the subjects. In this paper, we study the theoretical properties of IBP and pIBP under a binary factor model. We establish the posterior contraction rates for both IBP and pIBP and substantiate the theoretical results through simulation studies. This is the first work addressing the frequentist property of the posterior behaviors of IBP and pIBP. We also demonstrated its practical usefulness by applying pIBP prior to a real data example arising in the field of cancer genomics where the exchangeability among subjects is violated.

</details>

<details>

<summary>2015-05-19 21:09:14 - Tail fitting for truncated and non-truncated Pareto-type distributions</summary>

- *Jan Beirlant, Isabel Fraga Alves, Ivette Gomes*

- `1505.05189v1` - [abs](http://arxiv.org/abs/1505.05189v1) - [pdf](http://arxiv.org/pdf/1505.05189v1)

> Recently some papers, such as Aban, Meerschaert and Panorska (2006), Nuyts (2010) and Clark (2013), have drawn attention to possible truncation in Pareto tail modelling. Sometimes natural upper bounds exist that truncate the probability tail, such as the Maximum Possible Loss in insurance treaties. At other instances ultimately at the largest data, deviations from a Pareto tail behaviour become apparent. This matter is especially important when extrapolation outside the sample is required. Given that in practice one does not always know whether the distribution is truncated or not, we consider estimators for extreme quantiles both under truncated and non-truncated Pareto-type distributions. Hereby we make use of the estimator of the tail index for the truncated Pareto distribution first proposed in Aban {\it et al.} (2006). We also propose a truncated Pareto QQ-plot and a formal test for truncation in order to help deciding between a truncated and a non-truncated case. In this way we enlarge the possibilities of extreme value modelling using Pareto tails, offering an alternative scenario by adding a truncation point $T$ that is large with respect to the available data. In the mathematical modelling we hence let $T \to \infty$ at different speeds compared to the limiting fraction ($k/n \to 0$) of data used in the extreme value estimation. This work is motivated using practical examples from different fields of applications, simulation results, and some asymptotic results.

</details>

<details>

<summary>2015-05-19 22:04:19 - Curvature and Concentration of Hamiltonian Monte Carlo in High Dimensions</summary>

- *Susan Holmes, Simon Rubinstein-Salzedo, Christof Seiler*

- `1407.1114v4` - [abs](http://arxiv.org/abs/1407.1114v4) - [pdf](http://arxiv.org/pdf/1407.1114v4)

> In this article, we analyze Hamiltonian Monte Carlo (HMC) by placing it in the setting of Riemannian geometry using the Jacobi metric, so that each step corresponds to a geodesic on a suitable Riemannian manifold. We then combine the notion of curvature of a Markov chain due to Joulin and Ollivier with the classical sectional curvature from Riemannian geometry to derive error bounds for HMC in important cases, where we have positive curvature. These cases include several classical distributions such as multivariate Gaussians, and also distributions arising in the study of Bayesian image registration. The theoretical development suggests the sectional curvature as a new diagnostic tool for convergence for certain Markov chains.

</details>

<details>

<summary>2015-05-20 17:50:08 - Bayesian Estimation of the Kumaraswamy Inverse Weibull Distribution</summary>

- *Felipe R. S. de Gusmão, Vera L. D. Tomazella, Ricardo S. Ehlers*

- `1505.05466v1` - [abs](http://arxiv.org/abs/1505.05466v1) - [pdf](http://arxiv.org/pdf/1505.05466v1)

> The Kumaraswamy Inverse Weibull distribution has the ability to model failure rates that have unimodal shapes and are quite common in reliability and biological studies. The three-parameter Kumaraswamy Inverse Weibull distribution with decreasing and unimodal failure rate is introduced. We provide a comprehensive treatment of the mathematical properties of the Kumaraswany Inverse Weibull distribution and derive expressions for its moment generating function and the $r$-th generalized moment. Some properties of the model with some graphs of density and hazard function are discussed. We also discuss a Bayesian approach for this distribution and an application was made for a real data set.

</details>

<details>

<summary>2015-05-20 20:08:41 - Harmonic Exponential Families on Manifolds</summary>

- *Taco S. Cohen, Max Welling*

- `1505.04413v2` - [abs](http://arxiv.org/abs/1505.04413v2) - [pdf](http://arxiv.org/pdf/1505.04413v2)

> In a range of fields including the geosciences, molecular biology, robotics and computer vision, one encounters problems that involve random variables on manifolds. Currently, there is a lack of flexible probabilistic models on manifolds that are fast and easy to train. We define an extremely flexible class of exponential family distributions on manifolds such as the torus, sphere, and rotation groups, and show that for these distributions the gradient of the log-likelihood can be computed efficiently using a non-commutative generalization of the Fast Fourier Transform (FFT). We discuss applications to Bayesian camera motion estimation (where harmonic exponential families serve as conjugate priors), and modelling of the spatial distribution of earthquakes on the surface of the earth. Our experimental results show that harmonic densities yield a significantly higher likelihood than the best competing method, while being orders of magnitude faster to train.

</details>

<details>

<summary>2015-05-21 18:20:49 - Interactive Q-learning for Probabilities and Quantiles</summary>

- *Kristin A. Linn, Eric B. Laber, Leonard A. Stefanski*

- `1407.3414v2` - [abs](http://arxiv.org/abs/1407.3414v2) - [pdf](http://arxiv.org/pdf/1407.3414v2)

> A dynamic treatment regime is a sequence of decision rules in which each decision rule recommends treatment based on features of patient medical history such as past treatments and outcomes. Existing methods for estimating optimal dynamic treatment regimes from data optimize the mean of a response variable. However, the mean may not always be the most appropriate summary of performance. We derive estimators of decision rules for optimizing probabilities and quantiles computed with respect to the response distribution for two-stage, binary treatment settings. This enables estimation of dynamic treatment regimes that optimize the cumulative distribution function of the response at a prespecified point or a prespecified quantile of the response distribution such as the median. The proposed methods perform favorably in simulation experiments. We illustrate our approach with data from a sequentially randomized trial where the primary outcome is remission of depression symptoms.

</details>

<details>

<summary>2015-05-22 06:46:11 - Distributed Gaussian Processes</summary>

- *Marc Peter Deisenroth, Jun Wei Ng*

- `1502.02843v3` - [abs](http://arxiv.org/abs/1502.02843v3) - [pdf](http://arxiv.org/pdf/1502.02843v3)

> To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model for large-scale distributed GP regression. Unlike state-of-the-art sparse GP approximations, the rBCM is conceptually simple and does not rely on inducing or variational parameters. The key idea is to recursively distribute computations to independent computational units and, subsequently, recombine them to form an overall result. Efficient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint. The rBCM is independent of the computational graph and can be used on heterogeneous computing infrastructures, ranging from laptops to clusters. With sufficient computing resources our distributed GP model can handle arbitrarily large data sets.

</details>

<details>

<summary>2015-05-22 08:13:47 - Approximations and bounds for binary Markov random fields</summary>

- *Haakon Michael Austad, Håkon Tjelmeland*

- `1501.07414v2` - [abs](http://arxiv.org/abs/1501.07414v2) - [pdf](http://arxiv.org/pdf/1501.07414v2)

> Discrete Markov random fields form a natural class of models to represent images and spatial data sets. The use of such models is, however, hampered by a computationally intractable normalising constant. This makes parameter estimation and a fully Bayesian treatment of discrete Markov random fields difficult. We apply approximation theory for pseudo-Boolean functions to binary Markov random fields and construct approximations and upper and lower bounds for the associated computationally intractable normalising constant. As a by-product of this process we also get a partially ordered Markov model approximation of the binary Markov random field. We present numerical examples with both the pairwise interaction Ising model and with higher-order interaction models, showing the quality of our approximations and bounds. We also present simulation examples and one real data example demonstrating how the approximations and bounds can be applied for parameter estimation and to handle a fully-Bayesian model computationally.

</details>

<details>

<summary>2015-05-22 19:23:24 - A Robust Bayesian Dynamic Linear Model for Latin-American Economic Time Series: "The Mexico and Puerto Rico Cases"</summary>

- *Jairo Fuquene, Marta Alvarez, Luis Pericchi*

- `1303.6073v2` - [abs](http://arxiv.org/abs/1303.6073v2) - [pdf](http://arxiv.org/pdf/1303.6073v2)

> The traditional time series methodology requires at least a preliminary transformation of the data to get stationarity. On the other hand, Robust Bayesian Dynamic Models (RBDMs) do not assume a regular pattern or stability of the underlying system but can include points of statement breaks. In this paper we use RBDMs in order to account possible outliers and structural breaks in Latin-American economic time series. We work with important economic time series from Puerto Rico and Mexico. We show by using a random walk model how RBDMs can be applied for detecting historic changes in the economic inflation of Mexico. Also, we model the Consumer Price Index (CPI), the Economic Activity Index (EAI) and the total number of employments (TNE) economic time series in Puerto Rico using local linear trend and seasonal RBDMs with observational and states variances. The results illustrate how the model accounts the structural breaks for the historic recession periods in Puerto Rico.

</details>

<details>

<summary>2015-05-23 17:47:12 - Particle ancestor sampling for near-degenerate or intractable state transition models</summary>

- *Fredrik Lindsten, Pete Bunch, Sumeetpal S. Singh, Thomas B. Schön*

- `1505.06356v1` - [abs](http://arxiv.org/abs/1505.06356v1) - [pdf](http://arxiv.org/pdf/1505.06356v1)

> We consider Bayesian inference in sequential latent variable models in general, and in nonlinear state space models in particular (i.e., state smoothing). We work with sequential Monte Carlo (SMC) algorithms, which provide a powerful inference framework for addressing this problem. However, for certain challenging and common model classes the state-of-the-art algorithms still struggle. The work is motivated in particular by two such model classes: (i) models where the state transition kernel is (nearly) degenerate, i.e. (nearly) concentrated on a low-dimensional manifold, and (ii) models where point-wise evaluation of the state transition density is intractable. Both types of models arise in many applications of interest, including tracking, epidemiology, and econometrics. The difficulties with these types of models is that they essentially rule out forward-backward-based methods, which are known to be of great practical importance, not least to construct computationally efficient particle Markov chain Monte Carlo (PMCMC) algorithms. To alleviate this, we propose a "particle rejuvenation" technique to enable the use of the forward-backward strategy for (nearly) degenerate models and, by extension, for intractable models. We derive the proposed method specifically within the context of PMCMC, but we emphasise that it is applicable to any forward-backward-based Monte Carlo method.

</details>

<details>

<summary>2015-05-24 08:17:46 - Bayesian prediction of minimal repair times of a series system based on hybrid censored sample of components' lifetimes under Rayleigh distribution</summary>

- *S. M. T. K. MirMostafaee, Morteza Amini, A. Asgharzadeh*

- `1505.06417v1` - [abs](http://arxiv.org/abs/1505.06417v1) - [pdf](http://arxiv.org/pdf/1505.06417v1)

> In this paper, we develop Bayesian predictive inferential procedures for prediction of repair times of a series system, applying a minimal repair strategy, using the information contained in an independent observed hybrid censored sample of the lifetimes of the components of the system, assuming the underlying distribution of the lifetimes to be Rayleigh distribution. An illustrative real data example and a simulation study are presented for the purpose of illustration and comparison of the proposed predictors.

</details>

<details>

<summary>2015-05-25 19:44:36 - Stochastic Annealing for Variational Inference</summary>

- *San Gultekin, Aonan Zhang, John Paisley*

- `1505.06723v1` - [abs](http://arxiv.org/abs/1505.06723v1) - [pdf](http://arxiv.org/pdf/1505.06723v1)

> We empirically evaluate a stochastic annealing strategy for Bayesian posterior optimization with variational inference. Variational inference is a deterministic approach to approximate posterior inference in Bayesian models in which a typically non-convex objective function is locally optimized over the parameters of the approximating distribution. We investigate an annealing method for optimizing this objective with the aim of finding a better local optimal solution and compare with deterministic annealing methods and no annealing. We show that stochastic annealing can provide clear improvement on the GMM and HMM, while performance on LDA tends to favor deterministic annealing methods.

</details>

<details>

<summary>2015-05-26 03:03:50 - Optimal Bayesian estimation in stochastic block models</summary>

- *Debdeep Pati, Anirban Bhattacharya*

- `1505.06794v1` - [abs](http://arxiv.org/abs/1505.06794v1) - [pdf](http://arxiv.org/pdf/1505.06794v1)

> With the advent of structured data in the form of social networks, genetic circuits and protein interaction networks, statistical analysis of networks has gained popularity over recent years. Stochastic block model constitutes a classical cluster-exhibiting random graph model for networks. There is a substantial amount of literature devoted to proposing strategies for estimating and inferring parameters of the model, both from classical and Bayesian viewpoints. Unlike the classical counterpart, there is however a dearth of theoretical results on the accuracy of estimation in the Bayesian setting. In this article, we undertake a theoretical investigation of the posterior distribution of the parameters in a stochastic block model. In particular, we show that one obtains optimal rates of posterior convergence with routinely used multinomial-Dirichlet priors on cluster indicators and uniform priors on the probabilities of the random edge indicators. En route, we develop geometric embedding techniques to exploit the lower dimensional structure of the parameter space which may be of independent interest.

</details>

<details>

<summary>2015-05-26 06:02:05 - Discrete Independent Component Analysis (DICA) with Belief Propagation</summary>

- *Francesco A. N. Palmieri, Amedeo Buonanno*

- `1505.06814v1` - [abs](http://arxiv.org/abs/1505.06814v1) - [pdf](http://arxiv.org/pdf/1505.06814v1)

> We apply belief propagation to a Bayesian bipartite graph composed of discrete independent hidden variables and discrete visible variables. The network is the Discrete counterpart of Independent Component Analysis (DICA) and it is manipulated in a factor graph form for inference and learning. A full set of simulations is reported for character images from the MNIST dataset. The results show that the factorial code implemented by the sources contributes to build a good generative model for the data that can be used in various inference modes.

</details>

<details>

<summary>2015-05-26 07:34:35 - Searching Multiregression Dynamic Models of Resting-State fMRI Networks Using Integer Programming</summary>

- *Lilia Costa, Jim Smith, Thomas Nichols, James Cussens, Eugene P. Duff, Tamar R. Makin*

- `1505.06832v1` - [abs](http://arxiv.org/abs/1505.06832v1) - [pdf](http://arxiv.org/pdf/1505.06832v1)

> A Multiregression Dynamic Model (MDM) is a class of multivariate time series that represents various dynamic causal processes in a graphical way. One of the advantages of this class is that, in contrast to many other Dynamic Bayesian Networks, the hypothesised relationships accommodate conditional conjugate inference. We demonstrate for the first time how straightforward it is to search over all possible connectivity networks with dynamically changing intensity of transmission to find the Maximum a Posteriori Probability (MAP) model within this class. This search method is made feasible by using a novel application of an Integer Programming algorithm. The efficacy of applying this particular class of dynamic models to this domain is shown and more specifically the computational efficiency of a corresponding search of 11-node Directed Acyclic Graph (DAG) model space. We proceed to show how diagnostic methods, analogous to those defined for static Bayesian Networks, can be used to suggest embellishment of the model class to extend the process of model selection. All methods are illustrated using simulated and real resting-state functional Magnetic Resonance Imaging (fMRI) data.

</details>

<details>

<summary>2015-05-26 14:44:54 - Bayesian Nonparametric Estimation of Milky Way Model Parameters Using a New Matrix-Variate Gaussian Process Based Method</summary>

- *Dalia Chakrabarty, Munmun Biswas, Sourabh Bhattacharya*

- `1304.5967v4` - [abs](http://arxiv.org/abs/1304.5967v4) - [pdf](http://arxiv.org/pdf/1304.5967v4)

> In this paper we develop an inverse Bayesian approach to find the value of the unknown model parameter vector that supports the real (or test) data, where the data comprises measurements of a matrix-variate variable. The method is illustrated via the estimation of the unknown Milky Way feature parameter vector, using available test and simulated (training) stellar velocity data matrices. The data is represented as an unknown function of the model parameters, where this high-dimensional function is modelled using a high-dimensional Gaussian Process (${\cal GP}$). The model for this function is trained using available training data and inverted by Bayesian means, to estimate the sought value of the model parameter vector at which the test data is realised. We achieve a closed-form expression for the posterior of the unknown parameter vector and the parameters of the invoked ${\cal GP}$, given test and training data. We perform model fitting by comparing the observed data with predictions made at different summaries of the posterior probability of the model parameter vector. As a supplement, we undertake a leave-one-out cross validation of our method.

</details>

<details>

<summary>2015-05-26 17:57:32 - Belief Flows of Robust Online Learning</summary>

- *Pedro A. Ortega, Koby Crammer, Daniel D. Lee*

- `1505.07067v1` - [abs](http://arxiv.org/abs/1505.07067v1) - [pdf](http://arxiv.org/pdf/1505.07067v1)

> This paper introduces a new probabilistic model for online learning which dynamically incorporates information from stochastic gradients of an arbitrary loss function. Similar to probabilistic filtering, the model maintains a Gaussian belief over the optimal weight parameters. Unlike traditional Bayesian updates, the model incorporates a small number of gradient evaluations at locations chosen using Thompson sampling, making it computationally tractable. The belief is then transformed via a linear flow field which optimally updates the belief distribution using rules derived from information theoretic principles. Several versions of the algorithm are shown using different constraints on the flow field and compared with conventional online learning algorithms. Results are given for several classification tasks including logistic regression and multilayer neural networks.

</details>

<details>

<summary>2015-05-28 11:25:55 - A trust-region method for stochastic variational inference with applications to streaming data</summary>

- *Lucas Theis, Matthew D. Hoffman*

- `1505.07649v1` - [abs](http://arxiv.org/abs/1505.07649v1) - [pdf](http://arxiv.org/pdf/1505.07649v1)

> Stochastic variational inference allows for fast posterior inference in complex Bayesian models. However, the algorithm is prone to local optima which can make the quality of the posterior approximation sensitive to the choice of hyperparameters and initialization. We address this problem by replacing the natural gradient step of stochastic varitional inference with a trust-region update. We show that this leads to generally better results and reduced sensitivity to hyperparameters. We also describe a new strategy for variational inference on streaming data and show that here our trust-region method is crucial for getting good performance.

</details>

<details>

<summary>2015-05-28 12:32:17 - Continuous-time discrete-space models for animal movement</summary>

- *Ephraim M. Hanks, Mevin B. Hooten, Mat W. Alldredge*

- `1211.1992v2` - [abs](http://arxiv.org/abs/1211.1992v2) - [pdf](http://arxiv.org/pdf/1211.1992v2)

> The processes influencing animal movement and resource selection are complex and varied. Past efforts to model behavioral changes over time used Bayesian statistical models with variable parameter space, such as reversible-jump Markov chain Monte Carlo approaches, which are computationally demanding and inaccessible to many practitioners. We present a continuous-time discrete-space (CTDS) model of animal movement that can be fit using standard generalized linear modeling (GLM) methods. This CTDS approach allows for the joint modeling of location-based as well as directional drivers of movement. Changing behavior over time is modeled using a varying-coefficient framework which maintains the computational simplicity of a GLM approach, and variable selection is accomplished using a group lasso penalty. We apply our approach to a study of two mountain lions (Puma concolor) in Colorado, USA.

</details>

<details>

<summary>2015-05-28 14:58:35 - Bayesian Trend Filtering</summary>

- *Edward A. Roualdes*

- `1505.07710v1` - [abs](http://arxiv.org/abs/1505.07710v1) - [pdf](http://arxiv.org/pdf/1505.07710v1)

> We develop a fully Bayesian hierarchical model for trend filtering, itself a new development in nonparametric, univariate regression. The framework more broadly applies to the generalized lasso, but focus is on Bayesian trend filtering. We compare two shrinkage priors, double exponential and generalized double Pareto. A simulation study, comparing Bayesian trend filtering to the original formulation and a number of other popular methods shows our method to improve estimation error while maintaining if not improving coverage probability. Two time series data sets demonstrate Bayesian trend filtering's robustness to possible violations of its assumptions.

</details>

<details>

<summary>2015-05-28 19:00:05 - Bayesian Spectral Modeling of Microscale Spatial Distributions in a Multivariate Soil Matrix</summary>

- *Maria A. Terres, Montserrat Fuentes, Dean Hesterberg, Matthew Polizzotto*

- `1505.07798v1` - [abs](http://arxiv.org/abs/1505.07798v1) - [pdf](http://arxiv.org/pdf/1505.07798v1)

> Recent technological advances have enabled researchers in a variety of fields to collect accurately geocoded data for several variables simultaneously. In many cases it may be most appropriate to jointly model these multivariate spatial processes without constraints on their conditional relationships. When data have been collected on a regular lattice, the multivariate conditionally autoregressive (MCAR) models are a common choice. However, inference from these MCAR models relies heavily on the pre-specified neighborhood structure and often assumes a separable covariance structure. Here, we present a multivariate spatial model using a spectral analysis approach that enables inference on the conditional relationships between the variables that does not rely on a pre-specified neighborhood structure, is non-separable, and is computationally efficient. Covariance and cross-covariance functions are defined in the spectral domain to obtain computational efficiency. Posterior inference on the correlation matrix allows for quantification of the conditional dependencies. The approach is illustrated for the toxic element arsenic and four other soil elements whose relative concentrations were measured on a spatial lattice. Understanding conditional relationships between arsenic and other soil elements provides insights for mitigating poisoning in southern Asia and elsewhere.

</details>

<details>

<summary>2015-05-29 05:33:22 - On the Computational Complexity of High-Dimensional Bayesian Variable Selection</summary>

- *Yun Yang, Martin J. Wainwright, Michael I. Jordan*

- `1505.07925v1` - [abs](http://arxiv.org/abs/1505.07925v1) - [pdf](http://arxiv.org/pdf/1505.07925v1)

> We study the computational complexity of Markov chain Monte Carlo (MCMC) methods for high-dimensional Bayesian linear regression under sparsity constraints. We first show that a Bayesian approach can achieve variable-selection consistency under relatively mild conditions on the design matrix. We then demonstrate that the statistical criterion of posterior concentration need not imply the computational desideratum of rapid mixing of the MCMC algorithm. By introducing a truncated sparsity prior for variable selection, we provide a set of conditions that guarantee both variable-selection consistency and rapid mixing of a particular Metropolis-Hastings algorithm. The mixing time is linear in the number of covariates up to a logarithmic factor. Our proof controls the spectral gap of the Markov chain by constructing a canonical path ensemble that is inspired by the steps taken by greedy algorithms for variable selection.

</details>

<details>

<summary>2015-05-29 22:23:18 - Sampling, feasibility, and priors in Bayesian estimation</summary>

- *Alexandre J. Chorin, Fei Lu, Robert N. Miller, Matthias Morzfeld, Xuemin Tu*

- `1506.00043v1` - [abs](http://arxiv.org/abs/1506.00043v1) - [pdf](http://arxiv.org/pdf/1506.00043v1)

> Importance sampling algorithms are discussed in detail, with an emphasis on implicit sampling, and applied to data assimilation via particle filters. Implicit sampling makes it possible to use the data to find high-probability samples at relatively low cost, making the assimilation more efficient. A new analysis of the feasibility of data assimilation is presented, showing in detail why feasibility depends on the Frobenius norm of the covariance matrix of the noise and not on the number of variables. A discussion of the convergence of particular particle filters follows. A major open problem in numerical data assimilation is the determination of appropriate priors, a progress report on recent work on this problem is given. The analysis highlights the need for a careful attention both to the data and to the physics in data assimilation problems.

</details>


## 2015-06

<details>

<summary>2015-06-01 05:55:13 - Inferring causal impact using Bayesian structural time-series models</summary>

- *Kay H. Brodersen, Fabian Gallusser, Jim Koehler, Nicolas Remy, Steven L. Scott*

- `1506.00356v1` - [abs](http://arxiv.org/abs/1506.00356v1) - [pdf](http://arxiv.org/pdf/1506.00356v1)

> An important problem in econometrics and marketing is to infer the causal impact that a designed market intervention has exerted on an outcome metric over time. This paper proposes to infer causal impact on the basis of a diffusion-regression state-space model that predicts the counterfactual market response in a synthetic control that would have occurred had no intervention taken place. In contrast to classical difference-in-differences schemes, state-space models make it possible to (i) infer the temporal evolution of attributable impact, (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment, and (iii) flexibly accommodate multiple sources of variation, including local trends, seasonality and the time-varying influence of contemporaneous covariates. Using a Markov chain Monte Carlo algorithm for posterior inference, we illustrate the statistical properties of our approach on simulated data. We then demonstrate its practical utility by estimating the causal effect of an online advertising campaign on search-related site visits. We discuss the strengths and limitations of state-space models in enabling causal attribution in those settings where a randomised experiment is unavailable. The CausalImpact R package provides an implementation of our approach.

</details>

<details>

<summary>2015-06-01 09:33:08 - A Bayesian regression tree approach to identify the effect of nanoparticles' properties on toxicity profiles</summary>

- *Cecile Low-Kam, Donatello Telesca, Zhaoxia Ji, Haiyuan Zhang, Tian Xia, Jeffrey I. Zink, Andre E. Nel*

- `1506.00403v1` - [abs](http://arxiv.org/abs/1506.00403v1) - [pdf](http://arxiv.org/pdf/1506.00403v1)

> We introduce a Bayesian multiple regression tree model to characterize relationships between physico-chemical properties of nanoparticles and their in-vitro toxicity over multiple doses and times of exposure. Unlike conventional models that rely on data summaries, our model solves the low sample size issue and avoids arbitrary loss of information by combining all measurements from a general exposure experiment across doses, times of exposure, and replicates. The proposed technique integrates Bayesian trees for modeling threshold effects and interactions, and penalized B-splines for dose- and time-response surface smoothing. The resulting posterior distribution is sampled by Markov Chain Monte Carlo. This method allows for inference on a number of quantities of potential interest to substantive nanotoxicology, such as the importance of physico-chemical properties and their marginal effect on toxicity. We illustrate the application of our method to the analysis of a library of 24 nano metal oxides.

</details>

<details>

<summary>2015-06-01 10:27:23 - Bayesian Network Constraint-Based Structure Learning Algorithms: Parallel and Optimised Implementations in the bnlearn R Package</summary>

- *Marco Scutari*

- `1406.7648v2` - [abs](http://arxiv.org/abs/1406.7648v2) - [pdf](http://arxiv.org/pdf/1406.7648v2)

> It is well known in the literature that the problem of learning the structure of Bayesian networks is very hard to tackle: its computational complexity is super-exponential in the number of nodes in the worst case and polynomial in most real-world scenarios.   Efficient implementations of score-based structure learning benefit from past and current research in optimisation theory, which can be adapted to the task by using the network score as the objective function to maximise. This is not true for approaches based on conditional independence tests, called constraint-based learning algorithms. The only optimisation in widespread use, backtracking, leverages the symmetries implied by the definitions of neighbourhood and Markov blanket.   In this paper we illustrate how backtracking is implemented in recent versions of the bnlearn R package, and how it degrades the stability of Bayesian network structure learning for little gain in terms of speed. As an alternative, we describe a software architecture and framework that can be used to parallelise constraint-based structure learning algorithms (also implemented in bnlearn) and we demonstrate its performance using four reference networks and two real-world data sets from genetics and systems biology. We show that on modern multi-core or multiprocessor hardware parallel implementations are preferable over backtracking, which was developed when single-processor machines were the norm.

</details>

<details>

<summary>2015-06-01 10:30:00 - Modeling for seasonal marked point processes: An analysis of evolving hurricane occurrences</summary>

- *Sai Xiao, Athanasios Kottas, Bruno Sansó*

- `1506.00429v1` - [abs](http://arxiv.org/abs/1506.00429v1) - [pdf](http://arxiv.org/pdf/1506.00429v1)

> Seasonal point processes refer to stochastic models for random events which are only observed in a given season. We develop nonparametric Bayesian methodology to study the dynamic evolution of a seasonal marked point process intensity. We assume the point process is a nonhomogeneous Poisson process and propose a nonparametric mixture of beta densities to model dynamically evolving temporal Poisson process intensities. Dependence structure is built through a dependent Dirichlet process prior for the seasonally-varying mixing distributions. We extend the nonparametric model to incorporate time-varying marks, resulting in flexible inference for both the seasonal point process intensity and for the conditional mark distribution. The motivating application involves the analysis of hurricane landfalls with reported damages along the U.S. Gulf and Atlantic coasts from 1900 to 2010. We focus on studying the evolution of the intensity of the process of hurricane landfall occurrences, and the respective maximum wind speed and associated damages. Our results indicate an increase in the number of hurricane landfall occurrences and a decrease in the median maximum wind speed at the peak of the season. Introducing standardized damage as a mark, such that reported damages are comparable both in time and space, we find that there is no significant rising trend in hurricane damages over time.

</details>

<details>

<summary>2015-06-01 12:22:14 - Continuous inverse regression</summary>

- *François Portier*

- `1409.0752v2` - [abs](http://arxiv.org/abs/1409.0752v2) - [pdf](http://arxiv.org/pdf/1409.0752v2)

> We provide new theoretical results in the field of inverse regression methods for dimension reduction. Our approach is based on the study of some empirical processes that lie close to a certain dimension reduction subspace, called the central subspace. The study of these processes essentially includes weak convergence results and the consistency of some general bootstrap procedures. While such properties are used to obtain new results about sliced inverse regression, they mainly allow to define a natural family of methods for dimension reduction. First the estimation methods are shown to have root $n$ rates and the bootstrap is proved to be valid. Second, we describe a family of Cram\'er-von Mises test statistics that can be used in testing structural properties of the central subspace or the significancy of some sets of predictors. We show that the quantiles of those tests could be computed by bootstrap. Most of the existing methods related to inverse regression involve a slicing of the response that is difficult to select in practice. While our approach guarantee a comprehensive estimation, the slicing is no longer needed.

</details>

<details>

<summary>2015-06-01 12:36:12 - Bayesian nonparametric cross-study validation of prediction methods</summary>

- *Lorenzo Trippa, Levi Waldron, Curtis Huttenhower, Giovanni Parmigiani*

- `1506.00474v1` - [abs](http://arxiv.org/abs/1506.00474v1) - [pdf](http://arxiv.org/pdf/1506.00474v1)

> We consider comparisons of statistical learning algorithms using multiple data sets, via leave-one-in cross-study validation: each of the algorithms is trained on one data set; the resulting model is then validated on each remaining data set. This poses two statistical challenges that need to be addressed simultaneously. The first is the assessment of study heterogeneity, with the aim of identifying a subset of studies within which algorithm comparisons can be reliably carried out. The second is the comparison of algorithms using the ensemble of data sets. We address both problems by integrating clustering and model comparison. We formulate a Bayesian model for the array of cross-study validation statistics, which defines clusters of studies with similar properties and provides the basis for meaningful algorithm comparison in the presence of study heterogeneity. We illustrate our approach through simulations involving studies with varying severity of systematic errors, and in the context of medical prognosis for patients diagnosed with cancer, using high-throughput measurements of the transcriptional activity of the tumor's genes.

</details>

<details>

<summary>2015-06-02 07:16:29 - Network tomography for integer-valued traffic</summary>

- *Martin L. Hazelton*

- `1501.02514v2` - [abs](http://arxiv.org/abs/1501.02514v2) - [pdf](http://arxiv.org/pdf/1501.02514v2)

> A classic network tomography problem is estimation of properties of the distribution of route traffic volumes based on counts taken on the network links. We consider inference for a general class of models for integer-valued traffic. Model identifiability is examined. We investigate both maximum likelihood and Bayesian methods of estimation. In practice, these must be implemented using stochastic EM and MCMC approaches. This requires a methodology for sampling latent route flows conditional on the observed link counts. We show that existing algorithms for doing so can fail entirely, because inflexibility in the choice of sampling directions can leave the sampler trapped at a vertex of the convex polytope that describes the feasible set of route flows. We prove that so long as the network's link-path incidence matrix is totally unimodular, it is always possible to select a coordinate system representation of the polytope for which sampling parallel to the axes is adequate. This motivates a modified sampler in which the representation of the polytope adapts to provide good mixing behavior. This methodology is applied to three road traffic data sets. We conclude with a discussion of the ramifications of the unimodularity requirements for the routing matrix.

</details>

<details>

<summary>2015-06-02 07:23:36 - Of copulas, quantiles, ranks and spectra: An $L_1$-approach to spectral analysis</summary>

- *Holger Dette, Marc Hallin, Tobias Kley, Stanislav Volgushev*

- `1111.7205v3` - [abs](http://arxiv.org/abs/1111.7205v3) - [pdf](http://arxiv.org/pdf/1111.7205v3)

> In this paper, we present an alternative method for the spectral analysis of a univariate, strictly stationary time series $\{Y_t\}_{t\in \mathbb {Z}}$. We define a "new" spectrum as the Fourier transform of the differences between copulas of the pairs $(Y_t,Y_{t-k})$ and the independence copula. This object is called a copula spectral density kernel and allows to separate the marginal and serial aspects of a time series. We show that this spectrum is closely related to the concept of quantile regression. Like quantile regression, which provides much more information about conditional distributions than classical location-scale regression models, copula spectral density kernels are more informative than traditional spectral densities obtained from classical autocovariances. In particular, copula spectral density kernels, in their population versions, provide (asymptotically provide, in their sample versions) a complete description of the copulas of all pairs $(Y_t,Y_{t-k})$. Moreover, they inherit the robustness properties of classical quantile regression, and do not require any distributional assumptions such as the existence of finite moments. In order to estimate the copula spectral density kernel, we introduce rank-based Laplace periodograms which are calculated as bilinear forms of weighted $L_1$-projections of the ranks of the observed time series onto a harmonic regression model. We establish the asymptotic distribution of those periodograms, and the consistency of adequately smoothed versions. The finite-sample properties of the new methodology, and its potential for applications are briefly investigated by simulations and a short empirical example.

</details>

<details>

<summary>2015-06-02 07:49:22 - Bayesian nonparametric disclosure risk estimation via mixed effects log-linear models</summary>

- *Cinzia Carota, Maurizio Filippone, Roberto Leombruni, Silvia Polettini*

- `1306.5995v3` - [abs](http://arxiv.org/abs/1306.5995v3) - [pdf](http://arxiv.org/pdf/1306.5995v3)

> Statistical agencies and other institutions collect data under the promise to protect the confidentiality of respondents. When releasing microdata samples, the risk that records can be identified must be assessed. To this aim, a widely adopted approach is to isolate categorical variables key to the identification and analyze multi-way contingency tables of such variables. Common disclosure risk measures focus on sample unique cells in these tables and adopt parametric log-linear models as the standard statistical tools for the problem. Such models often have to deal with large and extremely sparse tables that pose a number of challenges to risk estimation. This paper proposes to overcome these problems by studying nonparametric alternatives based on Dirichlet process random effects. The main finding is that the inclusion of such random effects allows us to reduce considerably the number of fixed effects required to achieve reliable risk estimates. This is studied on applications to real data, suggesting, in particular, that our mixed models with main effects only produce roughly equivalent estimates compared to the all two-way interactions models, and are effective in defusing potential shortcomings of traditional log-linear models. This paper adopts a fully Bayesian approach that accounts for all sources of uncertainty, including that about the population frequencies, and supplies unconditional (posterior) variances and credible intervals.

</details>

<details>

<summary>2015-06-02 10:54:09 - Bayesian quantile regression with approximate likelihood</summary>

- *Yang Feng, Yuguo Chen, Xuming He*

- `1506.00834v1` - [abs](http://arxiv.org/abs/1506.00834v1) - [pdf](http://arxiv.org/pdf/1506.00834v1)

> Quantile regression is often used when a comprehensive relationship between a response variable and one or more explanatory variables is desired. The traditional frequentists' approach to quantile regression has been well developed around asymptotic theories and efficient algorithms. However, not much work has been published under the Bayesian framework. One challenging problem for Bayesian quantile regression is that the full likelihood has no parametric forms. In this paper, we propose a Bayesian quantile regression method, the linearly interpolated density (LID) method, which uses a linear interpolation of the quantiles to approximate the likelihood. Unlike most of the existing methods that aim at tackling one quantile at a time, our proposed method estimates the joint posterior distribution of multiple quantiles, leading to higher global efficiency for all quantiles of interest. Markov chain Monte Carlo algorithms are developed to carry out the proposed method. We provide convergence results that justify both the algorithmic convergence and statistical approximations to an integrated-likelihood-based posterior. From the simulation results, we verify that LID has a clear advantage over other existing methods in estimating quantities that relate to two or more quantiles.

</details>

<details>

<summary>2015-06-02 15:56:17 - A Bayesian Residual Transform for Signal Processing</summary>

- *Alexander Wong, Xiao Yu Wang*

- `1410.0669v2` - [abs](http://arxiv.org/abs/1410.0669v2) - [pdf](http://arxiv.org/pdf/1410.0669v2)

> Multi-scale decomposition has been an invaluable tool for the processing of physiological signals. Much focus in multi-scale decomposition for processing such signals have been based on scale-space theory and wavelet transforms. In this study, we take a different perspective on multi-scale decomposition by investigating the feasibility of utilizing a Bayesian-based method for multi-scale signal decomposition called Bayesian Residual Transform (BRT) for the purpose of physiological signal processing. In BRT, a signal is modeled as the summation of residual signals, each characterizing information from the signal at different scales. A deep cascading framework is introduced as a realization of the BRT. Signal-to-noise ratio (SNR) analysis using electrocardiography (ECG) signals was used to illustrate the feasibility of using the BRT for suppressing noise in physiological signals. Results in this study show that it is feasible to utilize the BRT for processing physiological signals for tasks such as noise suppression.

</details>

<details>

<summary>2015-06-02 21:22:29 - Maximum a posteriori probability estimates in infinite-dimensional Bayesian inverse problems</summary>

- *Tapio Helin, Martin Burger*

- `1412.5816v3` - [abs](http://arxiv.org/abs/1412.5816v3) - [pdf](http://arxiv.org/pdf/1412.5816v3)

> A demanding challenge in Bayesian inversion is to efficiently characterize the posterior distribution. This task is problematic especially in high-dimensional non-Gaussian problems, where the structure of the posterior can be very chaotic and difficult to analyse. Current inverse problem literature often approaches the problem by considering suitable point estimators for the task. Typically the choice is made between the maximum a posteriori (MAP) or the conditional mean (CM) estimate.   The benefits of either choice are not well-understood from the perspective of infinite-dimensional theory. Most importantly, there exists no general scheme regarding how to connect the topological description of a MAP estimate to a variational problem. The results by Dashti et. al. (2013) resolve this issue for non-linear inverse problems in Gaussian framework. In this work we improve the current understanding by introducing a novel concept called the weak MAP (wMAP) estimate. We show that any MAP estimate in the sense of Dashti et. al. (2013) is a wMAP estimate and, moreover, how in general infinite-dimensional non-Gaussian problems the wMAP estimate connects to a variational formulation. Such a formulation yields many properties of the estimate that were earlier impossible to study.   In a recent work by Burger and Lucka (2014) the MAP estimator was studied in the context of Bayes cost method. Using Bregman distances, proper convex Bayes cost functions were introduced for which the MAP estimator is the Bayes estimator. Here, we generalize these results to the infinite-dimensional setting. Moreover, we discuss the implications of our results for some examples of prior models such as the Besov prior and hierarchical prior.

</details>

<details>

<summary>2015-06-03 00:17:58 - Probabilistic Network Metrics: Variational Bayesian Network Centrality</summary>

- *Harold Soh*

- `1409.4141v2` - [abs](http://arxiv.org/abs/1409.4141v2) - [pdf](http://arxiv.org/pdf/1409.4141v2)

> Network metrics form a fundamental part of the network analysis toolbox. Used to quantitatively measure different aspects of the network, these metrics can give insights into the underlying network structure and function. In this work, we connect network metrics to modern probabilistic machine learning. We focus on the centrality metric, which is used a wide variety of applications from web search to gene-analysis. First, we formulate an eigenvector-based Bayesian centrality model for determining node importance. Compared to existing methods, our probabilistic model allows for the assimilation of multiple edge weight observations, the inclusion of priors and the extraction of uncertainties. To enable tractable inference, we develop a variational lower bound (VBC) that is demonstrated to be effective on a variety of networks (two synthetic and five real-world graphs). We then bridge this model to sparse Gaussian processes. The sparse variational Bayesian centrality Gaussian process (VBC-GP) learns a mapping between node attributes to latent centrality and hence, is capable of predicting centralities from node features and can potentially represent a large number of nodes using only a limited number of inducing inputs. Experiments show that the VBC-GP learns high-quality mappings and compares favorably to a two-step baseline, i.e., a full GP trained on the node attributes and pre-computed centralities. Finally, we present two case-studies using the VBC-GP: first, to ascertain relevant features in a taxi transport network and second, to distribute a limited number of vaccines to mitigate the severity of a viral outbreak.

</details>

<details>

<summary>2015-06-03 00:45:09 - Bayesian Hierarchical Clustering with Exponential Family: Small-Variance Asymptotics and Reducibility</summary>

- *Juho Lee, Seungjin Choi*

- `1501.07430v2` - [abs](http://arxiv.org/abs/1501.07430v2) - [pdf](http://arxiv.org/pdf/1501.07430v2)

> Bayesian hierarchical clustering (BHC) is an agglomerative clustering method, where a probabilistic model is defined and its marginal likelihoods are evaluated to decide which clusters to merge. While BHC provides a few advantages over traditional distance-based agglomerative clustering algorithms, successive evaluation of marginal likelihoods and careful hyperparameter tuning are cumbersome and limit the scalability. In this paper we relax BHC into a non-probabilistic formulation, exploring small-variance asymptotics in conjugate-exponential models. We develop a novel clustering algorithm, referred to as relaxed BHC (RBHC), from the asymptotic limit of the BHC model that exhibits the scalability of distance-based agglomerative clustering algorithms as well as the flexibility of Bayesian nonparametric models. We also investigate the reducibility of the dissimilarity measure emerged from the asymptotic limit of the BHC model, allowing us to use scalable algorithms such as the nearest neighbor chain algorithm. Numerical experiments on both synthetic and real-world datasets demonstrate the validity and high performance of our method.

</details>

<details>

<summary>2015-06-03 18:56:19 - Bayesian optimization for materials design</summary>

- *Peter I. Frazier, Jialei Wang*

- `1506.01349v1` - [abs](http://arxiv.org/abs/1506.01349v1) - [pdf](http://arxiv.org/pdf/1506.01349v1)

> We introduce Bayesian optimization, a technique developed for optimizing time-consuming engineering simulations and for fitting machine learning models on large datasets. Bayesian optimization guides the choice of experiments during materials design and discovery to find good material designs in as few experiments as possible. We focus on the case when materials designs are parameterized by a low-dimensional vector. Bayesian optimization is built on a statistical technique called Gaussian process regression, which allows predicting the performance of a new design based on previously tested designs. After providing a detailed introduction to Gaussian process regression, we introduce two Bayesian optimization methods: expected improvement, for design problems with noise-free evaluations; and the knowledge-gradient method, which generalizes expected improvement and may be used in design problems with noisy evaluations. Both methods are derived using a value-of-information analysis, and enjoy one-step Bayes-optimality.

</details>

<details>

<summary>2015-06-04 16:44:09 - Model selection in high-dimensional quantile regression with seamless $L_0$ penalty</summary>

- *Gabriela Ciuperca*

- `1506.01648v1` - [abs](http://arxiv.org/abs/1506.01648v1) - [pdf](http://arxiv.org/pdf/1506.01648v1)

> In this paper we are interested in parameters estimation of linear model when number of parameters increases with sample size. Without any assumption about moments of the model error, we propose and study the seamless $L_0$ quantile estimator. For this estimator we first give the convergence rate. Afterwards, we prove that it correctly distinguishes between zero and nonzero parameters and that the estimators of the nonzero parameters are asymptotically normal. A consistent BIC criterion to select the tuning parameters is given.

</details>

<details>

<summary>2015-06-04 20:01:37 - Combining Functional Data Registration and Factor Analysis</summary>

- *Cecilia Earls, Giles Hooker*

- `1502.00587v2` - [abs](http://arxiv.org/abs/1502.00587v2) - [pdf](http://arxiv.org/pdf/1502.00587v2)

> We extend the definition of functional data registration to encompass a larger class of registered functions. In contrast to traditional registration models, we allow for registered functions that have more than one primary direction of variation. The proposed Bayesian hierarchical model simultaneously registers the observed functions and estimates the two primary factors that characterize variation in the registered functions. Each registered function is assumed to be predominantly composed of a linear combination of these two primary factors, and the function-specific weights for each observation are estimated within the registration model. We show how these estimated weights can easily be used to classify functions after registration using both simulated data and a juggling data set.

</details>

<details>

<summary>2015-06-05 05:47:19 - Nonparametric Bayesian inference for multidimensional compound Poisson processes</summary>

- *Shota Gugushvili, Frank van der Meulen, Peter Spreij*

- `1412.7739v2` - [abs](http://arxiv.org/abs/1412.7739v2) - [pdf](http://arxiv.org/pdf/1412.7739v2)

> Given a sample from a discretely observed multidimensional compound Poisson process, we study the problem of nonparametric estimation of its jump size density $r_0$ and intensity $\lambda_0$. We take a nonparametric Bayesian approach to the problem and determine posterior contraction rates in this context, which, under some assumptions, we argue to be optimal posterior contraction rates. In particular, our results imply the existence of Bayesian point estimates that converge to the true parameter pair $(r_0,\lambda_0)$ at these rates. To the best of our knowledge, construction of nonparametric density estimators for inference in the class of discretely observed multidimensional L\'{e}vy processes, and the study of their rates of convergence is a new contribution to the literature.

</details>

<details>

<summary>2015-06-05 10:30:49 - Towards Automatic Model Comparison: An Adaptive Sequential Monte Carlo Approach</summary>

- *Yan Zhou, Adam M Johansen, John A D Aston*

- `1303.3123v2` - [abs](http://arxiv.org/abs/1303.3123v2) - [pdf](http://arxiv.org/pdf/1303.3123v2)

> Model comparison for the purposes of selection, averaging and validation is a problem found throughout statistics. Within the Bayesian paradigm, these problems all require the calculation of the posterior probabilities of models within a particular class. Substantial progress has been made in recent years, but difficulties remain in the implementation of existing schemes. This paper presents adaptive sequential Monte Carlo (\smc) sampling strategies to characterise the posterior distribution of a collection of models, as well as the parameters of those models. Both a simple product estimator and a combination of \smc and a path sampling estimator are considered and existing theoretical results are extended to include the path sampling variant. A novel approach to the automatic specification of distributions within \smc algorithms is presented and shown to outperform the state of the art in this area. The performance of the proposed strategies is demonstrated via an extensive empirical study. Comparisons with state of the art algorithms show that the proposed algorithms are always competitive, and often substantially superior to alternative techniques, at equal computational cost and considerably less application-specific implementation effort.

</details>

<details>

<summary>2015-06-05 14:55:19 - BayesPy: Variational Bayesian Inference in Python</summary>

- *Jaakko Luttinen*

- `1410.0870v3` - [abs](http://arxiv.org/abs/1410.0870v3) - [pdf](http://arxiv.org/pdf/1410.0870v3)

> BayesPy is an open-source Python software package for performing variational Bayesian inference. It is based on the variational message passing framework and supports conjugate exponential family models. By removing the tedious task of implementing the variational Bayesian update equations, the user can construct models faster and in a less error-prone way. Simple syntax, flexible model construction and efficient inference make BayesPy suitable for both average and expert Bayesian users. It also supports some advanced methods such as stochastic and collapsed variational inference.

</details>

<details>

<summary>2015-06-05 22:36:15 - Local Nonstationarity for Efficient Bayesian Optimization</summary>

- *Ruben Martinez-Cantin*

- `1506.02080v1` - [abs](http://arxiv.org/abs/1506.02080v1) - [pdf](http://arxiv.org/pdf/1506.02080v1)

> Bayesian optimization has shown to be a fundamental global optimization algorithm in many applications: ranging from automatic machine learning, robotics, reinforcement learning, experimental design, simulations, etc. The most popular and effective Bayesian optimization relies on a surrogate model in the form of a Gaussian process due to its flexibility to represent a prior over function. However, many algorithms and setups relies on the stationarity assumption of the Gaussian process. In this paper, we present a novel nonstationary strategy for Bayesian optimization that is able to outperform the state of the art in Bayesian optimization both in stationary and nonstationary problems.

</details>

<details>

<summary>2015-06-08 16:19:22 - Unscented Auxiliary Particle Filter Implementation of the Cardinalized Probability Hypothesis Density Filter</summary>

- *Meysam R. Danaee*

- `1506.02570v1` - [abs](http://arxiv.org/abs/1506.02570v1) - [pdf](http://arxiv.org/pdf/1506.02570v1)

> The probability hypothesis density (PHD) filter alleviates the computational expense of the optimal Bayesian multi-target filtering by approximating the intensity function of the random finite set (RFS) of targets in time. However, as a powerful decluttering algorithm, it suffers from lack of the precise estimation of the expected number of targets. The cardinalized PHD (CPHD) recursion, as a generalization of the PHD recursion, is to remedy this flaw, which jointly propagates the intensity function and the posterior cardinality distribution. While there are a few new approaches to enhance the Sequential Monte Carlo (SMC) implementation of the PHD filter, current SMC implementation for the CPHD filter suffers from poor performance in terms of accuracy of estimate. In this paper, based on the unscented transform (UT), we propose an auxiliary implementation of the CPHD filter for highly nonlinear systems. To that end, we approximate the elementary symmetric functions both with the predicted and with the update estimate of the linear functional. We subsequently demonstrate via numerical simulations that our algorithms significantly out performs both the SMC-CPHD filter and the auxiliary particle implementation of the PHD filter in difficult situations with high clutter. We also compare our proposed algorithm with its counterparts in terms of other metrics, such as run times and sensitivity to new target appearance.

</details>

<details>

<summary>2015-06-08 21:36:22 - Population Empirical Bayes</summary>

- *Alp Kucukelbir, David M. Blei*

- `1411.0292v2` - [abs](http://arxiv.org/abs/1411.0292v2) - [pdf](http://arxiv.org/pdf/1411.0292v2)

> Bayesian predictive inference analyzes a dataset to make predictions about new observations. When a model does not match the data, predictive accuracy suffers. We develop population empirical Bayes (POP-EB), a hierarchical framework that explicitly models the empirical population distribution as part of Bayesian analysis. We introduce a new concept, the latent dataset, as a hierarchical variable and set the empirical population as its prior. This leads to a new predictive density that mitigates model mismatch. We efficiently apply this method to complex models by proposing a stochastic variational inference algorithm, called bumping variational inference (BUMP-VI). We demonstrate improved predictive accuracy over classical Bayesian inference in three models: a linear regression model of health data, a Bayesian mixture model of natural images, and a latent Dirichlet allocation topic model of scientific documents.

</details>

<details>

<summary>2015-06-09 18:56:43 - The Wreath Process: A totally generative model of geometric shape based on nested symmetries</summary>

- *Diana Borsa, Thore Graepel, Andrew Gordon*

- `1506.03041v1` - [abs](http://arxiv.org/abs/1506.03041v1) - [pdf](http://arxiv.org/pdf/1506.03041v1)

> We consider the problem of modelling noisy but highly symmetric shapes that can be viewed as hierarchies of whole-part relationships in which higher level objects are composed of transformed collections of lower level objects. To this end, we propose the stochastic wreath process, a fully generative probabilistic model of drawings. Following Leyton's "Generative Theory of Shape", we represent shapes as sequences of transformation groups composed through a wreath product.   This representation emphasizes the maximization of transfer --- the idea that the most compact and meaningful representation of a given shape is achieved by maximizing the re-use of existing building blocks or parts.   The proposed stochastic wreath process extends Leyton's theory by defining a probability distribution over geometric shapes in terms of noise processes that are aligned with the generative group structure of the shape. We propose an inference scheme for recovering the generative history of given images in terms of the wreath process using reversible jump Markov chain Monte Carlo methods and Approximate Bayesian Computation. In the context of sketching we demonstrate the feasibility and limitations of this approach on model-generated and real data.

</details>

<details>

<summary>2015-06-09 20:00:48 - Variational consensus Monte Carlo</summary>

- *Maxim Rabinovich, Elaine Angelino, Michael I. Jordan*

- `1506.03074v1` - [abs](http://arxiv.org/abs/1506.03074v1) - [pdf](http://arxiv.org/pdf/1506.03074v1)

> Practitioners of Bayesian statistics have long depended on Markov chain Monte Carlo (MCMC) to obtain samples from intractable posterior distributions. Unfortunately, MCMC algorithms are typically serial, and do not scale to the large datasets typical of modern machine learning. The recently proposed consensus Monte Carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel (Scott et al, 2013). A fixed aggregation function then combines these samples, yielding approximate posterior samples. We introduce variational consensus Monte Carlo (VCMC), a variational Bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target. The resulting objective contains an intractable entropy term; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions. We illustrate the advantages of our algorithm on three inference tasks from the literature, demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step. Our algorithm achieves a relative error reduction (measured against serial MCMC) of up to 39% compared to consensus Monte Carlo on the task of estimating 300-dimensional probit regression parameter expectations; similarly, it achieves an error reduction of 92% on the task of estimating cluster comembership probabilities in a Gaussian mixture model with 8 components in 8 dimensions. Furthermore, these gains come at moderate cost compared to the runtime of serial MCMC, achieving near-ideal speedup in some instances.

</details>

<details>

<summary>2015-06-10 04:28:22 - Parallel Markov Chain Monte Carlo for Non-Gaussian Posterior Distributions</summary>

- *Alexey Miroshnikov, Erin M. Conlon*

- `1506.03162v1` - [abs](http://arxiv.org/abs/1506.03162v1) - [pdf](http://arxiv.org/pdf/1506.03162v1)

> Recent developments in big data and analytics research have produced an abundance of large data sets that are too big to be analyzed in their entirety, due to limits on computer memory or storage capacity. To address these issues, communication-free parallel Markov chain Monte Carlo (MCMC) methods have been developed for Bayesian analysis of big data. These methods partition data into manageable subsets, perform independent Bayesian MCMC analysis on each subset, and combine the subset posterior samples to estimate the full data posterior. Current approaches to combining subset posterior samples include sample averaging, weighted averaging, and kernel smoothing techniques. Although these methods work well for Gaussian posteriors, they are not well-suited to non-Gaussian posterior distributions. Here, we develop a new direct density product method for combining subset marginal posterior samples to estimate full data marginal posterior densities. Using a commonly-implemented distance metric, we show in simulation studies of Bayesian models with non-Gaussian posteriors that our method outperforms the existing methods in approximating the full data marginal posteriors. Since our method estimates only marginal densities, there is no limitation on the number of model parameters analyzed. Our procedure is suitable for Bayesian models with unknown parameters with fixed dimension in continuous parameter spaces.

</details>

<details>

<summary>2015-06-10 09:54:51 - A Sandwich Likelihood Correction for Bayesian Quantile Regression based on the Misspecified Asymmetric Laplace Density</summary>

- *Karthik Sriram*

- `1502.06481v2` - [abs](http://arxiv.org/abs/1502.06481v2) - [pdf](http://arxiv.org/pdf/1502.06481v2)

> A sandwich likelihood correction is proposed to remedy an inferential limitation of the Bayesian quantile regression approach based on the misspecified asymmetric Laplace density, by leveraging the benefits of the approach. Supporting theoretical results and simulations are presented.

</details>

<details>

<summary>2015-06-10 19:04:07 - Asymptotic Properties of Bayes Risk of a General Class of Shrinkage Priors in Multiple Hypothesis Testing Under Sparsity</summary>

- *Prasenjit Ghosh, Xueying Tang, Malay Ghosh, Arijit Chakrabarti*

- `1310.7462v6` - [abs](http://arxiv.org/abs/1310.7462v6) - [pdf](http://arxiv.org/pdf/1310.7462v6)

> Consider the problem of simultaneous testing for the means of independent normal observations. In this paper, we study some asymptotic optimality properties of certain multiple testing rules induced by a general class of one-group shrinkage priors in a Bayesian decision theoretic framework, where the overall loss is taken as the number of misclassified hypotheses. We assume a two-groups normal mixture model for the data and consider the asymptotic framework adopted in Bogdan et al. (2011) who introduced the notion of asymptotic Bayes optimality under sparsity in the context of multiple testing. The general class of one-group priors under study is rich enough to include, among others, the families of three parameter beta, generalized double Pareto priors, and in particular the horseshoe, the normal-exponential-gamma and the Strawderman-Berger priors. We establish that within our chosen asymptotic framework, the multiple testing rules under study asymptotically attain the risk of the Bayes Oracle up to a multiplicative factor, with the constant in the risk close to the constant in the Oracle risk. This is similar to a result obtained in Datta and Ghosh (2013) for the multiple testing rule based on the horseshoe estimator introduced in Carvalho et al. (2009, 2010). We further show that under very mild assumption on the underlying sparsity parameter, the induced decision rules based on an empirical Bayes estimate of the corresponding global shrinkage parameter proposed by van der Pas et al. (2014), attain the optimal Bayes risk up to the same multiplicative factor asymptotically. We provide a unifying argument applicable for the general class of priors under study. In the process, we settle a conjecture regarding optimality property of the generalized double Pareto priors made in Datta and Ghosh (2013). Our work also shows that the result in Datta and Ghosh (2013) can be improved further.

</details>

<details>

<summary>2015-06-10 19:28:01 - Bilinear Mixed-Effects Models for Affiliation Networks</summary>

- *Yanan Jia, Catherine A. Calder, Christopher R. Browning*

- `1406.5954v2` - [abs](http://arxiv.org/abs/1406.5954v2) - [pdf](http://arxiv.org/pdf/1406.5954v2)

> An affiliation network is a particular type of two-mode social network that consists of a set of `actors' and a set of `events' where ties indicate an actor's participation in an event. Although networks describe a variety of consequential social structures, statistical methods for studying affiliation networks are less well developed than methods for studying one-mode, or actor-actor, networks. One way to analyze affiliation networks is to consider one-mode network matrices that are derived from an affiliation network, but this approach may lead to the loss of important structural features of the data. The most comprehensive approach is to study both actors and events simultaneously. In this paper, we extend the bilinear mixed-effects model, a type of latent space model developed for one-mode networks, to the affiliation network setting by considering the dependence patterns in the interactions between actors and events and describe a Markov chain Monte Carlo algorithm for Bayesian inference. We use our model to explore patterns in extracurricular activity membership of students in a racially-diverse high school in a Midwestern metropolitan area. Using techniques from spatial point pattern analysis, we show how our model can provide insight into patterns of racial segregation in the voluntary extracurricular activity participation profiles of adolescents.

</details>

<details>

<summary>2015-06-10 21:49:31 - Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts</summary>

- *Aaron Schein, John Paisley, David M. Blei, Hanna Wallach*

- `1506.03493v1` - [abs](http://arxiv.org/abs/1506.03493v1) - [pdf](http://arxiv.org/pdf/1506.03493v1)

> We present a Bayesian tensor factorization model for inferring latent group structures from dynamic pairwise interaction patterns. For decades, political scientists have collected and analyzed records of the form "country $i$ took action $a$ toward country $j$ at time $t$"---known as dyadic events---in order to form and test theories of international relations. We represent these event data as a tensor of counts and develop Bayesian Poisson tensor factorization to infer a low-dimensional, interpretable representation of their salient patterns. We demonstrate that our model's predictive performance is better than that of standard non-negative tensor factorization methods. We also provide a comparison of our variational updates to their maximum likelihood counterparts. In doing so, we identify a better way to form point estimates of the latent factors than that typically used in Bayesian Poisson matrix factorization. Finally, we showcase our model as an exploratory analysis tool for political scientists. We show that the inferred latent factor matrices capture interpretable multilateral relations that both conform to and inform our knowledge of international affairs.

</details>

<details>

<summary>2015-06-12 18:17:55 - Automatic Variational Inference in Stan</summary>

- *Alp Kucukelbir, Rajesh Ranganath, Andrew Gelman, David M. Blei*

- `1506.03431v2` - [abs](http://arxiv.org/abs/1506.03431v2) - [pdf](http://arxiv.org/pdf/1506.03431v2)

> Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult to automate. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI). The user only provides a Bayesian model and a dataset; nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We implement ADVI in Stan (code available now), a probabilistic programming framework. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.

</details>

<details>

<summary>2015-06-14 02:10:28 - Bayesian precision matrix estimation for graphical Gaussian models with edge and vertex symmetries</summary>

- *Helene Massam, Qiong Li, Xin Gao*

- `1506.04347v1` - [abs](http://arxiv.org/abs/1506.04347v1) - [pdf](http://arxiv.org/pdf/1506.04347v1)

> Graphical Gaussian models with edge and vertex symmetries were introduced by \citet{HojLaur:2008} who also gave an algorithm to compute the maximum likelihood estimate of the precision matrix for such models. In this paper, we take a Bayesian approach to the estimation of the precision matrix. We consider only those models where the symmetry constraints are imposed on the precision matrix and which thus form a natural exponential family with the precision matrix as the canonical parameter.   We first identify the Diaconis-Ylvisaker conjugate prior for these models and develop a scheme to sample from the prior and posterior distributions. We thus obtain estimates of the posterior mean of the precision matrix.   Second, in order to verify the precision of our estimate, we derive the explicit analytic expression of the expected value of the precision matrix when the graph underlying our model is a tree, a complete graph on three vertices and a decomposable graph on four vertices with various symmetries. In those cases, we compare our estimates with the exact value of the mean of the prior distribution. We also verify the accuracy of our estimates of the posterior mean on simulated data for graphs with up to thirty vertices and various symmetries.

</details>

<details>

<summary>2015-06-14 14:51:44 - Tools for predicting rainfall from lightning records: events identification and rain prediction using a Bayesian hierarchical model</summary>

- *Edmondo Di Giuseppe, Giovanna Jona Lasinio, Massimiliano Pasqui, Stanislao Esposito*

- `1506.02276v2` - [abs](http://arxiv.org/abs/1506.02276v2) - [pdf](http://arxiv.org/pdf/1506.02276v2)

> We propose a new statistical protocol for the estimation of precipitation using lightning data. We first identify rainy events using a scan statistics, then we estimate Rainfall Lighting Ratio (RLR) to convert lightning number into rain volume given the storm intensity. Then we build a hierarchical Bayesian model aiming at the prediction of 15- and 30-minutes cumulated precipitation at unobserved locations and time using information on lightning in the same area. More specifically, we build a Bayesian hierarchical model in which precipitation is modeled as function of lightning count and space time variation is handled using specific structured (random) effects. The mean component of the model relates precipitation and lightning assuming that the number of lightning recorded on a regular grid depends on the number of lightning occurring in neighboring cells. We analyze several model formulations where storms propagation speed, spatial dependence and time variation incorporates different descriptions of the phenomena at hand. The space-time variation is assumed separable. The study area is located in Central Italy, where two storms, that differ for duration and intensity, are presented.

</details>

<details>

<summary>2015-06-15 07:25:16 - The Informational Approach to Global Optimization in presence of very noisy evaluation results. Application to the optimization of renewable energy integration strategies</summary>

- *Héloïse Dutrieux, Ivana Aleksovska, Julien Bect, Emmanuel Vazquez, Delille Gauthier, Bruno François*

- `1506.04493v1` - [abs](http://arxiv.org/abs/1506.04493v1) - [pdf](http://arxiv.org/pdf/1506.04493v1)

> We consider the problem of global optimization of a function f from very noisy evaluations. We adopt a Bayesian sequential approach: evaluation points are chosen so as to reduce the uncertainty about the position of the global optimum of f, as measured by the entropy of the corresponding random variable (Informational Approach to Global Optimization, Villemonteix et al., 2009). When evaluations are very noisy, the error coming from the estimation of the entropy using conditional simulations becomes non negligible compared to its variations on the input domain. We propose a solution to this problem by choosing evaluation points as if several evaluations were going to be made at these points. The method is applied to the optimization of a strategy for the integration of renewable energies into an electrical distribution network.

</details>

<details>

<summary>2015-06-15 08:11:01 - On the properties of variational approximations of Gibbs posteriors</summary>

- *Pierre Alquier, James Ridgway, Nicolas Chopin*

- `1506.04091v2` - [abs](http://arxiv.org/abs/1506.04091v2) - [pdf](http://arxiv.org/pdf/1506.04091v2)

> The PAC-Bayesian approach is a powerful set of techniques to derive non- asymptotic risk bounds for random estimators. The corresponding optimal distribution of estimators, usually called the Gibbs posterior, is unfortunately intractable. One may sample from it using Markov chain Monte Carlo, but this is often too slow for big datasets. We consider instead variational approximations of the Gibbs posterior, which are fast to compute. We undertake a general study of the properties of such approximations. Our main finding is that such a variational approximation has often the same rate of convergence as the original PAC-Bayesian procedure it approximates. We specialise our results to several learning tasks (classification, ranking, matrix completion),discuss how to implement a variational approximation in each case, and illustrate the good properties of said approximation on real datasets.

</details>

<details>

<summary>2015-06-15 09:54:15 - On the Generalization of the C-Bound to Structured Output Ensemble Methods</summary>

- *François Laviolette, Emilie Morvant, Liva Ralaivola, Jean-Francis Roy*

- `1408.1336v2` - [abs](http://arxiv.org/abs/1408.1336v2) - [pdf](http://arxiv.org/pdf/1408.1336v2)

> This paper generalizes an important result from the PAC-Bayesian literature for binary classification to the case of ensemble methods for structured outputs. We prove a generic version of the \Cbound, an upper bound over the risk of models expressed as a weighted majority vote that is based on the first and second statistical moments of the vote's margin. This bound may advantageously $(i)$ be applied on more complex outputs such as multiclass labels and multilabel, and $(ii)$ allow to consider margin relaxations. These results open the way to develop new ensemble methods for structured output prediction with PAC-Bayesian guarantees.

</details>

<details>

<summary>2015-06-16 05:10:05 - Inferring network structure from interventional time-course experiments</summary>

- *Simon E. F. Spencer, Steven M. Hill, Sach Mukherjee*

- `1504.07882v2` - [abs](http://arxiv.org/abs/1504.07882v2) - [pdf](http://arxiv.org/pdf/1504.07882v2)

> Graphical models are widely used to study biological networks. Interventions on network nodes are an important feature of many experimental designs for the study of biological networks. In this paper we put forward a causal variant of dynamic Bayesian networks (DBNs) for the purpose of modeling time-course data with interventions. The models inherit the simplicity and computational efficiency of DBNs but allow interventional data to be integrated into network inference. We show empirical results, on both simulated and experimental data, that demonstrate the need to appropriately handle interventions when interventions form part of the design.

</details>

<details>

<summary>2015-06-16 10:19:18 - Rediscovery of Good-Turing estimators via Bayesian nonparametrics</summary>

- *Stefano Favaro, Bernardo Nipoti, Yee Whye Teh*

- `1401.0303v4` - [abs](http://arxiv.org/abs/1401.0303v4) - [pdf](http://arxiv.org/pdf/1401.0303v4)

> The problem of estimating discovery probabilities originated in the context of statistical ecology, and in recent years it has become popular due to its frequent appearance in challenging applications arising in genetics, bioinformatics, linguistics, designs of experiments, machine learning, etc. A full range of statistical approaches, parametric and nonparametric as well as frequentist and Bayesian, has been proposed for estimating discovery probabilities. In this paper we investigate the relationships between the celebrated Good-Turing approach, which is a frequentist nonparametric approach developed in the 1940s, and a Bayesian nonparametric approach recently introduced in the literature. Specifically, under the assumption of a two parameter Poisson-Dirichlet prior, we show that Bayesian nonparametric estimators of discovery probabilities are asymptotically equivalent, for a large sample size, to suitably smoothed Good-Turing estimators. As a by-product of this result, we introduce and investigate a methodology for deriving exact and asymptotic credible intervals to be associated with the Bayesian nonparametric estimators of discovery probabilities. The proposed methodology is illustrated through a comprehensive simulation study and the analysis of Expressed Sequence Tags data generated by sequencing a benchmark complementary DNA library.

</details>

<details>

<summary>2015-06-17 09:39:06 - A Bayesian Particle Filtering Method For Brain Source Localisation</summary>

- *Xi Chen, Simo Särkkä, Simon Godsill*

- `1411.5838v3` - [abs](http://arxiv.org/abs/1411.5838v3) - [pdf](http://arxiv.org/pdf/1411.5838v3)

> In this paper, we explore the multiple source localisation problem in the cerebral cortex using magnetoencephalography (MEG) data. We model neural currents as point-wise dipolar sources which dynamically evolve over time, then model dipole dynamics using a probabilistic state space model in which dipole locations are strictly constrained to lie within the cortex. Based on the proposed models, we develop a Bayesian particle filtering algorithm for localisation of both known and unknown numbers of dipoles. The algorithm consists of a region of interest (ROI) estimation step for initial dipole number estimation, a Gibbs multiple particle filter (GMPF) step for individual dipole state estimation, and a selection criterion step for selecting the final estimates. The estimated results from the ROI estimation are used to adaptively adjust particle filter's sample size to reduce the overall computational cost. The proposed models and the algorithm are tested in numerical experiments. Results are compared with existing particle filtering methods. The numerical results show that the proposed methods can achieve improved performance metrics in terms of dipole number estimation and dipole localisation.

</details>

<details>

<summary>2015-06-17 10:24:27 - Bayesian Survival Model based on Moment Characterization</summary>

- *Julyan Arbel, Antonio Lijoi, Bernardo Nipoti*

- `1506.05269v1` - [abs](http://arxiv.org/abs/1506.05269v1) - [pdf](http://arxiv.org/pdf/1506.05269v1)

> Bayesian nonparametric marginal methods are very popular since they lead to fairly easy implementation due to the formal marginalization of the infinite-dimensional parameter of the model. However, the straightforwardness of these methods also entails some limitations: they typically yield point estimates in the form of posterior expectations, but cannot be used to estimate non-linear functionals of the posterior distribution, such as median, mode or credible intervals. This is particularly relevant in survival analysis where non-linear functionals such as e.g. the median survival time, play a central role for clinicians and practitioners. The main goal of this paper is to summarize the methodology introduced in [Arbel et al., Comput. Stat. Data. An., 2015] for hazard mixture models in order to draw approximate Bayesian inference on survival functions that is not limited to the posterior mean. In addition, we propose a practical implementation of an R package called momentify designed for moment-based density approximation, and, by means of an extensive simulation study, we thoroughly compare the introduced methodology with standard marginal methods and empirical estimation.

</details>

<details>

<summary>2015-06-18 08:39:39 - Bayesian model selection in logistic regression for the detection of adverse drug reactions</summary>

- *Matthieu Marbac, Pascale Tubert-Bitter, Mohammed Sedki*

- `1505.03366v2` - [abs](http://arxiv.org/abs/1505.03366v2) - [pdf](http://arxiv.org/pdf/1505.03366v2)

> Motivation: Spontaneous adverse event reports have a high potential for detecting adverse drug reactions. However, due to their dimension, exploring such databases requires statistical methods. In this context, disproportionality measures are used. However, by projecting the data onto contingency tables, these methods become sensitive to the problem of co-prescriptions and masking effects. Recently, logistic regressions have been used with a Lasso type penalty to perform the detection of associations between drugs and adverse events. However, the choice of the penalty value is open to criticism while it strongly influences the results. Results: In this paper, we propose to use a logistic regression whose sparsity is viewed as a model selection challenge. Since the model space is huge, a Metropolis-Hastings algorithm carries out the model selection by maximizing the BIC criterion. Thus, we avoid the calibration of penalty or threshold. During our application on the French pharmacovigilance database, the proposed method is compared to well established approaches on a reference data set, and obtains better rates of positive and negative controls. However, many signals are not detected by the proposed method. So, we conclude that this method should be used in parallel to existing measures in pharmacovigilance.

</details>

<details>

<summary>2015-06-18 09:23:37 - Collaborative Deep Learning for Recommender Systems</summary>

- *Hao Wang, Naiyan Wang, Dit-Yan Yeung*

- `1409.2944v2` - [abs](http://arxiv.org/abs/1409.2944v2) - [pdf](http://arxiv.org/pdf/1409.2944v2)

> Collaborative filtering (CF) is a successful approach commonly used by many recommender systems. Conventional CF-based methods use the ratings given to items by users as the sole source of information for learning to make recommendation. However, the ratings are often very sparse in many applications, causing CF-based methods to degrade significantly in their recommendation performance. To address this sparsity problem, auxiliary information such as item content information may be utilized. Collaborative topic regression (CTR) is an appealing recent method taking this approach which tightly couples the two components that learn from two different sources of information. Nevertheless, the latent representation learned by CTR may not be very effective when the auxiliary information is very sparse. To address this problem, we generalize recent advances in deep learning from i.i.d. input to non-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative filtering for the ratings (feedback) matrix. Extensive experiments on three real-world datasets from different domains show that CDL can significantly advance the state of the art.

</details>

<details>

<summary>2015-06-18 14:24:19 - A hybrid algorithm for Bayesian network structure learning with application to multi-label learning</summary>

- *Maxime Gasse, Alex Aussem, Haytham Elghazel*

- `1506.05692v1` - [abs](http://arxiv.org/abs/1506.05692v1) - [pdf](http://arxiv.org/pdf/1506.05692v1)

> We present a novel hybrid algorithm for Bayesian network structure learning, called H2PC. It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. The algorithm is based on divide-and-conquer constraint-based subroutines to learn the local structure around a target variable. We conduct two series of experimental comparisons of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the most powerful state-of-the-art algorithm for Bayesian network structure learning. First, we use eight well-known Bayesian network benchmarks with various data sizes to assess the quality of the learned structure returned by the algorithms. Our extensive experiments show that H2PC outperforms MMHC in terms of goodness of fit to new data and quality of the network structure with respect to the true dependence structure of the data. Second, we investigate H2PC's ability to solve the multi-label learning problem. We provide theoretical results to characterize and identify graphically the so-called minimal label powersets that appear as irreducible factors in the joint distribution under the faithfulness condition. The multi-label learning problem is then decomposed into a series of multi-class classification problems, where each multi-class variable encodes a label powerset. H2PC is shown to compare favorably to MMHC in terms of global classification accuracy over ten multi-label data sets covering different application domains. Overall, our experiments support the conclusions that local structural learning with H2PC in the form of local neighborhood induction is a theoretically well-motivated and empirically effective learning framework that is well suited to multi-label learning. The source code (in R) of H2PC as well as all data sets used for the empirical tests are publicly available.

</details>

<details>

<summary>2015-06-18 15:13:20 - Bayesian Covariance Modelling of Large Tensor-Variate Data Sets $\&$ Inverse Non-parametric Learning of the Unknown Model Parameter Vector</summary>

- *Kangrui Wang, Dalia Chakrabarty*

- `1506.05709v1` - [abs](http://arxiv.org/abs/1506.05709v1) - [pdf](http://arxiv.org/pdf/1506.05709v1)

> Tensor-valued data are being encountered increasingly more commonly, in the biological, natural as well as the social sciences. The learning of the unknown model parameter vector given such data, involves covariance modelling of such data, though this can be difficult owing to the high-dimensional nature of the data, where the numerical challenge of such modelling can only be compounded by the largeness of the available data set. Assuming such data to be modelled using a correspondingly high-dimensional Gaussian Process (${\cal   GP}$), the joint density of a finite set of such data sets is then a tensor normal distribution, with density parametrised by a mean tensor $\boldsymbol{M}$ (that is of the same dimensionality as the $k$-tensor valued observable), and the $k$ covariance matrices $\boldsymbol{\Sigma}_1,...,\boldsymbol{\Sigma}_k$. When aiming to model the covariance structure of the data, we need to estimate/learn $\{\boldsymbol{\Sigma}_1,...,\boldsymbol{\Sigma}_k \}$ and $\boldsymbol{M}$, given tha data. We present a new method in which we perform such covariance modelling by first expressing the probability density of the available data sets as tensor-normal. We then invoke appropriate priors on these unknown parameters and express the posterior of the unknowns given the data. We sample from this posterior using an appropriate variant of Metropolis Hastings. Since the classical MCMC is time and resource intensive in high-dimensional state spaces, we use an efficient variant of the Metropolis-Hastings algorithm--Transformation based MCMC--employed to perform efficient sampling from a high-dimensional state space. Once we perform the covariance modelling of such a data set, we will learn the unknown model parameter vector at which a measured (or test) data set has been obtained, given the already modelled data (training data), augmented by the test data.

</details>

<details>

<summary>2015-06-18 16:30:55 - Accelerated dimension-independent adaptive Metropolis</summary>

- *Yuxin Chen, David Keyes, Kody J. H. Law, Hatem Ltaief*

- `1506.05741v1` - [abs](http://arxiv.org/abs/1506.05741v1) - [pdf](http://arxiv.org/pdf/1506.05741v1)

> This work considers black-box Bayesian inference over high-dimensional parameter spaces. The well-known adaptive Metropolis (AM) algorithm of (Haario etal. 2001) is extended herein to scale asymptotically uniformly with respect to the underlying parameter dimension for Gaussian targets, by respecting the variance of the target. The resulting algorithm, referred to as the dimension-independent adaptive Metropolis (DIAM) algorithm, also shows improved performance with respect to adaptive Metropolis on non-Gaussian targets. This algorithm is further improved, and the possibility of probing high-dimensional targets is enabled, via GPU-accelerated numerical libraries and periodically synchronized concurrent chains (justified a posteriori). Asymptotically in dimension, this GPU implementation exhibits a factor of four improvement versus a competitive CPU-based Intel MKL parallel version alone. Strong scaling to concurrent chains is exhibited, through a combination of longer time per sample batch (weak scaling) and yet fewer necessary samples to convergence. The algorithm performance is illustrated on several Gaussian and non-Gaussian target examples, in which the dimension may be in excess of one thousand.

</details>

<details>

<summary>2015-06-18 17:45:53 - Bayesian Inference for the Multivariate Extended-Skew Normal Distribution</summary>

- *Mathieu Gerber, Florian Pelgrin*

- `1506.05757v1` - [abs](http://arxiv.org/abs/1506.05757v1) - [pdf](http://arxiv.org/pdf/1506.05757v1)

> The multivariate extended skew-normal distribution allows for accommodating raw data which are skewed and heavy tailed, and has at least three appealing statistical properties, namely closure under conditioning, affine transformations, and marginalization. In this paper we propose a Bayesian computational approach based on a sequential Monte Carlo (SMC) sampler to estimate such distributions. The practical implementation of each step of the algorithm is discussed and the elicitation of prior distributions takes into consideration some unusual behaviour of the likelihood function and the corresponding Fisher information matrix. Using Monte Carlo simulations, we provide strong evidence regarding the performances of the SMC sampler as well as some new insights regarding the parametrizations of the extended skew-normal distribution. A generalization to the extended skew-normal sample selection model is also presented. Finally we proceed with the analysis of two real datasets.

</details>

<details>

<summary>2015-06-18 18:19:08 - Multi-Context Models for Reasoning under Partial Knowledge: Generative Process and Inference Grammar</summary>

- *Ardavan Salehi Nobandegani, Ioannis N. Psaromiligkos*

- `1412.4271v2` - [abs](http://arxiv.org/abs/1412.4271v2) - [pdf](http://arxiv.org/pdf/1412.4271v2)

> Arriving at the complete probabilistic knowledge of a domain, i.e., learning how all variables interact, is indeed a demanding task. In reality, settings often arise for which an individual merely possesses partial knowledge of the domain, and yet, is expected to give adequate answers to a variety of posed queries. That is, although precise answers to some queries, in principle, cannot be achieved, a range of plausible answers is attainable for each query given the available partial knowledge. In this paper, we propose the Multi-Context Model (MCM), a new graphical model to represent the state of partial knowledge as to a domain. MCM is a middle ground between Probabilistic Logic, Bayesian Logic, and Probabilistic Graphical Models. For this model we discuss: (i) the dynamics of constructing a contradiction-free MCM, i.e., to form partial beliefs regarding a domain in a gradual and probabilistically consistent way, and (ii) how to perform inference, i.e., to evaluate a probability of interest involving some variables of the domain.

</details>

<details>

<summary>2015-06-18 19:22:42 - A tree augmented naive Bayesian network experiment for breast cancer prediction</summary>

- *Ping Ren*

- `1506.05776v1` - [abs](http://arxiv.org/abs/1506.05776v1) - [pdf](http://arxiv.org/pdf/1506.05776v1)

> In order to investigate the breast cancer prediction problem on the aging population with the grades of DCIS, we conduct a tree augmented naive Bayesian network experiment trained and tested on a large clinical dataset including consecutive diagnostic mammography examinations, consequent biopsy outcomes and related cancer registry records in the population of women across all ages. The aggregated results of our ten-fold cross validation method recommend a biopsy threshold higher than 2% for the aging population.

</details>

<details>

<summary>2015-06-18 19:29:32 - Simultaneous likelihood-based bootstrap confidence sets for a large number of models</summary>

- *Mayya Zhilova*

- `1506.05779v1` - [abs](http://arxiv.org/abs/1506.05779v1) - [pdf](http://arxiv.org/pdf/1506.05779v1)

> The paper studies a problem of constructing simultaneous likelihood-based confidence sets. We consider a simultaneous multiplier bootstrap procedure for estimating the quantiles of the joint distribution of the likelihood ratio statistics, and for adjusting the confidence level for multiplicity. Theoretical results state the bootstrap validity in the following setting: the sample size \(n\) is fixed, the maximal parameter dimension \(p_{\textrm{max}}\) and the number of considered parametric models \(K\) are s.t. \((\log K)^{12}p_{\max}^{3}/n\) is small. We also consider the situation when the parametric models are misspecified. If the models' misspecification is significant, then the bootstrap critical values exceed the true ones and the simultaneous bootstrap confidence set becomes conservative. Numerical experiments for local constant and local quadratic regressions illustrate the theoretical results.

</details>

<details>

<summary>2015-06-18 22:45:16 - Dependent Multinomial Models Made Easy: Stick Breaking with the Pólya-Gamma Augmentation</summary>

- *Scott W. Linderman, Matthew J. Johnson, Ryan P. Adams*

- `1506.05843v1` - [abs](http://arxiv.org/abs/1506.05843v1) - [pdf](http://arxiv.org/pdf/1506.05843v1)

> Many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions. For example, nucleotides in a DNA sequence, children's names in a given state and year, and text documents are all commonly modeled with multinomial distributions. In all of these cases, we expect some form of dependency between the draws: the nucleotide at one position in the DNA strand may depend on the preceding nucleotides, children's names are highly correlated from year to year, and topics in text may be correlated and dynamic. These dependencies are not naturally captured by the typical Dirichlet-multinomial formulation. Here, we leverage a logistic stick-breaking representation and recent innovations in P\'olya-gamma augmentation to reformulate the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods, enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead.

</details>

<details>

<summary>2015-06-19 09:44:53 - Sampling constrained probability distributions using Spherical Augmentation</summary>

- *Shiwei Lan, Babak Shahbaba*

- `1506.05936v1` - [abs](http://arxiv.org/abs/1506.05936v1) - [pdf](http://arxiv.org/pdf/1506.05936v1)

> Statistical models with constrained probability distributions are abundant in machine learning. Some examples include regression models with norm constraints (e.g., Lasso), probit, many copula models, and latent Dirichlet allocation (LDA). Bayesian inference involving probability distributions confined to constrained domains could be quite challenging for commonly used sampling algorithms. In this paper, we propose a novel augmentation technique that handles a wide range of constraints by mapping the constrained domain to a sphere in the augmented space. By moving freely on the surface of this sphere, sampling algorithms handle constraints implicitly and generate proposals that remain within boundaries when mapped back to the original space. Our proposed method, called {Spherical Augmentation}, provides a mathematically natural and computationally efficient framework for sampling from constrained probability distributions. We show the advantages of our method over state-of-the-art sampling algorithms, such as exact Hamiltonian Monte Carlo, using several examples including truncated Gaussian distributions, Bayesian Lasso, Bayesian bridge regression, reconstruction of quantized stationary Gaussian process, and LDA for topic modeling.

</details>

<details>

<summary>2015-06-19 15:03:22 - Tensor Analysis and Fusion of Multimodal Brain Images</summary>

- *Esin Karahan, Pedro A. Rojas-Lopez, Maria L. Bringas-Vega, Pedro A. Valdes-Hernandez, Pedro A. Valdes-Sosa*

- `1506.06040v1` - [abs](http://arxiv.org/abs/1506.06040v1) - [pdf](http://arxiv.org/pdf/1506.06040v1)

> Current high-throughput data acquisition technologies probe dynamical systems with different imaging modalities, generating massive data sets at different spatial and temporal resolutions posing challenging problems in multimodal data fusion. A case in point is the attempt to parse out the brain structures and networks that underpin human cognitive processes by analysis of different neuroimaging modalities (functional MRI, EEG, NIRS etc.). We emphasize that the multimodal, multi-scale nature of neuroimaging data is well reflected by a multi-way (tensor) structure where the underlying processes can be summarized by a relatively small number of components or "atoms". We introduce Markov-Penrose diagrams - an integration of Bayesian DAG and tensor network notation in order to analyze these models. These diagrams not only clarify matrix and tensor EEG and fMRI time/frequency analysis and inverse problems, but also help understand multimodal fusion via Multiway Partial Least Squares and Coupled Matrix-Tensor Factorization. We show here, for the first time, that Granger causal analysis of brain networks is a tensor regression problem, thus allowing the atomic decomposition of brain networks. Analysis of EEG and fMRI recordings shows the potential of the methods and suggests their use in other scientific domains.

</details>

<details>

<summary>2015-06-19 18:00:40 - Approximate Inference with the Variational Holder Bound</summary>

- *Guillaume Bouchard, Balaji Lakshminarayanan*

- `1506.06100v1` - [abs](http://arxiv.org/abs/1506.06100v1) - [pdf](http://arxiv.org/pdf/1506.06100v1)

> We introduce the Variational Holder (VH) bound as an alternative to Variational Bayes (VB) for approximate Bayesian inference. Unlike VB which typically involves maximization of a non-convex lower bound with respect to the variational parameters, the VH bound involves minimization of a convex upper bound to the intractable integral with respect to the variational parameters. Minimization of the VH bound is a convex optimization problem; hence the VH method can be applied using off-the-shelf convex optimization algorithms and the approximation error of the VH bound can also be analyzed using tools from convex optimization literature. We present experiments on the task of integrating a truncated multivariate Gaussian distribution and compare our method to VB, EP and a state-of-the-art numerical integration method for this problem.

</details>

<details>

<summary>2015-06-19 18:02:48 - Robust Bayesian inference via coarsening</summary>

- *Jeffrey W. Miller, David B. Dunson*

- `1506.06101v1` - [abs](http://arxiv.org/abs/1506.06101v1) - [pdf](http://arxiv.org/pdf/1506.06101v1)

> The standard approach to Bayesian inference is based on the assumption that the distribution of the data belongs to the chosen model class. However, even a small violation of this assumption can have a large impact on the outcome of a Bayesian procedure. We introduce a simple, coherent approach to Bayesian inference that improves robustness to perturbations from the model: rather than condition on the data exactly, one conditions on a neighborhood of the empirical distribution. When using neighborhoods based on relative entropy estimates, the resulting "coarsened" posterior can be approximated by simply tempering the likelihood---that is, by raising it to a fractional power---thus, inference is often easily implemented with standard methods, and one can even obtain analytical solutions when using conjugate priors. Some theoretical properties are derived, and we illustrate the approach with real and simulated data, using mixture models, autoregressive models of unknown order, and variable selection in linear regression.

</details>

<details>

<summary>2015-06-20 05:50:33 - Bayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists</summary>

- *Tanner Sorensen, Shravan Vasishth*

- `1506.06201v1` - [abs](http://arxiv.org/abs/1506.06201v1) - [pdf](http://arxiv.org/pdf/1506.06201v1)

> With the arrival of the R packages nlme and lme4, linear mixed models (LMMs) have come to be widely used in experimentally-driven areas like psychology, linguistics, and cognitive science. This tutorial provides a practical introduction to fitting LMMs in a Bayesian framework using the probabilistic programming language Stan. We choose Stan (rather than WinBUGS or JAGS) because it provides an elegant and scalable framework for fitting models in most of the standard applications of LMMs. We ease the reader into fitting increasingly complex LMMs, first using a two-condition repeated measures self-paced reading study, followed by a more complex $2\times 2$ repeated measures factorial design that can be generalized to much more complex designs.

</details>

<details>

<summary>2015-06-20 09:57:33 - Visualizing Probabilistic Proof</summary>

- *Enrique Guerra-Pujol*

- `1507.05057v1` - [abs](http://arxiv.org/abs/1507.05057v1) - [pdf](http://arxiv.org/pdf/1507.05057v1)

> The author revisits the Blue Bus Problem, a famous thought-experiment in law involving probabilistic proof, and presents simple Bayesian solutions to different versions of the blue bus hypothetical. In addition, the author expresses his solutions in standard and visual formats, i.e. in terms of probabilities and natural frequencies.

</details>

<details>

<summary>2015-06-22 14:37:07 - Scalable Approximations of Marginal Posteriors in Variable Selection</summary>

- *Willem van den Boom, Galen Reeves, David B. Dunson*

- `1506.06629v1` - [abs](http://arxiv.org/abs/1506.06629v1) - [pdf](http://arxiv.org/pdf/1506.06629v1)

> In many contexts, there is interest in selecting the most important variables from a very large collection, commonly referred to as support recovery or variable, feature or subset selection. There is an enormous literature proposing a rich variety of algorithms. In scientific applications, it is of crucial importance to quantify uncertainty in variable selection, providing measures of statistical significance for each variable. The overwhelming majority of algorithms fail to produce such measures. This has led to a focus in the scientific literature on independent screening methods, which examine each variable in isolation, obtaining p-values measuring the significance of marginal associations. Bayesian methods provide an alternative, with marginal inclusion probabilities used in place of p-values. Bayesian variable selection has advantages, but is impractical computationally beyond small problems. In this article, we show that approximate message passing (AMP) and Bayesian compressed regression (BCR) can be used to rapidly obtain accurate approximations to marginal inclusion probabilities in high-dimensional variable selection. Theoretical support is provided, simulation studies are conducted to assess performance, and the method is applied to a study relating brain networks to creative reasoning.

</details>

<details>

<summary>2015-06-24 02:05:50 - Fast Marginal Likelihood Estimation of the Ridge Parameter(s) in Ridge Regression and Generalized Ridge Regression for Big Data</summary>

- *George Karabatsos*

- `1409.2437v5` - [abs](http://arxiv.org/abs/1409.2437v5) - [pdf](http://arxiv.org/pdf/1409.2437v5)

> Unlike the ordinary least-squares (OLS) estimator for the linear model, a ridge regression linear model provides coefficient estimates via shrinkage, usually with improved mean-square and prediction error. This is true especially when the observed design matrix is ill-conditioned or singular, either as a result of highly-correlated covariates or the number of covariates exceeding the sample size. This paper introduces novel and fast marginal maximum likelihood (MML) algorithms for estimating the shrinkage parameter(s) for the Bayesian ridge and power ridge regression models, and an automatic plug-in MML estimator for the Bayesian generalized ridge regression model. With the aid of the singular value decomposition of the observed covariate design matrix, these MML estimation methods are quite fast even for data sets where either the sample size (n) or the number of covariates (p) is very large, and even when p>n. On several real data sets varying widely in terms of n and p, the computation times of the MML estimation methods for the three ridge models, respectively, are compared with the times of other methods for estimating the shrinkage parameter in ridge, LASSO and Elastic Net (EN) models, with the other methods based on minimizing prediction error according to cross-validation or information criteria. Also, the ridge, LASSO, and EN models, and their associated estimation methods, are compared in terms of prediction accuracy. Furthermore, a simulation study compares the ridge models under MML estimation, against the LASSO and EN models, in terms of their ability to differentiate between truly-significant covariates (i.e., with non-zero slope coefficients) and truly-insignificant covariates (with zero coefficients).

</details>

<details>

<summary>2015-06-24 15:09:25 - Scalable Bayesian nonparametric regression via a Plackett-Luce model for conditional ranks</summary>

- *Tristan Gray-Davies, Chris Holmes, Francois Caron*

- `1506.07412v1` - [abs](http://arxiv.org/abs/1506.07412v1) - [pdf](http://arxiv.org/pdf/1506.07412v1)

> We present a novel Bayesian nonparametric regression model for covariates X and continuous, real response variable Y. The model is parametrized in terms of marginal distributions for Y and X and a regression function which tunes the stochastic ordering of the conditional distributions F(y|x). By adopting an approximate composite likelihood approach, we show that the resulting posterior inference can be decoupled for the separate components of the model. This procedure can scale to very large datasets and allows for the use of standard, existing, software from Bayesian nonparametric density estimation and Plackett-Luce ranking estimation to be applied. As an illustration, we show an application of our approach to a US Census dataset, with over 1,300,000 data points and more than 100 covariates.

</details>

<details>

<summary>2015-06-24 16:31:04 - Comparisons of two quantile regression smoothers</summary>

- *Rand Wilcox*

- `1506.07456v1` - [abs](http://arxiv.org/abs/1506.07456v1) - [pdf](http://arxiv.org/pdf/1506.07456v1)

> The paper compares the small-sample properties of two non-parametric quantile regression estimators. The first is based on constrained B-spline smoothing (COBS) and the other is based on a variation and slight extension of a running interval smoother, which apparently has not been studied via simulations. The motivation for this paper stems from the Well Elderly 2 study, a portion of which was aimed at understanding the association between the cortisol awakening response and two measures of stress.   COBS indicated what appeared be an usual form of curvature. The modified running interval smoother gave a strikingly different estimate, which raised the issue of how it compares to COBS in terms of mean squared error and bias as well as its ability to avoid a spurious indication of curvature. R functions for applying the methods were used in conjunction with default settings for the various optional arguments. The results indicate that the modified running interval smoother has practical value. Manipulation of the optional arguments might impact the relative merits of the two methods, but the extent to which this is the case remains unknown.

</details>

<details>

<summary>2015-06-24 16:41:14 - Global comparisons of medians and other quantiles in a one-way design when there are tied values</summary>

- *Rand Wilcox*

- `1506.07461v1` - [abs](http://arxiv.org/abs/1506.07461v1) - [pdf](http://arxiv.org/pdf/1506.07461v1)

> For $J \ge 2$ independent groups, the paper deals with testing the global hypothesis that all $J$ groups have a common population median or identical quantiles, with an emphasis on the quartiles. Classic rank-based methods are sometimes suggested for comparing medians, but it is well known that under general conditions they do not adequately address this goal. Extant methods based on the usual sample median are unsatisfactory when there are tied values except for the special case $J=2$. A variation of the percentile bootstrap used in conjunction with the Harrell--Davis quantile estimator performs well in simulations. The method is illustrated with data from the Well Elderly 2 study.

</details>

<details>

<summary>2015-06-24 17:00:42 - ANCOVA: A global test based on a robust measure of location or quantiles when there is curvature</summary>

- *Rand Wilcox*

- `1506.07467v1` - [abs](http://arxiv.org/abs/1506.07467v1) - [pdf](http://arxiv.org/pdf/1506.07467v1)

> For two independent groups, let $M_j(x)$ be some conditional measure of location for the $j$th group associated with some random variable $Y$, given that some covariate $X=x$. When $M_j(x)$ is a robust measure of location, or even some conditional quantile of $Y$, given $X$, methods have been proposed and studied that are aimed at testing $H_0$: $M_1(x)=M_2(x)$ that deal with curvature in a flexible manner. In addition, methods have been studied where the goal is to control the probability of one or more Type I errors when testing $H_0$ for each $x \in \{x_1, \ldots, x_p\}$. This paper suggests a method for testing the global hypothesis $H_0$: $M_1(x)=M_2(x)$ for $\forall x \in \{x_1, \ldots, x_p\}$ when using a robust or quantile location estimator. An obvious advantage of testing $p$ hypotheses, rather than the global hypothesis, is that it can provide information about where regression lines differ and by how much. But the paper summarizes three general reasons to suspect that testing the global hypothesis can have more power. 2 Data from the Well Elderly 2 study illustrate that testing the global hypothesis can make a practical difference.

</details>

<details>

<summary>2015-06-25 03:00:21 - Markov Interacting Importance Samplers</summary>

- *Eduardo F. Mendes, Marcel Scharth, Robert Kohn*

- `1502.07039v2` - [abs](http://arxiv.org/abs/1502.07039v2) - [pdf](http://arxiv.org/pdf/1502.07039v2)

> We introduce a new Markov chain Monte Carlo (MCMC) sampler called the Markov Interacting Importance Sampler (MIIS). The MIIS sampler uses conditional importance sampling (IS) approximations to jointly sample the current state of the Markov Chain and estimate conditional expectations, possibly by incorporating a full range of variance reduction techniques. We compute Rao-Blackwellized estimates based on the conditional expectations to construct control variates for estimating expectations under the target distribution. The control variates are particularly efficient when there are substantial correlations between the variables in the target distribution, a challenging setting for MCMC. An important motivating application of MIIS occurs when the exact Gibbs sampler is not available because it is infeasible to directly simulate from the conditional distributions. In this case the MIIS method can be more efficient than a Metropolis-within-Gibbs approach. We also introduce the MIIS random walk algorithm, designed to accelerate convergence and improve upon the computational efficiency of standard random walk samplers. Simulated and empirical illustrations for Bayesian analysis show that the method significantly reduces the variance of Monte Carlo estimates compared to standard MCMC approaches, at equivalent implementation and computational effort.

</details>

<details>

<summary>2015-06-25 03:03:51 - An objective prior that unifies objective Bayes and information-based inference</summary>

- *Colin H. LaMont, Paul A. Wiggins*

- `1506.00745v2` - [abs](http://arxiv.org/abs/1506.00745v2) - [pdf](http://arxiv.org/pdf/1506.00745v2)

> There are three principle paradigms of statistical inference: (i) Bayesian, (ii) information-based and (iii) frequentist inference. We describe an objective prior (the weighting or $w$-prior) which unifies objective Bayes and information-based inference. The $w$-prior is chosen to make the marginal probability an unbiased estimator of the predictive performance of the model. This definition has several other natural interpretations. From the perspective of the information content of the prior, the $w$-prior is both uniformly and maximally uninformative. The $w$-prior can also be understood to result in a uniform density of distinguishable models in parameter space. Finally we demonstrate the the $w$-prior is equivalent to the Akaike Information Criterion (AIC) for regular models in the asymptotic limit. The $w$-prior appears to be generically applicable to statistical inference and is free of {\it ad hoc} regularization. The mechanism for suppressing complexity is analogous to AIC: model complexity reduces model predictivity. We expect this new objective-Bayes approach to inference to be widely-applicable to machine-learning problems including singular models.

</details>

<details>

<summary>2015-06-26 05:24:30 - Factorized Asymptotic Bayesian Inference for Factorial Hidden Markov Models</summary>

- *Shaohua Li, Ryohei Fujimaki, Chunyan Miao*

- `1506.07959v1` - [abs](http://arxiv.org/abs/1506.07959v1) - [pdf](http://arxiv.org/pdf/1506.07959v1)

> Factorial hidden Markov models (FHMMs) are powerful tools of modeling sequential data. Learning FHMMs yields a challenging simultaneous model selection issue, i.e., selecting the number of multiple Markov chains and the dimensionality of each chain. Our main contribution is to address this model selection issue by extending Factorized Asymptotic Bayesian (FAB) inference to FHMMs. First, we offer a better approximation of marginal log-likelihood than the previous FAB inference. Our key idea is to integrate out transition probabilities, yet still apply the Laplace approximation to emission probabilities. Second, we prove that if there are two very similar hidden states in an FHMM, i.e. one is redundant, then FAB will almost surely shrink and eliminate one of them, making the model parsimonious. Experimental results show that FAB for FHMMs significantly outperforms state-of-the-art nonparametric Bayesian iFHMM and Variational FHMM in model selection accuracy, with competitive held-out perplexity.

</details>

<details>

<summary>2015-06-26 15:03:33 - Modelling of directional data using Kent distributions</summary>

- *Parthan Kasarapu*

- `1506.08105v1` - [abs](http://arxiv.org/abs/1506.08105v1) - [pdf](http://arxiv.org/pdf/1506.08105v1)

> The modelling of data on a spherical surface requires the consideration of directional probability distributions. To model asymmetrically distributed data on a three-dimensional sphere, Kent distributions are often used. The moment estimates of the parameters are typically used in modelling tasks involving Kent distributions. However, these lack a rigorous statistical treatment. The focus of the paper is to introduce a Bayesian estimation of the parameters of the Kent distribution which has not been carried out in the literature, partly because of its complex mathematical form. We employ the Bayesian information-theoretic paradigm of Minimum Message Length (MML) to bridge this gap and derive reliable estimators. The inferred parameters are subsequently used in mixture modelling of Kent distributions. The problem of inferring the suitable number of mixture components is also addressed using the MML criterion. We demonstrate the superior performance of the derived MML-based parameter estimates against the traditional estimators. We apply the MML principle to infer mixtures of Kent distributions to model empirical data corresponding to protein conformations. We demonstrate the effectiveness of Kent models to act as improved descriptors of protein structural data as compared to commonly used von Mises-Fisher distributions.

</details>

<details>

<summary>2015-06-26 18:55:11 - An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process</summary>

- *Amar Shah, David A. Knowles, Zoubin Ghahramani*

- `1506.08180v1` - [abs](http://arxiv.org/abs/1506.08180v1) - [pdf](http://arxiv.org/pdf/1506.08180v1)

> Stochastic variational inference (SVI) is emerging as the most promising candidate for scaling inference in Bayesian probabilistic models to large datasets. However, the performance of these methods has been assessed primarily in the context of Bayesian topic models, particularly latent Dirichlet allocation (LDA). Deriving several new algorithms, and using synthetic, image and genomic datasets, we investigate whether the understanding gleaned from LDA applies in the setting of sparse latent factor models, specifically beta process factor analysis (BPFA). We demonstrate that the big picture is consistent: using Gibbs sampling within SVI to maintain certain posterior dependencies is extremely effective. However, we find that different posterior dependencies are important in BPFA relative to LDA. Particularly, approximations able to model intra-local variable dependence perform best.

</details>

<details>

<summary>2015-06-29 05:33:51 - Extreme sizes in the Gibbs-type exchangeable random partitions</summary>

- *Shuhei Mano*

- `1306.2056v5` - [abs](http://arxiv.org/abs/1306.2056v5) - [pdf](http://arxiv.org/pdf/1306.2056v5)

> Gibbs-type exchangeable random partitions, which is a class of multiplicative measures on the set of positive integer partitions, appear in various contexts, including Bayesian statistics, random combinatorial structures, and stochastic models of diversity in various phenomena. Some distributional results on ordered sizes in the Gibbs partition are established by introducing associated partial Bell polynomials and analysis of the generating functions. The combinatorial approach is applied to derive explicit results on asymptotic behavior of the extreme sizes in the Gibbs partition. Especially, Ewens-Pitman partition, which is the sample from the Poisson-Dirichlet process and has been discussed from rather model-specific viewpoints, and a random partition which was recently introduced by Gnedin, are discussed in the details. As by-products, some formulas for the associated partial Bell polynomials are presented.

</details>

<details>

<summary>2015-06-29 14:14:38 - Leave Pima Indians alone: binary regression as a benchmark for Bayesian computation</summary>

- *Nicolas Chopin, James Ridgway*

- `1506.08640v1` - [abs](http://arxiv.org/abs/1506.08640v1) - [pdf](http://arxiv.org/pdf/1506.08640v1)

> Abstract. Whenever a new approach to perform Bayesian computation is introduced, a common practice is to showcase this approach on a binary regression model and datasets of moderate size. This paper discusses to which extent this practice is sound. It also reviews the current state of the art of Bayesian computation, using binary regression as a running example. Both sampling-based algorithms (importance sampling, MCMC and SMC) and fast approximations (Laplace and EP) are covered. Extensive numerical results are provided, some of which might go against conventional wisdom regarding the effectiveness of certain algorithms. Implications for other problems (variable selection) and other models are also discussed.

</details>

<details>

<summary>2015-06-30 11:17:40 - Bayesian model averaging in model-based clustering and density estimation</summary>

- *Niamh Russell, Thomas Brendan Murphy, Adrian E Raftery*

- `1506.09035v1` - [abs](http://arxiv.org/abs/1506.09035v1) - [pdf](http://arxiv.org/pdf/1506.09035v1)

> We propose Bayesian model averaging (BMA) as a method for postprocessing the results of model-based clustering. Given a number of competing models, appropriate model summaries are averaged, using the posterior model probabilities, instead of being taken from a single "best" model. We demonstrate the use of BMA in model-based clustering for a number of datasets. We show that BMA provides a useful summary of the clustering of observations while taking model uncertainty into account. Further, we show that BMA in conjunction with model-based clustering gives a competitive method for density estimation in a multivariate setting. Applying BMA in the model-based context is fast and can give enhanced modeling performance.

</details>

<details>

<summary>2015-06-30 12:34:11 - Nonlinear Distortion Reduction in OFDM from Reliable Perturbations in Data Carriers</summary>

- *Ebrahim B. Al-Safadi, Tareq Y. Al-Naffouri, Mudassir Masood, Anum Ali*

- `1506.09060v1` - [abs](http://arxiv.org/abs/1506.09060v1) - [pdf](http://arxiv.org/pdf/1506.09060v1)

> A novel method for correcting the effect of nonlinear distortion in orthogonal frequency division multiplexing signals is proposed. The method depends on adaptively selecting the distortion over a subset of the data carriers, and then using tools from compressed sensing and sparse Bayesian recovery to estimate the distortion over the other carriers. Central to this method is the fact that carriers (or tones) are decoded with different levels of confidence, depending on a coupled function of the magnitude and phase of the distortion over each carrier, in addition to the respective channel strength. Moreover, as no pilots are required by this method, a significant improvement in terms of achievable rate can be achieved relative to previous work.

</details>

<details>

<summary>2015-06-30 18:46:47 - A note on monitoring ratios of two Weibull percentiles</summary>

- *Pasquale Erto*

- `1305.5962v3` - [abs](http://arxiv.org/abs/1305.5962v3) - [pdf](http://arxiv.org/pdf/1305.5962v3)

> This note introduces a new Bayesian control chart to compare two processes by monitoring the ratio of their percentiles under Weibull assumption. Both in-control and out-of-control parameters are supposed unknown. The chart analyses the sampling data directly, instead of transforming them in order to comply with the usual normality assumption, as most charts do. The chart uses the whole accumulated knowledge, resulting from the current and all the past samples, to monitor the current value of the ratio. Two real applications in the wood industry and in the concrete industry give a first picture of the features of the chart.

</details>


## 2015-07

<details>

<summary>2015-07-01 03:01:50 - On the Equivalence of Factorized Information Criterion Regularization and the Chinese Restaurant Process Prior</summary>

- *Shaohua Li*

- `1506.09068v2` - [abs](http://arxiv.org/abs/1506.09068v2) - [pdf](http://arxiv.org/pdf/1506.09068v2)

> Factorized Information Criterion (FIC) is a recently developed information criterion, based on which a novel model selection methodology, namely Factorized Asymptotic Bayesian (FAB) Inference, has been developed and successfully applied to various hierarchical Bayesian models. The Dirichlet Process (DP) prior, and one of its well known representations, the Chinese Restaurant Process (CRP), derive another line of model selection methods. FIC can be viewed as a prior distribution over the latent variable configurations. Under this view, we prove that when the parameter dimensionality $D_{c}=2$, FIC is equivalent to CRP. We argue that when $D_{c}>2$, FIC avoids an inherent problem of DP/CRP, i.e. the data likelihood will dominate the impact of the prior, and thus the model selection capability will weaken as $D_{c}$ increases. However, FIC overestimates the data likelihood. As a result, FIC may be overly biased towards models with less components. We propose a natural generalization of FIC, which finds a middle ground between CRP and FIC, and may yield more accurate model selection results than FIC.

</details>

<details>

<summary>2015-07-01 13:24:27 - Modeling Compositional Regression with uncorrelated and correlated errors: a Bayesian approach</summary>

- *Taciana K. O. Shimizu, Francisco Louzada, Adriano K. Suzuki, Ricardo S. Ehlers*

- `1507.00225v1` - [abs](http://arxiv.org/abs/1507.00225v1) - [pdf](http://arxiv.org/pdf/1507.00225v1)

> Compositional data consist of known compositions vectors whose components are positive and defined in the interval (0,1) representing proportions or fractions of a "whole". The sum of these components must be equal to one. Compositional data is present in different knowledge areas, as in geology, economy, medicine among many others. In this paper, we introduce a Bayesian analysis for compositional regression applying additive log-ratio (ALR) transformation and assuming uncorrelated and correlated errors. The Bayesian inference procedure based on Markov Chain Monte Carlo Methods (MCMC). The methodology is illustrated on an artificial and a real data set of volleyball.

</details>

<details>

<summary>2015-07-01 13:59:35 - Posterior asymptotics of nonparametric location-scale mixtures for multivariate density estimation</summary>

- *Antonio Canale, Pierpaolo De Blasi*

- `1306.2671v3` - [abs](http://arxiv.org/abs/1306.2671v3) - [pdf](http://arxiv.org/pdf/1306.2671v3)

> Density estimation represents one of the most successful applications of Bayesian nonparametrics. In particular, Dirichlet process mixtures of normals are the gold standard for density estimation and their asymptotic properties have been studied extensively, especially in the univariate case. However a gap between practitioners and the current theoretical literature is present. So far, posterior asymptotic results in the multivariate case are available only for location mixtures of Gaussian kernels with independent prior on the common covariance matrix, while in practice as well as from a conceptual point of view a location-scale mixture is often preferable. In this paper we address posterior consistency for such general mixture models by adapting a convergence rate result which combines the usual low-entropy, high-mass sieve approach with a suitable summability condition. Specifically, we establish consistency for Dirichlet process mixtures of Gaussian kernels with various prior specifications on the covariance matrix. Posterior convergence rates are also discussed.

</details>

<details>

<summary>2015-07-01 19:05:56 - Exploring Algorithmic Limits of Matrix Rank Minimization under Affine Constraints</summary>

- *Bo Xin, David Wipf*

- `1406.2504v3` - [abs](http://arxiv.org/abs/1406.2504v3) - [pdf](http://arxiv.org/pdf/1406.2504v3)

> Many applications require recovering a matrix of minimal rank within an affine constraint set, with matrix completion a notable special case. Because the problem is NP-hard in general, it is common to replace the matrix rank with the nuclear norm, which acts as a convenient convex surrogate. While elegant theoretical conditions elucidate when this replacement is likely to be successful, they are highly restrictive and convex algorithms fail when the ambient rank is too high or when the constraint set is poorly structured. Non-convex alternatives fare somewhat better when carefully tuned; however, convergence to locally optimal solutions remains a continuing source of failure. Against this backdrop we derive a deceptively simple and parameter-free probabilistic PCA-like algorithm that is capable, over a wide battery of empirical tests, of successful recovery even at the theoretical limit where the number of measurements equal the degrees of freedom in the unknown low-rank matrix. Somewhat surprisingly, this is possible even when the affine constraint set is highly ill-conditioned. While proving general recovery guarantees remains evasive for non-convex algorithms, Bayesian-inspired or otherwise, we nonetheless show conditions whereby the underlying cost function has a unique stationary point located at the global optimum; no existing cost function we are aware of satisfies this same property. We conclude with a simple computer vision application involving image rectification and a standard collaborative filtering benchmark.

</details>

<details>

<summary>2015-07-02 00:19:57 - The Parallel C++ Statistical Library for Bayesian Inference: QUESO</summary>

- *Damon McDougall, Nicholas Malaya, Robert D. Moser*

- `1507.00398v1` - [abs](http://arxiv.org/abs/1507.00398v1) - [pdf](http://arxiv.org/pdf/1507.00398v1)

> The Parallel C++ Statistical Library for the Quantification of Uncertainty for Estimation, Simulation and Optimization, Queso, is a collection of statistical algorithms and programming constructs supporting research into the quantification of uncertainty of models and their predictions. Queso is primarily focused on solving statistical inverse problems using Bayes's theorem, which expresses a distribution of possible values for a set of uncertain parameters (the posterior distribution) in terms of the existing knowledge of the system (the prior) and noisy observations of a physical process, represented by a likelihood distribution. The posterior distribution is not often known analytically, and so requires computational methods. It is typical to compute probabilities and moments from the posterior distribution, but this is often a high-dimensional object and standard Reimann-type methods for quadrature become prohibitively expensive. The approach Queso takes in this regard is to rely on Markov chain Monte Carlo (MCMC) methods which are well suited to evaluating quantities such as probabilities and moments of high-dimensional probability distributions. Queso's intended use is as tool to assist and facilitate coupling uncertainty quantification to a specific application called a forward problem. While many libraries presently exist that solve Bayesian inference problems, Queso is a specialized piece of software primarily designed to solve such problems by utilizing parallel environments demanded by large-scale forward problems. Queso is written in C++, uses MPI, and utilizes libraries already available to the scientific community.

</details>

<details>

<summary>2015-07-02 07:51:07 - Coauthorship and Citation Networks for Statisticians</summary>

- *Pengsheng Ji, Jiashun Jin*

- `1410.2840v2` - [abs](http://arxiv.org/abs/1410.2840v2) - [pdf](http://arxiv.org/pdf/1410.2840v2)

> We have collected and cleaned two network data sets: Coauthorship and Citation networks for statisticians. The data sets are based on all research papers published in four of the top journals in statistics from $2003$ to the first half of $2012$. We analyze the data sets from many different perspectives, focusing on (a) centrality, (b) community structures, and (c) productivity, patterns and trends.   For (a), we have identified the most prolific/collaborative/highly cited authors. We have also identified a handful of "hot" papers, suggesting "Variable Selection" as one of the "hot" areas.   For (b), we have identified about $15$ meaningful communities or research groups, including large-size ones such as "Spatial Statistics", "Large-Scale Multiple Testing", "Variable Selection" as well as small-size ones such as "Dimensional Reduction", "Objective Bayes", "Quantile Regression", and "Theoretical Machine Learning".   For (c), we find that over the 10-year period, both the average number of papers per author and the fraction of self citations have been decreasing, but the proportion of distant citations has been increasing. These suggest that the statistics community has become increasingly more collaborative, competitive, and globalized.   Our findings shed light on research habits, trends, and topological patterns of statisticians. The data sets provide a fertile ground for future researches on or related to social networks of statisticians.

</details>

<details>

<summary>2015-07-02 10:20:52 - Identification of stable models via nonparametric prediction error methods</summary>

- *Diego Romeres, Gianluigi Pillonetto, Alessandro Chiuso*

- `1507.00507v1` - [abs](http://arxiv.org/abs/1507.00507v1) - [pdf](http://arxiv.org/pdf/1507.00507v1)

> A new Bayesian approach to linear system identification has been proposed in a series of recent papers. The main idea is to frame linear system identification as predictor estimation in an infinite dimensional space, with the aid of regularization/Bayesian techniques. This approach guarantees the identification of stable predictors based on the prediction error minimization. Unluckily, the stability of the predictors does not guarantee the stability of the impulse response of the system. In this paper we propose and compare various techniques to address this issue. Simulations results comparing these techniques will be provided.

</details>

<details>

<summary>2015-07-02 12:28:41 - Classical vs. Bayesian methods for linear system identification: point estimators and confidence sets</summary>

- *D. Romeres, G. Prando, G. Pillonetto, A. Chiuso*

- `1507.00543v1` - [abs](http://arxiv.org/abs/1507.00543v1) - [pdf](http://arxiv.org/pdf/1507.00543v1)

> This paper compares classical parametric methods with recently developed Bayesian methods for system identification. A Full Bayes solution is considered together with one of the standard approximations based on the Empirical Bayes paradigm. Results regarding point estimators for the impulse response as well as for confidence regions are reported.

</details>

<details>

<summary>2015-07-02 14:47:23 - Bayesian complementary clustering, MCMC and Anglo-Saxon placenames</summary>

- *Giacomo Zanella*

- `1409.6994v4` - [abs](http://arxiv.org/abs/1409.6994v4) - [pdf](http://arxiv.org/pdf/1409.6994v4)

> Common cluster models for multi-type point processes model the aggregation of points of the same type. In complete contrast, in the study of Anglo-Saxon settlements it is hypothesized that administrative clusters involving complementary names tend to appear. We investigate the evidence for such an hypothesis by developing a Bayesian Random Partition Model based on clusters formed by points of different types (complementary clustering).   As a result we obtain an intractable posterior distribution on the space of matchings contained in a k-partite hypergraph. We apply the Metropolis-Hastings (MH) algorithm to sample from this posterior. We consider the problem of choosing an efficient MH proposal distribution and we obtain consistent mixing improvements compared to the choices found in the literature. Simulated Tempering techniques can be used to overcome multimodality and a multiple proposal scheme is developed to allow for parallel programming. Finally, we discuss results arising from the careful use of convergence diagnostic techniques.   This allows us to study a dataset including locations and placenames of 1316 Anglo-Saxon settlements dated approximately around 750-850 AD. Without strong prior knowledge, the model allows for explicit estimation of the number of clusters, the average intra-cluster dispersion and the level of interaction among placenames. The results support the hypothesis of organization of settlements into administrative clusters based on complementary names.

</details>

<details>

<summary>2015-07-02 23:54:22 - Exploiting the Quantile Optimality Ratio to Obtain Better Confidence Intervals for Quantiles</summary>

- *Luke A. Prendergast, Robert G. Staudte*

- `1505.04234v2` - [abs](http://arxiv.org/abs/1505.04234v2) - [pdf](http://arxiv.org/pdf/1505.04234v2)

> A standard approach to confidence intervals for quantiles requires good estimates of the quantile density. The optimal bandwidth for kernel estimation of the quantile density depends on an underlying location-scale family only through the quantile optimality ratio (QOR), which is the starting point for our results. While the QOR is not distribution-free, it turns out that what is optimal for one family often works quite well for families having similar shape. This allows one to rely on a single representative QOR if one has a rough idea of the distributional shape. Another option that we explore assumes the data can be modeled by the highly flexible generalized lambda distribution (GLD), already studied by others, and we show that using the QOR for the estimated GLD can lead to more than competitive intervals. Confidence intervals for the difference between quantiles from independent populations are also considered, with an application to heart rate data.

</details>

<details>

<summary>2015-07-03 06:14:26 - D-MFVI: Distributed Mean Field Variational Inference using Bregman ADMM</summary>

- *Behnam Babagholami-Mohamadabadi, Sejong Yoon, Vladimir Pavlovic*

- `1507.00824v1` - [abs](http://arxiv.org/abs/1507.00824v1) - [pdf](http://arxiv.org/pdf/1507.00824v1)

> Bayesian models provide a framework for probabilistic modelling of complex datasets. However, many of such models are computationally demanding especially in the presence of large datasets. On the other hand, in sensor network applications, statistical (Bayesian) parameter estimation usually needs distributed algorithms, in which both data and computation are distributed across the nodes of the network. In this paper we propose a general framework for distributed Bayesian learning using Bregman Alternating Direction Method of Multipliers (B-ADMM). We demonstrate the utility of our framework, with Mean Field Variational Bayes (MFVB) as the primitive for distributed Matrix Factorization (MF) and distributed affine structure from motion (SfM).

</details>

<details>

<summary>2015-07-03 18:10:07 - Globally adaptive quantile regression with ultra-high dimensional data</summary>

- *Qi Zheng, Limin Peng, Xuming He*

- `1507.00420v2` - [abs](http://arxiv.org/abs/1507.00420v2) - [pdf](http://arxiv.org/pdf/1507.00420v2)

> Quantile regression has become a valuable tool to analyze heterogeneous covaraite-response associations that are often encountered in practice. The development of quantile regression methodology for high-dimensional covariates primarily focuses on examination of model sparsity at a single or multiple quantile levels, which are typically pre-specified ad hoc by the users. The resulting models may be sensitive to the specific choices of the quantile levels, leading to difficulties in interpretation and erosion of confidence in the results. In this article, we propose a new penalization framework for quantile regression in the high-dimensional setting. We employ adaptive L1 penalties, and more importantly, propose a uniform selector of the tuning parameter for a set of quantile levels to avoid some of the potential problems with model selection at individual quantile levels. Our proposed approach achieves consistent shrinkage of regression quantile estimates across a continuous range of quantiles levels, enhancing the flexibility and robustness of the existing penalized quantile regression methods. Our theoretical results include the oracle rate of uniform convergence and weak convergence of the parameter estimators. We also use numerical studies to confirm our theoretical findings and illustrate the practical utility of our proposal

</details>

<details>

<summary>2015-07-03 19:07:08 - Bayesian Structure Learning for Stationary Time Series</summary>

- *Alex Tank, Nicholas Foti, Emily Fox*

- `1505.03131v2` - [abs](http://arxiv.org/abs/1505.03131v2) - [pdf](http://arxiv.org/pdf/1505.03131v2)

> While much work has explored probabilistic graphical models for independent data, less attention has been paid to time series. The goal in this setting is to determine conditional independence relations between entire time series, which for stationary series, are encoded by zeros in the inverse spectral density matrix. We take a Bayesian approach to structure learning, placing priors on (i) the graph structure and (ii) spectral matrices given the graph. We leverage a Whittle likelihood approximation and define a conjugate prior---the hyper complex inverse Wishart---on the complex-valued and graph-constrained spectral matrices. Due to conjugacy, we can analytically marginalize the spectral matrices and obtain a closed-form marginal likelihood of the time series given a graph. Importantly, our analytic marginal likelihood allows us to avoid inference of the complex spectral matrices themselves and places us back into the framework of standard (Bayesian) structure learning. In particular, combining this marginal likelihood with our graph prior leads to efficient inference of the time series graph itself, which we base on a stochastic search procedure, though any standard approach can be straightforwardly modified to our time series case. We demonstrate our methods on analyzing stock data and neuroimaging data of brain activity during various auditory tasks.

</details>

<details>

<summary>2015-07-03 22:28:36 - A first look at the performances of a Bayesian chart to monitor the ratio of two Weibull percentiles</summary>

- *Pasquale Erto*

- `1507.01044v1` - [abs](http://arxiv.org/abs/1507.01044v1) - [pdf](http://arxiv.org/pdf/1507.01044v1)

> The aim of the present work is to investigate the performances of a specific Bayesian control chart used to compare two processes. The chart monitors the ratio of the percentiles of a key characteristic associated with the processes. The variability of such a characteristic is modeled via the Weibull distribution and a practical Bayesian approach to deal with Weibull data is adopted. The percentiles of the two monitored processes are assumed to be independent random variables. The Weibull distributions of the key characteristic of both processes are assumed to have the same and stable shape parameter. This is usually experienced in practice because the Weibull shape parameter is related to the main involved factor of variability. However, if a change of the shape parameters of the processes is suspected, the involved distributions can be used to monitor their stability. We first tested the effects of the number of the training data on the responsiveness of the chart. Then we tested the robustness of the chart in spite of very poor prior information. To this end, the prior values were changed to reflect a 50% shift in both directions from the original values of the shape parameter and the percentiles of the two monitored processes. Finally, various combinations of shifts were considered for the sampling distributions after the Phase I, with the purpose of estimating the diagnostic ability of the charts to signal an out-of-control state. The traditional approach based on the Average Run Length, empirically computed via a Monte Carlo simulation, was adopted.

</details>

<details>

<summary>2015-07-05 08:47:24 - Bayesian inference for latent factor GARCH models</summary>

- *Michael K. Pitt, Jamie Hall, Robert Kohn*

- `1507.01179v1` - [abs](http://arxiv.org/abs/1507.01179v1) - [pdf](http://arxiv.org/pdf/1507.01179v1)

> Latent factor GARCH models are difficult to estimate using Bayesian methods because standard Markov chain Monte Carlo samplers produce slowly mixing and inefficient draws from the posterior distributions of the model parameters. This paper describes how to apply the particle Gibbs algorithm to estimate factor GARCH models efficiently. The method has two advantages over previous approaches. First, it generalises in a straightfoward way to models with multiple factors and to various members of the GARCH family. Second, it scales up well as the dimension of the o, bservation vector increases.

</details>

<details>

<summary>2015-07-05 16:44:23 - Modeling for Dynamic Ordinal Regression Relationships: An Application to Estimating Maturity of Rockfish in California</summary>

- *Maria DeYoreo, Athanasios Kottas*

- `1507.01242v1` - [abs](http://arxiv.org/abs/1507.01242v1) - [pdf](http://arxiv.org/pdf/1507.01242v1)

> We develop a Bayesian nonparametric framework for modeling ordinal regression relationships which evolve in discrete time. The motivating application involves a key problem in fisheries research on estimating dynamically evolving relationships between age, length and maturity, the latter recorded on an ordinal scale. The methodology builds from nonparametric mixture modeling for the joint stochastic mechanism of covariates and latent continuous responses. This approach yields highly flexible inference for ordinal regression functions while at the same time avoiding the computational challenges of parametric models. A novel dependent Dirichlet process prior for time-dependent mixing distributions extends the model to the dynamic setting. The methodology is used for a detailed study of relationships between maturity, age, and length for Chilipepper rockfish, using data collected over 15 years along the coast of California.

</details>

<details>

<summary>2015-07-06 14:54:58 - Optimal low-rank approximations of Bayesian linear inverse problems</summary>

- *Alessio Spantini, Antti Solonen, Tiangang Cui, James Martin, Luis Tenorio, Youssef Marzouk*

- `1407.3463v2` - [abs](http://arxiv.org/abs/1407.3463v2) - [pdf](http://arxiv.org/pdf/1407.3463v2)

> In the Bayesian approach to inverse problems, data are often informative, relative to the prior, only on a low-dimensional subspace of the parameter space. Significant computational savings can be achieved by using this subspace to characterize and approximate the posterior distribution of the parameters. We first investigate approximation of the posterior covariance matrix as a low-rank update of the prior covariance matrix. We prove optimality of a particular update, based on the leading eigendirections of the matrix pencil defined by the Hessian of the negative log-likelihood and the prior precision, for a broad class of loss functions. This class includes the F\"{o}rstner metric for symmetric positive definite matrices, as well as the Kullback-Leibler divergence and the Hellinger distance between the associated distributions. We also propose two fast approximations of the posterior mean and prove their optimality with respect to a weighted Bayes risk under squared-error loss. These approximations are deployed in an offline-online manner, where a more costly but data-independent offline calculation is followed by fast online evaluations. As a result, these approximations are particularly useful when repeated posterior mean evaluations are required for multiple data sets. We demonstrate our theoretical results with several numerical examples, including high-dimensional X-ray tomography and an inverse heat conduction problem. In both of these examples, the intrinsic low-dimensional structure of the inference problem can be exploited while producing results that are essentially indistinguishable from solutions computed in the full space.

</details>

<details>

<summary>2015-07-07 00:13:55 - Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse Optimization and Noisy Matrix Decomposition</summary>

- *Hanie Sedghi, Anima Anandkumar, Edmond Jonckheere*

- `1402.5131v6` - [abs](http://arxiv.org/abs/1402.5131v6) - [pdf](http://arxiv.org/pdf/1402.5131v6)

> We propose an efficient ADMM method with guarantees for high-dimensional problems. We provide explicit bounds for the sparse optimization problem and the noisy matrix decomposition problem. For sparse optimization, we establish that the modified ADMM method has an optimal convergence rate of $\mathcal{O}(s\log d/T)$, where $s$ is the sparsity level, $d$ is the data dimension and $T$ is the number of steps. This matches with the minimax lower bounds for sparse estimation. For matrix decomposition into sparse and low rank components, we provide the first guarantees for any online method, and prove a convergence rate of $\tilde{\mathcal{O}}((s+r)\beta^2(p) /T) + \mathcal{O}(1/p)$ for a $p\times p$ matrix, where $s$ is the sparsity level, $r$ is the rank and $\Theta(\sqrt{p})\leq \beta(p)\leq \Theta(p)$. Our guarantees match the minimax lower bound with respect to $s,r$ and $T$. In addition, we match the minimax lower bound with respect to the matrix dimension $p$, i.e. $\beta(p)=\Theta(\sqrt{p})$, for many important statistical models including the independent noise model, the linear Bayesian network and the latent Gaussian graphical model under some conditions. Our ADMM method is based on epoch-based annealing and consists of inexpensive steps which involve projections on to simple norm balls. Experiments show that for both sparse optimization and matrix decomposition problems, our algorithm outperforms the state-of-the-art methods. In particular, we reach higher accuracy with same time complexity.

</details>

<details>

<summary>2015-07-07 09:08:13 - Stochastic determination of matrix determinants</summary>

- *Sebastian Dorn, Torsten A. Enßlin*

- `1504.02661v2` - [abs](http://arxiv.org/abs/1504.02661v2) - [pdf](http://arxiv.org/pdf/1504.02661v2)

> Matrix determinants play an important role in data analysis, in particular when Gaussian processes are involved. Due to currently exploding data volumes, linear operations - matrices - acting on the data are often not accessible directly but are only represented indirectly in form of a computer routine. Such a routine implements the transformation a data vector undergoes under matrix multiplication. While efficient probing routines to estimate a matrix's diagonal or trace, based solely on such computationally affordable matrix-vector multiplications, are well known and frequently used in signal inference, there is no stochastic estimate for its determinant. We introduce a probing method for the logarithm of a determinant of a linear operator. Our method rests upon a reformulation of the log-determinant by an integral representation and the transformation of the involved terms into stochastic expressions. This stochastic determinant determination enables large-size applications in Bayesian inference, in particular evidence calculations, model comparison, and posterior determination.

</details>

<details>

<summary>2015-07-07 14:34:19 - Comparing Biomarkers as Trial Level General Surrogates</summary>

- *Erin E. Gabriel, Michael J. Daniels, M. Elizabeth Halloran*

- `1507.01825v1` - [abs](http://arxiv.org/abs/1507.01825v1) - [pdf](http://arxiv.org/pdf/1507.01825v1)

> An intermediate response measure that accurately predicts efficacy in a new setting can reduce trial cost and time to product licensure. In this paper, we define a trial level general surrogate as a trial level intermediate response that accurately predicts trial level clinical responses. Methods for evaluating trial level general surrogates have been developed previously. Many methods in the literature use trial level intermediate responses for prediction. However, all existing methods focus on surrogate evaluation and prediction in new settings, rather than comparison of candidate trial level surrogates, and few formalize the use of cross validation to quantify the expected prediction error. Our proposed method uses Bayesian non-parametric modeling and cross-validation to estimate the absolute prediction error for use in evaluating and comparing candidate trial level general surrogates. Simulations show that our method performs well across a variety of scenarios. We use our method to evaluate and to compare candidate trial level general surrogates in several multi-national trials of a pentavalent rotavirus vaccine. We identify two immune measures that have potential value as trial level general surrogates and use the measures to predict efficacy in a trial with no clinical outcomes measured.

</details>

<details>

<summary>2015-07-07 15:59:03 - Smoothing and mean-covariance estimation of functional data with a Bayesian hierarchical model</summary>

- *Jingjing Yang, Hongxiao Zhu, Taeryon Choi, Dennis D. Cox*

- `1402.5723v5` - [abs](http://arxiv.org/abs/1402.5723v5) - [pdf](http://arxiv.org/pdf/1402.5723v5)

> Functional data, with basic observational units being functions (e.g., curves, surfaces) varying over a continuum, are frequently encountered in various applications. While many statistical tools have been developed for functional data analysis, the issue of smoothing all functional observations simultaneously is less studied. Existing methods often focus on smoothing each individual function separately, at the risk of removing important systematic patterns common across functions. We propose a nonparametric Bayesian approach to smooth all functional observations simultaneously and nonparametrically. In the proposed approach, we assume that the functional observations are independent Gaussian processes subject to a common level of measurement errors, enabling the borrowing of strength across all observations. Unlike most Gaussian process regression models that rely on pre-specified structures for the covariance kernel, we adopt a hierarchical framework by assuming a Gaussian process prior for the mean function and an Inverse-Wishart process prior for the covariance function. These prior assumptions induce an automatic mean-covariance estimation in the posterior inference in addition to the simultaneous smoothing of all observations. Such a hierarchical framework is flexible enough to incorporate functional data with different characteristics, including data measured on either common or uncommon grids, and data with either stationary or nonstationary covariance structures. Simulations and real data analysis demonstrate that, in comparison with alternative methods, the proposed Bayesian approach achieves better smoothing accuracy and comparable mean-covariance estimation results. Furthermore, it can successfully retain the systematic patterns in the functional observations that are usually neglected by the existing functional data analyses based on individual-curve smoothing.

</details>

<details>

<summary>2015-07-07 19:55:33 - On Joint Estimation of Gaussian Graphical Models for Spatial and Temporal Data</summary>

- *Zhixiang Lin, Tao Wang, Can Yang, Hongyu Zhao*

- `1507.01933v1` - [abs](http://arxiv.org/abs/1507.01933v1) - [pdf](http://arxiv.org/pdf/1507.01933v1)

> In this paper, we first propose a Bayesian neighborhood selection method to estimate Gaussian Graphical Models (GGMs). We show the graph selection consistency of this method in the sense that the posterior probability of the true model converges to one. When there are multiple groups of data available, instead of estimating the networks independently for each group, joint estimation of the networks may utilize the shared information among groups and lead to improved estimation for each individual network. Our method is extended to jointly estimate GGMs in multiple groups of data with complex structures, including spatial data, temporal data and data with both spatial and temporal structures. Markov random field (MRF) models are used to efficiently incorporate the complex data structures. We develop and implement an efficient algorithm for statistical inference that enables parallel computing. Simulation studies suggest that our approach achieves better accuracy in network estimation compared with methods not incorporating spatial and temporal dependencies when there are shared structures among the networks, and that it performs comparably well otherwise. Finally, we illustrate our method using the human brain gene expression microarray dataset, where the expression levels of genes are measured in different brain regions across multiple time periods.

</details>

<details>

<summary>2015-07-07 22:27:35 - Correlated Multiarmed Bandit Problem: Bayesian Algorithms and Regret Analysis</summary>

- *Vaibhav Srivastava, Paul Reverdy, Naomi Ehrich Leonard*

- `1507.01160v2` - [abs](http://arxiv.org/abs/1507.01160v2) - [pdf](http://arxiv.org/pdf/1507.01160v2)

> We consider the correlated multiarmed bandit (MAB) problem in which the rewards associated with each arm are modeled by a multivariate Gaussian random variable, and we investigate the influence of the assumptions in the Bayesian prior on the performance of the upper credible limit (UCL) algorithm and a new correlated UCL algorithm. We rigorously characterize the influence of accuracy, confidence, and correlation scale in the prior on the decision-making performance of the algorithms. Our results show how priors and correlation structure can be leveraged to improve performance.

</details>

<details>

<summary>2015-07-08 14:08:27 - Bayesian Additive Regression Trees using Bayesian Model Averaging</summary>

- *Belinda Hernández, Adrian E. Raftery, Stephen R. Pennington, Andrew C. Parnell*

- `1507.00181v2` - [abs](http://arxiv.org/abs/1507.00181v2) - [pdf](http://arxiv.org/pdf/1507.00181v2)

> Bayesian Additive Regression Trees (BART) is a statistical sum of trees model. It can be considered a Bayesian version of machine learning tree ensemble methods where the individual trees are the base learners. However for data sets where the number of variables $p$ is large (e.g. $p>5,000$) the algorithm can become prohibitively expensive, computationally.   Another method which is popular for high dimensional data is random forests, a machine learning algorithm which grows trees using a greedy search for the best split points. However, as it is not a statistical model, it cannot produce probabilistic estimates or predictions.   We propose an alternative algorithm for BART called BART-BMA, which uses Bayesian Model Averaging and a greedy search algorithm to produce a model which is much more efficient than BART for datasets with large $p$. BART-BMA incorporates elements of both BART and random forests to offer a model-based algorithm which can deal with high-dimensional data.   We have found that BART-BMA can be run in a reasonable time on a standard laptop for the "small $n$ large $p$" scenario which is common in many areas of bioinformatics. We showcase this method using simulated data and data from two real proteomic experiments; one to distinguish between patients with cardiovascular disease and controls and another to classify agressive from non-agressive prostate cancer. We compare our results to their main competitors.   Open source code written in R and Rcpp to run BART-BMA can be found at: https://github.com/BelindaHernandez/BART-BMA.git

</details>

<details>

<summary>2015-07-09 10:31:26 - A New Approach to Probabilistic Programming Inference</summary>

- *Frank Wood, Jan Willem van de Meent, Vikash Mansinghka*

- `1507.00996v2` - [abs](http://arxiv.org/abs/1507.00996v2) - [pdf](http://arxiv.org/pdf/1507.00996v2)

> We introduce and demonstrate a new approach to inference in expressive probabilistic programming languages based on particle Markov chain Monte Carlo. Our approach is simple to implement and easy to parallelize. It applies to Turing-complete probabilistic programming languages and supports accurate inference in models that make use of complex control flow, including stochastic recursion. It also includes primitives from Bayesian nonparametric statistics. Our experiments show that this approach can be more efficient than previously introduced single-site Metropolis-Hastings methods.

</details>

<details>

<summary>2015-07-09 14:08:45 - Lidar waveform based analysis of depth images constructed using sparse single-photon data</summary>

- *Yoann Altmann, Ximing Ren, Aongus McCarthy, Gerald S. Buller, Steve McLaughlin*

- `1507.02511v1` - [abs](http://arxiv.org/abs/1507.02511v1) - [pdf](http://arxiv.org/pdf/1507.02511v1)

> This paper presents a new Bayesian model and algorithm used for depth and intensity profiling using full waveforms from the time-correlated single photon counting (TCSPC) measurement in the limit of very low photon counts. The model proposed represents each Lidar waveform as a combination of a known impulse response, weighted by the target intensity, and an unknown constant background, corrupted by Poisson noise. Prior knowledge about the problem is embedded in a hierarchical model that describes the dependence structure between the model parameters and their constraints. In particular, a gamma Markov random field (MRF) is used to model the joint distribution of the target intensity, and a second MRF is used to model the distribution of the target depth, which are both expected to exhibit significant spatial correlations. An adaptive Markov chain Monte Carlo algorithm is then proposed to compute the Bayesian estimates of interest and perform Bayesian inference. This algorithm is equipped with a stochastic optimization adaptation mechanism that automatically adjusts the parameters of the MRFs by maximum marginal likelihood estimation. Finally, the benefits of the proposed methodology are demonstrated through a serie of experiments using real data.

</details>

<details>

<summary>2015-07-09 15:44:16 - Sequential Monte Carlo with Highly Informative Observations</summary>

- *Pierre Del Moral, Lawrence M. Murray*

- `1405.4081v2` - [abs](http://arxiv.org/abs/1405.4081v2) - [pdf](http://arxiv.org/pdf/1405.4081v2)

> We propose sequential Monte Carlo (SMC) methods for sampling the posterior distribution of state-space models under highly informative observation regimes, a situation in which standard SMC methods can perform poorly. A special case is simulating bridges between given initial and final values. The basic idea is to introduce a schedule of intermediate weighting and resampling times between observation times, which guide particles towards the final state. This can always be done for continuous-time models, and may be done for discrete-time models under sparse observation regimes; our main focus is on continuous-time diffusion processes. The methods are broadly applicable in that they support multivariate models with partial observation, do not require simulation of the backward transition (which is often unavailable), and, where possible, avoid pointwise evaluation of the forward transition. When simulating bridges, the last cannot be avoided entirely without concessions, and we suggest an epsilon-ball approach (reminiscent of Approximate Bayesian Computation) as a workaround. Compared to the bootstrap particle filter, the new methods deliver substantially reduced mean squared error in normalising constant estimates, even after accounting for execution time. The methods are demonstrated for state estimation with two toy examples, and for parameter estimation (within a particle marginal Metropolis--Hastings sampler) with three applied examples in econometrics, epidemiology and marine biogeochemistry.

</details>

<details>

<summary>2015-07-09 23:23:43 - A Nonseparable Multivariate Space-Time Model for Analyzing County-Level Heart Disease Death Rates by Race and Gender</summary>

- *Harrison Quick, Lance A. Waller, Michele Casper*

- `1507.02741v1` - [abs](http://arxiv.org/abs/1507.02741v1) - [pdf](http://arxiv.org/pdf/1507.02741v1)

> While death rates due to diseases of the heart have experienced a sharp decline over the past 50 years, these diseases continue to be the leading cause of death in the United States, and the rate of decline varies by geographic location, race, and gender. We look to harness the power of hierarchical Bayesian methods to obtain a clearer picture of the declines from county-level, temporally varying heart disease death rates for men and women of different races in the US. Specifically, we propose a nonseparable multivariate spatio-temporal Bayesian model which allows for group-specific temporal correlations and temporally-evolving covariance structures in the multivariate spatio-temporal component of the model. After verifying the effectiveness of our model via simulation, we apply our model to a dataset of over 200,000 county-level heart disease death rates. In addition to yielding a superior fit than other common approaches for handling such data, the richness of our model provides insight into racial, gender, and geographic disparities underlying heart disease death rates in the US which are not permitted by more restrictive models.

</details>

<details>

<summary>2015-07-10 09:43:31 - Variable Selection in Covariate Dependent Random Partition Models: an Application to Urinary Tract Infection</summary>

- *William Barcella, Maria De Iorio, Gianluca Baio, James Malone-Lee*

- `1501.03537v2` - [abs](http://arxiv.org/abs/1501.03537v2) - [pdf](http://arxiv.org/pdf/1501.03537v2)

> Lower urinary tract symptoms (LUTS) can indicate the presence of urinary tract infection (UTI), a condition that if it becomes chronic requires expensive and time consuming care as well as leading to reduced quality of life. Detecting the presence and gravity of an infection from the earliest symptoms is then highly valuable. Typically, white blood cell count (WBC) measured in a sample of urine is used to assess UTI. We consider clinical data from 1341 patients at their first visit in which UTI (i.e. WBC$\geq 1$) is diagnosed. In addition, for each patient, a clinical profile of 34 symptoms was recorded. In this paper we propose a Bayesian nonparametric regression model based on the Dirichlet Process (DP) prior aimed at providing the clinicians with a meaningful clustering of the patients based on both the WBC (response variable) and possible patterns within the symptoms profiles (covariates). This is achieved by assuming a probability model for the symptoms as well as for the response variable. To identify the symptoms most associated to UTI, we specify a spike and slab base measure for the regression coefficients: this induces dependence of symptoms selection on cluster assignment. Posterior inference is performed through Markov Chain Monte Carlo methods.

</details>

<details>

<summary>2015-07-11 17:30:04 - Joint estimation of quantile planes over arbitrary predictor spaces</summary>

- *Yun Yang, Surya Tokdar*

- `1507.03130v1` - [abs](http://arxiv.org/abs/1507.03130v1) - [pdf](http://arxiv.org/pdf/1507.03130v1)

> In spite of the recent surge of interest in quantile regression, joint estimation of linear quantile planes remains a great challenge in statistics and econometrics. We propose a novel parametrization that characterizes any collection of non-crossing quantile planes over arbitrarily shaped convex predictor domains in any dimension by means of unconstrained scalar, vector and function valued parameters. Statistical models based on this parametrization inherit a fast computation of the likelihood function, enabling penalized likelihood or Bayesian approaches to model fitting. We introduce a complete Bayesian methodology by using Gaussian process prior distributions on the function valued parameters and develop a robust and efficient Markov chain Monte Carlo parameter estimation. The resulting method is shown to offer posterior consistency under mild tail and regularity conditions. We present several illustrative examples where the new method is compared against existing approaches and is found to offer better accuracy, coverage and model fit.

</details>

<details>

<summary>2015-07-12 12:59:28 - Scalable Bayesian Inference for Excitatory Point Process Networks</summary>

- *Scott W. Linderman, Ryan P. Adams*

- `1507.03228v1` - [abs](http://arxiv.org/abs/1507.03228v1) - [pdf](http://arxiv.org/pdf/1507.03228v1)

> Networks capture our intuition about relationships in the world. They describe the friendships between Facebook users, interactions in financial markets, and synapses connecting neurons in the brain. These networks are richly structured with cliques of friends, sectors of stocks, and a smorgasbord of cell types that govern how neurons connect. Some networks, like social network friendships, can be directly observed, but in many cases we only have an indirect view of the network through the actions of its constituents and an understanding of how the network mediates that activity. In this work, we focus on the problem of latent network discovery in the case where the observable activity takes the form of a mutually-excitatory point process known as a Hawkes process. We build on previous work that has taken a Bayesian approach to this problem, specifying prior distributions over the latent network structure and a likelihood of observed activity given this network. We extend this work by proposing a discrete-time formulation and developing a computationally efficient stochastic variational inference (SVI) algorithm that allows us to scale the approach to long sequences of observations. We demonstrate our algorithm on the calcium imaging data used in the Chalearn neural connectomics challenge.

</details>

<details>

<summary>2015-07-12 21:54:01 - Distributed multi-object tracking over sensor networks: a random finite set approach</summary>

- *Claudio Fantacci*

- `1508.04158v1` - [abs](http://arxiv.org/abs/1508.04158v1) - [pdf](http://arxiv.org/pdf/1508.04158v1)

> The aim of the present dissertation is to address distributed tracking over a network of heterogeneous and geographically dispersed nodes (or agents) with sensing, communication and processing capabilities. Tracking is carried out in the Bayesian framework and its extension to a distributed context is made possible via an information-theoretic approach to data fusion which exploits consensus algorithms and the notion of Kullback-Leibler Average (KLA) of the Probability Density Functions (PDFs) to be fused. The first step toward distributed tracking considers a single moving object. Consensus takes place in each agent for spreading information over the network so that each node can track the object. To achieve such a goal, consensus is carried out on the local single-object posterior distribution, which is the result of local data processing, in the Bayesian setting, exploiting the last available measurement about the object. The next step is in the direction of distributed estimation of multiple moving objects. In order to model, in a rigorous and elegant way, a possibly time-varying number of objects present in a given area of interest, the Random Finite Set (RFS) formulation is adopted since it provides the notion of probability density for multi-object states that allows to directly extend existing tools in distributed estimation to multi-object tracking. The last theoretical step of the present dissertation is toward distributed filtering with the further requirement of unique object identities. To this end the labeled RFS framework is adopted as it provides a tractable approach to the multi-object Bayesian recursion. A generalization of the KLA to the labeled RFS framework, enables the development of novel consensus multi-object tracking filters which are fully distributed, scalable and computationally efficient.

</details>

<details>

<summary>2015-07-13 15:47:13 - Scalable Bayesian Optimization Using Deep Neural Networks</summary>

- *Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md. Mostofa Ali Patwary, Prabhat, Ryan P. Adams*

- `1502.05700v2` - [abs](http://arxiv.org/abs/1502.05700v2) - [pdf](http://arxiv.org/pdf/1502.05700v2)

> Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization.   In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.

</details>

<details>

<summary>2015-07-13 15:54:40 - Priors for Random Count Matrices Derived from a Family of Negative Binomial Processes</summary>

- *Mingyuan Zhou, Oscar Hernan Madrid Padilla, James G. Scott*

- `1404.3331v3` - [abs](http://arxiv.org/abs/1404.3331v3) - [pdf](http://arxiv.org/pdf/1404.3331v3)

> We define a family of probability distributions for random count matrices with a potentially unbounded number of rows and columns. The three distributions we consider are derived from the gamma-Poisson, gamma-negative binomial, and beta-negative binomial processes. Because the models lead to closed-form Gibbs sampling update equations, they are natural candidates for nonparametric Bayesian priors over count matrices. A key aspect of our analysis is the recognition that, although the random count matrices within the family are defined by a row-wise construction, their columns can be shown to be i.i.d. This fact is used to derive explicit formulas for drawing all the columns at once. Moreover, by analyzing these matrices' combinatorial structure, we describe how to sequentially construct a column-i.i.d. random count matrix one row at a time, and derive the predictive distribution of a new row count vector with previously unseen features. We describe the similarities and differences between the three priors, and argue that the greater flexibility of the gamma- and beta- negative binomial processes, especially their ability to model over-dispersed, heavy-tailed count data, makes these well suited to a wide variety of real-world applications. As an example of our framework, we construct a naive-Bayes text classifier to categorize a count vector to one of several existing random count matrices of different categories. The classifier supports an unbounded number of features, and unlike most existing methods, it does not require a predefined finite vocabulary to be shared by all the categories, and needs neither feature selection nor parameter tuning. Both the gamma- and beta- negative binomial processes are shown to significantly outperform the gamma-Poisson process for document categorization, with comparable performance to other state-of-the-art supervised text classification algorithms.

</details>

<details>

<summary>2015-07-14 18:57:12 - Cluster-Robust Bootstrap Inference in Quantile Regression Models</summary>

- *Andreas Hagemann*

- `1407.7166v4` - [abs](http://arxiv.org/abs/1407.7166v4) - [pdf](http://arxiv.org/pdf/1407.7166v4)

> In this paper I develop a wild bootstrap procedure for cluster-robust inference in linear quantile regression models. I show that the bootstrap leads to asymptotically valid inference on the entire quantile regression process in a setting with a large number of small, heterogeneous clusters and provides consistent estimates of the asymptotic covariance function of that process. The proposed bootstrap procedure is easy to implement and performs well even when the number of clusters is much smaller than the sample size. An application to Project STAR data is provided.

</details>

<details>

<summary>2015-07-14 19:11:34 - A Menu-Driven Software Package of Bayesian Nonparametric (and Parametric) Mixed Models for Regression Analysis and Density Estimation</summary>

- *George Karabatsos*

- `1506.05435v4` - [abs](http://arxiv.org/abs/1506.05435v4) - [pdf](http://arxiv.org/pdf/1506.05435v4)

> Most of applied statistics involves regression analysis of data. This paper presents a stand-alone and menu-driven software package, Bayesian Regression: Nonparametric and Parametric Models. Currently, this package gives the user a choice from 83 Bayesian models for data analysis. They include 47 Bayesian nonparametric (BNP) infinite-mixture regression models; 5 BNP infinite-mixture models for density estimation; and 31 normal random effects models (HLMs), including normal linear models. Each of the 78 regression models handles either a continuous, binary, or ordinal dependent variable, and can handle multi-level (grouped) data. All 83 Bayesian models can handle the analysis of weighted observations (e.g., for meta-analysis), and the analysis of left-censored, right-censored, and/or interval-censored data. Each BNP infinite-mixture model has a mixture distribution assigned one of various BNP prior distributions, including priors defined by either the Dirichlet process, Pitman-Yor process (including the normalized stable process), beta (two-parameter) process, normalized inverse-Gaussian process, geometric weights prior, dependent Dirichlet process, or the dependent infinite-probits prior. The software user can mouse-click to select a Bayesian model and perform data analysis via Markov chain Monte Carlo (MCMC) sampling. After the sampling completes, the software automatically opens text output that reports MCMC-based estimates of the model's posterior distribution and model predictive fit to the data. Additional text and/or graphical output can be generated by mouse-clicking other menu options. This includes output of MCMC convergence analyses, and estimates of the model's posterior predictive distribution, for selected functionals and values of covariates. The software, constructed from MATLAB Compiler, is illustrated through the BNP regression analysis of real data.

</details>

<details>

<summary>2015-07-15 10:14:49 - Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks</summary>

- *José Miguel Hernández-Lobato, Ryan P. Adams*

- `1502.05336v2` - [abs](http://arxiv.org/abs/1502.05336v2) - [pdf](http://arxiv.org/pdf/1502.05336v2)

> Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.

</details>

<details>

<summary>2015-07-15 10:34:52 - Predictive Entropy Search for Bayesian Optimization with Unknown Constraints</summary>

- *José Miguel Hernández-Lobato, Michael A. Gelbart, Matthew W. Hoffman, Ryan P. Adams, Zoubin Ghahramani*

- `1502.05312v2` - [abs](http://arxiv.org/abs/1502.05312v2) - [pdf](http://arxiv.org/pdf/1502.05312v2)

> Unknown constraints arise in many types of expensive black-box optimization problems. Several methods have been proposed recently for performing Bayesian optimization with constraints, based on the expected improvement (EI) heuristic. However, EI can lead to pathologies when used with constraints. For example, in the case of decoupled constraints---i.e., when one can independently evaluate the objective or the constraints---EI can encounter a pathology that prevents exploration. Additionally, computing EI requires a current best solution, which may not exist if none of the data collected so far satisfy the constraints. By contrast, information-based approaches do not suffer from these failure modes. In this paper, we present a new information-based method called Predictive Entropy Search with Constraints (PESC). We analyze the performance of PESC and show that it compares favorably to EI-based approaches on synthetic and benchmark problems, as well as several real-world examples. We demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization.

</details>

<details>

<summary>2015-07-15 12:35:15 - A Lynden-Bell integral estimator for extremes of randomly truncated data</summary>

- *Julien Worms, Rym Worms*

- `1507.04189v1` - [abs](http://arxiv.org/abs/1507.04189v1) - [pdf](http://arxiv.org/pdf/1507.04189v1)

> This work deals with the estimation of the extreme value index and extreme quantiles for heavy tailed data,randomly right truncated by another heavy tailed variable. Under mild assumptions and the condition thatthe truncated variable is less heavy-tailed than the truncating variable, asymptotic normality is proved for bothestimators. The proposed estimator of the extreme value index is an adaptation of the Hill estimator, in thenatural form of a Lynden-Bell integral. Simulations illustrate the quality of the estimators under a variety ofsituations.

</details>

<details>

<summary>2015-07-15 13:04:29 - Bayesian Modeling with Gaussian Processes using the GPstuff Toolbox</summary>

- *Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari*

- `1206.5754v6` - [abs](http://arxiv.org/abs/1206.5754v6) - [pdf](http://arxiv.org/pdf/1206.5754v6)

> Gaussian processes (GP) are powerful tools for probabilistic modeling purposes. They can be used to define prior distributions over latent functions in hierarchical Bayesian models. The prior over functions is defined implicitly by the mean and covariance function, which determine the smoothness and variability of the function. The inference can then be conducted directly in the function space by evaluating or approximating the posterior process. Despite their attractive theoretical properties GPs provide practical challenges in their implementation. GPstuff is a versatile collection of computational tools for GP models compatible with Linux and Windows MATLAB and Octave. It includes, among others, various inference methods, sparse approximations and tools for model assessment. In this work, we review these tools and demonstrate the use of GPstuff in several models.

</details>

<details>

<summary>2015-07-15 13:06:49 - Evaluating the Causal Effect of University Grants on Student Dropout: Evidence from a Regression Discontinuity Design Using Principal Stratification</summary>

- *Fan Li, Alessandra Mattei, Fabrizia Mealli*

- `1507.04199v1` - [abs](http://arxiv.org/abs/1507.04199v1) - [pdf](http://arxiv.org/pdf/1507.04199v1)

> Regression discontinuity (RD) designs are often interpreted as local randomized experiments: a RD design can be considered as a randomized experiment for units with a realized value of a so-called forcing variable falling around a pre-fixed threshold. Motivated by the evaluation of Italian university grants, we consider a fuzzy RD design where the receipt of the treatment is based on both eligibility criteria and a voluntary application status. Resting on the fact that grant application and grant receipt statuses are post-assignment (post-eligibility) intermediate variables, we use the principal stratification framework to define causal estimands within the Rubin Causal Model. We propose a probabilistic formulation of the assignment mechanism underlying RD designs, by re-formulating the Stable Unit Treatment Value Assumption (SUTVA) and making an explicit local overlap assumption for a subpopulation around the threshold. A local randomization assumption is invoked instead of more standard continuity assumptions. We also develop a model-based Bayesian approach to select the target subpopulation(s) with adjustment for multiple comparisons, and to draw inference for the target causal estimands in this framework. Applying the method to the data from two Italian universities, we find evidence that university grants are effective in preventing students from low-income families from dropping out of higher education.

</details>

<details>

<summary>2015-07-16 09:37:32 - On the Convergence of Stochastic Variational Inference in Bayesian Networks</summary>

- *Ulrich Paquet*

- `1507.04505v1` - [abs](http://arxiv.org/abs/1507.04505v1) - [pdf](http://arxiv.org/pdf/1507.04505v1)

> We highlight a pitfall when applying stochastic variational inference to general Bayesian networks. For global random variables approximated by an exponential family distribution, natural gradient steps, commonly starting from a unit length step size, are averaged to convergence. This useful insight into the scaling of initial step sizes is lost when the approximation factorizes across a general Bayesian network, and care must be taken to ensure practical convergence. We experimentally investigate how much of the baby (well-scaled steps) is thrown out with the bath water (exact gradients).

</details>

<details>

<summary>2015-07-16 11:18:01 - A priori truncation method for posterior sampling from homogeneous normalized completely random measure mixture models</summary>

- *Raffaele Argiento, Ilaria Bianchini, Alessandra Guglielmi*

- `1507.04528v1` - [abs](http://arxiv.org/abs/1507.04528v1) - [pdf](http://arxiv.org/pdf/1507.04528v1)

> This paper adopts a Bayesian nonparametric mixture model where the mixing distribution belongs to the wide class of normalized homogeneous completely random measures. We propose a truncation method for the mixing distribution by discarding the weights of the unnormalized measure smaller than a threshold. We prove convergence in law of our approximation, provide some theoretical properties and characterize its posterior distribution so that a blocked Gibbs sampler is devised. The versatility of the approximation is illustrated by two different applications. In the first the normalized Bessel random measure, encompassing the Dirichlet process, is introduced; goodness of fit indexes show its good performances as mixing measure for density estimation. The second describes how to incorporate covariates in the support of the normalized measure, leading to a linear dependent model for regression and clustering.

</details>

<details>

<summary>2015-07-16 12:54:56 - Approximate Maximum Likelihood Estimation</summary>

- *Johanna Bertl, Gregory Ewing, Carolin Kosiol, Andreas Futschik*

- `1507.04553v1` - [abs](http://arxiv.org/abs/1507.04553v1) - [pdf](http://arxiv.org/pdf/1507.04553v1)

> In recent years, methods of approximate parameter estimation have attracted considerable interest in complex problems where exact likelihoods are hard to obtain. In their most basic form, Bayesian methods such as Approximate Bayesian Computation (ABC) involve sampling from the parameter space and keeping those parameters that produce data that fit sufficiently well to the actually observed data. Exploring the whole parameter space, however, makes this approach inefficient in high dimensional problems. This led to the proposal of more sophisticated iterative methods of inference such as particle filters.   Here, we propose an alternative approach that is based on stochastic gradient methods and applicable both in a frequentist and a Bayesian setting. By moving along a simulated gradient, the algorithm produces a sequence of estimates that will eventually converge either to the maximum likelihood estimate or to the maximum of the posterior distribution, in each case under a set of observed summary statistics. To avoid reaching only a local maximum, we propose to run the algorithm from a set of random starting values.   As good tuning of the algorithm is important, we explored several tuning strategies, and propose a set of guidelines that worked best in our simulations. We investigate the performance of our approach in simulation studies, and also apply the algorithm to two models with intractable likelihood functions. First, we present an application to inference in the context of queuing systems. We also re-analyze population genetic data and estimate parameters describing the demographic history of Sumatran and Bornean orang-utan populations.

</details>

<details>

<summary>2015-07-16 19:07:43 - Genome-wide modelling of transcription kinetics reveals patterns of RNA production delays</summary>

- *Antti Honkela, Jaakko Peltonen, Hande Topa, Iryna Charapitsa, Filomena Matarese, Korbinian Grote, Hendrik G. Stunnenberg, George Reid, Neil D. Lawrence, Magnus Rattray*

- `1503.01081v2` - [abs](http://arxiv.org/abs/1503.01081v2) - [pdf](http://arxiv.org/pdf/1503.01081v2)

> Genes with similar transcriptional activation kinetics can display very different temporal mRNA profiles due to differences in transcription time, degradation rate and RNA processing kinetics. Recent studies have shown that a splicing-associated RNA production delay can be significant. We introduce a joint model of transcriptional activation and mRNA accumulation which can be used for inference of transcription rate, RNA production delay and degradation rate given genome-wide data from high-throughput sequencing time course experiments. We combine a mechanistic differential equation model with a non-parametric statistical modelling approach allowing us to capture a broad range of activation kinetics, and use Bayesian parameter estimation to quantify the uncertainty in the estimates of the kinetic parameters. We apply the model to data from estrogen receptor (ER-{\alpha}) activation in the MCF-7 breast cancer cell line. We use RNA polymerase II (pol-II) ChIP-Seq time course data to characterise transcriptional activation and mRNA-Seq time course data to quantify mature transcripts. We find that 11% of genes with a good signal in the data display a delay of more than 20 minutes between completing transcription and mature mRNA production. The genes displaying these long delays are significantly more likely to be short. We also find a statistical association between high delay and late intron retention in pre-mRNA data, indicating significant splicing-associated production delays in many genes.

</details>

<details>

<summary>2015-07-17 19:22:41 - Riemann Manifold Langevin Methods on Stochastic Volatility Estimation</summary>

- *Mauricio Zevallos, Loretta Gasco, Ricardo Ehlers*

- `1507.05079v1` - [abs](http://arxiv.org/abs/1507.05079v1) - [pdf](http://arxiv.org/pdf/1507.05079v1)

> In this paper we perform Bayesian estimation of stochastic volatility models with heavy tail distributions using Metropolis adjusted Langevin (MALA) and Riemman manifold Langevin (MMALA) methods. We provide analytical expressions for the application of these methods, assess the performance of these methodologies in simulated data and illustrate their use on two financial time series data sets.

</details>

<details>

<summary>2015-07-17 19:57:38 - Type I and Type II Bayesian Methods for Sparse Signal Recovery using Scale Mixtures</summary>

- *Ritwik Giri, Bhaskar D. Rao*

- `1507.05087v1` - [abs](http://arxiv.org/abs/1507.05087v1) - [pdf](http://arxiv.org/pdf/1507.05087v1)

> In this paper, we propose a generalized scale mixture family of distributions, namely the Power Exponential Scale Mixture (PESM) family, to model the sparsity inducing priors currently in use for sparse signal recovery (SSR). We show that the successful and popular methods such as LASSO, Reweighted $\ell_1$ and Reweighted $\ell_2$ methods can be formulated in an unified manner in a maximum a posteriori (MAP) or Type I Bayesian framework using an appropriate member of the PESM family as the sparsity inducing prior. In addition, exploiting the natural hierarchical framework induced by the PESM family, we utilize these priors in a Type II framework and develop the corresponding EM based estimation algorithms. Some insight into the differences between Type I and Type II methods is provided and of particular interest in the algorithmic development is the Type II variant of the popular and successful reweighted $\ell_1$ method. Extensive empirical results are provided and they show that the Type II methods exhibit better support recovery than the corresponding Type I methods.

</details>

<details>

<summary>2015-07-17 21:03:53 - Fast Approximate Bayesian Computation for Estimating Parameters in Differential Equations</summary>

- *Sanmitra Ghosh, Srinandan Dasmahapatra, Koushik Maharatna*

- `1507.05117v1` - [abs](http://arxiv.org/abs/1507.05117v1) - [pdf](http://arxiv.org/pdf/1507.05117v1)

> Approximate Bayesian computation (ABC) using a sequential Monte Carlo method provides a comprehensive platform for parameter estimation, model selection and sensitivity analysis in differential equations. However, this method, like other Monte Carlo methods, incurs a significant computational cost as it requires explicit numerical integration of differential equations to carry out inference. In this paper we propose a novel method for circumventing the requirement of explicit integration by using derivatives of Gaussian processes to smooth the observations from which parameters are estimated. We evaluate our methods using synthetic data generated from model biological systems described by ordinary and delay differential equations. Upon comparing the performance of our method to existing ABC techniques, we demonstrate that it produces comparably reliable parameter estimates at a significantly reduced execution time.

</details>

<details>

<summary>2015-07-19 20:30:10 - Fast Adaptive Weight Noise</summary>

- *Justin Bayer, Maximilian Karl, Daniela Korhammer, Patrick van der Smagt*

- `1507.05331v1` - [abs](http://arxiv.org/abs/1507.05331v1) - [pdf](http://arxiv.org/pdf/1507.05331v1)

> Marginalising out uncertain quantities within the internal representations or parameters of neural networks is of central importance for a wide range of learning techniques, such as empirical, variational or full Bayesian methods. We set out to generalise fast dropout (Wang & Manning, 2013) to cover a wider variety of noise processes in neural networks. This leads to an efficient calculation of the marginal likelihood and predictive distribution which evades sampling and the consequential increase in training time due to highly variant gradient estimates. This allows us to approximate variational Bayes for the parameters of feed-forward neural networks. Inspired by the minimum description length principle, we also propose and experimentally verify the direct optimisation of the regularised predictive distribution. The methods yield results competitive with previous neural network based approaches and Gaussian processes on a wide range of regression tasks.

</details>

<details>

<summary>2015-07-21 18:19:01 - Principal causal effect identification and surrogate endpoint evaluation by multiple trials</summary>

- *Zhichao Jiang, Peng Ding, Zhi Geng*

- `1507.05935v1` - [abs](http://arxiv.org/abs/1507.05935v1) - [pdf](http://arxiv.org/pdf/1507.05935v1)

> Principal stratification is a causal framework to analyze randomized experiments with a post-treatment variable between the treatment and endpoint variables. Because the principal strata defined by the potential outcomes of the post-treatment variable are not observable, we generally cannot identify the causal effects within principal strata. Motivated by a real data set of phase III adjuvant colon clinical trials, we propose approaches to identifying and estimating the principal causal effects via multiple trials. For the identifiability, we remove the commonly-used exclusion restriction assumption by stipulating that the principal causal effects are homogeneous across these trials. To remove another commonly-used monotonicity assumption, we give a necessary condition for the local identifiability, which requires at least three trials. Applying our approaches to the data from adjuvant colon clinical trials, we find that the commonly-used monotonicity assumption is untenable, and disease-free survival with three-year follow-up is a valid surrogate endpoint for overall survival with five-year follow-up, which satisfies both the causal necessity and the causal sufficiency. We also propose a sensitivity analysis approach based on Bayesian hierarchical models to investigate the impact of the deviation from the homogeneity assumption.

</details>

<details>

<summary>2015-07-21 20:41:38 - The Population Posterior and Bayesian Inference on Streams</summary>

- *James McInerney, Rajesh Ranganath, David M. Blei*

- `1507.05253v2` - [abs](http://arxiv.org/abs/1507.05253v2) - [pdf](http://arxiv.org/pdf/1507.05253v2)

> Many modern data analysis problems involve inferences from streaming data. However, streaming data is not easily amenable to the standard probabilistic modeling approaches, which assume that we condition on finite data. We develop population variational Bayes, a new approach for using Bayesian modeling to analyze streams of data. It approximates a new type of distribution, the population posterior, which combines the notion of a population distribution of the data with Bayesian inference in a probabilistic model. We study our method with latent Dirichlet allocation and Dirichlet process mixtures on several large-scale data sets.

</details>

<details>

<summary>2015-07-22 15:29:56 - Bayesian Nonparametric Dynamic State Space Modeling with Circular Latent States</summary>

- *Satyaki Mazumder, Sourabh Bhattacharya*

- `1408.3041v2` - [abs](http://arxiv.org/abs/1408.3041v2) - [pdf](http://arxiv.org/pdf/1408.3041v2)

> State space models are well-known for their versatility in modeling dynamic systems that arise in various scientific disciplines. Although parametric state space models are well studied, nonparametric approaches are much less explored in comparison. In this article we propose a novel Bayesian nonparametric approach to state space modeling assuming that both the observational and evolutionary functions are unknown and are varying with time; crucially, we assume that the unknown evolutionary equation describes dynamic evolution of some latent circular random variable.   Based on appropriate kernel convolution of the standard Wiener process we model the time-varying observational and evolutionary functions as suitable Gaussian processes that take both linear and circular variables as arguments. Additionally, for the time-varying evolutionary function, we wrap the Gaussian process thus constructed around the unit circle to form an appropriate circular Gaussian process. We show that our process thus created satisfies desirable properties.   For the purpose of inference we develop an MCMC based methodology combining Gibbs sampling and Metropolis-Hastings algorithms. Applications to a simulated data set, a real wind speed data set and a real ozone data set demonstrated quite encouraging performances of our model and methodologies.

</details>

<details>

<summary>2015-07-23 08:39:34 - Arbitrariness of peer review: A Bayesian analysis of the NIPS experiment</summary>

- *Olivier Francois*

- `1507.06411v1` - [abs](http://arxiv.org/abs/1507.06411v1) - [pdf](http://arxiv.org/pdf/1507.06411v1)

> The principle of peer review is central to the evaluation of research, by ensuring that only high-quality items are funded or published. But peer review has also received criticism, as the selection of reviewers may introduce biases in the system. In 2014, the organizers of the ``Neural Information Processing Systems\rq\rq{} conference conducted an experiment in which $10\%$ of submitted manuscripts (166 items) went through the review process twice. Arbitrariness was measured as the conditional probability for an accepted submission to get rejected if examined by the second committee. This number was equal to $60\%$, for a total acceptance rate equal to $22.5\%$. Here we present a Bayesian analysis of those two numbers, by introducing a hidden parameter which measures the probability that a submission meets basic quality criteria. The standard quality criteria usually include novelty, clarity, reproducibility, correctness and no form of misconduct, and are met by a large proportions of submitted items. The Bayesian estimate for the hidden parameter was equal to $56\%$ ($95\%$CI: $ I = (0.34, 0.83)$), and had a clear interpretation. The result suggested the total acceptance rate should be increased in order to decrease arbitrariness estimates in future review processes.

</details>

<details>

<summary>2015-07-23 09:06:04 - Emulation of Higher-Order Tensors in Manifold Monte Carlo Methods for Bayesian Inverse Problems</summary>

- *Shiwei Lan, Tan Bui-Thanh, Mike Christie, Mark Girolami*

- `1507.06244v2` - [abs](http://arxiv.org/abs/1507.06244v2) - [pdf](http://arxiv.org/pdf/1507.06244v2)

> The Bayesian approach to Inverse Problems relies predominantly on Markov Chain Monte Carlo methods for posterior inference. The typical nonlinear concentration of posterior measure observed in many such Inverse Problems presents severe challenges to existing simulation based inference methods. Motivated by these challenges the exploitation of local geometric information in the form of covariant gradients, metric tensors, Levi-Civita connections, and local geodesic flows, have been introduced to more effectively locally explore the configuration space of the posterior measure. However, obtaining such geometric quantities usually requires extensive computational effort and despite their effectiveness affect the applicability of these geometrically-based Monte Carlo methods. In this paper we explore one way to address this issue by the construction of an emulator of the model from which all geometric objects can be obtained in a much more computationally feasible manner. The main concept is to approximate the geometric quantities using a Gaussian Process emulator which is conditioned on a carefully chosen design set of configuration points, which also determines the quality of the emulator. To this end we propose the use of statistical experiment design methods to refine a potentially arbitrarily initialized design online without destroying the convergence of the resulting Markov chain to the desired invariant measure. The practical examples considered in this paper provide a demonstration of the significant improvement possible in terms of computational loading suggesting this is a promising avenue of further development.

</details>

<details>

<summary>2015-07-23 17:32:49 - Multi-scale exploration of convex functions and bandit convex optimization</summary>

- *Sébastien Bubeck, Ronen Eldan*

- `1507.06580v1` - [abs](http://arxiv.org/abs/1507.06580v1) - [pdf](http://arxiv.org/pdf/1507.06580v1)

> We construct a new map from a convex function to a distribution on its domain, with the property that this distribution is a multi-scale exploration of the function. We use this map to solve a decade-old open problem in adversarial bandit convex optimization by showing that the minimax regret for this problem is $\tilde{O}(\mathrm{poly}(n) \sqrt{T})$, where $n$ is the dimension and $T$ the number of rounds. This bound is obtained by studying the dual Bayesian maximin regret via the information ratio analysis of Russo and Van Roy, and then using the multi-scale exploration to solve the Bayesian problem.

</details>

<details>

<summary>2015-07-24 00:33:04 - Nonparametric Bayesian Regression on Manifolds via Brownian Motion</summary>

- *Xu Wang, Gilad Lerman*

- `1507.06710v1` - [abs](http://arxiv.org/abs/1507.06710v1) - [pdf](http://arxiv.org/pdf/1507.06710v1)

> This paper proposes a novel framework for manifold-valued regression and establishes its consistency as well as its contraction rate. It assumes a predictor with values in the interval $[0,1]$ and response with values in a compact Riemannian manifold $M$. This setting is useful for applications such as modeling dynamic scenes or shape deformations, where the visual scene or the deformed objects can be modeled by a manifold. The proposed framework is nonparametric and uses the heat kernel (and its associated Brownian motion) on manifolds as an averaging procedure. It directly generalizes the use of the Gaussian kernel (as a natural model of additive noise) in vector-valued regression problems. In order to avoid explicit dependence on estimates of the heat kernel, we follow a Bayesian setting, where Brownian motion on $M$ induces a prior distribution on the space of continuous functions $C([0,1], M)$. For the case of discretized Brownian motion, we establish the consistency of the posterior distribution in terms of the $L_{q}$ distances for any $1 \leq q < \infty$. Most importantly, we establish contraction rate of order $O(n^{-1/4+\epsilon})$ for any fixed $\epsilon>0$, where $n$ is the number of observations. For the continuous Brownian motion we establish weak consistency.

</details>

<details>

<summary>2015-07-25 05:42:01 - The Extremal Index and the Maximum of a Dependent Stationary Pulse Load Process Observed above a High Threshold</summary>

- *Baidurya Bhattacharya*

- `1507.07070v1` - [abs](http://arxiv.org/abs/1507.07070v1) - [pdf](http://arxiv.org/pdf/1507.07070v1)

> Observing a load process above high thresholds, modeling it as a pulse process with random occurrence times and magnitudes, and extrapolating life-time maximum or design loads from the data is a common task in structural reliability analyses. In this paper, we consider a stationary live load sequence that arrive according to a dependent point process and allow for a weakened mixing-type dependence in the load pulse magnitudes that asymptotically decreases to zero with increasing separation in the sequence. Inclusion of dependence in the model eliminates the unnecessary conservatism introduced by the i.i.d. (independent and identically distributed) assumption often made in determining maximum live load distribution. The scale of fluctuation of the loading process is used to identify clusters of exceedances above high thresholds which in turn is used to estimate the extremal index of the process. A Bayesian updating of the empirical distribution function, derived from the distribution of order statistics in a dependent stationary series, is performed. The pulse arrival instants are modeled as a Cox process goverened by a stationary lognormal intensity. An illustrative example utilizes in-service peak strain data from ambient traffic collected on a high volume highway bridge, and analyzes the asymptotic behavior of the maximum load.

</details>

<details>

<summary>2015-07-26 19:43:36 - Estimator Selection: End-Performance Metric Aspects</summary>

- *Dimitrios Katselis, Cristian R. Rojas, Carolyn L. Beck*

- `1507.07238v1` - [abs](http://arxiv.org/abs/1507.07238v1) - [pdf](http://arxiv.org/pdf/1507.07238v1)

> Recently, a framework for application-oriented optimal experiment design has been introduced. In this context, the distance of the estimated system from the true one is measured in terms of a particular end-performance metric. This treatment leads to superior unknown system estimates to classical experiment designs based on usual pointwise functional distances of the estimated system from the true one. The separation of the system estimator from the experiment design is done within this new framework by choosing and fixing the estimation method to either a maximum likelihood (ML) approach or a Bayesian estimator such as the minimum mean square error (MMSE). Since the MMSE estimator delivers a system estimate with lower mean square error (MSE) than the ML estimator for finite-length experiments, it is usually considered the best choice in practice in signal processing and control applications. Within the application-oriented framework a related meaningful question is: Are there end-performance metrics for which the ML estimator outperforms the MMSE when the experiment is finite-length? In this paper, we affirmatively answer this question based on a simple linear Gaussian regression example.

</details>

<details>

<summary>2015-07-27 07:08:35 - The expected demise of the Bayes factor</summary>

- *Christian P. Robert*

- `1506.08292v2` - [abs](http://arxiv.org/abs/1506.08292v2) - [pdf](http://arxiv.org/pdf/1506.08292v2)

> This note is a discussion commenting on the paper by Ly et al. on "Harold Jeffreys's Default Bayes Factor Hypothesis Tests: Explanation, Extension, and Application in Psychology" and on the perceived shortcomings of the classical Bayesian approach to testing, while reporting on an alternative approach advanced by Kamary, Mengersen, Robert and Rousseau (2014. arxiv:1412.2044) as a solution to this quintessential inference problem.

</details>

<details>

<summary>2015-07-27 07:53:17 - Bayesian analysis of multivariate stable distributions using one-dimensional projections</summary>

- *Mike G. Tsionas*

- `1507.07323v1` - [abs](http://arxiv.org/abs/1507.07323v1) - [pdf](http://arxiv.org/pdf/1507.07323v1)

> In this paper we take up Bayesian inference in general multivariate stable distributions. We exploit the representation of Matsui and Takemura (2009) for univariate projections, and the representation of the distributions in terms of their spectral measure. We present efficient MCMC schemes to perform the computations when the spectral measure is approximated discretely or, as we propose, by a normal distribution. Appropriate latent variables are introduced to implement MCMC. In relation to the discrete approximation, we propose efficient computational schemes based on the characteristic function.

</details>

<details>

<summary>2015-07-27 10:56:46 - Variational Bayesian strategies for high-dimensional, stochastic design problems</summary>

- *Phaedon-Stelios Koutsourelakis*

- `1507.06759v2` - [abs](http://arxiv.org/abs/1507.06759v2) - [pdf](http://arxiv.org/pdf/1507.06759v2)

> This paper is concerned with a lesser-studied problem in the context of model-based, uncertainty quantification (UQ), that of optimization/design/control under uncertainty. The solution of such problems is hindered not only by the usual difficulties encountered in UQ tasks (e.g. the high computational cost of each forward simulation, the large number of random variables) but also by the need to solve a nonlinear optimization problem involving large numbers of design variables and potentially constraints. We propose a framework that is suitable for a large class of such problems and is based on the idea of recasting them as probabilistic inference tasks. To that end, we propose a Variational Bayesian (VB) formulation and an iterative VB-Expectation-Maximization scheme that is also capable of identifying a low-dimensional set of directions in the design space, along which, the objective exhibits the largest sensitivity.   We demonstrate the validity of the proposed approach in the context of two numerical examples involving $\mathcal{O}(10^3)$ random and design variables. In all cases considered the cost of the computations in terms of calls to the forward model was of the order $\mathcal{O}(10^2)$. The accuracy of the approximations provided is assessed by appropriate information-theoretic metrics.

</details>

<details>

<summary>2015-07-27 11:54:09 - Adaptive MCMC with online relabeling</summary>

- *Rémi Bardenet, Olivier Cappé, Gersende Fort, Balázs Kégl*

- `1210.2601v3` - [abs](http://arxiv.org/abs/1210.2601v3) - [pdf](http://arxiv.org/pdf/1210.2601v3)

> When targeting a distribution that is artificially invariant under some permutations, Markov chain Monte Carlo (MCMC) algorithms face the label-switching problem, rendering marginal inference particularly cumbersome. Such a situation arises, for example, in the Bayesian analysis of finite mixture models. Adaptive MCMC algorithms such as adaptive Metropolis (AM), which self-calibrates its proposal distribution using an online estimate of the covariance matrix of the target, are no exception. To address the label-switching issue, relabeling algorithms associate a permutation to each MCMC sample, trying to obtain reasonable marginals. In the case of adaptive Metropolis (Bernoulli 7 (2001) 223-242), an online relabeling strategy is required. This paper is devoted to the AMOR algorithm, a provably consistent variant of AM that can cope with the label-switching problem. The idea is to nest relabeling steps within the MCMC algorithm based on the estimation of a single covariance matrix that is used both for adapting the covariance of the proposal distribution in the Metropolis algorithm step and for online relabeling. We compare the behavior of AMOR to similar relabeling methods. In the case of compactly supported target distributions, we prove a strong law of large numbers for AMOR and its ergodicity. These are the first results on the consistency of an online relabeling algorithm to our knowledge. The proof underlines latent relations between relabeling and vector quantization.

</details>

<details>

<summary>2015-07-27 13:30:45 - Bayesian nonparametric dependent model for partially replicated data: the influence of fuel spills on species diversity</summary>

- *Julyan Arbel, Kerrie Mengersen, Judith Rousseau*

- `1402.3093v2` - [abs](http://arxiv.org/abs/1402.3093v2) - [pdf](http://arxiv.org/pdf/1402.3093v2)

> We introduce a dependent Bayesian nonparametric model for the probabilistic modeling of membership of subgroups in a community based on partially replicated data. The focus here is on species-by-site data, i.e. community data where observations at different sites are classified in distinct species. Our aim is to study the impact of additional covariates, for instance environmental variables, on the data structure, and in particular on the community diversity. To that purpose, we introduce dependence a priori across the covariates, and show that it improves posterior inference. We use a dependent version of the Griffiths-Engen-McCloskey distribution defined via the stick-breaking construction. This distribution is obtained by transforming a Gaussian process whose covariance function controls the desired dependence. The resulting posterior distribution is sampled by Markov chain Monte Carlo. We illustrate the application of our model to a soil microbial dataset acquired across a hydrocarbon contamination gradient at the site of a fuel spill in Antarctica. This method allows for inference on a number of quantities of interest in ecotoxicology, such as diversity or effective concentrations, and is broadly applicable to the general problem of communities response to environmental variables.

</details>

<details>

<summary>2015-07-27 14:04:29 - Bivariate ensemble model output statistics approach for joint forecasting of wind speed and temperature</summary>

- *Sándor Baran, Annette Möller*

- `1507.03479v2` - [abs](http://arxiv.org/abs/1507.03479v2) - [pdf](http://arxiv.org/pdf/1507.03479v2)

> Forecast ensembles are typically employed to account for prediction uncertainties in numerical weather prediction models. However, ensembles often exhibit biases and dispersion errors, thus they require statistical post-processing to improve their predictive performance. Two popular univariate post-processing models are the Bayesian model averaging (BMA) and the ensemble model output statistics (EMOS).   In the last few years increased interest has emerged in developing multivariate post-processing models, incorporating dependencies between weather quantities, such as for example a bivariate distribution for wind vectors or even a more general setting allowing to combine any types of weather variables.   In line with a recently proposed approach to model temperature and wind speed jointly by a bivariate BMA model, this paper introduces a bivariate EMOS model for these weather quantities based on a truncated normal distribution.   The bivariate EMOS model is applied to temperature and wind speed forecasts of the eight-member University of Washington mesoscale ensemble and of the eleven-member ALADIN-HUNEPS ensemble of the Hungarian Meteorological Service and its predictive performance is compared to the performance of the bivariate BMA model and a multivariate Gaussian copula approach, post-processing the margins with univariate EMOS. While the predictive skills of the compared methods are similar, the bivariate EMOS model requires considerably lower computation times than the bivariate BMA method.

</details>

<details>

<summary>2015-07-27 20:17:44 - Variational Inference for Gaussian Process Modulated Poisson Processes</summary>

- *Chris Lloyd, Tom Gunter, Michael A. Osborne, Stephen J. Roberts*

- `1411.0254v3` - [abs](http://arxiv.org/abs/1411.0254v3) - [pdf](http://arxiv.org/pdf/1411.0254v3)

> We present the first fully variational Bayesian inference scheme for continuous Gaussian-process-modulated Poisson processes. Such point processes are used in a variety of domains, including neuroscience, geo-statistics and astronomy, but their use is hindered by the computational cost of existing inference schemes. Our scheme: requires no discretisation of the domain; scales linearly in the number of observed events; and is many orders of magnitude faster than previous sampling based approaches. The resulting algorithm is shown to outperform standard methods on synthetic examples, coal mining disaster data and in the prediction of Malaria incidences in Kenya.

</details>

<details>

<summary>2015-07-28 13:14:30 - Local bilinear multiple-output quantile/depth regression</summary>

- *Marc Hallin, Zudi Lu, Davy Paindaveine, Miroslav Šiman*

- `1507.07754v1` - [abs](http://arxiv.org/abs/1507.07754v1) - [pdf](http://arxiv.org/pdf/1507.07754v1)

> A new quantile regression concept, based on a directional version of Koenker and Bassett's traditional single-output one, has been introduced in [Ann. Statist. (2010) 38 635-669] for multiple-output location/linear regression problems. The polyhedral contours provided by the empirical counterpart of that concept, however, cannot adapt to unknown nonlinear and/or heteroskedastic dependencies. This paper therefore introduces local constant and local linear (actually, bilinear) versions of those contours, which both allow to asymptotically recover the conditional halfspace depth contours that completely characterize the response's conditional distributions. Bahadur representation and asymptotic normality results are established. Illustrations are provided both on simulated and real data.

</details>

<details>

<summary>2015-07-28 15:35:54 - An Analytically Tractable Bayesian Approximation to Optimal Point Process Filtering</summary>

- *Yuval Harel, Ron Meir, Manfred Opper*

- `1507.07813v1` - [abs](http://arxiv.org/abs/1507.07813v1) - [pdf](http://arxiv.org/pdf/1507.07813v1)

> The process of dynamic state estimation (filtering) based on point process observations is in general intractable. Numerical sampling techniques are often practically useful, but lead to limited conceptual insight about optimal encoding/decoding strategies, which are of significant relevance to Computational Neuroscience. We develop an analytically tractable Bayesian approximation to optimal filtering based on point process observations, which allows us to introduce distributional assumptions about sensory cell properties, that greatly facilitates the analysis of optimal encoding in situations deviating from common assumptions of uniform coding. The analytic framework leads to insights which are difficult to obtain from numerical algorithms, and is consistent with experiments about the distribution of tuning curve centers. Interestingly, we find that the information gained from the absence of spikes may be crucial to performance.

</details>

<details>

<summary>2015-07-28 20:08:16 - Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a Learning Algorithm</summary>

- *Pascal Germain, Alexandre Lacasse, François Laviolette, Mario Marchand, Jean-Francis Roy*

- `1503.08329v2` - [abs](http://arxiv.org/abs/1503.08329v2) - [pdf](http://arxiv.org/pdf/1503.08329v2)

> We propose an extensive analysis of the behavior of majority votes in binary classification. In particular, we introduce a risk bound for majority votes, called the C-bound, that takes into account the average quality of the voters and their average disagreement. We also propose an extensive PAC-Bayesian analysis that shows how the C-bound can be estimated from various observations contained in the training data. The analysis intends to be self-contained and can be used as introductory material to PAC-Bayesian statistical learning theory. It starts from a general PAC-Bayesian perspective and ends with uncommon PAC-Bayesian bounds. Some of these bounds contain no Kullback-Leibler divergence and others allow kernel functions to be used as voters (via the sample compression setting). Finally, out of the analysis, we propose the MinCq learning algorithm that basically minimizes the C-bound. MinCq reduces to a simple quadratic program. Aside from being theoretically grounded, MinCq achieves state-of-the-art performance, as shown in our extensive empirical comparison with both AdaBoost and the Support Vector Machine.

</details>

<details>

<summary>2015-07-29 02:29:54 - Unification of field theory and maximum entropy methods for learning probability densities</summary>

- *Justin B. Kinney*

- `1411.5371v5` - [abs](http://arxiv.org/abs/1411.5371v5) - [pdf](http://arxiv.org/pdf/1411.5371v5)

> The need to estimate smooth probability distributions (a.k.a. probability densities) from finite sampled data is ubiquitous in science. Many approaches to this problem have been described, but none is yet regarded as providing a definitive solution. Maximum entropy estimation and Bayesian field theory are two such approaches. Both have origins in statistical physics, but the relationship between them has remained unclear. Here I unify these two methods by showing that every maximum entropy density estimate can be recovered in the infinite smoothness limit of an appropriate Bayesian field theory. I also show that Bayesian field theory estimation can be performed without imposing any boundary conditions on candidate densities, and that the infinite smoothness limit of these theories recovers the most common types of maximum entropy estimates. Bayesian field theory is thus seen to provide a natural test of the validity of the maximum entropy null hypothesis. Bayesian field theory also returns a lower entropy density estimate when the maximum entropy hypothesis is falsified. The computations necessary for this approach can be performed rapidly for one-dimensional data, and software for doing this is provided. Based on these results, I argue that Bayesian field theory is poised to provide a definitive solution to the density estimation problem in one dimension.

</details>

<details>

<summary>2015-07-29 05:59:41 - Multi-armed Bandit Models for the Optimal Design of Clinical Trials: Benefits and Challenges</summary>

- *Sofía S. Villar, Jack Bowden, James Wason*

- `1507.08025v1` - [abs](http://arxiv.org/abs/1507.08025v1) - [pdf](http://arxiv.org/pdf/1507.08025v1)

> Multi-armed bandit problems (MABPs) are a special type of optimal control problem well suited to model resource allocation under uncertainty in a wide variety of contexts. Since the first publication of the optimal solution of the classic MABP by a dynamic index rule, the bandit literature quickly diversified and emerged as an active research topic. Across this literature, the use of bandit models to optimally design clinical trials became a typical motivating application, yet little of the resulting theory has ever been used in the actual design and analysis of clinical trials. To this end, we review two MABP decision-theoretic approaches to the optimal allocation of treatments in a clinical trial: the infinite-horizon Bayesian Bernoulli MABP and the finite-horizon variant. These models possess distinct theoretical properties and lead to separate allocation rules in a clinical trial design context. We evaluate their performance compared to other allocation rules, including fixed randomization. Our results indicate that bandit approaches offer significant advantages, in terms of assigning more patients to better treatments, and severe limitations, in terms of their resulting statistical power. We propose a novel bandit-based patient allocation rule that overcomes the issue of low power, thus removing a potential barrier for their use in practice.

</details>

<details>

<summary>2015-07-29 07:25:51 - Posterior Model Consistency in Variable Selection as the Model Dimension Grows</summary>

- *Elías Moreno, Javier Girón, George Casella*

- `1507.08041v1` - [abs](http://arxiv.org/abs/1507.08041v1) - [pdf](http://arxiv.org/pdf/1507.08041v1)

> Most of the consistency analyses of Bayesian procedures for variable selection in regression refer to pairwise consistency, that is, consistency of Bayes factors. However, variable selection in regression is carried out in a given class of regression models where a natural variable selector is the posterior probability of the models. In this paper we analyze the consistency of the posterior model probabilities when the number of potential regressors grows as the sample size grows. The novelty in the posterior model consistency is that it depends not only on the priors for the model parameters through the Bayes factor, but also on the model priors, so that it is a useful tool for choosing priors for both models and model parameters. We have found that some classes of priors typically used in variable selection yield posterior model inconsistency, while mixtures of these priors improve this undesirable behavior. For moderate sample sizes, we evaluate Bayesian pairwise variable selection procedures by comparing their frequentist Type I and II error probabilities. This provides valuable information to discriminate between the priors for the model parameters commonly used for variable selection.

</details>

<details>

<summary>2015-07-29 08:20:22 - Probabilistic Programming in Python using PyMC</summary>

- *John Salvatier, Thomas Wiecki, Christopher Fonnesbeck*

- `1507.08050v1` - [abs](http://arxiv.org/abs/1507.08050v1) - [pdf](http://arxiv.org/pdf/1507.08050v1)

> Probabilistic programming (PP) allows flexible specification of Bayesian statistical models in code. PyMC3 is a new, open-source PP framework with an intutive and readable, yet powerful, syntax that is close to the natural syntax statisticians use to describe models. It features next-generation Markov chain Monte Carlo (MCMC) sampling algorithms such as the No-U-Turn Sampler (NUTS; Hoffman, 2014), a self-tuning variant of Hamiltonian Monte Carlo (HMC; Duane, 1987). Probabilistic programming in Python confers a number of advantages including multi-platform compatibility, an expressive yet clean and readable syntax, easy integration with other scientific libraries, and extensibility via C, C++, Fortran or Cython. These features make it relatively straightforward to write and use custom statistical distributions, samplers and transformation functions, as required by Bayesian analysis.

</details>

<details>

<summary>2015-07-29 08:30:27 - Quantile regression for longitudinal data: unobserved heterogeneity and informative missingness</summary>

- *Maria Francesca Marino, Nikos Tzavidis, Marco Alfo'*

- `1501.02157v2` - [abs](http://arxiv.org/abs/1501.02157v2) - [pdf](http://arxiv.org/pdf/1501.02157v2)

> Linear quantile regression models aim at providing a detailed and robust picture of the (conditional) response distribution as function of a set of observed covariates. Longitudinal data represent an interesting field of application of such models; due to their peculiar features, they represent a substantial challenge, in that the standard, cross-sectional, model representation needs to be extended for dealing with such kind of data. In fact, repeated observations from the same statistical unit poses a problem of dependence; in a conditional perspective, this dependence could be ascribed to sources of unobserved, individual-specific, heterogeneity. Along these lines, quantile regression models have recently been extended to the analysis of longitudinal, continuous, responses, by modelling dependence via time-constant or time-varying random effects. In this manuscript, we introduce a general quantile regression model for longitudinal, continuous, responses where time-varying and time-constant random parameters are jointly taken into account. A further feature of longitudinal designs is the presence of partially incomplete sequences, due to some individuals leaving the study before its designed end. The missing data process may produce a selection of units which can be informative with respect to the parameters of the longitudinal data model. To deal with the case of irretrievable drop-out, we introduce a pattern mixture version of the linear quantile hidden Markov model, where we account for time-varying heterogeneity and for changes in the fixed effect vector due to differential propensities to stay in the study. The proposed models are illustrated using a well known benchmark dataset on longitudinal dynamics of CD4 cells and by means of a large scale simulation study, entailing different quantiles and both complete and partially complete (ie subject to drop-out) individual sequences.

</details>

<details>

<summary>2015-07-29 14:37:00 - Asymptotic behaviour of weighted differential entropies in a Bayesian problem</summary>

- *Mark Kelbert, Pavel Mozgunov*

- `1504.01612v4` - [abs](http://arxiv.org/abs/1504.01612v4) - [pdf](http://arxiv.org/pdf/1504.01612v4)

> We consider a Bayesian problem of estimating of probability of success in a series of conditionally independent trials with binary outcomes. We study the asymptotic behaviour of differential entropy for posterior probability density function conditional on $x$ successes after $n$ conditionally independent trials, when $n \to \infty$. It is shown that after an appropriate normalization in cases $x \sim n$ and $x$ $\sim n^\beta$ ($0<\beta<1$) limiting distribution is Gaussian and the differential entropy of standardized RV converges to differential entropy of standard Gaussian random variable. When $x$ or $n-x$ is a constant the limiting distribution in not Gaussian, but still the asymptotic of differential entropy can be found explicitly.   Then suppose that one is interested to know whether the coin is fair or not and for large $n$ is interested in the true frequency. To do so the concept of weighted differential entropy introduced in \cite{Belis1968} is used when the frequency $\gamma$ is necessary to emphasize. It was found that the weight in suggested form does not change the asymptotic form of Shannon, Renyi, Tsallis and Fisher entropies, but change the constants. The main term in weighted Fisher Information is changed by some constant which depend on distance between the true frequency and the value we want to emphasize.   In third part we derived the weighted versions of Rao-Cram\'er, Bhattacharyya and Kullback inequalities. This result is applied to the Bayesian problem described above. The asymptotic forms of these inequalities are obtained for a particular class of weight functions.

</details>

<details>

<summary>2015-07-29 16:42:15 - The Crisis Of Evidence: Why Probability And Statistics Cannot Discover Cause</summary>

- *William M. Briggs*

- `1507.07244v2` - [abs](http://arxiv.org/abs/1507.07244v2) - [pdf](http://arxiv.org/pdf/1507.07244v2)

> Probability models are only useful at explaining the uncertainty of what we do not know, and should never be used to say what we already know. Probability and statistical models are useless at discerning cause. Classical statistical procedures, in both their frequentist and Bayesian implementations are, falsely imply they can speak about cause. No hypothesis test, or Bayes factor, should ever be used again. Even assuming we know the cause or partial cause for some set of observations, reporting via relative risk exagerates the certainty we have in the future, often by a lot. This over-certainty is made much worse when parametetric and not predictive methods are used. Unfortunately, predictive methods are rarely used; and even when they are, cause must still be an assumption, meaning (again) certainty in our scientific pronouncements is too high.

</details>

<details>

<summary>2015-07-30 09:52:07 - Robustness to outliers in location-scale parameter model using log-regularly varying distributions</summary>

- *Alain Desgagné*

- `1507.08436v1` - [abs](http://arxiv.org/abs/1507.08436v1) - [pdf](http://arxiv.org/pdf/1507.08436v1)

> Estimating the location and scale parameters is common in statistics, using, for instance, the well-known sample mean and standard deviation. However, inference can be contaminated by the presence of outliers if modeling is done with light-tailed distributions such as the normal distribution. In this paper, we study robustness to outliers in location-scale parameter models using both the Bayesian and frequentist approaches. We find sufficient conditions (e.g., on tail behavior of the model) to obtain whole robustness to outliers, in the sense that the impact of the outliers gradually decreases to nothing as the conflict grows infinitely. To this end, we introduce the family of log-Pareto-tailed symmetric distributions that belongs to the larger family of log-regularly varying distributions.

</details>

<details>

<summary>2015-07-30 11:47:31 - Structural Markov graph laws for Bayesian model uncertainty</summary>

- *Simon Byrne, A. Philip Dawid*

- `1403.5689v4` - [abs](http://arxiv.org/abs/1403.5689v4) - [pdf](http://arxiv.org/pdf/1403.5689v4)

> This paper considers the problem of defining distributions over graphical structures. We propose an extension of the hyper Markov properties of Dawid and Lauritzen [Ann. Statist. 21 (1993) 1272-1317], which we term structural Markov properties, for both undirected decomposable and directed acyclic graphs, which requires that the structure of distinct components of the graph be conditionally independent given the existence of a separating component. This allows the analysis and comparison of multiple graphical structures, while being able to take advantage of the common conditional independence constraints. Moreover, we show that these properties characterise exponential families, which form conjugate priors under sampling from compatible Markov distributions.

</details>

<details>

<summary>2015-07-30 19:33:07 - Heritability Estimation in Matrix-Variate Mixed models -- A Bayesian Approach</summary>

- *Najla Saad Elhezzani*

- `1507.08638v1` - [abs](http://arxiv.org/abs/1507.08638v1) - [pdf](http://arxiv.org/pdf/1507.08638v1)

> Since the emergence of genome-wide association studies (GWASs), estimation of the narrow sense heritability explained by common single-nucleotide polymorphisms (SNPs) via linear mixed model approaches became widely used. As in most GWASs, most of the heritability analyses are performed using univariate approaches i.e. considering each phenotype independently. In this study, we propose a Bayesian matrix-variate mixed model that takes into account the genetic correlation between phenotypes in addition to the genetic correlation between individuals which is usually modelled via a relatedness matrix. We showed that when the relatedness matrix is estimated using all the genome-wide SNPs, our model is equivalent to a matrix normal regression with matrix normal prior on the effect sizes. Using real data we demonstrate that there is a boost in the heritability explained when phenotypes are jointly modelled (25-35% increase). In fact based on their standard error, the joint modelling provides more accurate estimates of the heritability over the univariate modelling. Moreover, our Bayesian approach provides slightly higher estimates of heritability compared to the maximum likelihood method. On the other hand, although our method performs less well in phenotype prediction, we note that an initial imputation step relatively increases the prediction accuracy.

</details>

<details>

<summary>2015-07-31 22:25:35 - Dynamic Models of Animal Movement with Spatial Point Process Interactions</summary>

- *James C. Russell, Ephraim M. Hanks, Murali Haran*

- `1503.08692v3` - [abs](http://arxiv.org/abs/1503.08692v3) - [pdf](http://arxiv.org/pdf/1503.08692v3)

> When analyzing animal movement, it is important to account for interactions between individuals. However, statistical models for incorporating interaction behavior in movement models are limited. We propose an approach that models dependent movement by augmenting a dynamic marginal movement model with a spatial point process interaction function within a weighted distribution framework. The approach is flexible, as marginal movement behavior and interaction behavior can be modeled independently. Inference for model parameters is complicated by intractable normalizing constants. We develop a double Metropolis-Hastings algorithm to perform Bayesian inference. We illustrate our approach through the analysis of movement tracks of guppies (Poecilia reticulata)

</details>


## 2015-08

<details>

<summary>2015-08-02 20:08:49 - Efficient computation of Bayesian optimal discriminating designs</summary>

- *Holger Dette, Roman Guchenko, Viatcheslav B. Melas*

- `1508.00279v1` - [abs](http://arxiv.org/abs/1508.00279v1) - [pdf](http://arxiv.org/pdf/1508.00279v1)

> An efficient algorithm for the determination of Bayesian optimal discriminating designs for competing regression models is developed, where the main focus is on models with general distributional assumptions beyond the "classical" case of normally distributed homoscedastic errors. For this purpose we consider a Bayesian version of the Kullback- Leibler (KL) optimality criterion introduced by L\'opez-Fidalgo et al. (2007). Discretizing the prior distribution leads to local KL-optimal discriminating design problems for a large number of competing models. All currently available methods either require a large computation time or fail to calculate the optimal discriminating design, because they can only deal efficiently with a few model comparisons. In this paper we develop a new algorithm for the determination of Bayesian optimal discriminating designs with respect to the Kullback-Leibler criterion. It is demonstrated that the new algorithm is able to calculate the optimal discriminating designs with reasonable accuracy and computational time in situations where all currently available procedures are either slow or fail.

</details>

<details>

<summary>2015-08-02 22:23:39 - An efficient semiparametric maxima estimator of the extremal index</summary>

- *Paul J. Northrop*

- `1506.06831v3` - [abs](http://arxiv.org/abs/1506.06831v3) - [pdf](http://arxiv.org/pdf/1506.06831v3)

> The extremal index $\theta$, a measure of the degree of local dependence in the extremes of a stationary process, plays an important role in extreme value analyses. We estimate $\theta$ semiparametrically, using the relationship between the distribution of block maxima and the marginal distribution of a process to define a semiparametric model. We show that these semiparametric estimators are simpler and substantially more efficient than their parametric counterparts. We seek to improve efficiency further using maxima over sliding blocks. A simulation study shows that the semiparametric estimators are competitive with the leading estimators. An application to sea-surge heights combines inferences about $\theta$ with a standard extreme value analysis of block maxima to estimate marginal quantiles.

</details>

<details>

<summary>2015-08-03 15:42:14 - Data augmentation for models based on rejection sampling</summary>

- *Vinayak Rao, Lizhen Lin, David Dunson*

- `1406.6652v2` - [abs](http://arxiv.org/abs/1406.6652v2) - [pdf](http://arxiv.org/pdf/1406.6652v2)

> We present a data augmentation scheme to perform Markov chain Monte Carlo inference for models where data generation involves a rejection sampling algorithm. Our idea, which seems to be missing in the literature, is a simple scheme to instantiate the rejected proposals preceding each data point. The resulting joint probability over observed and rejected variables can be much simpler than the marginal distribution over the observed variables, which often involves intractable integrals. We consider three problems, the first being the modeling of flow-cytometry measurements subject to truncation. The second is a Bayesian analysis of the matrix Langevin distribution on the Stiefel manifold, and the third, Bayesian inference for a nonparametric Gaussian process density model. The latter two are instances of problems where Markov chain Monte Carlo inference is doubly-intractable. Our experiments demonstrate superior performance over state-of-the-art sampling algorithms for such problems.

</details>

<details>

<summary>2015-08-03 21:35:39 - Bayesian Nonparameteric Multiresolution Estimation for the American Community Survey</summary>

- *Terrance D. Savitsky*

- `1508.00604v1` - [abs](http://arxiv.org/abs/1508.00604v1) - [pdf](http://arxiv.org/pdf/1508.00604v1)

> Bayesian hierarchical methods implemented for small area estimation focus on reducing the noise variation in published government official statistics by borrowing information among dependent response values. Even the most flexible models confine parameters defined at the finest scale to link to each data observation in a one-to-one construction. We propose a Bayesian multiresolution formulation that utilizes an ensemble of observations at a variety of coarse scales in space and time to additively nest parameters we define at a finer scale, which serve as our focus for estimation. Our construction is motivated by and applied to the estimation of $1-$ year period employment levels, indexed by county, from statistics published at coarser areal domains and multi-year intervals in the American Community Survey (ACS). We construct a nonparametric mixture of Gaussian processes as the prior on a set of regression coefficients of county-indexed latent functions over multiple survey years. We evaluate a modified Dirichlet process prior that incorporates county-year predictors as the mixing measure. Each county-year parameter of a latent function is estimated from multiple coarse scale observations in space and time to which it links. The multiresolution formulation is evaluated on synthetic data and applied to the ACS.

</details>

<details>

<summary>2015-08-03 23:16:01 - Bayesian Nonparametric Functional Mixture Estimation for Time-Series Data, With Application to Estimation of State Employment Totals</summary>

- *Terrance D. Savitsky*

- `1508.00615v1` - [abs](http://arxiv.org/abs/1508.00615v1) - [pdf](http://arxiv.org/pdf/1508.00615v1)

> The U.S. Bureau of Labor Statistics use monthly, by-state employment totals from the Current Population Survey (CPS) as a key input to develop employment estimates for counties within the states. The monthly CPS by-state totals, however, express high levels of volatility that compromise the accuracy of resulting estimates composed for the counties. Typically-employed models for small area estimation produce de-noised, state-level employment estimates by borrowing information over the survey months, but assume independence among the collection of by-state time series, which is typically violated due to similarities in their underlying economies. We construct Gaussian process and Gaussian Markov random field alternative functional prior specifications, each in a mixture of multivariate Gaussian distributions with a Dirichlet process (DP) mixing measure over the parameters of their covariance or precision matrices. Our DP mixture of functions models allow the data to simultaneously estimate a dependence among the months and between states. A feature of our models is that those functions assigned to the same cluster are drawn from a distribution with the same covariance parameters, so that they are similar, but don't have to be identical. We compare the performances of our two alternatives on synthetic data and apply them to recover de-noised, by-state CPS employment totals for data from $2000-2013$.

</details>

<details>

<summary>2015-08-04 01:29:49 - Bayesian mixtures of spatial spline regressions</summary>

- *Faicel Chamroukhi*

- `1508.00635v1` - [abs](http://arxiv.org/abs/1508.00635v1) - [pdf](http://arxiv.org/pdf/1508.00635v1)

> This work relates the framework of model-based clustering for spatial functional data where the data are surfaces. We first introduce a Bayesian spatial spline regression model with mixed-effects (BSSR) for modeling spatial function data. The BSSR model is based on Nodal basis functions for spatial regression and accommodates both common mean behavior for the data through a fixed-effects part, and variability inter-individuals thanks to a random-effects part. Then, in order to model populations of spatial functional data issued from heterogeneous groups, we integrate the BSSR model into a mixture framework. The resulting model is a Bayesian mixture of spatial spline regressions with mixed-effects (BMSSR) used for density estimation and model-based surface clustering. The models, through their Bayesian formulation, allow to integrate possible prior knowledge on the data structure and constitute a good alternative to recent mixture of spatial spline regressions model estimated in a maximum likelihood framework via the expectation-maximization (EM) algorithm. The Bayesian model inference is performed by Markov Chain Monte Carlo (MCMC) sampling. We derive two Gibbs sampler to infer the BSSR and the BMSSR models and apply them on simulated surfaces and a real problem of handwritten digit recognition using the MNIST data set. The obtained results highlight the potential benefit of the proposed Bayesian approaches for modeling surfaces possibly dispersed in particular in clusters.

</details>

<details>

<summary>2015-08-05 13:24:15 - A MAP approach for $\ell_q$-norm regularized sparse parameter estimation using the EM algorithm</summary>

- *Rodrigo Carvajal, Juan C. Agüero, Boris I. Godoy, Dimitrios Katselis*

- `1508.01071v1` - [abs](http://arxiv.org/abs/1508.01071v1) - [pdf](http://arxiv.org/pdf/1508.01071v1)

> In this paper, Bayesian parameter estimation through the consideration of the Maximum A Posteriori (MAP) criterion is revisited under the prism of the Expectation-Maximization (EM) algorithm. By incorporating a sparsity-promoting penalty term in the cost function of the estimation problem through the use of an appropriate prior distribution, we show how the EM algorithm can be used to efficiently solve the corresponding optimization problem. To this end, we rely on variance-mean Gaussian mixtures (VMGM) to describe the prior distribution, while we incorporate many nice features of these mixtures to our estimation problem. The corresponding MAP estimation problem is completely expressed in terms of the EM algorithm, which allows for handling nonlinearities and hidden variables that cannot be easily handled with traditional methods. For comparison purposes, we also develop a Coordinate Descent algorithm for the $\ell_q$-norm penalized problem and present the performance results via simulations.

</details>

<details>

<summary>2015-08-05 19:28:12 - Hierarchical models for semi-competing risks data with application to quality of end-of-life care for pancreatic cancer</summary>

- *Kyu Ha Lee, Francesca Dominici, Deborah Schrag, Sebastien Haneuse*

- `1502.00526v2` - [abs](http://arxiv.org/abs/1502.00526v2) - [pdf](http://arxiv.org/pdf/1502.00526v2)

> Readmission following discharge from an initial hospitalization is a key marker of quality of health care in the United States. For the most part, readmission has been used to study quality of care for patients with acute health conditions, such as pneumonia and heart failure, with analyses typically based on a logistic-Normal generalized linear mixed model. Applying this model to the study readmission among patients with increasingly prevalent advanced health conditions such as pancreatic cancer is problematic, however, because it ignores death as a competing risk. A more appropriate analysis is to imbed such studies within the semi-competing risks framework. To our knowledge, however, no comprehensive statistical methods have been developed for cluster-correlated semi-competing risks data. In this paper we propose a novel hierarchical modeling framework for the analysis of cluster-correlated semi-competing risks data. The framework permits parametric or non-parametric specifications for a range of model components, including baseline hazard functions and distributions for key random effects, giving analysts substantial flexibility as they consider their own analyses. Estimation and inference is performed within the Bayesian paradigm since it facilitates the straightforward characterization of (posterior) uncertainty for all model parameters including hospital-specific random effects. The proposed framework is used to study the risk of readmission among 5,298 Medicare beneficiaries diagnosed with pancreatic cancer at 112 hospitals in the six New England states between 2000-2009, specifically to investigate the role of patient-level risk factors and to characterize variation in risk across hospitals that is not explained by differences in patient case-mix.

</details>

<details>

<summary>2015-08-05 23:40:40 - A Bayesian Multivariate Functional Dynamic Linear Model</summary>

- *Daniel R. Kowal, David S. Matteson, David Ruppert*

- `1411.0764v2` - [abs](http://arxiv.org/abs/1411.0764v2) - [pdf](http://arxiv.org/pdf/1411.0764v2)

> We present a Bayesian approach for modeling multivariate, dependent functional data. To account for the three dominant structural features in the data--functional, time dependent, and multivariate components--we extend hierarchical dynamic linear models for multivariate time series to the functional data setting. We also develop Bayesian spline theory in a more general constrained optimization framework. The proposed methods identify a time-invariant functional basis for the functional observations, which is smooth and interpretable, and can be made common across multivariate observations for additional information sharing. The Bayesian framework permits joint estimation of the model parameters, provides exact inference (up to MCMC error) on specific parameters, and allows generalized dependence structures. Sampling from the posterior distribution is accomplished with an efficient Gibbs sampling algorithm. We illustrate the proposed framework with two applications: (1) multi-economy yield curve data from the recent global recession, and (2) local field potential brain signals in rats, for which we develop a multivariate functional time series approach for multivariate time-frequency analysis. Supplementary materials, including R code and the multi-economy yield curve data, are available online.

</details>

<details>

<summary>2015-08-06 07:21:22 - Truncation map estimation based on bivariate probabilities and validation for the truncated plurigaussian model</summary>

- *Alina Astrakova, Dean S. Oliver, Christian Lantuéjoul*

- `1508.01090v2` - [abs](http://arxiv.org/abs/1508.01090v2) - [pdf](http://arxiv.org/pdf/1508.01090v2)

> The truncated plurigaussian model is often used to simulate the spatial distribution of random categorical variables such as geological facies. The problems addressed in this paper are the estimation of parameters of the truncation map for the truncated plurigaussian model. Unlike standard truncation maps, in this paper a colored Voronoi tessellation with number of nodes, locations of nodes, and category associated with each node all treated as unknowns in the optimization. Parameters were adjusted to match categorical bivariate unit-lag probabilities, which were obtained from a larger pattern joint distribution estimates from the Bayesian maximum-entropy approach conditioned to the unit-lag probabilities. The distribution of categorical variables generated from the estimated truncation map was close to the target unit-lag bivariate probabilities. The validation of the predictive performance of the model is evaluated using scoring rules, and conditioning of the latent Gaussian fields to log-data is generalized for the case when the truncated bigaussian model is governed by a colored Voronoi tessellation of the truncation map.

</details>

<details>

<summary>2015-08-06 17:04:21 - Bayesian modelling of skewness and kurtosis with two-piece scale and shape distributions</summary>

- *F. J. Rubio, M. F. J. Steel*

- `1406.7674v2` - [abs](http://arxiv.org/abs/1406.7674v2) - [pdf](http://arxiv.org/pdf/1406.7674v2)

> We formalise and generalise the definition of the family of univariate double two--piece distributions, obtained by using a density--based transformation of unimodal symmetric continuous distributions with a shape parameter. The resulting distributions contain five interpretable parameters that control the mode, as well as the scale and shape in each direction. Four-parameter subfamilies of this class of distributions that capture different types of asymmetry are discussed. We propose interpretable scale and location-invariant benchmark priors and derive conditions for the propriety of the corresponding posterior distribution. The prior structures used allow for meaningful comparisons through Bayes factors within flexible families of distributions. These distributions are applied to data from finance, internet traffic and medicine, comparing them with appropriate competitors.

</details>

<details>

<summary>2015-08-06 18:11:41 - Adiabatic Monte Carlo</summary>

- *M. J. Betancourt*

- `1405.3489v5` - [abs](http://arxiv.org/abs/1405.3489v5) - [pdf](http://arxiv.org/pdf/1405.3489v5)

> A common strategy for inference in complex models is the relaxation of a simple model into the more complex target model, for example the prior into the posterior in Bayesian inference. Existing approaches that attempt to generate such transformations, however, are sensitive to the pathologies of complex distributions and can be difficult to implement in practice. Leveraging the geometry of thermodynamic processes I introduce a principled and robust approach to deforming measures that presents a powerful new tool for inference.

</details>

<details>

<summary>2015-08-06 22:03:34 - A Knowledge Gradient Policy for Sequencing Experiments to Identify the Structure of RNA Molecules Using a Sparse Additive Belief Model</summary>

- *Yan Li, Kristofer G. Reyes, Jorge Vazquez-Anderson, Yingfei Wang, Lydia M. Contreras, Warren B. Powell*

- `1508.01551v1` - [abs](http://arxiv.org/abs/1508.01551v1) - [pdf](http://arxiv.org/pdf/1508.01551v1)

> We present a sparse knowledge gradient (SpKG) algorithm for adaptively selecting the targeted regions within a large RNA molecule to identify which regions are most amenable to interactions with other molecules. Experimentally, such regions can be inferred from fluorescence measurements obtained by binding a complementary probe with fluorescence markers to the targeted regions. We use a biophysical model which shows that the fluorescence ratio under the log scale has a sparse linear relationship with the coefficients describing the accessibility of each nucleotide, since not all sites are accessible (due to the folding of the molecule). The SpKG algorithm uniquely combines the Bayesian ranking and selection problem with the frequentist $\ell_1$ regularized regression approach Lasso. We use this algorithm to identify the sparsity pattern of the linear model as well as sequentially decide the best regions to test before experimental budget is exhausted. Besides, we also develop two other new algorithms: batch SpKG algorithm, which generates more suggestions sequentially to run parallel experiments; and batch SpKG with a procedure which we call length mutagenesis. It dynamically adds in new alternatives, in the form of types of probes, are created by inserting, deleting or mutating nucleotides within existing probes. In simulation, we demonstrate these algorithms on the Group I intron (a mid-size RNA molecule), showing that they efficiently learn the correct sparsity pattern, identify the most accessible region, and outperform several other policies.

</details>

<details>

<summary>2015-08-07 11:29:53 - GMM Estimation of Affine Term Structure Models</summary>

- *Jaroslava Hlouskova, Leopold Sögner*

- `1508.01661v1` - [abs](http://arxiv.org/abs/1508.01661v1) - [pdf](http://arxiv.org/pdf/1508.01661v1)

> This article investigates parameter estimation of affine term structure models by means of the generalized method of moments. Exact moments of the affine latent process as well as of the yields are obtained by using results derived for p-polynomial processes. Then the generalized method of moments, combined with Quasi-Bayesian methods, is used to get reliable parameter estimates and to perform inference. After a simulation study, the estimation procedure is applied to empirical interest rate data.

</details>

<details>

<summary>2015-08-09 10:30:57 - A Bayesian Hierarchical Model for Reconstructing Sea Levels: From Raw Data to Rates of Change</summary>

- *Niamh Cahill, Andrew C. Kemp, Benjamin P. Horton, Andrew C. Parnell*

- `1508.02010v1` - [abs](http://arxiv.org/abs/1508.02010v1) - [pdf](http://arxiv.org/pdf/1508.02010v1)

> We present a holistic Bayesian hierarchical model for reconstructing the continuous and dynamic evolution of relative sea-level (RSL) change with fully quantified uncertainty. The reconstruction is produced from biological (foraminifera) and geochemical ({\delta}13C) sea-level indicators preserved in dated cores of salt-marsh sediment. Our model is comprised of three modules: (1) A Bayesian transfer function for the calibration of foraminifera into tidal elevation, which is flexible enough to formally accommodate additional proxies (in this case bulk-sediment {\delta}13C values); (2) A chronology developed from an existing Bchron age-depth model, and (3) An existing errors-in-variables integrated Gaussian process (EIV-IGP) model for estimating rates of sea-level change. We illustrate our approach using a case study of Common Era sea-level variability from New Jersey, U.S.A. We develop a new Bayesian transfer function (B-TF), with and without the {\delta}13C proxy and compare our results to those from a widely-used weighted-averaging transfer function (WA-TF). The formal incorporation of a second proxy into the B-TF model results in smaller vertical uncertainties and improved accuracy for reconstructed RSL. The vertical uncertainty from the multi-proxy B-TF is ~28% smaller on average compared to the WA-TF. When evaluated against historic tide-gauge measurements, the multi-proxy B-TF most accurately reconstructs the RSL changes observed in the instrumental record (MSE = 0.003). The holistic model provides a single, unifying framework for reconstructing and analysing sea level through time. This approach is suitable for reconstructing other paleoenvironmental variables using biological proxies.

</details>

<details>

<summary>2015-08-10 00:03:32 - Beyond Bell's Theorem II: Scenarios with arbitrary causal structure</summary>

- *Tobias Fritz*

- `1404.4812v2` - [abs](http://arxiv.org/abs/1404.4812v2) - [pdf](http://arxiv.org/pdf/1404.4812v2)

> It has recently been found that Bell scenarios are only a small subclass of interesting setups for studying the non-classical features of quantum theory within spacetime. We find that it is possible to talk about classical correlations, quantum correlations and other kinds of correlations on any directed acyclic graph, and this captures various extensions of Bell scenarios which have been considered in the literature. From a conceptual point of view, the main feature of our approach is its high level of unification: while the notions of source, choice of setting and measurement play all seemingly different roles in a Bell scenario, our formalism shows that they are all instances of the same concept of "event".   Our work can also be understood as a contribution to the subject of causal inference with latent variables. Among other things, we introduce hidden Bayesian networks as a generalization of hidden Markov models.

</details>

<details>

<summary>2015-08-11 16:16:12 - Alternating Minimization Algorithm with Automatic Relevance Determination for Transmission Tomography under Poisson Noise</summary>

- *Yan Kaganovsky, Shaobo Han, Soysal Degirmenci, David G. Politte, David J. Brady, Joseph A. O'Sullivan, Lawrence Carin*

- `1412.8464v2` - [abs](http://arxiv.org/abs/1412.8464v2) - [pdf](http://arxiv.org/pdf/1412.8464v2)

> We propose a globally convergent alternating minimization (AM) algorithm for image reconstruction in transmission tomography, which extends automatic relevance determination (ARD) to Poisson noise models with Beer's law. The algorithm promotes solutions that are sparse in the pixel/voxel-differences domain by introducing additional latent variables, one for each pixel/voxel, and then learning these variables from the data using a hierarchical Bayesian model. Importantly, the proposed AM algorithm is free of any tuning parameters with image quality comparable to standard penalized likelihood methods. Our algorithm exploits optimization transfer principles which reduce the problem into parallel 1D optimization tasks (one for each pixel/voxel), making the algorithm feasible for large-scale problems. This approach considerably reduces the computational bottleneck of ARD associated with the posterior variances. Positivity constraints inherent in transmission tomography problems are also enforced. We demonstrate the performance of the proposed algorithm for x-ray computed tomography using synthetic and real-world datasets. The algorithm is shown to have much better performance than prior ARD algorithms based on approximate Gaussian noise models, even for high photon flux.

</details>

<details>

<summary>2015-08-12 03:07:33 - Bayesian Variable Selection with Structure Learning: Applications in Integrative Genomics</summary>

- *Suprateek Kundu, Minsuk Shin, Yichen Cheng, Ganiraju Manyam, Bani K. Mallick, Veera Baladandayuthapani*

- `1508.02803v1` - [abs](http://arxiv.org/abs/1508.02803v1) - [pdf](http://arxiv.org/pdf/1508.02803v1)

> Significant advances in biotechnology have allowed for simultaneous measurement of molecular data points across multiple genomic and transcriptomic levels from a single tumor/cancer sample. This has motivated systematic approaches to integrate multi-dimensional structured datasets since cancer development and progression is driven by numerous co-ordinated molecular alterations and the interactions between them. We propose a novel two-step Bayesian approach that combines a variable selection framework with integrative structure learning between multiple sources of data. The structure learning in the first step is accomplished through novel joint graphical models for heterogeneous (mixed scale) data allowing for flexible incorporation of prior knowledge. This structure learning subsequently informs the variable selection in the second step to identify groups of molecular features within and across platforms associated with outcomes of cancer progression. The variable selection strategy adjusts for collinearity and multiplicity, and also has theoretical justifications. We evaluate our methods through simulations and apply them to a motivating genomic (DNA copy number and methylation) and transcriptomic (mRNA expression) data for assessing important markers associated with Glioblastoma progression.

</details>

<details>

<summary>2015-08-12 05:56:41 - Uncertainty for calculating transport on Titan: a probabilistic description of bimolecular diffusion parameters</summary>

- *Sylvain Plessis, Damon McDougall, Kathy Mandt, Thomas Greathouse, Adrienn Luspay-Kuti*

- `1508.02818v1` - [abs](http://arxiv.org/abs/1508.02818v1) - [pdf](http://arxiv.org/pdf/1508.02818v1)

> Bimolecular diffusion coefficients are important parameters used by atmospheric models to calculate altitude profiles of minor constituents in an atmosphere. Unfortunately, laboratory measurements of these coefficients were never conducted at temperature conditions relevant to the atmosphere of Titan. Here we conduct a detailed uncertainty analysis of the bimolecular diffusion coefficient parameters as applied to Titan's upper atmosphere to provide a better understanding of the impact of uncertainty for this parameter on models. Because temperature and pressure conditions are much lower than the laboratory conditions in which bimolecular diffusion parameters were measured, we apply a Bayesian framework, a problem-agnostic framework, to determine parameter estimates and associated uncertainties. We solve the Bayesian calibration problem using the open-source QUESO library which also performs a propagation of uncertainties in the calibrated parameters to temperature and pressure conditions observed in Titan's upper atmosphere. Our results show that, after propagating uncertainty through the Massman model, the uncertainty in molecular diffusion is highly correlated to temperature and we observe no noticeable correlation with pressure. We propagate the calibrated molecular diffusion estimate and associated uncertainty to obtain an estimate with uncertainty due to bimolecular diffusion for the methane molar fraction as a function of altitude. Results show that the uncertainty in methane abundance due to molecular diffusion is in general small compared to eddy diffusion and the chemical kinetics description. However, methane abundance is most sensitive to uncertainty in molecular diffusion above 1200 km where the errors are nontrivial and could have important implications for scientific research based on diffusion models in this altitude range.

</details>

<details>

<summary>2015-08-13 07:56:58 - The Overlooked Potential of Generalized Linear Models in Astronomy-III: Bayesian Negative Binomial Regression and Globular Cluster Populations</summary>

- *R. S. de Souza, J. M. Hilbe, B. Buelens, J. D. Riggs, E. Cameron, E. E. O. Ishida, A. L. Chies-Santos, M. Killedar*

- `1506.04792v2` - [abs](http://arxiv.org/abs/1506.04792v2) - [pdf](http://arxiv.org/pdf/1506.04792v2)

> In this paper, the third in a series illustrating the power of generalized linear models (GLMs) for the astronomical community, we elucidate the potential of the class of GLMs which handles count data. The size of a galaxy's globular cluster population $N_{\rm GC}$ is a prolonged puzzle in the astronomical literature. It falls in the category of count data analysis, yet it is usually modelled as if it were a continuous response variable. We have developed a Bayesian negative binomial regression model to study the connection between $N_{\rm GC}$ and the following galaxy properties: central black hole mass, dynamical bulge mass, bulge velocity dispersion, and absolute visual magnitude. The methodology introduced herein naturally accounts for heteroscedasticity, intrinsic scatter, errors in measurements in both axes (either discrete or continuous), and allows modelling the population of globular clusters on their natural scale as a non-negative integer variable. Prediction intervals of 99% around the trend for expected $N_{\rm GC}$comfortably envelope the data, notably including the Milky Way, which has hitherto been considered a problematic outlier. Finally, we demonstrate how random intercept models can incorporate information of each particular galaxy morphological type. Bayesian variable selection methodology allows for automatically identifying galaxy types with different productions of GCs, suggesting that on average S0 galaxies have a GC population 35% smaller than other types with similar brightness.

</details>

<details>

<summary>2015-08-13 09:42:29 - Hypothesis testing for Markov chain Monte Carlo</summary>

- *Benjamin M. Gyori, Daniel Paulin*

- `1409.7986v2` - [abs](http://arxiv.org/abs/1409.7986v2) - [pdf](http://arxiv.org/pdf/1409.7986v2)

> Testing between hypotheses, when independent sampling is possible, is a well developed subject. In this paper, we propose hypothesis tests that are applicable when the samples are obtained using Markov chain Monte Carlo. These tests are useful when one is interested in deciding whether the expected value of a certain quantity is above or below a given threshold. We show non-asymptotic error bounds and bounds on the expected number of samples for three types of tests, a fixed sample size test, a sequential test with indifference region, and a sequential test without indifference region. Our tests can lead to significant savings in sample size. We illustrate our results on an example of Bayesian parameter inference involving an ODE model of a biochemical pathway.

</details>

<details>

<summary>2015-08-14 09:53:19 - Bayesian hierarchical modelling for inferring genetic interactions in yeast</summary>

- *Jonathan Heydari, Conor Lawless, David A. Lydall, Darren J. Wilkinson*

- `1508.03454v1` - [abs](http://arxiv.org/abs/1508.03454v1) - [pdf](http://arxiv.org/pdf/1508.03454v1)

> Quantitative Fitness Analysis (QFA) is a high-throughput experimental and computational methodology for measuring the growth of microbial populations. QFA screens can be used to compare the health of cell populations with and without a mutation in a query gene in order to infer genetic interaction strengths genome-wide, examining thousands of separate genotypes. We introduce Bayesian, hierarchical models of population growth rates and genetic interactions that better reflect QFA experimental design than current approaches. Our new approach models population dynamics and genetic interaction simultaneously, thereby avoiding passing information between models via a univariate fitness summary. Matching experimental structure more closely, Bayesian hierarchical approaches use data more efficiently and find new evidence for genes which interact with yeast telomeres within a published dataset.

</details>

<details>

<summary>2015-08-14 12:05:40 - Bayesian detection of abnormal segments in multiple time series</summary>

- *Lawrence Bardwell, Paul Fearnhead*

- `1412.5565v2` - [abs](http://arxiv.org/abs/1412.5565v2) - [pdf](http://arxiv.org/pdf/1412.5565v2)

> We present a novel Bayesian approach to analysing multiple time-series with the aim of detecting abnormal regions. These are regions where the properties of the data change from some normal or baseline behaviour. We allow for the possibility that such changes will only be present in a, potentially small, subset of the time-series. We develop a general model for this problem, and show how it is possible to accurately and efficiently perform Bayesian inference, based upon recursions that enable independent sampling from the posterior distribution. A motivating application for this problem comes from detecting copy number variation (CNVs), using data from multiple individuals. Pooling information across individuals can increase the power of detecting CNVs, but often a specific CNV will only be present in a small subset of the individuals. We evaluate the Bayesian method on both simulated and real CNV data, and give evidence that this approach is more accurate than a recently proposed method for analysing such data.

</details>

<details>

<summary>2015-08-14 21:10:46 - Unbounded Bayesian Optimization via Regularization</summary>

- *Bobak Shahriari, Alexandre Bouchard-Côté, Nando de Freitas*

- `1508.03666v1` - [abs](http://arxiv.org/abs/1508.03666v1) - [pdf](http://arxiv.org/pdf/1508.03666v1)

> Bayesian optimization has recently emerged as a popular and efficient tool for global optimization and hyperparameter tuning. Currently, the established Bayesian optimization practice requires a user-defined bounding box which is assumed to contain the optimizer. However, when little is known about the probed objective function, it can be difficult to prescribe such bounds. In this work we modify the standard Bayesian optimization framework in a principled way to allow automatic resizing of the search space. We introduce two alternative methods and compare them on two common synthetic benchmarking test functions as well as the tasks of tuning the stochastic gradient descent optimizer of a multi-layered perceptron and a convolutional neural network on MNIST.

</details>

<details>

<summary>2015-08-18 04:24:24 - Zero-Truncated Poisson Tensor Factorization for Massive Binary Tensors</summary>

- *Changwei Hu, Piyush Rai, Lawrence Carin*

- `1508.04210v1` - [abs](http://arxiv.org/abs/1508.04210v1) - [pdf](http://arxiv.org/pdf/1508.04210v1)

> We present a scalable Bayesian model for low-rank factorization of massive tensors with binary observations. The proposed model has the following key properties: (1) in contrast to the models based on the logistic or probit likelihood, using a zero-truncated Poisson likelihood for binary data allows our model to scale up in the number of \emph{ones} in the tensor, which is especially appealing for massive but sparse binary tensors; (2) side-information in form of binary pairwise relationships (e.g., an adjacency network) between objects in any tensor mode can also be leveraged, which can be especially useful in "cold-start" settings; and (3) the model admits simple Bayesian inference via batch, as well as \emph{online} MCMC; the latter allows scaling up even for \emph{dense} binary data (i.e., when the number of ones in the tensor/network is also massive). In addition, non-negative factor matrices in our model provide easy interpretability, and the tensor rank can be inferred from the data. We evaluate our model on several large-scale real-world binary tensors, achieving excellent computational scalability, and also demonstrate its usefulness in leveraging side-information provided in form of mode-network(s).

</details>

<details>

<summary>2015-08-18 04:28:56 - Scalable Bayesian Non-Negative Tensor Factorization for Massive Count Data</summary>

- *Changwei Hu, Piyush Rai, Changyou Chen, Matthew Harding, Lawrence Carin*

- `1508.04211v1` - [abs](http://arxiv.org/abs/1508.04211v1) - [pdf](http://arxiv.org/pdf/1508.04211v1)

> We present a Bayesian non-negative tensor factorization model for count-valued tensor data, and develop scalable inference algorithms (both batch and online) for dealing with massive tensors. Our generative model can handle overdispersed counts as well as infer the rank of the decomposition. Moreover, leveraging a reparameterization of the Poisson distribution as a multinomial facilitates conjugacy in the model and enables simple and efficient Gibbs sampling and variational Bayes (VB) inference updates, with a computational cost that only depends on the number of nonzeros in the tensor. The model also provides a nice interpretability for the factors; in our model, each factor corresponds to a "topic". We develop a set of online inference algorithms that allow further scaling up the model to massive tensors, for which batch inference methods may be infeasible. We apply our framework on diverse real-world applications, such as \emph{multiway} topic modeling on a scientific publications database, analyzing a political science data set, and analyzing a massive household transactions data set.

</details>

<details>

<summary>2015-08-18 04:57:55 - Approximate Bayesian Model Selection with the Deviance Statistic</summary>

- *Leonhard Held, Daniel Sabanés Bové, Isaac Gravestock*

- `1308.6780v3` - [abs](http://arxiv.org/abs/1308.6780v3) - [pdf](http://arxiv.org/pdf/1308.6780v3)

> Bayesian model selection poses two main challenges: the specification of parameter priors for all models, and the computation of the resulting Bayes factors between models. There is now a large literature on automatic and objective parameter priors in the linear model. One important class are $g$-priors, which were recently extended from linear to generalized linear models (GLMs). We show that the resulting Bayes factors can be approximated by test-based Bayes factors (Johnson [Scand. J. Stat. 35 (2008) 354-368]) using the deviance statistics of the models. To estimate the hyperparameter $g$, we propose empirical and fully Bayes approaches and link the former to minimum Bayes factors and shrinkage estimates from the literature. Furthermore, we describe how to approximate the corresponding posterior distribution of the regression coefficients based on the standard GLM output. We illustrate the approach with the development of a clinical prediction model for 30-day survival in the GUSTO-I trial using logistic regression.

</details>

<details>

<summary>2015-08-18 11:37:35 - Bayesian semiparametric power spectral density estimation with applications in gravitational wave data analysis</summary>

- *Matthew C. Edwards, Renate Meyer, Nelson Christensen*

- `1506.00185v2` - [abs](http://arxiv.org/abs/1506.00185v2) - [pdf](http://arxiv.org/pdf/1506.00185v2)

> The standard noise model in gravitational wave (GW) data analysis assumes detector noise is stationary and Gaussian distributed, with a known power spectral density (PSD) that is usually estimated using clean off-source data. Real GW data often depart from these assumptions, and misspecified parametric models of the PSD could result in misleading inferences. We propose a Bayesian semiparametric approach to improve this. We use a nonparametric Bernstein polynomial prior on the PSD, with weights attained via a Dirichlet process distribution, and update this using the Whittle likelihood. Posterior samples are obtained using a blocked Metropolis-within-Gibbs sampler. We simultaneously estimate the reconstruction parameters of a rotating core collapse supernova GW burst that has been embedded in simulated Advanced LIGO noise. We also discuss an approach to deal with non-stationary data by breaking longer data streams into smaller and locally stationary components.

</details>

<details>

<summary>2015-08-19 08:45:09 - Multiple imputation for continuous variables using a Bayesian principal component analysis</summary>

- *Vincent Audigier, François Husson, Julie Josse*

- `1401.5747v4` - [abs](http://arxiv.org/abs/1401.5747v4) - [pdf](http://arxiv.org/pdf/1401.5747v4)

> We propose a multiple imputation method based on principal component analysis (PCA) to deal with incomplete continuous data. To reflect the uncertainty of the parameters from one imputation to the next, we use a Bayesian treatment of the PCA model. Using a simulation study and real data sets, the method is compared to two classical approaches: multiple imputation based on joint modelling and on fully conditional modelling. Contrary to the others, the proposed method can be easily used on data sets where the number of individuals is less than the number of variables and when the variables are highly correlated. In addition, it provides unbiased point estimates of quantities of interest, such as an expectation, a regression coefficient or a correlation coefficient, with a smaller mean squared error. Furthermore, the widths of the confidence intervals built for the quantities of interest are often smaller whilst ensuring a valid coverage.

</details>

<details>

<summary>2015-08-19 22:06:43 - Multivariate Density Estimation via Adaptive Partitioning (II): Posterior Concentration</summary>

- *Linxi Liu, Wing Hung Wong*

- `1508.04812v1` - [abs](http://arxiv.org/abs/1508.04812v1) - [pdf](http://arxiv.org/pdf/1508.04812v1)

> In this paper, we study a class of non-parametric density estimators under Bayesian settings. The estimators are piecewise constant functions on binary partitions. We analyze the concentration rate of the posterior distribution under a suitable prior, and demonstrate that the rate does not directly depend on the dimension of the problem. This paper can be viewed as an extension of a parallel work where the convergence rate of a related sieve MLE was established. Compared to the sieve MLE, the main advantage of the Bayesian method is that it can adapt to the unknown complexity of the true density function, thus achieving the optimal convergence rate without artificial conditions on the density.

</details>

<details>

<summary>2015-08-20 03:31:12 - Bayesian inference for a flexible class of bivariate beta distributions</summary>

- *Roberto C. Crackel, James M. Flegal*

- `1402.1782v3` - [abs](http://arxiv.org/abs/1402.1782v3) - [pdf](http://arxiv.org/pdf/1402.1782v3)

> Several bivariate beta distributions have been proposed in the literature. In particular, Olkin and Liu (2003) proposed a 3 parameter bivariate beta model, which Arnold and Ng (2011) extend to 5 and 8 parameter models. The 3 parameter model allows for only positive correlation, while the latter models can accommodate both positive and negative correlation. However, these come at the expense of a density that is mathematically intractable. The focus of this research is on Bayesian estimation for the 5 and 8 parameter models. Since the likelihood does not exist in closed form, we apply approximate Bayesian computation, a likelihood free approach. Simulation studies have been carried out for the 5 and 8 parameter cases under various priors and tolerance levels. We apply the 5 parameter model to a real data set by allowing the model to serve as a prior to correlated proportions of a bivariate beta binomial model. Results and comparisons are then discussed.

</details>

<details>

<summary>2015-08-20 05:01:52 - Parallel and Interacting Stochastic Approximation Annealing algorithms for global optimisation</summary>

- *Georgios Karagiannis, Bledar A. Konomi, Guang Lin, Faming Liang*

- `1508.04876v1` - [abs](http://arxiv.org/abs/1508.04876v1) - [pdf](http://arxiv.org/pdf/1508.04876v1)

> We present the parallel and interacting stochastic approximation annealing (PISAA) algorithm, a stochastic simulation procedure for global optimisation, that extends and improves the stochastic approximation annealing (SAA) by using population Monte Carlo ideas. The standard SAA algorithm guarantees convergence to the global minimum when a square-root cooling schedule is used; however the efficiency of its performance depends crucially on its self-adjusting mechanism. Because its mechanism is based on information obtained from only a single chain, SAA may present slow convergence in complex optimisation problems. The proposed algorithm involves simulating a population of SAA chains that interact each other in a manner that ensures significant improvement of the self-adjusting mechanism and better exploration of the sampling space. Central to the proposed algorithm are the ideas of (i) recycling information from the whole population of Markov chains to design a more accurate/stable self-adjusting mechanism and (ii) incorporating more advanced proposals, such as crossover operations, for the exploration of the sampling space. PISAA presents a significantly improved performance in terms of convergence. PISAA can be implemented in parallel computing environments if available. We demonstrate the good performance of the proposed algorithm on challenging applications including Bayesian network learning and protein folding. Our numerical comparisons suggest that PISAA outperforms the simulated annealing, stochastic approximation annealing, and annealing evolutionary stochastic approximation Monte Carlo especially in high dimensional or rugged scenarios.

</details>

<details>

<summary>2015-08-20 09:27:28 - Posterior propriety in Bayesian extreme value analyses using reference priors</summary>

- *Paul J. Northrop, Nicolas Attalides*

- `1505.04983v3` - [abs](http://arxiv.org/abs/1505.04983v3) - [pdf](http://arxiv.org/pdf/1505.04983v3)

> The Generalized Pareto (GP) and Generalized extreme value (GEV) distributions play an important role in extreme value analyses, as models for threshold excesses and block maxima respectively. For each of these distributions we consider Bayesian inference using "reference" prior distributions (in the general sense of priors constructed using formal rules) for the model parameters, specifically a Jeffreys prior, the maximal data information (MDI) prior and independent uniform priors on separate model parameters. We investigate the important issue of whether these improper priors lead to proper posterior distributions. We show that, in the GP and GEV cases, the MDI prior, unless modified, never yields a proper posterior and that in the GEV case this also applies to the Jeffreys prior. We also show that a sample size of three (four) is sufficient for independent uniform priors to yield a proper posterior distribution in the GP (GEV) case.

</details>

<details>

<summary>2015-08-21 05:41:41 - On Consistency of Approximate Bayesian Computation</summary>

- *David T. Frazier, Gael M. Martin, Christian P. Robert*

- `1508.05178v1` - [abs](http://arxiv.org/abs/1508.05178v1) - [pdf](http://arxiv.org/pdf/1508.05178v1)

> Approximate Bayesian computation (ABC) methods have become increasingly prevalent of late, facilitating as they do the analysis of intractable, or challenging, statistical problems. With the initial focus being primarily on the practical import of ABC, exploration of its formal statistical properties has begun to attract more attention. The aim of this paper is to establish general conditions under which ABC methods are Bayesian consistent, in the sense of producing draws that yield a degenerate posterior distribution at the true parameter (vector) asymptotically (in the sample size). We derive conditions under which arbitrary summary statistics yield consistent inference in the Bayesian sense, with these conditions linked to identification of the true parameters. Using simple illustrative examples that have featured in the literature, we demonstrate that identification, and hence consistency, is unlikely to be achieved in many cases, and propose a simple diagnostic procedure that can indicate the presence of this problem. We also formally explore the link between consistency and the use of auxiliary models within ABC, and illustrate the subsequent results in the Lotka-Volterra predator-prey model.

</details>

<details>

<summary>2015-08-21 17:17:36 - Graphs for margins of Bayesian networks</summary>

- *Robin J. Evans*

- `1408.1809v2` - [abs](http://arxiv.org/abs/1408.1809v2) - [pdf](http://arxiv.org/pdf/1408.1809v2)

> Directed acyclic graph (DAG) models, also called Bayesian networks, impose conditional independence constraints on a multivariate probability distribution, and are widely used in probabilistic reasoning, machine learning and causal inference. If latent variables are included in such a model, then the set of possible marginal distributions over the remaining (observed) variables is generally complex, and not represented by any DAG. Larger classes of mixed graphical models, which use multiple edge types, have been introduced to overcome this; however, these classes do not represent all the models which can arise as margins of DAGs. In this paper we show that this is because ordinary mixed graphs are fundamentally insufficiently rich to capture the variety of marginal models.   We introduce a new class of hyper-graphs, called mDAGs, and a latent projection operation to obtain an mDAG from the margin of a DAG. We show that each distinct marginal of a DAG model is represented by at least one mDAG, and provide graphical results towards characterizing when two such marginal models are the same. Finally we show that mDAGs correctly capture the marginal structure of causally-interpreted DAGs under interventions on the observed variables.

</details>

<details>

<summary>2015-08-22 10:58:25 - Bayesian Hypothesis Testing for Block Sparse Signal Recovery</summary>

- *Mehdi Korki, Hadi Zayyani, Jingxin Zhang*

- `1508.05495v1` - [abs](http://arxiv.org/abs/1508.05495v1) - [pdf](http://arxiv.org/pdf/1508.05495v1)

> This letter presents a novel Block Bayesian Hypothesis Testing Algorithm (Block-BHTA) for reconstructing block sparse signals with unknown block structures. The Block-BHTA comprises the detection and recovery of the supports, and the estimation of the amplitudes of the block sparse signal. The support detection and recovery is performed using a Bayesian hypothesis testing. Then, based on the detected and reconstructed supports, the nonzero amplitudes are estimated by linear MMSE. The effectiveness of Block-BHTA is demonstrated by numerical experiments.

</details>

<details>

<summary>2015-08-23 09:17:01 - A joint model for authors characteristics and collaboration pattern in bibliometric networks: a Bayesian approach</summary>

- *Stefano Nasini, Víctor Martínez-de-Albéniz, Tahereh Dehdarirad*

- `1508.05580v1` - [abs](http://arxiv.org/abs/1508.05580v1) - [pdf](http://arxiv.org/pdf/1508.05580v1)

> Demographic and behavioral characteristics of journal authors are important indicators of homophily in co-authorship networks. In the presence of correlations between adjacent nodes (assortative mixing), combining the estimation of the individual characteristics and the network structure results in a well-fitting model, which is capable to provide a deep understanding of the linkage between individual and social properties. This paper aims to propose a novel probabilistic model for the joint distribution of nodal properties (authors' demographic and behavioral characteristics) and network structure (co-authorship connections), based on the nodal similarity effect. A Bayesian approach is used to estimate the model parameters, providing insights about the probabilistic properties of the observed data set. After a detailed analysis of the proposed statistical methodology, we illustrate our approach with an empirical analysis of co-authorship of 1007 journal articles indexed in the ISI Web of Science database in the field of neuroscience between 2009 and 2013.

</details>

<details>

<summary>2015-08-23 16:20:14 - An adaptive kriging method for solving nonlinear inverse statistical problems</summary>

- *Shuai Fu, Mathieu Couplet, Nicolas Bousquet*

- `1508.05628v1` - [abs](http://arxiv.org/abs/1508.05628v1) - [pdf](http://arxiv.org/pdf/1508.05628v1)

> In various industrial contexts, estimating the distribution of unobserved random vectors Xi from some noisy indirect observations H(Xi) + Ui is required. If the relation between Xi and the quantity H(Xi), measured with the error Ui, is implemented by a CPU-consuming computer model H, a major practical difficulty is to perform the statistical inference with a relatively small number of runs of H. Following Fu et al. (2014), a Bayesian statistical framework is considered to make use of possible prior knowledge on the parameters of the distribution of the Xi, which is assumed Gaussian. Moreover, a Markov Chain Monte Carlo (MCMC) algorithm is carried out to estimate their posterior distribution by replacing H by a kriging metamodel build from a limited number of simulated experiments. Two heuristics, involving two different criteria to be optimized, are proposed to sequentially design these computer experiments in the limits of a given computational budget. The first criterion is a Weighted Integrated Mean Square Error (WIMSE). The second one, called Expected Conditional Divergence (ECD), developed in the spirit of the Stepwise Uncertainty Reduction (SUR) criterion, is based on the discrepancy between two consecutive approximations of the target posterior distribution. Several numerical comparisons conducted over a toy example then a motivating real case-study show that such adaptive designs can significantly outperform the classical choice of a maximin Latin Hypercube Design (LHD) of experiments. Dealing with a major concern in hydraulic engineering, a particular emphasis is placed upon the prior elicitation of the case-study, highlighting the overall feasibility of the methodology. Faster convergences and manageability considerations lead to recommend the use of the ECD criterion in practical applications.

</details>

<details>

<summary>2015-08-24 01:30:29 - Bayesian approach to inverse problems for functions with variable index Besov prior</summary>

- *Junxiong Jia, Jigen Peng, Jinghuai Gao*

- `1508.05680v1` - [abs](http://arxiv.org/abs/1508.05680v1) - [pdf](http://arxiv.org/pdf/1508.05680v1)

> We adopt Bayesian approach to consider the inverse problem of estimate a function from noisy observations. One important component of this approach is the prior measure. Total variation prior has been proved with no discretization invariant property, so Besov prior has been proposed recently. Different prior measures usually connect to different regularization terms. Variable index TV, variable index Besov regularization terms have been proposed in image analysis, however, there are no such prior measure in Bayesian theory. So in this paper, we propose a variable index Besov prior measure which is a Non-Guassian measure. Based on the variable index Besov prior measure, we build the Bayesian inverse theory. Then applying our theory to integer and fractional order backward diffusion problems. Although there are many researches about fractional order backward diffusion problems, we firstly apply Bayesian inverse theory to this problem which provide an opportunity to quantify the uncertainties for this problem.

</details>

<details>

<summary>2015-08-24 04:23:50 - Overfitting Bayesian Mixture Models with an Unknown Number of Components</summary>

- *Zoe van Havre, Nicole White, Judith Rousseau, Kerrie Mengersen*

- `1502.05427v2` - [abs](http://arxiv.org/abs/1502.05427v2) - [pdf](http://arxiv.org/pdf/1502.05427v2)

> This paper proposes solutions to three issues pertaining to the estimation of finite mixture models with an unknown number of components: the non-identifiability induced by overfitting the number of components, the mixing limitations of standard Markov Chain Monte Carlo (MCMC) sampling techniques, and the related label switching problem. An overfitting approach is used to estimate the number of components in a finite mixture model via a Zmix algorithm. Zmix provides a bridge between multidimensional samplers and test based estimation methods, whereby priors are chosen to encourage extra groups to have weights approaching zero. MCMC sampling is made possible by the implementation of prior parallel tempering, an extension of parallel tempering. Zmix can accurately estimate the number of components, posterior parameter estimates and allocation probabilities given a sufficiently large sample size. The results will reflect uncertainty in the final model and will report the range of possible candidate models and their respective estimated probabilities from a single run. Label switching is resolved with a computationally light-weight method, Zswitch, developed for overfitted mixtures by exploiting the intuitiveness of allocation-based relabelling algorithms and the precision of label-invariant loss functions. Four simulation studies are included to illustrate Zmix and Zswitch, as well as three case studies from the literature. All methods are available as part of the R package Zmix, which can currently be applied to univariate Gaussian mixture models

</details>

<details>

<summary>2015-08-24 10:19:46 - An Experimental Comparison of Hybrid Algorithms for Bayesian Network Structure Learning</summary>

- *Maxime Gasse, Alex Aussem, Haytham Elghazel*

- `1505.05004v2` - [abs](http://arxiv.org/abs/1505.05004v2) - [pdf](http://arxiv.org/pdf/1505.05004v2)

> We present a novel hybrid algorithm for Bayesian network structure learning, called Hybrid HPC (H2PC). It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. It is based on a subroutine called HPC, that combines ideas from incremental and divide-and-conquer constraint-based methods to learn the parents and children of a target variable. We conduct an experimental comparison of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the most powerful state-of-the-art algorithm for Bayesian network structure learning, on several benchmarks with various data sizes. Our extensive experiments show that H2PC outperforms MMHC both in terms of goodness of fit to new data and in terms of the quality of the network structure itself, which is closer to the true dependence structure of the data. The source code (in R) of H2PC as well as all data sets used for the empirical tests are publicly available.

</details>

<details>

<summary>2015-08-24 16:05:09 - Spatio-Temporal Change of Support with Application to American Community Survey Multi-Year Period Estimates</summary>

- *Jonathan R. Bradley, Christopher K. Wikle, Scott H. Holan*

- `1508.01451v2` - [abs](http://arxiv.org/abs/1508.01451v2) - [pdf](http://arxiv.org/pdf/1508.01451v2)

> We present hierarchical Bayesian methodology to perform spatio-temporal change of support (COS) for survey data with Gaussian sampling errors. This methodology is motivated by the American Community Survey (ACS), which is an ongoing survey administered by the U.S. Census Bureau that provides timely information on several key demographic variables. The ACS has published 1-year, 3-year, and 5-year period-estimates, and margins of errors, for demographic and socio-economic variables recorded over predefined geographies. The spatio-temporal COS methodology considered here provides data users with a way to estimate ACS variables on customized geographies and time periods, while accounting for sampling errors. Additionally, 3-year ACS period estimates are to be discontinued, and this methodology can provide predictions of ACS variables for 3-year periods given the available period estimates. The methodology is based on a spatio-temporal mixed effects model with a low-dimensional spatio-temporal basis function representation, which provides multi-resolution estimates through basis function aggregation in space and time. This methodology includes a novel parameterization that uses a target dynamical process and recently proposed parsimonious Moran's I propagator structures. Our approach is demonstrated through two applications using public-use ACS estimates, and is shown to produce good predictions on a holdout set of 3-year period estimates.

</details>

<details>

<summary>2015-08-24 18:59:10 - Extended Dynamic Generalized Linear Models: the two-parameter exponential family</summary>

- *Mariana Albi de Oliveira Souza, Helio dos Santos Migon*

- `1508.05914v1` - [abs](http://arxiv.org/abs/1508.05914v1) - [pdf](http://arxiv.org/pdf/1508.05914v1)

> We develop a Bayesian framework for estimation and prediction of dynamic models for observations from the two-parameter exponential family. Different link functions are introduced to model both the mean and the precision in the exponential family allowing the introduction of covariates and time series components. We explore conjugacy and analytical approximations under the class of partial specified models to keep the computation fast. The algorithm of West, Harrison and Migon (1985) is extended to cope with the two-parameter exponential family models. The methodological novelties are illustrated with two applications to real data. The first, considers unemployment rates in Brazil and the second some macroeconomic variables for the United Kingdom.

</details>

<details>

<summary>2015-08-25 09:40:51 - A triangular treatment effect model with random coefficients in the selection equation</summary>

- *Eric Gautier, Stefan Hoderlein*

- `1109.0362v4` - [abs](http://arxiv.org/abs/1109.0362v4) - [pdf](http://arxiv.org/pdf/1109.0362v4)

> This paper considers treatment effects under endogeneity with complex heterogeneity in the selection equation. We model the outcome of an endogenous treatment as a triangular system, where both the outcome and first-stage equations consist of a random coefficients model. The first-stage specifically allows for nonmonotone selection into treatment. We provide conditions under which marginal distributions of potential outcomes, average and quantile treatment effects, all conditional on first-stage random coefficients, are identified. Under the same conditions, we derive bounds on the (conditional) joint distributions of potential outcomes and gains from treatment, and provide additional conditions for their point identification. All conditional quantities yield unconditional effects (\emph{e.g.}, the average treatment effect) by weighted integration.

</details>

<details>

<summary>2015-08-25 12:22:01 - A method for calculating quantile function and its further use for data fitting</summary>

- *Qing Xiao*

- `1508.06125v1` - [abs](http://arxiv.org/abs/1508.06125v1) - [pdf](http://arxiv.org/pdf/1508.06125v1)

> This paper introduces a polynomial transformation model based on Weibull distribution, whereby the analytical representation of the quantile function for many probability distributions can be obtained. Firstly, the target random variable $x$ with specified distribution is expressed as a polynomial of a Weibull random variable $z$, the coefficients are conveniently determined by the percentile matching method. Then, substituting $z$ with its quantile function $z=\lambda [-ln(1-u)]^{1/k}$ gives the analytical expression of the quantile function of $x$. Furthermore, using the probability weighted moments matching method, this polynomial transformation model can be used for data fitting. Through numerical experiment, it makes evident that the proposed model is capable of handling some distributions close to binomial which are difficult for the extant approaches, and the quantile functions of various distributions are accurately approximated within the probit range $[10^{-4},1-10^{-4}]$.

</details>

<details>

<summary>2015-08-26 19:03:15 - Uniform Asymptotics for Nonparametric Quantile Regression with an Application to Testing Monotonicity</summary>

- *Sokbae Lee, Kyungchul Song, Yoon-Jae Whang*

- `1506.05337v2` - [abs](http://arxiv.org/abs/1506.05337v2) - [pdf](http://arxiv.org/pdf/1506.05337v2)

> In this paper, we establish a uniform error rate of a Bahadur representation for local polynomial estimators of quantile regression functions. The error rate is uniform over a range of quantiles, a range of evaluation points in the regressors, and over a wide class of probabilities for observed random variables. Most of the existing results on Bahadur representations for local polynomial quantile regression estimators apply to the fixed data generating process. In the context of testing monotonicity where the null hypothesis is of a complex composite hypothesis, it is particularly relevant to establish Bahadur expansions that hold uniformly over a large class of data generating processes. In addition, we establish the same error rate for bootstrap local polynomial estimators which can be useful for various bootstrap inference. As an illustration, we apply to testing monotonicity of quantile regression and present Monte Carlo experiments based on this example.

</details>

<details>

<summary>2015-08-27 01:15:22 - MCMC-Based Inference in the Era of Big Data: A Fundamental Analysis of the Convergence Complexity of High-Dimensional Chains</summary>

- *Bala Rajaratnam, Doug Sparks*

- `1508.00947v2` - [abs](http://arxiv.org/abs/1508.00947v2) - [pdf](http://arxiv.org/pdf/1508.00947v2)

> Markov chain Monte Carlo (MCMC) lies at the core of modern Bayesian methodology, much of which would be impossible without it. Thus, the convergence properties of MCMCs have received significant attention, and in particular, proving (geometric) ergodicity is of critical interest. Trust in the ability of MCMCs to sample from modern-day high-dimensional posteriors, however, has been limited by a widespread perception that these chains typically experience serious convergence problems. In this paper, we first demonstrate that contemporary methods for obtaining convergence rates have serious limitations when the dimension grows. We then propose a framework for rigorously establishing the convergence behavior of commonly used high-dimensional MCMCs. In particular, we demonstrate theoretically the precise nature and severity of the convergence problems of popular MCMCs when implemented in high dimensions, including phase transitions in the convergence rates in various $n$ and $p$ regimes, and a universality result across an entire spectrum of models. We also show that convergence problems effectively eliminate the apparent safeguard of geometric ergodicity. We then demonstrate theoretical principles by which MCMCs can be constructed and analyzed to yield bounded geometric convergence rates even as the dimension $p$ grows without bound. Additionally, we propose a diagnostic tool for establishing convergence.

</details>

<details>

<summary>2015-08-27 08:14:45 - Nested Hierarchical Dirichlet Processes for Multi-Level Non-Parametric Admixture Modeling</summary>

- *Lavanya Sita Tekumalla, Priyanka Agrawal, Indrajit Bhattacharya*

- `1508.06446v2` - [abs](http://arxiv.org/abs/1508.06446v2) - [pdf](http://arxiv.org/pdf/1508.06446v2)

> Dirichlet Process(DP) is a Bayesian non-parametric prior for infinite mixture modeling, where the number of mixture components grows with the number of data items. The Hierarchical Dirichlet Process (HDP), is an extension of DP for grouped data, often used for non-parametric topic modeling, where each group is a mixture over shared mixture densities. The Nested Dirichlet Process (nDP), on the other hand, is an extension of the DP for learning group level distributions from data, simultaneously clustering the groups. It allows group level distributions to be shared across groups in a non-parametric setting, leading to a non-parametric mixture of mixtures. The nCRF extends the nDP for multilevel non-parametric mixture modeling, enabling modeling topic hierarchies. However, the nDP and nCRF do not allow sharing of distributions as required in many applications, motivating the need for multi-level non-parametric admixture modeling. We address this gap by proposing multi-level nested HDPs (nHDP) where the base distribution of the HDP is itself a HDP at each level thereby leading to admixtures of admixtures at each level. Because of couplings between various HDP levels, scaling up is naturally a challenge during inference. We propose a multi-level nested Chinese Restaurant Franchise (nCRF) representation for the nested HDP, with which we outline an inference algorithm based on Gibbs Sampling. We evaluate our model with the two level nHDP for non-parametric entity topic modeling where an inner HDP creates a countably infinite set of topic mixtures and associates them with author entities, while an outer HDP associates documents with these author entities. In our experiments on two real world research corpora, the nHDP is able to generalize significantly better than existing models and detect missing author entities with a reasonable level of accuracy.

</details>

<details>

<summary>2015-08-30 20:55:09 - Generalized Gompertz-power series distributions</summary>

- *Saeid Tahmasebi, Ali Akbar Jafari*

- `1508.07634v1` - [abs](http://arxiv.org/abs/1508.07634v1) - [pdf](http://arxiv.org/pdf/1508.07634v1)

> In this paper, we introduce the generalized Gompertz-power series class of distributions which is obtained by compounding generalized Gompertz and power series distributions. This compounding procedure follows same way that was previously carried out by Silva et al. (2013) and Barreto-Souza et al. (2011) in introducing the compound class of extended Weibull-power series distribution and the Weibull-geometric distribution, respectively. This distribution contains several lifetime models such as generalized Gompertz, generalized Gompertz-geometric, generalized Gompertz-poisson, generalized Gompertz-binomial distribution, and generalized Gompertz-logarithmic distribution as special cases. The hazard rate function of the new class of distributions can be increasing, decreasing and bathtub-shaped. We obtain several properties of this distribution such as its probability density function, Shannon entropy, its mean residual life and failure rate functions, quantiles and moments. The maximum likelihood estimation procedure via a EM-algorithm is presented, and sub-models of the distribution are studied in details.

</details>

<details>

<summary>2015-08-31 18:09:05 - Local and global robustness in conjugate Bayesian analysis</summary>

- *Vahed Maroufy, Paul Marriott*

- `1508.07937v1` - [abs](http://arxiv.org/abs/1508.07937v1) - [pdf](http://arxiv.org/pdf/1508.07937v1)

> This paper studies the influence of perturbations of conjugate priors in Bayesian inference. A perturbed prior is defined inside a larger family, local mixture models, and the effect on posterior inference is studied. The perturbation, in some sense, generalizes the linear perturbation studied in \cite{Gustafson1996}. It is intuitive, naturally normalized and is flexible for statistical applications. Both global and local sensitivity analyses are considered. A geometric approach is employed for optimizing the sensitivity direction function, the difference between posterior means and the divergence function between posterior predictive models. All the sensitivity measure functions are defined on a convex space with non-trivial boundary which is shown to be a smooth manifold.

</details>


## 2015-09

<details>

<summary>2015-09-01 01:32:51 - Bayesian Models for Heterogeneous Personalized Health Data</summary>

- *Kai Fan, Allison E. Aiello, Katherine A. Heller*

- `1509.00110v1` - [abs](http://arxiv.org/abs/1509.00110v1) - [pdf](http://arxiv.org/pdf/1509.00110v1)

> The purpose of this study is to leverage modern technology (such as mobile or web apps in Beckman et al. (2014)) to enrich epidemiology data and infer the transmission of disease. Homogeneity related research on population level has been intensively studied in previous work. In contrast, we develop hierarchical Graph-Coupled Hidden Markov Models (hGCHMMs) to simultaneously track the spread of infection in a small cell phone community and capture person-specific infection parameters by leveraging a link prior that incorporates additional covariates. We also reexamine the model evolution of the hGCHMM from simple HMMs and LDA, elucidating additional flexibility and interpretability. Due to the non-conjugacy of sparsely coupled HMMs, we design a new approximate distribution, allowing our approach to be more applicable to other application areas. Additionally, we investigate two common link functions, the beta-exponential prior and sigmoid function, both of which allow the development of a principled Bayesian hierarchical framework for disease transmission. The results of our model allow us to predict the probability of infection for each person on each day, and also to infer personal physical vulnerability and the relevant association with covariates. We demonstrate our approach experimentally on both simulation data and real epidemiological records.

</details>

<details>

<summary>2015-09-02 01:49:25 - A fast numerical method for max-convolution and the application to efficient max-product inference in Bayesian networks</summary>

- *Oliver Serang*

- `1501.02627v2` - [abs](http://arxiv.org/abs/1501.02627v2) - [pdf](http://arxiv.org/pdf/1501.02627v2)

> Observations depending on sums of random variables are common throughout many fields; however, no efficient solution is currently known for performing max-product inference on these sums of general discrete distributions (max-product inference can be used to obtain maximum a posteriori estimates). The limiting step to max-product inference is the max-convolution problem (sometimes presented in log-transformed form and denoted as "infimal convolution", "min-convolution", or "convolution on the tropical semiring"), for which no O(k log(k)) method is currently known. Here I present a O(k log(k)) numerical method for estimating the max-convolution of two nonnegative vectors (e.g., two probability mass functions), where k is the length of the larger vector. This numerical max-convolution method is then demonstrated by performing fast max-product inference on a convolution tree, a data structure for performing fast inference given information on the sum of n discrete random variables in O(n k log(n k) log(n) ) steps (where each random variable has an arbitrary prior distribution on k contiguous possible states). The numerical max-convolution method can be applied to specialized classes of hidden Markov models to reduce the runtime of computing the Viterbi path from n k^2 to n k log(k), and has potential application to the all-pairs shortest paths problem.

</details>

<details>

<summary>2015-09-02 03:25:10 - Scalable and efficient algorithms for the propagation of uncertainty from data through inference to prediction for large-scale problems, with application to flow of the Antarctic ice sheet</summary>

- *Tobin Isaac, Noemi Petra, Georg Stadler, Omar Ghattas*

- `1410.1221v2` - [abs](http://arxiv.org/abs/1410.1221v2) - [pdf](http://arxiv.org/pdf/1410.1221v2)

> The majority of research on efficient and scalable algorithms in computational science and engineering has focused on the forward problem: given parameter inputs, solve the governing equations to determine output quantities of interest. In contrast, here we consider the broader question: given a (large-scale) model containing uncertain parameters, (possibly) noisy observational data, and a prediction quantity of interest, how do we construct efficient and scalable algorithms to (1) infer the model parameters from the data (the deterministic inverse problem), (2) quantify the uncertainty in the inferred parameters (the Bayesian inference problem), and (3) propagate the resulting uncertain parameters through the model to issue predictions with quantified uncertainties (the forward uncertainty propagation problem)? We present efficient and scalable algorithms for this end-to-end, data-to-prediction process under the Gaussian approximation and in the context of modeling the flow of the Antarctic ice sheet and its effect on sea level. The ice is modeled as a viscous, incompressible, creeping, shear-thinning fluid. The observational data come from InSAR satellite measurements of surface ice flow velocity, and the uncertain parameter field to be inferred is the basal sliding parameter. The prediction quantity of interest is the present-day ice mass flux from the Antarctic continent to the ocean. We show that the work required for executing this data-to-prediction process is independent of the state dimension, parameter dimension, data dimension, and number of processor cores. The key to achieving this dimension independence is to exploit the fact that the observational data typically provide only sparse information on model parameters. This property can be exploited to construct a low rank approximation of the linearized parameter-to-observable map.

</details>

<details>

<summary>2015-09-02 11:34:31 - Compound random measures and their use in Bayesian nonparametrics</summary>

- *Jim E. Griffin, Fabrizio Leisen*

- `1410.0611v3` - [abs](http://arxiv.org/abs/1410.0611v3) - [pdf](http://arxiv.org/pdf/1410.0611v3)

> A new class of dependent random measures which we call {\it compound random measures} are proposed and the use of normalized versions of these random measures as priors in Bayesian nonparametric mixture models is considered. Their tractability allows the properties of both compound random measures and normalized compound random measures to be derived. In particular, we show how compound random measures can be constructed with gamma, $\sigma$-stable and generalized gamma process marginals. We also derive several forms of the Laplace exponent and characterize dependence through both the L\'evy copula and correlation function. A slice sampler and an augmented P\'olya urn scheme sampler are described for posterior inference when a normalized compound random measure is used as the mixing measure in a nonparametric mixture model and a data example is discussed.

</details>

<details>

<summary>2015-09-02 14:10:59 - Quasi-Newton particle Metropolis-Hastings</summary>

- *Johan Dahlin, Fredrik Lindsten, Thomas B. Schön*

- `1502.03656v2` - [abs](http://arxiv.org/abs/1502.03656v2) - [pdf](http://arxiv.org/pdf/1502.03656v2)

> Particle Metropolis-Hastings enables Bayesian parameter inference in general nonlinear state space models (SSMs). However, in many implementations a random walk proposal is used and this can result in poor mixing if not tuned correctly using tedious pilot runs. Therefore, we consider a new proposal inspired by quasi-Newton algorithms that may achieve similar (or better) mixing with less tuning. An advantage compared to other Hessian based proposals, is that it only requires estimates of the gradient of the log-posterior. A possible application is parameter inference in the challenging class of SSMs with intractable likelihoods. We exemplify this application and the benefits of the new proposal by modelling log-returns of future contracts on coffee by a stochastic volatility model with $\alpha$-stable observations.

</details>

<details>

<summary>2015-09-02 15:19:41 - Reliable ABC model choice via random forests</summary>

- *Pierre Pudlo, Jean-Michel Marin, Arnaud Estoup, Jean-Marie Cornuet, Mathieu Gautier, Christian P. Robert*

- `1406.6288v3` - [abs](http://arxiv.org/abs/1406.6288v3) - [pdf](http://arxiv.org/pdf/1406.6288v3)

> Approximate Bayesian computation (ABC) methods provide an elaborate approach to Bayesian inference on complex models, including model choice. Both theoretical arguments and simulation experiments indicate, however, that model posterior probabilities may be poorly evaluated by standard ABC techniques. We propose a novel approach based on a machine learning tool named random forests to conduct selection among the highly complex models covered by ABC algorithms. We thus modify the way Bayesian model selection is both understood and operated, in that we rephrase the inferential goal as a classification problem, first predicting the model that best fits the data with random forests and postponing the approximation of the posterior probability of the predicted MAP for a second stage also relying on random forests. Compared with earlier implementations of ABC model choice, the ABC random forest approach offers several potential improvements: (i) it often has a larger discriminative power among the competing models, (ii) it is more robust against the number and choice of statistics summarizing the data, (iii) the computing effort is drastically reduced (with a gain in computation efficiency of at least fifty), and (iv) it includes an approximation of the posterior probability of the selected model. The call to random forests will undoubtedly extend the range of size of datasets and complexity of models that ABC can handle. We illustrate the power of this novel methodology by analyzing controlled experiments as well as genuine population genetics datasets. The proposed methodologies are implemented in the R package abcrf available on the CRAN.

</details>

<details>

<summary>2015-09-02 18:51:02 - Bayesian Change Point Analysis of Linear Models on Graphs</summary>

- *Xiaofei Wang, John W. Emerson*

- `1509.00817v1` - [abs](http://arxiv.org/abs/1509.00817v1) - [pdf](http://arxiv.org/pdf/1509.00817v1)

> Consider observations $y_1,\dots,y_n$ on nodes of a connected graph, where the $y_i$ independently come from $N(\theta_i, \sigma^2)$ distributions and an unknown partition divides the $n$ observations into blocks. One well-studied class of change point problems assumes the means $\theta_i$ are equal for all nodes within contiguous blocks of a simple graph of sequential observations; both frequentist and Bayesian approaches have been used to estimate the $\theta_i$ and the change points of the underlying partition. This paper examines a broad class of change point problems on general connected graphs in which a regression model is assumed to apply within each block of the partition of the graph. This general class also supports multivariate change point problems. We use Bayesian methods to estimate change points or block boundaries of the underlying partition. This paper presents the methodology for the general class of change point problems and develops new algorithms for implementation via Markov Chain Monte Carlo. The paper concludes with simulations and real data examples to demonstrate application of the methodology on a wide range of problems.

</details>

<details>

<summary>2015-09-03 11:45:35 - Generalized Quantile Treatment Effect: A Flexible Bayesian Approach Using Quantile Ratio Smoothing</summary>

- *Sergio Venturini, Francesca Dominici, Giovanni Parmigiani*

- `1509.01042v1` - [abs](http://arxiv.org/abs/1509.01042v1) - [pdf](http://arxiv.org/pdf/1509.01042v1)

> We propose a new general approach for estimating the effect of a binary treatment on a continuous and potentially highly skewed response variable, the generalized quantile treatment effect (GQTE). The GQTE is defined as the difference between a function of the quantiles under the two treatment conditions. As such, it represents a generalization over the standard approaches typically used for estimating a treatment effect (i.e., the average treatment effect and the quantile treatment effect) because it allows the comparison of any arbitrary characteristic of the outcome's distribution under the two treatments. Following Dominici et al. (2005), we assume that a pre-specified transformation of the two quantiles is modeled as a smooth function of the percentiles. This assumption allows us to link the two quantile functions and thus to borrow information from one distribution to the other. The main theoretical contribution we provide is the analytical derivation of a closed form expression for the likelihood of the model. Exploiting this result we propose a novel Bayesian inferential methodology for the GQTE. We show some finite sample properties of our approach through a simulation study which confirms that in some cases it performs better than other nonparametric methods. As an illustration we finally apply our methodology to the 1987 National Medicare Expenditure Survey data to estimate the difference in the single hospitalization medical cost distributions between cases (i.e., subjects affected by smoking attributable diseases) and controls.

</details>

<details>

<summary>2015-09-03 12:46:27 - Necessary and Sufficient Conditions for High-Dimensional Posterior Consistency under $g$-Priors</summary>

- *Douglas K. Sparks, Kshitij Khare, Malay Ghosh*

- `1509.01060v1` - [abs](http://arxiv.org/abs/1509.01060v1) - [pdf](http://arxiv.org/pdf/1509.01060v1)

> We examine necessary and sufficient conditions for posterior consistency under $g$-priors, including extensions to hierarchical and empirical Bayesian models. The key features of this article are that we allow the number of regressors to grow at the same rate as the sample size and define posterior consistency under the sup vector norm instead of the more conventional Euclidean norm. We consider in particular the empirical Bayesian model of George and Foster (2000), the hyper-$g$-prior of Liang et al. (2008), and the prior considered by Zellner and Siow (1980).

</details>

<details>

<summary>2015-09-03 17:22:15 - Semi-described and semi-supervised learning with Gaussian processes</summary>

- *Andreas Damianou, Neil D. Lawrence*

- `1509.01168v1` - [abs](http://arxiv.org/abs/1509.01168v1) - [pdf](http://arxiv.org/pdf/1509.01168v1)

> Propagating input uncertainty through non-linear Gaussian process (GP) mappings is intractable. This hinders the task of training GPs using uncertain and partially observed inputs. In this paper we refer to this task as "semi-described learning". We then introduce a GP framework that solves both, the semi-described and the semi-supervised learning problems (where missing values occur in the outputs). Auto-regressive state space simulation is also recognised as a special case of semi-described learning. To achieve our goal we develop variational methods for handling semi-described inputs in GPs, and couple them with algorithms that allow for imputing the missing values while treating the uncertainty in a principled, Bayesian manner. Extensive experiments on simulated and real-world data study the problems of iterative forecasting and regression/classification with missing values. The results suggest that the principled propagation of uncertainty stemming from our framework can significantly improve performance in these tasks.

</details>

<details>

<summary>2015-09-04 05:33:37 - Frequentist coverage of adaptive nonparametric Bayesian credible sets</summary>

- *Botond Szabó, A. W. van der Vaart, J. H. van Zanten*

- `1310.4489v5` - [abs](http://arxiv.org/abs/1310.4489v5) - [pdf](http://arxiv.org/pdf/1310.4489v5)

> We investigate the frequentist coverage of Bayesian credible sets in a nonparametric setting. We consider a scale of priors of varying regularity and choose the regularity by an empirical Bayes method. Next we consider a central set of prescribed posterior probability in the posterior distribution of the chosen regularity. We show that such an adaptive Bayes credible set gives correct uncertainty quantification of "polished tail" parameters, in the sense of high probability of coverage of such parameters. On the negative side, we show by theory and example that adaptation of the prior necessarily leads to gross and haphazard uncertainty quantification for some true parameters that are still within the hyperrectangle regularity scale.

</details>

<details>

<summary>2015-09-04 05:49:15 - Bayesian Nonparametric Weighted Sampling Inference</summary>

- *Yajuan Si, Natesh S. Pillai, Andrew Gelman*

- `1309.1799v4` - [abs](http://arxiv.org/abs/1309.1799v4) - [pdf](http://arxiv.org/pdf/1309.1799v4)

> It has historically been a challenge to perform Bayesian inference in a design-based survey context. The present paper develops a Bayesian model for sampling inference in the presence of inverse-probability weights. We use a hierarchical approach in which we model the distribution of the weights of the nonsampled units in the population and simultaneously include them as predictors in a nonparametric Gaussian process regression. We use simulation studies to evaluate the performance of our procedure and compare it to the classical design-based estimator. We apply our method to the Fragile Family and Child Wellbeing Study. Our studies find the Bayesian nonparametric finite population estimator to be more robust than the classical design-based estimator without loss in efficiency, which works because we induce regularization for small cells and thus this is a way of automatically smoothing the highly variable weights.

</details>

<details>

<summary>2015-09-04 09:35:50 - Approximate Bayesian Computation for Forward Modeling in Cosmology</summary>

- *Joel Akeret, Alexandre Refregier, Adam Amara, Sebastian Seehars, Caspar Hasner*

- `1504.07245v3` - [abs](http://arxiv.org/abs/1504.07245v3) - [pdf](http://arxiv.org/pdf/1504.07245v3)

> Bayesian inference is often used in cosmology and astrophysics to derive constraints on model parameters from observations. This approach relies on the ability to compute the likelihood of the data given a choice of model parameters. In many practical situations, the likelihood function may however be unavailable or intractable due to non-gaussian errors, non-linear measurements processes, or complex data formats such as catalogs and maps. In these cases, the simulation of mock data sets can often be made through forward modeling. We discuss how Approximate Bayesian Computation (ABC) can be used in these cases to derive an approximation to the posterior constraints using simulated data sets. This technique relies on the sampling of the parameter set, a distance metric to quantify the difference between the observation and the simulations and summary statistics to compress the information in the data. We first review the principles of ABC and discuss its implementation using a Population Monte-Carlo (PMC) algorithm and the Mahalanobis distance metric. We test the performance of the implementation using a Gaussian toy model. We then apply the ABC technique to the practical case of the calibration of image simulations for wide field cosmological surveys. We find that the ABC analysis is able to provide reliable parameter constraints for this problem and is therefore a promising technique for other applications in cosmology and astrophysics. Our implementation of the ABC PMC method is made available via a public code release.

</details>

<details>

<summary>2015-09-04 11:20:47 - Latent drop-out transitions in quantile regression</summary>

- *Maria Francesca Marino, Marco Alfó*

- `1509.01405v1` - [abs](http://arxiv.org/abs/1509.01405v1) - [pdf](http://arxiv.org/pdf/1509.01405v1)

> Longitudinal data are characterized by the dependence between observations coming from the same individual. In a regression perspective, such a dependence can be usefully ascribed to unobserved features (covariates) specific to each individual. On these grounds, random parameter models with time-constant or time-varying structure are well established in the generalized linear model context. In the quantile regression framework, specifications based on random parameters have only recently known a flowering interest. We start from the recent proposal by Farcomeni (2012) on longitudinal quantile hidden Markov models, and extend it to handle potentially informative missing data mechanism. In particular, we focus on monotone missingness which may lead to selection bias and, therefore, to unreliable inferences on model parameters. We detail the proposed approach by re-analyzing a well known dataset on the dynamics of CD4 cell counts in HIV seroconverters and by means of a simulation study.

</details>

<details>

<summary>2015-09-04 23:13:27 - Stochastic gradient variational Bayes for gamma approximating distributions</summary>

- *David A. Knowles*

- `1509.01631v1` - [abs](http://arxiv.org/abs/1509.01631v1) - [pdf](http://arxiv.org/pdf/1509.01631v1)

> While stochastic variational inference is relatively well known for scaling inference in Bayesian probabilistic models, related methods also offer ways to circumnavigate the approximation of analytically intractable expectations. The key challenge in either setting is controlling the variance of gradient estimates: recent work has shown that for continuous latent variables, particularly multivariate Gaussians, this can be achieved by using the gradient of the log posterior. In this paper we apply the same idea to gamma distributed latent variables given gamma variational distributions, enabling straightforward "black box" variational inference in models where sparsity and non-negativity are appropriate. We demonstrate the method on a recently proposed gamma process model for network data, as well as a novel sparse factor analysis. We outperform generic sampling algorithms and the approach of using Gaussian variational distributions on transformed variables.

</details>

<details>

<summary>2015-09-07 04:42:42 - Discussion of "Frequentist coverage of adaptive nonparametric Bayesian credible sets"</summary>

- *Richard Nickl*

- `1410.7600v2` - [abs](http://arxiv.org/abs/1410.7600v2) - [pdf](http://arxiv.org/pdf/1410.7600v2)

> Discussion of "Frequentist coverage of adaptive nonparametric Bayesian credible sets" by Szab\'o, van der Vaart and van Zanten [arXiv:1310.4489v5].

</details>

<details>

<summary>2015-09-07 04:45:58 - Discussion of "Frequentist coverage of adaptive nonparametric Bayesian credible sets"</summary>

- *Ismaël Castillo*

- `1509.01900v1` - [abs](http://arxiv.org/abs/1509.01900v1) - [pdf](http://arxiv.org/pdf/1509.01900v1)

> Discussion of "Frequentist coverage of adaptive nonparametric Bayesian credible sets" by Szab\'o, van der Vaart and van Zanten [arXiv:1310.4489v5].

</details>

<details>

<summary>2015-09-07 05:07:34 - Discussion of "Frequentist coverage of adaptive nonparametric Bayesian credible sets"</summary>

- *Judith Rousseau*

- `1509.01903v1` - [abs](http://arxiv.org/abs/1509.01903v1) - [pdf](http://arxiv.org/pdf/1509.01903v1)

> Discussion of "Frequentist coverage of adaptive nonparametric Bayesian credible sets" by Szab\'o, van der Vaart and van Zanten [arXiv:1310.4489v5].

</details>

<details>

<summary>2015-09-07 05:10:13 - Discussion of "Frequentist coverage of adaptive nonparametric Bayesian credible sets"</summary>

- *Mark G. Low, Zongming Ma*

- `1509.01904v1` - [abs](http://arxiv.org/abs/1509.01904v1) - [pdf](http://arxiv.org/pdf/1509.01904v1)

> Discussion of "Frequentist coverage of adaptive nonparametric Bayesian credible sets" by Szab\'o, van der Vaart and van Zanten [arXiv:1310.4489v5].

</details>

<details>

<summary>2015-09-07 05:12:41 - Discussion of "Frequentist coverage of adaptive nonparametric Bayesian credible sets"</summary>

- *Subhashis Ghosal*

- `1509.01905v1` - [abs](http://arxiv.org/abs/1509.01905v1) - [pdf](http://arxiv.org/pdf/1509.01905v1)

> Discussion of "Frequentist coverage of adaptive nonparametric Bayesian credible sets" by Szab\'o, van der Vaart and van Zanten [arXiv:1310.4489v5].

</details>

<details>

<summary>2015-09-07 05:15:44 - Rejoinder to discussions of "Frequentist coverage of adaptive nonparametric Bayesian credible sets"</summary>

- *Botond Szabó, A. W. van der Vaart, J. H. van Zanten*

- `1509.01906v1` - [abs](http://arxiv.org/abs/1509.01906v1) - [pdf](http://arxiv.org/pdf/1509.01906v1)

> Rejoinder of "Frequentist coverage of adaptive nonparametric Bayesian credible sets" by Szab\'o, van der Vaart and van Zanten [arXiv:1310.4489v5].

</details>

<details>

<summary>2015-09-07 17:06:06 - Bayesian Latent Pattern Mixture Models for Handling Attrition in Panel Studies With Refreshment Samples</summary>

- *Yajuan Si, Jerome P. Reiter, D. Sunshine Hillygus*

- `1509.02124v1` - [abs](http://arxiv.org/abs/1509.02124v1) - [pdf](http://arxiv.org/pdf/1509.02124v1)

> Many panel studies collect refreshment samples---new, randomly sampled respondents who complete the questionnaire at the same time as a subsequent wave of the panel. With appropriate modeling, these samples can be leveraged to correct inferences for biases caused by non-ignorable attrition. We present such a model when the panel includes many categorical survey variables. The model relies on a Bayesian latent pattern mixture model, in which an indicator for attrition and the survey variables are modeled jointly via a latent class model. We allow the multinomial probabilities within classes to depend on the attrition indicator, which offers additional flexibility over standard applications of latent class models. We present results of simulation studies that illustrate the benefits of this flexibility. We apply the model to correct attrition bias in an analysis of data from the 2007-2008 Associated Press/Yahoo News election panel study.

</details>

<details>

<summary>2015-09-08 01:20:53 - Making the cut: improved ranking and selection for large-scale inference</summary>

- *Nicholas C. Henderson, Michael A. Newton*

- `1312.5776v5` - [abs](http://arxiv.org/abs/1312.5776v5) - [pdf](http://arxiv.org/pdf/1312.5776v5)

> Identifying leading measurement units from a large collection is a common inference task in various domains of large-scale inference. Testing approaches, which measure evidence against a null hypothesis rather than effect magnitude, tend to overpopulate lists of leading units with those associated with low measurement error. By contrast, local maximum likelihood (ML) approaches tend to favor units with high measurement error. Available Bayesian and empirical Bayesian approaches rely on specialized loss functions that result in similar deficiencies. We describe and evaluate a generic empirical Bayesian ranking procedure that populates the list of top units in a way that maximizes the expected overlap between the true and reported top lists for all list sizes. The procedure relates unit-specific posterior upper tail probabilities with their empirical distribution to yield a ranking variable. It discounts high-variance units less than popular non-ML methods and thus achieves improved operating characteristics in the models considered.

</details>

<details>

<summary>2015-09-08 16:39:41 - Parameter inference and model selection in deterministic and stochastic dynamical models via approximate Bayesian computation: modeling a wildlife epidemic</summary>

- *Libo Sun, Chihoon Lee, Jennifer A. Hoeting*

- `1409.7715v2` - [abs](http://arxiv.org/abs/1409.7715v2) - [pdf](http://arxiv.org/pdf/1409.7715v2)

> We consider the problem of selecting deterministic or stochastic models for a biological, ecological, or environmental dynamical process. In most cases, one prefers either deterministic or stochastic models as candidate models based on experience or subjective judgment. Due to the complex or intractable likelihood in most dynamical models, likelihood-based approaches for model selection are not suitable. We use approximate Bayesian computation for parameter estimation and model selection to gain further understanding of the dynamics of two epidemics of chronic wasting disease in mule deer. The main novel contribution of this work is that under a hierarchical model framework we compare three types of dynamical models: ordinary differential equation, continuous time Markov chain, and stochastic differential equation models. To our knowledge model selection between these types of models has not appeared previously. Since the practice of incorporating dynamical models into data models is becoming more common, the proposed approach may be very useful in a variety of applications.

</details>

<details>

<summary>2015-09-08 16:42:39 - A Variational Bayesian State-Space Approach to Online Passive-Aggressive Regression</summary>

- *Arnold Salas, Stephen J. Roberts, Michael A. Osborne*

- `1509.02438v1` - [abs](http://arxiv.org/abs/1509.02438v1) - [pdf](http://arxiv.org/pdf/1509.02438v1)

> Online Passive-Aggressive (PA) learning is a class of online margin-based algorithms suitable for a wide range of real-time prediction tasks, including classification and regression. PA algorithms are formulated in terms of deterministic point-estimation problems governed by a set of user-defined hyperparameters: the approach fails to capture model/prediction uncertainty and makes their performance highly sensitive to hyperparameter configurations. In this paper, we introduce a novel PA learning framework for regression that overcomes the above limitations. We contribute a Bayesian state-space interpretation of PA regression, along with a novel online variational inference scheme, that not only produces probabilistic predictions, but also offers the benefit of automatic hyperparameter tuning. Experiments with various real-world data sets show that our approach performs significantly better than a more standard, linear Gaussian state-space model.

</details>

<details>

<summary>2015-09-08 21:57:45 - A simple two-sample Bayesian t-test for hypothesis testing</summary>

- *Min Wang, Guangying Liu*

- `1509.02568v1` - [abs](http://arxiv.org/abs/1509.02568v1) - [pdf](http://arxiv.org/pdf/1509.02568v1)

> In this paper, we propose an explicit closed-form Bayes factor for the problem of two-sample hypothesis testing. The proposed approach can be regarded as a Bayesian version of the pooled-variance t-statistic and has various appealing properties in practical applications. It relies on data only through the t-statistic and can thus be calculated by using an Excel spreadsheet or a pocket calculator. It avoids several undesirable paradoxes, which may be encountered by the previous Bayesian approach of Gonen et al. (2005). Specifically, the proposed approach can be easily taught in an introductory statistics course with an emphasis on Bayesian thinking. Simulated and real data examples are provided for illustrative purposes.

</details>

<details>

<summary>2015-09-09 18:25:22 - On Parameter Estimation for Cusp-type Signals</summary>

- *Oleg Chernoyarov, Serguei Dachian, Yury Kutoyants*

- `1509.02880v1` - [abs](http://arxiv.org/abs/1509.02880v1) - [pdf](http://arxiv.org/pdf/1509.02880v1)

> We consider the problem of parameter estimation by the observations of deterministic signal in white gaussian noise. It is supposed that the signal has a singularity of cusp-type. The properties of the maximum likelihood and bayesian estimators are described in the asymptotics of small noise. Special attention is paid to the problem of parameter estimation in the situation of misspecification in regularity, i.e.; the statistician supposes that the observed signal has this singularity, but the real signal is smooth. The rate and the asymptotic distribution of the maximum likelihood estimator in this situation are described.

</details>

<details>

<summary>2015-09-10 09:34:03 - A Conversation with Alan Gelfand</summary>

- *Bradley P. Carlin, Amy H. Herring*

- `1509.03068v1` - [abs](http://arxiv.org/abs/1509.03068v1) - [pdf](http://arxiv.org/pdf/1509.03068v1)

> Alan E. Gelfand was born April 17, 1945, in the Bronx, New York. He attended public grade schools and did his undergraduate work at what was then called City College of New York (CCNY, now CUNY), excelling at mathematics. He then surprised and saddened his mother by going all the way across the country to Stanford to graduate school, where he completed his dissertation in 1969 under the direction of Professor Herbert Solomon, making him an academic grandson of Herman Rubin and Harold Hotelling. Alan then accepted a faculty position at the University of Connecticut (UConn) where he was promoted to tenured associate professor in 1975 and to full professor in 1980. A few years later he became interested in decision theory, then empirical Bayes, which eventually led to the publication of Gelfand and Smith [J. Amer. Statist. Assoc. 85 (1990) 398-409], the paper that introduced the Gibbs sampler to most statisticians and revolutionized Bayesian computing. In the mid-1990s, Alan's interests turned strongly to spatial statistics, leading to fundamental contributions in spatially-varying coefficient models, coregionalization, and spatial boundary analysis (wombling). He spent 33 years on the faculty at UConn, retiring in 2002 to become the James B. Duke Professor of Statistics and Decision Sciences at Duke University, serving as chair from 2007-2012. At Duke, he has continued his work in spatial methodology while increasing his impact in the environmental sciences. To date, he has published over 260 papers and 6 books; he has also supervised 36 Ph.D. dissertations and 10 postdocs. This interview was done just prior to a conference of his family, academic descendants, and colleagues to celebrate his 70th birthday and his contributions to statistics which took place on April 19-22, 2015 at Duke University.

</details>

<details>

<summary>2015-09-11 04:50:09 - Gaussian process surrogates for failure detection: a Bayesian experimental design approach</summary>

- *Hongqiao Wang, Guang Lin, Jinglai Li*

- `1509.04613v1` - [abs](http://arxiv.org/abs/1509.04613v1) - [pdf](http://arxiv.org/pdf/1509.04613v1)

> An important task of uncertainty quantification is to identify {the probability of} undesired events, in particular, system failures, caused by various sources of uncertainties. In this work we consider the construction of Gaussian {process} surrogates for failure detection and failure probability estimation. In particular, we consider the situation that the underlying computer models are extremely expensive, and in this setting, determining the sampling points in the state space is of essential importance. We formulate the problem as an optimal experimental design for Bayesian inferences of the limit state (i.e., the failure boundary) and propose an efficient numerical scheme to solve the resulting optimization problem. In particular, the proposed limit-state inference method is capable of determining multiple sampling points at a time, and thus it is well suited for problems where multiple computer simulations can be performed in parallel. The accuracy and performance of the proposed method is demonstrated by both academic and practical examples.

</details>

<details>

<summary>2015-09-11 12:49:28 - Modeling sea-level change using errors-in-variables integrated Gaussian processes</summary>

- *Niamh Cahill, Andrew C. Kemp, Benjamin P. Horton, Andrew C. Parnell*

- `1312.6761v5` - [abs](http://arxiv.org/abs/1312.6761v5) - [pdf](http://arxiv.org/pdf/1312.6761v5)

> We perform Bayesian inference on historical and late Holocene (last 2000 years) rates of sea-level change. The input data to our model are tide-gauge measurements and proxy reconstructions from cores of coastal sediment. These data are complicated by multiple sources of uncertainty, some of which arise as part of the data collection exercise. Notably, the proxy reconstructions include temporal uncertainty from dating of the sediment core using techniques such as radiocarbon. The model we propose places a Gaussian process prior on the rate of sea-level change, which is then integrated and set in an errors-in-variables framework to take account of age uncertainty. The resulting model captures the continuous and dynamic evolution of sea-level change with full consideration of all sources of uncertainty. We demonstrate the performance of our model using two real (and previously published) example data sets. The global tide-gauge data set indicates that sea-level rise increased from a rate with a posterior mean of 1.13 mm$/$yr in 1880 AD (0.89 to 1.28 mm$/$yr 95% credible interval for the posterior mean) to a posterior mean rate of 1.92 mm$/$yr in 2009 AD (1.84 to 2.03 mm$/$yr 95% credible interval for the posterior mean). The proxy reconstruction from North Carolina (USA) after correction for land-level change shows the 2000 AD rate of rise to have a posterior mean of 2.44 mm$/$yr (1.91 to 3.01 mm$/$yr 95% credible interval). This is unprecedented in at least the last 2000 years.

</details>

<details>

<summary>2015-09-11 13:12:56 - A shrinkage-thresholding Metropolis adjusted Langevin algorithm for Bayesian variable selection</summary>

- *Amandine Schreck, Gersende Fort, Sylvain Le Corff, Eric Moulines*

- `1312.5658v3` - [abs](http://arxiv.org/abs/1312.5658v3) - [pdf](http://arxiv.org/pdf/1312.5658v3)

> This paper introduces a new Markov Chain Monte Carlo method for Bayesian variable selection in high dimensional settings. The algorithm is a Hastings-Metropolis sampler with a proposal mechanism which combines a Metropolis Adjusted Langevin (MALA) step to propose local moves associated with a shrinkage-thresholding step allowing to propose new models. The geometric ergodicity of this new trans-dimensional Markov Chain Monte Carlo sampler is established. An extensive numerical experiment, on simulated and real data, is presented to illustrate the performance of the proposed algorithm in comparison with some more classical trans-dimensional algorithms.

</details>

<details>

<summary>2015-09-11 13:14:31 - Gradient Scan Gibbs Sampler: an efficient algorithm for high-dimensional Gaussian distributions</summary>

- *Olivier Féron, François Orieux, Jean-François Giovannelli*

- `1509.03495v1` - [abs](http://arxiv.org/abs/1509.03495v1) - [pdf](http://arxiv.org/pdf/1509.03495v1)

> This paper deals with Gibbs samplers that include high dimensional conditional Gaussian distributions. It proposes an efficient algorithm that avoids the high dimensional Gaussian sampling and relies on a random excursion along a small set of directions. The algorithm is proved to converge, i.e. the drawn samples are asymptotically distributed according to the target distribution. Our main motivation is in inverse problems related to general linear observation models and their solution in a hierarchical Bayesian framework implemented through sampling algorithms. It finds direct applications in semi-blind/unsupervised methods as well as in some non-Gaussian methods. The paper provides an illustration focused on the unsupervised estimation for super-resolution methods.

</details>

<details>

<summary>2015-09-11 18:02:57 - Gompertz - Power Series Distributions</summary>

- *Ali Akbar Jafari, Saeid Tahmasebi*

- `1509.03595v1` - [abs](http://arxiv.org/abs/1509.03595v1) - [pdf](http://arxiv.org/pdf/1509.03595v1)

> In this paper, we introduce the Gompertz power series class of distributions which is obtained by compounding Gompertz and power series distributions. This distribution contains several lifetime models such as Gompertz-geometric, Gompertz-Poisson, Gompertz-binomial, and Gompertz-logarithmic distributions as special cases. Sub-models of the GPS distribution are studied in details. The hazard rate function of the GPS distribution can be increasing, decreasing, and bathtub-shaped. We obtain several properties of the GPS distribution such as its probability density function, and failure rate function, Shannon entropy, mean residual life function, quantiles and moments. The maximum likelihood estimation procedure via a EM-algorithm is presented, and simulation studies are performed for evaluation of this estimation for complete data, and the MLE of parameters for censored data. At the end, a real example is given.

</details>

<details>

<summary>2015-09-13 09:17:43 - Statistical Analysis of Loopy Belief Propagation in Random Fields</summary>

- *Muneki Yasuda, Shun Kataoka, Kazuyuki Tanaka*

- `1503.04585v3` - [abs](http://arxiv.org/abs/1503.04585v3) - [pdf](http://arxiv.org/pdf/1503.04585v3)

> Loopy belief propagation (LBP), which is equivalent to the Bethe approximation in statistical mechanics, is a message-passing-type inference method that is widely used to analyze systems based on Markov random fields (MRFs). In this paper, we propose a message-passing-type method to analytically evaluate the quenched average of LBP in random fields by using the replica cluster variation method. The proposed analytical method is applicable to general pair-wise MRFs with random fields whose distributions differ from each other and can give the quenched averages of the Bethe free energies over random fields, which are consistent with numerical results. The order of its computational cost is equivalent to that of standard LBP. In the latter part of this paper, we describe the application of the proposed method to Bayesian image restoration, in which we observed that our theoretical results are in good agreement with the numerical results for natural images.

</details>

<details>

<summary>2015-09-13 18:53:41 - Stochastic simulators based optimization by Gaussian process metamodels - Application to maintenance investments planning issues</summary>

- *Thomas Browne, Bertrand Iooss, Loïc Le Gratiet, Jérome Lonchampt*

- `1509.03880v1` - [abs](http://arxiv.org/abs/1509.03880v1) - [pdf](http://arxiv.org/pdf/1509.03880v1)

> This paper deals with the construction of a metamodel (i.e. a simplified mathematical model) for a stochastic computer code (also called stochastic numerical model or stochastic simulator), where stochastic means that the code maps the realization of a random variable. The goal is to get, for a given model input, the main information about the output probability distribution by using this metamodel and without running the computer code. In practical applications, such a metamodel enables one to have estimations of every possible random variable properties, such as the expectation, the probability of exceeding a threshold or any quantile. The present work is concentrated on the emulation of the quantile function of the stochastic simulator by interpolating well chosen basis function and metamodeling their coefficients (using the Gaussian process metamodel). This quantile function metamodel is then used to treat a simple optimization strategy maintenance problem using a stochastic code, in order to optimize the quantile of an economic indicator. Using the Gaussian process framework, an adaptive design method (called QFEI) is defined by extending in our case the well known EGO algorithm. This allows to obtain an "optimal" solution using a small number of simulator runs.

</details>

<details>

<summary>2015-09-13 23:58:57 - When large n is not enough---Distribution-free Interval Estimators for Ratios of Quantiles</summary>

- *Luke A. Prendergast, Robert G. Staudte*

- `1508.06321v2` - [abs](http://arxiv.org/abs/1508.06321v2) - [pdf](http://arxiv.org/pdf/1508.06321v2)

> Ratios of sample percentiles or of quantiles based on a single sample are often published for skewed income data to illustrate aspects of income inequality, but distribution-free confidence intervals for such ratios are to our knowledge not in the literature. Here we derive and compare two large-sample methods for obtaining such intervals. They both require good distribution-free estimates of the quantile density at the quantiles of interest, and such estimates have recently become available. Simulation studies for various sample sizes are carried out for Pareto, lognormal and exponential distributions, as well as fitted generalized lambda distributions, to determine the coverage probabilities and widths of the intervals. Robustness of the estimators to contamination or a positive proportion of zero incomes is examined via influence functions. The motivating example is Australian household income data where ratios of quantiles measure inequality, but of course these results apply equally to data from other countries.

</details>

<details>

<summary>2015-09-14 03:11:40 - Causal Inference in Repeated Observational Studies: A Case Study of eBay Product Releases</summary>

- *Vadim von Brzeski, Matt Taddy, David Draper*

- `1509.03940v1` - [abs](http://arxiv.org/abs/1509.03940v1) - [pdf](http://arxiv.org/pdf/1509.03940v1)

> Causal inference in observational studies is notoriously difficult, due to the fact that the experimenter is not in charge of the treatment assignment mechanism. Many potential con- founding factors (PCFs) exist in such a scenario, and if one seeks to estimate the causal effect of the treatment on a response, one needs to control for such factors. Identifying all relevant PCFs may be difficult (or impossible) given a single observational study. Instead, we argue that if one can observe a sequence of similar treatments over the course of a lengthy time period, one can identify patterns of behavior in the experimental subjects that are correlated with the response of interest and control for those patterns directly. Specifically, in our case-study we find and control for an early-adopter effect: the scenario in which the magnitude of the response is highly correlated with how quickly one adopts a treatment after its release.   We provide a flexible hierarchical Bayesian framework that controls for such early-adopter effects in the analysis of the effects of multiple sequential treatments. The methods are presented and evaluated in the context of a detailed case-study involving product updates (newer versions of the same product) from eBay, Inc. The users in our study upgrade (or not) to a new version of the product at their own volition and timing. Our response variable is a measure of user actions, and we study the behavior of a large set of users (n = 10.5 million) in a targeted subset of eBay categories over a period of one year. We find that (a) naive causal estimates are hugely misleading and (b) our method, which is relatively insensitive to modeling assumptions and exhibits good out-of-sample predictive validation, yields sensible causal estimates that offer eBay a stable basis for decision-making.

</details>

<details>

<summary>2015-09-14 07:17:09 - Sex, lies and self-reported counts: Bayesian mixture models for heaping in longitudinal count data via birth-death processes</summary>

- *Forrest W. Crawford, Robert E. Weiss, Marc A. Suchard*

- `1405.4265v2` - [abs](http://arxiv.org/abs/1405.4265v2) - [pdf](http://arxiv.org/pdf/1405.4265v2)

> Surveys often ask respondents to report nonnegative counts, but respondents may misremember or round to a nearby multiple of 5 or 10. This phenomenon is called heaping, and the error inherent in heaped self-reported numbers can bias estimation. Heaped data may be collected cross-sectionally or longitudinally and there may be covariates that complicate the inferential task. Heaping is a well-known issue in many survey settings, and inference for heaped data is an important statistical problem. We propose a novel reporting distribution whose underlying parameters are readily interpretable as rates of misremembering and rounding. The process accommodates a variety of heaping grids and allows for quasi-heaping to values nearly but not equal to heaping multiples. We present a Bayesian hierarchical model for longitudinal samples with covariates to infer both the unobserved true distribution of counts and the parameters that control the heaping process. Finally, we apply our methods to longitudinal self-reported counts of sex partners in a study of high-risk behavior in HIV-positive youth.

</details>

<details>

<summary>2015-09-14 10:19:04 - Bayesian group Lasso for nonparametric varying-coefficient models with application to functional genome-wide association studies</summary>

- *Jiahan Li, Zhong Wang, Runze Li, Rongling Wu*

- `1509.04017v1` - [abs](http://arxiv.org/abs/1509.04017v1) - [pdf](http://arxiv.org/pdf/1509.04017v1)

> Although genome-wide association studies (GWAS) have proven powerful for comprehending the genetic architecture of complex traits, they are challenged by a high dimension of single-nucleotide polymorphisms (SNPs) as predictors, the presence of complex environmental factors, and longitudinal or functional natures of many complex traits or diseases. To address these challenges, we propose a high-dimensional varying-coefficient model for incorporating functional aspects of phenotypic traits into GWAS to formulate a so-called functional GWAS or fGWAS. The Bayesian group lasso and the associated MCMC algorithms are developed to identify significant SNPs and estimate how they affect longitudinal traits through time-varying genetic actions. The model is generalized to analyze the genetic control of complex traits using subject-specific sparse longitudinal data. The statistical properties of the new model are investigated through simulation studies. We use the new model to analyze a real GWAS data set from the Framingham Heart Study, leading to the identification of several significant SNPs associated with age-specific changes of body mass index. The fGWAS model, equipped with the Bayesian group lasso, will provide a useful tool for genetic and developmental analysis of complex traits or diseases.

</details>

<details>

<summary>2015-09-14 10:36:09 - A Bayesian feature allocation model for tumor heterogeneity</summary>

- *Juhee Lee, Peter Müller, Kamalakar Gulukota, Yuan Ji*

- `1509.04026v1` - [abs](http://arxiv.org/abs/1509.04026v1) - [pdf](http://arxiv.org/pdf/1509.04026v1)

> We develop a feature allocation model for inference on genetic tumor variation using next-generation sequencing data. Specifically, we record single nucleotide variants (SNVs) based on short reads mapped to human reference genome and characterize tumor heterogeneity by latent haplotypes defined as a scaffold of SNVs on the same homologous genome. For multiple samples from a single tumor, assuming that each sample is composed of some sample-specific proportions of these haplotypes, we then fit the observed variant allele fractions of SNVs for each sample and estimate the proportions of haplotypes. Varying proportions of haplotypes across samples is evidence of tumor heterogeneity since it implies varying composition of cell subpopulations. Taking a Bayesian perspective, we proceed with a prior probability model for all relevant unknown quantities, including, in particular, a prior probability model on the binary indicators that characterize the latent haplotypes. Such prior models are known as feature allocation models. Specifically, we define a simplified version of the Indian buffet process, one of the most traditional feature allocation models. The proposed model allows overlapping clustering of SNVs in defining latent haplotypes, which reflects the evolutionary process of subclonal expansion in tumor samples.

</details>

<details>

<summary>2015-09-14 12:56:23 - Spatial Bayesian variable selection and grouping for high-dimensional scalar-on-image regression</summary>

- *Fan Li, Tingting Zhang, Quanli Wang, Marlen Z. Gonzalez, Erin L. Maresh, James A. Coan*

- `1509.04069v1` - [abs](http://arxiv.org/abs/1509.04069v1) - [pdf](http://arxiv.org/pdf/1509.04069v1)

> Multi-subject functional magnetic resonance imaging (fMRI) data has been increasingly used to study the population-wide relationship between human brain activity and individual biological or behavioral traits. A common method is to regress the scalar individual response on imaging predictors, known as a scalar-on-image (SI) regression. Analysis and computation of such massive and noisy data with complex spatio-temporal correlation structure is challenging. In this article, motivated by a psychological study on human affective feelings using fMRI, we propose a joint Ising and Dirichlet Process (Ising-DP) prior within the framework of Bayesian stochastic search variable selection for selecting brain voxels in high-dimensional SI regressions. The Ising component of the prior makes use of the spatial information between voxels, and the DP component groups the coefficients of the large number of voxels to a small set of values and thus greatly reduces the posterior computational burden. To address the phase transition phenomenon of the Ising prior, we propose a new analytic approach to derive bounds for the hyperparameters, illustrated on 2- and 3-dimensional lattices. The proposed method is compared with several alternative methods via simulations, and is applied to the fMRI data collected from the KLIFF hand-holding experiment.

</details>

<details>

<summary>2015-09-14 18:16:48 - Bayesian Epidemic Detection in Multiple Populations</summary>

- *Michael Ludkovski, Katherine Shatskikh*

- `1509.04229v1` - [abs](http://arxiv.org/abs/1509.04229v1) - [pdf](http://arxiv.org/pdf/1509.04229v1)

> Traditional epidemic detection algorithms make decisions using only local information. We propose a novel approach that explicitly models spatial information fusion from several metapopulations. Our method also takes into account cost-benefit considerations regarding the announcement of epidemic. We utilize a compartmental stochastic model within a Bayesian detection framework which leads to a dynamic optimization problem. The resulting adaptive, non-parametric detection strategy optimally balances detection delay vis-a-vis probability of false alarms. Taking advantage of the underlying state-space structure, we represent the stopping rule in terms of a detection map which visualizes the relationship between the multivariate system state and policy making. It also allows us to obtain an efficient simulation-based solution algorithm that is based on the Sequential Regression Monte Carlo (SRMC) approach of Gramacy and Ludkovski (SIFIN, 2015). We illustrate our results on synthetic examples and also quantify the advantages of our adaptive detection relative to conventional threshold-based strategies.

</details>

<details>

<summary>2015-09-14 21:23:50 - Learning without Recall by Random Walks on Directed Graphs</summary>

- *Mohammad Amin Rahimian, Shahin Shahrampour, Ali Jadbabaie*

- `1509.04332v1` - [abs](http://arxiv.org/abs/1509.04332v1) - [pdf](http://arxiv.org/pdf/1509.04332v1)

> We consider a network of agents that aim to learn some unknown state of the world using private observations and exchange of beliefs. At each time, agents observe private signals generated based on the true unknown state. Each agent might not be able to distinguish the true state based only on her private observations. This occurs when some other states are observationally equivalent to the true state from the agent's perspective. To overcome this shortcoming, agents must communicate with each other to benefit from local observations. We propose a model where each agent selects one of her neighbors randomly at each time. Then, she refines her opinion using her private signal and the prior of that particular neighbor. The proposed rule can be thought of as a Bayesian agent who cannot recall the priors based on which other agents make inferences. This learning without recall approach preserves some aspects of the Bayesian inference while being computationally tractable. By establishing a correspondence with a random walk on the network graph, we prove that under the described protocol, agents learn the truth exponentially fast in the almost sure sense. The asymptotic rate is expressed as the sum of the relative entropies between the signal structures of every agent weighted by the stationary distribution of the random walk.

</details>

<details>

<summary>2015-09-14 21:29:54 - Mini-Minimax Uncertainty Quantification for Emulators</summary>

- *Jeffrey C. Regier, Philip B. Stark*

- `1303.3079v5` - [abs](http://arxiv.org/abs/1303.3079v5) - [pdf](http://arxiv.org/pdf/1303.3079v5)

> Consider approximating a "black box" function $f$ by an emulator $\hat{f}$ based on $n$ noiseless observations of $f$. Let $w$ be a point in the domain of $f$. How big might the error $|\hat{f}(w) - f(w)|$ be? If $f$ could be arbitrarily rough, this error could be arbitrarily large: we need some constraint on $f$ besides the data. Suppose $f$ is Lipschitz with known constant. We find a lower bound on the number of observations required to ensure that for the best emulator $\hat{f}$ based on the $n$ data, $|\hat{f}(w) - f(w)| \le \epsilon$. But in general, we will not know whether $f$ is Lipschitz, much less know its Lipschitz constant. Assume optimistically that $f$ is Lipschitz-continuous with the smallest constant consistent with the $n$ data. We find the maximum (over such regular $f$) of $|\hat{f}(w) - f(w)|$ for the best possible emulator $\hat{f}$; we call this the "mini-minimax uncertainty" at $w$. In reality, $f$ might not be Lipschitz or---if it is---it might not attain its Lipschitz constant on the data. Hence, the mini-minimax uncertainty at $w$ could be much smaller than $|\hat{f}(w) - f(w)|$. But if the mini-minimax uncertainty is large, then---even if $f$ satisfies the optimistic regularity assumption---$|\hat{f}(w) - f(w)|$ could be large, no matter how cleverly we choose $\hat{f}$. For the Community Atmosphere Model, the maximum (over $w$) of the mini-minimax uncertainty based on a set of 1154~observations of $f$ is no smaller than it would be for a single observation of $f$ at the centroid of the 21-dimensional parameter space. We also find lower confidence bounds for quantiles of the mini-minimax uncertainty and its mean over the domain of $f$. For the Community Atmosphere Model, these lower confidence bounds are an appreciable fraction of the maximum.

</details>

<details>

<summary>2015-09-15 13:49:18 - Bayesian Gaussian Copula Graphical Modeling for Dupuytren Disease</summary>

- *Abdolreza Mohammadi, Fentaw Abegaz, Edwin van den Heuvel, Ernst C. Wit*

- `1501.04849v4` - [abs](http://arxiv.org/abs/1501.04849v4) - [pdf](http://arxiv.org/pdf/1501.04849v4)

> Dupuytren disease is a fibroproliferative disorder with unknown etiology that often progresses and eventually can cause permanent contractures of the affected fingers. In this paper, we provide a computationally efficient Bayesian framework to discover potential risk factors and investigate which fingers are jointly affected. Our Bayesian approach is based on Gaussian copula graphical models, which are one potential way to discover the underlying conditional independence structure of variables in multivariate mixed data. In particular, we combine the semiparametric Gaussian copula with extended rank likelihood which is appropriate to analyse multivariate mixed data with arbitrary marginal distributions. For the graph structure learning, we construct a computationally efficient search algorithm which is a trans-dimensional MCMC algorithm based on a birth-death process. In addition, to make our statistical method easily accessible to other researchers, we have implemented our method in C++ and interfaced with R software as an R package BDgraph which is available online.

</details>

<details>

<summary>2015-09-15 14:22:44 - Bayesian Structured Sparsity Priors for EEG Source Localization Technical Report</summary>

- *Facundo Costa, Hadj Batatia, Thomas Oberlin, Jean-Yves Tourneret*

- `1509.04576v1` - [abs](http://arxiv.org/abs/1509.04576v1) - [pdf](http://arxiv.org/pdf/1509.04576v1)

> This report introduces a new hierarchical Bayesian model for the EEG source localization problem. This model promotes structured sparsity to search for focal brain activity. This sparsity is obtained via a multivariate Bernoulli Laplacian prior assigned to the brain activity approximating an $\ell_{20}$ pseudo norm regularization in a Bayesian framework. A partially collapsed Gibbs sampler is used to draw samples asymptotically distributed according to the posterior associated with the proposed Bayesian model. The generated samples are used to estimate the brain activity and the model hyperparameters jointly in an unsupervised framework. Two different kinds of Metropolis-Hastings moves are introduced to accelerate the convergence of the Gibbs sampler. The first move is based on multiple dipole shifts within each MCMC chain whereas the second one exploits proposals associated with different MCMC chains. We use both synthetic and real data to compare the performance of the proposed method with the weighted $\ell_{21}$ mixed norm regularization and a method based on a multiple sparse prior, showing that our algorithm presents advantages in several scenarios.

</details>

<details>

<summary>2015-09-15 18:56:30 - How Many Communities Are There?</summary>

- *Diego Franco Saldana, Yi Yu, Yang Feng*

- `1412.1684v2` - [abs](http://arxiv.org/abs/1412.1684v2) - [pdf](http://arxiv.org/pdf/1412.1684v2)

> Stochastic blockmodels and variants thereof are among the most widely used approaches to community detection for social networks and relational data. A stochastic blockmodel partitions the nodes of a network into disjoint sets, called communities. The approach is inherently related to clustering with mixture models; and raises a similar model selection problem for the number of communities. The Bayesian information criterion (BIC) is a popular solution, however, for stochastic blockmodels, the conditional independence assumption given the communities of the endpoints among different edges is usually violated in practice. In this regard, we propose composite likelihood BIC (CL-BIC) to select the number of communities, and we show it is robust against possible misspecifications in the underlying stochastic blockmodel assumptions. We derive the requisite methodology and illustrate the approach using both simulated and real data. Supplementary materials containing the relevant computer code are available online.

</details>

<details>

<summary>2015-09-15 19:40:46 - Learning the Structure for Structured Sparsity</summary>

- *Nino Shervashidze, Francis Bach*

- `1503.03082v2` - [abs](http://arxiv.org/abs/1503.03082v2) - [pdf](http://arxiv.org/pdf/1503.03082v2)

> Structured sparsity has recently emerged in statistics, machine learning and signal processing as a promising paradigm for learning in high-dimensional settings. All existing methods for learning under the assumption of structured sparsity rely on prior knowledge on how to weight (or how to penalize) individual subsets of variables during the subset selection process, which is not available in general. Inferring group weights from data is a key open research problem in structured sparsity.In this paper, we propose a Bayesian approach to the problem of group weight learning. We model the group weights as hyperparameters of heavy-tailed priors on groups of variables and derive an approximate inference scheme to infer these hyperparameters. We empirically show that we are able to recover the model hyperparameters when the data are generated from the model, and we demonstrate the utility of learning weights in synthetic and real denoising problems.

</details>

<details>

<summary>2015-09-16 06:14:38 - Comment on Article by Ferreira and Gamerman</summary>

- *Michael Chipeta, Peter J. Diggle*

- `1509.04817v1` - [abs](http://arxiv.org/abs/1509.04817v1) - [pdf](http://arxiv.org/pdf/1509.04817v1)

> Comment on Article by Ferreira and Gamerman [arXiv:1509.03410].

</details>

<details>

<summary>2015-09-16 06:22:31 - Comment on Article by Ferreira and Gamerman</summary>

- *Noel Cressie, Raymond L. Chambers*

- `1509.04819v1` - [abs](http://arxiv.org/abs/1509.04819v1) - [pdf](http://arxiv.org/pdf/1509.04819v1)

> A utility-function approach to optimal spatial sampling design is a powerful way to quantify what "optimality" means. The emphasis then should be to capture all possible contributions to utility, including scientific impact and the cost of sampling. The resulting sampling plan should contain a component of designed randomness that would allow for a non-parametric design-based analysis if model-based assumptions were in doubt. [arXiv:1509.03410]

</details>

<details>

<summary>2015-09-16 06:33:09 - Comment on Article by Ferreira and Gamerman</summary>

- *James V. Zidek*

- `1509.04821v1` - [abs](http://arxiv.org/abs/1509.04821v1) - [pdf](http://arxiv.org/pdf/1509.04821v1)

> Comment on Article by Ferreira and Gamerman [arXiv:1509.03410].

</details>

<details>

<summary>2015-09-16 07:30:55 - hmmSeq: A hidden Markov model for detecting differentially expressed genes from RNA-seq data</summary>

- *Shiqi Cui, Subharup Guha, Marco A. R. Ferreira, Allison N. Tegge*

- `1509.04838v1` - [abs](http://arxiv.org/abs/1509.04838v1) - [pdf](http://arxiv.org/pdf/1509.04838v1)

> We introduce hmmSeq, a model-based hierarchical Bayesian technique for detecting differentially expressed genes from RNA-seq data. Our novel hmmSeq methodology uses hidden Markov models to account for potential co-expression of neighboring genes. In addition, hmmSeq employs an integrated approach to studies with technical or biological replicates, automatically adjusting for any extra-Poisson variability. Moreover, for cases when paired data are available, hmmSeq includes a paired structure between treatments that incoporates subject-specific effects. To perform parameter estimation for the hmmSeq model, we develop an efficient Markov chain Monte Carlo algorithm. Further, we develop a procedure for detection of differentially expressed genes that automatically controls false discovery rate. A simulation study shows that the hmmSeq methodology performs better than competitors in terms of receiver operating characteristic curves. Finally, the analyses of three publicly available RNA-seq data sets demonstrate the power and flexibility of the hmmSeq methodology. An R package implementing the hmmSeq framework will be submitted to CRAN upon publication of the manuscript.

</details>

<details>

<summary>2015-09-16 07:40:43 - Tracking rapid intracellular movements: A Bayesian random set approach</summary>

- *Vasileios Maroulas, Andreas Nebenführ*

- `1509.04841v1` - [abs](http://arxiv.org/abs/1509.04841v1) - [pdf](http://arxiv.org/pdf/1509.04841v1)

> We focus on the biological problem of tracking organelles as they move through cells. In the past, most intracellular movements were recorded manually, however, the results are too incomplete to capture the full complexity of organelle motions. An automated tracking algorithm promises to provide a complete analysis of noisy microscopy data. In this paper, we adopt statistical techniques from a Bayesian random set point of view. Instead of considering each individual organelle, we examine a random set whose members are the organelle states and we establish a Bayesian filtering algorithm involving such set states. The propagated multi-object densities are approximated using a Gaussian mixture scheme. Our algorithm is applied to synthetic and experimental data.

</details>

<details>

<summary>2015-09-16 10:18:31 - Bayesian detection of embryonic gene expression onset in C. elegans</summary>

- *Jie Hu, Zhongying Zhao, Hari Krishna Yalamanchili, Junwen Wang, Kenny Ye, Xiaodan Fan*

- `1509.04872v1` - [abs](http://arxiv.org/abs/1509.04872v1) - [pdf](http://arxiv.org/pdf/1509.04872v1)

> To study how a zygote develops into an embryo with different tissues, large-scale 4D confocal movies of C. elegans embryos have been produced recently by experimental biologists. However, the lack of principled statistical methods for the highly noisy data has hindered the comprehensive analysis of these data sets. We introduced a probabilistic change point model on the cell lineage tree to estimate the embryonic gene expression onset time. A Bayesian approach is used to fit the 4D confocal movies data to the model. Subsequent classification methods are used to decide a model selection threshold and further refine the expression onset time from the branch level to the specific cell time level. Extensive simulations have shown the high accuracy of our method. Its application on real data yields both previously known results and new findings.

</details>

<details>

<summary>2015-09-16 10:27:12 - Assessing phenotypic correlation through the multivariate phylogenetic latent liability model</summary>

- *Gabriela B. Cybis, Janet S. Sinsheimer, Trevor Bedford, Alison E. Mather, Philippe Lemey, Marc A. Suchard*

- `1406.3863v2` - [abs](http://arxiv.org/abs/1406.3863v2) - [pdf](http://arxiv.org/pdf/1406.3863v2)

> Understanding which phenotypic traits are consistently correlated throughout evolution is a highly pertinent problem in modern evolutionary biology. Here, we propose a multivariate phylogenetic latent liability model for assessing the correlation between multiple types of data, while simultaneously controlling for their unknown shared evolutionary history informed through molecular sequences. The latent formulation enables us to consider in a single model combinations of continuous traits, discrete binary traits and discrete traits with multiple ordered and unordered states. Previous approaches have entertained a single data type generally along a fixed history, precluding estimation of correlation between traits and ignoring uncertainty in the history. We implement our model in a Bayesian phylogenetic framework, and discuss inference techniques for hypothesis testing. Finally, we showcase the method through applications to columbine flower morphology, antibiotic resistance in Salmonella and epitope evolution in influenza.

</details>

<details>

<summary>2015-09-17 01:44:49 - Case-Deletion Diagnostics for Quantile Regression Using the Asymmetric Laplace Distribution</summary>

- *Luis E. Benites, Víctor H. Lachos, Filidor E. Vilca*

- `1509.05099v1` - [abs](http://arxiv.org/abs/1509.05099v1) - [pdf](http://arxiv.org/pdf/1509.05099v1)

> To make inferences about the shape of a population distribution, the widely popular mean regression model, for example, is inadequate if the distribution is not approximately Gaussian (or symmetric). Compared to conventional mean regression (MR), quantile regression (QR) can characterize the entire conditional distribution of the outcome variable, and is more robust to outliers and misspecification of the error distribution. We present a likelihood-based approach to the estimation of the regression quantiles based on the asymmetric Laplace distribution (ALD), which has a hierarchical representation that facilitates the implementation of the EM algorithm for the maximum-likelihood estimation. We develop a case-deletion diagnostic analysis for QR models based on the conditional expectation of the complete-data log-likelihood function related to the EM algorithm. The techniques are illustrated with both simulated and real data sets, showing that our approach out-performed other common classic estimators. The proposed algorithm and methods are implemented in the R package ALDqr().

</details>

<details>

<summary>2015-09-17 12:24:30 - Bayesian structured additive distributional regression with an application to regional income inequality in Germany</summary>

- *Nadja Klein, Thomas Kneib, Stefan Lang, Alexander Sohn*

- `1509.05230v1` - [abs](http://arxiv.org/abs/1509.05230v1) - [pdf](http://arxiv.org/pdf/1509.05230v1)

> We propose a generic Bayesian framework for inference in distributional regression models in which each parameter of a potentially complex response distribution and not only the mean is related to a structured additive predictor. The latter is composed additively of a variety of different functional effect types such as nonlinear effects, spatial effects, random coefficients, interaction surfaces or other (possibly nonstandard) basis function representations. To enforce specific properties of the functional effects such as smoothness, informative multivariate Gaussian priors are assigned to the basis function coefficients. Inference can then be based on computationally efficient Markov chain Monte Carlo simulation techniques where a generic procedure makes use of distribution-specific iteratively weighted least squares approximations to the full conditionals. The framework of distributional regression encompasses many special cases relevant for treating nonstandard response structures such as highly skewed nonnegative responses, overdispersed and zero-inflated counts or shares including the possibility for zero- and one-inflation. We discuss distributional regression along a study on determinants of labour incomes for full-time working males in Germany with a particular focus on regional differences after the German reunification. Controlling for age, education, work experience and local disparities, we estimate full conditional income distributions allowing us to study various distributional quantities such as moments, quantiles or inequality measures in a consistent manner in one joint model. Detailed guidance on practical aspects of model choice including the selection of several competing distributions for labour incomes and the consideration of different covariate effects on the income distribution complete the distributional regression analysis. We find that next to a lower expected income, full-time working men in East Germany also face a more unequal income distribution than men in the West, ceteris paribus.

</details>

<details>

<summary>2015-09-17 16:17:58 - A Simulated Annealing Approach to Bayesian Inference</summary>

- *Carlo Albert*

- `1509.05315v1` - [abs](http://arxiv.org/abs/1509.05315v1) - [pdf](http://arxiv.org/pdf/1509.05315v1)

> A generic algorithm for the extraction of probabilistic (Bayesian) information about model parameters from data is presented. The algorithm propagates an ensemble of particles in the product space of model parameters and outputs. Each particle update consists of a random jump in parameter space followed by a simulation of a model output and a Metropolis acceptance/rejection step based on a comparison of the simulated output to the data. The distance of a particle to the data is interpreted as an energy and the algorithm is reducing the associated temperature of the ensemble such that entropy production is minimized. If this simulated annealing is not too fast compared to the mixing speed in parameter space, the parameter marginal of the ensemble approaches the Bayesian posterior distribution. Annealing is adaptive and depends on certain extensive thermodynamic quantities that can easily be measured throughout run-time. In the general case, we propose annealing with a constant entropy production rate, which is optimal as long as annealing is not too fast. For the practically relevant special case of no prior knowledge, we derive an optimal fast annealing schedule with a non-constant entropy production rate. The algorithm does not require the calculation of the density of the model likelihood, which makes it interesting for Bayesian parameter inference with stochastic models, whose likelihood functions are typically very high dimensional integrals.

</details>

<details>

<summary>2015-09-17 16:58:24 - Capture-recapture abundance estimation using a semi-complete data likelihood approach</summary>

- *Ruth King, Brett T. McClintock, Darren Kidney, David Borchers*

- `1508.06313v2` - [abs](http://arxiv.org/abs/1508.06313v2) - [pdf](http://arxiv.org/pdf/1508.06313v2)

> Capture-recapture data are often collected when abundance estimation is of interest. In the presence of unobserved individual heterogeneity, specified on a continuous scale for the capture probabilities, the likelihood is not generally available in closed form, but expressible only as an analytically intractable integral. Model-fitting algorithms to estimate abundance most notably include a numerical approximation for the likelihood or use of a Bayesian data augmentation technique considering the complete data likelihood. We consider a Bayesian hybrid approach, defining a "semi-complete" data likelihood, composed of the product of a complete data likelihood component for individuals seen at least once within the study and a marginal data likelihood component for the individuals not seen within the study, approximated using numerical integration. This approach combines the advantages of the two different approaches, with the semi-complete likelihood component specified as a single integral (over the dimension of the individual heterogeneity component). In addition, the models can be fitted within BUGS/JAGS (commonly used for the Bayesian complete data likelihood approach) but with significantly improved computational efficiency compared to the commonly used super-population data augmentation approaches (between about 10 and 77 times more efficient in the two examples we consider). The semi-complete likelihood approach is flexible and applicable to a range of models, including spatially explicit capture-recapture models. The model-fitting approach is applied to two different datasets corresponding to the closed population model $M_h$ for snowshoe hare data and a spatially explicit capture-recapture model applied to gibbon data.

</details>

<details>

<summary>2015-09-18 14:25:57 - Non parametric Bayesian approach to LR assessment in case of rare haplotype match</summary>

- *Giulia Cereda*

- `1506.08444v3` - [abs](http://arxiv.org/abs/1506.08444v3) - [pdf](http://arxiv.org/pdf/1506.08444v3)

> The evaluation of a match between the DNA profile of a stain found on a crime scene and that of a suspect (previously identified) involves the use of the unknown parameter $p=(p_1, p_2, ...)$, (the ordered vector which represents the proportions of the different DNA profiles in the population of potential donors) and the names of the different DNA types. We propose a Bayesian non parametric method which considers $P$ as a random variable distributed according to the two-parameter Poisson Dirichlet distribution, and discard information about names of DNA types. The ultimate goal of this model is to evaluate DNA matches in the rare type case, that is the situation in which the suspect's profile, matching the crime stain profile, is not one of those in the database of reference.

</details>

<details>

<summary>2015-09-21 16:29:16 - A Bayesian Compressed Sensing Kalman Filter for Direction of Arrival Estimation</summary>

- *Matthew Hawes, Lyudmila Mihaylova, Francois Septier, Simon Godsill*

- `1509.06290v1` - [abs](http://arxiv.org/abs/1509.06290v1) - [pdf](http://arxiv.org/pdf/1509.06290v1)

> In this paper, we look to address the problem of estimating the dynamic direction of arrival (DOA) of a narrowband signal impinging on a sensor array from the far field. The initial estimate is made using a Bayesian compressive sensing (BCS) framework and then tracked using a Bayesian compressed sensing Kalman filter (BCSKF). The BCS framework splits the angular region into N potential DOAs and enforces a belief that only a few of the DOAs will have a non-zero valued signal present. A BCSKF can then be used to track the change in the DOA using the same framework. There can be an issue when the DOA approaches the endfire of the array. In this angular region current methods can struggle to accurately estimate and track changes in the DOAs. To tackle this problem, we propose changing the traditional sparse belief associated with BCS to a belief that the estimated signals will match the predicted signals given a known DOA change. This is done by modelling the difference between the expected sparse received signals and the estimated sparse received signals as a Gaussian distribution. Example test scenarios are provided and comparisons made with the traditional BCS based estimation method. They show that an improvement in estimation accuracy is possible without a significant increase in computational complexity.

</details>

<details>

<summary>2015-09-21 20:55:12 - Monge-Kantorovich Depth, Quantiles, Ranks, and Signs</summary>

- *Victor Chernozhukov, Alfred Galichon, Marc Hallin, Marc Henry*

- `1412.8434v4` - [abs](http://arxiv.org/abs/1412.8434v4) - [pdf](http://arxiv.org/pdf/1412.8434v4)

> We propose new concepts of statistical depth, multivariate quantiles, ranks and signs, based on canonical transportation maps between a distribution of interest on $R^d$ and a reference distribution on the $d$-dimensional unit ball. The new depth concept, called Monge-Kantorovich depth, specializes to halfspace depth in the case of spherical distributions, but, for more general distributions, differs from the latter in the ability for its contours to account for non convex features of the distribution of interest. We propose empirical counterparts to the population versions of those Monge-Kantorovich depth contours, quantiles, ranks and signs, and show their consistency by establishing a uniform convergence property for empirical transport maps, which is of independent interest.

</details>

<details>

<summary>2015-09-22 03:13:42 - Estimation and uncertainty of reversible Markov models</summary>

- *Benjamin Trendelkamp-Schroer, Hao Wu, Fabian Paul, Frank Noé*

- `1507.05990v2` - [abs](http://arxiv.org/abs/1507.05990v2) - [pdf](http://arxiv.org/pdf/1507.05990v2)

> Reversibility is a key concept in Markov models and Master-equation models of molecular kinetics. The analysis and interpretation of the transition matrix encoding the kinetic properties of the model relies heavily on the reversibility property. The estimation of a reversible transition matrix from simulation data is therefore crucial to the successful application of the previously developed theory. In this work we discuss methods for the maximum likelihood estimation of transition matrices from finite simulation data and present a new algorithm for the estimation if reversibility with respect to a given stationary vector is desired. We also develop new methods for the Bayesian posterior inference of reversible transition matrices with and without given stationary vector taking into account the need for a suitable prior distribution preserving the meta- stable features of the observed process during posterior inference. All algorithms here are implemented in the PyEMMA software - http://pyemma.org - as of version 2.0.

</details>

<details>

<summary>2015-09-22 07:41:00 - Bayesian Conditional Density Filtering</summary>

- *Shaan Qamar, Rajarshi Guhaniyogi, David B. Dunson*

- `1401.3632v3` - [abs](http://arxiv.org/abs/1401.3632v3) - [pdf](http://arxiv.org/pdf/1401.3632v3)

> We propose a Conditional Density Filtering (C-DF) algorithm for efficient online Bayesian inference. C-DF adapts MCMC sampling to the online setting, sampling from approximations to conditional posterior distributions obtained by propagating surrogate conditional sufficient statistics (a function of data and parameter estimates) as new data arrive. These quantities eliminate the need to store or process the entire dataset simultaneously and offer a number of desirable features. Often, these include a reduction in memory requirements and runtime and improved mixing, along with state-of-the-art parameter inference and prediction. These improvements are demonstrated through several illustrative examples including an application to high dimensional compressed regression. Finally, we show that C-DF samples converge to the target posterior distribution asymptotically as sampling proceeds and more data arrives.

</details>

<details>

<summary>2015-09-22 08:07:15 - Bayesian Tensor Regression</summary>

- *Rajarshi Guhaniyogi, Shaan Qamar, David B. Dunson*

- `1509.06490v1` - [abs](http://arxiv.org/abs/1509.06490v1) - [pdf](http://arxiv.org/pdf/1509.06490v1)

> This article proposes a Bayesian approach to regression with a scalar response against vector and tensor covariates. Tensor covariates are commonly vectorized prior to analysis, failing to exploit the structure of the tensor, and resulting in poor estimation and predictive performance. We develop a novel class of multiway shrinkage priors for the coefficients in tensor regression models. Properties are described, including posterior consistency under mild conditions, and an efficient Markov chain Monte Carlo algorithm is developed for posterior computation. Simulation studies illustrate substantial gains over vectorizing or using existing tensor regression methods in terms of estimation and parameter inference. The approach is further illustrated in a neuroimaging application.   Keywords: Dimension reduction; multiway shrinkage prior; magnetic resonance imaging (MRI); parafac decomposition; posterior consistency; tensor regression

</details>

<details>

<summary>2015-09-22 09:05:56 - A New Robust Regression Method Based on Minimization of Geodesic Distances on a Probabilistic Manifold: Application to Power Laws</summary>

- *Geert Verdoolaege*

- `1509.06513v1` - [abs](http://arxiv.org/abs/1509.06513v1) - [pdf](http://arxiv.org/pdf/1509.06513v1)

> In regression analysis for deriving scaling laws that occur in various scientific disciplines, usually standard regression methods have been applied, of which ordinary least squares (OLS) is the most popular. In many situations, the assumptions underlying OLS are not fulfilled, and several other approaches have been proposed. However, most techniques address only part of the shortcomings of OLS. We here discuss a new and more general regression method, which we call geodesic least squares regression (GLS). The method is based on minimization of the Rao geodesic distance on a probabilistic manifold. For the case of a power law, we demonstrate the robustness of the method on synthetic data in the presence of significant uncertainty on both the data and the regression model. We then show good performance of the method in an application to a scaling law in magnetic confinement fusion.

</details>

<details>

<summary>2015-09-22 10:55:02 - Optimal Design in Geostatistics under Preferential Sampling</summary>

- *Gustavo da Silva Ferreira, Dani Gamerman*

- `1509.03410v2` - [abs](http://arxiv.org/abs/1509.03410v2) - [pdf](http://arxiv.org/pdf/1509.03410v2)

> This paper analyses the effect of preferential sampling in Geostatistics when the choice of new sampling locations is the main interest of the researcher. A Bayesian criterion based on maximizing utility functions is used. Simulated studies are presented and highlight the strong influence of preferential sampling in the decisions. The computational complexity is faced by treating the new local sampling locations as a model parameter and the optimal choice is then made by analysing its posterior distribution. Finally, an application is presented using rainfall data collected during spring in Rio de Janeiro. The results showed that the optimal design is substantially changed under preferential sampling effects. Furthermore, it was possible to identify other interesting aspects related to preferential sampling effects in estimation and prediction in Geostatistics. With the Rejoinder to Comments [arXiv:1509.04817], [arXiv:1509.04819], [arXiv:1509.04821].

</details>

<details>

<summary>2015-09-22 21:15:12 - A Bayesian framework for verification and recalibration of ensemble forecasts: How uncertain is NAO predictability?</summary>

- *Stefan Siegert, David B. Stephenson, Philip G. Sansom, Adam A. Scaife, Rosie Eade, Alberto Arribas*

- `1504.01933v2` - [abs](http://arxiv.org/abs/1504.01933v2) - [pdf](http://arxiv.org/pdf/1504.01933v2)

> Predictability estimates of ensemble prediction systems are uncertain due to limited numbers of past forecasts and observations. To account for such uncertainty, this paper proposes a Bayesian inferential framework that provides a simple 6-parameter representation of ensemble forecasting systems and the corresponding observations. The framework is probabilistic, and thus allows for quantifying uncertainty in predictability measures such as correlation skill and signal-to-noise ratios. It also provides a natural way to produce recalibrated probabilistic predictions from uncalibrated ensembles forecasts. The framework is used to address important questions concerning the skill of winter hindcasts of the North Atlantic Oscillation for 1992-2011 issued by the Met Office GloSea5 climate prediction system. Although there is much uncertainty in the correlation between ensemble mean and observations, there is strong evidence of skill: the 95% credible interval of the correlation coefficient of [0.19,0.68] does not overlap zero. There is also strong evidence that the forecasts are not exchangeable with the observations: With over 99% certainty, the signal-to-noise ratio of the forecasts is smaller than the signal-to-noise ratio of the observations, which suggests that raw forecasts should not be taken as representative scenarios of the observations. Forecast recalibration is thus required, which can be coherently addressed within the proposed framework.

</details>

<details>

<summary>2015-09-23 01:31:39 - Learning Subspaces of Different Dimension</summary>

- *Brian St. Thomas, Lizhen Lin, Lek-Heng Lim, Sayan Mukherjee*

- `1404.6841v3` - [abs](http://arxiv.org/abs/1404.6841v3) - [pdf](http://arxiv.org/pdf/1404.6841v3)

> We introduce a Bayesian model for inferring mixtures of subspaces of different dimensions. The key challenge in such a mixture model is specification of prior distributions over subspaces of different dimensions. We address this challenge by embedding subspaces or Grassmann manifolds into a sphere of relatively low dimension and specifying priors on the sphere. We provide an efficient sampling algorithm for the posterior distribution of the model parameters. We illustrate that a simple extension of our mixture of subspaces model can be applied to topic modeling. We also prove posterior consistency for the mixture of subspaces model. The utility of our approach is demonstrated with applications to real and simulated data.

</details>

<details>

<summary>2015-09-23 03:32:33 - Probabilistic Group Testing under Sum Observations: A Parallelizable 2-Approximation for Entropy Loss</summary>

- *Weidong Han, Purnima Rajan, Peter I. Frazier, Bruno M. Jedynak*

- `1407.4446v3` - [abs](http://arxiv.org/abs/1407.4446v3) - [pdf](http://arxiv.org/pdf/1407.4446v3)

> We consider the problem of group testing with sum observations and noiseless answers, in which we aim to locate multiple objects by querying the number of objects in each of a sequence of chosen sets. We study a probabilistic setting with entropy loss, in which we assume a joint Bayesian prior density on the locations of the objects and seek to choose the sets queried to minimize the expected entropy of the Bayesian posterior distribution after a fixed number of questions. We present a new non-adaptive policy, called the dyadic policy, show it is optimal among non-adaptive policies, and is within a factor of two of optimal among adaptive policies. This policy is quick to compute, its nonadaptive nature makes it easy to parallelize, and our bounds show it performs well even when compared with adaptive policies. We also study an adaptive greedy policy, which maximizes the one-step expected reduction in entropy, and show that it performs at least as well as the dyadic policy, offering greater query efficiency but reduced parallelism. Numerical experiments demonstrate that both procedures outperform a divide-and-conquer benchmark policy from the literature, called sequential bifurcation, and show how these procedures may be applied in a stylized computer vision problem.

</details>

<details>

<summary>2015-09-23 20:43:06 - SLOPE is Adaptive to Unknown Sparsity and Asymptotically Minimax</summary>

- *Weijie Su, Emmanuel Candes*

- `1503.08393v3` - [abs](http://arxiv.org/abs/1503.08393v3) - [pdf](http://arxiv.org/pdf/1503.08393v3)

> We consider high-dimensional sparse regression problems in which we observe $y = X \beta + z$, where $X$ is an $n \times p$ design matrix and $z$ is an $n$-dimensional vector of independent Gaussian errors, each with variance $\sigma^2$. Our focus is on the recently introduced SLOPE estimator ((Bogdan et al., 2014)), which regularizes the least-squares estimates with the rank-dependent penalty $\sum_{1 \le i \le p} \lambda_i |\hat \beta|_{(i)}$, where $|\hat \beta|_{(i)}$ is the $i$th largest magnitude of the fitted coefficients. Under Gaussian designs, where the entries of $X$ are i.i.d.~$\mathcal{N}(0, 1/n)$, we show that SLOPE, with weights $\lambda_i$ just about equal to $\sigma \cdot \Phi^{-1}(1-iq/(2p))$ ($\Phi^{-1}(\alpha)$ is the $\alpha$th quantile of a standard normal and $q$ is a fixed number in $(0,1)$) achieves a squared error of estimation obeying \[ \sup_{\| \beta\|_0 \le k} \,\, \mathbb{P} \left(\| \hat{\beta}_{\text{SLOPE}} - \beta \|^2 > (1+\epsilon) \, 2\sigma^2 k \log(p/k) \right) \longrightarrow 0 \] as the dimension $p$ increases to $\infty$, and where $\epsilon > 0$ is an arbitrary small constant. This holds under a weak assumption on the $\ell_0$-sparsity level, namely, $k/p \rightarrow 0$ and $(k\log p)/n \rightarrow 0$, and is sharp in the sense that this is the best possible error any estimator can achieve. A remarkable feature is that SLOPE does not require any knowledge of the degree of sparsity, and yet automatically adapts to yield optimal total squared errors over a wide range of $\ell_0$-sparsity classes. We are not aware of any other estimator with this property.

</details>

<details>

<summary>2015-09-24 00:31:44 - Supremum Norm Posterior Contraction and Credible Sets for Nonparametric Multivariate Regression</summary>

- *William Weimin Yoo, Subhashis Ghosal*

- `1411.6716v3` - [abs](http://arxiv.org/abs/1411.6716v3) - [pdf](http://arxiv.org/pdf/1411.6716v3)

> In the setting of nonparametric multivariate regression with unknown error variance, we study asymptotic properties of a Bayesian method for estimating a regression function f and its mixed partial derivatives. We use a random series of tensor product of B-splines with normal basis coefficients as a prior for f, and the error variance is either estimated using the empirical Bayes approach or is endowed with a suitable prior in a hierarchical Bayes approach. We establish pointwise, L2 and supremum norm posterior contraction rates for f and its mixed partial derivatives, and show that they coincide with the minimax rates. Our results cover even the anisotropic situation, where the true regression function may have different smoothness in different directions. Using the convergence bounds, we show that pointwise, L2 and supremum norm credible sets for f and its mixed partial derivatives have guaranteed frequentist coverage with optimal size. New results on tensor products of B-splines are also obtained in the course.

</details>

<details>

<summary>2015-09-24 13:58:17 - A hybrid sampler for Poisson-Kingman mixture models</summary>

- *Maria Lomeli, Stefano Favaro, Yee Whye Teh*

- `1509.07376v1` - [abs](http://arxiv.org/abs/1509.07376v1) - [pdf](http://arxiv.org/pdf/1509.07376v1)

> This paper concerns the introduction of a new Markov Chain Monte Carlo scheme for posterior sampling in Bayesian nonparametric mixture models with priors that belong to the general Poisson-Kingman class. We present a novel compact way of representing the infinite dimensional component of the model such that while explicitly representing this infinite component it has less memory and storage requirements than previous MCMC schemes. We describe comparative simulation results demonstrating the efficacy of the proposed MCMC algorithm against existing marginal and conditional MCMC samplers.

</details>

<details>

<summary>2015-09-24 14:37:43 - A marginal sampler for $σ$-Stable Poisson-Kingman mixture models</summary>

- *María Lomelí, Stefano Favaro, Yee Whye Teh*

- `1407.4211v3` - [abs](http://arxiv.org/abs/1407.4211v3) - [pdf](http://arxiv.org/pdf/1407.4211v3)

> We investigate the class of $\sigma$-stable Poisson-Kingman random probability measures (RPMs) in the context of Bayesian nonparametric mixture modeling. This is a large class of discrete RPMs which encompasses most of the the popular discrete RPMs used in Bayesian nonparametrics, such as the Dirichlet process, Pitman-Yor process, the normalized inverse Gaussian process and the normalized generalized Gamma process. We show how certain sampling properties and marginal characterizations of $\sigma$-stable Poisson-Kingman RPMs can be usefully exploited for devising a Markov chain Monte Carlo (MCMC) algorithm for making inference in Bayesian nonparametric mixture modeling. Specifically, we introduce a novel and efficient MCMC sampling scheme in an augmented space that has a fixed number of auxiliary variables per iteration. We apply our sampling scheme for a density estimation and clustering tasks with unidimensional and multidimensional datasets, and we compare it against competing sampling schemes.

</details>

<details>

<summary>2015-09-24 20:07:51 - Bayesian model selection on linear mixed-effects models for comparisons between multiple treatments and a control</summary>

- *Lei Gong, James M. Flegal, Stephen R. Spindler, Patricia L. Mote*

- `1509.07510v1` - [abs](http://arxiv.org/abs/1509.07510v1) - [pdf](http://arxiv.org/pdf/1509.07510v1)

> We propose a novel Bayesian model selection technique on linear mixed-effects models to compare multiple treatments with a control. A fully Bayesian approach is implemented to estimate the marginal inclusion probabilities that provide a direct measure of the difference between treatments and the control, along with the model-averaged posterior distributions. Default priors are proposed for model selection incorporating domain knowledge and a component-wise Gibbs sampler is developed for efficient posterior computation. We demonstrate the proposed method based on simulated data and an experimental dataset from a longitudinal study of mouse lifespan and weight trajectories.

</details>

<details>

<summary>2015-09-24 20:52:05 - Bayesian Nonparametric Graph Clustering</summary>

- *Sayantan Banerjee, Rehan Akbani, Veerabhadran Baladandayuthapani*

- `1509.07535v1` - [abs](http://arxiv.org/abs/1509.07535v1) - [pdf](http://arxiv.org/pdf/1509.07535v1)

> We present clustering methods for multivariate data exploiting the underlying geometry of the graphical structure between variables. As opposed to standard approaches that assume known graph structures, we first estimate the edge structure of the unknown graph using Bayesian neighborhood selection approaches, wherein we account for the uncertainty of graphical structure learning through model-averaged estimates of the suitable parameters. Subsequently, we develop a nonparametric graph clustering model on the lower dimensional projections of the graph based on Laplacian embeddings using Dirichlet process mixture models. In contrast to standard algorithmic approaches, this fully probabilistic approach allows incorporation of uncertainty in estimation and inference for both graph structure learning and clustering. More importantly, we formalize the arguments for Laplacian embeddings as suitable projections for graph clustering by providing theoretical support for the consistency of the eigenspace of the estimated graph Laplacians. We develop fast computational algorithms that allow our methods to scale to large number of nodes. Through extensive simulations we compare our clustering performance with standard clustering methods. We apply our methods to a novel pan-cancer proteomic data set, and evaluate protein networks and clusters across multiple different cancer types.

</details>

<details>

<summary>2015-09-25 05:41:59 - A Hierarchical Distance-dependent Bayesian Model for Event Coreference Resolution</summary>

- *Bishan Yang, Claire Cardie, Peter Frazier*

- `1504.05929v2` - [abs](http://arxiv.org/abs/1504.05929v2) - [pdf](http://arxiv.org/pdf/1504.05929v2)

> We present a novel hierarchical distance-dependent Bayesian model for event coreference resolution. While existing generative models for event coreference resolution are completely unsupervised, our model allows for the incorporation of pairwise distances between event mentions -- information that is widely used in supervised coreference models to guide the generative clustering processing for better event clustering both within and across documents. We model the distances between event mentions using a feature-rich learnable distance function and encode them as Bayesian priors for nonparametric clustering. Experiments on the ECB+ corpus show that our model outperforms state-of-the-art methods for both within- and cross-document event coreference resolution.

</details>

<details>

<summary>2015-09-25 21:23:07 - Bayesian sequential parameter estimation with a Laplace type approximation</summary>

- *Tiep Mai, Simon Wilson*

- `1509.07900v1` - [abs](http://arxiv.org/abs/1509.07900v1) - [pdf](http://arxiv.org/pdf/1509.07900v1)

> A method for sequential inference of the fixed parameters of a dynamic latent Gaussian models is proposed and evaluated that is based on the iterated Laplace approximation. The method provides a useful trade-off between computational performance and the accuracy of the approximation to the true posterior distribution. Approximation corrections are shown to improve the accuracy of the approximation in simulation studies. A population-based approach is also shown to provide a more robust inference method.

</details>

<details>

<summary>2015-09-25 22:36:22 - Clinical trial design enabling ε-optimal treatment rules</summary>

- *Charles F. Manski, Aleksey Tetenov*

- `1509.07913v1` - [abs](http://arxiv.org/abs/1509.07913v1) - [pdf](http://arxiv.org/pdf/1509.07913v1)

> Medical research has evolved conventions for choosing sample size in randomized clinical trials that rest on the theory of hypothesis testing. Bayesians have argued that trials should be designed to maximize subjective expected utility in settings of clinical interest. This perspective is compelling given a credible prior distribution on treatment response, but Bayesians have struggled to provide guidance on specification of priors. We use the frequentist statistical decision theory of Wald (1950) to study design of trials under ambiguity. We show that {\epsilon}-optimal rules exist when trials have large enough sample size. An {\epsilon}-optimal rule has expected welfare within {\epsilon} of the welfare of the best treatment in every state of nature. Equivalently, it has maximum regret no larger than {\epsilon}. We consider trials that draw predetermined numbers of subjects at random within groups stratified by covariates and treatments. The principal analytical findings are simple sufficient conditions on sample sizes that ensure existence of {\epsilon}-optimal treatment rules when outcomes are bounded. These conditions are obtained by application of Hoeffding (1963) large deviations inequalities to evaluate the performance of empirical success rules.

</details>

<details>

<summary>2015-09-26 01:27:43 - Unexpected links reflect the noise in networks</summary>

- *Anatoly Yambartsev, Michael Perlin, Yevgeniy Kovchegov, Natalia Shulzhenko, Karina L. Mine, Xiaoxi Dong, Andrey Morgun*

- `1310.8341v3` - [abs](http://arxiv.org/abs/1310.8341v3) - [pdf](http://arxiv.org/pdf/1310.8341v3)

> Gene covariation networks are commonly used to study biological processes. The inference of gene covariation networks from observational data can be challenging, especially considering the large number of players involved and the small number of biological replicates available for analysis. We propose a new statistical method for estimating the number of erroneous edges in reconstructed networks that strongly enhances commonly used inference approaches. This method is based on a special relationship between sign of correlation (positive/negative) and directionality (up/down) of gene regulation, and allows for the identification and removal of approximately half of all erroneous edges. Using the mathematical model of Bayesian networks and positive correlation inequalities we establish a mathematical foundation for our method. Analyzing existing biological datasets, we find a strong correlation between the results of our method and false discovery rate (FDR). Furthermore, simulation analysis demonstrates that our method provides a more accurate estimate of network error than FDR.

</details>

<details>

<summary>2015-09-26 10:57:20 - A Bayesian Approach to Sparse plus Low rank Network Identification</summary>

- *Mattia Zorzi, Alessandro Chiuso*

- `1503.07340v2` - [abs](http://arxiv.org/abs/1503.07340v2) - [pdf](http://arxiv.org/pdf/1503.07340v2)

> We consider the problem of modeling multivariate time series with parsimonious dynamical models which can be represented as sparse dynamic Bayesian networks with few latent nodes. This structure translates into a sparse plus low rank model. In this paper, we propose a Gaussian regression approach to identify such a model.

</details>

<details>

<summary>2015-09-27 10:13:07 - Exact ABC using Importance Sampling</summary>

- *Minh Ngoc Tran, Robert Kohn*

- `1509.08076v1` - [abs](http://arxiv.org/abs/1509.08076v1) - [pdf](http://arxiv.org/pdf/1509.08076v1)

> Approximate Bayesian Computation (ABC) is a powerful method for carrying out Bayesian inference when the likelihood is computationally intractable. However, a drawback of ABC is that it is an approximate method that induces a systematic error because it is necessary to set a tolerance level to make the computation tractable. The issue of how to optimally set this tolerance level has been the subject of extensive research. This paper proposes an ABC algorithm based on importance sampling that estimates expectations with respect to the "exact" posterior distribution given the observed summary statistics. This overcomes the need to select the tolerance level. By "exact" we mean that there is no systematic error and the Monte Carlo error can be made arbitrarily small by increasing the number of importance samples. We provide a formal justification for the method and study its convergence properties. The method is illustrated in two applications and the empirical results suggest that the proposed ABC based estimators consistently converge to the true values as the number of importance samples increases. Our proposed approach can be applied more generally to any importance sampling problem where an unbiased estimate of the likelihood is required.

</details>

<details>

<summary>2015-09-27 20:54:09 - Vector Quantile Regression: An Optimal Transport Approach</summary>

- *Guillaume Carlier, Victor Chernozhukov, Alfred Galichon*

- `1406.4643v4` - [abs](http://arxiv.org/abs/1406.4643v4) - [pdf](http://arxiv.org/pdf/1406.4643v4)

> We propose a notion of conditional vector quantile function and a vector quantile regression. A \emph{conditional vector quantile function} (CVQF) of a random vector $Y$, taking values in $\mathbb{R}^d$ given covariates $Z=z$, taking values in $\mathbb{R}% ^k$, is a map $u \longmapsto Q_{Y\mid Z}(u,z)$, which is monotone, in the sense of being a gradient of a convex function, and such that given that vector $U$ follows a reference non-atomic distribution $F_U$, for instance uniform distribution on a unit cube in $\mathbb{R}^d$, the random vector $Q_{Y\mid Z}(U,z)$ has the distribution of $Y$ conditional on $Z=z$. Moreover, we have a strong representation, $Y = Q_{Y\mid Z}(U,Z)$ almost surely, for some version of $U$. The \emph{vector quantile regression} (VQR) is a linear model for CVQF of $Y$ given $Z$. Under correct specification, the notion produces strong representation, $Y=\beta \left(U\right) ^\top f(Z)$, for $f(Z)$ denoting a known set of transformations of $Z$, where $u \longmapsto \beta(u)^\top f(Z)$ is a monotone map, the gradient of a convex function, and the quantile regression coefficients $u \longmapsto \beta(u)$ have the interpretations analogous to that of the standard scalar quantile regression. As $f(Z)$ becomes a richer class of transformations of $Z$, the model becomes nonparametric, as in series modelling. A key property of VQR is the embedding of the classical Monge-Kantorovich's optimal transportation problem at its core as a special case. In the classical case, where $Y$ is scalar, VQR reduces to a version of the classical QR, and CVQF reduces to the scalar conditional quantile function. An application to multiple Engel curve estimation is considered.

</details>

<details>

<summary>2015-09-28 06:31:00 - A simple sampler for the horseshoe estimator</summary>

- *Enes Makalic, Daniel F. Schmidt*

- `1508.03884v4` - [abs](http://arxiv.org/abs/1508.03884v4) - [pdf](http://arxiv.org/pdf/1508.03884v4)

> In this note we derive a simple Bayesian sampler for linear regression with the horseshoe hierarchy. A new interpretation of the horseshoe model is presented, and extensions to logistic regression and alternative hierarchies, such as horseshoe$+$, are discussed. Due to the conjugacy of the proposed hierarchy, Chib's algorithm may be used to easily compute the marginal likelihood of the model.

</details>

<details>

<summary>2015-09-28 13:27:45 - An Outline of the Bayesian Decision Theory</summary>

- *H. R. N. van Erp, R. O. Linger, P. H. A. J. M. van Gelder*

- `1509.08307v1` - [abs](http://arxiv.org/abs/1509.08307v1) - [pdf](http://arxiv.org/pdf/1509.08307v1)

> In this paper we give an outline on the Bayesian Decision Theory.

</details>

<details>

<summary>2015-09-28 19:29:06 - Adaptive sequential Monte Carlo for multiple changepoint analysis</summary>

- *Melissa J. M. Turcotte, Nicholas A. Heard*

- `1509.08442v1` - [abs](http://arxiv.org/abs/1509.08442v1) - [pdf](http://arxiv.org/pdf/1509.08442v1)

> Process monitoring and control requires detection of structural changes in a data stream in real time. This article introduces an efficient sequential Monte Carlo algorithm designed for learning unknown changepoints in continuous time. The method is intuitively simple: new changepoints for the latest window of data are proposed by conditioning only on data observed since the most recent estimated changepoint, as these carry most of the information about the state of the process prior to the update. The proposed method shows improved performance over the current state of the art. Another advantage of the proposed algorithm is that it can be made adaptive, varying the number of particles according to the apparent local complexity of the target changepoint probability distribution. This saves valuable computing time when changes in the change- point distribution are negligible, and enables re-balancing of the importance weights of ex- isting particles when a significant change in the target distribution is encountered. The plain and adaptive versions of the method are illustrated using the canonical con- tinuous time changepoint problem of inferring the intensity of an inhomogeneous Poisson process. Performance is demonstrated using both conjugate and non-conjugate Bayesian models for the intensity.

</details>

<details>

<summary>2015-09-29 03:44:36 - Tractable Fully Bayesian Inference via Convex Optimization and Optimal Transport Theory</summary>

- *Sanggyun Kim, Diego Mesa, Rui Ma, Todd P. Coleman*

- `1509.08582v1` - [abs](http://arxiv.org/abs/1509.08582v1) - [pdf](http://arxiv.org/pdf/1509.08582v1)

> We consider the problem of transforming samples from one continuous source distribution into samples from another target distribution. We demonstrate with optimal transport theory that when the source distribution can be easily sampled from and the target distribution is log-concave, this can be tractably solved with convex optimization. We show that a special case of this, when the source is the prior and the target is the posterior, is Bayesian inference. Here, we can tractably calculate the normalization constant and draw posterior i.i.d. samples. Remarkably, our Bayesian tractability criterion is simply log concavity of the prior and likelihood: the same criterion for tractable calculation of the maximum a posteriori point estimate. With simulated data, we demonstrate how we can attain the Bayes risk in simulations. With physiologic data, we demonstrate improvements over point estimation in intensive care unit outcome prediction and electroencephalography-based sleep staging.

</details>

<details>

<summary>2015-09-29 08:44:13 - On initial direction, orientation and discreteness in the analysis of circular variables</summary>

- *Gianluca Mastrantonio, Giovanna Jona Lasinio, Antonello Maruotti, Gianfranco Calise*

- `1509.08638v1` - [abs](http://arxiv.org/abs/1509.08638v1) - [pdf](http://arxiv.org/pdf/1509.08638v1)

> In this paper, we propose a discrete circular distribution obtained by extending the wrapped Poisson distribution. This new distribution, the Invariant Wrapped Poisson (IWP), enjoys numerous advantages: simple tractable density, parameter-parsimony and interpretability, good circular dependence structure and easy random number generation thanks to known marginal/conditional distributions. Existing discrete circular distributions strongly depend on the initial direction and orientation, i.e. a change of the reference system on the circle may lead to misleading inferential results. We investigate the invariance properties, i.e. invariance under change of initial direction and of the reference system orientation, for several continuous and discrete distributions. We prove that the introduced IWP distribution satisfies these two crucial properties. We estimate parameters in a Bayesian framework and provide all computational details to implement the algorithm. Inferential issues related to the invariance properties are discussed through numerical examples on artificial and real data.

</details>

<details>

<summary>2015-09-29 09:50:38 - Bayesian GARMA Models for Count Data</summary>

- *Marinho G. Andrade, Ricardo S. Ehlers, Breno S. Andrade*

- `1509.08666v1` - [abs](http://arxiv.org/abs/1509.08666v1) - [pdf](http://arxiv.org/pdf/1509.08666v1)

> Generalized autoregressive moving average (GARMA) models are a class of models that was developed for extending the univariate Gaussian ARMA time series model to a flexible observation-driven model for non-Gaussian time series data. This work presents Bayesian approach for GARMA models with Poisson, binomial and negative binomial distributions. A simulation study was carried out to investigate the performance of Bayesian estimation and Bayesian model selection criteria. Also three real datasets were analysed using the Bayesian approach on GARMA models.

</details>

<details>

<summary>2015-09-29 14:31:15 - A Bayesian length-based population dynamics model for northern shrimp (Pandalus Borealis)</summary>

- *Paul Blomstedt, Jarno Vanhatalo, Mats Ulmestrand, Anna Gårdmark, Samu Mäntyniemi*

- `1509.08774v1` - [abs](http://arxiv.org/abs/1509.08774v1) - [pdf](http://arxiv.org/pdf/1509.08774v1)

> We introduce a fully length-based Bayesian model for the population dynamics of northern shrimp (Pandalus Borealis). This has the advantage of structuring the population in terms of a directly observable quantity, requiring no indirect estimation of age distributions from measurements of size. The introduced model is intended as a simplistic prototype around which further developments and refinements can be built. As a case study, we use the model to analyze the population of Skagerrak and the Norwegian Deep in the years 1988-2012.

</details>

<details>

<summary>2015-09-29 17:34:48 - Bayesian model averaging: A systematic review and conceptual classification</summary>

- *Tiago M. Fragoso, Francisco Louzada Neto*

- `1509.08864v1` - [abs](http://arxiv.org/abs/1509.08864v1) - [pdf](http://arxiv.org/pdf/1509.08864v1)

> Bayesian Model Averaging (BMA) is an application of Bayesian inference to the problems of model selection, combined estimation and prediction that produces a straightforward model choice criteria and less risky predictions. However, the application of BMA is not always straightforward, leading to diverse assumptions and situational choices on its different aspects. Despite the widespread application of BMA in the literature, there were not many accounts of these differences and trends besides a few landmark revisions in the late 1990s and early 2000s, therefore not taking into account any advancements made in the last 15 years. In this work, we present an account of these developments through a careful content analysis of 587 articles in BMA published between 1996 and 2014. We also develop a conceptual classification scheme to better describe this vast literature, understand its trends and future directions and provide guidance for the researcher interested in both the application and development of the methodology. The results of the classification scheme and content review are then used to discuss the present and future of the BMA literature.

</details>

<details>

<summary>2015-09-29 20:59:06 - Model-based clustering of Gaussian copulas for mixed data</summary>

- *Matthieu Marbac, Christophe Biernacki, Vincent Vandewalle*

- `1405.1299v3` - [abs](http://arxiv.org/abs/1405.1299v3) - [pdf](http://arxiv.org/pdf/1405.1299v3)

> Clustering task of mixed data is a challenging problem. In a probabilistic framework, the main difficulty is due to a shortage of conventional distributions for such data. In this paper, we propose to achieve the mixed data clustering with a Gaussian copula mixture model, since copulas, and in particular the Gaussian ones, are powerful tools for easily modelling the distribution of multivariate variables. Indeed, considering a mixing of continuous, integer and ordinal variables (thus all having a cumulative distribution function), this copula mixture model defines intra-component dependencies similar to a Gaussian mixture, so with classical correlation meaning. Simultaneously, it preserves standard margins associated to continuous, integer and ordered features, namely the Gaussian, the Poisson and the ordered multinomial distributions. As an interesting by-product, the proposed mixture model generalizes many well-known ones and also provides tools of visualization based on the parameters. At a practical level, the Bayesian inference is retained and it is achieved with a Metropolis-within-Gibbs sampler. Experiments on simulated and real data sets finally illustrate the expected advantages of the proposed model for mixed data: flexible and meaningful parametrization combined with visualization features.

</details>

<details>

<summary>2015-09-30 01:43:09 - Learning without Recall: A Case for Log-Linear Learning</summary>

- *Mohammad Amin Rahimian, Ali Jadbabaie*

- `1509.08990v1` - [abs](http://arxiv.org/abs/1509.08990v1) - [pdf](http://arxiv.org/pdf/1509.08990v1)

> We analyze a model of learning and belief formation in networks in which agents follow Bayes rule yet they do not recall their history of past observations and cannot reason about how other agents' beliefs are formed. They do so by making rational inferences about their observations which include a sequence of independent and identically distributed private signals as well as the beliefs of their neighboring agents at each time. Fully rational agents would successively apply Bayes rule to the entire history of observations. This leads to forebodingly complex inferences due to lack of knowledge about the global network structure that causes those observations. To address these complexities, we consider a Learning without Recall model, which in addition to providing a tractable framework for analyzing the behavior of rational agents in social networks, can also provide a behavioral foundation for the variety of non-Bayesian update rules in the literature. We present the implications of various choices for time-varying priors of such agents and how this choice affects learning and its rate.

</details>

<details>

<summary>2015-09-30 17:15:23 - Higher order elicitability and Osband's principle</summary>

- *Tobias Fissler, Johanna F. Ziegel*

- `1503.08123v3` - [abs](http://arxiv.org/abs/1503.08123v3) - [pdf](http://arxiv.org/pdf/1503.08123v3)

> A statistical functional, such as the mean or the median, is called elicitable if there is a scoring function or loss function such that the correct forecast of the functional is the unique minimizer of the expected score. Such scoring functions are called strictly consistent for the functional. The elicitability of a functional opens the possibility to compare competing forecasts and to rank them in terms of their realized scores. In this paper, we explore the notion of elicitability for multi-dimensional functionals and give both necessary and sufficient conditions for strictly consistent scoring functions. We cover the case of functionals with elicitable components, but we also show that one-dimensional functionals that are not elicitable can be a component of a higher order elicitable functional. In the case of the variance this is a known result. However, an important result of this paper is that spectral risk measures with a spectral measure with finite support are jointly elicitable if one adds the `correct' quantiles. A direct consequence of applied interest is that the pair (Value at Risk, Expected Shortfall) is jointly elicitable under mild conditions that are usually fulfilled in risk management applications.

</details>


## 2015-10

<details>

<summary>2015-10-01 22:41:50 - Towards Machine Wald</summary>

- *Houman Owhadi, Clint Scovel*

- `1508.02449v2` - [abs](http://arxiv.org/abs/1508.02449v2) - [pdf](http://arxiv.org/pdf/1508.02449v2)

> The past century has seen a steady increase in the need of estimating and predicting complex systems and making (possibly critical) decisions with limited information. Although computers have made possible the numerical evaluation of sophisticated statistical models, these models are still designed \emph{by humans} because there is currently no known recipe or algorithm for dividing the design of a statistical model into a sequence of arithmetic operations. Indeed enabling computers to \emph{think} as \emph{humans} have the ability to do when faced with uncertainty is challenging in several major ways: (1) Finding optimal statistical models remains to be formulated as a well posed problem when information on the system of interest is incomplete and comes in the form of a complex combination of sample data, partial knowledge of constitutive relations and a limited description of the distribution of input random variables. (2) The space of admissible scenarios along with the space of relevant information, assumptions, and/or beliefs, tend to be infinite dimensional, whereas calculus on a computer is necessarily discrete and finite. With this purpose, this paper explores the foundations of a rigorous framework for the scientific computation of optimal statistical estimators/models and reviews their connections with Decision Theory, Machine Learning, Bayesian Inference, Stochastic Optimization, Robust Optimization, Optimal Uncertainty Quantification and Information Based Complexity.

</details>

<details>

<summary>2015-10-02 10:07:29 - Model Selection and Multiple Testing - A Bayesian and Empirical Bayes Overview and some New Results</summary>

- *Ritabrata Dutta, Malgortaza Bogdan, Jayanta K. Ghosh*

- `1510.00547v1` - [abs](http://arxiv.org/abs/1510.00547v1) - [pdf](http://arxiv.org/pdf/1510.00547v1)

> We provide a brief overview of both Bayes and classical model selection. We argue tentatively that model selection has at least two major goals, that of finding the correct model or predicting well, and that in general both these goals may not be achieved in an optimum manner by a single model selection rule. We discuss, briefly but critically, through a study of well-known model selection rules like AIC, BIC, DIC and Lasso, how these different goals are pursued in each paradigm. We introduce some new definitions of consistency, results and conjectures about consistency in high dimensional model selection problems. Finally we discuss some new or recent results in Full Bayes and Empirical Bayes multiple testing, and cross-validation. We show that when the number of parameters tends to infinity at a smaller rate than sample size, then it is best from the point of view of consistency to use most of the data for inference and only a negligible proportion to make an improper prior proper.

</details>

<details>

<summary>2015-10-03 10:23:25 - Robust Linear Spectral Unmixing using Anomaly Detection</summary>

- *Yoann Altmann, Steve McLaughlin, Alfred Hero*

- `1501.03731v2` - [abs](http://arxiv.org/abs/1501.03731v2) - [pdf](http://arxiv.org/pdf/1501.03731v2)

> This paper presents a Bayesian algorithm for linear spectral unmixing of hyperspectral images that accounts for anomalies present in the data. The model proposed assumes that the pixel reflectances are linear mixtures of unknown endmembers, corrupted by an additional nonlinear term modelling anomalies and additive Gaussian noise. A Markov random field is used for anomaly detection based on the spatial and spectral structures of the anomalies. This allows outliers to be identified in particular regions and wavelengths of the data cube. A Bayesian algorithm is proposed to estimate the parameters involved in the model yielding a joint linear unmixing and anomaly detection algorithm. Simulations conducted with synthetic and real hyperspectral images demonstrate the accuracy of the proposed unmixing and outlier detection strategy for the analysis of hyperspectral images.

</details>

<details>

<summary>2015-10-03 10:57:40 - Collaborative sparse regression using spatially correlated supports - Application to hyperspectral unmixing</summary>

- *Yoann Altmann, Marcelo Pereyra, Jose Bioucas-Dias*

- `1409.8129v2` - [abs](http://arxiv.org/abs/1409.8129v2) - [pdf](http://arxiv.org/pdf/1409.8129v2)

> This paper presents a new Bayesian collaborative sparse regression method for linear unmixing of hyperspectral images. Our contribution is twofold; first, we propose a new Bayesian model for structured sparse regression in which the supports of the sparse abundance vectors are a priori spatially correlated across pixels (i.e., materials are spatially organised rather than randomly distributed at a pixel level). This prior information is encoded in the model through a truncated multivariate Ising Markov random field, which also takes into consideration the facts that pixels cannot be empty (i.e, there is at least one material present in each pixel), and that different materials may exhibit different degrees of spatial regularity. Secondly, we propose an advanced Markov chain Monte Carlo algorithm to estimate the posterior probabilities that materials are present or absent in each pixel, and, conditionally to the maximum marginal a posteriori configuration of the support, compute the MMSE estimates of the abundance vectors. A remarkable property of this algorithm is that it self-adjusts the values of the parameters of the Markov random field, thus relieving practitioners from setting regularisation parameters by cross-validation. The performance of the proposed methodology is finally demonstrated through a series of experiments with synthetic and real data and comparisons with other algorithms from the literature.

</details>

<details>

<summary>2015-10-03 20:05:58 - A Geometric View of Posterior Approximation</summary>

- *Tian Chen, Jeffrey Streets, Babak Shahbaba*

- `1510.00861v1` - [abs](http://arxiv.org/abs/1510.00861v1) - [pdf](http://arxiv.org/pdf/1510.00861v1)

> Although Bayesian methods are robust and principled, their application in practice could be limited since they typically rely on computationally intensive Markov Chain Monte Carlo algorithms for their implementation. One possible solution is to find a fast approximation of posterior distribution and use it for statistical inference. For commonly used approximation methods, such as Laplace and variational free energy, the objective is mainly defined in terms of computational convenience as opposed to a true distance measure between the target and approximating distributions. In this paper, we provide a geometric view of posterior approximation based on a valid distance measure derived from ambient Fisher geometry. Our proposed framework is easily generalizable and can inspire a new class of methods for approximate Bayesian inference.

</details>

<details>

<summary>2015-10-05 12:18:07 - Spectral unmixing of Multispectral Lidar signals</summary>

- *Yoann Altmann, Andrew Wallace, Steve McLaughlin*

- `1501.01150v2` - [abs](http://arxiv.org/abs/1501.01150v2) - [pdf](http://arxiv.org/pdf/1501.01150v2)

> In this paper, we present a Bayesian approach for spectral unmixing of multispectral Lidar (MSL) data associated with surface reflection from targeted surfaces composed of several known materials. The problem addressed is the estimation of the positions and area distribution of each material. In the Bayesian framework, appropriate prior distributions are assigned to the unknown model parameters and a Markov chain Monte Carlo method is used to sample the resulting posterior distribution. The performance of the proposed algorithm is evaluated using synthetic MSL signals, for which single and multi-layered models are derived. To evaluate the expected estimation performance associated with MSL signal analysis, a Cramer-Rao lower bound associated with model considered is also derived, and compared with the experimental data. Both the theoretical lower bound and the experimental analysis will be of primary assistance in future instrument design.

</details>

<details>

<summary>2015-10-05 12:33:49 - Bayesian nonlinear hyperspectral unmixing with spatial residual component analysis</summary>

- *Yoann Altmann, Marcelo Pereyra, Stephen McLaughlin*

- `1412.4681v2` - [abs](http://arxiv.org/abs/1412.4681v2) - [pdf](http://arxiv.org/pdf/1412.4681v2)

> This paper presents a new Bayesian model and algorithm for nonlinear unmixing of hyperspectral images. The model proposed represents the pixel reflectances as linear combinations of the endmembers, corrupted by nonlinear (with respect to the endmembers) terms and additive Gaussian noise. Prior knowledge about the problem is embedded in a hierarchical model that describes the dependence structure between the model parameters and their constraints. In particular, a gamma Markov random field is used to model the joint distribution of the nonlinear terms, which are expected to exhibit significant spatial correlations. An adaptive Markov chain Monte Carlo algorithm is then proposed to compute the Bayesian estimates of interest and perform Bayesian inference. This algorithm is equipped with a stochastic optimisation adaptation mechanism that automatically adjusts the parameters of the gamma Markov random field by maximum marginal likelihood estimation. Finally, the proposed methodology is demonstrated through a series of experiments with comparisons using synthetic and real data and with competing state-of-the-art approaches.

</details>

<details>

<summary>2015-10-05 16:47:54 - Bayesian Inference via Approximation of Log-likelihood for Priors in Exponential Family</summary>

- *Tohid Ardeshiri, Umut Orguner, Fredrik Gustafsson*

- `1510.01225v1` - [abs](http://arxiv.org/abs/1510.01225v1) - [pdf](http://arxiv.org/pdf/1510.01225v1)

> In this paper, a Bayesian inference technique based on Taylor series approximation of the logarithm of the likelihood function is presented. The proposed approximation is devised for the case, where the prior distribution belongs to the exponential family of distributions. The logarithm of the likelihood function is linearized with respect to the sufficient statistic of the prior distribution in exponential family such that the posterior obtains the same exponential family form as the prior. Similarities between the proposed method and the extended Kalman filter for nonlinear filtering are illustrated. Furthermore, an extended target measurement update for target models where the target extent is represented by a random matrix having an inverse Wishart distribution is derived. The approximate update covers the important case where the spread of measurement is due to the target extent as well as the measurement noise in the sensor.

</details>

<details>

<summary>2015-10-06 06:07:55 - Bayesian Masking: Sparse Bayesian Estimation with Weaker Shrinkage Bias</summary>

- *Yohei Kondo, Kohei Hayashi, Shin-ichi Maeda*

- `1509.01004v2` - [abs](http://arxiv.org/abs/1509.01004v2) - [pdf](http://arxiv.org/pdf/1509.01004v2)

> A common strategy for sparse linear regression is to introduce regularization, which eliminates irrelevant features by letting the corresponding weights be zeros. However, regularization often shrinks the estimator for relevant features, which leads to incorrect feature selection. Motivated by the above-mentioned issue, we propose Bayesian masking (BM), a sparse estimation method which imposes no regularization on the weights. The key concept of BM is to introduce binary latent variables that randomly mask features. Estimating the masking rates determines the relevance of the features automatically. We derive a variational Bayesian inference algorithm that maximizes the lower bound of the factorized information criterion (FIC), which is a recently developed asymptotic criterion for evaluating the marginal log-likelihood. In addition, we propose reparametrization to accelerate the convergence of the derived algorithm. Finally, we show that BM outperforms Lasso and automatic relevance determination (ARD) in terms of the sparsity-shrinkage trade-off.

</details>

<details>

<summary>2015-10-06 09:06:11 - Bayesian Markov Blanket Estimation</summary>

- *Dinu Kaufmann, Sonali Parbhoo, Aleksander Wieczorek, Sebastian Keller, David Adametz, Volker Roth*

- `1510.01485v1` - [abs](http://arxiv.org/abs/1510.01485v1) - [pdf](http://arxiv.org/pdf/1510.01485v1)

> This paper considers a Bayesian view for estimating a sub-network in a Markov random field. The sub-network corresponds to the Markov blanket of a set of query variables, where the set of potential neighbours here is big. We factorize the posterior such that the Markov blanket is conditionally independent of the network of the potential neighbours. By exploiting this blockwise decoupling, we derive analytic expressions for posterior conditionals. Subsequently, we develop an inference scheme which makes use of the factorization. As a result, estimation of a sub-network is possible without inferring an entire network. Since the resulting Gibbs sampler scales linearly with the number of variables, it can handle relatively large neighbourhoods. The proposed scheme results in faster convergence and superior mixing of the Markov chain than existing Bayesian network estimation techniques.

</details>

<details>

<summary>2015-10-06 14:36:52 - Combining allele frequency uncertainty and population substructure corrections in forensic DNA calculations</summary>

- *Robert Cowell*

- `1509.08361v2` - [abs](http://arxiv.org/abs/1509.08361v2) - [pdf](http://arxiv.org/pdf/1509.08361v2)

> In forensic DNA calculations of relatedness of individuals and in DNA mixture analyses, two sources of uncertainty are present concerning the allele frequencies used for evaluating genotype probabilities when evaluating likelihoods. They are: (i) imprecision in the estimates of the allele frequencies in the population by using an inevitably finite database of DNA profiles to estimate them; and (ii) the existence of population substructure. Green and Mortera (2009) showed that these effects may be taken into account individually using a common Dirichlet model within a Bayesian network formulation, but that when taken in combination this is not the case; however they suggested an approximation that could be used. Here we develop a slightly different approximation that is shown to be exact in the case of a single individual. We demonstrate the closeness of the approximation numerically using a published database of allele counts, and illustrate the effect of incorporating the approximation into calculations of a recently published statistical model of DNA mixtures.

</details>

<details>

<summary>2015-10-06 22:19:48 - Estimating Stochastic Production Frontiers: A One-stage Multivariate Semi-Nonparametric Bayesian Concave Regression Method</summary>

- *José Luis Preciado Arreola, Andrew L. Johnson*

- `1510.01772v1` - [abs](http://arxiv.org/abs/1510.01772v1) - [pdf](http://arxiv.org/pdf/1510.01772v1)

> This paper describes a method to estimate a production frontier that satisfies the axioms of monotonicity and concavity in a non-parametric Bayesian setting. An inefficiency term that allows for significant departure from prior distributional assumptions is jointly estimated in a single stage with parametric prior assumptions. We introduce heteroscedasticity into the inefficiency terms by local hyperplane-specific shrinkage hyperparameters and impose monotonicity using bound-constrained local nonlinear regression. Our minimum-of-hyperplanes estimator imposes concavity. Our Monte Carlo simulation experiments demonstrate that the frontier and efficiency estimations are competitive, economically sound, and allow for the analysis of larger datasets than existing nonparametric methods. We validate the proposed method using data from 2007-2010 for Japan's concrete industry. The results show that the efficiency levels remain relatively high over the time period.

</details>

<details>

<summary>2015-10-08 15:06:03 - The Knowledge Gradient with Logistic Belief Models for Binary Classification</summary>

- *Yingfei Wang, Chu Wang, Warren Powell*

- `1510.02354v1` - [abs](http://arxiv.org/abs/1510.02354v1) - [pdf](http://arxiv.org/pdf/1510.02354v1)

> We consider sequential decision making problems for binary classification scenario in which the learner takes an active role in repeatedly selecting samples from the action pool and receives the binary label of the selected alternatives. Our problem is motivated by applications where observations are time consuming and/or expensive, resulting in small samples. The goal is to identify the best alternative with the highest response. We use Bayesian logistic regression to predict the response of each alternative. By formulating the problem as a Markov decision process, we develop a knowledge-gradient type policy to guide the experiment by maximizing the expected value of information of labeling each alternative and provide a finite-time analysis on the estimated error. Experiments on benchmark UCI datasets demonstrate the effectiveness of the proposed method.

</details>

<details>

<summary>2015-10-08 15:55:37 - Stochastic variational inference for large-scale discrete choice models using adaptive batch sizes</summary>

- *Linda S. L. Tan*

- `1405.5623v4` - [abs](http://arxiv.org/abs/1405.5623v4) - [pdf](http://arxiv.org/pdf/1405.5623v4)

> Discrete choice models describe the choices made by decision makers among alternatives and play an important role in transportation planning, marketing research and other applications. The mixed multinomial logit (MMNL) model is a popular discrete choice model that captures heterogeneity in the preferences of decision makers through random coefficients. While Markov chain Monte Carlo methods provide the Bayesian analogue to classical procedures for estimating MMNL models, computations can be prohibitively expensive for large datasets. Approximate inference can be obtained using variational methods at a lower computational cost with competitive accuracy. In this paper, we develop variational methods for estimating MMNL models that allow random coefficients to be correlated in the posterior and can be extended easily to large-scale datasets. We explore three alternatives: (1) Laplace variational inference, (2) nonconjugate variational message passing and (3) stochastic linear regression. Their performances are compared using real and simulated data. To accelerate convergence for large datasets, we develop stochastic variational inference for MMNL models using each of the above alternatives. Stochastic variational inference allows data to be processed in minibatches by optimizing global variational parameters using stochastic gradient approximation. A novel strategy for increasing minibatch sizes adaptively within stochastic variational inference is proposed.

</details>

<details>

<summary>2015-10-08 18:37:37 - Coordinate Transformation and Polynomial Chaos for the Bayesian Inference of a Gaussian Process with Parametrized Prior Covariance Function</summary>

- *Ihab Sraj, Olivier P. Le Maître, Omar M. Knio, Ibrahim Hoteit*

- `1501.03323v2` - [abs](http://arxiv.org/abs/1501.03323v2) - [pdf](http://arxiv.org/pdf/1501.03323v2)

> This paper addresses model dimensionality reduction for Bayesian inference based on prior Gaussian fields with uncertainty in the covariance function hyper-parameters. The dimensionality reduction is traditionally achieved using the Karhunen-\Loeve expansion of a prior Gaussian process assuming covariance function with fixed hyper-parameters, despite the fact that these are uncertain in nature. The posterior distribution of the Karhunen-Lo\`{e}ve coordinates is then inferred using available observations. The resulting inferred field is therefore dependent on the assumed hyper-parameters. Here, we seek to efficiently estimate both the field and covariance hyper-parameters using Bayesian inference. To this end, a generalized Karhunen-Lo\`{e}ve expansion is derived using a coordinate transformation to account for the dependence with respect to the covariance hyper-parameters. Polynomial Chaos expansions are employed for the acceleration of the Bayesian inference using similar coordinate transformations, enabling us to avoid expanding explicitly the solution dependence on the uncertain hyper-parameters. We demonstrate the feasibility of the proposed method on a transient diffusion equation by inferring spatially-varying log-diffusivity fields from noisy data. The inferred profiles were found closer to the true profiles when including the hyper-parameters' uncertainty in the inference formulation.

</details>

<details>

<summary>2015-10-08 18:40:50 - Distilling Model Knowledge</summary>

- *George Papamakarios*

- `1510.02437v1` - [abs](http://arxiv.org/abs/1510.02437v1) - [pdf](http://arxiv.org/pdf/1510.02437v1)

> Top-performing machine learning systems, such as deep neural networks, large ensembles and complex probabilistic graphical models, can be expensive to store, slow to evaluate and hard to integrate into larger systems. Ideally, we would like to replace such cumbersome models with simpler models that perform equally well.   In this thesis, we study knowledge distillation, the idea of extracting the knowledge contained in a complex model and injecting it into a more convenient model. We present a general framework for knowledge distillation, whereby a convenient model of our choosing learns how to mimic a complex model, by observing the latter's behaviour and being penalized whenever it fails to reproduce it.   We develop our framework within the context of three distinct machine learning applications: (a) model compression, where we compress large discriminative models, such as ensembles of neural networks, into models of much smaller size; (b) compact predictive distributions for Bayesian inference, where we distil large bags of MCMC samples into compact predictive distributions in closed form; (c) intractable generative models, where we distil unnormalizable models such as RBMs into tractable models such as NADEs.   We contribute to the state of the art with novel techniques and ideas. In model compression, we describe and implement derivative matching, which allows for better distillation when data is scarce. In compact predictive distributions, we introduce online distillation, which allows for significant savings in memory. Finally, in intractable generative models, we show how to use distilled models to robustly estimate intractable quantities of the original model, such as its intractable partition function.

</details>

<details>

<summary>2015-10-09 03:02:40 - A Model-Based Approach to Climate Reconstruction Using Tree-Ring Data</summary>

- *Matthew R. Schofield, Richard J. Barker, Andrew Gelman, Edward R. Cook, Keith R. Briffa*

- `1510.02557v1` - [abs](http://arxiv.org/abs/1510.02557v1) - [pdf](http://arxiv.org/pdf/1510.02557v1)

> Quantifying long-term historical climate is fundamental to understanding recent climate change. Most instrumentally recorded climate data are only available for the past 200 years, so proxy observations from natural archives are often considered. We describe a model-based approach to reconstructing climate defined in terms of raw tree-ring measurement data that simultaneously accounts for non-climatic and climatic variability. In this approach we specify a joint model for the tree-ring data and climate variable that we fit using Bayesian inference. We consider a range of prior densities and compare the modeling approach to current methodology using an example case of Scots pine from Tornetrask, Sweden to reconstruct growing season temperature. We describe how current approaches translate into particular model assumptions. We explore how changes to various components in the model-based approach affect the resulting reconstruction. We show that minor changes in model specification can have little effect on model fit but lead to large changes in the predictions. In particular, the periods of relatively warmer and cooler temperatures are robust between models, but the magnitude of the resulting temperatures are highly model dependent. Such sensitivity may not be apparent with traditional approaches because the underlying statistical model is often hidden or poorly described.

</details>

<details>

<summary>2015-10-09 06:55:25 - Asymptotic Analysis of the Random-Walk Metropolis Algorithm on Ridged Densities</summary>

- *Alexandros Beskos, Gareth Roberts, Alexandre Thiery, Natesh Pillai*

- `1510.02577v1` - [abs](http://arxiv.org/abs/1510.02577v1) - [pdf](http://arxiv.org/pdf/1510.02577v1)

> In this paper we study the asymptotic behavior of the Random-Walk Metropolis algorithm on probability densities with two different `scales', where most of the probability mass is distributed along certain key directions with the `orthogonal' directions containing relatively less mass. Such class of probability measures arise in various applied contexts including Bayesian inverse problems where the posterior measure concentrates on a sub-manifold when the noise variance goes to zero. When the target measure concentrates on a linear sub-manifold, we derive analytically a diffusion limit for the Random-Walk Metropolis Markov chain as the scale parameter goes to zero. In contrast to the existing works on scaling limits, our limiting Stochastic Differential Equation does not in general have a constant diffusion coefficient. Our results show that in some cases, the usual practice of adapting the step-size to control the acceptance probability might be sub-optimal as the optimal acceptance probability is zero (in the limit).

</details>

<details>

<summary>2015-10-09 09:46:38 - Sequential Monte Carlo Methods for State and Parameter Estimation in Abruptly Changing Environments</summary>

- *Christopher Nemeth, Paul Fearnhead, Lyudmila Mihaylova*

- `1510.02604v1` - [abs](http://arxiv.org/abs/1510.02604v1) - [pdf](http://arxiv.org/pdf/1510.02604v1)

> This paper develops a novel sequential Monte Carlo (SMC) approach for joint state and parameter estimation that can deal efficiently with abruptly changing parameters which is a common case when tracking maneuvering targets. The approach combines Bayesian methods for dealing with changepoints with methods for estimating static parameters within the SMC framework. The result is an approach which adaptively estimates the model parameters in accordance with changes to the target's trajectory. The developed approach is compared against the Interacting Multiple Model (IMM) filter for tracking a maneuvering target over a complex maneuvering scenario with nonlinear observations. In the IMM filter a large combination of models is required to account for unknown parameters. In contrast, the proposed approach circumvents the combinatorial complexity of applying multiple models in the IMM filter through Bayesian parameter estimation techniques. The developed approach is validated over complex maneuvering scenarios where both the system parameters and measurement noise parameters are unknown. Accurate estimation results are presented.

</details>

<details>

<summary>2015-10-09 15:12:10 - Trans-Dimensional Bayesian Inference for Gravitational Lens Substructures</summary>

- *Brendon J. Brewer, David Huijser, Geraint F. Lewis*

- `1508.00662v2` - [abs](http://arxiv.org/abs/1508.00662v2) - [pdf](http://arxiv.org/pdf/1508.00662v2)

> We introduce a Bayesian solution to the problem of inferring the density profile of strong gravitational lenses when the lens galaxy may contain multiple dark or faint substructures. The source and lens models are based on a superposition of an unknown number of non-negative basis functions (or "blobs") whose form was chosen with speed as a primary criterion. The prior distribution for the blobs' properties is specified hierarchically, so the mass function of substructures is a natural output of the method. We use reversible jump Markov Chain Monte Carlo (MCMC) within Diffusive Nested Sampling (DNS) to sample the posterior distribution and evaluate the marginal likelihood of the model, including the summation over the unknown number of blobs in the source and the lens. We demonstrate the method on two simulated data sets: one with a single substructure, and one with ten. We also apply the method to the g-band image of the "Cosmic Horseshoe" system, and find evidence for more than zero substructures. However, these have large spatial extent and probably only point to misspecifications in the model (such as the shape of the smooth lens component or the point spread function), which are difficult to guard against in full generality.

</details>

<details>

<summary>2015-10-09 15:19:09 - A nonlinear population Monte Carlo scheme for the Bayesian estimation of parameters of $α$-stable distributions</summary>

- *Eugenia Koblents, Joaquin Miguez, Marco A. Rodriguez, Alexandra M. Schmidt*

- `1510.02702v1` - [abs](http://arxiv.org/abs/1510.02702v1) - [pdf](http://arxiv.org/pdf/1510.02702v1)

> The class of $\alpha$-stable distributions enjoys multiple practical applications in signal processing, finance, biology and other areas because it allows to describe interesting and complex data patterns, such as asymmetry or heavy tails, in contrast with the simpler and widely used Gaussian distribution. The density associated with a general $\alpha$-stable distribution cannot be obtained in closed form, which hinders the process of estimating its parameters. A nonlinear population Monte Carlo (NPMC) scheme is applied in order to approximate the posterior probability distribution of the parameters of an $\alpha$-stable random variable given a set of random realizations of the latter. The approximate posterior distribution is computed by way of an iterative algorithm and it consists of a collection of samples in the parameter space with associated nonlinearly-transformed importance weights. A numerical comparison of the main existing methods to estimate the $\alpha$-stable parameters is provided, including the traditional frequentist techniques as well as a Markov chain Monte Carlo (MCMC) and a likelihood-free Bayesian approach. It is shown by means of computer simulations that the NPMC method outperforms the existing techniques in terms of parameter estimation error and failure rate for the whole range of values of $\alpha$, including the smaller values for which most existing methods fail to work properly. Furthermore, it is shown that accurate parameter estimates can often be computed based on a low number of observations. Additionally, numerical results based on a set of real fish displacement data are provided.

</details>

<details>

<summary>2015-10-09 21:44:25 - p-Markov Gaussian Processes for Scalable and Expressive Online Bayesian Nonparametric Time Series Forecasting</summary>

- *Yves-Laurent Kom Samo, Stephen J. Roberts*

- `1510.02830v1` - [abs](http://arxiv.org/abs/1510.02830v1) - [pdf](http://arxiv.org/pdf/1510.02830v1)

> In this paper we introduce a novel online time series forecasting model we refer to as the pM-GP filter. We show that our model is equivalent to Gaussian process regression, with the advantage that both online forecasting and online learning of the hyper-parameters have a constant (rather than cubic) time complexity and a constant (rather than squared) memory requirement in the number of observations, without resorting to approximations. Moreover, the proposed model is expressive in that the family of covariance functions of the implied latent process, namely the spectral Matern kernels, have recently been proven to be capable of approximating arbitrarily well any translation-invariant covariance function. The benefit of our approach compared to competing models is demonstrated using experiments on several real-life datasets.

</details>

<details>

<summary>2015-10-10 08:09:07 - Remarks on kernel Bayes' rule</summary>

- *Hisashi Johno, Kazunori Nakamoto, Tatsuhiko Saigo*

- `1507.01059v2` - [abs](http://arxiv.org/abs/1507.01059v2) - [pdf](http://arxiv.org/pdf/1507.01059v2)

> Kernel Bayes' rule has been proposed as a nonparametric kernel-based method to realize Bayesian inference in reproducing kernel Hilbert spaces. However, we demonstrate both theoretically and experimentally that the prediction result by kernel Bayes' rule is in some cases unnatural. We consider that this phenomenon is in part due to the fact that the assumptions in kernel Bayes' rule do not hold in general.

</details>

<details>

<summary>2015-10-10 18:25:38 - On the estimation of the order of smoothness of the regression function</summary>

- *Daniel Taylor-Rodriguez, Sujit Ghosh*

- `1510.02967v1` - [abs](http://arxiv.org/abs/1510.02967v1) - [pdf](http://arxiv.org/pdf/1510.02967v1)

> The order of smoothness chosen in nonparametric estimation problems is critical. This choice balances the tradeoff between model parsimony and data overfitting. The most common approach used in this context is cross-validation. However, cross-validation is computationally time consuming and often precludes valid post-selection inference without further considerations. With this in mind, borrowing elements from the objective Bayesian variable selection literature, we propose an approach to select the degree of a polynomial basis. Although the method can be extended to most series-based smoothers, we focus on estimates arising from Bernstein polynomials for the regression function, using mixtures of g-priors on the model parameter space and a hierarchical specification for the priors on the order of smoothness. We prove the asymptotic predictive optimality for the method, and through simulation experiments, demonstrate that, compared to cross-validation, our approach is one or two orders of magnitude faster and yields comparable predictive accuracy. Moreover, our method provides simultaneous quantification of model uncertainty and parameter estimates. We illustrate the method with real applications for continuous and binary responses.

</details>

<details>

<summary>2015-10-13 00:01:21 - Multiple Imputation of Missing Categorical and Continuous Values via Bayesian Mixture Models with Local Dependence</summary>

- *Jared S. Murray, Jerome P. Reiter*

- `1410.0438v2` - [abs](http://arxiv.org/abs/1410.0438v2) - [pdf](http://arxiv.org/pdf/1410.0438v2)

> We present a nonparametric Bayesian joint model for multivariate continuous and categorical variables, with the intention of developing a flexible engine for multiple imputation of missing values. The model fuses Dirichlet process mixtures of multinomial distributions for categorical variables with Dirichlet process mixtures of multivariate normal distributions for continuous variables. We incorporate dependence between the continuous and categorical variables by (i) modeling the means of the normal distributions as component-specific functions of the categorical variables and (ii) forming distinct mixture components for the categorical and continuous data with probabilities that are linked via a hierarchical model. This structure allows the model to capture complex dependencies between the categorical and continuous data with minimal tuning by the analyst. We apply the model to impute missing values due to item nonresponse in an evaluation of the redesign of the Survey of Income and Program Participation (SIPP). The goal is to compare estimates from a field test with the new design to estimates from selected individuals from a panel collected under the old design. We show that accounting for the missing data changes some conclusions about the comparability of the distributions in the two datasets. We also perform an extensive repeated sampling simulation using similar data from complete cases in an existing SIPP panel, comparing our proposed model to a default application of multiple imputation by chained equations. Imputations based on the proposed model tend to have better repeated sampling properties than the default application of chained equations in this realistic setting.

</details>

<details>

<summary>2015-10-13 11:04:55 - Conditions for Posterior Contraction in the Sparse Normal Means Problem</summary>

- *Stéphanie van der Pas, Jean-Bernard Salomond, Johannes Schmidt-Hieber*

- `1510.02232v2` - [abs](http://arxiv.org/abs/1510.02232v2) - [pdf](http://arxiv.org/pdf/1510.02232v2)

> The first Bayesian results for the sparse normal means problem were proven for spike-and-slab priors. However, these priors are less convenient from a computational point of view. In the meanwhile, a large number of continuous shrinkage priors has been proposed. Many of these shrinkage priors can be written as a scale mixture of normals, which makes them particularly easy to implement. We propose general conditions on the prior on the local variance in scale mixtures of normals, such that posterior contraction at the minimax rate is assured. The conditions require tails at least as heavy as Laplace, but not too heavy, and a large amount of mass around zero relative to the tails, more so as the sparsity increases. These conditions give some general guidelines for choosing a shrinkage prior for estimation under a nearly black sparsity assumption. We verify these conditions for the class of priors considered by Ghosh and Chakrabarti (2015), which includes the horseshoe and the normal-exponential gamma priors, and for the horseshoe+, the inverse-Gaussian prior, the normal-gamma prior, and the spike-and-slab Lasso, and thus extend the number of shrinkage priors which are known to lead to posterior contraction at the minimax estimation rate.

</details>

<details>

<summary>2015-10-13 16:54:41 - Gene network reconstruction using global-local shrinkage priors</summary>

- *Gwenaël G. R. Leday, Mathisca C. M. de Gunst, Gino B. Kpogbezan, Aad W. Van der Vaart, Wessel N. Van Wieringen, Mark A. Van de Wiel*

- `1510.03771v1` - [abs](http://arxiv.org/abs/1510.03771v1) - [pdf](http://arxiv.org/pdf/1510.03771v1)

> Reconstructing a gene network from high-throughput molecular data is often a challenging task, as the number of parameters to estimate easily is much larger than the sample size. A conventional remedy is to regularize or penalize the model likelihood. In network models, this is often done locally in the neighbourhood of each node or gene. However, estimation of the many regularization parameters is often difficult and can result in large statistical uncertainties. In this paper we propose to combine local regularization with global shrinkage of the regularization parameters to borrow strength between genes and improve inference. We employ a simple Bayesian model with non-sparse, conjugate priors to facilitate the use of fast variational approximations to posteriors. We discuss empirical Bayes estimation of hyper-parameters of the priors, and propose a novel approach to rank-based posterior thresholding. Using extensive model- and data-based simulations, we demonstrate that the proposed inference strategy outperforms popular (sparse) methods, yields more stable edges, and is more reproducible.

</details>

<details>

<summary>2015-10-14 07:44:56 - Varying-coefficient models with isotropic Gaussian process priors</summary>

- *Matthias Bussas, Christoph Sawade, Tobias Scheffer, Niels Landwehr*

- `1508.07192v2` - [abs](http://arxiv.org/abs/1508.07192v2) - [pdf](http://arxiv.org/pdf/1508.07192v2)

> We study learning problems in which the conditional distribution of the output given the input varies as a function of additional task variables. In varying-coefficient models with Gaussian process priors, a Gaussian process generates the functional relationship between the task variables and the parameters of this conditional. Varying-coefficient models subsume hierarchical Bayesian multitask models, but also generalizations in which the conditional varies continuously, for instance, in time or space. However, Bayesian inference in varying-coefficient models is generally intractable. We show that inference for varying-coefficient models with isotropic Gaussian process priors resolves to standard inference for a Gaussian process that can be solved efficiently. MAP inference in this model resolves to multitask learning using task and instance kernels, and inference for hierarchical Bayesian multitask models can be carried out efficiently using graph-Laplacian kernels. We report on experiments for geospatial prediction.

</details>

<details>

<summary>2015-10-14 09:04:57 - Bayesian linear regression with sparse priors</summary>

- *Ismaël Castillo, Johannes Schmidt-Hieber, Aad van der Vaart*

- `1403.0735v3` - [abs](http://arxiv.org/abs/1403.0735v3) - [pdf](http://arxiv.org/pdf/1403.0735v3)

> We study full Bayesian procedures for high-dimensional linear regression under sparsity constraints. The prior is a mixture of point masses at zero and continuous distributions. Under compatibility conditions on the design matrix, the posterior distribution is shown to contract at the optimal rate for recovery of the unknown sparse vector, and to give optimal prediction of the response vector. It is also shown to select the correct sparse model, or at least the coefficients that are significantly different from zero. The asymptotic shape of the posterior distribution is characterized and employed to the construction and study of credible sets for uncertainty quantification.

</details>

<details>

<summary>2015-10-15 00:16:57 - Batch Bayesian Optimization via Local Penalization</summary>

- *Javier González, Zhenwen Dai, Philipp Hennig, Neil D. Lawrence*

- `1505.08052v4` - [abs](http://arxiv.org/abs/1505.08052v4) - [pdf](http://arxiv.org/pdf/1505.08052v4)

> The popularity of Bayesian optimization methods for efficient exploration of parameter spaces has lead to a series of papers applying Gaussian processes as surrogates in the optimization of functions. However, most proposed approaches only allow the exploration of the parameter space to occur sequentially. Often, it is desirable to simultaneously propose batches of parameter values to explore. This is particularly the case when large parallel processing facilities are available. These facilities could be computational or physical facets of the process being optimized. E.g. in biological experiments many experimental set ups allow several samples to be simultaneously processed. Batch methods, however, require modeling of the interaction between the evaluations in the batch, which can be expensive in complex scenarios. We investigate a simple heuristic based on an estimate of the Lipschitz constant that captures the most important aspect of this interaction (i.e. local repulsion) at negligible computational overhead. The resulting algorithm compares well, in running time, with much more elaborate alternatives. The approach assumes that the function of interest, $f$, is a Lipschitz continuous function. A wrap-loop around the acquisition function is used to collect batches of points of certain size minimizing the non-parallelizable computational effort. The speed-up of our method with respect to previous approaches is significant in a set of computationally expensive experiments.

</details>

<details>

<summary>2015-10-15 18:54:13 - Detecting Unspecified Structure in Low-Count Images</summary>

- *Nathan M. Stein, David A. van Dyk, Vinay L. Kashyap, Aneta Siemiginowska*

- `1510.04662v1` - [abs](http://arxiv.org/abs/1510.04662v1) - [pdf](http://arxiv.org/pdf/1510.04662v1)

> Unexpected structure in images of astronomical sources often presents itself upon visual inspection of the image, but such apparent structure may either correspond to true features in the source or be due to noise in the data. This paper presents a method for testing whether inferred structure in an image with Poisson noise represents a significant departure from a baseline (null) model of the image. To infer image structure, we conduct a Bayesian analysis of a full model that uses a multiscale component to allow flexible departures from the posited null model. As a test statistic, we use a tail probability of the posterior distribution under the full model. This choice of test statistic allows us to estimate a computationally efficient upper bound on a p-value that enables us to draw strong conclusions even when there are limited computational resources that can be devoted to simulations under the null model. We demonstrate the statistical performance of our method on simulated images. Applying our method to an X-ray image of the quasar 0730+257, we find significant evidence against the null model of a single point source and uniform background, lending support to the claim of an X-ray jet.

</details>

<details>

<summary>2015-10-16 16:08:12 - Characterizing predictable classes of processes</summary>

- *Daniil Ryabko*

- `1408.2036v2` - [abs](http://arxiv.org/abs/1408.2036v2) - [pdf](http://arxiv.org/pdf/1408.2036v2)

> The problem is sequence prediction in the following setting. A sequence x1,..., xn,... of discrete-valued observations is generated according to some unknown probabilistic law (measure) mu. After observing each outcome, it is required to give the conditional probabilities of the next observation. The measure mu belongs to an arbitrary class C of stochastic processes. We are interested in predictors ? whose conditional probabilities converge to the 'true' mu-conditional probabilities if any mu { C is chosen to generate the data. We show that if such a predictor exists, then a predictor can also be obtained as a convex combination of a countably many elements of C. In other words, it can be obtained as a Bayesian predictor whose prior is concentrated on a countable set. This result is established for two very different measures of performance of prediction, one of which is very strong, namely, total variation, and the other is very weak, namely, prediction in expected average Kullback-Leibler divergence.

</details>

<details>

<summary>2015-10-17 05:11:55 - Dimension-independent likelihood-informed MCMC</summary>

- *Tiangang Cui, Kody J. H. Law, Youssef M. Marzouk*

- `1411.3688v2` - [abs](http://arxiv.org/abs/1411.3688v2) - [pdf](http://arxiv.org/pdf/1411.3688v2)

> Many Bayesian inference problems require exploring the posterior distribution of high-dimensional parameters that represent the discretization of an underlying function. This work introduces a family of Markov chain Monte Carlo (MCMC) samplers that can adapt to the particular structure of a posterior distribution over functions. Two distinct lines of research intersect in the methods developed here. First, we introduce a general class of operator-weighted proposal distributions that are well defined on function space, such that the performance of the resulting MCMC samplers is independent of the discretization of the function. Second, by exploiting local Hessian information and any associated low-dimensional structure in the change from prior to posterior distributions, we develop an inhomogeneous discretization scheme for the Langevin stochastic differential equation that yields operator-weighted proposals adapted to the non-Gaussian structure of the posterior. The resulting dimension-independent, likelihood-informed (DILI) MCMC samplers may be useful for a large class of high-dimensional problems where the target probability measure has a density with respect to a Gaussian reference measure. Two nonlinear inverse problems are used to demonstrate the efficiency of these DILI samplers: an elliptic PDE coefficient inverse problem and path reconstruction in a conditioned diffusion.

</details>

<details>

<summary>2015-10-17 18:54:27 - On Inverse Probability Weighting for Nonmonotone Missing at Random Data</summary>

- *BaoLuo Sun, Eric J. Tchetgen Tchetgen*

- `1411.5310v2` - [abs](http://arxiv.org/abs/1411.5310v2) - [pdf](http://arxiv.org/pdf/1411.5310v2)

> The development of coherent missing data models to account for nonmonotone missing at random (MAR) data by inverse probability weighting (IPW) remains to date largely unresolved. As a consequence, IPW has essentially been restricted for use only in monotone missing data settings. We propose a class of models for nonmonotone missing data mechanisms that spans the MAR model, while allowing the underlying full data law to remain unrestricted. For parametric specifications within the proposed class, we introduce an unconstrained maximum likelihood estimator for estimating the missing data probabilities which can be easily implemented using existing software. To circumvent potential convergence issues with this procedure, we also introduce a Bayesian constrained approach to estimate the missing data process which is guaranteed to yield inferences that respect all model restrictions. The efficiency of the standard IPW estimator is improved by incorporating information from incomplete cases through an augmented estimating equation which is optimal within a large class of estimating equations. We investigate the finite-sample properties of the proposed estimators in a simulation study and illustrate the new methodology in an application evaluating key correlates of preterm delivery for infants born to HIV infected mothers in Botswana, Africa.

</details>

<details>

<summary>2015-10-18 14:53:07 - Designs for Generalized Linear Models</summary>

- *Anthony C. Atkinson, David C. Woods*

- `1510.05253v1` - [abs](http://arxiv.org/abs/1510.05253v1) - [pdf](http://arxiv.org/pdf/1510.05253v1)

> This paper reviews the design of experiments for generalised linear models, including optimal design, Bayesian design and designs for models with random effects.

</details>

<details>

<summary>2015-10-19 11:31:38 - A Bayesian alternative to mutual information for the hierarchical clustering of dependent random variables</summary>

- *Guillaume Marrelec, Arnaud Messé, Pierre Bellec*

- `1501.05194v2` - [abs](http://arxiv.org/abs/1501.05194v2) - [pdf](http://arxiv.org/pdf/1501.05194v2)

> The use of mutual information as a similarity measure in agglomerative hierarchical clustering (AHC) raises an important issue: some correction needs to be applied for the dimensionality of variables. In this work, we formulate the decision of merging dependent multivariate normal variables in an AHC procedure as a Bayesian model comparison. We found that the Bayesian formulation naturally shrinks the empirical covariance matrix towards a matrix set a priori (e.g., the identity), provides an automated stopping rule, and corrects for dimensionality using a term that scales up the measure as a function of the dimensionality of the variables. Also, the resulting log Bayes factor is asymptotically proportional to the plug-in estimate of mutual information, with an additive correction for dimensionality in agreement with the Bayesian information criterion. We investigated the behavior of these Bayesian alternatives (in exact and asymptotic forms) to mutual information on simulated and real data. An encouraging result was first derived on simulations: the hierarchical clustering based on the log Bayes factor outperformed off-the-shelf clustering techniques as well as raw and normalized mutual information in terms of classification accuracy. On a toy example, we found that the Bayesian approaches led to results that were similar to those of mutual information clustering techniques, with the advantage of an automated thresholding. On real functional magnetic resonance imaging (fMRI) datasets measuring brain activity, it identified clusters consistent with the established outcome of standard procedures. On this application, normalized mutual information had a highly atypical behavior, in the sense that it systematically favored very large clusters. These initial experiments suggest that the proposed Bayesian alternatives to mutual information are a useful new tool for hierarchical clustering.

</details>

<details>

<summary>2015-10-19 15:31:51 - Void Probabilities and Cauchy-Schwarz Divergence for Generalized Labeled Multi-Bernoulli Models</summary>

- *Michael Beard, Ba-Tuong Vo, Ba-Ngu Vo, Sanjeev Arulampalam*

- `1510.05532v1` - [abs](http://arxiv.org/abs/1510.05532v1) - [pdf](http://arxiv.org/pdf/1510.05532v1)

> The generalized labeled multi-Bernoulli (GLMB) is a family of tractable models that alleviates the limitations of the Poisson family in dynamic Bayesian inference of point processes. In this paper, we derive closed form expressions for the void probability functional and the Cauchy-Schwarz divergence for GLMBs. The proposed analytic void probability functional is a necessary and sufficient statistic that uniquely characterizes a GLMB, while the proposed analytic Cauchy-Schwarz divergence provides a tractable measure of similarity between GLMBs. We demonstrate the use of both results on a partially observed Markov decision process for GLMBs, with Cauchy-Schwarz divergence based reward, and void probability constraint.

</details>

<details>

<summary>2015-10-19 23:53:30 - Models with time-varying predictors for meningitis in Navrongo, Ghana</summary>

- *Yolanda Hagar, Mary Hayden, Abudulai Adams Forgor, Patricia Akweongo, Abraham Hodgson, Christine Wiedinmyer, Vanja Dukic*

- `1510.05723v1` - [abs](http://arxiv.org/abs/1510.05723v1) - [pdf](http://arxiv.org/pdf/1510.05723v1)

> The "meningitis belt" is a region in sub-Saharan Africa where annual outbreaks of meningitis occur, with large epidemics observed cyclically. While we know that meningitis is heavily dependent on seasonal trends (in particular, weather), the exact pathways for contracting the disease are not fully understood and warrant further investigation. This manuscript examines meningitis trends in the context of survival analysis, quantifying underlying seasonal patterns in meningitis rates through the hazard rate for the population of Navrongo, Ghana. We compare three candidate models: the commonly used Poisson generalized linear model, the Bayesian multi-resolution hazard model, and the Poisson generalized additive model. We compare the accuracy and robustness of the models through the bias, RMSE, and the standard deviation. We provide a detailed case study of meningitis patterns for data collected in Navrongo, Ghana.

</details>

<details>

<summary>2015-10-20 12:13:24 - Partition MCMC for inference on acyclic digraphs</summary>

- *Jack Kuipers, Giusi Moffa*

- `1504.05006v2` - [abs](http://arxiv.org/abs/1504.05006v2) - [pdf](http://arxiv.org/pdf/1504.05006v2)

> Acyclic digraphs are the underlying representation of Bayesian networks, a widely used class of probabilistic graphical models. Learning the underlying graph from data is a way of gaining insights about the structural properties of a domain. Structure learning forms one of the inference challenges of statistical graphical models.   MCMC methods, notably structure MCMC, to sample graphs from the posterior distribution given the data are probably the only viable option for Bayesian model averaging. Score modularity and restrictions on the number of parents of each node allow the graphs to be grouped into larger collections, which can be scored as a whole to improve the chain's convergence. Current examples of algorithms taking advantage of grouping are the biased order MCMC, which acts on the alternative space of permuted triangular matrices, and non ergodic edge reversal moves.   Here we propose a novel algorithm, which employs the underlying combinatorial structure of DAGs to define a new grouping. As a result convergence is improved compared to structure MCMC, while still retaining the property of producing an unbiased sample. Finally the method can be combined with edge reversal moves to improve the sampler further.

</details>

<details>

<summary>2015-10-20 15:09:43 - Bayesian Nonparametric Modeling of Higher Order Markov Chains</summary>

- *Abhra Sarkar, David B. Dunson*

- `1506.06268v4` - [abs](http://arxiv.org/abs/1506.06268v4) - [pdf](http://arxiv.org/pdf/1506.06268v4)

> We consider the problem of flexible modeling of higher order Markov chains when an upper bound on the order of the chain is known but the true order and nature of the serial dependence are unknown. We propose Bayesian nonparametric methodology based on conditional tensor factorizations, which can characterize any transition probability with a specified maximal order. The methodology selects the important lags and captures higher order interactions among the lags, while also facilitating calculation of Bayes factors for a variety of hypotheses of interest. We design efficient Markov chain Monte Carlo algorithms for posterior computation, allowing for uncertainty in the set of important lags to be included and in the nature and order of the serial dependence. The methods are illustrated using simulation experiments and real world applications.

</details>

<details>

<summary>2015-10-20 23:07:55 - Quantile Versions of the Lorenz Curve</summary>

- *Luke A. Prendergast, Robert G. Staudte*

- `1510.06085v1` - [abs](http://arxiv.org/abs/1510.06085v1) - [pdf](http://arxiv.org/pdf/1510.06085v1)

> The classical Lorenz curve is often used to depict inequality in a population of incomes, and the associated Gini coefficient is relied upon to make comparisons between different countries and other groups. The sample estimates of these moment-based concepts are sensitive to outliers and so we investigate the extent to which quantile-based definitions can capture income inequality and lead to more robust procedures. Distribution-free estimates of the corresponding coefficients of inequality are obtained, as well as sample sizes required to estimate them to a given accuracy. Convexity, transference and robustness of the measures are examined and illustrated.

</details>

<details>

<summary>2015-10-21 06:13:49 - Multiple co-clustering based on nonparametric mixture models with heterogeneous marginal distributions</summary>

- *Tomoki Tokuda, Junichiro Yoshimoto, Yu Shimizu, Shigeru Toki, Go Okada, Masahiro Takamura, Tetsuya Yamamoto, Shinpei Yoshimura, Yasumasa Okamoto, Shigeto Yamawaki, Kenji Doya*

- `1510.06138v1` - [abs](http://arxiv.org/abs/1510.06138v1) - [pdf](http://arxiv.org/pdf/1510.06138v1)

> We propose a novel method for multiple clustering that assumes a co-clustering structure (partitions in both rows and columns of the data matrix) in each view. The new method is applicable to high-dimensional data. It is based on a nonparametric Bayesian approach in which the number of views and the number of feature-/subject clusters are inferred in a data-driven manner. We simultaneously model different distribution families, such as Gaussian, Poisson, and multinomial distributions in each cluster block. This makes our method applicable to datasets consisting of both numerical and categorical variables, which biomedical data typically do. Clustering solutions are based on variational inference with mean field approximation. We apply the proposed method to synthetic and real data, and show that our method outperforms other multiple clustering methods both in recovering true cluster structures and in computation time. Finally, we apply our method to a depression dataset with no true cluster structure available, from which useful inferences are drawn about possible clustering structures of the data.

</details>

<details>

<summary>2015-10-21 15:30:17 - GLASSES: Relieving The Myopia Of Bayesian Optimisation</summary>

- *Javier González, Michael Osborne, Neil D. Lawrence*

- `1510.06299v1` - [abs](http://arxiv.org/abs/1510.06299v1) - [pdf](http://arxiv.org/pdf/1510.06299v1)

> We present GLASSES: Global optimisation with Look-Ahead through Stochastic Simulation and Expected-loss Search. The majority of global optimisation approaches in use are myopic, in only considering the impact of the next function value; the non-myopic approaches that do exist are able to consider only a handful of future evaluations. Our novel algorithm, GLASSES, permits the consideration of dozens of evaluations into the future. This is done by approximating the ideal look-ahead loss function, which is expensive to evaluate, by a cheaper alternative in which the future steps of the algorithm are simulated beforehand. An Expectation Propagation algorithm is used to compute the expected value of the loss.We show that the far-horizon planning thus enabled leads to substantive performance gains in empirical tests.

</details>

<details>

<summary>2015-10-21 18:32:00 - The Odd Generalized Exponential Linear Failure Rate Distribution</summary>

- *M. A. El-Damcese, Abdelfattah Mustafa, B. S. El-Desouky, M. E. Mustafa*

- `1510.06395v1` - [abs](http://arxiv.org/abs/1510.06395v1) - [pdf](http://arxiv.org/pdf/1510.06395v1)

> In this paper we propose a new lifetime model, called the odd generalized exponential linear failure rate distribution. Some statistical properties of the proposed distribution such as the moments, the quantiles, the median, and the mode are investigated. The method of maximum likelihood is used for estimating the model parameters. An applications to real data is carried out to illustrate that the new distribution is more flexible and effective than other popular distributions in modeling lifetime data.

</details>

<details>

<summary>2015-10-21 21:27:31 - Statistical Inference for Partially Observed Markov Processes via the R Package pomp</summary>

- *Aaron A. King, Dao Nguyen, Edward L. Ionides*

- `1509.00503v2` - [abs](http://arxiv.org/abs/1509.00503v2) - [pdf](http://arxiv.org/pdf/1509.00503v2)

> Partially observed Markov process (POMP) models, also known as hidden Markov models or state space models, are ubiquitous tools for time series analysis. The R package pomp provides a very flexible framework for Monte Carlo statistical investigations using nonlinear, non-Gaussian POMP models. A range of modern statistical methods for POMP models have been implemented in this framework including sequential Monte Carlo, iterated filtering, particle Markov chain Monte Carlo, approximate Bayesian computation, maximum synthetic likelihood estimation, nonlinear forecasting, and trajectory matching. In this paper, we demonstrate the application of these methodologies using some simple toy problems. We also illustrate the specification of more complex POMP models, using a nonlinear epidemiological model with a discrete population, seasonality, and extra-demographic stochasticity. We discuss the specification of user-defined models and the development of additional methods within the programming environment provided by pomp.

</details>

<details>

<summary>2015-10-22 00:18:21 - A Bounded $p$-norm Approximation of Max-Convolution for Sub-Quadratic Bayesian Inference on Additive Factors</summary>

- *Julianus Pfeuffer, Oliver Serang*

- `1505.07519v2` - [abs](http://arxiv.org/abs/1505.07519v2) - [pdf](http://arxiv.org/pdf/1505.07519v2)

> Max-convolution is an important problem closely resembling standard convolution; as such, max-convolution occurs frequently across many fields. Here we extend the method with fastest known worst-case runtime, which can be applied to nonnegative vectors by numerically approximating the Chebyshev norm $\| \cdot \|_\infty$, and use this approach to derive two numerically stable methods based on the idea of computing $p$-norms via fast convolution: The first method proposed, with runtime in $O( k \log(k) \log(\log(k)) )$ (which is less than $18 k \log(k)$ for any vectors that can be practically realized), uses the $p$-norm as a direct approximation of the Chebyshev norm. The second approach proposed, with runtime in $O( k \log(k) )$ (although in practice both perform similarly), uses a novel null space projection method, which extracts information from a sequence of $p$-norms to estimate the maximum value in the vector (this is equivalent to querying a small number of moments from a distribution of bounded support in order to estimate the maximum). The $p$-norm approaches are compared to one another and are shown to compute an approximation of the Viterbi path in a hidden Markov model where the transition matrix is a Toeplitz matrix; the runtime of approximating the Viterbi path is thus reduced from $O( n k^2 )$ steps to $O( n $k \log(k))$ steps in practice, and is demonstrated by inferring the U.S. unemployment rate from the S&P 500 stock index.

</details>

<details>

<summary>2015-10-22 16:01:36 - Bayesian Nonparametric Density Estimation under Length Bias</summary>

- *Spyridon J. Hatjispyros, Theodoros Nicoleris, Stephen G. Walker*

- `1510.06307v2` - [abs](http://arxiv.org/abs/1510.06307v2) - [pdf](http://arxiv.org/pdf/1510.06307v2)

> A density estimation method in a Bayesian nonparametric framework is presented when recorded data are not coming directly from the distribution of interest, but from a length biased version. From a Bayesian perspective, efforts to computationally evaluate posterior quantities conditionally on length biased data were hindered by the inability to circumvent the problem of a normalizing constant. In this paper we present a novel Bayesian nonparametric approach to the length bias sampling problem which circumvents the issue of the normalizing constant. Numerical illustrations as well as a real data example are presented and the estimator is compared against its frequentist counterpart, the kernel density estimator for indirect data of Jones (1991).

</details>

<details>

<summary>2015-10-22 20:52:22 - A two-component normal mixture alternative to the Fay-Herriot model</summary>

- *Adrijo Chakraborty, Gauri Sankar Datta, Abhyuday Mandal*

- `1510.04482v2` - [abs](http://arxiv.org/abs/1510.04482v2) - [pdf](http://arxiv.org/pdf/1510.04482v2)

> This article considers a robust hierarchical Bayesian approach to deal with random effects of small area means when some of these effects assume extreme values, resulting in outliers. In presence of outliers, the standard Fay-Herriot model, used for modeling area-level data, under normality assumptions of the random effects may overestimate random effects variance, thus provides less than ideal shrinkage towards the synthetic regression predictions and inhibits borrowing information. Even a small number of substantive outliers of random effects result in a large estimate of the random effects variance in the Fay-Herriot model, thereby achieving little shrinkage to the synthetic part of the model or little reduction in posterior variance associated with the regular Bayes estimator for any of the small areas. While a scale mixture of normal distributions with known mixing distribution for the random effects has been found to be effective in presence of outliers, the solution depends on the mixing distribution. As a possible alternative solution to the problem, a two-component normal mixture model has been proposed based on noninformative priors on the model variance parameters, regression coefficients and the mixing probability. Data analysis and simulation studies based on real, simulated and synthetic data show advantage of the proposed method over the standard Bayesian Fay-Herriot solution derived under normality of random effects.

</details>

<details>

<summary>2015-10-24 10:28:14 - Bayesian Inference for High Dimensional Changing Linear Regression with Application to Minnesota House Price Index Data</summary>

- *Abhirup Datta, Hui Zou, Sudipto Banerjee*

- `1510.07129v1` - [abs](http://arxiv.org/abs/1510.07129v1) - [pdf](http://arxiv.org/pdf/1510.07129v1)

> In many applications, the dataset under investigation exhibits heterogeneous regimes that are more appropriately modeled using piece-wise linear models for each of the data segments separated by change-points. Although there have been much work on change point linear regression for the low dimensional case, high-dimensional change point regression is severely underdeveloped. Motivated by the analysis of Minnesota House Price Index data, we propose a fully Bayesian framework for fitting changing linear regression models in high-dimensional settings. Using segment-specific shrinkage and diffusion priors, we deliver full posterior inference for the change points and simultaneously obtain posterior probabilities of variable selection in each segment via an efficient Gibbs sampler. Additionally, our method can detect an unknown number of change points and accommodate different variable selection constraints like grouping or partial selection. We substantiate the accuracy of our method using simulation experiments for a wide range of scenarios. We apply our approach for a macro-economic analysis of Minnesota house price index data. The results strongly favor the change point model over a homogeneous (no change point) high-dimensional regression model.

</details>

<details>

<summary>2015-10-25 18:58:32 - A Bootstrap Likelihood approach to Bayesian Computation</summary>

- *Weixuan Zhu, Juan Miguel Marin, Fabrizio Leisen*

- `1510.07287v1` - [abs](http://arxiv.org/abs/1510.07287v1) - [pdf](http://arxiv.org/pdf/1510.07287v1)

> There is an increasing amount of literature focused on Bayesian computational methods to address problems with intractable likelihood. One approach is a set of algorithms known as Approximate Bayesian Computational (ABC) methods. One of the problems of these algorithms is that the performance depends on the tuning of some parameters, such as the summary statistics, distance and tolerance level. To bypass this problem, Mengersen, Pudlo and Robert (2013) introduced an alternative method based on empirical likelihood, which can be easily implemented when a set of constraints, related to the moments of the distribution, is known. However, the choice of the constraints is sometimes challenging. To overcome this problem, we propose an alternative method based on a bootstrap likelihood approach. The method is easy to implement and in some cases it is faster than the other approaches. The performance of the algorithm is illustrated with examples in Population Genetics, Time Series and Stochastic Differential Equations. Finally, we test the method on a real dataset.

</details>

<details>

<summary>2015-10-26 13:34:50 - Polynomial Chaos-based Bayesian Inference of K-Profile Parametrization in a General Circulation Model of the Tropical Pacific</summary>

- *Ihab Sraj, Sarah E. Zedler, Omar M. Knio, Charles S. Jackson, Ibrahim Hoteit*

- `1510.07476v1` - [abs](http://arxiv.org/abs/1510.07476v1) - [pdf](http://arxiv.org/pdf/1510.07476v1)

> The authors present a Polynomial Chaos (PC)-based Bayesian inference method for quantifying the uncertainties of the K-Profile Parametrization (KPP) within the MIT General Circulation Model (MITgcm) of the tropical pacific. The inference of the uncertain parameters is based on a Markov Chain Monte Carlo (MCMC) scheme that utilizes a newly formulated test statistic taking into account the different components representing the structures of turbulent mixing on both daily and seasonal timescales in addition to the data quality, and filters for the effects of parameter perturbations over those due to changes in the wind. To avoid the prohibitive computational cost of integrating the MITgcm model at each MCMC iteration, we build a surrogate model for the test statistic using the PC method. To filter out the noise in the model predictions and avoid related convergence issues, we resort to a Basis-Pursuit-DeNoising (BPDN) compressed sensing approach to determine the PC coefficients of a representative surrogate model. The PC surrogate is then used to evaluate the test statistic in the MCMC step for sampling the posterior of the uncertain parameters. Results of the posteriors indicate good agreement with the default values for two parameters of the KPP model namely the critical bulk and gradient Richardson numbers; while the posteriors of the remaining parameters were barely informative.

</details>

<details>

<summary>2015-10-26 18:13:55 - Parallelizing MCMC with Random Partition Trees</summary>

- *Xiangyu Wang, Fangjian Guo, Katherine A. Heller, David B. Dunson*

- `1506.03164v2` - [abs](http://arxiv.org/abs/1506.03164v2) - [pdf](http://arxiv.org/pdf/1506.03164v2)

> The modern scale of data has brought new challenges to Bayesian inference. In particular, conventional MCMC algorithms are computationally very expensive for large data sets. A promising approach to solve this problem is embarrassingly parallel MCMC (EP-MCMC), which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset. The subset posterior draws are then aggregated via some combining rules to obtain the final approximation. Existing EP-MCMC algorithms are limited by approximation accuracy and difficulty in resampling. In this article, we propose a new EP-MCMC algorithm PART that solves these problems. The new algorithm applies random partition trees to combine the subset posterior draws, which is distribution-free, easy to resample from and can adapt to multiple scales. We provide theoretical justification and extensive experiments illustrating empirical performance.

</details>

<details>

<summary>2015-10-27 10:43:24 - A Bayesian model for microarray datasets merging</summary>

- *Marie-Christine Roubaud, Bruno Torrésani*

- `1510.07850v1` - [abs](http://arxiv.org/abs/1510.07850v1) - [pdf](http://arxiv.org/pdf/1510.07850v1)

> The aggregation of microarray datasets originating from different studies is still a difficult open problem. Currently, best results are generally obtained by the so-called meta-analysis approach, which aggregates results from individual datasets, instead of analyzing aggre-gated datasets. In order to tackle such aggregation problems, it is necessary to correct for interstudy variability prior to aggregation. The goal of this paper is to present a new approach for microarray datasets merging, based upon explicit modeling of interstudy variability and gene variability. We develop and demonstrate a new algorithm for microarray datasets merging. The underlying model assumes normally distributed intrinsic gene expressions, distorted by a study-dependent nonlinear transformation, and study dependent (normally distributed) observation noise. The algorithm addresses both parameter estimation (the parameters being gene expression means and variances, observation noise variances and the nonlinear transformations) and data adjustment, and yields as a result adjusted datasets suitable for aggregation. The method is validated on two case studies. The first one concerns E. Coli expression data, artificially distorted by given nonlinear transformations and additive observation noise. The proposed method is able to correct for the distortion, and yields adjusted datasets from which the relevant biological effects can be recovered, as shown by a standard differential analysis. The second case study concerns the aggregation of two real prostate cancer datasets. After adjustment using the proposed algorithm, a differential analysis performed on adjusted datasets yields a larger number of differentially expressed genes (between control and tumor data). The proposed method has been implemented using the statistical software R 1, and Bioconductor packages 2. The source code (valid for merging two datasets), as well as the datasets used for the validation, and some complementary results, are made available on the web site

</details>

<details>

<summary>2015-10-29 09:40:42 - Establishing some order amongst exact approximations of MCMCs</summary>

- *Christophe Andrieu, Matti Vihola*

- `1404.6909v2` - [abs](http://arxiv.org/abs/1404.6909v2) - [pdf](http://arxiv.org/pdf/1404.6909v2)

> Exact approximations of Markov chain Monte Carlo (MCMC) algorithms are a general emerging class of sampling algorithms. One of the main ideas behind exact approximations consists of replacing intractable quantities required to run standard MCMC algorithms, such as the target probability density in a Metropolis-Hastings algorithm, with estimators. Perhaps surprisingly, such approximations lead to powerful algorithms which are exact in the sense that they are guaranteed to have correct limiting distributions. In this paper we discover a general framework which allows one to compare, or order, performance measures of two implementations of such algorithms. In particular, we establish an order with respect to the mean acceptance probability, the first autocorrelation coefficient, the asymptotic variance and the right spectral gap. The key notion to guarantee the ordering is that of the convex order between estimators used to implement the algorithms. We believe that our convex order condition is close to optimal, and this is supported by a counter-example which shows that a weaker variance order is not sufficient. The convex order plays a central role by allowing us to construct a martingale coupling which enables the comparison of performance measures of Markov chain with differing invariant distributions, contrary to existing results. We detail applications of our result by identifying extremal distributions within given classes of approximations, by showing that averaging replicas improves performance in a monotonic fashion and that stratification is guaranteed to improve performance for the standard implementation of the Approximate Bayesian Computation (ABC) MCMC method.

</details>

<details>

<summary>2015-10-29 17:41:25 - Langevin and Hamiltonian based Sequential MCMC for Efficient Bayesian Filtering in High-dimensional Spaces</summary>

- *Francois Septier, Gareth W. Peters*

- `1504.05715v2` - [abs](http://arxiv.org/abs/1504.05715v2) - [pdf](http://arxiv.org/pdf/1504.05715v2)

> Nonlinear non-Gaussian state-space models arise in numerous applications in statistics and signal processing. In this context, one of the most successful and popular approximation techniques is the Sequential Monte Carlo (SMC) algorithm, also known as particle filtering. Nevertheless, this method tends to be inefficient when applied to high dimensional problems. In this paper, we focus on another class of sequential inference methods, namely the Sequential Markov Chain Monte Carlo (SMCMC) techniques, which represent a promising alternative to SMC methods. After providing a unifying framework for the class of SMCMC approaches, we propose novel efficient strategies based on the principle of Langevin diffusion and Hamiltonian dynamics in order to cope with the increasing number of high-dimensional applications. Simulation results show that the proposed algorithms achieve significantly better performance compared to existing algorithms.

</details>

<details>

<summary>2015-10-29 18:01:52 - Fast Out-of-Sample Predictions for Bayesian Hierarchical Models of Latent Health States</summary>

- *Aaron J Fisher, R Yates Coley, Scott L Zeger*

- `1510.08802v1` - [abs](http://arxiv.org/abs/1510.08802v1) - [pdf](http://arxiv.org/pdf/1510.08802v1)

> Hierarchical Bayesian models can be especially useful in precision medicine settings, where clinicians are interested in estimating the patient-level latent variables associated with an individual's current health state and its trajectory. Such models are often fit using batch Markov Chain Monte Carlo (MCMC). However, the slow speed of batch MCMC computation makes it difficult to implement in clinical settings, where immediate latent variable estimates are often desired in response to new patient data. In this report, we discuss how importance sampling (IS) can instead be used to obtain fast, in-clinic estimates of patient-level latent variables. We apply IS to the hierarchical model proposed in Coley et al (2015) for predicting an individual's underlying prostate cancer state. We find that latent variable estimates via IS can typically be obtained in 1-10 seconds per person and have high agreement with estimates coming from longer-running batch MCMC methods. Alternative options for out-of-sample fitting and online updating are also discussed.

</details>

<details>

<summary>2015-10-29 20:04:49 - Nested Partially-Latent Class Models for Dependent Binary Data; Estimating Disease Etiology</summary>

- *Zhenke Wu, Maria Deloria-Knoll, Scott Zeger*

- `1510.08862v1` - [abs](http://arxiv.org/abs/1510.08862v1) - [pdf](http://arxiv.org/pdf/1510.08862v1)

> The Pneumonia Etiology Research for Child Health (PERCH) study seeks to use modern measurement technology to infer the causes of pneumonia for which gold-standard evidence is unavailable. The paper describes a latent variable model designed to infer from case-control data the etiology distribution for the population of cases, and for an individual case given his or her measurements. We assume each observation is drawn from a mixture model for which each component represents one cause or disease class. The model addresses a major limitation of the traditional latent class approach by taking account of residual dependence among multivariate binary outcome given disease class, hence reduces estimation bias, retains efficiency and offers more valid inference. Such "local dependence" on a single subject is induced in the model by nesting latent subclasses within each disease class. Measurement precision and covariation can be estimated using the control sample for whom the class is known. In a Bayesian framework, we use stick-breaking priors on the subclass indicators for model-averaged inference across different numbers of subclasses. Assessment of model fit and individual diagnosis are done using posterior samples drawn by Gibbs sampling. We demonstrate the utility of the method on simulated and on the motivating PERCH data.

</details>

<details>

<summary>2015-10-30 15:39:06 - Latent Bayesian melding for integrating individual and population models</summary>

- *Mingjun Zhong, Nigel Goddard, Charles Sutton*

- `1510.09130v1` - [abs](http://arxiv.org/abs/1510.09130v1) - [pdf](http://arxiv.org/pdf/1510.09130v1)

> In many statistical problems, a more coarse-grained model may be suitable for population-level behaviour, whereas a more detailed model is appropriate for accurate modelling of individual behaviour. This raises the question of how to integrate both types of models. Methods such as posterior regularization follow the idea of generalized moment matching, in that they allow matching expectations between two models, but sometimes both models are most conveniently expressed as latent variable models. We propose latent Bayesian melding, which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework. In a case study on electricity disaggregation, which is a type of single-channel blind source separation problem, we show that latent Bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching.

</details>

<details>

<summary>2015-10-30 17:04:33 - Streaming, Distributed Variational Inference for Bayesian Nonparametrics</summary>

- *Trevor Campbell, Julian Straub, John W. Fisher III, Jonathan P. How*

- `1510.09161v1` - [abs](http://arxiv.org/abs/1510.09161v1) - [pdf](http://arxiv.org/pdf/1510.09161v1)

> This paper presents a methodology for creating streaming, distributed inference algorithms for Bayesian nonparametric (BNP) models. In the proposed framework, processing nodes receive a sequence of data minibatches, compute a variational posterior for each, and make asynchronous streaming updates to a central model. In contrast to previous algorithms, the proposed framework is truly streaming, distributed, asynchronous, learning-rate-free, and truncation-free. The key challenge in developing the framework, arising from the fact that BNP models do not impose an inherent ordering on their components, is finding the correspondence between minibatch and central BNP posterior components before performing each update. To address this, the paper develops a combinatorial optimization problem over component correspondences, and provides an efficient solution technique. The paper concludes with an application of the methodology to the DP mixture model, with experimental results demonstrating its practical scalability and performance.

</details>

<details>

<summary>2015-10-30 18:09:54 - Sparse Variational Bayesian Approximations for Nonlinear Inverse Problems: applications in nonlinear elastography</summary>

- *Isabell M. Franck, P. S. Koutsourelakis*

- `1412.0473v4` - [abs](http://arxiv.org/abs/1412.0473v4) - [pdf](http://arxiv.org/pdf/1412.0473v4)

> This paper presents an efficient Bayesian framework for solving nonlinear, high-dimensional model calibration problems. It is based on a Variational Bayesian formulation that aims at approximating the exact posterior by means of solving an optimization problem over an appropriately selected family of distributions. The goal is two-fold. Firstly, to find lower-dimensional representations of the unknown parameter vector that capture as much as possible of the associated posterior density, and secondly to enable the computation of the approximate posterior density with as few forward calls as possible. We discuss how these objectives can be achieved by using a fully Bayesian argumentation and employing the marginal likelihood or evidence as the ultimate model validation metric for any proposed dimensionality reduction. We demonstrate the performance of the proposed methodology for problems in nonlinear elastography where the identification of the mechanical properties of biological materials can inform non-invasive, medical diagnosis. An Importance Sampling scheme is finally employed in order to validate the results and assess the efficacy of the approximations provided.

</details>

<details>

<summary>2015-10-31 03:13:21 - A Bayesian Approach to Graphical Record Linkage and De-duplication</summary>

- *Rebecca C. Steorts, Rob Hall, Stephen E. Fienberg*

- `1312.4645v4` - [abs](http://arxiv.org/abs/1312.4645v4) - [pdf](http://arxiv.org/pdf/1312.4645v4)

> We propose an unsupervised approach for linking records across arbitrarily many files, while simultaneously detecting duplicate records within files. Our key innovation involves the representation of the pattern of links between records as a bipartite graph, in which records are directly linked to latent true individuals, and only indirectly linked to other records. This flexible representation of the linkage structure naturally allows us to estimate the attributes of the unique observable people in the population, calculate transitive linkage probabilities across records (and represent this visually), and propagate the uncertainty of record linkage into later analyses. Our method makes it particularly easy to integrate record linkage with post-processing procedures such as logistic regression, capture-recapture, etc. Our linkage structure lends itself to an efficient, linear-time, hybrid Markov chain Monte Carlo algorithm, which overcomes many obstacles encountered by previously record linkage approaches, despite the high-dimensional parameter space. We illustrate our method using longitudinal data from the National Long Term Care Survey and with data from the Italian Survey on Household and Wealth, where we assess the accuracy of our method and show it to be better in terms of error rates and empirical scalability than other approaches in the literature.

</details>

<details>

<summary>2015-10-31 05:38:15 - Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm</summary>

- *Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang*

- `1508.06235v4` - [abs](http://arxiv.org/abs/1508.06235v4) - [pdf](http://arxiv.org/pdf/1508.06235v4)

> In this paper, we propose a model-based clustering method (TVClust) that robustly incorporates noisy side information as soft-constraints and aims to seek a consensus between side information and the observed data. Our method is based on a nonparametric Bayesian hierarchical model that combines the probabilistic model for the data instance and the one for the side-information. An efficient Gibbs sampling algorithm is proposed for posterior inference. Using the small-variance asymptotics of our probabilistic model, we then derive a new deterministic clustering algorithm (RDP-means). It can be viewed as an extension of K-means that allows for the inclusion of side information and has the additional property that the number of clusters does not need to be specified a priori. Empirical studies have been carried out to compare our work with many constrained clustering algorithms from the literature on both a variety of data sets and under a variety of conditions such as using noisy side information and erroneous k values. The results of our experiments show strong results for our probabilistic and deterministic approaches under these conditions when compared to other algorithms in the literature.

</details>


## 2015-11

<details>

<summary>2015-11-01 22:41:19 - The Dynamic Splitting Method with an application to portfolio credit risk</summary>

- *Kevin Lam, Zdravko Botev*

- `1511.00326v1` - [abs](http://arxiv.org/abs/1511.00326v1) - [pdf](http://arxiv.org/pdf/1511.00326v1)

> We consider the problem of accurately measuring the credit risk of a portfolio consisting of loss exposures such as loans, bonds and other financial assets. We are particularly interested in the probability of large portfolio losses. We describe the popular models in the credit risk framework including factor models and copula models. To this end, we revisit the most efficient probability estimation algorithms within current copula credit risk literature, namely importance sampling. We illustrate the workings and developments of these algorithms for large portfolio loss probability estimation and quantile estimation. We then propose a modification to the dynamic splitting method which allows application to the credit risk models described. Our proposed algorithm for the unbiased estimation of rare-event probabilities, exploits the quasi-monotonic property of functions to embed a static simulation problem within a time-dependent Markov process. A study of our proposed algorithm is then conducted through numerical experiments with its performance benchmarked against current popular importance sampling algorithms.

</details>

<details>

<summary>2015-11-02 19:04:35 - Partial Functional Linear Quantile Regression for Neuroimaging Data Analysis</summary>

- *Dengdeng Yu, Linglong Kong, Ivan Mizera*

- `1511.00632v1` - [abs](http://arxiv.org/abs/1511.00632v1) - [pdf](http://arxiv.org/pdf/1511.00632v1)

> We propose a prediction procedure for the functional linear quantile regression model by using partial quantile covariance techniques and develop a simple partial quantile regression (SIMPQR) algorithm to efficiently extract partial quantile regression (PQR) basis for estimating functional coefficients. We further extend our partial quantile covariance techniques to functional composite quantile regression (CQR) defining partial composite quantile covariance. There are three major contributions. (1) We define partial quantile covariance between two scalar variables through linear quantile regression. We compute PQR basis by sequentially maximizing the partial quantile covariance between the response and projections of functional covariates. (2) In order to efficiently extract PQR basis, we develop a SIMPQR algorithm analogous to simple partial least squares (SIMPLS). (3) Under the homoscedasticity assumption, we extend our techniques to partial composite quantile covariance and use it to find the partial composite quantile regression (PCQR) basis. The SIMPQR algorithm is then modified to obtain the SIMPCQR algorithm. Two simulation studies show the superiority of our proposed methods. Two real data from ADHD-200 sample and ADNI are analyzed using our proposed methods.

</details>

<details>

<summary>2015-11-02 22:22:33 - Regularized quantile regression under heterogeneous sparsity with application to quantitative genetic traits</summary>

- *Qianchuan He, Linglong Kong, Yanhua Wang, Sijian Wang, Timothy A. Chan, Eric Holland*

- `1511.00730v1` - [abs](http://arxiv.org/abs/1511.00730v1) - [pdf](http://arxiv.org/pdf/1511.00730v1)

> Genetic studies often involve quantitative traits. Identifying genetic features that influence quantitative traits can help to uncover the etiology of diseases. Quantile regression method considers the conditional quantiles of the response variable, and is able to characterize the underlying regression structure in a more comprehensive manner. On the other hand, genetic studies often involve high dimensional genomic features, and the underlying regression structure may be heterogeneous in terms of both effect sizes and sparsity. To account for the potential genetic heterogeneity, including the heterogeneous sparsity, a regularized quantile regression method is introduced. The theoretical property of the proposed method is investigated, and its performance is examined through a series of simulation studies. A real dataset is analyzed to demonstrate the application of the proposed method.

</details>

<details>

<summary>2015-11-03 03:37:40 - Optimal Gaussian approximations to the posterior for log-linear models with Diaconis-Ylvisaker priors</summary>

- *James E. Johndrow, Anirban Bhattacharya*

- `1511.00764v1` - [abs](http://arxiv.org/abs/1511.00764v1) - [pdf](http://arxiv.org/pdf/1511.00764v1)

> In contingency table analysis, sparse data is frequently encountered for even modest numbers of variables, resulting in non-existence of maximum likelihood estimates. A common solution is to obtain regularized estimates of the parameters of a log-linear model. Bayesian methods provide a coherent approach to regularization, but are often computationally intensive. Conjugate priors ease computational demands, but the conjugate Diaconis-Ylvisaker priors for the parameters of log-linear models do not give rise to closed form credible regions, complicating posterior inference. Here we derive the optimal Gaussian approximation to the posterior for log-linear models with Diaconis-Ylvisaker priors, and provide convergence rate and finite-sample bounds for the Kullback-Leibler divergence between the exact posterior and the optimal Gaussian approximation. We demonstrate empirically in simulations and a real data application that the approximation is highly accurate, even in relatively small samples. The proposed approximation provides a computationally scalable and principled approach to regularized estimation and approximate Bayesian inference for log-linear models.

</details>

<details>

<summary>2015-11-03 05:01:12 - A Fast and Scalable Method for A-Optimal Design of Experiments for Infinite-dimensional Bayesian Nonlinear Inverse Problems</summary>

- *Alen Alexanderian, Noemi Petra, Georg Stadler, Omar Ghattas*

- `1410.5899v2` - [abs](http://arxiv.org/abs/1410.5899v2) - [pdf](http://arxiv.org/pdf/1410.5899v2)

> We address the problem of optimal experimental design (OED) for Bayesian nonlinear inverse problems governed by PDEs. The goal is to find a placement of sensors, at which experimental data are collected, so as to minimize the uncertainty in the inferred parameter field. We formulate the OED objective function by generalizing the classical A-optimal experimental design criterion using the expected value of the trace of the posterior covariance. We seek a method that solves the OED problem at a cost (measured in the number of forward PDE solves) that is independent of both the parameter and sensor dimensions. To facilitate this, we construct a Gaussian approximation to the posterior at the maximum a posteriori probability (MAP) point, and use the resulting covariance operator to define the OED objective function. We use randomized trace estimation to compute the trace of this (implicitly defined) covariance operator. The resulting OED problem includes as constraints the PDEs characterizing the MAP point, and the PDEs describing the action of the covariance operator to vectors. The sparsity of the sensor configurations is controlled using sparsifying penalty functions. We elaborate our OED method for the problem of determining the sensor placement to best infer the coefficient of an elliptic PDE. Adjoint methods are used to compute the gradient of the PDE-constrained OED objective function. We provide numerical results for inference of the permeability field in a porous medium flow problem, and demonstrate that the number of PDE solves required for the evaluation of the OED objective function and its gradient is essentially independent of both the parameter and sensor dimensions. The number of quasi-Newton iterations for computing an OED also exhibits the same dimension invariance properties.

</details>

<details>

<summary>2015-11-03 11:20:26 - Information Theory and Statistics: an overview</summary>

- *Daniel Commenges*

- `1511.00860v1` - [abs](http://arxiv.org/abs/1511.00860v1) - [pdf](http://arxiv.org/pdf/1511.00860v1)

> We give an overview of the role of information theory in statistics, and particularly in biostatistics. We recall the basic quantities in information theory; entropy, cross-entropy, conditional entropy, mutual information and Kullback-Leibler risk. Then we examine the role of information theory in estimation theory, where the log-klikelihood can be identified as being an estimator of a cross-entropy. Then the basic quantities are extended to estimators, leading to criteria for estimator selection, such as Akaike criterion and its extensions. Finally we investigate the use of these concepts in Bayesian theory; the cross-entropy of the predictive distribution can be used for model selection; a cross-validation estimator of this cross-entropy is found to be equivalent to the pseudo-Bayes factor.

</details>

<details>

<summary>2015-11-04 13:38:12 - SLOPE - Adaptive variable selection via convex optimization</summary>

- *Małgorzata Bogdan, Ewout van den Berg, Chiara Sabatti, Weijie Su, Emmanuel J. Candès*

- `1407.3824v2` - [abs](http://arxiv.org/abs/1407.3824v2) - [pdf](http://arxiv.org/pdf/1407.3824v2)

> We introduce a new estimator for the vector of coefficients $\beta$ in the linear model $y=X\beta+z$, where $X$ has dimensions $n\times p$ with $p$ possibly larger than $n$. SLOPE, short for Sorted L-One Penalized Estimation, is the solution to \[\min_{b\in\mathbb{R}^p}\frac{1}{2}\Vert y-Xb\Vert _{\ell_2}^2+\lambda_1\vert b\vert _{(1)}+\lambda_2\vert b\vert_{(2)}+\cdots+\lambda_p\vert b\vert_{(p)},\] where $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_p\ge0$ and $\vert b\vert_{(1)}\ge\vert b\vert_{(2)}\ge\cdots\ge\vert b\vert_{(p)}$ are the decreasing absolute values of the entries of $b$. This is a convex program and we demonstrate a solution algorithm whose computational complexity is roughly comparable to that of classical $\ell_1$ procedures such as the Lasso. Here, the regularizer is a sorted $\ell_1$ norm, which penalizes the regression coefficients according to their rank: the higher the rank - that is, stronger the signal - the larger the penalty. This is similar to the Benjamini and Hochberg [J. Roy. Statist. Soc. Ser. B 57 (1995) 289-300] procedure (BH) which compares more significant $p$-values with more stringent thresholds. One notable choice of the sequence $\{\lambda_i\}$ is given by the BH critical values $\lambda_{\mathrm {BH}}(i)=z(1-i\cdot q/2p)$, where $q\in(0,1)$ and $z(\alpha)$ is the quantile of a standard normal distribution. SLOPE aims to provide finite sample guarantees on the selected model; of special interest is the false discovery rate (FDR), defined as the expected proportion of irrelevant regressors among all selected predictors. Under orthogonal designs, SLOPE with $\lambda_{\mathrm{BH}}$ provably controls FDR at level $q$. Moreover, it also appears to have appreciable inferential properties under more general designs $X$ while having substantial power, as demonstrated in a series of experiments running on both simulated and real data.

</details>

<details>

<summary>2015-11-04 22:50:41 - Regularization and Bayesian Learning in Dynamical Systems: Past, Present and Future</summary>

- *A. Chiuso*

- `1511.01543v1` - [abs](http://arxiv.org/abs/1511.01543v1) - [pdf](http://arxiv.org/pdf/1511.01543v1)

> Regularization and Bayesian methods for system identification have been repopularized in the recent years, and proved to be competitive w.r.t. classical parametric approaches. In this paper we shall make an attempt to illustrate how the use of regularization in system identification has evolved over the years, starting from the early contributions both in the Automatic Control as well as Econometrics and Statistics literature. In particular we shall discuss some fundamental issues such as compound estimation problems and exchangeability which play and important role in regularization and Bayesian approaches, as also illustrated in early publications in Statistics. The historical and foundational issues will be given more emphasis (and space), at the expense of the more recent developments which are only briefly discussed. The main reason for such a choice is that, while the recent literature is readily available, and surveys have already been published on the subject, in the author's opinion a clear link with past work had not been completely clarified.

</details>

<details>

<summary>2015-11-04 23:21:27 - A Bayesian Consistent Dual Ensemble Kalman Filter for State-Parameter Estimation in Subsurface Hydrology</summary>

- *Boujemaa Ait-El-Fquih, Mohamad El Gharamti, Ibrahim Hoteit*

- `1511.02178v1` - [abs](http://arxiv.org/abs/1511.02178v1) - [pdf](http://arxiv.org/pdf/1511.02178v1)

> Ensemble Kalman filtering (EnKF) is an efficient approach to addressing uncertainties in subsurface groundwater models. The EnKF sequentially integrates field data into simulation models to obtain a better characterization of the model's state and parameters. These are generally estimated following joint and dual filtering strategies, in which, at each assimilation cycle, a forecast step by the model is followed by an update step with incoming observations. The Joint-EnKF directly updates the augmented state-parameter vector while the Dual-EnKF employs two separate filters, first estimating the parameters and then estimating the state based on the updated parameters. In this paper, we reverse the order of the forecast-update steps following the one-step-ahead (OSA) smoothing formulation of the Bayesian filtering problem, based on which we propose a new dual EnKF scheme, the Dual-EnKF$_{\rm OSA}$. Compared to the Dual-EnKF, this introduces a new update step to the state in a fully consistent Bayesian framework, which is shown to enhance the performance of the dual filtering approach without any significant increase in the computational cost. Numerical experiments are conducted with a two-dimensional synthetic groundwater aquifer model to assess the performance and robustness of the proposed Dual-EnKF$_{\rm OSA}$, and to evaluate its results against those of the Joint- and Dual-EnKFs. The proposed scheme is able to successfully recover both the hydraulic head and the aquifer conductivity, further providing reliable estimates of their uncertainties. Compared with the standard Joint- and Dual-EnKFs, the proposed scheme is found more robust to different assimilation settings, such as the spatial and temporal distribution of the observations, and the level of noise in the data. Based on our experimental setups, it yields up to 25% more accurate state and parameters estimates.

</details>

<details>

<summary>2015-11-05 07:45:18 - A Bayesian spatiotemporal model for reconstructing climate from multiple pollen records</summary>

- *Lasse Holmström, Liisa Ilvonen, Heikki Seppä, Siim Veski*

- `1511.01639v1` - [abs](http://arxiv.org/abs/1511.01639v1) - [pdf](http://arxiv.org/pdf/1511.01639v1)

> Holocene (the last 12,000 years) temperature variation, including the transition out of the last Ice Age to a warmer climate, is reconstructed at multiple locations in southern Finland, Sweden and Estonia based on pollen fossil data from lake sediment cores. A novel Bayesian statistical approach is proposed that allows the reconstructed temperature histories to interact through shared environmental response parameters and spatial dependence. The prior distribution for past temperatures is partially based on numerical climate simulation. The features in the reconstructions are consistent with the quantitative climate reconstructions based on more commonly used reconstruction techniques. The results suggest that the novel spatio-temporal approach can provide quantitative reconstructions that are smoother, less uncertain and generally more realistic than the site-specific individual reconstructions.

</details>

<details>

<summary>2015-11-05 07:50:14 - Quantile regression for mixed models with an application to examine blood pressure trends in China</summary>

- *Luke B. Smith, Montserrat Fuentes, Penny Gordon-Larsen, Brian J. Reich*

- `1511.01641v1` - [abs](http://arxiv.org/abs/1511.01641v1) - [pdf](http://arxiv.org/pdf/1511.01641v1)

> Cardiometabolic diseases have substantially increased in China in the past 20 years and blood pressure is a primary modifiable risk factor. Using data from the China Health and Nutrition Survey, we examine blood pressure trends in China from 1991 to 2009, with a concentration on age cohorts and urbanicity. Very large values of blood pressure are of interest, so we model the conditional quantile functions of systolic and diastolic blood pressure. This allows the covariate effects in the middle of the distribution to vary from those in the upper tail, the focal point of our analysis. We join the distributions of systolic and diastolic blood pressure using a copula, which permits the relationships between the covariates and the two responses to share information and enables probabilistic statements about systolic and diastolic blood pressure jointly. Our copula maintains the marginal distributions of the group quantile effects while accounting for within-subject dependence, enabling inference at the population and subject levels. Our population-level regression effects change across quantile level, year and blood pressure type, providing a rich environment for inference. To our knowledge, this is the first quantile function model to explicitly model within-subject autocorrelation and is the first quantile function approach that simultaneously models multivariate conditional response. We find that the association between high blood pressure and living in an urban area has evolved from positive to negative, with the strongest changes occurring in the upper tail. The increase in urbanization over the last twenty years coupled with the transition from the positive association between urbanization and blood pressure in earlier years to a more uniform association with urbanization suggests increasing blood pressure over time throughout China, even in less urbanized areas. Our methods are available in the R package BSquare.

</details>

<details>

<summary>2015-11-05 08:01:05 - Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model</summary>

- *Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, David Madigan*

- `1511.01644v1` - [abs](http://arxiv.org/abs/1511.01644v1) - [pdf](http://arxiv.org/pdf/1511.01644v1)

> We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if...then... statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS$_2$ score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS$_2$, but more accurate.

</details>

<details>

<summary>2015-11-05 08:54:13 - A Bayesian approach to the evaluation of risk-based microbiological criteria for \uppercaseCampylobacter in broiler meat</summary>

- *Jukka Ranta, Roland Lindqvist, Ingrid Hansson, Pirkko Tuominen, Maarten Nauta*

- `1511.01654v1` - [abs](http://arxiv.org/abs/1511.01654v1) - [pdf](http://arxiv.org/pdf/1511.01654v1)

> Shifting from traditional hazard-based food safety management toward risk-based management requires statistical methods for evaluating intermediate targets in food production, such as microbiological criteria (MC), in terms of their effects on human risk of illness. A fully risk-based evaluation of MC involves several uncertainties that are related to both the underlying Quantitative Microbiological Risk Assessment (QMRA) model and the production-specific sample data on the prevalence and concentrations of microbes in production batches. We used Bayesian modeling for statistical inference and evidence synthesis of two sample data sets. Thus, parameter uncertainty was represented by a joint posterior distribution, which we then used to predict the risk and to evaluate the criteria for acceptance of production batches. We also applied the Bayesian model to compare alternative criteria, accounting for the statistical uncertainty of parameters, conditional on the data sets. Comparison of the posterior mean relative risk, $E(\mathit{RR}|\mathrm{data})=E(P(\mathrm{illness}|\mathrm{criterion is met})/P(\mathrm{illness})|\mathrm{data})$, and relative posterior risk, $\mathit{RPR}=P(\mathrm{illness}|\mathrm{data, criterion is met})/P(\mathrm{illness}|\mathrm{data})$, showed very similar results, but computing is more efficient for RPR. Based on the sample data, together with the QMRA model, one could achieve a relative risk of 0.4 by insisting that the default criterion be fulfilled for acceptance of each batch.

</details>

<details>

<summary>2015-11-06 15:01:50 - Efficient parametric inference for stochastic biological systems with measured variability</summary>

- *Iain G. Johnston*

- `1403.8057v2` - [abs](http://arxiv.org/abs/1403.8057v2) - [pdf](http://arxiv.org/pdf/1403.8057v2)

> Stochastic systems in biology often exhibit substantial variability within and between cells. This variability, as well as having dramatic functional consequences, provides information about the underlying details of the system's behaviour. It is often desirable to infer properties of the parameters governing such systems given experimental observations of the mean and variance of observed quantities. In some circumstances, analytic forms for the likelihood of these observations allow very efficient inference: we present these forms and demonstrate their usage. When likelihood functions are unavailable or difficult to calculate, we show that an implementation of approximate Bayesian computation (ABC) is a powerful tool for parametric inference in these systems. However, the calculations required to apply ABC to these systems can also be computationally expensive, relying on repeated stochastic simulations. We propose an ABC approach that cheaply eliminates unimportant regions of parameter space, by addressing computationally simple mean behaviour before explicitly simulating the more computationally demanding variance behaviour. We show that this approach leads to a substantial increase in speed when applied to synthetic and experimental datasets.

</details>

<details>

<summary>2015-11-06 23:51:30 - Bayesian Dark Knowledge</summary>

- *Anoop Korattikara, Vivek Rathod, Kevin Murphy, Max Welling*

- `1506.04416v3` - [abs](http://arxiv.org/abs/1506.04416v3) - [pdf](http://arxiv.org/pdf/1506.04416v3)

> We consider the problem of Bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/ or where we need accurate posterior predictive densities, e.g., for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time).   We describe a method for "distilling" a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [Hernandez-Lobato and Adams, 2015] and an approach based on variational Bayes [Blundell et al., 2015]. Our method performs better than both of these, is much simpler to implement, and uses less computation at test time.

</details>

<details>

<summary>2015-11-07 07:00:54 - Distributed Detection via Bayesian Updates and Consensus</summary>

- *Qipeng Liu, Jiuhua Zhao, Xiaofan Wang*

- `1412.6049v2` - [abs](http://arxiv.org/abs/1412.6049v2) - [pdf](http://arxiv.org/pdf/1412.6049v2)

> In this paper, we discuss a class of distributed detection algorithms which can be viewed as implementations of Bayes' law in distributed settings. Some of the algorithms are proposed in the literature most recently, and others are first developed in this paper. The common feature of these algorithms is that they all combine (i) certain kinds of consensus protocols with (ii) Bayesian updates. They are different mainly in the aspect of the type of consensus protocol and the order of the two operations. After discussing their similarities and differences, we compare these distributed algorithms by numerical examples. We focus on the rate at which these algorithms detect the underlying true state of an object. We find that (a) The algorithms with consensus via geometric average is more efficient than that via arithmetic average; (b) The order of consensus aggregation and Bayesian update does not apparently influence the performance of the algorithms; (c) The existence of communication delay dramatically slows down the rate of convergence; (d) More communication between agents with different signal structures improves the rate of convergence.

</details>

<details>

<summary>2015-11-09 01:54:07 - Estimation for bivariate quantile varying coefficient model</summary>

- *Linglong Kong, Haoxu Shu, Giseon Heo, Qianchuan Chad He*

- `1511.02552v1` - [abs](http://arxiv.org/abs/1511.02552v1) - [pdf](http://arxiv.org/pdf/1511.02552v1)

> We propose a bivariate quantile regression method for the bivariate varying coefficient model through a directional approach. The varying coefficients are approximated by the B-spline basis and an $L_{2}$ type penalty is imposed to achieve desired smoothness. We develop a multistage estimation procedure based the Propagation-Separation~(PS) approach to borrow information from nearby directions. The PS method is capable of handling the computational complexity raised by simultaneously considering multiple directions to efficiently estimate varying coefficients while guaranteeing certain smoothness along directions. We reformulate the optimization problem and solve it by the Alternating Direction Method of Multipliers~(ADMM), which is implemented using R while the core is written in C to speed it up. Simulation studies are conducted to confirm the finite sample performance of our proposed method. A real data on Diffusion Tensor Imaging~(DTI) properties from a clinical study on neurodevelopment is analyzed.

</details>

<details>

<summary>2015-11-09 11:45:38 - Approximate methods for dynamic ecological models</summary>

- *Matteo Fasiolo, Simon N. Wood*

- `1511.02644v1` - [abs](http://arxiv.org/abs/1511.02644v1) - [pdf](http://arxiv.org/pdf/1511.02644v1)

> This document is due to appear as a chapter of the forthcoming Handbook of Approximate Bayesian Computation (ABC) by S. Sisson, L. Fan, and M. Beaumont. Here we describe some of the circumstances under which statistical ecologists might benefit from using methods that base statistical inference on a set of summary statistics, rather than on the full data. We focus particularly on one such approach, Synthetic Likelihood, and we show how this method represents an alternative to particle filters, for the purpose of fitting State Space Models of ecological interest. As an example application, we consider the prey-predator model of Turchin and Ellner (2000), and we use it to analyse the observed population dynamics of Fennoscandian voles.

</details>

<details>

<summary>2015-11-09 14:50:25 - Biologically Inspired Dynamic Textures for Probing Motion Perception</summary>

- *Jonathan Vacher, Andrew Meso, Laurent U Perrinet, Gabriel Peyré*

- `1511.02705v1` - [abs](http://arxiv.org/abs/1511.02705v1) - [pdf](http://arxiv.org/pdf/1511.02705v1)

> Perception is often described as a predictive process based on an optimal inference with respect to a generative model. We study here the principled construction of a generative model specifically crafted to probe motion perception. In that context, we first provide an axiomatic, biologically-driven derivation of the model. This model synthesizes random dynamic textures which are defined by stationary Gaussian distributions obtained by the random aggregation of warped patterns. Importantly, we show that this model can equivalently be described as a stochastic partial differential equation. Using this characterization of motion in images, it allows us to recast motion-energy models into a principled Bayesian inference framework. Finally, we apply these textures in order to psychophysically probe speed perception in humans. In this framework, while the likelihood is derived from the generative model, the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion.

</details>

<details>

<summary>2015-11-09 18:27:22 - Bayesian Inference in Cumulative Distribution Fields</summary>

- *Ricardo Silva*

- `1511.02796v1` - [abs](http://arxiv.org/abs/1511.02796v1) - [pdf](http://arxiv.org/pdf/1511.02796v1)

> One approach for constructing copula functions is by multiplication. Given that products of cumulative distribution functions (CDFs) are also CDFs, an adjustment to this multiplication will result in a copula model, as discussed by Liebscher (J Mult Analysis, 2008). Parameterizing models via products of CDFs has some advantages, both from the copula perspective (e.g., it is well-defined for any dimensionality) and from general multivariate analysis (e.g., it provides models where small dimensional marginal distributions can be easily read-off from the parameters). Independently, Huang and Frey (J Mach Learn Res, 2011) showed the connection between certain sparse graphical models and products of CDFs, as well as message-passing (dynamic programming) schemes for computing the likelihood function of such models. Such schemes allows models to be estimated with likelihood-based methods. We discuss and demonstrate MCMC approaches for estimating such models in a Bayesian context, their application in copula modeling, and how message-passing can be strongly simplified. Importantly, our view of message-passing opens up possibilities to scaling up such methods, given that even dynamic programming is not a scalable solution for calculating likelihood functions in many models.

</details>

<details>

<summary>2015-11-10 22:57:49 - A Bayesian approach to the global estimation of maternal mortality</summary>

- *Leontine Alkema, Sanqian Zhang, Doris Chou, Alison Gemmill, Ann-Beth Moller, Doris Ma Fat, Lale Say, Colin Mathers, Daniel Hogan*

- `1511.03330v1` - [abs](http://arxiv.org/abs/1511.03330v1) - [pdf](http://arxiv.org/pdf/1511.03330v1)

> The maternal mortality ratio (MMR) is defined as the number of maternal deaths in a population per 100,000 live births. Country-specific MMR estimates are published on a regular basis by the United Nations Maternal Mortality Estimation Inter-agency Group (UN MMEIG) to track progress in reducing maternal deaths and to evaluate regional and national performance related to Millennium Development Goal (MDG) 5, which calls for a 75% reduction in the MMR between 1990 and 2015.   Until 2014, the UN MMEIG used a multilevel regression model for producing estimates for countries without sufficient data from vital registration systems. While this model worked well in the past to assess MMR levels for countries with limited data, it was deemed unsatisfactory for final MDG 5 reporting for countries where longer time series of observations had become available because by construction, estimated trends in the MMR were covariate-driven only and did not necessarily track data-driven trends.   We developed a Bayesian maternal mortality estimation model, which extends upon the UN MMEIG multilevel regression model. The new model assesses data-driven trends through the inclusion of an ARIMA time series model that captures accelerations and decelerations in the rate of change in the MMR. Varying reporting and data quality issues are accounted for in source-specific data models. The revised model provides data-driven estimates of MMR levels and trends and will be used for MDG 5 reporting for all countries.

</details>

<details>

<summary>2015-11-11 07:40:48 - Training Deep Gaussian Processes using Stochastic Expectation Propagation and Probabilistic Backpropagation</summary>

- *Thang D. Bui, José Miguel Hernández-Lobato, Yingzhen Li, Daniel Hernández-Lobato, Richard E. Turner*

- `1511.03405v1` - [abs](http://arxiv.org/abs/1511.03405v1) - [pdf](http://arxiv.org/pdf/1511.03405v1)

> Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are probabilistic and non-parametric and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. The focus of this paper is scalable approximate Bayesian learning of these networks. The paper develops a novel and efficient extension of probabilistic backpropagation, a state-of-the-art method for training Bayesian neural networks, that can be used to train DGPs. The new method leverages a recently proposed method for scaling Expectation Propagation, called stochastic Expectation Propagation. The method is able to automatically discover useful input warping, expansion or compression, and it is therefore is a flexible form of Bayesian kernel design. We demonstrate the success of the new method for supervised learning on several real-world datasets, showing that it typically outperforms GP regression and is never much worse.

</details>

<details>

<summary>2015-11-11 11:51:36 - Bayesian model selection for the glacial-interglacial cycle</summary>

- *Jake Carson, Michel Crucifix, Simon Preston, Richard D. Wilkinson*

- `1511.03467v1` - [abs](http://arxiv.org/abs/1511.03467v1) - [pdf](http://arxiv.org/pdf/1511.03467v1)

> A prevailing viewpoint in palaeoclimate science is that a single palaeoclimate record contains insufficient information to discriminate between most competing explanatory models. Results we present here suggest the contrary. Using SMC^2 combined with novel Brownian bridge type proposals for the state trajectories, we show that even with relatively short time series it is possible to estimate Bayes factors to sufficient accuracy to be able to select between competing models. The results show that Monte Carlo methodology and computer power have now advanced to the point where a full Bayesian analysis for a wide class of conceptual climate models is now possible. The results also highlight a problem with estimating the chronology of the climate record prior to further statistical analysis, a practice which is common in palaeoclimate science. Using two datasets based on the same record but with different estimated chronologies results in conflicting conclusions about the importance of the orbital forcing on the glacial cycle, and about the internal dynamics generating the glacial cycle, even though the difference between the two estimated chronologies is consistent with dating uncertainty. This highlights a need for chronology estimation and other inferential questions to be addressed in a joint statistical procedure.

</details>

<details>

<summary>2015-11-11 12:26:35 - ABC for climate: dealing with expensive simulators</summary>

- *Philip B. Holden, Neil R. Edwards, James Hensman, Richard D. Wilkinson*

- `1511.03475v1` - [abs](http://arxiv.org/abs/1511.03475v1) - [pdf](http://arxiv.org/pdf/1511.03475v1)

> This paper is due to appear as a chapter of the forthcoming Handbook of Approximate Bayesian Computation (ABC) by S. Sisson, L. Fan, and M. Beaumont. We describe the challenge of calibrating climate simulators, and discuss the differences in emphasis in climate science compared to many of the more traditional ABC application areas. The primary difficulty is how to do inference with a computationally expensive simulator which we can only afford to run a small number of times, and we describe how Gaussian process emulators are used as surrogate models in this case. We introduce the idea of history matching, which is a non-probabilistic calibration method, which divides the parameter space into (not im)plausible and implausible regions. History matching can be shown to be a special case of ABC, but with a greater emphasis on defining realistic simulator discrepancy bounds, and using these to define tolerances and metrics. We describe a design approach for choosing parameter values at which to run the simulator, and illustrate the approach on a toy climate model, showing that with careful design we can find the plausible region with a very small number of model evaluations. Finally, we describe how calibrated GENIE-1 (an earth system model of intermediate complexity) predictions have been used, and why it is important to accurately characterise parametric uncertainty.

</details>

<details>

<summary>2015-11-11 17:24:03 - Bayesian group latent factor analysis with structured sparsity</summary>

- *Shiwen Zhao, Chuan Gao, Sayan Mukherjee, Barbara E Engelhardt*

- `1411.2698v2` - [abs](http://arxiv.org/abs/1411.2698v2) - [pdf](http://arxiv.org/pdf/1411.2698v2)

> Latent factor models are the canonical statistical tool for exploratory analyses of low-dimensional linear structure for an observation matrix with p features across n samples. We develop a structured Bayesian group factor analysis model that extends the factor model to multiple coupled observation matrices; in the case of two observations, this reduces to a Bayesian model of canonical correlation analysis. The main contribution of this work is to carefully define a structured Bayesian prior that encourages both element-wise and column-wise shrinkage and leads to desirable behavior on high-dimensional data. In particular, our model puts a structured prior on the joint factor loading matrix, regularizing at three levels, which enables element-wise sparsity and unsupervised recovery of latent factors corresponding to structured variance across arbitrary subsets of the observations. In addition, our structured prior allows for both dense and sparse latent factors so that covariation among either all features or only a subset of features can both be recovered. We use fast parameter-expanded expectation-maximization for parameter estimation in this model. We validate our method on both simulated data with substantial structure and real data, comparing against a number of state-of-the-art approaches. These results illustrate useful properties of our model, including i) recovering sparse signal in the presence of dense effects; ii) the ability to scale naturally to large numbers of observations; iii) flexible observation- and factor-specific regularization to recover factors with a wide variety of sparsity levels and percentage of variance explained; and iv) tractable inference that scales to modern genomic and document data sizes.

</details>

<details>

<summary>2015-11-12 16:26:13 - Bayesian Analysis of Dynamic Linear Topic Models</summary>

- *Chris Glynn, Surya T. Tokdar, David L. Banks, Brian Howard*

- `1511.03947v1` - [abs](http://arxiv.org/abs/1511.03947v1) - [pdf](http://arxiv.org/pdf/1511.03947v1)

> In dynamic topic modeling, the proportional contribution of a topic to a document depends on the temporal dynamics of that topic's overall prevalence in the corpus. We extend the Dynamic Topic Model of Blei and Lafferty (2006) by explicitly modeling document level topic proportions with covariates and dynamic structure that includes polynomial trends and periodicity. A Markov Chain Monte Carlo (MCMC) algorithm that utilizes Polya-Gamma data augmentation is developed for posterior inference. Conditional independencies in the model and sampling are made explicit, and our MCMC algorithm is parallelized where possible to allow for inference in large corpora. To address computational bottlenecks associated with Polya-Gamma sampling, we appeal to the Central Limit Theorem to develop a Gaussian approximation to the Polya-Gamma random variable. This approximation is fast and reliable for parameter values relevant in the text mining domain. Our model and inference algorithm are validated with multiple simulation examples, and we consider the application of modeling trends in PubMed abstracts. We demonstrate that sharing information across documents is critical for accurately estimating document-specific topic proportions. We also show that explicitly modeling polynomial and periodic behavior improves our ability to predict topic prevalence at future time points.

</details>

<details>

<summary>2015-11-12 17:54:46 - Automatic Inference of the Quantile Parameter</summary>

- *Karthikeyan Natesan Ramamurthy, Aleksandr Y. Aravkin, Jayaraman J. Thiagarajan*

- `1511.03990v1` - [abs](http://arxiv.org/abs/1511.03990v1) - [pdf](http://arxiv.org/pdf/1511.03990v1)

> Supervised learning is an active research area, with numerous applications in diverse fields such as data analytics, computer vision, speech and audio processing, and image understanding. In most cases, the loss functions used in machine learning assume symmetric noise models, and seek to estimate the unknown function parameters. However, loss functions such as quantile and quantile Huber generalize the symmetric $\ell_1$ and Huber losses to the asymmetric setting, for a fixed quantile parameter. In this paper, we propose to jointly infer the quantile parameter and the unknown function parameters, for the asymmetric quantile Huber and quantile losses. We explore various properties of the quantile Huber loss and implement a convexity certificate that can be used to check convexity in the quantile parameter. When the loss if convex with respect to the parameter of the function, we prove that it is biconvex in both the function and the quantile parameters, and propose an algorithm to jointly estimate these. Results with synthetic and real data demonstrate that the proposed approach can automatically recover the quantile parameter corresponding to the noise and also provide an improved recovery of function parameters. To illustrate the potential of the framework, we extend the gradient boosting machines with quantile losses to automatically estimate the quantile parameter at each iteration.

</details>

<details>

<summary>2015-11-13 11:41:08 - Predictive Characterization of Mixtures of Markov Chains</summary>

- *Sandra Fortini, Sonia Petrone*

- `1406.5421v3` - [abs](http://arxiv.org/abs/1406.5421v3) - [pdf](http://arxiv.org/pdf/1406.5421v3)

> Predictive constructions are a powerful way of characterizing the probability law of stochastic processes with certain forms of invariance, such as exchangeability or Markov exchangeability. When de Finetti-like representation theorems are available, the predictive characterization implicitly defines the prior distribution, starting from assumptions on the observables; moreover, it often helps designing efficient computational strategies. In this paper we give necessary and sufficient conditions on the sequence of predictive distributions such that they characterize a Markov exchangeable probability law for a discrete valued process X. Under recurrence, Markov exchangeable processes are mixtures of Markov chains. Thus, our results help checking when a predictive scheme characterizes a prior for Bayesian inference on the unknown transition matrix of a Markov chain. Our predictive conditions are in some sense minimal sufficient conditions for Markov exchangeability; we also provide predictive conditions for recurrence. We illustrate their application in relevant examples from the literature and in novel constructions.

</details>

<details>

<summary>2015-11-15 18:11:44 - Model Space Priors for Objective Sparse Bayesian Regression</summary>

- *Andrew J Womack, Claudio Fuentes, Daniel Taylor-Rodriguez*

- `1511.04745v1` - [abs](http://arxiv.org/abs/1511.04745v1) - [pdf](http://arxiv.org/pdf/1511.04745v1)

> This paper investigates the construction of model space priors from an alternative point of view to the usual indicators for inclusion of covariates in a given model. Assumptions about indicator variables often lead to Beta-Binomial priors on the model space, which do not appropriately penalize for model complexity when the parameters are fixed. This can be alleviated by changing the parameters of the prior to depend on the number of covariates, though justification for this is lacking from a first-principles point of view. We propose viewing the model space as a partially ordered set. When the number of covariates increases, an isometry argument leads to the Poisson distribution as the unique, natural limiting prior over model dimension. This limiting prior is derived using two constructions that view an individual model as though it is a "local" null hypothesis and compares its prior probability to the probability of the alternatives that nest it. We show that this prior induces a posterior that concentrates on a finite true model asymptotically. Additionally, we provide a deterministic algorithm that takes advantage of the nature of the prior and explores good models in polynomial time.

</details>

<details>

<summary>2015-11-16 01:02:29 - Bayesian Particle Tracking of Traffic Flows</summary>

- *Nicholas Polson, Vadim Sokolov*

- `1411.5076v3` - [abs](http://arxiv.org/abs/1411.5076v3) - [pdf](http://arxiv.org/pdf/1411.5076v3)

> We develop a Bayesian particle filter for tracking traffic flows that is capable of capturing non-linearities and discontinuities present in flow dynamics. Our model includes a hidden state variable that captures sudden regime shifts between traffic free flow, breakdown and recovery. We develop an efficient particle learning algorithm for real time on-line inference of states and parameters. This requires a two step approach, first, resampling the current particles, with a mixture predictive distribution and second, propagation of states using the conditional posterior distribution. Particle learning of parameters follows from updating recursions for conditional sufficient statistics. To illustrate our methodology, we analyze measurements of daily traffic flow from the Illinois interstate I-55 highway system. We demonstrate how our filter can be used to inference the change of traffic flow regime on a highway road segment based on a measurement from freeway single-loop detectors. Finally, we conclude with directions for future research.

</details>

<details>

<summary>2015-11-16 09:22:01 - Active Contextual Entropy Search</summary>

- *Jan Hendrik Metzen*

- `1511.04211v2` - [abs](http://arxiv.org/abs/1511.04211v2) - [pdf](http://arxiv.org/pdf/1511.04211v2)

> Contextual policy search allows adapting robotic movement primitives to different situations. For instance, a locomotion primitive might be adapted to different terrain inclinations or desired walking speeds. Such an adaptation is often achievable by modifying a small number of hyperparameters. However, learning, when performed on real robotic systems, is typically restricted to a small number of trials. Bayesian optimization has recently been proposed as a sample-efficient means for contextual policy search that is well suited under these conditions. In this work, we extend entropy search, a variant of Bayesian optimization, such that it can be used for active contextual policy search where the agent selects those tasks during training in which it expects to learn the most. Empirical results in simulation suggest that this allows learning successful behavior with less trials.

</details>

<details>

<summary>2015-11-16 10:50:40 - Simulating Posterior Distributions for Zero-Inflated Automobile Insurance Data</summary>

- *J. M. Pérez-Sánchez, E. Gómez-Déniz*

- `1606.00361v1` - [abs](http://arxiv.org/abs/1606.00361v1) - [pdf](http://arxiv.org/pdf/1606.00361v1)

> Generalized linear models (GLMs) using a regression procedure to fit relationships between predictor and target variables are widely used in automobile insurance data. Here, in the process of ratemaking and in order to compute the premiums to be charged to the policy--holders it is crucial to detect the relevant variables which affect to the value of the premium since in this case the insurer could eventually fix more precisely the premiums. We propose here a methodology with a different perspective. Instead of the exponential family we pay attention to the Power Series Distributions and develop a Bayesian methodology using sampling--based methods in order to detect relevant variables in automobile insurance data set. This model, as the GLMs, allows to incorporate the presence of an excessive number of zero counts and overdispersion phenomena (variance larger than the mean). Following this spirit, in this paper we present a novel and flexible zero--inflated Bayesian regression model. This model includes other familiar models such as the zero--inflated Poisson and zero--inflated geometric models, as special cases. A Bayesian estimation method is developed as an alternative to traditionally used maximum likelihood based methods to analyze such data. For a real data collected from 2004 to 2005 in an Australian insurance company an example is provided by using Markov Chain Monte Carlo method which is developed in WinBUGS package. The results show that the new Bayesian method performs the previous models.

</details>

<details>

<summary>2015-11-16 22:27:13 - Bayesian Inference for Latent Biologic Structure with Determinantal Point Processes (DPP)</summary>

- *Yanxun Xu, Peter Mueller, Donatello Telesca*

- `1506.08253v2` - [abs](http://arxiv.org/abs/1506.08253v2) - [pdf](http://arxiv.org/pdf/1506.08253v2)

> We discuss the use of the determinantal point process (DPP) as a prior for latent structure in biomedical applications, where inference often centers on the interpretation of latent features as biologically or clinically meaningful structure. Typical examples include mixture models, when the terms of the mixture are meant to represent clinically meaningful subpopulations (of patients, genes, etc.). Another class of examples are feature allocation models. We propose the DPP prior as a repulsive prior on latent mixture components in the first example, and as prior on feature-specific parameters in the second case. We argue that the DPP is in general an attractive prior model for latent structure when biologically relevant interpretation of such structure is desired. We illustrate the advantages of DPP prior in three case studies, including inference in mixture models for magnetic resonance images (MRI) and for protein expression, and a feature allocation model for gene expression using data from The Cancer Genome Atlas. An important part of our argument are efficient and straightforward posterior simulation methods. We implement a variation of reversible jump Markov chain Monte Carlo simulation for inference under the DPP prior, using a density with respect to the unit rate Poisson process.

</details>

<details>

<summary>2015-11-17 09:17:06 - Adaptive empirical Bayesian smoothing splines</summary>

- *Paulo Serra, Tatyana Krivobokova*

- `1411.6860v2` - [abs](http://arxiv.org/abs/1411.6860v2) - [pdf](http://arxiv.org/pdf/1411.6860v2)

> In this paper we develop and study adaptive empirical Bayesian smoothing splines. These are smoothing splines with both smoothing parameter and penalty order determined via the empirical Bayes method from the marginal likelihood of the model. The selected order and smoothing parameter are used to construct adaptive credible sets with good frequentist coverage for the underlying regression function. We use these credible sets as a proxy to show the superior performance of adaptive empirical Bayesian smoothing splines compared to frequentist smoothing splines.

</details>

<details>

<summary>2015-11-17 11:52:34 - Inferring constructs of effective teaching from classroom observations: An application of Bayesian exploratory factor analysis without restrictions</summary>

- *J. R. Lockwood, Terrance D. Savitsky, Daniel F. McCaffrey*

- `1511.05360v1` - [abs](http://arxiv.org/abs/1511.05360v1) - [pdf](http://arxiv.org/pdf/1511.05360v1)

> Ratings of teachers' instructional practices using standardized classroom observation instruments are increasingly being used for both research and teacher accountability. There are multiple instruments in use, each attempting to evaluate many dimensions of teaching and classroom activities, and little is known about what underlying teaching quality attributes are being measured. We use data from multiple instruments collected from 458 middle school mathematics and English language arts teachers to inform research and practice on teacher performance measurement by modeling latent constructs of high-quality teaching. We make inferences about these constructs using a novel approach to Bayesian exploratory factor analysis (EFA) that, unlike commonly used approaches for identifying factor loadings in Bayesian EFA, is invariant to how the data dimensions are ordered. Applying this approach to ratings of lessons reveals two distinct teaching constructs in both mathematics and English language arts: (1) quality of instructional practices; and (2) quality of teacher management of classrooms. We demonstrate the relationships of these constructs to other indicators of teaching quality, including teacher content knowledge and student performance on standardized tests.

</details>

<details>

<summary>2015-11-17 12:20:14 - The Gibbs-plaid biclustering model</summary>

- *Thierry Chekouo, Alejandro Murua, Wolfgang Raffelsberger*

- `1511.05375v1` - [abs](http://arxiv.org/abs/1511.05375v1) - [pdf](http://arxiv.org/pdf/1511.05375v1)

> We propose and develop a Bayesian plaid model for biclustering that accounts for the prior dependency between genes (and/or conditions) through a stochastic relational graph. This work is motivated by the need for improved understanding of the molecular mechanisms of human diseases for which effective drugs are lacking, and based on the extensive raw data available through gene expression profiling. We model the prior dependency information from biological knowledge gathered from gene ontologies. Our model, the Gibbs-plaid model, assumes that the relational graph is governed by a Gibbs random field. To estimate the posterior distribution of the bicluster membership labels, we develop a stochastic algorithm that is partly based on the Wang-Landau flat-histogram algorithm. We apply our method to a gene expression database created from the study of retinal detachment, with the aim of confirming known or finding novel subnetworks of proteins associated with this disorder.

</details>

<details>

<summary>2015-11-17 13:08:10 - Bayesian Optimization with Dimension Scheduling: Application to Biological Systems</summary>

- *Doniyor Ulmasov, Caroline Baroukh, Benoit Chachuat, Marc Peter Deisenroth, Ruth Misener*

- `1511.05385v1` - [abs](http://arxiv.org/abs/1511.05385v1) - [pdf](http://arxiv.org/pdf/1511.05385v1)

> Bayesian Optimization (BO) is a data-efficient method for global black-box optimization of an expensive-to-evaluate fitness function. BO typically assumes that computation cost of BO is cheap, but experiments are time consuming or costly. In practice, this allows us to optimize ten or fewer critical parameters in up to 1,000 experiments. But experiments may be less expensive than BO methods assume: In some simulation models, we may be able to conduct multiple thousands of experiments in a few hours, and the computational burden of BO is no longer negligible compared to experimentation time. To address this challenge we introduce a new Dimension Scheduling Algorithm (DSA), which reduces the computational burden of BO for many experiments. The key idea is that DSA optimizes the fitness function only along a small set of dimensions at each iteration. This DSA strategy (1) reduces the necessary computation time, (2) finds good solutions faster than the traditional BO method, and (3) can be parallelized straightforwardly. We evaluate the DSA in the context of optimizing parameters of dynamic models of microalgae metabolism and show faster convergence than traditional BO.

</details>

<details>

<summary>2015-11-17 13:25:17 - Statistical unfolding of elementary particle spectra: Empirical Bayes estimation and bias-corrected uncertainty quantification</summary>

- *Mikael Kuusela, Victor M. Panaretos*

- `1505.04768v3` - [abs](http://arxiv.org/abs/1505.04768v3) - [pdf](http://arxiv.org/pdf/1505.04768v3)

> We consider the high energy physics unfolding problem where the goal is to estimate the spectrum of elementary particles given observations distorted by the limited resolution of a particle detector. This important statistical inverse problem arising in data analysis at the Large Hadron Collider at CERN consists in estimating the intensity function of an indirectly observed Poisson point process. Unfolding typically proceeds in two steps: one first produces a regularized point estimate of the unknown intensity and then uses the variability of this estimator to form frequentist confidence intervals that quantify the uncertainty of the solution. In this paper, we propose forming the point estimate using empirical Bayes estimation which enables a data-driven choice of the regularization strength through marginal maximum likelihood estimation. Observing that neither Bayesian credible intervals nor standard bootstrap confidence intervals succeed in achieving good frequentist coverage in this problem due to the inherent bias of the regularized point estimate, we introduce an iteratively bias-corrected bootstrap technique for constructing improved confidence intervals. We show using simulations that this enables us to achieve nearly nominal frequentist coverage with only a modest increase in interval length. The proposed methodology is applied to unfolding the $Z$ boson invariant mass spectrum as measured in the CMS experiment at the Large Hadron Collider.

</details>

<details>

<summary>2015-11-17 15:00:50 - Multivariate emulation of computer simulators: model selection and diagnostics with application to a humanitarian relief model</summary>

- *Antony Overstall, David Woods*

- `1506.04489v2` - [abs](http://arxiv.org/abs/1506.04489v2) - [pdf](http://arxiv.org/pdf/1506.04489v2)

> We present a common framework for Bayesian emulation methodologies for multivariate-output simulators, or computer models, that employ either parametric linear models or nonparametric Gaussian processes. Novel diagnostics suitable for multivariate covariance-separable emulators are developed and techniques to improve the adequacy of an emulator are discussed and implemented. A variety of emulators are compared for a humanitarian relief simulator, modelling aid missions to Sicily after a volcanic eruption and earthquake, and a sensitivity analysis is conducted to determine the sensitivity of the simulator output to changes in the input variables. The results from parametric and nonparametric emulators are compared in terms of prediction accuracy, uncertainty quantification and scientific interpretability.

</details>

<details>

<summary>2015-11-17 17:35:57 - Accelerating pseudo-marginal Metropolis-Hastings by correlating auxiliary variables</summary>

- *Johan Dahlin, Fredrik Lindsten, Joel Kronander, Thomas B. Schön*

- `1511.05483v1` - [abs](http://arxiv.org/abs/1511.05483v1) - [pdf](http://arxiv.org/pdf/1511.05483v1)

> Pseudo-marginal Metropolis-Hastings (pmMH) is a powerful method for Bayesian inference in models where the posterior distribution is analytical intractable or computationally costly to evaluate directly. It operates by introducing additional auxiliary variables into the model and form an extended target distribution, which then can be evaluated point-wise. In many cases, the standard Metropolis-Hastings is then applied to sample from the extended target and the sought posterior can be obtained by marginalisation. However, in some implementations this approach suffers from poor mixing as the auxiliary variables are sampled from an independent proposal. We propose a modification to the pmMH algorithm in which a Crank-Nicolson (CN) proposal is used instead. This results in that we introduce a positive correlation in the auxiliary variables. We investigate how to tune the CN proposal and its impact on the mixing of the resulting pmMH sampler. The conclusion is that the proposed modification can have a beneficial effect on both the mixing of the Markov chain and the computational cost for each iteration of the pmMH algorithm.

</details>

<details>

<summary>2015-11-18 03:16:27 - Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models</summary>

- *Juho Lee, Seungjin Choi*

- `1511.05650v1` - [abs](http://arxiv.org/abs/1511.05650v1) - [pdf](http://arxiv.org/pdf/1511.05650v1)

> Normalized random measures (NRMs) provide a broad class of discrete random measures that are often used as priors for Bayesian nonparametric models. Dirichlet process is a well-known example of NRMs. Most of posterior inference methods for NRM mixture models rely on MCMC methods since they are easy to implement and their convergence is well studied. However, MCMC often suffers from slow convergence when the acceptance rate is low. Tree-based inference is an alternative deterministic posterior inference method, where Bayesian hierarchical clustering (BHC) or incremental Bayesian hierarchical clustering (IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively. Although IBHC is a promising method for posterior inference for NRMM models due to its efficiency and applicability to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made. In this paper, we present a hybrid inference algorithm for NRMM models, which combines the merits of both MCMC and IBHC. Trees built by IBHC outlines partitions of data, which guides Metropolis-Hastings procedure to employ appropriate proposals. Inheriting the nature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and enjoys the fast convergence thanks to the effective proposals guided by trees. Experiments on both synthetic and real-world datasets demonstrate the benefit of our method.

</details>

<details>

<summary>2015-11-18 05:28:26 - Bayesian hypothesis testing for one bit compressed sensing with sensing matrix perturbation</summary>

- *H. Zayyani, M. Korki, F. Marvasti*

- `1511.05660v1` - [abs](http://arxiv.org/abs/1511.05660v1) - [pdf](http://arxiv.org/pdf/1511.05660v1)

> This letter proposes a low-computational Bayesian algorithm for noisy sparse recovery in the context of one bit compressed sensing with sensing matrix perturbation. The proposed algorithm which is called BHT-MLE comprises a sparse support detector and an amplitude estimator. The support detector utilizes Bayesian hypothesis test, while the amplitude estimator uses an ML estimator which is obtained by solving a convex optimization problem. Simulation results show that BHT-MLE algorithm offers more reconstruction accuracy than that of an ML estimator (MLE) at a low computational cost.

</details>

<details>

<summary>2015-11-18 10:52:17 - Stochastic Expectation Propagation</summary>

- *Yingzhen Li, Jose Miguel Hernandez-Lobato, Richard E. Turner*

- `1506.04132v2` - [abs](http://arxiv.org/abs/1506.04132v2) - [pdf](http://arxiv.org/pdf/1506.04132v2)

> Expectation propagation (EP) is a deterministic approximation algorithm that is often used to perform approximate Bayesian parameter learning. EP approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint. EP can offer analytic and computational advantages over other approximations, such as Variational Inference (VI), and is the method of choice for a number of models. The local nature of EP appears to make it an ideal candidate for performing Bayesian learning on large models in large-scale dataset settings. However, EP has a crucial limitation in this context: the number of approximating factors needs to increase with the number of data-points, N, which often entails a prohibitively large memory overhead. This paper presents an extension to EP, called stochastic expectation propagation (SEP), that maintains a global posterior approximation (like VI) but updates it in a local way (like EP). Experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that SEP performs almost as well as full EP, but reduces the memory consumption by a factor of $N$. SEP is therefore ideally suited to performing approximate Bayesian learning in the large model, large dataset setting.

</details>

<details>

<summary>2015-11-18 11:38:42 - Calibration of conditional composite likelihood for Bayesian inference on Gibbs random fields</summary>

- *Julien Stoehr, Nial Friel*

- `1502.01997v2` - [abs](http://arxiv.org/abs/1502.01997v2) - [pdf](http://arxiv.org/pdf/1502.01997v2)

> Gibbs random fields play an important role in statistics, however, the resulting likelihood is typically unavailable due to an intractable normalizing constant. Composite likelihoods offer a principled means to construct useful approximations. This paper provides a mean to calibrate the posterior distribution resulting from using a composite likelihood and illustrate its performance in several examples.

</details>

<details>

<summary>2015-11-18 15:10:19 - Factorization, Inference and Parameter Learning in Discrete AMP Chain Graphs</summary>

- *Jose M. Peña*

- `1501.06727v2` - [abs](http://arxiv.org/abs/1501.06727v2) - [pdf](http://arxiv.org/pdf/1501.06727v2)

> We address some computational issues that may hinder the use of AMP chain graphs in practice. Specifically, we show how a discrete probability distribution that satisfies all the independencies represented by an AMP chain graph factorizes according to it. We show how this factorization makes it possible to perform inference and parameter learning efficiently, by adapting existing algorithms for Markov and Bayesian networks. Finally, we turn our attention to another issue that may hinder the use of AMP CGs, namely the lack of an intuitive interpretation of their edges. We provide one such interpretation.

</details>

<details>

<summary>2015-11-18 20:11:44 - Bayesian quantile regression analysis for continuous data with a discrete component at zero</summary>

- *Bruno Santos, Heleno Bolfarine*

- `1511.05925v1` - [abs](http://arxiv.org/abs/1511.05925v1) - [pdf](http://arxiv.org/pdf/1511.05925v1)

> In this work we show a Bayesian quantile regression method to response variables with mixed discrete-continuous distribution with a point mass at zero, where these observations are believed to be left censored or true zeros. We combine the information provided by the quantile regression analysis to present a more complete description of the probability of being censored given that the observed value is equal to zero, while also studying the conditional quantiles of the continuous part. We build up an Markov Chain Monte Carlo method from related models in the literature to obtain samples from the posterior distribution. We demonstrate the suitability of the model to analyze this censoring probability with a simulated example and two applications with real data. The first is a well known dataset from the econometrics literature about women labor in Britain and the second considers the statistical analysis of expenditures with durable goods, considering information from Brazil.

</details>

<details>

<summary>2015-11-19 01:01:59 - Stochastic gradient method with accelerated stochastic dynamics</summary>

- *Masayuki Ohzeki*

- `1511.06036v1` - [abs](http://arxiv.org/abs/1511.06036v1) - [pdf](http://arxiv.org/pdf/1511.06036v1)

> In this paper, we propose a novel technique to implement stochastic gradient methods, which are beneficial for learning from large datasets, through accelerated stochastic dynamics. A stochastic gradient method is based on mini-batch learning for reducing the computational cost when the amount of data is large. The stochasticity of the gradient can be mitigated by the injection of Gaussian noise, which yields the stochastic Langevin gradient method; this method can be used for Bayesian posterior sampling. However, the performance of the stochastic Langevin gradient method depends on the mixing rate of the stochastic dynamics. In this study, we propose violating the detailed balance condition to enhance the mixing rate. Recent studies have revealed that violating the detailed balance condition accelerates the convergence to a stationary state and reduces the correlation time between the samplings. We implement this violation of the detailed balance condition in the stochastic gradient Langevin method and test our method for a simple model to demonstrate its performance.

</details>

<details>

<summary>2015-11-19 14:57:49 - Uniform Correlation Mixture of Bivariate Normal Distributions and Hypercubically-contoured Densities That Are Marginally Normal</summary>

- *Kai Zhang, Lawrence D. Brown, Edward George, Linda Zhao*

- `1511.06190v1` - [abs](http://arxiv.org/abs/1511.06190v1) - [pdf](http://arxiv.org/pdf/1511.06190v1)

> The bivariate normal density with unit variance and correlation $\rho$ is well-known. We show that by integrating out $\rho$, the result is a function of the maximum norm. The Bayesian interpretation of this result is that if we put a uniform prior over $\rho$, then the marginal bivariate density depends only on the maximal magnitude of the variables. The square-shaped isodensity contour of this resulting marginal bivariate density can also be regarded as the equally-weighted mixture of bivariate normal distributions over all possible correlation coefficients. This density links to the Khintchine mixture method of generating random variables. We use this method to construct the higher dimensional generalizations of this distribution. We further show that for each dimension, there is a unique multivariate density that is a differentiable function of the maximum norm and is marginally normal, and the bivariate density from the integral over $\rho$ is its special case in two dimensions.

</details>

<details>

<summary>2015-11-19 17:14:34 - PAC-Bayesian bounds for Principal Component Analysis in Hilbert spaces</summary>

- *Ilaria Giulini*

- `1511.06263v1` - [abs](http://arxiv.org/abs/1511.06263v1) - [pdf](http://arxiv.org/pdf/1511.06263v1)

> Based on some new robust estimators of the covariance matrix, we propose stable versions of Principal Component Analysis (PCA) and we qualify it independently of the dimension of the ambient space. We first provide a robust estimator of the orthogonal projector on the largest eigenvectors of the covariance matrix. The behavior of such an estimator is related to the size of the gap in the spectrum of the covariance matrix and in particular a large gap is needed in order to get a good approximation. To avoid the assumption of a large eigengap in the spectrum of the covariance matrix we propose a robust version of PCA that consists in performing a smooth cut-off of the spectrum via a Lipschitz function. We provide bounds on the approximation error in terms of the operator norm and of the Frobenius norm.

</details>

<details>

<summary>2015-11-19 22:08:22 - Fast Parallel SAME Gibbs Sampling on General Discrete Bayesian Networks</summary>

- *Daniel Seita, Haoyu Chen, John Canny*

- `1511.06416v1` - [abs](http://arxiv.org/abs/1511.06416v1) - [pdf](http://arxiv.org/pdf/1511.06416v1)

> A fundamental task in machine learning and related fields is to perform inference on Bayesian networks. Since exact inference takes exponential time in general, a variety of approximate methods are used. Gibbs sampling is one of the most accurate approaches and provides unbiased samples from the posterior but it has historically been too expensive for large models. In this paper, we present an optimized, parallel Gibbs sampler augmented with state replication (SAME or State Augmented Marginal Estimation) to decrease convergence time. We find that SAME can improve the quality of parameter estimates while accelerating convergence. Experiments on both synthetic and real data show that our Gibbs sampler is substantially faster than the state of the art sampler, JAGS, without sacrificing accuracy. Our ultimate objective is to introduce the Gibbs sampler to researchers in many fields to expand their range of feasible inference problems.

</details>

<details>

<summary>2015-11-19 22:40:30 - Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models</summary>

- *Bradly C. Stadie, Sergey Levine, Pieter Abbeel*

- `1507.00814v3` - [abs](http://arxiv.org/abs/1507.00814v3) - [pdf](http://arxiv.org/pdf/1507.00814v3)

> Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.

</details>

<details>

<summary>2015-11-20 20:43:43 - Bayesian SPLDA</summary>

- *Jesús Villalba*

- `1511.07318v1` - [abs](http://arxiv.org/abs/1511.07318v1) - [pdf](http://arxiv.org/pdf/1511.07318v1)

> In this document we are going to derive the equations needed to implement a Variational Bayes estimation of the parameters of the simplified probabilistic linear discriminant analysis (SPLDA) model. This can be used to adapt SPLDA from one database to another with few development data or to implement the fully Bayesian recipe. Our approach is similar to Bishop's VB PPCA.

</details>

<details>

<summary>2015-11-20 21:25:59 - Unsupervised Adaptation of SPLDA</summary>

- *Jesús Villalba*

- `1511.07421v1` - [abs](http://arxiv.org/abs/1511.07421v1) - [pdf](http://arxiv.org/pdf/1511.07421v1)

> State-of-the-art speaker recognition relays on models that need a large amount of training data. This models are successful in tasks like NIST SRE because there is sufficient data available. However, in real applications, we usually do not have so much data and, in many cases, the speaker labels are unknown. We present a method to adapt a PLDA model from a domain with a large amount of labeled data to another with unlabeled data. We describe a generative model that produces both sets of data where the unknown labels are modeled like latent variables. We used variational Bayes to estimate the hidden variables. Here, we derive the equations for this model. This model has been used in the papers: "UNSUPERVISED ADAPTATION OF PLDA BY USING VARIATIONAL BAYES METHODS" publised at ICASSP 2014, "Unsupervised Training of PLDA with Variational Bayes" published at Iberspeech 2014, and "VARIATIONAL BAYESIAN PLDA FOR SPEAKER DIARIZATION IN THE MGB CHALLENGE" published at ASRU 2015.

</details>

<details>

<summary>2015-11-21 07:30:56 - Practical survival analysis tools for heterogeneous cohorts and informative censoring</summary>

- *M. Rowley, H. Garmo, M. Van Hemelrijck, W. Wulaningsih, B. Grundmark, B. Zethelius, N. Hammar, G. Walldius, M. Inoue, L. Holmberg, A. C. C. Coolen*

- `1511.06849v1` - [abs](http://arxiv.org/abs/1511.06849v1) - [pdf](http://arxiv.org/pdf/1511.06849v1)

> In heterogeneous cohorts and those where censoring by non-primary risks is informative many conventional survival analysis methods are not applicable; the proportional hazards assumption is usually violated at population level and the observed crude hazard rates are no longer estimators of what they would have been in the absence of other risks. In this paper, we develop a fully Bayesian survival analysis to determine the probabilistically optimal description of a heterogeneous cohort and we propose a novel means of recovering hazard rates and survival functions `decontaminated' of the effects of any competing risks. Most competing risks studies implicitly assume that risk correlations are induced by cohort or disease heterogeneity that is not captured by covariates. We additionally assume that proportional hazards hold at the level of individuals, for all risks, leading to a generic statistical description that allows us to decontaminate the effects of informative censoring, and from which Cox regression, frailty and random effects models, and latent class models can all be recovered as special cases. Synthetic data confirm that our approach can map a cohort's substructure, and remove heterogeneity-induced false protectivity and false aetiology effects. Application to survival data from the ULSAM cohort leads to plausible alternative explanations for previous counter-intuitive inferences to prostate cancer. The importance of managing cardiovascular disease as a comorbidity in women diagnosed with breast cancer is suggested on application to survival data from the AMORIS study.

</details>

<details>

<summary>2015-11-21 14:57:48 - Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond</summary>

- *Chun Kai Ling, Kian Hsiang Low, Patrick Jaillet*

- `1511.06890v1` - [abs](http://arxiv.org/abs/1511.06890v1) - [pdf](http://arxiv.org/pdf/1511.06890v1)

> This paper presents a novel nonmyopic adaptive Gaussian process planning (GPP) framework endowed with a general class of Lipschitz continuous reward functions that can unify some active learning/sensing and Bayesian optimization criteria and offer practitioners some flexibility to specify their desired choices for defining new tasks/problems. In particular, it utilizes a principled Bayesian sequential decision problem framework for jointly and naturally optimizing the exploration-exploitation trade-off. In general, the resulting induced GPP policy cannot be derived exactly due to an uncountable set of candidate observations. A key contribution of our work here thus lies in exploiting the Lipschitz continuity of the reward functions to solve for a nonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real time, we further propose an asymptotically optimal, branch-and-bound anytime variant of epsilon-GPP with performance guarantee. We empirically demonstrate the effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian optimization and an energy harvesting task.

</details>

<details>

<summary>2015-11-22 22:25:10 - Near-optimal quantum tomography: estimators and bounds</summary>

- *Richard Kueng, Christopher Ferrie*

- `1503.00677v2` - [abs](http://arxiv.org/abs/1503.00677v2) - [pdf](http://arxiv.org/pdf/1503.00677v2)

> We give bounds on the average fidelity achievable by any quantum state estimator, which is arguably the most prominently used figure of merit in quantum state tomography. Moreover, these bounds can be computed online---that is, while the experiment is running. We show numerically that these bounds are quite tight for relevant distributions of density matrices. We also show that the Bayesian mean estimator is ideal in the sense of performing close to the bound without requiring optimization. Our results hold for all finite dimensional quantum systems.

</details>

<details>

<summary>2015-11-23 07:13:30 - Bayesian Evidence and Model Selection</summary>

- *Kevin H. Knuth, Michael Habeck, Nabin K. Malakar, Asim M. Mubeen, Ben Placek*

- `1411.3013v2` - [abs](http://arxiv.org/abs/1411.3013v2) - [pdf](http://arxiv.org/pdf/1411.3013v2)

> In this paper we review the concepts of Bayesian evidence and Bayes factors, also known as log odds ratios, and their application to model selection. The theory is presented along with a discussion of analytic, approximate and numerical techniques. Specific attention is paid to the Laplace approximation, variational Bayes, importance sampling, thermodynamic integration, and nested sampling and its recent variants. Analogies to statistical physics, from which many of these techniques originate, are discussed in order to provide readers with deeper insights that may lead to new techniques. The utility of Bayesian model testing in the domain sciences is demonstrated by presenting four specific practical examples considered within the context of signal processing in the areas of signal detection, sensor characterization, scientific model selection and molecular force characterization.

</details>

<details>

<summary>2015-11-23 08:21:17 - Parallel Predictive Entropy Search for Batch Global Optimization of Expensive Objective Functions</summary>

- *Amar Shah, Zoubin Ghahramani*

- `1511.07130v1` - [abs](http://arxiv.org/abs/1511.07130v1) - [pdf](http://arxiv.org/pdf/1511.07130v1)

> We develop parallel predictive entropy search (PPES), a novel algorithm for Bayesian optimization of expensive black-box objective functions. At each iteration, PPES aims to select a batch of points which will maximize the information gain about the global maximizer of the objective. Well known strategies exist for suggesting a single evaluation point based on previous observations, while far fewer are known for selecting batches of points to evaluate in parallel. The few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch. To the best of our knowledge, PPES is the first non-greedy batch Bayesian optimization strategy. We demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications, including problems in machine learning, rocket science and robotics.

</details>

<details>

<summary>2015-11-23 10:36:54 - A comparison of inferential methods for highly non-linear state space models in ecology and epidemiology</summary>

- *Matteo Fasiolo, Natalya Pya, Simon N. Wood*

- `1411.4564v2` - [abs](http://arxiv.org/abs/1411.4564v2) - [pdf](http://arxiv.org/pdf/1411.4564v2)

> Highly non-linear, chaotic or near chaotic, dynamic models are important in fields such as ecology and epidemiology: for example, pest species and diseases often display highly non-linear dynamics. However, such models are problematic from the point of view of statistical inference. The defining feature of chaotic and near chaotic systems is extreme sensitivity to small changes in system states and parameters, and this can interfere with inference. There are two main classes of methods for circumventing these difficulties: information reduction approaches, such as Approximate Bayesian Computation or Synthetic Likelihood and state space methods, such as Particle Markov chain Monte Carlo, Iterated Filtering or Parameter Cascading. The purpose of this article is to compare the methods, in order to reach conclusions about how to approach inference with such models in practice. We show that neither class of methods is universally superior to the other. We show that state space methods can suffer multimodality problems in settings with low process noise or model mis-specification, leading to bias toward stable dynamics and high process noise. Information reduction methods avoid this problem but, under the correct model and with sufficient process noise, state space methods lead to substantially sharper inference than information reduction methods. More practically, there are also differences in the tuning requirements of different methods. Our overall conclusion is that model development and checking should probably be performed using an information reduction method with low tuning requirements, while for final inference it is likely to be better to switch to a state space method, checking results against the information reduction approach.

</details>

<details>

<summary>2015-11-23 20:48:32 - Stick-Breaking Policy Learning in Dec-POMDPs</summary>

- *Miao Liu, Christopher Amato, Xuejun Liao, Lawrence Carin, Jonathan P. How*

- `1505.00274v2` - [abs](http://arxiv.org/abs/1505.00274v2) - [pdf](http://arxiv.org/pdf/1505.00274v2)

> Expectation maximization (EM) has recently been shown to be an efficient algorithm for learning finite-state controllers (FSCs) in large decentralized POMDPs (Dec-POMDPs). However, current methods use fixed-size FSCs and often converge to maxima that are far from optimal. This paper considers a variable-size FSC to represent the local policy of each agent. These variable-size FSCs are constructed using a stick-breaking prior, leading to a new framework called \emph{decentralized stick-breaking policy representation} (Dec-SBPR). This approach learns the controller parameters with a variational Bayesian algorithm without having to assume that the Dec-POMDP model is available. The performance of Dec-SBPR is demonstrated on several benchmark problems, showing that the algorithm scales to large problems while outperforming other state-of-the-art methods.

</details>

<details>

<summary>2015-11-24 03:15:06 - Functional Gaussian Process Model for Bayesian Nonparametric Analysis</summary>

- *Leo L. Duan, Xia Wang, Rhonda D. Szczesniak*

- `1502.03042v2` - [abs](http://arxiv.org/abs/1502.03042v2) - [pdf](http://arxiv.org/pdf/1502.03042v2)

> Gaussian process is a theoretically appealing model for nonparametric analysis, but its computational cumbersomeness hinders its use in large scale and the existing reduced-rank solutions are usually heuristic. In this work, we propose a novel construction of Gaussian process as a projection from fixed discrete frequencies to any continuous location. This leads to a valid stochastic process that has a theoretic support with the reduced rank in the spectral density, as well as a high-speed computing algorithm. Our method provides accurate estimates for the covariance parameters and concise form of predictive distribution for spatial prediction. For non-stationary data, we adopt the mixture framework with a customized spectral dependency structure. This enables clustering based on local stationarity, while maintains the joint Gaussianness. Our work is directly applicable in solving some of the challenges in the spatial data, such as large scale computation, anisotropic covariance, spatio-temporal modeling, etc. We illustrate the uses of the model via simulations and an application on a massive dataset.

</details>

<details>

<summary>2015-11-24 08:24:19 - Light and Widely Applicable MCMC: Approximate Bayesian Inference for Large Datasets</summary>

- *Florian Maire, Nial Friel, Pierre Alquier*

- `1503.04178v2` - [abs](http://arxiv.org/abs/1503.04178v2) - [pdf](http://arxiv.org/pdf/1503.04178v2)

> Light and Widely Applicable (LWA-) MCMC is a novel approximation of the Metropolis-Hastings kernel targeting a posterior distribution defined on a large number of observations. Inspired by Approximate Bayesian Computation, we design a Markov chain whose transition makes use of an unknown but fixed, fraction of the available data, where the random choice of sub-sample is guided by the fidelity of this sub-sample to the observed data, as measured by summary (or sufficient) statistics. LWA-MCMC is a generic and flexible approach, as illustrated by the diverse set of examples which we explore. In each case LWA-MCMC yields excellent performance and in some cases a dramatic improvement compared to existing methodologies.

</details>

<details>

<summary>2015-11-24 12:34:29 - Gradient-free Hamiltonian Monte Carlo with Efficient Kernel Exponential Families</summary>

- *Heiko Strathmann, Dino Sejdinovic, Samuel Livingstone, Zoltan Szabo, Arthur Gretton*

- `1506.02564v2` - [abs](http://arxiv.org/abs/1506.02564v2) - [pdf](http://arxiv.org/pdf/1506.02564v2)

> We propose Kernel Hamiltonian Monte Carlo (KMC), a gradient-free adaptive MCMC algorithm based on Hamiltonian Monte Carlo (HMC). On target densities where classical HMC is not an option due to intractable gradients, KMC adaptively learns the target's gradient structure by fitting an exponential family model in a Reproducing Kernel Hilbert Space. Computational costs are reduced by two novel efficient approximations to this gradient. While being asymptotically exact, KMC mimics HMC in terms of sampling efficiency, and offers substantial mixing improvements over state-of-the-art gradient free samplers. We support our claims with experimental studies on both toy and real-world applications, including Approximate Bayesian Computation and exact-approximate MCMC.

</details>

<details>

<summary>2015-11-27 11:27:25 - Non Parametric Hidden Markov Models with Finite State Space: Posterior Concentration Rates</summary>

- *Elodie Vernet*

- `1511.08624v1` - [abs](http://arxiv.org/abs/1511.08624v1) - [pdf](http://arxiv.org/pdf/1511.08624v1)

> The use of non parametric hidden Markov models with finite state space is flourishing in practice while few theoretical guarantees are known in this framework. Here, we study asymptotic guarantees for these models in the Bayesian framework. We obtain posterior concentration rates with respect to the $L_1$-norm on joint marginal densities of consecutive observations in a general theorem. We apply this theorem to two cases and obtain minimax concentration rates. We consider discrete observations with emission distributions distributed from a Dirichlet process and continuous observations with emission distributions distributed from Dirichlet process mixtures of Gaussian distributions.

</details>

<details>

<summary>2015-11-28 12:47:31 - Optimal ETF Selection for Passive Investing</summary>

- *David Puelz, Carlos M. Carvalho, P. Richard Hahn*

- `1510.03385v2` - [abs](http://arxiv.org/abs/1510.03385v2) - [pdf](http://arxiv.org/pdf/1510.03385v2)

> This paper considers the problem of isolating a small number of exchange traded funds (ETFs) that suffice to capture the fundamental dimensions of variation in U.S. financial markets. First, the data is fit to a vector-valued Bayesian regression model, which is a matrix-variate generalization of the well known stochastic search variable selection (SSVS) of George and McCulloch (1993). ETF selection is then performed using the decoupled shrinkage and selection (DSS) procedure described in Hahn and Carvalho (2015), adapted in two ways: to the vector-response setting and to incorporate stochastic covariates. The selected set of ETFs is obtained under a number of different penalty and modeling choices. Optimal portfolios are constructed from selected ETFs by maximizing the Sharpe ratio posterior mean, and they are compared to the (unknown) optimal portfolio based on the full Bayesian model. We compare our selection results to popular ETF advisor Wealthfront.com. Additionally, we consider selecting ETFs by modeling a large set of mutual funds.

</details>

<details>

<summary>2015-11-29 10:11:36 - Model comparison and assessment for single particle tracking in biological fluids</summary>

- *Martin Lysy, Natesh S. Pillai, David B. Hill, M. Gregory Forest, John Mellnik, Paula Vasquez, Scott A. McKinley*

- `1407.5962v2` - [abs](http://arxiv.org/abs/1407.5962v2) - [pdf](http://arxiv.org/pdf/1407.5962v2)

> State-of-the-art techniques in passive particle-tracking microscopy provide high-resolution path trajectories of diverse foreign particles in biological fluids. For particles on the order of 1 micron diameter, these paths are generally inconsistent with simple Brownian motion. Yet, despite an abundance of data confirming these findings and their wide-ranging scientific implications, stochastic modeling of the complex particle motion has received comparatively little attention. Even among posited models, there is virtually no literature on likelihood-based inference, model comparisons, and other quantitative assessments. In this article, we develop a rigorous and computationally efficient Bayesian methodology to address this gap. We analyze two of the most prevalent candidate models for 30 second paths of 1 micron diameter tracer particles in human lung mucus: fractional Brownian motion (fBM) and a Generalized Langevin Equation (GLE) consistent with viscoelastic theory. Our model comparisons distinctly favor GLE over fBM, with the former describing the data remarkably well up to the timescales for which we have reliable information.

</details>

<details>

<summary>2015-11-30 16:22:35 - Random projections for Bayesian regression</summary>

- *Leo N. Geppert, Katja Ickstadt, Alexander Munteanu, Jens Quedenfeld, Christian Sohler*

- `1504.06122v2` - [abs](http://arxiv.org/abs/1504.06122v2) - [pdf](http://arxiv.org/pdf/1504.06122v2)

> This article deals with random projections applied as a data reduction technique for Bayesian regression analysis. We show sufficient conditions under which the entire $d$-dimensional distribution is approximately preserved under random projections by reducing the number of data points from $n$ to $k\in O(\operatorname{poly}(d/\varepsilon))$ in the case $n\gg d$. Under mild assumptions, we prove that evaluating a Gaussian likelihood function based on the projected data instead of the original data yields a $(1+O(\varepsilon))$-approximation in terms of the $\ell_2$ Wasserstein distance. Our main result shows that the posterior distribution of Bayesian linear regression is approximated up to a small error depending on only an $\varepsilon$-fraction of its defining parameters. This holds when using arbitrary Gaussian priors or the degenerate case of uniform distributions over $\mathbb{R}^d$ for $\beta$. Our empirical evaluations involve different simulated settings of Bayesian linear regression. Our experiments underline that the proposed method is able to recover the regression model up to small error while considerably reducing the total running time.

</details>


## 2015-12

<details>

<summary>2015-12-01 10:16:46 - Divide and conquer in ABC: Expectation-Progagation algorithms for likelihood-free inference</summary>

- *Simon Barthelmé, Nicolas Chopin, Vincent Cottet*

- `1512.00205v1` - [abs](http://arxiv.org/abs/1512.00205v1) - [pdf](http://arxiv.org/pdf/1512.00205v1)

> ABC algorithms are notoriously expensive in computing time, as they require simulating many complete artificial datasets from the model. We advocate in this paper a "divide and conquer" approach to ABC, where we split the likelihood into n factors, and combine in some way n "local" ABC approximations of each factor. This has two advantages: (a) such an approach is typically much faster than standard ABC and (b) it makes it possible to use local summary statistics (i.e. summary statistics that depend only on the data-points that correspond to a single factor), rather than global summary statistics (that depend on the complete dataset). This greatly alleviates the bias introduced by summary statistics, and even removes it entirely in situations where local summary statistics are simply the identity function.   We focus on EP (Expectation-Propagation), a convenient and powerful way to combine n local approximations into a global approximation. Compared to the EP- ABC approach of Barthelm\'e and Chopin (2014), we present two variations, one based on the parallel EP algorithm of Cseke and Heskes (2011), which has the advantage of being implementable on a parallel architecture, and one version which bridges the gap between standard EP and parallel EP. We illustrate our approach with an expensive application of ABC, namely inference on spatial extremes.

</details>

<details>

<summary>2015-12-01 11:11:51 - Bayesian Manifold Learning: The Locally Linear Latent Variable Model (LL-LVM)</summary>

- *Mijung Park, Wittawat Jitkrittum, Ahmad Qamar, Zoltan Szabo, Lars Buesing, Maneesh Sahani*

- `1410.6791v4` - [abs](http://arxiv.org/abs/1410.6791v4) - [pdf](http://arxiv.org/pdf/1410.6791v4)

> We introduce the Locally Linear Latent Variable Model (LL-LVM), a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations, their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships. The model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data. Thus, the LL-LVM encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding (LLE). Its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships, select the intrinsic dimensionality of the manifold, construct out-of-sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the manifold.

</details>

<details>

<summary>2015-12-01 15:53:47 - A Likelihood-Free Reverse Sampler of the Posterior Distribution</summary>

- *Jean-Jacques Forneron, Serena Ng*

- `1506.04017v2` - [abs](http://arxiv.org/abs/1506.04017v2) - [pdf](http://arxiv.org/pdf/1506.04017v2)

> This paper considers properties of an optimization based sampler for targeting the posterior distribution when the likelihood is intractable and auxiliary statistics are used to summarize information in the data. Our reverse sampler approximates the likelihood-based posterior distribution by solving a sequence of simulated minimum distance problems. By a change of variable argument, these estimates are reweighted with a prior and the volume of the jacobian matrix to serve as draws from the desired posterior distribution. The sampler provides a conceptual framework to understand the difference between two types of likelihood free estimation. Because simulated minimum distance estimation always results in acceptable draws, the reverse sampler is potentially an alternative to existing approximate Bayesian methods that are computationally demanding because of a low acceptance rate.

</details>

<details>

<summary>2015-12-01 21:02:05 - Bayesian Estimation of Negative Binomial Parameters with Applications to RNA-Seq Data</summary>

- *Luis Leon-Novelo, Claudio Fuentes, Sarah Emerson*

- `1512.00475v1` - [abs](http://arxiv.org/abs/1512.00475v1) - [pdf](http://arxiv.org/pdf/1512.00475v1)

> RNA-Seq data characteristically exhibits large variances, which need to be appropriately accounted for in the model. We first explore the effects of this variability on the maximum likelihood estimator (MLE) of the overdispersion parameter of the negative binomial distribution, and propose instead the use an estimator obtained via maximization of the marginal likelihood in a conjugate Bayesian framework. We show, via simulation studies, that the marginal MLE can better control this variation and produce a more stable and reliable estimator. We then formulate a conjugate Bayesian hierarchical model, in which the estimate of overdispersion is a marginalized estimate and use this estimator to propose a Bayesian test to detect differentially expressed genes with RNA-Seq data. We use numerical studies to show that our much simpler approach is competitive with other negative binomial based procedures, and we use a real data set to illustrate the implementation and flexibility of the procedure.

</details>

<details>

<summary>2015-12-02 18:08:48 - Microclustering: When the Cluster Sizes Grow Sublinearly with the Size of the Data Set</summary>

- *Jeffrey Miller, Brenda Betancourt, Abbas Zaidi, Hanna Wallach, Rebecca C. Steorts*

- `1512.00792v1` - [abs](http://arxiv.org/abs/1512.00792v1) - [pdf](http://arxiv.org/pdf/1512.00792v1)

> Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models, Dirichlet process mixture models, and Pitman--Yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models. However, for some tasks, this assumption is undesirable. For example, when performing entity resolution, the size of each cluster is often unrelated to the size of the data set. Consequently, each cluster contains a negligible fraction of the total number of data points. Such tasks therefore require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the \emph{microclustering property} and introducing a new model that exhibits this property. We compare this model to several commonly used clustering models by checking model fit using real and simulated data sets.

</details>

<details>

<summary>2015-12-03 02:01:59 - Bayesian inference via rejection filtering</summary>

- *Nathan Wiebe, Christopher Granade, Ashish Kapoor, Krysta M Svore*

- `1511.06458v2` - [abs](http://arxiv.org/abs/1511.06458v2) - [pdf](http://arxiv.org/pdf/1511.06458v2)

> We provide a method for approximating Bayesian inference using rejection sampling. We not only make the process efficient, but also dramatically reduce the memory required relative to conventional methods by combining rejection sampling with particle filtering. We also provide an approximate form of rejection sampling that makes rejection filtering tractable in cases where exact rejection sampling is not efficient. Finally, we present several numerical examples of rejection filtering that show its ability to track time dependent parameters in online settings and also benchmark its performance on MNIST classification problems.

</details>

<details>

<summary>2015-12-03 07:10:07 - Posterior Belief Assessment: Extracting Meaningful Subjective Judgements from Bayesian Analyses with Complex Statistical Models</summary>

- *Daniel Williamson, Michael Goldstein*

- `1512.00969v1` - [abs](http://arxiv.org/abs/1512.00969v1) - [pdf](http://arxiv.org/pdf/1512.00969v1)

> In this paper, we are concerned with attributing meaning to the results of a Bayesian analysis for a problem which is sufficiently complex that we are unable to assert a precise correspondence between the expert probabilistic judgements of the analyst and the particular forms chosen for the prior specification and the likelihood for the analysis. In order to do this, we propose performing a finite collection of additional Bayesian analyses under alternative collections of prior and likelihood modelling judgements that we may also view as representative of our prior knowledge and the problem structure, and use these to compute posterior belief assessments for key quantities of interest. We show that these assessments are closer to our true underlying beliefs than the original Bayesian analysis and use the temporal sure preference principle to establish a probabilistic relationship between our true posterior judgements, our posterior belief assessment and our original Bayesian analysis to make this precise. We exploit second order exchangeability in order to generalise our approach to situations where there are infinitely many alternative Bayesian analyses we might consider as informative for our true judgements so that the method remains tractable even in these cases. We argue that posterior belief assessment is a tractable and powerful alternative to robust Bayesian analysis. We describe a methodology for computing posterior belief assessments in even the most complex of statistical models and illustrate with an example of calibrating an expensive ocean model in order to quantify uncertainty about global mean temperature in the real ocean.

</details>

<details>

<summary>2015-12-03 07:39:13 - Sequential Bayesian Model Selection of Regular Vine Copulas</summary>

- *Lutz Gruber, Claudia Czado*

- `1512.00976v1` - [abs](http://arxiv.org/abs/1512.00976v1) - [pdf](http://arxiv.org/pdf/1512.00976v1)

> Regular vine copulas can describe a wider array of dependency patterns than the multivariate Gaussian copula or the multivariate Student's t copula. This paper presents two contributions related to model selection of regular vine copulas. First, our pair copula family selection procedure extends existing Bayesian family selection methods by allowing pair families to be chosen from an arbitrary set of candidate families. Second, our method represents the first Bayesian model selection approach to include the regular vine density construction in its scope of inference. The merits of our approach are established in a simulation study that benchmarks against methods suggested in current literature. A real data example about forecasting of portfolio asset returns for risk measurement and investment allocation illustrates the viability and relevance of the proposed scheme.

</details>

<details>

<summary>2015-12-03 09:47:17 - Bayesian Variable Selection and Estimation for Group Lasso</summary>

- *Xiaofan Xu, Malay Ghosh*

- `1512.01013v1` - [abs](http://arxiv.org/abs/1512.01013v1) - [pdf](http://arxiv.org/pdf/1512.01013v1)

> The paper revisits the Bayesian group lasso and uses spike and slab priors for group variable selection. In the process, the connection of our model with penalized regression is demonstrated, and the role of posterior median for thresholding is pointed out. We show that the posterior median estimator has the oracle property for group variable selection and estimation under orthogonal designs, while the group lasso has suboptimal asymptotic estimation rate when variable selection consistency is achieved. Next we consider bi-level selection problem and propose the Bayesian sparse group selection again with spike and slab priors to select variables both at the group level and also within a group. We demonstrate via simulation that the posterior median estimator of our spike and slab models has excellent performance for both variable selection and estimation.

</details>

<details>

<summary>2015-12-03 10:07:49 - On Posterior Concentration in Misspecified Models</summary>

- *R. V. Ramamoorthi, Karthik Sriram, Ryan Martin*

- `1312.4620v4` - [abs](http://arxiv.org/abs/1312.4620v4) - [pdf](http://arxiv.org/pdf/1312.4620v4)

> We investigate the asymptotic behavior of Bayesian posterior distributions under independent and identically distributed ($i.i.d.$) misspecified models. More specifically, we study the concentration of the posterior distribution on neighborhoods of $f^{\star}$, the density that is closest in the Kullback--Leibler sense to the true model $f_0$. We note, through examples, the need for assumptions beyond the usual Kullback--Leibler support assumption. We then investigate consistency with respect to a general metric under three assumptions, each based on a notion of divergence measure, and then apply these to a weighted $L_1$-metric in convex models and non-convex models. Although a few results on this topic are available, we believe that these are somewhat inaccessible due, in part, to the technicalities and the subtle differences compared to the more familiar well-specified model case. One of our goals is to make some of the available results, especially that of , more accessible. Unlike their paper, our approach does not require construction of test sequences. We also discuss a preliminary extension of the $i.i.d.$ results to the independent but not identically distributed ($i.n.i.d.$) case.

</details>

<details>

<summary>2015-12-03 10:28:58 - Discrete Equilibrium Sampling with Arbitrary Nonequilibrium Processes</summary>

- *Firas Hamze, Evgeny Andryash*

- `1512.01027v1` - [abs](http://arxiv.org/abs/1512.01027v1) - [pdf](http://arxiv.org/pdf/1512.01027v1)

> We present a novel framework for performing statistical sampling, expectation estimation, and partition function approximation using \emph{arbitrary} heuristic stochastic processes defined over discrete state spaces. Using a highly parallel construction we call the \emph{sequential constraining process}, we are able to simultaneously generate states with the heuristic process and accurately estimate their probabilities, even when they are far too small to be realistically inferred by direct counting. After showing that both theoretically correct importance sampling and Markov chain Monte Carlo are possible using the sequential constraining process, we integrate it into a methodology called \emph{state space sampling}, extending the ideas of state space search from computer science to the sampling context. The methodology comprises a dynamic data structure that constructs a robust Bayesian model of the statistics generated by the heuristic process subject to an accuracy constraint, the posterior Kullback-Leibler divergence. Sampling from the dynamic structure will generally yield partial states, which are completed by recursively calling the heuristic to refine the structure and resuming the sampling. Our experiments on various Ising models suggest that state space sampling enables heuristic state generation with accurate probability estimates, demonstrated by illustrating the convergence of a simulated annealing process to the Boltzmann distribution with increasing run length. Consequently, heretofore unprecedented direct importance sampling using the \emph{final} (marginal) distribution of a generic stochastic process is allowed, potentially augmenting the range of algorithms at the Monte Carlo practitioner's disposal.

</details>

<details>

<summary>2015-12-03 13:55:57 - Restricted Covariance Priors with Applications in Spatial Statistics</summary>

- *Theresa R. Smith, Jon Wakefield, Adrian Dobra*

- `1402.5655v2` - [abs](http://arxiv.org/abs/1402.5655v2) - [pdf](http://arxiv.org/pdf/1402.5655v2)

> We present a Bayesian model for area-level count data that uses Gaussian random effects with a novel type of G-Wishart prior on the inverse variance--covariance matrix. Specifically, we introduce a new distribution called the truncated G-Wishart distribution that has support over precision matrices that lead to positive associations between the random effects of neighboring regions while preserving conditional independence of non-neighboring regions. We describe Markov chain Monte Carlo sampling algorithms for the truncated G-Wishart prior in a disease mapping context and compare our results to Bayesian hierarchical models based on intrinsic autoregression priors. A simulation study illustrates that using the truncated G-Wishart prior improves over the intrinsic autoregressive priors when there are discontinuities in the disease risk surface. The new model is applied to an analysis of cancer incidence data in Washington State.

</details>

<details>

<summary>2015-12-03 18:07:35 - The Human Kernel</summary>

- *Andrew Gordon Wilson, Christoph Dann, Christopher G. Lucas, Eric P. Xing*

- `1510.07389v3` - [abs](http://arxiv.org/abs/1510.07389v3) - [pdf](http://arxiv.org/pdf/1510.07389v3)

> Bayesian nonparametric models, such as Gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity. However, automating human expertise remains elusive; for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners. In this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments. We use the learned kernels to gain psychological insights and to extrapolate in human-like ways that go beyond traditional stationary and polynomial kernels. Finally, we investigate Occam's razor in human and Gaussian process based function learning.

</details>

<details>

<summary>2015-12-03 18:09:58 - Real-time financial surveillance via quickest change-point detection methods</summary>

- *Andrey Pepelyshev, Aleksey S. Polunchenko*

- `1509.01570v2` - [abs](http://arxiv.org/abs/1509.01570v2) - [pdf](http://arxiv.org/pdf/1509.01570v2)

> We consider the problem of efficient financial surveillance aimed at "on-the-go" detection of structural breaks (anomalies) in "live"-monitored financial time series. With the problem approached statistically, viz. as that of multi-cyclic sequential (quickest) change-point detection, we propose a semi-parametric multi-cyclic change-point detection procedure to promptly spot anomalies as they occur in the time series under surveillance. The proposed procedure is a derivative of the likelihood ratio-based Shiryaev-Roberts (SR) procedure; the latter is a quasi-Bayesian surveillance method known to deliver the fastest (in the multi-cyclic sense) speed of detection, whatever be the false alarm frequency. We offer a case study where we first carry out, step by step, statistical analysis of a set of real-world financial data, and then set up and devise (a) the proposed SR-based anomaly-detection procedure and (b) the celebrated Cumulative Sum (CUSUM) chart to detect structural breaks in the data. While both procedures performed well, the proposed SR-derivative, conforming to the intuition, seemed slightly better.

</details>

<details>

<summary>2015-12-03 22:39:37 - CrossCat: A Fully Bayesian Nonparametric Method for Analyzing Heterogeneous, High Dimensional Data</summary>

- *Vikash Mansinghka, Patrick Shafto, Eric Jonas, Cap Petschulat, Max Gasner, Joshua B. Tenenbaum*

- `1512.01272v1` - [abs](http://arxiv.org/abs/1512.01272v1) - [pdf](http://arxiv.org/pdf/1512.01272v1)

> There is a widespread need for statistical methods that can analyze high-dimensional datasets with- out imposing restrictive or opaque modeling assumptions. This paper describes a domain-general data analysis method called CrossCat. CrossCat infers multiple non-overlapping views of the data, each consisting of a subset of the variables, and uses a separate nonparametric mixture to model each view. CrossCat is based on approximately Bayesian inference in a hierarchical, nonparamet- ric model for data tables. This model consists of a Dirichlet process mixture over the columns of a data table in which each mixture component is itself an independent Dirichlet process mixture over the rows; the inner mixture components are simple parametric models whose form depends on the types of data in the table. CrossCat combines strengths of mixture modeling and Bayesian net- work structure learning. Like mixture modeling, CrossCat can model a broad class of distributions by positing latent variables, and produces representations that can be efficiently conditioned and sampled from for prediction. Like Bayesian networks, CrossCat represents the dependencies and independencies between variables, and thus remains accurate when there are multiple statistical signals. Inference is done via a scalable Gibbs sampling scheme; this paper shows that it works well in practice. This paper also includes empirical results on heterogeneous tabular data of up to 10 million cells, such as hospital cost and quality measures, voting records, unemployment rates, gene expression measurements, and images of handwritten digits. CrossCat infers structure that is consistent with accepted findings and common-sense knowledge in multiple domains and yields predictive accuracy competitive with generative, discriminative, and model-free alternatives.

</details>

<details>

<summary>2015-12-04 10:53:27 - MCMC convergence diagnosis using geometry of Bayesian LASSO</summary>

- *Azzouz Dermoune, Daoud Ounaissi, Nadji Rahmania*

- `1512.01366v1` - [abs](http://arxiv.org/abs/1512.01366v1) - [pdf](http://arxiv.org/pdf/1512.01366v1)

> Using posterior distribution of Bayesian LASSO we construct a semi-norm on the parameter space. We show that the partition function depends on the ratio of the l1 and l2 norms and present three regimes. We derive the concentration of Bayesian LASSO, and present MCMC convergence diagnosis.

</details>

<details>

<summary>2015-12-04 11:48:22 - Bayesian binary quantile regression for the analysis of Bachelor-Master transition</summary>

- *Cristina Mollica, Lea Petrella*

- `1511.06896v3` - [abs](http://arxiv.org/abs/1511.06896v3) - [pdf](http://arxiv.org/pdf/1511.06896v3)

> The multi-cycle organization of modern university systems stimulates the interest in studying the progression to higher level degree courses during the academic career. In particular, after the achievement of the first level qualification (Bachelor degree), students have to decide whether to continue their university studies, by enrolling in a second level (Master) programme, or to conclude their training experience. In this work we propose a binary quantile regression approach to analyze the Bachelor-Master transition phenomenon with the adoption of the Bayesian inferential perspective. In addition to the traditional predictors of academic outcomes, such as the personal characteristics and the field of study, different aspects of the student's performance are considered. Moreover, a new contextual variable, indicating the type of university regulations, is taken into account in the model specification. The utility of the Bayesian binary quantile regression to characterize the non-continuation decision after the first cycle studies is illustrated with an application to administrative data of Bachelor graduates at the School of Economics of Sapienza University of Rome and compared with a more conventional logistic regression approach.

</details>

<details>

<summary>2015-12-04 12:10:48 - Averaged extreme regression quantile</summary>

- *Jana Jureckova*

- `1512.01382v1` - [abs](http://arxiv.org/abs/1512.01382v1) - [pdf](http://arxiv.org/pdf/1512.01382v1)

> Various events in the nature, economics and in other areas force us to combine the study of extremes with regression and other methods. A useful tool for reducing the role of nuisance regression, while we are interested in the shape or tails of the basic distribution, is provided by the averaged regression quantile and namely by the average extreme regression quantile. Both are weighted means of regression quantile components, with weights depending on the regressors. Our primary interest is the averaged extreme regression quantile (AERQ), its structure, qualities and its applications, e.g. in investigation of a conditional loss given a value exogenous economic and market variables. AERQ has several interesting equivalent forms: While it is originally defined as an optimal solution of a specific linear programming problem, hence is a weighted mean of responses corresponding to the optimal base of the pertaining linear program, we give another equivalent form as a maximum residual of responses from a specific R-estimator of the slope components of regression parameter. The latter form shows that while AERQ equals to the maximum of some residuals of the responses, it has minimal possible perturbation by the regressors. Notice that these finite-sample results are true even for non-identically distributed model errors, e.g. under heteroscedasticity. Moreover, the representations are formally true even when the errors are dependent - this all provokes a question of the right interpretation and of other possible applications.

</details>

<details>

<summary>2015-12-04 17:56:41 - Embarrassingly Parallel Sequential Markov-chain Monte Carlo for Large Sets of Time Series</summary>

- *Roberto Casarin, Radu V. Craiu, Fabrizio Leisen*

- `1512.01496v1` - [abs](http://arxiv.org/abs/1512.01496v1) - [pdf](http://arxiv.org/pdf/1512.01496v1)

> Bayesian computation crucially relies on Markov chain Monte Carlo (MCMC) algorithms. In the case of massive data sets, running the Metropolis-Hastings sampler to draw from the posterior distribution becomes prohibitive due to the large number of likelihood terms that need to be calculated at each iteration. In order to perform Bayesian inference for a large set of time series, we consider an algorithm that combines 'divide and conquer" ideas previously used to design MCMC algorithms for big data with a sequential MCMC strategy. The performance of the method is illustrated using a large set of financial data.

</details>

<details>

<summary>2015-12-05 03:41:00 - A Bayesian test to identify variance effects</summary>

- *Bianca Dumitrascu, Gregory Darnell, Julien Ayroles, Barbara E Engelhardt*

- `1512.01616v1` - [abs](http://arxiv.org/abs/1512.01616v1) - [pdf](http://arxiv.org/pdf/1512.01616v1)

> Identifying genetic variants that regulate quantitative traits, or QTLs, is the primary focus of the field of statistical genetics. Most current methods are limited to identifying mean effects, or associations between genotype and the mean value of a quantitative trait. It is possible, however, that a genetic variant may affect the variance of the quantitative trait in lieu of, or in addition to, affecting the trait mean. Here, we develop a general methodological approach to identifying covariates with variance effects on a quantitative trait using a Bayesian heteroskedastic linear regression model. We show that our Bayesian test for heteroskedasticity (BTH) outperforms classical tests for differences in variation across a large range of simulations drawn from scenarios common to the analysis of quantitative traits. We apply BTH to methylation QTL study data and expression QTL study data to identify variance QTLs. When compared with three tests for heteroskedasticity used in genomics, we illustrate the benefits of using our approach, including avoiding overfitting by incorporating uncertainty and flexibly identifying heteroskedastic effects.

</details>

<details>

<summary>2015-12-05 03:49:23 - Quasi likelihood analysis of point processes for ultra high frequency data</summary>

- *Teppei Ogihara, Nakahiro Yoshida*

- `1512.01619v1` - [abs](http://arxiv.org/abs/1512.01619v1) - [pdf](http://arxiv.org/pdf/1512.01619v1)

> We introduce a point process regression model that is applicable to price models and limit order book models. Hawkes type autoregression in the intensity process is generalized to a stochastic regression to covariate processes. We establish the so-called quasi likelihood analysis, which gives a polynomial type large deviation estimate for the statistical random field. We derive large sample properties of the maximum likelihood type estimator and the Bayesian type estimator when the intensity processes become large under a finite time horizon. There appears non-ergodic statistics. A classical approach is also mentioned.

</details>

<details>

<summary>2015-12-05 08:42:51 - Bayesian Endogenous Tobit Quantile Regression</summary>

- *Genya Kobayashi*

- `1505.07541v3` - [abs](http://arxiv.org/abs/1505.07541v3) - [pdf](http://arxiv.org/pdf/1505.07541v3)

> This study proposes $p$-th Tobit quantile regression models with endogenous variables. In the first stage regression of the endogenous variable on the exogenous variables, the assumption that the $\alpha$-th quantile of the error term is zero is introduced. Then, the residual of this regression model is included in the $p$-th quantile regression model in such a way that the $p$-th conditional quantile of the new error term is zero. The error distribution of the first stage regression is modelled around the zero $\alpha$-th quantile assumption by using parametric and semiparametric approaches. Since the value of $\alpha$ is a priori unknown, it is treated as an additional parameter and is estimated from the data. The proposed models are then demonstrated by using simulated data and real data on the labour supply of married women.

</details>

<details>

<summary>2015-12-05 13:45:47 - Stochastic Collapsed Variational Inference for Sequential Data</summary>

- *Pengyu Wang, Phil Blunsom*

- `1512.01666v1` - [abs](http://arxiv.org/abs/1512.01666v1) - [pdf](http://arxiv.org/pdf/1512.01666v1)

> Stochastic variational inference for collapsed models has recently been successfully applied to large scale topic modelling. In this paper, we propose a stochastic collapsed variational inference algorithm in the sequential data setting. Our algorithm is applicable to both finite hidden Markov models and hierarchical Dirichlet process hidden Markov models, and to any datasets generated by emission distributions in the exponential family. Our experiment results on two discrete datasets show that our inference is both more efficient and more accurate than its uncollapsed version, stochastic variational inference.

</details>

<details>

<summary>2015-12-06 04:03:26 - On the Data Augmentation Algorithm for Bayesian Multivariate Linear Regression with Non-Gaussian Errors</summary>

- *Qian Qin, James P. Hobert*

- `1512.01734v1` - [abs](http://arxiv.org/abs/1512.01734v1) - [pdf](http://arxiv.org/pdf/1512.01734v1)

> Let $\pi$ denote the intractable posterior density that results when the likelihood from a multivariate linear regression model with errors from a scale mixture of normals is combined with the standard non-informative prior. There is a simple data augmentation algorithm (based on latent data from the mixing density) that can be used to explore $\pi$. Hobert et al. (2015) [arXiv:1506.03113v1] recently performed a convergence rate analysis of the Markov chain underlying this MCMC algorithm in the special case where the regression model is univariate. These authors provide simple sufficient conditions (on the mixing density) for geometric ergodicity of the Markov chain. In this note, we extend Hobert et al.'s (2015) result to the multivariate case.

</details>

<details>

<summary>2015-12-06 04:40:24 - Variational Particle Approximations</summary>

- *Ardavan Saeedi, Tejas D Kulkarni, Vikash Mansinghka, Samuel Gershman*

- `1402.5715v3` - [abs](http://arxiv.org/abs/1402.5715v3) - [pdf](http://arxiv.org/pdf/1402.5715v3)

> Approximate inference in high-dimensional, discrete probabilistic models is a central problem in computational statistics and machine learning. This paper describes discrete particle variational inference (DPVI), a new approach that combines key strengths of Monte Carlo, variational and search-based techniques. DPVI is based on a novel family of particle-based variational approximations that can be fit using simple, fast, deterministic search techniques. Like Monte Carlo, DPVI can handle multiple modes, and yields exact results in a well-defined limit. Like unstructured mean-field, DPVI is based on optimizing a lower bound on the partition function; when this quantity is not of intrinsic interest, it facilitates convergence assessment and debugging. Like both Monte Carlo and combinatorial search, DPVI can take advantage of factorization, sequential structure, and custom search operators. This paper defines DPVI particle-based approximation family and partition function lower bounds, along with the sequential DPVI and local DPVI algorithm templates for optimizing them. DPVI is illustrated and evaluated via experiments on lattice Markov Random Fields, nonparametric Bayesian mixtures and block-models, and parametric as well as non-parametric hidden Markov models. Results include applications to real-world spike-sorting and relational modeling problems, and show that DPVI can offer appealing time/accuracy trade-offs as compared to multiple alternatives.

</details>

<details>

<summary>2015-12-06 11:30:34 - Bayesian inference and model comparison for metallic fatigue data</summary>

- *Ivo Babuska, Zaid Sawlan, Marco Scavino, Barna Szabó, Raúl Tempone*

- `1512.01779v1` - [abs](http://arxiv.org/abs/1512.01779v1) - [pdf](http://arxiv.org/pdf/1512.01779v1)

> In this work, we present a statistical treatment of stress-life (S-N) data drawn from a collection of records of fatigue experiments that were performed on 75S-T6 aluminum alloys. Our main objective is to predict the fatigue life of materials by providing a systematic approach to model calibration, model selection and model ranking with reference to S-N data. To this purpose, we consider fatigue-limit models and random fatigue-limit models that are specially designed to allow the treatment of the run-outs (right-censored data). We first fit the models to the data by maximum likelihood methods and estimate the quantiles of the life distribution of the alloy specimen. To assess the robustness of the estimation of the quantile functions, we obtain bootstrap confidence bands by stratified resampling with respect to the cycle ratio. We then compare and rank the models by classical measures of fit based on information criteria. We also consider a Bayesian approach that provides, under the prior distribution of the model parameters selected by the user, their simulation-based posterior distributions. We implement and apply Bayesian model comparison methods, such as Bayes factor ranking and predictive information criteria based on cross-validation techniques under various a priori scenarios.

</details>

<details>

<summary>2015-12-06 22:05:30 - Frank-Wolfe Bayesian Quadrature: Probabilistic Integration with Theoretical Guarantees</summary>

- *François-Xavier Briol, Chris J. Oates, Mark Girolami, Michael A. Osborne*

- `1506.02681v3` - [abs](http://arxiv.org/abs/1506.02681v3) - [pdf](http://arxiv.org/pdf/1506.02681v3)

> There is renewed interest in formulating integration as an inference problem, motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation. Current methods, such as Bayesian Quadrature, demonstrate impressive empirical performance but lack theoretical analysis. An important challenge is to reconcile these probabilistic integrators with rigorous convergence guarantees. In this paper, we present the first probabilistic integrator that admits such theoretical treatment, called Frank-Wolfe Bayesian Quadrature (FWBQ). Under FWBQ, convergence to the true value of the integral is shown to be exponential and posterior contraction rates are proven to be superexponential. In simulations, FWBQ is competitive with state-of-the-art methods and out-performs alternatives based on Frank-Wolfe optimisation. Our approach is applied to successfully quantify numerical error in the solution to a challenging model choice problem in cellular biology.

</details>

<details>

<summary>2015-12-06 22:13:46 - Explaining reviews and ratings with PACO: Poisson Additive Co-Clustering</summary>

- *Chao-Yuan Wu, Alex Beutel, Amr Ahmed, Alexander J. Smola*

- `1512.01845v1` - [abs](http://arxiv.org/abs/1512.01845v1) - [pdf](http://arxiv.org/pdf/1512.01845v1)

> Understanding a user's motivations provides valuable information beyond the ability to recommend items. Quite often this can be accomplished by perusing both ratings and review texts, since it is the latter where the reasoning for specific preferences is explicitly expressed.   Unfortunately matrix factorization approaches to recommendation result in large, complex models that are difficult to interpret and give recommendations that are hard to clearly explain to users. In contrast, in this paper, we attack this problem through succinct additive co-clustering. We devise a novel Bayesian technique for summing co-clusterings of Poisson distributions. With this novel technique we propose a new Bayesian model for joint collaborative filtering of ratings and text reviews through a sum of simple co-clusterings. The simple structure of our model yields easily interpretable recommendations. Even with a simple, succinct structure, our model outperforms competitors in terms of predicting ratings with reviews.

</details>

<details>

<summary>2015-12-07 12:37:41 - Discriminative Nonparametric Latent Feature Relational Models with Data Augmentation</summary>

- *Bei Chen, Ning Chen, Jun Zhu, Jiaming Song, Bo Zhang*

- `1512.02016v1` - [abs](http://arxiv.org/abs/1512.02016v1) - [pdf](http://arxiv.org/pdf/1512.02016v1)

> We present a discriminative nonparametric latent feature relational model (LFRM) for link prediction to automatically infer the dimensionality of latent features. Under the generic RegBayes (regularized Bayesian inference) framework, we handily incorporate the prediction loss with probabilistic inference of a Bayesian model; set distinct regularization parameters for different types of links to handle the imbalance issue in real networks; and unify the analysis of both the smooth logistic log-loss and the piecewise linear hinge loss. For the nonconjugate posterior inference, we present a simple Gibbs sampler via data augmentation, without making restricting assumptions as done in variational methods. We further develop an approximate sampler using stochastic gradient Langevin dynamics to handle large networks with hundreds of thousands of entities and millions of links, orders of magnitude larger than what existing LFRM models can process. Extensive studies on various real networks show promising performance.

</details>

<details>

<summary>2015-12-07 18:13:32 - A Time-varying Parameter Based Seasonally-adjusted Bayesian State-space Model for Forecasting</summary>

- *Arnab Hazra*

- `1512.02149v1` - [abs](http://arxiv.org/abs/1512.02149v1) - [pdf](http://arxiv.org/pdf/1512.02149v1)

> In this paper, we develop a time-varying parameter based seasonally-adjusted Bayesian state-space model for non-stationary time series datasets where both the trend and seasonal components are present and it is the general scenario for most of the real datasets in various scientific disciplines. In spite of removing such terms using some do-and-check procedure to make the data stationary, our model directly fits a dataset and forecasts a number of future observations. For a specific prior construction we have considered, every parameter update is one-dimensional so that we don't need to invert any matrix and also we overcome the difficulty of Metropolis-Hastings steps simply by Gibbs sampling which is another advantage of this model. It can handle missing data as well which occurs very often in time series contexts. We implement it on the sufficiently large (24 years of monthly average temperature series, i.e. the number of observations =288) for 57 meteorological stations across India and show that for most of the cases, our method forecasts quite accurately for the months of the 25-th year.

</details>

<details>

<summary>2015-12-08 02:25:12 - Nonparametric Reduced-Rank Regression for Multi-SNP, Multi-Trait Association Mapping</summary>

- *Ashlee Valente, Geoffrey Ginsburg, Barbara E Engelhardt*

- `1512.02306v1` - [abs](http://arxiv.org/abs/1512.02306v1) - [pdf](http://arxiv.org/pdf/1512.02306v1)

> Genome-wide association studies have proven to be essential for understanding the genetic basis of disease. However, many complex traits---personality traits, facial features, disease subtyping---are inherently high-dimensional, impeding simple approaches to association mapping. We developed a nonparametric Bayesian reduced rank regression model for multi-SNP, multi-trait association mapping that does not require the rank of the linear subspace to be specified. We show in simulations and real data that our model shares strength over SNPs and over correlated traits, improving statistical power to identify genetic associations with an interpretable, SNP-supervised low-dimensional linear projection of the high-dimensional phenotype. On the HapMap phase 3 gene expression QTL study data, we identify pleiotropic expression QTLs that classical univariate tests are underpowered to find and that two step approaches cannot recover. Our Python software, BERRRI, is publicly available at GitHub: https://github.com/ashlee1031/BERRRI.

</details>

<details>

<summary>2015-12-08 13:20:03 - Sequential Markov Chain Monte Carlo for Bayesian Filtering with Massive Data</summary>

- *Allan De Freitas, François Septier, Lyudmila Mihaylova*

- `1512.02452v1` - [abs](http://arxiv.org/abs/1512.02452v1) - [pdf](http://arxiv.org/pdf/1512.02452v1)

> Advances in digital sensors, digital data storage and communications have resulted in systems being capable of accumulating large collections of data. In the light of dealing with the challenges that massive data present, this work proposes solutions to inference and filtering problems within the Bayesian framework. Two novel Bayesian inference algorithms are developed for non-linear and non-Gaussian state space models, able to deal with large volumes of data (or observations). These are sequential Markov chain Monte Carlo (MCMC) approaches relying on two key ideas: 1) subsample the massive data and utilise a smaller subset for filtering and inference, and 2) a divide and conquer type approach computing local filtering distributions each using a subset of the measurements. Simulation results highlight the accuracy and the large computational savings, that can reach 90% by the proposed algorithms when compared with standard techniques.

</details>

<details>

<summary>2015-12-08 18:33:05 - Robust Inference with Variational Bayes</summary>

- *Ryan Giordano, Tamara Broderick, Michael Jordan*

- `1512.02578v1` - [abs](http://arxiv.org/abs/1512.02578v1) - [pdf](http://arxiv.org/pdf/1512.02578v1)

> In Bayesian analysis, the posterior follows from the data and a choice of a prior and a likelihood. One hopes that the posterior is robust to reasonable variation in the choice of prior and likelihood, since this choice is made by the modeler and is necessarily somewhat subjective. Despite the fundamental importance of the problem and a considerable body of literature, the tools of robust Bayes are not commonly used in practice. This is in large part due to the difficulty of calculating robustness measures from MCMC draws. Although methods for computing robustness measures from MCMC draws exist, they lack generality and often require additional coding or computation.   In contrast to MCMC, variational Bayes (VB) techniques are readily amenable to robustness analysis. The derivative of a posterior expectation with respect to a prior or data perturbation is a measure of local robustness to the prior or likelihood. Because VB casts posterior inference as an optimization problem, its methodology is built on the ability to calculate derivatives of posterior quantities with respect to model parameters, even in very complex models. In the present work, we develop local prior robustness measures for mean-field variational Bayes(MFVB), a VB technique which imposes a particular factorization assumption on the variational posterior approximation. We start by outlining existing local prior measures of robustness. Next, we use these results to derive closed-form measures of the sensitivity of mean-field variational posterior approximation to prior specification. We demonstrate our method on a meta-analysis of randomized controlled interventions in access to microcredit in developing countries.

</details>

<details>

<summary>2015-12-09 18:47:45 - Stochastic modelling, Bayesian inference, and new in vivo measurements elucidate the debated mtDNA bottleneck mechanism</summary>

- *Iain G. Johnston, Joerg P. Burgstaller, Vitezslav Havlicek, Thomas Kolbe, Thomas Rulicke, Gottfried Brem, Jo Poulton, Nick S. Jones*

- `1512.02988v1` - [abs](http://arxiv.org/abs/1512.02988v1) - [pdf](http://arxiv.org/pdf/1512.02988v1)

> Dangerous damage to mitochondrial DNA (mtDNA) can be ameliorated during mammalian development through a highly debated mechanism called the mtDNA bottleneck. Uncertainty surrounding this process limits our ability to address inherited mtDNA diseases. We produce a new, physically motivated, generalisable theoretical model for mtDNA populations during development, allowing the first statistical comparison of proposed bottleneck mechanisms. Using approximate Bayesian computation and mouse data, we find most statistical support for a combination of binomial partitioning of mtDNAs at cell divisions and random mtDNA turnover, meaning that the debated exact magnitude of mtDNA copy number depletion is flexible. New experimental measurements from a wild-derived mtDNA pairing in mice confirm the theoretical predictions of this model. We analytically solve a mathematical description of this mechanism, computing probabilities of mtDNA disease onset, efficacy of clinical sampling strategies, and effects of potential dynamic interventions, thus developing a quantitative and experimentally-supported stochastic theory of the bottleneck.

</details>

<details>

<summary>2015-12-10 07:50:57 - Quantifying alternative splicing from paired-end RNA-sequencing data</summary>

- *David Rossell, Camille Stephan-Otto Attolini, Manuel Kroiss, Almond Stöcker*

- `1405.0788v2` - [abs](http://arxiv.org/abs/1405.0788v2) - [pdf](http://arxiv.org/pdf/1405.0788v2)

> RNA-sequencing has revolutionized biomedical research and, in particular, our ability to study gene alternative splicing. The problem has important implications for human health, as alternative splicing may be involved in malfunctions at the cellular level and multiple diseases. However, the high-dimensional nature of the data and the existence of experimental biases pose serious data analysis challenges. We find that the standard data summaries used to study alternative splicing are severely limited, as they ignore a substantial amount of valuable information. Current data analysis methods are based on such summaries and are hence suboptimal. Further, they have limited flexibility in accounting for technical biases. We propose novel data summaries and a Bayesian modeling framework that overcome these limitations and determine biases in a nonparametric, highly flexible manner. These summaries adapt naturally to the rapid improvements in sequencing technology. We provide efficient point estimates and uncertainty assessments. The approach allows to study alternative splicing patterns for individual samples and can also be the basis for downstream analyses. We found a severalfold improvement in estimation mean square error compared popular approaches in simulations, and substantially higher consistency between replicates in experimental data. Our findings indicate the need for adjusting the routine summarization and analysis of alternative splicing RNA-seq studies. We provide a software implementation in the R package casper.

</details>

<details>

<summary>2015-12-10 10:47:34 - On Russian Roulette Estimates for Bayesian Inference with Doubly-Intractable Likelihoods</summary>

- *Anne-Marie Lyne, Mark Girolami, Yves Atchadé, Heiko Strathmann, Daniel Simpson*

- `1306.4032v4` - [abs](http://arxiv.org/abs/1306.4032v4) - [pdf](http://arxiv.org/pdf/1306.4032v4)

> A large number of statistical models are "doubly-intractable": the likelihood normalising term, which is a function of the model parameters, is intractable, as well as the marginal likelihood (model evidence). This means that standard inference techniques to sample from the posterior, such as Markov chain Monte Carlo (MCMC), cannot be used. Examples include, but are not confined to, massive Gaussian Markov random fields, autologistic models and Exponential random graph models. A number of approximate schemes based on MCMC techniques, Approximate Bayesian computation (ABC) or analytic approximations to the posterior have been suggested, and these are reviewed here. Exact MCMC schemes, which can be applied to a subset of doubly-intractable distributions, have also been developed and are described in this paper. As yet, no general method exists which can be applied to all classes of models with doubly-intractable posteriors. In addition, taking inspiration from the Physics literature, we study an alternative method based on representing the intractable likelihood as an infinite series. Unbiased estimates of the likelihood can then be obtained by finite time stochastic truncation of the series via Russian Roulette sampling, although the estimates are not necessarily positive. Results from the Quantum Chromodynamics literature are exploited to allow the use of possibly negative estimates in a pseudo-marginal MCMC scheme such that expectations with respect to the posterior distribution are preserved. The methodology is reviewed on well-known examples such as the parameters in Ising models, the posterior for Fisher-Bingham distributions on the $d$-Sphere and a large-scale Gaussian Markov Random Field model describing the Ozone Column data. This leads to a critical assessment of the strengths and weaknesses of the methodology with pointers to ongoing research.

</details>

<details>

<summary>2015-12-10 17:19:46 - Bayesian variable selection with spherically symmetric priors</summary>

- *M. B. De Kock, H. C. Eggers*

- `1410.0891v2` - [abs](http://arxiv.org/abs/1410.0891v2) - [pdf](http://arxiv.org/pdf/1410.0891v2)

> We propose that Bayesian variable selection for linear parametrisations with Gaussian iid likelihoods be based on the spherical symmetry of the diagonalised parameter space. Our r-prior results in closed forms for the evidence for four examples, including the hyper-g prior and the Zellner-Siow prior, which are shown to be special cases. Scenarios of a single variable dispersion parameter and of fixed dispersion are studied, and asymptotic forms comparable to the traditional information criteria are derived. A simulation exercise shows that model comparison based on our r-prior gives good results comparable to or better than current model comparison schemes.

</details>

<details>

<summary>2015-12-12 23:37:51 - An iterative importance sampler for Bayesian parameter estimation in stochastic models of multicellular clocks</summary>

- *Inés P. Mariño, Joaquin Miguez, Alexey Zaikin*

- `1512.03976v1` - [abs](http://arxiv.org/abs/1512.03976v1) - [pdf](http://arxiv.org/pdf/1512.03976v1)

> We investigate a stochastic version of the synthetic multicellular clock model proposed by Garcia-Ojalvo, Elowitz and Strogatz. By introducing dynamical noise in the model and assuming that the partial observations of the system can be contaminated by additive noise, we enable a principled mechanism to represent experimental uncertainties in the synthesis of the multicellular system and pave the way for the design of probabilistic methods for the estimation of any unknowns in the model. Within this setup, we investigate the use of an iterative importance sampling scheme, termed nonlinear population Monte Carlo (NPMC), for the Bayesian estimation of the model parameters. The algorithm yields a stochastic approximation of the posterior probability distribution of the unknown parameters given the available data (partial and possibly noisy observations). We prove a new theoretical result for this algorithm, which indicates that the approximations converge almost surely to the actual distributions, even when the weights in the importance sampling scheme cannot be computed exactly. We also provide a detailed numerical assessment of the stochastic multicellular model and the accuracy of the proposed NPMC algorithm, including a comparison with the popular particle Metropolis-Hastings algorithm of Andrieu {\em et al.}, 2010, applied to the same model and an approximate Bayesian computation sequential Monte Carlo method introduced by Mari\~no {\em et al.}, 2013.

</details>

<details>

<summary>2015-12-15 09:23:47 - Improving the INLA approach for approximate Bayesian inference for latent Gaussian models</summary>

- *Egil Ferkingstad, Håvard Rue*

- `1503.07307v6` - [abs](http://arxiv.org/abs/1503.07307v6) - [pdf](http://arxiv.org/pdf/1503.07307v6)

> We introduce a new copula-based correction for generalized linear mixed models (GLMMs) within the integrated nested Laplace approximation (INLA) approach for approximate Bayesian inference for latent Gaussian models. While INLA is usually very accurate, some (rather extreme) cases of GLMMs with e.g. binomial or Poisson data have been seen to be problematic. Inaccuracies can occur when there is a very low degree of smoothing or "borrowing strength" within the model, and we have therefore developed a correction aiming to push the boundaries of the applicability of INLA. Our new correction has been implemented as part of the R-INLA package, and adds only negligible computational cost. Empirical evaluations on both real and simulated data indicate that the method works well.

</details>

<details>

<summary>2015-12-15 10:46:32 - Tail index estimation, concentration and adaptivity</summary>

- *Stéphane Boucheron, Maud Thomas*

- `1503.05077v3` - [abs](http://arxiv.org/abs/1503.05077v3) - [pdf](http://arxiv.org/pdf/1503.05077v3)

> This paper presents an adaptive version of the Hill estimator based on Lespki's model selection method. This simple data-driven index selection method is shown to satisfy an oracle inequality and is checked to achieve the lower bound recently derived by Carpentier and Kim. In order to establish the oracle inequality, we derive non-asymptotic variance bounds and concentration inequalities for Hill estimators. These concentration inequalities are derived from Talagrand's concentration inequality for smooth functions of independent exponentially distributed random variables combined with three tools of Extreme Value Theory: the quantile transform, Karamata's representation of slowly varying functions, and R\'enyi's characterisation of the order statistics of exponential samples. The performance of this computationally and conceptually simple method is illustrated using Monte-Carlo simulations.

</details>

<details>

<summary>2015-12-15 11:01:08 - Adapting the ABC distance function</summary>

- *Dennis Prangle*

- `1507.00874v3` - [abs](http://arxiv.org/abs/1507.00874v3) - [pdf](http://arxiv.org/pdf/1507.00874v3)

> Approximate Bayesian computation performs approximate inference for models where likelihood computations are expensive or impossible. Instead simulations from the model are performed for various parameter values and accepted if they are close enough to the observations. There has been much progress on deciding which summary statistics of the data should be used to judge closeness, but less work on how to weight them. Typically weights are chosen at the start of the algorithm which normalise the summary statistics to vary on similar scales. However these may not be appropriate in iterative ABC algorithms, where the distribution from which the parameters are proposed is updated. This can substantially alter the resulting distribution of summary statistics, so that different weights are needed for normalisation. This paper presents two iterative ABC algorithms which adaptively update their weights and demonstrates improved results on test applications.

</details>

<details>

<summary>2015-12-15 15:10:27 - A Bayesian approach to the g-formula</summary>

- *Alexander P. Keil, Eric J. Daza, Stephanie M. Engel, Jessie P. Buckley, Jessie K. Edwards*

- `1512.04809v1` - [abs](http://arxiv.org/abs/1512.04809v1) - [pdf](http://arxiv.org/pdf/1512.04809v1)

> Epidemiologists often wish to estimate quantities that are easy to communicate and correspond to the results of realistic public health scenarios. Methods from causal inference can answer these questions. We adopt the language of potential outcomes under Rubin's original Bayesian framework and show that the parametric g-formula is easily amenable to a Bayesian approach. We show that the frequentist properties of the Bayesian g-formula suggest it improves the accuracy of estimates of causal effects in small samples or when data may be sparse. We demonstrate our approach to estimate the effect of environmental tobacco smoke on body mass index z-scores among children aged 4-9 years who were enrolled in a longitudinal birth cohort in New York, USA. We give a general algorithm and supply SAS and Stan code that can be adopted to implement our computational approach in both time-fixed and longitudinal data.

</details>

<details>

<summary>2015-12-15 15:46:11 - Bayesian model selection for linear regression</summary>

- *Miguel de Benito Delgado, Philipp Wacker*

- `1512.04823v1` - [abs](http://arxiv.org/abs/1512.04823v1) - [pdf](http://arxiv.org/pdf/1512.04823v1)

> In this note we introduce linear regression with basis functions in order to apply Bayesian model selection. The goal is to incorporate Occam's razor as provided by Bayes analysis in order to automatically pick the model optimally able to explain the data without overfitting.

</details>

<details>

<summary>2015-12-16 13:33:31 - Bayesian analysis of Jolly-Seber type models; incorporating heterogeneity in arrival and departure</summary>

- *E. Matechou, G. Nicholls, B. J. T. Morgan, J. A. Collazo, J. E. Lyons*

- `1512.05170v1` - [abs](http://arxiv.org/abs/1512.05170v1) - [pdf](http://arxiv.org/pdf/1512.05170v1)

> We propose the use of finite mixtures of continuous distributions in modelling the process by which new individuals, that arrive in groups, become part of a wildlife population. We demonstrate this approach using a data set of migrating semipalmated sandpipers (Calidris pussila) for which we extend existing stopover models to allow for individuals to have different behaviour in terms of their stopover duration at the site. We demonstrate the use of reversible jump MCMC methods to derive posterior distributions for the model parameters and the models, simultaneously. The algorithm moves between models with different numbers of arrival groups as well as between models with different numbers of behavioural groups. The approach is shown to provide new ecological insights about the stopover behaviour of semipalmated sandpipers but is generally applicable to any population in which animals arrive in groups and potentially exhibit heterogeneity in terms of one or more other processes.

</details>

<details>

<summary>2015-12-16 13:33:39 - Covariant priors and model uncertainty</summary>

- *Giovanni Mana, Carlo Palmisano*

- `1512.05171v1` - [abs](http://arxiv.org/abs/1512.05171v1) - [pdf](http://arxiv.org/pdf/1512.05171v1)

> In the application of Bayesian methods to metrology, pre-data probabilities play a critical role in the estimation of the model uncertainty. Following the observation that distributions form Riemann's manifolds, methods of differential geometry can be applied to ensure covariant priors and uncertainties independent of parameterization. Paradoxes were found in multi-parameter problems and alternatives were developed; but, when different parameters are of interest, covariance may be lost. This paper overviews information geometry, investigates some key paradoxes, and proposes solutions that preserve covariance.

</details>

<details>

<summary>2015-12-17 11:24:48 - Bayesian Covariance Modelling of Large Tensor-Variate Data Sets $\&$ Inverse Non-parametric Learning of the Unknown Model Parameter Vector</summary>

- *Kangrui Wang, Dalia Chakrabarty*

- `1512.05538v1` - [abs](http://arxiv.org/abs/1512.05538v1) - [pdf](http://arxiv.org/pdf/1512.05538v1)

> We present a method for modelling the covariance structure of tensor-variate data, with the ulterior aim of learning an unknown model parameter vector using such data. We express the high-dimensional observable as a function of this sought model parameter vector, and attempt to learn such a high-dimensional function given training data, by modelling it as a realisation from a tensor-variate Gaussian Process (GP). The likelihood of the unknowns given training data, is then tensor-normal. We choose vague priors on the unknown GP parameters (mean tensor and covariance matrices) and write the posterior probability density of these unknowns given the data. We perform posterior sampling using Random-Walk Metropolis-Hastings. Thereafter we learn the aforementioned unknown model parameter vector by performing posterior sampling in two different ways, given test and training data, using MCMC, to generate 95$\%$ HPD credible region on each unknown. We make an application of this method to the learning of the location of the Sun in the Milky Way disk.

</details>

<details>

<summary>2015-12-17 15:38:34 - Summary Statistics in Approximate Bayesian Computation</summary>

- *Dennis Prangle*

- `1512.05633v1` - [abs](http://arxiv.org/abs/1512.05633v1) - [pdf](http://arxiv.org/pdf/1512.05633v1)

> This document is due to appear as a chapter of the forthcoming Handbook of Approximate Bayesian Computation (ABC) edited by S. Sisson, Y. Fan, and M. Beaumont.   Since the earliest work on ABC, it has been recognised that using summary statistics is essential to produce useful inference results. This is because ABC suffers from a curse of dimensionality effect, whereby using high dimensional inputs causes large approximation errors in the output. It is therefore crucial to find low dimensional summaries which are informative about the parameter inference or model choice task at hand. This chapter reviews the methods which have been proposed to select such summaries, extending the previous review paper of Blum et al. (2013) with recent developments. Related theoretical results on the ABC curse of dimensionality and sufficiency are also discussed.

</details>

<details>

<summary>2015-12-17 21:39:33 - Macau: Scalable Bayesian Multi-relational Factorization with Side Information using MCMC</summary>

- *Jaak Simm, Adam Arany, Pooya Zakeri, Tom Haber, Jörg K. Wegner, Vladimir Chupakhin, Hugo Ceulemans, Yves Moreau*

- `1509.04610v2` - [abs](http://arxiv.org/abs/1509.04610v2) - [pdf](http://arxiv.org/pdf/1509.04610v2)

> We propose Macau, a powerful and flexible Bayesian factorization method for heterogeneous data. Our model can factorize any set of entities and relations that can be represented by a relational model, including tensors and also multiple relations for each entity. Macau can also incorporate side information, specifically entity and relation features, which are crucial for predicting sparsely observed relations. Macau scales to millions of entity instances, hundred millions of observations, and sparse entity features with millions of dimensions. To achieve the scale up, we specially designed sampling procedure for entity and relation features that relies primarily on noise injection in linear regressions. We show performance and advanced features of Macau in a set of experiments, including challenging drug-protein activity prediction task.

</details>

<details>

<summary>2015-12-18 05:01:50 - A nonparametric Bayesian analysis of heterogeneous treatment effects in digital experimentation</summary>

- *Matt Taddy, Matt Gardner, Liyun Chen, David Draper*

- `1412.8563v4` - [abs](http://arxiv.org/abs/1412.8563v4) - [pdf](http://arxiv.org/pdf/1412.8563v4)

> Randomized controlled trials play an important role in how Internet companies predict the impact of policy decisions and product changes. In these `digital experiments', different units (people, devices, products) respond differently to the treatment. This article presents a fast and scalable Bayesian nonparametric analysis of such heterogeneous treatment effects and their measurement in relation to observable covariates. New results and algorithms are provided for quantifying the uncertainty associated with treatment effect measurement via both linear projections and nonlinear regression trees (CART and Random Forests). For linear projections, our inference strategy leads to results that are mostly in agreement with those from the frequentist literature. We find that linear regression adjustment of treatment effect averages (i.e., post-stratification) can provide some variance reduction, but that this reduction will be vanishingly small in the low-signal and large-sample setting of digital experiments. For regression trees, we provide uncertainty quantification for the machine learning algorithms that are commonly applied in tree-fitting. We argue that practitioners should look to ensembles of trees (forests) rather than individual trees in their analysis. The ideas are applied on and illustrated through an example experiment involving 21 million unique users of EBay.com.

</details>

<details>

<summary>2015-12-18 08:51:36 - Bayesian Inference of Online Social Network Statistics via Lightweight Random Walk Crawls</summary>

- *Konstantin Avrachenkov, Bruno Ribeiro, Jithin K. Sreedharan*

- `1510.05407v2` - [abs](http://arxiv.org/abs/1510.05407v2) - [pdf](http://arxiv.org/pdf/1510.05407v2)

> Online social networks (OSN) contain extensive amount of information about the underlying society that is yet to be explored. One of the most feasible technique to fetch information from OSN, crawling through Application Programming Interface (API) requests, poses serious concerns over the the guarantees of the estimates. In this work, we focus on making reliable statistical inference with limited API crawls. Based on regenerative properties of the random walks, we propose an unbiased estimator for the aggregated sum of functions over edges and proved the connection between variance of the estimator and spectral gap. In order to facilitate Bayesian inference on the true value of the estimator, we derive the approximate posterior distribution of the estimate. Later the proposed ideas are validated with numerical experiments on inference problems in real-world networks.

</details>

<details>

<summary>2015-12-18 11:53:11 - A novel Bayesian strategy for the identification of spatially-varying material properties and model validation: an application to static elastography</summary>

- *P. S. Koutsourelakis*

- `1512.05913v1` - [abs](http://arxiv.org/abs/1512.05913v1) - [pdf](http://arxiv.org/pdf/1512.05913v1)

> The present paper proposes a novel Bayesian, computational strategy in the context of model-based inverse problems in elastostatics. On one hand we attempt to provide probabilistic estimates of the material properties and their spatial variability that account for the various sources of uncertainty. On the other hand we attempt to address the question of model fidelity in relation to the experimental reality and particularly in the context of the material constitutive law adopted. This is especially important in biomedical settings when the inferred material properties will be used to make decisions/diagnoses. We propose an expanded parametrization that enables the quantification of model discrepancies in addition to the constitutive parameters. We propose scalable computational strategies for carrying out inference and learning tasks and demonstrate their effectiveness in numerical examples with noiseless and noisy synthetic data.

</details>

<details>

<summary>2015-12-18 19:37:24 - Bayesian anti-sparse coding</summary>

- *Clément Elvira, Pierre Chainais, Nicolas Dobigeon*

- `1512.06086v1` - [abs](http://arxiv.org/abs/1512.06086v1) - [pdf](http://arxiv.org/pdf/1512.06086v1)

> Sparse representations have proven their efficiency in solving a wide class of inverse problems encountered in signal and image processing. Conversely, enforcing the information to be spread uniformly over representation coefficients exhibits relevant properties in various applications such as digital communications. Anti-sparse regularization can be naturally expressed through an $\ell_{\infty}$-norm penalty. This paper derives a probabilistic formulation of such representations. A new probability distribution, referred to as the democratic prior, is first introduced. Its main properties as well as three random variate generators for this distribution are derived. Then this probability distribution is used as a prior to promote anti-sparsity in a Gaussian linear inverse problem, yielding a fully Bayesian formulation of anti-sparse coding. Two Markov chain Monte Carlo (MCMC) algorithms are proposed to generate samples according to the posterior distribution. The first one is a standard Gibbs sampler. The second one uses Metropolis-Hastings moves that exploit the proximity mapping of the log-posterior distribution. These samples are used to approximate maximum a posteriori and minimum mean square error estimators of both parameters and hyperparameters. Simulations on synthetic data illustrate the performances of the two proposed samplers, for both complete and over-complete dictionaries. All results are compared to the recent deterministic variational FITRA algorithm.

</details>

<details>

<summary>2015-12-19 10:00:01 - Bayesian bivariate meta-analysis of diagnostic test studies with interpretable priors</summary>

- *Jingyi Guo, Håvard Rue, Andrea Riebler*

- `1512.06217v1` - [abs](http://arxiv.org/abs/1512.06217v1) - [pdf](http://arxiv.org/pdf/1512.06217v1)

> In a bivariate meta-analysis the number of diagnostic studies involved is often very low so that frequentist methods may result in problems. Bayesian inference is attractive as informative priors that add small amount of information can stabilise the analysis without overwhelming the data. However, Bayesian analysis is often computationally demanding and the selection of the prior for the covariance matrix of the bivariate structure is crucial with little data. The integrated nested Laplace approximations (INLA) method provides an efficient solution to the computational issues by avoiding any sampling, but the important question of priors remain. We explore the penalised complexity (PC) prior framework for specifying informative priors for the variance parameters and the correlation parameter. PC priors facilitate model interpretation and hyperparameter specification as expert knowledge can be incorporated intuitively. We conduct a simulation study to compare the properties and behaviour of differently defined PC priors to currently used priors in the field. The simulation study shows that the use of PC priors results in more precise estimates when specified in a sensible neighbourhood around the truth. To investigate the usage of PC priors in practice we reanalyse a meta-analysis using the telomerase marker for the diagnosis of bladder cancer.

</details>

<details>

<summary>2015-12-20 16:07:38 - Variational Dropout and the Local Reparameterization Trick</summary>

- *Diederik P. Kingma, Tim Salimans, Max Welling*

- `1506.02557v2` - [abs](http://arxiv.org/abs/1506.02557v2) - [pdf](http://arxiv.org/pdf/1506.02557v2)

> We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.

</details>

<details>

<summary>2015-12-22 09:22:39 - On the Differential Privacy of Bayesian Inference</summary>

- *Zuhe Zhang, Benjamin Rubinstein, Christos Dimitrakakis*

- `1512.06992v1` - [abs](http://arxiv.org/abs/1512.06992v1) - [pdf](http://arxiv.org/pdf/1512.06992v1)

> We study how to communicate findings of Bayesian inference to third parties, while preserving the strong guarantee of differential privacy. Our main contributions are four different algorithms for private Bayesian inference on proba-bilistic graphical models. These include two mechanisms for adding noise to the Bayesian updates, either directly to the posterior parameters, or to their Fourier transform so as to preserve update consistency. We also utilise a recently introduced posterior sampling mechanism, for which we prove bounds for the specific but general case of discrete Bayesian networks; and we introduce a maximum-a-posteriori private mechanism. Our analysis includes utility and privacy bounds, with a novel focus on the influence of graph structure on privacy. Worked examples and experiments with Bayesian na{\"i}ve Bayes and Bayesian linear regression illustrate the application of our mechanisms.

</details>

<details>

<summary>2015-12-22 09:59:33 - A Stochastically Evolving Non-local Search and Solutions to Inverse Problems with Sparse Data</summary>

- *Mamatha Venugopal, Ram Mohan Vasu, Debasish Roy*

- `1512.07008v1` - [abs](http://arxiv.org/abs/1512.07008v1) - [pdf](http://arxiv.org/pdf/1512.07008v1)

> Building upon our earlier work of a martingale approach to global optimization, a powerful stochastic search scheme for the global optimum of cost functions is proposed on the basis of change of measures on the states that evolve as diffusion processes and splitting of the state-space along the lines of a Bayesian game. To begin with, the efficacy of the optimizer, when contrasted with one of the most efficient existing schemes, is assessed against a family of Np-hard benchmark problems. Then, using both simulated- and experimental data, potentialities of the new proposal are further explored in the context of an inverse problem of significance in medical imaging, wherein the superior reconstruction features of a global search vis-\`a-vis the commonly adopted local or quasi-local schemes are brought into relief.

</details>

<details>

<summary>2015-12-22 21:31:45 - Data-dependent Posterior Propriety of Bayesian Beta-Binomial-Logit Model</summary>

- *Hyungsuk Tak, Carl N. Morris*

- `1512.07267v1` - [abs](http://arxiv.org/abs/1512.07267v1) - [pdf](http://arxiv.org/pdf/1512.07267v1)

> A Beta-Binomial-Logit model is a Beta-Binomial model with covariate information incorporated via a logistic regression. Posterior propriety of a Bayesian Beta-Binomial-Logit model can be data-dependent for improper hyper-prior distributions. Various researchers in the literature have unknowingly used improper posterior distributions or have given incorrect statements about posterior propriety because checking posterior propriety can be challenging due to the complicated functional form of a Beta-Binomial-Logit model. We derive data-dependent necessary and sufficient conditions for posterior propriety within a class of hyper-prior distributions that encompass those used in previous studies.

</details>

<details>

<summary>2015-12-22 21:48:33 - Computationally Efficient Distribution Theory for Bayesian Inference of High-Dimensional Dependent Count-Valued Data</summary>

- *Jonathan R. Bradley, Scott H. Holan, Christopher K. Wikle*

- `1512.07273v1` - [abs](http://arxiv.org/abs/1512.07273v1) - [pdf](http://arxiv.org/pdf/1512.07273v1)

> We introduce a Bayesian approach for multivariate spatio-temporal prediction for high-dimensional count-valued data. Our primary interest is when there are possibly millions of data points referenced over different variables, geographic regions, and times. This problem requires extensive methodological advancements, as jointly modeling correlated data of this size leads to the so-called "big n problem." The computational complexity of prediction in this setting is further exacerbated by acknowledging that count-valued data are naturally non-Gaussian. Thus, we develop a new computationally efficient distribution theory for this setting. In particular, we introduce a multivariate log-gamma distribution and provide substantial theoretical development including: results regarding conditional distributions, marginal distributions, an asymptotic relationship with the multivariate normal distribution, and full-conditional distributions for a Gibbs sampler. To incorporate dependence between variables, regions, and time points, a multivariate spatio-temporal mixed effects model (MSTM) is used. The results in this manuscript are extremely general, and can be used for data that exhibit fewer sources of dependency than what we consider (e.g., multivariate, spatial-only, or spatio-temporal-only data). Hence, the implications of our modeling framework may have a large impact on the general problem of jointly modeling correlated count-valued data. We show the effectiveness of our approach through a simulation study. Additionally, we demonstrate our proposed methodology with an important application analyzing data obtained from the Longitudinal Employer-Household Dynamics (LEHD) program, which is administered by the U.S. Census Bureau.

</details>

<details>

<summary>2015-12-23 02:19:35 - Marginal likelihood and model selection for Gaussian latent tree and forest models</summary>

- *Mathias Drton, Shaowei Lin, Luca Weihs, Piotr Zwiernik*

- `1412.8285v2` - [abs](http://arxiv.org/abs/1412.8285v2) - [pdf](http://arxiv.org/pdf/1412.8285v2)

> Gaussian latent tree models, or more generally, Gaussian latent forest models have Fisher-information matrices that become singular along interesting submodels, namely, models that correspond to subforests. For these singularities, we compute the real log-canonical thresholds (also known as stochastic complexities or learning coefficients) that quantify the large-sample behavior of the marginal likelihood in Bayesian inference. This provides the information needed for a recently introduced generalization of the Bayesian information criterion. Our mathematical developments treat the general setting of Laplace integrals whose phase functions are sums of squared differences between monomials and constants. We clarify how in this case real log-canonical thresholds can be computed using polyhedral geometry, and we show how to apply the general theory to the Laplace integrals associated with Gaussian latent tree and forest models. In simulations and a data example, we demonstrate how the mathematical knowledge can be applied in model selection.

</details>

<details>

<summary>2015-12-23 03:10:29 - A Deep Generative Deconvolutional Image Model</summary>

- *Yunchen Pu, Xin Yuan, Andrew Stevens, Chunyuan Li, Lawrence Carin*

- `1512.07344v1` - [abs](http://arxiv.org/abs/1512.07344v1) - [pdf](http://arxiv.org/pdf/1512.07344v1)

> A deep generative model is developed for representation and analysis of images, based on a hierarchical convolutional dictionary-learning framework. Stochastic {\em unpooling} is employed to link consecutive layers in the model, yielding top-down image generation. A Bayesian support vector machine is linked to the top-layer features, yielding max-margin discrimination. Deep deconvolutional inference is employed when testing, to infer the latent features, and the top-layer features are connected with the max-margin classifier for discrimination tasks. The model is efficiently trained using a Monte Carlo expectation-maximization (MCEM) algorithm, with implementation on graphical processor units (GPUs) for efficient large-scale learning, and fast testing. Excellent results are obtained on several benchmark datasets, including ImageNet, demonstrating that the proposed model achieves results that are highly competitive with similarly sized convolutional neural networks.

</details>

<details>

<summary>2015-12-23 23:21:40 - High-Order Stochastic Gradient Thermostats for Bayesian Learning of Deep Models</summary>

- *Chunyuan Li, Changyou Chen, Kai Fan, Lawrence Carin*

- `1512.07662v1` - [abs](http://arxiv.org/abs/1512.07662v1) - [pdf](http://arxiv.org/pdf/1512.07662v1)

> Learning in deep models using Bayesian methods has generated significant attention recently. This is largely because of the feasibility of modern Bayesian methods to yield scalable learning and inference, while maintaining a measure of uncertainty in the model parameters. Stochastic gradient MCMC algorithms (SG-MCMC) are a family of diffusion-based sampling methods for large-scale Bayesian learning. In SG-MCMC, multivariate stochastic gradient thermostats (mSGNHT) augment each parameter of interest, with a momentum and a thermostat variable to maintain stationary distributions as target posterior distributions. As the number of variables in a continuous-time diffusion increases, its numerical approximation error becomes a practical bottleneck, so better use of a numerical integrator is desirable. To this end, we propose use of an efficient symmetric splitting integrator in mSGNHT, instead of the traditional Euler integrator. We demonstrate that the proposed scheme is more accurate, robust, and converges faster. These properties are demonstrated to be desirable in Bayesian deep learning. Extensive experiments on two canonical models and their deep extensions demonstrate that the proposed scheme improves general Bayesian posterior sampling, particularly for deep models.

</details>

<details>

<summary>2015-12-23 23:45:03 - Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks</summary>

- *Chunyuan Li, Changyou Chen, David Carlson, Lawrence Carin*

- `1512.07666v1` - [abs](http://arxiv.org/abs/1512.07666v1) - [pdf](http://arxiv.org/pdf/1512.07666v1)

> Effective training of deep neural networks suffers from two main issues. The first is that the parameter spaces of these models exhibit pathological curvature. Recent methods address this problem by using adaptive preconditioning for Stochastic Gradient Descent (SGD). These methods improve convergence by adapting to the local geometry of parameter space. A second issue is overfitting, which is typically addressed by early stopping. However, recent work has demonstrated that Bayesian model averaging mitigates this problem. The posterior can be sampled by using Stochastic Gradient Langevin Dynamics (SGLD). However, the rapidly changing curvature renders default SGLD methods inefficient. Here, we propose combining adaptive preconditioners with SGLD. In support of this idea, we give theoretical properties on asymptotic convergence and predictive risk. We also provide empirical results for Logistic Regression, Feedforward Neural Nets, and Convolutional Neural Nets, demonstrating that our preconditioned SGLD method gives state-of-the-art performance on these models.

</details>

<details>

<summary>2015-12-25 00:11:51 - A scalable quasi-Bayesian framework for Gaussian graphical models</summary>

- *Yves Atchade*

- `1512.07934v1` - [abs](http://arxiv.org/abs/1512.07934v1) - [pdf](http://arxiv.org/pdf/1512.07934v1)

> This paper deals with the Bayesian estimation of high dimensional Gaussian graphical models. We develop a quasi-Bayesian implementation of the neighborhood selection method of Meinshausen and Buhlmann (2006) for the estimation of Gaussian graphical models. The method produces a product-form quasi-posterior distribution that can be efficiently explored by parallel computing. We derive a non-asymptotic bound on the contraction rate of the quasi-posterior distribution. The result shows that the proposed quasi-posterior distribution contracts towards the true precision matrix at a rate given by the worst contraction rate of the linear regressions that are involved in the neighborhood selection. We develop a Markov Chain Monte Carlo algorithm for approximate computations, following an approach from Atchade (2015). We illustrate the methodology with a simulation study. The results show that the proposed method can fit Gaussian graphical models at a scale unmatched by other Bayesian methods for graphical models.

</details>

<details>

<summary>2015-12-25 05:30:20 - Histogram Meets Topic Model: Density Estimation by Mixture of Histograms</summary>

- *Hideaki Kim, Hiroshi Sawada*

- `1512.07960v1` - [abs](http://arxiv.org/abs/1512.07960v1) - [pdf](http://arxiv.org/pdf/1512.07960v1)

> The histogram method is a powerful non-parametric approach for estimating the probability density function of a continuous variable. But the construction of a histogram, compared to the parametric approaches, demands a large number of observations to capture the underlying density function. Thus it is not suitable for analyzing a sparse data set, a collection of units with a small size of data. In this paper, by employing the probabilistic topic model, we develop a novel Bayesian approach to alleviating the sparsity problem in the conventional histogram estimation. Our method estimates a unit's density function as a mixture of basis histograms, in which the number of bins for each basis, as well as their heights, is determined automatically. The estimation procedure is performed by using the fast and easy-to-implement collapsed Gibbs sampling. We apply the proposed method to synthetic data, showing that it performs well.

</details>

<details>

<summary>2015-12-26 16:57:55 - K2-ABC: Approximate Bayesian Computation with Kernel Embeddings</summary>

- *Mijung Park, Wittawat Jitkrittum, Dino Sejdinovic*

- `1502.02558v4` - [abs](http://arxiv.org/abs/1502.02558v4) - [pdf](http://arxiv.org/pdf/1502.02558v4)

> Complicated generative models often result in a situation where computing the likelihood of observed data is intractable, while simulating from the conditional density given a parameter value is relatively easy. Approximate Bayesian Computation (ABC) is a paradigm that enables simulation-based posterior inference in such cases by measuring the similarity between simulated and observed data in terms of a chosen set of summary statistics. However, there is no general rule to construct sufficient summary statistics for complex models. Insufficient summary statistics will "leak" information, which leads to ABC algorithms yielding samples from an incorrect (partial) posterior. In this paper, we propose a fully nonparametric ABC paradigm which circumvents the need for manually selecting summary statistics. Our approach, K2-ABC, uses maximum mean discrepancy (MMD) as a dissimilarity measure between the distributions over observed and simulated data. MMD is easily estimated as the squared difference between their empirical kernel embeddings. Experiments on a simulated scenario and a real-world biological problem illustrate the effectiveness of the proposed algorithm.

</details>

<details>

<summary>2015-12-28 23:05:26 - Rejection Odds and Rejection Ratios: A Proposal for Statistical Practice in Testing Hypotheses</summary>

- *M. J. Bayarri, Daniel J. Benjamin, James O. Berger, Thomas M. Sellke*

- `1512.08552v1` - [abs](http://arxiv.org/abs/1512.08552v1) - [pdf](http://arxiv.org/pdf/1512.08552v1)

> Much of science is (rightly or wrongly) driven by hypothesis testing. Even in situations where the hypothesis testing paradigm is correct, the common practice of basing inferences solely on p-values has been under intense criticism for over 50 years. We propose, as an alternative, the use of the odds of a correct rejection of the null hypothesis to incorrect rejection. Both pre-experimental versions (involving the power and Type I error) and post-experimental versions (depending on the actual data) are considered. Implementations are provided that range from depending only on the p-value to consideration of full Bayesian analysis. A surprise is that all implementations -- even the full Bayesian analysis -- have complete frequentist justification. Versions of our proposal can be implemented that require only minor modifications to existing practices yet overcome some of their most severe shortcomings.

</details>

<details>

<summary>2015-12-29 08:36:57 - A Relaxed Drift Diffusion Model for Phylogenetic Trait Evolution</summary>

- *Mandev S. Gill, Lam Si Tung Ho, Guy Baele, Philippe Lemey, Marc A. Suchard*

- `1512.07948v2` - [abs](http://arxiv.org/abs/1512.07948v2) - [pdf](http://arxiv.org/pdf/1512.07948v2)

> Understanding the processes that give rise to quantitative measurements associated with molecular sequence data remains an important issue in statistical phylogenetics. Examples of such measurements include geographic coordinates in the context of phylogeography and phenotypic traits in the context of comparative studies. A popular approach is to model the evolution of continuously varying traits as a Brownian diffusion process. However, standard Brownian diffusion is quite restrictive and may not accurately characterize certain trait evolutionary processes. Here, we relax one of the major restrictions of standard Brownian diffusion by incorporating a nontrivial estimable drift into the process. We introduce a relaxed drift diffusion model for the evolution of multivariate continuously varying traits along a phylogenetic tree via Brownian diffusion with drift. Notably, the relaxed drift model accommodates branch-specific variation of drift rates while preserving model identifiability. We implement the relaxed drift model in a Bayesian inference framework to simultaneously reconstruct the evolutionary histories of molecular sequence data and associated multivariate continuous trait data, and provide tools to visualize evolutionary reconstructions. We illustrate our approach in three viral examples. In the first two, we examine the spatiotemporal spread of HIV-1 in central Africa and West Nile virus in North America and show that a relaxed drift approach uncovers a clearer, more detailed picture of the dynamics of viral dispersal than standard Brownian diffusion. Finally, we study antigenic evolution in the context of HIV-1 resistance to three broadly neutralizing antibodies. Our analysis reveals evidence of a continuous drift at the HIV-1 population level towards enhanced resistance to neutralization by the VRC01 monoclonal antibody over the course of the epidemic.

</details>

<details>

<summary>2015-12-30 15:56:40 - Infinite Edge Partition Models for Overlapping Community Detection and Link Prediction</summary>

- *Mingyuan Zhou*

- `1501.06218v2` - [abs](http://arxiv.org/abs/1501.06218v2) - [pdf](http://arxiv.org/pdf/1501.06218v2)

> A hierarchical gamma process infinite edge partition model is proposed to factorize the binary adjacency matrix of an unweighted undirected relational network under a Bernoulli-Poisson link. The model describes both homophily and stochastic equivalence, and is scalable to big sparse networks by focusing its computation on pairs of linked nodes. It can not only discover overlapping communities and inter-community interactions, but also predict missing edges. A simplified version omitting inter-community interactions is also provided and we reveal its interesting connections to existing models. The number of communities is automatically inferred in a nonparametric Bayesian manner, and efficient inference via Gibbs sampling is derived using novel data augmentation techniques. Experimental results on four real networks demonstrate the models' scalability and state-of-the-art performance.

</details>

<details>

<summary>2015-12-30 16:28:55 - Nonparametric Bayesian Factor Analysis for Dynamic Count Matrices</summary>

- *Ayan Acharya, Joydeep Ghosh, Mingyuan Zhou*

- `1512.08996v1` - [abs](http://arxiv.org/abs/1512.08996v1) - [pdf](http://arxiv.org/pdf/1512.08996v1)

> A gamma process dynamic Poisson factor analysis model is proposed to factorize a dynamic count matrix, whose columns are sequentially observed count vectors. The model builds a novel Markov chain that sends the latent gamma random variables at time $(t-1)$ as the shape parameters of those at time $t$, which are linked to observed or latent counts under the Poisson likelihood. The significant challenge of inferring the gamma shape parameters is fully addressed, using unique data augmentation and marginalization techniques for the negative binomial distribution. The same nonparametric Bayesian model also applies to the factorization of a dynamic binary matrix, via a Bernoulli-Poisson link that connects a binary observation to a latent count, with closed-form conditional posteriors for the latent counts and efficient computation for sparse observations. We apply the model to text and music analysis, with state-of-the-art results.

</details>

<details>

<summary>2015-12-31 03:09:33 - Bayes-Optimal Effort Allocation in Crowdsourcing: Bounds and Index Policies</summary>

- *Weici Hu, Peter I. Frazier*

- `1512.09204v1` - [abs](http://arxiv.org/abs/1512.09204v1) - [pdf](http://arxiv.org/pdf/1512.09204v1)

> We consider effort allocation in crowdsourcing, where we wish to assign labeling tasks to imperfect homogeneous crowd workers to maximize overall accuracy in a continuous-time Bayesian setting, subject to budget and time constraints. The Bayes-optimal policy for this problem is the solution to a partially observable Markov decision process, but the curse of dimensionality renders the computation infeasible. Based on the Lagrangian Relaxation technique in Adelman & Mersereau (2008), we provide a computationally tractable instance-specific upper bound on the value of this Bayes-optimal policy, which can in turn be used to bound the optimality gap of any other sub-optimal policy. In an approach similar in spirit to the Whittle index for restless multiarmed bandits, we provide an index policy for effort allocation in crowdsourcing and demonstrate numerically that it outperforms other stateof- arts and performs close to optimal solution.

</details>

<details>

<summary>2015-12-31 15:19:21 - Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models</summary>

- *Michael U. Gutmann, Jukka Corander*

- `1501.03291v3` - [abs](http://arxiv.org/abs/1501.03291v3) - [pdf](http://arxiv.org/pdf/1501.03291v3)

> Our paper deals with inferring simulator-based statistical models given some observed data. A simulator-based model is a parametrized mechanism which specifies how data are generated. It is thus also referred to as generative model. We assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. This weak assumption is useful for devising realistic models but it renders statistical inference very difficult. The main challenge is the intractability of the likelihood function. Several likelihood-free inference methods have been proposed which share the basic idea of identifying the parameters by finding values for which the discrepancy between simulated and observed data is small. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude.

</details>

