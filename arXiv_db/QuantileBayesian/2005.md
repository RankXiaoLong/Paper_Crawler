# 2005

## TOC

- [2005-03](#2005-03)
- [2005-04](#2005-04)
- [2005-05](#2005-05)
- [2005-06](#2005-06)
- [2005-07](#2005-07)
- [2005-08](#2005-08)
- [2005-09](#2005-09)
- [2005-10](#2005-10)
- [2005-11](#2005-11)

## 2005-03

<details>

<summary>2005-03-19 03:42:38 - Baysian inference via classes of normalized random measures</summary>

- *Lancelot F. James, Antonio Lijoi, Igor Pruenster*

- `0503394v1` - [abs](http://arxiv.org/abs/0503394v1) - [pdf](http://arxiv.org/pdf/math/0503394v1)

> One of the main research areas in Bayesian Nonparametrics is the proposal and study of priors which generalize the Dirichlet process. Here we exploit theoretical properties of Poisson random measures in order to provide a comprehensive Bayesian analysis of random probabilities which are obtained by an appropriate normalization. Specifically we achieve explicit and tractable forms of the posterior and the marginal distributions, including an explicit and easily used description of generalizations of the important Blackwell-MacQueen P\'olya urn distribution. Such simplifications are achieved by the use of a latent variable which admits quite interesting interpretations which allow to gain a better understanding of the behaviour of these random probability measures. It is noteworthy that these models are generalizations of models considered by Kingman (1975) in a non-Bayesian context. Such models are known to play a significant role in a variety of applications including genetics, physics, and work involving random mappings and assemblies. Hence our analysis is of utility in those contexts as well. We also show how our results may be applied to Bayesian mixture models and describe computational schemes which are generalizations of known efficient methods for the case of the Dirichlet process. We illustrate new examples of processes which can play the role of priors for Bayesian nonparametric inference and finally point out some interesting connections with the theory of generalized gamma convolutions initiated by Thorin and further developed by Bondesson.

</details>

<details>

<summary>2005-03-29 10:45:51 - New approaches to Bayesian consistency</summary>

- *Stephen Walker*

- `0503672v1` - [abs](http://arxiv.org/abs/0503672v1) - [pdf](http://arxiv.org/pdf/math/0503672v1)

> We use martingales to study Bayesian consistency. We derive sufficient conditions for both Hellinger and Kullback-Leibler consistency, which do not rely on the use of a sieve. Alternative sufficient conditions for Hellinger consistency are also found and demonstrated on examples.

</details>

<details>

<summary>2005-03-29 10:56:15 - On the posterior distribution of the number of components in a finite mixture</summary>

- *Agostino Nobile*

- `0503673v1` - [abs](http://arxiv.org/abs/0503673v1) - [pdf](http://arxiv.org/pdf/math/0503673v1)

> The posterior distribution of the number of components k in a finite mixture satisfies a set of inequality constraints. The result holds irrespective of the parametric form of the mixture components and under assumptions on the prior distribution weaker than those routinely made in the literature on Bayesian analysis of finite mixtures. The inequality constraints can be used to perform an ``internal'' consistency check of MCMC estimates of the posterior distribution of k and to provide improved estimates which are required to satisfy the constraints. Bounds on the posterior probability of k components are derived using the constraints. Implications on prior distribution specification and on the adequacy of the posterior distribution of k as a tool for selecting an adequate number of components in the mixture are also explored.

</details>


## 2005-04

<details>

<summary>2005-04-25 12:27:20 - Generalized bootstrap for estimating equations</summary>

- *Snigdhansu Chatterjee, Arup Bose*

- `0504515v1` - [abs](http://arxiv.org/abs/0504515v1) - [pdf](http://arxiv.org/pdf/math/0504515v1)

> We introduce a generalized bootstrap technique for estimators obtained by solving estimating equations. Some special cases of this generalized bootstrap are the classical bootstrap of Efron, the delete-d jackknife and variations of the Bayesian bootstrap. The use of the proposed technique is discussed in some examples. Distributional consistency of the method is established and an asymptotic representation of the resampling variance estimator is obtained.

</details>

<details>

<summary>2005-04-25 12:37:14 - Iterated smoothed bootstrap confidence intervals for population quantiles</summary>

- *Yvonne H. S. Ho, Stephen M. S. Lee*

- `0504516v1` - [abs](http://arxiv.org/abs/0504516v1) - [pdf](http://arxiv.org/pdf/math/0504516v1)

> This paper investigates the effects of smoothed bootstrap iterations on coverage probabilities of smoothed bootstrap and bootstrap-t confidence intervals for population quantiles, and establishes the optimal kernel bandwidths at various stages of the smoothing procedures. The conventional smoothed bootstrap and bootstrap-t methods have been known to yield one-sided coverage errors of orders O(n^{-1/2}) and o(n^{-2/3}), respectively, for intervals based on the sample quantile of a random sample of size n. We sharpen the latter result to O(n^{-5/6}) with proper choices of bandwidths at the bootstrapping and Studentization steps. We show further that calibration of the nominal coverage level by means of the iterated bootstrap succeeds in reducing the coverage error of the smoothed bootstrap percentile interval to the order O(n^{-2/3}) and that of the smoothed bootstrap-t interval to O(n^{-58/57}), provided that bandwidths are selected of appropriate orders. Simulation results confirm our asymptotic findings, suggesting that the iterated smoothed bootstrap-t method yields the most accurate coverage. On the other hand, the iterated smoothed bootstrap percentile method interval has the advantage of being shorter and more stable than the bootstrap-t intervals.

</details>


## 2005-05

<details>

<summary>2005-05-27 09:51:05 - Default priors for Gaussian processes</summary>

- *Rui Paulo*

- `0505603v1` - [abs](http://arxiv.org/abs/0505603v1) - [pdf](http://arxiv.org/pdf/math/0505603v1)

> Motivated by the statistical evaluation of complex computer models, we deal with the issue of objective prior specification for the parameters of Gaussian processes. In particular, we derive the Jeffreys-rule, independence Jeffreys and reference priors for this situation, and prove that the resulting posterior distributions are proper under a quite general set of conditions. A proper flat prior strategy, based on maximum likelihood estimates, is also considered, and all priors are then compared on the grounds of the frequentist properties of the ensuing Bayesian procedures. Computational issues are also addressed in the paper, and we illustrate the proposed solutions by means of an example taken from the field of complex computer model validation.

</details>

<details>

<summary>2005-05-27 11:48:39 - Functionals of Dirichlet processes, the Cifarelli-Regazzini identity and Beta-Gamma processes</summary>

- *Lancelot F. James*

- `0505606v1` - [abs](http://arxiv.org/abs/0505606v1) - [pdf](http://arxiv.org/pdf/math/0505606v1)

> Suppose that P_{\theta}(g) is a linear functional of a Dirichlet process with shape \theta H, where \theta >0 is the total mass and H is a fixed probability measure. This paper describes how one can use the well-known Bayesian prior to posterior analysis of the Dirichlet process, and a posterior calculus for Gamma processes to ascertain properties of linear functionals of Dirichlet processes. In particular, in conjunction with a Gamma identity, we show easily that a generalized Cauchy-Stieltjes transform of a linear functional of a Dirichlet process is equivalent to the Laplace functional of a class of, what we define as, Beta-Gamma processes. This represents a generalization of an identity due to Cifarelli and Regazzini, which is also known as the Markov-Krein identity for mean functionals of Dirichlet processes. These results also provide new explanations and interpretations of results in the literature. The identities are analogues to quite useful identities for Beta and Gamma random variables. We give a result which can be used to ascertain specifications on H such that the Dirichlet functional is Beta distributed. This avoids the need for an inversion formula for these cases and points to the special nature of the Dirichlet process, and indeed the functional Beta-Gamma calculus developed in this paper.

</details>

<details>

<summary>2005-05-30 07:05:37 - Multiprocess parallel antithetic coupling for backward and forward Markov Chain Monte Carlo</summary>

- *Radu V. Craiu, Xiao-Li Meng*

- `0505631v1` - [abs](http://arxiv.org/abs/0505631v1) - [pdf](http://arxiv.org/pdf/math/0505631v1)

> Antithetic coupling is a general stratification strategy for reducing Monte Carlo variance without increasing the simulation size. The use of the antithetic principle in the Monte Carlo literature typically employs two strata via antithetic quantile coupling. We demonstrate here that further stratification, obtained by using k>2 (e.g., k=3-10) antithetically coupled variates, can offer substantial additional gain in Monte Carlo efficiency, in terms of both variance and bias. The reason for reduced bias is that antithetically coupled chains can provide a more dispersed search of the state space than multiple independent chains. The emerging area of perfect simulation provides a perfect setting for implementing the k-process parallel antithetic coupling for MCMC because, without antithetic coupling, this class of methods delivers genuine independent draws. Furthermore, antithetic backward coupling provides a very convenient theoretical tool for investigating antithetic forward coupling. However, the generation of k>2 antithetic variates that are negatively associated, that is, they preserve negative correlation under monotone transformations, and extremely antithetic, that is, they are as negatively correlated as possible, is more complicated compared to the case with k=2. In this paper, we establish a theoretical framework for investigating such issues. Among the generating methods that we compare, Latin hypercube sampling and its iterative extension appear to be general-purpose choices, making another direct link between Monte Carlo and quasi Monte Carlo.

</details>

<details>

<summary>2005-05-30 08:03:43 - Spike and slab variable selection: Frequentist and Bayesian strategies</summary>

- *Hemant Ishwaran, J. Sunil Rao*

- `0505633v1` - [abs](http://arxiv.org/abs/0505633v1) - [pdf](http://arxiv.org/pdf/math/0505633v1)

> Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure's ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty.

</details>

<details>

<summary>2005-05-30 09:57:15 - Extremal quantile regression</summary>

- *Victor Chernozhukov*

- `0505639v1` - [abs](http://arxiv.org/abs/0505639v1) - [pdf](http://arxiv.org/pdf/math/0505639v1)

> Quantile regression is an important tool for estimation of conditional quantiles of a response Y given a vector of covariates X. It can be used to measure the effect of covariates not only in the center of a distribution, but also in the upper and lower tails. This paper develops a theory of quantile regression in the tails. Specifically, it obtains the large sample properties of extremal (extreme order and intermediate order) quantile regression estimators for the linear quantile regression model with the tails restricted to the domain of minimum attraction and closed under tail equivalence across regressor values. This modeling setup combines restrictions of extreme value theory with leading homoscedastic and heteroscedastic linear specifications of regression analysis. In large samples, extreme order regression quantiles converge weakly to \argmin functionals of stochastic integrals of Poisson processes that depend on regressors, while intermediate regression quantiles and their functionals converge to normal vectors with variance matrices dependent on the tail parameters and the regressor design.

</details>


## 2005-06

<details>

<summary>2005-06-08 09:07:23 - Asymptotics of Discrete MDL for Online Prediction</summary>

- *Jan Poland, Marcus Hutter*

- `0506022v1` - [abs](http://arxiv.org/abs/0506022v1) - [pdf](http://arxiv.org/pdf/cs/0506022v1)

> Minimum Description Length (MDL) is an important principle for induction and prediction, with strong relations to optimal Bayesian learning. This paper deals with learning non-i.i.d. processes by means of two-part MDL, where the underlying model class is countable. We consider the online learning framework, i.e. observations come in one by one, and the predictor is allowed to update his state of mind after each time step. We identify two ways of predicting by MDL for this setup, namely a static} and a dynamic one. (A third variant, hybrid MDL, will turn out inferior.) We will prove that under the only assumption that the data is generated by a distribution contained in the model class, the MDL predictions converge to the true values almost surely. This is accomplished by proving finite bounds on the quadratic, the Hellinger, and the Kullback-Leibler loss of the MDL learner, which are however exponentially worse than for Bayesian prediction. We demonstrate that these bounds are sharp, even for model classes containing only Bernoulli distributions. We show how these bounds imply regret bounds for arbitrary loss functions. Our results apply to a wide range of setups, namely sequence prediction, pattern classification, regression, and universal induction in the sense of Algorithmic Information Theory among others.

</details>

<details>

<summary>2005-06-15 10:53:08 - Invariant Bayesian estimation on manifolds</summary>

- *Ian H. Jermyn*

- `0506296v1` - [abs](http://arxiv.org/abs/0506296v1) - [pdf](http://arxiv.org/pdf/math/0506296v1)

> A frequent and well-founded criticism of the maximum a posteriori (MAP) and minimum mean squared error (MMSE) estimates of a continuous parameter \gamma taking values in a differentiable manifold \Gamma is that they are not invariant to arbitrary ``reparameterizations'' of \Gamma. This paper clarifies the issues surrounding this problem, by pointing out the difference between coordinate invariance, which is a sine qua non for a mathematically well-defined problem, and diffeomorphism invariance, which is a substantial issue, and then provides a solution. We first show that the presence of a metric structure on \Gamma can be used to define coordinate-invariant MAP and MMSE estimates, and we argue that this is the natural way to proceed. We then discuss the choice of a metric structure on \Gamma. By imposing an invariance criterion natural within a Bayesian framework, we show that this choice is essentially unique. It does not necessarily correspond to a choice of coordinates. In cases of complete prior ignorance, when Jeffreys' prior is used, the invariant MAP estimate reduces to the maximum likelihood estimate. The invariant MAP estimate coincides with the minimum message length (MML) estimate, but no discretization or approximation is used in its derivation.

</details>


## 2005-07

<details>

<summary>2005-07-01 14:37:22 - Bayesian alignment using hierarchical models, with applications in protein bioinformatics</summary>

- *Peter J. Green, Kanti Mardia*

- `0503712v2` - [abs](http://arxiv.org/abs/0503712v2) - [pdf](http://arxiv.org/pdf/math/0503712v2)

> An important problem in shape analysis is to match configurations of points in space filtering out some geometrical transformation. In this paper we introduce hierarchical models for such tasks, in which the points in the configurations are either unlabelled, or have at most a partial labelling constraining the matching, and in which some points may only appear in one of the configurations. We derive procedures for simultaneous inference about the matching and the transformation, using a Bayesian approach. Our model is based on a Poisson process for hidden true point locations; this leads to considerable mathematical simplification and efficiency of implementation. We find a novel use for classic distributions from directional statistics in a conditionally conjugate specification for the case where the geometrical transformation includes an unknown rotation. Throughout, we focus on the case of affine or rigid motion transformations. Under a broad parametric family of loss functions, an optimal Bayesian point estimate of the matching matrix can be constructed, that depends only on a single parameter of the family.   Our methods are illustrated by two applications from bioinformatics. The first problem is of matching protein gels in 2 dimensions, and the second consists of aligning active sites of proteins in 3 dimensions. In the latter case, we also use information related to the grouping of the amino acids. We discuss some open problems and suggest directions for future work.

</details>

<details>

<summary>2005-07-21 07:46:05 - Hierarchical testing designs for pattern recognition</summary>

- *Gilles Blanchard, Donald Geman*

- `0507421v1` - [abs](http://arxiv.org/abs/0507421v1) - [pdf](http://arxiv.org/pdf/math/0507421v1)

> We explore the theoretical foundations of a ``twenty questions'' approach to pattern recognition. The object of the analysis is the computational process itself rather than probability distributions (Bayesian inference) or decision boundaries (statistical learning). Our formulation is motivated by applications to scene interpretation in which there are a great many possible explanations for the data, one (``background'') is statistically dominant, and it is imperative to restrict intensive computation to genuinely ambiguous regions. The focus here is then on pattern filtering: Given a large set Y of possible patterns or explanations, narrow down the true one Y to a small (random) subset \hat Y\subsetY of ``detected'' patterns to be subjected to further, more intense, processing. To this end, we consider a family of hypothesis tests for Y\in A versus the nonspecific alternatives Y\in A^c. Each test has null type I error and the candidate sets A\subsetY are arranged in a hierarchy of nested partitions. These tests are then characterized by scope (|A|), power (or type II error) and algorithmic cost. We consider sequential testing strategies in which decisions are made iteratively, based on past outcomes, about which test to perform next and when to stop testing. The set \hat Y is then taken to be the set of patterns that have not been ruled out by the tests performed. The total cost of a strategy is the sum of the ``testing cost'' and the ``postprocessing cost'' (proportional to |\hat Y|) and the corresponding optimization problem is analyzed.

</details>


## 2005-08

<details>

<summary>2005-08-13 02:26:44 - Analysis of a Class of Likelihood Based Continuous Time Stochastic Volatility Models including Ornstein-Uhlenbeck Models in Financial Economics</summary>

- *Lancelot F. James*

- `0503055v3` - [abs](http://arxiv.org/abs/0503055v3) - [pdf](http://arxiv.org/pdf/math/0503055v3)

> In a series of recent papers Barndorff-Nielsen and Shephard introduce an attractive class of continuous time stochastic volatility models for financial assets where the volatility processes are functions of positive Ornstein-Uhlenbeck(OU) processes. This models are known to be substantially more flexible than Gaussian based models. One current problem of this approach is the unavailability of a tractable exact analysis of likelihood based stochastic volatility models for the returns of log prices of stocks.   With this point in mind, the likelihood models of Barndorff-Nielsen and Shephard are viewed as members of a much larger class of models. That is likelihoods based on n conditionally independent Normal random variables whose mean and variance are representable as linear functionals of a common unobserved Poisson random measure. The analysis of these models is facilitated by applying the methods in James (2005, 2002), in particular an Esscher type transform of Poisson random measures; in conjunction with a special case of the Weber-Sonine formula. It is shown that the marginal likelihood may be expressed in terms of a multidimensional Fourier-cosine transform. This yields tractable forms of the likelihood and also allows a full Bayesian posterior analysis of the integrated volatility process. A general formula for the posterior density of the log price given the observed data is derived, which could potentially have applications to option pricing. We extend the models to include leverage effects in section 5. It is shown that inference does not necessarily require simulation of random measures. Rather, classical numerical integration can be used in the most general cases.

</details>

<details>

<summary>2005-08-16 10:47:50 - Bayesian Poisson process partition calculus with an application to Bayesian Lévy moving averages</summary>

- *Lancelot F. James*

- `0508283v1` - [abs](http://arxiv.org/abs/0508283v1) - [pdf](http://arxiv.org/pdf/math/0508283v1)

> This article develops, and describes how to use, results concerning disintegrations of Poisson random measures. These results are fashioned as simple tools that can be tailor-made to address inferential questions arising in a wide range of Bayesian nonparametric and spatial statistical models. The Poisson disintegration method is based on the formal statement of two results concerning a Laplace functional change of measure and a Poisson Palm/Fubini calculus in terms of random partitions of the integers {1,...,n}. The techniques are analogous to, but much more general than, techniques for the Dirichlet process and weighted gamma process developed in [Ann. Statist. 12 (1984) 351-357] and [Ann. Inst. Statist. Math. 41 (1989) 227-245]. In order to illustrate the flexibility of the approach, large classes of random probability measures and random hazards or intensities which can be expressed as functionals of Poisson random measures are described. We describe a unified posterior analysis of classes of discrete random probability which identifies and exploits features common to all these models. The analysis circumvents many of the difficult issues involved in Bayesian nonparametric calculus, including a combinatorial component. This allows one to focus on the unique features of each process which are characterized via real valued functions h. The applicability of the technique is further illustrated by obtaining explicit posterior expressions for L\'evy-Cox moving average processes within the general setting of multiplicative intensity models.

</details>

<details>

<summary>2005-08-17 07:06:30 - On the Bahadur representation of sample quantiles for dependent sequences</summary>

- *Wei Biao Wu*

- `0508313v1` - [abs](http://arxiv.org/abs/0508313v1) - [pdf](http://arxiv.org/pdf/math/0508313v1)

> We establish the Bahadur representation of sample quantiles for linear and some widely used nonlinear processes. Local fluctuations of empirical processes are discussed. Applications to the trimmed and Winsorized means are given. Our results extend previous ones by establishing sharper bounds under milder conditions and thus provide new insight into the theory of empirical processes for dependent random variables.

</details>

<details>

<summary>2005-08-30 05:45:39 - Normalized random measures driven by increasing additive processes</summary>

- *Luis E. Nieto-Barajas, Igor Prunster, Stephen G. Walker*

- `0508592v1` - [abs](http://arxiv.org/abs/0508592v1) - [pdf](http://arxiv.org/pdf/math/0508592v1)

> This paper introduces and studies a new class of nonparametric prior distributions. Random probability distribution functions are constructed via normalization of random measures driven by increasing additive processes. In particular, we present results for the distribution of means under both prior and posterior conditions and, via the use of strategic latent variables, undertake a full Bayesian analysis. Our class of priors includes the well-known and widely used mixture of a Dirichlet process.

</details>

<details>

<summary>2005-08-30 05:48:17 - A Bayesian χ^2 test for goodness-of-fit</summary>

- *Valen E. Johnson*

- `0508593v1` - [abs](http://arxiv.org/abs/0508593v1) - [pdf](http://arxiv.org/pdf/math/0508593v1)

> This article describes an extension of classical \chi^2 goodness-of-fit tests to Bayesian model assessment. The extension, which essentially involves evaluating Pearson's goodness-of-fit statistic at a parameter value drawn from its posterior distribution, has the important property that it is asymptotically distributed as a \chi^2 random variable on K-1 degrees of freedom, independently of the dimension of the underlying parameter vector. By examining the posterior distribution of this statistic, global goodness-of-fit diagnostics are obtained. Advantages of these diagnostics include ease of interpretation, computational convenience and favorable power properties. The proposed diagnostics can be used to assess the adequacy of a broad class of Bayesian models, essentially requiring only a finite-dimensional parameter vector and conditionally independent observations.

</details>

<details>

<summary>2005-08-30 06:00:42 - Central limit theorem for sequential Monte Carlo methods and its application to Bayesian inference</summary>

- *Nicolas Chopin*

- `0508594v1` - [abs](http://arxiv.org/abs/0508594v1) - [pdf](http://arxiv.org/pdf/math/0508594v1)

> The term ``sequential Monte Carlo methods'' or, equivalently, ``particle filters,'' refers to a general class of iterative algorithms that performs Monte Carlo approximations of a given sequence of distributions of interest (\pi_t). We establish in this paper a central limit theorem for the Monte Carlo estimates produced by these computational methods. This result holds under minimal assumptions on the distributions \pi_t, and applies in a general framework which encompasses most of the sequential Monte Carlo methods that have been considered in the literature, including the resample-move algorithm of Gilks and Berzuini [J. R. Stat. Soc. Ser. B Stat. Methodol. 63 (2001) 127-146] and the residual resampling scheme. The corresponding asymptotic variances provide a convenient measurement of the precision of a given particle filter. We study, in particular, in some typical examples of Bayesian applications, whether and at which rate these asymptotic variances diverge in time, in order to assess the long term reliability of the considered algorithm.

</details>

<details>

<summary>2005-08-30 06:27:05 - Bayesian-motivated tests of function fit and their asymptotic frequentist properties</summary>

- *Marc Aerts, Gerda Claeskens, Jeffrey D. Hart*

- `0508601v1` - [abs](http://arxiv.org/abs/0508601v1) - [pdf](http://arxiv.org/pdf/math/0508601v1)

> We propose and analyze nonparametric tests of the null hypothesis that a function belongs to a specified parametric family. The tests are based on BIC approximations, \pi_{BIC}, to the posterior probability of the null model, and may be carried out in either Bayesian or frequentist fashion. We obtain results on the asymptotic distribution of \pi_{BIC} under both the null hypothesis and local alternatives. One version of \pi_{BIC}, call it \pi_{BIC}^*, uses a class of models that are orthogonal to each other and growing in number without bound as sample size, n, tends to infinity. We show that \sqrtn(1-\pi_{BIC}^*) converges in distribution to a stable law under the null hypothesis. We also show that \pi_{BIC}^* can detect local alternatives converging to the null at the rate \sqrt\log n/n. A particularly interesting finding is that the power of the \pi_{BIC}^*-based test is asymptotically equal to that of a test based on the maximum of alternative log-likelihoods. Simulation results and an example involving variable star data illustrate desirable features of the proposed tests.

</details>


## 2005-09

<details>

<summary>2005-09-19 13:13:52 - Precise finite-sample quantiles of the Jarque-Bera adjusted Lagrange multiplier test</summary>

- *Diethelm Wuertz, Helmut G. Katzgraber*

- `0509423v1` - [abs](http://arxiv.org/abs/0509423v1) - [pdf](http://arxiv.org/pdf/math/0509423v1)

> It is well known that the finite-sample null distribution of the Jarque-Bera Lagrange Multiplier (LM) test for normality and its adjusted version (ALM) introduced by Urzua differ considerably from their asymptotic chi^2(2) limit. Here, we present results from Monte Carlo simulations using 10^7 replications which yield very precise numbers for the LM and ALM statistic over a wide range of critical values and sample sizes. This enables a precise implementation of the Jarque-Bera LM and ALM test for finite samples.

</details>


## 2005-10

<details>

<summary>2005-10-20 19:41:18 - Improving Classification When a Class Hierarchy is Available Using a Hierarchy-Based Prior</summary>

- *Babak Shahbaba, Radford M. Neal*

- `0510449v1` - [abs](http://arxiv.org/abs/0510449v1) - [pdf](http://arxiv.org/pdf/math/0510449v1)

> We introduce a new method for building classification models when we have prior knowledge of how the classes can be arranged in a hierarchy, based on how easily they can be distinguished. The new method uses a Bayesian form of the multinomial logit (MNL, a.k.a. ``softmax'') model, with a prior that introduces correlations between the parameters for classes that are nearby in the tree. We compare the performance on simulated data of the new method, the ordinary MNL model, and a model that uses the hierarchy in different way. We also test the new method on a document labelling problem, and find that it performs better than the other methods, particularly when the amount of training data is small.

</details>

<details>

<summary>2005-10-26 12:10:16 - Asymptotic Expansion of the Risk Difference of the Bayesian Spectral Density in the ARMA model</summary>

- *Fuyuhiko Tanaka, Fumiyasu Komaki*

- `0510558v1` - [abs](http://arxiv.org/abs/0510558v1) - [pdf](http://arxiv.org/pdf/math/0510558v1)

> The autoregressive moving average (ARMA) model is one of the most important models in time series analysis.We consider the Bayesian estimation of an unknown spectral density in the ARMA model.In the i.i.d. cases, Komaki showed that Bayesian predictive densities based on a superharmonic prior asymptotically dominate those based on the Jeffreys prior.It is shown by using the asymptotic expansion of the risk difference.We obtain the corresponding result in the ARMA model.

</details>

<details>

<summary>2005-10-28 10:34:06 - Towards Reconciliation between Bayesian and Frequentist Reasoning</summary>

- *Tomaz Podobnik, Tomi Zivko*

- `0510628v1` - [abs](http://arxiv.org/abs/0510628v1) - [pdf](http://arxiv.org/pdf/math/0510628v1)

> A theory of quantitative inference about the parameters of sampling distributions is constructed deductively by following very general rules, referred to as the Cox-Polya-Jaynes Desiderata. The inferences are made in terms of probability distributions that are assigned to the parameters. The Desiderata, focusing primarily on consistency of the plausible reasoning, lead to unique assignments of these probabilities in the case of sampling distributions that are invariant under Lie groups. In the scalar cases, e.g. in the case of inferring a single location or scale parameter, the requirement for logical consistency is equivalent to the requirement for calibration: the consistent probability distributions are automatically also the ones with the exact calibration and vice versa. This equivalence speaks in favour of reconciliation between the Bayesian and Frequentist schools of reasoning.

</details>


## 2005-11

<details>

<summary>2005-11-08 21:37:07 - Estimating Ratios of Normalizing Constants Using Linked Importance Sampling</summary>

- *Radford M. Neal*

- `0511216v1` - [abs](http://arxiv.org/abs/0511216v1) - [pdf](http://arxiv.org/pdf/math/0511216v1)

> Ratios of normalizing constants for two distributions are needed in both Bayesian statistics, where they are used to compare models, and in statistical physics, where they correspond to differences in free energy. Two approaches have long been used to estimate ratios of normalizing constants. The `simple importance sampling' (SIS) or `free energy perturbation' method uses a sample drawn from just one of the two distributions. The `bridge sampling' or `acceptance ratio' estimate can be viewed as the ratio of two SIS estimates involving a bridge distribution. For both methods, difficult problems must be handled by introducing a sequence of intermediate distributions linking the two distributions of interest, with the final ratio of normalizing constants being estimated by the product of estimates of ratios for adjacent distributions in this sequence. Recently, work by Jarzynski, and independently by Neal, has shown how one can view such a product of estimates, each based on simple importance sampling using a single point, as an SIS estimate on an extended state space. This `Annealed Importance Sampling' (AIS) method produces an exactly unbiased estimate for the ratio of normalizing constants even when the Markov transitions used do not reach equilibrium. In this paper, I show how a corresponding `Linked Importance Sampling' (LIS) method can be constructed in which the estimates for individual ratios are similar to bridge sampling estimates. I show empirically that for some problems, LIS estimates are much more accurate than AIS estimates found using the same computation time, although for other problems the two methods have similar performance. Linked sampling methods similar to LIS are useful for other purposes as well.

</details>

<details>

<summary>2005-11-21 04:07:16 - Quantile regression in transformation models</summary>

- *Dorota M. Dabrowska*

- `0511508v1` - [abs](http://arxiv.org/abs/0511508v1) - [pdf](http://arxiv.org/pdf/math/0511508v1)

> Conditional quantiles provide a natural tool for reporting results from regression analyses based on semiparametric transformation models. We consider their estimation and construction of confidence sets in the presence of censoring.

</details>

