# 2001

## TOC

- [2001-06](#2001-06)
- [2001-09](#2001-09)
- [2001-12](#2001-12)

## 2001-06

<details>

<summary>2001-06-20 17:27:08 - Reconstruction of Gray-scale Images</summary>

- *Pablo A. Ferrari, Marco D. Gubitoso, E. Jordao Neves*

- `0003098v2` - [abs](http://arxiv.org/abs/0003098v2) - [pdf](http://arxiv.org/pdf/math/0003098v2)

> Reconstruction of images corrupted by noise is an important problem in Image Analysis. In the standard Bayesian approach the unknown original image is assumed to be a realization of a Markov random field on a finite two dimensional finite region. This image is degraded by some noise, which acts independently in each site of and has the same distribution on all sites. The reconstructed image is the maximum a posteriori (MAP) estimator. When the number of colors is bigger than two the standard a priori measure is the Potts model, but with this choice reconstruction algorithms need exponential growing time in the number of pixels. We propose a different a priori measure that allows to compute the MAP estimator in polinomial time. The algorithm binary decomposes the intensity of each color and reconstructs each component, reducing the problem to the computation of the two-color MAP estimator. The latter is done in polinomial time solving a min-cut max-flow problem in a binary graph as proposed by Greig, Porteous and Seheult (1989).

</details>


## 2001-09

<details>

<summary>2001-09-19 09:12:56 - General Loss Bounds for Universal Sequence Prediction</summary>

- *Marcus Hutter*

- `0101019v2` - [abs](http://arxiv.org/abs/0101019v2) - [pdf](http://arxiv.org/pdf/cs/0101019v2)

> The Bayesian framework is ideally suited for induction problems. The probability of observing $x_t$ at time $t$, given past observations $x_1...x_{t-1}$ can be computed with Bayes' rule if the true distribution $\mu$ of the sequences $x_1x_2x_3...$ is known. The problem, however, is that in many cases one does not even have a reasonable estimate of the true distribution. In order to overcome this problem a universal distribution $\xi$ is defined as a weighted sum of distributions $\mu_i\inM$, where $M$ is any countable set of distributions including $\mu$. This is a generalization of Solomonoff induction, in which $M$ is the set of all enumerable semi-measures. Systems which predict $y_t$, given $x_1...x_{t-1}$ and which receive loss $l_{x_t y_t}$ if $x_t$ is the true next symbol of the sequence are considered. It is proven that using the universal $\xi$ as a prior is nearly as good as using the unknown true distribution $\mu$. Furthermore, games of chance, defined as a sequence of bets, observations, and rewards are studied. The time needed to reach the winning zone is bounded in terms of the relative entropy of $\mu$ and $\xi$. Extensions to arbitrary alphabets, partial and delayed prediction, and more active systems are discussed.

</details>


## 2001-12

<details>

<summary>2001-12-15 12:58:39 - Distribution of Mutual Information</summary>

- *Marcus Hutter*

- `0112019v1` - [abs](http://arxiv.org/abs/0112019v1) - [pdf](http://arxiv.org/pdf/cs/0112019v1)

> The mutual information of two random variables i and j with joint probabilities t_ij is commonly used in learning Bayesian nets as well as in many other fields. The chances t_ij are usually estimated by the empirical sampling frequency n_ij/n leading to a point estimate I(n_ij/n) for the mutual information. To answer questions like "is I(n_ij/n) consistent with zero?" or "what is the probability that the true mutual information is much larger than the point estimate?" one has to go beyond the point estimate. In the Bayesian framework one can answer these questions by utilizing a (second order) prior distribution p(t) comprising prior information about t. From the prior p(t) one can compute the posterior p(t|n), from which the distribution p(I|n) of the mutual information can be calculated. We derive reliable and quickly computable approximations for p(I|n). We concentrate on the mean, variance, skewness, and kurtosis, and non-informative priors. For the mean we also give an exact expression. Numerical issues and the range of validity are discussed.

</details>

